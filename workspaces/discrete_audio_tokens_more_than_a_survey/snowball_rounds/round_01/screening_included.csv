doi,title,abstract,included,decision_source,final_verdict,landing_url,pdf_url,year,source_anchor,source_term,metadata_json,arxiv_id,openalex_id
10.48550/arxiv.2502.06490,Recent Advances in Discrete Speech Tokens: A Review,"The rapid advancement of speech generation technologies in the era of large language models (LLMs) has established discrete speech tokens as a foundational paradigm for speech representation. These tokens, characterized by their discrete, compact, and concise nature, are not only advantageous for efficient transmission and storage, but also inherently compatible with the language modeling framework, enabling seamless integration of speech into text-dominated LLM architectures. Current research categorizes discrete speech tokens into two principal classes: acoustic tokens and semantic tokens, each of which has evolved into a rich research domain characterized by unique design philosophies and methodological approaches. This survey systematically synthesizes the existing taxonomy and recent innovations in discrete speech tokenization, conducts a critical examination of the strengths and limitations of each paradigm, and presents systematic experimental comparisons across token types. Furthermore, we identify persistent challenges in the field and propose potential research directions, aiming to offer actionable insights to inspire future advancements in the development and application of discrete speech tokens.",1,,include (seed_filter),https://arxiv.org/abs/2502.06490v4,https://arxiv.org/pdf/2502.06490v4,2025,discrete speech tokens,offline clustering,"{""arxiv_id"": ""2502.06490"", ""title"": ""Recent Advances in Discrete Speech Tokens: A Review"", ""summary"": ""The rapid advancement of speech generation technologies in the era of large language models (LLMs) has established discrete speech tokens as a foundational paradigm for speech representation. These tokens, characterized by their discrete, compact, and concise nature, are not only advantageous for efficient transmission and storage, but also inherently compatible with the language modeling framework, enabling seamless integration of speech into text-dominated LLM architectures. Current research categorizes discrete speech tokens into two principal classes: acoustic tokens and semantic tokens, each of which has evolved into a rich research domain characterized by unique design philosophies and methodological approaches. This survey systematically synthesizes the existing taxonomy and recent innovations in discrete speech tokenization, conducts a critical examination of the strengths and limitations of each paradigm, and presents systematic experimental comparisons across token types. Furthermore, we identify persistent challenges in the field and propose potential research directions, aiming to offer actionable insights to inspire future advancements in the development and application of discrete speech tokens."", ""authors"": [""Yiwei Guo"", ""Zhihan Li"", ""Hankun Wang"", ""Bohan Li"", ""Chongtian Shao"", ""Hanglei Zhang"", ""Chenpeng Du"", ""Xie Chen"", ""Shujie Liu"", ""Kai Yu""], ""published"": ""2025-02-10T14:08:25Z"", ""updated"": ""2025-12-12T05:18:11Z"", ""categories"": [""eess.AS"", ""cs.AI"", ""cs.MM"", ""cs.SD"", ""eess.SP""], ""pdf_url"": ""https://arxiv.org/pdf/2502.06490v4"", ""landing_url"": ""https://arxiv.org/abs/2502.06490v4"", ""doi"": ""https://doi.org/10.48550/arXiv.2502.06490""}",2502.0649,https://openalex.org/W4407359094
10.1109/icassp.2017.7953141,Personalized Acoustic Modeling by Weakly Supervised Multi-Task Deep Learning using Acoustic Tokens Discovered from Unlabeled Data,"It is well known that recognizers personalized to each user are much more effective than user-independent recognizers. With the popularity of smartphones today, although it is not difficult to collect a large set of audio data for each user, it is difficult to transcribe it. However, it is now possible to automatically discover acoustic tokens from unlabeled personal data in an unsupervised way. We therefore propose a multi-task deep learning framework called a phoneme-token deep neural network (PTDNN), jointly trained from unsupervised acoustic tokens discovered from unlabeled data and very limited transcribed data for personalized acoustic modeling. We term this scenario ""weakly supervised"". The underlying intuition is that the high degree of similarity between the HMM states of acoustic token models and phoneme models may help them learn from each other in this multi-task learning framework. Initial experiments performed over a personalized audio data set recorded from Facebook posts demonstrated that very good improvements can be achieved in both frame accuracy and word accuracy over popularly-considered baselines such as fDLR, speaker code and lightly supervised adaptation. This approach complements existing speaker adaptation approaches and can be used jointly with such techniques to yield improved results.",1,,include (junior:5),https://arxiv.org/abs/1706.07793v1,https://arxiv.org/pdf/1706.07793v1,2017,dblp_title,Personalized acoustic modeling by weakly supervised multi-task deep learning using acoustic tokens discovered from unlabeled data.,"{""arxiv_id"": ""1706.07793"", ""title"": ""Personalized Acoustic Modeling by Weakly Supervised Multi-Task Deep Learning using Acoustic Tokens Discovered from Unlabeled Data"", ""summary"": ""It is well known that recognizers personalized to each user are much more effective than user-independent recognizers. With the popularity of smartphones today, although it is not difficult to collect a large set of audio data for each user, it is difficult to transcribe it. However, it is now possible to automatically discover acoustic tokens from unlabeled personal data in an unsupervised way. We therefore propose a multi-task deep learning framework called a phoneme-token deep neural network (PTDNN), jointly trained from unsupervised acoustic tokens discovered from unlabeled data and very limited transcribed data for personalized acoustic modeling. We term this scenario \""weakly supervised\"". The underlying intuition is that the high degree of similarity between the HMM states of acoustic token models and phoneme models may help them learn from each other in this multi-task learning framework. Initial experiments performed over a personalized audio data set recorded from Facebook posts demonstrated that very good improvements can be achieved in both frame accuracy and word accuracy over popularly-considered baselines such as fDLR, speaker code and lightly supervised adaptation. This approach complements existing speaker adaptation approaches and can be used jointly with such techniques to yield improved results."", ""authors"": [""Cheng-Kuan Wei"", ""Cheng-Tao Chung"", ""Hung-Yi Lee"", ""Lin-Shan Lee""], ""published"": ""2017-06-23T12:54:43Z"", ""updated"": ""2017-06-23T12:54:43Z"", ""categories"": [""cs.SD""], ""pdf_url"": ""https://arxiv.org/pdf/1706.07793v1"", ""landing_url"": ""https://arxiv.org/abs/1706.07793v1"", ""doi"": ""https://doi.org/10.1109/ICASSP.2017.7953141""}",1706.07793,https://openalex.org/W2618500857
10.48550/arxiv.2207.07036,u-HuBERT: Unified Mixed-Modal Speech Pretraining And Zero-Shot Transfer to Unlabeled Modality,"While audio-visual speech models can yield superior performance and robustness compared to audio-only models, their development and adoption are hindered by the lack of labeled and unlabeled audio-visual data and the cost to deploy one model per modality. In this paper, we present u-HuBERT, a self-supervised pre-training framework that can leverage both multimodal and unimodal speech with a unified masked cluster prediction objective. By utilizing modality dropout during pre-training, we demonstrate that a single fine-tuned model can achieve performance on par or better than the state-of-the-art modality-specific models. Moreover, our model fine-tuned only on audio can perform well with audio-visual and visual speech input, achieving zero-shot modality generalization for multiple speech processing tasks. In particular, our single model yields 1.2%/1.4%/27.2% speech recognition word error rate on LRS3 with audio-visual/audio/visual input. Codes and models are available at https://github.com/facebookresearch/av_hubert",1,,include (junior:5),https://arxiv.org/abs/2207.07036v2,https://arxiv.org/pdf/2207.07036v2,2022,discrete speech tokens,hubert,"{""arxiv_id"": ""2207.07036"", ""title"": ""u-HuBERT: Unified Mixed-Modal Speech Pretraining And Zero-Shot Transfer to Unlabeled Modality"", ""summary"": ""While audio-visual speech models can yield superior performance and robustness compared to audio-only models, their development and adoption are hindered by the lack of labeled and unlabeled audio-visual data and the cost to deploy one model per modality. In this paper, we present u-HuBERT, a self-supervised pre-training framework that can leverage both multimodal and unimodal speech with a unified masked cluster prediction objective. By utilizing modality dropout during pre-training, we demonstrate that a single fine-tuned model can achieve performance on par or better than the state-of-the-art modality-specific models. Moreover, our model fine-tuned only on audio can perform well with audio-visual and visual speech input, achieving zero-shot modality generalization for multiple speech processing tasks. In particular, our single model yields 1.2%/1.4%/27.2% speech recognition word error rate on LRS3 with audio-visual/audio/visual input. Codes and models are available at https://github.com/facebookresearch/av_hubert"", ""authors"": [""Wei-Ning Hsu"", ""Bowen Shi""], ""published"": ""2022-07-14T16:21:33Z"", ""updated"": ""2022-11-28T03:12:14Z"", ""categories"": [""cs.CL"", ""cs.AI"", ""cs.CV"", ""cs.SD"", ""eess.AS"", ""eess.IV""], ""pdf_url"": ""https://arxiv.org/pdf/2207.07036v2"", ""landing_url"": ""https://arxiv.org/abs/2207.07036v2"", ""doi"": ""https://doi.org/10.48550/arXiv.2207.07036""}",2207.07036,https://openalex.org/W4285595742
10.48550/arxiv.2210.16755,token2vec: A Joint Self-Supervised Pre-training Framework Using Unpaired Speech and Text,"Self-supervised pre-training has been successful in both text and speech processing. Speech and text offer different but complementary information. The question is whether we are able to perform a speech-text joint pre-training on unpaired speech and text. In this paper, we take the idea of self-supervised pre-training one step further and propose token2vec, a novel joint pre-training framework for unpaired speech and text based on discrete representations of speech. Firstly, due to the distinct characteristics between speech and text modalities, where speech is continuous while text is discrete, we first discretize speech into a sequence of discrete speech tokens to solve the modality mismatch problem. Secondly, to solve the length mismatch problem, where the speech sequence is usually much longer than text sequence, we convert the words of text into phoneme sequences and randomly repeat each phoneme in the sequences. Finally, we feed the discrete speech and text tokens into a modality-agnostic Transformer encoder and pre-train with token-level masking language modeling (tMLM). Experiments show that token2vec is significantly superior to various speech-only pre-training baselines, with up to 17.7% relative WER reduction. Token2vec model is also validated on a non-ASR task, i.e., spoken intent classification, and shows good transferability.",1,,include (junior:5),https://arxiv.org/abs/2210.16755v1,https://arxiv.org/pdf/2210.16755v1,2022,discrete speech tokens,offline clustering,"{""arxiv_id"": ""2210.16755"", ""title"": ""token2vec: A Joint Self-Supervised Pre-training Framework Using Unpaired Speech and Text"", ""summary"": ""Self-supervised pre-training has been successful in both text and speech processing. Speech and text offer different but complementary information. The question is whether we are able to perform a speech-text joint pre-training on unpaired speech and text. In this paper, we take the idea of self-supervised pre-training one step further and propose token2vec, a novel joint pre-training framework for unpaired speech and text based on discrete representations of speech. Firstly, due to the distinct characteristics between speech and text modalities, where speech is continuous while text is discrete, we first discretize speech into a sequence of discrete speech tokens to solve the modality mismatch problem. Secondly, to solve the length mismatch problem, where the speech sequence is usually much longer than text sequence, we convert the words of text into phoneme sequences and randomly repeat each phoneme in the sequences. Finally, we feed the discrete speech and text tokens into a modality-agnostic Transformer encoder and pre-train with token-level masking language modeling (tMLM). Experiments show that token2vec is significantly superior to various speech-only pre-training baselines, with up to 17.7% relative WER reduction. Token2vec model is also validated on a non-ASR task, i.e., spoken intent classification, and shows good transferability."", ""authors"": [""Xianghu Yue"", ""Junyi Ao"", ""Xiaoxue Gao"", ""Haizhou Li""], ""published"": ""2022-10-30T06:38:19Z"", ""updated"": ""2022-10-30T06:38:19Z"", ""categories"": [""cs.CL"", ""cs.SD"", ""eess.AS""], ""pdf_url"": ""https://arxiv.org/pdf/2210.16755v1"", ""landing_url"": ""https://arxiv.org/abs/2210.16755v1"", ""doi"": ""https://doi.org/10.48550/arXiv.2210.16755""}",2210.16755,https://openalex.org/W4307937713
10.48550/arxiv.2212.09058,BEATs: Audio Pre-Training with Acoustic Tokenizers,"The massive growth of self-supervised learning (SSL) has been witnessed in language, vision, speech, and audio domains over the past few years. While discrete label prediction is widely adopted for other modalities, the state-of-the-art audio SSL models still employ reconstruction loss for pre-training. Compared with reconstruction loss, semantic-rich discrete label prediction encourages the SSL model to abstract the high-level audio semantics and discard the redundant details as in human perception. However, a semantic-rich acoustic tokenizer for general audio pre-training is usually not straightforward to obtain, due to the continuous property of audio and unavailable phoneme sequences like speech. To tackle this challenge, we propose BEATs, an iterative audio pre-training framework to learn Bidirectional Encoder representation from Audio Transformers, where an acoustic tokenizer and an audio SSL model are optimized by iterations. In the first iteration, we use random projection as the acoustic tokenizer to train an audio SSL model in a mask and label prediction manner. Then, we train an acoustic tokenizer for the next iteration by distilling the semantic knowledge from the pre-trained or fine-tuned audio SSL model. The iteration is repeated with the hope of mutual promotion of the acoustic tokenizer and audio SSL model. The experimental results demonstrate our acoustic tokenizers can generate discrete labels with rich audio semantics and our audio SSL models achieve state-of-the-art results across various audio classification benchmarks, even outperforming previous models that use more training data and model parameters significantly. Specifically, we set a new state-of-the-art mAP 50.6% on AudioSet-2M for audio-only models without using any external data, and 98.1% accuracy on ESC-50. The code and pre-trained models are available at https://aka.ms/beats.",1,,include (junior:5),https://arxiv.org/abs/2212.09058v1,https://arxiv.org/pdf/2212.09058v1,2022,discrete speech tokens,acoustic tokens,"{""arxiv_id"": ""2212.09058"", ""title"": ""BEATs: Audio Pre-Training with Acoustic Tokenizers"", ""summary"": ""The massive growth of self-supervised learning (SSL) has been witnessed in language, vision, speech, and audio domains over the past few years. While discrete label prediction is widely adopted for other modalities, the state-of-the-art audio SSL models still employ reconstruction loss for pre-training. Compared with reconstruction loss, semantic-rich discrete label prediction encourages the SSL model to abstract the high-level audio semantics and discard the redundant details as in human perception. However, a semantic-rich acoustic tokenizer for general audio pre-training is usually not straightforward to obtain, due to the continuous property of audio and unavailable phoneme sequences like speech. To tackle this challenge, we propose BEATs, an iterative audio pre-training framework to learn Bidirectional Encoder representation from Audio Transformers, where an acoustic tokenizer and an audio SSL model are optimized by iterations. In the first iteration, we use random projection as the acoustic tokenizer to train an audio SSL model in a mask and label prediction manner. Then, we train an acoustic tokenizer for the next iteration by distilling the semantic knowledge from the pre-trained or fine-tuned audio SSL model. The iteration is repeated with the hope of mutual promotion of the acoustic tokenizer and audio SSL model. The experimental results demonstrate our acoustic tokenizers can generate discrete labels with rich audio semantics and our audio SSL models achieve state-of-the-art results across various audio classification benchmarks, even outperforming previous models that use more training data and model parameters significantly. Specifically, we set a new state-of-the-art mAP 50.6% on AudioSet-2M for audio-only models without using any external data, and 98.1% accuracy on ESC-50. The code and pre-trained models are available at https://aka.ms/beats."", ""authors"": [""Sanyuan Chen"", ""Yu Wu"", ""Chengyi Wang"", ""Shujie Liu"", ""Daniel Tompkins"", ""Zhuo Chen"", ""Furu Wei""], ""published"": ""2022-12-18T10:41:55Z"", ""updated"": ""2022-12-18T10:41:55Z"", ""categories"": [""eess.AS"", ""cs.AI"", ""cs.CL"", ""cs.LG"", ""cs.SD""], ""pdf_url"": ""https://arxiv.org/pdf/2212.09058v1"", ""landing_url"": ""https://arxiv.org/abs/2212.09058v1"", ""doi"": ""https://doi.org/10.48550/arXiv.2212.09058""}",2212.09058,https://openalex.org/W4312048190
10.48550/arxiv.2301.02111,Neural Codec Language Models are Zero-Shot Text to Speech Synthesizers,"We introduce a language modeling approach for text to speech synthesis (TTS). Specifically, we train a neural codec language model (called Vall-E) using discrete codes derived from an off-the-shelf neural audio codec model, and regard TTS as a conditional language modeling task rather than continuous signal regression as in previous work. During the pre-training stage, we scale up the TTS training data to 60K hours of English speech which is hundreds of times larger than existing systems. Vall-E emerges in-context learning capabilities and can be used to synthesize high-quality personalized speech with only a 3-second enrolled recording of an unseen speaker as an acoustic prompt. Experiment results show that Vall-E significantly outperforms the state-of-the-art zero-shot TTS system in terms of speech naturalness and speaker similarity. In addition, we find Vall-E could preserve the speaker's emotion and acoustic environment of the acoustic prompt in synthesis. See https://aka.ms/valle for demos of our work.",1,,include (junior:5),https://arxiv.org/abs/2301.02111v1,https://arxiv.org/pdf/2301.02111v1,2023,discrete speech tokens,neural codec,"{""arxiv_id"": ""2301.02111"", ""title"": ""Neural Codec Language Models are Zero-Shot Text to Speech Synthesizers"", ""summary"": ""We introduce a language modeling approach for text to speech synthesis (TTS). Specifically, we train a neural codec language model (called Vall-E) using discrete codes derived from an off-the-shelf neural audio codec model, and regard TTS as a conditional language modeling task rather than continuous signal regression as in previous work. During the pre-training stage, we scale up the TTS training data to 60K hours of English speech which is hundreds of times larger than existing systems. Vall-E emerges in-context learning capabilities and can be used to synthesize high-quality personalized speech with only a 3-second enrolled recording of an unseen speaker as an acoustic prompt. Experiment results show that Vall-E significantly outperforms the state-of-the-art zero-shot TTS system in terms of speech naturalness and speaker similarity. In addition, we find Vall-E could preserve the speaker's emotion and acoustic environment of the acoustic prompt in synthesis. See https://aka.ms/valle for demos of our work."", ""authors"": [""Chengyi Wang"", ""Sanyuan Chen"", ""Yu Wu"", ""Ziqiang Zhang"", ""Long Zhou"", ""Shujie Liu"", ""Zhuo Chen"", ""Yanqing Liu"", ""Huaming Wang"", ""Jinyu Li"", ""Lei He"", ""Sheng Zhao"", ""Furu Wei""], ""published"": ""2023-01-05T15:37:15Z"", ""updated"": ""2023-01-05T15:37:15Z"", ""categories"": [""cs.CL"", ""cs.SD"", ""eess.AS""], ""pdf_url"": ""https://arxiv.org/pdf/2301.02111v1"", ""landing_url"": ""https://arxiv.org/abs/2301.02111v1"", ""doi"": ""https://doi.org/10.48550/arXiv.2301.02111""}",2301.02111,https://openalex.org/W4313679638
10.48550/arxiv.2301.13662,InstructTTS: Modelling Expressive TTS in Discrete Latent Space with Natural Language Style Prompt,"Expressive text-to-speech (TTS) aims to synthesize different speaking style speech according to human's demands. Nowadays, there are two common ways to control speaking styles: (1) Pre-defining a group of speaking style and using categorical index to denote different speaking style. However, there are limitations in the diversity of expressiveness, as these models can only generate the pre-defined styles. (2) Using reference speech as style input, which results in a problem that the extracted style information is not intuitive or interpretable. In this study, we attempt to use natural language as style prompt to control the styles in the synthetic speech, e.g., ""Sigh tone in full of sad mood with some helpless feeling"". Considering that there is no existing TTS corpus which is proper to benchmark this novel task, we first construct a speech corpus, whose speech samples are annotated with not only content transcriptions but also style descriptions in natural language. Then we propose an expressive TTS model, named as InstructTTS, which is novel in the sense of following aspects: (1) We fully take the advantage of self-supervised learning and cross-modal metric learning, and propose a novel three-stage training procedure to obtain a robust sentence embedding model, which can effectively capture semantic information from the style prompts and control the speaking style in the generated speech. (2) We propose to model acoustic features in discrete latent space and train a novel discrete diffusion probabilistic model to generate vector-quantized (VQ) acoustic tokens rather than the commonly-used mel spectrogram. (3) We jointly apply mutual information (MI) estimation and minimization during acoustic model training to minimize style-speaker and style-content MI, avoiding possible content and speaker information leakage from the style prompt.",1,,include (junior:5),https://arxiv.org/abs/2301.13662v2,https://arxiv.org/pdf/2301.13662v2,2023,discrete speech tokens,acoustic tokens,"{""arxiv_id"": ""2301.13662"", ""title"": ""InstructTTS: Modelling Expressive TTS in Discrete Latent Space with Natural Language Style Prompt"", ""summary"": ""Expressive text-to-speech (TTS) aims to synthesize different speaking style speech according to human's demands. Nowadays, there are two common ways to control speaking styles: (1) Pre-defining a group of speaking style and using categorical index to denote different speaking style. However, there are limitations in the diversity of expressiveness, as these models can only generate the pre-defined styles. (2) Using reference speech as style input, which results in a problem that the extracted style information is not intuitive or interpretable. In this study, we attempt to use natural language as style prompt to control the styles in the synthetic speech, e.g., \""Sigh tone in full of sad mood with some helpless feeling\"". Considering that there is no existing TTS corpus which is proper to benchmark this novel task, we first construct a speech corpus, whose speech samples are annotated with not only content transcriptions but also style descriptions in natural language. Then we propose an expressive TTS model, named as InstructTTS, which is novel in the sense of following aspects: (1) We fully take the advantage of self-supervised learning and cross-modal metric learning, and propose a novel three-stage training procedure to obtain a robust sentence embedding model, which can effectively capture semantic information from the style prompts and control the speaking style in the generated speech. (2) We propose to model acoustic features in discrete latent space and train a novel discrete diffusion probabilistic model to generate vector-quantized (VQ) acoustic tokens rather than the commonly-used mel spectrogram. (3) We jointly apply mutual information (MI) estimation and minimization during acoustic model training to minimize style-speaker and style-content MI, avoiding possible content and speaker information leakage from the style prompt."", ""authors"": [""Dongchao Yang"", ""Songxiang Liu"", ""Rongjie Huang"", ""Chao Weng"", ""Helen Meng""], ""published"": ""2023-01-31T14:26:52Z"", ""updated"": ""2023-06-25T11:42:52Z"", ""categories"": [""cs.SD"", ""eess.AS""], ""pdf_url"": ""https://arxiv.org/pdf/2301.13662v2"", ""landing_url"": ""https://arxiv.org/abs/2301.13662v2"", ""doi"": ""https://doi.org/10.48550/arXiv.2301.13662""}",2301.13662,https://openalex.org/W4318907788
10.48550/arxiv.2302.03540,"Speak, Read and Prompt: High-Fidelity Text-to-Speech with Minimal Supervision","We introduce SPEAR-TTS, a multi-speaker text-to-speech (TTS) system that can be trained with minimal supervision. By combining two types of discrete speech representations, we cast TTS as a composition of two sequence-to-sequence tasks: from text to high-level semantic tokens (akin to ""reading"") and from semantic tokens to low-level acoustic tokens (""speaking""). Decoupling these two tasks enables training of the ""speaking"" module using abundant audio-only data, and unlocks the highly efficient combination of pretraining and backtranslation to reduce the need for parallel data when training the ""reading"" component. To control the speaker identity, we adopt example prompting, which allows SPEAR-TTS to generalize to unseen speakers using only a short sample of 3 seconds, without any explicit speaker representation or speaker-id labels. Our experiments demonstrate that SPEAR-TTS achieves a character error rate that is competitive with state-of-the-art methods using only 15 minutes of parallel data, while matching ground-truth speech in terms of naturalness and acoustic quality, as measured in subjective tests.",1,,include (junior:5),https://arxiv.org/abs/2302.03540v1,https://arxiv.org/pdf/2302.03540v1,2023,discrete speech tokens,acoustic tokens,"{""arxiv_id"": ""2302.03540"", ""title"": ""Speak, Read and Prompt: High-Fidelity Text-to-Speech with Minimal Supervision"", ""summary"": ""We introduce SPEAR-TTS, a multi-speaker text-to-speech (TTS) system that can be trained with minimal supervision. By combining two types of discrete speech representations, we cast TTS as a composition of two sequence-to-sequence tasks: from text to high-level semantic tokens (akin to \""reading\"") and from semantic tokens to low-level acoustic tokens (\""speaking\""). Decoupling these two tasks enables training of the \""speaking\"" module using abundant audio-only data, and unlocks the highly efficient combination of pretraining and backtranslation to reduce the need for parallel data when training the \""reading\"" component. To control the speaker identity, we adopt example prompting, which allows SPEAR-TTS to generalize to unseen speakers using only a short sample of 3 seconds, without any explicit speaker representation or speaker-id labels. Our experiments demonstrate that SPEAR-TTS achieves a character error rate that is competitive with state-of-the-art methods using only 15 minutes of parallel data, while matching ground-truth speech in terms of naturalness and acoustic quality, as measured in subjective tests."", ""authors"": [""Eugene Kharitonov"", ""Damien Vincent"", ""Zalán Borsos"", ""Raphaël Marinier"", ""Sertan Girgin"", ""Olivier Pietquin"", ""Matt Sharifi"", ""Marco Tagliasacchi"", ""Neil Zeghidour""], ""published"": ""2023-02-07T15:48:31Z"", ""updated"": ""2023-02-07T15:48:31Z"", ""categories"": [""cs.SD"", ""eess.AS""], ""pdf_url"": ""https://arxiv.org/pdf/2302.03540v1"", ""landing_url"": ""https://arxiv.org/abs/2302.03540v1"", ""doi"": ""https://doi.org/10.48550/arXiv.2302.03540""}",2302.0354,https://openalex.org/W4319653946
10.48550/arxiv.2302.08342,Speech Enhancement with Multi-granularity Vector Quantization,"With advances in deep learning, neural network based speech enhancement (SE) has developed rapidly in the last decade. Meanwhile, the self-supervised pre-trained model and vector quantization (VQ) have achieved excellent performance on many speech-related tasks, while they are less explored on SE. As it was shown in our previous work that utilizing a VQ module to discretize noisy speech representations is beneficial for speech denoising, in this work we therefore study the impact of using VQ at different layers with different number of codebooks. Different VQ modules indeed enable to extract multiple-granularity speech features. Following an attention mechanism, the contextual features extracted by a pre-trained model are fused with the local features extracted by the encoder, such that both global and local information are preserved to reconstruct the enhanced speech. Experimental results on the Valentini dataset show that the proposed model can improve the SE performance, where the impact of choosing pre-trained models is also revealed.",1,,include (junior:5),https://arxiv.org/abs/2302.08342v1,https://arxiv.org/pdf/2302.08342v1,2023,discrete speech tokens,vector quantization,"{""arxiv_id"": ""2302.08342"", ""title"": ""Speech Enhancement with Multi-granularity Vector Quantization"", ""summary"": ""With advances in deep learning, neural network based speech enhancement (SE) has developed rapidly in the last decade. Meanwhile, the self-supervised pre-trained model and vector quantization (VQ) have achieved excellent performance on many speech-related tasks, while they are less explored on SE. As it was shown in our previous work that utilizing a VQ module to discretize noisy speech representations is beneficial for speech denoising, in this work we therefore study the impact of using VQ at different layers with different number of codebooks. Different VQ modules indeed enable to extract multiple-granularity speech features. Following an attention mechanism, the contextual features extracted by a pre-trained model are fused with the local features extracted by the encoder, such that both global and local information are preserved to reconstruct the enhanced speech. Experimental results on the Valentini dataset show that the proposed model can improve the SE performance, where the impact of choosing pre-trained models is also revealed."", ""authors"": [""Xiao-Ying Zhao"", ""Qiu-Shi Zhu"", ""Jie Zhang""], ""published"": ""2023-02-16T14:53:41Z"", ""updated"": ""2023-02-16T14:53:41Z"", ""categories"": [""eess.AS"", ""cs.SD""], ""pdf_url"": ""https://arxiv.org/pdf/2302.08342v1"", ""landing_url"": ""https://arxiv.org/abs/2302.08342v1"", ""doi"": ""https://doi.org/10.48550/arXiv.2302.08342""}",2302.08342,https://openalex.org/W4321277156
10.48550/arxiv.2303.02939,FoundationTTS: Text-to-Speech for ASR Customization with Generative Language Model,"Neural text-to-speech (TTS) generally consists of cascaded architecture with separately optimized acoustic model and vocoder, or end-to-end architecture with continuous mel-spectrograms or self-extracted speech frames as the intermediate representations to bridge acoustic model and vocoder, which suffers from two limitations: 1) the continuous acoustic frames are hard to predict with phoneme only, and acoustic information like duration or pitch is also needed to solve the one-to-many problem, which is not easy to scale on large scale and noise datasets; 2) to achieve diverse speech output based on continuous speech features, complex VAE or flow-based models are usually required. In this paper, we propose FoundationTTS, a new speech synthesis system with a neural audio codec for discrete speech token extraction and waveform reconstruction and a large language model for discrete token generation from linguistic (phoneme) tokens. Specifically, 1) we propose a hierarchical codec network based on vector-quantized auto-encoders with adversarial training (VQ-GAN), which first extracts continuous frame-level speech representations with fine-grained codec, and extracts a discrete token from each continuous speech frame with coarse-grained codec; 2) we jointly optimize speech token, linguistic tokens, speaker token together with a large language model and predict the discrete speech tokens autoregressively. Experiments show that FoundationTTS achieves a MOS gain of +0.14 compared to the baseline system. In ASR customization tasks, our method achieves 7.09\% and 10.35\% WERR respectively over two strong customized ASR baselines.",1,,include (junior:5),https://arxiv.org/abs/2303.02939v3,https://arxiv.org/pdf/2303.02939v3,2023,discrete speech tokens,offline clustering,"{""arxiv_id"": ""2303.02939"", ""title"": ""FoundationTTS: Text-to-Speech for ASR Customization with Generative Language Model"", ""summary"": ""Neural text-to-speech (TTS) generally consists of cascaded architecture with separately optimized acoustic model and vocoder, or end-to-end architecture with continuous mel-spectrograms or self-extracted speech frames as the intermediate representations to bridge acoustic model and vocoder, which suffers from two limitations: 1) the continuous acoustic frames are hard to predict with phoneme only, and acoustic information like duration or pitch is also needed to solve the one-to-many problem, which is not easy to scale on large scale and noise datasets; 2) to achieve diverse speech output based on continuous speech features, complex VAE or flow-based models are usually required. In this paper, we propose FoundationTTS, a new speech synthesis system with a neural audio codec for discrete speech token extraction and waveform reconstruction and a large language model for discrete token generation from linguistic (phoneme) tokens. Specifically, 1) we propose a hierarchical codec network based on vector-quantized auto-encoders with adversarial training (VQ-GAN), which first extracts continuous frame-level speech representations with fine-grained codec, and extracts a discrete token from each continuous speech frame with coarse-grained codec; 2) we jointly optimize speech token, linguistic tokens, speaker token together with a large language model and predict the discrete speech tokens autoregressively. Experiments show that FoundationTTS achieves a MOS gain of +0.14 compared to the baseline system. In ASR customization tasks, our method achieves 7.09\\% and 10.35\\% WERR respectively over two strong customized ASR baselines."", ""authors"": [""Ruiqing Xue"", ""Yanqing Liu"", ""Lei He"", ""Xu Tan"", ""Linquan Liu"", ""Edward Lin"", ""Sheng Zhao""], ""published"": ""2023-03-06T07:17:15Z"", ""updated"": ""2023-03-08T03:06:47Z"", ""categories"": [""eess.AS"", ""cs.SD""], ""pdf_url"": ""https://arxiv.org/pdf/2303.02939v3"", ""landing_url"": ""https://arxiv.org/abs/2303.02939v3"", ""doi"": ""https://doi.org/10.48550/arXiv.2303.02939""}",2303.02939,https://openalex.org/W4323697273
10.48550/arxiv.2303.03926,Speak Foreign Languages with Your Own Voice: Cross-Lingual Neural Codec Language Modeling,"We propose a cross-lingual neural codec language model, VALL-E X, for cross-lingual speech synthesis. Specifically, we extend VALL-E and train a multi-lingual conditional codec language model to predict the acoustic token sequences of the target language speech by using both the source language speech and the target language text as prompts. VALL-E X inherits strong in-context learning capabilities and can be applied for zero-shot cross-lingual text-to-speech synthesis and zero-shot speech-to-speech translation tasks. Experimental results show that it can generate high-quality speech in the target language via just one speech utterance in the source language as a prompt while preserving the unseen speaker's voice, emotion, and acoustic environment. Moreover, VALL-E X effectively alleviates the foreign accent problems, which can be controlled by a language ID. Audio samples are available at \url{https://aka.ms/vallex}.",1,,include (junior:5),https://arxiv.org/abs/2303.03926v1,https://arxiv.org/pdf/2303.03926v1,2023,discrete speech tokens,acoustic tokens,"{""arxiv_id"": ""2303.03926"", ""title"": ""Speak Foreign Languages with Your Own Voice: Cross-Lingual Neural Codec Language Modeling"", ""summary"": ""We propose a cross-lingual neural codec language model, VALL-E X, for cross-lingual speech synthesis. Specifically, we extend VALL-E and train a multi-lingual conditional codec language model to predict the acoustic token sequences of the target language speech by using both the source language speech and the target language text as prompts. VALL-E X inherits strong in-context learning capabilities and can be applied for zero-shot cross-lingual text-to-speech synthesis and zero-shot speech-to-speech translation tasks. Experimental results show that it can generate high-quality speech in the target language via just one speech utterance in the source language as a prompt while preserving the unseen speaker's voice, emotion, and acoustic environment. Moreover, VALL-E X effectively alleviates the foreign accent problems, which can be controlled by a language ID. Audio samples are available at \\url{https://aka.ms/vallex}."", ""authors"": [""Ziqiang Zhang"", ""Long Zhou"", ""Chengyi Wang"", ""Sanyuan Chen"", ""Yu Wu"", ""Shujie Liu"", ""Zhuo Chen"", ""Yanqing Liu"", ""Huaming Wang"", ""Jinyu Li"", ""Lei He"", ""Sheng Zhao"", ""Furu Wei""], ""published"": ""2023-03-07T14:31:55Z"", ""updated"": ""2023-03-07T14:31:55Z"", ""categories"": [""cs.CL"", ""cs.AI"", ""cs.SD"", ""eess.AS""], ""pdf_url"": ""https://arxiv.org/pdf/2303.03926v1"", ""landing_url"": ""https://arxiv.org/abs/2303.03926v1"", ""doi"": ""https://doi.org/10.48550/arXiv.2303.03926""}",2303.03926,https://openalex.org/W4323651091
10.48550/arxiv.2303.11131,Cocktail HuBERT: Generalized Self-Supervised Pre-training for Mixture and Single-Source Speech,"Self-supervised learning leverages unlabeled data effectively, improving label efficiency and generalization to domains without labeled data. While recent work has studied generalization to more acoustic/linguistic domains, languages, and modalities, these investigations are limited to single-source speech with one primary speaker in the recording. This paper presents Cocktail HuBERT, a self-supervised learning framework that generalizes to mixture speech using a masked pseudo source separation objective. This objective encourages the model to identify the number of sources, separate and understand the context, and infer the content of masked regions represented as discovered units. Cocktail HuBERT outperforms state-of-the-art results with 69% lower WER on multi-speaker ASR, 31% lower DER on diarization, and is competitive on single- and multi-speaker tasks from SUPERB.",1,,include (junior:5),https://arxiv.org/abs/2303.11131v1,https://arxiv.org/pdf/2303.11131v1,2023,discrete speech tokens,hubert,"{""arxiv_id"": ""2303.11131"", ""title"": ""Cocktail HuBERT: Generalized Self-Supervised Pre-training for Mixture and Single-Source Speech"", ""summary"": ""Self-supervised learning leverages unlabeled data effectively, improving label efficiency and generalization to domains without labeled data. While recent work has studied generalization to more acoustic/linguistic domains, languages, and modalities, these investigations are limited to single-source speech with one primary speaker in the recording. This paper presents Cocktail HuBERT, a self-supervised learning framework that generalizes to mixture speech using a masked pseudo source separation objective. This objective encourages the model to identify the number of sources, separate and understand the context, and infer the content of masked regions represented as discovered units. Cocktail HuBERT outperforms state-of-the-art results with 69% lower WER on multi-speaker ASR, 31% lower DER on diarization, and is competitive on single- and multi-speaker tasks from SUPERB."", ""authors"": [""Maryam Fazel-Zarandi"", ""Wei-Ning Hsu""], ""published"": ""2023-03-20T14:07:13Z"", ""updated"": ""2023-03-20T14:07:13Z"", ""categories"": [""cs.CL"", ""cs.LG"", ""cs.SD"", ""eess.AS""], ""pdf_url"": ""https://arxiv.org/pdf/2303.11131v1"", ""landing_url"": ""https://arxiv.org/abs/2303.11131v1"", ""doi"": ""https://doi.org/10.48550/arXiv.2303.11131""}",2303.11131,https://openalex.org/W4353007424
10.48550/arxiv.2304.03940,Unsupervised Speech Representation Pooling Using Vector Quantization,"With the advent of general-purpose speech representations from large-scale self-supervised models, applying a single model to multiple downstream tasks is becoming a de-facto approach. However, the pooling problem remains; the length of speech representations is inherently variable. The naive average pooling is often used, even though it ignores the characteristics of speech, such as differently lengthed phonemes. Hence, we design a novel pooling method to squash acoustically similar representations via vector quantization, which does not require additional training, unlike attention-based pooling. Further, we evaluate various unsupervised pooling methods on various self-supervised models. We gather diverse methods scattered around speech and text to evaluate on various tasks: keyword spotting, speaker identification, intent classification, and emotion recognition. Finally, we quantitatively and qualitatively analyze our method, comparing it with supervised pooling methods.",1,,include (junior:4),https://arxiv.org/abs/2304.03940v1,https://arxiv.org/pdf/2304.03940v1,2023,discrete speech tokens,vector quantization,"{""arxiv_id"": ""2304.03940"", ""title"": ""Unsupervised Speech Representation Pooling Using Vector Quantization"", ""summary"": ""With the advent of general-purpose speech representations from large-scale self-supervised models, applying a single model to multiple downstream tasks is becoming a de-facto approach. However, the pooling problem remains; the length of speech representations is inherently variable. The naive average pooling is often used, even though it ignores the characteristics of speech, such as differently lengthed phonemes. Hence, we design a novel pooling method to squash acoustically similar representations via vector quantization, which does not require additional training, unlike attention-based pooling. Further, we evaluate various unsupervised pooling methods on various self-supervised models. We gather diverse methods scattered around speech and text to evaluate on various tasks: keyword spotting, speaker identification, intent classification, and emotion recognition. Finally, we quantitatively and qualitatively analyze our method, comparing it with supervised pooling methods."", ""authors"": [""Jeongkyun Park"", ""Kwanghee Choi"", ""Hyunjun Heo"", ""Hyung-Min Park""], ""published"": ""2023-04-08T07:03:01Z"", ""updated"": ""2023-04-08T07:03:01Z"", ""categories"": [""cs.LG"", ""cs.AI"", ""cs.SD"", ""eess.AS""], ""pdf_url"": ""https://arxiv.org/pdf/2304.03940v1"", ""landing_url"": ""https://arxiv.org/abs/2304.03940v1"", ""doi"": ""https://doi.org/10.48550/arXiv.2304.03940""}",2304.0394,https://openalex.org/W4365143360
10.1016/j.cviu.2025.104362,A vector quantized masked autoencoder for audiovisual speech emotion recognition,"An important challenge in emotion recognition is to develop methods that can leverage unlabeled training data. In this paper, we propose the VQ-MAE-AV model, a self-supervised multimodal model that leverages masked autoencoders to learn representations of audiovisual speech without labels. The model includes vector quantized variational autoencoders that compress raw audio and visual speech data into discrete tokens. The audiovisual speech tokens are used to train a multimodal masked autoencoder that consists of an encoder-decoder architecture with attention mechanisms. The model is designed to extract both local (i.e., at the frame level) and global (i.e., at the sequence level) representations of audiovisual speech. During self-supervised pre-training, the VQ-MAE-AV model is trained on a large-scale unlabeled dataset of audiovisual speech, for the task of reconstructing randomly masked audiovisual speech tokens and with a contrastive learning strategy. During this pre-training, the encoder learns to extract a representation of audiovisual speech that can be subsequently leveraged for emotion recognition. During the supervised fine-tuning stage, a small classification model is trained on top of the VQ-MAE-AV encoder for an emotion recognition task. The proposed approach achieves state-of-the-art emotion recognition results across several datasets in both controlled and in-the-wild conditions.",1,,include (junior:5),https://arxiv.org/abs/2305.03568v3,https://arxiv.org/pdf/2305.03568v3,2023,discrete speech tokens,speech tokenization,"{""arxiv_id"": ""2305.03568"", ""title"": ""A vector quantized masked autoencoder for audiovisual speech emotion recognition"", ""summary"": ""An important challenge in emotion recognition is to develop methods that can leverage unlabeled training data. In this paper, we propose the VQ-MAE-AV model, a self-supervised multimodal model that leverages masked autoencoders to learn representations of audiovisual speech without labels. The model includes vector quantized variational autoencoders that compress raw audio and visual speech data into discrete tokens. The audiovisual speech tokens are used to train a multimodal masked autoencoder that consists of an encoder-decoder architecture with attention mechanisms. The model is designed to extract both local (i.e., at the frame level) and global (i.e., at the sequence level) representations of audiovisual speech. During self-supervised pre-training, the VQ-MAE-AV model is trained on a large-scale unlabeled dataset of audiovisual speech, for the task of reconstructing randomly masked audiovisual speech tokens and with a contrastive learning strategy. During this pre-training, the encoder learns to extract a representation of audiovisual speech that can be subsequently leveraged for emotion recognition. During the supervised fine-tuning stage, a small classification model is trained on top of the VQ-MAE-AV encoder for an emotion recognition task. The proposed approach achieves state-of-the-art emotion recognition results across several datasets in both controlled and in-the-wild conditions."", ""authors"": [""Samir Sadok"", ""Simon Leglaive"", ""Renaud Séguier""], ""published"": ""2023-05-05T14:19:46Z"", ""updated"": ""2025-05-09T08:19:45Z"", ""categories"": [""cs.SD"", ""cs.LG"", ""cs.MM"", ""eess.AS""], ""pdf_url"": ""https://arxiv.org/pdf/2305.03568v3"", ""landing_url"": ""https://arxiv.org/abs/2305.03568v3"", ""doi"": ""https://doi.org/10.1016/j.cviu.2025.104362""}",2305.03568,https://openalex.org/W4409472566
10.48550/arxiv.2305.09636,SoundStorm: Efficient Parallel Audio Generation,"We present SoundStorm, a model for efficient, non-autoregressive audio generation. SoundStorm receives as input the semantic tokens of AudioLM, and relies on bidirectional attention and confidence-based parallel decoding to generate the tokens of a neural audio codec. Compared to the autoregressive generation approach of AudioLM, our model produces audio of the same quality and with higher consistency in voice and acoustic conditions, while being two orders of magnitude faster. SoundStorm generates 30 seconds of audio in 0.5 seconds on a TPU-v4. We demonstrate the ability of our model to scale audio generation to longer sequences by synthesizing high-quality, natural dialogue segments, given a transcript annotated with speaker turns and a short prompt with the speakers' voices.",1,,include (junior:5),https://arxiv.org/abs/2305.09636v1,https://arxiv.org/pdf/2305.09636v1,2023,discrete speech tokens,semantic tokens,"{""arxiv_id"": ""2305.09636"", ""title"": ""SoundStorm: Efficient Parallel Audio Generation"", ""summary"": ""We present SoundStorm, a model for efficient, non-autoregressive audio generation. SoundStorm receives as input the semantic tokens of AudioLM, and relies on bidirectional attention and confidence-based parallel decoding to generate the tokens of a neural audio codec. Compared to the autoregressive generation approach of AudioLM, our model produces audio of the same quality and with higher consistency in voice and acoustic conditions, while being two orders of magnitude faster. SoundStorm generates 30 seconds of audio in 0.5 seconds on a TPU-v4. We demonstrate the ability of our model to scale audio generation to longer sequences by synthesizing high-quality, natural dialogue segments, given a transcript annotated with speaker turns and a short prompt with the speakers' voices."", ""authors"": [""Zalán Borsos"", ""Matt Sharifi"", ""Damien Vincent"", ""Eugene Kharitonov"", ""Neil Zeghidour"", ""Marco Tagliasacchi""], ""published"": ""2023-05-16T17:41:25Z"", ""updated"": ""2023-05-16T17:41:25Z"", ""categories"": [""cs.SD"", ""cs.LG"", ""eess.AS""], ""pdf_url"": ""https://arxiv.org/pdf/2305.09636v1"", ""landing_url"": ""https://arxiv.org/abs/2305.09636v1"", ""doi"": ""https://doi.org/10.48550/arXiv.2305.09636""}",2305.09636,https://openalex.org/W4377010126
10.48550/arxiv.2305.15719,Efficient Neural Music Generation,"Recent progress in music generation has been remarkably advanced by the state-of-the-art MusicLM, which comprises a hierarchy of three LMs, respectively, for semantic, coarse acoustic, and fine acoustic modelings. Yet, sampling with the MusicLM requires processing through these LMs one by one to obtain the fine-grained acoustic tokens, making it computationally expensive and prohibitive for a real-time generation. Efficient music generation with a quality on par with MusicLM remains a significant challenge. In this paper, we present MeLoDy (M for music; L for LM; D for diffusion), an LM-guided diffusion model that generates music audios of state-of-the-art quality meanwhile reducing 95.7% or 99.6% forward passes in MusicLM, respectively, for sampling 10s or 30s music. MeLoDy inherits the highest-level LM from MusicLM for semantic modeling, and applies a novel dual-path diffusion (DPD) model and an audio VAE-GAN to efficiently decode the conditioning semantic tokens into waveform. DPD is proposed to simultaneously model the coarse and fine acoustics by incorporating the semantic information into segments of latents effectively via cross-attention at each denoising step. Our experimental results suggest the superiority of MeLoDy, not only in its practical advantages on sampling speed and infinitely continuable generation, but also in its state-of-the-art musicality, audio quality, and text correlation. Our samples are available at https://Efficient-MeLoDy.github.io/.",1,,include (junior:4),https://arxiv.org/abs/2305.15719v1,https://arxiv.org/pdf/2305.15719v1,2023,discrete speech tokens,acoustic tokens,"{""arxiv_id"": ""2305.15719"", ""title"": ""Efficient Neural Music Generation"", ""summary"": ""Recent progress in music generation has been remarkably advanced by the state-of-the-art MusicLM, which comprises a hierarchy of three LMs, respectively, for semantic, coarse acoustic, and fine acoustic modelings. Yet, sampling with the MusicLM requires processing through these LMs one by one to obtain the fine-grained acoustic tokens, making it computationally expensive and prohibitive for a real-time generation. Efficient music generation with a quality on par with MusicLM remains a significant challenge. In this paper, we present MeLoDy (M for music; L for LM; D for diffusion), an LM-guided diffusion model that generates music audios of state-of-the-art quality meanwhile reducing 95.7% or 99.6% forward passes in MusicLM, respectively, for sampling 10s or 30s music. MeLoDy inherits the highest-level LM from MusicLM for semantic modeling, and applies a novel dual-path diffusion (DPD) model and an audio VAE-GAN to efficiently decode the conditioning semantic tokens into waveform. DPD is proposed to simultaneously model the coarse and fine acoustics by incorporating the semantic information into segments of latents effectively via cross-attention at each denoising step. Our experimental results suggest the superiority of MeLoDy, not only in its practical advantages on sampling speed and infinitely continuable generation, but also in its state-of-the-art musicality, audio quality, and text correlation.\n  Our samples are available at https://Efficient-MeLoDy.github.io/."", ""authors"": [""Max W. Y. Lam"", ""Qiao Tian"", ""Tang Li"", ""Zongyu Yin"", ""Siyuan Feng"", ""Ming Tu"", ""Yuliang Ji"", ""Rui Xia"", ""Mingbo Ma"", ""Xuchen Song"", ""Jitong Chen"", ""Yuping Wang"", ""Yuxuan Wang""], ""published"": ""2023-05-25T05:02:35Z"", ""updated"": ""2023-05-25T05:02:35Z"", ""categories"": [""cs.SD"", ""cs.AI"", ""cs.LG"", ""eess.AS""], ""pdf_url"": ""https://arxiv.org/pdf/2305.15719v1"", ""landing_url"": ""https://arxiv.org/abs/2305.15719v1"", ""doi"": ""https://doi.org/10.48550/arXiv.2305.15719""}",2305.15719,https://openalex.org/W4378499140
10.48550/arxiv.2305.19269,Make-A-Voice: Unified Voice Synthesis With Discrete Representation,"Various applications of voice synthesis have been developed independently despite the fact that they generate ""voice"" as output in common. In addition, the majority of voice synthesis models currently rely on annotated audio data, but it is crucial to scale them to self-supervised datasets in order to effectively capture the wide range of acoustic variations present in human voice, including speaker identity, emotion, and prosody. In this work, we propose Make-A-Voice, a unified framework for synthesizing and manipulating voice signals from discrete representations. Make-A-Voice leverages a ""coarse-to-fine"" approach to model the human voice, which involves three stages: 1) semantic stage: model high-level transformation between linguistic content and self-supervised semantic tokens, 2) acoustic stage: introduce varying control signals as acoustic conditions for semantic-to-acoustic modeling, and 3) generation stage: synthesize high-fidelity waveforms from acoustic tokens. Make-A-Voice offers notable benefits as a unified voice synthesis framework: 1) Data scalability: the major backbone (i.e., acoustic and generation stage) does not require any annotations, and thus the training data could be scaled up. 2) Controllability and conditioning flexibility: we investigate different conditioning mechanisms and effectively handle three voice synthesis applications, including text-to-speech (TTS), voice conversion (VC), and singing voice synthesis (SVS) by re-synthesizing the discrete voice representations with prompt guidance. Experimental results demonstrate that Make-A-Voice exhibits superior audio quality and style similarity compared with competitive baseline models. Audio samples are available at https://Make-A-Voice.github.io",1,,include (junior:5),https://arxiv.org/abs/2305.19269v1,https://arxiv.org/pdf/2305.19269v1,2023,discrete speech tokens,acoustic tokens,"{""arxiv_id"": ""2305.19269"", ""title"": ""Make-A-Voice: Unified Voice Synthesis With Discrete Representation"", ""summary"": ""Various applications of voice synthesis have been developed independently despite the fact that they generate \""voice\"" as output in common. In addition, the majority of voice synthesis models currently rely on annotated audio data, but it is crucial to scale them to self-supervised datasets in order to effectively capture the wide range of acoustic variations present in human voice, including speaker identity, emotion, and prosody. In this work, we propose Make-A-Voice, a unified framework for synthesizing and manipulating voice signals from discrete representations. Make-A-Voice leverages a \""coarse-to-fine\"" approach to model the human voice, which involves three stages: 1) semantic stage: model high-level transformation between linguistic content and self-supervised semantic tokens, 2) acoustic stage: introduce varying control signals as acoustic conditions for semantic-to-acoustic modeling, and 3) generation stage: synthesize high-fidelity waveforms from acoustic tokens. Make-A-Voice offers notable benefits as a unified voice synthesis framework: 1) Data scalability: the major backbone (i.e., acoustic and generation stage) does not require any annotations, and thus the training data could be scaled up. 2) Controllability and conditioning flexibility: we investigate different conditioning mechanisms and effectively handle three voice synthesis applications, including text-to-speech (TTS), voice conversion (VC), and singing voice synthesis (SVS) by re-synthesizing the discrete voice representations with prompt guidance. Experimental results demonstrate that Make-A-Voice exhibits superior audio quality and style similarity compared with competitive baseline models. Audio samples are available at https://Make-A-Voice.github.io"", ""authors"": [""Rongjie Huang"", ""Chunlei Zhang"", ""Yongqi Wang"", ""Dongchao Yang"", ""Luping Liu"", ""Zhenhui Ye"", ""Ziyue Jiang"", ""Chao Weng"", ""Zhou Zhao"", ""Dong Yu""], ""published"": ""2023-05-30T17:59:26Z"", ""updated"": ""2023-05-30T17:59:26Z"", ""categories"": [""eess.AS"", ""cs.AI"", ""cs.CL"", ""cs.SD""], ""pdf_url"": ""https://arxiv.org/pdf/2305.19269v1"", ""landing_url"": ""https://arxiv.org/abs/2305.19269v1"", ""doi"": ""https://doi.org/10.48550/arXiv.2305.19269""}",2305.19269,https://openalex.org/W4378945745
10.1609/aaai.v38i16.29747,UniCATS: A Unified Context-Aware Text-to-Speech Framework with Contextual VQ-Diffusion and Vocoding,"The utilization of discrete speech tokens, divided into semantic tokens and acoustic tokens, has been proven superior to traditional acoustic feature mel-spectrograms in terms of naturalness and robustness for text-to-speech (TTS) synthesis. Recent popular models, such as VALL-E and SPEAR-TTS, allow zero-shot speaker adaptation through auto-regressive (AR) continuation of acoustic tokens extracted from a short speech prompt. However, these AR models are restricted to generate speech only in a left-to-right direction, making them unsuitable for speech editing where both preceding and following contexts are provided. Furthermore, these models rely on acoustic tokens, which have audio quality limitations imposed by the performance of audio codec models. In this study, we propose a unified context-aware TTS framework called UniCATS, which is capable of both speech continuation and editing. UniCATS comprises two components, an acoustic model CTX-txt2vec and a vocoder CTX-vec2wav. CTX-txt2vec employs contextual VQ-diffusion to predict semantic tokens from the input text, enabling it to incorporate the semantic context and maintain seamless concatenation with the surrounding context. Following that, CTX-vec2wav utilizes contextual vocoding to convert these semantic tokens into waveforms, taking into consideration the acoustic context. Our experimental results demonstrate that CTX-vec2wav outperforms HifiGAN and AudioLM in terms of speech resynthesis from semantic tokens. Moreover, we show that UniCATS achieves state-of-the-art performance in both speech continuation and editing.",1,,include (junior:5),https://arxiv.org/abs/2306.07547v6,https://arxiv.org/pdf/2306.07547v6,2023,discrete speech tokens,offline clustering,"{""arxiv_id"": ""2306.07547"", ""title"": ""UniCATS: A Unified Context-Aware Text-to-Speech Framework with Contextual VQ-Diffusion and Vocoding"", ""summary"": ""The utilization of discrete speech tokens, divided into semantic tokens and acoustic tokens, has been proven superior to traditional acoustic feature mel-spectrograms in terms of naturalness and robustness for text-to-speech (TTS) synthesis. Recent popular models, such as VALL-E and SPEAR-TTS, allow zero-shot speaker adaptation through auto-regressive (AR) continuation of acoustic tokens extracted from a short speech prompt. However, these AR models are restricted to generate speech only in a left-to-right direction, making them unsuitable for speech editing where both preceding and following contexts are provided. Furthermore, these models rely on acoustic tokens, which have audio quality limitations imposed by the performance of audio codec models. In this study, we propose a unified context-aware TTS framework called UniCATS, which is capable of both speech continuation and editing. UniCATS comprises two components, an acoustic model CTX-txt2vec and a vocoder CTX-vec2wav. CTX-txt2vec employs contextual VQ-diffusion to predict semantic tokens from the input text, enabling it to incorporate the semantic context and maintain seamless concatenation with the surrounding context. Following that, CTX-vec2wav utilizes contextual vocoding to convert these semantic tokens into waveforms, taking into consideration the acoustic context. Our experimental results demonstrate that CTX-vec2wav outperforms HifiGAN and AudioLM in terms of speech resynthesis from semantic tokens. Moreover, we show that UniCATS achieves state-of-the-art performance in both speech continuation and editing."", ""authors"": [""Chenpeng Du"", ""Yiwei Guo"", ""Feiyu Shen"", ""Zhijun Liu"", ""Zheng Liang"", ""Xie Chen"", ""Shuai Wang"", ""Hui Zhang"", ""Kai Yu""], ""published"": ""2023-06-13T05:38:34Z"", ""updated"": ""2024-03-28T13:56:33Z"", ""categories"": [""cs.SD"", ""eess.AS""], ""pdf_url"": ""https://arxiv.org/pdf/2306.07547v6"", ""landing_url"": ""https://arxiv.org/abs/2306.07547v6"", ""doi"": ""https://doi.org/10.1609/aaai.v38i16.29747""}",2306.07547,https://openalex.org/W4393147067
10.48550/arxiv.2306.08920,Pushing the Limits of Unsupervised Unit Discovery for SSL Speech Representation,"The excellent generalization ability of self-supervised learning (SSL) for speech foundation models has garnered significant attention. HuBERT is a successful example that utilizes offline clustering to convert speech features into discrete units for a masked language modeling pretext task. However, simply clustering features as targets by k-means does not fully inspire the model's performance. In this work, we present an unsupervised method to improve SSL targets. Two models are proposed, MonoBERT and PolyBERT, which leverage context-independent and context-dependent phoneme-based units for pre-training. Our models outperform other SSL models significantly on the LibriSpeech benchmark without the need for iterative re-clustering and re-training. Furthermore, our models equipped with context-dependent units even outperform target-improvement models that use labeled data during pre-training. How we progressively improve the unit discovery process is demonstrated through experiments.",1,,include (junior:5),https://arxiv.org/abs/2306.08920v1,https://arxiv.org/pdf/2306.08920v1,2023,discrete speech tokens,offline clustering,"{""arxiv_id"": ""2306.08920"", ""title"": ""Pushing the Limits of Unsupervised Unit Discovery for SSL Speech Representation"", ""summary"": ""The excellent generalization ability of self-supervised learning (SSL) for speech foundation models has garnered significant attention. HuBERT is a successful example that utilizes offline clustering to convert speech features into discrete units for a masked language modeling pretext task. However, simply clustering features as targets by k-means does not fully inspire the model's performance. In this work, we present an unsupervised method to improve SSL targets. Two models are proposed, MonoBERT and PolyBERT, which leverage context-independent and context-dependent phoneme-based units for pre-training. Our models outperform other SSL models significantly on the LibriSpeech benchmark without the need for iterative re-clustering and re-training. Furthermore, our models equipped with context-dependent units even outperform target-improvement models that use labeled data during pre-training. How we progressively improve the unit discovery process is demonstrated through experiments."", ""authors"": [""Ziyang Ma"", ""Zhisheng Zheng"", ""Guanrou Yang"", ""Yu Wang"", ""Chao Zhang"", ""Xie Chen""], ""published"": ""2023-06-15T07:45:12Z"", ""updated"": ""2023-06-15T07:45:12Z"", ""categories"": [""cs.CL"", ""cs.SD"", ""eess.AS""], ""pdf_url"": ""https://arxiv.org/pdf/2306.08920v1"", ""landing_url"": ""https://arxiv.org/abs/2306.08920v1"", ""doi"": ""https://doi.org/10.48550/arXiv.2306.08920""}",2306.0892,https://openalex.org/W4380994056
10.48550/arxiv.2306.10521,LM-VC: Zero-shot Voice Conversion via Speech Generation based on Language Models,"Language model (LM) based audio generation frameworks, e.g., AudioLM, have recently achieved new state-of-the-art performance in zero-shot audio generation. In this paper, we explore the feasibility of LMs for zero-shot voice conversion. An intuitive approach is to follow AudioLM - Tokenizing speech into semantic and acoustic tokens respectively by HuBERT and SoundStream, and converting source semantic tokens to target acoustic tokens conditioned on acoustic tokens of the target speaker. However, such an approach encounters several issues: 1) the linguistic content contained in semantic tokens may get dispersed during multi-layer modeling while the lengthy speech input in the voice conversion task makes contextual learning even harder; 2) the semantic tokens still contain speaker-related information, which may be leaked to the target speech, lowering the target speaker similarity; 3) the generation diversity in the sampling of the LM can lead to unexpected outcomes during inference, leading to unnatural pronunciation and speech quality degradation. To mitigate these problems, we propose LM-VC, a two-stage language modeling approach that generates coarse acoustic tokens for recovering the source linguistic content and target speaker's timbre, and then reconstructs the fine for acoustic details as converted speech. Specifically, to enhance content preservation and facilitates better disentanglement, a masked prefix LM with a mask prediction strategy is used for coarse acoustic modeling. This model is encouraged to recover the masked content from the surrounding context and generate target speech based on the target speaker's utterance and corrupted semantic tokens. Besides, to further alleviate the sampling error in the generation, an external LM, which employs window attention to capture the local acoustic relations, is introduced to participate in the coarse acoustic modeling.",1,,include (junior:5),https://arxiv.org/abs/2306.10521v2,https://arxiv.org/pdf/2306.10521v2,2023,discrete speech tokens,acoustic tokens,"{""arxiv_id"": ""2306.10521"", ""title"": ""LM-VC: Zero-shot Voice Conversion via Speech Generation based on Language Models"", ""summary"": ""Language model (LM) based audio generation frameworks, e.g., AudioLM, have recently achieved new state-of-the-art performance in zero-shot audio generation. In this paper, we explore the feasibility of LMs for zero-shot voice conversion. An intuitive approach is to follow AudioLM - Tokenizing speech into semantic and acoustic tokens respectively by HuBERT and SoundStream, and converting source semantic tokens to target acoustic tokens conditioned on acoustic tokens of the target speaker. However, such an approach encounters several issues: 1) the linguistic content contained in semantic tokens may get dispersed during multi-layer modeling while the lengthy speech input in the voice conversion task makes contextual learning even harder; 2) the semantic tokens still contain speaker-related information, which may be leaked to the target speech, lowering the target speaker similarity; 3) the generation diversity in the sampling of the LM can lead to unexpected outcomes during inference, leading to unnatural pronunciation and speech quality degradation. To mitigate these problems, we propose LM-VC, a two-stage language modeling approach that generates coarse acoustic tokens for recovering the source linguistic content and target speaker's timbre, and then reconstructs the fine for acoustic details as converted speech. Specifically, to enhance content preservation and facilitates better disentanglement, a masked prefix LM with a mask prediction strategy is used for coarse acoustic modeling. This model is encouraged to recover the masked content from the surrounding context and generate target speech based on the target speaker's utterance and corrupted semantic tokens. Besides, to further alleviate the sampling error in the generation, an external LM, which employs window attention to capture the local acoustic relations, is introduced to participate in the coarse acoustic modeling."", ""authors"": [""Zhichao Wang"", ""Yuanzhe Chen"", ""Lei Xie"", ""Qiao Tian"", ""Yuping Wang""], ""published"": ""2023-06-18T10:59:06Z"", ""updated"": ""2023-08-21T02:21:06Z"", ""categories"": [""eess.AS"", ""cs.SD""], ""pdf_url"": ""https://arxiv.org/pdf/2306.10521v2"", ""landing_url"": ""https://arxiv.org/abs/2306.10521v2"", ""doi"": ""https://doi.org/10.48550/arXiv.2306.10521""}",2306.10521,https://openalex.org/W4381558479
10.48550/arxiv.2308.06873,SpeechX: Neural Codec Language Model as a Versatile Speech Transformer,"Recent advancements in generative speech models based on audio-text prompts have enabled remarkable innovations like high-quality zero-shot text-to-speech. However, existing models still face limitations in handling diverse audio-text speech generation tasks involving transforming input speech and processing audio captured in adverse acoustic conditions. This paper introduces SpeechX, a versatile speech generation model capable of zero-shot TTS and various speech transformation tasks, dealing with both clean and noisy signals. SpeechX combines neural codec language modeling with multi-task learning using task-dependent prompting, enabling unified and extensible modeling and providing a consistent way for leveraging textual input in speech enhancement and transformation tasks. Experimental results show SpeechX's efficacy in various tasks, including zero-shot TTS, noise suppression, target speaker extraction, speech removal, and speech editing with or without background noise, achieving comparable or superior performance to specialized models across tasks. See https://aka.ms/speechx for demo samples.",1,,include (senior:5),https://arxiv.org/abs/2308.06873v2,https://arxiv.org/pdf/2308.06873v2,2023,discrete speech tokens,neural codec,"{""arxiv_id"": ""2308.06873"", ""title"": ""SpeechX: Neural Codec Language Model as a Versatile Speech Transformer"", ""summary"": ""Recent advancements in generative speech models based on audio-text prompts have enabled remarkable innovations like high-quality zero-shot text-to-speech. However, existing models still face limitations in handling diverse audio-text speech generation tasks involving transforming input speech and processing audio captured in adverse acoustic conditions. This paper introduces SpeechX, a versatile speech generation model capable of zero-shot TTS and various speech transformation tasks, dealing with both clean and noisy signals. SpeechX combines neural codec language modeling with multi-task learning using task-dependent prompting, enabling unified and extensible modeling and providing a consistent way for leveraging textual input in speech enhancement and transformation tasks. Experimental results show SpeechX's efficacy in various tasks, including zero-shot TTS, noise suppression, target speaker extraction, speech removal, and speech editing with or without background noise, achieving comparable or superior performance to specialized models across tasks. See https://aka.ms/speechx for demo samples."", ""authors"": [""Xiaofei Wang"", ""Manthan Thakker"", ""Zhuo Chen"", ""Naoyuki Kanda"", ""Sefik Emre Eskimez"", ""Sanyuan Chen"", ""Min Tang"", ""Shujie Liu"", ""Jinyu Li"", ""Takuya Yoshioka""], ""published"": ""2023-08-14T01:01:19Z"", ""updated"": ""2024-06-25T18:38:28Z"", ""categories"": [""eess.AS"", ""cs.CL"", ""cs.LG"", ""cs.SD""], ""pdf_url"": ""https://arxiv.org/pdf/2308.06873v2"", ""landing_url"": ""https://arxiv.org/abs/2308.06873v2"", ""doi"": ""https://doi.org/10.48550/arXiv.2308.06873""}",2308.06873,https://openalex.org/W4385848954
10.48550/arxiv.2308.10415,"TokenSplit: Using Discrete Speech Representations for Direct, Refined, and Transcript-Conditioned Speech Separation and Recognition","We present TokenSplit, a speech separation model that acts on discrete token sequences. The model is trained on multiple tasks simultaneously: separate and transcribe each speech source, and generate speech from text. The model operates on transcripts and audio token sequences and achieves multiple tasks through masking of inputs. The model is a sequence-to-sequence encoder-decoder model that uses the Transformer architecture. We also present a ""refinement"" version of the model that predicts enhanced audio tokens from the audio tokens of speech separated by a conventional separation model. Using both objective metrics and subjective MUSHRA listening tests, we show that our model achieves excellent performance in terms of separation, both with or without transcript conditioning. We also measure the automatic speech recognition (ASR) performance and provide audio samples of speech synthesis to demonstrate the additional utility of our model.",1,,include (junior:5),https://arxiv.org/abs/2308.10415v1,https://arxiv.org/pdf/2308.10415v1,2023,dblp_title,"TokenSplit: Using Discrete Speech Representations for Direct, Refined, and Transcript-Conditioned Speech Separation and Recognition.","{""arxiv_id"": ""2308.10415"", ""title"": ""TokenSplit: Using Discrete Speech Representations for Direct, Refined, and Transcript-Conditioned Speech Separation and Recognition"", ""summary"": ""We present TokenSplit, a speech separation model that acts on discrete token sequences. The model is trained on multiple tasks simultaneously: separate and transcribe each speech source, and generate speech from text. The model operates on transcripts and audio token sequences and achieves multiple tasks through masking of inputs. The model is a sequence-to-sequence encoder-decoder model that uses the Transformer architecture. We also present a \""refinement\"" version of the model that predicts enhanced audio tokens from the audio tokens of speech separated by a conventional separation model. Using both objective metrics and subjective MUSHRA listening tests, we show that our model achieves excellent performance in terms of separation, both with or without transcript conditioning. We also measure the automatic speech recognition (ASR) performance and provide audio samples of speech synthesis to demonstrate the additional utility of our model."", ""authors"": [""Hakan Erdogan"", ""Scott Wisdom"", ""Xuankai Chang"", ""Zalán Borsos"", ""Marco Tagliasacchi"", ""Neil Zeghidour"", ""John R. Hershey""], ""published"": ""2023-08-21T01:52:01Z"", ""updated"": ""2023-08-21T01:52:01Z"", ""categories"": [""cs.SD"", ""cs.LG"", ""eess.AS""], ""pdf_url"": ""https://arxiv.org/pdf/2308.10415v1"", ""landing_url"": ""https://arxiv.org/abs/2308.10415v1"", ""doi"": ""https://doi.org/10.48550/arXiv.2308.10415""}",2308.10415,https://openalex.org/W4386081397
10.48550/arxiv.2308.16692,SpeechTokenizer: Unified Speech Tokenizer for Speech Large Language Models,"Current speech large language models build upon discrete speech representations, which can be categorized into semantic tokens and acoustic tokens. However, existing speech tokens are not specifically designed for speech language modeling. To assess the suitability of speech tokens for building speech language models, we established the first benchmark, SLMTokBench. Our results indicate that neither semantic nor acoustic tokens are ideal for this purpose. Therefore, we propose SpeechTokenizer, a unified speech tokenizer for speech large language models. SpeechTokenizer adopts the Encoder-Decoder architecture with residual vector quantization (RVQ). Unifying semantic and acoustic tokens, SpeechTokenizer disentangles different aspects of speech information hierarchically across different RVQ layers. Furthermore, We construct a Unified Speech Language Model (USLM) leveraging SpeechTokenizer. Experiments show that SpeechTokenizer performs comparably to EnCodec in speech reconstruction and demonstrates strong performance on the SLMTokBench benchmark. Also, USLM outperforms VALL-E in zero-shot Text-to-Speech tasks. Code and models are available at https://github.com/ZhangXInFD/SpeechTokenizer/.",1,,include (junior:5),https://arxiv.org/abs/2308.16692v2,https://arxiv.org/pdf/2308.16692v2,2023,discrete speech tokens,acoustic tokens,"{""arxiv_id"": ""2308.16692"", ""title"": ""SpeechTokenizer: Unified Speech Tokenizer for Speech Large Language Models"", ""summary"": ""Current speech large language models build upon discrete speech representations, which can be categorized into semantic tokens and acoustic tokens. However, existing speech tokens are not specifically designed for speech language modeling. To assess the suitability of speech tokens for building speech language models, we established the first benchmark, SLMTokBench. Our results indicate that neither semantic nor acoustic tokens are ideal for this purpose. Therefore, we propose SpeechTokenizer, a unified speech tokenizer for speech large language models. SpeechTokenizer adopts the Encoder-Decoder architecture with residual vector quantization (RVQ). Unifying semantic and acoustic tokens, SpeechTokenizer disentangles different aspects of speech information hierarchically across different RVQ layers. Furthermore, We construct a Unified Speech Language Model (USLM) leveraging SpeechTokenizer. Experiments show that SpeechTokenizer performs comparably to EnCodec in speech reconstruction and demonstrates strong performance on the SLMTokBench benchmark. Also, USLM outperforms VALL-E in zero-shot Text-to-Speech tasks. Code and models are available at https://github.com/ZhangXInFD/SpeechTokenizer/."", ""authors"": [""Xin Zhang"", ""Dong Zhang"", ""Shimin Li"", ""Yaqian Zhou"", ""Xipeng Qiu""], ""published"": ""2023-08-31T12:53:09Z"", ""updated"": ""2024-01-23T01:56:57Z"", ""categories"": [""cs.CL"", ""cs.SD"", ""eess.AS""], ""pdf_url"": ""https://arxiv.org/pdf/2308.16692v2"", ""landing_url"": ""https://arxiv.org/abs/2308.16692v2"", ""doi"": ""https://doi.org/10.48550/arXiv.2308.16692""}",2308.16692,https://openalex.org/W4386384714
10.48550/arxiv.2309.00169,RepCodec: A Speech Representation Codec for Speech Tokenization,"With recent rapid growth of large language models (LLMs), discrete speech tokenization has played an important role for injecting speech into LLMs. However, this discretization gives rise to a loss of information, consequently impairing overall performance. To improve the performance of these discrete speech tokens, we present RepCodec, a novel speech representation codec for semantic speech tokenization. In contrast to audio codecs which reconstruct the raw audio, RepCodec learns a vector quantization codebook through reconstructing speech representations from speech encoders like HuBERT or data2vec. Together, the speech encoder, the codec encoder and the vector quantization codebook form a pipeline for converting speech waveforms into semantic tokens. The extensive experiments illustrate that RepCodec, by virtue of its enhanced information retention capacity, significantly outperforms the widely used k-means clustering approach in both speech understanding and generation. Furthermore, this superiority extends across various speech encoders and languages, affirming the robustness of RepCodec. We believe our method can facilitate large language modeling research on speech processing.",1,,include (junior:5),https://arxiv.org/abs/2309.00169v3,https://arxiv.org/pdf/2309.00169v3,2023,discrete speech tokens,offline clustering,"{""arxiv_id"": ""2309.00169"", ""title"": ""RepCodec: A Speech Representation Codec for Speech Tokenization"", ""summary"": ""With recent rapid growth of large language models (LLMs), discrete speech tokenization has played an important role for injecting speech into LLMs. However, this discretization gives rise to a loss of information, consequently impairing overall performance. To improve the performance of these discrete speech tokens, we present RepCodec, a novel speech representation codec for semantic speech tokenization. In contrast to audio codecs which reconstruct the raw audio, RepCodec learns a vector quantization codebook through reconstructing speech representations from speech encoders like HuBERT or data2vec. Together, the speech encoder, the codec encoder and the vector quantization codebook form a pipeline for converting speech waveforms into semantic tokens. The extensive experiments illustrate that RepCodec, by virtue of its enhanced information retention capacity, significantly outperforms the widely used k-means clustering approach in both speech understanding and generation. Furthermore, this superiority extends across various speech encoders and languages, affirming the robustness of RepCodec. We believe our method can facilitate large language modeling research on speech processing."", ""authors"": [""Zhichao Huang"", ""Chutong Meng"", ""Tom Ko""], ""published"": ""2023-08-31T23:26:10Z"", ""updated"": ""2024-07-22T09:53:44Z"", ""categories"": [""eess.AS"", ""cs.LG"", ""cs.SD""], ""pdf_url"": ""https://arxiv.org/pdf/2309.00169v3"", ""landing_url"": ""https://arxiv.org/abs/2309.00169v3"", ""doi"": ""https://doi.org/10.48550/arXiv.2309.00169""}",2309.00169,https://openalex.org/W4386437507
10.48550/arxiv.2309.07937,Voxtlm: unified decoder-only models for consolidating speech recognition/synthesis and speech/text continuation tasks,"We propose a decoder-only language model, VoxtLM, that can perform four tasks: speech recognition, speech synthesis, text generation, and speech continuation. VoxtLM integrates text vocabulary with discrete speech tokens from self-supervised speech features and uses special tokens to enable multitask learning. Compared to a single-task model, VoxtLM exhibits a significant improvement in speech synthesis, with improvements in both speech intelligibility from 28.9 to 5.6 and objective quality from 2.68 to 3.90. VoxtLM also improves speech generation and speech recognition performance over the single-task counterpart. Further, VoxtLM is trained with publicly available data and training recipes and model checkpoints are open-sourced to make fully reproducible work.",1,,include (junior:5),https://arxiv.org/abs/2309.07937v3,https://arxiv.org/pdf/2309.07937v3,2023,discrete speech tokens,offline clustering,"{""arxiv_id"": ""2309.07937"", ""title"": ""Voxtlm: unified decoder-only models for consolidating speech recognition/synthesis and speech/text continuation tasks"", ""summary"": ""We propose a decoder-only language model, VoxtLM, that can perform four tasks: speech recognition, speech synthesis, text generation, and speech continuation. VoxtLM integrates text vocabulary with discrete speech tokens from self-supervised speech features and uses special tokens to enable multitask learning. Compared to a single-task model, VoxtLM exhibits a significant improvement in speech synthesis, with improvements in both speech intelligibility from 28.9 to 5.6 and objective quality from 2.68 to 3.90. VoxtLM also improves speech generation and speech recognition performance over the single-task counterpart. Further, VoxtLM is trained with publicly available data and training recipes and model checkpoints are open-sourced to make fully reproducible work."", ""authors"": [""Soumi Maiti"", ""Yifan Peng"", ""Shukjae Choi"", ""Jee-weon Jung"", ""Xuankai Chang"", ""Shinji Watanabe""], ""published"": ""2023-09-14T03:13:18Z"", ""updated"": ""2024-01-24T15:36:31Z"", ""categories"": [""eess.AS"", ""cs.LG"", ""cs.SD""], ""pdf_url"": ""https://arxiv.org/pdf/2309.07937v3"", ""landing_url"": ""https://arxiv.org/abs/2309.07937v3"", ""doi"": ""https://doi.org/10.48550/arXiv.2309.07937""}",2309.07937,https://openalex.org/W4386874750
10.48550/arxiv.2309.11977,Improving Language Model-Based Zero-Shot Text-to-Speech Synthesis with Multi-Scale Acoustic Prompts,"Zero-shot text-to-speech (TTS) synthesis aims to clone any unseen speaker's voice without adaptation parameters. By quantizing speech waveform into discrete acoustic tokens and modeling these tokens with the language model, recent language model-based TTS models show zero-shot speaker adaptation capabilities with only a 3-second acoustic prompt of an unseen speaker. However, they are limited by the length of the acoustic prompt, which makes it difficult to clone personal speaking style. In this paper, we propose a novel zero-shot TTS model with the multi-scale acoustic prompts based on a neural codec language model VALL-E. A speaker-aware text encoder is proposed to learn the personal speaking style at the phoneme-level from the style prompt consisting of multiple sentences. Following that, a VALL-E based acoustic decoder is utilized to model the timbre from the timbre prompt at the frame-level and generate speech. The experimental results show that our proposed method outperforms baselines in terms of naturalness and speaker similarity, and can achieve better performance by scaling out to a longer style prompt.",1,,include (junior:5),https://arxiv.org/abs/2309.11977v3,https://arxiv.org/pdf/2309.11977v3,2023,discrete speech tokens,acoustic tokens,"{""arxiv_id"": ""2309.11977"", ""title"": ""Improving Language Model-Based Zero-Shot Text-to-Speech Synthesis with Multi-Scale Acoustic Prompts"", ""summary"": ""Zero-shot text-to-speech (TTS) synthesis aims to clone any unseen speaker's voice without adaptation parameters. By quantizing speech waveform into discrete acoustic tokens and modeling these tokens with the language model, recent language model-based TTS models show zero-shot speaker adaptation capabilities with only a 3-second acoustic prompt of an unseen speaker. However, they are limited by the length of the acoustic prompt, which makes it difficult to clone personal speaking style. In this paper, we propose a novel zero-shot TTS model with the multi-scale acoustic prompts based on a neural codec language model VALL-E. A speaker-aware text encoder is proposed to learn the personal speaking style at the phoneme-level from the style prompt consisting of multiple sentences. Following that, a VALL-E based acoustic decoder is utilized to model the timbre from the timbre prompt at the frame-level and generate speech. The experimental results show that our proposed method outperforms baselines in terms of naturalness and speaker similarity, and can achieve better performance by scaling out to a longer style prompt."", ""authors"": [""Shun Lei"", ""Yixuan Zhou"", ""Liyang Chen"", ""Dan Luo"", ""Zhiyong Wu"", ""Xixin Wu"", ""Shiyin Kang"", ""Tao Jiang"", ""Yahui Zhou"", ""Yuxing Han"", ""Helen Meng""], ""published"": ""2023-09-21T11:22:22Z"", ""updated"": ""2024-04-09T08:39:52Z"", ""categories"": [""cs.SD"", ""eess.AS""], ""pdf_url"": ""https://arxiv.org/pdf/2309.11977v3"", ""landing_url"": ""https://arxiv.org/abs/2309.11977v3"", ""doi"": ""https://doi.org/10.48550/arXiv.2309.11977""}",2309.11977,https://openalex.org/W4386977793
10.48550/arxiv.2309.14324,Towards General-Purpose Text-Instruction-Guided Voice Conversion,"This paper introduces a novel voice conversion (VC) model, guided by text instructions such as ""articulate slowly with a deep tone"" or ""speak in a cheerful boyish voice"". Unlike traditional methods that rely on reference utterances to determine the attributes of the converted speech, our model adds versatility and specificity to voice conversion. The proposed VC model is a neural codec language model which processes a sequence of discrete codes, resulting in the code sequence of converted speech. It utilizes text instructions as style prompts to modify the prosody and emotional information of the given speech. In contrast to previous approaches, which often rely on employing separate encoders like prosody and content encoders to handle different aspects of the source speech, our model handles various information of speech in an end-to-end manner. Experiments have demonstrated the impressive capabilities of our model in comprehending instructions and delivering reasonable results.",1,,include (junior:4),https://arxiv.org/abs/2309.14324v2,https://arxiv.org/pdf/2309.14324v2,2023,discrete speech tokens,neural codec,"{""arxiv_id"": ""2309.14324"", ""title"": ""Towards General-Purpose Text-Instruction-Guided Voice Conversion"", ""summary"": ""This paper introduces a novel voice conversion (VC) model, guided by text instructions such as \""articulate slowly with a deep tone\"" or \""speak in a cheerful boyish voice\"". Unlike traditional methods that rely on reference utterances to determine the attributes of the converted speech, our model adds versatility and specificity to voice conversion. The proposed VC model is a neural codec language model which processes a sequence of discrete codes, resulting in the code sequence of converted speech. It utilizes text instructions as style prompts to modify the prosody and emotional information of the given speech. In contrast to previous approaches, which often rely on employing separate encoders like prosody and content encoders to handle different aspects of the source speech, our model handles various information of speech in an end-to-end manner. Experiments have demonstrated the impressive capabilities of our model in comprehending instructions and delivering reasonable results."", ""authors"": [""Chun-Yi Kuan"", ""Chen An Li"", ""Tsu-Yuan Hsu"", ""Tse-Yang Lin"", ""Ho-Lam Chung"", ""Kai-Wei Chang"", ""Shuo-yiin Chang"", ""Hung-yi Lee""], ""published"": ""2023-09-25T17:52:09Z"", ""updated"": ""2024-01-16T13:53:56Z"", ""categories"": [""eess.AS"", ""cs.CL"", ""cs.LG"", ""cs.SD""], ""pdf_url"": ""https://arxiv.org/pdf/2309.14324v2"", ""landing_url"": ""https://arxiv.org/abs/2309.14324v2"", ""doi"": ""https://doi.org/10.48550/arXiv.2309.14324""}",2309.14324,https://openalex.org/W4387076541
10.48550/arxiv.2310.07246,Vec-Tok Speech: speech vectorization and tokenization for neural speech generation,"Language models (LMs) have recently flourished in natural language processing and computer vision, generating high-fidelity texts or images in various tasks. In contrast, the current speech generative models are still struggling regarding speech quality and task generalization. This paper presents Vec-Tok Speech, an extensible framework that resembles multiple speech generation tasks, generating expressive and high-fidelity speech. Specifically, we propose a novel speech codec based on speech vectors and semantic tokens. Speech vectors contain acoustic details contributing to high-fidelity speech reconstruction, while semantic tokens focus on the linguistic content of speech, facilitating language modeling. Based on the proposed speech codec, Vec-Tok Speech leverages an LM to undertake the core of speech generation. Moreover, Byte-Pair Encoding (BPE) is introduced to reduce the token length and bit rate for lower exposure bias and longer context coverage, improving the performance of LMs. Vec-Tok Speech can be used for intra- and cross-lingual zero-shot voice conversion (VC), zero-shot speaking style transfer text-to-speech (TTS), speech-to-speech translation (S2ST), speech denoising, and speaker de-identification and anonymization. Experiments show that Vec-Tok Speech, built on 50k hours of speech, performs better than other SOTA models. Code will be available at https://github.com/BakerBunker/VecTok .",1,,include (junior:5),https://arxiv.org/abs/2310.07246v2,https://arxiv.org/pdf/2310.07246v2,2023,discrete speech tokens,semantic tokens,"{""arxiv_id"": ""2310.07246"", ""title"": ""Vec-Tok Speech: speech vectorization and tokenization for neural speech generation"", ""summary"": ""Language models (LMs) have recently flourished in natural language processing and computer vision, generating high-fidelity texts or images in various tasks. In contrast, the current speech generative models are still struggling regarding speech quality and task generalization. This paper presents Vec-Tok Speech, an extensible framework that resembles multiple speech generation tasks, generating expressive and high-fidelity speech. Specifically, we propose a novel speech codec based on speech vectors and semantic tokens. Speech vectors contain acoustic details contributing to high-fidelity speech reconstruction, while semantic tokens focus on the linguistic content of speech, facilitating language modeling. Based on the proposed speech codec, Vec-Tok Speech leverages an LM to undertake the core of speech generation. Moreover, Byte-Pair Encoding (BPE) is introduced to reduce the token length and bit rate for lower exposure bias and longer context coverage, improving the performance of LMs. Vec-Tok Speech can be used for intra- and cross-lingual zero-shot voice conversion (VC), zero-shot speaking style transfer text-to-speech (TTS), speech-to-speech translation (S2ST), speech denoising, and speaker de-identification and anonymization. Experiments show that Vec-Tok Speech, built on 50k hours of speech, performs better than other SOTA models. Code will be available at https://github.com/BakerBunker/VecTok ."", ""authors"": [""Xinfa Zhu"", ""Yuanjun Lv"", ""Yi Lei"", ""Tao Li"", ""Wendi He"", ""Hongbin Zhou"", ""Heng Lu"", ""Lei Xie""], ""published"": ""2023-10-11T07:23:27Z"", ""updated"": ""2023-10-12T05:49:52Z"", ""categories"": [""cs.SD"", ""eess.AS""], ""pdf_url"": ""https://arxiv.org/pdf/2310.07246v2"", ""landing_url"": ""https://arxiv.org/abs/2310.07246v2"", ""doi"": ""https://doi.org/10.48550/arXiv.2310.07246""}",2310.07246,https://openalex.org/W4387634372
10.48550/arxiv.2310.08981,Low-latency Speech Enhancement via Speech Token Generation,"Existing deep learning based speech enhancement mainly employ a data-driven approach, which leverage large amounts of data with a variety of noise types to achieve noise removal from noisy signal. However, the high dependence on the data limits its generalization on the unseen complex noises in real-life environment. In this paper, we focus on the low-latency scenario and regard speech enhancement as a speech generation problem conditioned on the noisy signal, where we generate clean speech instead of identifying and removing noises. Specifically, we propose a conditional generative framework for speech enhancement, which models clean speech by acoustic codes of a neural speech codec and generates the speech codes conditioned on past noisy frames in an auto-regressive way. Moreover, we propose an explicit-alignment approach to align noisy frames with the generated speech tokens to improve the robustness and scalability to different input lengths. Different from other methods that leverage multiple stages to generate speech codes, we leverage a single-stage speech generation approach based on the TF-Codec neural codec to achieve high speech quality with low latency. Extensive results on both synthetic and real-recorded test set show its superiority over data-driven approaches in terms of noise robustness and temporal speech coherence.",1,,include (junior:5),https://arxiv.org/abs/2310.08981v3,https://arxiv.org/pdf/2310.08981v3,2023,discrete speech tokens,speech tokenization,"{""arxiv_id"": ""2310.08981"", ""title"": ""Low-latency Speech Enhancement via Speech Token Generation"", ""summary"": ""Existing deep learning based speech enhancement mainly employ a data-driven approach, which leverage large amounts of data with a variety of noise types to achieve noise removal from noisy signal. However, the high dependence on the data limits its generalization on the unseen complex noises in real-life environment. In this paper, we focus on the low-latency scenario and regard speech enhancement as a speech generation problem conditioned on the noisy signal, where we generate clean speech instead of identifying and removing noises. Specifically, we propose a conditional generative framework for speech enhancement, which models clean speech by acoustic codes of a neural speech codec and generates the speech codes conditioned on past noisy frames in an auto-regressive way. Moreover, we propose an explicit-alignment approach to align noisy frames with the generated speech tokens to improve the robustness and scalability to different input lengths. Different from other methods that leverage multiple stages to generate speech codes, we leverage a single-stage speech generation approach based on the TF-Codec neural codec to achieve high speech quality with low latency. Extensive results on both synthetic and real-recorded test set show its superiority over data-driven approaches in terms of noise robustness and temporal speech coherence."", ""authors"": [""Huaying Xue"", ""Xiulian Peng"", ""Yan Lu""], ""published"": ""2023-10-13T09:57:09Z"", ""updated"": ""2024-01-23T06:13:04Z"", ""categories"": [""cs.SD"", ""cs.MM"", ""eess.AS""], ""pdf_url"": ""https://arxiv.org/pdf/2310.08981v3"", ""landing_url"": ""https://arxiv.org/abs/2310.08981v3"", ""doi"": ""https://doi.org/10.48550/arXiv.2310.08981""}",2310.08981,https://openalex.org/W4387687749
10.48550/arxiv.2310.10803,SD-HuBERT: Sentence-Level Self-Distillation Induces Syllabic Organization in HuBERT,"Data-driven unit discovery in self-supervised learning (SSL) of speech has embarked on a new era of spoken language processing. Yet, the discovered units often remain in phonetic space and the units beyond phonemes are largely underexplored. Here, we demonstrate that a syllabic organization emerges in learning sentence-level representation of speech. In particular, we adopt ""self-distillation"" objective to fine-tune the pretrained HuBERT with an aggregator token that summarizes the entire sentence. Without any supervision, the resulting model draws definite boundaries in speech, and the representations across frames exhibit salient syllabic structures. We demonstrate that this emergent structure largely corresponds to the ground truth syllables. Furthermore, we propose a new benchmark task, Spoken Speech ABX, for evaluating sentence-level representation of speech. When compared to previous models, our model outperforms in both unsupervised syllable discovery and learning sentence-level representation. Together, we demonstrate that the self-distillation of HuBERT gives rise to syllabic organization without relying on external labels or modalities, and potentially provides novel data-driven units for spoken language modeling.",1,,include (senior:4),https://arxiv.org/abs/2310.10803v3,https://arxiv.org/pdf/2310.10803v3,2023,discrete speech tokens,hubert,"{""arxiv_id"": ""2310.10803"", ""title"": ""SD-HuBERT: Sentence-Level Self-Distillation Induces Syllabic Organization in HuBERT"", ""summary"": ""Data-driven unit discovery in self-supervised learning (SSL) of speech has embarked on a new era of spoken language processing. Yet, the discovered units often remain in phonetic space and the units beyond phonemes are largely underexplored. Here, we demonstrate that a syllabic organization emerges in learning sentence-level representation of speech. In particular, we adopt \""self-distillation\"" objective to fine-tune the pretrained HuBERT with an aggregator token that summarizes the entire sentence. Without any supervision, the resulting model draws definite boundaries in speech, and the representations across frames exhibit salient syllabic structures. We demonstrate that this emergent structure largely corresponds to the ground truth syllables. Furthermore, we propose a new benchmark task, Spoken Speech ABX, for evaluating sentence-level representation of speech. When compared to previous models, our model outperforms in both unsupervised syllable discovery and learning sentence-level representation. Together, we demonstrate that the self-distillation of HuBERT gives rise to syllabic organization without relying on external labels or modalities, and potentially provides novel data-driven units for spoken language modeling."", ""authors"": [""Cheol Jun Cho"", ""Abdelrahman Mohamed"", ""Shang-Wen Li"", ""Alan W Black"", ""Gopala K. Anumanchipalli""], ""published"": ""2023-10-16T20:05:36Z"", ""updated"": ""2025-04-10T11:20:55Z"", ""categories"": [""cs.CL"", ""eess.AS""], ""pdf_url"": ""https://arxiv.org/pdf/2310.10803v3"", ""landing_url"": ""https://arxiv.org/abs/2310.10803v3"", ""doi"": ""https://doi.org/10.48550/arXiv.2310.10803""}",2310.10803,https://openalex.org/W4387796678
10.48550/arxiv.2310.14580,Acoustic BPE for Speech Generation with Discrete Tokens,"Discrete audio tokens derived from self-supervised learning models have gained widespread usage in speech generation. However, current practice of directly utilizing audio tokens poses challenges for sequence modeling due to the length of the token sequence. Additionally, this approach places the burden on the model to establish correlations between tokens, further complicating the modeling process. To address this issue, we propose acoustic BPE which encodes frequent audio token patterns by utilizing byte-pair encoding. Acoustic BPE effectively reduces the sequence length and leverages the prior morphological information present in token sequence, which alleviates the modeling challenges of token correlation. Through comprehensive investigations on a speech language model trained with acoustic BPE, we confirm the notable advantages it offers, including faster inference and improved syntax capturing capabilities. In addition, we propose a novel rescore method to select the optimal synthetic speech among multiple candidates generated by rich-diversity TTS system. Experiments prove that rescore selection aligns closely with human preference, which highlights acoustic BPE's potential to other speech generation tasks.",1,,include (junior:5),https://arxiv.org/abs/2310.14580v4,https://arxiv.org/pdf/2310.14580v4,2023,discrete speech tokens,acoustic bpe,"{""arxiv_id"": ""2310.14580"", ""title"": ""Acoustic BPE for Speech Generation with Discrete Tokens"", ""summary"": ""Discrete audio tokens derived from self-supervised learning models have gained widespread usage in speech generation. However, current practice of directly utilizing audio tokens poses challenges for sequence modeling due to the length of the token sequence. Additionally, this approach places the burden on the model to establish correlations between tokens, further complicating the modeling process. To address this issue, we propose acoustic BPE which encodes frequent audio token patterns by utilizing byte-pair encoding. Acoustic BPE effectively reduces the sequence length and leverages the prior morphological information present in token sequence, which alleviates the modeling challenges of token correlation. Through comprehensive investigations on a speech language model trained with acoustic BPE, we confirm the notable advantages it offers, including faster inference and improved syntax capturing capabilities. In addition, we propose a novel rescore method to select the optimal synthetic speech among multiple candidates generated by rich-diversity TTS system. Experiments prove that rescore selection aligns closely with human preference, which highlights acoustic BPE's potential to other speech generation tasks."", ""authors"": [""Feiyu Shen"", ""Yiwei Guo"", ""Chenpeng Du"", ""Xie Chen"", ""Kai Yu""], ""published"": ""2023-10-23T05:38:41Z"", ""updated"": ""2024-01-15T05:53:31Z"", ""categories"": [""cs.SD"", ""eess.AS""], ""pdf_url"": ""https://arxiv.org/pdf/2310.14580v4"", ""landing_url"": ""https://arxiv.org/abs/2310.14580v4"", ""doi"": ""https://doi.org/10.48550/arXiv.2310.14580""}",2310.1458,https://openalex.org/W4387929314
10.48550/arxiv.2311.02898,Transduce and Speak: Neural Transducer for Text-to-Speech with Semantic Token Prediction,"We introduce a text-to-speech(TTS) framework based on a neural transducer. We use discretized semantic tokens acquired from wav2vec2.0 embeddings, which makes it easy to adopt a neural transducer for the TTS framework enjoying its monotonic alignment constraints. The proposed model first generates aligned semantic tokens using the neural transducer, then synthesizes a speech sample from the semantic tokens using a non-autoregressive(NAR) speech generator. This decoupled framework alleviates the training complexity of TTS and allows each stage to focus on 1) linguistic and alignment modeling and 2) fine-grained acoustic modeling, respectively. Experimental results on the zero-shot adaptive TTS show that the proposed model exceeds the baselines in speech quality and speaker similarity via objective and subjective measures. We also investigate the inference speed and prosody controllability of our proposed model, showing the potential of the neural transducer for TTS frameworks.",1,,include (junior:5),https://arxiv.org/abs/2311.02898v2,https://arxiv.org/pdf/2311.02898v2,2023,discrete speech tokens,semantic tokens,"{""arxiv_id"": ""2311.02898"", ""title"": ""Transduce and Speak: Neural Transducer for Text-to-Speech with Semantic Token Prediction"", ""summary"": ""We introduce a text-to-speech(TTS) framework based on a neural transducer. We use discretized semantic tokens acquired from wav2vec2.0 embeddings, which makes it easy to adopt a neural transducer for the TTS framework enjoying its monotonic alignment constraints. The proposed model first generates aligned semantic tokens using the neural transducer, then synthesizes a speech sample from the semantic tokens using a non-autoregressive(NAR) speech generator. This decoupled framework alleviates the training complexity of TTS and allows each stage to focus on 1) linguistic and alignment modeling and 2) fine-grained acoustic modeling, respectively. Experimental results on the zero-shot adaptive TTS show that the proposed model exceeds the baselines in speech quality and speaker similarity via objective and subjective measures. We also investigate the inference speed and prosody controllability of our proposed model, showing the potential of the neural transducer for TTS frameworks."", ""authors"": [""Minchan Kim"", ""Myeonghun Jeong"", ""Byoung Jin Choi"", ""Dongjune Lee"", ""Nam Soo Kim""], ""published"": ""2023-11-06T06:13:39Z"", ""updated"": ""2023-11-08T05:52:39Z"", ""categories"": [""eess.AS"", ""cs.LG""], ""pdf_url"": ""https://arxiv.org/pdf/2311.02898v2"", ""landing_url"": ""https://arxiv.org/abs/2311.02898v2"", ""doi"": ""https://doi.org/10.48550/arXiv.2311.02898""}",2311.02898,https://openalex.org/W4388512545
10.48550/arxiv.2311.04534,Loss Masking Is Not Needed in Decoder-only Transformer for Discrete-token-based ASR,"Recently, unified speech-text models, such as SpeechGPT, VioLA, and AudioPaLM, have achieved remarkable performance on various speech tasks. These models discretize speech signals into tokens (speech discretization) and use a shared vocabulary for both text and speech tokens. Then they train a single decoder-only Transformer on a mixture of speech tasks. However, these models rely on the Loss Masking strategy for the ASR task, which ignores the dependency among speech tokens. In this paper, we propose to model speech tokens in an autoregressive way, similar to text. We find that applying the conventional cross-entropy loss on input speech tokens does not consistently improve the ASR performance over the Loss Masking approach. To address this issue, we propose a novel approach denoted Smoothed Label Distillation (SLD), which applies a KL divergence loss with smoothed labels on speech tokens. Our experiments show that SLD effectively models speech tokens and outperforms Loss Masking for decoder-only Transformers in ASR tasks with different speech discretization methods. The source code can be found here: https://github.com/alibaba-damo-academy/SpokenNLP/tree/main/sld",1,,include (junior:5),https://arxiv.org/abs/2311.04534v2,https://arxiv.org/pdf/2311.04534v2,2023,discrete speech tokens,speech tokenization,"{""arxiv_id"": ""2311.04534"", ""title"": ""Loss Masking Is Not Needed in Decoder-only Transformer for Discrete-token-based ASR"", ""summary"": ""Recently, unified speech-text models, such as SpeechGPT, VioLA, and AudioPaLM, have achieved remarkable performance on various speech tasks. These models discretize speech signals into tokens (speech discretization) and use a shared vocabulary for both text and speech tokens. Then they train a single decoder-only Transformer on a mixture of speech tasks. However, these models rely on the Loss Masking strategy for the ASR task, which ignores the dependency among speech tokens. In this paper, we propose to model speech tokens in an autoregressive way, similar to text. We find that applying the conventional cross-entropy loss on input speech tokens does not consistently improve the ASR performance over the Loss Masking approach. To address this issue, we propose a novel approach denoted Smoothed Label Distillation (SLD), which applies a KL divergence loss with smoothed labels on speech tokens. Our experiments show that SLD effectively models speech tokens and outperforms Loss Masking for decoder-only Transformers in ASR tasks with different speech discretization methods. The source code can be found here: https://github.com/alibaba-damo-academy/SpokenNLP/tree/main/sld"", ""authors"": [""Qian Chen"", ""Wen Wang"", ""Qinglin Zhang"", ""Siqi Zheng"", ""Shiliang Zhang"", ""Chong Deng"", ""Yukun Ma"", ""Hai Yu"", ""Jiaqing Liu"", ""Chong Zhang""], ""published"": ""2023-11-08T08:45:14Z"", ""updated"": ""2024-02-05T02:42:57Z"", ""categories"": [""cs.CL"", ""cs.SD"", ""eess.AS""], ""pdf_url"": ""https://arxiv.org/pdf/2311.04534v2"", ""landing_url"": ""https://arxiv.org/abs/2311.04534v2"", ""doi"": ""https://doi.org/10.48550/arXiv.2311.04534""}",2311.04534,https://openalex.org/W4388555567
10.48550/arxiv.2401.01498,Utilizing Neural Transducers for Two-Stage Text-to-Speech via Semantic Token Prediction,"We propose a novel text-to-speech (TTS) framework centered around a neural transducer. Our approach divides the whole TTS pipeline into semantic-level sequence-to-sequence (seq2seq) modeling and fine-grained acoustic modeling stages, utilizing discrete semantic tokens obtained from wav2vec2.0 embeddings. For a robust and efficient alignment modeling, we employ a neural transducer named token transducer for the semantic token prediction, benefiting from its hard monotonic alignment constraints. Subsequently, a non-autoregressive (NAR) speech generator efficiently synthesizes waveforms from these semantic tokens. Additionally, a reference speech controls temporal dynamics and acoustic conditions at each stage. This decoupled framework reduces the training complexity of TTS while allowing each stage to focus on semantic and acoustic modeling. Our experimental results on zero-shot adaptive TTS demonstrate that our model surpasses the baseline in terms of speech quality and speaker similarity, both objectively and subjectively. We also delve into the inference speed and prosody control capabilities of our approach, highlighting the potential of neural transducers in TTS frameworks.",1,,include (junior:5),https://arxiv.org/abs/2401.01498v1,https://arxiv.org/pdf/2401.01498v1,2024,discrete speech tokens,semantic tokens,"{""arxiv_id"": ""2401.01498"", ""title"": ""Utilizing Neural Transducers for Two-Stage Text-to-Speech via Semantic Token Prediction"", ""summary"": ""We propose a novel text-to-speech (TTS) framework centered around a neural transducer. Our approach divides the whole TTS pipeline into semantic-level sequence-to-sequence (seq2seq) modeling and fine-grained acoustic modeling stages, utilizing discrete semantic tokens obtained from wav2vec2.0 embeddings. For a robust and efficient alignment modeling, we employ a neural transducer named token transducer for the semantic token prediction, benefiting from its hard monotonic alignment constraints. Subsequently, a non-autoregressive (NAR) speech generator efficiently synthesizes waveforms from these semantic tokens. Additionally, a reference speech controls temporal dynamics and acoustic conditions at each stage. This decoupled framework reduces the training complexity of TTS while allowing each stage to focus on semantic and acoustic modeling. Our experimental results on zero-shot adaptive TTS demonstrate that our model surpasses the baseline in terms of speech quality and speaker similarity, both objectively and subjectively. We also delve into the inference speed and prosody control capabilities of our approach, highlighting the potential of neural transducers in TTS frameworks."", ""authors"": [""Minchan Kim"", ""Myeonghun Jeong"", ""Byoung Jin Choi"", ""Semin Kim"", ""Joun Yeop Lee"", ""Nam Soo Kim""], ""published"": ""2024-01-03T02:03:36Z"", ""updated"": ""2024-01-03T02:03:36Z"", ""categories"": [""eess.AS"", ""cs.CL"", ""cs.LG"", ""cs.SD""], ""pdf_url"": ""https://arxiv.org/pdf/2401.01498v1"", ""landing_url"": ""https://arxiv.org/abs/2401.01498v1"", ""doi"": ""https://doi.org/10.48550/arXiv.2401.01498""}",2401.01498,https://openalex.org/W4390601257
10.48550/arxiv.2401.07333,ELLA-V: Stable Neural Codec Language Modeling with Alignment-guided Sequence Reordering,"The language model (LM) approach based on acoustic and linguistic prompts, such as VALL-E, has achieved remarkable progress in the field of zero-shot audio generation. However, existing methods still have some limitations: 1) repetitions, transpositions, and omissions in the output synthesized speech due to limited alignment constraints between audio and phoneme tokens; 2) challenges of fine-grained control over the synthesized speech with autoregressive (AR) language model; 3) infinite silence generation due to the nature of AR-based decoding, especially under the greedy strategy. To alleviate these issues, we propose ELLA-V, a simple but efficient LM-based zero-shot text-to-speech (TTS) framework, which enables fine-grained control over synthesized audio at the phoneme level. The key to ELLA-V is interleaving sequences of acoustic and phoneme tokens, where phoneme tokens appear ahead of the corresponding acoustic tokens. The experimental findings reveal that our model outperforms VALL-E in terms of accuracy and delivers more stable results using both greedy and sampling-based decoding strategies. The code of ELLA-V will be open-sourced after cleanups. Audio samples are available at https://ereboas.github.io/ELLAV/.",1,,include (junior:5),https://arxiv.org/abs/2401.07333v1,https://arxiv.org/pdf/2401.07333v1,2024,discrete speech tokens,acoustic tokens,"{""arxiv_id"": ""2401.07333"", ""title"": ""ELLA-V: Stable Neural Codec Language Modeling with Alignment-guided Sequence Reordering"", ""summary"": ""The language model (LM) approach based on acoustic and linguistic prompts, such as VALL-E, has achieved remarkable progress in the field of zero-shot audio generation. However, existing methods still have some limitations: 1) repetitions, transpositions, and omissions in the output synthesized speech due to limited alignment constraints between audio and phoneme tokens; 2) challenges of fine-grained control over the synthesized speech with autoregressive (AR) language model; 3) infinite silence generation due to the nature of AR-based decoding, especially under the greedy strategy. To alleviate these issues, we propose ELLA-V, a simple but efficient LM-based zero-shot text-to-speech (TTS) framework, which enables fine-grained control over synthesized audio at the phoneme level. The key to ELLA-V is interleaving sequences of acoustic and phoneme tokens, where phoneme tokens appear ahead of the corresponding acoustic tokens. The experimental findings reveal that our model outperforms VALL-E in terms of accuracy and delivers more stable results using both greedy and sampling-based decoding strategies. The code of ELLA-V will be open-sourced after cleanups. Audio samples are available at https://ereboas.github.io/ELLAV/."", ""authors"": [""Yakun Song"", ""Zhuo Chen"", ""Xiaofei Wang"", ""Ziyang Ma"", ""Xie Chen""], ""published"": ""2024-01-14T17:43:55Z"", ""updated"": ""2024-01-14T17:43:55Z"", ""categories"": [""cs.CL"", ""cs.AI"", ""cs.SD"", ""eess.AS""], ""pdf_url"": ""https://arxiv.org/pdf/2401.07333v1"", ""landing_url"": ""https://arxiv.org/abs/2401.07333v1"", ""doi"": ""https://doi.org/10.48550/arXiv.2401.07333""}",2401.07333,https://openalex.org/W4390962167
10.48550/arxiv.2402.02302,Predicting positive transfer for improved low-resource speech recognition using acoustic pseudo-tokens,"While massively multilingual speech models like wav2vec 2.0 XLSR-128 can be directly fine-tuned for automatic speech recognition (ASR), downstream performance can still be relatively poor on languages that are under-represented in the pre-training data. Continued pre-training on 70-200 hours of untranscribed speech in these languages can help -- but what about languages without that much recorded data? For such cases, we show that supplementing the target language with data from a similar, higher-resource 'donor' language can help. For example, continued pre-training on only 10 hours of low-resource Punjabi supplemented with 60 hours of donor Hindi is almost as good as continued pretraining on 70 hours of Punjabi. By contrast, sourcing data from less similar donors like Bengali does not improve ASR performance. To inform donor language selection, we propose a novel similarity metric based on the sequence distribution of induced acoustic units: the Acoustic Token Distribution Similarity (ATDS). Across a set of typologically different target languages (Punjabi, Galician, Iban, Setswana), we show that the ATDS between the target language and its candidate donors precisely predicts target language ASR performance.",1,,include (junior:5),https://arxiv.org/abs/2402.02302v1,https://arxiv.org/pdf/2402.02302v1,2024,discrete speech tokens,acoustic tokens,"{""arxiv_id"": ""2402.02302"", ""title"": ""Predicting positive transfer for improved low-resource speech recognition using acoustic pseudo-tokens"", ""summary"": ""While massively multilingual speech models like wav2vec 2.0 XLSR-128 can be directly fine-tuned for automatic speech recognition (ASR), downstream performance can still be relatively poor on languages that are under-represented in the pre-training data. Continued pre-training on 70-200 hours of untranscribed speech in these languages can help -- but what about languages without that much recorded data? For such cases, we show that supplementing the target language with data from a similar, higher-resource 'donor' language can help. For example, continued pre-training on only 10 hours of low-resource Punjabi supplemented with 60 hours of donor Hindi is almost as good as continued pretraining on 70 hours of Punjabi. By contrast, sourcing data from less similar donors like Bengali does not improve ASR performance. To inform donor language selection, we propose a novel similarity metric based on the sequence distribution of induced acoustic units: the Acoustic Token Distribution Similarity (ATDS). Across a set of typologically different target languages (Punjabi, Galician, Iban, Setswana), we show that the ATDS between the target language and its candidate donors precisely predicts target language ASR performance."", ""authors"": [""Nay San"", ""Georgios Paraskevopoulos"", ""Aryaman Arora"", ""Xiluo He"", ""Prabhjot Kaur"", ""Oliver Adams"", ""Dan Jurafsky""], ""published"": ""2024-02-03T23:54:03Z"", ""updated"": ""2024-02-03T23:54:03Z"", ""categories"": [""eess.AS"", ""cs.CL""], ""pdf_url"": ""https://arxiv.org/pdf/2402.02302v1"", ""landing_url"": ""https://arxiv.org/abs/2402.02302v1"", ""doi"": ""https://doi.org/10.48550/arXiv.2402.02302""}",2402.02302,https://openalex.org/W4391590935
10.48550/arxiv.2402.08093,BASE TTS: Lessons from building a billion-parameter Text-to-Speech model on 100K hours of data,"We introduce a text-to-speech (TTS) model called BASE TTS, which stands for $\textbf{B}$ig $\textbf{A}$daptive $\textbf{S}$treamable TTS with $\textbf{E}$mergent abilities. BASE TTS is the largest TTS model to-date, trained on 100K hours of public domain speech data, achieving a new state-of-the-art in speech naturalness. It deploys a 1-billion-parameter autoregressive Transformer that converts raw texts into discrete codes (""speechcodes"") followed by a convolution-based decoder which converts these speechcodes into waveforms in an incremental, streamable manner. Further, our speechcodes are built using a novel speech tokenization technique that features speaker ID disentanglement and compression with byte-pair encoding. Echoing the widely-reported ""emergent abilities"" of large language models when trained on increasing volume of data, we show that BASE TTS variants built with 10K+ hours and 500M+ parameters begin to demonstrate natural prosody on textually complex sentences. We design and share a specialized dataset to measure these emergent abilities for text-to-speech. We showcase state-of-the-art naturalness of BASE TTS by evaluating against baselines that include publicly available large-scale text-to-speech systems: YourTTS, Bark and TortoiseTTS. Audio samples generated by the model can be heard at https://amazon-ltts-paper.com/.",1,,include (junior:5),https://arxiv.org/abs/2402.08093v2,https://arxiv.org/pdf/2402.08093v2,2024,discrete speech tokens,speech tokenization,"{""arxiv_id"": ""2402.08093"", ""title"": ""BASE TTS: Lessons from building a billion-parameter Text-to-Speech model on 100K hours of data"", ""summary"": ""We introduce a text-to-speech (TTS) model called BASE TTS, which stands for $\\textbf{B}$ig $\\textbf{A}$daptive $\\textbf{S}$treamable TTS with $\\textbf{E}$mergent abilities. BASE TTS is the largest TTS model to-date, trained on 100K hours of public domain speech data, achieving a new state-of-the-art in speech naturalness. It deploys a 1-billion-parameter autoregressive Transformer that converts raw texts into discrete codes (\""speechcodes\"") followed by a convolution-based decoder which converts these speechcodes into waveforms in an incremental, streamable manner. Further, our speechcodes are built using a novel speech tokenization technique that features speaker ID disentanglement and compression with byte-pair encoding. Echoing the widely-reported \""emergent abilities\"" of large language models when trained on increasing volume of data, we show that BASE TTS variants built with 10K+ hours and 500M+ parameters begin to demonstrate natural prosody on textually complex sentences. We design and share a specialized dataset to measure these emergent abilities for text-to-speech. We showcase state-of-the-art naturalness of BASE TTS by evaluating against baselines that include publicly available large-scale text-to-speech systems: YourTTS, Bark and TortoiseTTS. Audio samples generated by the model can be heard at https://amazon-ltts-paper.com/."", ""authors"": [""Mateusz Łajszczak"", ""Guillermo Cámbara"", ""Yang Li"", ""Fatih Beyhan"", ""Arent van Korlaar"", ""Fan Yang"", ""Arnaud Joly"", ""Álvaro Martín-Cortinas"", ""Ammar Abbas"", ""Adam Michalski"", ""Alexis Moinet"", ""Sri Karlapati"", ""Ewa Muszyńska"", ""Haohan Guo"", ""Bartosz Putrycz"", ""Soledad López Gambino"", ""Kayeon Yoo"", ""Elena Sokolova"", ""Thomas Drugman""], ""published"": ""2024-02-12T22:21:30Z"", ""updated"": ""2024-02-15T18:57:26Z"", ""categories"": [""cs.LG"", ""cs.CL"", ""eess.AS""], ""pdf_url"": ""https://arxiv.org/pdf/2402.08093v2"", ""landing_url"": ""https://arxiv.org/abs/2402.08093v2"", ""doi"": ""https://doi.org/10.48550/arXiv.2402.08093""}",2402.08093,https://openalex.org/W4391833199
10.48550/arxiv.2402.12208,Language-Codec: Bridging Discrete Codec Representations and Speech Language Models,"In recent years, large language models have achieved significant success in generative tasks related to speech, audio, music, and other signal domains. A crucial element of these models is the discrete acoustic codecs, which serve as an intermediate representation replacing the mel-spectrogram. However, there exist several gaps between discrete codecs and downstream speech language models. Specifically, 1) Due to the reconstruction paradigm of the Codec model and the structure of residual vector quantization, the initial channel of the codebooks contains excessive information, making it challenging to directly generate acoustic tokens from weakly supervised signals such as text in downstream tasks. 2) numerous codebooks increases the burden on downstream speech language models. Consequently, leveraging the characteristics of speech language models, we propose Language-Codec. In the Language-Codec, we introduce a Masked Channel Residual Vector Quantization (MCRVQ) mechanism along with improved fourier transform structures and attention blocks, refined discriminator design to address the aforementioned gaps. We compare our method with competing audio compression algorithms and observe significant outperformance across extensive evaluations. Furthermore, we also validate the efficiency of the Language-Codec on downstream speech language models. The source code and pre-trained models can be accessed at https://github.com/jishengpeng/languagecodec .",1,,include (junior:5),https://arxiv.org/abs/2402.12208v4,https://arxiv.org/pdf/2402.12208v4,2024,discrete speech tokens,acoustic tokens,"{""arxiv_id"": ""2402.12208"", ""title"": ""Language-Codec: Bridging Discrete Codec Representations and Speech Language Models"", ""summary"": ""In recent years, large language models have achieved significant success in generative tasks related to speech, audio, music, and other signal domains. A crucial element of these models is the discrete acoustic codecs, which serve as an intermediate representation replacing the mel-spectrogram. However, there exist several gaps between discrete codecs and downstream speech language models. Specifically, 1) Due to the reconstruction paradigm of the Codec model and the structure of residual vector quantization, the initial channel of the codebooks contains excessive information, making it challenging to directly generate acoustic tokens from weakly supervised signals such as text in downstream tasks. 2) numerous codebooks increases the burden on downstream speech language models. Consequently, leveraging the characteristics of speech language models, we propose Language-Codec. In the Language-Codec, we introduce a Masked Channel Residual Vector Quantization (MCRVQ) mechanism along with improved fourier transform structures and attention blocks, refined discriminator design to address the aforementioned gaps. We compare our method with competing audio compression algorithms and observe significant outperformance across extensive evaluations. Furthermore, we also validate the efficiency of the Language-Codec on downstream speech language models. The source code and pre-trained models can be accessed at https://github.com/jishengpeng/languagecodec ."", ""authors"": [""Shengpeng Ji"", ""Minghui Fang"", ""Jialong Zuo"", ""Ziyue Jiang"", ""Dingdong Wang"", ""Hanting Wang"", ""Hai Huang"", ""Zhou Zhao""], ""published"": ""2024-02-19T15:12:12Z"", ""updated"": ""2025-06-04T05:50:15Z"", ""categories"": [""eess.AS"", ""cs.SD""], ""pdf_url"": ""https://arxiv.org/pdf/2402.12208v4"", ""landing_url"": ""https://arxiv.org/abs/2402.12208v4"", ""doi"": ""https://doi.org/10.48550/arXiv.2402.12208""}",2402.12208,https://openalex.org/W4392019360
10.48550/arxiv.2402.15725,Text-guided HuBERT: Self-Supervised Speech Pre-training via Generative Adversarial Networks,"Human language can be expressed in either written or spoken form, i.e. text or speech. Humans can acquire knowledge from text to improve speaking and listening. However, the quest for speech pre-trained models to leverage unpaired text has just started. In this paper, we investigate a new way to pre-train such a joint speech-text model to learn enhanced speech representations and benefit various speech-related downstream tasks. Specifically, we propose a novel pre-training method, text-guided HuBERT, or T-HuBERT, which performs self-supervised learning over speech to derive phoneme-like discrete representations. And these phoneme-like pseudo-label sequences are firstly derived from speech via the generative adversarial networks (GAN) to be statistically similar to those from additional unpaired textual data. In this way, we build a bridge between unpaired speech and text in an unsupervised manner. Extensive experiments demonstrate the significant superiority of our proposed method over various strong baselines, which achieves up to 15.3% relative Word Error Rate (WER) reduction on the LibriSpeech dataset.",1,,include (junior:5),https://arxiv.org/abs/2402.15725v5,https://arxiv.org/pdf/2402.15725v5,2024,discrete speech tokens,hubert,"{""arxiv_id"": ""2402.15725"", ""title"": ""Text-guided HuBERT: Self-Supervised Speech Pre-training via Generative Adversarial Networks"", ""summary"": ""Human language can be expressed in either written or spoken form, i.e. text or speech. Humans can acquire knowledge from text to improve speaking and listening. However, the quest for speech pre-trained models to leverage unpaired text has just started. In this paper, we investigate a new way to pre-train such a joint speech-text model to learn enhanced speech representations and benefit various speech-related downstream tasks. Specifically, we propose a novel pre-training method, text-guided HuBERT, or T-HuBERT, which performs self-supervised learning over speech to derive phoneme-like discrete representations. And these phoneme-like pseudo-label sequences are firstly derived from speech via the generative adversarial networks (GAN) to be statistically similar to those from additional unpaired textual data. In this way, we build a bridge between unpaired speech and text in an unsupervised manner. Extensive experiments demonstrate the significant superiority of our proposed method over various strong baselines, which achieves up to 15.3% relative Word Error Rate (WER) reduction on the LibriSpeech dataset."", ""authors"": [""Duo Ma"", ""Xianghu Yue"", ""Junyi Ao"", ""Xiaoxue Gao"", ""Haizhou Li""], ""published"": ""2024-02-24T05:30:23Z"", ""updated"": ""2024-08-03T12:58:45Z"", ""categories"": [""eess.AS""], ""pdf_url"": ""https://arxiv.org/pdf/2402.15725v5"", ""landing_url"": ""https://arxiv.org/abs/2402.15725v5"", ""doi"": ""https://doi.org/10.48550/arXiv.2402.15725""}",2402.15725,https://openalex.org/W4392223588
10.48550/arxiv.2402.15985,Phonetic and Lexical Discovery of a Canine Language using HuBERT,"This paper delves into the pioneering exploration of potential communication patterns within dog vocalizations and transcends traditional linguistic analysis barriers, which heavily relies on human priori knowledge on limited datasets to find sound units in dog vocalization. We present a self-supervised approach with HuBERT, enabling the accurate classification of phoneme labels and the identification of vocal patterns that suggest a rudimentary vocabulary within dog vocalizations. Our findings indicate a significant acoustic consistency in these identified canine vocabulary, covering the entirety of observed dog vocalization sequences. We further develop a web-based dog vocalization labeling system. This system can highlight phoneme n-grams, present in the vocabulary, in the dog audio uploaded by users.",1,,include (junior:4),https://arxiv.org/abs/2402.15985v1,https://arxiv.org/pdf/2402.15985v1,2024,discrete speech tokens,hubert,"{""arxiv_id"": ""2402.15985"", ""title"": ""Phonetic and Lexical Discovery of a Canine Language using HuBERT"", ""summary"": ""This paper delves into the pioneering exploration of potential communication patterns within dog vocalizations and transcends traditional linguistic analysis barriers, which heavily relies on human priori knowledge on limited datasets to find sound units in dog vocalization. We present a self-supervised approach with HuBERT, enabling the accurate classification of phoneme labels and the identification of vocal patterns that suggest a rudimentary vocabulary within dog vocalizations. Our findings indicate a significant acoustic consistency in these identified canine vocabulary, covering the entirety of observed dog vocalization sequences. We further develop a web-based dog vocalization labeling system. This system can highlight phoneme n-grams, present in the vocabulary, in the dog audio uploaded by users."", ""authors"": [""Xingyuan Li"", ""Sinong Wang"", ""Zeyu Xie"", ""Mengyue Wu"", ""Kenny Q. Zhu""], ""published"": ""2024-02-25T04:35:45Z"", ""updated"": ""2024-02-25T04:35:45Z"", ""categories"": [""cs.SD"", ""cs.CL"", ""cs.LG"", ""eess.AS""], ""pdf_url"": ""https://arxiv.org/pdf/2402.15985v1"", ""landing_url"": ""https://arxiv.org/abs/2402.15985v1"", ""doi"": ""https://doi.org/10.48550/arXiv.2402.15985""}",2402.15985,https://openalex.org/W4392224112
10.48550/arxiv.2403.16973,VoiceCraft: Zero-Shot Speech Editing and Text-to-Speech in the Wild,"We introduce VoiceCraft, a token infilling neural codec language model, that achieves state-of-the-art performance on both speech editing and zero-shot text-to-speech (TTS) on audiobooks, internet videos, and podcasts. VoiceCraft employs a Transformer decoder architecture and introduces a token rearrangement procedure that combines causal masking and delayed stacking to enable generation within an existing sequence. On speech editing tasks, VoiceCraft produces edited speech that is nearly indistinguishable from unedited recordings in terms of naturalness, as evaluated by humans; for zero-shot TTS, our model outperforms prior SotA models including VALLE and the popular commercial model XTTS-v2. Crucially, the models are evaluated on challenging and realistic datasets, that consist of diverse accents, speaking styles, recording conditions, and background noise and music, and our model performs consistently well compared to other models and real recordings. In particular, for speech editing evaluation, we introduce a high quality, challenging, and realistic dataset named RealEdit. We encourage readers to listen to the demos at https://jasonppy.github.io/VoiceCraft_web.",1,,include (junior:4),https://arxiv.org/abs/2403.16973v3,https://arxiv.org/pdf/2403.16973v3,2024,discrete speech tokens,neural codec,"{""arxiv_id"": ""2403.16973"", ""title"": ""VoiceCraft: Zero-Shot Speech Editing and Text-to-Speech in the Wild"", ""summary"": ""We introduce VoiceCraft, a token infilling neural codec language model, that achieves state-of-the-art performance on both speech editing and zero-shot text-to-speech (TTS) on audiobooks, internet videos, and podcasts. VoiceCraft employs a Transformer decoder architecture and introduces a token rearrangement procedure that combines causal masking and delayed stacking to enable generation within an existing sequence. On speech editing tasks, VoiceCraft produces edited speech that is nearly indistinguishable from unedited recordings in terms of naturalness, as evaluated by humans; for zero-shot TTS, our model outperforms prior SotA models including VALLE and the popular commercial model XTTS-v2. Crucially, the models are evaluated on challenging and realistic datasets, that consist of diverse accents, speaking styles, recording conditions, and background noise and music, and our model performs consistently well compared to other models and real recordings. In particular, for speech editing evaluation, we introduce a high quality, challenging, and realistic dataset named RealEdit. We encourage readers to listen to the demos at https://jasonppy.github.io/VoiceCraft_web."", ""authors"": [""Puyuan Peng"", ""Po-Yao Huang"", ""Shang-Wen Li"", ""Abdelrahman Mohamed"", ""David Harwath""], ""published"": ""2024-03-25T17:38:32Z"", ""updated"": ""2024-06-14T00:29:46Z"", ""categories"": [""eess.AS"", ""cs.AI"", ""cs.CL"", ""cs.LG"", ""cs.SD""], ""pdf_url"": ""https://arxiv.org/pdf/2403.16973v3"", ""landing_url"": ""https://arxiv.org/abs/2403.16973v3"", ""doi"": ""https://doi.org/10.48550/arXiv.2403.16973""}",2403.16973,https://openalex.org/W4393213897
10.18653/v1/2024.emnlp-main.21,Scaling Properties of Speech Language Models,"Speech Language Models (SLMs) aim to learn language from raw audio, without textual resources. Despite significant advances, our current models exhibit weak syntax and semantic abilities. However, if the scaling properties of neural language models hold for the speech modality, these abilities will improve as the amount of compute used for training increases. In this paper, we use models of this scaling behavior to estimate the scale at which our current methods will yield a SLM with the English proficiency of text-based Large Language Models (LLMs). We establish a strong correlation between pre-training loss and downstream syntactic and semantic performance in SLMs and LLMs, which results in predictable scaling of linguistic performance. We show that the linguistic performance of SLMs scales up to three orders of magnitude more slowly than that of text-based LLMs. Additionally, we study the benefits of synthetic data designed to boost semantic understanding and the effects of coarser speech tokenization.",1,,include (senior:4),https://arxiv.org/abs/2404.00685v2,https://arxiv.org/pdf/2404.00685v2,2024,discrete speech tokens,speech tokenization,"{""arxiv_id"": ""2404.00685"", ""title"": ""Scaling Properties of Speech Language Models"", ""summary"": ""Speech Language Models (SLMs) aim to learn language from raw audio, without textual resources. Despite significant advances, our current models exhibit weak syntax and semantic abilities. However, if the scaling properties of neural language models hold for the speech modality, these abilities will improve as the amount of compute used for training increases. In this paper, we use models of this scaling behavior to estimate the scale at which our current methods will yield a SLM with the English proficiency of text-based Large Language Models (LLMs). We establish a strong correlation between pre-training loss and downstream syntactic and semantic performance in SLMs and LLMs, which results in predictable scaling of linguistic performance. We show that the linguistic performance of SLMs scales up to three orders of magnitude more slowly than that of text-based LLMs. Additionally, we study the benefits of synthetic data designed to boost semantic understanding and the effects of coarser speech tokenization."", ""authors"": [""Santiago Cuervo"", ""Ricard Marxer""], ""published"": ""2024-03-31T13:30:12Z"", ""updated"": ""2024-04-16T06:46:18Z"", ""categories"": [""eess.AS"", ""cs.AI"", ""cs.CL"", ""cs.NE""], ""pdf_url"": ""https://arxiv.org/pdf/2404.00685v2"", ""landing_url"": ""https://arxiv.org/abs/2404.00685v2"", ""doi"": ""https://doi.org/10.18653/v1/2024.emnlp-main.21""}",2404.00685,https://openalex.org/W4404782909
10.48550/arxiv.2404.02781,CLaM-TTS: Improving Neural Codec Language Model for Zero-Shot Text-to-Speech,"With the emergence of neural audio codecs, which encode multiple streams of discrete tokens from audio, large language models have recently gained attention as a promising approach for zero-shot Text-to-Speech (TTS) synthesis. Despite the ongoing rush towards scaling paradigms, audio tokenization ironically amplifies the scalability challenge, stemming from its long sequence length and the complexity of modelling the multiple sequences. To mitigate these issues, we present CLaM-TTS that employs a probabilistic residual vector quantization to (1) achieve superior compression in the token length, and (2) allow a language model to generate multiple tokens at once, thereby eliminating the need for cascaded modeling to handle the number of token streams. Our experimental results demonstrate that CLaM-TTS is better than or comparable to state-of-the-art neural codec-based TTS models regarding naturalness, intelligibility, speaker similarity, and inference speed. In addition, we examine the impact of the pretraining extent of the language models and their text tokenization strategies on performances.",1,,include (junior:5),https://arxiv.org/abs/2404.02781v1,https://arxiv.org/pdf/2404.02781v1,2024,discrete speech tokens,neural codec,"{""arxiv_id"": ""2404.02781"", ""title"": ""CLaM-TTS: Improving Neural Codec Language Model for Zero-Shot Text-to-Speech"", ""summary"": ""With the emergence of neural audio codecs, which encode multiple streams of discrete tokens from audio, large language models have recently gained attention as a promising approach for zero-shot Text-to-Speech (TTS) synthesis. Despite the ongoing rush towards scaling paradigms, audio tokenization ironically amplifies the scalability challenge, stemming from its long sequence length and the complexity of modelling the multiple sequences. To mitigate these issues, we present CLaM-TTS that employs a probabilistic residual vector quantization to (1) achieve superior compression in the token length, and (2) allow a language model to generate multiple tokens at once, thereby eliminating the need for cascaded modeling to handle the number of token streams. Our experimental results demonstrate that CLaM-TTS is better than or comparable to state-of-the-art neural codec-based TTS models regarding naturalness, intelligibility, speaker similarity, and inference speed. In addition, we examine the impact of the pretraining extent of the language models and their text tokenization strategies on performances."", ""authors"": [""Jaehyeon Kim"", ""Keon Lee"", ""Seungjun Chung"", ""Jaewoong Cho""], ""published"": ""2024-04-03T14:52:20Z"", ""updated"": ""2024-04-03T14:52:20Z"", ""categories"": [""eess.AS"", ""cs.SD""], ""pdf_url"": ""https://arxiv.org/pdf/2404.02781v1"", ""landing_url"": ""https://arxiv.org/abs/2404.02781v1"", ""doi"": ""https://doi.org/10.48550/arXiv.2404.02781""}",2404.02781,https://openalex.org/W4393969253
10.48550/arxiv.2404.06079,The X-LANCE Technical Report for Interspeech 2024 Speech Processing Using Discrete Speech Unit Challenge,"Discrete speech tokens have been more and more popular in multiple speech processing fields, including automatic speech recognition (ASR), text-to-speech (TTS) and singing voice synthesis (SVS). In this paper, we describe the systems developed by the SJTU X-LANCE group for the TTS (acoustic + vocoder), SVS, and ASR tracks in the Interspeech 2024 Speech Processing Using Discrete Speech Unit Challenge. Notably, we achieved 1st rank on the leaderboard in the TTS track both with the whole training set and only 1h training data, with the highest UTMOS score and lowest bitrate among all submissions.",1,,include (junior:5),https://arxiv.org/abs/2404.06079v2,https://arxiv.org/pdf/2404.06079v2,2024,discrete speech tokens,offline clustering,"{""arxiv_id"": ""2404.06079"", ""title"": ""The X-LANCE Technical Report for Interspeech 2024 Speech Processing Using Discrete Speech Unit Challenge"", ""summary"": ""Discrete speech tokens have been more and more popular in multiple speech processing fields, including automatic speech recognition (ASR), text-to-speech (TTS) and singing voice synthesis (SVS). In this paper, we describe the systems developed by the SJTU X-LANCE group for the TTS (acoustic + vocoder), SVS, and ASR tracks in the Interspeech 2024 Speech Processing Using Discrete Speech Unit Challenge. Notably, we achieved 1st rank on the leaderboard in the TTS track both with the whole training set and only 1h training data, with the highest UTMOS score and lowest bitrate among all submissions."", ""authors"": [""Yiwei Guo"", ""Chenrun Wang"", ""Yifan Yang"", ""Hankun Wang"", ""Ziyang Ma"", ""Chenpeng Du"", ""Shuai Wang"", ""Hanzheng Li"", ""Shuai Fan"", ""Hui Zhang"", ""Xie Chen"", ""Kai Yu""], ""published"": ""2024-04-09T07:37:41Z"", ""updated"": ""2024-04-10T00:33:25Z"", ""categories"": [""eess.AS"", ""cs.AI""], ""pdf_url"": ""https://arxiv.org/pdf/2404.06079v2"", ""landing_url"": ""https://arxiv.org/abs/2404.06079v2"", ""doi"": ""https://doi.org/10.48550/arXiv.2404.06079""}",2404.06079,https://openalex.org/W4394774425
10.48550/arxiv.2404.19441,ESC: Efficient Speech Coding with Cross-Scale Residual Vector Quantized Transformers,"Neural speech codecs aim to compress input signals into minimal bits while maintaining content quality in a low-latency manner. However, existing neural codecs often trade model complexity for reconstruction performance. These codecs primarily use convolutional blocks for feature transformation, which are not inherently suited for capturing the local redundancies in speech signals. To compensate, they require either adversarial discriminators or a large number of model parameters to enhance audio quality. In response to these challenges, we introduce the Efficient Speech Codec (ESC), a lightweight, parameter-efficient speech codec based on a cross-scale residual vector quantization scheme and transformers. Our model employs mirrored hierarchical window transformer blocks and performs step-wise decoding from coarse-to-fine feature representations. To enhance bitrate efficiency, we propose a novel combination of vector quantization techniques along with a pre-training paradigm. Extensive experiments demonstrate that ESC can achieve high-fidelity speech reconstruction with significantly lower model complexity, making it a promising alternative to existing convolutional audio codecs.",1,,include (junior:5),https://arxiv.org/abs/2404.19441v3,https://arxiv.org/pdf/2404.19441v3,2024,discrete speech tokens,vector quantization,"{""arxiv_id"": ""2404.19441"", ""title"": ""ESC: Efficient Speech Coding with Cross-Scale Residual Vector Quantized Transformers"", ""summary"": ""Neural speech codecs aim to compress input signals into minimal bits while maintaining content quality in a low-latency manner. However, existing neural codecs often trade model complexity for reconstruction performance. These codecs primarily use convolutional blocks for feature transformation, which are not inherently suited for capturing the local redundancies in speech signals. To compensate, they require either adversarial discriminators or a large number of model parameters to enhance audio quality. In response to these challenges, we introduce the Efficient Speech Codec (ESC), a lightweight, parameter-efficient speech codec based on a cross-scale residual vector quantization scheme and transformers. Our model employs mirrored hierarchical window transformer blocks and performs step-wise decoding from coarse-to-fine feature representations. To enhance bitrate efficiency, we propose a novel combination of vector quantization techniques along with a pre-training paradigm. Extensive experiments demonstrate that ESC can achieve high-fidelity speech reconstruction with significantly lower model complexity, making it a promising alternative to existing convolutional audio codecs."", ""authors"": [""Yuzhe Gu"", ""Enmao Diao""], ""published"": ""2024-04-30T10:44:33Z"", ""updated"": ""2024-10-03T12:23:26Z"", ""categories"": [""cs.SD"", ""eess.AS""], ""pdf_url"": ""https://arxiv.org/pdf/2404.19441v3"", ""landing_url"": ""https://arxiv.org/abs/2404.19441v3"", ""doi"": ""https://doi.org/10.48550/arXiv.2404.19441""}",2404.19441,https://openalex.org/W4396821491
10.48550/arxiv.2405.09768,Evaluating Text-to-Speech Synthesis from a Large Discrete Token-based Speech Language Model,"Recent advances in generative language modeling applied to discrete speech tokens presented a new avenue for text-to-speech (TTS) synthesis. These speech language models (SLMs), similarly to their textual counterparts, are scalable, probabilistic, and context-aware. While they can produce diverse and natural outputs, they sometimes face issues such as unintelligibility and the inclusion of non-speech noises or hallucination. As the adoption of this innovative paradigm in speech synthesis increases, there is a clear need for an in-depth evaluation of its capabilities and limitations. In this paper, we evaluate TTS from a discrete token-based SLM, through both automatic metrics and listening tests. We examine five key dimensions: speaking style, intelligibility, speaker consistency, prosodic variation, spontaneous behaviour. Our results highlight the model's strength in generating varied prosody and spontaneous outputs. It is also rated higher in naturalness and context appropriateness in listening tests compared to a conventional TTS. However, the model's performance in intelligibility and speaker consistency lags behind traditional TTS. Additionally, we show that increasing the scale of SLMs offers a modest boost in robustness. Our findings aim to serve as a benchmark for future advancements in generative SLMs for speech synthesis.",1,,include (junior:5),https://arxiv.org/abs/2405.09768v1,https://arxiv.org/pdf/2405.09768v1,2024,discrete speech tokens,offline clustering,"{""arxiv_id"": ""2405.09768"", ""title"": ""Evaluating Text-to-Speech Synthesis from a Large Discrete Token-based Speech Language Model"", ""summary"": ""Recent advances in generative language modeling applied to discrete speech tokens presented a new avenue for text-to-speech (TTS) synthesis. These speech language models (SLMs), similarly to their textual counterparts, are scalable, probabilistic, and context-aware. While they can produce diverse and natural outputs, they sometimes face issues such as unintelligibility and the inclusion of non-speech noises or hallucination. As the adoption of this innovative paradigm in speech synthesis increases, there is a clear need for an in-depth evaluation of its capabilities and limitations. In this paper, we evaluate TTS from a discrete token-based SLM, through both automatic metrics and listening tests. We examine five key dimensions: speaking style, intelligibility, speaker consistency, prosodic variation, spontaneous behaviour. Our results highlight the model's strength in generating varied prosody and spontaneous outputs. It is also rated higher in naturalness and context appropriateness in listening tests compared to a conventional TTS. However, the model's performance in intelligibility and speaker consistency lags behind traditional TTS. Additionally, we show that increasing the scale of SLMs offers a modest boost in robustness. Our findings aim to serve as a benchmark for future advancements in generative SLMs for speech synthesis."", ""authors"": [""Siyang Wang"", ""Éva Székely""], ""published"": ""2024-05-16T02:18:41Z"", ""updated"": ""2024-05-16T02:18:41Z"", ""categories"": [""eess.AS"", ""cs.SD""], ""pdf_url"": ""https://arxiv.org/pdf/2405.09768v1"", ""landing_url"": ""https://arxiv.org/abs/2405.09768v1"", ""doi"": ""https://doi.org/10.48550/arXiv.2405.09768""}",2405.09768,https://openalex.org/W4397028227
10.48550/arxiv.2405.16136,C3LLM: Conditional Multimodal Content Generation Using Large Language Models,"We introduce C3LLM (Conditioned-on-Three-Modalities Large Language Models), a novel framework combining three tasks of video-to-audio, audio-to-text, and text-to-audio together. C3LLM adapts the Large Language Model (LLM) structure as a bridge for aligning different modalities, synthesizing the given conditional information, and making multimodal generation in a discrete manner. Our contributions are as follows. First, we adapt a hierarchical structure for audio generation tasks with pre-trained audio codebooks. Specifically, we train the LLM to generate audio semantic tokens from the given conditions, and further use a non-autoregressive transformer to generate different levels of acoustic tokens in layers to better enhance the fidelity of the generated audio. Second, based on the intuition that LLMs were originally designed for discrete tasks with the next-word prediction method, we use the discrete representation for audio generation and compress their semantic meanings into acoustic tokens, similar to adding ""acoustic vocabulary"" to LLM. Third, our method combines the previous tasks of audio understanding, video-to-audio generation, and text-to-audio generation together into one unified model, providing more versatility in an end-to-end fashion. Our C3LLM achieves improved results through various automated evaluation metrics, providing better semantic alignment compared to previous methods.",1,,include (junior:5),https://arxiv.org/abs/2405.16136v1,https://arxiv.org/pdf/2405.16136v1,2024,discrete speech tokens,acoustic tokens,"{""arxiv_id"": ""2405.16136"", ""title"": ""C3LLM: Conditional Multimodal Content Generation Using Large Language Models"", ""summary"": ""We introduce C3LLM (Conditioned-on-Three-Modalities Large Language Models), a novel framework combining three tasks of video-to-audio, audio-to-text, and text-to-audio together. C3LLM adapts the Large Language Model (LLM) structure as a bridge for aligning different modalities, synthesizing the given conditional information, and making multimodal generation in a discrete manner. Our contributions are as follows. First, we adapt a hierarchical structure for audio generation tasks with pre-trained audio codebooks. Specifically, we train the LLM to generate audio semantic tokens from the given conditions, and further use a non-autoregressive transformer to generate different levels of acoustic tokens in layers to better enhance the fidelity of the generated audio. Second, based on the intuition that LLMs were originally designed for discrete tasks with the next-word prediction method, we use the discrete representation for audio generation and compress their semantic meanings into acoustic tokens, similar to adding \""acoustic vocabulary\"" to LLM. Third, our method combines the previous tasks of audio understanding, video-to-audio generation, and text-to-audio generation together into one unified model, providing more versatility in an end-to-end fashion. Our C3LLM achieves improved results through various automated evaluation metrics, providing better semantic alignment compared to previous methods."", ""authors"": [""Zixuan Wang"", ""Qinkai Duan"", ""Yu-Wing Tai"", ""Chi-Keung Tang""], ""published"": ""2024-05-25T09:10:12Z"", ""updated"": ""2024-05-25T09:10:12Z"", ""categories"": [""cs.AI"", ""cs.CL"", ""cs.LG"", ""cs.SD"", ""eess.AS""], ""pdf_url"": ""https://arxiv.org/pdf/2405.16136v1"", ""landing_url"": ""https://arxiv.org/abs/2405.16136v1"", ""doi"": ""https://doi.org/10.48550/arXiv.2405.16136""}",2405.16136,https://openalex.org/W4399115330
10.48550/arxiv.2406.00976,Generative Pre-trained Speech Language Model with Efficient Hierarchical Transformer,"While recent advancements in speech language models have achieved significant progress, they face remarkable challenges in modeling the long acoustic sequences of neural audio codecs. In this paper, we introduce \textbf{G}enerative \textbf{P}re-trained \textbf{S}peech \textbf{T}ransformer (GPST), a hierarchical transformer designed for efficient speech language modeling. GPST quantizes audio waveforms into two distinct types of discrete speech representations and integrates them within a hierarchical transformer architecture, allowing for a unified one-stage generation process and enhancing Hi-Res audio generation capabilities. By training on large corpora of speeches in an end-to-end unsupervised manner, GPST can generate syntactically consistent speech with diverse speaker identities. Given a brief 3-second prompt, GPST can produce natural and coherent personalized speech, demonstrating in-context learning abilities. Moreover, our approach can be easily extended to spoken cross-lingual speech generation by incorporating multi-lingual semantic tokens and universal acoustic tokens. Experimental results indicate that GPST significantly outperforms the existing speech language models in terms of word error rate, speech quality, and speaker similarity. The code is available at \url{https://github.com/youngsheen/GPST}.",1,,include (junior:5),https://arxiv.org/abs/2406.00976v2,https://arxiv.org/pdf/2406.00976v2,2024,discrete speech tokens,acoustic tokens,"{""arxiv_id"": ""2406.00976"", ""title"": ""Generative Pre-trained Speech Language Model with Efficient Hierarchical Transformer"", ""summary"": ""While recent advancements in speech language models have achieved significant progress, they face remarkable challenges in modeling the long acoustic sequences of neural audio codecs. In this paper, we introduce \\textbf{G}enerative \\textbf{P}re-trained \\textbf{S}peech \\textbf{T}ransformer (GPST), a hierarchical transformer designed for efficient speech language modeling. GPST quantizes audio waveforms into two distinct types of discrete speech representations and integrates them within a hierarchical transformer architecture, allowing for a unified one-stage generation process and enhancing Hi-Res audio generation capabilities. By training on large corpora of speeches in an end-to-end unsupervised manner, GPST can generate syntactically consistent speech with diverse speaker identities. Given a brief 3-second prompt, GPST can produce natural and coherent personalized speech, demonstrating in-context learning abilities. Moreover, our approach can be easily extended to spoken cross-lingual speech generation by incorporating multi-lingual semantic tokens and universal acoustic tokens. Experimental results indicate that GPST significantly outperforms the existing speech language models in terms of word error rate, speech quality, and speaker similarity. The code is available at \\url{https://github.com/youngsheen/GPST}."", ""authors"": [""Yongxin Zhu"", ""Dan Su"", ""Liqiang He"", ""Linli Xu"", ""Dong Yu""], ""published"": ""2024-06-03T04:16:30Z"", ""updated"": ""2024-11-01T13:54:48Z"", ""categories"": [""cs.CL"", ""cs.SD"", ""eess.AS""], ""pdf_url"": ""https://arxiv.org/pdf/2406.00976v2"", ""landing_url"": ""https://arxiv.org/abs/2406.00976v2"", ""doi"": ""https://doi.org/10.48550/arXiv.2406.00976""}",2406.00976,https://openalex.org/W4399400026
10.48550/arxiv.2406.02092,MaskSR: Masked Language Model for Full-band Speech Restoration,"Speech restoration aims at restoring high quality speech in the presence of a diverse set of distortions. Although several deep learning paradigms have been studied for this task, the power of the recently emerging language models has not been fully explored. In this paper, we propose MaskSR, a masked language model capable of restoring full-band 44.1 kHz speech jointly considering noise, reverb, clipping, and low bandwidth. MaskSR works with discrete acoustic tokens extracted using a pre-trained neural codec. During training, MaskSR is optimized to predict randomly masked tokens extracted from the high quality target speech, conditioned on the corrupted speech with various distortions. During inference, MaskSR reconstructs the target speech tokens with efficient iterative sampling. Extensive experiments show that MaskSR obtains competitive results on both the full-band speech restoration task and also on sub-tasks compared with a wide range of models.",1,,include (junior:5),https://arxiv.org/abs/2406.02092v1,https://arxiv.org/pdf/2406.02092v1,2024,discrete speech tokens,acoustic tokens,"{""arxiv_id"": ""2406.02092"", ""title"": ""MaskSR: Masked Language Model for Full-band Speech Restoration"", ""summary"": ""Speech restoration aims at restoring high quality speech in the presence of a diverse set of distortions. Although several deep learning paradigms have been studied for this task, the power of the recently emerging language models has not been fully explored. In this paper, we propose MaskSR, a masked language model capable of restoring full-band 44.1 kHz speech jointly considering noise, reverb, clipping, and low bandwidth. MaskSR works with discrete acoustic tokens extracted using a pre-trained neural codec. During training, MaskSR is optimized to predict randomly masked tokens extracted from the high quality target speech, conditioned on the corrupted speech with various distortions. During inference, MaskSR reconstructs the target speech tokens with efficient iterative sampling. Extensive experiments show that MaskSR obtains competitive results on both the full-band speech restoration task and also on sub-tasks compared with a wide range of models."", ""authors"": [""Xu Li"", ""Qirui Wang"", ""Xiaoyu Liu""], ""published"": ""2024-06-04T08:23:57Z"", ""updated"": ""2024-06-04T08:23:57Z"", ""categories"": [""cs.SD"", ""cs.AI"", ""cs.LG"", ""eess.AS"", ""eess.SP""], ""pdf_url"": ""https://arxiv.org/pdf/2406.02092v1"", ""landing_url"": ""https://arxiv.org/abs/2406.02092v1"", ""doi"": ""https://doi.org/10.48550/arXiv.2406.02092""}",2406.02092,https://openalex.org/W4399416856
10.48550/arxiv.2406.02940,Addressing Index Collapse of Large-Codebook Speech Tokenizer with Dual-Decoding Product-Quantized Variational Auto-Encoder,"VQ-VAE, as a mainstream approach of speech tokenizer, has been troubled by ``index collapse'', where only a small number of codewords are activated in large codebooks. This work proposes product-quantized (PQ) VAE with more codebooks but fewer codewords to address this problem and build large-codebook speech tokenizers. It encodes speech features into multiple VQ subspaces and composes them into codewords in a larger codebook. Besides, to utilize each VQ subspace well, we also enhance PQ-VAE via a dual-decoding training strategy with the encoding and quantized sequences. The experimental results demonstrate that PQ-VAE addresses ``index collapse"" effectively, especially for larger codebooks. The model with the proposed training strategy further improves codebook perplexity and reconstruction quality, outperforming other multi-codebook VQ approaches. Finally, PQ-VAE demonstrates its effectiveness in language-model-based TTS, supporting higher-quality speech generation with larger codebooks.",1,,include (junior:5),https://arxiv.org/abs/2406.02940v1,https://arxiv.org/pdf/2406.02940v1,2024,discrete speech tokens,speech tokenization,"{""arxiv_id"": ""2406.02940"", ""title"": ""Addressing Index Collapse of Large-Codebook Speech Tokenizer with Dual-Decoding Product-Quantized Variational Auto-Encoder"", ""summary"": ""VQ-VAE, as a mainstream approach of speech tokenizer, has been troubled by ``index collapse'', where only a small number of codewords are activated in large codebooks. This work proposes product-quantized (PQ) VAE with more codebooks but fewer codewords to address this problem and build large-codebook speech tokenizers. It encodes speech features into multiple VQ subspaces and composes them into codewords in a larger codebook. Besides, to utilize each VQ subspace well, we also enhance PQ-VAE via a dual-decoding training strategy with the encoding and quantized sequences. The experimental results demonstrate that PQ-VAE addresses ``index collapse\"" effectively, especially for larger codebooks. The model with the proposed training strategy further improves codebook perplexity and reconstruction quality, outperforming other multi-codebook VQ approaches. Finally, PQ-VAE demonstrates its effectiveness in language-model-based TTS, supporting higher-quality speech generation with larger codebooks."", ""authors"": [""Haohan Guo"", ""Fenglong Xie"", ""Dongchao Yang"", ""Hui Lu"", ""Xixin Wu"", ""Helen Meng""], ""published"": ""2024-06-05T04:54:49Z"", ""updated"": ""2024-06-05T04:54:49Z"", ""categories"": [""cs.SD"", ""eess.AS""], ""pdf_url"": ""https://arxiv.org/pdf/2406.02940v1"", ""landing_url"": ""https://arxiv.org/abs/2406.02940v1"", ""doi"": ""https://doi.org/10.48550/arXiv.2406.02940""}",2406.0294,https://openalex.org/W4399447613
10.48550/arxiv.2406.03706,Improving Audio Codec-based Zero-Shot Text-to-Speech Synthesis with Multi-Modal Context and Large Language Model,"Recent advances in large language models (LLMs) and development of audio codecs greatly propel the zero-shot TTS. They can synthesize personalized speech with only a 3-second speech of an unseen speaker as acoustic prompt. However, they only support short speech prompts and cannot leverage longer context information, as required in audiobook and conversational TTS scenarios. In this paper, we introduce a novel audio codec-based TTS model to adapt context features with multiple enhancements. Inspired by the success of Qformer, we propose a multi-modal context-enhanced Qformer (MMCE-Qformer) to utilize additional multi-modal context information. Besides, we adapt a pretrained LLM to leverage its understanding ability to predict semantic tokens, and use a SoundStorm to generate acoustic tokens thereby enhancing audio quality and speaker similarity. The extensive objective and subjective evaluations show that our proposed method outperforms baselines across various context TTS scenarios.",1,,include (junior:4),https://arxiv.org/abs/2406.03706v1,https://arxiv.org/pdf/2406.03706v1,2024,discrete speech tokens,acoustic tokens,"{""arxiv_id"": ""2406.03706"", ""title"": ""Improving Audio Codec-based Zero-Shot Text-to-Speech Synthesis with Multi-Modal Context and Large Language Model"", ""summary"": ""Recent advances in large language models (LLMs) and development of audio codecs greatly propel the zero-shot TTS. They can synthesize personalized speech with only a 3-second speech of an unseen speaker as acoustic prompt. However, they only support short speech prompts and cannot leverage longer context information, as required in audiobook and conversational TTS scenarios. In this paper, we introduce a novel audio codec-based TTS model to adapt context features with multiple enhancements. Inspired by the success of Qformer, we propose a multi-modal context-enhanced Qformer (MMCE-Qformer) to utilize additional multi-modal context information. Besides, we adapt a pretrained LLM to leverage its understanding ability to predict semantic tokens, and use a SoundStorm to generate acoustic tokens thereby enhancing audio quality and speaker similarity. The extensive objective and subjective evaluations show that our proposed method outperforms baselines across various context TTS scenarios."", ""authors"": [""Jinlong Xue"", ""Yayue Deng"", ""Yicheng Han"", ""Yingming Gao"", ""Ya Li""], ""published"": ""2024-06-06T03:06:45Z"", ""updated"": ""2024-06-06T03:06:45Z"", ""categories"": [""cs.SD"", ""cs.CL"", ""eess.AS""], ""pdf_url"": ""https://arxiv.org/pdf/2406.03706v1"", ""landing_url"": ""https://arxiv.org/abs/2406.03706v1"", ""doi"": ""https://doi.org/10.48550/arXiv.2406.03706""}",2406.03706,https://openalex.org/W4399454058
10.48550/arxiv.2406.04633,Boosting Diffusion Model for Spectrogram Up-sampling in Text-to-speech: An Empirical Study,"Scaling text-to-speech (TTS) with autoregressive language model (LM) to large-scale datasets by quantizing waveform into discrete speech tokens is making great progress to capture the diversity and expressiveness in human speech, but the speech reconstruction quality from discrete speech token is far from satisfaction depending on the compressed speech token compression ratio. Generative diffusion models trained with score-matching loss and continuous normalized flow trained with flow-matching loss have become prominent in generation of images as well as speech. LM based TTS systems usually quantize speech into discrete tokens and generate these tokens autoregressively, and finally use a diffusion model to up sample coarse-grained speech tokens into fine-grained codec features or mel-spectrograms before reconstructing into waveforms with vocoder, which has a high latency and is not realistic for real time speech applications. In this paper, we systematically investigate varied diffusion models for up sampling stage, which is the main bottleneck for streaming synthesis of LM and diffusion-based architecture, we present the model architecture, objective and subjective metrics to show quality and efficiency improvement.",1,,include (junior:5),https://arxiv.org/abs/2406.04633v1,https://arxiv.org/pdf/2406.04633v1,2024,discrete speech tokens,offline clustering,"{""arxiv_id"": ""2406.04633"", ""title"": ""Boosting Diffusion Model for Spectrogram Up-sampling in Text-to-speech: An Empirical Study"", ""summary"": ""Scaling text-to-speech (TTS) with autoregressive language model (LM) to large-scale datasets by quantizing waveform into discrete speech tokens is making great progress to capture the diversity and expressiveness in human speech, but the speech reconstruction quality from discrete speech token is far from satisfaction depending on the compressed speech token compression ratio. Generative diffusion models trained with score-matching loss and continuous normalized flow trained with flow-matching loss have become prominent in generation of images as well as speech. LM based TTS systems usually quantize speech into discrete tokens and generate these tokens autoregressively, and finally use a diffusion model to up sample coarse-grained speech tokens into fine-grained codec features or mel-spectrograms before reconstructing into waveforms with vocoder, which has a high latency and is not realistic for real time speech applications. In this paper, we systematically investigate varied diffusion models for up sampling stage, which is the main bottleneck for streaming synthesis of LM and diffusion-based architecture, we present the model architecture, objective and subjective metrics to show quality and efficiency improvement."", ""authors"": [""Chong Zhang"", ""Yanqing Liu"", ""Yang Zheng"", ""Sheng Zhao""], ""published"": ""2024-06-07T04:34:03Z"", ""updated"": ""2024-06-07T04:34:03Z"", ""categories"": [""eess.AS""], ""pdf_url"": ""https://arxiv.org/pdf/2406.04633v1"", ""landing_url"": ""https://arxiv.org/abs/2406.04633v1"", ""doi"": ""https://doi.org/10.48550/arXiv.2406.04633""}",2406.04633,https://openalex.org/W4399511870
10.48550/arxiv.2406.05370,VALL-E 2: Neural Codec Language Models are Human Parity Zero-Shot Text to Speech Synthesizers,"This paper introduces VALL-E 2, the latest advancement in neural codec language models that marks a milestone in zero-shot text-to-speech synthesis (TTS), achieving human parity for the first time. Based on its predecessor, VALL-E, the new iteration introduces two significant enhancements: Repetition Aware Sampling refines the original nucleus sampling process by accounting for token repetition in the decoding history. It not only stabilizes the decoding but also circumvents the infinite loop issue. Grouped Code Modeling organizes codec codes into groups to effectively shorten the sequence length, which not only boosts inference speed but also addresses the challenges of long sequence modeling. Our experiments on the LibriSpeech and VCTK datasets show that VALL-E 2 surpasses previous systems in speech robustness, naturalness, and speaker similarity. It is the first of its kind to reach human parity on these benchmarks. Moreover, VALL-E 2 consistently synthesizes high-quality speech, even for sentences that are traditionally challenging due to their complexity or repetitive phrases. The advantages of this work could contribute to valuable endeavors, such as generating speech for individuals with aphasia or people with amyotrophic lateral sclerosis. See https://aka.ms/valle2 for demos of VALL-E 2.",1,,include (junior:5),https://arxiv.org/abs/2406.05370v2,https://arxiv.org/pdf/2406.05370v2,2024,discrete speech tokens,neural codec,"{""arxiv_id"": ""2406.05370"", ""title"": ""VALL-E 2: Neural Codec Language Models are Human Parity Zero-Shot Text to Speech Synthesizers"", ""summary"": ""This paper introduces VALL-E 2, the latest advancement in neural codec language models that marks a milestone in zero-shot text-to-speech synthesis (TTS), achieving human parity for the first time. Based on its predecessor, VALL-E, the new iteration introduces two significant enhancements: Repetition Aware Sampling refines the original nucleus sampling process by accounting for token repetition in the decoding history. It not only stabilizes the decoding but also circumvents the infinite loop issue. Grouped Code Modeling organizes codec codes into groups to effectively shorten the sequence length, which not only boosts inference speed but also addresses the challenges of long sequence modeling. Our experiments on the LibriSpeech and VCTK datasets show that VALL-E 2 surpasses previous systems in speech robustness, naturalness, and speaker similarity. It is the first of its kind to reach human parity on these benchmarks. Moreover, VALL-E 2 consistently synthesizes high-quality speech, even for sentences that are traditionally challenging due to their complexity or repetitive phrases. The advantages of this work could contribute to valuable endeavors, such as generating speech for individuals with aphasia or people with amyotrophic lateral sclerosis. See https://aka.ms/valle2 for demos of VALL-E 2."", ""authors"": [""Sanyuan Chen"", ""Shujie Liu"", ""Long Zhou"", ""Yanqing Liu"", ""Xu Tan"", ""Jinyu Li"", ""Sheng Zhao"", ""Yao Qian"", ""Furu Wei""], ""published"": ""2024-06-08T06:31:03Z"", ""updated"": ""2024-06-17T04:39:08Z"", ""categories"": [""cs.CL"", ""cs.SD"", ""eess.AS""], ""pdf_url"": ""https://arxiv.org/pdf/2406.05370v2"", ""landing_url"": ""https://arxiv.org/abs/2406.05370v2"", ""doi"": ""https://doi.org/10.48550/arXiv.2406.05370""}",2406.0537,https://openalex.org/W4399554695
10.48550/arxiv.2406.06582,Discrete Multimodal Transformers with a Pretrained Large Language Model for Mixed-Supervision Speech Processing,"Recent work on discrete speech tokenization has paved the way for models that can seamlessly perform multiple tasks across modalities, e.g., speech recognition, text to speech, speech to speech translation. Moreover, large language models (LLMs) pretrained from vast text corpora contain rich linguistic information that can improve accuracy in a variety of tasks. In this paper, we present a decoder-only Discrete Multimodal Language Model (DMLM), which can be flexibly applied to multiple tasks (ASR, T2S, S2TT, etc.) and modalities (text, speech, vision). We explore several critical aspects of discrete multi-modal models, including the loss function, weight initialization, mixed training supervision, and codebook. Our results show that DMLM benefits significantly, across multiple tasks and datasets, from a combination of supervised and unsupervised training. Moreover, for ASR, it benefits from initializing DMLM from a pretrained LLM, and from a codebook derived from Whisper activations.",1,,include (junior:5),https://arxiv.org/abs/2406.06582v2,https://arxiv.org/pdf/2406.06582v2,2024,discrete speech tokens,offline clustering,"{""arxiv_id"": ""2406.06582"", ""title"": ""Discrete Multimodal Transformers with a Pretrained Large Language Model for Mixed-Supervision Speech Processing"", ""summary"": ""Recent work on discrete speech tokenization has paved the way for models that can seamlessly perform multiple tasks across modalities, e.g., speech recognition, text to speech, speech to speech translation. Moreover, large language models (LLMs) pretrained from vast text corpora contain rich linguistic information that can improve accuracy in a variety of tasks. In this paper, we present a decoder-only Discrete Multimodal Language Model (DMLM), which can be flexibly applied to multiple tasks (ASR, T2S, S2TT, etc.) and modalities (text, speech, vision). We explore several critical aspects of discrete multi-modal models, including the loss function, weight initialization, mixed training supervision, and codebook. Our results show that DMLM benefits significantly, across multiple tasks and datasets, from a combination of supervised and unsupervised training. Moreover, for ASR, it benefits from initializing DMLM from a pretrained LLM, and from a codebook derived from Whisper activations."", ""authors"": [""Viet Anh Trinh"", ""Rosy Southwell"", ""Yiwen Guan"", ""Xinlu He"", ""Zhiyong Wang"", ""Jacob Whitehill""], ""published"": ""2024-06-04T20:08:25Z"", ""updated"": ""2024-06-25T17:44:00Z"", ""categories"": [""cs.CL"", ""cs.LG"", ""eess.AS""], ""pdf_url"": ""https://arxiv.org/pdf/2406.06582v2"", ""landing_url"": ""https://arxiv.org/abs/2406.06582v2"", ""doi"": ""https://doi.org/10.48550/arXiv.2406.06582""}",2406.06582,https://openalex.org/W4399597385
10.48550/arxiv.2406.07855,VALL-E R: Robust and Efficient Zero-Shot Text-to-Speech Synthesis via Monotonic Alignment,"With the help of discrete neural audio codecs, large language models (LLM) have increasingly been recognized as a promising methodology for zero-shot Text-to-Speech (TTS) synthesis. However, sampling based decoding strategies bring astonishing diversity to generation, but also pose robustness issues such as typos, omissions and repetition. In addition, the high sampling rate of audio also brings huge computational overhead to the inference process of autoregression. To address these issues, we propose VALL-E R, a robust and efficient zero-shot TTS system, building upon the foundation of VALL-E. Specifically, we introduce a phoneme monotonic alignment strategy to strengthen the connection between phonemes and acoustic sequence, ensuring a more precise alignment by constraining the acoustic tokens to match their associated phonemes. Furthermore, we employ a codec-merging approach to downsample the discrete codes in shallow quantization layer, thereby accelerating the decoding speed while preserving the high quality of speech output. Benefiting from these strategies, VALL-E R obtains controllablity over phonemes and demonstrates its strong robustness by approaching the WER of ground truth. In addition, it requires fewer autoregressive steps, with over 60% time reduction during inference. This research has the potential to be applied to meaningful projects, including the creation of speech for those affected by aphasia. Audio samples will be available at: https://aka.ms/valler.",1,,include (junior:5),https://arxiv.org/abs/2406.07855v1,https://arxiv.org/pdf/2406.07855v1,2024,discrete speech tokens,acoustic tokens,"{""arxiv_id"": ""2406.07855"", ""title"": ""VALL-E R: Robust and Efficient Zero-Shot Text-to-Speech Synthesis via Monotonic Alignment"", ""summary"": ""With the help of discrete neural audio codecs, large language models (LLM) have increasingly been recognized as a promising methodology for zero-shot Text-to-Speech (TTS) synthesis. However, sampling based decoding strategies bring astonishing diversity to generation, but also pose robustness issues such as typos, omissions and repetition. In addition, the high sampling rate of audio also brings huge computational overhead to the inference process of autoregression. To address these issues, we propose VALL-E R, a robust and efficient zero-shot TTS system, building upon the foundation of VALL-E. Specifically, we introduce a phoneme monotonic alignment strategy to strengthen the connection between phonemes and acoustic sequence, ensuring a more precise alignment by constraining the acoustic tokens to match their associated phonemes. Furthermore, we employ a codec-merging approach to downsample the discrete codes in shallow quantization layer, thereby accelerating the decoding speed while preserving the high quality of speech output. Benefiting from these strategies, VALL-E R obtains controllablity over phonemes and demonstrates its strong robustness by approaching the WER of ground truth. In addition, it requires fewer autoregressive steps, with over 60% time reduction during inference. This research has the potential to be applied to meaningful projects, including the creation of speech for those affected by aphasia. Audio samples will be available at: https://aka.ms/valler."", ""authors"": [""Bing Han"", ""Long Zhou"", ""Shujie Liu"", ""Sanyuan Chen"", ""Lingwei Meng"", ""Yanming Qian"", ""Yanqing Liu"", ""Sheng Zhao"", ""Jinyu Li"", ""Furu Wei""], ""published"": ""2024-06-12T04:09:44Z"", ""updated"": ""2024-06-12T04:09:44Z"", ""categories"": [""cs.CL"", ""cs.SD"", ""eess.AS""], ""pdf_url"": ""https://arxiv.org/pdf/2406.07855v1"", ""landing_url"": ""https://arxiv.org/abs/2406.07855v1"", ""doi"": ""https://doi.org/10.48550/arXiv.2406.07855""}",2406.07855,https://openalex.org/W4399657366
10.48550/arxiv.2406.08336,CoLM-DSR: Leveraging Neural Codec Language Modeling for Multi-Modal Dysarthric Speech Reconstruction,"Dysarthric speech reconstruction (DSR) aims to transform dysarthric speech into normal speech. It still suffers from low speaker similarity and poor prosody naturalness. In this paper, we propose a multi-modal DSR model by leveraging neural codec language modeling to improve the reconstruction results, especially for the speaker similarity and prosody naturalness. Our proposed model consists of: (i) a multi-modal content encoder to extract robust phoneme embeddings from dysarthric speech with auxiliary visual inputs; (ii) a speaker codec encoder to extract and normalize the speaker-aware codecs from the dysarthric speech, in order to provide original timbre and normal prosody; (iii) a codec language model based speech decoder to reconstruct the speech based on the extracted phoneme embeddings and normalized codecs. Evaluations on the commonly used UASpeech corpus show that our proposed model can achieve significant improvements in terms of speaker similarity and prosody naturalness.",1,,include (senior:4),https://arxiv.org/abs/2406.08336v2,https://arxiv.org/pdf/2406.08336v2,2024,discrete speech tokens,neural codec,"{""arxiv_id"": ""2406.08336"", ""title"": ""CoLM-DSR: Leveraging Neural Codec Language Modeling for Multi-Modal Dysarthric Speech Reconstruction"", ""summary"": ""Dysarthric speech reconstruction (DSR) aims to transform dysarthric speech into normal speech. It still suffers from low speaker similarity and poor prosody naturalness. In this paper, we propose a multi-modal DSR model by leveraging neural codec language modeling to improve the reconstruction results, especially for the speaker similarity and prosody naturalness. Our proposed model consists of: (i) a multi-modal content encoder to extract robust phoneme embeddings from dysarthric speech with auxiliary visual inputs; (ii) a speaker codec encoder to extract and normalize the speaker-aware codecs from the dysarthric speech, in order to provide original timbre and normal prosody; (iii) a codec language model based speech decoder to reconstruct the speech based on the extracted phoneme embeddings and normalized codecs. Evaluations on the commonly used UASpeech corpus show that our proposed model can achieve significant improvements in terms of speaker similarity and prosody naturalness."", ""authors"": [""Xueyuan Chen"", ""Dongchao Yang"", ""Dingdong Wang"", ""Xixin Wu"", ""Zhiyong Wu"", ""Helen Meng""], ""published"": ""2024-06-12T15:42:21Z"", ""updated"": ""2024-06-24T06:09:42Z"", ""categories"": [""cs.SD"", ""cs.CV"", ""eess.AS""], ""pdf_url"": ""https://arxiv.org/pdf/2406.08336v2"", ""landing_url"": ""https://arxiv.org/abs/2406.08336v2"", ""doi"": ""https://doi.org/10.48550/arXiv.2406.08336""}",2406.08336,https://openalex.org/W4399657731
10.48550/arxiv.2406.10735,How Should We Extract Discrete Audio Tokens from Self-Supervised Models?,"Discrete audio tokens have recently gained attention for their potential to bridge the gap between audio and language processing. Ideal audio tokens must preserve content, paralinguistic elements, speaker identity, and many other audio details. Current audio tokenization methods fall into two categories: Semantic tokens, acquired through quantization of Self-Supervised Learning (SSL) models, and Neural compression-based tokens (codecs). Although previous studies have benchmarked codec models to identify optimal configurations, the ideal setup for quantizing pretrained SSL models remains unclear. This paper explores the optimal configuration of semantic tokens across discriminative and generative tasks. We propose a scalable solution to train a universal vocoder across multiple SSL layers. Furthermore, an attention mechanism is employed to identify task-specific influential layers, enhancing the adaptability and performance of semantic tokens in diverse audio applications.",1,,include (junior:5),https://arxiv.org/abs/2406.10735v1,https://arxiv.org/pdf/2406.10735v1,2024,discrete speech tokens,semantic tokens,"{""arxiv_id"": ""2406.10735"", ""title"": ""How Should We Extract Discrete Audio Tokens from Self-Supervised Models?"", ""summary"": ""Discrete audio tokens have recently gained attention for their potential to bridge the gap between audio and language processing. Ideal audio tokens must preserve content, paralinguistic elements, speaker identity, and many other audio details. Current audio tokenization methods fall into two categories: Semantic tokens, acquired through quantization of Self-Supervised Learning (SSL) models, and Neural compression-based tokens (codecs). Although previous studies have benchmarked codec models to identify optimal configurations, the ideal setup for quantizing pretrained SSL models remains unclear. This paper explores the optimal configuration of semantic tokens across discriminative and generative tasks. We propose a scalable solution to train a universal vocoder across multiple SSL layers. Furthermore, an attention mechanism is employed to identify task-specific influential layers, enhancing the adaptability and performance of semantic tokens in diverse audio applications."", ""authors"": [""Pooneh Mousavi"", ""Jarod Duret"", ""Salah Zaiem"", ""Luca Della Libera"", ""Artem Ploujnikov"", ""Cem Subakan"", ""Mirco Ravanelli""], ""published"": ""2024-06-15T20:43:07Z"", ""updated"": ""2024-06-15T20:43:07Z"", ""categories"": [""cs.SD"", ""cs.AI"", ""cs.CL"", ""eess.AS""], ""pdf_url"": ""https://arxiv.org/pdf/2406.10735v1"", ""landing_url"": ""https://arxiv.org/abs/2406.10735v1"", ""doi"": ""https://doi.org/10.48550/arXiv.2406.10735""}",2406.10735,https://openalex.org/W4399794549
10.48550/arxiv.2406.11037,NAST: Noise Aware Speech Tokenization for Speech Language Models,"Speech tokenization is the task of representing speech signals as a sequence of discrete units. Such representations can be later used for various downstream tasks including automatic speech recognition, text-to-speech, etc. More relevant to this study, such representation serves as the basis of Speech Language Models. In this work, we tackle the task of speech tokenization under the noisy setup and present NAST: Noise Aware Speech Tokenization for Speech Language Models. NAST is composed of three main components: (i) a predictor; (ii) a residual encoder; and (iii) a decoder. We evaluate the efficiency of NAST considering several spoken language modeling tasks and show that NAST is superior to the evaluated baselines across all setups. Lastly, we analyze NAST and show its disentanglement properties and robustness to signal variations in the form of noise, reverberation, pitch-shift, and time-stretch. Code and pre-trained models are available at https://github.com/ShovalMessica/NAST.",1,,include (junior:5),https://arxiv.org/abs/2406.11037v1,https://arxiv.org/pdf/2406.11037v1,2024,discrete speech tokens,speech tokenization,"{""arxiv_id"": ""2406.11037"", ""title"": ""NAST: Noise Aware Speech Tokenization for Speech Language Models"", ""summary"": ""Speech tokenization is the task of representing speech signals as a sequence of discrete units. Such representations can be later used for various downstream tasks including automatic speech recognition, text-to-speech, etc. More relevant to this study, such representation serves as the basis of Speech Language Models. In this work, we tackle the task of speech tokenization under the noisy setup and present NAST: Noise Aware Speech Tokenization for Speech Language Models. NAST is composed of three main components: (i) a predictor; (ii) a residual encoder; and (iii) a decoder. We evaluate the efficiency of NAST considering several spoken language modeling tasks and show that NAST is superior to the evaluated baselines across all setups. Lastly, we analyze NAST and show its disentanglement properties and robustness to signal variations in the form of noise, reverberation, pitch-shift, and time-stretch. Code and pre-trained models are available at https://github.com/ShovalMessica/NAST."", ""authors"": [""Shoval Messica"", ""Yossi Adi""], ""published"": ""2024-06-16T18:20:45Z"", ""updated"": ""2024-06-16T18:20:45Z"", ""categories"": [""cs.SD"", ""eess.AS""], ""pdf_url"": ""https://arxiv.org/pdf/2406.11037v1"", ""landing_url"": ""https://arxiv.org/abs/2406.11037v1"", ""doi"": ""https://doi.org/10.48550/arXiv.2406.11037""}",2406.11037,https://openalex.org/W4399794786
10.48550/arxiv.2406.13431,Children's Speech Recognition through Discrete Token Enhancement,"Children's speech recognition is considered a low-resource task mainly due to the lack of publicly available data. There are several reasons for such data scarcity, including expensive data collection and annotation processes, and data privacy, among others. Transforming speech signals into discrete tokens that do not carry sensitive information but capture both linguistic and acoustic information could be a solution for privacy concerns. In this study, we investigate the integration of discrete speech tokens into children's speech recognition systems as input without significantly degrading the ASR performance. Additionally, we explored single-view and multi-view strategies for creating these discrete labels. Furthermore, we tested the models for generalization capabilities with unseen domain and nativity dataset. Results reveal that the discrete token ASR for children achieves nearly equivalent performance with an approximate 83% reduction in parameters.",1,,include (junior:5),https://arxiv.org/abs/2406.13431v2,https://arxiv.org/pdf/2406.13431v2,2024,discrete speech tokens,offline clustering,"{""arxiv_id"": ""2406.13431"", ""title"": ""Children's Speech Recognition through Discrete Token Enhancement"", ""summary"": ""Children's speech recognition is considered a low-resource task mainly due to the lack of publicly available data. There are several reasons for such data scarcity, including expensive data collection and annotation processes, and data privacy, among others. Transforming speech signals into discrete tokens that do not carry sensitive information but capture both linguistic and acoustic information could be a solution for privacy concerns. In this study, we investigate the integration of discrete speech tokens into children's speech recognition systems as input without significantly degrading the ASR performance. Additionally, we explored single-view and multi-view strategies for creating these discrete labels. Furthermore, we tested the models for generalization capabilities with unseen domain and nativity dataset. Results reveal that the discrete token ASR for children achieves nearly equivalent performance with an approximate 83% reduction in parameters."", ""authors"": [""Vrunda N. Sukhadia"", ""Shammur Absar Chowdhury""], ""published"": ""2024-06-19T10:45:12Z"", ""updated"": ""2024-06-24T15:31:59Z"", ""categories"": [""cs.CL"", ""cs.SD"", ""eess.AS""], ""pdf_url"": ""https://arxiv.org/pdf/2406.13431v2"", ""landing_url"": ""https://arxiv.org/abs/2406.13431v2"", ""doi"": ""https://doi.org/10.48550/arXiv.2406.13431""}",2406.13431,https://openalex.org/W4399912799
10.48550/arxiv.2406.14294,DASB - Discrete Audio and Speech Benchmark,"Discrete audio tokens have recently gained considerable attention for their potential to connect audio and language processing, enabling the creation of modern multimodal large language models. Ideal audio tokens must effectively preserve phonetic and semantic content along with paralinguistic information, speaker identity, and other details. While several types of audio tokens have been recently proposed, identifying the optimal tokenizer for various tasks is challenging due to the inconsistent evaluation settings in existing studies. To address this gap, we release the Discrete Audio and Speech Benchmark (DASB), a comprehensive leaderboard for benchmarking discrete audio tokens across a wide range of discriminative tasks, including speech recognition, speaker identification and verification, emotion recognition, keyword spotting, and intent classification, as well as generative tasks such as speech enhancement, separation, and text-to-speech. Our results show that, on average, semantic tokens outperform compression tokens across most discriminative and generative tasks. However, the performance gap between semantic tokens and standard continuous representations remains substantial, highlighting the need for further research in this field.",1,,include (junior:5),https://arxiv.org/abs/2406.14294v2,https://arxiv.org/pdf/2406.14294v2,2024,discrete speech tokens,semantic tokens,"{""arxiv_id"": ""2406.14294"", ""title"": ""DASB - Discrete Audio and Speech Benchmark"", ""summary"": ""Discrete audio tokens have recently gained considerable attention for their potential to connect audio and language processing, enabling the creation of modern multimodal large language models. Ideal audio tokens must effectively preserve phonetic and semantic content along with paralinguistic information, speaker identity, and other details. While several types of audio tokens have been recently proposed, identifying the optimal tokenizer for various tasks is challenging due to the inconsistent evaluation settings in existing studies. To address this gap, we release the Discrete Audio and Speech Benchmark (DASB), a comprehensive leaderboard for benchmarking discrete audio tokens across a wide range of discriminative tasks, including speech recognition, speaker identification and verification, emotion recognition, keyword spotting, and intent classification, as well as generative tasks such as speech enhancement, separation, and text-to-speech. Our results show that, on average, semantic tokens outperform compression tokens across most discriminative and generative tasks. However, the performance gap between semantic tokens and standard continuous representations remains substantial, highlighting the need for further research in this field."", ""authors"": [""Pooneh Mousavi"", ""Luca Della Libera"", ""Jarod Duret"", ""Artem Ploujnikov"", ""Cem Subakan"", ""Mirco Ravanelli""], ""published"": ""2024-06-20T13:23:27Z"", ""updated"": ""2024-06-21T17:07:17Z"", ""categories"": [""cs.SD"", ""cs.AI"", ""eess.AS""], ""pdf_url"": ""https://arxiv.org/pdf/2406.14294v2"", ""landing_url"": ""https://arxiv.org/abs/2406.14294v2"", ""doi"": ""https://doi.org/10.48550/arXiv.2406.14294""}",2406.14294,https://openalex.org/W4399911677
10.48550/arxiv.2406.17310,High Fidelity Text-to-Speech Via Discrete Tokens Using Token Transducer and Group Masked Language Model,"We propose a novel two-stage text-to-speech (TTS) framework with two types of discrete tokens, i.e., semantic and acoustic tokens, for high-fidelity speech synthesis. It features two core components: the Interpreting module, which processes text and a speech prompt into semantic tokens focusing on linguistic contents and alignment, and the Speaking module, which captures the timbre of the target voice to generate acoustic tokens from semantic tokens, enriching speech reconstruction. The Interpreting stage employs a transducer for its robustness in aligning text to speech. In contrast, the Speaking stage utilizes a Conformer-based architecture integrated with a Grouped Masked Language Model (G-MLM) to boost computational efficiency. Our experiments verify that this innovative structure surpasses the conventional models in the zero-shot scenario in terms of speech quality and speaker similarity.",1,,include (junior:5),https://arxiv.org/abs/2406.17310v1,https://arxiv.org/pdf/2406.17310v1,2024,discrete speech tokens,acoustic tokens,"{""arxiv_id"": ""2406.17310"", ""title"": ""High Fidelity Text-to-Speech Via Discrete Tokens Using Token Transducer and Group Masked Language Model"", ""summary"": ""We propose a novel two-stage text-to-speech (TTS) framework with two types of discrete tokens, i.e., semantic and acoustic tokens, for high-fidelity speech synthesis. It features two core components: the Interpreting module, which processes text and a speech prompt into semantic tokens focusing on linguistic contents and alignment, and the Speaking module, which captures the timbre of the target voice to generate acoustic tokens from semantic tokens, enriching speech reconstruction. The Interpreting stage employs a transducer for its robustness in aligning text to speech. In contrast, the Speaking stage utilizes a Conformer-based architecture integrated with a Grouped Masked Language Model (G-MLM) to boost computational efficiency. Our experiments verify that this innovative structure surpasses the conventional models in the zero-shot scenario in terms of speech quality and speaker similarity."", ""authors"": [""Joun Yeop Lee"", ""Myeonghun Jeong"", ""Minchan Kim"", ""Ji-Hyun Lee"", ""Hoon-Young Cho"", ""Nam Soo Kim""], ""published"": ""2024-06-25T06:46:47Z"", ""updated"": ""2024-06-25T06:46:47Z"", ""categories"": [""eess.AS""], ""pdf_url"": ""https://arxiv.org/pdf/2406.17310v1"", ""landing_url"": ""https://arxiv.org/abs/2406.17310v1"", ""doi"": ""https://doi.org/10.48550/arXiv.2406.17310""}",2406.1731,https://openalex.org/W4400064835
10.48550/arxiv.2407.03892,On the Effectiveness of Acoustic BPE in Decoder-Only TTS,"Discretizing speech into tokens and generating them by a decoder-only model have been a promising direction for text-to-speech (TTS) and spoken language modeling (SLM). To shorten the sequence length of speech tokens, acoustic byte-pair encoding (BPE) has emerged in SLM that treats speech tokens from self-supervised semantic representations as characters to further compress the token sequence. But the gain in TTS has not been fully investigated, and the proper choice of acoustic BPE remains unclear. In this work, we conduct a comprehensive study on various settings of acoustic BPE to explore its effectiveness in decoder-only TTS models with semantic speech tokens. Experiments on LibriTTS verify that acoustic BPE uniformly increases the intelligibility and diversity of synthesized speech, while showing different features across BPE settings. Hence, acoustic BPE is a favorable tool for decoder-only TTS.",1,,include (junior:5),https://arxiv.org/abs/2407.03892v1,https://arxiv.org/pdf/2407.03892v1,2024,discrete speech tokens,acoustic bpe,"{""arxiv_id"": ""2407.03892"", ""title"": ""On the Effectiveness of Acoustic BPE in Decoder-Only TTS"", ""summary"": ""Discretizing speech into tokens and generating them by a decoder-only model have been a promising direction for text-to-speech (TTS) and spoken language modeling (SLM). To shorten the sequence length of speech tokens, acoustic byte-pair encoding (BPE) has emerged in SLM that treats speech tokens from self-supervised semantic representations as characters to further compress the token sequence. But the gain in TTS has not been fully investigated, and the proper choice of acoustic BPE remains unclear. In this work, we conduct a comprehensive study on various settings of acoustic BPE to explore its effectiveness in decoder-only TTS models with semantic speech tokens. Experiments on LibriTTS verify that acoustic BPE uniformly increases the intelligibility and diversity of synthesized speech, while showing different features across BPE settings. Hence, acoustic BPE is a favorable tool for decoder-only TTS."", ""authors"": [""Bohan Li"", ""Feiyu Shen"", ""Yiwei Guo"", ""Shuai Wang"", ""Xie Chen"", ""Kai Yu""], ""published"": ""2024-07-04T12:35:32Z"", ""updated"": ""2024-07-04T12:35:32Z"", ""categories"": [""cs.SD"", ""cs.AI"", ""eess.AS""], ""pdf_url"": ""https://arxiv.org/pdf/2407.03892v1"", ""landing_url"": ""https://arxiv.org/abs/2407.03892v1"", ""doi"": ""https://doi.org/10.48550/arXiv.2407.03892""}",2407.03892,https://openalex.org/W4400434132
10.48550/arxiv.2407.05407,CosyVoice: A Scalable Multilingual Zero-shot Text-to-speech Synthesizer based on Supervised Semantic Tokens,"Recent years have witnessed a trend that large language model (LLM) based text-to-speech (TTS) emerges into the mainstream due to their high naturalness and zero-shot capacity. In this paradigm, speech signals are discretized into token sequences, which are modeled by an LLM with text as prompts and reconstructed by a token-based vocoder to waveforms. Obviously, speech tokens play a critical role in LLM-based TTS models. Current speech tokens are learned in an unsupervised manner, which lacks explicit semantic information and alignment to the text. In this paper, we propose to represent speech with supervised semantic tokens, which are derived from a multilingual speech recognition model by inserting vector quantization into the encoder. Based on the tokens, we further propose a scalable zero-shot TTS synthesizer, CosyVoice, which consists of an LLM for text-to-token generation and a conditional flow matching model for token-to-speech synthesis. Experimental results show that supervised semantic tokens significantly outperform existing unsupervised tokens in terms of content consistency and speaker similarity for zero-shot voice cloning. Moreover, we find that utilizing large-scale data further improves the synthesis performance, indicating the scalable capacity of CosyVoice. To the best of our knowledge, this is the first attempt to involve supervised speech tokens into TTS models.",1,,include (junior:5),https://arxiv.org/abs/2407.05407v2,https://arxiv.org/pdf/2407.05407v2,2024,discrete speech tokens,semantic tokens,"{""arxiv_id"": ""2407.05407"", ""title"": ""CosyVoice: A Scalable Multilingual Zero-shot Text-to-speech Synthesizer based on Supervised Semantic Tokens"", ""summary"": ""Recent years have witnessed a trend that large language model (LLM) based text-to-speech (TTS) emerges into the mainstream due to their high naturalness and zero-shot capacity. In this paradigm, speech signals are discretized into token sequences, which are modeled by an LLM with text as prompts and reconstructed by a token-based vocoder to waveforms. Obviously, speech tokens play a critical role in LLM-based TTS models. Current speech tokens are learned in an unsupervised manner, which lacks explicit semantic information and alignment to the text. In this paper, we propose to represent speech with supervised semantic tokens, which are derived from a multilingual speech recognition model by inserting vector quantization into the encoder. Based on the tokens, we further propose a scalable zero-shot TTS synthesizer, CosyVoice, which consists of an LLM for text-to-token generation and a conditional flow matching model for token-to-speech synthesis. Experimental results show that supervised semantic tokens significantly outperform existing unsupervised tokens in terms of content consistency and speaker similarity for zero-shot voice cloning. Moreover, we find that utilizing large-scale data further improves the synthesis performance, indicating the scalable capacity of CosyVoice. To the best of our knowledge, this is the first attempt to involve supervised speech tokens into TTS models."", ""authors"": [""Zhihao Du"", ""Qian Chen"", ""Shiliang Zhang"", ""Kai Hu"", ""Heng Lu"", ""Yexin Yang"", ""Hangrui Hu"", ""Siqi Zheng"", ""Yue Gu"", ""Ziyang Ma"", ""Zhifu Gao"", ""Zhijie Yan""], ""published"": ""2024-07-07T15:16:19Z"", ""updated"": ""2024-07-09T07:42:51Z"", ""categories"": [""cs.SD"", ""cs.AI"", ""eess.AS""], ""pdf_url"": ""https://arxiv.org/pdf/2407.05407v2"", ""landing_url"": ""https://arxiv.org/abs/2407.05407v2"", ""doi"": ""https://doi.org/10.48550/arXiv.2407.05407""}",2407.05407,https://openalex.org/W4400518537
10.48550/arxiv.2407.15835,dMel: Speech Tokenization made Simple,"Large language models have revolutionized natural language processing by leveraging self-supervised pretraining on vast textual data. Inspired by this success, researchers have investigated various compression-based speech tokenization methods to discretize continuous speech signals, enabling the application of language modeling techniques to discrete tokens. However, audio compressor introduces additional complexity and computational cost, and often fail on out-of-domain audio signals. In this work, we introduce a novel speech representation (dmel) that discretizes mel-filterbank channels into intensity bins, creating a simpler yet more effective representation compared to existing speech tokenization methods. Our approach demonstrates superior performance in preserving audio content, robustness to out-of-domain data, and offers a training-free, natural, and streamable representation. To address the high-dimensional nature of log-mel spectrograms, we propose an efficient parallel encoding and decoding method for high-dimensional tokens using an LM-style transformer architecture. This innovation enables us to develop RichTTS and RichASR, two models sharing the same architecture while achieving comparable or better results than specialized existing methods. Our results demonstrate the effectiveness of dmel in achieving high performance on both speech synthesis and recognition tasks within a unified framework, paving the way for efficient and effective joint modeling of speech and text.",1,,include (junior:5),https://arxiv.org/abs/2407.15835v3,https://arxiv.org/pdf/2407.15835v3,2024,discrete speech tokens,speech tokenization,"{""arxiv_id"": ""2407.15835"", ""title"": ""dMel: Speech Tokenization made Simple"", ""summary"": ""Large language models have revolutionized natural language processing by leveraging self-supervised pretraining on vast textual data. Inspired by this success, researchers have investigated various compression-based speech tokenization methods to discretize continuous speech signals, enabling the application of language modeling techniques to discrete tokens. However, audio compressor introduces additional complexity and computational cost, and often fail on out-of-domain audio signals. In this work, we introduce a novel speech representation (dmel) that discretizes mel-filterbank channels into intensity bins, creating a simpler yet more effective representation compared to existing speech tokenization methods. Our approach demonstrates superior performance in preserving audio content, robustness to out-of-domain data, and offers a training-free, natural, and streamable representation. To address the high-dimensional nature of log-mel spectrograms, we propose an efficient parallel encoding and decoding method for high-dimensional tokens using an LM-style transformer architecture. This innovation enables us to develop RichTTS and RichASR, two models sharing the same architecture while achieving comparable or better results than specialized existing methods. Our results demonstrate the effectiveness of dmel in achieving high performance on both speech synthesis and recognition tasks within a unified framework, paving the way for efficient and effective joint modeling of speech and text."", ""authors"": [""Richard He Bai"", ""Tatiana Likhomanenko"", ""Ruixiang Zhang"", ""Zijin Gu"", ""Zakaria Aldeneh"", ""Navdeep Jaitly""], ""published"": ""2024-07-22T17:51:53Z"", ""updated"": ""2025-05-21T16:55:34Z"", ""categories"": [""cs.CL"", ""cs.AI"", ""cs.SD"", ""eess.AS""], ""pdf_url"": ""https://arxiv.org/pdf/2407.15835v3"", ""landing_url"": ""https://arxiv.org/abs/2407.15835v3"", ""doi"": ""https://doi.org/10.48550/arXiv.2407.15835""}",2407.15835,https://openalex.org/W4402856752
10.48550/arxiv.2408.16373,Enabling Beam Search for Language Model-Based Text-to-Speech Synthesis,"Tokenising continuous speech into sequences of discrete tokens and modelling them with language models (LMs) has led to significant success in text-to-speech (TTS) synthesis. Although these models can generate speech with high quality and naturalness, their synthesised samples can still suffer from artefacts, mispronunciation, word repeating, etc. In this paper, we argue these undesirable properties could partly be caused by the randomness of sampling-based strategies during the autoregressive decoding of LMs. Therefore, we look at maximisation-based decoding approaches and propose Temporal Repetition Aware Diverse Beam Search (TRAD-BS) to find the most probable sequences of the generated speech tokens. Experiments with two state-of-the-art LM-based TTS models demonstrate that our proposed maximisation-based decoding strategy generates speech with fewer mispronunciations and improved speaker consistency.",1,,include (junior:5),https://arxiv.org/abs/2408.16373v1,https://arxiv.org/pdf/2408.16373v1,2024,speech tokenization,acoustic bpe,"{""arxiv_id"": ""2408.16373"", ""title"": ""Enabling Beam Search for Language Model-Based Text-to-Speech Synthesis"", ""summary"": ""Tokenising continuous speech into sequences of discrete tokens and modelling them with language models (LMs) has led to significant success in text-to-speech (TTS) synthesis. Although these models can generate speech with high quality and naturalness, their synthesised samples can still suffer from artefacts, mispronunciation, word repeating, etc. In this paper, we argue these undesirable properties could partly be caused by the randomness of sampling-based strategies during the autoregressive decoding of LMs. Therefore, we look at maximisation-based decoding approaches and propose Temporal Repetition Aware Diverse Beam Search (TRAD-BS) to find the most probable sequences of the generated speech tokens. Experiments with two state-of-the-art LM-based TTS models demonstrate that our proposed maximisation-based decoding strategy generates speech with fewer mispronunciations and improved speaker consistency."", ""authors"": [""Zehai Tu"", ""Guangyan Zhang"", ""Yiting Lu"", ""Adaeze Adigwe"", ""Simon King"", ""Yiwen Guo""], ""published"": ""2024-08-29T09:31:06Z"", ""updated"": ""2024-08-29T09:31:06Z"", ""categories"": [""cs.SD"", ""eess.AS""], ""pdf_url"": ""https://arxiv.org/pdf/2408.16373v1"", ""landing_url"": ""https://arxiv.org/abs/2408.16373v1"", ""doi"": ""https://doi.org/10.48550/arXiv.2408.16373""}",2408.16373,https://openalex.org/W4402706590
10.48550/arxiv.2408.17175,Codec Does Matter: Exploring the Semantic Shortcoming of Codec for Audio Language Model,"Recent advancements in audio generation have been significantly propelled by the capabilities of Large Language Models (LLMs). The existing research on audio LLM has primarily focused on enhancing the architecture and scale of audio language models, as well as leveraging larger datasets, and generally, acoustic codecs, such as EnCodec, are used for audio tokenization. However, these codecs were originally designed for audio compression, which may lead to suboptimal performance in the context of audio LLM. Our research aims to address the shortcomings of current audio LLM codecs, particularly their challenges in maintaining semantic integrity in generated audio. For instance, existing methods like VALL-E, which condition acoustic token generation on text transcriptions, often suffer from content inaccuracies and elevated word error rates (WER) due to semantic misinterpretations of acoustic tokens, resulting in word skipping and errors. To overcome these issues, we propose a straightforward yet effective approach called X-Codec. X-Codec incorporates semantic features from a pre-trained semantic encoder before the Residual Vector Quantization (RVQ) stage and introduces a semantic reconstruction loss after RVQ. By enhancing the semantic ability of the codec, X-Codec significantly reduces WER in speech synthesis tasks and extends these benefits to non-speech applications, including music and sound generation. Our experiments in text-to-speech, music continuation, and text-to-sound tasks demonstrate that integrating semantic information substantially improves the overall performance of language models in audio generation. Our code and demo are available (Demo: https://x-codec-audio.github.io Code: https://github.com/zhenye234/xcodec)",1,,include (junior:5),https://arxiv.org/abs/2408.17175v3,https://arxiv.org/pdf/2408.17175v3,2024,discrete speech tokens,acoustic tokens,"{""arxiv_id"": ""2408.17175"", ""title"": ""Codec Does Matter: Exploring the Semantic Shortcoming of Codec for Audio Language Model"", ""summary"": ""Recent advancements in audio generation have been significantly propelled by the capabilities of Large Language Models (LLMs). The existing research on audio LLM has primarily focused on enhancing the architecture and scale of audio language models, as well as leveraging larger datasets, and generally, acoustic codecs, such as EnCodec, are used for audio tokenization. However, these codecs were originally designed for audio compression, which may lead to suboptimal performance in the context of audio LLM. Our research aims to address the shortcomings of current audio LLM codecs, particularly their challenges in maintaining semantic integrity in generated audio. For instance, existing methods like VALL-E, which condition acoustic token generation on text transcriptions, often suffer from content inaccuracies and elevated word error rates (WER) due to semantic misinterpretations of acoustic tokens, resulting in word skipping and errors. To overcome these issues, we propose a straightforward yet effective approach called X-Codec. X-Codec incorporates semantic features from a pre-trained semantic encoder before the Residual Vector Quantization (RVQ) stage and introduces a semantic reconstruction loss after RVQ. By enhancing the semantic ability of the codec, X-Codec significantly reduces WER in speech synthesis tasks and extends these benefits to non-speech applications, including music and sound generation. Our experiments in text-to-speech, music continuation, and text-to-sound tasks demonstrate that integrating semantic information substantially improves the overall performance of language models in audio generation. Our code and demo are available (Demo: https://x-codec-audio.github.io Code: https://github.com/zhenye234/xcodec)"", ""authors"": [""Zhen Ye"", ""Peiwen Sun"", ""Jiahe Lei"", ""Hongzhan Lin"", ""Xu Tan"", ""Zheqi Dai"", ""Qiuqiang Kong"", ""Jianyi Chen"", ""Jiahao Pan"", ""Qifeng Liu"", ""Yike Guo"", ""Wei Xue""], ""published"": ""2024-08-30T10:24:07Z"", ""updated"": ""2024-11-27T11:47:45Z"", ""categories"": [""eess.AS"", ""cs.AI"", ""cs.CL"", ""cs.SD""], ""pdf_url"": ""https://arxiv.org/pdf/2408.17175v3"", ""landing_url"": ""https://arxiv.org/abs/2408.17175v3"", ""doi"": ""https://doi.org/10.48550/arXiv.2408.17175""}",2408.17175,https://openalex.org/W4403746867
10.48550/arxiv.2409.00750,MaskGCT: Zero-Shot Text-to-Speech with Masked Generative Codec Transformer,"The recent large-scale text-to-speech (TTS) systems are usually grouped as autoregressive and non-autoregressive systems. The autoregressive systems implicitly model duration but exhibit certain deficiencies in robustness and lack of duration controllability. Non-autoregressive systems require explicit alignment information between text and speech during training and predict durations for linguistic units (e.g. phone), which may compromise their naturalness. In this paper, we introduce Masked Generative Codec Transformer (MaskGCT), a fully non-autoregressive TTS model that eliminates the need for explicit alignment information between text and speech supervision, as well as phone-level duration prediction. MaskGCT is a two-stage model: in the first stage, the model uses text to predict semantic tokens extracted from a speech self-supervised learning (SSL) model, and in the second stage, the model predicts acoustic tokens conditioned on these semantic tokens. MaskGCT follows the mask-and-predict learning paradigm. During training, MaskGCT learns to predict masked semantic or acoustic tokens based on given conditions and prompts. During inference, the model generates tokens of a specified length in a parallel manner. Experiments with 100K hours of in-the-wild speech demonstrate that MaskGCT outperforms the current state-of-the-art zero-shot TTS systems in terms of quality, similarity, and intelligibility. Audio samples are available at https://maskgct.github.io/. We release our code and model checkpoints at https://github.com/open-mmlab/Amphion/blob/main/models/tts/maskgct.",1,,include (junior:5),https://arxiv.org/abs/2409.00750v3,https://arxiv.org/pdf/2409.00750v3,2024,discrete speech tokens,acoustic tokens,"{""arxiv_id"": ""2409.00750"", ""title"": ""MaskGCT: Zero-Shot Text-to-Speech with Masked Generative Codec Transformer"", ""summary"": ""The recent large-scale text-to-speech (TTS) systems are usually grouped as autoregressive and non-autoregressive systems. The autoregressive systems implicitly model duration but exhibit certain deficiencies in robustness and lack of duration controllability. Non-autoregressive systems require explicit alignment information between text and speech during training and predict durations for linguistic units (e.g. phone), which may compromise their naturalness. In this paper, we introduce Masked Generative Codec Transformer (MaskGCT), a fully non-autoregressive TTS model that eliminates the need for explicit alignment information between text and speech supervision, as well as phone-level duration prediction. MaskGCT is a two-stage model: in the first stage, the model uses text to predict semantic tokens extracted from a speech self-supervised learning (SSL) model, and in the second stage, the model predicts acoustic tokens conditioned on these semantic tokens. MaskGCT follows the mask-and-predict learning paradigm. During training, MaskGCT learns to predict masked semantic or acoustic tokens based on given conditions and prompts. During inference, the model generates tokens of a specified length in a parallel manner. Experiments with 100K hours of in-the-wild speech demonstrate that MaskGCT outperforms the current state-of-the-art zero-shot TTS systems in terms of quality, similarity, and intelligibility. Audio samples are available at https://maskgct.github.io/. We release our code and model checkpoints at https://github.com/open-mmlab/Amphion/blob/main/models/tts/maskgct."", ""authors"": [""Yuancheng Wang"", ""Haoyue Zhan"", ""Liwei Liu"", ""Ruihong Zeng"", ""Haotian Guo"", ""Jiachen Zheng"", ""Qiang Zhang"", ""Xueyao Zhang"", ""Shunsi Zhang"", ""Zhizheng Wu""], ""published"": ""2024-09-01T15:26:30Z"", ""updated"": ""2024-10-20T14:25:49Z"", ""categories"": [""cs.SD"", ""cs.AI"", ""cs.LG"", ""eess.AS""], ""pdf_url"": ""https://arxiv.org/pdf/2409.00750v3"", ""landing_url"": ""https://arxiv.org/abs/2409.00750v3"", ""doi"": ""https://doi.org/10.48550/arXiv.2409.00750""}",2409.0075,https://openalex.org/W4402954014
10.48550/arxiv.2409.01995,vec2wav 2.0: Advancing Voice Conversion via Discrete Token Vocoders,"We propose a new speech discrete token vocoder, vec2wav 2.0, which advances voice conversion (VC). We use discrete tokens from speech self-supervised models as the content features of source speech, and treat VC as a prompted vocoding task. To amend the loss of speaker timbre in the content tokens, vec2wav 2.0 utilizes the WavLM features to provide strong timbre-dependent information. A novel adaptive Snake activation function is proposed to better incorporate timbre into the waveform reconstruction process. In this way, vec2wav 2.0 learns to alter the speaker timbre appropriately given different reference prompts. Also, no supervised data is required for vec2wav 2.0 to be effectively trained. Experimental results demonstrate that vec2wav 2.0 outperforms all other baselines to a considerable margin in terms of audio quality and speaker similarity in any-to-any VC. Ablation studies verify the effects made by the proposed techniques. Moreover, vec2wav 2.0 achieves competitive cross-lingual VC even only trained on monolingual corpus. Thus, vec2wav 2.0 shows timbre can potentially be manipulated only by speech token vocoders, pushing the frontiers of VC and speech synthesis.",1,,include (junior:5),https://arxiv.org/abs/2409.01995v4,https://arxiv.org/pdf/2409.01995v4,2024,discrete speech tokens,speech token vocoder,"{""arxiv_id"": ""2409.01995"", ""title"": ""vec2wav 2.0: Advancing Voice Conversion via Discrete Token Vocoders"", ""summary"": ""We propose a new speech discrete token vocoder, vec2wav 2.0, which advances voice conversion (VC). We use discrete tokens from speech self-supervised models as the content features of source speech, and treat VC as a prompted vocoding task. To amend the loss of speaker timbre in the content tokens, vec2wav 2.0 utilizes the WavLM features to provide strong timbre-dependent information. A novel adaptive Snake activation function is proposed to better incorporate timbre into the waveform reconstruction process. In this way, vec2wav 2.0 learns to alter the speaker timbre appropriately given different reference prompts. Also, no supervised data is required for vec2wav 2.0 to be effectively trained. Experimental results demonstrate that vec2wav 2.0 outperforms all other baselines to a considerable margin in terms of audio quality and speaker similarity in any-to-any VC. Ablation studies verify the effects made by the proposed techniques. Moreover, vec2wav 2.0 achieves competitive cross-lingual VC even only trained on monolingual corpus. Thus, vec2wav 2.0 shows timbre can potentially be manipulated only by speech token vocoders, pushing the frontiers of VC and speech synthesis."", ""authors"": [""Yiwei Guo"", ""Zhihan Li"", ""Junjie Li"", ""Chenpeng Du"", ""Hankun Wang"", ""Shuai Wang"", ""Xie Chen"", ""Kai Yu""], ""published"": ""2024-09-03T15:41:07Z"", ""updated"": ""2025-05-24T13:50:34Z"", ""categories"": [""eess.AS"", ""cs.AI"", ""cs.SD""], ""pdf_url"": ""https://arxiv.org/pdf/2409.01995v4"", ""landing_url"": ""https://arxiv.org/abs/2409.01995v4"", ""doi"": ""https://doi.org/10.48550/arXiv.2409.01995""}",2409.01995,https://openalex.org/W4403619928
10.48550/arxiv.2409.02384,STAB: Speech Tokenizer Assessment Benchmark,"Representing speech as discrete tokens provides a framework for transforming speech into a format that closely resembles text, thus enabling the use of speech as an input to the widely successful large language models (LLMs). Currently, while several speech tokenizers have been proposed, there is ambiguity regarding the properties that are desired from a tokenizer for specific downstream tasks and its overall generalizability. Evaluating the performance of tokenizers across different downstream tasks is a computationally intensive effort that poses challenges for scalability. To circumvent this requirement, we present STAB (Speech Tokenizer Assessment Benchmark), a systematic evaluation framework designed to assess speech tokenizers comprehensively and shed light on their inherent characteristics. This framework provides a deeper understanding of the underlying mechanisms of speech tokenization, thereby offering a valuable resource for expediting the advancement of future tokenizer models and enabling comparative analysis using a standardized benchmark. We evaluate the STAB metrics and correlate this with downstream task performance across a range of speech tasks and tokenizer choices.",1,,include (junior:5),https://arxiv.org/abs/2409.02384v1,https://arxiv.org/pdf/2409.02384v1,2024,discrete speech tokens,speech tokenization,"{""arxiv_id"": ""2409.02384"", ""title"": ""STAB: Speech Tokenizer Assessment Benchmark"", ""summary"": ""Representing speech as discrete tokens provides a framework for transforming speech into a format that closely resembles text, thus enabling the use of speech as an input to the widely successful large language models (LLMs). Currently, while several speech tokenizers have been proposed, there is ambiguity regarding the properties that are desired from a tokenizer for specific downstream tasks and its overall generalizability. Evaluating the performance of tokenizers across different downstream tasks is a computationally intensive effort that poses challenges for scalability. To circumvent this requirement, we present STAB (Speech Tokenizer Assessment Benchmark), a systematic evaluation framework designed to assess speech tokenizers comprehensively and shed light on their inherent characteristics. This framework provides a deeper understanding of the underlying mechanisms of speech tokenization, thereby offering a valuable resource for expediting the advancement of future tokenizer models and enabling comparative analysis using a standardized benchmark. We evaluate the STAB metrics and correlate this with downstream task performance across a range of speech tasks and tokenizer choices."", ""authors"": [""Shikhar Vashishth"", ""Harman Singh"", ""Shikhar Bharadwaj"", ""Sriram Ganapathy"", ""Chulayuth Asawaroengchai"", ""Kartik Audhkhasi"", ""Andrew Rosenberg"", ""Ankur Bapna"", ""Bhuvana Ramabhadran""], ""published"": ""2024-09-04T02:20:59Z"", ""updated"": ""2024-09-04T02:20:59Z"", ""categories"": [""cs.CL"", ""cs.SD"", ""eess.AS""], ""pdf_url"": ""https://arxiv.org/pdf/2409.02384v1"", ""landing_url"": ""https://arxiv.org/abs/2409.02384v1"", ""doi"": ""https://doi.org/10.48550/arXiv.2409.02384""}",2409.02384,https://openalex.org/W4403160068
10.48550/arxiv.2409.03283,FireRedTTS: A Foundation Text-To-Speech Framework for Industry-Level Generative Speech Applications,"This work proposes FireRedTTS, a foundation text-to-speech framework, to meet the growing demands for personalized and diverse generative speech applications. The framework comprises three parts: data processing, foundation system, and downstream applications. First, we comprehensively present our data processing pipeline, which transforms massive raw audio into a large-scale high-quality TTS dataset with rich annotations and a wide coverage of content, speaking style, and timbre. Then, we propose a language-model-based foundation TTS system. The speech signal is compressed into discrete semantic tokens via a semantic-aware speech tokenizer, and can be generated by a language model from the prompt text and audio. Then, a two-stage waveform generator is proposed to decode them to the high-fidelity waveform. We present two applications of this system: voice cloning for dubbing and human-like speech generation for chatbots. The experimental results demonstrate the solid in-context learning capability of FireRedTTS, which can stably synthesize high-quality speech consistent with the prompt text and audio. For dubbing, FireRedTTS can clone target voices in a zero-shot way for the UGC scenario and adapt to studio-level expressive voice characters in the PUGC scenario via few-shot fine-tuning with 1-hour recording. Moreover, FireRedTTS achieves controllable human-like speech generation in a casual style with paralinguistic behaviors and emotions via instruction tuning, to better serve spoken chatbots.",1,,include (junior:5),https://arxiv.org/abs/2409.03283v2,https://arxiv.org/pdf/2409.03283v2,2024,speech tokenization,semantic tokens,"{""arxiv_id"": ""2409.03283"", ""title"": ""FireRedTTS: A Foundation Text-To-Speech Framework for Industry-Level Generative Speech Applications"", ""summary"": ""This work proposes FireRedTTS, a foundation text-to-speech framework, to meet the growing demands for personalized and diverse generative speech applications. The framework comprises three parts: data processing, foundation system, and downstream applications. First, we comprehensively present our data processing pipeline, which transforms massive raw audio into a large-scale high-quality TTS dataset with rich annotations and a wide coverage of content, speaking style, and timbre. Then, we propose a language-model-based foundation TTS system. The speech signal is compressed into discrete semantic tokens via a semantic-aware speech tokenizer, and can be generated by a language model from the prompt text and audio. Then, a two-stage waveform generator is proposed to decode them to the high-fidelity waveform. We present two applications of this system: voice cloning for dubbing and human-like speech generation for chatbots. The experimental results demonstrate the solid in-context learning capability of FireRedTTS, which can stably synthesize high-quality speech consistent with the prompt text and audio. For dubbing, FireRedTTS can clone target voices in a zero-shot way for the UGC scenario and adapt to studio-level expressive voice characters in the PUGC scenario via few-shot fine-tuning with 1-hour recording. Moreover, FireRedTTS achieves controllable human-like speech generation in a casual style with paralinguistic behaviors and emotions via instruction tuning, to better serve spoken chatbots."", ""authors"": [""Hao-Han Guo"", ""Yao Hu"", ""Kun Liu"", ""Fei-Yu Shen"", ""Xu Tang"", ""Yi-Chen Wu"", ""Feng-Long Xie"", ""Kun Xie"", ""Kai-Tuo Xu""], ""published"": ""2024-09-05T06:48:02Z"", ""updated"": ""2025-04-11T07:36:53Z"", ""categories"": [""cs.SD"", ""eess.AS""], ""pdf_url"": ""https://arxiv.org/pdf/2409.03283v2"", ""landing_url"": ""https://arxiv.org/abs/2409.03283v2"", ""doi"": ""https://doi.org/10.48550/arXiv.2409.03283""}",2409.03283,https://openalex.org/W4403556084
10.48550/arxiv.2409.03701,LAST: Language Model Aware Speech Tokenization,"Speech tokenization serves as the foundation of speech language model (LM), enabling them to perform various tasks such as spoken language modeling, text-to-speech, speech-to-text, etc. Most speech tokenizers are trained independently of the LM training process, relying on separate acoustic models and quantization methods. Following such an approach may create a mismatch between the tokenization process and its usage afterward. In this study, we propose a novel approach to training a speech tokenizer by leveraging objectives from pre-trained textual LMs. We advocate for the integration of this objective into the process of learning discrete speech representations. Our aim is to transform features from a pre-trained speech model into a new feature space that enables better clustering for speech LMs. We empirically investigate the impact of various model design choices, including speech vocabulary size and text LM size. Our results demonstrate the proposed tokenization method outperforms the evaluated baselines considering both spoken language modeling and speech-to-text. More importantly, unlike prior work, the proposed method allows the utilization of a single pre-trained LM for processing both speech and text inputs, setting it apart from conventional tokenization approaches.",1,,include (junior:5),https://arxiv.org/abs/2409.03701v2,https://arxiv.org/pdf/2409.03701v2,2024,discrete speech tokens,speech tokenization,"{""arxiv_id"": ""2409.03701"", ""title"": ""LAST: Language Model Aware Speech Tokenization"", ""summary"": ""Speech tokenization serves as the foundation of speech language model (LM), enabling them to perform various tasks such as spoken language modeling, text-to-speech, speech-to-text, etc. Most speech tokenizers are trained independently of the LM training process, relying on separate acoustic models and quantization methods. Following such an approach may create a mismatch between the tokenization process and its usage afterward. In this study, we propose a novel approach to training a speech tokenizer by leveraging objectives from pre-trained textual LMs. We advocate for the integration of this objective into the process of learning discrete speech representations. Our aim is to transform features from a pre-trained speech model into a new feature space that enables better clustering for speech LMs. We empirically investigate the impact of various model design choices, including speech vocabulary size and text LM size. Our results demonstrate the proposed tokenization method outperforms the evaluated baselines considering both spoken language modeling and speech-to-text. More importantly, unlike prior work, the proposed method allows the utilization of a single pre-trained LM for processing both speech and text inputs, setting it apart from conventional tokenization approaches."", ""authors"": [""Arnon Turetzky"", ""Yossi Adi""], ""published"": ""2024-09-05T16:57:39Z"", ""updated"": ""2024-09-10T14:45:15Z"", ""categories"": [""cs.CL"", ""cs.SD"", ""eess.AS""], ""pdf_url"": ""https://arxiv.org/pdf/2409.03701v2"", ""landing_url"": ""https://arxiv.org/abs/2409.03701v2"", ""doi"": ""https://doi.org/10.48550/arXiv.2409.03701""}",2409.03701,https://openalex.org/W4403590045
10.48550/arxiv.2409.04016,Investigating Neural Audio Codecs for Speech Language Model-Based Speech Generation,"Neural audio codec tokens serve as the fundamental building blocks for speech language model (SLM)-based speech generation. However, there is no systematic understanding on how the codec system affects the speech generation performance of the SLM. In this work, we examine codec tokens within SLM framework for speech generation to provide insights for effective codec design. We retrain existing high-performing neural codec models on the same data set and loss functions to compare their performance in a uniform setting. We integrate codec tokens into two SLM systems: masked-based parallel speech generation system and an auto-regressive (AR) plus non-auto-regressive (NAR) model-based system. Our findings indicate that better speech reconstruction in codec systems does not guarantee improved speech generation in SLM. A high-quality codec decoder is crucial for natural speech production in SLM, while speech intelligibility depends more on quantization mechanism.",1,,include (junior:5),https://arxiv.org/abs/2409.04016v1,https://arxiv.org/pdf/2409.04016v1,2024,acoustic tokens,neural codec,"{""arxiv_id"": ""2409.04016"", ""title"": ""Investigating Neural Audio Codecs for Speech Language Model-Based Speech Generation"", ""summary"": ""Neural audio codec tokens serve as the fundamental building blocks for speech language model (SLM)-based speech generation. However, there is no systematic understanding on how the codec system affects the speech generation performance of the SLM. In this work, we examine codec tokens within SLM framework for speech generation to provide insights for effective codec design. We retrain existing high-performing neural codec models on the same data set and loss functions to compare their performance in a uniform setting. We integrate codec tokens into two SLM systems: masked-based parallel speech generation system and an auto-regressive (AR) plus non-auto-regressive (NAR) model-based system. Our findings indicate that better speech reconstruction in codec systems does not guarantee improved speech generation in SLM. A high-quality codec decoder is crucial for natural speech production in SLM, while speech intelligibility depends more on quantization mechanism."", ""authors"": [""Jiaqi Li"", ""Dongmei Wang"", ""Xiaofei Wang"", ""Yao Qian"", ""Long Zhou"", ""Shujie Liu"", ""Midia Yousefi"", ""Canrun Li"", ""Chung-Hsien Tsai"", ""Zhen Xiao"", ""Yanqing Liu"", ""Junkun Chen"", ""Sheng Zhao"", ""Jinyu Li"", ""Zhizheng Wu"", ""Michael Zeng""], ""published"": ""2024-09-06T04:06:50Z"", ""updated"": ""2024-09-06T04:06:50Z"", ""categories"": [""cs.SD"", ""eess.AS""], ""pdf_url"": ""https://arxiv.org/pdf/2409.04016v1"", ""landing_url"": ""https://arxiv.org/abs/2409.04016v1"", ""doi"": ""https://doi.org/10.48550/arXiv.2409.04016""}",2409.04016,https://openalex.org/W4403586121
10.48550/arxiv.2409.05004,Disentangling the Prosody and Semantic Information with Pre-trained Model for In-Context Learning based Zero-Shot Voice Conversion,"Voice conversion (VC) aims to modify the speaker's timbre while retaining speech content. Previous approaches have tokenized the outputs from self-supervised into semantic tokens, facilitating disentanglement of speech content information. Recently, in-context learning (ICL) has emerged in text-to-speech (TTS) systems for effectively modeling specific characteristics such as timbre through context conditioning. This paper proposes an ICL capability enhanced VC system (ICL-VC) employing a mask and reconstruction training strategy based on flow-matching generative models. Augmented with semantic tokens, our experiments on the LibriTTS dataset demonstrate that ICL-VC improves speaker similarity. Additionally, we find that k-means is a versatile tokenization method applicable to various pre-trained models. However, the ICL-VC system faces challenges in preserving the prosody of the source speech. To mitigate this issue, we propose incorporating prosody embeddings extracted from a pre-trained emotion recognition model into our system. Integration of prosody embeddings notably enhances the system's capability to preserve source speech prosody, as validated on the Emotional Speech Database.",1,,include (junior:4),https://arxiv.org/abs/2409.05004v2,https://arxiv.org/pdf/2409.05004v2,2024,discrete speech tokens,semantic tokens,"{""arxiv_id"": ""2409.05004"", ""title"": ""Disentangling the Prosody and Semantic Information with Pre-trained Model for In-Context Learning based Zero-Shot Voice Conversion"", ""summary"": ""Voice conversion (VC) aims to modify the speaker's timbre while retaining speech content. Previous approaches have tokenized the outputs from self-supervised into semantic tokens, facilitating disentanglement of speech content information. Recently, in-context learning (ICL) has emerged in text-to-speech (TTS) systems for effectively modeling specific characteristics such as timbre through context conditioning. This paper proposes an ICL capability enhanced VC system (ICL-VC) employing a mask and reconstruction training strategy based on flow-matching generative models. Augmented with semantic tokens, our experiments on the LibriTTS dataset demonstrate that ICL-VC improves speaker similarity. Additionally, we find that k-means is a versatile tokenization method applicable to various pre-trained models. However, the ICL-VC system faces challenges in preserving the prosody of the source speech. To mitigate this issue, we propose incorporating prosody embeddings extracted from a pre-trained emotion recognition model into our system. Integration of prosody embeddings notably enhances the system's capability to preserve source speech prosody, as validated on the Emotional Speech Database."", ""authors"": [""Zhengyang Chen"", ""Shuai Wang"", ""Mingyang Zhang"", ""Xuechen Liu"", ""Junichi Yamagishi"", ""Yanmin Qian""], ""published"": ""2024-09-08T07:24:03Z"", ""updated"": ""2024-09-10T07:36:03Z"", ""categories"": [""cs.SD"", ""eess.AS""], ""pdf_url"": ""https://arxiv.org/pdf/2409.05004v2"", ""landing_url"": ""https://arxiv.org/abs/2409.05004v2"", ""doi"": ""https://doi.org/10.48550/arXiv.2409.05004""}",2409.05004,https://openalex.org/W4403590122
10.48550/arxiv.2409.09357,Joint Semantic Knowledge Distillation and Masked Acoustic Modeling for Full-band Speech Restoration with Improved Intelligibility,"Speech restoration aims at restoring full-band speech with high quality and intelligibility, considering a diverse set of distortions. MaskSR is a recently proposed generative model for this task. As other models of its kind, MaskSR attains high quality but, as we show, intelligibility can be substantially improved. We do so by boosting the speech encoder component of MaskSR with predictions of semantic representations of the target speech, using a pre-trained self-supervised teacher model. Then, a masked language model is conditioned on the learned semantic features to predict acoustic tokens that encode low level spectral details of the target speech. We show that, with the same MaskSR model capacity and inference time, the proposed model, MaskSR2, significantly reduces the word error rate, a typical metric for intelligibility. MaskSR2 also achieves competitive word error rate among other models, while providing superior quality. An ablation study shows the effectiveness of various semantic representations.",1,,include (junior:4),https://arxiv.org/abs/2409.09357v1,https://arxiv.org/pdf/2409.09357v1,2024,discrete speech tokens,acoustic tokens,"{""arxiv_id"": ""2409.09357"", ""title"": ""Joint Semantic Knowledge Distillation and Masked Acoustic Modeling for Full-band Speech Restoration with Improved Intelligibility"", ""summary"": ""Speech restoration aims at restoring full-band speech with high quality and intelligibility, considering a diverse set of distortions. MaskSR is a recently proposed generative model for this task. As other models of its kind, MaskSR attains high quality but, as we show, intelligibility can be substantially improved. We do so by boosting the speech encoder component of MaskSR with predictions of semantic representations of the target speech, using a pre-trained self-supervised teacher model. Then, a masked language model is conditioned on the learned semantic features to predict acoustic tokens that encode low level spectral details of the target speech. We show that, with the same MaskSR model capacity and inference time, the proposed model, MaskSR2, significantly reduces the word error rate, a typical metric for intelligibility. MaskSR2 also achieves competitive word error rate among other models, while providing superior quality. An ablation study shows the effectiveness of various semantic representations."", ""authors"": [""Xiaoyu Liu"", ""Xu Li"", ""Joan Serrà"", ""Santiago Pascual""], ""published"": ""2024-09-14T08:09:55Z"", ""updated"": ""2024-09-14T08:09:55Z"", ""categories"": [""cs.SD"", ""cs.AI"", ""eess.AS"", ""eess.SP""], ""pdf_url"": ""https://arxiv.org/pdf/2409.09357v1"", ""landing_url"": ""https://arxiv.org/abs/2409.09357v1"", ""doi"": ""https://doi.org/10.48550/arXiv.2409.09357""}",2409.09357,https://openalex.org/W4403667153
10.48550/arxiv.2409.10103,Self-Supervised Syllable Discovery Based on Speaker-Disentangled HuBERT,"Self-supervised speech representation learning has become essential for extracting meaningful features from untranscribed audio. Recent advances highlight the potential of deriving discrete symbols from the features correlated with linguistic units, which enables text-less training across diverse tasks. In particular, sentence-level Self-Distillation of the pretrained HuBERT (SD-HuBERT) induces syllabic structures within latent speech frame representations extracted from an intermediate Transformer layer. In SD-HuBERT, sentence-level representation is accumulated from speech frame features through self-attention layers using a special CLS token. However, we observe that the information aggregated in the CLS token correlates more with speaker identity than with linguistic content. To address this, we propose a speech-only self-supervised fine-tuning approach that separates syllabic units from speaker information. Our method introduces speaker perturbation as data augmentation and adopts a frame-level training objective to prevent the CLS token from aggregating paralinguistic information. Experimental results show that our approach surpasses the current state-of-the-art method in most syllable segmentation and syllabic unit quality metrics on Librispeech, underscoring its effectiveness in promoting syllabic organization within speech-only models.",1,,include (junior:4),https://arxiv.org/abs/2409.10103v1,https://arxiv.org/pdf/2409.10103v1,2024,discrete speech tokens,hubert,"{""arxiv_id"": ""2409.10103"", ""title"": ""Self-Supervised Syllable Discovery Based on Speaker-Disentangled HuBERT"", ""summary"": ""Self-supervised speech representation learning has become essential for extracting meaningful features from untranscribed audio. Recent advances highlight the potential of deriving discrete symbols from the features correlated with linguistic units, which enables text-less training across diverse tasks. In particular, sentence-level Self-Distillation of the pretrained HuBERT (SD-HuBERT) induces syllabic structures within latent speech frame representations extracted from an intermediate Transformer layer. In SD-HuBERT, sentence-level representation is accumulated from speech frame features through self-attention layers using a special CLS token. However, we observe that the information aggregated in the CLS token correlates more with speaker identity than with linguistic content. To address this, we propose a speech-only self-supervised fine-tuning approach that separates syllabic units from speaker information. Our method introduces speaker perturbation as data augmentation and adopts a frame-level training objective to prevent the CLS token from aggregating paralinguistic information. Experimental results show that our approach surpasses the current state-of-the-art method in most syllable segmentation and syllabic unit quality metrics on Librispeech, underscoring its effectiveness in promoting syllabic organization within speech-only models."", ""authors"": [""Ryota Komatsu"", ""Takahiro Shinozaki""], ""published"": ""2024-09-16T09:07:08Z"", ""updated"": ""2024-09-16T09:07:08Z"", ""categories"": [""cs.CL"", ""cs.SD"", ""eess.AS""], ""pdf_url"": ""https://arxiv.org/pdf/2409.10103v1"", ""landing_url"": ""https://arxiv.org/abs/2409.10103v1"", ""doi"": ""https://doi.org/10.48550/arXiv.2409.10103""}",2409.10103,https://openalex.org/W4403702982
10.48550/arxiv.2409.11003,Single-stage TTS with Masked Audio Token Modeling and Semantic Knowledge Distillation,"Audio token modeling has become a powerful framework for speech synthesis, with two-stage approaches employing semantic tokens remaining prevalent. In this paper, we aim to simplify this process by introducing a semantic knowledge distillation method that enables high-quality speech generation in a single stage. Our proposed model improves speech quality, intelligibility, and speaker similarity compared to a single-stage baseline. Although two-stage systems still lead in intelligibility, our model significantly narrows the gap while delivering comparable speech quality. These findings showcase the potential of single-stage models to achieve efficient, high-quality TTS with a more compact and streamlined architecture.",1,,include (junior:4),https://arxiv.org/abs/2409.11003v1,https://arxiv.org/pdf/2409.11003v1,2024,discrete speech tokens,semantic tokens,"{""arxiv_id"": ""2409.11003"", ""title"": ""Single-stage TTS with Masked Audio Token Modeling and Semantic Knowledge Distillation"", ""summary"": ""Audio token modeling has become a powerful framework for speech synthesis, with two-stage approaches employing semantic tokens remaining prevalent. In this paper, we aim to simplify this process by introducing a semantic knowledge distillation method that enables high-quality speech generation in a single stage. Our proposed model improves speech quality, intelligibility, and speaker similarity compared to a single-stage baseline. Although two-stage systems still lead in intelligibility, our model significantly narrows the gap while delivering comparable speech quality. These findings showcase the potential of single-stage models to achieve efficient, high-quality TTS with a more compact and streamlined architecture."", ""authors"": [""Gerard I. Gállego"", ""Roy Fejgin"", ""Chunghsin Yeh"", ""Xiaoyu Liu"", ""Gautam Bhattacharya""], ""published"": ""2024-09-17T09:08:43Z"", ""updated"": ""2024-09-17T09:08:43Z"", ""categories"": [""cs.SD"", ""cs.AI"", ""eess.AS"", ""eess.SP""], ""pdf_url"": ""https://arxiv.org/pdf/2409.11003v1"", ""landing_url"": ""https://arxiv.org/abs/2409.11003v1"", ""doi"": ""https://doi.org/10.48550/arXiv.2409.11003""}",2409.11003,https://openalex.org/W4403706273
10.48550/arxiv.2409.11228,Learning Source Disentanglement in Neural Audio Codec,"Neural audio codecs have significantly advanced audio compression by efficiently converting continuous audio signals into discrete tokens. These codecs preserve high-quality sound and enable sophisticated sound generation through generative models trained on these tokens. However, existing neural codec models are typically trained on large, undifferentiated audio datasets, neglecting the essential discrepancies between sound domains like speech, music, and environmental sound effects. This oversight complicates data modeling and poses additional challenges to the controllability of sound generation. To tackle these issues, we introduce the Source-Disentangled Neural Audio Codec (SD-Codec), a novel approach that combines audio coding and source separation. By jointly learning audio resynthesis and separation, SD-Codec explicitly assigns audio signals from different domains to distinct codebooks, sets of discrete representations. Experimental results indicate that SD-Codec not only maintains competitive resynthesis quality but also, supported by the separation results, demonstrates successful disentanglement of different sources in the latent space, thereby enhancing interpretability in audio codec and providing potential finer control over the audio generation process.",1,,include (junior:5),https://arxiv.org/abs/2409.11228v2,https://arxiv.org/pdf/2409.11228v2,2024,acoustic tokens,neural codec,"{""arxiv_id"": ""2409.11228"", ""title"": ""Learning Source Disentanglement in Neural Audio Codec"", ""summary"": ""Neural audio codecs have significantly advanced audio compression by efficiently converting continuous audio signals into discrete tokens. These codecs preserve high-quality sound and enable sophisticated sound generation through generative models trained on these tokens. However, existing neural codec models are typically trained on large, undifferentiated audio datasets, neglecting the essential discrepancies between sound domains like speech, music, and environmental sound effects. This oversight complicates data modeling and poses additional challenges to the controllability of sound generation. To tackle these issues, we introduce the Source-Disentangled Neural Audio Codec (SD-Codec), a novel approach that combines audio coding and source separation. By jointly learning audio resynthesis and separation, SD-Codec explicitly assigns audio signals from different domains to distinct codebooks, sets of discrete representations. Experimental results indicate that SD-Codec not only maintains competitive resynthesis quality but also, supported by the separation results, demonstrates successful disentanglement of different sources in the latent space, thereby enhancing interpretability in audio codec and providing potential finer control over the audio generation process."", ""authors"": [""Xiaoyu Bie"", ""Xubo Liu"", ""Gaël Richard""], ""published"": ""2024-09-17T14:21:02Z"", ""updated"": ""2025-02-11T10:35:04Z"", ""categories"": [""cs.SD"", ""cs.AI"", ""cs.LG"", ""eess.AS""], ""pdf_url"": ""https://arxiv.org/pdf/2409.11228v2"", ""landing_url"": ""https://arxiv.org/abs/2409.11228v2"", ""doi"": ""https://doi.org/10.48550/arXiv.2409.11228""}",2409.11228,https://openalex.org/W4403707096
10.48550/arxiv.2409.11630,Speaking from Coarse to Fine: Improving Neural Codec Language Model via Multi-Scale Speech Coding and Generation,"The neural codec language model (CLM) has demonstrated remarkable performance in text-to-speech (TTS) synthesis. However, troubled by ``recency bias"", CLM lacks sufficient attention to coarse-grained information at a higher temporal scale, often producing unnatural or even unintelligible speech. This work proposes CoFi-Speech, a coarse-to-fine CLM-TTS approach, employing multi-scale speech coding and generation to address this issue. We train a multi-scale neural codec, CoFi-Codec, to encode speech into a multi-scale discrete representation, comprising multiple token sequences with different time resolutions. Then, we propose CoFi-LM that can generate this representation in two modes: the single-LM-based chain-of-scale generation and the multiple-LM-based stack-of-scale generation. In experiments, CoFi-Speech significantly outperforms single-scale baseline systems on naturalness and speaker similarity in zero-shot TTS. The analysis of multi-scale coding demonstrates the effectiveness of CoFi-Codec in learning multi-scale discrete speech representations while keeping high-quality speech reconstruction. The coarse-to-fine multi-scale generation, especially for the stack-of-scale approach, is also validated as a crucial approach in pursuing a high-quality neural codec language model for TTS.",1,,include (junior:5),https://arxiv.org/abs/2409.11630v1,https://arxiv.org/pdf/2409.11630v1,2024,discrete speech tokens,neural codec,"{""arxiv_id"": ""2409.11630"", ""title"": ""Speaking from Coarse to Fine: Improving Neural Codec Language Model via Multi-Scale Speech Coding and Generation"", ""summary"": ""The neural codec language model (CLM) has demonstrated remarkable performance in text-to-speech (TTS) synthesis. However, troubled by ``recency bias\"", CLM lacks sufficient attention to coarse-grained information at a higher temporal scale, often producing unnatural or even unintelligible speech. This work proposes CoFi-Speech, a coarse-to-fine CLM-TTS approach, employing multi-scale speech coding and generation to address this issue. We train a multi-scale neural codec, CoFi-Codec, to encode speech into a multi-scale discrete representation, comprising multiple token sequences with different time resolutions. Then, we propose CoFi-LM that can generate this representation in two modes: the single-LM-based chain-of-scale generation and the multiple-LM-based stack-of-scale generation. In experiments, CoFi-Speech significantly outperforms single-scale baseline systems on naturalness and speaker similarity in zero-shot TTS. The analysis of multi-scale coding demonstrates the effectiveness of CoFi-Codec in learning multi-scale discrete speech representations while keeping high-quality speech reconstruction. The coarse-to-fine multi-scale generation, especially for the stack-of-scale approach, is also validated as a crucial approach in pursuing a high-quality neural codec language model for TTS."", ""authors"": [""Haohan Guo"", ""Fenglong Xie"", ""Dongchao Yang"", ""Xixin Wu"", ""Helen Meng""], ""published"": ""2024-09-18T01:31:19Z"", ""updated"": ""2024-09-18T01:31:19Z"", ""categories"": [""cs.SD"", ""eess.AS""], ""pdf_url"": ""https://arxiv.org/pdf/2409.11630v1"", ""landing_url"": ""https://arxiv.org/abs/2409.11630v1"", ""doi"": ""https://doi.org/10.48550/arXiv.2409.11630""}",2409.1163,https://openalex.org/W4403707928
10.48550/arxiv.2409.12717,NDVQ: Robust Neural Audio Codec with Normal Distribution-Based Vector Quantization,"Built upon vector quantization (VQ), discrete audio codec models have achieved great success in audio compression and auto-regressive audio generation. However, existing models face substantial challenges in perceptual quality and signal distortion, especially when operating in extremely low bandwidth, rooted in the sensitivity of the VQ codebook to noise. This degradation poses significant challenges for several downstream tasks, such as codec-based speech synthesis. To address this issue, we propose a novel VQ method, Normal Distribution-based Vector Quantization (NDVQ), by introducing an explicit margin between the VQ codes via learning a variance. Specifically, our approach involves mapping the waveform to a latent space and quantizing it by selecting the most likely normal distribution, with each codebook entry representing a unique normal distribution defined by its mean and variance. Using these distribution-based VQ codec codes, a decoder reconstructs the input waveform. NDVQ is trained with additional distribution-related losses, alongside reconstruction and discrimination losses. Experiments demonstrate that NDVQ outperforms existing audio compression baselines, such as EnCodec, in terms of audio quality and zero-shot TTS, particularly in very low bandwidth scenarios.",1,,include (junior:5),https://arxiv.org/abs/2409.12717v1,https://arxiv.org/pdf/2409.12717v1,2024,discrete speech tokens,vector quantization,"{""arxiv_id"": ""2409.12717"", ""title"": ""NDVQ: Robust Neural Audio Codec with Normal Distribution-Based Vector Quantization"", ""summary"": ""Built upon vector quantization (VQ), discrete audio codec models have achieved great success in audio compression and auto-regressive audio generation. However, existing models face substantial challenges in perceptual quality and signal distortion, especially when operating in extremely low bandwidth, rooted in the sensitivity of the VQ codebook to noise. This degradation poses significant challenges for several downstream tasks, such as codec-based speech synthesis. To address this issue, we propose a novel VQ method, Normal Distribution-based Vector Quantization (NDVQ), by introducing an explicit margin between the VQ codes via learning a variance. Specifically, our approach involves mapping the waveform to a latent space and quantizing it by selecting the most likely normal distribution, with each codebook entry representing a unique normal distribution defined by its mean and variance. Using these distribution-based VQ codec codes, a decoder reconstructs the input waveform. NDVQ is trained with additional distribution-related losses, alongside reconstruction and discrimination losses. Experiments demonstrate that NDVQ outperforms existing audio compression baselines, such as EnCodec, in terms of audio quality and zero-shot TTS, particularly in very low bandwidth scenarios."", ""authors"": [""Zhikang Niu"", ""Sanyuan Chen"", ""Long Zhou"", ""Ziyang Ma"", ""Xie Chen"", ""Shujie Liu""], ""published"": ""2024-09-19T12:41:30Z"", ""updated"": ""2024-09-19T12:41:30Z"", ""categories"": [""eess.AS"", ""cs.SD""], ""pdf_url"": ""https://arxiv.org/pdf/2409.12717v1"", ""landing_url"": ""https://arxiv.org/abs/2409.12717v1"", ""doi"": ""https://doi.org/10.48550/arXiv.2409.12717""}",2409.12717,https://openalex.org/W4403747696
10.48550/arxiv.2409.15897,"ESPnet-Codec: Comprehensive Training and Evaluation of Neural Codecs for Audio, Music, and Speech","Neural codecs have become crucial to recent speech and audio generation research. In addition to signal compression capabilities, discrete codecs have also been found to enhance downstream training efficiency and compatibility with autoregressive language models. However, as extensive downstream applications are investigated, challenges have arisen in ensuring fair comparisons across diverse applications. To address these issues, we present a new open-source platform ESPnet-Codec, which is built on ESPnet and focuses on neural codec training and evaluation. ESPnet-Codec offers various recipes in audio, music, and speech for training and evaluation using several widely adopted codec models. Together with ESPnet-Codec, we present VERSA, a standalone evaluation toolkit, which provides a comprehensive evaluation of codec performance over 20 audio evaluation metrics. Notably, we demonstrate that ESPnet-Codec can be integrated into six ESPnet tasks, supporting diverse applications.",1,,include (senior:5),https://arxiv.org/abs/2409.15897v2,https://arxiv.org/pdf/2409.15897v2,2024,discrete speech tokens,neural codec,"{""arxiv_id"": ""2409.15897"", ""title"": ""ESPnet-Codec: Comprehensive Training and Evaluation of Neural Codecs for Audio, Music, and Speech"", ""summary"": ""Neural codecs have become crucial to recent speech and audio generation research. In addition to signal compression capabilities, discrete codecs have also been found to enhance downstream training efficiency and compatibility with autoregressive language models. However, as extensive downstream applications are investigated, challenges have arisen in ensuring fair comparisons across diverse applications. To address these issues, we present a new open-source platform ESPnet-Codec, which is built on ESPnet and focuses on neural codec training and evaluation. ESPnet-Codec offers various recipes in audio, music, and speech for training and evaluation using several widely adopted codec models. Together with ESPnet-Codec, we present VERSA, a standalone evaluation toolkit, which provides a comprehensive evaluation of codec performance over 20 audio evaluation metrics. Notably, we demonstrate that ESPnet-Codec can be integrated into six ESPnet tasks, supporting diverse applications."", ""authors"": [""Jiatong Shi"", ""Jinchuan Tian"", ""Yihan Wu"", ""Jee-weon Jung"", ""Jia Qi Yip"", ""Yoshiki Masuyama"", ""William Chen"", ""Yuning Wu"", ""Yuxun Tang"", ""Massa Baali"", ""Dareen Alharhi"", ""Dong Zhang"", ""Ruifan Deng"", ""Tejes Srivastava"", ""Haibin Wu"", ""Alexander H. Liu"", ""Bhiksha Raj"", ""Qin Jin"", ""Ruihua Song"", ""Shinji Watanabe""], ""published"": ""2024-09-24T09:16:11Z"", ""updated"": ""2025-02-24T18:34:41Z"", ""categories"": [""eess.AS"", ""cs.SD""], ""pdf_url"": ""https://arxiv.org/pdf/2409.15897v2"", ""landing_url"": ""https://arxiv.org/abs/2409.15897v2"", ""doi"": ""https://doi.org/10.48550/arXiv.2409.15897""}",2409.15897,https://openalex.org/W4403787069
10.48550/arxiv.2409.19283,Analyzing and Mitigating Inconsistency in Discrete Audio Tokens for Neural Codec Language Models,"Building upon advancements in Large Language Models (LLMs), the field of audio processing has seen increased interest in training audio generation tasks with discrete audio token sequences. However, directly discretizing audio by neural audio codecs often results in sequences that fundamentally differ from text sequences. Unlike text, where text token sequences are deterministic, discrete audio tokens can exhibit significant variability based on contextual factors, while still producing perceptually identical audio segments. We refer to this phenomenon as \textbf{Discrete Representation Inconsistency (DRI)}. This inconsistency can lead to a single audio segment being represented by multiple divergent sequences, which creates confusion in neural codec language models and results in omissions and repetitions during speech generation. In this paper, we quantitatively analyze the DRI phenomenon within popular audio tokenizers such as EnCodec. Our approach effectively mitigates the DRI phenomenon of the neural audio codec. Furthermore, extensive experiments on the neural codec language model over LibriTTS and large-scale MLS datases (44,000 hours) demonstrate the effectiveness and generality of our method. The demo of audio samples is available online~\footnote{\url{https://consistencyinneuralcodec.github.io}}.",1,,include (junior:5),https://arxiv.org/abs/2409.19283v2,https://arxiv.org/pdf/2409.19283v2,2024,discrete speech tokens,neural codec,"{""arxiv_id"": ""2409.19283"", ""title"": ""Analyzing and Mitigating Inconsistency in Discrete Audio Tokens for Neural Codec Language Models"", ""summary"": ""Building upon advancements in Large Language Models (LLMs), the field of audio processing has seen increased interest in training audio generation tasks with discrete audio token sequences. However, directly discretizing audio by neural audio codecs often results in sequences that fundamentally differ from text sequences. Unlike text, where text token sequences are deterministic, discrete audio tokens can exhibit significant variability based on contextual factors, while still producing perceptually identical audio segments. We refer to this phenomenon as \\textbf{Discrete Representation Inconsistency (DRI)}. This inconsistency can lead to a single audio segment being represented by multiple divergent sequences, which creates confusion in neural codec language models and results in omissions and repetitions during speech generation. In this paper, we quantitatively analyze the DRI phenomenon within popular audio tokenizers such as EnCodec. Our approach effectively mitigates the DRI phenomenon of the neural audio codec. Furthermore, extensive experiments on the neural codec language model over LibriTTS and large-scale MLS datases (44,000 hours) demonstrate the effectiveness and generality of our method. The demo of audio samples is available online~\\footnote{\\url{https://consistencyinneuralcodec.github.io}}."", ""authors"": [""Wenrui Liu"", ""Zhifang Guo"", ""Jin Xu"", ""Yuanjun Lv"", ""Yunfei Chu"", ""Zhou Zhao"", ""Junyang Lin""], ""published"": ""2024-09-28T08:36:44Z"", ""updated"": ""2024-10-04T22:34:38Z"", ""categories"": [""eess.AS"", ""cs.SD""], ""pdf_url"": ""https://arxiv.org/pdf/2409.19283v2"", ""landing_url"": ""https://arxiv.org/abs/2409.19283v2"", ""doi"": ""https://doi.org/10.48550/arXiv.2409.19283""}",2409.19283,https://openalex.org/W4403321974
10.48550/arxiv.2410.00037,Moshi: a speech-text foundation model for real-time dialogue,"We introduce Moshi, a speech-text foundation model and full-duplex spoken dialogue framework. Current systems for spoken dialogue rely on pipelines of independent components, namely voice activity detection, speech recognition, textual dialogue and text-to-speech. Such frameworks cannot emulate the experience of real conversations. First, their complexity induces a latency of several seconds between interactions. Second, text being the intermediate modality for dialogue, non-linguistic information that modifies meaning -- such as emotion or non-speech sounds -- is lost in the interaction. Finally, they rely on a segmentation into speaker turns, which does not take into account overlapping speech, interruptions and interjections. Moshi solves these independent issues altogether by casting spoken dialogue as speech-to-speech generation. Starting from a text language model backbone, Moshi generates speech as tokens from the residual quantizer of a neural audio codec, while modeling separately its own speech and that of the user into parallel streams. This allows for the removal of explicit speaker turns, and the modeling of arbitrary conversational dynamics. We moreover extend the hierarchical semantic-to-acoustic token generation of previous work to first predict time-aligned text tokens as a prefix to audio tokens. Not only this ""Inner Monologue"" method significantly improves the linguistic quality of generated speech, but we also illustrate how it can provide streaming speech recognition and text-to-speech. Our resulting model is the first real-time full-duplex spoken large language model, with a theoretical latency of 160ms, 200ms in practice, and is available at https://github.com/kyutai-labs/moshi.",1,,include (junior:5),https://arxiv.org/abs/2410.00037v2,https://arxiv.org/pdf/2410.00037v2,2024,discrete speech tokens,acoustic tokens,"{""arxiv_id"": ""2410.00037"", ""title"": ""Moshi: a speech-text foundation model for real-time dialogue"", ""summary"": ""We introduce Moshi, a speech-text foundation model and full-duplex spoken dialogue framework. Current systems for spoken dialogue rely on pipelines of independent components, namely voice activity detection, speech recognition, textual dialogue and text-to-speech. Such frameworks cannot emulate the experience of real conversations. First, their complexity induces a latency of several seconds between interactions. Second, text being the intermediate modality for dialogue, non-linguistic information that modifies meaning -- such as emotion or non-speech sounds -- is lost in the interaction. Finally, they rely on a segmentation into speaker turns, which does not take into account overlapping speech, interruptions and interjections. Moshi solves these independent issues altogether by casting spoken dialogue as speech-to-speech generation. Starting from a text language model backbone, Moshi generates speech as tokens from the residual quantizer of a neural audio codec, while modeling separately its own speech and that of the user into parallel streams. This allows for the removal of explicit speaker turns, and the modeling of arbitrary conversational dynamics. We moreover extend the hierarchical semantic-to-acoustic token generation of previous work to first predict time-aligned text tokens as a prefix to audio tokens. Not only this \""Inner Monologue\"" method significantly improves the linguistic quality of generated speech, but we also illustrate how it can provide streaming speech recognition and text-to-speech. Our resulting model is the first real-time full-duplex spoken large language model, with a theoretical latency of 160ms, 200ms in practice, and is available at https://github.com/kyutai-labs/moshi."", ""authors"": [""Alexandre Défossez"", ""Laurent Mazaré"", ""Manu Orsini"", ""Amélie Royer"", ""Patrick Pérez"", ""Hervé Jégou"", ""Edouard Grave"", ""Neil Zeghidour""], ""published"": ""2024-09-17T17:55:39Z"", ""updated"": ""2024-10-02T09:11:45Z"", ""categories"": [""eess.AS"", ""cs.AI"", ""cs.CL"", ""cs.LG"", ""cs.SD""], ""pdf_url"": ""https://arxiv.org/pdf/2410.00037v2"", ""landing_url"": ""https://arxiv.org/abs/2410.00037v2"", ""doi"": ""https://doi.org/10.48550/arXiv.2410.00037""}",2410.00037,https://openalex.org/W4403883071
10.48550/arxiv.2410.03298,Textless Streaming Speech-to-Speech Translation using Semantic Speech Tokens,"Cascaded speech-to-speech translation systems often suffer from the error accumulation problem and high latency, which is a result of cascaded modules whose inference delays accumulate. In this paper, we propose a transducer-based speech translation model that outputs discrete speech tokens in a low-latency streaming fashion. This approach eliminates the need for generating text output first, followed by machine translation (MT) and text-to-speech (TTS) systems. The produced speech tokens can be directly used to generate a speech signal with low latency by utilizing an acoustic language model (LM) to obtain acoustic tokens and an audio codec model to retrieve the waveform. Experimental results show that the proposed method outperforms other existing approaches and achieves state-of-the-art results for streaming translation in terms of BLEU, average latency, and BLASER 2.0 scores for multiple language pairs using the CVSS-C dataset as a benchmark.",1,,include (junior:5),https://arxiv.org/abs/2410.03298v1,https://arxiv.org/pdf/2410.03298v1,2024,discrete speech tokens,offline clustering,"{""arxiv_id"": ""2410.03298"", ""title"": ""Textless Streaming Speech-to-Speech Translation using Semantic Speech Tokens"", ""summary"": ""Cascaded speech-to-speech translation systems often suffer from the error accumulation problem and high latency, which is a result of cascaded modules whose inference delays accumulate. In this paper, we propose a transducer-based speech translation model that outputs discrete speech tokens in a low-latency streaming fashion. This approach eliminates the need for generating text output first, followed by machine translation (MT) and text-to-speech (TTS) systems. The produced speech tokens can be directly used to generate a speech signal with low latency by utilizing an acoustic language model (LM) to obtain acoustic tokens and an audio codec model to retrieve the waveform. Experimental results show that the proposed method outperforms other existing approaches and achieves state-of-the-art results for streaming translation in terms of BLEU, average latency, and BLASER 2.0 scores for multiple language pairs using the CVSS-C dataset as a benchmark."", ""authors"": [""Jinzheng Zhao"", ""Niko Moritz"", ""Egor Lakomkin"", ""Ruiming Xie"", ""Zhiping Xiu"", ""Katerina Zmolikova"", ""Zeeshan Ahmed"", ""Yashesh Gaur"", ""Duc Le"", ""Christian Fuegen""], ""published"": ""2024-10-04T10:21:15Z"", ""updated"": ""2024-10-04T10:21:15Z"", ""categories"": [""eess.AS""], ""pdf_url"": ""https://arxiv.org/pdf/2410.03298v1"", ""landing_url"": ""https://arxiv.org/abs/2410.03298v1"", ""doi"": ""https://doi.org/10.48550/arXiv.2410.03298""}",2410.03298,https://openalex.org/W4403899613
10.48550/arxiv.2410.04380,HALL-E: Hierarchical Neural Codec Language Model for Minute-Long Zero-Shot Text-to-Speech Synthesis,"Recently, Text-to-speech (TTS) models based on large language models (LLMs) that translate natural language text into sequences of discrete audio tokens have gained great research attention, with advances in neural audio codec (NAC) models using residual vector quantization (RVQ). However, long-form speech synthesis remains a significant challenge due to the high frame rate, which increases the length of audio tokens and makes it difficult for autoregressive language models to generate audio tokens for even a minute of speech. To address this challenge, this paper introduces two novel post-training approaches: 1) Multi-Resolution Requantization (MReQ) and 2) HALL-E. MReQ is a framework to reduce the frame rate of pre-trained NAC models. Specifically, it incorporates multi-resolution residual vector quantization (MRVQ) module that hierarchically reorganizes discrete audio tokens through teacher-student distillation. HALL-E is an LLM-based TTS model designed to predict hierarchical tokens of MReQ. Specifically, it incorporates the technique of using MRVQ sub-modules and continues training from a pre-trained LLM-based TTS model. Furthermore, to promote TTS research, we create MinutesSpeech, a new benchmark dataset consisting of 40k hours of filtered speech data for training and evaluating speech synthesis ranging from 3s up to 180s. In experiments, we demonstrated the effectiveness of our approaches by applying our post-training framework to VALL-E. We achieved the frame rate down to as low as 8 Hz, enabling the stable minitue-long speech synthesis in a single inference step. Audio samples, dataset, codes and pre-trained models are available at https://yutonishimura-v2.github.io/HALL-E_DEMO/.",1,,include (junior:5),https://arxiv.org/abs/2410.04380v1,https://arxiv.org/pdf/2410.04380v1,2024,discrete speech tokens,neural codec,"{""arxiv_id"": ""2410.04380"", ""title"": ""HALL-E: Hierarchical Neural Codec Language Model for Minute-Long Zero-Shot Text-to-Speech Synthesis"", ""summary"": ""Recently, Text-to-speech (TTS) models based on large language models (LLMs) that translate natural language text into sequences of discrete audio tokens have gained great research attention, with advances in neural audio codec (NAC) models using residual vector quantization (RVQ). However, long-form speech synthesis remains a significant challenge due to the high frame rate, which increases the length of audio tokens and makes it difficult for autoregressive language models to generate audio tokens for even a minute of speech. To address this challenge, this paper introduces two novel post-training approaches: 1) Multi-Resolution Requantization (MReQ) and 2) HALL-E. MReQ is a framework to reduce the frame rate of pre-trained NAC models. Specifically, it incorporates multi-resolution residual vector quantization (MRVQ) module that hierarchically reorganizes discrete audio tokens through teacher-student distillation. HALL-E is an LLM-based TTS model designed to predict hierarchical tokens of MReQ. Specifically, it incorporates the technique of using MRVQ sub-modules and continues training from a pre-trained LLM-based TTS model. Furthermore, to promote TTS research, we create MinutesSpeech, a new benchmark dataset consisting of 40k hours of filtered speech data for training and evaluating speech synthesis ranging from 3s up to 180s. In experiments, we demonstrated the effectiveness of our approaches by applying our post-training framework to VALL-E. We achieved the frame rate down to as low as 8 Hz, enabling the stable minitue-long speech synthesis in a single inference step. Audio samples, dataset, codes and pre-trained models are available at https://yutonishimura-v2.github.io/HALL-E_DEMO/."", ""authors"": [""Yuto Nishimura"", ""Takumi Hirose"", ""Masanari Ohi"", ""Hideki Nakayama"", ""Nakamasa Inoue""], ""published"": ""2024-10-06T07:20:58Z"", ""updated"": ""2024-10-06T07:20:58Z"", ""categories"": [""eess.AS"", ""cs.SD""], ""pdf_url"": ""https://arxiv.org/pdf/2410.04380v1"", ""landing_url"": ""https://arxiv.org/abs/2410.04380v1"", ""doi"": ""https://doi.org/10.48550/arXiv.2410.04380""}",2410.0438,https://openalex.org/W4403963885
10.48550/arxiv.2410.06016,Variable Bitrate Residual Vector Quantization for Audio Coding,"Recent state-of-the-art neural audio compression models have progressively adopted residual vector quantization (RVQ). Despite this success, these models employ a fixed number of codebooks per frame, which can be suboptimal in terms of rate-distortion tradeoff, particularly in scenarios with simple input audio, such as silence. To address this limitation, we propose variable bitrate RVQ (VRVQ) for audio codecs, which allows for more efficient coding by adapting the number of codebooks used per frame. Furthermore, we propose a gradient estimation method for the non-differentiable masking operation that transforms from the importance map to the binary importance mask, improving model training via a straight-through estimator. We demonstrate that the proposed training framework achieves superior results compared to the baseline method and shows further improvement when applied to the current state-of-the-art codec.",1,,include (junior:5),https://arxiv.org/abs/2410.06016v3,https://arxiv.org/pdf/2410.06016v3,2024,discrete speech tokens,vector quantization,"{""arxiv_id"": ""2410.06016"", ""title"": ""Variable Bitrate Residual Vector Quantization for Audio Coding"", ""summary"": ""Recent state-of-the-art neural audio compression models have progressively adopted residual vector quantization (RVQ). Despite this success, these models employ a fixed number of codebooks per frame, which can be suboptimal in terms of rate-distortion tradeoff, particularly in scenarios with simple input audio, such as silence. To address this limitation, we propose variable bitrate RVQ (VRVQ) for audio codecs, which allows for more efficient coding by adapting the number of codebooks used per frame. Furthermore, we propose a gradient estimation method for the non-differentiable masking operation that transforms from the importance map to the binary importance mask, improving model training via a straight-through estimator. We demonstrate that the proposed training framework achieves superior results compared to the baseline method and shows further improvement when applied to the current state-of-the-art codec."", ""authors"": [""Yunkee Chae"", ""Woosung Choi"", ""Yuhta Takida"", ""Junghyun Koo"", ""Yukara Ikemiya"", ""Zhi Zhong"", ""Kin Wai Cheuk"", ""Marco A. Martínez-Ramírez"", ""Kyogu Lee"", ""Wei-Hsiang Liao"", ""Yuki Mitsufuji""], ""published"": ""2024-10-08T13:18:24Z"", ""updated"": ""2025-04-27T15:10:16Z"", ""categories"": [""cs.SD"", ""cs.LG"", ""eess.AS""], ""pdf_url"": ""https://arxiv.org/pdf/2410.06016v3"", ""landing_url"": ""https://arxiv.org/abs/2410.06016v3"", ""doi"": ""https://doi.org/10.48550/arXiv.2410.06016""}",2410.06016,https://openalex.org/W4403344546
10.48550/arxiv.2410.07168,Sylber: Syllabic Embedding Representation of Speech from Raw Audio,"Syllables are compositional units of spoken language that efficiently structure human speech perception and production. However, current neural speech representations lack such structure, resulting in dense token sequences that are costly to process. To bridge this gap, we propose a new model, Sylber, that produces speech representations with clean and robust syllabic structure. Specifically, we propose a self-supervised learning (SSL) framework that bootstraps syllabic embeddings by distilling from its own initial unsupervised syllabic segmentation. This results in a highly structured representation of speech features, offering three key benefits: 1) a fast, linear-time syllable segmentation algorithm, 2) efficient syllabic tokenization with an average of 4.27 tokens per second, and 3) novel phonological units suited for efficient spoken language modeling. Our proposed segmentation method is highly robust and generalizes to out-of-domain data and unseen languages without any tuning. By training token-to-speech generative models, fully intelligible speech can be reconstructed from Sylber tokens with a significantly lower bitrate than baseline SSL tokens. This suggests that our model effectively compresses speech into a compact sequence of tokens with minimal information loss. Lastly, we demonstrate that categorical perception-a linguistic phenomenon in speech perception-emerges naturally in Sylber, making the embedding space more categorical and sparse than previous speech features and thus supporting the high efficiency of our tokenization. Together, we present a novel SSL approach for representing speech as syllables, with significant potential for efficient speech tokenization and spoken language modeling.",1,,include (junior:5),https://arxiv.org/abs/2410.07168v2,https://arxiv.org/pdf/2410.07168v2,2024,discrete speech tokens,speech tokenization,"{""arxiv_id"": ""2410.07168"", ""title"": ""Sylber: Syllabic Embedding Representation of Speech from Raw Audio"", ""summary"": ""Syllables are compositional units of spoken language that efficiently structure human speech perception and production. However, current neural speech representations lack such structure, resulting in dense token sequences that are costly to process. To bridge this gap, we propose a new model, Sylber, that produces speech representations with clean and robust syllabic structure. Specifically, we propose a self-supervised learning (SSL) framework that bootstraps syllabic embeddings by distilling from its own initial unsupervised syllabic segmentation. This results in a highly structured representation of speech features, offering three key benefits: 1) a fast, linear-time syllable segmentation algorithm, 2) efficient syllabic tokenization with an average of 4.27 tokens per second, and 3) novel phonological units suited for efficient spoken language modeling. Our proposed segmentation method is highly robust and generalizes to out-of-domain data and unseen languages without any tuning. By training token-to-speech generative models, fully intelligible speech can be reconstructed from Sylber tokens with a significantly lower bitrate than baseline SSL tokens. This suggests that our model effectively compresses speech into a compact sequence of tokens with minimal information loss. Lastly, we demonstrate that categorical perception-a linguistic phenomenon in speech perception-emerges naturally in Sylber, making the embedding space more categorical and sparse than previous speech features and thus supporting the high efficiency of our tokenization. Together, we present a novel SSL approach for representing speech as syllables, with significant potential for efficient speech tokenization and spoken language modeling."", ""authors"": [""Cheol Jun Cho"", ""Nicholas Lee"", ""Akshat Gupta"", ""Dhruv Agarwal"", ""Ethan Chen"", ""Alan W Black"", ""Gopala K. Anumanchipalli""], ""published"": ""2024-10-09T17:59:04Z"", ""updated"": ""2025-03-02T09:16:05Z"", ""categories"": [""cs.CL"", ""cs.SD"", ""eess.AS""], ""pdf_url"": ""https://arxiv.org/pdf/2410.07168v2"", ""landing_url"": ""https://arxiv.org/abs/2410.07168v2"", ""doi"": ""https://doi.org/10.48550/arXiv.2410.07168""}",2410.07168,https://openalex.org/W4403345965
10.21437/interspeech.2024-2366,Low Bitrate High-Quality RVQGAN-based Discrete Speech Tokenizer,"Discrete Audio codecs (or audio tokenizers) have recently regained interest due to the ability of Large Language Models (LLMs) to learn their compressed acoustic representations. Various publicly available trainable discrete tokenizers recently demonstrated impressive results for audio tokenization, yet they mostly require high token rates to gain high-quality reconstruction. In this study, we fine-tuned an open-source general audio RVQGAN model using diverse open-source speech data, considering various recording conditions and quality levels. The resulting wideband (24kHz) speech-only model achieves speech reconstruction, which is nearly indistinguishable from PCM (pulse-code modulation) with a rate of 150-300 tokens per second (1500-3000 bps). The evaluation used comprehensive English speech data encompassing different recording conditions, including studio settings. Speech samples are made publicly available in http://ibm.biz/IS24SpeechRVQ . The model is officially released in https://huggingface.co/ibm/DAC.speech.v1.0",1,,include (junior:5),https://arxiv.org/abs/2410.08325v1,https://arxiv.org/pdf/2410.08325v1,2024,discrete speech tokens,offline clustering,"{""arxiv_id"": ""2410.08325"", ""title"": ""Low Bitrate High-Quality RVQGAN-based Discrete Speech Tokenizer"", ""summary"": ""Discrete Audio codecs (or audio tokenizers) have recently regained interest due to the ability of Large Language Models (LLMs) to learn their compressed acoustic representations. Various publicly available trainable discrete tokenizers recently demonstrated impressive results for audio tokenization, yet they mostly require high token rates to gain high-quality reconstruction. In this study, we fine-tuned an open-source general audio RVQGAN model using diverse open-source speech data, considering various recording conditions and quality levels. The resulting wideband (24kHz) speech-only model achieves speech reconstruction, which is nearly indistinguishable from PCM (pulse-code modulation) with a rate of 150-300 tokens per second (1500-3000 bps). The evaluation used comprehensive English speech data encompassing different recording conditions, including studio settings. Speech samples are made publicly available in http://ibm.biz/IS24SpeechRVQ . The model is officially released in https://huggingface.co/ibm/DAC.speech.v1.0"", ""authors"": [""Slava Shechtman"", ""Avihu Dekel""], ""published"": ""2024-10-10T19:29:05Z"", ""updated"": ""2024-10-10T19:29:05Z"", ""categories"": [""eess.AS""], ""pdf_url"": ""https://arxiv.org/pdf/2410.08325v1"", ""landing_url"": ""https://arxiv.org/abs/2410.08325v1"", ""doi"": ""https://doi.org/10.21437/Interspeech.2024-2366""}",2410.08325,https://openalex.org/W4402112548
10.1109/icassp49660.2025.10890096,Code Drift: Towards Idempotent Neural Audio Codecs,"Neural codecs have demonstrated strong performance in high-fidelity compression of audio signals at low bitrates. The token-based representations produced by these codecs have proven particularly useful for generative modeling. While much research has focused on improvements in compression ratio and perceptual transparency, recent works have largely overlooked another desirable codec property -- idempotence, the stability of compressed outputs under multiple rounds of encoding. We find that state-of-the-art neural codecs exhibit varied degrees of idempotence, with some degrading audio outputs significantly after as few as three encodings. We investigate possible causes of low idempotence and devise a method for improving idempotence through fine-tuning a codec model. We then examine the effect of idempotence on a simple conditional generative modeling task, and find that increased idempotence can be achieved without negatively impacting downstream modeling performance -- potentially extending the usefulness of neural codecs for practical file compression and iterative generative modeling workflows.",1,,include (junior:5),https://arxiv.org/abs/2410.11025v2,https://arxiv.org/pdf/2410.11025v2,2024,discrete speech tokens,neural codec,"{""arxiv_id"": ""2410.11025"", ""title"": ""Code Drift: Towards Idempotent Neural Audio Codecs"", ""summary"": ""Neural codecs have demonstrated strong performance in high-fidelity compression of audio signals at low bitrates. The token-based representations produced by these codecs have proven particularly useful for generative modeling. While much research has focused on improvements in compression ratio and perceptual transparency, recent works have largely overlooked another desirable codec property -- idempotence, the stability of compressed outputs under multiple rounds of encoding. We find that state-of-the-art neural codecs exhibit varied degrees of idempotence, with some degrading audio outputs significantly after as few as three encodings. We investigate possible causes of low idempotence and devise a method for improving idempotence through fine-tuning a codec model. We then examine the effect of idempotence on a simple conditional generative modeling task, and find that increased idempotence can be achieved without negatively impacting downstream modeling performance -- potentially extending the usefulness of neural codecs for practical file compression and iterative generative modeling workflows."", ""authors"": [""Patrick O'Reilly"", ""Prem Seetharaman"", ""Jiaqi Su"", ""Zeyu Jin"", ""Bryan Pardo""], ""published"": ""2024-10-14T19:21:28Z"", ""updated"": ""2025-04-14T23:07:19Z"", ""categories"": [""eess.AS"", ""cs.SD""], ""pdf_url"": ""https://arxiv.org/pdf/2410.11025v2"", ""landing_url"": ""https://arxiv.org/abs/2410.11025v2"", ""doi"": ""https://doi.org/10.1109/ICASSP49660.2025.10890096""}",2410.11025,https://openalex.org/W4408353695
10.48550/arxiv.2410.12359,ERVQ: Enhanced Residual Vector Quantization with Intra-and-Inter-Codebook Optimization for Neural Audio Codecs,"Current neural audio codecs typically use residual vector quantization (RVQ) to discretize speech signals. However, they often experience codebook collapse, which reduces the effective codebook size and leads to suboptimal performance. To address this problem, we introduce ERVQ, Enhanced Residual Vector Quantization, a novel enhancement strategy for the RVQ framework in neural audio codecs. ERVQ mitigates codebook collapse and boosts codec performance through both intra- and inter-codebook optimization. Intra-codebook optimization incorporates an online clustering strategy and a code balancing loss to ensure balanced and efficient codebook utilization. Inter-codebook optimization improves the diversity of quantized features by minimizing the similarity between successive quantizations. Our experiments show that ERVQ significantly enhances audio codec performance across different models, sampling rates, and bitrates, achieving superior quality and generalization capabilities. It also achieves 100% codebook utilization on one of the most advanced neural audio codecs. Further experiments indicate that audio codecs improved by the ERVQ strategy can improve unified speech-and-text large language models (LLMs). Specifically, there is a notable improvement in the naturalness of generated speech in downstream zero-shot text-to-speech tasks. Audio samples are available here.",1,,include (junior:5),https://arxiv.org/abs/2410.12359v2,https://arxiv.org/pdf/2410.12359v2,2024,discrete speech tokens,vector quantization,"{""arxiv_id"": ""2410.12359"", ""title"": ""ERVQ: Enhanced Residual Vector Quantization with Intra-and-Inter-Codebook Optimization for Neural Audio Codecs"", ""summary"": ""Current neural audio codecs typically use residual vector quantization (RVQ) to discretize speech signals. However, they often experience codebook collapse, which reduces the effective codebook size and leads to suboptimal performance. To address this problem, we introduce ERVQ, Enhanced Residual Vector Quantization, a novel enhancement strategy for the RVQ framework in neural audio codecs. ERVQ mitigates codebook collapse and boosts codec performance through both intra- and inter-codebook optimization. Intra-codebook optimization incorporates an online clustering strategy and a code balancing loss to ensure balanced and efficient codebook utilization. Inter-codebook optimization improves the diversity of quantized features by minimizing the similarity between successive quantizations. Our experiments show that ERVQ significantly enhances audio codec performance across different models, sampling rates, and bitrates, achieving superior quality and generalization capabilities. It also achieves 100% codebook utilization on one of the most advanced neural audio codecs. Further experiments indicate that audio codecs improved by the ERVQ strategy can improve unified speech-and-text large language models (LLMs). Specifically, there is a notable improvement in the naturalness of generated speech in downstream zero-shot text-to-speech tasks. Audio samples are available here."", ""authors"": [""Rui-Chen Zheng"", ""Hui-Peng Du"", ""Xiao-Hang Jiang"", ""Yang Ai"", ""Zhen-Hua Ling""], ""published"": ""2024-10-16T08:21:37Z"", ""updated"": ""2025-06-11T08:43:47Z"", ""categories"": [""eess.AS""], ""pdf_url"": ""https://arxiv.org/pdf/2410.12359v2"", ""landing_url"": ""https://arxiv.org/abs/2410.12359v2"", ""doi"": ""https://doi.org/10.48550/arXiv.2410.12359""}",2410.12359,https://openalex.org/W4403578055
10.48550/arxiv.2410.14411,SNAC: Multi-Scale Neural Audio Codec,"Neural audio codecs have recently gained popularity because they can represent audio signals with high fidelity at very low bitrates, making it feasible to use language modeling approaches for audio generation and understanding. Residual Vector Quantization (RVQ) has become the standard technique for neural audio compression using a cascade of VQ codebooks. This paper proposes the Multi-Scale Neural Audio Codec, a simple extension of RVQ where the quantizers can operate at different temporal resolutions. By applying a hierarchy of quantizers at variable frame rates, the codec adapts to the audio structure across multiple timescales. This leads to more efficient compression, as demonstrated by extensive objective and subjective evaluations. The code and model weights are open-sourced at https://github.com/hubertsiuzdak/snac.",1,,include (junior:5),https://arxiv.org/abs/2410.14411v1,https://arxiv.org/pdf/2410.14411v1,2024,discrete speech tokens,variable frame rate,"{""arxiv_id"": ""2410.14411"", ""title"": ""SNAC: Multi-Scale Neural Audio Codec"", ""summary"": ""Neural audio codecs have recently gained popularity because they can represent audio signals with high fidelity at very low bitrates, making it feasible to use language modeling approaches for audio generation and understanding. Residual Vector Quantization (RVQ) has become the standard technique for neural audio compression using a cascade of VQ codebooks. This paper proposes the Multi-Scale Neural Audio Codec, a simple extension of RVQ where the quantizers can operate at different temporal resolutions. By applying a hierarchy of quantizers at variable frame rates, the codec adapts to the audio structure across multiple timescales. This leads to more efficient compression, as demonstrated by extensive objective and subjective evaluations. The code and model weights are open-sourced at https://github.com/hubertsiuzdak/snac."", ""authors"": [""Hubert Siuzdak"", ""Florian Grötschla"", ""Luca A. Lanzendörfer""], ""published"": ""2024-10-18T12:24:05Z"", ""updated"": ""2024-10-18T12:24:05Z"", ""categories"": [""cs.SD"", ""cs.LG"", ""eess.AS""], ""pdf_url"": ""https://arxiv.org/pdf/2410.14411v1"", ""landing_url"": ""https://arxiv.org/abs/2410.14411v1"", ""doi"": ""https://doi.org/10.48550/arXiv.2410.14411""}",2410.14411,https://openalex.org/W4403995790
10.48550/arxiv.2410.15017,DM-Codec: Distilling Multimodal Representations for Speech Tokenization,"Recent advancements in speech-language models have yielded significant improvements in speech tokenization and synthesis. However, effectively mapping the complex, multidimensional attributes of speech into discrete tokens remains challenging. This process demands acoustic, semantic, and contextual information for precise speech representations. Existing speech representations generally fall into two categories: acoustic tokens from audio codecs and semantic tokens from speech self-supervised learning models. Although recent efforts have unified acoustic and semantic tokens for improved performance, they overlook the crucial role of contextual representation in comprehensive speech modeling. Our empirical investigations reveal that the absence of contextual representations results in elevated Word Error Rate (WER) and Word Information Lost (WIL) scores in speech transcriptions. To address these limitations, we propose two novel distillation approaches: (1) a language model (LM)-guided distillation method that incorporates contextual information, and (2) a combined LM and self-supervised speech model (SM)-guided distillation technique that effectively distills multimodal representations (acoustic, semantic, and contextual) into a comprehensive speech tokenizer, termed DM-Codec. The DM-Codec architecture adopts a streamlined encoder-decoder framework with a Residual Vector Quantizer (RVQ) and incorporates the LM and SM during the training process. Experiments show DM-Codec significantly outperforms state-of-the-art speech tokenization models, reducing WER by up to 13.46%, WIL by 9.82%, and improving speech quality by 5.84% and intelligibility by 1.85% on the LibriSpeech benchmark dataset. Code, samples, and checkpoints are available at https://github.com/mubtasimahasan/DM-Codec.",1,,include (junior:5),https://arxiv.org/abs/2410.15017v2,https://arxiv.org/pdf/2410.15017v2,2024,discrete speech tokens,acoustic tokens,"{""arxiv_id"": ""2410.15017"", ""title"": ""DM-Codec: Distilling Multimodal Representations for Speech Tokenization"", ""summary"": ""Recent advancements in speech-language models have yielded significant improvements in speech tokenization and synthesis. However, effectively mapping the complex, multidimensional attributes of speech into discrete tokens remains challenging. This process demands acoustic, semantic, and contextual information for precise speech representations. Existing speech representations generally fall into two categories: acoustic tokens from audio codecs and semantic tokens from speech self-supervised learning models. Although recent efforts have unified acoustic and semantic tokens for improved performance, they overlook the crucial role of contextual representation in comprehensive speech modeling. Our empirical investigations reveal that the absence of contextual representations results in elevated Word Error Rate (WER) and Word Information Lost (WIL) scores in speech transcriptions. To address these limitations, we propose two novel distillation approaches: (1) a language model (LM)-guided distillation method that incorporates contextual information, and (2) a combined LM and self-supervised speech model (SM)-guided distillation technique that effectively distills multimodal representations (acoustic, semantic, and contextual) into a comprehensive speech tokenizer, termed DM-Codec. The DM-Codec architecture adopts a streamlined encoder-decoder framework with a Residual Vector Quantizer (RVQ) and incorporates the LM and SM during the training process. Experiments show DM-Codec significantly outperforms state-of-the-art speech tokenization models, reducing WER by up to 13.46%, WIL by 9.82%, and improving speech quality by 5.84% and intelligibility by 1.85% on the LibriSpeech benchmark dataset. Code, samples, and checkpoints are available at https://github.com/mubtasimahasan/DM-Codec."", ""authors"": [""Md Mubtasim Ahasan"", ""Md Fahim"", ""Tasnim Mohiuddin"", ""A K M Mahbubur Rahman"", ""Aman Chadha"", ""Tariq Iqbal"", ""M Ashraful Amin"", ""Md Mofijul Islam"", ""Amin Ahsan Ali""], ""published"": ""2024-10-19T07:14:14Z"", ""updated"": ""2025-09-29T08:08:40Z"", ""categories"": [""cs.CL"", ""cs.AI"", ""cs.SD"", ""eess.AS""], ""pdf_url"": ""https://arxiv.org/pdf/2410.15017v2"", ""landing_url"": ""https://arxiv.org/abs/2410.15017v2"", ""doi"": ""https://doi.org/10.48550/arXiv.2410.15017""}",2410.15017,https://openalex.org/W4404088256
10.48550/arxiv.2410.15764,LSCodec: Low-Bitrate and Speaker-Decoupled Discrete Speech Codec,"Although discrete speech tokens have exhibited strong potential for language model-based speech generation, their high bitrates and redundant timbre information restrict the development of such models. In this work, we propose LSCodec, a discrete speech codec that has both low bitrate and speaker decoupling ability. LSCodec adopts a multi-stage unsupervised training framework with a speaker perturbation technique. A continuous information bottleneck is first established, followed by vector quantization that produces a discrete speaker-decoupled space. A discrete token vocoder finally refines acoustic details from LSCodec. By reconstruction evaluations, LSCodec demonstrates superior intelligibility and audio quality with only a single codebook and smaller vocabulary size than baselines. Voice conversion and speaker probing experiments prove the excellent speaker disentanglement of LSCodec, and ablation study verifies the effectiveness of the proposed training framework.",1,,include (junior:5),https://arxiv.org/abs/2410.15764v3,https://arxiv.org/pdf/2410.15764v3,2024,discrete speech tokens,offline clustering,"{""arxiv_id"": ""2410.15764"", ""title"": ""LSCodec: Low-Bitrate and Speaker-Decoupled Discrete Speech Codec"", ""summary"": ""Although discrete speech tokens have exhibited strong potential for language model-based speech generation, their high bitrates and redundant timbre information restrict the development of such models. In this work, we propose LSCodec, a discrete speech codec that has both low bitrate and speaker decoupling ability. LSCodec adopts a multi-stage unsupervised training framework with a speaker perturbation technique. A continuous information bottleneck is first established, followed by vector quantization that produces a discrete speaker-decoupled space. A discrete token vocoder finally refines acoustic details from LSCodec. By reconstruction evaluations, LSCodec demonstrates superior intelligibility and audio quality with only a single codebook and smaller vocabulary size than baselines. Voice conversion and speaker probing experiments prove the excellent speaker disentanglement of LSCodec, and ablation study verifies the effectiveness of the proposed training framework."", ""authors"": [""Yiwei Guo"", ""Zhihan Li"", ""Chenpeng Du"", ""Hankun Wang"", ""Xie Chen"", ""Kai Yu""], ""published"": ""2024-10-21T08:23:31Z"", ""updated"": ""2025-05-21T16:46:32Z"", ""categories"": [""eess.AS"", ""cs.AI"", ""cs.SD""], ""pdf_url"": ""https://arxiv.org/pdf/2410.15764v3"", ""landing_url"": ""https://arxiv.org/abs/2410.15764v3"", ""doi"": ""https://doi.org/10.48550/arXiv.2410.15764""}",2410.15764,https://openalex.org/W4404088948
10.48550/arxiv.2410.22448,A Closer Look at Neural Codec Resynthesis: Bridging the Gap between Codec and Waveform Generation,"Neural Audio Codecs, initially designed as a compression technique, have gained more attention recently for speech generation. Codec models represent each audio frame as a sequence of tokens, i.e., discrete embeddings. The discrete and low-frequency nature of neural codecs introduced a new way to generate speech with token-based models. As these tokens encode information at various levels of granularity, from coarse to fine, most existing works focus on how to better generate the coarse tokens. In this paper, we focus on an equally important but often overlooked question: How can we better resynthesize the waveform from coarse tokens? We point out that both the choice of learning target and resynthesis approach have a dramatic impact on the generated audio quality. Specifically, we study two different strategies based on token prediction and regression, and introduce a new method based on Schrödinger Bridge. We examine how different design choices affect machine and human perception.",1,,include (junior:5),https://arxiv.org/abs/2410.22448v1,https://arxiv.org/pdf/2410.22448v1,2024,discrete speech tokens,neural codec,"{""arxiv_id"": ""2410.22448"", ""title"": ""A Closer Look at Neural Codec Resynthesis: Bridging the Gap between Codec and Waveform Generation"", ""summary"": ""Neural Audio Codecs, initially designed as a compression technique, have gained more attention recently for speech generation. Codec models represent each audio frame as a sequence of tokens, i.e., discrete embeddings. The discrete and low-frequency nature of neural codecs introduced a new way to generate speech with token-based models. As these tokens encode information at various levels of granularity, from coarse to fine, most existing works focus on how to better generate the coarse tokens. In this paper, we focus on an equally important but often overlooked question: How can we better resynthesize the waveform from coarse tokens? We point out that both the choice of learning target and resynthesis approach have a dramatic impact on the generated audio quality. Specifically, we study two different strategies based on token prediction and regression, and introduce a new method based on Schrödinger Bridge. We examine how different design choices affect machine and human perception."", ""authors"": [""Alexander H. Liu"", ""Qirui Wang"", ""Yuan Gong"", ""James Glass""], ""published"": ""2024-10-29T18:29:39Z"", ""updated"": ""2024-10-29T18:29:39Z"", ""categories"": [""eess.AS"", ""cs.CL"", ""cs.LG"", ""cs.SD""], ""pdf_url"": ""https://arxiv.org/pdf/2410.22448v1"", ""landing_url"": ""https://arxiv.org/abs/2410.22448v1"", ""doi"": ""https://doi.org/10.48550/arXiv.2410.22448""}",2410.22448,https://openalex.org/W4404342252
10.48550/arxiv.2410.24177,DC-Spin: A Speaker-invariant Speech Tokenizer for Spoken Language Models,"Spoken language models (SLMs) have gained increasing attention with advancements in text-based, decoder-only language models. SLMs process text and speech, enabling simultaneous speech understanding and generation. This paper presents Double-Codebook Speaker-invariant Clustering (DC-Spin), which aims to improve speech tokenization by bridging audio signals and SLM tokens. DC-Spin extracts speaker-invariant tokens rich in phonetic information and resilient to input variations, enhancing zero-shot SLM tasks and speech resynthesis. We propose a chunk-wise approach to enable streamable DC-Spin without retraining and degradation. Comparisons of tokenization methods (self-supervised and neural audio codecs), model scalability, and downstream task proxies show that tokens easily modeled by an n-gram LM or aligned with phonemes offer strong performance, providing insights for designing speech tokenizers for SLMs.",1,,include (junior:5),https://arxiv.org/abs/2410.24177v1,https://arxiv.org/pdf/2410.24177v1,2024,discrete speech tokens,speech tokenization,"{""arxiv_id"": ""2410.24177"", ""title"": ""DC-Spin: A Speaker-invariant Speech Tokenizer for Spoken Language Models"", ""summary"": ""Spoken language models (SLMs) have gained increasing attention with advancements in text-based, decoder-only language models. SLMs process text and speech, enabling simultaneous speech understanding and generation. This paper presents Double-Codebook Speaker-invariant Clustering (DC-Spin), which aims to improve speech tokenization by bridging audio signals and SLM tokens. DC-Spin extracts speaker-invariant tokens rich in phonetic information and resilient to input variations, enhancing zero-shot SLM tasks and speech resynthesis. We propose a chunk-wise approach to enable streamable DC-Spin without retraining and degradation. Comparisons of tokenization methods (self-supervised and neural audio codecs), model scalability, and downstream task proxies show that tokens easily modeled by an n-gram LM or aligned with phonemes offer strong performance, providing insights for designing speech tokenizers for SLMs."", ""authors"": [""Heng-Jui Chang"", ""Hongyu Gong"", ""Changhan Wang"", ""James Glass"", ""Yu-An Chung""], ""published"": ""2024-10-31T17:43:13Z"", ""updated"": ""2024-10-31T17:43:13Z"", ""categories"": [""eess.AS"", ""cs.CL"", ""cs.LG"", ""cs.SD""], ""pdf_url"": ""https://arxiv.org/pdf/2410.24177v1"", ""landing_url"": ""https://arxiv.org/abs/2410.24177v1"", ""doi"": ""https://doi.org/10.48550/arXiv.2410.24177""}",2410.24177,https://openalex.org/W4404348734
10.48550/arxiv.2411.08742,A Comparative Study of Discrete Speech Tokens for Semantic-Related Tasks with Large Language Models,"With the rise of Speech Large Language Models (Speech LLMs), there has been growing interest in discrete speech tokens for their ability to integrate with text-based tokens seamlessly. Compared to most studies that focus on continuous speech features, although discrete-token based LLMs have shown promising results on certain tasks, the performance gap between these two paradigms is rarely explored. In this paper, we present a fair and thorough comparison between discrete and continuous features across a variety of semantic-related tasks using a light-weight LLM (Qwen1.5-0.5B). Our findings reveal that continuous features generally outperform discrete tokens, particularly in tasks requiring fine-grained semantic understanding. Moreover, this study goes beyond surface-level comparison by identifying key factors behind the under-performance of discrete tokens, such as limited token granularity and inefficient information retention. To enhance the performance of discrete tokens, we explore potential aspects based on our analysis. We hope our results can offer new insights into the opportunities for advancing discrete speech tokens in Speech LLMs.",1,,include (junior:4),https://arxiv.org/abs/2411.08742v1,https://arxiv.org/pdf/2411.08742v1,2024,discrete speech tokens,offline clustering,"{""arxiv_id"": ""2411.08742"", ""title"": ""A Comparative Study of Discrete Speech Tokens for Semantic-Related Tasks with Large Language Models"", ""summary"": ""With the rise of Speech Large Language Models (Speech LLMs), there has been growing interest in discrete speech tokens for their ability to integrate with text-based tokens seamlessly. Compared to most studies that focus on continuous speech features, although discrete-token based LLMs have shown promising results on certain tasks, the performance gap between these two paradigms is rarely explored. In this paper, we present a fair and thorough comparison between discrete and continuous features across a variety of semantic-related tasks using a light-weight LLM (Qwen1.5-0.5B). Our findings reveal that continuous features generally outperform discrete tokens, particularly in tasks requiring fine-grained semantic understanding. Moreover, this study goes beyond surface-level comparison by identifying key factors behind the under-performance of discrete tokens, such as limited token granularity and inefficient information retention. To enhance the performance of discrete tokens, we explore potential aspects based on our analysis. We hope our results can offer new insights into the opportunities for advancing discrete speech tokens in Speech LLMs."", ""authors"": [""Dingdong Wang"", ""Mingyu Cui"", ""Dongchao Yang"", ""Xueyuan Chen"", ""Helen Meng""], ""published"": ""2024-11-13T16:20:20Z"", ""updated"": ""2024-11-13T16:20:20Z"", ""categories"": [""cs.CL"", ""cs.SD"", ""eess.AS""], ""pdf_url"": ""https://arxiv.org/pdf/2411.08742v1"", ""landing_url"": ""https://arxiv.org/abs/2411.08742v1"", ""doi"": ""https://doi.org/10.48550/arXiv.2411.08742""}",2411.08742,https://openalex.org/W4404408071
10.48550/arxiv.2411.14100,BEST-STD: Bidirectional Mamba-Enhanced Speech Tokenization for Spoken Term Detection,"Spoken term detection (STD) is often hindered by reliance on frame-level features and the computationally intensive DTW-based template matching, limiting its practicality. To address these challenges, we propose a novel approach that encodes speech into discrete, speaker-agnostic semantic tokens. This facilitates fast retrieval using text-based search algorithms and effectively handles out-of-vocabulary terms. Our approach focuses on generating consistent token sequences across varying utterances of the same term. We also propose a bidirectional state space modeling within the Mamba encoder, trained in a self-supervised learning framework, to learn contextual frame-level features that are further encoded into discrete tokens. Our analysis shows that our speech tokens exhibit greater speaker invariance than those from existing tokenizers, making them more suitable for STD tasks. Empirical evaluation on LibriSpeech and TIMIT databases indicates that our method outperforms existing STD baselines while being more efficient.",1,,include (junior:5),https://arxiv.org/abs/2411.14100v2,https://arxiv.org/pdf/2411.14100v2,2024,discrete speech tokens,semantic tokens,"{""arxiv_id"": ""2411.14100"", ""title"": ""BEST-STD: Bidirectional Mamba-Enhanced Speech Tokenization for Spoken Term Detection"", ""summary"": ""Spoken term detection (STD) is often hindered by reliance on frame-level features and the computationally intensive DTW-based template matching, limiting its practicality. To address these challenges, we propose a novel approach that encodes speech into discrete, speaker-agnostic semantic tokens. This facilitates fast retrieval using text-based search algorithms and effectively handles out-of-vocabulary terms. Our approach focuses on generating consistent token sequences across varying utterances of the same term. We also propose a bidirectional state space modeling within the Mamba encoder, trained in a self-supervised learning framework, to learn contextual frame-level features that are further encoded into discrete tokens. Our analysis shows that our speech tokens exhibit greater speaker invariance than those from existing tokenizers, making them more suitable for STD tasks. Empirical evaluation on LibriSpeech and TIMIT databases indicates that our method outperforms existing STD baselines while being more efficient."", ""authors"": [""Anup Singh"", ""Kris Demuynck"", ""Vipul Arora""], ""published"": ""2024-11-21T13:05:18Z"", ""updated"": ""2024-12-21T19:15:27Z"", ""categories"": [""eess.AS"", ""cs.CL"", ""cs.IR""], ""pdf_url"": ""https://arxiv.org/pdf/2411.14100v2"", ""landing_url"": ""https://arxiv.org/abs/2411.14100v2"", ""doi"": ""https://doi.org/10.48550/arXiv.2411.14100""}",2411.141,https://openalex.org/W4404652703
10.48550/arxiv.2411.17607,Scaling Speech-Text Pre-training with Synthetic Interleaved Data,"Speech language models (SpeechLMs) accept speech input and produce speech output, allowing for more natural human-computer interaction compared to text-based large language models (LLMs). Traditional approaches for developing SpeechLMs are constrained by the limited availability of unsupervised speech data and parallel speech-text data, which are significantly less abundant than text pre-training data, thereby limiting their scalability as LLMs. We propose a novel approach to scaling speech-text pre-training by leveraging large-scale synthetic interleaved data derived from text corpora, eliminating the need for parallel speech-text datasets. Our method efficiently constructs speech-text interleaved data by sampling text spans from existing text corpora and synthesizing corresponding speech spans using a text-to-token model, bypassing the need to generate actual speech. We also employ a supervised speech tokenizer derived from an automatic speech recognition (ASR) model by incorporating a vector-quantized bottleneck into the encoder. This supervised training approach results in discrete speech tokens with strong semantic preservation even at lower frame rates (e.g. 12.5Hz), while still maintaining speech reconstruction quality. Starting from a pre-trained language model and scaling our pre-training to 1 trillion tokens (with 600B synthetic interleaved speech-text data), we achieve state-of-the-art performance in speech language modeling and spoken question answering, improving performance on spoken questions tasks from the previous SOTA of 13% (Moshi) to 31%. We further demonstrate that by fine-tuning the pre-trained model with speech dialogue data, we can develop an end-to-end spoken chatbot that achieves competitive performance comparable to existing baselines in both conversational abilities and speech quality, even operating exclusively in the speech domain.",1,,include (junior:5),https://arxiv.org/abs/2411.17607v2,https://arxiv.org/pdf/2411.17607v2,2024,discrete speech tokens,offline clustering,"{""arxiv_id"": ""2411.17607"", ""title"": ""Scaling Speech-Text Pre-training with Synthetic Interleaved Data"", ""summary"": ""Speech language models (SpeechLMs) accept speech input and produce speech output, allowing for more natural human-computer interaction compared to text-based large language models (LLMs). Traditional approaches for developing SpeechLMs are constrained by the limited availability of unsupervised speech data and parallel speech-text data, which are significantly less abundant than text pre-training data, thereby limiting their scalability as LLMs. We propose a novel approach to scaling speech-text pre-training by leveraging large-scale synthetic interleaved data derived from text corpora, eliminating the need for parallel speech-text datasets. Our method efficiently constructs speech-text interleaved data by sampling text spans from existing text corpora and synthesizing corresponding speech spans using a text-to-token model, bypassing the need to generate actual speech. We also employ a supervised speech tokenizer derived from an automatic speech recognition (ASR) model by incorporating a vector-quantized bottleneck into the encoder. This supervised training approach results in discrete speech tokens with strong semantic preservation even at lower frame rates (e.g. 12.5Hz), while still maintaining speech reconstruction quality. Starting from a pre-trained language model and scaling our pre-training to 1 trillion tokens (with 600B synthetic interleaved speech-text data), we achieve state-of-the-art performance in speech language modeling and spoken question answering, improving performance on spoken questions tasks from the previous SOTA of 13% (Moshi) to 31%. We further demonstrate that by fine-tuning the pre-trained model with speech dialogue data, we can develop an end-to-end spoken chatbot that achieves competitive performance comparable to existing baselines in both conversational abilities and speech quality, even operating exclusively in the speech domain."", ""authors"": [""Aohan Zeng"", ""Zhengxiao Du"", ""Mingdao Liu"", ""Lei Zhang"", ""Shengmin Jiang"", ""Yuxiao Dong"", ""Jie Tang""], ""published"": ""2024-11-26T17:19:09Z"", ""updated"": ""2024-12-02T16:13:24Z"", ""categories"": [""cs.CL"", ""cs.SD"", ""eess.AS""], ""pdf_url"": ""https://arxiv.org/pdf/2411.17607v2"", ""landing_url"": ""https://arxiv.org/abs/2411.17607v2"", ""doi"": ""https://doi.org/10.48550/arXiv.2411.17607""}",2411.17607,https://openalex.org/W4405031987
10.48550/arxiv.2412.10117,CosyVoice 2: Scalable Streaming Speech Synthesis with Large Language Models,"In our previous work, we introduced CosyVoice, a multilingual speech synthesis model based on supervised discrete speech tokens. By employing progressive semantic decoding with two popular generative models, language models (LMs) and Flow Matching, CosyVoice demonstrated high prosody naturalness, content consistency, and speaker similarity in speech in-context learning. Recently, significant progress has been made in multi-modal large language models (LLMs), where the response latency and real-time factor of speech synthesis play a crucial role in the interactive experience. Therefore, in this report, we present an improved streaming speech synthesis model, CosyVoice 2, which incorporates comprehensive and systematic optimizations. Specifically, we introduce finite-scalar quantization to improve the codebook utilization of speech tokens. For the text-speech LM, we streamline the model architecture to allow direct use of a pre-trained LLM as the backbone. In addition, we develop a chunk-aware causal flow matching model to support various synthesis scenarios, enabling both streaming and non-streaming synthesis within a single model. By training on a large-scale multilingual dataset, CosyVoice 2 achieves human-parity naturalness, minimal response latency, and virtually lossless synthesis quality in the streaming mode. We invite readers to listen to the demos at https://funaudiollm.github.io/cosyvoice2.",1,,include (junior:5),https://arxiv.org/abs/2412.10117v3,https://arxiv.org/pdf/2412.10117v3,2024,discrete speech tokens,offline clustering,"{""arxiv_id"": ""2412.10117"", ""title"": ""CosyVoice 2: Scalable Streaming Speech Synthesis with Large Language Models"", ""summary"": ""In our previous work, we introduced CosyVoice, a multilingual speech synthesis model based on supervised discrete speech tokens. By employing progressive semantic decoding with two popular generative models, language models (LMs) and Flow Matching, CosyVoice demonstrated high prosody naturalness, content consistency, and speaker similarity in speech in-context learning. Recently, significant progress has been made in multi-modal large language models (LLMs), where the response latency and real-time factor of speech synthesis play a crucial role in the interactive experience. Therefore, in this report, we present an improved streaming speech synthesis model, CosyVoice 2, which incorporates comprehensive and systematic optimizations. Specifically, we introduce finite-scalar quantization to improve the codebook utilization of speech tokens. For the text-speech LM, we streamline the model architecture to allow direct use of a pre-trained LLM as the backbone. In addition, we develop a chunk-aware causal flow matching model to support various synthesis scenarios, enabling both streaming and non-streaming synthesis within a single model. By training on a large-scale multilingual dataset, CosyVoice 2 achieves human-parity naturalness, minimal response latency, and virtually lossless synthesis quality in the streaming mode. We invite readers to listen to the demos at https://funaudiollm.github.io/cosyvoice2."", ""authors"": [""Zhihao Du"", ""Yuxuan Wang"", ""Qian Chen"", ""Xian Shi"", ""Xiang Lv"", ""Tianyu Zhao"", ""Zhifu Gao"", ""Yexin Yang"", ""Changfeng Gao"", ""Hui Wang"", ""Fan Yu"", ""Huadai Liu"", ""Zhengyan Sheng"", ""Yue Gu"", ""Chong Deng"", ""Wen Wang"", ""Shiliang Zhang"", ""Zhijie Yan"", ""Jingren Zhou""], ""published"": ""2024-12-13T12:59:39Z"", ""updated"": ""2024-12-25T11:54:03Z"", ""categories"": [""cs.SD"", ""cs.AI"", ""cs.LG"", ""eess.AS""], ""pdf_url"": ""https://arxiv.org/pdf/2412.10117v3"", ""landing_url"": ""https://arxiv.org/abs/2412.10117v3"", ""doi"": ""https://doi.org/10.48550/arXiv.2412.10117""}",2412.10117,https://openalex.org/W4405433404
10.48550/arxiv.2412.11449,Whisper-GPT: A Hybrid Representation Audio Large Language Model,"We propose WHISPER-GPT: A generative large language model (LLM) for speech and music that allows us to work with continuous audio representations and discrete tokens simultaneously as part of a single architecture. There has been a huge surge in generative audio, speech, and music models that utilize discrete audio tokens derived from neural compression algorithms, e.g. ENCODEC. However, one of the major drawbacks of this approach is handling the context length. It blows up for high-fidelity generative architecture if one has to account for all the audio contents at various frequencies for the next token prediction. By combining continuous audio representation like the spectrogram and discrete acoustic tokens, we retain the best of both worlds: Have all the information needed from the audio at a specific time instance in a single token, yet allow LLM to predict the future token to allow for sampling and other benefits discrete space provides. We show how our architecture improves the perplexity and negative log-likelihood scores for the next token prediction compared to a token-based LLM for speech and music.",1,,include (junior:5),https://arxiv.org/abs/2412.11449v1,https://arxiv.org/pdf/2412.11449v1,2024,discrete speech tokens,acoustic tokens,"{""arxiv_id"": ""2412.11449"", ""title"": ""Whisper-GPT: A Hybrid Representation Audio Large Language Model"", ""summary"": ""We propose WHISPER-GPT: A generative large language model (LLM) for speech and music that allows us to work with continuous audio representations and discrete tokens simultaneously as part of a single architecture. There has been a huge surge in generative audio, speech, and music models that utilize discrete audio tokens derived from neural compression algorithms, e.g. ENCODEC. However, one of the major drawbacks of this approach is handling the context length. It blows up for high-fidelity generative architecture if one has to account for all the audio contents at various frequencies for the next token prediction. By combining continuous audio representation like the spectrogram and discrete acoustic tokens, we retain the best of both worlds: Have all the information needed from the audio at a specific time instance in a single token, yet allow LLM to predict the future token to allow for sampling and other benefits discrete space provides. We show how our architecture improves the perplexity and negative log-likelihood scores for the next token prediction compared to a token-based LLM for speech and music."", ""authors"": [""Prateek Verma""], ""published"": ""2024-12-16T05:03:48Z"", ""updated"": ""2024-12-16T05:03:48Z"", ""categories"": [""cs.SD"", ""cs.AI"", ""cs.CL"", ""cs.LG"", ""eess.AS""], ""pdf_url"": ""https://arxiv.org/pdf/2412.11449v1"", ""landing_url"": ""https://arxiv.org/abs/2412.11449v1"", ""doi"": ""https://doi.org/10.48550/arXiv.2412.11449""}",2412.11449,https://openalex.org/W4405470171
10.48550/arxiv.2412.15649,SLAM-Omni: Timbre-Controllable Voice Interaction System with Single-Stage Training,"Recent advancements highlight the potential of end-to-end real-time spoken dialogue systems, showcasing their low latency and high quality. In this paper, we introduce SLAM-Omni, a timbre-controllable, end-to-end voice interaction system with single-stage training. SLAM-Omni achieves zero-shot timbre control by modeling spoken language with semantic tokens and decoupling speaker information to a vocoder. By predicting grouped speech semantic tokens at each step, our method significantly reduces the sequence length of audio tokens, accelerating both training and inference. Additionally, we propose historical text prompting to compress dialogue history, facilitating efficient multi-round interactions. Comprehensive evaluations reveal that SLAM-Omni outperforms prior models of similar scale, requiring only 15 hours of training on 4 GPUs with limited data. Notably, it is the first spoken dialogue system to achieve competitive performance with a single-stage training approach, eliminating the need for pre-training on TTS or ASR tasks. Further experiments validate its multilingual and multi-turn dialogue capabilities on larger datasets.",1,,include (junior:5),https://arxiv.org/abs/2412.15649v1,https://arxiv.org/pdf/2412.15649v1,2024,discrete speech tokens,semantic tokens,"{""arxiv_id"": ""2412.15649"", ""title"": ""SLAM-Omni: Timbre-Controllable Voice Interaction System with Single-Stage Training"", ""summary"": ""Recent advancements highlight the potential of end-to-end real-time spoken dialogue systems, showcasing their low latency and high quality. In this paper, we introduce SLAM-Omni, a timbre-controllable, end-to-end voice interaction system with single-stage training. SLAM-Omni achieves zero-shot timbre control by modeling spoken language with semantic tokens and decoupling speaker information to a vocoder. By predicting grouped speech semantic tokens at each step, our method significantly reduces the sequence length of audio tokens, accelerating both training and inference. Additionally, we propose historical text prompting to compress dialogue history, facilitating efficient multi-round interactions. Comprehensive evaluations reveal that SLAM-Omni outperforms prior models of similar scale, requiring only 15 hours of training on 4 GPUs with limited data. Notably, it is the first spoken dialogue system to achieve competitive performance with a single-stage training approach, eliminating the need for pre-training on TTS or ASR tasks. Further experiments validate its multilingual and multi-turn dialogue capabilities on larger datasets."", ""authors"": [""Wenxi Chen"", ""Ziyang Ma"", ""Ruiqi Yan"", ""Yuzhe Liang"", ""Xiquan Li"", ""Ruiyang Xu"", ""Zhikang Niu"", ""Yanqiao Zhu"", ""Yifan Yang"", ""Zhanxun Liu"", ""Kai Yu"", ""Yuxuan Hu"", ""Jinyu Li"", ""Yan Lu"", ""Shujie Liu"", ""Xie Chen""], ""published"": ""2024-12-20T08:05:55Z"", ""updated"": ""2024-12-20T08:05:55Z"", ""categories"": [""eess.AS""], ""pdf_url"": ""https://arxiv.org/pdf/2412.15649v1"", ""landing_url"": ""https://arxiv.org/abs/2412.15649v1"", ""doi"": ""https://doi.org/10.48550/arXiv.2412.15649""}",2412.15649,https://openalex.org/W4405714880
10.48550/arxiv.2412.16102,Interleaved Speech-Text Language Models for Simple Streaming Text-to-Speech Synthesis,"This paper introduces Interleaved Speech-Text Language Model (IST-LM) for zero-shot streaming Text-to-Speech (TTS). Unlike many previous approaches, IST-LM is directly trained on interleaved sequences of text and speech tokens with a fixed ratio, eliminating the need for additional efforts like forced alignment or complex designs. The ratio of text chunk size to speech chunk size is crucial for the performance of IST-LM. To explore this, we conducted a comprehensive series of statistical analyses on the training data and performed correlation analysis with the final performance, uncovering several key factors: 1) the distance between speech tokens and their corresponding text tokens, 2) the number of future text tokens accessible to each speech token, and 3) the frequency of speech tokens precedes their corresponding text tokens. Experimental results demonstrate how to achieve an optimal streaming TTS system with a limited performance gap compared to its non-streaming counterpart. IST-LM is conceptually simple and empirically powerful, enabling streaming TTS with minimal overhead while largely preserving performance, and offering broad potential for integration with real-time text streams from large language models.",1,,include (junior:5),https://arxiv.org/abs/2412.16102v3,https://arxiv.org/pdf/2412.16102v3,2024,discrete speech tokens,speech tokenization,"{""arxiv_id"": ""2412.16102"", ""title"": ""Interleaved Speech-Text Language Models for Simple Streaming Text-to-Speech Synthesis"", ""summary"": ""This paper introduces Interleaved Speech-Text Language Model (IST-LM) for zero-shot streaming Text-to-Speech (TTS). Unlike many previous approaches, IST-LM is directly trained on interleaved sequences of text and speech tokens with a fixed ratio, eliminating the need for additional efforts like forced alignment or complex designs. The ratio of text chunk size to speech chunk size is crucial for the performance of IST-LM. To explore this, we conducted a comprehensive series of statistical analyses on the training data and performed correlation analysis with the final performance, uncovering several key factors: 1) the distance between speech tokens and their corresponding text tokens, 2) the number of future text tokens accessible to each speech token, and 3) the frequency of speech tokens precedes their corresponding text tokens. Experimental results demonstrate how to achieve an optimal streaming TTS system with a limited performance gap compared to its non-streaming counterpart. IST-LM is conceptually simple and empirically powerful, enabling streaming TTS with minimal overhead while largely preserving performance, and offering broad potential for integration with real-time text streams from large language models."", ""authors"": [""Yifan Yang"", ""Shujie Liu"", ""Jinyu Li"", ""Hui Wang"", ""Lingwei Meng"", ""Haiyang Sun"", ""Yuzhe Liang"", ""Ziyang Ma"", ""Yuxuan Hu"", ""Rui Zhao"", ""Jianwei Yu"", ""Yan Lu"", ""Xie Chen""], ""published"": ""2024-12-20T17:43:50Z"", ""updated"": ""2025-08-09T10:01:51Z"", ""categories"": [""eess.AS""], ""pdf_url"": ""https://arxiv.org/pdf/2412.16102v3"", ""landing_url"": ""https://arxiv.org/abs/2412.16102v3"", ""doi"": ""https://doi.org/10.48550/arXiv.2412.16102""}",2412.16102,https://openalex.org/W4405715750
10.48550/arxiv.2412.19248,Causal Speech Enhancement with Predicting Semantics based on Quantized Self-supervised Learning Features,"Real-time speech enhancement (SE) is essential to online speech communication. Causal SE models use only the previous context while predicting future information, such as phoneme continuation, may help performing causal SE. The phonetic information is often represented by quantizing latent features of self-supervised learning (SSL) models. This work is the first to incorporate SSL features with causality into an SE model. The causal SSL features are encoded and combined with spectrogram features using feature-wise linear modulation to estimate a mask for enhancing the noisy input speech. Simultaneously, we quantize the causal SSL features using vector quantization to represent phonetic characteristics as semantic tokens. The model not only encodes SSL features but also predicts the future semantic tokens in multi-task learning (MTL). The experimental results using VoiceBank + DEMAND dataset show that our proposed method achieves 2.88 in PESQ, especially with semantic prediction MTL, in which we confirm that the semantic prediction played an important role in causal SE.",1,,include (junior:5),https://arxiv.org/abs/2412.19248v1,https://arxiv.org/pdf/2412.19248v1,2024,discrete speech tokens,semantic tokens,"{""arxiv_id"": ""2412.19248"", ""title"": ""Causal Speech Enhancement with Predicting Semantics based on Quantized Self-supervised Learning Features"", ""summary"": ""Real-time speech enhancement (SE) is essential to online speech communication. Causal SE models use only the previous context while predicting future information, such as phoneme continuation, may help performing causal SE. The phonetic information is often represented by quantizing latent features of self-supervised learning (SSL) models. This work is the first to incorporate SSL features with causality into an SE model. The causal SSL features are encoded and combined with spectrogram features using feature-wise linear modulation to estimate a mask for enhancing the noisy input speech. Simultaneously, we quantize the causal SSL features using vector quantization to represent phonetic characteristics as semantic tokens. The model not only encodes SSL features but also predicts the future semantic tokens in multi-task learning (MTL). The experimental results using VoiceBank + DEMAND dataset show that our proposed method achieves 2.88 in PESQ, especially with semantic prediction MTL, in which we confirm that the semantic prediction played an important role in causal SE."", ""authors"": [""Emiru Tsunoo"", ""Yuki Saito"", ""Wataru Nakata"", ""Hiroshi Saruwatari""], ""published"": ""2024-12-26T15:08:36Z"", ""updated"": ""2024-12-26T15:08:36Z"", ""categories"": [""eess.AS"", ""cs.SD""], ""pdf_url"": ""https://arxiv.org/pdf/2412.19248v1"", ""landing_url"": ""https://arxiv.org/abs/2412.19248v1"", ""doi"": ""https://doi.org/10.48550/arXiv.2412.19248""}",2412.19248,https://openalex.org/W4405902171
10.48550/arxiv.2501.04379,Phone-purity Guided Discrete Tokens for Dysarthric Speech Recognition,"Discrete tokens extracted provide efficient and domain adaptable speech features. Their application to disordered speech that exhibits articulation imprecision and large mismatch against normal voice remains unexplored. To improve their phonetic discrimination that is weakened during unsupervised K-means or vector quantization of continuous features, this paper proposes novel phone-purity guided (PPG) discrete tokens for dysarthric speech recognition. Phonetic label supervision is used to regularize maximum likelihood and reconstruction error costs used in standard K-means and VAE-VQ based discrete token extraction. Experiments conducted on the UASpeech corpus suggest that the proposed PPG discrete token features extracted from HuBERT consistently outperform hybrid TDNN and End-to-End (E2E) Conformer systems using non-PPG based K-means or VAE-VQ tokens across varying codebook sizes by statistically significant word error rate (WER) reductions up to 0.99\% and 1.77\% absolute (3.21\% and 4.82\% relative) respectively on the UASpeech test set of 16 dysarthric speakers. The lowest WER of 23.25\% was obtained by combining systems using different token features. Consistent improvements on the phone purity metric were also achieved. T-SNE visualization further demonstrates sharper decision boundaries were produced between K-means/VAE-VQ clusters after introducing phone-purity guidance.",1,,include (junior:5),https://arxiv.org/abs/2501.04379v1,https://arxiv.org/pdf/2501.04379v1,2025,dblp_title,Phone-purity Guided Discrete Tokens for Dysarthric Speech Recognition.,"{""arxiv_id"": ""2501.04379"", ""title"": ""Phone-purity Guided Discrete Tokens for Dysarthric Speech Recognition"", ""summary"": ""Discrete tokens extracted provide efficient and domain adaptable speech features. Their application to disordered speech that exhibits articulation imprecision and large mismatch against normal voice remains unexplored. To improve their phonetic discrimination that is weakened during unsupervised K-means or vector quantization of continuous features, this paper proposes novel phone-purity guided (PPG) discrete tokens for dysarthric speech recognition. Phonetic label supervision is used to regularize maximum likelihood and reconstruction error costs used in standard K-means and VAE-VQ based discrete token extraction. Experiments conducted on the UASpeech corpus suggest that the proposed PPG discrete token features extracted from HuBERT consistently outperform hybrid TDNN and End-to-End (E2E) Conformer systems using non-PPG based K-means or VAE-VQ tokens across varying codebook sizes by statistically significant word error rate (WER) reductions up to 0.99\\% and 1.77\\% absolute (3.21\\% and 4.82\\% relative) respectively on the UASpeech test set of 16 dysarthric speakers. The lowest WER of 23.25\\% was obtained by combining systems using different token features. Consistent improvements on the phone purity metric were also achieved. T-SNE visualization further demonstrates sharper decision boundaries were produced between K-means/VAE-VQ clusters after introducing phone-purity guidance."", ""authors"": [""Huimeng Wang"", ""Xurong Xie"", ""Mengzhe Geng"", ""Shujie Hu"", ""Haoning Xu"", ""Youjun Chen"", ""Zhaoqing Li"", ""Jiajun Deng"", ""Xunying Liu""], ""published"": ""2025-01-08T09:45:14Z"", ""updated"": ""2025-01-08T09:45:14Z"", ""categories"": [""cs.SD"", ""eess.AS""], ""pdf_url"": ""https://arxiv.org/pdf/2501.04379v1"", ""landing_url"": ""https://arxiv.org/abs/2501.04379v1"", ""doi"": ""https://doi.org/10.48550/arXiv.2501.04379""}",2501.04379,https://openalex.org/W4406231319
10.48550/arxiv.2501.05787,MARS6: A Small and Robust Hierarchical-Codec Text-to-Speech Model,"Codec-based text-to-speech (TTS) models have shown impressive quality with zero-shot voice cloning abilities. However, they often struggle with more expressive references or complex text inputs. We present MARS6, a robust encoder-decoder transformer for rapid, expressive TTS. MARS6 is built on recent improvements in spoken language modelling. Utilizing a hierarchical setup for its decoder, new speech tokens are processed at a rate of only 12 Hz, enabling efficient modelling of long-form text while retaining reconstruction quality. We combine several recent training and inference techniques to reduce repetitive generation and improve output stability and quality. This enables the 70M-parameter MARS6 to achieve similar performance to models many times larger. We show this in objective and subjective evaluations, comparing TTS output quality and reference speaker cloning ability. Project page: https://camb-ai.github.io/mars6-turbo/",1,,include (junior:4),https://arxiv.org/abs/2501.05787v1,https://arxiv.org/pdf/2501.05787v1,2025,discrete speech tokens,speech tokenization,"{""arxiv_id"": ""2501.05787"", ""title"": ""MARS6: A Small and Robust Hierarchical-Codec Text-to-Speech Model"", ""summary"": ""Codec-based text-to-speech (TTS) models have shown impressive quality with zero-shot voice cloning abilities. However, they often struggle with more expressive references or complex text inputs. We present MARS6, a robust encoder-decoder transformer for rapid, expressive TTS. MARS6 is built on recent improvements in spoken language modelling. Utilizing a hierarchical setup for its decoder, new speech tokens are processed at a rate of only 12 Hz, enabling efficient modelling of long-form text while retaining reconstruction quality. We combine several recent training and inference techniques to reduce repetitive generation and improve output stability and quality. This enables the 70M-parameter MARS6 to achieve similar performance to models many times larger. We show this in objective and subjective evaluations, comparing TTS output quality and reference speaker cloning ability. Project page: https://camb-ai.github.io/mars6-turbo/"", ""authors"": [""Matthew Baas"", ""Pieter Scholtz"", ""Arnav Mehta"", ""Elliott Dyson"", ""Akshat Prakash"", ""Herman Kamper""], ""published"": ""2025-01-10T08:41:42Z"", ""updated"": ""2025-01-10T08:41:42Z"", ""categories"": [""eess.AS"", ""cs.CL""], ""pdf_url"": ""https://arxiv.org/pdf/2501.05787v1"", ""landing_url"": ""https://arxiv.org/abs/2501.05787v1"", ""doi"": ""https://doi.org/10.48550/arXiv.2501.05787""}",2501.05787,https://openalex.org/W4406316876
10.48550/arxiv.2502.02942,GenSE: Generative Speech Enhancement via Language Models using Hierarchical Modeling,"Semantic information refers to the meaning conveyed through words, phrases, and contextual relationships within a given linguistic structure. Humans can leverage semantic information, such as familiar linguistic patterns and contextual cues, to reconstruct incomplete or masked speech signals in noisy environments. However, existing speech enhancement (SE) approaches often overlook the rich semantic information embedded in speech, which is crucial for improving intelligibility, speaker consistency, and overall quality of enhanced speech signals. To enrich the SE model with semantic information, we employ language models as an efficient semantic learner and propose a comprehensive framework tailored for language model-based speech enhancement, called \textit{GenSE}. Specifically, we approach SE as a conditional language modeling task rather than a continuous signal regression problem defined in existing works. This is achieved by tokenizing speech signals into semantic tokens using a pre-trained self-supervised model and into acoustic tokens using a custom-designed single-quantizer neural codec model. To improve the stability of language model predictions, we propose a hierarchical modeling method that decouples the generation of clean semantic tokens and clean acoustic tokens into two distinct stages. Moreover, we introduce a token chain prompting mechanism during the acoustic token generation stage to ensure timbre consistency throughout the speech enhancement process. Experimental results on benchmark datasets demonstrate that our proposed approach outperforms state-of-the-art SE systems in terms of speech quality and generalization capability.",1,,include (junior:5),https://arxiv.org/abs/2502.02942v1,https://arxiv.org/pdf/2502.02942v1,2025,discrete speech tokens,acoustic tokens,"{""arxiv_id"": ""2502.02942"", ""title"": ""GenSE: Generative Speech Enhancement via Language Models using Hierarchical Modeling"", ""summary"": ""Semantic information refers to the meaning conveyed through words, phrases, and contextual relationships within a given linguistic structure. Humans can leverage semantic information, such as familiar linguistic patterns and contextual cues, to reconstruct incomplete or masked speech signals in noisy environments. However, existing speech enhancement (SE) approaches often overlook the rich semantic information embedded in speech, which is crucial for improving intelligibility, speaker consistency, and overall quality of enhanced speech signals. To enrich the SE model with semantic information, we employ language models as an efficient semantic learner and propose a comprehensive framework tailored for language model-based speech enhancement, called \\textit{GenSE}. Specifically, we approach SE as a conditional language modeling task rather than a continuous signal regression problem defined in existing works. This is achieved by tokenizing speech signals into semantic tokens using a pre-trained self-supervised model and into acoustic tokens using a custom-designed single-quantizer neural codec model. To improve the stability of language model predictions, we propose a hierarchical modeling method that decouples the generation of clean semantic tokens and clean acoustic tokens into two distinct stages. Moreover, we introduce a token chain prompting mechanism during the acoustic token generation stage to ensure timbre consistency throughout the speech enhancement process. Experimental results on benchmark datasets demonstrate that our proposed approach outperforms state-of-the-art SE systems in terms of speech quality and generalization capability."", ""authors"": [""Jixun Yao"", ""Hexin Liu"", ""Chen Chen"", ""Yuchen Hu"", ""EngSiong Chng"", ""Lei Xie""], ""published"": ""2025-02-05T07:14:39Z"", ""updated"": ""2025-02-05T07:14:39Z"", ""categories"": [""eess.AS"", ""cs.SD""], ""pdf_url"": ""https://arxiv.org/pdf/2502.02942v1"", ""landing_url"": ""https://arxiv.org/abs/2502.02942v1"", ""doi"": ""https://doi.org/10.48550/arXiv.2502.02942""}",2502.02942,https://openalex.org/W4407212698
10.48550/arxiv.2502.03128,Metis: A Foundation Speech Generation Model with Masked Generative Pre-training,"We introduce Metis, a foundation model for unified speech generation. Unlike previous task-specific or multi-task models, Metis follows a pre-training and fine-tuning paradigm. It is pre-trained on large-scale unlabeled speech data using masked generative modeling and then fine-tuned to adapt to diverse speech generation tasks. Specifically, 1) Metis utilizes two discrete speech representations: SSL tokens derived from speech self-supervised learning (SSL) features, and acoustic tokens directly quantized from waveforms. 2) Metis performs masked generative pre-training on SSL tokens, utilizing 300K hours of diverse speech data, without any additional condition. 3) Through fine-tuning with task-specific conditions, Metis achieves efficient adaptation to various speech generation tasks while supporting multimodal input, even when using limited data and trainable parameters. Experiments demonstrate that Metis can serve as a foundation model for unified speech generation: Metis outperforms state-of-the-art task-specific or multi-task systems across five speech generation tasks, including zero-shot text-to-speech, voice conversion, target speaker extraction, speech enhancement, and lip-to-speech, even with fewer than 20M trainable parameters or 300 times less training data. Audio samples are are available at https://metis-demo.github.io/.",1,,include (junior:5),https://arxiv.org/abs/2502.03128v1,https://arxiv.org/pdf/2502.03128v1,2025,discrete speech tokens,acoustic tokens,"{""arxiv_id"": ""2502.03128"", ""title"": ""Metis: A Foundation Speech Generation Model with Masked Generative Pre-training"", ""summary"": ""We introduce Metis, a foundation model for unified speech generation. Unlike previous task-specific or multi-task models, Metis follows a pre-training and fine-tuning paradigm. It is pre-trained on large-scale unlabeled speech data using masked generative modeling and then fine-tuned to adapt to diverse speech generation tasks. Specifically, 1) Metis utilizes two discrete speech representations: SSL tokens derived from speech self-supervised learning (SSL) features, and acoustic tokens directly quantized from waveforms. 2) Metis performs masked generative pre-training on SSL tokens, utilizing 300K hours of diverse speech data, without any additional condition. 3) Through fine-tuning with task-specific conditions, Metis achieves efficient adaptation to various speech generation tasks while supporting multimodal input, even when using limited data and trainable parameters. Experiments demonstrate that Metis can serve as a foundation model for unified speech generation: Metis outperforms state-of-the-art task-specific or multi-task systems across five speech generation tasks, including zero-shot text-to-speech, voice conversion, target speaker extraction, speech enhancement, and lip-to-speech, even with fewer than 20M trainable parameters or 300 times less training data. Audio samples are are available at https://metis-demo.github.io/."", ""authors"": [""Yuancheng Wang"", ""Jiachen Zheng"", ""Junan Zhang"", ""Xueyao Zhang"", ""Huan Liao"", ""Zhizheng Wu""], ""published"": ""2025-02-05T12:36:21Z"", ""updated"": ""2025-02-05T12:36:21Z"", ""categories"": [""cs.SD"", ""cs.AI"", ""cs.LG"", ""eess.AS"", ""eess.SP""], ""pdf_url"": ""https://arxiv.org/pdf/2502.03128v1"", ""landing_url"": ""https://arxiv.org/abs/2502.03128v1"", ""doi"": ""https://doi.org/10.48550/arXiv.2502.03128""}",2502.03128,https://openalex.org/W4407221739
10.48550/arxiv.2502.04519,GenVC: Self-Supervised Zero-Shot Voice Conversion,"Most current zero-shot voice conversion methods rely on externally supervised components, particularly speaker encoders, for training. To explore alternatives that eliminate this dependency, this paper introduces GenVC, a novel framework that disentangles speaker identity and linguistic content from speech signals in a self-supervised manner. GenVC leverages speech tokenizers and an autoregressive, Transformer-based language model as its backbone for speech generation. This design supports large-scale training while enhancing both source speaker privacy protection and target speaker cloning fidelity. Experimental results demonstrate that GenVC achieves notably higher speaker similarity, with naturalness on par with leading zero-shot approaches. Moreover, due to its autoregressive formulation, GenVC introduces flexibility in temporal alignment, reducing the preservation of source prosody and speaker-specific traits, and making it highly effective for voice anonymization.",1,,include (junior:4),https://arxiv.org/abs/2502.04519v2,https://arxiv.org/pdf/2502.04519v2,2025,discrete speech tokens,speech tokenization,"{""arxiv_id"": ""2502.04519"", ""title"": ""GenVC: Self-Supervised Zero-Shot Voice Conversion"", ""summary"": ""Most current zero-shot voice conversion methods rely on externally supervised components, particularly speaker encoders, for training. To explore alternatives that eliminate this dependency, this paper introduces GenVC, a novel framework that disentangles speaker identity and linguistic content from speech signals in a self-supervised manner. GenVC leverages speech tokenizers and an autoregressive, Transformer-based language model as its backbone for speech generation. This design supports large-scale training while enhancing both source speaker privacy protection and target speaker cloning fidelity. Experimental results demonstrate that GenVC achieves notably higher speaker similarity, with naturalness on par with leading zero-shot approaches. Moreover, due to its autoregressive formulation, GenVC introduces flexibility in temporal alignment, reducing the preservation of source prosody and speaker-specific traits, and making it highly effective for voice anonymization."", ""authors"": [""Zexin Cai"", ""Henry Li Xinyuan"", ""Ashi Garg"", ""Leibny Paola García-Perera"", ""Kevin Duh"", ""Sanjeev Khudanpur"", ""Matthew Wiesner"", ""Nicholas Andrews""], ""published"": ""2025-02-06T21:40:09Z"", ""updated"": ""2025-08-20T17:34:21Z"", ""categories"": [""eess.AS"", ""cs.LG""], ""pdf_url"": ""https://arxiv.org/pdf/2502.04519v2"", ""landing_url"": ""https://arxiv.org/abs/2502.04519v2"", ""doi"": ""https://doi.org/10.48550/arXiv.2502.04519""}",2502.04519,https://openalex.org/W4407308894
10.48550/arxiv.2502.05236,Koel-TTS: Enhancing LLM based Speech Generation with Preference Alignment and Classifier Free Guidance,"While autoregressive speech token generation models produce speech with remarkable variety and naturalness, their inherent lack of controllability often results in issues such as hallucinations and undesired vocalizations that do not conform to conditioning inputs. We introduce Koel-TTS, a suite of enhanced encoder-decoder Transformer TTS models that address these challenges by incorporating preference alignment techniques guided by automatic speech recognition and speaker verification models. Additionally, we incorporate classifier-free guidance to further improve synthesis adherence to the transcript and reference speaker audio. Our experiments demonstrate that these optimizations significantly enhance target speaker similarity, intelligibility, and naturalness of synthesized speech. Notably, Koel-TTS directly maps text and context audio to acoustic tokens, and on the aforementioned metrics, outperforms state-of-the-art TTS models, despite being trained on a significantly smaller dataset. Audio samples and demos are available on our website.",1,,include (junior:5),https://arxiv.org/abs/2502.05236v2,https://arxiv.org/pdf/2502.05236v2,2025,discrete speech tokens,acoustic tokens,"{""arxiv_id"": ""2502.05236"", ""title"": ""Koel-TTS: Enhancing LLM based Speech Generation with Preference Alignment and Classifier Free Guidance"", ""summary"": ""While autoregressive speech token generation models produce speech with remarkable variety and naturalness, their inherent lack of controllability often results in issues such as hallucinations and undesired vocalizations that do not conform to conditioning inputs. We introduce Koel-TTS, a suite of enhanced encoder-decoder Transformer TTS models that address these challenges by incorporating preference alignment techniques guided by automatic speech recognition and speaker verification models. Additionally, we incorporate classifier-free guidance to further improve synthesis adherence to the transcript and reference speaker audio. Our experiments demonstrate that these optimizations significantly enhance target speaker similarity, intelligibility, and naturalness of synthesized speech. Notably, Koel-TTS directly maps text and context audio to acoustic tokens, and on the aforementioned metrics, outperforms state-of-the-art TTS models, despite being trained on a significantly smaller dataset. Audio samples and demos are available on our website."", ""authors"": [""Shehzeen Hussain"", ""Paarth Neekhara"", ""Xuesong Yang"", ""Edresson Casanova"", ""Subhankar Ghosh"", ""Mikyas T. Desta"", ""Roy Fejgin"", ""Rafael Valle"", ""Jason Li""], ""published"": ""2025-02-07T06:47:11Z"", ""updated"": ""2025-07-22T21:32:13Z"", ""categories"": [""cs.SD"", ""cs.AI"", ""cs.LG"", ""eess.AS""], ""pdf_url"": ""https://arxiv.org/pdf/2502.05236v2"", ""landing_url"": ""https://arxiv.org/abs/2502.05236v2"", ""doi"": ""https://doi.org/10.48550/arXiv.2502.05236""}",2502.05236,https://openalex.org/W4407368664
10.48550/arxiv.2502.16897,Balancing Speech Understanding and Generation Using Continual Pre-training for Codec-based Speech LLM,"Recent advances in speech language models (LLMs) have extended textual LLMs to the speech domain, but balancing speech understanding and generation remains challenging, especially with codec-based representations. We propose a continual pre-training (CPT) framework that adapts a textual LLM to handle codec-discretized speech, mitigating modality mismatch and preserving linguistic reasoning. Our unified model supports both understanding and generation, achieving strong results across ASR, TTS, S2T-Trans, and S2S-Trans. Notably, we present the first end-to-end, single-pass S2S-Trans system using only neural codec tokens, without intermediate transcriptions, translations, or semantic tokens. CPT proves essential for cross-modal alignment and task generalization, making it a powerful tool for building robust, unified speech LLMs.",1,,include (junior:5),https://arxiv.org/abs/2502.16897v2,https://arxiv.org/pdf/2502.16897v2,2025,discrete speech tokens,neural codec,"{""arxiv_id"": ""2502.16897"", ""title"": ""Balancing Speech Understanding and Generation Using Continual Pre-training for Codec-based Speech LLM"", ""summary"": ""Recent advances in speech language models (LLMs) have extended textual LLMs to the speech domain, but balancing speech understanding and generation remains challenging, especially with codec-based representations. We propose a continual pre-training (CPT) framework that adapts a textual LLM to handle codec-discretized speech, mitigating modality mismatch and preserving linguistic reasoning. Our unified model supports both understanding and generation, achieving strong results across ASR, TTS, S2T-Trans, and S2S-Trans. Notably, we present the first end-to-end, single-pass S2S-Trans system using only neural codec tokens, without intermediate transcriptions, translations, or semantic tokens. CPT proves essential for cross-modal alignment and task generalization, making it a powerful tool for building robust, unified speech LLMs."", ""authors"": [""Jiatong Shi"", ""Chunlei Zhang"", ""Jinchuan Tian"", ""Junrui Ni"", ""Hao Zhang"", ""Shinji Watanabe"", ""Dong Yu""], ""published"": ""2025-02-24T06:50:40Z"", ""updated"": ""2025-11-27T18:46:39Z"", ""categories"": [""eess.AS""], ""pdf_url"": ""https://arxiv.org/pdf/2502.16897v2"", ""landing_url"": ""https://arxiv.org/abs/2502.16897v2"", ""doi"": ""https://doi.org/10.48550/arXiv.2502.16897""}",2502.16897,https://openalex.org/W4414845989
10.48550/arxiv.2503.01710,Spark-TTS: An Efficient LLM-Based Text-to-Speech Model with Single-Stream Decoupled Speech Tokens,"Recent advancements in large language models (LLMs) have driven significant progress in zero-shot text-to-speech (TTS) synthesis. However, existing foundation models rely on multi-stage processing or complex architectures for predicting multiple codebooks, limiting efficiency and integration flexibility. To overcome these challenges, we introduce Spark-TTS, a novel system powered by BiCodec, a single-stream speech codec that decomposes speech into two complementary token types: low-bitrate semantic tokens for linguistic content and fixed-length global tokens for speaker attributes. This disentangled representation, combined with the Qwen2.5 LLM and a chain-of-thought (CoT) generation approach, enables both coarse-grained control (e.g., gender, speaking style) and fine-grained adjustments (e.g., precise pitch values, speaking rate). To facilitate research in controllable TTS, we introduce VoxBox, a meticulously curated 100,000-hour dataset with comprehensive attribute annotations. Extensive experiments demonstrate that Spark-TTS not only achieves state-of-the-art zero-shot voice cloning but also generates highly customizable voices that surpass the limitations of reference-based synthesis. Source code, pre-trained models, and audio samples are available at https://github.com/SparkAudio/Spark-TTS.",1,,include (junior:5),https://arxiv.org/abs/2503.01710v1,https://arxiv.org/pdf/2503.01710v1,2025,discrete speech tokens,speech tokenization,"{""arxiv_id"": ""2503.01710"", ""title"": ""Spark-TTS: An Efficient LLM-Based Text-to-Speech Model with Single-Stream Decoupled Speech Tokens"", ""summary"": ""Recent advancements in large language models (LLMs) have driven significant progress in zero-shot text-to-speech (TTS) synthesis. However, existing foundation models rely on multi-stage processing or complex architectures for predicting multiple codebooks, limiting efficiency and integration flexibility. To overcome these challenges, we introduce Spark-TTS, a novel system powered by BiCodec, a single-stream speech codec that decomposes speech into two complementary token types: low-bitrate semantic tokens for linguistic content and fixed-length global tokens for speaker attributes. This disentangled representation, combined with the Qwen2.5 LLM and a chain-of-thought (CoT) generation approach, enables both coarse-grained control (e.g., gender, speaking style) and fine-grained adjustments (e.g., precise pitch values, speaking rate). To facilitate research in controllable TTS, we introduce VoxBox, a meticulously curated 100,000-hour dataset with comprehensive attribute annotations. Extensive experiments demonstrate that Spark-TTS not only achieves state-of-the-art zero-shot voice cloning but also generates highly customizable voices that surpass the limitations of reference-based synthesis. Source code, pre-trained models, and audio samples are available at https://github.com/SparkAudio/Spark-TTS."", ""authors"": [""Xinsheng Wang"", ""Mingqi Jiang"", ""Ziyang Ma"", ""Ziyu Zhang"", ""Songxiang Liu"", ""Linqin Li"", ""Zheng Liang"", ""Qixi Zheng"", ""Rui Wang"", ""Xiaoqin Feng"", ""Weizhen Bian"", ""Zhen Ye"", ""Sitong Cheng"", ""Ruibin Yuan"", ""Zhixian Zhao"", ""Xinfa Zhu"", ""Jiahao Pan"", ""Liumeng Xue"", ""Pengcheng Zhu"", ""Yunlin Chen"", ""Zhifei Li"", ""Xie Chen"", ""Lei Xie"", ""Yike Guo"", ""Wei Xue""], ""published"": ""2025-03-03T16:23:10Z"", ""updated"": ""2025-03-03T16:23:10Z"", ""categories"": [""cs.SD"", ""cs.AI"", ""eess.AS""], ""pdf_url"": ""https://arxiv.org/pdf/2503.01710v1"", ""landing_url"": ""https://arxiv.org/abs/2503.01710v1"", ""doi"": ""https://doi.org/10.48550/arXiv.2503.01710""}",2503.0171,https://openalex.org/W4415085369
10.48550/arxiv.2503.03304,On the Relation Between Speech Quality and Quantized Latent Representations of Neural Codecs,"Neural audio signal codecs have attracted significant attention in recent years. In essence, the impressive low bitrate achieved by such encoders is enabled by learning an abstract representation that captures the properties of encoded signals, e.g., speech. In this work, we investigate the relation between the latent representation of the input signal learned by a neural codec and the quality of speech signals. To do so, we introduce Latent-representation-to-Quantization error Ratio (LQR) measures, which quantify the distance from the idealized neural codec's speech signal model for a given speech signal. We compare the proposed metrics to intrusive measures as well as data-driven supervised methods using two subjective speech quality datasets. This analysis shows that the proposed LQR correlates strongly (up to 0.9 Pearson's correlation) with the subjective quality of speech. Despite being a non-intrusive metric, this yields a competitive performance with, or even better than, other pre-trained and intrusive measures. These results show that LQR is a promising basis for more sophisticated speech quality measures.",1,,include (senior:5),https://arxiv.org/abs/2503.03304v1,https://arxiv.org/pdf/2503.03304v1,2025,discrete speech tokens,neural codec,"{""arxiv_id"": ""2503.03304"", ""title"": ""On the Relation Between Speech Quality and Quantized Latent Representations of Neural Codecs"", ""summary"": ""Neural audio signal codecs have attracted significant attention in recent years. In essence, the impressive low bitrate achieved by such encoders is enabled by learning an abstract representation that captures the properties of encoded signals, e.g., speech. In this work, we investigate the relation between the latent representation of the input signal learned by a neural codec and the quality of speech signals. To do so, we introduce Latent-representation-to-Quantization error Ratio (LQR) measures, which quantify the distance from the idealized neural codec's speech signal model for a given speech signal. We compare the proposed metrics to intrusive measures as well as data-driven supervised methods using two subjective speech quality datasets. This analysis shows that the proposed LQR correlates strongly (up to 0.9 Pearson's correlation) with the subjective quality of speech. Despite being a non-intrusive metric, this yields a competitive performance with, or even better than, other pre-trained and intrusive measures. These results show that LQR is a promising basis for more sophisticated speech quality measures."", ""authors"": [""Mhd Modar Halimeh"", ""Matteo Torcoli"", ""Philipp Grundhuber"", ""Emanuël A. P. Habets""], ""published"": ""2025-03-05T09:37:14Z"", ""updated"": ""2025-03-05T09:37:14Z"", ""categories"": [""eess.AS""], ""pdf_url"": ""https://arxiv.org/pdf/2503.03304v1"", ""landing_url"": ""https://arxiv.org/abs/2503.03304v1"", ""doi"": ""https://doi.org/10.48550/arXiv.2503.03304""}",2503.03304,https://openalex.org/W4408345865
10.48550/arxiv.2503.11315,MMS-LLaMA: Efficient LLM-based Audio-Visual Speech Recognition with Minimal Multimodal Speech Tokens,"Audio-Visual Speech Recognition (AVSR) achieves robust speech recognition in noisy environments by combining auditory and visual information. However, recent Large Language Model (LLM) based AVSR systems incur high computational costs due to the high temporal resolution of audio-visual speech processed by LLMs. In this work, we introduce an efficient multimodal speech LLM framework that minimizes token length while preserving essential linguistic content. Our approach employs an early AV-fusion module for streamlined feature integration, an audio-visual speech Q-Former that dynamically allocates tokens based on input duration, and a refined query allocation strategy with a speech rate predictor to adjust token allocation according to speaking speed of each audio sample. Extensive experiments on the LRS3 dataset show that our method achieves state-of-the-art performance with a WER of 0.72% while using only 3.5 tokens per second. Moreover, our approach not only reduces token usage by 86% compared to the previous multimodal speech LLM framework, but also improves computational efficiency by reducing FLOPs by 35.7%.",1,,include (junior:5),https://arxiv.org/abs/2503.11315v2,https://arxiv.org/pdf/2503.11315v2,2025,discrete speech tokens,speech tokenization,"{""arxiv_id"": ""2503.11315"", ""title"": ""MMS-LLaMA: Efficient LLM-based Audio-Visual Speech Recognition with Minimal Multimodal Speech Tokens"", ""summary"": ""Audio-Visual Speech Recognition (AVSR) achieves robust speech recognition in noisy environments by combining auditory and visual information. However, recent Large Language Model (LLM) based AVSR systems incur high computational costs due to the high temporal resolution of audio-visual speech processed by LLMs. In this work, we introduce an efficient multimodal speech LLM framework that minimizes token length while preserving essential linguistic content. Our approach employs an early AV-fusion module for streamlined feature integration, an audio-visual speech Q-Former that dynamically allocates tokens based on input duration, and a refined query allocation strategy with a speech rate predictor to adjust token allocation according to speaking speed of each audio sample. Extensive experiments on the LRS3 dataset show that our method achieves state-of-the-art performance with a WER of 0.72% while using only 3.5 tokens per second. Moreover, our approach not only reduces token usage by 86% compared to the previous multimodal speech LLM framework, but also improves computational efficiency by reducing FLOPs by 35.7%."", ""authors"": [""Jeong Hun Yeo"", ""Hyeongseop Rha"", ""Se Jin Park"", ""Yong Man Ro""], ""published"": ""2025-03-14T11:31:30Z"", ""updated"": ""2025-06-05T05:58:37Z"", ""categories"": [""cs.CV"", ""cs.MM"", ""cs.SD"", ""eess.AS""], ""pdf_url"": ""https://arxiv.org/pdf/2503.11315v2"", ""landing_url"": ""https://arxiv.org/abs/2503.11315v2"", ""doi"": ""https://doi.org/10.48550/arXiv.2503.11315""}",2503.11315,https://openalex.org/W4417285518
10.1109/jstsp.2024.3488557,Universal Speech Token Learning via Low-Bitrate Neural Codec and Pretrained Representations,"Current large speech language models are mainly based on semantic tokens from discretization of self-supervised learned representations and acoustic tokens from a neural codec, following a semantic-modeling and acoustic-synthesis paradigm. However, semantic tokens discard paralinguistic attributes of speakers that is important for natural spoken communication, while prompt-based acoustic synthesis from semantic tokens has limits in recovering paralinguistic details and suffers from robustness issues, especially when there are domain gaps between the prompt and the target. This paper unifies two types of tokens and proposes the UniCodec, a universal speech token learning that encapsulates all semantics of speech, including linguistic and paralinguistic information, into a compact and semantically-disentangled unified token. Such a unified token can not only benefit speech language models in understanding with paralinguistic hints but also help speech generation with high-quality output. A low-bitrate neural codec is leveraged to learn such disentangled discrete representations at global and local scales, with knowledge distilled from self-supervised learned features. Extensive evaluations on multilingual datasets demonstrate its effectiveness in generating natural, expressive and long-term consistent output quality with paralinguistic attributes well preserved in several speech processing tasks.",1,,include (junior:5),https://arxiv.org/abs/2503.12115v2,https://arxiv.org/pdf/2503.12115v2,2025,discrete speech tokens,acoustic tokens,"{""arxiv_id"": ""2503.12115"", ""title"": ""Universal Speech Token Learning via Low-Bitrate Neural Codec and Pretrained Representations"", ""summary"": ""Current large speech language models are mainly based on semantic tokens from discretization of self-supervised learned representations and acoustic tokens from a neural codec, following a semantic-modeling and acoustic-synthesis paradigm. However, semantic tokens discard paralinguistic attributes of speakers that is important for natural spoken communication, while prompt-based acoustic synthesis from semantic tokens has limits in recovering paralinguistic details and suffers from robustness issues, especially when there are domain gaps between the prompt and the target. This paper unifies two types of tokens and proposes the UniCodec, a universal speech token learning that encapsulates all semantics of speech, including linguistic and paralinguistic information, into a compact and semantically-disentangled unified token. Such a unified token can not only benefit speech language models in understanding with paralinguistic hints but also help speech generation with high-quality output. A low-bitrate neural codec is leveraged to learn such disentangled discrete representations at global and local scales, with knowledge distilled from self-supervised learned features. Extensive evaluations on multilingual datasets demonstrate its effectiveness in generating natural, expressive and long-term consistent output quality with paralinguistic attributes well preserved in several speech processing tasks."", ""authors"": [""Xue Jiang"", ""Xiulian Peng"", ""Yuan Zhang"", ""Yan Lu""], ""published"": ""2025-03-15T12:50:43Z"", ""updated"": ""2025-10-15T06:52:30Z"", ""categories"": [""cs.SD"", ""cs.AI"", ""eess.AS""], ""pdf_url"": ""https://arxiv.org/pdf/2503.12115v2"", ""landing_url"": ""https://arxiv.org/abs/2503.12115v2"", ""doi"": ""https://doi.org/10.1109/JSTSP.2024.3488557""}",2503.12115,https://openalex.org/W4403918744
10.48550/arxiv.2503.14928,Shushing! Let's Imagine an Authentic Speech from the Silent Video,"Vision-guided speech generation aims to produce authentic speech from facial appearance or lip motions without relying on auditory signals, offering significant potential for applications such as dubbing in filmmaking and assisting individuals with aphonia. Despite recent progress, existing methods struggle to achieve unified cross-modal alignment across semantics, timbre, and emotional prosody from visual cues, prompting us to propose Consistent Video-to-Speech (CV2S) as an extended task to enhance cross-modal consistency. To tackle emerging challenges, we introduce ImaginTalk, a novel cross-modal diffusion framework that generates faithful speech using only visual input, operating within a discrete space. Specifically, we propose a discrete lip aligner that predicts discrete speech tokens from lip videos to capture semantic information, while an error detector identifies misaligned tokens, which are subsequently refined through masked language modeling with BERT. To further enhance the expressiveness of the generated speech, we develop a style diffusion transformer equipped with a face-style adapter that adaptively customizes identity and prosody dynamics across both the channel and temporal dimensions while ensuring synchronization with lip-aware semantic features. Extensive experiments demonstrate that ImaginTalk can generate high-fidelity speech with more accurate semantic details and greater expressiveness in timbre and emotion compared to state-of-the-art baselines. Demos are shown at our project page: https://imagintalk.github.io.",1,,include (junior:5),https://arxiv.org/abs/2503.14928v1,https://arxiv.org/pdf/2503.14928v1,2025,discrete speech tokens,offline clustering,"{""arxiv_id"": ""2503.14928"", ""title"": ""Shushing! Let's Imagine an Authentic Speech from the Silent Video"", ""summary"": ""Vision-guided speech generation aims to produce authentic speech from facial appearance or lip motions without relying on auditory signals, offering significant potential for applications such as dubbing in filmmaking and assisting individuals with aphonia. Despite recent progress, existing methods struggle to achieve unified cross-modal alignment across semantics, timbre, and emotional prosody from visual cues, prompting us to propose Consistent Video-to-Speech (CV2S) as an extended task to enhance cross-modal consistency. To tackle emerging challenges, we introduce ImaginTalk, a novel cross-modal diffusion framework that generates faithful speech using only visual input, operating within a discrete space. Specifically, we propose a discrete lip aligner that predicts discrete speech tokens from lip videos to capture semantic information, while an error detector identifies misaligned tokens, which are subsequently refined through masked language modeling with BERT. To further enhance the expressiveness of the generated speech, we develop a style diffusion transformer equipped with a face-style adapter that adaptively customizes identity and prosody dynamics across both the channel and temporal dimensions while ensuring synchronization with lip-aware semantic features. Extensive experiments demonstrate that ImaginTalk can generate high-fidelity speech with more accurate semantic details and greater expressiveness in timbre and emotion compared to state-of-the-art baselines. Demos are shown at our project page: https://imagintalk.github.io."", ""authors"": [""Jiaxin Ye"", ""Hongming Shan""], ""published"": ""2025-03-19T06:28:17Z"", ""updated"": ""2025-03-19T06:28:17Z"", ""categories"": [""cs.CV"", ""cs.AI"", ""cs.SD"", ""eess.AS""], ""pdf_url"": ""https://arxiv.org/pdf/2503.14928v1"", ""landing_url"": ""https://arxiv.org/abs/2503.14928v1"", ""doi"": ""https://doi.org/10.48550/arXiv.2503.14928""}",2503.14928,https://openalex.org/W4414902300
10.48550/arxiv.2503.20499,FireRedTTS-1S: An Upgraded Streamable Foundation Text-to-Speech System,"In this work, we upgrade FireRedTTS to a new version, FireRedTTS-1S, a high-quality streaming foundation text-to-speech system. FireRedTTS-1S achieves streaming speech generation via two steps: text-to-semantic decoding and semantic-to-acoustic decoding. In text-to-semantic decoding, a semantic-aware speech tokenizer converts the speech signal into semantic tokens, which can be synthesized from the text via a language model in an auto-regressive manner. Meanwhile, the semantic-to-acoustic decoding module simultaneously translates generated semantic tokens into the speech signal in a streaming way. We implement two approaches to achieve this module: 1) a chunk-wise streamable flow-matching approach, and 2) a multi-stream language model-based approach. They both present high-quality and streamable speech generation but differ in real-time factor (RTF) and latency. Specifically, flow-matching decoding can generate speech by chunks, presenting a lower RTF of 0.1 but a higher latency of 300ms. Instead, the multi-stream language model generates speech by frames in an autoregressive manner, presenting a higher RTF of 0.3 but a low latency of 150ms. In experiments on zero-shot voice cloning, the objective results validate FireRedTTS-1S as a high-quality foundation model with comparable intelligibility and speaker similarity over industrial baseline systems. Furthermore, the subjective score of FireRedTTS-1S highlights its impressive synthesis performance, achieving comparable quality to the ground-truth recordings. These results validate FireRedTTS-1S as a high-quality streaming foundation TTS system.",1,,include (junior:5),https://arxiv.org/abs/2503.20499v3,https://arxiv.org/pdf/2503.20499v3,2025,discrete speech tokens,semantic tokens,"{""arxiv_id"": ""2503.20499"", ""title"": ""FireRedTTS-1S: An Upgraded Streamable Foundation Text-to-Speech System"", ""summary"": ""In this work, we upgrade FireRedTTS to a new version, FireRedTTS-1S, a high-quality streaming foundation text-to-speech system. FireRedTTS-1S achieves streaming speech generation via two steps: text-to-semantic decoding and semantic-to-acoustic decoding. In text-to-semantic decoding, a semantic-aware speech tokenizer converts the speech signal into semantic tokens, which can be synthesized from the text via a language model in an auto-regressive manner. Meanwhile, the semantic-to-acoustic decoding module simultaneously translates generated semantic tokens into the speech signal in a streaming way. We implement two approaches to achieve this module: 1) a chunk-wise streamable flow-matching approach, and 2) a multi-stream language model-based approach. They both present high-quality and streamable speech generation but differ in real-time factor (RTF) and latency. Specifically, flow-matching decoding can generate speech by chunks, presenting a lower RTF of 0.1 but a higher latency of 300ms. Instead, the multi-stream language model generates speech by frames in an autoregressive manner, presenting a higher RTF of 0.3 but a low latency of 150ms. In experiments on zero-shot voice cloning, the objective results validate FireRedTTS-1S as a high-quality foundation model with comparable intelligibility and speaker similarity over industrial baseline systems. Furthermore, the subjective score of FireRedTTS-1S highlights its impressive synthesis performance, achieving comparable quality to the ground-truth recordings. These results validate FireRedTTS-1S as a high-quality streaming foundation TTS system."", ""authors"": [""Hao-Han Guo"", ""Yao Hu"", ""Fei-Yu Shen"", ""Xu Tang"", ""Yi-Chen Wu"", ""Feng-Long Xie"", ""Kun Xie""], ""published"": ""2025-03-26T12:39:06Z"", ""updated"": ""2025-05-26T11:34:20Z"", ""categories"": [""cs.SD"", ""eess.AS""], ""pdf_url"": ""https://arxiv.org/pdf/2503.20499v3"", ""landing_url"": ""https://arxiv.org/abs/2503.20499v3"", ""doi"": ""https://doi.org/10.48550/arXiv.2503.20499""}",2503.20499,https://openalex.org/W4416417394
10.48550/arxiv.2504.07053,TASTE: Text-Aligned Speech Tokenization and Embedding for Spoken Language Modeling,"Recent efforts target spoken language models (SLMs) that not only listen but also speak for more natural human-LLM interaction. Joint speech-text modeling is a promising direction to achieve this. However, the effectiveness of recent speech tokens for joint modeling remains underexplored. To address this, we introduce Text-Aligned Speech Tokenization and Embedding (TASTE), a method that directly addresses the modality gap by aligning speech token with the corresponding text transcription during the tokenization stage. We propose a method that can achieve this through a attention-based aggregation mechanism and with speech reconstruction as the training objective. We conduct extensive experiments and show that TASTE can preserve essential paralinguistic information while dramatically reducing the token sequence length. With TASTE, we perform straightforward joint spoken language modeling by using Low-Rank Adaptation on the pre-trained text LLM. Experimental results show that TASTE-based SLMs perform comparable to previous work on SALMON and StoryCloze; while significantly outperform other pre-trained SLMs on speech continuation across subjective and objective evaluations. To our knowledge, TASTE is the first end-to-end approach that utilizes a reconstruction objective to automatically learn a text-aligned speech tokenization and embedding suitable for spoken language modeling. Our demo, code, and model are available at https://mtkresearch.github.io/TASTE-SpokenLM.github.io.",1,,include (junior:5),https://arxiv.org/abs/2504.07053v2,https://arxiv.org/pdf/2504.07053v2,2025,discrete speech tokens,speech tokenization,"{""arxiv_id"": ""2504.07053"", ""title"": ""TASTE: Text-Aligned Speech Tokenization and Embedding for Spoken Language Modeling"", ""summary"": ""Recent efforts target spoken language models (SLMs) that not only listen but also speak for more natural human-LLM interaction. Joint speech-text modeling is a promising direction to achieve this. However, the effectiveness of recent speech tokens for joint modeling remains underexplored. To address this, we introduce Text-Aligned Speech Tokenization and Embedding (TASTE), a method that directly addresses the modality gap by aligning speech token with the corresponding text transcription during the tokenization stage. We propose a method that can achieve this through a attention-based aggregation mechanism and with speech reconstruction as the training objective. We conduct extensive experiments and show that TASTE can preserve essential paralinguistic information while dramatically reducing the token sequence length. With TASTE, we perform straightforward joint spoken language modeling by using Low-Rank Adaptation on the pre-trained text LLM. Experimental results show that TASTE-based SLMs perform comparable to previous work on SALMON and StoryCloze; while significantly outperform other pre-trained SLMs on speech continuation across subjective and objective evaluations. To our knowledge, TASTE is the first end-to-end approach that utilizes a reconstruction objective to automatically learn a text-aligned speech tokenization and embedding suitable for spoken language modeling. Our demo, code, and model are available at https://mtkresearch.github.io/TASTE-SpokenLM.github.io."", ""authors"": [""Liang-Hsuan Tseng"", ""Yi-Chang Chen"", ""Kuan-Yi Lee"", ""Da-Shan Shiu"", ""Hung-yi Lee""], ""published"": ""2025-04-09T17:14:33Z"", ""updated"": ""2025-05-22T14:49:03Z"", ""categories"": [""cs.CL"", ""cs.SD"", ""eess.AS""], ""pdf_url"": ""https://arxiv.org/pdf/2504.07053v2"", ""landing_url"": ""https://arxiv.org/abs/2504.07053v2"", ""doi"": ""https://doi.org/10.48550/arXiv.2504.07053""}",2504.07053,https://openalex.org/W4417248906
10.48550/arxiv.2504.10352,Pseudo-Autoregressive Neural Codec Language Models for Efficient Zero-Shot Text-to-Speech Synthesis,"Recent zero-shot text-to-speech (TTS) systems face a common dilemma: autoregressive (AR) models suffer from slow generation and lack duration controllability, while non-autoregressive (NAR) models lack temporal modeling and typically require complex designs. In this paper, we introduce a novel pseudo-autoregressive (PAR) codec language modeling approach that unifies AR and NAR modeling. Combining explicit temporal modeling from AR with parallel generation from NAR, PAR generates dynamic-length spans at fixed time steps. Building on PAR, we propose PALLE, a two-stage TTS system that leverages PAR for initial generation followed by NAR refinement. In the first stage, PAR progressively generates speech tokens along the time dimension, with each step predicting all positions in parallel but only retaining the left-most span. In the second stage, low-confidence tokens are iteratively refined in parallel, leveraging the global contextual information. Experiments demonstrate that PALLE, trained on LibriTTS, outperforms state-of-the-art systems trained on large-scale data, including F5-TTS, E2-TTS, and MaskGCT, on the LibriSpeech test-clean set in terms of speech quality, speaker similarity, and intelligibility, while achieving up to ten times faster inference speed. Audio samples are available at https://microsoft.com/research/project/vall-e-x/palle.",1,,include (junior:5),https://arxiv.org/abs/2504.10352v3,https://arxiv.org/pdf/2504.10352v3,2025,discrete speech tokens,neural codec,"{""arxiv_id"": ""2504.10352"", ""title"": ""Pseudo-Autoregressive Neural Codec Language Models for Efficient Zero-Shot Text-to-Speech Synthesis"", ""summary"": ""Recent zero-shot text-to-speech (TTS) systems face a common dilemma: autoregressive (AR) models suffer from slow generation and lack duration controllability, while non-autoregressive (NAR) models lack temporal modeling and typically require complex designs. In this paper, we introduce a novel pseudo-autoregressive (PAR) codec language modeling approach that unifies AR and NAR modeling. Combining explicit temporal modeling from AR with parallel generation from NAR, PAR generates dynamic-length spans at fixed time steps. Building on PAR, we propose PALLE, a two-stage TTS system that leverages PAR for initial generation followed by NAR refinement. In the first stage, PAR progressively generates speech tokens along the time dimension, with each step predicting all positions in parallel but only retaining the left-most span. In the second stage, low-confidence tokens are iteratively refined in parallel, leveraging the global contextual information. Experiments demonstrate that PALLE, trained on LibriTTS, outperforms state-of-the-art systems trained on large-scale data, including F5-TTS, E2-TTS, and MaskGCT, on the LibriSpeech test-clean set in terms of speech quality, speaker similarity, and intelligibility, while achieving up to ten times faster inference speed. Audio samples are available at https://microsoft.com/research/project/vall-e-x/palle."", ""authors"": [""Yifan Yang"", ""Shujie Liu"", ""Jinyu Li"", ""Yuxuan Hu"", ""Haibin Wu"", ""Hui Wang"", ""Jianwei Yu"", ""Lingwei Meng"", ""Haiyang Sun"", ""Yanqing Liu"", ""Yan Lu"", ""Kai Yu"", ""Xie Chen""], ""published"": ""2025-04-14T16:03:21Z"", ""updated"": ""2025-08-05T15:33:39Z"", ""categories"": [""eess.AS"", ""cs.CL""], ""pdf_url"": ""https://arxiv.org/pdf/2504.10352v3"", ""landing_url"": ""https://arxiv.org/abs/2504.10352v3"", ""doi"": ""https://doi.org/10.48550/arXiv.2504.10352""}",2504.10352,https://openalex.org/W4415160782
10.48550/arxiv.2504.12339,GOAT-TTS: Expressive and Realistic Speech Generation via A Dual-Branch LLM,"While large language models (LLMs) have revolutionized text-to-speech (TTS) synthesis through discrete tokenization paradigms, current architectures exhibit fundamental tensions between three critical dimensions: 1) irreversible loss of acoustic characteristics caused by quantization of speech prompts; 2) stringent dependence on precisely aligned prompt speech-text pairs that limit real-world deployment; and 3) catastrophic forgetting of the LLM's native text comprehension during optimization for speech token generation. To address these challenges, we propose an LLM-based text-to-speech Generation approach Optimized via a novel dual-branch ArchiTecture (GOAT-TTS). Our framework introduces two key innovations: (1) The modality-alignment branch combines a speech encoder and projector to capture continuous acoustic embeddings, enabling bidirectional correlation between paralinguistic features (language, timbre, emotion) and semantic text representations without transcript dependency; (2) The speech-generation branch employs modular fine-tuning on top-k layers of an LLM for speech token prediction while freezing the bottom-n layers to preserve foundational linguistic knowledge. Moreover, multi-token prediction is introduced to support real-time streaming TTS synthesis. Experimental results demonstrate that our GOAT-TTS achieves performance comparable to state-of-the-art TTS models while validating the efficacy of synthesized dialect speech data.",1,,include (junior:4),https://arxiv.org/abs/2504.12339v2,https://arxiv.org/pdf/2504.12339v2,2025,discrete speech tokens,speech tokenization,"{""arxiv_id"": ""2504.12339"", ""title"": ""GOAT-TTS: Expressive and Realistic Speech Generation via A Dual-Branch LLM"", ""summary"": ""While large language models (LLMs) have revolutionized text-to-speech (TTS) synthesis through discrete tokenization paradigms, current architectures exhibit fundamental tensions between three critical dimensions: 1) irreversible loss of acoustic characteristics caused by quantization of speech prompts; 2) stringent dependence on precisely aligned prompt speech-text pairs that limit real-world deployment; and 3) catastrophic forgetting of the LLM's native text comprehension during optimization for speech token generation. To address these challenges, we propose an LLM-based text-to-speech Generation approach Optimized via a novel dual-branch ArchiTecture (GOAT-TTS). Our framework introduces two key innovations: (1) The modality-alignment branch combines a speech encoder and projector to capture continuous acoustic embeddings, enabling bidirectional correlation between paralinguistic features (language, timbre, emotion) and semantic text representations without transcript dependency; (2) The speech-generation branch employs modular fine-tuning on top-k layers of an LLM for speech token prediction while freezing the bottom-n layers to preserve foundational linguistic knowledge. Moreover, multi-token prediction is introduced to support real-time streaming TTS synthesis. Experimental results demonstrate that our GOAT-TTS achieves performance comparable to state-of-the-art TTS models while validating the efficacy of synthesized dialect speech data."", ""authors"": [""Yaodong Song"", ""Hongjie Chen"", ""Jie Lian"", ""Yuxin Zhang"", ""Guangmin Xia"", ""Zehan Li"", ""Genliang Zhao"", ""Jian Kang"", ""Jie Li"", ""Yongxiang Li"", ""Xuelong Li""], ""published"": ""2025-04-15T01:44:56Z"", ""updated"": ""2025-05-28T14:24:12Z"", ""categories"": [""cs.CL"", ""cs.SD"", ""eess.AS""], ""pdf_url"": ""https://arxiv.org/pdf/2504.12339v2"", ""landing_url"": ""https://arxiv.org/abs/2504.12339v2"", ""doi"": ""https://doi.org/10.48550/arXiv.2504.12339""}",2504.12339,https://openalex.org/W4416045834
10.48550/arxiv.2504.15509,SimulS2S-LLM: Unlocking Simultaneous Inference of Speech LLMs for Speech-to-Speech Translation,"Simultaneous speech translation (SST) outputs translations in parallel with streaming speech input, balancing translation quality and latency. While large language models (LLMs) have been extended to handle the speech modality, streaming remains challenging as speech is prepended as a prompt for the entire generation process. To unlock LLM streaming capability, this paper proposes SimulS2S-LLM, which trains speech LLMs offline and employs a test-time policy to guide simultaneous inference. SimulS2S-LLM alleviates the mismatch between training and inference by extracting boundary-aware speech prompts that allows it to be better matched with text input data. SimulS2S-LLM achieves simultaneous speech-to-speech translation (Simul-S2ST) by predicting discrete output speech tokens and then synthesising output speech using a pre-trained vocoder. An incremental beam search is designed to expand the search space of speech token prediction without increasing latency. Experiments on the CVSS speech data show that SimulS2S-LLM offers a better translation quality-latency trade-off than existing methods that use the same training data, such as improving ASR-BLEU scores by 3 points at similar latency.",1,,include (junior:5),https://arxiv.org/abs/2504.15509v1,https://arxiv.org/pdf/2504.15509v1,2025,discrete speech tokens,speech tokenization,"{""arxiv_id"": ""2504.15509"", ""title"": ""SimulS2S-LLM: Unlocking Simultaneous Inference of Speech LLMs for Speech-to-Speech Translation"", ""summary"": ""Simultaneous speech translation (SST) outputs translations in parallel with streaming speech input, balancing translation quality and latency. While large language models (LLMs) have been extended to handle the speech modality, streaming remains challenging as speech is prepended as a prompt for the entire generation process. To unlock LLM streaming capability, this paper proposes SimulS2S-LLM, which trains speech LLMs offline and employs a test-time policy to guide simultaneous inference. SimulS2S-LLM alleviates the mismatch between training and inference by extracting boundary-aware speech prompts that allows it to be better matched with text input data. SimulS2S-LLM achieves simultaneous speech-to-speech translation (Simul-S2ST) by predicting discrete output speech tokens and then synthesising output speech using a pre-trained vocoder. An incremental beam search is designed to expand the search space of speech token prediction without increasing latency. Experiments on the CVSS speech data show that SimulS2S-LLM offers a better translation quality-latency trade-off than existing methods that use the same training data, such as improving ASR-BLEU scores by 3 points at similar latency."", ""authors"": [""Keqi Deng"", ""Wenxi Chen"", ""Xie Chen"", ""Philip C. Woodland""], ""published"": ""2025-04-22T01:05:32Z"", ""updated"": ""2025-04-22T01:05:32Z"", ""categories"": [""cs.CL"", ""cs.SD"", ""eess.AS""], ""pdf_url"": ""https://arxiv.org/pdf/2504.15509v1"", ""landing_url"": ""https://arxiv.org/abs/2504.15509v1"", ""doi"": ""https://doi.org/10.48550/arXiv.2504.15509""}",2504.15509,https://openalex.org/W4414632048
10.48550/arxiv.2505.13830,Improving Noise Robustness of LLM-based Zero-shot TTS via Discrete Acoustic Token Denoising,"Large language model (LLM) based zero-shot text-to-speech (TTS) methods tend to preserve the acoustic environment of the audio prompt, leading to degradation in synthesized speech quality when the audio prompt contains noise. In this paper, we propose a novel neural codec-based speech denoiser and integrate it with the advanced LLM-based TTS model, LauraTTS, to achieve noise-robust zero-shot TTS. The proposed codec denoiser consists of an audio codec, a token denoiser, and an embedding refiner. The token denoiser predicts the first two groups of clean acoustic tokens from the noisy ones, which can serve as the acoustic prompt for LauraTTS to synthesize high-quality personalized speech or be converted to clean speech waveforms through the embedding refiner and codec decoder. Experimental results show that our proposed codec denoiser outperforms state-of-the-art speech enhancement (SE) methods, and the proposed noise-robust LauraTTS surpasses the approach using additional SE models.",1,,include (junior:5),https://arxiv.org/abs/2505.13830v2,https://arxiv.org/pdf/2505.13830v2,2025,discrete speech tokens,acoustic tokens,"{""arxiv_id"": ""2505.13830"", ""title"": ""Improving Noise Robustness of LLM-based Zero-shot TTS via Discrete Acoustic Token Denoising"", ""summary"": ""Large language model (LLM) based zero-shot text-to-speech (TTS) methods tend to preserve the acoustic environment of the audio prompt, leading to degradation in synthesized speech quality when the audio prompt contains noise. In this paper, we propose a novel neural codec-based speech denoiser and integrate it with the advanced LLM-based TTS model, LauraTTS, to achieve noise-robust zero-shot TTS. The proposed codec denoiser consists of an audio codec, a token denoiser, and an embedding refiner. The token denoiser predicts the first two groups of clean acoustic tokens from the noisy ones, which can serve as the acoustic prompt for LauraTTS to synthesize high-quality personalized speech or be converted to clean speech waveforms through the embedding refiner and codec decoder. Experimental results show that our proposed codec denoiser outperforms state-of-the-art speech enhancement (SE) methods, and the proposed noise-robust LauraTTS surpasses the approach using additional SE models."", ""authors"": [""Ye-Xin Lu"", ""Hui-Peng Du"", ""Fei Liu"", ""Yang Ai"", ""Zhen-Hua Ling""], ""published"": ""2025-05-20T02:18:45Z"", ""updated"": ""2025-05-22T04:41:35Z"", ""categories"": [""eess.AS"", ""cs.SD""], ""pdf_url"": ""https://arxiv.org/pdf/2505.13830v2"", ""landing_url"": ""https://arxiv.org/abs/2505.13830v2"", ""doi"": ""https://doi.org/10.48550/arXiv.2505.13830""}",2505.1383,https://openalex.org/W4416445296
10.48550/arxiv.2505.14470,PAST: Phonetic-Acoustic Speech Tokenizer,"We present PAST, a novel end-to-end framework that jointly models phonetic information alongside signal reconstruction, eliminating the need for external pretrained models. Unlike previous approaches that rely on pretrained self-supervised models, PAST employs supervised phonetic data, directly integrating domain knowledge into the tokenization process via auxiliary tasks. Additionally, we introduce a streamable, causal variant of PAST, enabling real-time speech applications. Results demonstrate that PAST surpasses existing evaluated baseline tokenizers across common evaluation metrics, including phonetic representation and speech reconstruction. Notably, PAST also achieves superior performance when serving as a speech representation for speech language models, further highlighting its effectiveness as a foundation for spoken language generation. To foster further research, we release the full implementation. For code, model checkpoints, and samples see: https://pages.cs.huji.ac.il/adiyoss-lab/PAST",1,,include (junior:5),https://arxiv.org/abs/2505.14470v2,https://arxiv.org/pdf/2505.14470v2,2025,discrete speech tokens,speech tokenization,"{""arxiv_id"": ""2505.14470"", ""title"": ""PAST: Phonetic-Acoustic Speech Tokenizer"", ""summary"": ""We present PAST, a novel end-to-end framework that jointly models phonetic information alongside signal reconstruction, eliminating the need for external pretrained models. Unlike previous approaches that rely on pretrained self-supervised models, PAST employs supervised phonetic data, directly integrating domain knowledge into the tokenization process via auxiliary tasks. Additionally, we introduce a streamable, causal variant of PAST, enabling real-time speech applications. Results demonstrate that PAST surpasses existing evaluated baseline tokenizers across common evaluation metrics, including phonetic representation and speech reconstruction. Notably, PAST also achieves superior performance when serving as a speech representation for speech language models, further highlighting its effectiveness as a foundation for spoken language generation. To foster further research, we release the full implementation. For code, model checkpoints, and samples see: https://pages.cs.huji.ac.il/adiyoss-lab/PAST"", ""authors"": [""Nadav Har-Tuv"", ""Or Tal"", ""Yossi Adi""], ""published"": ""2025-05-20T15:05:14Z"", ""updated"": ""2025-06-04T08:23:18Z"", ""categories"": [""cs.SD"", ""cs.CL"", ""cs.LG"", ""eess.AS""], ""pdf_url"": ""https://arxiv.org/pdf/2505.14470v2"", ""landing_url"": ""https://arxiv.org/abs/2505.14470v2"", ""doi"": ""https://doi.org/10.48550/arXiv.2505.14470""}",2505.1447,https://openalex.org/W4417298838
10.48550/arxiv.2505.14989,Discrete Audio Representations for Automated Audio Captioning,"Discrete audio representations, termed audio tokens, are broadly categorized into semantic and acoustic tokens, typically generated through unsupervised tokenization of continuous audio representations. However, their applicability to automated audio captioning (AAC) remains underexplored. This paper systematically investigates the viability of audio token-driven models for AAC through comparative analyses of various tokenization methods. Our findings reveal that audio tokenization leads to performance degradation in AAC models compared to those that directly utilize continuous audio representations. To address this issue, we introduce a supervised audio tokenizer trained with an audio tagging objective. Unlike unsupervised tokenizers, which lack explicit semantic understanding, the proposed tokenizer effectively captures audio event information. Experiments conducted on the Clotho dataset demonstrate that the proposed audio tokens outperform conventional audio tokens in the AAC task.",1,,include (junior:5),https://arxiv.org/abs/2505.14989v1,https://arxiv.org/pdf/2505.14989v1,2025,discrete speech tokens,acoustic tokens,"{""arxiv_id"": ""2505.14989"", ""title"": ""Discrete Audio Representations for Automated Audio Captioning"", ""summary"": ""Discrete audio representations, termed audio tokens, are broadly categorized into semantic and acoustic tokens, typically generated through unsupervised tokenization of continuous audio representations. However, their applicability to automated audio captioning (AAC) remains underexplored. This paper systematically investigates the viability of audio token-driven models for AAC through comparative analyses of various tokenization methods. Our findings reveal that audio tokenization leads to performance degradation in AAC models compared to those that directly utilize continuous audio representations. To address this issue, we introduce a supervised audio tokenizer trained with an audio tagging objective. Unlike unsupervised tokenizers, which lack explicit semantic understanding, the proposed tokenizer effectively captures audio event information. Experiments conducted on the Clotho dataset demonstrate that the proposed audio tokens outperform conventional audio tokens in the AAC task."", ""authors"": [""Jingguang Tian"", ""Haoqin Sun"", ""Xinhui Hu"", ""Xinkang Xu""], ""published"": ""2025-05-21T00:27:38Z"", ""updated"": ""2025-05-21T00:27:38Z"", ""categories"": [""cs.SD"", ""eess.AS""], ""pdf_url"": ""https://arxiv.org/pdf/2505.14989v1"", ""landing_url"": ""https://arxiv.org/abs/2505.14989v1"", ""doi"": ""https://doi.org/10.48550/arXiv.2505.14989""}",2505.14989,https://openalex.org/W4415327346
10.48550/arxiv.2505.16182,Discrete Tokens Exhibit Interlanguage Speech Intelligibility Benefit: an Analytical Study Towards Accent-robust ASR Only with Native Speech Data,"In this study, we gained insight that contributes to achieving accent-robust ASR using only native speech data. In human perception of non-native speech, the phenomenon known as ""interlanguage speech intelligibility benefit"" (ISIB) is observed, where non-native listeners who share the native language with the speaker understand the speech better compared even to native listeners. Based on the idea that discrete tokens extracted from self-supervised learning (SSL) models represent the human perception of speech, we conducted an analytical study on the robustness of discrete token-based ASR to non-native speech, varying the language used for training the tokenization, which is viewed as a technical implementation of ISIB. The results showed that ISIB actually occurred in the discrete token-based ASR. Since our approach relies only on native speech data to simulate the behavior of human perception, it is expected to be applicable to a wide range of accents for which speech data is scarce.",1,,include (junior:5),https://arxiv.org/abs/2505.16182v1,https://arxiv.org/pdf/2505.16182v1,2025,dblp_title,Discrete Tokens Exhibit Interlanguage Speech Intelligibility Benefit: an Analytical Study Towards Accent-robust ASR Only with Native Speech Data.,"{""arxiv_id"": ""2505.16182"", ""title"": ""Discrete Tokens Exhibit Interlanguage Speech Intelligibility Benefit: an Analytical Study Towards Accent-robust ASR Only with Native Speech Data"", ""summary"": ""In this study, we gained insight that contributes to achieving accent-robust ASR using only native speech data. In human perception of non-native speech, the phenomenon known as \""interlanguage speech intelligibility benefit\"" (ISIB) is observed, where non-native listeners who share the native language with the speaker understand the speech better compared even to native listeners. Based on the idea that discrete tokens extracted from self-supervised learning (SSL) models represent the human perception of speech, we conducted an analytical study on the robustness of discrete token-based ASR to non-native speech, varying the language used for training the tokenization, which is viewed as a technical implementation of ISIB. The results showed that ISIB actually occurred in the discrete token-based ASR. Since our approach relies only on native speech data to simulate the behavior of human perception, it is expected to be applicable to a wide range of accents for which speech data is scarce."", ""authors"": [""Kentaro Onda"", ""Keisuke Imoto"", ""Satoru Fukayama"", ""Daisuke Saito"", ""Nobuaki Minematsu""], ""published"": ""2025-05-22T03:36:28Z"", ""updated"": ""2025-05-22T03:36:28Z"", ""categories"": [""cs.SD"", ""eess.AS""], ""pdf_url"": ""https://arxiv.org/pdf/2505.16182v1"", ""landing_url"": ""https://arxiv.org/abs/2505.16182v1"", ""doi"": ""https://doi.org/10.48550/arXiv.2505.16182""}",2505.16182,https://openalex.org/W4416447784
10.48550/arxiv.2505.17076,Impact of Frame Rates on Speech Tokenizer: A Case Study on Mandarin and English,"The speech tokenizer plays a crucial role in recent speech tasks, generally serving as a bridge between speech signals and language models. While low-frame-rate codecs are widely employed as speech tokenizers, the impact of frame rates on speech tokens remains underexplored. In this study, we investigate how varying frame rates affect speech tokenization by examining Mandarin and English, two typologically distinct languages. We encode speech at different frame rates and evaluate the resulting semantic tokens in the speech recognition task. Our findings reveal that frame rate variations influence speech tokenization differently for each language, highlighting the interplay between frame rates, phonetic density, and language-specific acoustic features. The results provide insights into optimizing frame rate selection for speech tokenizers, with implications for automatic speech recognition, text-to-speech, and other speech-related applications.",1,,include (junior:5),https://arxiv.org/abs/2505.17076v3,https://arxiv.org/pdf/2505.17076v3,2025,discrete speech tokens,semantic tokens,"{""arxiv_id"": ""2505.17076"", ""title"": ""Impact of Frame Rates on Speech Tokenizer: A Case Study on Mandarin and English"", ""summary"": ""The speech tokenizer plays a crucial role in recent speech tasks, generally serving as a bridge between speech signals and language models. While low-frame-rate codecs are widely employed as speech tokenizers, the impact of frame rates on speech tokens remains underexplored. In this study, we investigate how varying frame rates affect speech tokenization by examining Mandarin and English, two typologically distinct languages. We encode speech at different frame rates and evaluate the resulting semantic tokens in the speech recognition task. Our findings reveal that frame rate variations influence speech tokenization differently for each language, highlighting the interplay between frame rates, phonetic density, and language-specific acoustic features. The results provide insights into optimizing frame rate selection for speech tokenizers, with implications for automatic speech recognition, text-to-speech, and other speech-related applications."", ""authors"": [""Haoyang Zhang"", ""Hexin Liu"", ""Xiangyu Zhang"", ""Qiquan Zhang"", ""Yuchen Hu"", ""Junqi Zhao"", ""Fei Tian"", ""Xuerui Yang"", ""Leibny Paola Garcia"", ""Eng Siong Chng""], ""published"": ""2025-05-20T06:01:19Z"", ""updated"": ""2025-06-13T17:21:25Z"", ""categories"": [""cs.CL"", ""cs.AI"", ""cs.SD"", ""eess.AS""], ""pdf_url"": ""https://arxiv.org/pdf/2505.17076v3"", ""landing_url"": ""https://arxiv.org/abs/2505.17076v3"", ""doi"": ""https://doi.org/10.48550/arXiv.2505.17076""}",2505.17076,https://openalex.org/W2401084598
10.48550/arxiv.2505.17446,Exploring the Effect of Segmentation and Vocabulary Size on Speech Tokenization for Speech Language Models,"The purpose of speech tokenization is to transform a speech signal into a sequence of discrete representations, serving as the foundation for speech language models (SLMs). While speech tokenization has many options, their effect on the performance of SLMs remains unclear. This paper investigates two key aspects of speech tokenization: the segmentation width and the cluster size of discrete units. First, we segment speech signals into fixed/variable widths and pooled representations. We then train K-means models in multiple cluster sizes. Through the evaluation on zero-shot spoken language understanding benchmarks, we find the positive effect of moderately coarse segmentation and bigger cluster size. Notably, among the best-performing models, the most efficient one achieves a 50% reduction in training data and a 70% decrease in training runtime. Our analysis highlights the importance of combining multiple tokens to enhance fine-grained spoken language understanding.",1,,include (junior:5),https://arxiv.org/abs/2505.17446v2,https://arxiv.org/pdf/2505.17446v2,2025,discrete speech tokens,speech tokenization,"{""arxiv_id"": ""2505.17446"", ""title"": ""Exploring the Effect of Segmentation and Vocabulary Size on Speech Tokenization for Speech Language Models"", ""summary"": ""The purpose of speech tokenization is to transform a speech signal into a sequence of discrete representations, serving as the foundation for speech language models (SLMs). While speech tokenization has many options, their effect on the performance of SLMs remains unclear. This paper investigates two key aspects of speech tokenization: the segmentation width and the cluster size of discrete units. First, we segment speech signals into fixed/variable widths and pooled representations. We then train K-means models in multiple cluster sizes. Through the evaluation on zero-shot spoken language understanding benchmarks, we find the positive effect of moderately coarse segmentation and bigger cluster size. Notably, among the best-performing models, the most efficient one achieves a 50% reduction in training data and a 70% decrease in training runtime. Our analysis highlights the importance of combining multiple tokens to enhance fine-grained spoken language understanding."", ""authors"": [""Shunsuke Kando"", ""Yusuke Miyao"", ""Shinnosuke Takamichi""], ""published"": ""2025-05-23T04:03:27Z"", ""updated"": ""2025-05-31T13:32:13Z"", ""categories"": [""cs.CL"", ""cs.SD"", ""eess.AS""], ""pdf_url"": ""https://arxiv.org/pdf/2505.17446v2"", ""landing_url"": ""https://arxiv.org/abs/2505.17446v2"", ""doi"": ""https://doi.org/10.48550/arXiv.2505.17446""}",2505.17446,https://openalex.org/W4415433821
10.48550/arxiv.2505.18864,Audio Jailbreak Attacks: Exposing Vulnerabilities in SpeechGPT in a White-Box Framework,"Recent advances in Multimodal Large Language Models (MLLMs) have significantly enhanced the naturalness and flexibility of human computer interaction by enabling seamless understanding across text, vision, and audio modalities. Among these, voice enabled models such as SpeechGPT have demonstrated considerable improvements in usability, offering expressive, and emotionally responsive interactions that foster deeper connections in real world communication scenarios. However, the use of voice introduces new security risks, as attackers can exploit the unique characteristics of spoken language, such as timing, pronunciation variability, and speech to text translation, to craft inputs that bypass defenses in ways not seen in text-based systems. Despite substantial research on text based jailbreaks, the voice modality remains largely underexplored in terms of both attack strategies and defense mechanisms. In this work, we present an adversarial attack targeting the speech input of aligned MLLMs in a white box scenario. Specifically, we introduce a novel token level attack that leverages access to the model's speech tokenization to generate adversarial token sequences. These sequences are then synthesized into audio prompts, which effectively bypass alignment safeguards and to induce prohibited outputs. Evaluated on SpeechGPT, our approach achieves up to 89 percent attack success rate across multiple restricted tasks, significantly outperforming existing voice based jailbreak methods. Our findings shed light on the vulnerabilities of voice-enabled multimodal systems and to help guide the development of more robust next-generation MLLMs.",1,,include (junior:5),https://arxiv.org/abs/2505.18864v1,https://arxiv.org/pdf/2505.18864v1,2025,discrete speech tokens,speech tokenization,"{""arxiv_id"": ""2505.18864"", ""title"": ""Audio Jailbreak Attacks: Exposing Vulnerabilities in SpeechGPT in a White-Box Framework"", ""summary"": ""Recent advances in Multimodal Large Language Models (MLLMs) have significantly enhanced the naturalness and flexibility of human computer interaction by enabling seamless understanding across text, vision, and audio modalities. Among these, voice enabled models such as SpeechGPT have demonstrated considerable improvements in usability, offering expressive, and emotionally responsive interactions that foster deeper connections in real world communication scenarios. However, the use of voice introduces new security risks, as attackers can exploit the unique characteristics of spoken language, such as timing, pronunciation variability, and speech to text translation, to craft inputs that bypass defenses in ways not seen in text-based systems. Despite substantial research on text based jailbreaks, the voice modality remains largely underexplored in terms of both attack strategies and defense mechanisms. In this work, we present an adversarial attack targeting the speech input of aligned MLLMs in a white box scenario. Specifically, we introduce a novel token level attack that leverages access to the model's speech tokenization to generate adversarial token sequences. These sequences are then synthesized into audio prompts, which effectively bypass alignment safeguards and to induce prohibited outputs. Evaluated on SpeechGPT, our approach achieves up to 89 percent attack success rate across multiple restricted tasks, significantly outperforming existing voice based jailbreak methods. Our findings shed light on the vulnerabilities of voice-enabled multimodal systems and to help guide the development of more robust next-generation MLLMs."", ""authors"": [""Binhao Ma"", ""Hanqing Guo"", ""Zhengping Jay Luo"", ""Rui Duan""], ""published"": ""2025-05-24T20:46:36Z"", ""updated"": ""2025-05-24T20:46:36Z"", ""categories"": [""cs.CL""], ""pdf_url"": ""https://arxiv.org/pdf/2505.18864v1"", ""landing_url"": ""https://arxiv.org/abs/2505.18864v1"", ""doi"": ""https://doi.org/10.48550/arXiv.2505.18864""}",2505.18864,https://openalex.org/W4414581766
10.48550/arxiv.2505.24314,DS-Codec: Dual-Stage Training with Mirror-to-NonMirror Architecture Switching for Speech Codec,"Neural speech codecs are essential for advancing text-to-speech (TTS) systems. With the recent success of large language models in text generation, developing high-quality speech tokenizers has become increasingly important. This paper introduces DS-Codec, a novel neural speech codec featuring a dual-stage training framework with mirror and non-mirror architectures switching, designed to achieve superior speech reconstruction. We conduct extensive experiments and ablation studies to evaluate the effectiveness of our training strategy and compare the performance of the two architectures. Our results show that the mirrored structure significantly enhances the robustness of the learned codebooks, and the training strategy balances the advantages between mirrored and non-mirrored structures, leading to improved high-fidelity speech reconstruction.",1,,include (senior:5),https://arxiv.org/abs/2505.24314v1,https://arxiv.org/pdf/2505.24314v1,2025,discrete speech tokens,speech tokenization,"{""arxiv_id"": ""2505.24314"", ""title"": ""DS-Codec: Dual-Stage Training with Mirror-to-NonMirror Architecture Switching for Speech Codec"", ""summary"": ""Neural speech codecs are essential for advancing text-to-speech (TTS) systems. With the recent success of large language models in text generation, developing high-quality speech tokenizers has become increasingly important. This paper introduces DS-Codec, a novel neural speech codec featuring a dual-stage training framework with mirror and non-mirror architectures switching, designed to achieve superior speech reconstruction. We conduct extensive experiments and ablation studies to evaluate the effectiveness of our training strategy and compare the performance of the two architectures. Our results show that the mirrored structure significantly enhances the robustness of the learned codebooks, and the training strategy balances the advantages between mirrored and non-mirrored structures, leading to improved high-fidelity speech reconstruction."", ""authors"": [""Peijie Chen"", ""Wenhao Guan"", ""Kaidi Wang"", ""Weijie Wu"", ""Hukai Huang"", ""Qingyang Hong"", ""Lin Li""], ""published"": ""2025-05-30T07:53:01Z"", ""updated"": ""2025-05-30T07:53:01Z"", ""categories"": [""cs.SD"", ""eess.AS""], ""pdf_url"": ""https://arxiv.org/pdf/2505.24314v1"", ""landing_url"": ""https://arxiv.org/abs/2505.24314v1"", ""doi"": ""https://doi.org/10.48550/arXiv.2505.24314""}",2505.24314,https://openalex.org/W4414856919
10.48550/arxiv.2505.24496,Speech Token Prediction via Compressed-to-fine Language Modeling for Speech Generation,"Neural audio codecs, used as speech tokenizers, have demonstrated remarkable potential in the field of speech generation. However, to ensure high-fidelity audio reconstruction, neural audio codecs typically encode audio into long sequences of speech tokens, posing a significant challenge for downstream language models in long-context modeling. We observe that speech token sequences exhibit short-range dependency: due to the monotonic alignment between text and speech in text-to-speech (TTS) tasks, the prediction of the current token primarily relies on its local context, while long-range tokens contribute less to the current token prediction and often contain redundant information. Inspired by this observation, we propose a \textbf{compressed-to-fine language modeling} approach to address the challenge of long sequence speech tokens within neural codec language models: (1) \textbf{Fine-grained Initial and Short-range Information}: Our approach retains the prompt and local tokens during prediction to ensure text alignment and the integrity of paralinguistic information; (2) \textbf{Compressed Long-range Context}: Our approach compresses long-range token spans into compact representations to reduce redundant information while preserving essential semantics. Extensive experiments on various neural audio codecs and downstream language models validate the effectiveness and generalizability of the proposed approach, highlighting the importance of token compression in improving speech generation within neural codec language models. The demo of audio samples will be available at https://anonymous.4open.science/r/SpeechTokenPredictionViaCompressedToFinedLM.",1,,include (junior:5),https://arxiv.org/abs/2505.24496v1,https://arxiv.org/pdf/2505.24496v1,2025,discrete speech tokens,neural codec,"{""arxiv_id"": ""2505.24496"", ""title"": ""Speech Token Prediction via Compressed-to-fine Language Modeling for Speech Generation"", ""summary"": ""Neural audio codecs, used as speech tokenizers, have demonstrated remarkable potential in the field of speech generation. However, to ensure high-fidelity audio reconstruction, neural audio codecs typically encode audio into long sequences of speech tokens, posing a significant challenge for downstream language models in long-context modeling. We observe that speech token sequences exhibit short-range dependency: due to the monotonic alignment between text and speech in text-to-speech (TTS) tasks, the prediction of the current token primarily relies on its local context, while long-range tokens contribute less to the current token prediction and often contain redundant information. Inspired by this observation, we propose a \\textbf{compressed-to-fine language modeling} approach to address the challenge of long sequence speech tokens within neural codec language models: (1) \\textbf{Fine-grained Initial and Short-range Information}: Our approach retains the prompt and local tokens during prediction to ensure text alignment and the integrity of paralinguistic information; (2) \\textbf{Compressed Long-range Context}: Our approach compresses long-range token spans into compact representations to reduce redundant information while preserving essential semantics. Extensive experiments on various neural audio codecs and downstream language models validate the effectiveness and generalizability of the proposed approach, highlighting the importance of token compression in improving speech generation within neural codec language models. The demo of audio samples will be available at https://anonymous.4open.science/r/SpeechTokenPredictionViaCompressedToFinedLM."", ""authors"": [""Wenrui Liu"", ""Qian Chen"", ""Wen Wang"", ""Yafeng Chen"", ""Jin Xu"", ""Zhifang Guo"", ""Guanrou Yang"", ""Weiqin Li"", ""Xiaoda Yang"", ""Tao Jin"", ""Minghui Fang"", ""Jialong Zuo"", ""Bai Jionghao"", ""Zemin Liu""], ""published"": ""2025-05-30T11:47:29Z"", ""updated"": ""2025-05-30T11:47:29Z"", ""categories"": [""eess.AS""], ""pdf_url"": ""https://arxiv.org/pdf/2505.24496v1"", ""landing_url"": ""https://arxiv.org/abs/2505.24496v1"", ""doi"": ""https://doi.org/10.48550/arXiv.2505.24496""}",2505.24496,https://openalex.org/W4414857663
10.48550/arxiv.2506.00809,FUSE: Universal Speech Enhancement using Multi-Stage Fusion of Sparse Compression and Token Generation Models for the URGENT 2025 Challenge,"We propose a multi-stage framework for universal speech enhancement, designed for the Interspeech 2025 URGENT Challenge. Our system first employs a Sparse Compression Network to robustly separate sources and extract an initial clean speech estimate from noisy inputs. This is followed by an efficient generative model that refines speech quality by leveraging self-supervised features and optimizing a masked language modeling objective on acoustic tokens derived from a neural audio codec. In the final stage, a fusion network integrates the outputs of the first two stages with the original noisy signal, achieving a balanced improvement in both signal fidelity and perceptual quality. Additionally, a shift trick that aggregates multiple time-shifted predictions, along with output blending, further boosts performance. Experimental results on challenging multilingual datasets with variable sampling rates and diverse distortion types validate the effectiveness of our approach.",1,,include (junior:5),https://arxiv.org/abs/2506.00809v1,https://arxiv.org/pdf/2506.00809v1,2025,discrete speech tokens,acoustic tokens,"{""arxiv_id"": ""2506.00809"", ""title"": ""FUSE: Universal Speech Enhancement using Multi-Stage Fusion of Sparse Compression and Token Generation Models for the URGENT 2025 Challenge"", ""summary"": ""We propose a multi-stage framework for universal speech enhancement, designed for the Interspeech 2025 URGENT Challenge. Our system first employs a Sparse Compression Network to robustly separate sources and extract an initial clean speech estimate from noisy inputs. This is followed by an efficient generative model that refines speech quality by leveraging self-supervised features and optimizing a masked language modeling objective on acoustic tokens derived from a neural audio codec. In the final stage, a fusion network integrates the outputs of the first two stages with the original noisy signal, achieving a balanced improvement in both signal fidelity and perceptual quality. Additionally, a shift trick that aggregates multiple time-shifted predictions, along with output blending, further boosts performance. Experimental results on challenging multilingual datasets with variable sampling rates and diverse distortion types validate the effectiveness of our approach."", ""authors"": [""Nabarun Goswami"", ""Tatsuya Harada""], ""published"": ""2025-06-01T03:23:27Z"", ""updated"": ""2025-06-01T03:23:27Z"", ""categories"": [""cs.SD"", ""eess.AS""], ""pdf_url"": ""https://arxiv.org/pdf/2506.00809v1"", ""landing_url"": ""https://arxiv.org/abs/2506.00809v1"", ""doi"": ""https://doi.org/10.48550/arXiv.2506.00809""}",2506.00809,https://openalex.org/W4414893697
10.48550/arxiv.2506.00843,HASRD: Hierarchical Acoustic and Semantic Representation Disentanglement,"Effective speech representations for spoken language models must balance semantic relevance with acoustic fidelity for high-quality reconstruction. However, existing approaches struggle to achieve both simultaneously. To address this, we introduce Hierarchical Acoustic and Semantic Representation Disentanglement (HASRD, pronounced `hazard'), a framework that factorizes self-supervised learning representations into discrete semantic and acoustic tokens. HASRD assigns the semantic representation to the first codebook, while encoding acoustic residuals in subsequent codebooks. This preserves ASR performance while achieving high-quality reconstruction. Additionally, we enhance HASRD's encoder efficiency, improving ASR performance without compromising reconstruction quality. Compared to SpeechTokenizer, HASRD achieves a 44% relative WER improvement, superior reconstruction quality, and 2x lower bitrate, demonstrating its effectiveness in disentangling acoustic and semantic information.",1,,include (junior:5),https://arxiv.org/abs/2506.00843v1,https://arxiv.org/pdf/2506.00843v1,2025,discrete speech tokens,acoustic tokens,"{""arxiv_id"": ""2506.00843"", ""title"": ""HASRD: Hierarchical Acoustic and Semantic Representation Disentanglement"", ""summary"": ""Effective speech representations for spoken language models must balance semantic relevance with acoustic fidelity for high-quality reconstruction. However, existing approaches struggle to achieve both simultaneously. To address this, we introduce Hierarchical Acoustic and Semantic Representation Disentanglement (HASRD, pronounced `hazard'), a framework that factorizes self-supervised learning representations into discrete semantic and acoustic tokens. HASRD assigns the semantic representation to the first codebook, while encoding acoustic residuals in subsequent codebooks. This preserves ASR performance while achieving high-quality reconstruction. Additionally, we enhance HASRD's encoder efficiency, improving ASR performance without compromising reconstruction quality. Compared to SpeechTokenizer, HASRD achieves a 44% relative WER improvement, superior reconstruction quality, and 2x lower bitrate, demonstrating its effectiveness in disentangling acoustic and semantic information."", ""authors"": [""Amir Hussein"", ""Sameer Khurana"", ""Gordon Wichern"", ""Francois G. Germain"", ""Jonathan Le Roux""], ""published"": ""2025-06-01T05:38:39Z"", ""updated"": ""2025-06-01T05:38:39Z"", ""categories"": [""eess.AS"", ""cs.SD""], ""pdf_url"": ""https://arxiv.org/pdf/2506.00843v1"", ""landing_url"": ""https://arxiv.org/abs/2506.00843v1"", ""doi"": ""https://doi.org/10.48550/arXiv.2506.00843""}",2506.00843,https://openalex.org/W4414893720
