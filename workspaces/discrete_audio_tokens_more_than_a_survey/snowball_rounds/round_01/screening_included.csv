doi,title,abstract,included,decision_source,final_verdict,landing_url,pdf_url,year,source_anchor,source_term,metadata_json,arxiv_id,openalex_id
10.48550/arxiv.2502.06490,Recent Advances in Discrete Speech Tokens: A Review,"The rapid advancement of speech generation technologies in the era of large language models (LLMs) has established discrete speech tokens as a foundational paradigm for speech representation. These tokens, characterized by their discrete, compact, and concise nature, are not only advantageous for efficient transmission and storage, but also inherently compatible with the language modeling framework, enabling seamless integration of speech into text-dominated LLM architectures. Current research categorizes discrete speech tokens into two principal classes: acoustic tokens and semantic tokens, each of which has evolved into a rich research domain characterized by unique design philosophies and methodological approaches. This survey systematically synthesizes the existing taxonomy and recent innovations in discrete speech tokenization, conducts a critical examination of the strengths and limitations of each paradigm, and presents systematic experimental comparisons across token types. Furthermore, we identify persistent challenges in the field and propose potential research directions, aiming to offer actionable insights to inspire future advancements in the development and application of discrete speech tokens.",1,,include (seed_filter),https://arxiv.org/abs/2502.06490v4,https://arxiv.org/pdf/2502.06490v4,2025,discrete speech tokens,speech tokens,"{""arxiv_id"": ""2502.06490"", ""title"": ""Recent Advances in Discrete Speech Tokens: A Review"", ""summary"": ""The rapid advancement of speech generation technologies in the era of large language models (LLMs) has established discrete speech tokens as a foundational paradigm for speech representation. These tokens, characterized by their discrete, compact, and concise nature, are not only advantageous for efficient transmission and storage, but also inherently compatible with the language modeling framework, enabling seamless integration of speech into text-dominated LLM architectures. Current research categorizes discrete speech tokens into two principal classes: acoustic tokens and semantic tokens, each of which has evolved into a rich research domain characterized by unique design philosophies and methodological approaches. This survey systematically synthesizes the existing taxonomy and recent innovations in discrete speech tokenization, conducts a critical examination of the strengths and limitations of each paradigm, and presents systematic experimental comparisons across token types. Furthermore, we identify persistent challenges in the field and propose potential research directions, aiming to offer actionable insights to inspire future advancements in the development and application of discrete speech tokens."", ""authors"": [""Yiwei Guo"", ""Zhihan Li"", ""Hankun Wang"", ""Bohan Li"", ""Chongtian Shao"", ""Hanglei Zhang"", ""Chenpeng Du"", ""Xie Chen"", ""Shujie Liu"", ""Kai Yu""], ""published"": ""2025-02-10T14:08:25Z"", ""updated"": ""2025-12-12T05:18:11Z"", ""categories"": [""eess.AS"", ""cs.AI"", ""cs.MM"", ""cs.SD"", ""eess.SP""], ""pdf_url"": ""https://arxiv.org/pdf/2502.06490v4"", ""landing_url"": ""https://arxiv.org/abs/2502.06490v4"", ""doi"": ""https://doi.org/10.48550/arXiv.2502.06490""}",2502.0649,https://openalex.org/W4407359094
10.48550/arxiv.2206.08790,Self-supervised speech unit discovery from articulatory and acoustic features using VQ-VAE,"The human perception system is often assumed to recruit motor knowledge when processing auditory speech inputs. Using articulatory modeling and deep learning, this study examines how this articulatory information can be used for discovering speech units in a self-supervised setting. We used vector-quantized variational autoencoders (VQ-VAE) to learn discrete representations from articulatory and acoustic speech data. In line with the zero-resource paradigm, an ABX test was then used to investigate how the extracted representations encode phonetically relevant properties. Experiments were conducted on three different corpora in English and French. We found that articulatory information rather organises the latent representations in terms of place of articulation whereas the speech acoustics mainly structure the latent space in terms of manner of articulation. We show that an optimal fusion of the two modalities can lead to a joint representation of these phonetic dimensions more accurate than each modality considered individually. Since articulatory information is usually not available in a practical situation, we finally investigate the benefit it provides when inferred from the speech acoustics in a self-supervised manner.",1,,include (junior:5),https://arxiv.org/abs/2206.08790v1,https://arxiv.org/pdf/2206.08790v1,2022,discrete speech tokens,vq vae,"{""arxiv_id"": ""2206.08790"", ""title"": ""Self-supervised speech unit discovery from articulatory and acoustic features using VQ-VAE"", ""summary"": ""The human perception system is often assumed to recruit motor knowledge when processing auditory speech inputs. Using articulatory modeling and deep learning, this study examines how this articulatory information can be used for discovering speech units in a self-supervised setting. We used vector-quantized variational autoencoders (VQ-VAE) to learn discrete representations from articulatory and acoustic speech data. In line with the zero-resource paradigm, an ABX test was then used to investigate how the extracted representations encode phonetically relevant properties. Experiments were conducted on three different corpora in English and French. We found that articulatory information rather organises the latent representations in terms of place of articulation whereas the speech acoustics mainly structure the latent space in terms of manner of articulation. We show that an optimal fusion of the two modalities can lead to a joint representation of these phonetic dimensions more accurate than each modality considered individually. Since articulatory information is usually not available in a practical situation, we finally investigate the benefit it provides when inferred from the speech acoustics in a self-supervised manner."", ""authors"": [""Marc-Antoine Georges"", ""Jean-Luc Schwartz"", ""Thomas Hueber""], ""published"": ""2022-06-17T14:04:24Z"", ""updated"": ""2022-06-17T14:04:24Z"", ""categories"": [""cs.CL""], ""pdf_url"": ""https://arxiv.org/pdf/2206.08790v1"", ""landing_url"": ""https://arxiv.org/abs/2206.08790v1"", ""doi"": ""https://doi.org/10.48550/arXiv.2206.08790""}",2206.0879,https://openalex.org/W4283207314
10.1109/jstsp.2022.3193761,A Comparative Study of Self-supervised Speech Representation Based Voice Conversion,"We present a large-scale comparative study of self-supervised speech representation (S3R)-based voice conversion (VC). In the context of recognition-synthesis VC, S3Rs are attractive owing to their potential to replace expensive supervised representations such as phonetic posteriorgrams (PPGs), which are commonly adopted by state-of-the-art VC systems. Using S3PRL-VC, an open-source VC software we previously developed, we provide a series of in-depth objective and subjective analyses under three VC settings: intra-/cross-lingual any-to-one (A2O) and any-to-any (A2A) VC, using the voice conversion challenge 2020 (VCC2020) dataset. We investigated S3R-based VC in various aspects, including model type, multilinguality, and supervision. We also studied the effect of a post-discretization process with k-means clustering and showed how it improves in the A2A setting. Finally, the comparison with state-of-the-art VC systems demonstrates the competitiveness of S3R-based VC and also sheds light on the possible improving directions.",1,,include (senior:4),https://arxiv.org/abs/2207.04356v1,https://arxiv.org/pdf/2207.04356v1,2022,speech representation,speech tokens,"{""arxiv_id"": ""2207.04356"", ""title"": ""A Comparative Study of Self-supervised Speech Representation Based Voice Conversion"", ""summary"": ""We present a large-scale comparative study of self-supervised speech representation (S3R)-based voice conversion (VC). In the context of recognition-synthesis VC, S3Rs are attractive owing to their potential to replace expensive supervised representations such as phonetic posteriorgrams (PPGs), which are commonly adopted by state-of-the-art VC systems. Using S3PRL-VC, an open-source VC software we previously developed, we provide a series of in-depth objective and subjective analyses under three VC settings: intra-/cross-lingual any-to-one (A2O) and any-to-any (A2A) VC, using the voice conversion challenge 2020 (VCC2020) dataset. We investigated S3R-based VC in various aspects, including model type, multilinguality, and supervision. We also studied the effect of a post-discretization process with k-means clustering and showed how it improves in the A2A setting. Finally, the comparison with state-of-the-art VC systems demonstrates the competitiveness of S3R-based VC and also sheds light on the possible improving directions."", ""authors"": [""Wen-Chin Huang"", ""Shu-Wen Yang"", ""Tomoki Hayashi"", ""Tomoki Toda""], ""published"": ""2022-07-10T01:02:22Z"", ""updated"": ""2022-07-10T01:02:22Z"", ""categories"": [""cs.SD"", ""cs.LG"", ""eess.AS""], ""pdf_url"": ""https://arxiv.org/pdf/2207.04356v1"", ""landing_url"": ""https://arxiv.org/abs/2207.04356v1"", ""doi"": ""https://doi.org/10.1109/JSTSP.2022.3193761""}",2207.04356,https://openalex.org/W4287889585
10.48550/arxiv.2207.04646,DelightfulTTS 2: End-to-End Speech Synthesis with Adversarial Vector-Quantized Auto-Encoders,"Current text to speech (TTS) systems usually leverage a cascaded acoustic model and vocoder pipeline with mel-spectrograms as the intermediate representations, which suffer from two limitations: 1) the acoustic model and vocoder are separately trained instead of jointly optimized, which incurs cascaded errors; 2) the intermediate speech representations (e.g., mel-spectrogram) are pre-designed and lose phase information, which are sub-optimal. To solve these problems, in this paper, we develop DelightfulTTS 2, a new end-to-end speech synthesis system with automatically learned speech representations and jointly optimized acoustic model and vocoder. Specifically, 1) we propose a new codec network based on vector-quantized auto-encoders with adversarial training (VQ-GAN) to extract intermediate frame-level speech representations (instead of traditional representations like mel-spectrograms) and reconstruct speech waveform; 2) we jointly optimize the acoustic model (based on DelightfulTTS) and the vocoder (the decoder of VQ-GAN), with an auxiliary loss on the acoustic model to predict intermediate speech representations. Experiments show that DelightfulTTS 2 achieves a CMOS gain +0.14 over DelightfulTTS, and more method analyses further verify the effectiveness of the developed system.",1,,include (junior:4),https://arxiv.org/abs/2207.04646v1,https://arxiv.org/pdf/2207.04646v1,2022,discrete speech tokens,vq gan,"{""arxiv_id"": ""2207.04646"", ""title"": ""DelightfulTTS 2: End-to-End Speech Synthesis with Adversarial Vector-Quantized Auto-Encoders"", ""summary"": ""Current text to speech (TTS) systems usually leverage a cascaded acoustic model and vocoder pipeline with mel-spectrograms as the intermediate representations, which suffer from two limitations: 1) the acoustic model and vocoder are separately trained instead of jointly optimized, which incurs cascaded errors; 2) the intermediate speech representations (e.g., mel-spectrogram) are pre-designed and lose phase information, which are sub-optimal. To solve these problems, in this paper, we develop DelightfulTTS 2, a new end-to-end speech synthesis system with automatically learned speech representations and jointly optimized acoustic model and vocoder. Specifically, 1) we propose a new codec network based on vector-quantized auto-encoders with adversarial training (VQ-GAN) to extract intermediate frame-level speech representations (instead of traditional representations like mel-spectrograms) and reconstruct speech waveform; 2) we jointly optimize the acoustic model (based on DelightfulTTS) and the vocoder (the decoder of VQ-GAN), with an auxiliary loss on the acoustic model to predict intermediate speech representations. Experiments show that DelightfulTTS 2 achieves a CMOS gain +0.14 over DelightfulTTS, and more method analyses further verify the effectiveness of the developed system."", ""authors"": [""Yanqing Liu"", ""Ruiqing Xue"", ""Lei He"", ""Xu Tan"", ""Sheng Zhao""], ""published"": ""2022-07-11T06:15:45Z"", ""updated"": ""2022-07-11T06:15:45Z"", ""categories"": [""cs.SD"", ""eess.AS"", ""eess.SP""], ""pdf_url"": ""https://arxiv.org/pdf/2207.04646v1"", ""landing_url"": ""https://arxiv.org/abs/2207.04646v1"", ""doi"": ""https://doi.org/10.48550/arXiv.2207.04646""}",2207.04646,https://openalex.org/W4285077573
10.48550/arxiv.2209.10887,A Multi-Stage Multi-Codebook VQ-VAE Approach to High-Performance Neural TTS,"We propose a Multi-Stage, Multi-Codebook (MSMC) approach to high-performance neural TTS synthesis. A vector-quantized, variational autoencoder (VQ-VAE) based feature analyzer is used to encode Mel spectrograms of speech training data by down-sampling progressively in multiple stages into MSMC Representations (MSMCRs) with different time resolutions, and quantizing them with multiple VQ codebooks, respectively. Multi-stage predictors are trained to map the input text sequence to MSMCRs progressively by minimizing a combined loss of the reconstruction Mean Square Error (MSE) and ""triplet loss"". In synthesis, the neural vocoder converts the predicted MSMCRs into final speech waveforms. The proposed approach is trained and tested with an English TTS database of 16 hours by a female speaker. The proposed TTS achieves an MOS score of 4.41, which outperforms the baseline with an MOS of 3.62. Compact versions of the proposed TTS with much less parameters can still preserve high MOS scores. Ablation studies show that both multiple stages and multiple codebooks are effective for achieving high TTS performance.",1,,include (junior:5),https://arxiv.org/abs/2209.10887v1,https://arxiv.org/pdf/2209.10887v1,2022,discrete speech tokens,vq vae,"{""arxiv_id"": ""2209.10887"", ""title"": ""A Multi-Stage Multi-Codebook VQ-VAE Approach to High-Performance Neural TTS"", ""summary"": ""We propose a Multi-Stage, Multi-Codebook (MSMC) approach to high-performance neural TTS synthesis. A vector-quantized, variational autoencoder (VQ-VAE) based feature analyzer is used to encode Mel spectrograms of speech training data by down-sampling progressively in multiple stages into MSMC Representations (MSMCRs) with different time resolutions, and quantizing them with multiple VQ codebooks, respectively. Multi-stage predictors are trained to map the input text sequence to MSMCRs progressively by minimizing a combined loss of the reconstruction Mean Square Error (MSE) and \""triplet loss\"". In synthesis, the neural vocoder converts the predicted MSMCRs into final speech waveforms. The proposed approach is trained and tested with an English TTS database of 16 hours by a female speaker. The proposed TTS achieves an MOS score of 4.41, which outperforms the baseline with an MOS of 3.62. Compact versions of the proposed TTS with much less parameters can still preserve high MOS scores. Ablation studies show that both multiple stages and multiple codebooks are effective for achieving high TTS performance."", ""authors"": [""Haohan Guo"", ""Fenglong Xie"", ""Frank K. Soong"", ""Xixin Wu"", ""Helen Meng""], ""published"": ""2022-09-22T09:43:17Z"", ""updated"": ""2022-09-22T09:43:17Z"", ""categories"": [""cs.SD"", ""cs.CL"", ""eess.AS""], ""pdf_url"": ""https://arxiv.org/pdf/2209.10887v1"", ""landing_url"": ""https://arxiv.org/abs/2209.10887v1"", ""doi"": ""https://doi.org/10.48550/arXiv.2209.10887""}",2209.10887,https://openalex.org/W4296972946
10.48550/arxiv.2210.04062,CoBERT: Self-Supervised Speech Representation Learning Through Code Representation Learning,"Speech is the surface form of a finite set of phonetic units, which can be represented by discrete codes. We propose the Code BERT (CoBERT) approach for self-supervised speech representation learning. The idea is to convert an utterance to a sequence of discrete codes, and perform code representation learning, where we predict the code representations based on a masked view of the original speech input. Unlike the prior self-distillation approaches of which the teacher and the student are of the same modality, our target model predicts representations from a different modality. CoBERT outperforms the most recent state-of-the-art performance on the ASR task and brings significant improvements on the SUPERB speech translation (ST) task. Our code and models are released at https://github.com/mct10/CoBERT.",1,,include (junior:5),https://arxiv.org/abs/2210.04062v3,https://arxiv.org/pdf/2210.04062v3,2022,speech representation,speech tokens,"{""arxiv_id"": ""2210.04062"", ""title"": ""CoBERT: Self-Supervised Speech Representation Learning Through Code Representation Learning"", ""summary"": ""Speech is the surface form of a finite set of phonetic units, which can be represented by discrete codes. We propose the Code BERT (CoBERT) approach for self-supervised speech representation learning. The idea is to convert an utterance to a sequence of discrete codes, and perform code representation learning, where we predict the code representations based on a masked view of the original speech input. Unlike the prior self-distillation approaches of which the teacher and the student are of the same modality, our target model predicts representations from a different modality. CoBERT outperforms the most recent state-of-the-art performance on the ASR task and brings significant improvements on the SUPERB speech translation (ST) task. Our code and models are released at https://github.com/mct10/CoBERT."", ""authors"": [""Chutong Meng"", ""Junyi Ao"", ""Tom Ko"", ""Mingxuan Wang"", ""Haizhou Li""], ""published"": ""2022-10-08T17:15:46Z"", ""updated"": ""2023-07-05T16:30:48Z"", ""categories"": [""cs.SD"", ""eess.AS""], ""pdf_url"": ""https://arxiv.org/pdf/2210.04062v3"", ""landing_url"": ""https://arxiv.org/abs/2210.04062v3"", ""doi"": ""https://doi.org/10.48550/arXiv.2210.04062""}",2210.04062,https://openalex.org/W4304701018
10.48550/arxiv.2210.06007,JukeDrummer: Conditional Beat-aware Audio-domain Drum Accompaniment Generation via Transformer VQ-VAE,"This paper proposes a model that generates a drum track in the audio domain to play along to a user-provided drum-free recording. Specifically, using paired data of drumless tracks and the corresponding human-made drum tracks, we train a Transformer model to improvise the drum part of an unseen drumless recording. We combine two approaches to encode the input audio. First, we train a vector-quantized variational autoencoder (VQ-VAE) to represent the input audio with discrete codes, which can then be readily used in a Transformer. Second, using an audio-domain beat tracking model, we compute beat-related features of the input audio and use them as embeddings in the Transformer. Instead of generating the drum track directly as waveforms, we use a separate VQ-VAE to encode the mel-spectrogram of a drum track into another set of discrete codes, and train the Transformer to predict the sequence of drum-related discrete codes. The output codes are then converted to a mel-spectrogram with a decoder, and then to the waveform with a vocoder. We report both objective and subjective evaluations of variants of the proposed model, demonstrating that the model with beat information generates drum accompaniment that is rhythmically and stylistically consistent with the input audio.",1,,include (junior:5),https://arxiv.org/abs/2210.06007v2,https://arxiv.org/pdf/2210.06007v2,2022,discrete speech tokens,vq vae,"{""arxiv_id"": ""2210.06007"", ""title"": ""JukeDrummer: Conditional Beat-aware Audio-domain Drum Accompaniment Generation via Transformer VQ-VAE"", ""summary"": ""This paper proposes a model that generates a drum track in the audio domain to play along to a user-provided drum-free recording. Specifically, using paired data of drumless tracks and the corresponding human-made drum tracks, we train a Transformer model to improvise the drum part of an unseen drumless recording. We combine two approaches to encode the input audio. First, we train a vector-quantized variational autoencoder (VQ-VAE) to represent the input audio with discrete codes, which can then be readily used in a Transformer. Second, using an audio-domain beat tracking model, we compute beat-related features of the input audio and use them as embeddings in the Transformer. Instead of generating the drum track directly as waveforms, we use a separate VQ-VAE to encode the mel-spectrogram of a drum track into another set of discrete codes, and train the Transformer to predict the sequence of drum-related discrete codes. The output codes are then converted to a mel-spectrogram with a decoder, and then to the waveform with a vocoder. We report both objective and subjective evaluations of variants of the proposed model, demonstrating that the model with beat information generates drum accompaniment that is rhythmically and stylistically consistent with the input audio."", ""authors"": [""Yueh-Kao Wu"", ""Ching-Yu Chiu"", ""Yi-Hsuan Yang""], ""published"": ""2022-10-12T08:23:20Z"", ""updated"": ""2022-10-31T08:54:08Z"", ""categories"": [""cs.SD"", ""cs.AI"", ""cs.LG"", ""eess.AS""], ""pdf_url"": ""https://arxiv.org/pdf/2210.06007v2"", ""landing_url"": ""https://arxiv.org/abs/2210.06007v2"", ""doi"": ""https://doi.org/10.48550/arXiv.2210.06007""}",2210.06007,https://openalex.org/W4306177919
10.48550/arxiv.2210.15131,Towards High-Quality Neural TTS for Low-Resource Languages by Learning Compact Speech Representations,"This paper aims to enhance low-resource TTS by reducing training data requirements using compact speech representations. A Multi-Stage Multi-Codebook (MSMC) VQ-GAN is trained to learn the representation, MSMCR, and decode it to waveforms. Subsequently, we train the multi-stage predictor to predict MSMCRs from the text for TTS synthesis. Moreover, we optimize the training strategy by leveraging more audio to learn MSMCRs better for low-resource languages. It selects audio from other languages using speaker similarity metric to augment the training set, and applies transfer learning to improve training quality. In MOS tests, the proposed system significantly outperforms FastSpeech and VITS in standard and low-resource scenarios, showing lower data requirements. The proposed training strategy effectively enhances MSMCRs on waveform reconstruction. It improves TTS performance further, which wins 77% votes in the preference test for the low-resource TTS with only 15 minutes of paired data.",1,,include (junior:5),https://arxiv.org/abs/2210.15131v1,https://arxiv.org/pdf/2210.15131v1,2022,discrete speech tokens,vq gan,"{""arxiv_id"": ""2210.15131"", ""title"": ""Towards High-Quality Neural TTS for Low-Resource Languages by Learning Compact Speech Representations"", ""summary"": ""This paper aims to enhance low-resource TTS by reducing training data requirements using compact speech representations. A Multi-Stage Multi-Codebook (MSMC) VQ-GAN is trained to learn the representation, MSMCR, and decode it to waveforms. Subsequently, we train the multi-stage predictor to predict MSMCRs from the text for TTS synthesis. Moreover, we optimize the training strategy by leveraging more audio to learn MSMCRs better for low-resource languages. It selects audio from other languages using speaker similarity metric to augment the training set, and applies transfer learning to improve training quality. In MOS tests, the proposed system significantly outperforms FastSpeech and VITS in standard and low-resource scenarios, showing lower data requirements. The proposed training strategy effectively enhances MSMCRs on waveform reconstruction. It improves TTS performance further, which wins 77% votes in the preference test for the low-resource TTS with only 15 minutes of paired data."", ""authors"": [""Haohan Guo"", ""Fenglong Xie"", ""Xixin Wu"", ""Hui Lu"", ""Helen Meng""], ""published"": ""2022-10-27T02:32:00Z"", ""updated"": ""2022-10-27T02:32:00Z"", ""categories"": [""cs.SD"", ""cs.CL"", ""eess.AS""], ""pdf_url"": ""https://arxiv.org/pdf/2210.15131v1"", ""landing_url"": ""https://arxiv.org/abs/2210.15131v1"", ""doi"": ""https://doi.org/10.48550/arXiv.2210.15131""}",2210.15131,https://openalex.org/W4307535422
10.48550/arxiv.2210.16755,token2vec: A Joint Self-Supervised Pre-training Framework Using Unpaired Speech and Text,"Self-supervised pre-training has been successful in both text and speech processing. Speech and text offer different but complementary information. The question is whether we are able to perform a speech-text joint pre-training on unpaired speech and text. In this paper, we take the idea of self-supervised pre-training one step further and propose token2vec, a novel joint pre-training framework for unpaired speech and text based on discrete representations of speech. Firstly, due to the distinct characteristics between speech and text modalities, where speech is continuous while text is discrete, we first discretize speech into a sequence of discrete speech tokens to solve the modality mismatch problem. Secondly, to solve the length mismatch problem, where the speech sequence is usually much longer than text sequence, we convert the words of text into phoneme sequences and randomly repeat each phoneme in the sequences. Finally, we feed the discrete speech and text tokens into a modality-agnostic Transformer encoder and pre-train with token-level masking language modeling (tMLM). Experiments show that token2vec is significantly superior to various speech-only pre-training baselines, with up to 17.7% relative WER reduction. Token2vec model is also validated on a non-ASR task, i.e., spoken intent classification, and shows good transferability.",1,,include (junior:5),https://arxiv.org/abs/2210.16755v1,https://arxiv.org/pdf/2210.16755v1,2022,discrete speech tokens,speech tokens,"{""arxiv_id"": ""2210.16755"", ""title"": ""token2vec: A Joint Self-Supervised Pre-training Framework Using Unpaired Speech and Text"", ""summary"": ""Self-supervised pre-training has been successful in both text and speech processing. Speech and text offer different but complementary information. The question is whether we are able to perform a speech-text joint pre-training on unpaired speech and text. In this paper, we take the idea of self-supervised pre-training one step further and propose token2vec, a novel joint pre-training framework for unpaired speech and text based on discrete representations of speech. Firstly, due to the distinct characteristics between speech and text modalities, where speech is continuous while text is discrete, we first discretize speech into a sequence of discrete speech tokens to solve the modality mismatch problem. Secondly, to solve the length mismatch problem, where the speech sequence is usually much longer than text sequence, we convert the words of text into phoneme sequences and randomly repeat each phoneme in the sequences. Finally, we feed the discrete speech and text tokens into a modality-agnostic Transformer encoder and pre-train with token-level masking language modeling (tMLM). Experiments show that token2vec is significantly superior to various speech-only pre-training baselines, with up to 17.7% relative WER reduction. Token2vec model is also validated on a non-ASR task, i.e., spoken intent classification, and shows good transferability."", ""authors"": [""Xianghu Yue"", ""Junyi Ao"", ""Xiaoxue Gao"", ""Haizhou Li""], ""published"": ""2022-10-30T06:38:19Z"", ""updated"": ""2022-10-30T06:38:19Z"", ""categories"": [""cs.CL"", ""cs.SD"", ""eess.AS""], ""pdf_url"": ""https://arxiv.org/pdf/2210.16755v1"", ""landing_url"": ""https://arxiv.org/abs/2210.16755v1"", ""doi"": ""https://doi.org/10.48550/arXiv.2210.16755""}",2210.16755,https://openalex.org/W4307937713
10.48550/arxiv.2212.04559,SpeechLMScore: Evaluating speech generation using speech language model,"While human evaluation is the most reliable metric for evaluating speech generation systems, it is generally costly and time-consuming. Previous studies on automatic speech quality assessment address the problem by predicting human evaluation scores with machine learning models. However, they rely on supervised learning and thus suffer from high annotation costs and domain-shift problems. We propose SpeechLMScore, an unsupervised metric to evaluate generated speech using a speech-language model. SpeechLMScore computes the average log-probability of a speech signal by mapping it into discrete tokens and measures the average probability of generating the sequence of tokens. Therefore, it does not require human annotation and is a highly scalable framework. Evaluation results demonstrate that the proposed metric shows a promising correlation with human evaluation scores on different speech generation tasks including voice conversion, text-to-speech, and speech enhancement.",1,,include (junior:4),https://arxiv.org/abs/2212.04559v1,https://arxiv.org/pdf/2212.04559v1,2022,discrete speech tokens,speech generation,"{""arxiv_id"": ""2212.04559"", ""title"": ""SpeechLMScore: Evaluating speech generation using speech language model"", ""summary"": ""While human evaluation is the most reliable metric for evaluating speech generation systems, it is generally costly and time-consuming. Previous studies on automatic speech quality assessment address the problem by predicting human evaluation scores with machine learning models. However, they rely on supervised learning and thus suffer from high annotation costs and domain-shift problems. We propose SpeechLMScore, an unsupervised metric to evaluate generated speech using a speech-language model. SpeechLMScore computes the average log-probability of a speech signal by mapping it into discrete tokens and measures the average probability of generating the sequence of tokens. Therefore, it does not require human annotation and is a highly scalable framework. Evaluation results demonstrate that the proposed metric shows a promising correlation with human evaluation scores on different speech generation tasks including voice conversion, text-to-speech, and speech enhancement."", ""authors"": [""Soumi Maiti"", ""Yifan Peng"", ""Takaaki Saeki"", ""Shinji Watanabe""], ""published"": ""2022-12-08T21:00:15Z"", ""updated"": ""2022-12-08T21:00:15Z"", ""categories"": [""eess.AS"", ""cs.LG"", ""cs.SD""], ""pdf_url"": ""https://arxiv.org/pdf/2212.04559v1"", ""landing_url"": ""https://arxiv.org/abs/2212.04559v1"", ""doi"": ""https://doi.org/10.48550/arXiv.2212.04559""}",2212.04559,https://openalex.org/W4311247353
10.48550/arxiv.2212.09058,BEATs: Audio Pre-Training with Acoustic Tokenizers,"The massive growth of self-supervised learning (SSL) has been witnessed in language, vision, speech, and audio domains over the past few years. While discrete label prediction is widely adopted for other modalities, the state-of-the-art audio SSL models still employ reconstruction loss for pre-training. Compared with reconstruction loss, semantic-rich discrete label prediction encourages the SSL model to abstract the high-level audio semantics and discard the redundant details as in human perception. However, a semantic-rich acoustic tokenizer for general audio pre-training is usually not straightforward to obtain, due to the continuous property of audio and unavailable phoneme sequences like speech. To tackle this challenge, we propose BEATs, an iterative audio pre-training framework to learn Bidirectional Encoder representation from Audio Transformers, where an acoustic tokenizer and an audio SSL model are optimized by iterations. In the first iteration, we use random projection as the acoustic tokenizer to train an audio SSL model in a mask and label prediction manner. Then, we train an acoustic tokenizer for the next iteration by distilling the semantic knowledge from the pre-trained or fine-tuned audio SSL model. The iteration is repeated with the hope of mutual promotion of the acoustic tokenizer and audio SSL model. The experimental results demonstrate our acoustic tokenizers can generate discrete labels with rich audio semantics and our audio SSL models achieve state-of-the-art results across various audio classification benchmarks, even outperforming previous models that use more training data and model parameters significantly. Specifically, we set a new state-of-the-art mAP 50.6% on AudioSet-2M for audio-only models without using any external data, and 98.1% accuracy on ESC-50. The code and pre-trained models are available at https://aka.ms/beats.",1,,include (junior:5),https://arxiv.org/abs/2212.09058v1,https://arxiv.org/pdf/2212.09058v1,2022,acoustic tokens,speech tokens,"{""arxiv_id"": ""2212.09058"", ""title"": ""BEATs: Audio Pre-Training with Acoustic Tokenizers"", ""summary"": ""The massive growth of self-supervised learning (SSL) has been witnessed in language, vision, speech, and audio domains over the past few years. While discrete label prediction is widely adopted for other modalities, the state-of-the-art audio SSL models still employ reconstruction loss for pre-training. Compared with reconstruction loss, semantic-rich discrete label prediction encourages the SSL model to abstract the high-level audio semantics and discard the redundant details as in human perception. However, a semantic-rich acoustic tokenizer for general audio pre-training is usually not straightforward to obtain, due to the continuous property of audio and unavailable phoneme sequences like speech. To tackle this challenge, we propose BEATs, an iterative audio pre-training framework to learn Bidirectional Encoder representation from Audio Transformers, where an acoustic tokenizer and an audio SSL model are optimized by iterations. In the first iteration, we use random projection as the acoustic tokenizer to train an audio SSL model in a mask and label prediction manner. Then, we train an acoustic tokenizer for the next iteration by distilling the semantic knowledge from the pre-trained or fine-tuned audio SSL model. The iteration is repeated with the hope of mutual promotion of the acoustic tokenizer and audio SSL model. The experimental results demonstrate our acoustic tokenizers can generate discrete labels with rich audio semantics and our audio SSL models achieve state-of-the-art results across various audio classification benchmarks, even outperforming previous models that use more training data and model parameters significantly. Specifically, we set a new state-of-the-art mAP 50.6% on AudioSet-2M for audio-only models without using any external data, and 98.1% accuracy on ESC-50. The code and pre-trained models are available at https://aka.ms/beats."", ""authors"": [""Sanyuan Chen"", ""Yu Wu"", ""Chengyi Wang"", ""Shujie Liu"", ""Daniel Tompkins"", ""Zhuo Chen"", ""Furu Wei""], ""published"": ""2022-12-18T10:41:55Z"", ""updated"": ""2022-12-18T10:41:55Z"", ""categories"": [""eess.AS"", ""cs.AI"", ""cs.CL"", ""cs.LG"", ""cs.SD""], ""pdf_url"": ""https://arxiv.org/pdf/2212.09058v1"", ""landing_url"": ""https://arxiv.org/abs/2212.09058v1"", ""doi"": ""https://doi.org/10.48550/arXiv.2212.09058""}",2212.09058,https://openalex.org/W4312048190
10.1109/icassp49357.2023.10097097,Analysing Discrete Self Supervised Speech Representation for Spoken Language Modeling,"This work profoundly analyzes discrete self-supervised speech representations (units) through the eyes of Generative Spoken Language Modeling (GSLM). Following the findings of such an analysis, we propose practical improvements to the discrete unit for the GSLM. First, we start comprehending these units by analyzing them in three axes: interpretation, visualization, and resynthesis. Our analysis finds a high correlation between the speech units to phonemes and phoneme families, while their correlation with speaker or gender is weaker. Additionally, we found redundancies in the extracted units and claim that one reason may be the units' context. Following this analysis, we propose a new, unsupervised metric to measure unit redundancies. Finally, we use this metric to develop new methods that improve the robustness of units' clustering and show significant improvement considering zero-resource speech metrics such as ABX. Code and analysis tools are available under the following link: https://github.com/slp-rl/SLM-Discrete-Representations",1,,include (junior:5),https://arxiv.org/abs/2301.00591v3,https://arxiv.org/pdf/2301.00591v3,2023,speech representation,offline clustering,"{""arxiv_id"": ""2301.00591"", ""title"": ""Analysing Discrete Self Supervised Speech Representation for Spoken Language Modeling"", ""summary"": ""This work profoundly analyzes discrete self-supervised speech representations (units) through the eyes of Generative Spoken Language Modeling (GSLM). Following the findings of such an analysis, we propose practical improvements to the discrete unit for the GSLM. First, we start comprehending these units by analyzing them in three axes: interpretation, visualization, and resynthesis. Our analysis finds a high correlation between the speech units to phonemes and phoneme families, while their correlation with speaker or gender is weaker. Additionally, we found redundancies in the extracted units and claim that one reason may be the units' context. Following this analysis, we propose a new, unsupervised metric to measure unit redundancies. Finally, we use this metric to develop new methods that improve the robustness of units' clustering and show significant improvement considering zero-resource speech metrics such as ABX. Code and analysis tools are available under the following link: https://github.com/slp-rl/SLM-Discrete-Representations"", ""authors"": [""Amitay Sicherman"", ""Yossi Adi""], ""published"": ""2023-01-02T10:36:40Z"", ""updated"": ""2023-03-01T09:59:54Z"", ""categories"": [""cs.CL"", ""cs.SD"", ""eess.AS""], ""pdf_url"": ""https://arxiv.org/pdf/2301.00591v3"", ""landing_url"": ""https://arxiv.org/abs/2301.00591v3"", ""doi"": ""https://doi.org/10.1109/ICASSP49357.2023.10097097""}",2301.00591,https://openalex.org/W4375868953
10.48550/arxiv.2301.02111,Neural Codec Language Models are Zero-Shot Text to Speech Synthesizers,"We introduce a language modeling approach for text to speech synthesis (TTS). Specifically, we train a neural codec language model (called Vall-E) using discrete codes derived from an off-the-shelf neural audio codec model, and regard TTS as a conditional language modeling task rather than continuous signal regression as in previous work. During the pre-training stage, we scale up the TTS training data to 60K hours of English speech which is hundreds of times larger than existing systems. Vall-E emerges in-context learning capabilities and can be used to synthesize high-quality personalized speech with only a 3-second enrolled recording of an unseen speaker as an acoustic prompt. Experiment results show that Vall-E significantly outperforms the state-of-the-art zero-shot TTS system in terms of speech naturalness and speaker similarity. In addition, we find Vall-E could preserve the speaker's emotion and acoustic environment of the acoustic prompt in synthesis. See https://aka.ms/valle for demos of our work.",1,,include (junior:5),https://arxiv.org/abs/2301.02111v1,https://arxiv.org/pdf/2301.02111v1,2023,discrete speech tokens,text to speech,"{""arxiv_id"": ""2301.02111"", ""title"": ""Neural Codec Language Models are Zero-Shot Text to Speech Synthesizers"", ""summary"": ""We introduce a language modeling approach for text to speech synthesis (TTS). Specifically, we train a neural codec language model (called Vall-E) using discrete codes derived from an off-the-shelf neural audio codec model, and regard TTS as a conditional language modeling task rather than continuous signal regression as in previous work. During the pre-training stage, we scale up the TTS training data to 60K hours of English speech which is hundreds of times larger than existing systems. Vall-E emerges in-context learning capabilities and can be used to synthesize high-quality personalized speech with only a 3-second enrolled recording of an unseen speaker as an acoustic prompt. Experiment results show that Vall-E significantly outperforms the state-of-the-art zero-shot TTS system in terms of speech naturalness and speaker similarity. In addition, we find Vall-E could preserve the speaker's emotion and acoustic environment of the acoustic prompt in synthesis. See https://aka.ms/valle for demos of our work."", ""authors"": [""Chengyi Wang"", ""Sanyuan Chen"", ""Yu Wu"", ""Ziqiang Zhang"", ""Long Zhou"", ""Shujie Liu"", ""Zhuo Chen"", ""Yanqing Liu"", ""Huaming Wang"", ""Jinyu Li"", ""Lei He"", ""Sheng Zhao"", ""Furu Wei""], ""published"": ""2023-01-05T15:37:15Z"", ""updated"": ""2023-01-05T15:37:15Z"", ""categories"": [""cs.CL"", ""cs.SD"", ""eess.AS""], ""pdf_url"": ""https://arxiv.org/pdf/2301.02111v1"", ""landing_url"": ""https://arxiv.org/abs/2301.02111v1"", ""doi"": ""https://doi.org/10.48550/arXiv.2301.02111""}",2301.02111,https://openalex.org/W4313679638
10.48550/arxiv.2301.13662,InstructTTS: Modelling Expressive TTS in Discrete Latent Space with Natural Language Style Prompt,"Expressive text-to-speech (TTS) aims to synthesize different speaking style speech according to human's demands. Nowadays, there are two common ways to control speaking styles: (1) Pre-defining a group of speaking style and using categorical index to denote different speaking style. However, there are limitations in the diversity of expressiveness, as these models can only generate the pre-defined styles. (2) Using reference speech as style input, which results in a problem that the extracted style information is not intuitive or interpretable. In this study, we attempt to use natural language as style prompt to control the styles in the synthetic speech, e.g., ""Sigh tone in full of sad mood with some helpless feeling"". Considering that there is no existing TTS corpus which is proper to benchmark this novel task, we first construct a speech corpus, whose speech samples are annotated with not only content transcriptions but also style descriptions in natural language. Then we propose an expressive TTS model, named as InstructTTS, which is novel in the sense of following aspects: (1) We fully take the advantage of self-supervised learning and cross-modal metric learning, and propose a novel three-stage training procedure to obtain a robust sentence embedding model, which can effectively capture semantic information from the style prompts and control the speaking style in the generated speech. (2) We propose to model acoustic features in discrete latent space and train a novel discrete diffusion probabilistic model to generate vector-quantized (VQ) acoustic tokens rather than the commonly-used mel spectrogram. (3) We jointly apply mutual information (MI) estimation and minimization during acoustic model training to minimize style-speaker and style-content MI, avoiding possible content and speaker information leakage from the style prompt.",1,,include (junior:5),https://arxiv.org/abs/2301.13662v2,https://arxiv.org/pdf/2301.13662v2,2023,acoustic tokens,offline clustering,"{""arxiv_id"": ""2301.13662"", ""title"": ""InstructTTS: Modelling Expressive TTS in Discrete Latent Space with Natural Language Style Prompt"", ""summary"": ""Expressive text-to-speech (TTS) aims to synthesize different speaking style speech according to human's demands. Nowadays, there are two common ways to control speaking styles: (1) Pre-defining a group of speaking style and using categorical index to denote different speaking style. However, there are limitations in the diversity of expressiveness, as these models can only generate the pre-defined styles. (2) Using reference speech as style input, which results in a problem that the extracted style information is not intuitive or interpretable. In this study, we attempt to use natural language as style prompt to control the styles in the synthetic speech, e.g., \""Sigh tone in full of sad mood with some helpless feeling\"". Considering that there is no existing TTS corpus which is proper to benchmark this novel task, we first construct a speech corpus, whose speech samples are annotated with not only content transcriptions but also style descriptions in natural language. Then we propose an expressive TTS model, named as InstructTTS, which is novel in the sense of following aspects: (1) We fully take the advantage of self-supervised learning and cross-modal metric learning, and propose a novel three-stage training procedure to obtain a robust sentence embedding model, which can effectively capture semantic information from the style prompts and control the speaking style in the generated speech. (2) We propose to model acoustic features in discrete latent space and train a novel discrete diffusion probabilistic model to generate vector-quantized (VQ) acoustic tokens rather than the commonly-used mel spectrogram. (3) We jointly apply mutual information (MI) estimation and minimization during acoustic model training to minimize style-speaker and style-content MI, avoiding possible content and speaker information leakage from the style prompt."", ""authors"": [""Dongchao Yang"", ""Songxiang Liu"", ""Rongjie Huang"", ""Chao Weng"", ""Helen Meng""], ""published"": ""2023-01-31T14:26:52Z"", ""updated"": ""2023-06-25T11:42:52Z"", ""categories"": [""cs.SD"", ""eess.AS""], ""pdf_url"": ""https://arxiv.org/pdf/2301.13662v2"", ""landing_url"": ""https://arxiv.org/abs/2301.13662v2"", ""doi"": ""https://doi.org/10.48550/arXiv.2301.13662""}",2301.13662,https://openalex.org/W4318907788
10.48550/arxiv.2302.03540,"Speak, Read and Prompt: High-Fidelity Text-to-Speech with Minimal Supervision","We introduce SPEAR-TTS, a multi-speaker text-to-speech (TTS) system that can be trained with minimal supervision. By combining two types of discrete speech representations, we cast TTS as a composition of two sequence-to-sequence tasks: from text to high-level semantic tokens (akin to ""reading"") and from semantic tokens to low-level acoustic tokens (""speaking""). Decoupling these two tasks enables training of the ""speaking"" module using abundant audio-only data, and unlocks the highly efficient combination of pretraining and backtranslation to reduce the need for parallel data when training the ""reading"" component. To control the speaker identity, we adopt example prompting, which allows SPEAR-TTS to generalize to unseen speakers using only a short sample of 3 seconds, without any explicit speaker representation or speaker-id labels. Our experiments demonstrate that SPEAR-TTS achieves a character error rate that is competitive with state-of-the-art methods using only 15 minutes of parallel data, while matching ground-truth speech in terms of naturalness and acoustic quality, as measured in subjective tests.",1,,include (junior:5),https://arxiv.org/abs/2302.03540v1,https://arxiv.org/pdf/2302.03540v1,2023,acoustic tokens,offline clustering,"{""arxiv_id"": ""2302.03540"", ""title"": ""Speak, Read and Prompt: High-Fidelity Text-to-Speech with Minimal Supervision"", ""summary"": ""We introduce SPEAR-TTS, a multi-speaker text-to-speech (TTS) system that can be trained with minimal supervision. By combining two types of discrete speech representations, we cast TTS as a composition of two sequence-to-sequence tasks: from text to high-level semantic tokens (akin to \""reading\"") and from semantic tokens to low-level acoustic tokens (\""speaking\""). Decoupling these two tasks enables training of the \""speaking\"" module using abundant audio-only data, and unlocks the highly efficient combination of pretraining and backtranslation to reduce the need for parallel data when training the \""reading\"" component. To control the speaker identity, we adopt example prompting, which allows SPEAR-TTS to generalize to unseen speakers using only a short sample of 3 seconds, without any explicit speaker representation or speaker-id labels. Our experiments demonstrate that SPEAR-TTS achieves a character error rate that is competitive with state-of-the-art methods using only 15 minutes of parallel data, while matching ground-truth speech in terms of naturalness and acoustic quality, as measured in subjective tests."", ""authors"": [""Eugene Kharitonov"", ""Damien Vincent"", ""Zalán Borsos"", ""Raphaël Marinier"", ""Sertan Girgin"", ""Olivier Pietquin"", ""Matt Sharifi"", ""Marco Tagliasacchi"", ""Neil Zeghidour""], ""published"": ""2023-02-07T15:48:31Z"", ""updated"": ""2023-02-07T15:48:31Z"", ""categories"": [""cs.SD"", ""eess.AS""], ""pdf_url"": ""https://arxiv.org/pdf/2302.03540v1"", ""landing_url"": ""https://arxiv.org/abs/2302.03540v1"", ""doi"": ""https://doi.org/10.48550/arXiv.2302.03540""}",2302.0354,https://openalex.org/W4319653946
10.48550/arxiv.2302.04215,A Vector Quantized Approach for Text to Speech Synthesis on Real-World Spontaneous Speech,"Recent Text-to-Speech (TTS) systems trained on reading or acted corpora have achieved near human-level naturalness. The diversity of human speech, however, often goes beyond the coverage of these corpora. We believe the ability to handle such diversity is crucial for AI systems to achieve human-level communication. Our work explores the use of more abundant real-world data for building speech synthesizers. We train TTS systems using real-world speech from YouTube and podcasts. We observe the mismatch between training and inference alignments in mel-spectrogram based autoregressive models, leading to unintelligible synthesis, and demonstrate that learned discrete codes within multiple code groups effectively resolves this issue. We introduce our MQTTS system whose architecture is designed for multiple code generation and monotonic alignment, along with the use of a clean silence prompt to improve synthesis quality. We conduct ablation analyses to identify the efficacy of our methods. We show that MQTTS outperforms existing TTS systems in several objective and subjective measures.",1,,include (junior:4),https://arxiv.org/abs/2302.04215v1,https://arxiv.org/pdf/2302.04215v1,2023,discrete speech tokens,text to speech,"{""arxiv_id"": ""2302.04215"", ""title"": ""A Vector Quantized Approach for Text to Speech Synthesis on Real-World Spontaneous Speech"", ""summary"": ""Recent Text-to-Speech (TTS) systems trained on reading or acted corpora have achieved near human-level naturalness. The diversity of human speech, however, often goes beyond the coverage of these corpora. We believe the ability to handle such diversity is crucial for AI systems to achieve human-level communication. Our work explores the use of more abundant real-world data for building speech synthesizers. We train TTS systems using real-world speech from YouTube and podcasts. We observe the mismatch between training and inference alignments in mel-spectrogram based autoregressive models, leading to unintelligible synthesis, and demonstrate that learned discrete codes within multiple code groups effectively resolves this issue. We introduce our MQTTS system whose architecture is designed for multiple code generation and monotonic alignment, along with the use of a clean silence prompt to improve synthesis quality. We conduct ablation analyses to identify the efficacy of our methods. We show that MQTTS outperforms existing TTS systems in several objective and subjective measures."", ""authors"": [""Li-Wei Chen"", ""Shinji Watanabe"", ""Alexander Rudnicky""], ""published"": ""2023-02-08T17:34:32Z"", ""updated"": ""2023-02-08T17:34:32Z"", ""categories"": [""eess.AS"", ""cs.AI"", ""cs.LG"", ""cs.SD"", ""eess.SP""], ""pdf_url"": ""https://arxiv.org/pdf/2302.04215v1"", ""landing_url"": ""https://arxiv.org/abs/2302.04215v1"", ""doi"": ""https://doi.org/10.48550/arXiv.2302.04215""}",2302.04215,https://openalex.org/W4319793849
10.48550/arxiv.2302.08342,Speech Enhancement with Multi-granularity Vector Quantization,"With advances in deep learning, neural network based speech enhancement (SE) has developed rapidly in the last decade. Meanwhile, the self-supervised pre-trained model and vector quantization (VQ) have achieved excellent performance on many speech-related tasks, while they are less explored on SE. As it was shown in our previous work that utilizing a VQ module to discretize noisy speech representations is beneficial for speech denoising, in this work we therefore study the impact of using VQ at different layers with different number of codebooks. Different VQ modules indeed enable to extract multiple-granularity speech features. Following an attention mechanism, the contextual features extracted by a pre-trained model are fused with the local features extracted by the encoder, such that both global and local information are preserved to reconstruct the enhanced speech. Experimental results on the Valentini dataset show that the proposed model can improve the SE performance, where the impact of choosing pre-trained models is also revealed.",1,,include (junior:5),https://arxiv.org/abs/2302.08342v1,https://arxiv.org/pdf/2302.08342v1,2023,discrete speech tokens,vector quantization,"{""arxiv_id"": ""2302.08342"", ""title"": ""Speech Enhancement with Multi-granularity Vector Quantization"", ""summary"": ""With advances in deep learning, neural network based speech enhancement (SE) has developed rapidly in the last decade. Meanwhile, the self-supervised pre-trained model and vector quantization (VQ) have achieved excellent performance on many speech-related tasks, while they are less explored on SE. As it was shown in our previous work that utilizing a VQ module to discretize noisy speech representations is beneficial for speech denoising, in this work we therefore study the impact of using VQ at different layers with different number of codebooks. Different VQ modules indeed enable to extract multiple-granularity speech features. Following an attention mechanism, the contextual features extracted by a pre-trained model are fused with the local features extracted by the encoder, such that both global and local information are preserved to reconstruct the enhanced speech. Experimental results on the Valentini dataset show that the proposed model can improve the SE performance, where the impact of choosing pre-trained models is also revealed."", ""authors"": [""Xiao-Ying Zhao"", ""Qiu-Shi Zhu"", ""Jie Zhang""], ""published"": ""2023-02-16T14:53:41Z"", ""updated"": ""2023-02-16T14:53:41Z"", ""categories"": [""eess.AS"", ""cs.SD""], ""pdf_url"": ""https://arxiv.org/pdf/2302.08342v1"", ""landing_url"": ""https://arxiv.org/abs/2302.08342v1"", ""doi"": ""https://doi.org/10.48550/arXiv.2302.08342""}",2302.08342,https://openalex.org/W4321277156
10.48550/arxiv.2303.02939,FoundationTTS: Text-to-Speech for ASR Customization with Generative Language Model,"Neural text-to-speech (TTS) generally consists of cascaded architecture with separately optimized acoustic model and vocoder, or end-to-end architecture with continuous mel-spectrograms or self-extracted speech frames as the intermediate representations to bridge acoustic model and vocoder, which suffers from two limitations: 1) the continuous acoustic frames are hard to predict with phoneme only, and acoustic information like duration or pitch is also needed to solve the one-to-many problem, which is not easy to scale on large scale and noise datasets; 2) to achieve diverse speech output based on continuous speech features, complex VAE or flow-based models are usually required. In this paper, we propose FoundationTTS, a new speech synthesis system with a neural audio codec for discrete speech token extraction and waveform reconstruction and a large language model for discrete token generation from linguistic (phoneme) tokens. Specifically, 1) we propose a hierarchical codec network based on vector-quantized auto-encoders with adversarial training (VQ-GAN), which first extracts continuous frame-level speech representations with fine-grained codec, and extracts a discrete token from each continuous speech frame with coarse-grained codec; 2) we jointly optimize speech token, linguistic tokens, speaker token together with a large language model and predict the discrete speech tokens autoregressively. Experiments show that FoundationTTS achieves a MOS gain of +0.14 compared to the baseline system. In ASR customization tasks, our method achieves 7.09\% and 10.35\% WERR respectively over two strong customized ASR baselines.",1,,include (junior:5),https://arxiv.org/abs/2303.02939v3,https://arxiv.org/pdf/2303.02939v3,2023,discrete speech tokens,speech tokens,"{""arxiv_id"": ""2303.02939"", ""title"": ""FoundationTTS: Text-to-Speech for ASR Customization with Generative Language Model"", ""summary"": ""Neural text-to-speech (TTS) generally consists of cascaded architecture with separately optimized acoustic model and vocoder, or end-to-end architecture with continuous mel-spectrograms or self-extracted speech frames as the intermediate representations to bridge acoustic model and vocoder, which suffers from two limitations: 1) the continuous acoustic frames are hard to predict with phoneme only, and acoustic information like duration or pitch is also needed to solve the one-to-many problem, which is not easy to scale on large scale and noise datasets; 2) to achieve diverse speech output based on continuous speech features, complex VAE or flow-based models are usually required. In this paper, we propose FoundationTTS, a new speech synthesis system with a neural audio codec for discrete speech token extraction and waveform reconstruction and a large language model for discrete token generation from linguistic (phoneme) tokens. Specifically, 1) we propose a hierarchical codec network based on vector-quantized auto-encoders with adversarial training (VQ-GAN), which first extracts continuous frame-level speech representations with fine-grained codec, and extracts a discrete token from each continuous speech frame with coarse-grained codec; 2) we jointly optimize speech token, linguistic tokens, speaker token together with a large language model and predict the discrete speech tokens autoregressively. Experiments show that FoundationTTS achieves a MOS gain of +0.14 compared to the baseline system. In ASR customization tasks, our method achieves 7.09\\% and 10.35\\% WERR respectively over two strong customized ASR baselines."", ""authors"": [""Ruiqing Xue"", ""Yanqing Liu"", ""Lei He"", ""Xu Tan"", ""Linquan Liu"", ""Edward Lin"", ""Sheng Zhao""], ""published"": ""2023-03-06T07:17:15Z"", ""updated"": ""2023-03-08T03:06:47Z"", ""categories"": [""eess.AS"", ""cs.SD""], ""pdf_url"": ""https://arxiv.org/pdf/2303.02939v3"", ""landing_url"": ""https://arxiv.org/abs/2303.02939v3"", ""doi"": ""https://doi.org/10.48550/arXiv.2303.02939""}",2303.02939,https://openalex.org/W4323697273
10.48550/arxiv.2303.03926,Speak Foreign Languages with Your Own Voice: Cross-Lingual Neural Codec Language Modeling,"We propose a cross-lingual neural codec language model, VALL-E X, for cross-lingual speech synthesis. Specifically, we extend VALL-E and train a multi-lingual conditional codec language model to predict the acoustic token sequences of the target language speech by using both the source language speech and the target language text as prompts. VALL-E X inherits strong in-context learning capabilities and can be applied for zero-shot cross-lingual text-to-speech synthesis and zero-shot speech-to-speech translation tasks. Experimental results show that it can generate high-quality speech in the target language via just one speech utterance in the source language as a prompt while preserving the unseen speaker's voice, emotion, and acoustic environment. Moreover, VALL-E X effectively alleviates the foreign accent problems, which can be controlled by a language ID. Audio samples are available at \url{https://aka.ms/vallex}.",1,,include (junior:5),https://arxiv.org/abs/2303.03926v1,https://arxiv.org/pdf/2303.03926v1,2023,acoustic tokens,offline clustering,"{""arxiv_id"": ""2303.03926"", ""title"": ""Speak Foreign Languages with Your Own Voice: Cross-Lingual Neural Codec Language Modeling"", ""summary"": ""We propose a cross-lingual neural codec language model, VALL-E X, for cross-lingual speech synthesis. Specifically, we extend VALL-E and train a multi-lingual conditional codec language model to predict the acoustic token sequences of the target language speech by using both the source language speech and the target language text as prompts. VALL-E X inherits strong in-context learning capabilities and can be applied for zero-shot cross-lingual text-to-speech synthesis and zero-shot speech-to-speech translation tasks. Experimental results show that it can generate high-quality speech in the target language via just one speech utterance in the source language as a prompt while preserving the unseen speaker's voice, emotion, and acoustic environment. Moreover, VALL-E X effectively alleviates the foreign accent problems, which can be controlled by a language ID. Audio samples are available at \\url{https://aka.ms/vallex}."", ""authors"": [""Ziqiang Zhang"", ""Long Zhou"", ""Chengyi Wang"", ""Sanyuan Chen"", ""Yu Wu"", ""Shujie Liu"", ""Zhuo Chen"", ""Yanqing Liu"", ""Huaming Wang"", ""Jinyu Li"", ""Lei He"", ""Sheng Zhao"", ""Furu Wei""], ""published"": ""2023-03-07T14:31:55Z"", ""updated"": ""2023-03-07T14:31:55Z"", ""categories"": [""cs.CL"", ""cs.AI"", ""cs.SD"", ""eess.AS""], ""pdf_url"": ""https://arxiv.org/pdf/2303.03926v1"", ""landing_url"": ""https://arxiv.org/abs/2303.03926v1"", ""doi"": ""https://doi.org/10.48550/arXiv.2303.03926""}",2303.03926,https://openalex.org/W4323651091
10.1016/j.cviu.2025.104362,A vector quantized masked autoencoder for audiovisual speech emotion recognition,"An important challenge in emotion recognition is to develop methods that can leverage unlabeled training data. In this paper, we propose the VQ-MAE-AV model, a self-supervised multimodal model that leverages masked autoencoders to learn representations of audiovisual speech without labels. The model includes vector quantized variational autoencoders that compress raw audio and visual speech data into discrete tokens. The audiovisual speech tokens are used to train a multimodal masked autoencoder that consists of an encoder-decoder architecture with attention mechanisms. The model is designed to extract both local (i.e., at the frame level) and global (i.e., at the sequence level) representations of audiovisual speech. During self-supervised pre-training, the VQ-MAE-AV model is trained on a large-scale unlabeled dataset of audiovisual speech, for the task of reconstructing randomly masked audiovisual speech tokens and with a contrastive learning strategy. During this pre-training, the encoder learns to extract a representation of audiovisual speech that can be subsequently leveraged for emotion recognition. During the supervised fine-tuning stage, a small classification model is trained on top of the VQ-MAE-AV encoder for an emotion recognition task. The proposed approach achieves state-of-the-art emotion recognition results across several datasets in both controlled and in-the-wild conditions.",1,,include (junior:5),https://arxiv.org/abs/2305.03568v3,https://arxiv.org/pdf/2305.03568v3,2023,discrete speech tokens,speech tokens,"{""arxiv_id"": ""2305.03568"", ""title"": ""A vector quantized masked autoencoder for audiovisual speech emotion recognition"", ""summary"": ""An important challenge in emotion recognition is to develop methods that can leverage unlabeled training data. In this paper, we propose the VQ-MAE-AV model, a self-supervised multimodal model that leverages masked autoencoders to learn representations of audiovisual speech without labels. The model includes vector quantized variational autoencoders that compress raw audio and visual speech data into discrete tokens. The audiovisual speech tokens are used to train a multimodal masked autoencoder that consists of an encoder-decoder architecture with attention mechanisms. The model is designed to extract both local (i.e., at the frame level) and global (i.e., at the sequence level) representations of audiovisual speech. During self-supervised pre-training, the VQ-MAE-AV model is trained on a large-scale unlabeled dataset of audiovisual speech, for the task of reconstructing randomly masked audiovisual speech tokens and with a contrastive learning strategy. During this pre-training, the encoder learns to extract a representation of audiovisual speech that can be subsequently leveraged for emotion recognition. During the supervised fine-tuning stage, a small classification model is trained on top of the VQ-MAE-AV encoder for an emotion recognition task. The proposed approach achieves state-of-the-art emotion recognition results across several datasets in both controlled and in-the-wild conditions."", ""authors"": [""Samir Sadok"", ""Simon Leglaive"", ""Renaud Séguier""], ""published"": ""2023-05-05T14:19:46Z"", ""updated"": ""2025-05-09T08:19:45Z"", ""categories"": [""cs.SD"", ""cs.LG"", ""cs.MM"", ""eess.AS""], ""pdf_url"": ""https://arxiv.org/pdf/2305.03568v3"", ""landing_url"": ""https://arxiv.org/abs/2305.03568v3"", ""doi"": ""https://doi.org/10.1016/j.cviu.2025.104362""}",2305.03568,https://openalex.org/W4409472566
10.1016/j.neunet.2024.106120,A multimodal dynamical variational autoencoder for audiovisual speech representation learning,"In this paper, we present a multimodal and dynamical VAE (MDVAE) applied to unsupervised audio-visual speech representation learning. The latent space is structured to dissociate the latent dynamical factors that are shared between the modalities from those that are specific to each modality. A static latent variable is also introduced to encode the information that is constant over time within an audiovisual speech sequence. The model is trained in an unsupervised manner on an audiovisual emotional speech dataset, in two stages. In the first stage, a vector quantized VAE (VQ-VAE) is learned independently for each modality, without temporal modeling. The second stage consists in learning the MDVAE model on the intermediate representation of the VQ-VAEs before quantization. The disentanglement between static versus dynamical and modality-specific versus modality-common information occurs during this second training stage. Extensive experiments are conducted to investigate how audiovisual speech latent factors are encoded in the latent space of MDVAE. These experiments include manipulating audiovisual speech, audiovisual facial image denoising, and audiovisual speech emotion recognition. The results show that MDVAE effectively combines the audio and visual information in its latent space. They also show that the learned static representation of audiovisual speech can be used for emotion recognition with few labeled data, and with better accuracy compared with unimodal baselines and a state-of-the-art supervised model based on an audiovisual transformer architecture.",1,,include (junior:4),https://arxiv.org/abs/2305.03582v3,https://arxiv.org/pdf/2305.03582v3,2023,speech representation,vector quantization,"{""arxiv_id"": ""2305.03582"", ""title"": ""A multimodal dynamical variational autoencoder for audiovisual speech representation learning"", ""summary"": ""In this paper, we present a multimodal and dynamical VAE (MDVAE) applied to unsupervised audio-visual speech representation learning. The latent space is structured to dissociate the latent dynamical factors that are shared between the modalities from those that are specific to each modality. A static latent variable is also introduced to encode the information that is constant over time within an audiovisual speech sequence. The model is trained in an unsupervised manner on an audiovisual emotional speech dataset, in two stages. In the first stage, a vector quantized VAE (VQ-VAE) is learned independently for each modality, without temporal modeling. The second stage consists in learning the MDVAE model on the intermediate representation of the VQ-VAEs before quantization. The disentanglement between static versus dynamical and modality-specific versus modality-common information occurs during this second training stage. Extensive experiments are conducted to investigate how audiovisual speech latent factors are encoded in the latent space of MDVAE. These experiments include manipulating audiovisual speech, audiovisual facial image denoising, and audiovisual speech emotion recognition. The results show that MDVAE effectively combines the audio and visual information in its latent space. They also show that the learned static representation of audiovisual speech can be used for emotion recognition with few labeled data, and with better accuracy compared with unimodal baselines and a state-of-the-art supervised model based on an audiovisual transformer architecture."", ""authors"": [""Samir Sadok"", ""Simon Leglaive"", ""Laurent Girin"", ""Xavier Alameda-Pineda"", ""Renaud Séguier""], ""published"": ""2023-05-05T14:37:26Z"", ""updated"": ""2024-02-20T16:18:45Z"", ""categories"": [""cs.SD"", ""cs.LG"", ""cs.MM"", ""eess.AS""], ""pdf_url"": ""https://arxiv.org/pdf/2305.03582v3"", ""landing_url"": ""https://arxiv.org/abs/2305.03582v3"", ""doi"": ""https://doi.org/10.1016/j.neunet.2024.106120""}",2305.03582,https://openalex.org/W4390742209
10.48550/arxiv.2305.09636,SoundStorm: Efficient Parallel Audio Generation,"We present SoundStorm, a model for efficient, non-autoregressive audio generation. SoundStorm receives as input the semantic tokens of AudioLM, and relies on bidirectional attention and confidence-based parallel decoding to generate the tokens of a neural audio codec. Compared to the autoregressive generation approach of AudioLM, our model produces audio of the same quality and with higher consistency in voice and acoustic conditions, while being two orders of magnitude faster. SoundStorm generates 30 seconds of audio in 0.5 seconds on a TPU-v4. We demonstrate the ability of our model to scale audio generation to longer sequences by synthesizing high-quality, natural dialogue segments, given a transcript annotated with speaker turns and a short prompt with the speakers' voices.",1,,include (junior:5),https://arxiv.org/abs/2305.09636v1,https://arxiv.org/pdf/2305.09636v1,2023,semantic tokens,offline clustering,"{""arxiv_id"": ""2305.09636"", ""title"": ""SoundStorm: Efficient Parallel Audio Generation"", ""summary"": ""We present SoundStorm, a model for efficient, non-autoregressive audio generation. SoundStorm receives as input the semantic tokens of AudioLM, and relies on bidirectional attention and confidence-based parallel decoding to generate the tokens of a neural audio codec. Compared to the autoregressive generation approach of AudioLM, our model produces audio of the same quality and with higher consistency in voice and acoustic conditions, while being two orders of magnitude faster. SoundStorm generates 30 seconds of audio in 0.5 seconds on a TPU-v4. We demonstrate the ability of our model to scale audio generation to longer sequences by synthesizing high-quality, natural dialogue segments, given a transcript annotated with speaker turns and a short prompt with the speakers' voices."", ""authors"": [""Zalán Borsos"", ""Matt Sharifi"", ""Damien Vincent"", ""Eugene Kharitonov"", ""Neil Zeghidour"", ""Marco Tagliasacchi""], ""published"": ""2023-05-16T17:41:25Z"", ""updated"": ""2023-05-16T17:41:25Z"", ""categories"": [""cs.SD"", ""cs.LG"", ""eess.AS""], ""pdf_url"": ""https://arxiv.org/pdf/2305.09636v1"", ""landing_url"": ""https://arxiv.org/abs/2305.09636v1"", ""doi"": ""https://doi.org/10.48550/arXiv.2305.09636""}",2305.09636,https://openalex.org/W4377010126
10.48550/arxiv.2305.19269,Make-A-Voice: Unified Voice Synthesis With Discrete Representation,"Various applications of voice synthesis have been developed independently despite the fact that they generate ""voice"" as output in common. In addition, the majority of voice synthesis models currently rely on annotated audio data, but it is crucial to scale them to self-supervised datasets in order to effectively capture the wide range of acoustic variations present in human voice, including speaker identity, emotion, and prosody. In this work, we propose Make-A-Voice, a unified framework for synthesizing and manipulating voice signals from discrete representations. Make-A-Voice leverages a ""coarse-to-fine"" approach to model the human voice, which involves three stages: 1) semantic stage: model high-level transformation between linguistic content and self-supervised semantic tokens, 2) acoustic stage: introduce varying control signals as acoustic conditions for semantic-to-acoustic modeling, and 3) generation stage: synthesize high-fidelity waveforms from acoustic tokens. Make-A-Voice offers notable benefits as a unified voice synthesis framework: 1) Data scalability: the major backbone (i.e., acoustic and generation stage) does not require any annotations, and thus the training data could be scaled up. 2) Controllability and conditioning flexibility: we investigate different conditioning mechanisms and effectively handle three voice synthesis applications, including text-to-speech (TTS), voice conversion (VC), and singing voice synthesis (SVS) by re-synthesizing the discrete voice representations with prompt guidance. Experimental results demonstrate that Make-A-Voice exhibits superior audio quality and style similarity compared with competitive baseline models. Audio samples are available at https://Make-A-Voice.github.io",1,,include (junior:5),https://arxiv.org/abs/2305.19269v1,https://arxiv.org/pdf/2305.19269v1,2023,acoustic tokens,offline clustering,"{""arxiv_id"": ""2305.19269"", ""title"": ""Make-A-Voice: Unified Voice Synthesis With Discrete Representation"", ""summary"": ""Various applications of voice synthesis have been developed independently despite the fact that they generate \""voice\"" as output in common. In addition, the majority of voice synthesis models currently rely on annotated audio data, but it is crucial to scale them to self-supervised datasets in order to effectively capture the wide range of acoustic variations present in human voice, including speaker identity, emotion, and prosody. In this work, we propose Make-A-Voice, a unified framework for synthesizing and manipulating voice signals from discrete representations. Make-A-Voice leverages a \""coarse-to-fine\"" approach to model the human voice, which involves three stages: 1) semantic stage: model high-level transformation between linguistic content and self-supervised semantic tokens, 2) acoustic stage: introduce varying control signals as acoustic conditions for semantic-to-acoustic modeling, and 3) generation stage: synthesize high-fidelity waveforms from acoustic tokens. Make-A-Voice offers notable benefits as a unified voice synthesis framework: 1) Data scalability: the major backbone (i.e., acoustic and generation stage) does not require any annotations, and thus the training data could be scaled up. 2) Controllability and conditioning flexibility: we investigate different conditioning mechanisms and effectively handle three voice synthesis applications, including text-to-speech (TTS), voice conversion (VC), and singing voice synthesis (SVS) by re-synthesizing the discrete voice representations with prompt guidance. Experimental results demonstrate that Make-A-Voice exhibits superior audio quality and style similarity compared with competitive baseline models. Audio samples are available at https://Make-A-Voice.github.io"", ""authors"": [""Rongjie Huang"", ""Chunlei Zhang"", ""Yongqi Wang"", ""Dongchao Yang"", ""Luping Liu"", ""Zhenhui Ye"", ""Ziyue Jiang"", ""Chao Weng"", ""Zhou Zhao"", ""Dong Yu""], ""published"": ""2023-05-30T17:59:26Z"", ""updated"": ""2023-05-30T17:59:26Z"", ""categories"": [""eess.AS"", ""cs.AI"", ""cs.CL"", ""cs.SD""], ""pdf_url"": ""https://arxiv.org/pdf/2305.19269v1"", ""landing_url"": ""https://arxiv.org/abs/2305.19269v1"", ""doi"": ""https://doi.org/10.48550/arXiv.2305.19269""}",2305.19269,https://openalex.org/W4378945745
10.1609/aaai.v38i16.29747,UniCATS: A Unified Context-Aware Text-to-Speech Framework with Contextual VQ-Diffusion and Vocoding,"The utilization of discrete speech tokens, divided into semantic tokens and acoustic tokens, has been proven superior to traditional acoustic feature mel-spectrograms in terms of naturalness and robustness for text-to-speech (TTS) synthesis. Recent popular models, such as VALL-E and SPEAR-TTS, allow zero-shot speaker adaptation through auto-regressive (AR) continuation of acoustic tokens extracted from a short speech prompt. However, these AR models are restricted to generate speech only in a left-to-right direction, making them unsuitable for speech editing where both preceding and following contexts are provided. Furthermore, these models rely on acoustic tokens, which have audio quality limitations imposed by the performance of audio codec models. In this study, we propose a unified context-aware TTS framework called UniCATS, which is capable of both speech continuation and editing. UniCATS comprises two components, an acoustic model CTX-txt2vec and a vocoder CTX-vec2wav. CTX-txt2vec employs contextual VQ-diffusion to predict semantic tokens from the input text, enabling it to incorporate the semantic context and maintain seamless concatenation with the surrounding context. Following that, CTX-vec2wav utilizes contextual vocoding to convert these semantic tokens into waveforms, taking into consideration the acoustic context. Our experimental results demonstrate that CTX-vec2wav outperforms HifiGAN and AudioLM in terms of speech resynthesis from semantic tokens. Moreover, we show that UniCATS achieves state-of-the-art performance in both speech continuation and editing.",1,,include (junior:5),https://arxiv.org/abs/2306.07547v6,https://arxiv.org/pdf/2306.07547v6,2023,discrete speech tokens,speech tokens,"{""arxiv_id"": ""2306.07547"", ""title"": ""UniCATS: A Unified Context-Aware Text-to-Speech Framework with Contextual VQ-Diffusion and Vocoding"", ""summary"": ""The utilization of discrete speech tokens, divided into semantic tokens and acoustic tokens, has been proven superior to traditional acoustic feature mel-spectrograms in terms of naturalness and robustness for text-to-speech (TTS) synthesis. Recent popular models, such as VALL-E and SPEAR-TTS, allow zero-shot speaker adaptation through auto-regressive (AR) continuation of acoustic tokens extracted from a short speech prompt. However, these AR models are restricted to generate speech only in a left-to-right direction, making them unsuitable for speech editing where both preceding and following contexts are provided. Furthermore, these models rely on acoustic tokens, which have audio quality limitations imposed by the performance of audio codec models. In this study, we propose a unified context-aware TTS framework called UniCATS, which is capable of both speech continuation and editing. UniCATS comprises two components, an acoustic model CTX-txt2vec and a vocoder CTX-vec2wav. CTX-txt2vec employs contextual VQ-diffusion to predict semantic tokens from the input text, enabling it to incorporate the semantic context and maintain seamless concatenation with the surrounding context. Following that, CTX-vec2wav utilizes contextual vocoding to convert these semantic tokens into waveforms, taking into consideration the acoustic context. Our experimental results demonstrate that CTX-vec2wav outperforms HifiGAN and AudioLM in terms of speech resynthesis from semantic tokens. Moreover, we show that UniCATS achieves state-of-the-art performance in both speech continuation and editing."", ""authors"": [""Chenpeng Du"", ""Yiwei Guo"", ""Feiyu Shen"", ""Zhijun Liu"", ""Zheng Liang"", ""Xie Chen"", ""Shuai Wang"", ""Hui Zhang"", ""Kai Yu""], ""published"": ""2023-06-13T05:38:34Z"", ""updated"": ""2024-03-28T13:56:33Z"", ""categories"": [""cs.SD"", ""eess.AS""], ""pdf_url"": ""https://arxiv.org/pdf/2306.07547v6"", ""landing_url"": ""https://arxiv.org/abs/2306.07547v6"", ""doi"": ""https://doi.org/10.1609/aaai.v38i16.29747""}",2306.07547,https://openalex.org/W4393147067
10.48550/arxiv.2306.08920,Pushing the Limits of Unsupervised Unit Discovery for SSL Speech Representation,"The excellent generalization ability of self-supervised learning (SSL) for speech foundation models has garnered significant attention. HuBERT is a successful example that utilizes offline clustering to convert speech features into discrete units for a masked language modeling pretext task. However, simply clustering features as targets by k-means does not fully inspire the model's performance. In this work, we present an unsupervised method to improve SSL targets. Two models are proposed, MonoBERT and PolyBERT, which leverage context-independent and context-dependent phoneme-based units for pre-training. Our models outperform other SSL models significantly on the LibriSpeech benchmark without the need for iterative re-clustering and re-training. Furthermore, our models equipped with context-dependent units even outperform target-improvement models that use labeled data during pre-training. How we progressively improve the unit discovery process is demonstrated through experiments.",1,,include (junior:5),https://arxiv.org/abs/2306.08920v1,https://arxiv.org/pdf/2306.08920v1,2023,discrete speech tokens,offline clustering,"{""arxiv_id"": ""2306.08920"", ""title"": ""Pushing the Limits of Unsupervised Unit Discovery for SSL Speech Representation"", ""summary"": ""The excellent generalization ability of self-supervised learning (SSL) for speech foundation models has garnered significant attention. HuBERT is a successful example that utilizes offline clustering to convert speech features into discrete units for a masked language modeling pretext task. However, simply clustering features as targets by k-means does not fully inspire the model's performance. In this work, we present an unsupervised method to improve SSL targets. Two models are proposed, MonoBERT and PolyBERT, which leverage context-independent and context-dependent phoneme-based units for pre-training. Our models outperform other SSL models significantly on the LibriSpeech benchmark without the need for iterative re-clustering and re-training. Furthermore, our models equipped with context-dependent units even outperform target-improvement models that use labeled data during pre-training. How we progressively improve the unit discovery process is demonstrated through experiments."", ""authors"": [""Ziyang Ma"", ""Zhisheng Zheng"", ""Guanrou Yang"", ""Yu Wang"", ""Chao Zhang"", ""Xie Chen""], ""published"": ""2023-06-15T07:45:12Z"", ""updated"": ""2023-06-15T07:45:12Z"", ""categories"": [""cs.CL"", ""cs.SD"", ""eess.AS""], ""pdf_url"": ""https://arxiv.org/pdf/2306.08920v1"", ""landing_url"": ""https://arxiv.org/abs/2306.08920v1"", ""doi"": ""https://doi.org/10.48550/arXiv.2306.08920""}",2306.0892,https://openalex.org/W4380994056
10.48550/arxiv.2306.10521,LM-VC: Zero-shot Voice Conversion via Speech Generation based on Language Models,"Language model (LM) based audio generation frameworks, e.g., AudioLM, have recently achieved new state-of-the-art performance in zero-shot audio generation. In this paper, we explore the feasibility of LMs for zero-shot voice conversion. An intuitive approach is to follow AudioLM - Tokenizing speech into semantic and acoustic tokens respectively by HuBERT and SoundStream, and converting source semantic tokens to target acoustic tokens conditioned on acoustic tokens of the target speaker. However, such an approach encounters several issues: 1) the linguistic content contained in semantic tokens may get dispersed during multi-layer modeling while the lengthy speech input in the voice conversion task makes contextual learning even harder; 2) the semantic tokens still contain speaker-related information, which may be leaked to the target speech, lowering the target speaker similarity; 3) the generation diversity in the sampling of the LM can lead to unexpected outcomes during inference, leading to unnatural pronunciation and speech quality degradation. To mitigate these problems, we propose LM-VC, a two-stage language modeling approach that generates coarse acoustic tokens for recovering the source linguistic content and target speaker's timbre, and then reconstructs the fine for acoustic details as converted speech. Specifically, to enhance content preservation and facilitates better disentanglement, a masked prefix LM with a mask prediction strategy is used for coarse acoustic modeling. This model is encouraged to recover the masked content from the surrounding context and generate target speech based on the target speaker's utterance and corrupted semantic tokens. Besides, to further alleviate the sampling error in the generation, an external LM, which employs window attention to capture the local acoustic relations, is introduced to participate in the coarse acoustic modeling.",1,,include (junior:5),https://arxiv.org/abs/2306.10521v2,https://arxiv.org/pdf/2306.10521v2,2023,acoustic tokens,speech tokens,"{""arxiv_id"": ""2306.10521"", ""title"": ""LM-VC: Zero-shot Voice Conversion via Speech Generation based on Language Models"", ""summary"": ""Language model (LM) based audio generation frameworks, e.g., AudioLM, have recently achieved new state-of-the-art performance in zero-shot audio generation. In this paper, we explore the feasibility of LMs for zero-shot voice conversion. An intuitive approach is to follow AudioLM - Tokenizing speech into semantic and acoustic tokens respectively by HuBERT and SoundStream, and converting source semantic tokens to target acoustic tokens conditioned on acoustic tokens of the target speaker. However, such an approach encounters several issues: 1) the linguistic content contained in semantic tokens may get dispersed during multi-layer modeling while the lengthy speech input in the voice conversion task makes contextual learning even harder; 2) the semantic tokens still contain speaker-related information, which may be leaked to the target speech, lowering the target speaker similarity; 3) the generation diversity in the sampling of the LM can lead to unexpected outcomes during inference, leading to unnatural pronunciation and speech quality degradation. To mitigate these problems, we propose LM-VC, a two-stage language modeling approach that generates coarse acoustic tokens for recovering the source linguistic content and target speaker's timbre, and then reconstructs the fine for acoustic details as converted speech. Specifically, to enhance content preservation and facilitates better disentanglement, a masked prefix LM with a mask prediction strategy is used for coarse acoustic modeling. This model is encouraged to recover the masked content from the surrounding context and generate target speech based on the target speaker's utterance and corrupted semantic tokens. Besides, to further alleviate the sampling error in the generation, an external LM, which employs window attention to capture the local acoustic relations, is introduced to participate in the coarse acoustic modeling."", ""authors"": [""Zhichao Wang"", ""Yuanzhe Chen"", ""Lei Xie"", ""Qiao Tian"", ""Yuping Wang""], ""published"": ""2023-06-18T10:59:06Z"", ""updated"": ""2023-08-21T02:21:06Z"", ""categories"": [""eess.AS"", ""cs.SD""], ""pdf_url"": ""https://arxiv.org/pdf/2306.10521v2"", ""landing_url"": ""https://arxiv.org/abs/2306.10521v2"", ""doi"": ""https://doi.org/10.48550/arXiv.2306.10521""}",2306.10521,https://openalex.org/W4381558479
10.48550/arxiv.2307.04686,VampNet: Music Generation via Masked Acoustic Token Modeling,"We introduce VampNet, a masked acoustic token modeling approach to music synthesis, compression, inpainting, and variation. We use a variable masking schedule during training which allows us to sample coherent music from the model by applying a variety of masking approaches (called prompts) during inference. VampNet is non-autoregressive, leveraging a bidirectional transformer architecture that attends to all tokens in a forward pass. With just 36 sampling passes, VampNet can generate coherent high-fidelity musical waveforms. We show that by prompting VampNet in various ways, we can apply it to tasks like music compression, inpainting, outpainting, continuation, and looping with variation (vamping). Appropriately prompted, VampNet is capable of maintaining style, genre, instrumentation, and other high-level aspects of the music. This flexible prompting capability makes VampNet a powerful music co-creation tool. Code and audio samples are available online.",1,,include (junior:4),https://arxiv.org/abs/2307.04686v2,https://arxiv.org/pdf/2307.04686v2,2023,acoustic tokens,speech tokens,"{""arxiv_id"": ""2307.04686"", ""title"": ""VampNet: Music Generation via Masked Acoustic Token Modeling"", ""summary"": ""We introduce VampNet, a masked acoustic token modeling approach to music synthesis, compression, inpainting, and variation. We use a variable masking schedule during training which allows us to sample coherent music from the model by applying a variety of masking approaches (called prompts) during inference. VampNet is non-autoregressive, leveraging a bidirectional transformer architecture that attends to all tokens in a forward pass. With just 36 sampling passes, VampNet can generate coherent high-fidelity musical waveforms. We show that by prompting VampNet in various ways, we can apply it to tasks like music compression, inpainting, outpainting, continuation, and looping with variation (vamping). Appropriately prompted, VampNet is capable of maintaining style, genre, instrumentation, and other high-level aspects of the music. This flexible prompting capability makes VampNet a powerful music co-creation tool. Code and audio samples are available online."", ""authors"": [""Hugo Flores Garcia"", ""Prem Seetharaman"", ""Rithesh Kumar"", ""Bryan Pardo""], ""published"": ""2023-07-10T16:42:03Z"", ""updated"": ""2023-07-12T17:06:41Z"", ""categories"": [""cs.SD"", ""cs.AI"", ""eess.AS""], ""pdf_url"": ""https://arxiv.org/pdf/2307.04686v2"", ""landing_url"": ""https://arxiv.org/abs/2307.04686v2"", ""doi"": ""https://doi.org/10.48550/arXiv.2307.04686""}",2307.04686,https://openalex.org/W4383993968
10.48550/arxiv.2308.16692,SpeechTokenizer: Unified Speech Tokenizer for Speech Large Language Models,"Current speech large language models build upon discrete speech representations, which can be categorized into semantic tokens and acoustic tokens. However, existing speech tokens are not specifically designed for speech language modeling. To assess the suitability of speech tokens for building speech language models, we established the first benchmark, SLMTokBench. Our results indicate that neither semantic nor acoustic tokens are ideal for this purpose. Therefore, we propose SpeechTokenizer, a unified speech tokenizer for speech large language models. SpeechTokenizer adopts the Encoder-Decoder architecture with residual vector quantization (RVQ). Unifying semantic and acoustic tokens, SpeechTokenizer disentangles different aspects of speech information hierarchically across different RVQ layers. Furthermore, We construct a Unified Speech Language Model (USLM) leveraging SpeechTokenizer. Experiments show that SpeechTokenizer performs comparably to EnCodec in speech reconstruction and demonstrates strong performance on the SLMTokBench benchmark. Also, USLM outperforms VALL-E in zero-shot Text-to-Speech tasks. Code and models are available at https://github.com/ZhangXInFD/SpeechTokenizer/.",1,,include (junior:5),https://arxiv.org/abs/2308.16692v2,https://arxiv.org/pdf/2308.16692v2,2023,discrete speech tokens,speech tokens,"{""arxiv_id"": ""2308.16692"", ""title"": ""SpeechTokenizer: Unified Speech Tokenizer for Speech Large Language Models"", ""summary"": ""Current speech large language models build upon discrete speech representations, which can be categorized into semantic tokens and acoustic tokens. However, existing speech tokens are not specifically designed for speech language modeling. To assess the suitability of speech tokens for building speech language models, we established the first benchmark, SLMTokBench. Our results indicate that neither semantic nor acoustic tokens are ideal for this purpose. Therefore, we propose SpeechTokenizer, a unified speech tokenizer for speech large language models. SpeechTokenizer adopts the Encoder-Decoder architecture with residual vector quantization (RVQ). Unifying semantic and acoustic tokens, SpeechTokenizer disentangles different aspects of speech information hierarchically across different RVQ layers. Furthermore, We construct a Unified Speech Language Model (USLM) leveraging SpeechTokenizer. Experiments show that SpeechTokenizer performs comparably to EnCodec in speech reconstruction and demonstrates strong performance on the SLMTokBench benchmark. Also, USLM outperforms VALL-E in zero-shot Text-to-Speech tasks. Code and models are available at https://github.com/ZhangXInFD/SpeechTokenizer/."", ""authors"": [""Xin Zhang"", ""Dong Zhang"", ""Shimin Li"", ""Yaqian Zhou"", ""Xipeng Qiu""], ""published"": ""2023-08-31T12:53:09Z"", ""updated"": ""2024-01-23T01:56:57Z"", ""categories"": [""cs.CL"", ""cs.SD"", ""eess.AS""], ""pdf_url"": ""https://arxiv.org/pdf/2308.16692v2"", ""landing_url"": ""https://arxiv.org/abs/2308.16692v2"", ""doi"": ""https://doi.org/10.48550/arXiv.2308.16692""}",2308.16692,https://openalex.org/W4386384714
10.48550/arxiv.2309.00126,QS-TTS: Towards Semi-Supervised Text-to-Speech Synthesis via Vector-Quantized Self-Supervised Speech Representation Learning,"This paper proposes a novel semi-supervised TTS framework, QS-TTS, to improve TTS quality with lower supervised data requirements via Vector-Quantized Self-Supervised Speech Representation Learning (VQ-S3RL) utilizing more unlabeled speech audio. This framework comprises two VQ-S3R learners: first, the principal learner aims to provide a generative Multi-Stage Multi-Codebook (MSMC) VQ-S3R via the MSMC-VQ-GAN combined with the contrastive S3RL, while decoding it back to the high-quality audio; then, the associate learner further abstracts the MSMC representation into a highly-compact VQ representation through a VQ-VAE. These two generative VQ-S3R learners provide profitable speech representations and pre-trained models for TTS, significantly improving synthesis quality with the lower requirement for supervised data. QS-TTS is evaluated comprehensively under various scenarios via subjective and objective tests in experiments. The results powerfully demonstrate the superior performance of QS-TTS, winning the highest MOS over supervised or semi-supervised baseline TTS approaches, especially in low-resource scenarios. Moreover, comparing various speech representations and transfer learning methods in TTS further validates the notable improvement of the proposed VQ-S3RL to TTS, showing the best audio quality and intelligibility metrics. The trend of slower decay in the synthesis quality of QS-TTS with decreasing supervised data further highlights its lower requirements for supervised data, indicating its great potential in low-resource scenarios.",1,,include (junior:5),https://arxiv.org/abs/2309.00126v1,https://arxiv.org/pdf/2309.00126v1,2023,discrete speech tokens,vq gan,"{""arxiv_id"": ""2309.00126"", ""title"": ""QS-TTS: Towards Semi-Supervised Text-to-Speech Synthesis via Vector-Quantized Self-Supervised Speech Representation Learning"", ""summary"": ""This paper proposes a novel semi-supervised TTS framework, QS-TTS, to improve TTS quality with lower supervised data requirements via Vector-Quantized Self-Supervised Speech Representation Learning (VQ-S3RL) utilizing more unlabeled speech audio. This framework comprises two VQ-S3R learners: first, the principal learner aims to provide a generative Multi-Stage Multi-Codebook (MSMC) VQ-S3R via the MSMC-VQ-GAN combined with the contrastive S3RL, while decoding it back to the high-quality audio; then, the associate learner further abstracts the MSMC representation into a highly-compact VQ representation through a VQ-VAE. These two generative VQ-S3R learners provide profitable speech representations and pre-trained models for TTS, significantly improving synthesis quality with the lower requirement for supervised data. QS-TTS is evaluated comprehensively under various scenarios via subjective and objective tests in experiments. The results powerfully demonstrate the superior performance of QS-TTS, winning the highest MOS over supervised or semi-supervised baseline TTS approaches, especially in low-resource scenarios. Moreover, comparing various speech representations and transfer learning methods in TTS further validates the notable improvement of the proposed VQ-S3RL to TTS, showing the best audio quality and intelligibility metrics. The trend of slower decay in the synthesis quality of QS-TTS with decreasing supervised data further highlights its lower requirements for supervised data, indicating its great potential in low-resource scenarios."", ""authors"": [""Haohan Guo"", ""Fenglong Xie"", ""Jiawen Kang"", ""Yujia Xiao"", ""Xixin Wu"", ""Helen Meng""], ""published"": ""2023-08-31T20:25:44Z"", ""updated"": ""2023-08-31T20:25:44Z"", ""categories"": [""cs.SD"", ""cs.CL"", ""eess.AS""], ""pdf_url"": ""https://arxiv.org/pdf/2309.00126v1"", ""landing_url"": ""https://arxiv.org/abs/2309.00126v1"", ""doi"": ""https://doi.org/10.48550/arXiv.2309.00126""}",2309.00126,https://openalex.org/W4386435838
10.48550/arxiv.2309.00169,RepCodec: A Speech Representation Codec for Speech Tokenization,"With recent rapid growth of large language models (LLMs), discrete speech tokenization has played an important role for injecting speech into LLMs. However, this discretization gives rise to a loss of information, consequently impairing overall performance. To improve the performance of these discrete speech tokens, we present RepCodec, a novel speech representation codec for semantic speech tokenization. In contrast to audio codecs which reconstruct the raw audio, RepCodec learns a vector quantization codebook through reconstructing speech representations from speech encoders like HuBERT or data2vec. Together, the speech encoder, the codec encoder and the vector quantization codebook form a pipeline for converting speech waveforms into semantic tokens. The extensive experiments illustrate that RepCodec, by virtue of its enhanced information retention capacity, significantly outperforms the widely used k-means clustering approach in both speech understanding and generation. Furthermore, this superiority extends across various speech encoders and languages, affirming the robustness of RepCodec. We believe our method can facilitate large language modeling research on speech processing.",1,,include (junior:5),https://arxiv.org/abs/2309.00169v3,https://arxiv.org/pdf/2309.00169v3,2023,discrete speech tokens,speech tokens,"{""arxiv_id"": ""2309.00169"", ""title"": ""RepCodec: A Speech Representation Codec for Speech Tokenization"", ""summary"": ""With recent rapid growth of large language models (LLMs), discrete speech tokenization has played an important role for injecting speech into LLMs. However, this discretization gives rise to a loss of information, consequently impairing overall performance. To improve the performance of these discrete speech tokens, we present RepCodec, a novel speech representation codec for semantic speech tokenization. In contrast to audio codecs which reconstruct the raw audio, RepCodec learns a vector quantization codebook through reconstructing speech representations from speech encoders like HuBERT or data2vec. Together, the speech encoder, the codec encoder and the vector quantization codebook form a pipeline for converting speech waveforms into semantic tokens. The extensive experiments illustrate that RepCodec, by virtue of its enhanced information retention capacity, significantly outperforms the widely used k-means clustering approach in both speech understanding and generation. Furthermore, this superiority extends across various speech encoders and languages, affirming the robustness of RepCodec. We believe our method can facilitate large language modeling research on speech processing."", ""authors"": [""Zhichao Huang"", ""Chutong Meng"", ""Tom Ko""], ""published"": ""2023-08-31T23:26:10Z"", ""updated"": ""2024-07-22T09:53:44Z"", ""categories"": [""eess.AS"", ""cs.LG"", ""cs.SD""], ""pdf_url"": ""https://arxiv.org/pdf/2309.00169v3"", ""landing_url"": ""https://arxiv.org/abs/2309.00169v3"", ""doi"": ""https://doi.org/10.48550/arXiv.2309.00169""}",2309.00169,https://openalex.org/W4386437507
10.1109/lsp.2023.3313513,Direct Text to Speech Translation System using Acoustic Units,"This paper proposes a direct text to speech translation system using discrete acoustic units. This framework employs text in different source languages as input to generate speech in the target language without the need for text transcriptions in this language. Motivated by the success of acoustic units in previous works for direct speech to speech translation systems, we use the same pipeline to extract the acoustic units using a speech encoder combined with a clustering algorithm. Once units are obtained, an encoder-decoder architecture is trained to predict them. Then a vocoder generates speech from units. Our approach for direct text to speech translation was tested on the new CVSS corpus with two different text mBART models employed as initialisation. The systems presented report competitive performance for most of the language pairs evaluated. Besides, results show a remarkable improvement when initialising our proposed architecture with a model pre-trained with more languages.",1,,include (junior:5),https://arxiv.org/abs/2309.07478v1,https://arxiv.org/pdf/2309.07478v1,2023,discrete speech tokens,text to speech,"{""arxiv_id"": ""2309.07478"", ""title"": ""Direct Text to Speech Translation System using Acoustic Units"", ""summary"": ""This paper proposes a direct text to speech translation system using discrete acoustic units. This framework employs text in different source languages as input to generate speech in the target language without the need for text transcriptions in this language. Motivated by the success of acoustic units in previous works for direct speech to speech translation systems, we use the same pipeline to extract the acoustic units using a speech encoder combined with a clustering algorithm. Once units are obtained, an encoder-decoder architecture is trained to predict them. Then a vocoder generates speech from units. Our approach for direct text to speech translation was tested on the new CVSS corpus with two different text mBART models employed as initialisation. The systems presented report competitive performance for most of the language pairs evaluated. Besides, results show a remarkable improvement when initialising our proposed architecture with a model pre-trained with more languages."", ""authors"": [""Victoria Mingote"", ""Pablo Gimeno"", ""Luis Vicente"", ""Sameer Khurana"", ""Antoine Laurent"", ""Jarod Duret""], ""published"": ""2023-09-14T07:35:14Z"", ""updated"": ""2023-09-14T07:35:14Z"", ""categories"": [""cs.CL"", ""cs.LG"", ""cs.SD"", ""eess.AS""], ""pdf_url"": ""https://arxiv.org/pdf/2309.07478v1"", ""landing_url"": ""https://arxiv.org/abs/2309.07478v1"", ""doi"": ""https://doi.org/10.1109/LSP.2023.3313513""}",2309.07478,https://openalex.org/W4386590854
10.48550/arxiv.2309.07937,Voxtlm: unified decoder-only models for consolidating speech recognition/synthesis and speech/text continuation tasks,"We propose a decoder-only language model, VoxtLM, that can perform four tasks: speech recognition, speech synthesis, text generation, and speech continuation. VoxtLM integrates text vocabulary with discrete speech tokens from self-supervised speech features and uses special tokens to enable multitask learning. Compared to a single-task model, VoxtLM exhibits a significant improvement in speech synthesis, with improvements in both speech intelligibility from 28.9 to 5.6 and objective quality from 2.68 to 3.90. VoxtLM also improves speech generation and speech recognition performance over the single-task counterpart. Further, VoxtLM is trained with publicly available data and training recipes and model checkpoints are open-sourced to make fully reproducible work.",1,,include (junior:5),https://arxiv.org/abs/2309.07937v3,https://arxiv.org/pdf/2309.07937v3,2023,discrete speech tokens,speech tokens,"{""arxiv_id"": ""2309.07937"", ""title"": ""Voxtlm: unified decoder-only models for consolidating speech recognition/synthesis and speech/text continuation tasks"", ""summary"": ""We propose a decoder-only language model, VoxtLM, that can perform four tasks: speech recognition, speech synthesis, text generation, and speech continuation. VoxtLM integrates text vocabulary with discrete speech tokens from self-supervised speech features and uses special tokens to enable multitask learning. Compared to a single-task model, VoxtLM exhibits a significant improvement in speech synthesis, with improvements in both speech intelligibility from 28.9 to 5.6 and objective quality from 2.68 to 3.90. VoxtLM also improves speech generation and speech recognition performance over the single-task counterpart. Further, VoxtLM is trained with publicly available data and training recipes and model checkpoints are open-sourced to make fully reproducible work."", ""authors"": [""Soumi Maiti"", ""Yifan Peng"", ""Shukjae Choi"", ""Jee-weon Jung"", ""Xuankai Chang"", ""Shinji Watanabe""], ""published"": ""2023-09-14T03:13:18Z"", ""updated"": ""2024-01-24T15:36:31Z"", ""categories"": [""eess.AS"", ""cs.LG"", ""cs.SD""], ""pdf_url"": ""https://arxiv.org/pdf/2309.07937v3"", ""landing_url"": ""https://arxiv.org/abs/2309.07937v3"", ""doi"": ""https://doi.org/10.48550/arXiv.2309.07937""}",2309.07937,https://openalex.org/W4386874750
10.48550/arxiv.2309.11977,Improving Language Model-Based Zero-Shot Text-to-Speech Synthesis with Multi-Scale Acoustic Prompts,"Zero-shot text-to-speech (TTS) synthesis aims to clone any unseen speaker's voice without adaptation parameters. By quantizing speech waveform into discrete acoustic tokens and modeling these tokens with the language model, recent language model-based TTS models show zero-shot speaker adaptation capabilities with only a 3-second acoustic prompt of an unseen speaker. However, they are limited by the length of the acoustic prompt, which makes it difficult to clone personal speaking style. In this paper, we propose a novel zero-shot TTS model with the multi-scale acoustic prompts based on a neural codec language model VALL-E. A speaker-aware text encoder is proposed to learn the personal speaking style at the phoneme-level from the style prompt consisting of multiple sentences. Following that, a VALL-E based acoustic decoder is utilized to model the timbre from the timbre prompt at the frame-level and generate speech. The experimental results show that our proposed method outperforms baselines in terms of naturalness and speaker similarity, and can achieve better performance by scaling out to a longer style prompt.",1,,include (junior:5),https://arxiv.org/abs/2309.11977v3,https://arxiv.org/pdf/2309.11977v3,2023,acoustic tokens,offline clustering,"{""arxiv_id"": ""2309.11977"", ""title"": ""Improving Language Model-Based Zero-Shot Text-to-Speech Synthesis with Multi-Scale Acoustic Prompts"", ""summary"": ""Zero-shot text-to-speech (TTS) synthesis aims to clone any unseen speaker's voice without adaptation parameters. By quantizing speech waveform into discrete acoustic tokens and modeling these tokens with the language model, recent language model-based TTS models show zero-shot speaker adaptation capabilities with only a 3-second acoustic prompt of an unseen speaker. However, they are limited by the length of the acoustic prompt, which makes it difficult to clone personal speaking style. In this paper, we propose a novel zero-shot TTS model with the multi-scale acoustic prompts based on a neural codec language model VALL-E. A speaker-aware text encoder is proposed to learn the personal speaking style at the phoneme-level from the style prompt consisting of multiple sentences. Following that, a VALL-E based acoustic decoder is utilized to model the timbre from the timbre prompt at the frame-level and generate speech. The experimental results show that our proposed method outperforms baselines in terms of naturalness and speaker similarity, and can achieve better performance by scaling out to a longer style prompt."", ""authors"": [""Shun Lei"", ""Yixuan Zhou"", ""Liyang Chen"", ""Dan Luo"", ""Zhiyong Wu"", ""Xixin Wu"", ""Shiyin Kang"", ""Tao Jiang"", ""Yahui Zhou"", ""Yuxing Han"", ""Helen Meng""], ""published"": ""2023-09-21T11:22:22Z"", ""updated"": ""2024-04-09T08:39:52Z"", ""categories"": [""cs.SD"", ""eess.AS""], ""pdf_url"": ""https://arxiv.org/pdf/2309.11977v3"", ""landing_url"": ""https://arxiv.org/abs/2309.11977v3"", ""doi"": ""https://doi.org/10.48550/arXiv.2309.11977""}",2309.11977,https://openalex.org/W4386977793
10.48550/arxiv.2309.14324,Towards General-Purpose Text-Instruction-Guided Voice Conversion,"This paper introduces a novel voice conversion (VC) model, guided by text instructions such as ""articulate slowly with a deep tone"" or ""speak in a cheerful boyish voice"". Unlike traditional methods that rely on reference utterances to determine the attributes of the converted speech, our model adds versatility and specificity to voice conversion. The proposed VC model is a neural codec language model which processes a sequence of discrete codes, resulting in the code sequence of converted speech. It utilizes text instructions as style prompts to modify the prosody and emotional information of the given speech. In contrast to previous approaches, which often rely on employing separate encoders like prosody and content encoders to handle different aspects of the source speech, our model handles various information of speech in an end-to-end manner. Experiments have demonstrated the impressive capabilities of our model in comprehending instructions and delivering reasonable results.",1,,include (junior:5),https://arxiv.org/abs/2309.14324v2,https://arxiv.org/pdf/2309.14324v2,2023,discrete speech tokens,voice conversion,"{""arxiv_id"": ""2309.14324"", ""title"": ""Towards General-Purpose Text-Instruction-Guided Voice Conversion"", ""summary"": ""This paper introduces a novel voice conversion (VC) model, guided by text instructions such as \""articulate slowly with a deep tone\"" or \""speak in a cheerful boyish voice\"". Unlike traditional methods that rely on reference utterances to determine the attributes of the converted speech, our model adds versatility and specificity to voice conversion. The proposed VC model is a neural codec language model which processes a sequence of discrete codes, resulting in the code sequence of converted speech. It utilizes text instructions as style prompts to modify the prosody and emotional information of the given speech. In contrast to previous approaches, which often rely on employing separate encoders like prosody and content encoders to handle different aspects of the source speech, our model handles various information of speech in an end-to-end manner. Experiments have demonstrated the impressive capabilities of our model in comprehending instructions and delivering reasonable results."", ""authors"": [""Chun-Yi Kuan"", ""Chen An Li"", ""Tsu-Yuan Hsu"", ""Tse-Yang Lin"", ""Ho-Lam Chung"", ""Kai-Wei Chang"", ""Shuo-yiin Chang"", ""Hung-yi Lee""], ""published"": ""2023-09-25T17:52:09Z"", ""updated"": ""2024-01-16T13:53:56Z"", ""categories"": [""eess.AS"", ""cs.CL"", ""cs.LG"", ""cs.SD""], ""pdf_url"": ""https://arxiv.org/pdf/2309.14324v2"", ""landing_url"": ""https://arxiv.org/abs/2309.14324v2"", ""doi"": ""https://doi.org/10.48550/arXiv.2309.14324""}",2309.14324,https://openalex.org/W4387076541
10.48550/arxiv.2310.07246,Vec-Tok Speech: speech vectorization and tokenization for neural speech generation,"Language models (LMs) have recently flourished in natural language processing and computer vision, generating high-fidelity texts or images in various tasks. In contrast, the current speech generative models are still struggling regarding speech quality and task generalization. This paper presents Vec-Tok Speech, an extensible framework that resembles multiple speech generation tasks, generating expressive and high-fidelity speech. Specifically, we propose a novel speech codec based on speech vectors and semantic tokens. Speech vectors contain acoustic details contributing to high-fidelity speech reconstruction, while semantic tokens focus on the linguistic content of speech, facilitating language modeling. Based on the proposed speech codec, Vec-Tok Speech leverages an LM to undertake the core of speech generation. Moreover, Byte-Pair Encoding (BPE) is introduced to reduce the token length and bit rate for lower exposure bias and longer context coverage, improving the performance of LMs. Vec-Tok Speech can be used for intra- and cross-lingual zero-shot voice conversion (VC), zero-shot speaking style transfer text-to-speech (TTS), speech-to-speech translation (S2ST), speech denoising, and speaker de-identification and anonymization. Experiments show that Vec-Tok Speech, built on 50k hours of speech, performs better than other SOTA models. Code will be available at https://github.com/BakerBunker/VecTok .",1,,include (junior:5),https://arxiv.org/abs/2310.07246v2,https://arxiv.org/pdf/2310.07246v2,2023,discrete speech tokens,speech generation,"{""arxiv_id"": ""2310.07246"", ""title"": ""Vec-Tok Speech: speech vectorization and tokenization for neural speech generation"", ""summary"": ""Language models (LMs) have recently flourished in natural language processing and computer vision, generating high-fidelity texts or images in various tasks. In contrast, the current speech generative models are still struggling regarding speech quality and task generalization. This paper presents Vec-Tok Speech, an extensible framework that resembles multiple speech generation tasks, generating expressive and high-fidelity speech. Specifically, we propose a novel speech codec based on speech vectors and semantic tokens. Speech vectors contain acoustic details contributing to high-fidelity speech reconstruction, while semantic tokens focus on the linguistic content of speech, facilitating language modeling. Based on the proposed speech codec, Vec-Tok Speech leverages an LM to undertake the core of speech generation. Moreover, Byte-Pair Encoding (BPE) is introduced to reduce the token length and bit rate for lower exposure bias and longer context coverage, improving the performance of LMs. Vec-Tok Speech can be used for intra- and cross-lingual zero-shot voice conversion (VC), zero-shot speaking style transfer text-to-speech (TTS), speech-to-speech translation (S2ST), speech denoising, and speaker de-identification and anonymization. Experiments show that Vec-Tok Speech, built on 50k hours of speech, performs better than other SOTA models. Code will be available at https://github.com/BakerBunker/VecTok ."", ""authors"": [""Xinfa Zhu"", ""Yuanjun Lv"", ""Yi Lei"", ""Tao Li"", ""Wendi He"", ""Hongbin Zhou"", ""Heng Lu"", ""Lei Xie""], ""published"": ""2023-10-11T07:23:27Z"", ""updated"": ""2023-10-12T05:49:52Z"", ""categories"": [""cs.SD"", ""eess.AS""], ""pdf_url"": ""https://arxiv.org/pdf/2310.07246v2"", ""landing_url"": ""https://arxiv.org/abs/2310.07246v2"", ""doi"": ""https://doi.org/10.48550/arXiv.2310.07246""}",2310.07246,https://openalex.org/W4387634372
10.48550/arxiv.2310.08981,Low-latency Speech Enhancement via Speech Token Generation,"Existing deep learning based speech enhancement mainly employ a data-driven approach, which leverage large amounts of data with a variety of noise types to achieve noise removal from noisy signal. However, the high dependence on the data limits its generalization on the unseen complex noises in real-life environment. In this paper, we focus on the low-latency scenario and regard speech enhancement as a speech generation problem conditioned on the noisy signal, where we generate clean speech instead of identifying and removing noises. Specifically, we propose a conditional generative framework for speech enhancement, which models clean speech by acoustic codes of a neural speech codec and generates the speech codes conditioned on past noisy frames in an auto-regressive way. Moreover, we propose an explicit-alignment approach to align noisy frames with the generated speech tokens to improve the robustness and scalability to different input lengths. Different from other methods that leverage multiple stages to generate speech codes, we leverage a single-stage speech generation approach based on the TF-Codec neural codec to achieve high speech quality with low latency. Extensive results on both synthetic and real-recorded test set show its superiority over data-driven approaches in terms of noise robustness and temporal speech coherence.",1,,include (senior:5),https://arxiv.org/abs/2310.08981v3,https://arxiv.org/pdf/2310.08981v3,2023,discrete speech tokens,speech tokens,"{""arxiv_id"": ""2310.08981"", ""title"": ""Low-latency Speech Enhancement via Speech Token Generation"", ""summary"": ""Existing deep learning based speech enhancement mainly employ a data-driven approach, which leverage large amounts of data with a variety of noise types to achieve noise removal from noisy signal. However, the high dependence on the data limits its generalization on the unseen complex noises in real-life environment. In this paper, we focus on the low-latency scenario and regard speech enhancement as a speech generation problem conditioned on the noisy signal, where we generate clean speech instead of identifying and removing noises. Specifically, we propose a conditional generative framework for speech enhancement, which models clean speech by acoustic codes of a neural speech codec and generates the speech codes conditioned on past noisy frames in an auto-regressive way. Moreover, we propose an explicit-alignment approach to align noisy frames with the generated speech tokens to improve the robustness and scalability to different input lengths. Different from other methods that leverage multiple stages to generate speech codes, we leverage a single-stage speech generation approach based on the TF-Codec neural codec to achieve high speech quality with low latency. Extensive results on both synthetic and real-recorded test set show its superiority over data-driven approaches in terms of noise robustness and temporal speech coherence."", ""authors"": [""Huaying Xue"", ""Xiulian Peng"", ""Yan Lu""], ""published"": ""2023-10-13T09:57:09Z"", ""updated"": ""2024-01-23T06:13:04Z"", ""categories"": [""cs.SD"", ""cs.MM"", ""eess.AS""], ""pdf_url"": ""https://arxiv.org/pdf/2310.08981v3"", ""landing_url"": ""https://arxiv.org/abs/2310.08981v3"", ""doi"": ""https://doi.org/10.48550/arXiv.2310.08981""}",2310.08981,https://openalex.org/W4387687749
10.48550/arxiv.2310.14044,Composer Style-specific Symbolic Music Generation Using Vector Quantized Discrete Diffusion Models,"Emerging Denoising Diffusion Probabilistic Models (DDPM) have become increasingly utilised because of promising results they have achieved in diverse generative tasks with continuous data, such as image and sound synthesis. Nonetheless, the success of diffusion models has not been fully extended to discrete symbolic music. We propose to combine a vector quantized variational autoencoder (VQ-VAE) and discrete diffusion models for the generation of symbolic music with desired composer styles. The trained VQ-VAE can represent symbolic music as a sequence of indexes that correspond to specific entries in a learned codebook. Subsequently, a discrete diffusion model is used to model the VQ-VAE's discrete latent space. The diffusion model is trained to generate intermediate music sequences consisting of codebook indexes, which are then decoded to symbolic music using the VQ-VAE's decoder. The evaluation results demonstrate our model can generate symbolic music with target composer styles that meet the given conditions with a high accuracy of 72.36%. Our code is available at https://github.com/jinchengzhanggg/VQVAE-Diffusion.",1,,include (junior:5),https://arxiv.org/abs/2310.14044v2,https://arxiv.org/pdf/2310.14044v2,2023,discrete speech tokens,vq vae,"{""arxiv_id"": ""2310.14044"", ""title"": ""Composer Style-specific Symbolic Music Generation Using Vector Quantized Discrete Diffusion Models"", ""summary"": ""Emerging Denoising Diffusion Probabilistic Models (DDPM) have become increasingly utilised because of promising results they have achieved in diverse generative tasks with continuous data, such as image and sound synthesis. Nonetheless, the success of diffusion models has not been fully extended to discrete symbolic music. We propose to combine a vector quantized variational autoencoder (VQ-VAE) and discrete diffusion models for the generation of symbolic music with desired composer styles. The trained VQ-VAE can represent symbolic music as a sequence of indexes that correspond to specific entries in a learned codebook. Subsequently, a discrete diffusion model is used to model the VQ-VAE's discrete latent space. The diffusion model is trained to generate intermediate music sequences consisting of codebook indexes, which are then decoded to symbolic music using the VQ-VAE's decoder. The evaluation results demonstrate our model can generate symbolic music with target composer styles that meet the given conditions with a high accuracy of 72.36%. Our code is available at https://github.com/jinchengzhanggg/VQVAE-Diffusion."", ""authors"": [""Jincheng Zhang"", ""György Fazekas"", ""Charalampos Saitis""], ""published"": ""2023-10-21T15:41:50Z"", ""updated"": ""2024-09-03T19:14:25Z"", ""categories"": [""cs.SD"", ""cs.AI"", ""eess.AS""], ""pdf_url"": ""https://arxiv.org/pdf/2310.14044v2"", ""landing_url"": ""https://arxiv.org/abs/2310.14044v2"", ""doi"": ""https://doi.org/10.48550/arXiv.2310.14044""}",2310.14044,https://openalex.org/W4387928711
10.48550/arxiv.2310.14580,Acoustic BPE for Speech Generation with Discrete Tokens,"Discrete audio tokens derived from self-supervised learning models have gained widespread usage in speech generation. However, current practice of directly utilizing audio tokens poses challenges for sequence modeling due to the length of the token sequence. Additionally, this approach places the burden on the model to establish correlations between tokens, further complicating the modeling process. To address this issue, we propose acoustic BPE which encodes frequent audio token patterns by utilizing byte-pair encoding. Acoustic BPE effectively reduces the sequence length and leverages the prior morphological information present in token sequence, which alleviates the modeling challenges of token correlation. Through comprehensive investigations on a speech language model trained with acoustic BPE, we confirm the notable advantages it offers, including faster inference and improved syntax capturing capabilities. In addition, we propose a novel rescore method to select the optimal synthetic speech among multiple candidates generated by rich-diversity TTS system. Experiments prove that rescore selection aligns closely with human preference, which highlights acoustic BPE's potential to other speech generation tasks.",1,,include (junior:5),https://arxiv.org/abs/2310.14580v4,https://arxiv.org/pdf/2310.14580v4,2023,discrete speech tokens,acoustic bpe,"{""arxiv_id"": ""2310.14580"", ""title"": ""Acoustic BPE for Speech Generation with Discrete Tokens"", ""summary"": ""Discrete audio tokens derived from self-supervised learning models have gained widespread usage in speech generation. However, current practice of directly utilizing audio tokens poses challenges for sequence modeling due to the length of the token sequence. Additionally, this approach places the burden on the model to establish correlations between tokens, further complicating the modeling process. To address this issue, we propose acoustic BPE which encodes frequent audio token patterns by utilizing byte-pair encoding. Acoustic BPE effectively reduces the sequence length and leverages the prior morphological information present in token sequence, which alleviates the modeling challenges of token correlation. Through comprehensive investigations on a speech language model trained with acoustic BPE, we confirm the notable advantages it offers, including faster inference and improved syntax capturing capabilities. In addition, we propose a novel rescore method to select the optimal synthetic speech among multiple candidates generated by rich-diversity TTS system. Experiments prove that rescore selection aligns closely with human preference, which highlights acoustic BPE's potential to other speech generation tasks."", ""authors"": [""Feiyu Shen"", ""Yiwei Guo"", ""Chenpeng Du"", ""Xie Chen"", ""Kai Yu""], ""published"": ""2023-10-23T05:38:41Z"", ""updated"": ""2024-01-15T05:53:31Z"", ""categories"": [""cs.SD"", ""eess.AS""], ""pdf_url"": ""https://arxiv.org/pdf/2310.14580v4"", ""landing_url"": ""https://arxiv.org/abs/2310.14580v4"", ""doi"": ""https://doi.org/10.48550/arXiv.2310.14580""}",2310.1458,https://openalex.org/W4387929314
10.48550/arxiv.2311.02898,Transduce and Speak: Neural Transducer for Text-to-Speech with Semantic Token Prediction,"We introduce a text-to-speech(TTS) framework based on a neural transducer. We use discretized semantic tokens acquired from wav2vec2.0 embeddings, which makes it easy to adopt a neural transducer for the TTS framework enjoying its monotonic alignment constraints. The proposed model first generates aligned semantic tokens using the neural transducer, then synthesizes a speech sample from the semantic tokens using a non-autoregressive(NAR) speech generator. This decoupled framework alleviates the training complexity of TTS and allows each stage to focus on 1) linguistic and alignment modeling and 2) fine-grained acoustic modeling, respectively. Experimental results on the zero-shot adaptive TTS show that the proposed model exceeds the baselines in speech quality and speaker similarity via objective and subjective measures. We also investigate the inference speed and prosody controllability of our proposed model, showing the potential of the neural transducer for TTS frameworks.",1,,include (junior:5),https://arxiv.org/abs/2311.02898v2,https://arxiv.org/pdf/2311.02898v2,2023,semantic tokens,speech tokens,"{""arxiv_id"": ""2311.02898"", ""title"": ""Transduce and Speak: Neural Transducer for Text-to-Speech with Semantic Token Prediction"", ""summary"": ""We introduce a text-to-speech(TTS) framework based on a neural transducer. We use discretized semantic tokens acquired from wav2vec2.0 embeddings, which makes it easy to adopt a neural transducer for the TTS framework enjoying its monotonic alignment constraints. The proposed model first generates aligned semantic tokens using the neural transducer, then synthesizes a speech sample from the semantic tokens using a non-autoregressive(NAR) speech generator. This decoupled framework alleviates the training complexity of TTS and allows each stage to focus on 1) linguistic and alignment modeling and 2) fine-grained acoustic modeling, respectively. Experimental results on the zero-shot adaptive TTS show that the proposed model exceeds the baselines in speech quality and speaker similarity via objective and subjective measures. We also investigate the inference speed and prosody controllability of our proposed model, showing the potential of the neural transducer for TTS frameworks."", ""authors"": [""Minchan Kim"", ""Myeonghun Jeong"", ""Byoung Jin Choi"", ""Dongjune Lee"", ""Nam Soo Kim""], ""published"": ""2023-11-06T06:13:39Z"", ""updated"": ""2023-11-08T05:52:39Z"", ""categories"": [""eess.AS"", ""cs.LG""], ""pdf_url"": ""https://arxiv.org/pdf/2311.02898v2"", ""landing_url"": ""https://arxiv.org/abs/2311.02898v2"", ""doi"": ""https://doi.org/10.48550/arXiv.2311.02898""}",2311.02898,https://openalex.org/W4388512545
10.48550/arxiv.2311.04534,Loss Masking Is Not Needed in Decoder-only Transformer for Discrete-token-based ASR,"Recently, unified speech-text models, such as SpeechGPT, VioLA, and AudioPaLM, have achieved remarkable performance on various speech tasks. These models discretize speech signals into tokens (speech discretization) and use a shared vocabulary for both text and speech tokens. Then they train a single decoder-only Transformer on a mixture of speech tasks. However, these models rely on the Loss Masking strategy for the ASR task, which ignores the dependency among speech tokens. In this paper, we propose to model speech tokens in an autoregressive way, similar to text. We find that applying the conventional cross-entropy loss on input speech tokens does not consistently improve the ASR performance over the Loss Masking approach. To address this issue, we propose a novel approach denoted Smoothed Label Distillation (SLD), which applies a KL divergence loss with smoothed labels on speech tokens. Our experiments show that SLD effectively models speech tokens and outperforms Loss Masking for decoder-only Transformers in ASR tasks with different speech discretization methods. The source code can be found here: https://github.com/alibaba-damo-academy/SpokenNLP/tree/main/sld",1,,include (senior:5),https://arxiv.org/abs/2311.04534v2,https://arxiv.org/pdf/2311.04534v2,2023,discrete speech tokens,speech tokens,"{""arxiv_id"": ""2311.04534"", ""title"": ""Loss Masking Is Not Needed in Decoder-only Transformer for Discrete-token-based ASR"", ""summary"": ""Recently, unified speech-text models, such as SpeechGPT, VioLA, and AudioPaLM, have achieved remarkable performance on various speech tasks. These models discretize speech signals into tokens (speech discretization) and use a shared vocabulary for both text and speech tokens. Then they train a single decoder-only Transformer on a mixture of speech tasks. However, these models rely on the Loss Masking strategy for the ASR task, which ignores the dependency among speech tokens. In this paper, we propose to model speech tokens in an autoregressive way, similar to text. We find that applying the conventional cross-entropy loss on input speech tokens does not consistently improve the ASR performance over the Loss Masking approach. To address this issue, we propose a novel approach denoted Smoothed Label Distillation (SLD), which applies a KL divergence loss with smoothed labels on speech tokens. Our experiments show that SLD effectively models speech tokens and outperforms Loss Masking for decoder-only Transformers in ASR tasks with different speech discretization methods. The source code can be found here: https://github.com/alibaba-damo-academy/SpokenNLP/tree/main/sld"", ""authors"": [""Qian Chen"", ""Wen Wang"", ""Qinglin Zhang"", ""Siqi Zheng"", ""Shiliang Zhang"", ""Chong Deng"", ""Yukun Ma"", ""Hai Yu"", ""Jiaqing Liu"", ""Chong Zhang""], ""published"": ""2023-11-08T08:45:14Z"", ""updated"": ""2024-02-05T02:42:57Z"", ""categories"": [""cs.CL"", ""cs.SD"", ""eess.AS""], ""pdf_url"": ""https://arxiv.org/pdf/2311.04534v2"", ""landing_url"": ""https://arxiv.org/abs/2311.04534v2"", ""doi"": ""https://doi.org/10.48550/arXiv.2311.04534""}",2311.04534,https://openalex.org/W4388555567
10.48550/arxiv.2312.09747,SELM: Speech Enhancement Using Discrete Tokens and Language Models,"Language models (LMs) have shown superior performances in various speech generation tasks recently, demonstrating their powerful ability for semantic context modeling. Given the intrinsic similarity between speech generation and speech enhancement, harnessing semantic information holds potential advantages for speech enhancement tasks. In light of this, we propose SELM, a novel paradigm for speech enhancement, which integrates discrete tokens and leverages language models. SELM comprises three stages: encoding, modeling, and decoding. We transform continuous waveform signals into discrete tokens using pre-trained self-supervised learning (SSL) models and a k-means tokenizer. Language models then capture comprehensive contextual information within these tokens. Finally, a detokenizer and HiFi-GAN restore them into enhanced speech. Experimental results demonstrate that SELM achieves comparable performance in objective metrics alongside superior results in subjective perception. Our demos are available https://honee-w.github.io/SELM/.",1,,include (junior:5),https://arxiv.org/abs/2312.09747v2,https://arxiv.org/pdf/2312.09747v2,2023,discrete speech tokens,speech generation,"{""arxiv_id"": ""2312.09747"", ""title"": ""SELM: Speech Enhancement Using Discrete Tokens and Language Models"", ""summary"": ""Language models (LMs) have shown superior performances in various speech generation tasks recently, demonstrating their powerful ability for semantic context modeling. Given the intrinsic similarity between speech generation and speech enhancement, harnessing semantic information holds potential advantages for speech enhancement tasks. In light of this, we propose SELM, a novel paradigm for speech enhancement, which integrates discrete tokens and leverages language models. SELM comprises three stages: encoding, modeling, and decoding. We transform continuous waveform signals into discrete tokens using pre-trained self-supervised learning (SSL) models and a k-means tokenizer. Language models then capture comprehensive contextual information within these tokens. Finally, a detokenizer and HiFi-GAN restore them into enhanced speech. Experimental results demonstrate that SELM achieves comparable performance in objective metrics alongside superior results in subjective perception. Our demos are available https://honee-w.github.io/SELM/."", ""authors"": [""Ziqian Wang"", ""Xinfa Zhu"", ""Zihan Zhang"", ""YuanJun Lv"", ""Ning Jiang"", ""Guoqing Zhao"", ""Lei Xie""], ""published"": ""2023-12-15T12:36:05Z"", ""updated"": ""2024-01-07T09:02:52Z"", ""categories"": [""eess.AS"", ""eess.SP""], ""pdf_url"": ""https://arxiv.org/pdf/2312.09747v2"", ""landing_url"": ""https://arxiv.org/abs/2312.09747v2"", ""doi"": ""https://doi.org/10.48550/arXiv.2312.09747""}",2312.09747,https://openalex.org/W4389911919
10.48550/arxiv.2401.01498,Utilizing Neural Transducers for Two-Stage Text-to-Speech via Semantic Token Prediction,"We propose a novel text-to-speech (TTS) framework centered around a neural transducer. Our approach divides the whole TTS pipeline into semantic-level sequence-to-sequence (seq2seq) modeling and fine-grained acoustic modeling stages, utilizing discrete semantic tokens obtained from wav2vec2.0 embeddings. For a robust and efficient alignment modeling, we employ a neural transducer named token transducer for the semantic token prediction, benefiting from its hard monotonic alignment constraints. Subsequently, a non-autoregressive (NAR) speech generator efficiently synthesizes waveforms from these semantic tokens. Additionally, a reference speech controls temporal dynamics and acoustic conditions at each stage. This decoupled framework reduces the training complexity of TTS while allowing each stage to focus on semantic and acoustic modeling. Our experimental results on zero-shot adaptive TTS demonstrate that our model surpasses the baseline in terms of speech quality and speaker similarity, both objectively and subjectively. We also delve into the inference speed and prosody control capabilities of our approach, highlighting the potential of neural transducers in TTS frameworks.",1,,include (junior:5),https://arxiv.org/abs/2401.01498v1,https://arxiv.org/pdf/2401.01498v1,2024,semantic tokens,speech tokens,"{""arxiv_id"": ""2401.01498"", ""title"": ""Utilizing Neural Transducers for Two-Stage Text-to-Speech via Semantic Token Prediction"", ""summary"": ""We propose a novel text-to-speech (TTS) framework centered around a neural transducer. Our approach divides the whole TTS pipeline into semantic-level sequence-to-sequence (seq2seq) modeling and fine-grained acoustic modeling stages, utilizing discrete semantic tokens obtained from wav2vec2.0 embeddings. For a robust and efficient alignment modeling, we employ a neural transducer named token transducer for the semantic token prediction, benefiting from its hard monotonic alignment constraints. Subsequently, a non-autoregressive (NAR) speech generator efficiently synthesizes waveforms from these semantic tokens. Additionally, a reference speech controls temporal dynamics and acoustic conditions at each stage. This decoupled framework reduces the training complexity of TTS while allowing each stage to focus on semantic and acoustic modeling. Our experimental results on zero-shot adaptive TTS demonstrate that our model surpasses the baseline in terms of speech quality and speaker similarity, both objectively and subjectively. We also delve into the inference speed and prosody control capabilities of our approach, highlighting the potential of neural transducers in TTS frameworks."", ""authors"": [""Minchan Kim"", ""Myeonghun Jeong"", ""Byoung Jin Choi"", ""Semin Kim"", ""Joun Yeop Lee"", ""Nam Soo Kim""], ""published"": ""2024-01-03T02:03:36Z"", ""updated"": ""2024-01-03T02:03:36Z"", ""categories"": [""eess.AS"", ""cs.CL"", ""cs.LG"", ""cs.SD""], ""pdf_url"": ""https://arxiv.org/pdf/2401.01498v1"", ""landing_url"": ""https://arxiv.org/abs/2401.01498v1"", ""doi"": ""https://doi.org/10.48550/arXiv.2401.01498""}",2401.01498,https://openalex.org/W4390601257
10.48550/arxiv.2402.02302,Predicting positive transfer for improved low-resource speech recognition using acoustic pseudo-tokens,"While massively multilingual speech models like wav2vec 2.0 XLSR-128 can be directly fine-tuned for automatic speech recognition (ASR), downstream performance can still be relatively poor on languages that are under-represented in the pre-training data. Continued pre-training on 70-200 hours of untranscribed speech in these languages can help -- but what about languages without that much recorded data? For such cases, we show that supplementing the target language with data from a similar, higher-resource 'donor' language can help. For example, continued pre-training on only 10 hours of low-resource Punjabi supplemented with 60 hours of donor Hindi is almost as good as continued pretraining on 70 hours of Punjabi. By contrast, sourcing data from less similar donors like Bengali does not improve ASR performance. To inform donor language selection, we propose a novel similarity metric based on the sequence distribution of induced acoustic units: the Acoustic Token Distribution Similarity (ATDS). Across a set of typologically different target languages (Punjabi, Galician, Iban, Setswana), we show that the ATDS between the target language and its candidate donors precisely predicts target language ASR performance.",1,,include (senior:4),https://arxiv.org/abs/2402.02302v1,https://arxiv.org/pdf/2402.02302v1,2024,acoustic tokens,offline clustering,"{""arxiv_id"": ""2402.02302"", ""title"": ""Predicting positive transfer for improved low-resource speech recognition using acoustic pseudo-tokens"", ""summary"": ""While massively multilingual speech models like wav2vec 2.0 XLSR-128 can be directly fine-tuned for automatic speech recognition (ASR), downstream performance can still be relatively poor on languages that are under-represented in the pre-training data. Continued pre-training on 70-200 hours of untranscribed speech in these languages can help -- but what about languages without that much recorded data? For such cases, we show that supplementing the target language with data from a similar, higher-resource 'donor' language can help. For example, continued pre-training on only 10 hours of low-resource Punjabi supplemented with 60 hours of donor Hindi is almost as good as continued pretraining on 70 hours of Punjabi. By contrast, sourcing data from less similar donors like Bengali does not improve ASR performance. To inform donor language selection, we propose a novel similarity metric based on the sequence distribution of induced acoustic units: the Acoustic Token Distribution Similarity (ATDS). Across a set of typologically different target languages (Punjabi, Galician, Iban, Setswana), we show that the ATDS between the target language and its candidate donors precisely predicts target language ASR performance."", ""authors"": [""Nay San"", ""Georgios Paraskevopoulos"", ""Aryaman Arora"", ""Xiluo He"", ""Prabhjot Kaur"", ""Oliver Adams"", ""Dan Jurafsky""], ""published"": ""2024-02-03T23:54:03Z"", ""updated"": ""2024-02-03T23:54:03Z"", ""categories"": [""eess.AS"", ""cs.CL""], ""pdf_url"": ""https://arxiv.org/pdf/2402.02302v1"", ""landing_url"": ""https://arxiv.org/abs/2402.02302v1"", ""doi"": ""https://doi.org/10.48550/arXiv.2402.02302""}",2402.02302,https://openalex.org/W4391590935
10.48550/arxiv.2402.08093,BASE TTS: Lessons from building a billion-parameter Text-to-Speech model on 100K hours of data,"We introduce a text-to-speech (TTS) model called BASE TTS, which stands for $\textbf{B}$ig $\textbf{A}$daptive $\textbf{S}$treamable TTS with $\textbf{E}$mergent abilities. BASE TTS is the largest TTS model to-date, trained on 100K hours of public domain speech data, achieving a new state-of-the-art in speech naturalness. It deploys a 1-billion-parameter autoregressive Transformer that converts raw texts into discrete codes (""speechcodes"") followed by a convolution-based decoder which converts these speechcodes into waveforms in an incremental, streamable manner. Further, our speechcodes are built using a novel speech tokenization technique that features speaker ID disentanglement and compression with byte-pair encoding. Echoing the widely-reported ""emergent abilities"" of large language models when trained on increasing volume of data, we show that BASE TTS variants built with 10K+ hours and 500M+ parameters begin to demonstrate natural prosody on textually complex sentences. We design and share a specialized dataset to measure these emergent abilities for text-to-speech. We showcase state-of-the-art naturalness of BASE TTS by evaluating against baselines that include publicly available large-scale text-to-speech systems: YourTTS, Bark and TortoiseTTS. Audio samples generated by the model can be heard at https://amazon-ltts-paper.com/.",1,,include (junior:5),https://arxiv.org/abs/2402.08093v2,https://arxiv.org/pdf/2402.08093v2,2024,discrete speech tokens,speech tokenization,"{""arxiv_id"": ""2402.08093"", ""title"": ""BASE TTS: Lessons from building a billion-parameter Text-to-Speech model on 100K hours of data"", ""summary"": ""We introduce a text-to-speech (TTS) model called BASE TTS, which stands for $\\textbf{B}$ig $\\textbf{A}$daptive $\\textbf{S}$treamable TTS with $\\textbf{E}$mergent abilities. BASE TTS is the largest TTS model to-date, trained on 100K hours of public domain speech data, achieving a new state-of-the-art in speech naturalness. It deploys a 1-billion-parameter autoregressive Transformer that converts raw texts into discrete codes (\""speechcodes\"") followed by a convolution-based decoder which converts these speechcodes into waveforms in an incremental, streamable manner. Further, our speechcodes are built using a novel speech tokenization technique that features speaker ID disentanglement and compression with byte-pair encoding. Echoing the widely-reported \""emergent abilities\"" of large language models when trained on increasing volume of data, we show that BASE TTS variants built with 10K+ hours and 500M+ parameters begin to demonstrate natural prosody on textually complex sentences. We design and share a specialized dataset to measure these emergent abilities for text-to-speech. We showcase state-of-the-art naturalness of BASE TTS by evaluating against baselines that include publicly available large-scale text-to-speech systems: YourTTS, Bark and TortoiseTTS. Audio samples generated by the model can be heard at https://amazon-ltts-paper.com/."", ""authors"": [""Mateusz Łajszczak"", ""Guillermo Cámbara"", ""Yang Li"", ""Fatih Beyhan"", ""Arent van Korlaar"", ""Fan Yang"", ""Arnaud Joly"", ""Álvaro Martín-Cortinas"", ""Ammar Abbas"", ""Adam Michalski"", ""Alexis Moinet"", ""Sri Karlapati"", ""Ewa Muszyńska"", ""Haohan Guo"", ""Bartosz Putrycz"", ""Soledad López Gambino"", ""Kayeon Yoo"", ""Elena Sokolova"", ""Thomas Drugman""], ""published"": ""2024-02-12T22:21:30Z"", ""updated"": ""2024-02-15T18:57:26Z"", ""categories"": [""cs.LG"", ""cs.CL"", ""eess.AS""], ""pdf_url"": ""https://arxiv.org/pdf/2402.08093v2"", ""landing_url"": ""https://arxiv.org/abs/2402.08093v2"", ""doi"": ""https://doi.org/10.48550/arXiv.2402.08093""}",2402.08093,https://openalex.org/W4391833199
10.48550/arxiv.2402.09378,MobileSpeech: A Fast and High-Fidelity Framework for Mobile Zero-Shot Text-to-Speech,"Zero-shot text-to-speech (TTS) has gained significant attention due to its powerful voice cloning capabilities, requiring only a few seconds of unseen speaker voice prompts. However, all previous work has been developed for cloud-based systems. Taking autoregressive models as an example, although these approaches achieve high-fidelity voice cloning, they fall short in terms of inference speed, model size, and robustness. Therefore, we propose MobileSpeech, which is a fast, lightweight, and robust zero-shot text-to-speech system based on mobile devices for the first time. Specifically: 1) leveraging discrete codec, we design a parallel speech mask decoder module called SMD, which incorporates hierarchical information from the speech codec and weight mechanisms across different codec layers during the generation process. Moreover, to bridge the gap between text and speech, we introduce a high-level probabilistic mask that simulates the progression of information flow from less to more during speech generation. 2) For speaker prompts, we extract fine-grained prompt duration from the prompt speech and incorporate text, prompt speech by cross attention in SMD. We demonstrate the effectiveness of MobileSpeech on multilingual datasets at different levels, achieving state-of-the-art results in terms of generating speed and speech quality. MobileSpeech achieves RTF of 0.09 on a single A100 GPU and we have successfully deployed MobileSpeech on mobile devices. Audio samples are available at \url{https://mobilespeech.github.io/} .",1,,include (senior:4),https://arxiv.org/abs/2402.09378v2,https://arxiv.org/pdf/2402.09378v2,2024,discrete speech tokens,text to speech,"{""arxiv_id"": ""2402.09378"", ""title"": ""MobileSpeech: A Fast and High-Fidelity Framework for Mobile Zero-Shot Text-to-Speech"", ""summary"": ""Zero-shot text-to-speech (TTS) has gained significant attention due to its powerful voice cloning capabilities, requiring only a few seconds of unseen speaker voice prompts. However, all previous work has been developed for cloud-based systems. Taking autoregressive models as an example, although these approaches achieve high-fidelity voice cloning, they fall short in terms of inference speed, model size, and robustness. Therefore, we propose MobileSpeech, which is a fast, lightweight, and robust zero-shot text-to-speech system based on mobile devices for the first time. Specifically: 1) leveraging discrete codec, we design a parallel speech mask decoder module called SMD, which incorporates hierarchical information from the speech codec and weight mechanisms across different codec layers during the generation process. Moreover, to bridge the gap between text and speech, we introduce a high-level probabilistic mask that simulates the progression of information flow from less to more during speech generation. 2) For speaker prompts, we extract fine-grained prompt duration from the prompt speech and incorporate text, prompt speech by cross attention in SMD. We demonstrate the effectiveness of MobileSpeech on multilingual datasets at different levels, achieving state-of-the-art results in terms of generating speed and speech quality. MobileSpeech achieves RTF of 0.09 on a single A100 GPU and we have successfully deployed MobileSpeech on mobile devices. Audio samples are available at \\url{https://mobilespeech.github.io/} ."", ""authors"": [""Shengpeng Ji"", ""Ziyue Jiang"", ""Hanting Wang"", ""Jialong Zuo"", ""Zhou Zhao""], ""published"": ""2024-02-14T18:24:41Z"", ""updated"": ""2024-06-02T16:11:18Z"", ""categories"": [""eess.AS"", ""cs.SD""], ""pdf_url"": ""https://arxiv.org/pdf/2402.09378v2"", ""landing_url"": ""https://arxiv.org/abs/2402.09378v2"", ""doi"": ""https://doi.org/10.48550/arXiv.2402.09378""}",2402.09378,https://openalex.org/W4399351054
10.48550/arxiv.2402.12208,Language-Codec: Bridging Discrete Codec Representations and Speech Language Models,"In recent years, large language models have achieved significant success in generative tasks related to speech, audio, music, and other signal domains. A crucial element of these models is the discrete acoustic codecs, which serve as an intermediate representation replacing the mel-spectrogram. However, there exist several gaps between discrete codecs and downstream speech language models. Specifically, 1) Due to the reconstruction paradigm of the Codec model and the structure of residual vector quantization, the initial channel of the codebooks contains excessive information, making it challenging to directly generate acoustic tokens from weakly supervised signals such as text in downstream tasks. 2) numerous codebooks increases the burden on downstream speech language models. Consequently, leveraging the characteristics of speech language models, we propose Language-Codec. In the Language-Codec, we introduce a Masked Channel Residual Vector Quantization (MCRVQ) mechanism along with improved fourier transform structures and attention blocks, refined discriminator design to address the aforementioned gaps. We compare our method with competing audio compression algorithms and observe significant outperformance across extensive evaluations. Furthermore, we also validate the efficiency of the Language-Codec on downstream speech language models. The source code and pre-trained models can be accessed at https://github.com/jishengpeng/languagecodec .",1,,include (junior:5),https://arxiv.org/abs/2402.12208v4,https://arxiv.org/pdf/2402.12208v4,2024,acoustic tokens,offline clustering,"{""arxiv_id"": ""2402.12208"", ""title"": ""Language-Codec: Bridging Discrete Codec Representations and Speech Language Models"", ""summary"": ""In recent years, large language models have achieved significant success in generative tasks related to speech, audio, music, and other signal domains. A crucial element of these models is the discrete acoustic codecs, which serve as an intermediate representation replacing the mel-spectrogram. However, there exist several gaps between discrete codecs and downstream speech language models. Specifically, 1) Due to the reconstruction paradigm of the Codec model and the structure of residual vector quantization, the initial channel of the codebooks contains excessive information, making it challenging to directly generate acoustic tokens from weakly supervised signals such as text in downstream tasks. 2) numerous codebooks increases the burden on downstream speech language models. Consequently, leveraging the characteristics of speech language models, we propose Language-Codec. In the Language-Codec, we introduce a Masked Channel Residual Vector Quantization (MCRVQ) mechanism along with improved fourier transform structures and attention blocks, refined discriminator design to address the aforementioned gaps. We compare our method with competing audio compression algorithms and observe significant outperformance across extensive evaluations. Furthermore, we also validate the efficiency of the Language-Codec on downstream speech language models. The source code and pre-trained models can be accessed at https://github.com/jishengpeng/languagecodec ."", ""authors"": [""Shengpeng Ji"", ""Minghui Fang"", ""Jialong Zuo"", ""Ziyue Jiang"", ""Dingdong Wang"", ""Hanting Wang"", ""Hai Huang"", ""Zhou Zhao""], ""published"": ""2024-02-19T15:12:12Z"", ""updated"": ""2025-06-04T05:50:15Z"", ""categories"": [""eess.AS"", ""cs.SD""], ""pdf_url"": ""https://arxiv.org/pdf/2402.12208v4"", ""landing_url"": ""https://arxiv.org/abs/2402.12208v4"", ""doi"": ""https://doi.org/10.48550/arXiv.2402.12208""}",2402.12208,https://openalex.org/W4392019360
10.48550/arxiv.2404.06079,The X-LANCE Technical Report for Interspeech 2024 Speech Processing Using Discrete Speech Unit Challenge,"Discrete speech tokens have been more and more popular in multiple speech processing fields, including automatic speech recognition (ASR), text-to-speech (TTS) and singing voice synthesis (SVS). In this paper, we describe the systems developed by the SJTU X-LANCE group for the TTS (acoustic + vocoder), SVS, and ASR tracks in the Interspeech 2024 Speech Processing Using Discrete Speech Unit Challenge. Notably, we achieved 1st rank on the leaderboard in the TTS track both with the whole training set and only 1h training data, with the highest UTMOS score and lowest bitrate among all submissions.",1,,include (junior:4),https://arxiv.org/abs/2404.06079v2,https://arxiv.org/pdf/2404.06079v2,2024,discrete speech tokens,speech tokens,"{""arxiv_id"": ""2404.06079"", ""title"": ""The X-LANCE Technical Report for Interspeech 2024 Speech Processing Using Discrete Speech Unit Challenge"", ""summary"": ""Discrete speech tokens have been more and more popular in multiple speech processing fields, including automatic speech recognition (ASR), text-to-speech (TTS) and singing voice synthesis (SVS). In this paper, we describe the systems developed by the SJTU X-LANCE group for the TTS (acoustic + vocoder), SVS, and ASR tracks in the Interspeech 2024 Speech Processing Using Discrete Speech Unit Challenge. Notably, we achieved 1st rank on the leaderboard in the TTS track both with the whole training set and only 1h training data, with the highest UTMOS score and lowest bitrate among all submissions."", ""authors"": [""Yiwei Guo"", ""Chenrun Wang"", ""Yifan Yang"", ""Hankun Wang"", ""Ziyang Ma"", ""Chenpeng Du"", ""Shuai Wang"", ""Hanzheng Li"", ""Shuai Fan"", ""Hui Zhang"", ""Xie Chen"", ""Kai Yu""], ""published"": ""2024-04-09T07:37:41Z"", ""updated"": ""2024-04-10T00:33:25Z"", ""categories"": [""eess.AS"", ""cs.AI""], ""pdf_url"": ""https://arxiv.org/pdf/2404.06079v2"", ""landing_url"": ""https://arxiv.org/abs/2404.06079v2"", ""doi"": ""https://doi.org/10.48550/arXiv.2404.06079""}",2404.06079,https://openalex.org/W4394774425
10.48550/arxiv.2404.19441,ESC: Efficient Speech Coding with Cross-Scale Residual Vector Quantized Transformers,"Neural speech codecs aim to compress input signals into minimal bits while maintaining content quality in a low-latency manner. However, existing neural codecs often trade model complexity for reconstruction performance. These codecs primarily use convolutional blocks for feature transformation, which are not inherently suited for capturing the local redundancies in speech signals. To compensate, they require either adversarial discriminators or a large number of model parameters to enhance audio quality. In response to these challenges, we introduce the Efficient Speech Codec (ESC), a lightweight, parameter-efficient speech codec based on a cross-scale residual vector quantization scheme and transformers. Our model employs mirrored hierarchical window transformer blocks and performs step-wise decoding from coarse-to-fine feature representations. To enhance bitrate efficiency, we propose a novel combination of vector quantization techniques along with a pre-training paradigm. Extensive experiments demonstrate that ESC can achieve high-fidelity speech reconstruction with significantly lower model complexity, making it a promising alternative to existing convolutional audio codecs.",1,,include (junior:4),https://arxiv.org/abs/2404.19441v3,https://arxiv.org/pdf/2404.19441v3,2024,discrete speech tokens,vector quantization,"{""arxiv_id"": ""2404.19441"", ""title"": ""ESC: Efficient Speech Coding with Cross-Scale Residual Vector Quantized Transformers"", ""summary"": ""Neural speech codecs aim to compress input signals into minimal bits while maintaining content quality in a low-latency manner. However, existing neural codecs often trade model complexity for reconstruction performance. These codecs primarily use convolutional blocks for feature transformation, which are not inherently suited for capturing the local redundancies in speech signals. To compensate, they require either adversarial discriminators or a large number of model parameters to enhance audio quality. In response to these challenges, we introduce the Efficient Speech Codec (ESC), a lightweight, parameter-efficient speech codec based on a cross-scale residual vector quantization scheme and transformers. Our model employs mirrored hierarchical window transformer blocks and performs step-wise decoding from coarse-to-fine feature representations. To enhance bitrate efficiency, we propose a novel combination of vector quantization techniques along with a pre-training paradigm. Extensive experiments demonstrate that ESC can achieve high-fidelity speech reconstruction with significantly lower model complexity, making it a promising alternative to existing convolutional audio codecs."", ""authors"": [""Yuzhe Gu"", ""Enmao Diao""], ""published"": ""2024-04-30T10:44:33Z"", ""updated"": ""2024-10-03T12:23:26Z"", ""categories"": [""cs.SD"", ""eess.AS""], ""pdf_url"": ""https://arxiv.org/pdf/2404.19441v3"", ""landing_url"": ""https://arxiv.org/abs/2404.19441v3"", ""doi"": ""https://doi.org/10.48550/arXiv.2404.19441""}",2404.19441,https://openalex.org/W4396821491
10.48550/arxiv.2405.16136,C3LLM: Conditional Multimodal Content Generation Using Large Language Models,"We introduce C3LLM (Conditioned-on-Three-Modalities Large Language Models), a novel framework combining three tasks of video-to-audio, audio-to-text, and text-to-audio together. C3LLM adapts the Large Language Model (LLM) structure as a bridge for aligning different modalities, synthesizing the given conditional information, and making multimodal generation in a discrete manner. Our contributions are as follows. First, we adapt a hierarchical structure for audio generation tasks with pre-trained audio codebooks. Specifically, we train the LLM to generate audio semantic tokens from the given conditions, and further use a non-autoregressive transformer to generate different levels of acoustic tokens in layers to better enhance the fidelity of the generated audio. Second, based on the intuition that LLMs were originally designed for discrete tasks with the next-word prediction method, we use the discrete representation for audio generation and compress their semantic meanings into acoustic tokens, similar to adding ""acoustic vocabulary"" to LLM. Third, our method combines the previous tasks of audio understanding, video-to-audio generation, and text-to-audio generation together into one unified model, providing more versatility in an end-to-end fashion. Our C3LLM achieves improved results through various automated evaluation metrics, providing better semantic alignment compared to previous methods.",1,,include (senior:4),https://arxiv.org/abs/2405.16136v1,https://arxiv.org/pdf/2405.16136v1,2024,acoustic tokens,speech tokens,"{""arxiv_id"": ""2405.16136"", ""title"": ""C3LLM: Conditional Multimodal Content Generation Using Large Language Models"", ""summary"": ""We introduce C3LLM (Conditioned-on-Three-Modalities Large Language Models), a novel framework combining three tasks of video-to-audio, audio-to-text, and text-to-audio together. C3LLM adapts the Large Language Model (LLM) structure as a bridge for aligning different modalities, synthesizing the given conditional information, and making multimodal generation in a discrete manner. Our contributions are as follows. First, we adapt a hierarchical structure for audio generation tasks with pre-trained audio codebooks. Specifically, we train the LLM to generate audio semantic tokens from the given conditions, and further use a non-autoregressive transformer to generate different levels of acoustic tokens in layers to better enhance the fidelity of the generated audio. Second, based on the intuition that LLMs were originally designed for discrete tasks with the next-word prediction method, we use the discrete representation for audio generation and compress their semantic meanings into acoustic tokens, similar to adding \""acoustic vocabulary\"" to LLM. Third, our method combines the previous tasks of audio understanding, video-to-audio generation, and text-to-audio generation together into one unified model, providing more versatility in an end-to-end fashion. Our C3LLM achieves improved results through various automated evaluation metrics, providing better semantic alignment compared to previous methods."", ""authors"": [""Zixuan Wang"", ""Qinkai Duan"", ""Yu-Wing Tai"", ""Chi-Keung Tang""], ""published"": ""2024-05-25T09:10:12Z"", ""updated"": ""2024-05-25T09:10:12Z"", ""categories"": [""cs.AI"", ""cs.CL"", ""cs.LG"", ""cs.SD"", ""eess.AS""], ""pdf_url"": ""https://arxiv.org/pdf/2405.16136v1"", ""landing_url"": ""https://arxiv.org/abs/2405.16136v1"", ""doi"": ""https://doi.org/10.48550/arXiv.2405.16136""}",2405.16136,https://openalex.org/W4399115330
10.48550/arxiv.2406.00976,Generative Pre-trained Speech Language Model with Efficient Hierarchical Transformer,"While recent advancements in speech language models have achieved significant progress, they face remarkable challenges in modeling the long acoustic sequences of neural audio codecs. In this paper, we introduce \textbf{G}enerative \textbf{P}re-trained \textbf{S}peech \textbf{T}ransformer (GPST), a hierarchical transformer designed for efficient speech language modeling. GPST quantizes audio waveforms into two distinct types of discrete speech representations and integrates them within a hierarchical transformer architecture, allowing for a unified one-stage generation process and enhancing Hi-Res audio generation capabilities. By training on large corpora of speeches in an end-to-end unsupervised manner, GPST can generate syntactically consistent speech with diverse speaker identities. Given a brief 3-second prompt, GPST can produce natural and coherent personalized speech, demonstrating in-context learning abilities. Moreover, our approach can be easily extended to spoken cross-lingual speech generation by incorporating multi-lingual semantic tokens and universal acoustic tokens. Experimental results indicate that GPST significantly outperforms the existing speech language models in terms of word error rate, speech quality, and speaker similarity. The code is available at \url{https://github.com/youngsheen/GPST}.",1,,include (junior:5),https://arxiv.org/abs/2406.00976v2,https://arxiv.org/pdf/2406.00976v2,2024,acoustic tokens,offline clustering,"{""arxiv_id"": ""2406.00976"", ""title"": ""Generative Pre-trained Speech Language Model with Efficient Hierarchical Transformer"", ""summary"": ""While recent advancements in speech language models have achieved significant progress, they face remarkable challenges in modeling the long acoustic sequences of neural audio codecs. In this paper, we introduce \\textbf{G}enerative \\textbf{P}re-trained \\textbf{S}peech \\textbf{T}ransformer (GPST), a hierarchical transformer designed for efficient speech language modeling. GPST quantizes audio waveforms into two distinct types of discrete speech representations and integrates them within a hierarchical transformer architecture, allowing for a unified one-stage generation process and enhancing Hi-Res audio generation capabilities. By training on large corpora of speeches in an end-to-end unsupervised manner, GPST can generate syntactically consistent speech with diverse speaker identities. Given a brief 3-second prompt, GPST can produce natural and coherent personalized speech, demonstrating in-context learning abilities. Moreover, our approach can be easily extended to spoken cross-lingual speech generation by incorporating multi-lingual semantic tokens and universal acoustic tokens. Experimental results indicate that GPST significantly outperforms the existing speech language models in terms of word error rate, speech quality, and speaker similarity. The code is available at \\url{https://github.com/youngsheen/GPST}."", ""authors"": [""Yongxin Zhu"", ""Dan Su"", ""Liqiang He"", ""Linli Xu"", ""Dong Yu""], ""published"": ""2024-06-03T04:16:30Z"", ""updated"": ""2024-11-01T13:54:48Z"", ""categories"": [""cs.CL"", ""cs.SD"", ""eess.AS""], ""pdf_url"": ""https://arxiv.org/pdf/2406.00976v2"", ""landing_url"": ""https://arxiv.org/abs/2406.00976v2"", ""doi"": ""https://doi.org/10.48550/arXiv.2406.00976""}",2406.00976,https://openalex.org/W4399400026
10.48550/arxiv.2406.02092,MaskSR: Masked Language Model for Full-band Speech Restoration,"Speech restoration aims at restoring high quality speech in the presence of a diverse set of distortions. Although several deep learning paradigms have been studied for this task, the power of the recently emerging language models has not been fully explored. In this paper, we propose MaskSR, a masked language model capable of restoring full-band 44.1 kHz speech jointly considering noise, reverb, clipping, and low bandwidth. MaskSR works with discrete acoustic tokens extracted using a pre-trained neural codec. During training, MaskSR is optimized to predict randomly masked tokens extracted from the high quality target speech, conditioned on the corrupted speech with various distortions. During inference, MaskSR reconstructs the target speech tokens with efficient iterative sampling. Extensive experiments show that MaskSR obtains competitive results on both the full-band speech restoration task and also on sub-tasks compared with a wide range of models.",1,,include (junior:5),https://arxiv.org/abs/2406.02092v1,https://arxiv.org/pdf/2406.02092v1,2024,discrete speech tokens,speech tokens,"{""arxiv_id"": ""2406.02092"", ""title"": ""MaskSR: Masked Language Model for Full-band Speech Restoration"", ""summary"": ""Speech restoration aims at restoring high quality speech in the presence of a diverse set of distortions. Although several deep learning paradigms have been studied for this task, the power of the recently emerging language models has not been fully explored. In this paper, we propose MaskSR, a masked language model capable of restoring full-band 44.1 kHz speech jointly considering noise, reverb, clipping, and low bandwidth. MaskSR works with discrete acoustic tokens extracted using a pre-trained neural codec. During training, MaskSR is optimized to predict randomly masked tokens extracted from the high quality target speech, conditioned on the corrupted speech with various distortions. During inference, MaskSR reconstructs the target speech tokens with efficient iterative sampling. Extensive experiments show that MaskSR obtains competitive results on both the full-band speech restoration task and also on sub-tasks compared with a wide range of models."", ""authors"": [""Xu Li"", ""Qirui Wang"", ""Xiaoyu Liu""], ""published"": ""2024-06-04T08:23:57Z"", ""updated"": ""2024-06-04T08:23:57Z"", ""categories"": [""cs.SD"", ""cs.AI"", ""cs.LG"", ""eess.AS"", ""eess.SP""], ""pdf_url"": ""https://arxiv.org/pdf/2406.02092v1"", ""landing_url"": ""https://arxiv.org/abs/2406.02092v1"", ""doi"": ""https://doi.org/10.48550/arXiv.2406.02092""}",2406.02092,https://openalex.org/W4399416856
10.48550/arxiv.2406.02940,Addressing Index Collapse of Large-Codebook Speech Tokenizer with Dual-Decoding Product-Quantized Variational Auto-Encoder,"VQ-VAE, as a mainstream approach of speech tokenizer, has been troubled by ``index collapse'', where only a small number of codewords are activated in large codebooks. This work proposes product-quantized (PQ) VAE with more codebooks but fewer codewords to address this problem and build large-codebook speech tokenizers. It encodes speech features into multiple VQ subspaces and composes them into codewords in a larger codebook. Besides, to utilize each VQ subspace well, we also enhance PQ-VAE via a dual-decoding training strategy with the encoding and quantized sequences. The experimental results demonstrate that PQ-VAE addresses ``index collapse"" effectively, especially for larger codebooks. The model with the proposed training strategy further improves codebook perplexity and reconstruction quality, outperforming other multi-codebook VQ approaches. Finally, PQ-VAE demonstrates its effectiveness in language-model-based TTS, supporting higher-quality speech generation with larger codebooks.",1,,include (junior:5),https://arxiv.org/abs/2406.02940v1,https://arxiv.org/pdf/2406.02940v1,2024,discrete speech tokens,speech tokens,"{""arxiv_id"": ""2406.02940"", ""title"": ""Addressing Index Collapse of Large-Codebook Speech Tokenizer with Dual-Decoding Product-Quantized Variational Auto-Encoder"", ""summary"": ""VQ-VAE, as a mainstream approach of speech tokenizer, has been troubled by ``index collapse'', where only a small number of codewords are activated in large codebooks. This work proposes product-quantized (PQ) VAE with more codebooks but fewer codewords to address this problem and build large-codebook speech tokenizers. It encodes speech features into multiple VQ subspaces and composes them into codewords in a larger codebook. Besides, to utilize each VQ subspace well, we also enhance PQ-VAE via a dual-decoding training strategy with the encoding and quantized sequences. The experimental results demonstrate that PQ-VAE addresses ``index collapse\"" effectively, especially for larger codebooks. The model with the proposed training strategy further improves codebook perplexity and reconstruction quality, outperforming other multi-codebook VQ approaches. Finally, PQ-VAE demonstrates its effectiveness in language-model-based TTS, supporting higher-quality speech generation with larger codebooks."", ""authors"": [""Haohan Guo"", ""Fenglong Xie"", ""Dongchao Yang"", ""Hui Lu"", ""Xixin Wu"", ""Helen Meng""], ""published"": ""2024-06-05T04:54:49Z"", ""updated"": ""2024-06-05T04:54:49Z"", ""categories"": [""cs.SD"", ""eess.AS""], ""pdf_url"": ""https://arxiv.org/pdf/2406.02940v1"", ""landing_url"": ""https://arxiv.org/abs/2406.02940v1"", ""doi"": ""https://doi.org/10.48550/arXiv.2406.02940""}",2406.0294,https://openalex.org/W4399447613
10.48550/arxiv.2406.03706,Improving Audio Codec-based Zero-Shot Text-to-Speech Synthesis with Multi-Modal Context and Large Language Model,"Recent advances in large language models (LLMs) and development of audio codecs greatly propel the zero-shot TTS. They can synthesize personalized speech with only a 3-second speech of an unseen speaker as acoustic prompt. However, they only support short speech prompts and cannot leverage longer context information, as required in audiobook and conversational TTS scenarios. In this paper, we introduce a novel audio codec-based TTS model to adapt context features with multiple enhancements. Inspired by the success of Qformer, we propose a multi-modal context-enhanced Qformer (MMCE-Qformer) to utilize additional multi-modal context information. Besides, we adapt a pretrained LLM to leverage its understanding ability to predict semantic tokens, and use a SoundStorm to generate acoustic tokens thereby enhancing audio quality and speaker similarity. The extensive objective and subjective evaluations show that our proposed method outperforms baselines across various context TTS scenarios.",1,,include (junior:4),https://arxiv.org/abs/2406.03706v1,https://arxiv.org/pdf/2406.03706v1,2024,acoustic tokens,offline clustering,"{""arxiv_id"": ""2406.03706"", ""title"": ""Improving Audio Codec-based Zero-Shot Text-to-Speech Synthesis with Multi-Modal Context and Large Language Model"", ""summary"": ""Recent advances in large language models (LLMs) and development of audio codecs greatly propel the zero-shot TTS. They can synthesize personalized speech with only a 3-second speech of an unseen speaker as acoustic prompt. However, they only support short speech prompts and cannot leverage longer context information, as required in audiobook and conversational TTS scenarios. In this paper, we introduce a novel audio codec-based TTS model to adapt context features with multiple enhancements. Inspired by the success of Qformer, we propose a multi-modal context-enhanced Qformer (MMCE-Qformer) to utilize additional multi-modal context information. Besides, we adapt a pretrained LLM to leverage its understanding ability to predict semantic tokens, and use a SoundStorm to generate acoustic tokens thereby enhancing audio quality and speaker similarity. The extensive objective and subjective evaluations show that our proposed method outperforms baselines across various context TTS scenarios."", ""authors"": [""Jinlong Xue"", ""Yayue Deng"", ""Yicheng Han"", ""Yingming Gao"", ""Ya Li""], ""published"": ""2024-06-06T03:06:45Z"", ""updated"": ""2024-06-06T03:06:45Z"", ""categories"": [""cs.SD"", ""cs.CL"", ""eess.AS""], ""pdf_url"": ""https://arxiv.org/pdf/2406.03706v1"", ""landing_url"": ""https://arxiv.org/abs/2406.03706v1"", ""doi"": ""https://doi.org/10.48550/arXiv.2406.03706""}",2406.03706,https://openalex.org/W4399454058
10.48550/arxiv.2406.06582,Discrete Multimodal Transformers with a Pretrained Large Language Model for Mixed-Supervision Speech Processing,"Recent work on discrete speech tokenization has paved the way for models that can seamlessly perform multiple tasks across modalities, e.g., speech recognition, text to speech, speech to speech translation. Moreover, large language models (LLMs) pretrained from vast text corpora contain rich linguistic information that can improve accuracy in a variety of tasks. In this paper, we present a decoder-only Discrete Multimodal Language Model (DMLM), which can be flexibly applied to multiple tasks (ASR, T2S, S2TT, etc.) and modalities (text, speech, vision). We explore several critical aspects of discrete multi-modal models, including the loss function, weight initialization, mixed training supervision, and codebook. Our results show that DMLM benefits significantly, across multiple tasks and datasets, from a combination of supervised and unsupervised training. Moreover, for ASR, it benefits from initializing DMLM from a pretrained LLM, and from a codebook derived from Whisper activations.",1,,include (junior:5),https://arxiv.org/abs/2406.06582v2,https://arxiv.org/pdf/2406.06582v2,2024,discrete speech tokens,speech tokens,"{""arxiv_id"": ""2406.06582"", ""title"": ""Discrete Multimodal Transformers with a Pretrained Large Language Model for Mixed-Supervision Speech Processing"", ""summary"": ""Recent work on discrete speech tokenization has paved the way for models that can seamlessly perform multiple tasks across modalities, e.g., speech recognition, text to speech, speech to speech translation. Moreover, large language models (LLMs) pretrained from vast text corpora contain rich linguistic information that can improve accuracy in a variety of tasks. In this paper, we present a decoder-only Discrete Multimodal Language Model (DMLM), which can be flexibly applied to multiple tasks (ASR, T2S, S2TT, etc.) and modalities (text, speech, vision). We explore several critical aspects of discrete multi-modal models, including the loss function, weight initialization, mixed training supervision, and codebook. Our results show that DMLM benefits significantly, across multiple tasks and datasets, from a combination of supervised and unsupervised training. Moreover, for ASR, it benefits from initializing DMLM from a pretrained LLM, and from a codebook derived from Whisper activations."", ""authors"": [""Viet Anh Trinh"", ""Rosy Southwell"", ""Yiwen Guan"", ""Xinlu He"", ""Zhiyong Wang"", ""Jacob Whitehill""], ""published"": ""2024-06-04T20:08:25Z"", ""updated"": ""2024-06-25T17:44:00Z"", ""categories"": [""cs.CL"", ""cs.LG"", ""eess.AS""], ""pdf_url"": ""https://arxiv.org/pdf/2406.06582v2"", ""landing_url"": ""https://arxiv.org/abs/2406.06582v2"", ""doi"": ""https://doi.org/10.48550/arXiv.2406.06582""}",2406.06582,https://openalex.org/W4399597385
10.48550/arxiv.2406.07855,VALL-E R: Robust and Efficient Zero-Shot Text-to-Speech Synthesis via Monotonic Alignment,"With the help of discrete neural audio codecs, large language models (LLM) have increasingly been recognized as a promising methodology for zero-shot Text-to-Speech (TTS) synthesis. However, sampling based decoding strategies bring astonishing diversity to generation, but also pose robustness issues such as typos, omissions and repetition. In addition, the high sampling rate of audio also brings huge computational overhead to the inference process of autoregression. To address these issues, we propose VALL-E R, a robust and efficient zero-shot TTS system, building upon the foundation of VALL-E. Specifically, we introduce a phoneme monotonic alignment strategy to strengthen the connection between phonemes and acoustic sequence, ensuring a more precise alignment by constraining the acoustic tokens to match their associated phonemes. Furthermore, we employ a codec-merging approach to downsample the discrete codes in shallow quantization layer, thereby accelerating the decoding speed while preserving the high quality of speech output. Benefiting from these strategies, VALL-E R obtains controllablity over phonemes and demonstrates its strong robustness by approaching the WER of ground truth. In addition, it requires fewer autoregressive steps, with over 60% time reduction during inference. This research has the potential to be applied to meaningful projects, including the creation of speech for those affected by aphasia. Audio samples will be available at: https://aka.ms/valler.",1,,include (junior:5),https://arxiv.org/abs/2406.07855v1,https://arxiv.org/pdf/2406.07855v1,2024,acoustic tokens,offline clustering,"{""arxiv_id"": ""2406.07855"", ""title"": ""VALL-E R: Robust and Efficient Zero-Shot Text-to-Speech Synthesis via Monotonic Alignment"", ""summary"": ""With the help of discrete neural audio codecs, large language models (LLM) have increasingly been recognized as a promising methodology for zero-shot Text-to-Speech (TTS) synthesis. However, sampling based decoding strategies bring astonishing diversity to generation, but also pose robustness issues such as typos, omissions and repetition. In addition, the high sampling rate of audio also brings huge computational overhead to the inference process of autoregression. To address these issues, we propose VALL-E R, a robust and efficient zero-shot TTS system, building upon the foundation of VALL-E. Specifically, we introduce a phoneme monotonic alignment strategy to strengthen the connection between phonemes and acoustic sequence, ensuring a more precise alignment by constraining the acoustic tokens to match their associated phonemes. Furthermore, we employ a codec-merging approach to downsample the discrete codes in shallow quantization layer, thereby accelerating the decoding speed while preserving the high quality of speech output. Benefiting from these strategies, VALL-E R obtains controllablity over phonemes and demonstrates its strong robustness by approaching the WER of ground truth. In addition, it requires fewer autoregressive steps, with over 60% time reduction during inference. This research has the potential to be applied to meaningful projects, including the creation of speech for those affected by aphasia. Audio samples will be available at: https://aka.ms/valler."", ""authors"": [""Bing Han"", ""Long Zhou"", ""Shujie Liu"", ""Sanyuan Chen"", ""Lingwei Meng"", ""Yanming Qian"", ""Yanqing Liu"", ""Sheng Zhao"", ""Jinyu Li"", ""Furu Wei""], ""published"": ""2024-06-12T04:09:44Z"", ""updated"": ""2024-06-12T04:09:44Z"", ""categories"": [""cs.CL"", ""cs.SD"", ""eess.AS""], ""pdf_url"": ""https://arxiv.org/pdf/2406.07855v1"", ""landing_url"": ""https://arxiv.org/abs/2406.07855v1"", ""doi"": ""https://doi.org/10.48550/arXiv.2406.07855""}",2406.07855,https://openalex.org/W4399657366
10.48550/arxiv.2406.10735,How Should We Extract Discrete Audio Tokens from Self-Supervised Models?,"Discrete audio tokens have recently gained attention for their potential to bridge the gap between audio and language processing. Ideal audio tokens must preserve content, paralinguistic elements, speaker identity, and many other audio details. Current audio tokenization methods fall into two categories: Semantic tokens, acquired through quantization of Self-Supervised Learning (SSL) models, and Neural compression-based tokens (codecs). Although previous studies have benchmarked codec models to identify optimal configurations, the ideal setup for quantizing pretrained SSL models remains unclear. This paper explores the optimal configuration of semantic tokens across discriminative and generative tasks. We propose a scalable solution to train a universal vocoder across multiple SSL layers. Furthermore, an attention mechanism is employed to identify task-specific influential layers, enhancing the adaptability and performance of semantic tokens in diverse audio applications.",1,,include (junior:5),https://arxiv.org/abs/2406.10735v1,https://arxiv.org/pdf/2406.10735v1,2024,semantic tokens,speech tokens,"{""arxiv_id"": ""2406.10735"", ""title"": ""How Should We Extract Discrete Audio Tokens from Self-Supervised Models?"", ""summary"": ""Discrete audio tokens have recently gained attention for their potential to bridge the gap between audio and language processing. Ideal audio tokens must preserve content, paralinguistic elements, speaker identity, and many other audio details. Current audio tokenization methods fall into two categories: Semantic tokens, acquired through quantization of Self-Supervised Learning (SSL) models, and Neural compression-based tokens (codecs). Although previous studies have benchmarked codec models to identify optimal configurations, the ideal setup for quantizing pretrained SSL models remains unclear. This paper explores the optimal configuration of semantic tokens across discriminative and generative tasks. We propose a scalable solution to train a universal vocoder across multiple SSL layers. Furthermore, an attention mechanism is employed to identify task-specific influential layers, enhancing the adaptability and performance of semantic tokens in diverse audio applications."", ""authors"": [""Pooneh Mousavi"", ""Jarod Duret"", ""Salah Zaiem"", ""Luca Della Libera"", ""Artem Ploujnikov"", ""Cem Subakan"", ""Mirco Ravanelli""], ""published"": ""2024-06-15T20:43:07Z"", ""updated"": ""2024-06-15T20:43:07Z"", ""categories"": [""cs.SD"", ""cs.AI"", ""cs.CL"", ""eess.AS""], ""pdf_url"": ""https://arxiv.org/pdf/2406.10735v1"", ""landing_url"": ""https://arxiv.org/abs/2406.10735v1"", ""doi"": ""https://doi.org/10.48550/arXiv.2406.10735""}",2406.10735,https://openalex.org/W4399794549
10.48550/arxiv.2406.11037,NAST: Noise Aware Speech Tokenization for Speech Language Models,"Speech tokenization is the task of representing speech signals as a sequence of discrete units. Such representations can be later used for various downstream tasks including automatic speech recognition, text-to-speech, etc. More relevant to this study, such representation serves as the basis of Speech Language Models. In this work, we tackle the task of speech tokenization under the noisy setup and present NAST: Noise Aware Speech Tokenization for Speech Language Models. NAST is composed of three main components: (i) a predictor; (ii) a residual encoder; and (iii) a decoder. We evaluate the efficiency of NAST considering several spoken language modeling tasks and show that NAST is superior to the evaluated baselines across all setups. Lastly, we analyze NAST and show its disentanglement properties and robustness to signal variations in the form of noise, reverberation, pitch-shift, and time-stretch. Code and pre-trained models are available at https://github.com/ShovalMessica/NAST.",1,,include (junior:4),https://arxiv.org/abs/2406.11037v1,https://arxiv.org/pdf/2406.11037v1,2024,discrete speech tokens,speech tokens,"{""arxiv_id"": ""2406.11037"", ""title"": ""NAST: Noise Aware Speech Tokenization for Speech Language Models"", ""summary"": ""Speech tokenization is the task of representing speech signals as a sequence of discrete units. Such representations can be later used for various downstream tasks including automatic speech recognition, text-to-speech, etc. More relevant to this study, such representation serves as the basis of Speech Language Models. In this work, we tackle the task of speech tokenization under the noisy setup and present NAST: Noise Aware Speech Tokenization for Speech Language Models. NAST is composed of three main components: (i) a predictor; (ii) a residual encoder; and (iii) a decoder. We evaluate the efficiency of NAST considering several spoken language modeling tasks and show that NAST is superior to the evaluated baselines across all setups. Lastly, we analyze NAST and show its disentanglement properties and robustness to signal variations in the form of noise, reverberation, pitch-shift, and time-stretch. Code and pre-trained models are available at https://github.com/ShovalMessica/NAST."", ""authors"": [""Shoval Messica"", ""Yossi Adi""], ""published"": ""2024-06-16T18:20:45Z"", ""updated"": ""2024-06-16T18:20:45Z"", ""categories"": [""cs.SD"", ""eess.AS""], ""pdf_url"": ""https://arxiv.org/pdf/2406.11037v1"", ""landing_url"": ""https://arxiv.org/abs/2406.11037v1"", ""doi"": ""https://doi.org/10.48550/arXiv.2406.11037""}",2406.11037,https://openalex.org/W4399794786
10.48550/arxiv.2406.14294,DASB - Discrete Audio and Speech Benchmark,"Discrete audio tokens have recently gained considerable attention for their potential to connect audio and language processing, enabling the creation of modern multimodal large language models. Ideal audio tokens must effectively preserve phonetic and semantic content along with paralinguistic information, speaker identity, and other details. While several types of audio tokens have been recently proposed, identifying the optimal tokenizer for various tasks is challenging due to the inconsistent evaluation settings in existing studies. To address this gap, we release the Discrete Audio and Speech Benchmark (DASB), a comprehensive leaderboard for benchmarking discrete audio tokens across a wide range of discriminative tasks, including speech recognition, speaker identification and verification, emotion recognition, keyword spotting, and intent classification, as well as generative tasks such as speech enhancement, separation, and text-to-speech. Our results show that, on average, semantic tokens outperform compression tokens across most discriminative and generative tasks. However, the performance gap between semantic tokens and standard continuous representations remains substantial, highlighting the need for further research in this field.",1,,include (junior:5),https://arxiv.org/abs/2406.14294v2,https://arxiv.org/pdf/2406.14294v2,2024,semantic tokens,speech tokenization,"{""arxiv_id"": ""2406.14294"", ""title"": ""DASB - Discrete Audio and Speech Benchmark"", ""summary"": ""Discrete audio tokens have recently gained considerable attention for their potential to connect audio and language processing, enabling the creation of modern multimodal large language models. Ideal audio tokens must effectively preserve phonetic and semantic content along with paralinguistic information, speaker identity, and other details. While several types of audio tokens have been recently proposed, identifying the optimal tokenizer for various tasks is challenging due to the inconsistent evaluation settings in existing studies. To address this gap, we release the Discrete Audio and Speech Benchmark (DASB), a comprehensive leaderboard for benchmarking discrete audio tokens across a wide range of discriminative tasks, including speech recognition, speaker identification and verification, emotion recognition, keyword spotting, and intent classification, as well as generative tasks such as speech enhancement, separation, and text-to-speech. Our results show that, on average, semantic tokens outperform compression tokens across most discriminative and generative tasks. However, the performance gap between semantic tokens and standard continuous representations remains substantial, highlighting the need for further research in this field."", ""authors"": [""Pooneh Mousavi"", ""Luca Della Libera"", ""Jarod Duret"", ""Artem Ploujnikov"", ""Cem Subakan"", ""Mirco Ravanelli""], ""published"": ""2024-06-20T13:23:27Z"", ""updated"": ""2024-06-21T17:07:17Z"", ""categories"": [""cs.SD"", ""cs.AI"", ""eess.AS""], ""pdf_url"": ""https://arxiv.org/pdf/2406.14294v2"", ""landing_url"": ""https://arxiv.org/abs/2406.14294v2"", ""doi"": ""https://doi.org/10.48550/arXiv.2406.14294""}",2406.14294,https://openalex.org/W4399911677
10.48550/arxiv.2406.17310,High Fidelity Text-to-Speech Via Discrete Tokens Using Token Transducer and Group Masked Language Model,"We propose a novel two-stage text-to-speech (TTS) framework with two types of discrete tokens, i.e., semantic and acoustic tokens, for high-fidelity speech synthesis. It features two core components: the Interpreting module, which processes text and a speech prompt into semantic tokens focusing on linguistic contents and alignment, and the Speaking module, which captures the timbre of the target voice to generate acoustic tokens from semantic tokens, enriching speech reconstruction. The Interpreting stage employs a transducer for its robustness in aligning text to speech. In contrast, the Speaking stage utilizes a Conformer-based architecture integrated with a Grouped Masked Language Model (G-MLM) to boost computational efficiency. Our experiments verify that this innovative structure surpasses the conventional models in the zero-shot scenario in terms of speech quality and speaker similarity.",1,,include (junior:4),https://arxiv.org/abs/2406.17310v1,https://arxiv.org/pdf/2406.17310v1,2024,discrete speech tokens,text to speech,"{""arxiv_id"": ""2406.17310"", ""title"": ""High Fidelity Text-to-Speech Via Discrete Tokens Using Token Transducer and Group Masked Language Model"", ""summary"": ""We propose a novel two-stage text-to-speech (TTS) framework with two types of discrete tokens, i.e., semantic and acoustic tokens, for high-fidelity speech synthesis. It features two core components: the Interpreting module, which processes text and a speech prompt into semantic tokens focusing on linguistic contents and alignment, and the Speaking module, which captures the timbre of the target voice to generate acoustic tokens from semantic tokens, enriching speech reconstruction. The Interpreting stage employs a transducer for its robustness in aligning text to speech. In contrast, the Speaking stage utilizes a Conformer-based architecture integrated with a Grouped Masked Language Model (G-MLM) to boost computational efficiency. Our experiments verify that this innovative structure surpasses the conventional models in the zero-shot scenario in terms of speech quality and speaker similarity."", ""authors"": [""Joun Yeop Lee"", ""Myeonghun Jeong"", ""Minchan Kim"", ""Ji-Hyun Lee"", ""Hoon-Young Cho"", ""Nam Soo Kim""], ""published"": ""2024-06-25T06:46:47Z"", ""updated"": ""2024-06-25T06:46:47Z"", ""categories"": [""eess.AS""], ""pdf_url"": ""https://arxiv.org/pdf/2406.17310v1"", ""landing_url"": ""https://arxiv.org/abs/2406.17310v1"", ""doi"": ""https://doi.org/10.48550/arXiv.2406.17310""}",2406.1731,https://openalex.org/W4400064835
10.48550/arxiv.2407.03892,On the Effectiveness of Acoustic BPE in Decoder-Only TTS,"Discretizing speech into tokens and generating them by a decoder-only model have been a promising direction for text-to-speech (TTS) and spoken language modeling (SLM). To shorten the sequence length of speech tokens, acoustic byte-pair encoding (BPE) has emerged in SLM that treats speech tokens from self-supervised semantic representations as characters to further compress the token sequence. But the gain in TTS has not been fully investigated, and the proper choice of acoustic BPE remains unclear. In this work, we conduct a comprehensive study on various settings of acoustic BPE to explore its effectiveness in decoder-only TTS models with semantic speech tokens. Experiments on LibriTTS verify that acoustic BPE uniformly increases the intelligibility and diversity of synthesized speech, while showing different features across BPE settings. Hence, acoustic BPE is a favorable tool for decoder-only TTS.",1,,include (junior:5),https://arxiv.org/abs/2407.03892v1,https://arxiv.org/pdf/2407.03892v1,2024,discrete speech tokens,speech tokens,"{""arxiv_id"": ""2407.03892"", ""title"": ""On the Effectiveness of Acoustic BPE in Decoder-Only TTS"", ""summary"": ""Discretizing speech into tokens and generating them by a decoder-only model have been a promising direction for text-to-speech (TTS) and spoken language modeling (SLM). To shorten the sequence length of speech tokens, acoustic byte-pair encoding (BPE) has emerged in SLM that treats speech tokens from self-supervised semantic representations as characters to further compress the token sequence. But the gain in TTS has not been fully investigated, and the proper choice of acoustic BPE remains unclear. In this work, we conduct a comprehensive study on various settings of acoustic BPE to explore its effectiveness in decoder-only TTS models with semantic speech tokens. Experiments on LibriTTS verify that acoustic BPE uniformly increases the intelligibility and diversity of synthesized speech, while showing different features across BPE settings. Hence, acoustic BPE is a favorable tool for decoder-only TTS."", ""authors"": [""Bohan Li"", ""Feiyu Shen"", ""Yiwei Guo"", ""Shuai Wang"", ""Xie Chen"", ""Kai Yu""], ""published"": ""2024-07-04T12:35:32Z"", ""updated"": ""2024-07-04T12:35:32Z"", ""categories"": [""cs.SD"", ""cs.AI"", ""eess.AS""], ""pdf_url"": ""https://arxiv.org/pdf/2407.03892v1"", ""landing_url"": ""https://arxiv.org/abs/2407.03892v1"", ""doi"": ""https://doi.org/10.48550/arXiv.2407.03892""}",2407.03892,https://openalex.org/W4400434132
10.48550/arxiv.2407.05407,CosyVoice: A Scalable Multilingual Zero-shot Text-to-speech Synthesizer based on Supervised Semantic Tokens,"Recent years have witnessed a trend that large language model (LLM) based text-to-speech (TTS) emerges into the mainstream due to their high naturalness and zero-shot capacity. In this paradigm, speech signals are discretized into token sequences, which are modeled by an LLM with text as prompts and reconstructed by a token-based vocoder to waveforms. Obviously, speech tokens play a critical role in LLM-based TTS models. Current speech tokens are learned in an unsupervised manner, which lacks explicit semantic information and alignment to the text. In this paper, we propose to represent speech with supervised semantic tokens, which are derived from a multilingual speech recognition model by inserting vector quantization into the encoder. Based on the tokens, we further propose a scalable zero-shot TTS synthesizer, CosyVoice, which consists of an LLM for text-to-token generation and a conditional flow matching model for token-to-speech synthesis. Experimental results show that supervised semantic tokens significantly outperform existing unsupervised tokens in terms of content consistency and speaker similarity for zero-shot voice cloning. Moreover, we find that utilizing large-scale data further improves the synthesis performance, indicating the scalable capacity of CosyVoice. To the best of our knowledge, this is the first attempt to involve supervised speech tokens into TTS models.",1,,include (junior:5),https://arxiv.org/abs/2407.05407v2,https://arxiv.org/pdf/2407.05407v2,2024,discrete speech tokens,speech tokens,"{""arxiv_id"": ""2407.05407"", ""title"": ""CosyVoice: A Scalable Multilingual Zero-shot Text-to-speech Synthesizer based on Supervised Semantic Tokens"", ""summary"": ""Recent years have witnessed a trend that large language model (LLM) based text-to-speech (TTS) emerges into the mainstream due to their high naturalness and zero-shot capacity. In this paradigm, speech signals are discretized into token sequences, which are modeled by an LLM with text as prompts and reconstructed by a token-based vocoder to waveforms. Obviously, speech tokens play a critical role in LLM-based TTS models. Current speech tokens are learned in an unsupervised manner, which lacks explicit semantic information and alignment to the text. In this paper, we propose to represent speech with supervised semantic tokens, which are derived from a multilingual speech recognition model by inserting vector quantization into the encoder. Based on the tokens, we further propose a scalable zero-shot TTS synthesizer, CosyVoice, which consists of an LLM for text-to-token generation and a conditional flow matching model for token-to-speech synthesis. Experimental results show that supervised semantic tokens significantly outperform existing unsupervised tokens in terms of content consistency and speaker similarity for zero-shot voice cloning. Moreover, we find that utilizing large-scale data further improves the synthesis performance, indicating the scalable capacity of CosyVoice. To the best of our knowledge, this is the first attempt to involve supervised speech tokens into TTS models."", ""authors"": [""Zhihao Du"", ""Qian Chen"", ""Shiliang Zhang"", ""Kai Hu"", ""Heng Lu"", ""Yexin Yang"", ""Hangrui Hu"", ""Siqi Zheng"", ""Yue Gu"", ""Ziyang Ma"", ""Zhifu Gao"", ""Zhijie Yan""], ""published"": ""2024-07-07T15:16:19Z"", ""updated"": ""2024-07-09T07:42:51Z"", ""categories"": [""cs.SD"", ""cs.AI"", ""eess.AS""], ""pdf_url"": ""https://arxiv.org/pdf/2407.05407v2"", ""landing_url"": ""https://arxiv.org/abs/2407.05407v2"", ""doi"": ""https://doi.org/10.48550/arXiv.2407.05407""}",2407.05407,https://openalex.org/W4400518537
10.48550/arxiv.2407.15835,dMel: Speech Tokenization made Simple,"Large language models have revolutionized natural language processing by leveraging self-supervised pretraining on vast textual data. Inspired by this success, researchers have investigated various compression-based speech tokenization methods to discretize continuous speech signals, enabling the application of language modeling techniques to discrete tokens. However, audio compressor introduces additional complexity and computational cost, and often fail on out-of-domain audio signals. In this work, we introduce a novel speech representation (dmel) that discretizes mel-filterbank channels into intensity bins, creating a simpler yet more effective representation compared to existing speech tokenization methods. Our approach demonstrates superior performance in preserving audio content, robustness to out-of-domain data, and offers a training-free, natural, and streamable representation. To address the high-dimensional nature of log-mel spectrograms, we propose an efficient parallel encoding and decoding method for high-dimensional tokens using an LM-style transformer architecture. This innovation enables us to develop RichTTS and RichASR, two models sharing the same architecture while achieving comparable or better results than specialized existing methods. Our results demonstrate the effectiveness of dmel in achieving high performance on both speech synthesis and recognition tasks within a unified framework, paving the way for efficient and effective joint modeling of speech and text.",1,,include (junior:5),https://arxiv.org/abs/2407.15835v3,https://arxiv.org/pdf/2407.15835v3,2024,discrete speech tokens,speech tokens,"{""arxiv_id"": ""2407.15835"", ""title"": ""dMel: Speech Tokenization made Simple"", ""summary"": ""Large language models have revolutionized natural language processing by leveraging self-supervised pretraining on vast textual data. Inspired by this success, researchers have investigated various compression-based speech tokenization methods to discretize continuous speech signals, enabling the application of language modeling techniques to discrete tokens. However, audio compressor introduces additional complexity and computational cost, and often fail on out-of-domain audio signals. In this work, we introduce a novel speech representation (dmel) that discretizes mel-filterbank channels into intensity bins, creating a simpler yet more effective representation compared to existing speech tokenization methods. Our approach demonstrates superior performance in preserving audio content, robustness to out-of-domain data, and offers a training-free, natural, and streamable representation. To address the high-dimensional nature of log-mel spectrograms, we propose an efficient parallel encoding and decoding method for high-dimensional tokens using an LM-style transformer architecture. This innovation enables us to develop RichTTS and RichASR, two models sharing the same architecture while achieving comparable or better results than specialized existing methods. Our results demonstrate the effectiveness of dmel in achieving high performance on both speech synthesis and recognition tasks within a unified framework, paving the way for efficient and effective joint modeling of speech and text."", ""authors"": [""Richard He Bai"", ""Tatiana Likhomanenko"", ""Ruixiang Zhang"", ""Zijin Gu"", ""Zakaria Aldeneh"", ""Navdeep Jaitly""], ""published"": ""2024-07-22T17:51:53Z"", ""updated"": ""2025-05-21T16:55:34Z"", ""categories"": [""cs.CL"", ""cs.AI"", ""cs.SD"", ""eess.AS""], ""pdf_url"": ""https://arxiv.org/pdf/2407.15835v3"", ""landing_url"": ""https://arxiv.org/abs/2407.15835v3"", ""doi"": ""https://doi.org/10.48550/arXiv.2407.15835""}",2407.15835,https://openalex.org/W4402856752
10.1145/3664647.3681680,VoxInstruct: Expressive Human Instruction-to-Speech Generation with Unified Multilingual Codec Language Modelling,"Recent AIGC systems possess the capability to generate digital multimedia content based on human language instructions, such as text, image and video. However, when it comes to speech, existing methods related to human instruction-to-speech generation exhibit two limitations. Firstly, they require the division of inputs into content prompt (transcript) and description prompt (style and speaker), instead of directly supporting human instruction. This division is less natural in form and does not align with other AIGC models. Secondly, the practice of utilizing an independent description prompt to model speech style, without considering the transcript content, restricts the ability to control speech at a fine-grained level. To address these limitations, we propose VoxInstruct, a novel unified multilingual codec language modeling framework that extends traditional text-to-speech tasks into a general human instruction-to-speech task. Our approach enhances the expressiveness of human instruction-guided speech generation and aligns the speech generation paradigm with other modalities. To enable the model to automatically extract the content of synthesized speech from raw text instructions, we introduce speech semantic tokens as an intermediate representation for instruction-to-content guidance. We also incorporate multiple Classifier-Free Guidance (CFG) strategies into our codec language model, which strengthens the generated speech following human instructions. Furthermore, our model architecture and training strategies allow for the simultaneous support of combining speech prompt and descriptive human instruction for expressive speech synthesis, which is a first-of-its-kind attempt. Codes, models and demos are at: https://github.com/thuhcsi/VoxInstruct.",1,,include (senior:4),https://arxiv.org/abs/2408.15676v1,https://arxiv.org/pdf/2408.15676v1,2024,discrete speech tokens,speech generation,"{""arxiv_id"": ""2408.15676"", ""title"": ""VoxInstruct: Expressive Human Instruction-to-Speech Generation with Unified Multilingual Codec Language Modelling"", ""summary"": ""Recent AIGC systems possess the capability to generate digital multimedia content based on human language instructions, such as text, image and video. However, when it comes to speech, existing methods related to human instruction-to-speech generation exhibit two limitations. Firstly, they require the division of inputs into content prompt (transcript) and description prompt (style and speaker), instead of directly supporting human instruction. This division is less natural in form and does not align with other AIGC models. Secondly, the practice of utilizing an independent description prompt to model speech style, without considering the transcript content, restricts the ability to control speech at a fine-grained level. To address these limitations, we propose VoxInstruct, a novel unified multilingual codec language modeling framework that extends traditional text-to-speech tasks into a general human instruction-to-speech task. Our approach enhances the expressiveness of human instruction-guided speech generation and aligns the speech generation paradigm with other modalities. To enable the model to automatically extract the content of synthesized speech from raw text instructions, we introduce speech semantic tokens as an intermediate representation for instruction-to-content guidance. We also incorporate multiple Classifier-Free Guidance (CFG) strategies into our codec language model, which strengthens the generated speech following human instructions. Furthermore, our model architecture and training strategies allow for the simultaneous support of combining speech prompt and descriptive human instruction for expressive speech synthesis, which is a first-of-its-kind attempt. Codes, models and demos are at: https://github.com/thuhcsi/VoxInstruct."", ""authors"": [""Yixuan Zhou"", ""Xiaoyu Qin"", ""Zeyu Jin"", ""Shuoyi Zhou"", ""Shun Lei"", ""Songtao Zhou"", ""Zhiyong Wu"", ""Jia Jia""], ""published"": ""2024-08-28T09:57:17Z"", ""updated"": ""2024-08-28T09:57:17Z"", ""categories"": [""cs.SD"", ""eess.AS""], ""pdf_url"": ""https://arxiv.org/pdf/2408.15676v1"", ""landing_url"": ""https://arxiv.org/abs/2408.15676v1"", ""doi"": ""https://doi.org/10.1145/3664647.3681680""}",2408.15676,https://openalex.org/W4402705930
10.48550/arxiv.2408.16373,Enabling Beam Search for Language Model-Based Text-to-Speech Synthesis,"Tokenising continuous speech into sequences of discrete tokens and modelling them with language models (LMs) has led to significant success in text-to-speech (TTS) synthesis. Although these models can generate speech with high quality and naturalness, their synthesised samples can still suffer from artefacts, mispronunciation, word repeating, etc. In this paper, we argue these undesirable properties could partly be caused by the randomness of sampling-based strategies during the autoregressive decoding of LMs. Therefore, we look at maximisation-based decoding approaches and propose Temporal Repetition Aware Diverse Beam Search (TRAD-BS) to find the most probable sequences of the generated speech tokens. Experiments with two state-of-the-art LM-based TTS models demonstrate that our proposed maximisation-based decoding strategy generates speech with fewer mispronunciations and improved speaker consistency.",1,,include (senior:4),https://arxiv.org/abs/2408.16373v1,https://arxiv.org/pdf/2408.16373v1,2024,discrete speech tokens,speech tokens,"{""arxiv_id"": ""2408.16373"", ""title"": ""Enabling Beam Search for Language Model-Based Text-to-Speech Synthesis"", ""summary"": ""Tokenising continuous speech into sequences of discrete tokens and modelling them with language models (LMs) has led to significant success in text-to-speech (TTS) synthesis. Although these models can generate speech with high quality and naturalness, their synthesised samples can still suffer from artefacts, mispronunciation, word repeating, etc. In this paper, we argue these undesirable properties could partly be caused by the randomness of sampling-based strategies during the autoregressive decoding of LMs. Therefore, we look at maximisation-based decoding approaches and propose Temporal Repetition Aware Diverse Beam Search (TRAD-BS) to find the most probable sequences of the generated speech tokens. Experiments with two state-of-the-art LM-based TTS models demonstrate that our proposed maximisation-based decoding strategy generates speech with fewer mispronunciations and improved speaker consistency."", ""authors"": [""Zehai Tu"", ""Guangyan Zhang"", ""Yiting Lu"", ""Adaeze Adigwe"", ""Simon King"", ""Yiwen Guo""], ""published"": ""2024-08-29T09:31:06Z"", ""updated"": ""2024-08-29T09:31:06Z"", ""categories"": [""cs.SD"", ""eess.AS""], ""pdf_url"": ""https://arxiv.org/pdf/2408.16373v1"", ""landing_url"": ""https://arxiv.org/abs/2408.16373v1"", ""doi"": ""https://doi.org/10.48550/arXiv.2408.16373""}",2408.16373,https://openalex.org/W4402706590
10.48550/arxiv.2408.17175,Codec Does Matter: Exploring the Semantic Shortcoming of Codec for Audio Language Model,"Recent advancements in audio generation have been significantly propelled by the capabilities of Large Language Models (LLMs). The existing research on audio LLM has primarily focused on enhancing the architecture and scale of audio language models, as well as leveraging larger datasets, and generally, acoustic codecs, such as EnCodec, are used for audio tokenization. However, these codecs were originally designed for audio compression, which may lead to suboptimal performance in the context of audio LLM. Our research aims to address the shortcomings of current audio LLM codecs, particularly their challenges in maintaining semantic integrity in generated audio. For instance, existing methods like VALL-E, which condition acoustic token generation on text transcriptions, often suffer from content inaccuracies and elevated word error rates (WER) due to semantic misinterpretations of acoustic tokens, resulting in word skipping and errors. To overcome these issues, we propose a straightforward yet effective approach called X-Codec. X-Codec incorporates semantic features from a pre-trained semantic encoder before the Residual Vector Quantization (RVQ) stage and introduces a semantic reconstruction loss after RVQ. By enhancing the semantic ability of the codec, X-Codec significantly reduces WER in speech synthesis tasks and extends these benefits to non-speech applications, including music and sound generation. Our experiments in text-to-speech, music continuation, and text-to-sound tasks demonstrate that integrating semantic information substantially improves the overall performance of language models in audio generation. Our code and demo are available (Demo: https://x-codec-audio.github.io Code: https://github.com/zhenye234/xcodec)",1,,include (junior:5),https://arxiv.org/abs/2408.17175v3,https://arxiv.org/pdf/2408.17175v3,2024,acoustic tokens,offline clustering,"{""arxiv_id"": ""2408.17175"", ""title"": ""Codec Does Matter: Exploring the Semantic Shortcoming of Codec for Audio Language Model"", ""summary"": ""Recent advancements in audio generation have been significantly propelled by the capabilities of Large Language Models (LLMs). The existing research on audio LLM has primarily focused on enhancing the architecture and scale of audio language models, as well as leveraging larger datasets, and generally, acoustic codecs, such as EnCodec, are used for audio tokenization. However, these codecs were originally designed for audio compression, which may lead to suboptimal performance in the context of audio LLM. Our research aims to address the shortcomings of current audio LLM codecs, particularly their challenges in maintaining semantic integrity in generated audio. For instance, existing methods like VALL-E, which condition acoustic token generation on text transcriptions, often suffer from content inaccuracies and elevated word error rates (WER) due to semantic misinterpretations of acoustic tokens, resulting in word skipping and errors. To overcome these issues, we propose a straightforward yet effective approach called X-Codec. X-Codec incorporates semantic features from a pre-trained semantic encoder before the Residual Vector Quantization (RVQ) stage and introduces a semantic reconstruction loss after RVQ. By enhancing the semantic ability of the codec, X-Codec significantly reduces WER in speech synthesis tasks and extends these benefits to non-speech applications, including music and sound generation. Our experiments in text-to-speech, music continuation, and text-to-sound tasks demonstrate that integrating semantic information substantially improves the overall performance of language models in audio generation. Our code and demo are available (Demo: https://x-codec-audio.github.io Code: https://github.com/zhenye234/xcodec)"", ""authors"": [""Zhen Ye"", ""Peiwen Sun"", ""Jiahe Lei"", ""Hongzhan Lin"", ""Xu Tan"", ""Zheqi Dai"", ""Qiuqiang Kong"", ""Jianyi Chen"", ""Jiahao Pan"", ""Qifeng Liu"", ""Yike Guo"", ""Wei Xue""], ""published"": ""2024-08-30T10:24:07Z"", ""updated"": ""2024-11-27T11:47:45Z"", ""categories"": [""eess.AS"", ""cs.AI"", ""cs.CL"", ""cs.SD""], ""pdf_url"": ""https://arxiv.org/pdf/2408.17175v3"", ""landing_url"": ""https://arxiv.org/abs/2408.17175v3"", ""doi"": ""https://doi.org/10.48550/arXiv.2408.17175""}",2408.17175,https://openalex.org/W4403746867
10.48550/arxiv.2409.00750,MaskGCT: Zero-Shot Text-to-Speech with Masked Generative Codec Transformer,"The recent large-scale text-to-speech (TTS) systems are usually grouped as autoregressive and non-autoregressive systems. The autoregressive systems implicitly model duration but exhibit certain deficiencies in robustness and lack of duration controllability. Non-autoregressive systems require explicit alignment information between text and speech during training and predict durations for linguistic units (e.g. phone), which may compromise their naturalness. In this paper, we introduce Masked Generative Codec Transformer (MaskGCT), a fully non-autoregressive TTS model that eliminates the need for explicit alignment information between text and speech supervision, as well as phone-level duration prediction. MaskGCT is a two-stage model: in the first stage, the model uses text to predict semantic tokens extracted from a speech self-supervised learning (SSL) model, and in the second stage, the model predicts acoustic tokens conditioned on these semantic tokens. MaskGCT follows the mask-and-predict learning paradigm. During training, MaskGCT learns to predict masked semantic or acoustic tokens based on given conditions and prompts. During inference, the model generates tokens of a specified length in a parallel manner. Experiments with 100K hours of in-the-wild speech demonstrate that MaskGCT outperforms the current state-of-the-art zero-shot TTS systems in terms of quality, similarity, and intelligibility. Audio samples are available at https://maskgct.github.io/. We release our code and model checkpoints at https://github.com/open-mmlab/Amphion/blob/main/models/tts/maskgct.",1,,include (junior:5),https://arxiv.org/abs/2409.00750v3,https://arxiv.org/pdf/2409.00750v3,2024,discrete speech tokens,text to speech,"{""arxiv_id"": ""2409.00750"", ""title"": ""MaskGCT: Zero-Shot Text-to-Speech with Masked Generative Codec Transformer"", ""summary"": ""The recent large-scale text-to-speech (TTS) systems are usually grouped as autoregressive and non-autoregressive systems. The autoregressive systems implicitly model duration but exhibit certain deficiencies in robustness and lack of duration controllability. Non-autoregressive systems require explicit alignment information between text and speech during training and predict durations for linguistic units (e.g. phone), which may compromise their naturalness. In this paper, we introduce Masked Generative Codec Transformer (MaskGCT), a fully non-autoregressive TTS model that eliminates the need for explicit alignment information between text and speech supervision, as well as phone-level duration prediction. MaskGCT is a two-stage model: in the first stage, the model uses text to predict semantic tokens extracted from a speech self-supervised learning (SSL) model, and in the second stage, the model predicts acoustic tokens conditioned on these semantic tokens. MaskGCT follows the mask-and-predict learning paradigm. During training, MaskGCT learns to predict masked semantic or acoustic tokens based on given conditions and prompts. During inference, the model generates tokens of a specified length in a parallel manner. Experiments with 100K hours of in-the-wild speech demonstrate that MaskGCT outperforms the current state-of-the-art zero-shot TTS systems in terms of quality, similarity, and intelligibility. Audio samples are available at https://maskgct.github.io/. We release our code and model checkpoints at https://github.com/open-mmlab/Amphion/blob/main/models/tts/maskgct."", ""authors"": [""Yuancheng Wang"", ""Haoyue Zhan"", ""Liwei Liu"", ""Ruihong Zeng"", ""Haotian Guo"", ""Jiachen Zheng"", ""Qiang Zhang"", ""Xueyao Zhang"", ""Shunsi Zhang"", ""Zhizheng Wu""], ""published"": ""2024-09-01T15:26:30Z"", ""updated"": ""2024-10-20T14:25:49Z"", ""categories"": [""cs.SD"", ""cs.AI"", ""cs.LG"", ""eess.AS""], ""pdf_url"": ""https://arxiv.org/pdf/2409.00750v3"", ""landing_url"": ""https://arxiv.org/abs/2409.00750v3"", ""doi"": ""https://doi.org/10.48550/arXiv.2409.00750""}",2409.0075,https://openalex.org/W4402954014
10.48550/arxiv.2409.02384,STAB: Speech Tokenizer Assessment Benchmark,"Representing speech as discrete tokens provides a framework for transforming speech into a format that closely resembles text, thus enabling the use of speech as an input to the widely successful large language models (LLMs). Currently, while several speech tokenizers have been proposed, there is ambiguity regarding the properties that are desired from a tokenizer for specific downstream tasks and its overall generalizability. Evaluating the performance of tokenizers across different downstream tasks is a computationally intensive effort that poses challenges for scalability. To circumvent this requirement, we present STAB (Speech Tokenizer Assessment Benchmark), a systematic evaluation framework designed to assess speech tokenizers comprehensively and shed light on their inherent characteristics. This framework provides a deeper understanding of the underlying mechanisms of speech tokenization, thereby offering a valuable resource for expediting the advancement of future tokenizer models and enabling comparative analysis using a standardized benchmark. We evaluate the STAB metrics and correlate this with downstream task performance across a range of speech tasks and tokenizer choices.",1,,include (junior:5),https://arxiv.org/abs/2409.02384v1,https://arxiv.org/pdf/2409.02384v1,2024,discrete speech tokens,speech tokens,"{""arxiv_id"": ""2409.02384"", ""title"": ""STAB: Speech Tokenizer Assessment Benchmark"", ""summary"": ""Representing speech as discrete tokens provides a framework for transforming speech into a format that closely resembles text, thus enabling the use of speech as an input to the widely successful large language models (LLMs). Currently, while several speech tokenizers have been proposed, there is ambiguity regarding the properties that are desired from a tokenizer for specific downstream tasks and its overall generalizability. Evaluating the performance of tokenizers across different downstream tasks is a computationally intensive effort that poses challenges for scalability. To circumvent this requirement, we present STAB (Speech Tokenizer Assessment Benchmark), a systematic evaluation framework designed to assess speech tokenizers comprehensively and shed light on their inherent characteristics. This framework provides a deeper understanding of the underlying mechanisms of speech tokenization, thereby offering a valuable resource for expediting the advancement of future tokenizer models and enabling comparative analysis using a standardized benchmark. We evaluate the STAB metrics and correlate this with downstream task performance across a range of speech tasks and tokenizer choices."", ""authors"": [""Shikhar Vashishth"", ""Harman Singh"", ""Shikhar Bharadwaj"", ""Sriram Ganapathy"", ""Chulayuth Asawaroengchai"", ""Kartik Audhkhasi"", ""Andrew Rosenberg"", ""Ankur Bapna"", ""Bhuvana Ramabhadran""], ""published"": ""2024-09-04T02:20:59Z"", ""updated"": ""2024-09-04T02:20:59Z"", ""categories"": [""cs.CL"", ""cs.SD"", ""eess.AS""], ""pdf_url"": ""https://arxiv.org/pdf/2409.02384v1"", ""landing_url"": ""https://arxiv.org/abs/2409.02384v1"", ""doi"": ""https://doi.org/10.48550/arXiv.2409.02384""}",2409.02384,https://openalex.org/W4403160068
10.48550/arxiv.2409.03283,FireRedTTS: A Foundation Text-To-Speech Framework for Industry-Level Generative Speech Applications,"This work proposes FireRedTTS, a foundation text-to-speech framework, to meet the growing demands for personalized and diverse generative speech applications. The framework comprises three parts: data processing, foundation system, and downstream applications. First, we comprehensively present our data processing pipeline, which transforms massive raw audio into a large-scale high-quality TTS dataset with rich annotations and a wide coverage of content, speaking style, and timbre. Then, we propose a language-model-based foundation TTS system. The speech signal is compressed into discrete semantic tokens via a semantic-aware speech tokenizer, and can be generated by a language model from the prompt text and audio. Then, a two-stage waveform generator is proposed to decode them to the high-fidelity waveform. We present two applications of this system: voice cloning for dubbing and human-like speech generation for chatbots. The experimental results demonstrate the solid in-context learning capability of FireRedTTS, which can stably synthesize high-quality speech consistent with the prompt text and audio. For dubbing, FireRedTTS can clone target voices in a zero-shot way for the UGC scenario and adapt to studio-level expressive voice characters in the PUGC scenario via few-shot fine-tuning with 1-hour recording. Moreover, FireRedTTS achieves controllable human-like speech generation in a casual style with paralinguistic behaviors and emotions via instruction tuning, to better serve spoken chatbots.",1,,include (junior:4),https://arxiv.org/abs/2409.03283v2,https://arxiv.org/pdf/2409.03283v2,2024,discrete speech tokens,speech tokenizer,"{""arxiv_id"": ""2409.03283"", ""title"": ""FireRedTTS: A Foundation Text-To-Speech Framework for Industry-Level Generative Speech Applications"", ""summary"": ""This work proposes FireRedTTS, a foundation text-to-speech framework, to meet the growing demands for personalized and diverse generative speech applications. The framework comprises three parts: data processing, foundation system, and downstream applications. First, we comprehensively present our data processing pipeline, which transforms massive raw audio into a large-scale high-quality TTS dataset with rich annotations and a wide coverage of content, speaking style, and timbre. Then, we propose a language-model-based foundation TTS system. The speech signal is compressed into discrete semantic tokens via a semantic-aware speech tokenizer, and can be generated by a language model from the prompt text and audio. Then, a two-stage waveform generator is proposed to decode them to the high-fidelity waveform. We present two applications of this system: voice cloning for dubbing and human-like speech generation for chatbots. The experimental results demonstrate the solid in-context learning capability of FireRedTTS, which can stably synthesize high-quality speech consistent with the prompt text and audio. For dubbing, FireRedTTS can clone target voices in a zero-shot way for the UGC scenario and adapt to studio-level expressive voice characters in the PUGC scenario via few-shot fine-tuning with 1-hour recording. Moreover, FireRedTTS achieves controllable human-like speech generation in a casual style with paralinguistic behaviors and emotions via instruction tuning, to better serve spoken chatbots."", ""authors"": [""Hao-Han Guo"", ""Yao Hu"", ""Kun Liu"", ""Fei-Yu Shen"", ""Xu Tang"", ""Yi-Chen Wu"", ""Feng-Long Xie"", ""Kun Xie"", ""Kai-Tuo Xu""], ""published"": ""2024-09-05T06:48:02Z"", ""updated"": ""2025-04-11T07:36:53Z"", ""categories"": [""cs.SD"", ""eess.AS""], ""pdf_url"": ""https://arxiv.org/pdf/2409.03283v2"", ""landing_url"": ""https://arxiv.org/abs/2409.03283v2"", ""doi"": ""https://doi.org/10.48550/arXiv.2409.03283""}",2409.03283,https://openalex.org/W4403556084
10.48550/arxiv.2409.03701,LAST: Language Model Aware Speech Tokenization,"Speech tokenization serves as the foundation of speech language model (LM), enabling them to perform various tasks such as spoken language modeling, text-to-speech, speech-to-text, etc. Most speech tokenizers are trained independently of the LM training process, relying on separate acoustic models and quantization methods. Following such an approach may create a mismatch between the tokenization process and its usage afterward. In this study, we propose a novel approach to training a speech tokenizer by leveraging objectives from pre-trained textual LMs. We advocate for the integration of this objective into the process of learning discrete speech representations. Our aim is to transform features from a pre-trained speech model into a new feature space that enables better clustering for speech LMs. We empirically investigate the impact of various model design choices, including speech vocabulary size and text LM size. Our results demonstrate the proposed tokenization method outperforms the evaluated baselines considering both spoken language modeling and speech-to-text. More importantly, unlike prior work, the proposed method allows the utilization of a single pre-trained LM for processing both speech and text inputs, setting it apart from conventional tokenization approaches.",1,,include (junior:5),https://arxiv.org/abs/2409.03701v2,https://arxiv.org/pdf/2409.03701v2,2024,discrete speech tokens,speech tokens,"{""arxiv_id"": ""2409.03701"", ""title"": ""LAST: Language Model Aware Speech Tokenization"", ""summary"": ""Speech tokenization serves as the foundation of speech language model (LM), enabling them to perform various tasks such as spoken language modeling, text-to-speech, speech-to-text, etc. Most speech tokenizers are trained independently of the LM training process, relying on separate acoustic models and quantization methods. Following such an approach may create a mismatch between the tokenization process and its usage afterward. In this study, we propose a novel approach to training a speech tokenizer by leveraging objectives from pre-trained textual LMs. We advocate for the integration of this objective into the process of learning discrete speech representations. Our aim is to transform features from a pre-trained speech model into a new feature space that enables better clustering for speech LMs. We empirically investigate the impact of various model design choices, including speech vocabulary size and text LM size. Our results demonstrate the proposed tokenization method outperforms the evaluated baselines considering both spoken language modeling and speech-to-text. More importantly, unlike prior work, the proposed method allows the utilization of a single pre-trained LM for processing both speech and text inputs, setting it apart from conventional tokenization approaches."", ""authors"": [""Arnon Turetzky"", ""Yossi Adi""], ""published"": ""2024-09-05T16:57:39Z"", ""updated"": ""2024-09-10T14:45:15Z"", ""categories"": [""cs.CL"", ""cs.SD"", ""eess.AS""], ""pdf_url"": ""https://arxiv.org/pdf/2409.03701v2"", ""landing_url"": ""https://arxiv.org/abs/2409.03701v2"", ""doi"": ""https://doi.org/10.48550/arXiv.2409.03701""}",2409.03701,https://openalex.org/W4403590045
10.48550/arxiv.2409.04016,Investigating Neural Audio Codecs for Speech Language Model-Based Speech Generation,"Neural audio codec tokens serve as the fundamental building blocks for speech language model (SLM)-based speech generation. However, there is no systematic understanding on how the codec system affects the speech generation performance of the SLM. In this work, we examine codec tokens within SLM framework for speech generation to provide insights for effective codec design. We retrain existing high-performing neural codec models on the same data set and loss functions to compare their performance in a uniform setting. We integrate codec tokens into two SLM systems: masked-based parallel speech generation system and an auto-regressive (AR) plus non-auto-regressive (NAR) model-based system. Our findings indicate that better speech reconstruction in codec systems does not guarantee improved speech generation in SLM. A high-quality codec decoder is crucial for natural speech production in SLM, while speech intelligibility depends more on quantization mechanism.",1,,include (junior:5),https://arxiv.org/abs/2409.04016v1,https://arxiv.org/pdf/2409.04016v1,2024,discrete speech tokens,speech generation,"{""arxiv_id"": ""2409.04016"", ""title"": ""Investigating Neural Audio Codecs for Speech Language Model-Based Speech Generation"", ""summary"": ""Neural audio codec tokens serve as the fundamental building blocks for speech language model (SLM)-based speech generation. However, there is no systematic understanding on how the codec system affects the speech generation performance of the SLM. In this work, we examine codec tokens within SLM framework for speech generation to provide insights for effective codec design. We retrain existing high-performing neural codec models on the same data set and loss functions to compare their performance in a uniform setting. We integrate codec tokens into two SLM systems: masked-based parallel speech generation system and an auto-regressive (AR) plus non-auto-regressive (NAR) model-based system. Our findings indicate that better speech reconstruction in codec systems does not guarantee improved speech generation in SLM. A high-quality codec decoder is crucial for natural speech production in SLM, while speech intelligibility depends more on quantization mechanism."", ""authors"": [""Jiaqi Li"", ""Dongmei Wang"", ""Xiaofei Wang"", ""Yao Qian"", ""Long Zhou"", ""Shujie Liu"", ""Midia Yousefi"", ""Canrun Li"", ""Chung-Hsien Tsai"", ""Zhen Xiao"", ""Yanqing Liu"", ""Junkun Chen"", ""Sheng Zhao"", ""Jinyu Li"", ""Zhizheng Wu"", ""Michael Zeng""], ""published"": ""2024-09-06T04:06:50Z"", ""updated"": ""2024-09-06T04:06:50Z"", ""categories"": [""cs.SD"", ""eess.AS""], ""pdf_url"": ""https://arxiv.org/pdf/2409.04016v1"", ""landing_url"": ""https://arxiv.org/abs/2409.04016v1"", ""doi"": ""https://doi.org/10.48550/arXiv.2409.04016""}",2409.04016,https://openalex.org/W4403586121
10.48550/arxiv.2409.05004,Disentangling the Prosody and Semantic Information with Pre-trained Model for In-Context Learning based Zero-Shot Voice Conversion,"Voice conversion (VC) aims to modify the speaker's timbre while retaining speech content. Previous approaches have tokenized the outputs from self-supervised into semantic tokens, facilitating disentanglement of speech content information. Recently, in-context learning (ICL) has emerged in text-to-speech (TTS) systems for effectively modeling specific characteristics such as timbre through context conditioning. This paper proposes an ICL capability enhanced VC system (ICL-VC) employing a mask and reconstruction training strategy based on flow-matching generative models. Augmented with semantic tokens, our experiments on the LibriTTS dataset demonstrate that ICL-VC improves speaker similarity. Additionally, we find that k-means is a versatile tokenization method applicable to various pre-trained models. However, the ICL-VC system faces challenges in preserving the prosody of the source speech. To mitigate this issue, we propose incorporating prosody embeddings extracted from a pre-trained emotion recognition model into our system. Integration of prosody embeddings notably enhances the system's capability to preserve source speech prosody, as validated on the Emotional Speech Database.",1,,include (junior:4),https://arxiv.org/abs/2409.05004v2,https://arxiv.org/pdf/2409.05004v2,2024,semantic tokens,speech tokenization,"{""arxiv_id"": ""2409.05004"", ""title"": ""Disentangling the Prosody and Semantic Information with Pre-trained Model for In-Context Learning based Zero-Shot Voice Conversion"", ""summary"": ""Voice conversion (VC) aims to modify the speaker's timbre while retaining speech content. Previous approaches have tokenized the outputs from self-supervised into semantic tokens, facilitating disentanglement of speech content information. Recently, in-context learning (ICL) has emerged in text-to-speech (TTS) systems for effectively modeling specific characteristics such as timbre through context conditioning. This paper proposes an ICL capability enhanced VC system (ICL-VC) employing a mask and reconstruction training strategy based on flow-matching generative models. Augmented with semantic tokens, our experiments on the LibriTTS dataset demonstrate that ICL-VC improves speaker similarity. Additionally, we find that k-means is a versatile tokenization method applicable to various pre-trained models. However, the ICL-VC system faces challenges in preserving the prosody of the source speech. To mitigate this issue, we propose incorporating prosody embeddings extracted from a pre-trained emotion recognition model into our system. Integration of prosody embeddings notably enhances the system's capability to preserve source speech prosody, as validated on the Emotional Speech Database."", ""authors"": [""Zhengyang Chen"", ""Shuai Wang"", ""Mingyang Zhang"", ""Xuechen Liu"", ""Junichi Yamagishi"", ""Yanmin Qian""], ""published"": ""2024-09-08T07:24:03Z"", ""updated"": ""2024-09-10T07:36:03Z"", ""categories"": [""cs.SD"", ""eess.AS""], ""pdf_url"": ""https://arxiv.org/pdf/2409.05004v2"", ""landing_url"": ""https://arxiv.org/abs/2409.05004v2"", ""doi"": ""https://doi.org/10.48550/arXiv.2409.05004""}",2409.05004,https://openalex.org/W4403590122
10.1145/3677388.3696320,ProbTalk3D: Non-Deterministic Emotion Controllable Speech-Driven 3D Facial Animation Synthesis Using VQ-VAE,"Audio-driven 3D facial animation synthesis has been an active field of research with attention from both academia and industry. While there are promising results in this area, recent approaches largely focus on lip-sync and identity control, neglecting the role of emotions and emotion control in the generative process. That is mainly due to the lack of emotionally rich facial animation data and algorithms that can synthesize speech animations with emotional expressions at the same time. In addition, majority of the models are deterministic, meaning given the same audio input, they produce the same output motion. We argue that emotions and non-determinism are crucial to generate diverse and emotionally-rich facial animations. In this paper, we propose ProbTalk3D a non-deterministic neural network approach for emotion controllable speech-driven 3D facial animation synthesis using a two-stage VQ-VAE model and an emotionally rich facial animation dataset 3DMEAD. We provide an extensive comparative analysis of our model against the recent 3D facial animation synthesis approaches, by evaluating the results objectively, qualitatively, and with a perceptual user study. We highlight several objective metrics that are more suitable for evaluating stochastic outputs and use both in-the-wild and ground truth data for subjective evaluation. To our knowledge, that is the first non-deterministic 3D facial animation synthesis method incorporating a rich emotion dataset and emotion control with emotion labels and intensity levels. Our evaluation demonstrates that the proposed model achieves superior performance compared to state-of-the-art emotion-controlled, deterministic and non-deterministic models. We recommend watching the supplementary video for quality judgement. The entire codebase is publicly available (https://github.com/uuembodiedsocialai/ProbTalk3D/).",1,,include (senior:4),https://arxiv.org/abs/2409.07966v4,https://arxiv.org/pdf/2409.07966v4,2024,discrete speech tokens,vq vae,"{""arxiv_id"": ""2409.07966"", ""title"": ""ProbTalk3D: Non-Deterministic Emotion Controllable Speech-Driven 3D Facial Animation Synthesis Using VQ-VAE"", ""summary"": ""Audio-driven 3D facial animation synthesis has been an active field of research with attention from both academia and industry. While there are promising results in this area, recent approaches largely focus on lip-sync and identity control, neglecting the role of emotions and emotion control in the generative process. That is mainly due to the lack of emotionally rich facial animation data and algorithms that can synthesize speech animations with emotional expressions at the same time. In addition, majority of the models are deterministic, meaning given the same audio input, they produce the same output motion. We argue that emotions and non-determinism are crucial to generate diverse and emotionally-rich facial animations. In this paper, we propose ProbTalk3D a non-deterministic neural network approach for emotion controllable speech-driven 3D facial animation synthesis using a two-stage VQ-VAE model and an emotionally rich facial animation dataset 3DMEAD. We provide an extensive comparative analysis of our model against the recent 3D facial animation synthesis approaches, by evaluating the results objectively, qualitatively, and with a perceptual user study. We highlight several objective metrics that are more suitable for evaluating stochastic outputs and use both in-the-wild and ground truth data for subjective evaluation. To our knowledge, that is the first non-deterministic 3D facial animation synthesis method incorporating a rich emotion dataset and emotion control with emotion labels and intensity levels. Our evaluation demonstrates that the proposed model achieves superior performance compared to state-of-the-art emotion-controlled, deterministic and non-deterministic models. We recommend watching the supplementary video for quality judgement. The entire codebase is publicly available (https://github.com/uuembodiedsocialai/ProbTalk3D/)."", ""authors"": [""Sichun Wu"", ""Kazi Injamamul Haque"", ""Zerrin Yumak""], ""published"": ""2024-09-12T11:53:05Z"", ""updated"": ""2025-02-16T14:23:08Z"", ""categories"": [""cs.CV"", ""cs.AI""], ""pdf_url"": ""https://arxiv.org/pdf/2409.07966v4"", ""landing_url"": ""https://arxiv.org/abs/2409.07966v4"", ""doi"": ""https://doi.org/10.1145/3677388.3696320""}",2409.07966,https://openalex.org/W4403788878
10.48550/arxiv.2409.12717,NDVQ: Robust Neural Audio Codec with Normal Distribution-Based Vector Quantization,"Built upon vector quantization (VQ), discrete audio codec models have achieved great success in audio compression and auto-regressive audio generation. However, existing models face substantial challenges in perceptual quality and signal distortion, especially when operating in extremely low bandwidth, rooted in the sensitivity of the VQ codebook to noise. This degradation poses significant challenges for several downstream tasks, such as codec-based speech synthesis. To address this issue, we propose a novel VQ method, Normal Distribution-based Vector Quantization (NDVQ), by introducing an explicit margin between the VQ codes via learning a variance. Specifically, our approach involves mapping the waveform to a latent space and quantizing it by selecting the most likely normal distribution, with each codebook entry representing a unique normal distribution defined by its mean and variance. Using these distribution-based VQ codec codes, a decoder reconstructs the input waveform. NDVQ is trained with additional distribution-related losses, alongside reconstruction and discrimination losses. Experiments demonstrate that NDVQ outperforms existing audio compression baselines, such as EnCodec, in terms of audio quality and zero-shot TTS, particularly in very low bandwidth scenarios.",1,,include (junior:5),https://arxiv.org/abs/2409.12717v1,https://arxiv.org/pdf/2409.12717v1,2024,discrete speech tokens,vector quantization,"{""arxiv_id"": ""2409.12717"", ""title"": ""NDVQ: Robust Neural Audio Codec with Normal Distribution-Based Vector Quantization"", ""summary"": ""Built upon vector quantization (VQ), discrete audio codec models have achieved great success in audio compression and auto-regressive audio generation. However, existing models face substantial challenges in perceptual quality and signal distortion, especially when operating in extremely low bandwidth, rooted in the sensitivity of the VQ codebook to noise. This degradation poses significant challenges for several downstream tasks, such as codec-based speech synthesis. To address this issue, we propose a novel VQ method, Normal Distribution-based Vector Quantization (NDVQ), by introducing an explicit margin between the VQ codes via learning a variance. Specifically, our approach involves mapping the waveform to a latent space and quantizing it by selecting the most likely normal distribution, with each codebook entry representing a unique normal distribution defined by its mean and variance. Using these distribution-based VQ codec codes, a decoder reconstructs the input waveform. NDVQ is trained with additional distribution-related losses, alongside reconstruction and discrimination losses. Experiments demonstrate that NDVQ outperforms existing audio compression baselines, such as EnCodec, in terms of audio quality and zero-shot TTS, particularly in very low bandwidth scenarios."", ""authors"": [""Zhikang Niu"", ""Sanyuan Chen"", ""Long Zhou"", ""Ziyang Ma"", ""Xie Chen"", ""Shujie Liu""], ""published"": ""2024-09-19T12:41:30Z"", ""updated"": ""2024-09-19T12:41:30Z"", ""categories"": [""eess.AS"", ""cs.SD""], ""pdf_url"": ""https://arxiv.org/pdf/2409.12717v1"", ""landing_url"": ""https://arxiv.org/abs/2409.12717v1"", ""doi"": ""https://doi.org/10.48550/arXiv.2409.12717""}",2409.12717,https://openalex.org/W4403747696
10.48550/arxiv.2409.18042,"EMOVA: Empowering Language Models to See, Hear and Speak with Vivid Emotions","GPT-4o, an omni-modal model that enables vocal conversations with diverse emotions and tones, marks a milestone for omni-modal foundation models. However, empowering Large Language Models to perceive and generate images, texts, and speeches end-to-end with publicly available data remains challenging for the open-source community. Existing vision-language models rely on external tools for speech processing, while speech-language models still suffer from limited or totally without vision-understanding capabilities. To address this gap, we propose the EMOVA (EMotionally Omni-present Voice Assistant), to enable Large Language Models with end-to-end speech abilities while maintaining the leading vision-language performance. With a semantic-acoustic disentangled speech tokenizer, we surprisingly notice that omni-modal alignment can further enhance vision-language and speech abilities compared with the bi-modal aligned counterparts. Moreover, a lightweight style module is introduced for the flexible speech style controls including emotions and pitches. For the first time, EMOVA achieves state-of-the-art performance on both the vision-language and speech benchmarks, and meanwhile, supporting omni-modal spoken dialogue with vivid emotions.",1,,include (junior:4),https://arxiv.org/abs/2409.18042v4,https://arxiv.org/pdf/2409.18042v4,2024,discrete speech tokens,speech tokenizer,"{""arxiv_id"": ""2409.18042"", ""title"": ""EMOVA: Empowering Language Models to See, Hear and Speak with Vivid Emotions"", ""summary"": ""GPT-4o, an omni-modal model that enables vocal conversations with diverse emotions and tones, marks a milestone for omni-modal foundation models. However, empowering Large Language Models to perceive and generate images, texts, and speeches end-to-end with publicly available data remains challenging for the open-source community. Existing vision-language models rely on external tools for speech processing, while speech-language models still suffer from limited or totally without vision-understanding capabilities. To address this gap, we propose the EMOVA (EMotionally Omni-present Voice Assistant), to enable Large Language Models with end-to-end speech abilities while maintaining the leading vision-language performance. With a semantic-acoustic disentangled speech tokenizer, we surprisingly notice that omni-modal alignment can further enhance vision-language and speech abilities compared with the bi-modal aligned counterparts. Moreover, a lightweight style module is introduced for the flexible speech style controls including emotions and pitches. For the first time, EMOVA achieves state-of-the-art performance on both the vision-language and speech benchmarks, and meanwhile, supporting omni-modal spoken dialogue with vivid emotions."", ""authors"": [""Kai Chen"", ""Yunhao Gou"", ""Runhui Huang"", ""Zhili Liu"", ""Daxin Tan"", ""Jing Xu"", ""Chunwei Wang"", ""Yi Zhu"", ""Yihan Zeng"", ""Kuo Yang"", ""Dingdong Wang"", ""Kun Xiang"", ""Haoyuan Li"", ""Haoli Bai"", ""Jianhua Han"", ""Xiaohui Li"", ""Weike Jin"", ""Nian Xie"", ""Yu Zhang"", ""James T. Kwok"", ""Hengshuang Zhao"", ""Xiaodan Liang"", ""Dit-Yan Yeung"", ""Xiao Chen"", ""Zhenguo Li"", ""Wei Zhang"", ""Qun Liu"", ""Jun Yao"", ""Lanqing Hong"", ""Lu Hou"", ""Hang Xu""], ""published"": ""2024-09-26T16:44:02Z"", ""updated"": ""2025-03-20T08:47:39Z"", ""categories"": [""cs.CV"", ""cs.CL""], ""pdf_url"": ""https://arxiv.org/pdf/2409.18042v4"", ""landing_url"": ""https://arxiv.org/abs/2409.18042v4"", ""doi"": ""https://doi.org/10.48550/arXiv.2409.18042""}",2409.18042,https://openalex.org/W4403797068
10.48550/arxiv.2410.00037,Moshi: a speech-text foundation model for real-time dialogue,"We introduce Moshi, a speech-text foundation model and full-duplex spoken dialogue framework. Current systems for spoken dialogue rely on pipelines of independent components, namely voice activity detection, speech recognition, textual dialogue and text-to-speech. Such frameworks cannot emulate the experience of real conversations. First, their complexity induces a latency of several seconds between interactions. Second, text being the intermediate modality for dialogue, non-linguistic information that modifies meaning -- such as emotion or non-speech sounds -- is lost in the interaction. Finally, they rely on a segmentation into speaker turns, which does not take into account overlapping speech, interruptions and interjections. Moshi solves these independent issues altogether by casting spoken dialogue as speech-to-speech generation. Starting from a text language model backbone, Moshi generates speech as tokens from the residual quantizer of a neural audio codec, while modeling separately its own speech and that of the user into parallel streams. This allows for the removal of explicit speaker turns, and the modeling of arbitrary conversational dynamics. We moreover extend the hierarchical semantic-to-acoustic token generation of previous work to first predict time-aligned text tokens as a prefix to audio tokens. Not only this ""Inner Monologue"" method significantly improves the linguistic quality of generated speech, but we also illustrate how it can provide streaming speech recognition and text-to-speech. Our resulting model is the first real-time full-duplex spoken large language model, with a theoretical latency of 160ms, 200ms in practice, and is available at https://github.com/kyutai-labs/moshi.",1,,include (junior:5),https://arxiv.org/abs/2410.00037v2,https://arxiv.org/pdf/2410.00037v2,2024,acoustic tokens,offline clustering,"{""arxiv_id"": ""2410.00037"", ""title"": ""Moshi: a speech-text foundation model for real-time dialogue"", ""summary"": ""We introduce Moshi, a speech-text foundation model and full-duplex spoken dialogue framework. Current systems for spoken dialogue rely on pipelines of independent components, namely voice activity detection, speech recognition, textual dialogue and text-to-speech. Such frameworks cannot emulate the experience of real conversations. First, their complexity induces a latency of several seconds between interactions. Second, text being the intermediate modality for dialogue, non-linguistic information that modifies meaning -- such as emotion or non-speech sounds -- is lost in the interaction. Finally, they rely on a segmentation into speaker turns, which does not take into account overlapping speech, interruptions and interjections. Moshi solves these independent issues altogether by casting spoken dialogue as speech-to-speech generation. Starting from a text language model backbone, Moshi generates speech as tokens from the residual quantizer of a neural audio codec, while modeling separately its own speech and that of the user into parallel streams. This allows for the removal of explicit speaker turns, and the modeling of arbitrary conversational dynamics. We moreover extend the hierarchical semantic-to-acoustic token generation of previous work to first predict time-aligned text tokens as a prefix to audio tokens. Not only this \""Inner Monologue\"" method significantly improves the linguistic quality of generated speech, but we also illustrate how it can provide streaming speech recognition and text-to-speech. Our resulting model is the first real-time full-duplex spoken large language model, with a theoretical latency of 160ms, 200ms in practice, and is available at https://github.com/kyutai-labs/moshi."", ""authors"": [""Alexandre Défossez"", ""Laurent Mazaré"", ""Manu Orsini"", ""Amélie Royer"", ""Patrick Pérez"", ""Hervé Jégou"", ""Edouard Grave"", ""Neil Zeghidour""], ""published"": ""2024-09-17T17:55:39Z"", ""updated"": ""2024-10-02T09:11:45Z"", ""categories"": [""eess.AS"", ""cs.AI"", ""cs.CL"", ""cs.LG"", ""cs.SD""], ""pdf_url"": ""https://arxiv.org/pdf/2410.00037v2"", ""landing_url"": ""https://arxiv.org/abs/2410.00037v2"", ""doi"": ""https://doi.org/10.48550/arXiv.2410.00037""}",2410.00037,https://openalex.org/W4403883071
10.48550/arxiv.2410.03298,Textless Streaming Speech-to-Speech Translation using Semantic Speech Tokens,"Cascaded speech-to-speech translation systems often suffer from the error accumulation problem and high latency, which is a result of cascaded modules whose inference delays accumulate. In this paper, we propose a transducer-based speech translation model that outputs discrete speech tokens in a low-latency streaming fashion. This approach eliminates the need for generating text output first, followed by machine translation (MT) and text-to-speech (TTS) systems. The produced speech tokens can be directly used to generate a speech signal with low latency by utilizing an acoustic language model (LM) to obtain acoustic tokens and an audio codec model to retrieve the waveform. Experimental results show that the proposed method outperforms other existing approaches and achieves state-of-the-art results for streaming translation in terms of BLEU, average latency, and BLASER 2.0 scores for multiple language pairs using the CVSS-C dataset as a benchmark.",1,,include (junior:5),https://arxiv.org/abs/2410.03298v1,https://arxiv.org/pdf/2410.03298v1,2024,discrete speech tokens,speech tokens,"{""arxiv_id"": ""2410.03298"", ""title"": ""Textless Streaming Speech-to-Speech Translation using Semantic Speech Tokens"", ""summary"": ""Cascaded speech-to-speech translation systems often suffer from the error accumulation problem and high latency, which is a result of cascaded modules whose inference delays accumulate. In this paper, we propose a transducer-based speech translation model that outputs discrete speech tokens in a low-latency streaming fashion. This approach eliminates the need for generating text output first, followed by machine translation (MT) and text-to-speech (TTS) systems. The produced speech tokens can be directly used to generate a speech signal with low latency by utilizing an acoustic language model (LM) to obtain acoustic tokens and an audio codec model to retrieve the waveform. Experimental results show that the proposed method outperforms other existing approaches and achieves state-of-the-art results for streaming translation in terms of BLEU, average latency, and BLASER 2.0 scores for multiple language pairs using the CVSS-C dataset as a benchmark."", ""authors"": [""Jinzheng Zhao"", ""Niko Moritz"", ""Egor Lakomkin"", ""Ruiming Xie"", ""Zhiping Xiu"", ""Katerina Zmolikova"", ""Zeeshan Ahmed"", ""Yashesh Gaur"", ""Duc Le"", ""Christian Fuegen""], ""published"": ""2024-10-04T10:21:15Z"", ""updated"": ""2024-10-04T10:21:15Z"", ""categories"": [""eess.AS""], ""pdf_url"": ""https://arxiv.org/pdf/2410.03298v1"", ""landing_url"": ""https://arxiv.org/abs/2410.03298v1"", ""doi"": ""https://doi.org/10.48550/arXiv.2410.03298""}",2410.03298,https://openalex.org/W4403899613
10.48550/arxiv.2410.06016,Variable Bitrate Residual Vector Quantization for Audio Coding,"Recent state-of-the-art neural audio compression models have progressively adopted residual vector quantization (RVQ). Despite this success, these models employ a fixed number of codebooks per frame, which can be suboptimal in terms of rate-distortion tradeoff, particularly in scenarios with simple input audio, such as silence. To address this limitation, we propose variable bitrate RVQ (VRVQ) for audio codecs, which allows for more efficient coding by adapting the number of codebooks used per frame. Furthermore, we propose a gradient estimation method for the non-differentiable masking operation that transforms from the importance map to the binary importance mask, improving model training via a straight-through estimator. We demonstrate that the proposed training framework achieves superior results compared to the baseline method and shows further improvement when applied to the current state-of-the-art codec.",1,,include (junior:4),https://arxiv.org/abs/2410.06016v3,https://arxiv.org/pdf/2410.06016v3,2024,discrete speech tokens,vector quantization,"{""arxiv_id"": ""2410.06016"", ""title"": ""Variable Bitrate Residual Vector Quantization for Audio Coding"", ""summary"": ""Recent state-of-the-art neural audio compression models have progressively adopted residual vector quantization (RVQ). Despite this success, these models employ a fixed number of codebooks per frame, which can be suboptimal in terms of rate-distortion tradeoff, particularly in scenarios with simple input audio, such as silence. To address this limitation, we propose variable bitrate RVQ (VRVQ) for audio codecs, which allows for more efficient coding by adapting the number of codebooks used per frame. Furthermore, we propose a gradient estimation method for the non-differentiable masking operation that transforms from the importance map to the binary importance mask, improving model training via a straight-through estimator. We demonstrate that the proposed training framework achieves superior results compared to the baseline method and shows further improvement when applied to the current state-of-the-art codec."", ""authors"": [""Yunkee Chae"", ""Woosung Choi"", ""Yuhta Takida"", ""Junghyun Koo"", ""Yukara Ikemiya"", ""Zhi Zhong"", ""Kin Wai Cheuk"", ""Marco A. Martínez-Ramírez"", ""Kyogu Lee"", ""Wei-Hsiang Liao"", ""Yuki Mitsufuji""], ""published"": ""2024-10-08T13:18:24Z"", ""updated"": ""2025-04-27T15:10:16Z"", ""categories"": [""cs.SD"", ""cs.LG"", ""eess.AS""], ""pdf_url"": ""https://arxiv.org/pdf/2410.06016v3"", ""landing_url"": ""https://arxiv.org/abs/2410.06016v3"", ""doi"": ""https://doi.org/10.48550/arXiv.2410.06016""}",2410.06016,https://openalex.org/W4403344546
10.48550/arxiv.2410.07168,Sylber: Syllabic Embedding Representation of Speech from Raw Audio,"Syllables are compositional units of spoken language that efficiently structure human speech perception and production. However, current neural speech representations lack such structure, resulting in dense token sequences that are costly to process. To bridge this gap, we propose a new model, Sylber, that produces speech representations with clean and robust syllabic structure. Specifically, we propose a self-supervised learning (SSL) framework that bootstraps syllabic embeddings by distilling from its own initial unsupervised syllabic segmentation. This results in a highly structured representation of speech features, offering three key benefits: 1) a fast, linear-time syllable segmentation algorithm, 2) efficient syllabic tokenization with an average of 4.27 tokens per second, and 3) novel phonological units suited for efficient spoken language modeling. Our proposed segmentation method is highly robust and generalizes to out-of-domain data and unseen languages without any tuning. By training token-to-speech generative models, fully intelligible speech can be reconstructed from Sylber tokens with a significantly lower bitrate than baseline SSL tokens. This suggests that our model effectively compresses speech into a compact sequence of tokens with minimal information loss. Lastly, we demonstrate that categorical perception-a linguistic phenomenon in speech perception-emerges naturally in Sylber, making the embedding space more categorical and sparse than previous speech features and thus supporting the high efficiency of our tokenization. Together, we present a novel SSL approach for representing speech as syllables, with significant potential for efficient speech tokenization and spoken language modeling.",1,,include (junior:5),https://arxiv.org/abs/2410.07168v2,https://arxiv.org/pdf/2410.07168v2,2024,discrete speech tokens,speech tokenization,"{""arxiv_id"": ""2410.07168"", ""title"": ""Sylber: Syllabic Embedding Representation of Speech from Raw Audio"", ""summary"": ""Syllables are compositional units of spoken language that efficiently structure human speech perception and production. However, current neural speech representations lack such structure, resulting in dense token sequences that are costly to process. To bridge this gap, we propose a new model, Sylber, that produces speech representations with clean and robust syllabic structure. Specifically, we propose a self-supervised learning (SSL) framework that bootstraps syllabic embeddings by distilling from its own initial unsupervised syllabic segmentation. This results in a highly structured representation of speech features, offering three key benefits: 1) a fast, linear-time syllable segmentation algorithm, 2) efficient syllabic tokenization with an average of 4.27 tokens per second, and 3) novel phonological units suited for efficient spoken language modeling. Our proposed segmentation method is highly robust and generalizes to out-of-domain data and unseen languages without any tuning. By training token-to-speech generative models, fully intelligible speech can be reconstructed from Sylber tokens with a significantly lower bitrate than baseline SSL tokens. This suggests that our model effectively compresses speech into a compact sequence of tokens with minimal information loss. Lastly, we demonstrate that categorical perception-a linguistic phenomenon in speech perception-emerges naturally in Sylber, making the embedding space more categorical and sparse than previous speech features and thus supporting the high efficiency of our tokenization. Together, we present a novel SSL approach for representing speech as syllables, with significant potential for efficient speech tokenization and spoken language modeling."", ""authors"": [""Cheol Jun Cho"", ""Nicholas Lee"", ""Akshat Gupta"", ""Dhruv Agarwal"", ""Ethan Chen"", ""Alan W Black"", ""Gopala K. Anumanchipalli""], ""published"": ""2024-10-09T17:59:04Z"", ""updated"": ""2025-03-02T09:16:05Z"", ""categories"": [""cs.CL"", ""cs.SD"", ""eess.AS""], ""pdf_url"": ""https://arxiv.org/pdf/2410.07168v2"", ""landing_url"": ""https://arxiv.org/abs/2410.07168v2"", ""doi"": ""https://doi.org/10.48550/arXiv.2410.07168""}",2410.07168,https://openalex.org/W4403345965
10.21437/interspeech.2024-2366,Low Bitrate High-Quality RVQGAN-based Discrete Speech Tokenizer,"Discrete Audio codecs (or audio tokenizers) have recently regained interest due to the ability of Large Language Models (LLMs) to learn their compressed acoustic representations. Various publicly available trainable discrete tokenizers recently demonstrated impressive results for audio tokenization, yet they mostly require high token rates to gain high-quality reconstruction. In this study, we fine-tuned an open-source general audio RVQGAN model using diverse open-source speech data, considering various recording conditions and quality levels. The resulting wideband (24kHz) speech-only model achieves speech reconstruction, which is nearly indistinguishable from PCM (pulse-code modulation) with a rate of 150-300 tokens per second (1500-3000 bps). The evaluation used comprehensive English speech data encompassing different recording conditions, including studio settings. Speech samples are made publicly available in http://ibm.biz/IS24SpeechRVQ . The model is officially released in https://huggingface.co/ibm/DAC.speech.v1.0",1,,include (junior:5),https://arxiv.org/abs/2410.08325v1,https://arxiv.org/pdf/2410.08325v1,2024,discrete speech tokens,speech tokens,"{""arxiv_id"": ""2410.08325"", ""title"": ""Low Bitrate High-Quality RVQGAN-based Discrete Speech Tokenizer"", ""summary"": ""Discrete Audio codecs (or audio tokenizers) have recently regained interest due to the ability of Large Language Models (LLMs) to learn their compressed acoustic representations. Various publicly available trainable discrete tokenizers recently demonstrated impressive results for audio tokenization, yet they mostly require high token rates to gain high-quality reconstruction. In this study, we fine-tuned an open-source general audio RVQGAN model using diverse open-source speech data, considering various recording conditions and quality levels. The resulting wideband (24kHz) speech-only model achieves speech reconstruction, which is nearly indistinguishable from PCM (pulse-code modulation) with a rate of 150-300 tokens per second (1500-3000 bps). The evaluation used comprehensive English speech data encompassing different recording conditions, including studio settings. Speech samples are made publicly available in http://ibm.biz/IS24SpeechRVQ . The model is officially released in https://huggingface.co/ibm/DAC.speech.v1.0"", ""authors"": [""Slava Shechtman"", ""Avihu Dekel""], ""published"": ""2024-10-10T19:29:05Z"", ""updated"": ""2024-10-10T19:29:05Z"", ""categories"": [""eess.AS""], ""pdf_url"": ""https://arxiv.org/pdf/2410.08325v1"", ""landing_url"": ""https://arxiv.org/abs/2410.08325v1"", ""doi"": ""https://doi.org/10.21437/Interspeech.2024-2366""}",2410.08325,https://openalex.org/W4402112548
10.48550/arxiv.2410.12359,ERVQ: Enhanced Residual Vector Quantization with Intra-and-Inter-Codebook Optimization for Neural Audio Codecs,"Current neural audio codecs typically use residual vector quantization (RVQ) to discretize speech signals. However, they often experience codebook collapse, which reduces the effective codebook size and leads to suboptimal performance. To address this problem, we introduce ERVQ, Enhanced Residual Vector Quantization, a novel enhancement strategy for the RVQ framework in neural audio codecs. ERVQ mitigates codebook collapse and boosts codec performance through both intra- and inter-codebook optimization. Intra-codebook optimization incorporates an online clustering strategy and a code balancing loss to ensure balanced and efficient codebook utilization. Inter-codebook optimization improves the diversity of quantized features by minimizing the similarity between successive quantizations. Our experiments show that ERVQ significantly enhances audio codec performance across different models, sampling rates, and bitrates, achieving superior quality and generalization capabilities. It also achieves 100% codebook utilization on one of the most advanced neural audio codecs. Further experiments indicate that audio codecs improved by the ERVQ strategy can improve unified speech-and-text large language models (LLMs). Specifically, there is a notable improvement in the naturalness of generated speech in downstream zero-shot text-to-speech tasks. Audio samples are available here.",1,,include (junior:5),https://arxiv.org/abs/2410.12359v2,https://arxiv.org/pdf/2410.12359v2,2024,discrete speech tokens,vector quantization,"{""arxiv_id"": ""2410.12359"", ""title"": ""ERVQ: Enhanced Residual Vector Quantization with Intra-and-Inter-Codebook Optimization for Neural Audio Codecs"", ""summary"": ""Current neural audio codecs typically use residual vector quantization (RVQ) to discretize speech signals. However, they often experience codebook collapse, which reduces the effective codebook size and leads to suboptimal performance. To address this problem, we introduce ERVQ, Enhanced Residual Vector Quantization, a novel enhancement strategy for the RVQ framework in neural audio codecs. ERVQ mitigates codebook collapse and boosts codec performance through both intra- and inter-codebook optimization. Intra-codebook optimization incorporates an online clustering strategy and a code balancing loss to ensure balanced and efficient codebook utilization. Inter-codebook optimization improves the diversity of quantized features by minimizing the similarity between successive quantizations. Our experiments show that ERVQ significantly enhances audio codec performance across different models, sampling rates, and bitrates, achieving superior quality and generalization capabilities. It also achieves 100% codebook utilization on one of the most advanced neural audio codecs. Further experiments indicate that audio codecs improved by the ERVQ strategy can improve unified speech-and-text large language models (LLMs). Specifically, there is a notable improvement in the naturalness of generated speech in downstream zero-shot text-to-speech tasks. Audio samples are available here."", ""authors"": [""Rui-Chen Zheng"", ""Hui-Peng Du"", ""Xiao-Hang Jiang"", ""Yang Ai"", ""Zhen-Hua Ling""], ""published"": ""2024-10-16T08:21:37Z"", ""updated"": ""2025-06-11T08:43:47Z"", ""categories"": [""eess.AS""], ""pdf_url"": ""https://arxiv.org/pdf/2410.12359v2"", ""landing_url"": ""https://arxiv.org/abs/2410.12359v2"", ""doi"": ""https://doi.org/10.48550/arXiv.2410.12359""}",2410.12359,https://openalex.org/W4403578055
10.48550/arxiv.2410.15017,DM-Codec: Distilling Multimodal Representations for Speech Tokenization,"Recent advancements in speech-language models have yielded significant improvements in speech tokenization and synthesis. However, effectively mapping the complex, multidimensional attributes of speech into discrete tokens remains challenging. This process demands acoustic, semantic, and contextual information for precise speech representations. Existing speech representations generally fall into two categories: acoustic tokens from audio codecs and semantic tokens from speech self-supervised learning models. Although recent efforts have unified acoustic and semantic tokens for improved performance, they overlook the crucial role of contextual representation in comprehensive speech modeling. Our empirical investigations reveal that the absence of contextual representations results in elevated Word Error Rate (WER) and Word Information Lost (WIL) scores in speech transcriptions. To address these limitations, we propose two novel distillation approaches: (1) a language model (LM)-guided distillation method that incorporates contextual information, and (2) a combined LM and self-supervised speech model (SM)-guided distillation technique that effectively distills multimodal representations (acoustic, semantic, and contextual) into a comprehensive speech tokenizer, termed DM-Codec. The DM-Codec architecture adopts a streamlined encoder-decoder framework with a Residual Vector Quantizer (RVQ) and incorporates the LM and SM during the training process. Experiments show DM-Codec significantly outperforms state-of-the-art speech tokenization models, reducing WER by up to 13.46%, WIL by 9.82%, and improving speech quality by 5.84% and intelligibility by 1.85% on the LibriSpeech benchmark dataset. Code, samples, and checkpoints are available at https://github.com/mubtasimahasan/DM-Codec.",1,,include (junior:5),https://arxiv.org/abs/2410.15017v2,https://arxiv.org/pdf/2410.15017v2,2024,discrete speech tokens,speech tokens,"{""arxiv_id"": ""2410.15017"", ""title"": ""DM-Codec: Distilling Multimodal Representations for Speech Tokenization"", ""summary"": ""Recent advancements in speech-language models have yielded significant improvements in speech tokenization and synthesis. However, effectively mapping the complex, multidimensional attributes of speech into discrete tokens remains challenging. This process demands acoustic, semantic, and contextual information for precise speech representations. Existing speech representations generally fall into two categories: acoustic tokens from audio codecs and semantic tokens from speech self-supervised learning models. Although recent efforts have unified acoustic and semantic tokens for improved performance, they overlook the crucial role of contextual representation in comprehensive speech modeling. Our empirical investigations reveal that the absence of contextual representations results in elevated Word Error Rate (WER) and Word Information Lost (WIL) scores in speech transcriptions. To address these limitations, we propose two novel distillation approaches: (1) a language model (LM)-guided distillation method that incorporates contextual information, and (2) a combined LM and self-supervised speech model (SM)-guided distillation technique that effectively distills multimodal representations (acoustic, semantic, and contextual) into a comprehensive speech tokenizer, termed DM-Codec. The DM-Codec architecture adopts a streamlined encoder-decoder framework with a Residual Vector Quantizer (RVQ) and incorporates the LM and SM during the training process. Experiments show DM-Codec significantly outperforms state-of-the-art speech tokenization models, reducing WER by up to 13.46%, WIL by 9.82%, and improving speech quality by 5.84% and intelligibility by 1.85% on the LibriSpeech benchmark dataset. Code, samples, and checkpoints are available at https://github.com/mubtasimahasan/DM-Codec."", ""authors"": [""Md Mubtasim Ahasan"", ""Md Fahim"", ""Tasnim Mohiuddin"", ""A K M Mahbubur Rahman"", ""Aman Chadha"", ""Tariq Iqbal"", ""M Ashraful Amin"", ""Md Mofijul Islam"", ""Amin Ahsan Ali""], ""published"": ""2024-10-19T07:14:14Z"", ""updated"": ""2025-09-29T08:08:40Z"", ""categories"": [""cs.CL"", ""cs.AI"", ""cs.SD"", ""eess.AS""], ""pdf_url"": ""https://arxiv.org/pdf/2410.15017v2"", ""landing_url"": ""https://arxiv.org/abs/2410.15017v2"", ""doi"": ""https://doi.org/10.48550/arXiv.2410.15017""}",2410.15017,https://openalex.org/W4404088256
10.48550/arxiv.2410.15764,LSCodec: Low-Bitrate and Speaker-Decoupled Discrete Speech Codec,"Although discrete speech tokens have exhibited strong potential for language model-based speech generation, their high bitrates and redundant timbre information restrict the development of such models. In this work, we propose LSCodec, a discrete speech codec that has both low bitrate and speaker decoupling ability. LSCodec adopts a multi-stage unsupervised training framework with a speaker perturbation technique. A continuous information bottleneck is first established, followed by vector quantization that produces a discrete speaker-decoupled space. A discrete token vocoder finally refines acoustic details from LSCodec. By reconstruction evaluations, LSCodec demonstrates superior intelligibility and audio quality with only a single codebook and smaller vocabulary size than baselines. Voice conversion and speaker probing experiments prove the excellent speaker disentanglement of LSCodec, and ablation study verifies the effectiveness of the proposed training framework.",1,,include (junior:5),https://arxiv.org/abs/2410.15764v3,https://arxiv.org/pdf/2410.15764v3,2024,discrete speech tokens,speech tokens,"{""arxiv_id"": ""2410.15764"", ""title"": ""LSCodec: Low-Bitrate and Speaker-Decoupled Discrete Speech Codec"", ""summary"": ""Although discrete speech tokens have exhibited strong potential for language model-based speech generation, their high bitrates and redundant timbre information restrict the development of such models. In this work, we propose LSCodec, a discrete speech codec that has both low bitrate and speaker decoupling ability. LSCodec adopts a multi-stage unsupervised training framework with a speaker perturbation technique. A continuous information bottleneck is first established, followed by vector quantization that produces a discrete speaker-decoupled space. A discrete token vocoder finally refines acoustic details from LSCodec. By reconstruction evaluations, LSCodec demonstrates superior intelligibility and audio quality with only a single codebook and smaller vocabulary size than baselines. Voice conversion and speaker probing experiments prove the excellent speaker disentanglement of LSCodec, and ablation study verifies the effectiveness of the proposed training framework."", ""authors"": [""Yiwei Guo"", ""Zhihan Li"", ""Chenpeng Du"", ""Hankun Wang"", ""Xie Chen"", ""Kai Yu""], ""published"": ""2024-10-21T08:23:31Z"", ""updated"": ""2025-05-21T16:46:32Z"", ""categories"": [""eess.AS"", ""cs.AI"", ""cs.SD""], ""pdf_url"": ""https://arxiv.org/pdf/2410.15764v3"", ""landing_url"": ""https://arxiv.org/abs/2410.15764v3"", ""doi"": ""https://doi.org/10.48550/arXiv.2410.15764""}",2410.15764,https://openalex.org/W4404088948
10.48550/arxiv.2410.24177,DC-Spin: A Speaker-invariant Speech Tokenizer for Spoken Language Models,"Spoken language models (SLMs) have gained increasing attention with advancements in text-based, decoder-only language models. SLMs process text and speech, enabling simultaneous speech understanding and generation. This paper presents Double-Codebook Speaker-invariant Clustering (DC-Spin), which aims to improve speech tokenization by bridging audio signals and SLM tokens. DC-Spin extracts speaker-invariant tokens rich in phonetic information and resilient to input variations, enhancing zero-shot SLM tasks and speech resynthesis. We propose a chunk-wise approach to enable streamable DC-Spin without retraining and degradation. Comparisons of tokenization methods (self-supervised and neural audio codecs), model scalability, and downstream task proxies show that tokens easily modeled by an n-gram LM or aligned with phonemes offer strong performance, providing insights for designing speech tokenizers for SLMs.",1,,include (junior:5),https://arxiv.org/abs/2410.24177v1,https://arxiv.org/pdf/2410.24177v1,2024,discrete speech tokens,speech tokens,"{""arxiv_id"": ""2410.24177"", ""title"": ""DC-Spin: A Speaker-invariant Speech Tokenizer for Spoken Language Models"", ""summary"": ""Spoken language models (SLMs) have gained increasing attention with advancements in text-based, decoder-only language models. SLMs process text and speech, enabling simultaneous speech understanding and generation. This paper presents Double-Codebook Speaker-invariant Clustering (DC-Spin), which aims to improve speech tokenization by bridging audio signals and SLM tokens. DC-Spin extracts speaker-invariant tokens rich in phonetic information and resilient to input variations, enhancing zero-shot SLM tasks and speech resynthesis. We propose a chunk-wise approach to enable streamable DC-Spin without retraining and degradation. Comparisons of tokenization methods (self-supervised and neural audio codecs), model scalability, and downstream task proxies show that tokens easily modeled by an n-gram LM or aligned with phonemes offer strong performance, providing insights for designing speech tokenizers for SLMs."", ""authors"": [""Heng-Jui Chang"", ""Hongyu Gong"", ""Changhan Wang"", ""James Glass"", ""Yu-An Chung""], ""published"": ""2024-10-31T17:43:13Z"", ""updated"": ""2024-10-31T17:43:13Z"", ""categories"": [""eess.AS"", ""cs.CL"", ""cs.LG"", ""cs.SD""], ""pdf_url"": ""https://arxiv.org/pdf/2410.24177v1"", ""landing_url"": ""https://arxiv.org/abs/2410.24177v1"", ""doi"": ""https://doi.org/10.48550/arXiv.2410.24177""}",2410.24177,https://openalex.org/W4404348734
10.48550/arxiv.2411.08742,A Comparative Study of Discrete Speech Tokens for Semantic-Related Tasks with Large Language Models,"With the rise of Speech Large Language Models (Speech LLMs), there has been growing interest in discrete speech tokens for their ability to integrate with text-based tokens seamlessly. Compared to most studies that focus on continuous speech features, although discrete-token based LLMs have shown promising results on certain tasks, the performance gap between these two paradigms is rarely explored. In this paper, we present a fair and thorough comparison between discrete and continuous features across a variety of semantic-related tasks using a light-weight LLM (Qwen1.5-0.5B). Our findings reveal that continuous features generally outperform discrete tokens, particularly in tasks requiring fine-grained semantic understanding. Moreover, this study goes beyond surface-level comparison by identifying key factors behind the under-performance of discrete tokens, such as limited token granularity and inefficient information retention. To enhance the performance of discrete tokens, we explore potential aspects based on our analysis. We hope our results can offer new insights into the opportunities for advancing discrete speech tokens in Speech LLMs.",1,,include (junior:4),https://arxiv.org/abs/2411.08742v1,https://arxiv.org/pdf/2411.08742v1,2024,discrete speech tokens,speech tokens,"{""arxiv_id"": ""2411.08742"", ""title"": ""A Comparative Study of Discrete Speech Tokens for Semantic-Related Tasks with Large Language Models"", ""summary"": ""With the rise of Speech Large Language Models (Speech LLMs), there has been growing interest in discrete speech tokens for their ability to integrate with text-based tokens seamlessly. Compared to most studies that focus on continuous speech features, although discrete-token based LLMs have shown promising results on certain tasks, the performance gap between these two paradigms is rarely explored. In this paper, we present a fair and thorough comparison between discrete and continuous features across a variety of semantic-related tasks using a light-weight LLM (Qwen1.5-0.5B). Our findings reveal that continuous features generally outperform discrete tokens, particularly in tasks requiring fine-grained semantic understanding. Moreover, this study goes beyond surface-level comparison by identifying key factors behind the under-performance of discrete tokens, such as limited token granularity and inefficient information retention. To enhance the performance of discrete tokens, we explore potential aspects based on our analysis. We hope our results can offer new insights into the opportunities for advancing discrete speech tokens in Speech LLMs."", ""authors"": [""Dingdong Wang"", ""Mingyu Cui"", ""Dongchao Yang"", ""Xueyuan Chen"", ""Helen Meng""], ""published"": ""2024-11-13T16:20:20Z"", ""updated"": ""2024-11-13T16:20:20Z"", ""categories"": [""cs.CL"", ""cs.SD"", ""eess.AS""], ""pdf_url"": ""https://arxiv.org/pdf/2411.08742v1"", ""landing_url"": ""https://arxiv.org/abs/2411.08742v1"", ""doi"": ""https://doi.org/10.48550/arXiv.2411.08742""}",2411.08742,https://openalex.org/W4404408071
10.48550/arxiv.2411.14100,BEST-STD: Bidirectional Mamba-Enhanced Speech Tokenization for Spoken Term Detection,"Spoken term detection (STD) is often hindered by reliance on frame-level features and the computationally intensive DTW-based template matching, limiting its practicality. To address these challenges, we propose a novel approach that encodes speech into discrete, speaker-agnostic semantic tokens. This facilitates fast retrieval using text-based search algorithms and effectively handles out-of-vocabulary terms. Our approach focuses on generating consistent token sequences across varying utterances of the same term. We also propose a bidirectional state space modeling within the Mamba encoder, trained in a self-supervised learning framework, to learn contextual frame-level features that are further encoded into discrete tokens. Our analysis shows that our speech tokens exhibit greater speaker invariance than those from existing tokenizers, making them more suitable for STD tasks. Empirical evaluation on LibriSpeech and TIMIT databases indicates that our method outperforms existing STD baselines while being more efficient.",1,,include (senior:4),https://arxiv.org/abs/2411.14100v2,https://arxiv.org/pdf/2411.14100v2,2024,discrete speech tokens,speech tokens,"{""arxiv_id"": ""2411.14100"", ""title"": ""BEST-STD: Bidirectional Mamba-Enhanced Speech Tokenization for Spoken Term Detection"", ""summary"": ""Spoken term detection (STD) is often hindered by reliance on frame-level features and the computationally intensive DTW-based template matching, limiting its practicality. To address these challenges, we propose a novel approach that encodes speech into discrete, speaker-agnostic semantic tokens. This facilitates fast retrieval using text-based search algorithms and effectively handles out-of-vocabulary terms. Our approach focuses on generating consistent token sequences across varying utterances of the same term. We also propose a bidirectional state space modeling within the Mamba encoder, trained in a self-supervised learning framework, to learn contextual frame-level features that are further encoded into discrete tokens. Our analysis shows that our speech tokens exhibit greater speaker invariance than those from existing tokenizers, making them more suitable for STD tasks. Empirical evaluation on LibriSpeech and TIMIT databases indicates that our method outperforms existing STD baselines while being more efficient."", ""authors"": [""Anup Singh"", ""Kris Demuynck"", ""Vipul Arora""], ""published"": ""2024-11-21T13:05:18Z"", ""updated"": ""2024-12-21T19:15:27Z"", ""categories"": [""eess.AS"", ""cs.CL"", ""cs.IR""], ""pdf_url"": ""https://arxiv.org/pdf/2411.14100v2"", ""landing_url"": ""https://arxiv.org/abs/2411.14100v2"", ""doi"": ""https://doi.org/10.48550/arXiv.2411.14100""}",2411.141,https://openalex.org/W4404652703
10.48550/arxiv.2411.14642,VQalAttent: a Transparent Speech Generation Pipeline based on Transformer-learned VQ-VAE Latent Space,"Generating high-quality speech efficiently remains a key challenge for generative models in speech synthesis. This paper introduces VQalAttent, a lightweight model designed to generate fake speech with tunable performance and interpretability. Leveraging the AudioMNIST dataset, consisting of human utterances of decimal digits (0-9), our method employs a two-step architecture: first, a scalable vector quantized autoencoder (VQ-VAE) that compresses audio spectrograms into discrete latent representations, and second, a decoder-only transformer that learns the probability model of these latents. Trained transformer generates similar latent sequences, convertible to audio spectrograms by the VQ-VAE decoder, from which we generate fake utterances. Interpreting statistical and perceptual quality of the fakes, depending on the dimension and the extrinsic information of the latent space, enables guided improvements in larger, commercial generative models. As a valuable tool for understanding and refining audio synthesis, our results demonstrate VQalAttent's capacity to generate intelligible speech samples with limited computational resources, while the modularity and transparency of the training pipeline helps easily correlate the analytics with modular modifications, hence providing insights for the more complex models.",1,,include (junior:5),https://arxiv.org/abs/2411.14642v1,https://arxiv.org/pdf/2411.14642v1,2024,discrete speech tokens,vq vae,"{""arxiv_id"": ""2411.14642"", ""title"": ""VQalAttent: a Transparent Speech Generation Pipeline based on Transformer-learned VQ-VAE Latent Space"", ""summary"": ""Generating high-quality speech efficiently remains a key challenge for generative models in speech synthesis. This paper introduces VQalAttent, a lightweight model designed to generate fake speech with tunable performance and interpretability. Leveraging the AudioMNIST dataset, consisting of human utterances of decimal digits (0-9), our method employs a two-step architecture: first, a scalable vector quantized autoencoder (VQ-VAE) that compresses audio spectrograms into discrete latent representations, and second, a decoder-only transformer that learns the probability model of these latents. Trained transformer generates similar latent sequences, convertible to audio spectrograms by the VQ-VAE decoder, from which we generate fake utterances. Interpreting statistical and perceptual quality of the fakes, depending on the dimension and the extrinsic information of the latent space, enables guided improvements in larger, commercial generative models. As a valuable tool for understanding and refining audio synthesis, our results demonstrate VQalAttent's capacity to generate intelligible speech samples with limited computational resources, while the modularity and transparency of the training pipeline helps easily correlate the analytics with modular modifications, hence providing insights for the more complex models."", ""authors"": [""Armani Rodriguez"", ""Silvija Kokalj-Filipovic""], ""published"": ""2024-11-22T00:21:39Z"", ""updated"": ""2024-11-22T00:21:39Z"", ""categories"": [""cs.LG"", ""eess.AS""], ""pdf_url"": ""https://arxiv.org/pdf/2411.14642v1"", ""landing_url"": ""https://arxiv.org/abs/2411.14642v1"", ""doi"": ""https://doi.org/10.48550/arXiv.2411.14642""}",2411.14642,https://openalex.org/W4404985564
10.48550/arxiv.2411.17607,Scaling Speech-Text Pre-training with Synthetic Interleaved Data,"Speech language models (SpeechLMs) accept speech input and produce speech output, allowing for more natural human-computer interaction compared to text-based large language models (LLMs). Traditional approaches for developing SpeechLMs are constrained by the limited availability of unsupervised speech data and parallel speech-text data, which are significantly less abundant than text pre-training data, thereby limiting their scalability as LLMs. We propose a novel approach to scaling speech-text pre-training by leveraging large-scale synthetic interleaved data derived from text corpora, eliminating the need for parallel speech-text datasets. Our method efficiently constructs speech-text interleaved data by sampling text spans from existing text corpora and synthesizing corresponding speech spans using a text-to-token model, bypassing the need to generate actual speech. We also employ a supervised speech tokenizer derived from an automatic speech recognition (ASR) model by incorporating a vector-quantized bottleneck into the encoder. This supervised training approach results in discrete speech tokens with strong semantic preservation even at lower frame rates (e.g. 12.5Hz), while still maintaining speech reconstruction quality. Starting from a pre-trained language model and scaling our pre-training to 1 trillion tokens (with 600B synthetic interleaved speech-text data), we achieve state-of-the-art performance in speech language modeling and spoken question answering, improving performance on spoken questions tasks from the previous SOTA of 13% (Moshi) to 31%. We further demonstrate that by fine-tuning the pre-trained model with speech dialogue data, we can develop an end-to-end spoken chatbot that achieves competitive performance comparable to existing baselines in both conversational abilities and speech quality, even operating exclusively in the speech domain.",1,,include (junior:5),https://arxiv.org/abs/2411.17607v2,https://arxiv.org/pdf/2411.17607v2,2024,discrete speech tokens,speech tokens,"{""arxiv_id"": ""2411.17607"", ""title"": ""Scaling Speech-Text Pre-training with Synthetic Interleaved Data"", ""summary"": ""Speech language models (SpeechLMs) accept speech input and produce speech output, allowing for more natural human-computer interaction compared to text-based large language models (LLMs). Traditional approaches for developing SpeechLMs are constrained by the limited availability of unsupervised speech data and parallel speech-text data, which are significantly less abundant than text pre-training data, thereby limiting their scalability as LLMs. We propose a novel approach to scaling speech-text pre-training by leveraging large-scale synthetic interleaved data derived from text corpora, eliminating the need for parallel speech-text datasets. Our method efficiently constructs speech-text interleaved data by sampling text spans from existing text corpora and synthesizing corresponding speech spans using a text-to-token model, bypassing the need to generate actual speech. We also employ a supervised speech tokenizer derived from an automatic speech recognition (ASR) model by incorporating a vector-quantized bottleneck into the encoder. This supervised training approach results in discrete speech tokens with strong semantic preservation even at lower frame rates (e.g. 12.5Hz), while still maintaining speech reconstruction quality. Starting from a pre-trained language model and scaling our pre-training to 1 trillion tokens (with 600B synthetic interleaved speech-text data), we achieve state-of-the-art performance in speech language modeling and spoken question answering, improving performance on spoken questions tasks from the previous SOTA of 13% (Moshi) to 31%. We further demonstrate that by fine-tuning the pre-trained model with speech dialogue data, we can develop an end-to-end spoken chatbot that achieves competitive performance comparable to existing baselines in both conversational abilities and speech quality, even operating exclusively in the speech domain."", ""authors"": [""Aohan Zeng"", ""Zhengxiao Du"", ""Mingdao Liu"", ""Lei Zhang"", ""Shengmin Jiang"", ""Yuxiao Dong"", ""Jie Tang""], ""published"": ""2024-11-26T17:19:09Z"", ""updated"": ""2024-12-02T16:13:24Z"", ""categories"": [""cs.CL"", ""cs.SD"", ""eess.AS""], ""pdf_url"": ""https://arxiv.org/pdf/2411.17607v2"", ""landing_url"": ""https://arxiv.org/abs/2411.17607v2"", ""doi"": ""https://doi.org/10.48550/arXiv.2411.17607""}",2411.17607,https://openalex.org/W4405031987
10.48550/arxiv.2412.02612,GLM-4-Voice: Towards Intelligent and Human-Like End-to-End Spoken Chatbot,"We introduce GLM-4-Voice, an intelligent and human-like end-to-end spoken chatbot. It supports both Chinese and English, engages in real-time voice conversations, and varies vocal nuances such as emotion, intonation, speech rate, and dialect according to user instructions. GLM-4-Voice uses an ultra-low bitrate (175bps), single-codebook speech tokenizer with 12.5Hz frame rate derived from an automatic speech recognition (ASR) model by incorporating a vector-quantized bottleneck into the encoder. To efficiently transfer knowledge from text to speech modalities, we synthesize speech-text interleaved data from existing text pre-training corpora using a text-to-token model. We continue pre-training from the pre-trained text language model GLM-4-9B with a combination of unsupervised speech data, interleaved speech-text data, and supervised speech-text data, scaling up to 1 trillion tokens, achieving state-of-the-art performance in both speech language modeling and spoken question answering. We then fine-tune the pre-trained model with high-quality conversational speech data, achieving superior performance compared to existing baselines in both conversational ability and speech quality. The open models can be accessed through https://github.com/THUDM/GLM-4-Voice and https://huggingface.co/THUDM/glm-4-voice-9b.",1,,include (junior:5),https://arxiv.org/abs/2412.02612v1,https://arxiv.org/pdf/2412.02612v1,2024,discrete speech tokens,speech tokenizer,"{""arxiv_id"": ""2412.02612"", ""title"": ""GLM-4-Voice: Towards Intelligent and Human-Like End-to-End Spoken Chatbot"", ""summary"": ""We introduce GLM-4-Voice, an intelligent and human-like end-to-end spoken chatbot. It supports both Chinese and English, engages in real-time voice conversations, and varies vocal nuances such as emotion, intonation, speech rate, and dialect according to user instructions. GLM-4-Voice uses an ultra-low bitrate (175bps), single-codebook speech tokenizer with 12.5Hz frame rate derived from an automatic speech recognition (ASR) model by incorporating a vector-quantized bottleneck into the encoder. To efficiently transfer knowledge from text to speech modalities, we synthesize speech-text interleaved data from existing text pre-training corpora using a text-to-token model. We continue pre-training from the pre-trained text language model GLM-4-9B with a combination of unsupervised speech data, interleaved speech-text data, and supervised speech-text data, scaling up to 1 trillion tokens, achieving state-of-the-art performance in both speech language modeling and spoken question answering. We then fine-tune the pre-trained model with high-quality conversational speech data, achieving superior performance compared to existing baselines in both conversational ability and speech quality. The open models can be accessed through https://github.com/THUDM/GLM-4-Voice and https://huggingface.co/THUDM/glm-4-voice-9b."", ""authors"": [""Aohan Zeng"", ""Zhengxiao Du"", ""Mingdao Liu"", ""Kedong Wang"", ""Shengmin Jiang"", ""Lei Zhao"", ""Yuxiao Dong"", ""Jie Tang""], ""published"": ""2024-12-03T17:41:24Z"", ""updated"": ""2024-12-03T17:41:24Z"", ""categories"": [""cs.CL"", ""cs.SD"", ""eess.AS""], ""pdf_url"": ""https://arxiv.org/pdf/2412.02612v1"", ""landing_url"": ""https://arxiv.org/abs/2412.02612v1"", ""doi"": ""https://doi.org/10.48550/arXiv.2412.02612""}",2412.02612,https://openalex.org/W4405057125
10.48550/arxiv.2412.10117,CosyVoice 2: Scalable Streaming Speech Synthesis with Large Language Models,"In our previous work, we introduced CosyVoice, a multilingual speech synthesis model based on supervised discrete speech tokens. By employing progressive semantic decoding with two popular generative models, language models (LMs) and Flow Matching, CosyVoice demonstrated high prosody naturalness, content consistency, and speaker similarity in speech in-context learning. Recently, significant progress has been made in multi-modal large language models (LLMs), where the response latency and real-time factor of speech synthesis play a crucial role in the interactive experience. Therefore, in this report, we present an improved streaming speech synthesis model, CosyVoice 2, which incorporates comprehensive and systematic optimizations. Specifically, we introduce finite-scalar quantization to improve the codebook utilization of speech tokens. For the text-speech LM, we streamline the model architecture to allow direct use of a pre-trained LLM as the backbone. In addition, we develop a chunk-aware causal flow matching model to support various synthesis scenarios, enabling both streaming and non-streaming synthesis within a single model. By training on a large-scale multilingual dataset, CosyVoice 2 achieves human-parity naturalness, minimal response latency, and virtually lossless synthesis quality in the streaming mode. We invite readers to listen to the demos at https://funaudiollm.github.io/cosyvoice2.",1,,include (junior:4),https://arxiv.org/abs/2412.10117v3,https://arxiv.org/pdf/2412.10117v3,2024,discrete speech tokens,speech tokens,"{""arxiv_id"": ""2412.10117"", ""title"": ""CosyVoice 2: Scalable Streaming Speech Synthesis with Large Language Models"", ""summary"": ""In our previous work, we introduced CosyVoice, a multilingual speech synthesis model based on supervised discrete speech tokens. By employing progressive semantic decoding with two popular generative models, language models (LMs) and Flow Matching, CosyVoice demonstrated high prosody naturalness, content consistency, and speaker similarity in speech in-context learning. Recently, significant progress has been made in multi-modal large language models (LLMs), where the response latency and real-time factor of speech synthesis play a crucial role in the interactive experience. Therefore, in this report, we present an improved streaming speech synthesis model, CosyVoice 2, which incorporates comprehensive and systematic optimizations. Specifically, we introduce finite-scalar quantization to improve the codebook utilization of speech tokens. For the text-speech LM, we streamline the model architecture to allow direct use of a pre-trained LLM as the backbone. In addition, we develop a chunk-aware causal flow matching model to support various synthesis scenarios, enabling both streaming and non-streaming synthesis within a single model. By training on a large-scale multilingual dataset, CosyVoice 2 achieves human-parity naturalness, minimal response latency, and virtually lossless synthesis quality in the streaming mode. We invite readers to listen to the demos at https://funaudiollm.github.io/cosyvoice2."", ""authors"": [""Zhihao Du"", ""Yuxuan Wang"", ""Qian Chen"", ""Xian Shi"", ""Xiang Lv"", ""Tianyu Zhao"", ""Zhifu Gao"", ""Yexin Yang"", ""Changfeng Gao"", ""Hui Wang"", ""Fan Yu"", ""Huadai Liu"", ""Zhengyan Sheng"", ""Yue Gu"", ""Chong Deng"", ""Wen Wang"", ""Shiliang Zhang"", ""Zhijie Yan"", ""Jingren Zhou""], ""published"": ""2024-12-13T12:59:39Z"", ""updated"": ""2024-12-25T11:54:03Z"", ""categories"": [""cs.SD"", ""cs.AI"", ""cs.LG"", ""eess.AS""], ""pdf_url"": ""https://arxiv.org/pdf/2412.10117v3"", ""landing_url"": ""https://arxiv.org/abs/2412.10117v3"", ""doi"": ""https://doi.org/10.48550/arXiv.2412.10117""}",2412.10117,https://openalex.org/W4405433404
10.48550/arxiv.2412.11449,Whisper-GPT: A Hybrid Representation Audio Large Language Model,"We propose WHISPER-GPT: A generative large language model (LLM) for speech and music that allows us to work with continuous audio representations and discrete tokens simultaneously as part of a single architecture. There has been a huge surge in generative audio, speech, and music models that utilize discrete audio tokens derived from neural compression algorithms, e.g. ENCODEC. However, one of the major drawbacks of this approach is handling the context length. It blows up for high-fidelity generative architecture if one has to account for all the audio contents at various frequencies for the next token prediction. By combining continuous audio representation like the spectrogram and discrete acoustic tokens, we retain the best of both worlds: Have all the information needed from the audio at a specific time instance in a single token, yet allow LLM to predict the future token to allow for sampling and other benefits discrete space provides. We show how our architecture improves the perplexity and negative log-likelihood scores for the next token prediction compared to a token-based LLM for speech and music.",1,,include (junior:4),https://arxiv.org/abs/2412.11449v1,https://arxiv.org/pdf/2412.11449v1,2024,acoustic tokens,offline clustering,"{""arxiv_id"": ""2412.11449"", ""title"": ""Whisper-GPT: A Hybrid Representation Audio Large Language Model"", ""summary"": ""We propose WHISPER-GPT: A generative large language model (LLM) for speech and music that allows us to work with continuous audio representations and discrete tokens simultaneously as part of a single architecture. There has been a huge surge in generative audio, speech, and music models that utilize discrete audio tokens derived from neural compression algorithms, e.g. ENCODEC. However, one of the major drawbacks of this approach is handling the context length. It blows up for high-fidelity generative architecture if one has to account for all the audio contents at various frequencies for the next token prediction. By combining continuous audio representation like the spectrogram and discrete acoustic tokens, we retain the best of both worlds: Have all the information needed from the audio at a specific time instance in a single token, yet allow LLM to predict the future token to allow for sampling and other benefits discrete space provides. We show how our architecture improves the perplexity and negative log-likelihood scores for the next token prediction compared to a token-based LLM for speech and music."", ""authors"": [""Prateek Verma""], ""published"": ""2024-12-16T05:03:48Z"", ""updated"": ""2024-12-16T05:03:48Z"", ""categories"": [""cs.SD"", ""cs.AI"", ""cs.CL"", ""cs.LG"", ""eess.AS""], ""pdf_url"": ""https://arxiv.org/pdf/2412.11449v1"", ""landing_url"": ""https://arxiv.org/abs/2412.11449v1"", ""doi"": ""https://doi.org/10.48550/arXiv.2412.11449""}",2412.11449,https://openalex.org/W4405470171
10.48550/arxiv.2412.15649,SLAM-Omni: Timbre-Controllable Voice Interaction System with Single-Stage Training,"Recent advancements highlight the potential of end-to-end real-time spoken dialogue systems, showcasing their low latency and high quality. In this paper, we introduce SLAM-Omni, a timbre-controllable, end-to-end voice interaction system with single-stage training. SLAM-Omni achieves zero-shot timbre control by modeling spoken language with semantic tokens and decoupling speaker information to a vocoder. By predicting grouped speech semantic tokens at each step, our method significantly reduces the sequence length of audio tokens, accelerating both training and inference. Additionally, we propose historical text prompting to compress dialogue history, facilitating efficient multi-round interactions. Comprehensive evaluations reveal that SLAM-Omni outperforms prior models of similar scale, requiring only 15 hours of training on 4 GPUs with limited data. Notably, it is the first spoken dialogue system to achieve competitive performance with a single-stage training approach, eliminating the need for pre-training on TTS or ASR tasks. Further experiments validate its multilingual and multi-turn dialogue capabilities on larger datasets.",1,,include (junior:4),https://arxiv.org/abs/2412.15649v1,https://arxiv.org/pdf/2412.15649v1,2024,semantic tokens,speech tokenization,"{""arxiv_id"": ""2412.15649"", ""title"": ""SLAM-Omni: Timbre-Controllable Voice Interaction System with Single-Stage Training"", ""summary"": ""Recent advancements highlight the potential of end-to-end real-time spoken dialogue systems, showcasing their low latency and high quality. In this paper, we introduce SLAM-Omni, a timbre-controllable, end-to-end voice interaction system with single-stage training. SLAM-Omni achieves zero-shot timbre control by modeling spoken language with semantic tokens and decoupling speaker information to a vocoder. By predicting grouped speech semantic tokens at each step, our method significantly reduces the sequence length of audio tokens, accelerating both training and inference. Additionally, we propose historical text prompting to compress dialogue history, facilitating efficient multi-round interactions. Comprehensive evaluations reveal that SLAM-Omni outperforms prior models of similar scale, requiring only 15 hours of training on 4 GPUs with limited data. Notably, it is the first spoken dialogue system to achieve competitive performance with a single-stage training approach, eliminating the need for pre-training on TTS or ASR tasks. Further experiments validate its multilingual and multi-turn dialogue capabilities on larger datasets."", ""authors"": [""Wenxi Chen"", ""Ziyang Ma"", ""Ruiqi Yan"", ""Yuzhe Liang"", ""Xiquan Li"", ""Ruiyang Xu"", ""Zhikang Niu"", ""Yanqiao Zhu"", ""Yifan Yang"", ""Zhanxun Liu"", ""Kai Yu"", ""Yuxuan Hu"", ""Jinyu Li"", ""Yan Lu"", ""Shujie Liu"", ""Xie Chen""], ""published"": ""2024-12-20T08:05:55Z"", ""updated"": ""2024-12-20T08:05:55Z"", ""categories"": [""eess.AS""], ""pdf_url"": ""https://arxiv.org/pdf/2412.15649v1"", ""landing_url"": ""https://arxiv.org/abs/2412.15649v1"", ""doi"": ""https://doi.org/10.48550/arXiv.2412.15649""}",2412.15649,https://openalex.org/W4405714880
10.48550/arxiv.2412.19248,Causal Speech Enhancement with Predicting Semantics based on Quantized Self-supervised Learning Features,"Real-time speech enhancement (SE) is essential to online speech communication. Causal SE models use only the previous context while predicting future information, such as phoneme continuation, may help performing causal SE. The phonetic information is often represented by quantizing latent features of self-supervised learning (SSL) models. This work is the first to incorporate SSL features with causality into an SE model. The causal SSL features are encoded and combined with spectrogram features using feature-wise linear modulation to estimate a mask for enhancing the noisy input speech. Simultaneously, we quantize the causal SSL features using vector quantization to represent phonetic characteristics as semantic tokens. The model not only encodes SSL features but also predicts the future semantic tokens in multi-task learning (MTL). The experimental results using VoiceBank + DEMAND dataset show that our proposed method achieves 2.88 in PESQ, especially with semantic prediction MTL, in which we confirm that the semantic prediction played an important role in causal SE.",1,,include (junior:5),https://arxiv.org/abs/2412.19248v1,https://arxiv.org/pdf/2412.19248v1,2024,semantic tokens,speech tokenization,"{""arxiv_id"": ""2412.19248"", ""title"": ""Causal Speech Enhancement with Predicting Semantics based on Quantized Self-supervised Learning Features"", ""summary"": ""Real-time speech enhancement (SE) is essential to online speech communication. Causal SE models use only the previous context while predicting future information, such as phoneme continuation, may help performing causal SE. The phonetic information is often represented by quantizing latent features of self-supervised learning (SSL) models. This work is the first to incorporate SSL features with causality into an SE model. The causal SSL features are encoded and combined with spectrogram features using feature-wise linear modulation to estimate a mask for enhancing the noisy input speech. Simultaneously, we quantize the causal SSL features using vector quantization to represent phonetic characteristics as semantic tokens. The model not only encodes SSL features but also predicts the future semantic tokens in multi-task learning (MTL). The experimental results using VoiceBank + DEMAND dataset show that our proposed method achieves 2.88 in PESQ, especially with semantic prediction MTL, in which we confirm that the semantic prediction played an important role in causal SE."", ""authors"": [""Emiru Tsunoo"", ""Yuki Saito"", ""Wataru Nakata"", ""Hiroshi Saruwatari""], ""published"": ""2024-12-26T15:08:36Z"", ""updated"": ""2024-12-26T15:08:36Z"", ""categories"": [""eess.AS"", ""cs.SD""], ""pdf_url"": ""https://arxiv.org/pdf/2412.19248v1"", ""landing_url"": ""https://arxiv.org/abs/2412.19248v1"", ""doi"": ""https://doi.org/10.48550/arXiv.2412.19248""}",2412.19248,https://openalex.org/W4405902171
10.48550/arxiv.2501.00018,SECodec: Structural Entropy-based Compressive Speech Representation Codec for Speech Language Models,"With the rapid advancement of large language models (LLMs), discrete speech representations have become crucial for integrating speech into LLMs. Existing methods for speech representation discretization rely on a predefined codebook size and Euclidean distance-based quantization. However, 1) the size of codebook is a critical parameter that affects both codec performance and downstream task training efficiency. 2) The Euclidean distance-based quantization may lead to audio distortion when the size of the codebook is controlled within a reasonable range. In fact, in the field of information compression, structural information and entropy guidance are crucial, but previous methods have largely overlooked these factors. Therefore, we address the above issues from an information-theoretic perspective, we present SECodec, a novel speech representation codec based on structural entropy (SE) for building speech language models. Specifically, we first model speech as a graph, clustering the speech features nodes within the graph and extracting the corresponding codebook by hierarchically and disentangledly minimizing 2D SE. Then, to address the issue of audio distortion, we propose a new quantization method. This method still adheres to the 2D SE minimization principle, adaptively selecting the most suitable token corresponding to the cluster for each incoming original speech node. Furthermore, we develop a Structural Entropy-based Speech Language Model (SESLM) that leverages SECodec. Experimental results demonstrate that SECodec performs comparably to EnCodec in speech reconstruction, and SESLM surpasses VALL-E in zero-shot text-to-speech tasks. Code, demo speeches, speech feature graph, SE codebook, and models are available at https://github.com/wlq2019/SECodec.",1,,include (junior:5),https://arxiv.org/abs/2501.00018v1,https://arxiv.org/pdf/2501.00018v1,2024,speech representation,speech tokens,"{""arxiv_id"": ""2501.00018"", ""title"": ""SECodec: Structural Entropy-based Compressive Speech Representation Codec for Speech Language Models"", ""summary"": ""With the rapid advancement of large language models (LLMs), discrete speech representations have become crucial for integrating speech into LLMs. Existing methods for speech representation discretization rely on a predefined codebook size and Euclidean distance-based quantization. However, 1) the size of codebook is a critical parameter that affects both codec performance and downstream task training efficiency. 2) The Euclidean distance-based quantization may lead to audio distortion when the size of the codebook is controlled within a reasonable range. In fact, in the field of information compression, structural information and entropy guidance are crucial, but previous methods have largely overlooked these factors. Therefore, we address the above issues from an information-theoretic perspective, we present SECodec, a novel speech representation codec based on structural entropy (SE) for building speech language models. Specifically, we first model speech as a graph, clustering the speech features nodes within the graph and extracting the corresponding codebook by hierarchically and disentangledly minimizing 2D SE. Then, to address the issue of audio distortion, we propose a new quantization method. This method still adheres to the 2D SE minimization principle, adaptively selecting the most suitable token corresponding to the cluster for each incoming original speech node. Furthermore, we develop a Structural Entropy-based Speech Language Model (SESLM) that leverages SECodec. Experimental results demonstrate that SECodec performs comparably to EnCodec in speech reconstruction, and SESLM surpasses VALL-E in zero-shot text-to-speech tasks. Code, demo speeches, speech feature graph, SE codebook, and models are available at https://github.com/wlq2019/SECodec."", ""authors"": [""Linqin Wang"", ""Yaping Liu"", ""Zhengtao Yu"", ""Shengxiang Gao"", ""Cunli Mao"", ""Yuxin Huang"", ""Wenjun Wang"", ""Ling Dong""], ""published"": ""2024-12-16T03:33:05Z"", ""updated"": ""2024-12-16T03:33:05Z"", ""categories"": [""cs.SD"", ""eess.AS""], ""pdf_url"": ""https://arxiv.org/pdf/2501.00018v1"", ""landing_url"": ""https://arxiv.org/abs/2501.00018v1"", ""doi"": ""https://doi.org/10.48550/arXiv.2501.00018""}",2501.00018,https://openalex.org/W4406051734
10.48550/arxiv.2501.05787,MARS6: A Small and Robust Hierarchical-Codec Text-to-Speech Model,"Codec-based text-to-speech (TTS) models have shown impressive quality with zero-shot voice cloning abilities. However, they often struggle with more expressive references or complex text inputs. We present MARS6, a robust encoder-decoder transformer for rapid, expressive TTS. MARS6 is built on recent improvements in spoken language modelling. Utilizing a hierarchical setup for its decoder, new speech tokens are processed at a rate of only 12 Hz, enabling efficient modelling of long-form text while retaining reconstruction quality. We combine several recent training and inference techniques to reduce repetitive generation and improve output stability and quality. This enables the 70M-parameter MARS6 to achieve similar performance to models many times larger. We show this in objective and subjective evaluations, comparing TTS output quality and reference speaker cloning ability. Project page: https://camb-ai.github.io/mars6-turbo/",1,,include (junior:4),https://arxiv.org/abs/2501.05787v1,https://arxiv.org/pdf/2501.05787v1,2025,discrete speech tokens,speech tokens,"{""arxiv_id"": ""2501.05787"", ""title"": ""MARS6: A Small and Robust Hierarchical-Codec Text-to-Speech Model"", ""summary"": ""Codec-based text-to-speech (TTS) models have shown impressive quality with zero-shot voice cloning abilities. However, they often struggle with more expressive references or complex text inputs. We present MARS6, a robust encoder-decoder transformer for rapid, expressive TTS. MARS6 is built on recent improvements in spoken language modelling. Utilizing a hierarchical setup for its decoder, new speech tokens are processed at a rate of only 12 Hz, enabling efficient modelling of long-form text while retaining reconstruction quality. We combine several recent training and inference techniques to reduce repetitive generation and improve output stability and quality. This enables the 70M-parameter MARS6 to achieve similar performance to models many times larger. We show this in objective and subjective evaluations, comparing TTS output quality and reference speaker cloning ability. Project page: https://camb-ai.github.io/mars6-turbo/"", ""authors"": [""Matthew Baas"", ""Pieter Scholtz"", ""Arnav Mehta"", ""Elliott Dyson"", ""Akshat Prakash"", ""Herman Kamper""], ""published"": ""2025-01-10T08:41:42Z"", ""updated"": ""2025-01-10T08:41:42Z"", ""categories"": [""eess.AS"", ""cs.CL""], ""pdf_url"": ""https://arxiv.org/pdf/2501.05787v1"", ""landing_url"": ""https://arxiv.org/abs/2501.05787v1"", ""doi"": ""https://doi.org/10.48550/arXiv.2501.05787""}",2501.05787,https://openalex.org/W4406316876
10.48550/arxiv.2501.16131,Optimized Self-supervised Training with BEST-RQ for Speech Recognition,"Self-supervised learning has been successfully used for various speech related tasks, including automatic speech recognition. BERT-based Speech pre-Training with Random-projection Quantizer (BEST-RQ) has achieved state-of-the-art results in speech recognition. In this work, we further optimize the BEST-RQ approach using Kullback-Leibler divergence as an additional regularizing loss and multi-codebook extension per cluster derived from low-level feature clustering. Preliminary experiments on train-100 split of LibriSpeech result in a relative improvement of 11.2% on test-clean by using multiple codebooks, utilizing a combination of cross-entropy and Kullback-Leibler divergence further reduces the word error rate by 4.5%. The proposed optimizations on full LibriSpeech pre-training and fine-tuning result in relative word error rate improvements of up to 23.8% on test-clean and 30.6% on test-other using 6 codebooks. Furthermore, the proposed setup leads to faster convergence in pre-training and fine-tuning and additionally stabilizes the pre-training.",1,,include (senior:4),https://arxiv.org/abs/2501.16131v1,https://arxiv.org/pdf/2501.16131v1,2025,discrete speech tokens,word error rate,"{""arxiv_id"": ""2501.16131"", ""title"": ""Optimized Self-supervised Training with BEST-RQ for Speech Recognition"", ""summary"": ""Self-supervised learning has been successfully used for various speech related tasks, including automatic speech recognition. BERT-based Speech pre-Training with Random-projection Quantizer (BEST-RQ) has achieved state-of-the-art results in speech recognition. In this work, we further optimize the BEST-RQ approach using Kullback-Leibler divergence as an additional regularizing loss and multi-codebook extension per cluster derived from low-level feature clustering. Preliminary experiments on train-100 split of LibriSpeech result in a relative improvement of 11.2% on test-clean by using multiple codebooks, utilizing a combination of cross-entropy and Kullback-Leibler divergence further reduces the word error rate by 4.5%. The proposed optimizations on full LibriSpeech pre-training and fine-tuning result in relative word error rate improvements of up to 23.8% on test-clean and 30.6% on test-other using 6 codebooks. Furthermore, the proposed setup leads to faster convergence in pre-training and fine-tuning and additionally stabilizes the pre-training."", ""authors"": [""Ilja Baumann"", ""Dominik Wagner"", ""Korbinian Riedhammer"", ""Tobias Bocklet""], ""published"": ""2025-01-27T15:20:50Z"", ""updated"": ""2025-01-27T15:20:50Z"", ""categories"": [""cs.SD""], ""pdf_url"": ""https://arxiv.org/pdf/2501.16131v1"", ""landing_url"": ""https://arxiv.org/abs/2501.16131v1"", ""doi"": ""https://doi.org/10.48550/arXiv.2501.16131""}",2501.16131,https://openalex.org/W4406890454
10.48550/arxiv.2502.02942,GenSE: Generative Speech Enhancement via Language Models using Hierarchical Modeling,"Semantic information refers to the meaning conveyed through words, phrases, and contextual relationships within a given linguistic structure. Humans can leverage semantic information, such as familiar linguistic patterns and contextual cues, to reconstruct incomplete or masked speech signals in noisy environments. However, existing speech enhancement (SE) approaches often overlook the rich semantic information embedded in speech, which is crucial for improving intelligibility, speaker consistency, and overall quality of enhanced speech signals. To enrich the SE model with semantic information, we employ language models as an efficient semantic learner and propose a comprehensive framework tailored for language model-based speech enhancement, called \textit{GenSE}. Specifically, we approach SE as a conditional language modeling task rather than a continuous signal regression problem defined in existing works. This is achieved by tokenizing speech signals into semantic tokens using a pre-trained self-supervised model and into acoustic tokens using a custom-designed single-quantizer neural codec model. To improve the stability of language model predictions, we propose a hierarchical modeling method that decouples the generation of clean semantic tokens and clean acoustic tokens into two distinct stages. Moreover, we introduce a token chain prompting mechanism during the acoustic token generation stage to ensure timbre consistency throughout the speech enhancement process. Experimental results on benchmark datasets demonstrate that our proposed approach outperforms state-of-the-art SE systems in terms of speech quality and generalization capability.",1,,include (junior:5),https://arxiv.org/abs/2502.02942v1,https://arxiv.org/pdf/2502.02942v1,2025,acoustic tokens,speech tokens,"{""arxiv_id"": ""2502.02942"", ""title"": ""GenSE: Generative Speech Enhancement via Language Models using Hierarchical Modeling"", ""summary"": ""Semantic information refers to the meaning conveyed through words, phrases, and contextual relationships within a given linguistic structure. Humans can leverage semantic information, such as familiar linguistic patterns and contextual cues, to reconstruct incomplete or masked speech signals in noisy environments. However, existing speech enhancement (SE) approaches often overlook the rich semantic information embedded in speech, which is crucial for improving intelligibility, speaker consistency, and overall quality of enhanced speech signals. To enrich the SE model with semantic information, we employ language models as an efficient semantic learner and propose a comprehensive framework tailored for language model-based speech enhancement, called \\textit{GenSE}. Specifically, we approach SE as a conditional language modeling task rather than a continuous signal regression problem defined in existing works. This is achieved by tokenizing speech signals into semantic tokens using a pre-trained self-supervised model and into acoustic tokens using a custom-designed single-quantizer neural codec model. To improve the stability of language model predictions, we propose a hierarchical modeling method that decouples the generation of clean semantic tokens and clean acoustic tokens into two distinct stages. Moreover, we introduce a token chain prompting mechanism during the acoustic token generation stage to ensure timbre consistency throughout the speech enhancement process. Experimental results on benchmark datasets demonstrate that our proposed approach outperforms state-of-the-art SE systems in terms of speech quality and generalization capability."", ""authors"": [""Jixun Yao"", ""Hexin Liu"", ""Chen Chen"", ""Yuchen Hu"", ""EngSiong Chng"", ""Lei Xie""], ""published"": ""2025-02-05T07:14:39Z"", ""updated"": ""2025-02-05T07:14:39Z"", ""categories"": [""eess.AS"", ""cs.SD""], ""pdf_url"": ""https://arxiv.org/pdf/2502.02942v1"", ""landing_url"": ""https://arxiv.org/abs/2502.02942v1"", ""doi"": ""https://doi.org/10.48550/arXiv.2502.02942""}",2502.02942,https://openalex.org/W4407212698
10.48550/arxiv.2502.03128,Metis: A Foundation Speech Generation Model with Masked Generative Pre-training,"We introduce Metis, a foundation model for unified speech generation. Unlike previous task-specific or multi-task models, Metis follows a pre-training and fine-tuning paradigm. It is pre-trained on large-scale unlabeled speech data using masked generative modeling and then fine-tuned to adapt to diverse speech generation tasks. Specifically, 1) Metis utilizes two discrete speech representations: SSL tokens derived from speech self-supervised learning (SSL) features, and acoustic tokens directly quantized from waveforms. 2) Metis performs masked generative pre-training on SSL tokens, utilizing 300K hours of diverse speech data, without any additional condition. 3) Through fine-tuning with task-specific conditions, Metis achieves efficient adaptation to various speech generation tasks while supporting multimodal input, even when using limited data and trainable parameters. Experiments demonstrate that Metis can serve as a foundation model for unified speech generation: Metis outperforms state-of-the-art task-specific or multi-task systems across five speech generation tasks, including zero-shot text-to-speech, voice conversion, target speaker extraction, speech enhancement, and lip-to-speech, even with fewer than 20M trainable parameters or 300 times less training data. Audio samples are are available at https://metis-demo.github.io/.",1,,include (junior:5),https://arxiv.org/abs/2502.03128v1,https://arxiv.org/pdf/2502.03128v1,2025,discrete speech tokens,speech generation,"{""arxiv_id"": ""2502.03128"", ""title"": ""Metis: A Foundation Speech Generation Model with Masked Generative Pre-training"", ""summary"": ""We introduce Metis, a foundation model for unified speech generation. Unlike previous task-specific or multi-task models, Metis follows a pre-training and fine-tuning paradigm. It is pre-trained on large-scale unlabeled speech data using masked generative modeling and then fine-tuned to adapt to diverse speech generation tasks. Specifically, 1) Metis utilizes two discrete speech representations: SSL tokens derived from speech self-supervised learning (SSL) features, and acoustic tokens directly quantized from waveforms. 2) Metis performs masked generative pre-training on SSL tokens, utilizing 300K hours of diverse speech data, without any additional condition. 3) Through fine-tuning with task-specific conditions, Metis achieves efficient adaptation to various speech generation tasks while supporting multimodal input, even when using limited data and trainable parameters. Experiments demonstrate that Metis can serve as a foundation model for unified speech generation: Metis outperforms state-of-the-art task-specific or multi-task systems across five speech generation tasks, including zero-shot text-to-speech, voice conversion, target speaker extraction, speech enhancement, and lip-to-speech, even with fewer than 20M trainable parameters or 300 times less training data. Audio samples are are available at https://metis-demo.github.io/."", ""authors"": [""Yuancheng Wang"", ""Jiachen Zheng"", ""Junan Zhang"", ""Xueyao Zhang"", ""Huan Liao"", ""Zhizheng Wu""], ""published"": ""2025-02-05T12:36:21Z"", ""updated"": ""2025-02-05T12:36:21Z"", ""categories"": [""cs.SD"", ""cs.AI"", ""cs.LG"", ""eess.AS"", ""eess.SP""], ""pdf_url"": ""https://arxiv.org/pdf/2502.03128v1"", ""landing_url"": ""https://arxiv.org/abs/2502.03128v1"", ""doi"": ""https://doi.org/10.48550/arXiv.2502.03128""}",2502.03128,https://openalex.org/W4407221739
10.48550/arxiv.2502.05512,IndexTTS: An Industrial-Level Controllable and Efficient Zero-Shot Text-To-Speech System,"Recently, large language model (LLM) based text-to-speech (TTS) systems have gradually become the mainstream in the industry due to their high naturalness and powerful zero-shot voice cloning capabilities.Here, we introduce the IndexTTS system, which is mainly based on the XTTS and Tortoise model. We add some novel improvements. Specifically, in Chinese scenarios, we adopt a hybrid modeling method that combines characters and pinyin, making the pronunciations of polyphonic characters and long-tail characters controllable. We also performed a comparative analysis of the Vector Quantization (VQ) with Finite-Scalar Quantization (FSQ) for codebook utilization of acoustic speech tokens. To further enhance the effect and stability of voice cloning, we introduce a conformer-based speech conditional encoder and replace the speechcode decoder with BigVGAN2. Compared with XTTS, it has achieved significant improvements in naturalness, content consistency, and zero-shot voice cloning. As for the popular TTS systems in the open-source, such as Fish-Speech, CosyVoice2, FireRedTTS and F5-TTS, IndexTTS has a relatively simple training process, more controllable usage, and faster inference speed. Moreover, its performance surpasses that of these systems. Our demos are available at https://index-tts.github.io.",1,,include (junior:4),https://arxiv.org/abs/2502.05512v1,https://arxiv.org/pdf/2502.05512v1,2025,discrete speech tokens,speech tokens,"{""arxiv_id"": ""2502.05512"", ""title"": ""IndexTTS: An Industrial-Level Controllable and Efficient Zero-Shot Text-To-Speech System"", ""summary"": ""Recently, large language model (LLM) based text-to-speech (TTS) systems have gradually become the mainstream in the industry due to their high naturalness and powerful zero-shot voice cloning capabilities.Here, we introduce the IndexTTS system, which is mainly based on the XTTS and Tortoise model. We add some novel improvements. Specifically, in Chinese scenarios, we adopt a hybrid modeling method that combines characters and pinyin, making the pronunciations of polyphonic characters and long-tail characters controllable. We also performed a comparative analysis of the Vector Quantization (VQ) with Finite-Scalar Quantization (FSQ) for codebook utilization of acoustic speech tokens. To further enhance the effect and stability of voice cloning, we introduce a conformer-based speech conditional encoder and replace the speechcode decoder with BigVGAN2. Compared with XTTS, it has achieved significant improvements in naturalness, content consistency, and zero-shot voice cloning. As for the popular TTS systems in the open-source, such as Fish-Speech, CosyVoice2, FireRedTTS and F5-TTS, IndexTTS has a relatively simple training process, more controllable usage, and faster inference speed. Moreover, its performance surpasses that of these systems. Our demos are available at https://index-tts.github.io."", ""authors"": [""Wei Deng"", ""Siyi Zhou"", ""Jingchen Shu"", ""Jinchao Wang"", ""Lu Wang""], ""published"": ""2025-02-08T10:23:20Z"", ""updated"": ""2025-02-08T10:23:20Z"", ""categories"": [""cs.SD"", ""cs.AI"", ""eess.AS""], ""pdf_url"": ""https://arxiv.org/pdf/2502.05512v1"", ""landing_url"": ""https://arxiv.org/abs/2502.05512v1"", ""doi"": ""https://doi.org/10.48550/arXiv.2502.05512""}",2502.05512,https://openalex.org/W4407384896
10.48550/arxiv.2502.16897,Balancing Speech Understanding and Generation Using Continual Pre-training for Codec-based Speech LLM,"Recent advances in speech language models (LLMs) have extended textual LLMs to the speech domain, but balancing speech understanding and generation remains challenging, especially with codec-based representations. We propose a continual pre-training (CPT) framework that adapts a textual LLM to handle codec-discretized speech, mitigating modality mismatch and preserving linguistic reasoning. Our unified model supports both understanding and generation, achieving strong results across ASR, TTS, S2T-Trans, and S2S-Trans. Notably, we present the first end-to-end, single-pass S2S-Trans system using only neural codec tokens, without intermediate transcriptions, translations, or semantic tokens. CPT proves essential for cross-modal alignment and task generalization, making it a powerful tool for building robust, unified speech LLMs.",1,,include (junior:5),https://arxiv.org/abs/2502.16897v2,https://arxiv.org/pdf/2502.16897v2,2025,semantic tokens,offline clustering,"{""arxiv_id"": ""2502.16897"", ""title"": ""Balancing Speech Understanding and Generation Using Continual Pre-training for Codec-based Speech LLM"", ""summary"": ""Recent advances in speech language models (LLMs) have extended textual LLMs to the speech domain, but balancing speech understanding and generation remains challenging, especially with codec-based representations. We propose a continual pre-training (CPT) framework that adapts a textual LLM to handle codec-discretized speech, mitigating modality mismatch and preserving linguistic reasoning. Our unified model supports both understanding and generation, achieving strong results across ASR, TTS, S2T-Trans, and S2S-Trans. Notably, we present the first end-to-end, single-pass S2S-Trans system using only neural codec tokens, without intermediate transcriptions, translations, or semantic tokens. CPT proves essential for cross-modal alignment and task generalization, making it a powerful tool for building robust, unified speech LLMs."", ""authors"": [""Jiatong Shi"", ""Chunlei Zhang"", ""Jinchuan Tian"", ""Junrui Ni"", ""Hao Zhang"", ""Shinji Watanabe"", ""Dong Yu""], ""published"": ""2025-02-24T06:50:40Z"", ""updated"": ""2025-11-27T18:46:39Z"", ""categories"": [""eess.AS""], ""pdf_url"": ""https://arxiv.org/pdf/2502.16897v2"", ""landing_url"": ""https://arxiv.org/abs/2502.16897v2"", ""doi"": ""https://doi.org/10.48550/arXiv.2502.16897""}",2502.16897,https://openalex.org/W4414845989
10.48550/arxiv.2502.17239,Baichuan-Audio: A Unified Framework for End-to-End Speech Interaction,"We introduce Baichuan-Audio, an end-to-end audio large language model that seamlessly integrates audio understanding and generation. It features a text-guided aligned speech generation mechanism, enabling real-time speech interaction with both comprehension and generation capabilities. Baichuan-Audio leverages a pre-trained ASR model, followed by multi-codebook discretization of speech at a frame rate of 12.5 Hz. This multi-codebook setup ensures that speech tokens retain both semantic and acoustic information. To further enhance modeling, an independent audio head is employed to process audio tokens, effectively capturing their unique characteristics. To mitigate the loss of intelligence during pre-training and preserve the original capabilities of the LLM, we propose a two-stage pre-training strategy that maintains language understanding while enhancing audio modeling. Following alignment, the model excels in real-time speech-based conversation and exhibits outstanding question-answering capabilities, demonstrating its versatility and efficiency. The proposed model demonstrates superior performance in real-time spoken dialogue and exhibits strong question-answering abilities. Our code, model and training data are available at https://github.com/baichuan-inc/Baichuan-Audio",1,,include (junior:5),https://arxiv.org/abs/2502.17239v1,https://arxiv.org/pdf/2502.17239v1,2025,discrete speech tokens,speech tokens,"{""arxiv_id"": ""2502.17239"", ""title"": ""Baichuan-Audio: A Unified Framework for End-to-End Speech Interaction"", ""summary"": ""We introduce Baichuan-Audio, an end-to-end audio large language model that seamlessly integrates audio understanding and generation. It features a text-guided aligned speech generation mechanism, enabling real-time speech interaction with both comprehension and generation capabilities. Baichuan-Audio leverages a pre-trained ASR model, followed by multi-codebook discretization of speech at a frame rate of 12.5 Hz. This multi-codebook setup ensures that speech tokens retain both semantic and acoustic information. To further enhance modeling, an independent audio head is employed to process audio tokens, effectively capturing their unique characteristics. To mitigate the loss of intelligence during pre-training and preserve the original capabilities of the LLM, we propose a two-stage pre-training strategy that maintains language understanding while enhancing audio modeling. Following alignment, the model excels in real-time speech-based conversation and exhibits outstanding question-answering capabilities, demonstrating its versatility and efficiency. The proposed model demonstrates superior performance in real-time spoken dialogue and exhibits strong question-answering abilities. Our code, model and training data are available at https://github.com/baichuan-inc/Baichuan-Audio"", ""authors"": [""Tianpeng Li"", ""Jun Liu"", ""Tao Zhang"", ""Yuanbo Fang"", ""Da Pan"", ""Mingrui Wang"", ""Zheng Liang"", ""Zehuan Li"", ""Mingan Lin"", ""Guosheng Dong"", ""Jianhua Xu"", ""Haoze Sun"", ""Zenan Zhou"", ""Weipeng Chen""], ""published"": ""2025-02-24T15:16:34Z"", ""updated"": ""2025-02-24T15:16:34Z"", ""categories"": [""cs.CL"", ""cs.SD"", ""eess.AS""], ""pdf_url"": ""https://arxiv.org/pdf/2502.17239v1"", ""landing_url"": ""https://arxiv.org/abs/2502.17239v1"", ""doi"": ""https://doi.org/10.48550/arXiv.2502.17239""}",2502.17239,https://openalex.org/W4414849600
10.48550/arxiv.2503.00493,LLaSE-G1: Incentivizing Generalization Capability for LLaMA-based Speech Enhancement,"Recent advancements in language models (LMs) have demonstrated strong capabilities in semantic understanding and contextual modeling, which have flourished in generative speech enhancement (SE). However, many LM-based SE approaches primarily focus on semantic information, often neglecting the critical role of acoustic information, which leads to acoustic inconsistency after enhancement and limited generalization across diverse SE tasks. In this paper, we introduce LLaSE-G1, a LLaMA-based language model that incentivizes generalization capabilities for speech enhancement. LLaSE-G1 offers the following key contributions: First, to mitigate acoustic inconsistency, LLaSE-G1 employs continuous representations from WavLM as input and predicts speech tokens from X-Codec2, maximizing acoustic preservation. Second, to promote generalization capability, LLaSE-G1 introduces dual-channel inputs and outputs, unifying multiple SE tasks without requiring task-specific IDs. Third, LLaSE-G1 outperforms prior task-specific discriminative and generative SE models, demonstrating scaling effects at test time and emerging capabilities for unseen SE tasks. Additionally, we release our code and models to support further research in this area.",1,,include (senior:4),https://arxiv.org/abs/2503.00493v4,https://arxiv.org/pdf/2503.00493v4,2025,discrete speech tokens,speech tokens,"{""arxiv_id"": ""2503.00493"", ""title"": ""LLaSE-G1: Incentivizing Generalization Capability for LLaMA-based Speech Enhancement"", ""summary"": ""Recent advancements in language models (LMs) have demonstrated strong capabilities in semantic understanding and contextual modeling, which have flourished in generative speech enhancement (SE). However, many LM-based SE approaches primarily focus on semantic information, often neglecting the critical role of acoustic information, which leads to acoustic inconsistency after enhancement and limited generalization across diverse SE tasks. In this paper, we introduce LLaSE-G1, a LLaMA-based language model that incentivizes generalization capabilities for speech enhancement. LLaSE-G1 offers the following key contributions: First, to mitigate acoustic inconsistency, LLaSE-G1 employs continuous representations from WavLM as input and predicts speech tokens from X-Codec2, maximizing acoustic preservation. Second, to promote generalization capability, LLaSE-G1 introduces dual-channel inputs and outputs, unifying multiple SE tasks without requiring task-specific IDs. Third, LLaSE-G1 outperforms prior task-specific discriminative and generative SE models, demonstrating scaling effects at test time and emerging capabilities for unseen SE tasks. Additionally, we release our code and models to support further research in this area."", ""authors"": [""Boyi Kang"", ""Xinfa Zhu"", ""Zihan Zhang"", ""Zhen Ye"", ""Mingshuai Liu"", ""Ziqian Wang"", ""Yike Zhu"", ""Guobin Ma"", ""Jun Chen"", ""Longshuai Xiao"", ""Chao Weng"", ""Wei Xue"", ""Lei Xie""], ""published"": ""2025-03-01T13:44:50Z"", ""updated"": ""2025-06-10T06:55:05Z"", ""categories"": [""eess.AS"", ""cs.AI"", ""cs.CL"", ""cs.SD""], ""pdf_url"": ""https://arxiv.org/pdf/2503.00493v4"", ""landing_url"": ""https://arxiv.org/abs/2503.00493v4"", ""doi"": ""https://doi.org/10.48550/arXiv.2503.00493""}",2503.00493,https://openalex.org/W4417252290
10.48550/arxiv.2503.01710,Spark-TTS: An Efficient LLM-Based Text-to-Speech Model with Single-Stream Decoupled Speech Tokens,"Recent advancements in large language models (LLMs) have driven significant progress in zero-shot text-to-speech (TTS) synthesis. However, existing foundation models rely on multi-stage processing or complex architectures for predicting multiple codebooks, limiting efficiency and integration flexibility. To overcome these challenges, we introduce Spark-TTS, a novel system powered by BiCodec, a single-stream speech codec that decomposes speech into two complementary token types: low-bitrate semantic tokens for linguistic content and fixed-length global tokens for speaker attributes. This disentangled representation, combined with the Qwen2.5 LLM and a chain-of-thought (CoT) generation approach, enables both coarse-grained control (e.g., gender, speaking style) and fine-grained adjustments (e.g., precise pitch values, speaking rate). To facilitate research in controllable TTS, we introduce VoxBox, a meticulously curated 100,000-hour dataset with comprehensive attribute annotations. Extensive experiments demonstrate that Spark-TTS not only achieves state-of-the-art zero-shot voice cloning but also generates highly customizable voices that surpass the limitations of reference-based synthesis. Source code, pre-trained models, and audio samples are available at https://github.com/SparkAudio/Spark-TTS.",1,,include (junior:5),https://arxiv.org/abs/2503.01710v1,https://arxiv.org/pdf/2503.01710v1,2025,discrete speech tokens,speech tokens,"{""arxiv_id"": ""2503.01710"", ""title"": ""Spark-TTS: An Efficient LLM-Based Text-to-Speech Model with Single-Stream Decoupled Speech Tokens"", ""summary"": ""Recent advancements in large language models (LLMs) have driven significant progress in zero-shot text-to-speech (TTS) synthesis. However, existing foundation models rely on multi-stage processing or complex architectures for predicting multiple codebooks, limiting efficiency and integration flexibility. To overcome these challenges, we introduce Spark-TTS, a novel system powered by BiCodec, a single-stream speech codec that decomposes speech into two complementary token types: low-bitrate semantic tokens for linguistic content and fixed-length global tokens for speaker attributes. This disentangled representation, combined with the Qwen2.5 LLM and a chain-of-thought (CoT) generation approach, enables both coarse-grained control (e.g., gender, speaking style) and fine-grained adjustments (e.g., precise pitch values, speaking rate). To facilitate research in controllable TTS, we introduce VoxBox, a meticulously curated 100,000-hour dataset with comprehensive attribute annotations. Extensive experiments demonstrate that Spark-TTS not only achieves state-of-the-art zero-shot voice cloning but also generates highly customizable voices that surpass the limitations of reference-based synthesis. Source code, pre-trained models, and audio samples are available at https://github.com/SparkAudio/Spark-TTS."", ""authors"": [""Xinsheng Wang"", ""Mingqi Jiang"", ""Ziyang Ma"", ""Ziyu Zhang"", ""Songxiang Liu"", ""Linqin Li"", ""Zheng Liang"", ""Qixi Zheng"", ""Rui Wang"", ""Xiaoqin Feng"", ""Weizhen Bian"", ""Zhen Ye"", ""Sitong Cheng"", ""Ruibin Yuan"", ""Zhixian Zhao"", ""Xinfa Zhu"", ""Jiahao Pan"", ""Liumeng Xue"", ""Pengcheng Zhu"", ""Yunlin Chen"", ""Zhifei Li"", ""Xie Chen"", ""Lei Xie"", ""Yike Guo"", ""Wei Xue""], ""published"": ""2025-03-03T16:23:10Z"", ""updated"": ""2025-03-03T16:23:10Z"", ""categories"": [""cs.SD"", ""cs.AI"", ""eess.AS""], ""pdf_url"": ""https://arxiv.org/pdf/2503.01710v1"", ""landing_url"": ""https://arxiv.org/abs/2503.01710v1"", ""doi"": ""https://doi.org/10.48550/arXiv.2503.01710""}",2503.0171,https://openalex.org/W4415085369
10.1109/jstsp.2024.3488557,Universal Speech Token Learning via Low-Bitrate Neural Codec and Pretrained Representations,"Current large speech language models are mainly based on semantic tokens from discretization of self-supervised learned representations and acoustic tokens from a neural codec, following a semantic-modeling and acoustic-synthesis paradigm. However, semantic tokens discard paralinguistic attributes of speakers that is important for natural spoken communication, while prompt-based acoustic synthesis from semantic tokens has limits in recovering paralinguistic details and suffers from robustness issues, especially when there are domain gaps between the prompt and the target. This paper unifies two types of tokens and proposes the UniCodec, a universal speech token learning that encapsulates all semantics of speech, including linguistic and paralinguistic information, into a compact and semantically-disentangled unified token. Such a unified token can not only benefit speech language models in understanding with paralinguistic hints but also help speech generation with high-quality output. A low-bitrate neural codec is leveraged to learn such disentangled discrete representations at global and local scales, with knowledge distilled from self-supervised learned features. Extensive evaluations on multilingual datasets demonstrate its effectiveness in generating natural, expressive and long-term consistent output quality with paralinguistic attributes well preserved in several speech processing tasks.",1,,include (junior:5),https://arxiv.org/abs/2503.12115v2,https://arxiv.org/pdf/2503.12115v2,2025,discrete speech tokens,speech tokens,"{""arxiv_id"": ""2503.12115"", ""title"": ""Universal Speech Token Learning via Low-Bitrate Neural Codec and Pretrained Representations"", ""summary"": ""Current large speech language models are mainly based on semantic tokens from discretization of self-supervised learned representations and acoustic tokens from a neural codec, following a semantic-modeling and acoustic-synthesis paradigm. However, semantic tokens discard paralinguistic attributes of speakers that is important for natural spoken communication, while prompt-based acoustic synthesis from semantic tokens has limits in recovering paralinguistic details and suffers from robustness issues, especially when there are domain gaps between the prompt and the target. This paper unifies two types of tokens and proposes the UniCodec, a universal speech token learning that encapsulates all semantics of speech, including linguistic and paralinguistic information, into a compact and semantically-disentangled unified token. Such a unified token can not only benefit speech language models in understanding with paralinguistic hints but also help speech generation with high-quality output. A low-bitrate neural codec is leveraged to learn such disentangled discrete representations at global and local scales, with knowledge distilled from self-supervised learned features. Extensive evaluations on multilingual datasets demonstrate its effectiveness in generating natural, expressive and long-term consistent output quality with paralinguistic attributes well preserved in several speech processing tasks."", ""authors"": [""Xue Jiang"", ""Xiulian Peng"", ""Yuan Zhang"", ""Yan Lu""], ""published"": ""2025-03-15T12:50:43Z"", ""updated"": ""2025-10-15T06:52:30Z"", ""categories"": [""cs.SD"", ""cs.AI"", ""eess.AS""], ""pdf_url"": ""https://arxiv.org/pdf/2503.12115v2"", ""landing_url"": ""https://arxiv.org/abs/2503.12115v2"", ""doi"": ""https://doi.org/10.1109/JSTSP.2024.3488557""}",2503.12115,https://openalex.org/W4403918744
10.48550/arxiv.2503.14928,Shushing! Let's Imagine an Authentic Speech from the Silent Video,"Vision-guided speech generation aims to produce authentic speech from facial appearance or lip motions without relying on auditory signals, offering significant potential for applications such as dubbing in filmmaking and assisting individuals with aphonia. Despite recent progress, existing methods struggle to achieve unified cross-modal alignment across semantics, timbre, and emotional prosody from visual cues, prompting us to propose Consistent Video-to-Speech (CV2S) as an extended task to enhance cross-modal consistency. To tackle emerging challenges, we introduce ImaginTalk, a novel cross-modal diffusion framework that generates faithful speech using only visual input, operating within a discrete space. Specifically, we propose a discrete lip aligner that predicts discrete speech tokens from lip videos to capture semantic information, while an error detector identifies misaligned tokens, which are subsequently refined through masked language modeling with BERT. To further enhance the expressiveness of the generated speech, we develop a style diffusion transformer equipped with a face-style adapter that adaptively customizes identity and prosody dynamics across both the channel and temporal dimensions while ensuring synchronization with lip-aware semantic features. Extensive experiments demonstrate that ImaginTalk can generate high-fidelity speech with more accurate semantic details and greater expressiveness in timbre and emotion compared to state-of-the-art baselines. Demos are shown at our project page: https://imagintalk.github.io.",1,,include (junior:5),https://arxiv.org/abs/2503.14928v1,https://arxiv.org/pdf/2503.14928v1,2025,discrete speech tokens,speech tokens,"{""arxiv_id"": ""2503.14928"", ""title"": ""Shushing! Let's Imagine an Authentic Speech from the Silent Video"", ""summary"": ""Vision-guided speech generation aims to produce authentic speech from facial appearance or lip motions without relying on auditory signals, offering significant potential for applications such as dubbing in filmmaking and assisting individuals with aphonia. Despite recent progress, existing methods struggle to achieve unified cross-modal alignment across semantics, timbre, and emotional prosody from visual cues, prompting us to propose Consistent Video-to-Speech (CV2S) as an extended task to enhance cross-modal consistency. To tackle emerging challenges, we introduce ImaginTalk, a novel cross-modal diffusion framework that generates faithful speech using only visual input, operating within a discrete space. Specifically, we propose a discrete lip aligner that predicts discrete speech tokens from lip videos to capture semantic information, while an error detector identifies misaligned tokens, which are subsequently refined through masked language modeling with BERT. To further enhance the expressiveness of the generated speech, we develop a style diffusion transformer equipped with a face-style adapter that adaptively customizes identity and prosody dynamics across both the channel and temporal dimensions while ensuring synchronization with lip-aware semantic features. Extensive experiments demonstrate that ImaginTalk can generate high-fidelity speech with more accurate semantic details and greater expressiveness in timbre and emotion compared to state-of-the-art baselines. Demos are shown at our project page: https://imagintalk.github.io."", ""authors"": [""Jiaxin Ye"", ""Hongming Shan""], ""published"": ""2025-03-19T06:28:17Z"", ""updated"": ""2025-03-19T06:28:17Z"", ""categories"": [""cs.CV"", ""cs.AI"", ""cs.SD"", ""eess.AS""], ""pdf_url"": ""https://arxiv.org/pdf/2503.14928v1"", ""landing_url"": ""https://arxiv.org/abs/2503.14928v1"", ""doi"": ""https://doi.org/10.48550/arXiv.2503.14928""}",2503.14928,https://openalex.org/W4414902300
10.48550/arxiv.2503.20499,FireRedTTS-1S: An Upgraded Streamable Foundation Text-to-Speech System,"In this work, we upgrade FireRedTTS to a new version, FireRedTTS-1S, a high-quality streaming foundation text-to-speech system. FireRedTTS-1S achieves streaming speech generation via two steps: text-to-semantic decoding and semantic-to-acoustic decoding. In text-to-semantic decoding, a semantic-aware speech tokenizer converts the speech signal into semantic tokens, which can be synthesized from the text via a language model in an auto-regressive manner. Meanwhile, the semantic-to-acoustic decoding module simultaneously translates generated semantic tokens into the speech signal in a streaming way. We implement two approaches to achieve this module: 1) a chunk-wise streamable flow-matching approach, and 2) a multi-stream language model-based approach. They both present high-quality and streamable speech generation but differ in real-time factor (RTF) and latency. Specifically, flow-matching decoding can generate speech by chunks, presenting a lower RTF of 0.1 but a higher latency of 300ms. Instead, the multi-stream language model generates speech by frames in an autoregressive manner, presenting a higher RTF of 0.3 but a low latency of 150ms. In experiments on zero-shot voice cloning, the objective results validate FireRedTTS-1S as a high-quality foundation model with comparable intelligibility and speaker similarity over industrial baseline systems. Furthermore, the subjective score of FireRedTTS-1S highlights its impressive synthesis performance, achieving comparable quality to the ground-truth recordings. These results validate FireRedTTS-1S as a high-quality streaming foundation TTS system.",1,,include (senior:4),https://arxiv.org/abs/2503.20499v3,https://arxiv.org/pdf/2503.20499v3,2025,discrete speech tokens,speech tokenizer,"{""arxiv_id"": ""2503.20499"", ""title"": ""FireRedTTS-1S: An Upgraded Streamable Foundation Text-to-Speech System"", ""summary"": ""In this work, we upgrade FireRedTTS to a new version, FireRedTTS-1S, a high-quality streaming foundation text-to-speech system. FireRedTTS-1S achieves streaming speech generation via two steps: text-to-semantic decoding and semantic-to-acoustic decoding. In text-to-semantic decoding, a semantic-aware speech tokenizer converts the speech signal into semantic tokens, which can be synthesized from the text via a language model in an auto-regressive manner. Meanwhile, the semantic-to-acoustic decoding module simultaneously translates generated semantic tokens into the speech signal in a streaming way. We implement two approaches to achieve this module: 1) a chunk-wise streamable flow-matching approach, and 2) a multi-stream language model-based approach. They both present high-quality and streamable speech generation but differ in real-time factor (RTF) and latency. Specifically, flow-matching decoding can generate speech by chunks, presenting a lower RTF of 0.1 but a higher latency of 300ms. Instead, the multi-stream language model generates speech by frames in an autoregressive manner, presenting a higher RTF of 0.3 but a low latency of 150ms. In experiments on zero-shot voice cloning, the objective results validate FireRedTTS-1S as a high-quality foundation model with comparable intelligibility and speaker similarity over industrial baseline systems. Furthermore, the subjective score of FireRedTTS-1S highlights its impressive synthesis performance, achieving comparable quality to the ground-truth recordings. These results validate FireRedTTS-1S as a high-quality streaming foundation TTS system."", ""authors"": [""Hao-Han Guo"", ""Yao Hu"", ""Fei-Yu Shen"", ""Xu Tang"", ""Yi-Chen Wu"", ""Feng-Long Xie"", ""Kun Xie""], ""published"": ""2025-03-26T12:39:06Z"", ""updated"": ""2025-05-26T11:34:20Z"", ""categories"": [""cs.SD"", ""eess.AS""], ""pdf_url"": ""https://arxiv.org/pdf/2503.20499v3"", ""landing_url"": ""https://arxiv.org/abs/2503.20499v3"", ""doi"": ""https://doi.org/10.48550/arXiv.2503.20499""}",2503.20499,https://openalex.org/W4416417394
10.48550/arxiv.2504.10352,Pseudo-Autoregressive Neural Codec Language Models for Efficient Zero-Shot Text-to-Speech Synthesis,"Recent zero-shot text-to-speech (TTS) systems face a common dilemma: autoregressive (AR) models suffer from slow generation and lack duration controllability, while non-autoregressive (NAR) models lack temporal modeling and typically require complex designs. In this paper, we introduce a novel pseudo-autoregressive (PAR) codec language modeling approach that unifies AR and NAR modeling. Combining explicit temporal modeling from AR with parallel generation from NAR, PAR generates dynamic-length spans at fixed time steps. Building on PAR, we propose PALLE, a two-stage TTS system that leverages PAR for initial generation followed by NAR refinement. In the first stage, PAR progressively generates speech tokens along the time dimension, with each step predicting all positions in parallel but only retaining the left-most span. In the second stage, low-confidence tokens are iteratively refined in parallel, leveraging the global contextual information. Experiments demonstrate that PALLE, trained on LibriTTS, outperforms state-of-the-art systems trained on large-scale data, including F5-TTS, E2-TTS, and MaskGCT, on the LibriSpeech test-clean set in terms of speech quality, speaker similarity, and intelligibility, while achieving up to ten times faster inference speed. Audio samples are available at https://microsoft.com/research/project/vall-e-x/palle.",1,,include (senior:4),https://arxiv.org/abs/2504.10352v3,https://arxiv.org/pdf/2504.10352v3,2025,discrete speech tokens,speech tokens,"{""arxiv_id"": ""2504.10352"", ""title"": ""Pseudo-Autoregressive Neural Codec Language Models for Efficient Zero-Shot Text-to-Speech Synthesis"", ""summary"": ""Recent zero-shot text-to-speech (TTS) systems face a common dilemma: autoregressive (AR) models suffer from slow generation and lack duration controllability, while non-autoregressive (NAR) models lack temporal modeling and typically require complex designs. In this paper, we introduce a novel pseudo-autoregressive (PAR) codec language modeling approach that unifies AR and NAR modeling. Combining explicit temporal modeling from AR with parallel generation from NAR, PAR generates dynamic-length spans at fixed time steps. Building on PAR, we propose PALLE, a two-stage TTS system that leverages PAR for initial generation followed by NAR refinement. In the first stage, PAR progressively generates speech tokens along the time dimension, with each step predicting all positions in parallel but only retaining the left-most span. In the second stage, low-confidence tokens are iteratively refined in parallel, leveraging the global contextual information. Experiments demonstrate that PALLE, trained on LibriTTS, outperforms state-of-the-art systems trained on large-scale data, including F5-TTS, E2-TTS, and MaskGCT, on the LibriSpeech test-clean set in terms of speech quality, speaker similarity, and intelligibility, while achieving up to ten times faster inference speed. Audio samples are available at https://microsoft.com/research/project/vall-e-x/palle."", ""authors"": [""Yifan Yang"", ""Shujie Liu"", ""Jinyu Li"", ""Yuxuan Hu"", ""Haibin Wu"", ""Hui Wang"", ""Jianwei Yu"", ""Lingwei Meng"", ""Haiyang Sun"", ""Yanqing Liu"", ""Yan Lu"", ""Kai Yu"", ""Xie Chen""], ""published"": ""2025-04-14T16:03:21Z"", ""updated"": ""2025-08-05T15:33:39Z"", ""categories"": [""eess.AS"", ""cs.CL""], ""pdf_url"": ""https://arxiv.org/pdf/2504.10352v3"", ""landing_url"": ""https://arxiv.org/abs/2504.10352v3"", ""doi"": ""https://doi.org/10.48550/arXiv.2504.10352""}",2504.10352,https://openalex.org/W4415160782
10.48550/arxiv.2504.15509,SimulS2S-LLM: Unlocking Simultaneous Inference of Speech LLMs for Speech-to-Speech Translation,"Simultaneous speech translation (SST) outputs translations in parallel with streaming speech input, balancing translation quality and latency. While large language models (LLMs) have been extended to handle the speech modality, streaming remains challenging as speech is prepended as a prompt for the entire generation process. To unlock LLM streaming capability, this paper proposes SimulS2S-LLM, which trains speech LLMs offline and employs a test-time policy to guide simultaneous inference. SimulS2S-LLM alleviates the mismatch between training and inference by extracting boundary-aware speech prompts that allows it to be better matched with text input data. SimulS2S-LLM achieves simultaneous speech-to-speech translation (Simul-S2ST) by predicting discrete output speech tokens and then synthesising output speech using a pre-trained vocoder. An incremental beam search is designed to expand the search space of speech token prediction without increasing latency. Experiments on the CVSS speech data show that SimulS2S-LLM offers a better translation quality-latency trade-off than existing methods that use the same training data, such as improving ASR-BLEU scores by 3 points at similar latency.",1,,include (junior:5),https://arxiv.org/abs/2504.15509v1,https://arxiv.org/pdf/2504.15509v1,2025,discrete speech tokens,speech tokens,"{""arxiv_id"": ""2504.15509"", ""title"": ""SimulS2S-LLM: Unlocking Simultaneous Inference of Speech LLMs for Speech-to-Speech Translation"", ""summary"": ""Simultaneous speech translation (SST) outputs translations in parallel with streaming speech input, balancing translation quality and latency. While large language models (LLMs) have been extended to handle the speech modality, streaming remains challenging as speech is prepended as a prompt for the entire generation process. To unlock LLM streaming capability, this paper proposes SimulS2S-LLM, which trains speech LLMs offline and employs a test-time policy to guide simultaneous inference. SimulS2S-LLM alleviates the mismatch between training and inference by extracting boundary-aware speech prompts that allows it to be better matched with text input data. SimulS2S-LLM achieves simultaneous speech-to-speech translation (Simul-S2ST) by predicting discrete output speech tokens and then synthesising output speech using a pre-trained vocoder. An incremental beam search is designed to expand the search space of speech token prediction without increasing latency. Experiments on the CVSS speech data show that SimulS2S-LLM offers a better translation quality-latency trade-off than existing methods that use the same training data, such as improving ASR-BLEU scores by 3 points at similar latency."", ""authors"": [""Keqi Deng"", ""Wenxi Chen"", ""Xie Chen"", ""Philip C. Woodland""], ""published"": ""2025-04-22T01:05:32Z"", ""updated"": ""2025-04-22T01:05:32Z"", ""categories"": [""cs.CL"", ""cs.SD"", ""eess.AS""], ""pdf_url"": ""https://arxiv.org/pdf/2504.15509v1"", ""landing_url"": ""https://arxiv.org/abs/2504.15509v1"", ""doi"": ""https://doi.org/10.48550/arXiv.2504.15509""}",2504.15509,https://openalex.org/W4414632048
10.48550/arxiv.2505.13000,"DualCodec: A Low-Frame-Rate, Semantically-Enhanced Neural Audio Codec for Speech Generation","Neural audio codecs form the foundational building blocks for language model (LM)-based speech generation. Typically, there is a trade-off between frame rate and audio quality. This study introduces a low-frame-rate, semantically enhanced codec model. Existing approaches distill semantically rich self-supervised (SSL) representations into the first-layer codec tokens. This work proposes DualCodec, a dual-stream encoding approach that integrates SSL and waveform representations within an end-to-end codec framework. In this setting, DualCodec enhances the semantic information in the first-layer codec and enables the codec system to maintain high audio quality while operating at a low frame rate. Note that a low-frame-rate codec improves the efficiency of speech generation. Experimental results on audio codec and speech generation tasks confirm the effectiveness of the proposed DualCodec compared to state-of-the-art codec systems, such as Mimi Codec, SpeechTokenizer, DAC, and Encodec. Demos are available at: https://dualcodec.github.io, code is available at: https://github.com/jiaqili3/DualCodec",1,,include (junior:4),https://arxiv.org/abs/2505.13000v2,https://arxiv.org/pdf/2505.13000v2,2025,discrete speech tokens,speech generation,"{""arxiv_id"": ""2505.13000"", ""title"": ""DualCodec: A Low-Frame-Rate, Semantically-Enhanced Neural Audio Codec for Speech Generation"", ""summary"": ""Neural audio codecs form the foundational building blocks for language model (LM)-based speech generation. Typically, there is a trade-off between frame rate and audio quality. This study introduces a low-frame-rate, semantically enhanced codec model. Existing approaches distill semantically rich self-supervised (SSL) representations into the first-layer codec tokens. This work proposes DualCodec, a dual-stream encoding approach that integrates SSL and waveform representations within an end-to-end codec framework. In this setting, DualCodec enhances the semantic information in the first-layer codec and enables the codec system to maintain high audio quality while operating at a low frame rate. Note that a low-frame-rate codec improves the efficiency of speech generation. Experimental results on audio codec and speech generation tasks confirm the effectiveness of the proposed DualCodec compared to state-of-the-art codec systems, such as Mimi Codec, SpeechTokenizer, DAC, and Encodec. Demos are available at: https://dualcodec.github.io, code is available at: https://github.com/jiaqili3/DualCodec"", ""authors"": [""Jiaqi Li"", ""Xiaolong Lin"", ""Zhekai Li"", ""Shixi Huang"", ""Yuancheng Wang"", ""Chaoren Wang"", ""Zhenpeng Zhan"", ""Zhizheng Wu""], ""published"": ""2025-05-19T11:41:08Z"", ""updated"": ""2025-10-01T15:01:57Z"", ""categories"": [""cs.SD"", ""eess.AS""], ""pdf_url"": ""https://arxiv.org/pdf/2505.13000v2"", ""landing_url"": ""https://arxiv.org/abs/2505.13000v2"", ""doi"": ""https://doi.org/10.48550/arXiv.2505.13000""}",2505.13,https://openalex.org/W4415433988
10.48550/arxiv.2505.13830,Improving Noise Robustness of LLM-based Zero-shot TTS via Discrete Acoustic Token Denoising,"Large language model (LLM) based zero-shot text-to-speech (TTS) methods tend to preserve the acoustic environment of the audio prompt, leading to degradation in synthesized speech quality when the audio prompt contains noise. In this paper, we propose a novel neural codec-based speech denoiser and integrate it with the advanced LLM-based TTS model, LauraTTS, to achieve noise-robust zero-shot TTS. The proposed codec denoiser consists of an audio codec, a token denoiser, and an embedding refiner. The token denoiser predicts the first two groups of clean acoustic tokens from the noisy ones, which can serve as the acoustic prompt for LauraTTS to synthesize high-quality personalized speech or be converted to clean speech waveforms through the embedding refiner and codec decoder. Experimental results show that our proposed codec denoiser outperforms state-of-the-art speech enhancement (SE) methods, and the proposed noise-robust LauraTTS surpasses the approach using additional SE models.",1,,include (junior:5),https://arxiv.org/abs/2505.13830v2,https://arxiv.org/pdf/2505.13830v2,2025,acoustic tokens,speech tokens,"{""arxiv_id"": ""2505.13830"", ""title"": ""Improving Noise Robustness of LLM-based Zero-shot TTS via Discrete Acoustic Token Denoising"", ""summary"": ""Large language model (LLM) based zero-shot text-to-speech (TTS) methods tend to preserve the acoustic environment of the audio prompt, leading to degradation in synthesized speech quality when the audio prompt contains noise. In this paper, we propose a novel neural codec-based speech denoiser and integrate it with the advanced LLM-based TTS model, LauraTTS, to achieve noise-robust zero-shot TTS. The proposed codec denoiser consists of an audio codec, a token denoiser, and an embedding refiner. The token denoiser predicts the first two groups of clean acoustic tokens from the noisy ones, which can serve as the acoustic prompt for LauraTTS to synthesize high-quality personalized speech or be converted to clean speech waveforms through the embedding refiner and codec decoder. Experimental results show that our proposed codec denoiser outperforms state-of-the-art speech enhancement (SE) methods, and the proposed noise-robust LauraTTS surpasses the approach using additional SE models."", ""authors"": [""Ye-Xin Lu"", ""Hui-Peng Du"", ""Fei Liu"", ""Yang Ai"", ""Zhen-Hua Ling""], ""published"": ""2025-05-20T02:18:45Z"", ""updated"": ""2025-05-22T04:41:35Z"", ""categories"": [""eess.AS"", ""cs.SD""], ""pdf_url"": ""https://arxiv.org/pdf/2505.13830v2"", ""landing_url"": ""https://arxiv.org/abs/2505.13830v2"", ""doi"": ""https://doi.org/10.48550/arXiv.2505.13830""}",2505.1383,https://openalex.org/W4416445296
10.48550/arxiv.2505.14470,PAST: Phonetic-Acoustic Speech Tokenizer,"We present PAST, a novel end-to-end framework that jointly models phonetic information alongside signal reconstruction, eliminating the need for external pretrained models. Unlike previous approaches that rely on pretrained self-supervised models, PAST employs supervised phonetic data, directly integrating domain knowledge into the tokenization process via auxiliary tasks. Additionally, we introduce a streamable, causal variant of PAST, enabling real-time speech applications. Results demonstrate that PAST surpasses existing evaluated baseline tokenizers across common evaluation metrics, including phonetic representation and speech reconstruction. Notably, PAST also achieves superior performance when serving as a speech representation for speech language models, further highlighting its effectiveness as a foundation for spoken language generation. To foster further research, we release the full implementation. For code, model checkpoints, and samples see: https://pages.cs.huji.ac.il/adiyoss-lab/PAST",1,,include (junior:4),https://arxiv.org/abs/2505.14470v2,https://arxiv.org/pdf/2505.14470v2,2025,discrete speech tokens,speech tokens,"{""arxiv_id"": ""2505.14470"", ""title"": ""PAST: Phonetic-Acoustic Speech Tokenizer"", ""summary"": ""We present PAST, a novel end-to-end framework that jointly models phonetic information alongside signal reconstruction, eliminating the need for external pretrained models. Unlike previous approaches that rely on pretrained self-supervised models, PAST employs supervised phonetic data, directly integrating domain knowledge into the tokenization process via auxiliary tasks. Additionally, we introduce a streamable, causal variant of PAST, enabling real-time speech applications. Results demonstrate that PAST surpasses existing evaluated baseline tokenizers across common evaluation metrics, including phonetic representation and speech reconstruction. Notably, PAST also achieves superior performance when serving as a speech representation for speech language models, further highlighting its effectiveness as a foundation for spoken language generation. To foster further research, we release the full implementation. For code, model checkpoints, and samples see: https://pages.cs.huji.ac.il/adiyoss-lab/PAST"", ""authors"": [""Nadav Har-Tuv"", ""Or Tal"", ""Yossi Adi""], ""published"": ""2025-05-20T15:05:14Z"", ""updated"": ""2025-06-04T08:23:18Z"", ""categories"": [""cs.SD"", ""cs.CL"", ""cs.LG"", ""eess.AS""], ""pdf_url"": ""https://arxiv.org/pdf/2505.14470v2"", ""landing_url"": ""https://arxiv.org/abs/2505.14470v2"", ""doi"": ""https://doi.org/10.48550/arXiv.2505.14470""}",2505.1447,https://openalex.org/W4417298838
10.48550/arxiv.2505.14989,Discrete Audio Representations for Automated Audio Captioning,"Discrete audio representations, termed audio tokens, are broadly categorized into semantic and acoustic tokens, typically generated through unsupervised tokenization of continuous audio representations. However, their applicability to automated audio captioning (AAC) remains underexplored. This paper systematically investigates the viability of audio token-driven models for AAC through comparative analyses of various tokenization methods. Our findings reveal that audio tokenization leads to performance degradation in AAC models compared to those that directly utilize continuous audio representations. To address this issue, we introduce a supervised audio tokenizer trained with an audio tagging objective. Unlike unsupervised tokenizers, which lack explicit semantic understanding, the proposed tokenizer effectively captures audio event information. Experiments conducted on the Clotho dataset demonstrate that the proposed audio tokens outperform conventional audio tokens in the AAC task.",1,,include (junior:4),https://arxiv.org/abs/2505.14989v1,https://arxiv.org/pdf/2505.14989v1,2025,acoustic tokens,speech tokenization,"{""arxiv_id"": ""2505.14989"", ""title"": ""Discrete Audio Representations for Automated Audio Captioning"", ""summary"": ""Discrete audio representations, termed audio tokens, are broadly categorized into semantic and acoustic tokens, typically generated through unsupervised tokenization of continuous audio representations. However, their applicability to automated audio captioning (AAC) remains underexplored. This paper systematically investigates the viability of audio token-driven models for AAC through comparative analyses of various tokenization methods. Our findings reveal that audio tokenization leads to performance degradation in AAC models compared to those that directly utilize continuous audio representations. To address this issue, we introduce a supervised audio tokenizer trained with an audio tagging objective. Unlike unsupervised tokenizers, which lack explicit semantic understanding, the proposed tokenizer effectively captures audio event information. Experiments conducted on the Clotho dataset demonstrate that the proposed audio tokens outperform conventional audio tokens in the AAC task."", ""authors"": [""Jingguang Tian"", ""Haoqin Sun"", ""Xinhui Hu"", ""Xinkang Xu""], ""published"": ""2025-05-21T00:27:38Z"", ""updated"": ""2025-05-21T00:27:38Z"", ""categories"": [""cs.SD"", ""eess.AS""], ""pdf_url"": ""https://arxiv.org/pdf/2505.14989v1"", ""landing_url"": ""https://arxiv.org/abs/2505.14989v1"", ""doi"": ""https://doi.org/10.48550/arXiv.2505.14989""}",2505.14989,https://openalex.org/W4415327346
10.48550/arxiv.2505.16691,EZ-VC: Easy Zero-shot Any-to-Any Voice Conversion,"Voice Conversion research in recent times has increasingly focused on improving the zero-shot capabilities of existing methods. Despite remarkable advancements, current architectures still tend to struggle in zero-shot cross-lingual settings. They are also often unable to generalize for speakers of unseen languages and accents. In this paper, we adopt a simple yet effective approach that combines discrete speech representations from self-supervised models with a non-autoregressive Diffusion-Transformer based conditional flow matching speech decoder. We show that this architecture allows us to train a voice-conversion model in a purely textless, self-supervised fashion. Our technique works without requiring multiple encoders to disentangle speech features. Our model also manages to excel in zero-shot cross-lingual settings even for unseen languages. For Demo: https://ez-vc.github.io/EZ-VC-Demo/",1,,include (junior:5),https://arxiv.org/abs/2505.16691v2,https://arxiv.org/pdf/2505.16691v2,2025,speech representation,voice conversion,"{""arxiv_id"": ""2505.16691"", ""title"": ""EZ-VC: Easy Zero-shot Any-to-Any Voice Conversion"", ""summary"": ""Voice Conversion research in recent times has increasingly focused on improving the zero-shot capabilities of existing methods. Despite remarkable advancements, current architectures still tend to struggle in zero-shot cross-lingual settings. They are also often unable to generalize for speakers of unseen languages and accents. In this paper, we adopt a simple yet effective approach that combines discrete speech representations from self-supervised models with a non-autoregressive Diffusion-Transformer based conditional flow matching speech decoder. We show that this architecture allows us to train a voice-conversion model in a purely textless, self-supervised fashion. Our technique works without requiring multiple encoders to disentangle speech features. Our model also manages to excel in zero-shot cross-lingual settings even for unseen languages. For Demo: https://ez-vc.github.io/EZ-VC-Demo/"", ""authors"": [""Advait Joglekar"", ""Divyanshu Singh"", ""Rooshil Rohit Bhatia"", ""S. Umesh""], ""published"": ""2025-05-22T13:57:02Z"", ""updated"": ""2025-05-23T05:07:17Z"", ""categories"": [""cs.SD"", ""cs.AI"", ""eess.AS""], ""pdf_url"": ""https://arxiv.org/pdf/2505.16691v2"", ""landing_url"": ""https://arxiv.org/abs/2505.16691v2"", ""doi"": ""https://doi.org/10.48550/arXiv.2505.16691""}",2505.16691,https://openalex.org/W4416034139
10.48550/arxiv.2505.17076,Impact of Frame Rates on Speech Tokenizer: A Case Study on Mandarin and English,"The speech tokenizer plays a crucial role in recent speech tasks, generally serving as a bridge between speech signals and language models. While low-frame-rate codecs are widely employed as speech tokenizers, the impact of frame rates on speech tokens remains underexplored. In this study, we investigate how varying frame rates affect speech tokenization by examining Mandarin and English, two typologically distinct languages. We encode speech at different frame rates and evaluate the resulting semantic tokens in the speech recognition task. Our findings reveal that frame rate variations influence speech tokenization differently for each language, highlighting the interplay between frame rates, phonetic density, and language-specific acoustic features. The results provide insights into optimizing frame rate selection for speech tokenizers, with implications for automatic speech recognition, text-to-speech, and other speech-related applications.",1,,include (junior:5),https://arxiv.org/abs/2505.17076v3,https://arxiv.org/pdf/2505.17076v3,2025,discrete speech tokens,speech tokens,"{""arxiv_id"": ""2505.17076"", ""title"": ""Impact of Frame Rates on Speech Tokenizer: A Case Study on Mandarin and English"", ""summary"": ""The speech tokenizer plays a crucial role in recent speech tasks, generally serving as a bridge between speech signals and language models. While low-frame-rate codecs are widely employed as speech tokenizers, the impact of frame rates on speech tokens remains underexplored. In this study, we investigate how varying frame rates affect speech tokenization by examining Mandarin and English, two typologically distinct languages. We encode speech at different frame rates and evaluate the resulting semantic tokens in the speech recognition task. Our findings reveal that frame rate variations influence speech tokenization differently for each language, highlighting the interplay between frame rates, phonetic density, and language-specific acoustic features. The results provide insights into optimizing frame rate selection for speech tokenizers, with implications for automatic speech recognition, text-to-speech, and other speech-related applications."", ""authors"": [""Haoyang Zhang"", ""Hexin Liu"", ""Xiangyu Zhang"", ""Qiquan Zhang"", ""Yuchen Hu"", ""Junqi Zhao"", ""Fei Tian"", ""Xuerui Yang"", ""Leibny Paola Garcia"", ""Eng Siong Chng""], ""published"": ""2025-05-20T06:01:19Z"", ""updated"": ""2025-06-13T17:21:25Z"", ""categories"": [""cs.CL"", ""cs.AI"", ""cs.SD"", ""eess.AS""], ""pdf_url"": ""https://arxiv.org/pdf/2505.17076v3"", ""landing_url"": ""https://arxiv.org/abs/2505.17076v3"", ""doi"": ""https://doi.org/10.48550/arXiv.2505.17076""}",2505.17076,https://openalex.org/W2401084598
10.48550/arxiv.2505.17446,Exploring the Effect of Segmentation and Vocabulary Size on Speech Tokenization for Speech Language Models,"The purpose of speech tokenization is to transform a speech signal into a sequence of discrete representations, serving as the foundation for speech language models (SLMs). While speech tokenization has many options, their effect on the performance of SLMs remains unclear. This paper investigates two key aspects of speech tokenization: the segmentation width and the cluster size of discrete units. First, we segment speech signals into fixed/variable widths and pooled representations. We then train K-means models in multiple cluster sizes. Through the evaluation on zero-shot spoken language understanding benchmarks, we find the positive effect of moderately coarse segmentation and bigger cluster size. Notably, among the best-performing models, the most efficient one achieves a 50% reduction in training data and a 70% decrease in training runtime. Our analysis highlights the importance of combining multiple tokens to enhance fine-grained spoken language understanding.",1,,include (junior:5),https://arxiv.org/abs/2505.17446v2,https://arxiv.org/pdf/2505.17446v2,2025,discrete speech tokens,speech tokens,"{""arxiv_id"": ""2505.17446"", ""title"": ""Exploring the Effect of Segmentation and Vocabulary Size on Speech Tokenization for Speech Language Models"", ""summary"": ""The purpose of speech tokenization is to transform a speech signal into a sequence of discrete representations, serving as the foundation for speech language models (SLMs). While speech tokenization has many options, their effect on the performance of SLMs remains unclear. This paper investigates two key aspects of speech tokenization: the segmentation width and the cluster size of discrete units. First, we segment speech signals into fixed/variable widths and pooled representations. We then train K-means models in multiple cluster sizes. Through the evaluation on zero-shot spoken language understanding benchmarks, we find the positive effect of moderately coarse segmentation and bigger cluster size. Notably, among the best-performing models, the most efficient one achieves a 50% reduction in training data and a 70% decrease in training runtime. Our analysis highlights the importance of combining multiple tokens to enhance fine-grained spoken language understanding."", ""authors"": [""Shunsuke Kando"", ""Yusuke Miyao"", ""Shinnosuke Takamichi""], ""published"": ""2025-05-23T04:03:27Z"", ""updated"": ""2025-05-31T13:32:13Z"", ""categories"": [""cs.CL"", ""cs.SD"", ""eess.AS""], ""pdf_url"": ""https://arxiv.org/pdf/2505.17446v2"", ""landing_url"": ""https://arxiv.org/abs/2505.17446v2"", ""doi"": ""https://doi.org/10.48550/arXiv.2505.17446""}",2505.17446,https://openalex.org/W4415433821
10.48550/arxiv.2505.19462,VoiceStar: Robust Zero-Shot Autoregressive TTS with Duration Control and Extrapolation,"We present VoiceStar, the first zero-shot TTS model that achieves both output duration control and extrapolation. VoiceStar is an autoregressive encoder-decoder neural codec language model, that leverages a novel Progress-Monitoring Rotary Position Embedding (PM-RoPE) and is trained with Continuation-Prompt Mixed (CPM) training. PM-RoPE enables the model to better align text and speech tokens, indicates the target duration for the generated speech, and also allows the model to generate speech waveforms much longer in duration than those seen during. CPM training also helps to mitigate the training/inference mismatch, and significantly improves the quality of the generated speech in terms of speaker similarity and intelligibility. VoiceStar outperforms or is on par with current state-of-the-art models on short-form benchmarks such as Librispeech and Seed-TTS, and significantly outperforms these models on long-form/extrapolation benchmarks (20-50s) in terms of intelligibility and naturalness. Code and models: https://github.com/jasonppy/VoiceStar. Audio samples: https://jasonppy.github.io/VoiceStar_web",1,,include (senior:4),https://arxiv.org/abs/2505.19462v2,https://arxiv.org/pdf/2505.19462v2,2025,discrete speech tokens,speech tokens,"{""arxiv_id"": ""2505.19462"", ""title"": ""VoiceStar: Robust Zero-Shot Autoregressive TTS with Duration Control and Extrapolation"", ""summary"": ""We present VoiceStar, the first zero-shot TTS model that achieves both output duration control and extrapolation. VoiceStar is an autoregressive encoder-decoder neural codec language model, that leverages a novel Progress-Monitoring Rotary Position Embedding (PM-RoPE) and is trained with Continuation-Prompt Mixed (CPM) training. PM-RoPE enables the model to better align text and speech tokens, indicates the target duration for the generated speech, and also allows the model to generate speech waveforms much longer in duration than those seen during. CPM training also helps to mitigate the training/inference mismatch, and significantly improves the quality of the generated speech in terms of speaker similarity and intelligibility. VoiceStar outperforms or is on par with current state-of-the-art models on short-form benchmarks such as Librispeech and Seed-TTS, and significantly outperforms these models on long-form/extrapolation benchmarks (20-50s) in terms of intelligibility and naturalness. Code and models: https://github.com/jasonppy/VoiceStar. Audio samples: https://jasonppy.github.io/VoiceStar_web"", ""authors"": [""Puyuan Peng"", ""Shang-Wen Li"", ""Abdelrahman Mohamed"", ""David Harwath""], ""published"": ""2025-05-26T03:35:44Z"", ""updated"": ""2025-05-31T22:36:04Z"", ""categories"": [""eess.AS"", ""cs.SD""], ""pdf_url"": ""https://arxiv.org/pdf/2505.19462v2"", ""landing_url"": ""https://arxiv.org/abs/2505.19462v2"", ""doi"": ""https://doi.org/10.48550/arXiv.2505.19462""}",2505.19462,
10.48550/arxiv.2505.24496,Speech Token Prediction via Compressed-to-fine Language Modeling for Speech Generation,"Neural audio codecs, used as speech tokenizers, have demonstrated remarkable potential in the field of speech generation. However, to ensure high-fidelity audio reconstruction, neural audio codecs typically encode audio into long sequences of speech tokens, posing a significant challenge for downstream language models in long-context modeling. We observe that speech token sequences exhibit short-range dependency: due to the monotonic alignment between text and speech in text-to-speech (TTS) tasks, the prediction of the current token primarily relies on its local context, while long-range tokens contribute less to the current token prediction and often contain redundant information. Inspired by this observation, we propose a \textbf{compressed-to-fine language modeling} approach to address the challenge of long sequence speech tokens within neural codec language models: (1) \textbf{Fine-grained Initial and Short-range Information}: Our approach retains the prompt and local tokens during prediction to ensure text alignment and the integrity of paralinguistic information; (2) \textbf{Compressed Long-range Context}: Our approach compresses long-range token spans into compact representations to reduce redundant information while preserving essential semantics. Extensive experiments on various neural audio codecs and downstream language models validate the effectiveness and generalizability of the proposed approach, highlighting the importance of token compression in improving speech generation within neural codec language models. The demo of audio samples will be available at https://anonymous.4open.science/r/SpeechTokenPredictionViaCompressedToFinedLM.",1,,include (junior:5),https://arxiv.org/abs/2505.24496v1,https://arxiv.org/pdf/2505.24496v1,2025,discrete speech tokens,speech tokens,"{""arxiv_id"": ""2505.24496"", ""title"": ""Speech Token Prediction via Compressed-to-fine Language Modeling for Speech Generation"", ""summary"": ""Neural audio codecs, used as speech tokenizers, have demonstrated remarkable potential in the field of speech generation. However, to ensure high-fidelity audio reconstruction, neural audio codecs typically encode audio into long sequences of speech tokens, posing a significant challenge for downstream language models in long-context modeling. We observe that speech token sequences exhibit short-range dependency: due to the monotonic alignment between text and speech in text-to-speech (TTS) tasks, the prediction of the current token primarily relies on its local context, while long-range tokens contribute less to the current token prediction and often contain redundant information. Inspired by this observation, we propose a \\textbf{compressed-to-fine language modeling} approach to address the challenge of long sequence speech tokens within neural codec language models: (1) \\textbf{Fine-grained Initial and Short-range Information}: Our approach retains the prompt and local tokens during prediction to ensure text alignment and the integrity of paralinguistic information; (2) \\textbf{Compressed Long-range Context}: Our approach compresses long-range token spans into compact representations to reduce redundant information while preserving essential semantics. Extensive experiments on various neural audio codecs and downstream language models validate the effectiveness and generalizability of the proposed approach, highlighting the importance of token compression in improving speech generation within neural codec language models. The demo of audio samples will be available at https://anonymous.4open.science/r/SpeechTokenPredictionViaCompressedToFinedLM."", ""authors"": [""Wenrui Liu"", ""Qian Chen"", ""Wen Wang"", ""Yafeng Chen"", ""Jin Xu"", ""Zhifang Guo"", ""Guanrou Yang"", ""Weiqin Li"", ""Xiaoda Yang"", ""Tao Jin"", ""Minghui Fang"", ""Jialong Zuo"", ""Bai Jionghao"", ""Zemin Liu""], ""published"": ""2025-05-30T11:47:29Z"", ""updated"": ""2025-05-30T11:47:29Z"", ""categories"": [""eess.AS""], ""pdf_url"": ""https://arxiv.org/pdf/2505.24496v1"", ""landing_url"": ""https://arxiv.org/abs/2505.24496v1"", ""doi"": ""https://doi.org/10.48550/arXiv.2505.24496""}",2505.24496,https://openalex.org/W4414857663
10.48550/arxiv.2506.00809,FUSE: Universal Speech Enhancement using Multi-Stage Fusion of Sparse Compression and Token Generation Models for the URGENT 2025 Challenge,"We propose a multi-stage framework for universal speech enhancement, designed for the Interspeech 2025 URGENT Challenge. Our system first employs a Sparse Compression Network to robustly separate sources and extract an initial clean speech estimate from noisy inputs. This is followed by an efficient generative model that refines speech quality by leveraging self-supervised features and optimizing a masked language modeling objective on acoustic tokens derived from a neural audio codec. In the final stage, a fusion network integrates the outputs of the first two stages with the original noisy signal, achieving a balanced improvement in both signal fidelity and perceptual quality. Additionally, a shift trick that aggregates multiple time-shifted predictions, along with output blending, further boosts performance. Experimental results on challenging multilingual datasets with variable sampling rates and diverse distortion types validate the effectiveness of our approach.",1,,include (senior:4),https://arxiv.org/abs/2506.00809v1,https://arxiv.org/pdf/2506.00809v1,2025,acoustic tokens,speech tokenization,"{""arxiv_id"": ""2506.00809"", ""title"": ""FUSE: Universal Speech Enhancement using Multi-Stage Fusion of Sparse Compression and Token Generation Models for the URGENT 2025 Challenge"", ""summary"": ""We propose a multi-stage framework for universal speech enhancement, designed for the Interspeech 2025 URGENT Challenge. Our system first employs a Sparse Compression Network to robustly separate sources and extract an initial clean speech estimate from noisy inputs. This is followed by an efficient generative model that refines speech quality by leveraging self-supervised features and optimizing a masked language modeling objective on acoustic tokens derived from a neural audio codec. In the final stage, a fusion network integrates the outputs of the first two stages with the original noisy signal, achieving a balanced improvement in both signal fidelity and perceptual quality. Additionally, a shift trick that aggregates multiple time-shifted predictions, along with output blending, further boosts performance. Experimental results on challenging multilingual datasets with variable sampling rates and diverse distortion types validate the effectiveness of our approach."", ""authors"": [""Nabarun Goswami"", ""Tatsuya Harada""], ""published"": ""2025-06-01T03:23:27Z"", ""updated"": ""2025-06-01T03:23:27Z"", ""categories"": [""cs.SD"", ""eess.AS""], ""pdf_url"": ""https://arxiv.org/pdf/2506.00809v1"", ""landing_url"": ""https://arxiv.org/abs/2506.00809v1"", ""doi"": ""https://doi.org/10.48550/arXiv.2506.00809""}",2506.00809,https://openalex.org/W4414893697
10.48550/arxiv.2506.00843,HASRD: Hierarchical Acoustic and Semantic Representation Disentanglement,"Effective speech representations for spoken language models must balance semantic relevance with acoustic fidelity for high-quality reconstruction. However, existing approaches struggle to achieve both simultaneously. To address this, we introduce Hierarchical Acoustic and Semantic Representation Disentanglement (HASRD, pronounced `hazard'), a framework that factorizes self-supervised learning representations into discrete semantic and acoustic tokens. HASRD assigns the semantic representation to the first codebook, while encoding acoustic residuals in subsequent codebooks. This preserves ASR performance while achieving high-quality reconstruction. Additionally, we enhance HASRD's encoder efficiency, improving ASR performance without compromising reconstruction quality. Compared to SpeechTokenizer, HASRD achieves a 44% relative WER improvement, superior reconstruction quality, and 2x lower bitrate, demonstrating its effectiveness in disentangling acoustic and semantic information.",1,,include (junior:5),https://arxiv.org/abs/2506.00843v1,https://arxiv.org/pdf/2506.00843v1,2025,acoustic tokens,speech tokenization,"{""arxiv_id"": ""2506.00843"", ""title"": ""HASRD: Hierarchical Acoustic and Semantic Representation Disentanglement"", ""summary"": ""Effective speech representations for spoken language models must balance semantic relevance with acoustic fidelity for high-quality reconstruction. However, existing approaches struggle to achieve both simultaneously. To address this, we introduce Hierarchical Acoustic and Semantic Representation Disentanglement (HASRD, pronounced `hazard'), a framework that factorizes self-supervised learning representations into discrete semantic and acoustic tokens. HASRD assigns the semantic representation to the first codebook, while encoding acoustic residuals in subsequent codebooks. This preserves ASR performance while achieving high-quality reconstruction. Additionally, we enhance HASRD's encoder efficiency, improving ASR performance without compromising reconstruction quality. Compared to SpeechTokenizer, HASRD achieves a 44% relative WER improvement, superior reconstruction quality, and 2x lower bitrate, demonstrating its effectiveness in disentangling acoustic and semantic information."", ""authors"": [""Amir Hussein"", ""Sameer Khurana"", ""Gordon Wichern"", ""Francois G. Germain"", ""Jonathan Le Roux""], ""published"": ""2025-06-01T05:38:39Z"", ""updated"": ""2025-06-01T05:38:39Z"", ""categories"": [""eess.AS"", ""cs.SD""], ""pdf_url"": ""https://arxiv.org/pdf/2506.00843v1"", ""landing_url"": ""https://arxiv.org/abs/2506.00843v1"", ""doi"": ""https://doi.org/10.48550/arXiv.2506.00843""}",2506.00843,https://openalex.org/W4414893720
