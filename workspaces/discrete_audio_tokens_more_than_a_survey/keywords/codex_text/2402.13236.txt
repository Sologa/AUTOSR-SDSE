1
Towards audio language modeling - an overview
Haibin Wu1, Xuanjun Chen1∗, Yi-Cheng Lin1∗, Kai-wei Chang1, Ho-Lam Chung1,
Alexander H. Liu2, Hung-yi Lee1
Abstract—Neural audio codecs are initially introduced to codec models to cover their training methodologies, imple-
compress audio data into compact codes to reduce transmission mentation settings, and training data. Secondly, we expand
latency.Researchersrecentlydiscoveredthepotentialofcodecsas
our analysis to include eleven diverse codec-based language
suitable tokenizers for converting continuous audio into discrete
models, examining how they utilize the codecs and the tasks
codes,whichcanbeemployedtodevelopaudiolanguagemodels
(LMs). Numerous high-performance neural audio codecs and to which they can be applied. Through this comprehensive
codec-basedLMshavebeendeveloped.Thepaperaimstoprovide review,weaimtoofferthecommunityinsightsintothediverse
a thorough and systematic overview of the neural audio codec methodologies and potential directions in the field of neural
models and codec-based LMs.
codecs and codec-based language modeling.
Index Terms—Neural codec, codec-based language model
II. COMPREHENSIVECOMPARISONFORNEURALAUDIO
I. INTRODUCTION CODECMODELS
Neural audio codec models were first introduced to com- Codec models aim to compress and decompress speech
press audio for efficient data transmission. The encoder con- signals efficiently. Traditional codecs are developed based on
verts the audio into codec codes, which are then transmitted. psycho-acoustics and speech synthesis [21], [22]. Recently,
The receiver then uses the codec decoder to reconstruct the the neural codec models demonstrated highly effective for
audio using the received codes. compression and signal reconstruction, outperforming tradi-
Language modeling has proven to be highly successful tionalcodecs.Consideringthebroadspectrumofcodecmodels
in the field of Natural Language Processing (NLP). Audio within the research community, each trained with its distinct
data encompasses not only textual content but also rich in- configurationsandtrainingtechniques,thereisaclearneedfor
formation about speaker timbre, emotion, and general audio, athoroughexaminationthatcoversthetrainingmethodologies,
offering deeper possibilities for language model applications. implementation settings, and training data employed across
Researchers, especially those in large companies with sig- these codec models. The six codec models have distinct
nificant computational resources, recently leverage the po- training details, resulting in a collection of fifteen different
tential of neural codecs [1]–[8] as suitable tokenizers for codec models, as summarized in Table I.
convertingcontinuousaudiointodiscretecodes,whichcanbe
employed to develop audio language models (LMs) [9]–[20]. A. Brief method overview for codecs
The current codec-based language models and codec models
SoundStream[2]standsasoneofthepioneeringimplemen-
aresummarizedinFigure1.Thesefindingspromptlygarnered
tations of neural codec models, embodying a classic neural
the community’s attention, sparking a fervor for developing
codecarchitecturecomprisingencoder,quantizer,anddecoder
codecs tailored to audio language modeling. Numerous high-
modules.ItutilizesthestreamingSEANets[23]asitsencoder
performance neural audio codec models and audio LMs have
anddecoder.Thequantizerincorporatesaspeechenhancement
been developed.
system with a Residual Vector Quantization (RVQ) [2], [24]
An ideal codec should maintain content while preserving
bottleneck to obtain parallel token streams. During training,
paralinguistic and speaker-related information. Similarly, a
the model parameters are optimized using a combination of
universal audio language model should be able to generalize
reconstruction and adversarial loss. SoundStorm [3] is an
acrossvariousaudiotypes,suchasspeech,music,andgeneral
improved version of SoundStream to achieve both efficiency
audio, covering a wide range of applications. The arms race
andhigh-qualityaudiogeneration.Itaccomplishesthisbyem-
in developing codecs and audio LMs is still ongoing.
ploying an architecture specifically tailored to the hierarchical
Given the significant advancements in codecs and audio
structureofaudiotokens.Moreover,itpioneersaparallel,non-
language models over the past three years as shown in Fig-
autoregressive decoding scheme, which relies on confidence-
ure 1, there has yet to be a comprehensive review comparing
basedstrategiesforresidualvector-quantizedtokensequences.
themandprovidinginspirationtothecommunity.Inthisstudy,
Encodec [1] builds upon a framework similar to Sound-
we aim to fill this research gap by thoroughly reviewing and
Stream. Nonetheless, it further augments its capabilities by
comparing various existing neural codec models and audio
integrating supplementary LSTM [25] layers and harnessing
codec-based language models. Firstly, we specifically conduct
a Transformer-based language model [26] to model the RVQ
an in-depth analysis of six representative open-source neural
codes,therebyamplifyingitssequencemodelingperformance.
Then, there is a stream of work aimed at making codec
∗Equalsecondcontribution.1NationalTaiwanUniversity.2Massachusetts
InstituteofTechnology. models more general and powerful. AudioDec [4] represents
4202
beF
02
]SA.ssee[
1v63231.2042:viXra

2
S o u n d Strea m [ 2] A u di o L M [ 9] A u di o Ge n [ 2 0] E n C o dec [ 1] V A L L- E [ 1 2] M usic L M [ 1 1] V A L L- E X [ 1 3] Vi o L A [ 1 4] A u di o Dec [ 4] Aca de mic C o dec [
5]
S o u n d St or m [ 3] D A C [ 8] M usic Ge n [ 1 8] A u di o Pa L M [ 1 0] S peec h To ke nizer S [
7
p
]
eec h X [ 1 7] F u nc C o dec [ 6] La ura G P T [ 1 6] U ni A u di o [ 1 5]
2021.06 2022.12 2023.06 2023.12
Neural Codec Model Codec-based Language Model
Fig.1. Timelineofcurrentneuralcodecmodelsandcodec-basedlanguagemodels.
an enhanced version of Encodec, implementing a group con- B. Comparison from methodology angles
volutionmechanismtofacilitatethereal-timeoperationofthe
streamable network while also harnessing the capabilities of We compare several techniques proposed by these codecs
HiFi-GAN [27] to effectively generate high-fidelity audio at a in Table II. The abbreviation “A-F” represents different codec
high sampling rate of 48 kHz. models. Please refer to Table I for the corresponding model
In the AcademiCodec model introduced by [5], a novel full name. The design of discriminators constitutes a pivotal
technique known as group-residual vector quantization is elementwithincodecmodels.Encodecinitiallyintroducesthe
presented. It employs multiple parallel RVQ groups. This Multi-scale-STFT Discriminator (MS-STFTD). In contrast to
techniqueisspecificallytailoredforgenerationtasks.Itaimsto the multi-scale discriminator (MSD) proposed in MelGAN
enhance the reconstruction performance while using a limited [24],whichcaptureslong-termdependencies,themulti-period
numberofcodebooks,consequentlyachievinganimpressively discriminator (MPD) proposed in HiFi-GAN [30] exhibits
low bit rate per second (BPS). This low BPS is of utmost a capacity to discern more nuanced periodic details. Con-
significanceasiteffectivelyaddressesthechallengeoflengthy sequently, AudioDec replaces the conventionally employed
speech tokens in speech language modeling, resulting in STFTDwithaHiFi-GAN-basedMPD,observinganenhance-
reduced sequence lengths. ment in audio quality within their model. AcademiCodec
SpeechTokenizer [7] is a unified speech tokenizer designed integrates prior research efforts by incorporating the MS-
for speech language models. It implements an Encoder- STFTD from Encodec and both HiFi-GAN-based MPD and
DecoderarchitectureenhancedwithRVQ.Byintegratingboth MSD. Both SpeechTokenizer and Funcodec adopt identical
semantic and acoustic tokens, SpeechTokenizer hierarchically discriminators to AcademiCodec, with Funcodec offering a
separates various aspects of speech information across dif- unified interface adaptable to any combination of these three
ferent RVQ layers. Specifically, SpeechTokenizer is designed discriminator types. DAC identifies that employing MSD and
to regularize the first RVQ layer to highlight semantic in- MPDalonegeneratesaudiodisplayingblurrinessandartifacts.
formation by learning the Hubert tokens [28]. Using such To address this, they propose the application of a multi-scale,
techniques can enhance the disentanglement of information multi-band STFT discriminator (MS-MB-STFTD) to improve
across different RVQ layers. phase modeling and mitigate aliasing artifacts.
Descript-audio-codec (DAC) [8], a universal neural codec SpeechTokenizer utilizes semantic tokens from Hubert L9
model, distinguishes itself through its exceptional ability to as a teacher for the RVQ process. This guidance enables the
maintainhigh-fidelityaudioqualityacrossawidespectrumof disentanglement of content information into the first layer
data types, encompassing general audio, music, and speech. It of the tokenizer, while paralinguistic information is retained
accomplishes this feature by employing a number of train- in subsequent layers. FunCodec seeks to integrate semantic
ing techniques, such as periodic activation functions [29], information by combining, adding, or residualizing the audio
enhanced residual vector quantization using factorized and codec with semantic tokens. The study reveals that including
L2-normalized codes, random quantizer dropout to preserve semantic tokens enhances audio quality, particularly with the
audioreconstructionquality,aswellasrefiningadversarialand residual inclusion method. Additionally, SpeechTokenizer and
reconstruction loss during the training process. The authors FunCodecutilizeK-meanstoclustersamplesinthefirstmini-
highlight the crucial importance of the periodic activation batch for initializing the VQ codebook, leading to improved
function among the employed techniques. codeutilization.DACfollowstheapproachofBigVGAN[31],
Unlike most models focusing on the time domain, Fun- employing snake activation [29] for trainable control over the
Codec [6] proposes a frequency-domain codec. The authors frequency of periodic signals. AcademiCodec employs mul-
claim they can achieve comparable performance with fewer tiple RVQ codebooks (multiple residual groups) to represent
parameters and lower computation complexity. Meanwhile, it intermediate features. They demonstrate that using multiple
alsofindsthatincorporatingsemanticinformationinthecodec residual groups achieves good reconstruction performance
tokens improves speech quality at low bit rates. while employing only a few codebooks. Encodec trains an

3
TABLEI
CODECINFORMATIONCOMPARISON.”A-F”REPRESENTSDIFFERENT
NEURALCODECMODELS,WHERE”A”ISSPEECHTOKENIZER[7],”B∼”
ISACADEMICODEC[5],”C”ISAUDIODEC[4],”D∼”ISDAC[8],”E∼” Codec Language Modeling Codec Decoder
ISENCODEC[1],AND”F∼”ISFUNCODEC[6].ncREPRESENTSTHE
CODEBOOKNUMBER,SRREPRESENTSTHESAMPLERATE,ANDBPS
REPRESENTSTHEBITRATEINUNITBITSPERSECOND.
Codecinformation Trainingdata nc SR BPS
Neural Codec LM
A 16k Librispeech 8 16 4
B1 hifi 16k 320d LibriTTS 4 16 2
Context code
B2 hifi 16k 320d large uni VCTK 4 16 2
B3 hifi 24k 320d AISHELL 4 24 3 Codec code
Context Codec
C 24k 320d Valentini 8 24 6.4 Encoder Tokenizer
D1 16k CommonVoice,DAPS 12 16 6
(Context) Text,
D2 24k VCTK,MUSDB 32 24 24 MIDI, …
D3 44k Jamendo,AudioSet 9 44.1 8
Fig.2. Codec-basedLanguageModeling
E1 24k 1.5bps 2 24 1.5
E2 24k 3bps CommonVoice 4 24 3
E3 24k 6bps DAPS,Jamendo 8 24 6 III. CURRENTCODEC-BASEDSPEECHLANGUAGEMODELS
E4 24k 12bps AudioSet,FSD50K 16 24 12
AsshowninFigure2,theprocessofneuralcodec-basedaudio
E5 24k 24bps 32 24 24
language modeling begins by converting context information,
F1 en libritts 16k gr1nq32ds320 32 16 16
such as text and MIDI, into context codes, while simulta-
F2 en libritts 16k gr8nq32ds320 SubsetofLibriTTS 32 16 16
neously encoding the audio into codec codes. These context
F3 en libritts 16k nq32ds320 32 16 16
F4 en libritts 16k nq32ds640 32 16 8 and codec codes are then employed in the language modeling
F5 zh en 16k nq32ds320 25khourscollecteddata 32 16 16 phase to generate the desired target codec code sequence.
F6 zh en 16k nq32ds640 (enandzh-cn) 32 16 8 Subsequently, the target codec code sequence is passed to the
codecdecodertoproducetheaudiooutput.Theentirepipeline
embodies an audio-to-audio modeling approach.
TABLEII
COMPARISONBETWEENCODECIMPLEMENTATIONSTRATEGY.SEM
REPRESENTSCODECINCLUDINGSEMANTICTOKENS.SNAKEREPRESENTS
THECODECMODELTHATEMPLOYSSNAKEACTIVATION.MRG A. Overview for codec-based LMs
REPRESENTSCODECHASMULTIPLERESIDUALGROUPS.NOISY
REPRESENTSCODECUTILIZESNOISYDATAINTRAINING.LM AudioLM [9] is the pioneering model in introducing codec
REPRESENTSTHEMODELINCLUDINGLANGUAGEMODELTRAINING.KM codesforlanguagemodeling,utilizingahierarchicalapproach
REPRESENTSCODECUSESK-MEANSTOCLUSTERSAMPLESAS
thatencompassestwodistinctstages.Thefirststagegenerates
INITIALIZATIONOFVQCODEBOOK.
semantic tokens using a self-supervised w2v-BERT model
[32]. These tokens are then leveraged in the second stage
Codec Discriminators SEM Snake MRG Noisy LM KM
as conditioning elements to create acoustic tokens using a
A MSD+MPD+MS-STFTD ✓ ✗ ✗ ✗ ✗ ✓
B MSD+MPD+MS-STFTD ✗ ✗ ✓ ✗ ✗ ✗ SoundStream neural codec [2].
C MPD ✗ ✗ ✗ ✗ ✗ ✗ VALL-E [12], VALL-E X [13], and SpeechX [17], all orig-
D MPD+MS-MB-STFTD ✗ ✓ ✗ ✗ ✗ ✗ inate from Microsoft and are neural codec language models
E MS-STFTD ✗ ✗ ✗ ✗ ✓ ✗ trained to generate discrete codes derived from EnCodec [1],
F MSD+MPD+MS-STFTD ✓ ✗ ✗ ✓ ✗ ✓
based on textual or acoustic inputs. VALL-E can generate
high-quality personalized speech with only a 3-second enroll-
ment recording from an unseen speaker. Furthermore, VALL-
additional small transformer model for entropy coding over
E X can produce high-quality speech in the target language
the quantized units, which reduces bandwidth and accelerates
withjustasinglespeechutteranceinthesourcelanguageasa
encoding and decoding.
prompt.Additionally,SpeechXintroducesaunifiedframework
to address not only zero-shot TTS but also various types of
speech transformation tasks, including speech enhancement
C. Implementation details
and speech editing.
We compare the codebook number, training data, sampling What sets ViaLA [14], AudioPaLM [10], and LauraGPT
rate, and bit rate per second in Table I. From the training [16] apart is their dual capability to generate both text and
data perspective, SpeechTokenizer [7], AudioDec [4], and audio. VioLA tries to tackle the question “Is one decoder-
FunCodec [6] utilize only English speech dataset. Academi- only generative model all you need for speech recognition,
Codec [5] incorporates bilingual speech datasets, including synthesis, and translation?” by employing language modeling
AISHELL for Chinese and LibriTTS and VCTK for English. thatintegrates bothtext tokensand audiotokens (extractedby
Both DAC [8], and Encodec [1] encompass diverse modality EnCodec [1]), along with the use of task IDs and language
data,includingspeech,music,andaudio,inthetrainingdata. IDs. AudioPaLM constructs a unified vocabulary comprising

4
both text and audio tokens. It is a decoder-only, autoregres- TABLEIII
sive model capable of processing and generating both text CODEC-BASEDLANGUAGEMODELSCOMPARISON.”T”MEANSTEXT,
”AUD”MEANSAUDIO,”P”MEANSPHONEME,AND”M”MEANSMIDI.
and speech. Additionally, AudioPaLM’s initialization stems
fromPaLM-2[33],atext-onlylanguagemodel.AudioPaLM’s
approach to audio tokenization resembles that of AudioLM. CLM Task Input Output Codec
Moreover, AudioPaLM adopts and extends the SoundStream
AudioLM[9] SC,PC AUD AUD SoundStream[2]
model to SoundStorm [3]. LauraGPT [16] is a versatile
AudioGen[20] AC AUD,T AUD SoundStream[2]
language model built on a decoder-only text-based language
VALL-E[12] TTS AUD,T AUD EnCodec[1]
model, Qwen-2B [34]. LauraGPT has the capability to pro- MusicLM[11] MG AUD,T AUD SoundStream[2]
cess both audio and text inputs, generating outputs in either VALL-EX[13] TTS,S2ST AUD,T AUD EnCodec[1]
modality. LauraGPT encodes input audio into continuous VioLA[14] ASR,S2TT,TTS,MT AUD,T AUD,T EnCodec[1]
MusicGen[18] MG,SG AUD AUD EnCodec[1]
representationsusingaConformerencoderanddecodesoutput
AudioPaLM[10] ASR,S2TT,TTS,MT AUD,T AUD,T SoundStorm[3]
audio using FunCodec [6] discrete codes. The authors claim
SpeechX[17] SE,SR,TSE,TTS,SPED AUD,T AUD EnCodec[1]
this specific audio features design for inputs and outputs will
ASR,S2TT,TTS,MT,SE
result in improved performance for speech generation using LauraGPT[16] AUD,T AUD,T FunCodec[6]
AAC,SER,SLU
some preliminary experimental results.
TTS,VC,SE,TSE,SVS
UniAudio [15] utilizes language modeling to generate a P,M,
UniAudio[15] TTSO,TTM,AUED AUD EnCodec[1]
wide range of audio types, including speech, sounds, music, SD,ITTS,SPED AUD,T
and singing, using textual or acoustic tokens as inputs. Uni-
Audio stands out for its ability to enhance autoregressive pre-
diction speed by introducing a multi-scale Transformer model modeling and can conduct the task of speech continuation. In
[35], which employs a large global transformer to predict the thefieldofspeechtranslation,recentadvancementshavebeen
first-layercodeccodesandasmalllocaltransformertopredict made possible through these discrete units. [43] pre-trained a
the codec codes for the subsequent codec layers. The codec Unit mBART combined with a wav2vec 2.0 [44] encoder to
model in UniAudio is revised from EnCodec. directlypredictthetranslateddiscreteunits.UnitY[45]further
Additionally, there are other codec-based language models incorporates text modality to enhance speech translation. The
designedforsoundmodeling.AudioGen[20]trainedaSound- Seamless models [46], [47] integrate the UnitY framework to
Stream model to get audio tokens and subsequently trained a perform expressive and streaming speech-to-text and speech-
language model to utilize textual features as conditions for to-speech translation. With the development of these powerful
generating audio tokens. MusicLM [11] follows a training speech LMs, researchers have begun to explore the use of
strategy similar to AudioLM but extends its scope to en- promptingonspeechLMsforvariousspeechprocessingtasks,
compass music features. It approaches the task of conditional including prompt tuning [48]–[50], in-context learning [51],
music generation through a hierarchical sequence-to-sequence and instruction tuning [52], [53].
modeling approach. Initially, it utilizes music tokens from
Mulan [36] to generate semantic tokens from the w2v-BERT B. Comparison for Codec-based audio language models
model.Subsequently,itemploysbothmusictokensandseman- In Table III, we compare the inputs, outputs, and down-
tic tokens to generate acoustic features through Soundstream. stream tasks of different codec-based language models. We
MusicGen [18] is a music language model designed to work alsosummarizethatthedownstreamtasksconductedbydiffer-
with EnCodec discrete tokens. It accepts textual descriptions ent codec-based language models: Speech Continuation (SC),
or melodic features as input conditions to generate tokens, Piano Continuation (PC), Audio Continuation (AC), Text-to-
which can be reconstructed to high-fidelity music. Speech (TTS), Music Generation (MG), Stereophonic Gener-
Anotherbranchofspeechlanguagemodelingaimstoutilize ation (SG), Speech to Speech Translation (S2ST), Automatic
discrete units obtained by quantizing self-supervised speech Speech Recognition (ASR), Spoken Language Understand-
representations. While these discrete units contain rich acous- ing (SLU), Automated Audio Captioning (AAC), Speech to
tic and linguistic information [37], they lack speaker and Text Translation (S2TT), Machine Translation (MT), Speech
paralinguisticinformation[38].Thisresearchdirectionfocuses Enhancement (SE), Speech Removal (SR), Target Speaker
onmodelingthesemanticsofspeech,withtheoptionaluseof Extraction (TSE), Speech Editing (SPED), Voice Conversion
encoders to learn about speaker characteristics and prosody. (VC),SingingVoiceSynthesis(SVS),Text-to-Sound(TTSO),
Pioneering work is speech-resynthesis [38], which utilizes Text-to-Music (TTM), Audio Editing (AUED), Speech Dere-
these discrete units in conjunction with prosody and speaker verb(SD),InstructedTTS(ITTS).Finally,weshowthecodec
encoderstoencodespeechintolow-bitratecodes.Thesecodes models adopted by different LMs.
can then be resynthesized into a speech signal with a decoder
to achieve low-bitrate transmission. Additionally, these dis- IV. CONCLUSION
creteunitscanberegardedas“pseudo-text,”servingasafoun- Thepaperfillstheresearchblanktoreviewtheneuralcodec
dation for training textless speech language models. Notable modelsandLMsbuiltuponthem.Wehopethecomprehensive
examples include GSLM [39], pGSLM [40], dGSLM [41], review and comparisons can inspire future research works to
and TWIST [42]. By engaging in the pre-trained task of next- boost the development of neural codec models and codec-
token prediction, these speech LMs perform spoken language based LMs.

5
REFERENCES [30] JungilKong,JaehyeonKim,andJaekyoungBae, “Hifi-gan:Generative
adversarial networks for efficient and high fidelity speech synthesis,”
[1] Alexandre De´fossez et al., “High fidelity neural audio compression,” 2020.
arXivpreprintarXiv:2210.13438,2022. [31] Sang-gilLee,WeiPing,BorisGinsburg,BryanCatanzaro,andSungroh
[2] NeilZeghidouretal.,“Soundstream:Anend-to-endneuralaudiocodec,” Yoon, “Bigvgan:Auniversalneuralvocoderwithlarge-scaletraining,”
IEEE/ACM Transactions on Audio, Speech, and Language Processing, arXivpreprintarXiv:2206.04658,2022.
vol.30,pp.495–507,2021. [32] Yu-An Chung et al., “W2v-bert: Combining contrastive learning and
[3] Zala´nBorsosetal., “Soundstorm:Efficientparallelaudiogeneration,” maskedlanguagemodelingforself-supervisedspeechpre-training,” in
arXivpreprintarXiv:2305.09636,2023. 2021IEEEAutomaticSpeechRecognitionandUnderstandingWorkshop
[4] Yi-Chiao Wu et al., “Audiodec: An open-source streaming high- (ASRU).IEEE,2021,pp.244–250.
fidelityneuralaudiocodec,” inICASSP2023-2023IEEEInternational [33] Rohan Anil et al., “Palm 2 technical report,” arXiv preprint
Conference on Acoustics, Speech and Signal Processing (ICASSP). arXiv:2305.10403,2023.
IEEE,2023,pp.1–5. [34] JinzeBai,ShuaiBai,YunfeiChu,ZeyuCui,KaiDang,XiaodongDeng,
[5] DongchaoYangetal., “Hifi-codec:Group-residualvectorquantization Yang Fan, Wenbin Ge, Yu Han, Fei Huang, et al., “Qwen technical
forhighfidelityaudiocodec,” arXivpreprintarXiv:2305.02765,2023. report,” arXivpreprintarXiv:2309.16609,2023.
[6] Zhihao Du, Shiliang Zhang, Kai Hu, and Siqi Zheng, “Funcodec: A [35] LiliYu,Da´nielSimig,ColinFlaherty,ArmenAghajanyan,LukeZettle-
fundamental,reproducibleandintegrableopen-sourcetoolkitforneural moyer,andMikeLewis, “Megabyte:Predictingmillion-bytesequences
speechcodec,” arXivpreprintarXiv:2309.07405,2023. withmultiscaletransformers,” arXivpreprintarXiv:2305.07185,2023.
[7] Xin Zhang, Dong Zhang, Shimin Li, Yaqian Zhou, and Xipeng Qiu, [36] Qingqing Huang, Aren Jansen, Joonseok Lee, Ravi Ganti, Judith Yue
“Speechtokenizer: Unified speech tokenizer for speech large language Li, and Daniel PW Ellis, “Mulan: A joint embedding of music audio
models,” arXivpreprintarXiv:2308.16692,2023. andnaturallanguage,” arXivpreprintarXiv:2208.12415,2022.
[8] Rithesh Kumar, Prem Seetharaman, Alejandro Luebs, Ishaan Kumar, [37] DanWells,HaoTang,andKorinRichmond,“PhoneticAnalysisofSelf-
and Kundan Kumar, “High-fidelity audio compression with improved supervised Representations of English Speech,” in Proc. Interspeech
rvqgan,” arXivpreprintarXiv:2306.06546,2023. 2022,2022,pp.3583–3587.
[9] Zala´nBorsosetal., “Audiolm:alanguagemodelingapproachtoaudio [38] Adam Polyak et al., “Speech resynthesis from discrete disentangled
generation,” IEEE/ACMTransactionsonAudio,Speech,andLanguage self-supervisedrepresentations,” inInterspeech,2021,pp.3615–3619.
Processing,2023. [39] Kushal Lakhotia et al., “On generative spoken language modeling
from raw audio,” Transactions of the Association for Computational
[10] PaulKRubensteinetal., “Audiopalm:Alargelanguagemodelthatcan
speakandlisten,” arXivpreprintarXiv:2306.12925,2023.
Linguistics,vol.9,pp.1336–1354,2021.
[40] EugeneKharitonovetal., “Text-freeprosody-awaregenerativespoken
[11] AndreaAgostinellietal.,“Musiclm:Generatingmusicfromtext,”arXiv
languagemodeling,” arXivpreprintarXiv:2109.03264,2021.
preprintarXiv:2301.11325,2023.
[41] TuAnhNguyenetal.,“Generativespokendialoguelanguagemodeling,”
[12] ChengyiWangetal., “Neuralcodeclanguagemodelsarezero-shottext
TransactionsoftheAssociationforComputationalLinguistics,vol.11,
tospeechsynthesizers,” arXivpreprintarXiv:2301.02111,2023.
pp.250–266,2023.
[13] Ziqiang Zhang et al., “Speak foreign languages with your own
[42] MichaelHassidetal., “Textuallypretrainedspeechlanguagemodels,”
voice: Cross-lingual neural codec language modeling,” arXiv preprint
arXivpreprintarXiv:2305.13009,2023.
arXiv:2303.03926,2023.
[43] Sravya Popuri et al., “Enhanced Direct Speech-to-Speech Translation
[14] Tianrui Wang et al., “Viola: Unified codec language models
Using Self-supervised Pre-training and Data Augmentation,” in Proc.
for speech recognition, synthesis, and translation,” arXiv preprint
Interspeech2022,2022,pp.5195–5199.
arXiv:2305.16107,2023.
[44] Alexei Baevski et al., “wav2vec 2.0: A framework for self-supervised
[15] Dongchao Yang et al., “Uniaudio: An audio foundation model toward
learning of speech representations,” Advances in neural information
universalaudiogeneration,” arXivpreprintarXiv:2310.00704,2023.
processingsystems,vol.33,pp.12449–12460,2020.
[16] QianChenetal., “Lauragpt:Listen,attend,understand,andregenerate
[45] Hirofumi Inaguma et al., “Unity: Two-pass direct speech-to-speech
audiowithgpt,” arXivpreprintarXiv:2310.04673,2023.
translationwithdiscreteunits,” arXivpreprintarXiv:2212.08055,2022.
[17] Xiaofei Wang et al., “Speechx: Neural codec language model as a
[46] Lo¨ıcBarraultetal.,“Seamlessm4t-massivelymultilingual&multimodal
versatilespeechtransformer,” arXivpreprintarXiv:2308.06873,2023.
machinetranslation,” arXivpreprintarXiv:2308.11596,2023.
[18] Jade Copet et al., “Simple and controllable music generation,” arXiv
[47] Lo¨ıcBarraultetal., “Seamless:Multilingualexpressiveandstreaming
preprintarXiv:2306.05284,2023.
speechtranslation,” arXivpreprintarXiv:2312.05187,2023.
[19] GaelLeLanetal.,“Stack-and-delay:anewcodebookpatternformusic
[48] Kai-WeiChangetal.,“AnExplorationofPromptTuningonGenerative
generation,” arXivpreprintarXiv:2309.08804,2023. Spoken Language Model for Speech Processing Tasks,” in Proc.
[20] FelixKreuketal.,“Audiogen:Textuallyguidedaudiogeneration,”arXiv Interspeech2022,2022,pp.5005–5009.
preprintarXiv:2209.15352,2022. [49] Kai-Wei Chang et al., “Speechprompt v2: Prompt tuning for speech
[21] Jean-MarcValinetal., “Rfc6716:Definitionoftheopusaudiocodec,” classificationtasks,” arXivpreprintarXiv:2303.00733,2023.
2012. [50] HaibinWu,Kai-WeiChang,Yuan-KueiWu,andHung-yiLee,“Speech-
[22] Martin Dietz et al., “Overview of the evs codec architecture,” in gen: Unlocking the generative power of speech language models with
2015 IEEE International Conference on Acoustics, Speech and Signal prompts,” arXivpreprintarXiv:2306.02207,2023.
Processing(ICASSP).IEEE,2015,pp.5698–5702. [51] Ming-HaoHsuetal., “Anexplorationofin-contextlearningforspeech
[23] MarcoTagliasacchietal., “Seanet:Amulti-modalspeechenhancement languagemodel,” arXivpreprintarXiv:2310.12477,2023.
network,” arXivpreprintarXiv:2009.02095,2020. [52] Chun-Yi Kuan, Chen-An Li, et al., “Towards general-purpose text-
[24] Kundan Kumar et al., “Melgan: Generative adversarial networks for instruction-guided voice conversion,” in 2023 IEEE Automatic Speech
conditional waveform synthesis,” Advances in neural information RecognitionandUnderstandingWorkshop(ASRU).IEEE,2023,pp.1–8.
processingsystems,vol.32,2019. [53] Chien-yu Huang, Ke-Han Lu, et al., “Dynamic-superb: Towards a dy-
[25] SeppHochreiterandJu¨rgenSchmidhuber, “Longshort-termmemory,” namic, collaborative, and comprehensive instruction-tuning benchmark
Neuralcomputation,vol.9,no.8,pp.1735–1780,1997. forspeech,” arXivpreprintarXiv:2309.09510,2023.
[26] AshishVaswanietal., “Attentionisallyouneed,” Advancesinneural
informationprocessingsystems,vol.30,2017.
[27] JungilKong,JaehyeonKim,andJaekyoungBae, “Hifi-gan:Generative
adversarial networks for efficient and high fidelity speech synthesis,”
AdvancesinNeuralInformationProcessingSystems,vol.33,pp.17022–
17033,2020.
[28] Wei-Ning Hsu et al., “Hubert: Self-supervised speech representation
learningbymaskedpredictionofhiddenunits,”IEEE/ACMTransactions
on Audio, Speech, and Language Processing, vol. 29, pp. 3451–3460,
2021.
[29] LiuZiyin,TilmanHartwig,andMasahitoUeda, “Neuralnetworksfail
to learn periodic functions and how to fix it,” Advances in Neural
InformationProcessingSystems,vol.33,pp.1583–1594,2020.
