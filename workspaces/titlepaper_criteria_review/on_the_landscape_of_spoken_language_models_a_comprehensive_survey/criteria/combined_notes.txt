### PDF Background (Survey Summaries)
### PDF Topic Definition
本篇為「On The Landscape of Spoken Language Models: A Comprehensive Survey」的系統性綜述，聚焦於口語語言模型（Spoken Language Models, SLMs）的發展現況。主題涵蓋SLM的架構設計、訓練策略、資料來源、評估方法，以及其作為「通用語音處理系統」的潛力。核心概念包括純語音模型（pure speech LM）、語音+文字模型（speech+text LM）、語音感知文字模型（speech-aware text LM），以及這些模型如何從單一任務導向逐步邁向能以自然語言指令處理多種語音任務的通用系統。評估面向則涵蓋生成品質、任務泛化能力、語音與文字對齊、即時對話能力、信任性（如幻覺、偏見、deepfake）等。

### PDF Key Trends
- 語音處理領域正從任務專屬模型轉向通用、可指令化的SLM，類比於文字LLM的發展路徑。
- 模型架構趨向多模態融合，結合語音編碼器、文字編碼器、序列建模與模態轉換器。
- 訓練策略從自監督預訓練（如wav2vec 2.0, HuBERT）延伸至多階段、跨模態對齊與指令微調（instruction tuning）。
- 資料來源多元，包括大規模未標註語音、語音-文字配對資料、合成語音對話數據等。
- 研究重點逐漸從單一任務（如ASR、TTS）擴展至複雜推理、開放式問答、全雙工對話與語音生成。
- 評估方法快速演進，從傳統ASR/TTS指標到多維度基準（如Dynamic-SUPERB、AIR-Bench）、LLM-as-a-judge、信任性測試等。

### PDF Capability Highlights
- 能處理語音輸入/輸出，並可選擇性支援文字輸入/輸出。
- 具備以自然語言指令處理多種語音任務的能力。
- 支援語音與文字的對齊與融合，實現跨模態理解與生成。
- 具備端到端語音生成、語音理解、對話管理等複合任務能力。
- 可進行即時或全雙工語音對話，並處理語者特徵、情緒、韻律等非語義資訊。

### PDF Inclusion Signals
- 關鍵字：「spoken language model」、「speech+text LM」、「universal speech processing」、「instruction tuning」、「duplex dialogue」、「speech-aware text LM」、「full-duplex」、「speech-text alignment」。
- 章節參考：Section 2（Overall architecture）、Section 3（SLM components）、Section 4（Training Strategies）、Section 6（Duplex speech dialogue）、Section 7（Benchmarking and Evaluating SLMs）。
- 涉及多模態語音-文字融合、自然語言指令、跨任務泛化能力、語音生成與理解等主題。
- 涉及SLM模型設計、訓練流程、評估基準（如Dynamic-SUPERB, AIR-Bench, VoiceBench）。
- 以通用語音處理為目標，非僅針對單一ASR/TTS任務。

### PDF Exclusion Signals
- 僅聚焦於單一傳統語音任務（如ASR、TTS、SID）且無泛化或指令化能力。
- 只討論音樂、環境聲音等非語音音訊模型，未與語音處理深度結合。
- 僅為語音編碼器、語音解碼器、tokenizer等單一組件技術，未涉及完整SLM架構。
- 缺乏自然語言指令、無多任務/多模態能力，或僅以任務標籤（task token）操作。
- 僅為傳統對話系統、規則式語音處理，未結合現代LLM/SLM技術。

### PDF Notes
- On_The_Landscape_of_Spoken_Language_Models_A_Comprehensive_Survey.pdf: 
  - 系統性梳理SLM發展，明確分類純語音、語音+文字、語音感知文字模型，並比較其架構、訓練、評估方法與挑戰。
  - 強調通用語音處理系統的目標與現階段限制，提出標準化評測、開源資源、跨語言/跨領域應用等未來方向。
  - 詳細整理各類SLM的設計選擇、代表性模型、訓練與評估基準，並提供發展時間線與模型對照表。
  - 深入探討全雙工對話、信任性評估（幻覺、偏見、deepfake）、語音-文字對齊等議題。
  - 提供豐富的模型與基準資料，便於後續篩選與比較。
### Web Search Notes
以下內容依照你的指示撰寫，並以可直接用於系統性回顧（systematic review / survey）之收錄與排除規則為目標。

---

### Topic Definition
本主題聚焦於「Spoken Language Models（SLMs）」的整體研究版圖，涵蓋能直接以語音為核心建模單位、並支援語音理解、生成或推理的通用模型。相較於傳統以語音辨識（ASR）或語音合成（TTS）為主的模組化管線，SLMs 嘗試將語音視為一級語言表示，類似文字領域中大型語言模型（LLMs）的發展脈絡。

在技術內涵上，Spoken Language Models 包含純語音語言模型（直接對離散或連續語音 token 建模），以及語音與文字深度整合的模型（例如以語音編碼器結合文字 LLM，或以音訊 token 作為跨模態表示）。研究重點涵蓋模型架構、訓練策略、資料與評估設定，以及其在通用語音理解與互動上的潛力。([arxiv.org](https://arxiv.org/abs/2504.08528?utm_source=openai))

---

### Summary
近年研究顯示，語音處理正從任務導向、級聯式系統，轉向可重用、可泛化的 Spoken Language Models。此趨勢與文字 LLM 的發展相呼應，並促進語音與語言之間更緊密的表示共享。整體亮點在於跨模態整合方法的多樣化，以及對通用評估與基準仍未成熟的共識。([arxiv.org](https://arxiv.org/abs/2504.08528?utm_source=openai))

---

### Summary Topics
S1: Spoken Language Models 的定義與發展脈絡  
S2: 模型架構與語音表示方式（連續、離散、音訊 token）  
S3: 語音與大型語言模型（LLMs）的整合策略  
S4: 訓練資料、評估任務與未來挑戰  

---

### Inclusion Criteria (Required)
- 必須同時滿足以下所有條件：

1. 主題定義：研究內容必須符合「Spoken Language Models 為以語音作為核心建模對象，並具備通用語音理解、生成或跨模態語言能力之模型」的定義，且明確討論模型層級（非僅單一應用系統）。  
   source: https://arxiv.org/abs/2504.08528  
   topic ids: S1, S2

2. 論文需以英文撰寫，並提供可評估的技術描述（如模型架構、訓練方式或實驗設計），以利一致性審查。  
   source: https://arxiv.org/abs/2504.08528  
   topic ids: S1

---

### Inclusion Criteria (Any-of Groups)
- Group 技術取向與模型差異
  * Option: 探討「純語音語言模型」或以離散／連續語音 token 為核心表示的研究。  
    source: https://arxiv.org/abs/2504.08528  
    topic ids: S2

  * Option: 探討語音與大型語言模型（LLMs）之整合方法（如文字介面式、潛在表示式或音訊 token 式整合）。  
    source: https://arxiv.org/abs/2502.19548  
    topic ids: S3

  * Option: 提出或系統性分析具跨模態（語音–文字–其他模態）能力的語言模型架構。  
    source: https://arxiv.org/abs/2305.11000  
    topic ids: S3, S4

---

### Exclusion Criteria
- 僅聚焦於單一傳統任務（如 ASR、TTS），且未討論通用語音語言建模或語言層級能力者，予以排除。  
  source: https://arxiv.org/abs/2504.08528  
  topic ids: S1

- 僅為資料集介紹、評測基準或應用系統展示，未提出或分析 Spoken Language Model 架構與訓練原則者，予以排除。  
  source: https://arxiv.org/abs/2104.13225  
  topic ids: S4

- 非研究論文（如教學投影片、部落格文章、未經審查之簡報），或缺乏完整技術細節者，予以排除。  
  source: internal  
  topic ids: S1

---

### Sources
https://arxiv.org/abs/2504.08528  
https://arxiv.org/abs/2502.19548  
https://arxiv.org/abs/2104.13225  
https://arxiv.org/abs/2305.11000