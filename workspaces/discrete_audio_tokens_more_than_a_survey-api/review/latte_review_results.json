[
  {
    "title": "Recent Advances in Discrete Speech Tokens: A Review",
    "abstract": "The rapid advancement of speech generation technologies in the era of large language models (LLMs) has established discrete speech tokens as a foundational paradigm for speech representation. These tokens, characterized by their discrete, compact, and concise nature, are not only advantageous for efficient transmission and storage, but also inherently compatible with the language modeling framework, enabling seamless integration of speech into text-dominated LLM architectures. Current research categorizes discrete speech tokens into two principal classes: acoustic tokens and semantic tokens, each of which has evolved into a rich research domain characterized by unique design philosophies and methodological approaches. This survey systematically synthesizes the existing taxonomy and recent innovations in discrete speech tokenization, conducts a critical examination of the strengths and limitations of each paradigm, and presents systematic experimental comparisons across token types. Furthermore, we identify persistent challenges in the field and propose potential research directions, aiming to offer actionable insights to inspire future advancements in the development and application of discrete speech tokens.",
    "metadata": {
      "arxiv_id": "2502.06490",
      "title": "Recent Advances in Discrete Speech Tokens: A Review",
      "summary": "The rapid advancement of speech generation technologies in the era of large language models (LLMs) has established discrete speech tokens as a foundational paradigm for speech representation. These tokens, characterized by their discrete, compact, and concise nature, are not only advantageous for efficient transmission and storage, but also inherently compatible with the language modeling framework, enabling seamless integration of speech into text-dominated LLM architectures. Current research categorizes discrete speech tokens into two principal classes: acoustic tokens and semantic tokens, each of which has evolved into a rich research domain characterized by unique design philosophies and methodological approaches. This survey systematically synthesizes the existing taxonomy and recent innovations in discrete speech tokenization, conducts a critical examination of the strengths and limitations of each paradigm, and presents systematic experimental comparisons across token types. Furthermore, we identify persistent challenges in the field and propose potential research directions, aiming to offer actionable insights to inspire future advancements in the development and application of discrete speech tokens.",
      "authors": [
        "Yiwei Guo",
        "Zhihan Li",
        "Hankun Wang",
        "Bohan Li",
        "Chongtian Shao",
        "Hanglei Zhang",
        "Chenpeng Du",
        "Xie Chen",
        "Shujie Liu",
        "Kai Yu"
      ],
      "published": "2025-02-10T14:08:25Z",
      "updated": "2025-12-12T05:18:11Z",
      "categories": [
        "eess.AS",
        "cs.AI",
        "cs.MM",
        "cs.SD",
        "eess.SP"
      ],
      "pdf_url": "https://arxiv.org/pdf/2502.06490v4",
      "landing_url": "https://arxiv.org/abs/2502.06490v4",
      "doi": "https://doi.org/10.48550/arXiv.2502.06490"
    },
    "final_verdict": "include (seed_filter)",
    "review_skipped": true,
    "discard_reason": null,
    "force_include_reason": "seed_filter_selected"
  },
  {
    "title": "Personalized Acoustic Modeling by Weakly Supervised Multi-Task Deep Learning using Acoustic Tokens Discovered from Unlabeled Data",
    "abstract": "It is well known that recognizers personalized to each user are much more effective than user-independent recognizers. With the popularity of smartphones today, although it is not difficult to collect a large set of audio data for each user, it is difficult to transcribe it. However, it is now possible to automatically discover acoustic tokens from unlabeled personal data in an unsupervised way. We therefore propose a multi-task deep learning framework called a phoneme-token deep neural network (PTDNN), jointly trained from unsupervised acoustic tokens discovered from unlabeled data and very limited transcribed data for personalized acoustic modeling. We term this scenario \"weakly supervised\". The underlying intuition is that the high degree of similarity between the HMM states of acoustic token models and phoneme models may help them learn from each other in this multi-task learning framework. Initial experiments performed over a personalized audio data set recorded from Facebook posts demonstrated that very good improvements can be achieved in both frame accuracy and word accuracy over popularly-considered baselines such as fDLR, speaker code and lightly supervised adaptation. This approach complements existing speaker adaptation approaches and can be used jointly with such techniques to yield improved results.",
    "metadata": {
      "arxiv_id": "1706.07793",
      "title": "Personalized Acoustic Modeling by Weakly Supervised Multi-Task Deep Learning using Acoustic Tokens Discovered from Unlabeled Data",
      "summary": "It is well known that recognizers personalized to each user are much more effective than user-independent recognizers. With the popularity of smartphones today, although it is not difficult to collect a large set of audio data for each user, it is difficult to transcribe it. However, it is now possible to automatically discover acoustic tokens from unlabeled personal data in an unsupervised way. We therefore propose a multi-task deep learning framework called a phoneme-token deep neural network (PTDNN), jointly trained from unsupervised acoustic tokens discovered from unlabeled data and very limited transcribed data for personalized acoustic modeling. We term this scenario \"weakly supervised\". The underlying intuition is that the high degree of similarity between the HMM states of acoustic token models and phoneme models may help them learn from each other in this multi-task learning framework. Initial experiments performed over a personalized audio data set recorded from Facebook posts demonstrated that very good improvements can be achieved in both frame accuracy and word accuracy over popularly-considered baselines such as fDLR, speaker code and lightly supervised adaptation. This approach complements existing speaker adaptation approaches and can be used jointly with such techniques to yield improved results.",
      "authors": [
        "Cheng-Kuan Wei",
        "Cheng-Tao Chung",
        "Hung-Yi Lee",
        "Lin-Shan Lee"
      ],
      "published": "2017-06-23T12:54:43Z",
      "updated": "2017-06-23T12:54:43Z",
      "categories": [
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/1706.07793v1",
      "landing_url": "https://arxiv.org/abs/1706.07793v1",
      "doi": "https://doi.org/10.1109/ICASSP.2017.7953141"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The abstract explicitly centers on discrete acoustic tokens learned from unlabeled data and uses them as a core component in a multi-task model for personalized speech, satisfying the inclusion criteria and presenting no exclusion concerns.",
      "evaluation": 5
    },
    "round-A_JuniorNano_reasoning": "The abstract explicitly centers on discrete acoustic tokens learned from unlabeled data and uses them as a core component in a multi-task model for personalized speech, satisfying the inclusion criteria and presenting no exclusion concerns.",
    "round-A_JuniorNano_evaluation": 5,
    "round-A_JuniorMini_output": {
      "reasoning": "The study explicitly focuses on discovering acoustic tokens from unlabeled speech data and utilizes these discrete tokens within a weakly supervised multi-task deep learning framework for personalized acoustic modeling. This aligns well with the inclusion criterion that requires discrete audio/speech tokenization as a core object of study, specifically converting continuous audio signals into discrete symbols to improve speech recognition tasks. The work is methodological and experimentally validated, and the text is in English, fulfilling the academic availability requirement. There is no indication that the study only uses continuous features without discrete tokenization or that it falls into any exclusion categories such as focusing solely on traditional codecs or being an engineering-only report. Therefore, this study strongly meets all inclusion criteria and none of the exclusion criteria.",
      "evaluation": 5
    },
    "round-A_JuniorMini_reasoning": "The study explicitly focuses on discovering acoustic tokens from unlabeled speech data and utilizes these discrete tokens within a weakly supervised multi-task deep learning framework for personalized acoustic modeling. This aligns well with the inclusion criterion that requires discrete audio/speech tokenization as a core object of study, specifically converting continuous audio signals into discrete symbols to improve speech recognition tasks. The work is methodological and experimentally validated, and the text is in English, fulfilling the academic availability requirement. There is no indication that the study only uses continuous features without discrete tokenization or that it falls into any exclusion categories such as focusing solely on traditional codecs or being an engineering-only report. Therefore, this study strongly meets all inclusion criteria and none of the exclusion criteria.",
    "round-A_JuniorMini_evaluation": 5,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "include (junior:5)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Synthesizing Mathematical Identities with E-Graphs",
    "abstract": "Identities compactly describe properties of a mathematical expression and can be leveraged into faster and more accurate function implementations. However, identities must currently be discovered manually, which requires a lot of expertise. We propose a two-phase synthesis and deduplication pipeline that discovers these identities automatically. In the synthesis step, a set of rewrite rules is composed, using an e-graph, to discover candidate identities. However, most of these candidates are duplicates, which a secondary deduplication step discards using integer linear programming and another e-graph. Applied to a set of 61 benchmarks, the synthesis phase generates 7215 candidate identities which the deduplication phase then reduces down to 125 core identities.",
    "metadata": {
      "arxiv_id": "2206.07086",
      "title": "Synthesizing Mathematical Identities with E-Graphs",
      "summary": "Identities compactly describe properties of a mathematical expression and can be leveraged into faster and more accurate function implementations. However, identities must currently be discovered manually, which requires a lot of expertise. We propose a two-phase synthesis and deduplication pipeline that discovers these identities automatically. In the synthesis step, a set of rewrite rules is composed, using an e-graph, to discover candidate identities. However, most of these candidates are duplicates, which a secondary deduplication step discards using integer linear programming and another e-graph. Applied to a set of 61 benchmarks, the synthesis phase generates 7215 candidate identities which the deduplication phase then reduces down to 125 core identities.",
      "authors": [
        "Ian Briggs",
        "Pavel Panchekha"
      ],
      "published": "2022-06-14T18:21:01Z",
      "updated": "2022-06-14T18:21:01Z",
      "categories": [
        "cs.SC"
      ],
      "pdf_url": "https://arxiv.org/pdf/2206.07086v1",
      "landing_url": "https://arxiv.org/abs/2206.07086v1",
      "doi": "https://doi.org/10.48550/arXiv.2206.07086"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item discusses mathematical identities and e-graphs with no discrete audio tokenization or speech-related analysis.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item discusses mathematical identities and e-graphs with no discrete audio tokenization or speech-related analysis.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on synthesizing mathematical identities using e-graphs and does not address discrete audio/speech tokenization or any related audio signal processing tasks; thus, it does not meet the inclusion criteria and falls under exclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on synthesizing mathematical identities using e-graphs and does not address discrete audio/speech tokenization or any related audio signal processing tasks; thus, it does not meet the inclusion criteria and falls under exclusion criteria.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Misspelling Semantics In Thai",
    "abstract": "User-generated content is full of misspellings. Rather than being just random noise, we hypothesise that many misspellings contain hidden semantics that can be leveraged for language understanding tasks. This paper presents a fine-grained annotated corpus of misspelling in Thai, together with an analysis of misspelling intention and its possible semantics to get a better understanding of the misspelling patterns observed in the corpus. In addition, we introduce two approaches to incorporate the semantics of misspelling: Misspelling Average Embedding (MAE) and Misspelling Semantic Tokens (MST). Experiments on a sentiment analysis task confirm our overall hypothesis: additional semantics from misspelling can boost the micro F1 score up to 0.4-2%, while blindly normalising misspelling is harmful and suboptimal.",
    "metadata": {
      "arxiv_id": "2206.09680",
      "title": "Misspelling Semantics In Thai",
      "summary": "User-generated content is full of misspellings. Rather than being just random noise, we hypothesise that many misspellings contain hidden semantics that can be leveraged for language understanding tasks. This paper presents a fine-grained annotated corpus of misspelling in Thai, together with an analysis of misspelling intention and its possible semantics to get a better understanding of the misspelling patterns observed in the corpus. In addition, we introduce two approaches to incorporate the semantics of misspelling: Misspelling Average Embedding (MAE) and Misspelling Semantic Tokens (MST). Experiments on a sentiment analysis task confirm our overall hypothesis: additional semantics from misspelling can boost the micro F1 score up to 0.4-2%, while blindly normalising misspelling is harmful and suboptimal.",
      "authors": [
        "Pakawat Nakwijit",
        "Matthew Purver"
      ],
      "published": "2022-06-20T09:42:50Z",
      "updated": "2022-06-20T09:42:50Z",
      "categories": [
        "cs.CL"
      ],
      "pdf_url": "https://arxiv.org/pdf/2206.09680v1",
      "landing_url": "https://arxiv.org/abs/2206.09680v1",
      "doi": "https://doi.org/10.48550/arXiv.2206.09680"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The article focuses on textual misspellings in Thai and semantic analysis for sentiment tasks, not on discrete audio/speech tokenization into a finite vocabulary for audio reconstruction or understanding; it thus fails the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The article focuses on textual misspellings in Thai and semantic analysis for sentiment tasks, not on discrete audio/speech tokenization into a finite vocabulary for audio reconstruction or understanding; it thus fails the inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper focuses on misspelling semantics in Thai text and proposes methods to incorporate misspelling semantics for language understanding tasks; it does not discuss discrete audio/speech tokenization or any aspect of converting continuous audio signals into discrete tokens, which is a core inclusion criterion. Therefore, it does not meet the inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper focuses on misspelling semantics in Thai text and proposes methods to incorporate misspelling semantics for language understanding tasks; it does not discuss discrete audio/speech tokenization or any aspect of converting continuous audio signals into discrete tokens, which is a core inclusion criterion. Therefore, it does not meet the inclusion criteria and should be excluded.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Self Supervised Learning for Few Shot Hyperspectral Image Classification",
    "abstract": "Deep learning has proven to be a very effective approach for Hyperspectral Image (HSI) classification. However, deep neural networks require large annotated datasets to generalize well. This limits the applicability of deep learning for HSI classification, where manually labelling thousands of pixels for every scene is impractical. In this paper, we propose to leverage Self Supervised Learning (SSL) for HSI classification. We show that by pre-training an encoder on unlabeled pixels using Barlow-Twins, a state-of-the-art SSL algorithm, we can obtain accurate models with a handful of labels. Experimental results demonstrate that this approach significantly outperforms vanilla supervised learning.",
    "metadata": {
      "arxiv_id": "2206.12117",
      "title": "Self Supervised Learning for Few Shot Hyperspectral Image Classification",
      "summary": "Deep learning has proven to be a very effective approach for Hyperspectral Image (HSI) classification. However, deep neural networks require large annotated datasets to generalize well. This limits the applicability of deep learning for HSI classification, where manually labelling thousands of pixels for every scene is impractical. In this paper, we propose to leverage Self Supervised Learning (SSL) for HSI classification. We show that by pre-training an encoder on unlabeled pixels using Barlow-Twins, a state-of-the-art SSL algorithm, we can obtain accurate models with a handful of labels. Experimental results demonstrate that this approach significantly outperforms vanilla supervised learning.",
      "authors": [
        "Nassim Ait Ali Braham",
        "Lichao Mou",
        "Jocelyn Chanussot",
        "Julien Mairal",
        "Xiao Xiang Zhu"
      ],
      "published": "2022-06-24T07:21:53Z",
      "updated": "2022-06-24T07:21:53Z",
      "categories": [
        "cs.CV",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2206.12117v1",
      "landing_url": "https://arxiv.org/abs/2206.12117v1",
      "doi": "https://doi.org/10.48550/arXiv.2206.12117"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The article deals with self-supervised learning for hyperspectral image classification and does not propose or analyze discrete audio tokenization or any speech token sequences, so it fails the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The article deals with self-supervised learning for hyperspectral image classification and does not propose or analyze discrete audio tokenization or any speech token sequences, so it fails the inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on self-supervised learning for hyperspectral image classification and does not address discrete audio/speech tokenization or any related audio signal processing; therefore, it does not meet the inclusion criteria related to audio tokenization nor does it discuss any form of discrete tokenization for audio signals.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on self-supervised learning for hyperspectral image classification and does not address discrete audio/speech tokenization or any related audio signal processing; therefore, it does not meet the inclusion criteria related to audio tokenization nor does it discuss any form of discrete tokenization for audio signals.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Attention-based conditioning methods using variable frame rate for style-robust speaker verification",
    "abstract": "We propose an approach to extract speaker embeddings that are robust to speaking style variations in text-independent speaker verification. Typically, speaker embedding extraction includes training a DNN for speaker classification and using the bottleneck features as speaker representations. Such a network has a pooling layer to transform frame-level to utterance-level features by calculating statistics over all utterance frames, with equal weighting. However, self-attentive embeddings perform weighted pooling such that the weights correspond to the importance of the frames in a speaker classification task. Entropy can capture acoustic variability due to speaking style variations. Hence, an entropy-based variable frame rate vector is proposed as an external conditioning vector for the self-attention layer to provide the network with information that can address style effects. This work explores five different approaches to conditioning. The best conditioning approach, concatenation with gating, provided statistically significant improvements over the x-vector baseline in 12/23 tasks and was the same as the baseline in 11/23 tasks when using the UCLA speaker variability database. It also significantly outperformed self-attention without conditioning in 9/23 tasks and was worse in 1/23. The method also showed significant improvements in multi-speaker scenarios of SITW.",
    "metadata": {
      "arxiv_id": "2206.13680",
      "title": "Attention-based conditioning methods using variable frame rate for style-robust speaker verification",
      "summary": "We propose an approach to extract speaker embeddings that are robust to speaking style variations in text-independent speaker verification. Typically, speaker embedding extraction includes training a DNN for speaker classification and using the bottleneck features as speaker representations. Such a network has a pooling layer to transform frame-level to utterance-level features by calculating statistics over all utterance frames, with equal weighting. However, self-attentive embeddings perform weighted pooling such that the weights correspond to the importance of the frames in a speaker classification task. Entropy can capture acoustic variability due to speaking style variations. Hence, an entropy-based variable frame rate vector is proposed as an external conditioning vector for the self-attention layer to provide the network with information that can address style effects. This work explores five different approaches to conditioning. The best conditioning approach, concatenation with gating, provided statistically significant improvements over the x-vector baseline in 12/23 tasks and was the same as the baseline in 11/23 tasks when using the UCLA speaker variability database. It also significantly outperformed self-attention without conditioning in 9/23 tasks and was worse in 1/23. The method also showed significant improvements in multi-speaker scenarios of SITW.",
      "authors": [
        "Amber Afshan",
        "Abeer Alwan"
      ],
      "published": "2022-06-28T01:14:09Z",
      "updated": "2022-06-28T01:14:09Z",
      "categories": [
        "eess.AS",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2206.13680v1",
      "landing_url": "https://arxiv.org/abs/2206.13680v1",
      "doi": "https://doi.org/10.48550/arXiv.2206.13680"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The work analyzes continuous speaker-embedding features and an attention-based frame weighting scheme for style robustness, with no discrete audio tokenization or quantized tokens, so it should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The work analyzes continuous speaker-embedding features and an attention-based frame weighting scheme for style robustness, with no discrete audio tokenization or quantized tokens, so it should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on speaker verification using continuous frame-level features with weighted pooling via self-attention and entropy-based variable frame rate conditioning, but does not propose or analyze any discrete audio/speech tokenization methods converting continuous signals into discrete token sequences as required by the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on speaker verification using continuous frame-level features with weighted pooling via self-attention and entropy-based variable frame rate conditioning, but does not propose or analyze any discrete audio/speech tokenization methods converting continuous signals into discrete token sequences as required by the inclusion criteria.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "GLD-Net: Improving Monaural Speech Enhancement by Learning Global and Local Dependency Features with GLD Block",
    "abstract": "For monaural speech enhancement, contextual information is important for accurate speech estimation. However, commonly used convolution neural networks (CNNs) are weak in capturing temporal contexts since they only build blocks that process one local neighborhood at a time. To address this problem, we learn from human auditory perception to introduce a two-stage trainable reasoning mechanism, referred as global-local dependency (GLD) block. GLD blocks capture long-term dependency of time-frequency bins both in global level and local level from the noisy spectrogram to help detecting correlations among speech part, noise part, and whole noisy input. What is more, we conduct a monaural speech enhancement network called GLD-Net, which adopts encoder-decoder architecture and consists of speech object branch, interference branch, and global noisy branch. The extracted speech feature at global-level and local-level are efficiently reasoned and aggregated in each of the branches. We compare the proposed GLD-Net with existing state-of-art methods on WSJ0 and DEMAND dataset. The results show that GLD-Net outperforms the state-of-the-art methods in terms of PESQ and STOI.",
    "metadata": {
      "arxiv_id": "2206.14962",
      "title": "GLD-Net: Improving Monaural Speech Enhancement by Learning Global and Local Dependency Features with GLD Block",
      "summary": "For monaural speech enhancement, contextual information is important for accurate speech estimation. However, commonly used convolution neural networks (CNNs) are weak in capturing temporal contexts since they only build blocks that process one local neighborhood at a time. To address this problem, we learn from human auditory perception to introduce a two-stage trainable reasoning mechanism, referred as global-local dependency (GLD) block. GLD blocks capture long-term dependency of time-frequency bins both in global level and local level from the noisy spectrogram to help detecting correlations among speech part, noise part, and whole noisy input. What is more, we conduct a monaural speech enhancement network called GLD-Net, which adopts encoder-decoder architecture and consists of speech object branch, interference branch, and global noisy branch. The extracted speech feature at global-level and local-level are efficiently reasoned and aggregated in each of the branches. We compare the proposed GLD-Net with existing state-of-art methods on WSJ0 and DEMAND dataset. The results show that GLD-Net outperforms the state-of-the-art methods in terms of PESQ and STOI.",
      "authors": [
        "Xinmeng Xu",
        "Yang Wang",
        "Jie Jia",
        "Binbin Chen",
        "Jianjun Hao"
      ],
      "published": "2022-06-30T01:16:40Z",
      "updated": "2022-06-30T01:16:40Z",
      "categories": [
        "eess.AS",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2206.14962v1",
      "landing_url": "https://arxiv.org/abs/2206.14962v1",
      "doi": "https://doi.org/10.48550/arXiv.2206.14962"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This article uses continuous spectrogram features for monaural speech enhancement and does not introduce discrete audio tokenization or quantization, so it does not meet the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "This article uses continuous spectrogram features for monaural speech enhancement and does not introduce discrete audio tokenization or quantization, so it does not meet the inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on a monaural speech enhancement method using a neural network architecture that processes continuous audio features (noisy spectrogram) and captures dependencies in the time-frequency domain, but it does not mention any discrete audio/speech tokenization or conversion of continuous signals into discrete symbols, nor analyzing or proposing tokenization schemes as per the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on a monaural speech enhancement method using a neural network architecture that processes continuous audio features (noisy spectrogram) and captures dependencies in the time-frequency domain, but it does not mention any discrete audio/speech tokenization or conversion of continuous signals into discrete symbols, nor analyzing or proposing tokenization schemes as per the inclusion criteria.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "esCorpius: A Massive Spanish Crawling Corpus",
    "abstract": "In the recent years, transformer-based models have lead to significant advances in language modelling for natural language processing. However, they require a vast amount of data to be (pre-)trained and there is a lack of corpora in languages other than English. Recently, several initiatives have presented multilingual datasets obtained from automatic web crawling. However, the results in Spanish present important shortcomings, as they are either too small in comparison with other languages, or present a low quality derived from sub-optimal cleaning and deduplication. In this paper, we introduce esCorpius, a Spanish crawling corpus obtained from near 1 Pb of Common Crawl data. It is the most extensive corpus in Spanish with this level of quality in the extraction, purification and deduplication of web textual content. Our data curation process involves a novel highly parallel cleaning pipeline and encompasses a series of deduplication mechanisms that together ensure the integrity of both document and paragraph boundaries. Additionally, we maintain both the source web page URL and the WARC shard origin URL in order to complain with EU regulations. esCorpius has been released under CC BY-NC-ND 4.0 license and is available on HuggingFace.",
    "metadata": {
      "arxiv_id": "2206.15147",
      "title": "esCorpius: A Massive Spanish Crawling Corpus",
      "summary": "In the recent years, transformer-based models have lead to significant advances in language modelling for natural language processing. However, they require a vast amount of data to be (pre-)trained and there is a lack of corpora in languages other than English. Recently, several initiatives have presented multilingual datasets obtained from automatic web crawling. However, the results in Spanish present important shortcomings, as they are either too small in comparison with other languages, or present a low quality derived from sub-optimal cleaning and deduplication. In this paper, we introduce esCorpius, a Spanish crawling corpus obtained from near 1 Pb of Common Crawl data. It is the most extensive corpus in Spanish with this level of quality in the extraction, purification and deduplication of web textual content. Our data curation process involves a novel highly parallel cleaning pipeline and encompasses a series of deduplication mechanisms that together ensure the integrity of both document and paragraph boundaries. Additionally, we maintain both the source web page URL and the WARC shard origin URL in order to complain with EU regulations. esCorpius has been released under CC BY-NC-ND 4.0 license and is available on HuggingFace.",
      "authors": [
        "Asier Gutiérrez-Fandiño",
        "David Pérez-Fernández",
        "Jordi Armengol-Estapé",
        "David Griol",
        "Zoraida Callejas"
      ],
      "published": "2022-06-30T09:29:18Z",
      "updated": "2022-07-01T08:22:32Z",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2206.15147v2",
      "landing_url": "https://arxiv.org/abs/2206.15147v2",
      "doi": "https://doi.org/10.48550/arXiv.2206.15147"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item discusses a Spanish web-crawling corpus and data curation, with no focus on discrete audio/speech tokenization or analysis of discrete audio representations, so it fails all inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item discusses a Spanish web-crawling corpus and data curation, with no focus on discrete audio/speech tokenization or analysis of discrete audio representations, so it fails all inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study introduces esCorpius, a large Spanish text corpus derived from web crawling, focusing on text data collection and cleaning rather than on discrete audio/speech tokenization or analysis of discrete audio tokens, and does not involve any audio signal processing or tokenization methods.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study introduces esCorpius, a large Spanish text corpus derived from web crawling, focusing on text data collection and cleaning rather than on discrete audio/speech tokenization or analysis of discrete audio tokens, and does not involve any audio signal processing or tokenization methods.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Online Target Speaker Voice Activity Detection for Speaker Diarization",
    "abstract": "This paper proposes an online target speaker voice activity detection system for speaker diarization tasks, which does not require a priori knowledge from the clustering-based diarization system to obtain the target speaker embeddings. First, we employ a ResNet-based front-end model to extract the frame-level speaker embeddings for each coming block of a signal. Next, we predict the detection state of each speaker based on these frame-level speaker embeddings and the previously estimated target speaker embedding. Then, the target speaker embeddings are updated by aggregating these frame-level speaker embeddings according to the predictions in the current block. We iteratively extract the results for each block and update the target speaker embedding until reaching the end of the signal. Experimental results show that the proposed method is better than the offline clustering-based diarization system on the AliMeeting dataset.",
    "metadata": {
      "arxiv_id": "2207.05920",
      "title": "Online Target Speaker Voice Activity Detection for Speaker Diarization",
      "summary": "This paper proposes an online target speaker voice activity detection system for speaker diarization tasks, which does not require a priori knowledge from the clustering-based diarization system to obtain the target speaker embeddings. First, we employ a ResNet-based front-end model to extract the frame-level speaker embeddings for each coming block of a signal. Next, we predict the detection state of each speaker based on these frame-level speaker embeddings and the previously estimated target speaker embedding. Then, the target speaker embeddings are updated by aggregating these frame-level speaker embeddings according to the predictions in the current block. We iteratively extract the results for each block and update the target speaker embedding until reaching the end of the signal. Experimental results show that the proposed method is better than the offline clustering-based diarization system on the AliMeeting dataset.",
      "authors": [
        "Weiqing Wang",
        "Qingjian Lin",
        "Ming Li"
      ],
      "published": "2022-07-13T01:56:31Z",
      "updated": "2022-07-13T01:56:31Z",
      "categories": [
        "eess.AS",
        "cs.SD",
        "eess.SP"
      ],
      "pdf_url": "https://arxiv.org/pdf/2207.05920v1",
      "landing_url": "https://arxiv.org/abs/2207.05920v1",
      "doi": "https://doi.org/10.48550/arXiv.2207.05920"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The abstract describes online target speaker voice activity detection for diarization using frame-level embeddings and iterative target embedding updates, with no mention of converting audio into a discrete sequence of symbols or tokens; it relies on continuous features (embeddings) rather than discrete tokenization, thus it does not meet the inclusion criterion for discrete audio tokenization. Therefore, it should be excluded.",
      "evaluation": 2
    },
    "round-A_JuniorNano_reasoning": "The abstract describes online target speaker voice activity detection for diarization using frame-level embeddings and iterative target embedding updates, with no mention of converting audio into a discrete sequence of symbols or tokens; it relies on continuous features (embeddings) rather than discrete tokenization, thus it does not meet the inclusion criterion for discrete audio tokenization. Therefore, it should be excluded.",
    "round-A_JuniorNano_evaluation": 2,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on an online speaker diarization system involving continuous frame-level speaker embeddings and does not mention discrete audio/speech tokenization or converting continuous signals into discrete token sequences, which is the core inclusion criterion.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on an online speaker diarization system involving continuous frame-level speaker embeddings and does not mention discrete audio/speech tokenization or converting continuous signals into discrete token sequences, which is the core inclusion criterion.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:2)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:2)"
  },
  {
    "title": "u-HuBERT: Unified Mixed-Modal Speech Pretraining And Zero-Shot Transfer to Unlabeled Modality",
    "abstract": "While audio-visual speech models can yield superior performance and robustness compared to audio-only models, their development and adoption are hindered by the lack of labeled and unlabeled audio-visual data and the cost to deploy one model per modality. In this paper, we present u-HuBERT, a self-supervised pre-training framework that can leverage both multimodal and unimodal speech with a unified masked cluster prediction objective. By utilizing modality dropout during pre-training, we demonstrate that a single fine-tuned model can achieve performance on par or better than the state-of-the-art modality-specific models. Moreover, our model fine-tuned only on audio can perform well with audio-visual and visual speech input, achieving zero-shot modality generalization for multiple speech processing tasks. In particular, our single model yields 1.2%/1.4%/27.2% speech recognition word error rate on LRS3 with audio-visual/audio/visual input. Codes and models are available at https://github.com/facebookresearch/av_hubert",
    "metadata": {
      "arxiv_id": "2207.07036",
      "title": "u-HuBERT: Unified Mixed-Modal Speech Pretraining And Zero-Shot Transfer to Unlabeled Modality",
      "summary": "While audio-visual speech models can yield superior performance and robustness compared to audio-only models, their development and adoption are hindered by the lack of labeled and unlabeled audio-visual data and the cost to deploy one model per modality. In this paper, we present u-HuBERT, a self-supervised pre-training framework that can leverage both multimodal and unimodal speech with a unified masked cluster prediction objective. By utilizing modality dropout during pre-training, we demonstrate that a single fine-tuned model can achieve performance on par or better than the state-of-the-art modality-specific models. Moreover, our model fine-tuned only on audio can perform well with audio-visual and visual speech input, achieving zero-shot modality generalization for multiple speech processing tasks. In particular, our single model yields 1.2%/1.4%/27.2% speech recognition word error rate on LRS3 with audio-visual/audio/visual input. Codes and models are available at https://github.com/facebookresearch/av_hubert",
      "authors": [
        "Wei-Ning Hsu",
        "Bowen Shi"
      ],
      "published": "2022-07-14T16:21:33Z",
      "updated": "2022-11-28T03:12:14Z",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.CV",
        "cs.SD",
        "eess.AS",
        "eess.IV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2207.07036v2",
      "landing_url": "https://arxiv.org/abs/2207.07036v2",
      "doi": "https://doi.org/10.48550/arXiv.2207.07036"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "It employs discrete speech tokens learned via clustering (masked cluster prediction) as the core pretraining target, thereby fulfilling the discrete-tokenization inclusion criterion, and the abstract discusses a speech-focused model with zero-shot modality transfer rather than purely engineering aspects.",
      "evaluation": 5
    },
    "round-A_JuniorNano_reasoning": "It employs discrete speech tokens learned via clustering (masked cluster prediction) as the core pretraining target, thereby fulfilling the discrete-tokenization inclusion criterion, and the abstract discusses a speech-focused model with zero-shot modality transfer rather than purely engineering aspects.",
    "round-A_JuniorNano_evaluation": 5,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper proposes u-HuBERT, a self-supervised speech pretraining framework that uses a unified masked cluster prediction objective, implying discrete token prediction from audio, focusing on speech tasks with zero-shot transfer across modalities. This suggests the study analyzes discrete audio tokenization as a core component rather than just continuous features. Therefore, it meets the inclusion criteria and does not fall under any exclusion criteria.",
      "evaluation": 5
    },
    "round-A_JuniorMini_reasoning": "The paper proposes u-HuBERT, a self-supervised speech pretraining framework that uses a unified masked cluster prediction objective, implying discrete token prediction from audio, focusing on speech tasks with zero-shot transfer across modalities. This suggests the study analyzes discrete audio tokenization as a core component rather than just continuous features. Therefore, it meets the inclusion criteria and does not fall under any exclusion criteria.",
    "round-A_JuniorMini_evaluation": 5,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "include (junior:5)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Federated Self-Supervised Learning in Heterogeneous Settings: Limits of a Baseline Approach on HAR",
    "abstract": "Federated Learning is a new machine learning paradigm dealing with distributed model learning on independent devices. One of the many advantages of federated learning is that training data stay on devices (such as smartphones), and only learned models are shared with a centralized server. In the case of supervised learning, labeling is entrusted to the clients. However, acquiring such labels can be prohibitively expensive and error-prone for many tasks, such as human activity recognition. Hence, a wealth of data remains unlabelled and unexploited. Most existing federated learning approaches that focus mainly on supervised learning have mostly ignored this mass of unlabelled data. Furthermore, it is unclear whether standard federated Learning approaches are suited to self-supervised learning. The few studies that have dealt with the problem have limited themselves to the favorable situation of homogeneous datasets. This work lays the groundwork for a reference evaluation of federated Learning with Semi-Supervised Learning in a realistic setting. We show that standard lightweight autoencoder and standard Federated Averaging fail to learn a robust representation for Human Activity Recognition with several realistic heterogeneous datasets. These findings advocate for a more intensive research effort in Federated Self Supervised Learning to exploit the mass of heterogeneous unlabelled data present on mobile devices.",
    "metadata": {
      "arxiv_id": "2207.08187",
      "title": "Federated Self-Supervised Learning in Heterogeneous Settings: Limits of a Baseline Approach on HAR",
      "summary": "Federated Learning is a new machine learning paradigm dealing with distributed model learning on independent devices. One of the many advantages of federated learning is that training data stay on devices (such as smartphones), and only learned models are shared with a centralized server. In the case of supervised learning, labeling is entrusted to the clients. However, acquiring such labels can be prohibitively expensive and error-prone for many tasks, such as human activity recognition. Hence, a wealth of data remains unlabelled and unexploited. Most existing federated learning approaches that focus mainly on supervised learning have mostly ignored this mass of unlabelled data. Furthermore, it is unclear whether standard federated Learning approaches are suited to self-supervised learning. The few studies that have dealt with the problem have limited themselves to the favorable situation of homogeneous datasets. This work lays the groundwork for a reference evaluation of federated Learning with Semi-Supervised Learning in a realistic setting. We show that standard lightweight autoencoder and standard Federated Averaging fail to learn a robust representation for Human Activity Recognition with several realistic heterogeneous datasets. These findings advocate for a more intensive research effort in Federated Self Supervised Learning to exploit the mass of heterogeneous unlabelled data present on mobile devices.",
      "authors": [
        "Sannara Ek",
        "Romain Rombourg",
        "François Portet",
        "Philippe Lalanda"
      ],
      "published": "2022-07-17T14:15:45Z",
      "updated": "2022-07-17T14:15:45Z",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2207.08187v1",
      "landing_url": "https://arxiv.org/abs/2207.08187v1",
      "doi": "https://doi.org/10.1109/PerComWorkshops53856.2022.9767369"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "Reasoning: The article discusses federated self-supervised learning for human activity recognition with heterogeneous datasets and autoencoders, and it does not propose or analyze discrete audio tokenization or any discrete speech tokens.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "Reasoning: The article discusses federated self-supervised learning for human activity recognition with heterogeneous datasets and autoencoders, and it does not propose or analyze discrete audio tokenization or any discrete speech tokens.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on federated self-supervised learning applied to human activity recognition using heterogeneous datasets, and it does not mention any topic related to discrete audio/speech tokenization, quantization of audio signals, or related neural encoding for discrete token sequences. Hence, it does not meet the inclusion criteria, which require the core study to involve discrete audio/speech tokenization, and it falls outside the exclusion criteria by not addressing audio tokenization at all.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on federated self-supervised learning applied to human activity recognition using heterogeneous datasets, and it does not mention any topic related to discrete audio/speech tokenization, quantization of audio signals, or related neural encoding for discrete token sequences. Hence, it does not meet the inclusion criteria, which require the core study to involve discrete audio/speech tokenization, and it falls outside the exclusion criteria by not addressing audio tokenization at all.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Vector Quantized Image-to-Image Translation",
    "abstract": "Current image-to-image translation methods formulate the task with conditional generation models, leading to learning only the recolorization or regional changes as being constrained by the rich structural information provided by the conditional contexts. In this work, we propose introducing the vector quantization technique into the image-to-image translation framework. The vector quantized content representation can facilitate not only the translation, but also the unconditional distribution shared among different domains. Meanwhile, along with the disentangled style representation, the proposed method further enables the capability of image extension with flexibility in both intra- and inter-domains. Qualitative and quantitative experiments demonstrate that our framework achieves comparable performance to the state-of-the-art image-to-image translation and image extension methods. Compared to methods for individual tasks, the proposed method, as a unified framework, unleashes applications combining image-to-image translation, unconditional generation, and image extension altogether. For example, it provides style variability for image generation and extension, and equips image-to-image translation with further extension capabilities.",
    "metadata": {
      "arxiv_id": "2207.13286",
      "title": "Vector Quantized Image-to-Image Translation",
      "summary": "Current image-to-image translation methods formulate the task with conditional generation models, leading to learning only the recolorization or regional changes as being constrained by the rich structural information provided by the conditional contexts. In this work, we propose introducing the vector quantization technique into the image-to-image translation framework. The vector quantized content representation can facilitate not only the translation, but also the unconditional distribution shared among different domains. Meanwhile, along with the disentangled style representation, the proposed method further enables the capability of image extension with flexibility in both intra- and inter-domains. Qualitative and quantitative experiments demonstrate that our framework achieves comparable performance to the state-of-the-art image-to-image translation and image extension methods. Compared to methods for individual tasks, the proposed method, as a unified framework, unleashes applications combining image-to-image translation, unconditional generation, and image extension altogether. For example, it provides style variability for image generation and extension, and equips image-to-image translation with further extension capabilities.",
      "authors": [
        "Yu-Jie Chen",
        "Shin-I Cheng",
        "Wei-Chen Chiu",
        "Hung-Yu Tseng",
        "Hsin-Ying Lee"
      ],
      "published": "2022-07-27T04:22:29Z",
      "updated": "2022-07-27T04:22:29Z",
      "categories": [
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2207.13286v1",
      "landing_url": "https://arxiv.org/abs/2207.13286v1",
      "doi": "https://doi.org/10.48550/arXiv.2207.13286"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The title/abstract describe vector quantized image-to-image translation in computer vision, with no discrete audio/speech tokenization or analysis of discrete audio tokens; therefore it does not meet the inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The title/abstract describe vector quantized image-to-image translation in computer vision, with no discrete audio/speech tokenization or analysis of discrete audio tokens; therefore it does not meet the inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on image-to-image translation using vector quantization for images, with no mention of audio or speech tokenization, discrete audio representations, or related audio tasks; thus, it does not meet the inclusion criteria nor address any of the exclusion criteria related to audio token content.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on image-to-image translation using vector quantization for images, with no mention of audio or speech tokenization, discrete audio representations, or related audio tasks; thus, it does not meet the inclusion criteria nor address any of the exclusion criteria related to audio token content.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Memory-Guided Collaborative Attention for Nighttime Thermal Infrared Image Colorization",
    "abstract": "Nighttime thermal infrared (NTIR) image colorization, also known as translation of NTIR images into daytime color images (NTIR2DC), is a promising research direction to facilitate nighttime scene perception for humans and intelligent systems under unfavorable conditions (e.g., complete darkness). However, previously developed methods have poor colorization performance for small sample classes. Moreover, reducing the high confidence noise in pseudo-labels and addressing the problem of image gradient disappearance during translation are still under-explored, and keeping edges from being distorted during translation is also challenging. To address the aforementioned issues, we propose a novel learning framework called Memory-guided cOllaboRative atteNtion Generative Adversarial Network (MornGAN), which is inspired by the analogical reasoning mechanisms of humans. Specifically, a memory-guided sample selection strategy and adaptive collaborative attention loss are devised to enhance the semantic preservation of small sample categories. In addition, we propose an online semantic distillation module to mine and refine the pseudo-labels of NTIR images. Further, conditional gradient repair loss is introduced for reducing edge distortion during translation. Extensive experiments on the NTIR2DC task show that the proposed MornGAN significantly outperforms other image-to-image translation methods in terms of semantic preservation and edge consistency, which helps improve the object detection accuracy remarkably.",
    "metadata": {
      "arxiv_id": "2208.02960",
      "title": "Memory-Guided Collaborative Attention for Nighttime Thermal Infrared Image Colorization",
      "summary": "Nighttime thermal infrared (NTIR) image colorization, also known as translation of NTIR images into daytime color images (NTIR2DC), is a promising research direction to facilitate nighttime scene perception for humans and intelligent systems under unfavorable conditions (e.g., complete darkness). However, previously developed methods have poor colorization performance for small sample classes. Moreover, reducing the high confidence noise in pseudo-labels and addressing the problem of image gradient disappearance during translation are still under-explored, and keeping edges from being distorted during translation is also challenging. To address the aforementioned issues, we propose a novel learning framework called Memory-guided cOllaboRative atteNtion Generative Adversarial Network (MornGAN), which is inspired by the analogical reasoning mechanisms of humans. Specifically, a memory-guided sample selection strategy and adaptive collaborative attention loss are devised to enhance the semantic preservation of small sample categories. In addition, we propose an online semantic distillation module to mine and refine the pseudo-labels of NTIR images. Further, conditional gradient repair loss is introduced for reducing edge distortion during translation. Extensive experiments on the NTIR2DC task show that the proposed MornGAN significantly outperforms other image-to-image translation methods in terms of semantic preservation and edge consistency, which helps improve the object detection accuracy remarkably.",
      "authors": [
        "Fu-Ya Luo",
        "Yi-Jun Cao",
        "Kai-Fu Yang",
        "Yong-Jie Li"
      ],
      "published": "2022-08-05T03:04:04Z",
      "updated": "2022-08-05T03:04:04Z",
      "categories": [
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2208.02960v1",
      "landing_url": "https://arxiv.org/abs/2208.02960v1",
      "doi": "https://doi.org/10.48550/arXiv.2208.02960"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The article concerns NTIR image colorization with GANs and does not involve discrete audio/speech tokenization, hence fails the audio-tokenization criterion.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The article concerns NTIR image colorization with GANs and does not involve discrete audio/speech tokenization, hence fails the audio-tokenization criterion.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on nighttime thermal infrared image colorization and does not address discrete audio/speech tokenization or the conversion of continuous audio signals into discrete tokens. Therefore, it does not meet the inclusion criteria related to discrete audio tokenization and falls outside the scope of the review task.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on nighttime thermal infrared image colorization and does not address discrete audio/speech tokenization or the conversion of continuous audio signals into discrete tokens. Therefore, it does not meet the inclusion criteria related to discrete audio tokenization and falls outside the scope of the review task.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Non-Contrastive Self-supervised Learning for Utterance-Level Information Extraction from Speech",
    "abstract": "In recent studies, self-supervised pre-trained models tend to outperform supervised pre-trained models in transfer learning. In particular, self-supervised learning (SSL) of utterance-level speech representation can be used in speech applications that require discriminative representation of consistent attributes within an utterance: speaker, language, emotion, and age. Existing frame-level self-supervised speech representation, e.g., wav2vec, can be used as utterance-level representation with pooling, but the models are usually large. There are also SSL techniques to learn utterance-level representation. One of the most successful is a contrastive method, which requires negative sampling: selecting alternative samples to contrast with the current sample (anchor). However, this does not ensure that all the negative samples belong to classes different from the anchor class without labels. This paper applies a non-contrastive self-supervised method to learn utterance-level embeddings. We adapted DIstillation with NO labels (DINO) from computer vision to speech. Unlike contrastive methods, DINO does not require negative sampling. We compared DINO to x-vector trained in a supervised manner. When transferred to down-stream tasks (speaker verification, speech emotion recognition (SER), and Alzheimer's disease detection), DINO outperformed x-vector. We studied the influence of several aspects during transfer learning such as dividing the fine-tuning process into steps, chunk lengths, or augmentation. During fine-tuning, tuning the last affine layers first and then the whole network surpassed fine-tuning all at once. Using shorter chunk lengths, although they generate more diverse inputs, did not necessarily improve performance, implying speech segments at least with a specific length are required for better performance per application. Augmentation was helpful in SER.",
    "metadata": {
      "arxiv_id": "2208.05445",
      "title": "Non-Contrastive Self-supervised Learning for Utterance-Level Information Extraction from Speech",
      "summary": "In recent studies, self-supervised pre-trained models tend to outperform supervised pre-trained models in transfer learning. In particular, self-supervised learning (SSL) of utterance-level speech representation can be used in speech applications that require discriminative representation of consistent attributes within an utterance: speaker, language, emotion, and age. Existing frame-level self-supervised speech representation, e.g., wav2vec, can be used as utterance-level representation with pooling, but the models are usually large. There are also SSL techniques to learn utterance-level representation. One of the most successful is a contrastive method, which requires negative sampling: selecting alternative samples to contrast with the current sample (anchor). However, this does not ensure that all the negative samples belong to classes different from the anchor class without labels. This paper applies a non-contrastive self-supervised method to learn utterance-level embeddings. We adapted DIstillation with NO labels (DINO) from computer vision to speech. Unlike contrastive methods, DINO does not require negative sampling. We compared DINO to x-vector trained in a supervised manner. When transferred to down-stream tasks (speaker verification, speech emotion recognition (SER), and Alzheimer's disease detection), DINO outperformed x-vector. We studied the influence of several aspects during transfer learning such as dividing the fine-tuning process into steps, chunk lengths, or augmentation. During fine-tuning, tuning the last affine layers first and then the whole network surpassed fine-tuning all at once. Using shorter chunk lengths, although they generate more diverse inputs, did not necessarily improve performance, implying speech segments at least with a specific length are required for better performance per application. Augmentation was helpful in SER.",
      "authors": [
        "Jaejin Cho",
        "Jes'us Villalba",
        "Laureano Moro-Velazquez",
        "Najim Dehak"
      ],
      "published": "2022-08-10T16:56:39Z",
      "updated": "2022-08-10T16:56:39Z",
      "categories": [
        "eess.AS",
        "cs.AI",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2208.05445v1",
      "landing_url": "https://arxiv.org/abs/2208.05445v1",
      "doi": "https://doi.org/10.1109/JSTSP.2022.3197315"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The abstract focuses on non-contrastive self-supervised learning for utterance-level speech representations (DINO) and processing through pooling, with no discrete audio tokenization or quantization to a finite symbol vocabulary, thus failing the inclusion criterion which requires explicit discrete tokenization of audio.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The abstract focuses on non-contrastive self-supervised learning for utterance-level speech representations (DINO) and processing through pooling, with no discrete audio tokenization or quantization to a finite symbol vocabulary, thus failing the inclusion criterion which requires explicit discrete tokenization of audio.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses primarily on non-contrastive self-supervised learning methods for utterance-level speech representations and transfer learning for speech-related tasks. However, it does not explicitly address or analyze discrete audio/speech tokenization—specifically the conversion of continuous audio signals into discrete token sequences with a finite vocabulary. Instead, it deals with continuous embeddings and representation learning without discrete quantization or token design as the core object of study. It uses continuous representations like x-vectors and does not propose a discrete tokenization scheme relevant to language-model-style sequences. Therefore, it does not fulfill the inclusion criteria and matches the exclusion criteria that studies using only continuous audio features without discrete quantization or tokenization should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses primarily on non-contrastive self-supervised learning methods for utterance-level speech representations and transfer learning for speech-related tasks. However, it does not explicitly address or analyze discrete audio/speech tokenization—specifically the conversion of continuous audio signals into discrete token sequences with a finite vocabulary. Instead, it deals with continuous embeddings and representation learning without discrete quantization or token design as the core object of study. It uses continuous representations like x-vectors and does not propose a discrete tokenization scheme relevant to language-model-style sequences. Therefore, it does not fulfill the inclusion criteria and matches the exclusion criteria that studies using only continuous audio features without discrete quantization or tokenization should be excluded.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "A Secure and Efficient Data Deduplication Scheme with Dynamic Ownership Management in Cloud Computing",
    "abstract": "Encrypted data deduplication is an important technique for saving storage space and network bandwidth, which has been widely used in cloud storage. Recently, a number of schemes that solve the problem of data deduplication with dynamic ownership management have been proposed. However, these schemes suffer from low efficiency when the dynamic ownership changes a lot. To this end, in this paper, we propose a novel server-side deduplication scheme for encrypted data in a hybrid cloud architecture, where a public cloud (Pub-CSP) manages the storage and a private cloud (Pri-CSP) plays a role as the data owner to perform deduplication and dynamic ownership management. Further, to reduce the communication overhead we use an initial uploader check mechanism to ensure only the first uploader needs to perform encryption, and adopt an access control technique that verifies the validity of the data users before they download data. Our security analysis and performance evaluation demonstrate that our proposed server-side deduplication scheme has better performance in terms of security, effectiveness, and practicability compared with previous schemes. Meanwhile, our method can efficiently resist collusion attacks and duplicate faking attacks.",
    "metadata": {
      "arxiv_id": "2208.09030",
      "title": "A Secure and Efficient Data Deduplication Scheme with Dynamic Ownership Management in Cloud Computing",
      "summary": "Encrypted data deduplication is an important technique for saving storage space and network bandwidth, which has been widely used in cloud storage. Recently, a number of schemes that solve the problem of data deduplication with dynamic ownership management have been proposed. However, these schemes suffer from low efficiency when the dynamic ownership changes a lot. To this end, in this paper, we propose a novel server-side deduplication scheme for encrypted data in a hybrid cloud architecture, where a public cloud (Pub-CSP) manages the storage and a private cloud (Pri-CSP) plays a role as the data owner to perform deduplication and dynamic ownership management. Further, to reduce the communication overhead we use an initial uploader check mechanism to ensure only the first uploader needs to perform encryption, and adopt an access control technique that verifies the validity of the data users before they download data. Our security analysis and performance evaluation demonstrate that our proposed server-side deduplication scheme has better performance in terms of security, effectiveness, and practicability compared with previous schemes. Meanwhile, our method can efficiently resist collusion attacks and duplicate faking attacks.",
      "authors": [
        "Xuewei Ma",
        "Wenyuan Yang",
        "Yuesheng Zhu",
        "Zhiqiang Bai"
      ],
      "published": "2022-08-18T19:02:30Z",
      "updated": "2022-08-31T15:47:52Z",
      "categories": [
        "cs.CR",
        "cs.DC"
      ],
      "pdf_url": "https://arxiv.org/pdf/2208.09030v3",
      "landing_url": "https://arxiv.org/abs/2208.09030v3",
      "doi": "https://doi.org/10.48550/arXiv.2208.09030"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This item does not involve discrete audio/speech tokenization; it focuses on encrypted data deduplication in cloud computing, which does not meet the audio tokenization criteria, so it should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "This item does not involve discrete audio/speech tokenization; it focuses on encrypted data deduplication in cloud computing, which does not meet the audio tokenization criteria, so it should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on data deduplication schemes in cloud computing related to encrypted data management and dynamic ownership, without any mention of discrete audio/speech tokenization or audio signal processing; thus, it does not meet the inclusion criteria related to discrete audio tokenization.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on data deduplication schemes in cloud computing related to encrypted data management and dynamic ownership, without any mention of discrete audio/speech tokenization or audio signal processing; thus, it does not meet the inclusion criteria related to discrete audio tokenization.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Robot Active Neural Sensing and Planning in Unknown Cluttered Environments",
    "abstract": "Active sensing and planning in unknown, cluttered environments is an open challenge for robots intending to provide home service, search and rescue, narrow-passage inspection, and medical assistance. Although many active sensing methods exist, they often consider open spaces, assume known settings, or mostly do not generalize to real-world scenarios. We present the active neural sensing approach that generates the kinematically feasible viewpoint sequences for the robot manipulator with an in-hand camera to gather the minimum number of observations needed to reconstruct the underlying environment. Our framework actively collects the visual RGBD observations, aggregates them into scene representation, and performs object shape inference to avoid unnecessary robot interactions with the environment. We train our approach on synthetic data with domain randomization and demonstrate its successful execution via sim-to-real transfer in reconstructing narrow, covered, real-world cabinet environments cluttered with unknown objects. The natural cabinet scenarios impose significant challenges for robot motion and scene reconstruction due to surrounding obstacles and low ambient lighting conditions. However, despite unfavorable settings, our method exhibits high performance compared to its baselines in terms of various environment reconstruction metrics, including planning speed, the number of viewpoints, and overall scene coverage.",
    "metadata": {
      "arxiv_id": "2208.11079",
      "title": "Robot Active Neural Sensing and Planning in Unknown Cluttered Environments",
      "summary": "Active sensing and planning in unknown, cluttered environments is an open challenge for robots intending to provide home service, search and rescue, narrow-passage inspection, and medical assistance. Although many active sensing methods exist, they often consider open spaces, assume known settings, or mostly do not generalize to real-world scenarios. We present the active neural sensing approach that generates the kinematically feasible viewpoint sequences for the robot manipulator with an in-hand camera to gather the minimum number of observations needed to reconstruct the underlying environment. Our framework actively collects the visual RGBD observations, aggregates them into scene representation, and performs object shape inference to avoid unnecessary robot interactions with the environment. We train our approach on synthetic data with domain randomization and demonstrate its successful execution via sim-to-real transfer in reconstructing narrow, covered, real-world cabinet environments cluttered with unknown objects. The natural cabinet scenarios impose significant challenges for robot motion and scene reconstruction due to surrounding obstacles and low ambient lighting conditions. However, despite unfavorable settings, our method exhibits high performance compared to its baselines in terms of various environment reconstruction metrics, including planning speed, the number of viewpoints, and overall scene coverage.",
      "authors": [
        "Hanwen Ren",
        "Ahmed H. Qureshi"
      ],
      "published": "2022-08-23T16:56:54Z",
      "updated": "2022-08-24T00:52:09Z",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.CV",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2208.11079v2",
      "landing_url": "https://arxiv.org/abs/2208.11079v2",
      "doi": "https://doi.org/10.48550/arXiv.2208.11079"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The abstract concerns robotic active sensing and scene reconstruction with RGBD data, and does not address discrete audio/speech tokenization, so it fails the inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The abstract concerns robotic active sensing and scene reconstruction with RGBD data, and does not address discrete audio/speech tokenization, so it fails the inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on robotic active sensing and planning in cluttered environments without any mention of discrete audio or speech tokenization, neural quantization of audio signals, or audio-related token analysis, which are central to the inclusion criteria. Therefore, it does not meet the inclusion criteria related to discrete audio/speech tokenization, and there is no indication of audio-related content in the abstract.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on robotic active sensing and planning in cluttered environments without any mention of discrete audio or speech tokenization, neural quantization of audio signals, or audio-related token analysis, which are central to the inclusion criteria. Therefore, it does not meet the inclusion criteria related to discrete audio/speech tokenization, and there is no indication of audio-related content in the abstract.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Autoencoder Based Iterative Modeling and Multivariate Time-Series Subsequence Clustering Algorithm",
    "abstract": "This paper introduces an algorithm for the detection of change-points and the identification of the corresponding subsequences in transient multivariate time-series data (MTSD). The analysis of such data has become more and more important due to the increase of availability in many industrial fields. Labeling, sorting or filtering highly transient measurement data for training condition based maintenance (CbM) models is cumbersome and error-prone. For some applications it can be sufficient to filter measurements by simple thresholds or finding change-points based on changes in mean value and variation. But a robust diagnosis of a component within a component group for example, which has a complex non-linear correlation between multiple sensor values, a simple approach would not be feasible. No meaningful and coherent measurement data which could be used for training a CbM model would emerge. Therefore, we introduce an algorithm which uses a recurrent neural network (RNN) based Autoencoder (AE) which is iteratively trained on incoming data. The scoring function uses the reconstruction error and latent space information. A model of the identified subsequence is saved and used for recognition of repeating subsequences as well as fast offline clustering. For evaluation, we propose a new similarity measure based on the curvature for a more intuitive time-series subsequence clustering metric. A comparison with seven other state-of-the-art algorithms and eight datasets shows the capability and the increased performance of our algorithm to cluster MTSD online and offline in conjunction with mechatronic systems.",
    "metadata": {
      "arxiv_id": "2209.04213",
      "title": "Autoencoder Based Iterative Modeling and Multivariate Time-Series Subsequence Clustering Algorithm",
      "summary": "This paper introduces an algorithm for the detection of change-points and the identification of the corresponding subsequences in transient multivariate time-series data (MTSD). The analysis of such data has become more and more important due to the increase of availability in many industrial fields. Labeling, sorting or filtering highly transient measurement data for training condition based maintenance (CbM) models is cumbersome and error-prone. For some applications it can be sufficient to filter measurements by simple thresholds or finding change-points based on changes in mean value and variation. But a robust diagnosis of a component within a component group for example, which has a complex non-linear correlation between multiple sensor values, a simple approach would not be feasible. No meaningful and coherent measurement data which could be used for training a CbM model would emerge. Therefore, we introduce an algorithm which uses a recurrent neural network (RNN) based Autoencoder (AE) which is iteratively trained on incoming data. The scoring function uses the reconstruction error and latent space information. A model of the identified subsequence is saved and used for recognition of repeating subsequences as well as fast offline clustering. For evaluation, we propose a new similarity measure based on the curvature for a more intuitive time-series subsequence clustering metric. A comparison with seven other state-of-the-art algorithms and eight datasets shows the capability and the increased performance of our algorithm to cluster MTSD online and offline in conjunction with mechatronic systems.",
      "authors": [
        "Jonas Köhne",
        "Lars Henning",
        "Clemens Gühmann"
      ],
      "published": "2022-09-09T09:59:56Z",
      "updated": "2022-09-23T11:07:27Z",
      "categories": [
        "eess.SP",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2209.04213v2",
      "landing_url": "https://arxiv.org/abs/2209.04213v2",
      "doi": "https://doi.org/10.1109/ACCESS.2023.3247564"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The abstract describes change-point detection in multivariate time-series using an RNN autoencoder and clustering, with no focus on discrete audio tokenization or discrete token representations for speech; thus it does not meet the audio tokenization inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The abstract describes change-point detection in multivariate time-series using an RNN autoencoder and clustering, with no focus on discrete audio tokenization or discrete token representations for speech; thus it does not meet the audio tokenization inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper focuses on an algorithm for multivariate time-series subsequence clustering and change-point detection using autoencoder-based recurrent neural networks, without any mention or analysis of discrete audio/speech tokenization or discrete symbol sequences from audio signals; thus, it does not meet the inclusion criteria related to discrete audio tokenization and is outside the scope of the study's requirements.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper focuses on an algorithm for multivariate time-series subsequence clustering and change-point detection using autoencoder-based recurrent neural networks, without any mention or analysis of discrete audio/speech tokenization or discrete symbol sequences from audio signals; thus, it does not meet the inclusion criteria related to discrete audio tokenization and is outside the scope of the study's requirements.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Critical Gaussian Multiplicative Chaos revisited",
    "abstract": "We present new, short and self-contained proofs of the convergence (with an adequate renormalization) of four different sequences to the critical Gaussian Multiplicative Chaos:(a) the derivative martingale (b) the critical martingale (c) the exponential of the mollified field (d) the subcritical Gaussian Multiplicative Chaos.",
    "metadata": {
      "arxiv_id": "2209.06683",
      "title": "Critical Gaussian Multiplicative Chaos revisited",
      "summary": "We present new, short and self-contained proofs of the convergence (with an adequate renormalization) of four different sequences to the critical Gaussian Multiplicative Chaos:(a) the derivative martingale (b) the critical martingale (c) the exponential of the mollified field (d) the subcritical Gaussian Multiplicative Chaos.",
      "authors": [
        "Hubert Lacoin"
      ],
      "published": "2022-09-14T14:43:14Z",
      "updated": "2022-09-14T14:43:14Z",
      "categories": [
        "math.PR",
        "math-ph"
      ],
      "pdf_url": "https://arxiv.org/pdf/2209.06683v1",
      "landing_url": "https://arxiv.org/abs/2209.06683v1",
      "doi": "https://doi.org/10.48550/arXiv.2209.06683"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item is a mathematical paper on Gaussian Multiplicative Chaos with no discrete audio tokenization or speech-related analysis, so it does not meet the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item is a mathematical paper on Gaussian Multiplicative Chaos with no discrete audio tokenization or speech-related analysis, so it does not meet the inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The title and abstract focus on theoretical aspects of Gaussian Multiplicative Chaos and do not mention any study or analysis of discrete audio/speech tokenization, or any audio signal processing related to token sequences; thus, the study does not meet the inclusion criteria regarding discrete audio tokenization.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The title and abstract focus on theoretical aspects of Gaussian Multiplicative Chaos and do not mention any study or analysis of discrete audio/speech tokenization, or any audio signal processing related to token sequences; thus, the study does not meet the inclusion criteria regarding discrete audio tokenization.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Self-Supervised Learning with an Information Maximization Criterion",
    "abstract": "Self-supervised learning allows AI systems to learn effective representations from large amounts of data using tasks that do not require costly labeling. Mode collapse, i.e., the model producing identical representations for all inputs, is a central problem to many self-supervised learning approaches, making self-supervised tasks, such as matching distorted variants of the inputs, ineffective. In this article, we argue that a straightforward application of information maximization among alternative latent representations of the same input naturally solves the collapse problem and achieves competitive empirical results. We propose a self-supervised learning method, CorInfoMax, that uses a second-order statistics-based mutual information measure that reflects the level of correlation among its arguments. Maximizing this correlative information measure between alternative representations of the same input serves two purposes: (1) it avoids the collapse problem by generating feature vectors with non-degenerate covariances; (2) it establishes relevance among alternative representations by increasing the linear dependence among them. An approximation of the proposed information maximization objective simplifies to a Euclidean distance-based objective function regularized by the log-determinant of the feature covariance matrix. The regularization term acts as a natural barrier against feature space degeneracy. Consequently, beyond avoiding complete output collapse to a single point, the proposed approach also prevents dimensional collapse by encouraging the spread of information across the whole feature space. Numerical experiments demonstrate that CorInfoMax achieves better or competitive performance results relative to the state-of-the-art SSL approaches.",
    "metadata": {
      "arxiv_id": "2209.07999",
      "title": "Self-Supervised Learning with an Information Maximization Criterion",
      "summary": "Self-supervised learning allows AI systems to learn effective representations from large amounts of data using tasks that do not require costly labeling. Mode collapse, i.e., the model producing identical representations for all inputs, is a central problem to many self-supervised learning approaches, making self-supervised tasks, such as matching distorted variants of the inputs, ineffective. In this article, we argue that a straightforward application of information maximization among alternative latent representations of the same input naturally solves the collapse problem and achieves competitive empirical results. We propose a self-supervised learning method, CorInfoMax, that uses a second-order statistics-based mutual information measure that reflects the level of correlation among its arguments. Maximizing this correlative information measure between alternative representations of the same input serves two purposes: (1) it avoids the collapse problem by generating feature vectors with non-degenerate covariances; (2) it establishes relevance among alternative representations by increasing the linear dependence among them. An approximation of the proposed information maximization objective simplifies to a Euclidean distance-based objective function regularized by the log-determinant of the feature covariance matrix. The regularization term acts as a natural barrier against feature space degeneracy. Consequently, beyond avoiding complete output collapse to a single point, the proposed approach also prevents dimensional collapse by encouraging the spread of information across the whole feature space. Numerical experiments demonstrate that CorInfoMax achieves better or competitive performance results relative to the state-of-the-art SSL approaches.",
      "authors": [
        "Serdar Ozsoy",
        "Shadi Hamdan",
        "Sercan Ö. Arik",
        "Deniz Yuret",
        "Alper T. Erdogan"
      ],
      "published": "2022-09-16T15:26:19Z",
      "updated": "2022-09-16T15:26:19Z",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CV",
        "cs.IT",
        "eess.IV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2209.07999v1",
      "landing_url": "https://arxiv.org/abs/2209.07999v1",
      "doi": "https://doi.org/10.48550/arXiv.2209.07999"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The abstract discusses a self-supervised learning method focused on information maximization for representations and covariance-based regularization; it does not propose or analyze discrete audio/speech tokenization or discrete token design, so it does not meet the inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The abstract discusses a self-supervised learning method focused on information maximization for representations and covariance-based regularization; it does not propose or analyze discrete audio/speech tokenization or discrete token design, so it does not meet the inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study proposes a self-supervised learning method focused on information maximization to avoid mode collapse and improve representation learning; however, it does not mention or focus on discrete audio/speech tokenization, quantization of continuous audio signals into discrete tokens, or analyze discrete token representations relevant to audio or speech tasks. Therefore, it does not meet the inclusion criteria and falls outside the scope of discrete audio tokenization research.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study proposes a self-supervised learning method focused on information maximization to avoid mode collapse and improve representation learning; however, it does not mention or focus on discrete audio/speech tokenization, quantization of continuous audio signals into discrete tokens, or analyze discrete token representations relevant to audio or speech tasks. Therefore, it does not meet the inclusion criteria and falls outside the scope of discrete audio tokenization research.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Self-supervised learning of hologram reconstruction using physics consistency",
    "abstract": "The past decade has witnessed transformative applications of deep learning in various computational imaging, sensing and microscopy tasks. Due to the supervised learning schemes employed, these methods mostly depend on large-scale, diverse, and labeled training data. The acquisition and preparation of such training image datasets are often laborious and costly, also leading to biased estimation and limited generalization to new sample types. Here, we report a self-supervised learning model, termed GedankenNet, that eliminates the need for labeled or experimental training data, and demonstrate its effectiveness and superior generalization on hologram reconstruction tasks. Without prior knowledge about the sample types to be imaged, the self-supervised learning model was trained using a physics-consistency loss and artificial random images that are synthetically generated without any experiments or resemblance to real-world samples. After its self-supervised training, GedankenNet successfully generalized to experimental holograms of various unseen biological samples, reconstructing the phase and amplitude images of different types of objects using experimentally acquired test holograms. Without access to experimental data or knowledge of real samples of interest or their spatial features, GedankenNet's self-supervised learning achieved complex-valued image reconstructions that are consistent with the Maxwell's equations, and its output inference and object solutions accurately represent the wave propagation in free-space. GedankenNet framework also exhibits resilience to random, unknown perturbations in the physical forward model, including changes in the hologram distances, pixel size and illumination wavelength. This self-supervised learning of image reconstruction tasks creates new opportunities for various inverse problems in holography, microscopy and computational imaging fields.",
    "metadata": {
      "arxiv_id": "2209.08288",
      "title": "Self-supervised learning of hologram reconstruction using physics consistency",
      "summary": "The past decade has witnessed transformative applications of deep learning in various computational imaging, sensing and microscopy tasks. Due to the supervised learning schemes employed, these methods mostly depend on large-scale, diverse, and labeled training data. The acquisition and preparation of such training image datasets are often laborious and costly, also leading to biased estimation and limited generalization to new sample types. Here, we report a self-supervised learning model, termed GedankenNet, that eliminates the need for labeled or experimental training data, and demonstrate its effectiveness and superior generalization on hologram reconstruction tasks. Without prior knowledge about the sample types to be imaged, the self-supervised learning model was trained using a physics-consistency loss and artificial random images that are synthetically generated without any experiments or resemblance to real-world samples. After its self-supervised training, GedankenNet successfully generalized to experimental holograms of various unseen biological samples, reconstructing the phase and amplitude images of different types of objects using experimentally acquired test holograms. Without access to experimental data or knowledge of real samples of interest or their spatial features, GedankenNet's self-supervised learning achieved complex-valued image reconstructions that are consistent with the Maxwell's equations, and its output inference and object solutions accurately represent the wave propagation in free-space. GedankenNet framework also exhibits resilience to random, unknown perturbations in the physical forward model, including changes in the hologram distances, pixel size and illumination wavelength. This self-supervised learning of image reconstruction tasks creates new opportunities for various inverse problems in holography, microscopy and computational imaging fields.",
      "authors": [
        "Luzhe Huang",
        "Hanlong Chen",
        "Tairan Liu",
        "Aydogan Ozcan"
      ],
      "published": "2022-09-17T09:02:10Z",
      "updated": "2023-06-17T08:53:11Z",
      "categories": [
        "cs.CV",
        "cs.LG",
        "eess.IV",
        "physics.optics"
      ],
      "pdf_url": "https://arxiv.org/pdf/2209.08288v2",
      "landing_url": "https://arxiv.org/abs/2209.08288v2",
      "doi": "https://doi.org/10.1038/s42256-023-00704-7"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "Its focus is on self-supervised hologram reconstruction with physics-consistency loss and does not involve discrete audio/speech tokenization, so it fails the inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "Its focus is on self-supervised hologram reconstruction with physics-consistency loss and does not involve discrete audio/speech tokenization, so it fails the inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on self-supervised learning for hologram reconstruction using physics consistency and does not involve discrete audio or speech tokenization, nor does it address any audio-related tasks; therefore, it does not meet the inclusion criteria related to audio tokenization.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on self-supervised learning for hologram reconstruction using physics consistency and does not involve discrete audio or speech tokenization, nor does it address any audio-related tasks; therefore, it does not meet the inclusion criteria related to audio tokenization.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Lightweight Image Codec via Multi-Grid Multi-Block-Size Vector Quantization (MGBVQ)",
    "abstract": "A multi-grid multi-block-size vector quantization (MGBVQ) method is proposed for image coding in this work. The fundamental idea of image coding is to remove correlations among pixels before quantization and entropy coding, e.g., the discrete cosine transform (DCT) and intra predictions, adopted by modern image coding standards. We present a new method to remove pixel correlations. First, by decomposing correlations into long- and short-range correlations, we represent long-range correlations in coarser grids due to their smoothness, thus leading to a multi-grid (MG) coding architecture. Second, we show that short-range correlations can be effectively coded by a suite of vector quantizers (VQs). Along this line, we argue the effectiveness of VQs of very large block sizes and present a convenient way to implement them. It is shown by experimental results that MGBVQ offers excellent rate-distortion (RD) performance, which is comparable with existing image coders, at much lower complexity. Besides, it provides a progressive coded bitstream.",
    "metadata": {
      "arxiv_id": "2209.12139",
      "title": "Lightweight Image Codec via Multi-Grid Multi-Block-Size Vector Quantization (MGBVQ)",
      "summary": "A multi-grid multi-block-size vector quantization (MGBVQ) method is proposed for image coding in this work. The fundamental idea of image coding is to remove correlations among pixels before quantization and entropy coding, e.g., the discrete cosine transform (DCT) and intra predictions, adopted by modern image coding standards. We present a new method to remove pixel correlations. First, by decomposing correlations into long- and short-range correlations, we represent long-range correlations in coarser grids due to their smoothness, thus leading to a multi-grid (MG) coding architecture. Second, we show that short-range correlations can be effectively coded by a suite of vector quantizers (VQs). Along this line, we argue the effectiveness of VQs of very large block sizes and present a convenient way to implement them. It is shown by experimental results that MGBVQ offers excellent rate-distortion (RD) performance, which is comparable with existing image coders, at much lower complexity. Besides, it provides a progressive coded bitstream.",
      "authors": [
        "Yifan Wang",
        "Zhanxuan Mei",
        "Ioannis Katsavounidis",
        "C. -C. Jay Kuo"
      ],
      "published": "2022-09-25T04:14:26Z",
      "updated": "2022-09-25T04:14:26Z",
      "categories": [
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2209.12139v1",
      "landing_url": "https://arxiv.org/abs/2209.12139v1",
      "doi": "https://doi.org/10.48550/arXiv.2209.12139"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The abstract describes an image codec using multi-grid multi-block-size vector quantization, with no focus on discrete audio/speech tokenization or evaluation of discrete audio tokens; thus it does not meet the inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The abstract describes an image codec using multi-grid multi-block-size vector quantization, with no focus on discrete audio/speech tokenization or evaluation of discrete audio tokens; thus it does not meet the inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper proposes a method for image coding via vector quantization without any mention or focus on audio or speech tokenization, discrete audio tokens, or related audio signal processing tasks, which does not satisfy the inclusion criteria focusing on discrete audio/speech tokenization and explicitly excludes image-only tokenization studies.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper proposes a method for image coding via vector quantization without any mention or focus on audio or speech tokenization, discrete audio tokens, or related audio signal processing tasks, which does not satisfy the inclusion criteria focusing on discrete audio/speech tokenization and explicitly excludes image-only tokenization studies.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Joint Embedding Self-Supervised Learning in the Kernel Regime",
    "abstract": "The fundamental goal of self-supervised learning (SSL) is to produce useful representations of data without access to any labels for classifying the data. Modern methods in SSL, which form representations based on known or constructed relationships between samples, have been particularly effective at this task. Here, we aim to extend this framework to incorporate algorithms based on kernel methods where embeddings are constructed by linear maps acting on the feature space of a kernel. In this kernel regime, we derive methods to find the optimal form of the output representations for contrastive and non-contrastive loss functions. This procedure produces a new representation space with an inner product denoted as the induced kernel which generally correlates points which are related by an augmentation in kernel space and de-correlates points otherwise. We analyze our kernel model on small datasets to identify common features of self-supervised learning algorithms and gain theoretical insights into their performance on downstream tasks.",
    "metadata": {
      "arxiv_id": "2209.14884",
      "title": "Joint Embedding Self-Supervised Learning in the Kernel Regime",
      "summary": "The fundamental goal of self-supervised learning (SSL) is to produce useful representations of data without access to any labels for classifying the data. Modern methods in SSL, which form representations based on known or constructed relationships between samples, have been particularly effective at this task. Here, we aim to extend this framework to incorporate algorithms based on kernel methods where embeddings are constructed by linear maps acting on the feature space of a kernel. In this kernel regime, we derive methods to find the optimal form of the output representations for contrastive and non-contrastive loss functions. This procedure produces a new representation space with an inner product denoted as the induced kernel which generally correlates points which are related by an augmentation in kernel space and de-correlates points otherwise. We analyze our kernel model on small datasets to identify common features of self-supervised learning algorithms and gain theoretical insights into their performance on downstream tasks.",
      "authors": [
        "Bobak T. Kiani",
        "Randall Balestriero",
        "Yubei Chen",
        "Seth Lloyd",
        "Yann LeCun"
      ],
      "published": "2022-09-29T15:53:19Z",
      "updated": "2022-09-29T15:53:19Z",
      "categories": [
        "cs.LG",
        "cs.AI",
        "stat.ML"
      ],
      "pdf_url": "https://arxiv.org/pdf/2209.14884v1",
      "landing_url": "https://arxiv.org/abs/2209.14884v1",
      "doi": "https://doi.org/10.48550/arXiv.2209.14884"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The abstract focuses on kernel-based self-supervised learning and does not address discrete audio/speech tokenization or any discrete token design for audio; therefore it does not meet the inclusion criteria and should be excluded according to the stated exclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The abstract focuses on kernel-based self-supervised learning and does not address discrete audio/speech tokenization or any discrete token design for audio; therefore it does not meet the inclusion criteria and should be excluded according to the stated exclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on self-supervised learning in the kernel regime with theoretical insights on representations but does not mention discrete audio/speech tokenization, nor does it analyze or propose quantized or discrete token representations relevant to audio or speech; therefore, it does not meet the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on self-supervised learning in the kernel regime with theoretical insights on representations but does not mention discrete audio/speech tokenization, nor does it analyze or propose quantized or discrete token representations relevant to audio or speech; therefore, it does not meet the inclusion criteria.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Binaural Speech Enhancement Using STOI-Optimal Masks",
    "abstract": "STOI-optimal masking has been previously proposed and developed for single-channel speech enhancement. In this paper, we consider the extension to the task of binaural speech enhancement in which spatial information is known to be important to speech understanding and therefore should be preserved by the enhancement processing. Masks are estimated for each of the binaural channels individually and a `better-ear listening' mask is computed by choosing the maximum of the two masks. The estimated mask is used to supply probability information about the speech presence in each time-frequency bin to an Optimally-modified Log Spectral Amplitude (OM-LSA) enhancer. We show that using the proposed method for binaural signals with a directional noise not only improves the SNR of the noisy signal but also preserves the binaural cues and intelligibility.",
    "metadata": {
      "arxiv_id": "2209.15472",
      "title": "Binaural Speech Enhancement Using STOI-Optimal Masks",
      "summary": "STOI-optimal masking has been previously proposed and developed for single-channel speech enhancement. In this paper, we consider the extension to the task of binaural speech enhancement in which spatial information is known to be important to speech understanding and therefore should be preserved by the enhancement processing. Masks are estimated for each of the binaural channels individually and a `better-ear listening' mask is computed by choosing the maximum of the two masks. The estimated mask is used to supply probability information about the speech presence in each time-frequency bin to an Optimally-modified Log Spectral Amplitude (OM-LSA) enhancer. We show that using the proposed method for binaural signals with a directional noise not only improves the SNR of the noisy signal but also preserves the binaural cues and intelligibility.",
      "authors": [
        "Vikas Tokala",
        "Mike Brookes",
        "Patrick A. Naylor"
      ],
      "published": "2022-09-30T13:56:25Z",
      "updated": "2022-09-30T13:56:25Z",
      "categories": [
        "eess.AS",
        "eess.SP"
      ],
      "pdf_url": "https://arxiv.org/pdf/2209.15472v1",
      "landing_url": "https://arxiv.org/abs/2209.15472v1",
      "doi": "https://doi.org/10.48550/arXiv.2209.15472"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The paper targets binaural speech enhancement using STOI-optimal masks and continuous signal processing; it does not propose or analyze discrete audio tokenization into a finite vocabulary for reconstruction or understanding tasks.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The paper targets binaural speech enhancement using STOI-optimal masks and continuous signal processing; it does not propose or analyze discrete audio tokenization into a finite vocabulary for reconstruction or understanding tasks.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper focuses on binaural speech enhancement using STOI-optimal masks and processes speech signals with masks that augment signal-to-noise ratio and preserve binaural cues, but does not discuss or propose discrete audio/speech tokenization or any discrete token representations; thus, it does not meet the inclusion criteria related to discrete audio tokenization.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper focuses on binaural speech enhancement using STOI-optimal masks and processes speech signals with masks that augment signal-to-noise ratio and preserve binaural cues, but does not discuss or propose discrete audio/speech tokenization or any discrete token representations; thus, it does not meet the inclusion criteria related to discrete audio tokenization.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "A survey on Self Supervised learning approaches for improving Multimodal representation learning",
    "abstract": "Recently self supervised learning has seen explosive growth and use in variety of machine learning tasks because of its ability to avoid the cost of annotating large-scale datasets. This paper gives an overview for best self supervised learning approaches for multimodal learning. The presented approaches have been aggregated by extensive study of the literature and tackle the application of self supervised learning in different ways. The approaches discussed are cross modal generation, cross modal pretraining, cyclic translation, and generating unimodal labels in self supervised fashion.",
    "metadata": {
      "arxiv_id": "2210.11024",
      "title": "A survey on Self Supervised learning approaches for improving Multimodal representation learning",
      "summary": "Recently self supervised learning has seen explosive growth and use in variety of machine learning tasks because of its ability to avoid the cost of annotating large-scale datasets.\n  This paper gives an overview for best self supervised learning approaches for multimodal learning. The presented approaches have been aggregated by extensive study of the literature and tackle the application of self supervised learning in different ways. The approaches discussed are cross modal generation, cross modal pretraining, cyclic translation, and generating unimodal labels in self supervised fashion.",
      "authors": [
        "Naman Goyal"
      ],
      "published": "2022-10-20T05:19:49Z",
      "updated": "2022-10-20T05:19:49Z",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2210.11024v1",
      "landing_url": "https://arxiv.org/abs/2210.11024v1",
      "doi": "https://doi.org/10.48550/arXiv.2210.11024"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The abstract covers self-supervised learning for multimodal representation without addressing discrete audio/speech tokenization, thus it does not meet the inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The abstract covers self-supervised learning for multimodal representation without addressing discrete audio/speech tokenization, thus it does not meet the inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper is a survey on self supervised learning approaches for multimodal representation learning without explicit focus on audio/speech tokenization or analysis of discrete audio tokenization methods, which are required by the inclusion criteria; it discusses general self supervised learning methods rather than discrete tokenization of audio signals.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper is a survey on self supervised learning approaches for multimodal representation learning without explicit focus on audio/speech tokenization or analysis of discrete audio tokenization methods, which are required by the inclusion criteria; it discusses general self supervised learning methods rather than discrete tokenization of audio signals.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Towards Real-Time Text2Video via CLIP-Guided, Pixel-Level Optimization",
    "abstract": "We introduce an approach to generating videos based on a series of given language descriptions. Frames of the video are generated sequentially and optimized by guidance from the CLIP image-text encoder; iterating through language descriptions, weighting the current description higher than others. As opposed to optimizing through an image generator model itself, which tends to be computationally heavy, the proposed approach computes the CLIP loss directly at the pixel level, achieving general content at a speed suitable for near real-time systems. The approach can generate videos in up to 720p resolution, variable frame-rates, and arbitrary aspect ratios at a rate of 1-2 frames per second. Please visit our website to view videos and access our open-source code: https://pschaldenbrand.github.io/text2video/ .",
    "metadata": {
      "arxiv_id": "2210.12826",
      "title": "Towards Real-Time Text2Video via CLIP-Guided, Pixel-Level Optimization",
      "summary": "We introduce an approach to generating videos based on a series of given language descriptions. Frames of the video are generated sequentially and optimized by guidance from the CLIP image-text encoder; iterating through language descriptions, weighting the current description higher than others. As opposed to optimizing through an image generator model itself, which tends to be computationally heavy, the proposed approach computes the CLIP loss directly at the pixel level, achieving general content at a speed suitable for near real-time systems. The approach can generate videos in up to 720p resolution, variable frame-rates, and arbitrary aspect ratios at a rate of 1-2 frames per second. Please visit our website to view videos and access our open-source code: https://pschaldenbrand.github.io/text2video/ .",
      "authors": [
        "Peter Schaldenbrand",
        "Zhixuan Liu",
        "Jean Oh"
      ],
      "published": "2022-10-23T19:14:50Z",
      "updated": "2022-10-23T19:14:50Z",
      "categories": [
        "cs.CV",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2210.12826v1",
      "landing_url": "https://arxiv.org/abs/2210.12826v1",
      "doi": "https://doi.org/10.48550/arXiv.2210.12826"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This item describes real-time text-to-video generation using CLIP-guided pixel-level optimization and does not involve analysis of discrete audio tokenization, so it does not meet the audio-tokenization inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "This item describes real-time text-to-video generation using CLIP-guided pixel-level optimization and does not involve analysis of discrete audio tokenization, so it does not meet the audio-tokenization inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on generating videos from language descriptions using CLIP-guided pixel-level optimization but does not propose, analyze, or evaluate discrete audio/speech tokenization or any discrete audio token representation relevant to speech tasks, which is the core inclusion criterion.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on generating videos from language descriptions using CLIP-guided pixel-level optimization but does not propose, analyze, or evaluate discrete audio/speech tokenization or any discrete audio token representation relevant to speech tasks, which is the core inclusion criterion.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "TridentSE: Guiding Speech Enhancement with 32 Global Tokens",
    "abstract": "In this paper, we present TridentSE, a novel architecture for speech enhancement, which is capable of efficiently capturing both global information and local details. TridentSE maintains T-F bin level representation to capture details, and uses a small number of global tokens to process the global information. Information is propagated between the local and the global representations through cross attention modules. To capture both inter- and intra-frame information, the global tokens are divided into two groups to process along the time and the frequency axis respectively. A metric discriminator is further employed to guide our model to achieve higher perceptual quality. Even with significantly lower computational cost, TridentSE outperforms a variety of previous speech enhancement methods, achieving a PESQ of 3.47 on VoiceBank+DEMAND dataset and a PESQ of 3.44 on DNS no-reverb test set. Visualization shows that the global tokens learn diverse and interpretable global patterns.",
    "metadata": {
      "arxiv_id": "2210.12995",
      "title": "TridentSE: Guiding Speech Enhancement with 32 Global Tokens",
      "summary": "In this paper, we present TridentSE, a novel architecture for speech enhancement, which is capable of efficiently capturing both global information and local details. TridentSE maintains T-F bin level representation to capture details, and uses a small number of global tokens to process the global information. Information is propagated between the local and the global representations through cross attention modules. To capture both inter- and intra-frame information, the global tokens are divided into two groups to process along the time and the frequency axis respectively. A metric discriminator is further employed to guide our model to achieve higher perceptual quality. Even with significantly lower computational cost, TridentSE outperforms a variety of previous speech enhancement methods, achieving a PESQ of 3.47 on VoiceBank+DEMAND dataset and a PESQ of 3.44 on DNS no-reverb test set. Visualization shows that the global tokens learn diverse and interpretable global patterns.",
      "authors": [
        "Dacheng Yin",
        "Zhiyuan Zhao",
        "Chuanxin Tang",
        "Zhiwei Xiong",
        "Chong Luo"
      ],
      "published": "2022-10-24T07:30:42Z",
      "updated": "2022-10-24T07:30:42Z",
      "categories": [
        "eess.AS",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2210.12995v1",
      "landing_url": "https://arxiv.org/abs/2210.12995v1",
      "doi": "https://doi.org/10.48550/arXiv.2210.12995"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "Despite proposing a token-based architectural component ('global tokens') for speech enhancement, the abstract does not discuss discrete audio tokenization into a finite vocabulary nor analyze discretized token representations; it relies on continuous time-frequency representations, so it fails the inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "Despite proposing a token-based architectural component ('global tokens') for speech enhancement, the abstract does not discuss discrete audio tokenization into a finite vocabulary nor analyze discretized token representations; it relies on continuous time-frequency representations, so it fails the inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on speech enhancement using an architecture that processes global and local continuous representations, with no evidence of discrete audio/speech tokenization or quantization as defined in the inclusion criteria; it uses global tokens conceptually but not as discrete tokens from a finite vocabulary for audio reconstruction or generation, thus it does not meet the core requirement of analyzing discrete tokenization.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on speech enhancement using an architecture that processes global and local continuous representations, with no evidence of discrete audio/speech tokenization or quantization as defined in the inclusion criteria; it uses global tokens conceptually but not as discrete tokens from a finite vocabulary for audio reconstruction or generation, thus it does not meet the core requirement of analyzing discrete tokenization.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "FUSSL: Fuzzy Uncertain Self Supervised Learning",
    "abstract": "Self supervised learning (SSL) has become a very successful technique to harness the power of unlabeled data, with no annotation effort. A number of developed approaches are evolving with the goal of outperforming supervised alternatives, which have been relatively successful. One main issue in SSL is robustness of the approaches under different settings. In this paper, for the first time, we recognize the fundamental limits of SSL coming from the use of a single-supervisory signal. To address this limitation, we leverage the power of uncertainty representation to devise a robust and general standard hierarchical learning/training protocol for any SSL baseline, regardless of their assumptions and approaches. Essentially, using the information bottleneck principle, we decompose feature learning into a two-stage training procedure, each with a distinct supervision signal. This double supervision approach is captured in two key steps: 1) invariance enforcement to data augmentation, and 2) fuzzy pseudo labeling (both hard and soft annotation). This simple, yet, effective protocol which enables cross-class/cluster feature learning, is instantiated via an initial training of an ensemble of models through invariance enforcement to data augmentation as first training phase, and then assigning fuzzy labels to the original samples for the second training phase. We consider multiple alternative scenarios with double supervision and evaluate the effectiveness of our approach on recent baselines, covering four different SSL paradigms, including geometrical, contrastive, non-contrastive, and hard/soft whitening (redundancy reduction) baselines. Extensive experiments under multiple settings show that the proposed training protocol consistently improves the performance of the former baselines, independent of their respective underlying principles.",
    "metadata": {
      "arxiv_id": "2210.15818",
      "title": "FUSSL: Fuzzy Uncertain Self Supervised Learning",
      "summary": "Self supervised learning (SSL) has become a very successful technique to harness the power of unlabeled data, with no annotation effort. A number of developed approaches are evolving with the goal of outperforming supervised alternatives, which have been relatively successful. One main issue in SSL is robustness of the approaches under different settings. In this paper, for the first time, we recognize the fundamental limits of SSL coming from the use of a single-supervisory signal. To address this limitation, we leverage the power of uncertainty representation to devise a robust and general standard hierarchical learning/training protocol for any SSL baseline, regardless of their assumptions and approaches. Essentially, using the information bottleneck principle, we decompose feature learning into a two-stage training procedure, each with a distinct supervision signal. This double supervision approach is captured in two key steps: 1) invariance enforcement to data augmentation, and 2) fuzzy pseudo labeling (both hard and soft annotation). This simple, yet, effective protocol which enables cross-class/cluster feature learning, is instantiated via an initial training of an ensemble of models through invariance enforcement to data augmentation as first training phase, and then assigning fuzzy labels to the original samples for the second training phase. We consider multiple alternative scenarios with double supervision and evaluate the effectiveness of our approach on recent baselines, covering four different SSL paradigms, including geometrical, contrastive, non-contrastive, and hard/soft whitening (redundancy reduction) baselines. Extensive experiments under multiple settings show that the proposed training protocol consistently improves the performance of the former baselines, independent of their respective underlying principles.",
      "authors": [
        "Salman Mohamadi",
        "Gianfranco Doretto",
        "Donald A. Adjeroh"
      ],
      "published": "2022-10-28T01:06:10Z",
      "updated": "2022-10-28T01:06:10Z",
      "categories": [
        "cs.CV",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2210.15818v1",
      "landing_url": "https://arxiv.org/abs/2210.15818v1",
      "doi": "https://doi.org/10.48550/arXiv.2210.15818"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The paper discusses general self-supervised learning with uncertainty and label augmentation, but does not propose or evaluate discrete audio/speech tokenization or token-based representations.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The paper discusses general self-supervised learning with uncertainty and label augmentation, but does not propose or evaluate discrete audio/speech tokenization or token-based representations.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper focuses on self-supervised learning methods using fuzzy pseudo labeling and hierarchical training, without any specific mention or analysis of discrete audio/speech tokenization or quantization from continuous audio signals into discrete symbols, which is the core inclusion criterion.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper focuses on self-supervised learning methods using fuzzy pseudo labeling and hierarchical training, without any specific mention or analysis of discrete audio/speech tokenization or quantization from continuous audio signals into discrete symbols, which is the core inclusion criterion.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Elastic Weight Consolidation Improves the Robustness of Self-Supervised Learning Methods under Transfer",
    "abstract": "Self-supervised representation learning (SSL) methods provide an effective label-free initial condition for fine-tuning downstream tasks. However, in numerous realistic scenarios, the downstream task might be biased with respect to the target label distribution. This in turn moves the learned fine-tuned model posterior away from the initial (label) bias-free self-supervised model posterior. In this work, we re-interpret SSL fine-tuning under the lens of Bayesian continual learning and consider regularization through the Elastic Weight Consolidation (EWC) framework. We demonstrate that self-regularization against an initial SSL backbone improves worst sub-group performance in Waterbirds by 5% and Celeb-A by 2% when using the ViT-B/16 architecture. Furthermore, to help simplify the use of EWC with SSL, we pre-compute and publicly release the Fisher Information Matrix (FIM), evaluated with 10,000 ImageNet-1K variates evaluated on large modern SSL architectures including ViT-B/16 and ResNet50 trained with DINO.",
    "metadata": {
      "arxiv_id": "2210.16365",
      "title": "Elastic Weight Consolidation Improves the Robustness of Self-Supervised Learning Methods under Transfer",
      "summary": "Self-supervised representation learning (SSL) methods provide an effective label-free initial condition for fine-tuning downstream tasks. However, in numerous realistic scenarios, the downstream task might be biased with respect to the target label distribution. This in turn moves the learned fine-tuned model posterior away from the initial (label) bias-free self-supervised model posterior. In this work, we re-interpret SSL fine-tuning under the lens of Bayesian continual learning and consider regularization through the Elastic Weight Consolidation (EWC) framework. We demonstrate that self-regularization against an initial SSL backbone improves worst sub-group performance in Waterbirds by 5% and Celeb-A by 2% when using the ViT-B/16 architecture. Furthermore, to help simplify the use of EWC with SSL, we pre-compute and publicly release the Fisher Information Matrix (FIM), evaluated with 10,000 ImageNet-1K variates evaluated on large modern SSL architectures including ViT-B/16 and ResNet50 trained with DINO.",
      "authors": [
        "Andrius Ovsianas",
        "Jason Ramapuram",
        "Dan Busbridge",
        "Eeshan Gunesh Dhekane",
        "Russ Webb"
      ],
      "published": "2022-10-28T19:00:25Z",
      "updated": "2022-10-28T19:00:25Z",
      "categories": [
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2210.16365v1",
      "landing_url": "https://arxiv.org/abs/2210.16365v1",
      "doi": "https://doi.org/10.48550/arXiv.2210.16365"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item concerns Elastic Weight Consolidation in self-supervised learning for vision tasks and does not involve any discrete audio/speech tokenization or analysis thereof, so it fails the audio tokenization inclusion criteria entirely and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item concerns Elastic Weight Consolidation in self-supervised learning for vision tasks and does not involve any discrete audio/speech tokenization or analysis thereof, so it fails the audio tokenization inclusion criteria entirely and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on self-supervised learning robustness and transfer learning using Elastic Weight Consolidation (EWC) but does not discuss discrete audio or speech tokenization or any discrete symbol-based audio representation; it is unrelated to the core inclusion requirement about discrete audio tokenization.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on self-supervised learning robustness and transfer learning using Elastic Weight Consolidation (EWC) but does not discuss discrete audio or speech tokenization or any discrete symbol-based audio representation; it is unrelated to the core inclusion requirement about discrete audio tokenization.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Application of Knowledge Distillation to Multi-task Speech Representation Learning",
    "abstract": "Model architectures such as wav2vec 2.0 and HuBERT have been proposed to learn speech representations from audio waveforms in a self-supervised manner. When they are combined with downstream tasks such as keyword spotting and speaker verification, they provide state-of-the-art performance. However, these models use a large number of parameters, the smallest version of which has 95 million parameters. This constitutes a challenge for edge AI device deployments. In this paper, we investigate the application of knowledge distillation to speech representation learning (SRL) models followed by joint fine-tuning with multiple downstream voice-activated tasks. In our experiments on two such tasks, our approach results in nearly 75% reduction in model size while suffering only 0.1% accuracy and 0.9% equal error rate degradation compared to the full-size model. In addition, we show that fine-tuning the SRL models results in a significant performance boost compared to using frozen SRL models.",
    "metadata": {
      "arxiv_id": "2210.16611",
      "title": "Application of Knowledge Distillation to Multi-task Speech Representation Learning",
      "summary": "Model architectures such as wav2vec 2.0 and HuBERT have been proposed to learn speech representations from audio waveforms in a self-supervised manner. When they are combined with downstream tasks such as keyword spotting and speaker verification, they provide state-of-the-art performance. However, these models use a large number of parameters, the smallest version of which has 95 million parameters. This constitutes a challenge for edge AI device deployments. In this paper, we investigate the application of knowledge distillation to speech representation learning (SRL) models followed by joint fine-tuning with multiple downstream voice-activated tasks. In our experiments on two such tasks, our approach results in nearly 75% reduction in model size while suffering only 0.1% accuracy and 0.9% equal error rate degradation compared to the full-size model. In addition, we show that fine-tuning the SRL models results in a significant performance boost compared to using frozen SRL models.",
      "authors": [
        "Mine Kerpicci",
        "Van Nguyen",
        "Shuhua Zhang",
        "Erik Visser"
      ],
      "published": "2022-10-29T14:22:43Z",
      "updated": "2023-05-19T17:16:53Z",
      "categories": [
        "eess.AS",
        "cs.CL",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2210.16611v2",
      "landing_url": "https://arxiv.org/abs/2210.16611v2",
      "doi": "https://doi.org/10.48550/arXiv.2210.16611"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The abstract discusses distillation and multi-task learning for speech representation learning but does not explicitly analyze or propose discrete audio tokenization (quantized speech tokens) as a core object, which is required by the inclusion criteria.",
      "evaluation": 2
    },
    "round-A_JuniorNano_reasoning": "The abstract discusses distillation and multi-task learning for speech representation learning but does not explicitly analyze or propose discrete audio tokenization (quantized speech tokens) as a core object, which is required by the inclusion criteria.",
    "round-A_JuniorNano_evaluation": 2,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper focuses on applying knowledge distillation to speech representation learning models and improving downstream tasks like keyword spotting and speaker verification, but it does not explicitly propose, analyze, or evaluate discrete audio tokenization methods converting continuous audio signals into discrete symbol sequences; rather, it mainly deals with model compression and fine-tuning of continuous speech representations.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper focuses on applying knowledge distillation to speech representation learning models and improving downstream tasks like keyword spotting and speaker verification, but it does not explicitly propose, analyze, or evaluate discrete audio tokenization methods converting continuous audio signals into discrete symbol sequences; rather, it mainly deals with model compression and fine-tuning of continuous speech representations.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:2)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:2)"
  },
  {
    "title": "token2vec: A Joint Self-Supervised Pre-training Framework Using Unpaired Speech and Text",
    "abstract": "Self-supervised pre-training has been successful in both text and speech processing. Speech and text offer different but complementary information. The question is whether we are able to perform a speech-text joint pre-training on unpaired speech and text. In this paper, we take the idea of self-supervised pre-training one step further and propose token2vec, a novel joint pre-training framework for unpaired speech and text based on discrete representations of speech. Firstly, due to the distinct characteristics between speech and text modalities, where speech is continuous while text is discrete, we first discretize speech into a sequence of discrete speech tokens to solve the modality mismatch problem. Secondly, to solve the length mismatch problem, where the speech sequence is usually much longer than text sequence, we convert the words of text into phoneme sequences and randomly repeat each phoneme in the sequences. Finally, we feed the discrete speech and text tokens into a modality-agnostic Transformer encoder and pre-train with token-level masking language modeling (tMLM). Experiments show that token2vec is significantly superior to various speech-only pre-training baselines, with up to 17.7% relative WER reduction. Token2vec model is also validated on a non-ASR task, i.e., spoken intent classification, and shows good transferability.",
    "metadata": {
      "arxiv_id": "2210.16755",
      "title": "token2vec: A Joint Self-Supervised Pre-training Framework Using Unpaired Speech and Text",
      "summary": "Self-supervised pre-training has been successful in both text and speech processing. Speech and text offer different but complementary information. The question is whether we are able to perform a speech-text joint pre-training on unpaired speech and text. In this paper, we take the idea of self-supervised pre-training one step further and propose token2vec, a novel joint pre-training framework for unpaired speech and text based on discrete representations of speech. Firstly, due to the distinct characteristics between speech and text modalities, where speech is continuous while text is discrete, we first discretize speech into a sequence of discrete speech tokens to solve the modality mismatch problem. Secondly, to solve the length mismatch problem, where the speech sequence is usually much longer than text sequence, we convert the words of text into phoneme sequences and randomly repeat each phoneme in the sequences. Finally, we feed the discrete speech and text tokens into a modality-agnostic Transformer encoder and pre-train with token-level masking language modeling (tMLM). Experiments show that token2vec is significantly superior to various speech-only pre-training baselines, with up to 17.7% relative WER reduction. Token2vec model is also validated on a non-ASR task, i.e., spoken intent classification, and shows good transferability.",
      "authors": [
        "Xianghu Yue",
        "Junyi Ao",
        "Xiaoxue Gao",
        "Haizhou Li"
      ],
      "published": "2022-10-30T06:38:19Z",
      "updated": "2022-10-30T06:38:19Z",
      "categories": [
        "cs.CL",
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2210.16755v1",
      "landing_url": "https://arxiv.org/abs/2210.16755v1",
      "doi": "https://doi.org/10.48550/arXiv.2210.16755"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The paper discretizes speech into discrete tokens and builds a joint pre-training framework with unpaired speech and text, making discrete tokenization a core contribution and evaluating its impact, which aligns with the inclusion criteria and avoids exclusion criteria.",
      "evaluation": 5
    },
    "round-A_JuniorNano_reasoning": "The paper discretizes speech into discrete tokens and builds a joint pre-training framework with unpaired speech and text, making discrete tokenization a core contribution and evaluating its impact, which aligns with the inclusion criteria and avoids exclusion criteria.",
    "round-A_JuniorNano_evaluation": 5,
    "round-A_JuniorMini_output": {
      "reasoning": "The study explicitly proposes a joint self-supervised pre-training framework using discrete speech tokenization, where continuous speech is discretized into tokens as a core part of the method, with thorough experimental validation demonstrating the effectiveness of the discrete token approach in speech understanding tasks, meeting all inclusion criteria and none of the exclusion criteria.",
      "evaluation": 5
    },
    "round-A_JuniorMini_reasoning": "The study explicitly proposes a joint self-supervised pre-training framework using discrete speech tokenization, where continuous speech is discretized into tokens as a core part of the method, with thorough experimental validation demonstrating the effectiveness of the discrete token approach in speech understanding tasks, meeting all inclusion criteria and none of the exclusion criteria.",
    "round-A_JuniorMini_evaluation": 5,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "include (junior:5)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "DUEL: Adaptive Duplicate Elimination on Working Memory for Self-Supervised Learning",
    "abstract": "In Self-Supervised Learning (SSL), it is known that frequent occurrences of the collision in which target data and its negative samples share the same class can decrease performance. Especially in real-world data such as crawled data or robot-gathered observations, collisions may occur more often due to the duplicates in the data. To deal with this problem, we claim that sampling negative samples from the adaptively debiased distribution in the memory makes the model more stable than sampling from a biased dataset directly. In this paper, we introduce a novel SSL framework with adaptive Duplicate Elimination (DUEL) inspired by the human working memory. The proposed framework successfully prevents the downstream task performance from degradation due to a dramatic inter-class imbalance.",
    "metadata": {
      "arxiv_id": "2210.17052",
      "title": "DUEL: Adaptive Duplicate Elimination on Working Memory for Self-Supervised Learning",
      "summary": "In Self-Supervised Learning (SSL), it is known that frequent occurrences of the collision in which target data and its negative samples share the same class can decrease performance. Especially in real-world data such as crawled data or robot-gathered observations, collisions may occur more often due to the duplicates in the data. To deal with this problem, we claim that sampling negative samples from the adaptively debiased distribution in the memory makes the model more stable than sampling from a biased dataset directly. In this paper, we introduce a novel SSL framework with adaptive Duplicate Elimination (DUEL) inspired by the human working memory. The proposed framework successfully prevents the downstream task performance from degradation due to a dramatic inter-class imbalance.",
      "authors": [
        "Won-Seok Choi",
        "Dong-Sig Han",
        "Hyundo Lee",
        "Junseok Park",
        "Byoung-Tak Zhang"
      ],
      "published": "2022-10-31T04:04:48Z",
      "updated": "2022-10-31T04:04:48Z",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2210.17052v1",
      "landing_url": "https://arxiv.org/abs/2210.17052v1",
      "doi": "https://doi.org/10.48550/arXiv.2210.17052"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The paper focuses on adaptive duplicate elimination in self-supervised learning and negative sampling, with no discussion of discrete audio/speech tokenization or discrete token representations, so it does not meet the inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The paper focuses on adaptive duplicate elimination in self-supervised learning and negative sampling, with no discussion of discrete audio/speech tokenization or discrete token representations, so it does not meet the inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on a self-supervised learning framework with adaptive duplicate elimination to improve model stability in data with duplicates, but it does not mention discrete audio/speech tokenization or the conversion of audio signals into discrete tokens; thus, it does not meet the inclusion criteria related to discrete audio/speech tokenization analysis or methods.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on a self-supervised learning framework with adaptive duplicate elimination to improve model stability in data with duplicates, but it does not mention discrete audio/speech tokenization or the conversion of audio signals into discrete tokens; thus, it does not meet the inclusion criteria related to discrete audio/speech tokenization analysis or methods.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "RecD: Deduplication for End-to-End Deep Learning Recommendation Model Training Infrastructure",
    "abstract": "We present RecD (Recommendation Deduplication), a suite of end-to-end infrastructure optimizations across the Deep Learning Recommendation Model (DLRM) training pipeline. RecD addresses immense storage, preprocessing, and training overheads caused by feature duplication inherent in industry-scale DLRM training datasets. Feature duplication arises because DLRM datasets are generated from interactions. While each user session can generate multiple training samples, many features' values do not change across these samples. We demonstrate how RecD exploits this property, end-to-end, across a deployed training pipeline. RecD optimizes data generation pipelines to decrease dataset storage and preprocessing resource demands and to maximize duplication within a training batch. RecD introduces a new tensor format, InverseKeyedJaggedTensors (IKJTs), to deduplicate feature values in each batch. We show how DLRM model architectures can leverage IKJTs to drastically increase training throughput. RecD improves the training and preprocessing throughput and storage efficiency by up to 2.48x, 1.79x, and 3.71x, respectively, in an industry-scale DLRM training system.",
    "metadata": {
      "arxiv_id": "2211.05239",
      "title": "RecD: Deduplication for End-to-End Deep Learning Recommendation Model Training Infrastructure",
      "summary": "We present RecD (Recommendation Deduplication), a suite of end-to-end infrastructure optimizations across the Deep Learning Recommendation Model (DLRM) training pipeline. RecD addresses immense storage, preprocessing, and training overheads caused by feature duplication inherent in industry-scale DLRM training datasets. Feature duplication arises because DLRM datasets are generated from interactions. While each user session can generate multiple training samples, many features' values do not change across these samples. We demonstrate how RecD exploits this property, end-to-end, across a deployed training pipeline. RecD optimizes data generation pipelines to decrease dataset storage and preprocessing resource demands and to maximize duplication within a training batch. RecD introduces a new tensor format, InverseKeyedJaggedTensors (IKJTs), to deduplicate feature values in each batch. We show how DLRM model architectures can leverage IKJTs to drastically increase training throughput. RecD improves the training and preprocessing throughput and storage efficiency by up to 2.48x, 1.79x, and 3.71x, respectively, in an industry-scale DLRM training system.",
      "authors": [
        "Mark Zhao",
        "Dhruv Choudhary",
        "Devashish Tyagi",
        "Ajay Somani",
        "Max Kaplan",
        "Sung-Han Lin",
        "Sarunya Pumma",
        "Jongsoo Park",
        "Aarti Basant",
        "Niket Agarwal",
        "Carole-Jean Wu",
        "Christos Kozyrakis"
      ],
      "published": "2022-11-09T22:21:19Z",
      "updated": "2023-05-01T19:37:39Z",
      "categories": [
        "cs.LG",
        "cs.DC",
        "cs.IR",
        "cs.PF"
      ],
      "pdf_url": "https://arxiv.org/pdf/2211.05239v4",
      "landing_url": "https://arxiv.org/abs/2211.05239v4",
      "doi": "https://doi.org/10.48550/arXiv.2211.05239"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This item concerns deduplication and efficiency in a DLRM training pipeline and does not analyze or propose discrete audio/speech tokenization, so it should be excluded per the audio tokenization criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "This item concerns deduplication and efficiency in a DLRM training pipeline and does not analyze or propose discrete audio/speech tokenization, so it should be excluded per the audio tokenization criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on infrastructure optimizations for deep learning recommendation model training, specifically addressing feature deduplication and training efficiency, without any mention of discrete audio/speech tokenization or audio signal processing. It does not meet the inclusion criteria of studying discrete audio/speech tokenization and falls outside the scope of audio-related research as defined.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on infrastructure optimizations for deep learning recommendation model training, specifically addressing feature deduplication and training efficiency, without any mention of discrete audio/speech tokenization or audio signal processing. It does not meet the inclusion criteria of studying discrete audio/speech tokenization and falls outside the scope of audio-related research as defined.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Contrastive Self-Supervised Learning for Skeleton Representations",
    "abstract": "Human skeleton point clouds are commonly used to automatically classify and predict the behaviour of others. In this paper, we use a contrastive self-supervised learning method, SimCLR, to learn representations that capture the semantics of skeleton point clouds. This work focuses on systematically evaluating the effects that different algorithmic decisions (including augmentations, dataset partitioning and backbone architecture) have on the learned skeleton representations. To pre-train the representations, we normalise six existing datasets to obtain more than 40 million skeleton frames. We evaluate the quality of the learned representations with three downstream tasks: skeleton reconstruction, motion prediction, and activity classification. Our results demonstrate the importance of 1) combining spatial and temporal augmentations, 2) including additional datasets for encoder training, and 3) and using a graph neural network as an encoder.",
    "metadata": {
      "arxiv_id": "2211.05304",
      "title": "Contrastive Self-Supervised Learning for Skeleton Representations",
      "summary": "Human skeleton point clouds are commonly used to automatically classify and predict the behaviour of others. In this paper, we use a contrastive self-supervised learning method, SimCLR, to learn representations that capture the semantics of skeleton point clouds. This work focuses on systematically evaluating the effects that different algorithmic decisions (including augmentations, dataset partitioning and backbone architecture) have on the learned skeleton representations. To pre-train the representations, we normalise six existing datasets to obtain more than 40 million skeleton frames. We evaluate the quality of the learned representations with three downstream tasks: skeleton reconstruction, motion prediction, and activity classification. Our results demonstrate the importance of 1) combining spatial and temporal augmentations, 2) including additional datasets for encoder training, and 3) and using a graph neural network as an encoder.",
      "authors": [
        "Nico Lingg",
        "Miguel Sarabia",
        "Luca Zappella",
        "Barry-John Theobald"
      ],
      "published": "2022-11-10T02:45:36Z",
      "updated": "2022-11-10T02:45:36Z",
      "categories": [
        "cs.CV",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2211.05304v1",
      "landing_url": "https://arxiv.org/abs/2211.05304v1",
      "doi": "https://doi.org/10.48550/arXiv.2211.05304"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The article focuses on contrastive self-supervised learning for skeleton point clouds and encoder design; it does not involve discrete audio/speech tokenization or analysis of discrete audio representations, so it does not meet the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The article focuses on contrastive self-supervised learning for skeleton point clouds and encoder design; it does not involve discrete audio/speech tokenization or analysis of discrete audio representations, so it does not meet the inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on contrastive self-supervised learning for skeleton representations from point cloud data, with no mention of discrete audio/speech tokenization or discrete symbol conversion from audio signals, therefore it does not meet the inclusion criteria related to audio tokenization.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on contrastive self-supervised learning for skeleton representations from point cloud data, with no mention of discrete audio/speech tokenization or discrete symbol conversion from audio signals, therefore it does not meet the inclusion criteria related to audio tokenization.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Is the Statistical Interpretation of Quantum Mechanics $ψ$-Ontic or $ψ$-Epistemic?",
    "abstract": "The ontological models framework distinguishes $ψ$-ontic from $ψ$-epistemic wavefunctions. It is, in general, quite straightforward to categorize the wave-function of a certain quantum theory. Nevertheless, there has been a debate about the ontological status of the wave-function in the statistical interpretation of quantum mechanics: is it $ψ$-epistemic and incomplete or $ψ$-ontic and complete? I will argue that the wavefunction in this interpretation is best regarded as $ψ$-ontic and incomplete.",
    "metadata": {
      "arxiv_id": "2211.07057",
      "title": "Is the Statistical Interpretation of Quantum Mechanics $ψ$-Ontic or $ψ$-Epistemic?",
      "summary": "The ontological models framework distinguishes $ψ$-ontic from $ψ$-epistemic wavefunctions. It is, in general, quite straightforward to categorize the wave-function of a certain quantum theory. Nevertheless, there has been a debate about the ontological status of the wave-function in the statistical interpretation of quantum mechanics: is it $ψ$-epistemic and incomplete or $ψ$-ontic and complete? I will argue that the wavefunction in this interpretation is best regarded as $ψ$-ontic and incomplete.",
      "authors": [
        "Mario Hubert"
      ],
      "published": "2022-11-14T00:30:56Z",
      "updated": "2022-11-14T00:30:56Z",
      "categories": [
        "quant-ph"
      ],
      "pdf_url": "https://arxiv.org/pdf/2211.07057v1",
      "landing_url": "https://arxiv.org/abs/2211.07057v1",
      "doi": "https://doi.org/10.1007/s10701-022-00651-0"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item is a theoretical physics/philosophy paper on ψ-ontic vs ψ-epistemic interpretations of the wavefunction; there is no discrete audio/speech tokenization, no core object of discrete token design, and it does not evaluate token-based representations. It fails all inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item is a theoretical physics/philosophy paper on ψ-ontic vs ψ-epistemic interpretations of the wavefunction; there is no discrete audio/speech tokenization, no core object of discrete token design, and it does not evaluate token-based representations. It fails all inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study discusses the ontological status of the wave-function in quantum mechanics, a topic in foundational physics, and does not mention any aspect of audio/speech tokenization or discrete audio representations as required by the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study discusses the ontological status of the wave-function in quantum mechanics, a topic in foundational physics, and does not mention any aspect of audio/speech tokenization or discrete audio representations as required by the inclusion criteria.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Homomorphic Self-Supervised Learning",
    "abstract": "In this work, we observe that many existing self-supervised learning algorithms can be both unified and generalized when seen through the lens of equivariant representations. Specifically, we introduce a general framework we call Homomorphic Self-Supervised Learning, and theoretically show how it may subsume the use of input-augmentations provided an augmentation-homomorphic feature extractor. We validate this theory experimentally for simple augmentations, demonstrate how the framework fails when representational structure is removed, and further empirically explore how the parameters of this framework relate to those of traditional augmentation-based self-supervised learning. We conclude with a discussion of the potential benefits afforded by this new perspective on self-supervised learning.",
    "metadata": {
      "arxiv_id": "2211.08282",
      "title": "Homomorphic Self-Supervised Learning",
      "summary": "In this work, we observe that many existing self-supervised learning algorithms can be both unified and generalized when seen through the lens of equivariant representations. Specifically, we introduce a general framework we call Homomorphic Self-Supervised Learning, and theoretically show how it may subsume the use of input-augmentations provided an augmentation-homomorphic feature extractor. We validate this theory experimentally for simple augmentations, demonstrate how the framework fails when representational structure is removed, and further empirically explore how the parameters of this framework relate to those of traditional augmentation-based self-supervised learning. We conclude with a discussion of the potential benefits afforded by this new perspective on self-supervised learning.",
      "authors": [
        "T. Anderson Keller",
        "Xavier Suau",
        "Luca Zappella"
      ],
      "published": "2022-11-15T16:32:36Z",
      "updated": "2022-11-15T16:32:36Z",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2211.08282v1",
      "landing_url": "https://arxiv.org/abs/2211.08282v1",
      "doi": "https://doi.org/10.48550/arXiv.2211.08282"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The abstract presents a framework for self-supervised learning with augmentation, not discrete speech/audio tokenization or analysis of discrete audio tokens; it fails the inclusion criteria and aligns with exclusion criteria for non-tokenized/discrete audio work.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The abstract presents a framework for self-supervised learning with augmentation, not discrete speech/audio tokenization or analysis of discrete audio tokens; it fails the inclusion criteria and aligns with exclusion criteria for non-tokenized/discrete audio work.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on a general framework for self-supervised learning based on equivariant representations and augmentations, without any mention or analysis of discrete audio/speech tokenization or quantization methods relevant to converting continuous audio signals into discrete tokens; thus, it does not meet the inclusion criteria nor does it touch on discrete audio token representations as required.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on a general framework for self-supervised learning based on equivariant representations and augmentations, without any mention or analysis of discrete audio/speech tokenization or quantization methods relevant to converting continuous audio signals into discrete tokens; thus, it does not meet the inclusion criteria nor does it touch on discrete audio token representations as required.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "MAGE: MAsked Generative Encoder to Unify Representation Learning and Image Synthesis",
    "abstract": "Generative modeling and representation learning are two key tasks in computer vision. However, these models are typically trained independently, which ignores the potential for each task to help the other, and leads to training and model maintenance overheads. In this work, we propose MAsked Generative Encoder (MAGE), the first framework to unify SOTA image generation and self-supervised representation learning. Our key insight is that using variable masking ratios in masked image modeling pre-training can allow generative training (very high masking ratio) and representation learning (lower masking ratio) under the same training framework. Inspired by previous generative models, MAGE uses semantic tokens learned by a vector-quantized GAN at inputs and outputs, combining this with masking. We can further improve the representation by adding a contrastive loss to the encoder output. We extensively evaluate the generation and representation learning capabilities of MAGE. On ImageNet-1K, a single MAGE ViT-L model obtains 9.10 FID in the task of class-unconditional image generation and 78.9% top-1 accuracy for linear probing, achieving state-of-the-art performance in both image generation and representation learning. Code is available at https://github.com/LTH14/mage.",
    "metadata": {
      "arxiv_id": "2211.09117",
      "title": "MAGE: MAsked Generative Encoder to Unify Representation Learning and Image Synthesis",
      "summary": "Generative modeling and representation learning are two key tasks in computer vision. However, these models are typically trained independently, which ignores the potential for each task to help the other, and leads to training and model maintenance overheads. In this work, we propose MAsked Generative Encoder (MAGE), the first framework to unify SOTA image generation and self-supervised representation learning. Our key insight is that using variable masking ratios in masked image modeling pre-training can allow generative training (very high masking ratio) and representation learning (lower masking ratio) under the same training framework. Inspired by previous generative models, MAGE uses semantic tokens learned by a vector-quantized GAN at inputs and outputs, combining this with masking. We can further improve the representation by adding a contrastive loss to the encoder output. We extensively evaluate the generation and representation learning capabilities of MAGE. On ImageNet-1K, a single MAGE ViT-L model obtains 9.10 FID in the task of class-unconditional image generation and 78.9% top-1 accuracy for linear probing, achieving state-of-the-art performance in both image generation and representation learning. Code is available at https://github.com/LTH14/mage.",
      "authors": [
        "Tianhong Li",
        "Huiwen Chang",
        "Shlok Kumar Mishra",
        "Han Zhang",
        "Dina Katabi",
        "Dilip Krishnan"
      ],
      "published": "2022-11-16T18:59:02Z",
      "updated": "2023-06-29T15:30:25Z",
      "categories": [
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2211.09117v2",
      "landing_url": "https://arxiv.org/abs/2211.09117v2",
      "doi": "https://doi.org/10.48550/arXiv.2211.09117"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The paper targets image generation and self-supervised learning with visual tokens, not discrete audio/speech tokenization, so it does not satisfy the inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The paper targets image generation and self-supervised learning with visual tokens, not discrete audio/speech tokenization, so it does not satisfy the inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on image generation and representation learning, using masked generative encoding and vector-quantized GANs for image tokens, without any indication of dealing with audio or speech tokenization; thus it does not meet the inclusion criteria related to discrete audio/speech tokenization.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on image generation and representation learning, using masked generative encoding and vector-quantized GANs for image tokens, without any indication of dealing with audio or speech tokenization; thus it does not meet the inclusion criteria related to discrete audio/speech tokenization.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "MelHuBERT: A simplified HuBERT on Mel spectrograms",
    "abstract": "Self-supervised models have had great success in learning speech representations that can generalize to various downstream tasks. However, most self-supervised models require a large amount of compute and multiple GPUs to train, significantly hampering the development of self-supervised learning. In an attempt to reduce the computation of training, we revisit the training of HuBERT, a highly successful self-supervised model. We improve and simplify several key components, including the loss function, input representation, and training in multiple stages. Our model, MelHuBERT, is able to achieve favorable performance on phone recognition, speaker identification, and automatic speech recognition against HuBERT, while saving 31.2% of the pre-training time, or equivalently 33.5% MACs per one second speech. The code and pre-trained models are available in https://github.com/nervjack2/MelHuBERT.",
    "metadata": {
      "arxiv_id": "2211.09944",
      "title": "MelHuBERT: A simplified HuBERT on Mel spectrograms",
      "summary": "Self-supervised models have had great success in learning speech representations that can generalize to various downstream tasks. However, most self-supervised models require a large amount of compute and multiple GPUs to train, significantly hampering the development of self-supervised learning. In an attempt to reduce the computation of training, we revisit the training of HuBERT, a highly successful self-supervised model. We improve and simplify several key components, including the loss function, input representation, and training in multiple stages. Our model, MelHuBERT, is able to achieve favorable performance on phone recognition, speaker identification, and automatic speech recognition against HuBERT, while saving 31.2% of the pre-training time, or equivalently 33.5% MACs per one second speech. The code and pre-trained models are available in https://github.com/nervjack2/MelHuBERT.",
      "authors": [
        "Tzu-Quan Lin",
        "Hung-yi Lee",
        "Hao Tang"
      ],
      "published": "2022-11-17T23:38:29Z",
      "updated": "2024-08-29T19:25:59Z",
      "categories": [
        "cs.CL",
        "cs.LG",
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2211.09944v3",
      "landing_url": "https://arxiv.org/abs/2211.09944v3",
      "doi": "https://doi.org/10.48550/arXiv.2211.09944"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The work focuses on simplifying HuBERT and reducing pre-training compute using Mel spectrogram inputs; it does not explicitly propose, analyze, or evaluate discrete audio/speech tokenization as a core object, and continuous features (Mel spectrograms) are used rather than discrete tokens.",
      "evaluation": 2
    },
    "round-A_JuniorNano_reasoning": "The work focuses on simplifying HuBERT and reducing pre-training compute using Mel spectrogram inputs; it does not explicitly propose, analyze, or evaluate discrete audio/speech tokenization as a core object, and continuous features (Mel spectrograms) are used rather than discrete tokens.",
    "round-A_JuniorNano_evaluation": 2,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on simplifying HuBERT, a self-supervised model that works on Mel spectrograms for speech tasks, and mentions improved performance on phone recognition and ASR; however, it does not explicitly discuss the proposal, analysis, or evaluation of discrete audio/speech tokenization as a core part, nor does it present a quantization or discrete token design as analyzed objects, so it likely only uses continuous features without discrete tokenization design or analysis.",
      "evaluation": 2
    },
    "round-A_JuniorMini_reasoning": "The study focuses on simplifying HuBERT, a self-supervised model that works on Mel spectrograms for speech tasks, and mentions improved performance on phone recognition and ASR; however, it does not explicitly discuss the proposal, analysis, or evaluation of discrete audio/speech tokenization as a core part, nor does it present a quantization or discrete token design as analyzed objects, so it likely only uses continuous features without discrete tokenization design or analysis.",
    "round-A_JuniorMini_evaluation": 2,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:2)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:2)"
  },
  {
    "title": "Weighted Ensemble Self-Supervised Learning",
    "abstract": "Ensembling has proven to be a powerful technique for boosting model performance, uncertainty estimation, and robustness in supervised learning. Advances in self-supervised learning (SSL) enable leveraging large unlabeled corpora for state-of-the-art few-shot and supervised learning performance. In this paper, we explore how ensemble methods can improve recent SSL techniques by developing a framework that permits data-dependent weighted cross-entropy losses. We refrain from ensembling the representation backbone; this choice yields an efficient ensemble method that incurs a small training cost and requires no architectural changes or computational overhead to downstream evaluation. The effectiveness of our method is demonstrated with two state-of-the-art SSL methods, DINO (Caron et al., 2021) and MSN (Assran et al., 2022). Our method outperforms both in multiple evaluation metrics on ImageNet-1K, particularly in the few-shot setting. We explore several weighting schemes and find that those which increase the diversity of ensemble heads lead to better downstream evaluation results. Thorough experiments yield improved prior art baselines which our method still surpasses; e.g., our overall improvement with MSN ViT-B/16 is 3.9 p.p. for 1-shot learning.",
    "metadata": {
      "arxiv_id": "2211.09981",
      "title": "Weighted Ensemble Self-Supervised Learning",
      "summary": "Ensembling has proven to be a powerful technique for boosting model performance, uncertainty estimation, and robustness in supervised learning. Advances in self-supervised learning (SSL) enable leveraging large unlabeled corpora for state-of-the-art few-shot and supervised learning performance. In this paper, we explore how ensemble methods can improve recent SSL techniques by developing a framework that permits data-dependent weighted cross-entropy losses. We refrain from ensembling the representation backbone; this choice yields an efficient ensemble method that incurs a small training cost and requires no architectural changes or computational overhead to downstream evaluation. The effectiveness of our method is demonstrated with two state-of-the-art SSL methods, DINO (Caron et al., 2021) and MSN (Assran et al., 2022). Our method outperforms both in multiple evaluation metrics on ImageNet-1K, particularly in the few-shot setting. We explore several weighting schemes and find that those which increase the diversity of ensemble heads lead to better downstream evaluation results. Thorough experiments yield improved prior art baselines which our method still surpasses; e.g., our overall improvement with MSN ViT-B/16 is 3.9 p.p. for 1-shot learning.",
      "authors": [
        "Yangjun Ruan",
        "Saurabh Singh",
        "Warren Morningstar",
        "Alexander A. Alemi",
        "Sergey Ioffe",
        "Ian Fischer",
        "Joshua V. Dillon"
      ],
      "published": "2022-11-18T02:00:17Z",
      "updated": "2023-04-09T19:15:39Z",
      "categories": [
        "cs.LG",
        "cs.AI",
        "stat.ML"
      ],
      "pdf_url": "https://arxiv.org/pdf/2211.09981v3",
      "landing_url": "https://arxiv.org/abs/2211.09981v3",
      "doi": "https://doi.org/10.48550/arXiv.2211.09981"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The paper discusses ensemble methods for self-supervised learning in vision and does not involve discrete audio/speech tokenization, so it does not meet the required inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The paper discusses ensemble methods for self-supervised learning in vision and does not involve discrete audio/speech tokenization, so it does not meet the required inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on self-supervised learning ensemble methods for image-based tasks (ImageNet-1K) and does not mention discrete audio/speech tokenization, nor does it address audio signals or tokenization for audio reconstruction or understanding, which are required by the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on self-supervised learning ensemble methods for image-based tasks (ImageNet-1K) and does not mention discrete audio/speech tokenization, nor does it address audio signals or tokenization for audio reconstruction or understanding, which are required by the inclusion criteria.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "The Monocular Depth Estimation Challenge",
    "abstract": "This paper summarizes the results of the first Monocular Depth Estimation Challenge (MDEC) organized at WACV2023. This challenge evaluated the progress of self-supervised monocular depth estimation on the challenging SYNS-Patches dataset. The challenge was organized on CodaLab and received submissions from 4 valid teams. Participants were provided a devkit containing updated reference implementations for 16 State-of-the-Art algorithms and 4 novel techniques. The threshold for acceptance for novel techniques was to outperform every one of the 16 SotA baselines. All participants outperformed the baseline in traditional metrics such as MAE or AbsRel. However, pointcloud reconstruction metrics were challenging to improve upon. We found predictions were characterized by interpolation artefacts at object boundaries and errors in relative object positioning. We hope this challenge is a valuable contribution to the community and encourage authors to participate in future editions.",
    "metadata": {
      "arxiv_id": "2211.12174",
      "title": "The Monocular Depth Estimation Challenge",
      "summary": "This paper summarizes the results of the first Monocular Depth Estimation Challenge (MDEC) organized at WACV2023. This challenge evaluated the progress of self-supervised monocular depth estimation on the challenging SYNS-Patches dataset. The challenge was organized on CodaLab and received submissions from 4 valid teams. Participants were provided a devkit containing updated reference implementations for 16 State-of-the-Art algorithms and 4 novel techniques. The threshold for acceptance for novel techniques was to outperform every one of the 16 SotA baselines. All participants outperformed the baseline in traditional metrics such as MAE or AbsRel. However, pointcloud reconstruction metrics were challenging to improve upon. We found predictions were characterized by interpolation artefacts at object boundaries and errors in relative object positioning. We hope this challenge is a valuable contribution to the community and encourage authors to participate in future editions.",
      "authors": [
        "Jaime Spencer",
        "C. Stella Qian",
        "Chris Russell",
        "Simon Hadfield",
        "Erich Graf",
        "Wendy Adams",
        "Andrew J. Schofield",
        "James Elder",
        "Richard Bowden",
        "Heng Cong",
        "Stefano Mattoccia",
        "Matteo Poggi",
        "Zeeshan Khan Suri",
        "Yang Tang",
        "Fabio Tosi",
        "Hao Wang",
        "Youmin Zhang",
        "Yusheng Zhang",
        "Chaoqiang Zhao"
      ],
      "published": "2022-11-22T11:04:15Z",
      "updated": "2022-11-22T11:04:15Z",
      "categories": [
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2211.12174v1",
      "landing_url": "https://arxiv.org/abs/2211.12174v1",
      "doi": "https://doi.org/10.48550/arXiv.2211.12174"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The paper investigates monocular depth estimation in computer vision and contains no discussion of discrete audio tokenization or speech token representations.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The paper investigates monocular depth estimation in computer vision and contains no discussion of discrete audio tokenization or speech token representations.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper focuses on monocular depth estimation and evaluates image-based techniques; it does not discuss any aspect of audio or speech tokenization, discrete audio symbols, or related methods, thus it does not meet the inclusion criteria related to audio tokenization and is not relevant to the review scope.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper focuses on monocular depth estimation and evaluates image-based techniques; it does not discuss any aspect of audio or speech tokenization, discrete audio symbols, or related methods, thus it does not meet the inclusion criteria related to audio tokenization and is not relevant to the review scope.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Global $k$-means$++$: an effective relaxation of the global $k$-means clustering algorithm",
    "abstract": "The $k$-means algorithm is a prevalent clustering method due to its simplicity, effectiveness, and speed. However, its main disadvantage is its high sensitivity to the initial positions of the cluster centers. The global $k$-means is a deterministic algorithm proposed to tackle the random initialization problem of k-means but its well-known that requires high computational cost. It partitions the data to $K$ clusters by solving all $k$-means sub-problems incrementally for all $k=1,\\ldots, K$. For each $k$ cluster problem, the method executes the $k$-means algorithm $N$ times, where $N$ is the number of datapoints. In this paper, we propose the \\emph{global $k$-means\\texttt{++}} clustering algorithm, which is an effective way of acquiring quality clustering solutions akin to those of global $k$-means with a reduced computational load. This is achieved by exploiting the center selection probability that is effectively used in the $k$-means\\texttt{++} algorithm. The proposed method has been tested and compared in various benchmark datasets yielding very satisfactory results in terms of clustering quality and execution speed.",
    "metadata": {
      "arxiv_id": "2211.12271",
      "title": "Global $k$-means$++$: an effective relaxation of the global $k$-means clustering algorithm",
      "summary": "The $k$-means algorithm is a prevalent clustering method due to its simplicity, effectiveness, and speed. However, its main disadvantage is its high sensitivity to the initial positions of the cluster centers. The global $k$-means is a deterministic algorithm proposed to tackle the random initialization problem of k-means but its well-known that requires high computational cost. It partitions the data to $K$ clusters by solving all $k$-means sub-problems incrementally for all $k=1,\\ldots, K$. For each $k$ cluster problem, the method executes the $k$-means algorithm $N$ times, where $N$ is the number of datapoints. In this paper, we propose the \\emph{global $k$-means\\texttt{++}} clustering algorithm, which is an effective way of acquiring quality clustering solutions akin to those of global $k$-means with a reduced computational load. This is achieved by exploiting the center selection probability that is effectively used in the $k$-means\\texttt{++} algorithm. The proposed method has been tested and compared in various benchmark datasets yielding very satisfactory results in terms of clustering quality and execution speed.",
      "authors": [
        "Georgios Vardakas",
        "Aristidis Likas"
      ],
      "published": "2022-11-22T13:42:53Z",
      "updated": "2023-07-14T11:39:36Z",
      "categories": [
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2211.12271v3",
      "landing_url": "https://arxiv.org/abs/2211.12271v3",
      "doi": "https://doi.org/10.48550/arXiv.2211.12271"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The abstract discusses a generic clustering algorithm (global k-means++) and its computational efficiency, with no discrete audio/speech tokenization or analysis of discrete audio tokens, so it does not meet the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The abstract discusses a generic clustering algorithm (global k-means++) and its computational efficiency, with no discrete audio/speech tokenization or analysis of discrete audio tokens, so it does not meet the inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on clustering algorithms (global k-means++), which is unrelated to audio or speech tokenization, discrete audio symbol quantization, or token sequences for audio tasks as required by the inclusion criteria. There is no mention of audio or speech data, discrete tokenization, or neural quantization-based techniques relevant to audio language modeling; thus, it does not meet the inclusion criteria and falls outside the scope described.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on clustering algorithms (global k-means++), which is unrelated to audio or speech tokenization, discrete audio symbol quantization, or token sequences for audio tasks as required by the inclusion criteria. There is no mention of audio or speech data, discrete tokenization, or neural quantization-based techniques relevant to audio language modeling; thus, it does not meet the inclusion criteria and falls outside the scope described.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Privileged Prior Information Distillation for Image Matting",
    "abstract": "Performance of trimap-free image matting methods is limited when trying to decouple the deterministic and undetermined regions, especially in the scenes where foregrounds are semantically ambiguous, chromaless, or high transmittance. In this paper, we propose a novel framework named Privileged Prior Information Distillation for Image Matting (PPID-IM) that can effectively transfer privileged prior environment-aware information to improve the performance of students in solving hard foregrounds. The prior information of trimap regulates only the teacher model during the training stage, while not being fed into the student network during actual inference. In order to achieve effective privileged cross-modality (i.e. trimap and RGB) information distillation, we introduce a Cross-Level Semantic Distillation (CLSD) module that reinforces the trimap-free students with more knowledgeable semantic representations and environment-aware information. We also propose an Attention-Guided Local Distillation module that efficiently transfers privileged local attributes from the trimap-based teacher to trimap-free students for the guidance of local-region optimization. Extensive experiments demonstrate the effectiveness and superiority of our PPID framework on the task of image matting. In addition, our trimap-free IndexNet-PPID surpasses the other competing state-of-the-art methods by a large margin, especially in scenarios with chromaless, weak texture, or irregular objects.",
    "metadata": {
      "arxiv_id": "2211.14036",
      "title": "Privileged Prior Information Distillation for Image Matting",
      "summary": "Performance of trimap-free image matting methods is limited when trying to decouple the deterministic and undetermined regions, especially in the scenes where foregrounds are semantically ambiguous, chromaless, or high transmittance. In this paper, we propose a novel framework named Privileged Prior Information Distillation for Image Matting (PPID-IM) that can effectively transfer privileged prior environment-aware information to improve the performance of students in solving hard foregrounds. The prior information of trimap regulates only the teacher model during the training stage, while not being fed into the student network during actual inference. In order to achieve effective privileged cross-modality (i.e. trimap and RGB) information distillation, we introduce a Cross-Level Semantic Distillation (CLSD) module that reinforces the trimap-free students with more knowledgeable semantic representations and environment-aware information. We also propose an Attention-Guided Local Distillation module that efficiently transfers privileged local attributes from the trimap-based teacher to trimap-free students for the guidance of local-region optimization. Extensive experiments demonstrate the effectiveness and superiority of our PPID framework on the task of image matting. In addition, our trimap-free IndexNet-PPID surpasses the other competing state-of-the-art methods by a large margin, especially in scenarios with chromaless, weak texture, or irregular objects.",
      "authors": [
        "Cheng Lyu",
        "Jiake Xie",
        "Bo Xu",
        "Cheng Lu",
        "Han Huang",
        "Xin Huang",
        "Ming Wu",
        "Chuang Zhang",
        "Yong Tang"
      ],
      "published": "2022-11-25T11:24:04Z",
      "updated": "2022-11-25T11:24:04Z",
      "categories": [
        "cs.CV",
        "cs.MM"
      ],
      "pdf_url": "https://arxiv.org/pdf/2211.14036v1",
      "landing_url": "https://arxiv.org/abs/2211.14036v1",
      "doi": "https://doi.org/10.48550/arXiv.2211.14036"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This item focuses on image matting and cross-modality distillation in vision, with no discrete audio tokenization analysis or token design; it does not meet the audio-tokenization inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "This item focuses on image matting and cross-modality distillation in vision, with no discrete audio tokenization analysis or token design; it does not meet the audio-tokenization inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on image matting, a computer vision task involving foreground-background separation in images, and does not address audio or speech tokenization, discrete audio token design, or analysis relevant to the given inclusion criteria. It is also unrelated to audio signal processing or neural discrete token representations for audio reconstruction or understanding.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on image matting, a computer vision task involving foreground-background separation in images, and does not address audio or speech tokenization, discrete audio token design, or analysis relevant to the given inclusion criteria. It is also unrelated to audio signal processing or neural discrete token representations for audio reconstruction or understanding.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Homology-constrained vector quantization entropy regularizer",
    "abstract": "This paper describes an entropy regularization term for vector quantization (VQ) based on the analysis of persistent homology of the VQ embeddings. Higher embedding entropy positively correlates with higher codebook utilization, mitigating overfit towards the identity and codebook collapse in VQ-based autoencoders [1]. We show that homology-constrained regularization is an effective way to increase entropy of the VQ process (approximated to input entropy) while preserving the approximated topology in the quantized latent space, averaged over mini batches. This work further explores some patterns of persistent homology diagrams of latents formed by vector quantization. We implement and test the proposed algorithm as a module integrated into a sample VQ-VAE. Linked code repository provides a functioning implementation of the proposed architecture, referred to as homology-constrained vector quantization (HC-VQ) further in this work.",
    "metadata": {
      "arxiv_id": "2211.14363",
      "title": "Homology-constrained vector quantization entropy regularizer",
      "summary": "This paper describes an entropy regularization term for vector quantization (VQ) based on the analysis of persistent homology of the VQ embeddings. Higher embedding entropy positively correlates with higher codebook utilization, mitigating overfit towards the identity and codebook collapse in VQ-based autoencoders [1]. We show that homology-constrained regularization is an effective way to increase entropy of the VQ process (approximated to input entropy) while preserving the approximated topology in the quantized latent space, averaged over mini batches. This work further explores some patterns of persistent homology diagrams of latents formed by vector quantization. We implement and test the proposed algorithm as a module integrated into a sample VQ-VAE. Linked code repository provides a functioning implementation of the proposed architecture, referred to as homology-constrained vector quantization (HC-VQ) further in this work.",
      "authors": [
        "Ivan Volkov"
      ],
      "published": "2022-11-25T20:09:22Z",
      "updated": "2022-11-25T20:09:22Z",
      "categories": [
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2211.14363v1",
      "landing_url": "https://arxiv.org/abs/2211.14363v1",
      "doi": "https://doi.org/10.48550/arXiv.2211.14363"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "Although the work discusses discrete codebooks and VQ-VAE, the abstract does not explicitly address discrete audio/speech tokenization as its core focus; it appears to be a general VQ entropy regularization method not specific to audio tokens.",
      "evaluation": 2
    },
    "round-A_JuniorNano_reasoning": "Although the work discusses discrete codebooks and VQ-VAE, the abstract does not explicitly address discrete audio/speech tokenization as its core focus; it appears to be a general VQ entropy regularization method not specific to audio tokens.",
    "round-A_JuniorNano_evaluation": 2,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper presents a methodological contribution involving vector quantization embeddings and entropy regularization, but it does not explicitly mention discrete audio or speech tokenization nor demonstrate applications or evaluations specifically related to discrete audio token sequences for speech tasks, thus failing to meet the core inclusion criterion.",
      "evaluation": 2
    },
    "round-A_JuniorMini_reasoning": "The paper presents a methodological contribution involving vector quantization embeddings and entropy regularization, but it does not explicitly mention discrete audio or speech tokenization nor demonstrate applications or evaluations specifically related to discrete audio token sequences for speech tasks, thus failing to meet the core inclusion criterion.",
    "round-A_JuniorMini_evaluation": 2,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:2)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:2)"
  },
  {
    "title": "On the Global Solution of Soft k-Means",
    "abstract": "This paper presents an algorithm to solve the Soft k-Means problem globally. Unlike Fuzzy c-Means, Soft k-Means (SkM) has a matrix factorization-type objective and has been shown to have a close relation with the popular probability decomposition-type clustering methods, e.g., Left Stochastic Clustering (LSC). Though some work has been done for solving the Soft k-Means problem, they usually use an alternating minimization scheme or the projected gradient descent method, which cannot guarantee global optimality since the non-convexity of SkM. In this paper, we present a sufficient condition for a feasible solution of Soft k-Means problem to be globally optimal and show the output of the proposed algorithm satisfies it. Moreover, for the Soft k-Means problem, we provide interesting discussions on stability, solutions non-uniqueness, and connection with LSC. Then, a new model, named Minimal Volume Soft k-Means (MVSkM), is proposed to address the solutions non-uniqueness issue. Finally, experimental results support our theoretical results.",
    "metadata": {
      "arxiv_id": "2212.03589",
      "title": "On the Global Solution of Soft k-Means",
      "summary": "This paper presents an algorithm to solve the Soft k-Means problem globally. Unlike Fuzzy c-Means, Soft k-Means (SkM) has a matrix factorization-type objective and has been shown to have a close relation with the popular probability decomposition-type clustering methods, e.g., Left Stochastic Clustering (LSC). Though some work has been done for solving the Soft k-Means problem, they usually use an alternating minimization scheme or the projected gradient descent method, which cannot guarantee global optimality since the non-convexity of SkM. In this paper, we present a sufficient condition for a feasible solution of Soft k-Means problem to be globally optimal and show the output of the proposed algorithm satisfies it. Moreover, for the Soft k-Means problem, we provide interesting discussions on stability, solutions non-uniqueness, and connection with LSC. Then, a new model, named Minimal Volume Soft k-Means (MVSkM), is proposed to address the solutions non-uniqueness issue. Finally, experimental results support our theoretical results.",
      "authors": [
        "Feiping Nie",
        "Hong Chen",
        "Rong Wang",
        "Xuelong Li"
      ],
      "published": "2022-12-07T12:06:55Z",
      "updated": "2022-12-07T12:06:55Z",
      "categories": [
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2212.03589v1",
      "landing_url": "https://arxiv.org/abs/2212.03589v1",
      "doi": "https://doi.org/10.48550/arXiv.2212.03589"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item addresses a general optimization method for Soft k-Means clustering and does not involve discrete audio/speech tokenization or discrete token representations for audio; thus it does not meet the inclusion criteria and is not excluded by any specific exclusion beyond irrelevance to tokenization.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item addresses a general optimization method for Soft k-Means clustering and does not involve discrete audio/speech tokenization or discrete token representations for audio; thus it does not meet the inclusion criteria and is not excluded by any specific exclusion beyond irrelevance to tokenization.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper focuses on the Soft k-Means clustering algorithm and its global optimization, with no mention of discrete audio or speech tokenization, quantization, or any related analysis; it is purely about clustering methods without relevance to audio tokenization as defined by the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper focuses on the Soft k-Means clustering algorithm and its global optimization, with no mention of discrete audio or speech tokenization, quantization, or any related analysis; it is purely about clustering methods without relevance to audio tokenization as defined by the inclusion criteria.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "BEATs: Audio Pre-Training with Acoustic Tokenizers",
    "abstract": "The massive growth of self-supervised learning (SSL) has been witnessed in language, vision, speech, and audio domains over the past few years. While discrete label prediction is widely adopted for other modalities, the state-of-the-art audio SSL models still employ reconstruction loss for pre-training. Compared with reconstruction loss, semantic-rich discrete label prediction encourages the SSL model to abstract the high-level audio semantics and discard the redundant details as in human perception. However, a semantic-rich acoustic tokenizer for general audio pre-training is usually not straightforward to obtain, due to the continuous property of audio and unavailable phoneme sequences like speech. To tackle this challenge, we propose BEATs, an iterative audio pre-training framework to learn Bidirectional Encoder representation from Audio Transformers, where an acoustic tokenizer and an audio SSL model are optimized by iterations. In the first iteration, we use random projection as the acoustic tokenizer to train an audio SSL model in a mask and label prediction manner. Then, we train an acoustic tokenizer for the next iteration by distilling the semantic knowledge from the pre-trained or fine-tuned audio SSL model. The iteration is repeated with the hope of mutual promotion of the acoustic tokenizer and audio SSL model. The experimental results demonstrate our acoustic tokenizers can generate discrete labels with rich audio semantics and our audio SSL models achieve state-of-the-art results across various audio classification benchmarks, even outperforming previous models that use more training data and model parameters significantly. Specifically, we set a new state-of-the-art mAP 50.6% on AudioSet-2M for audio-only models without using any external data, and 98.1% accuracy on ESC-50. The code and pre-trained models are available at https://aka.ms/beats.",
    "metadata": {
      "arxiv_id": "2212.09058",
      "title": "BEATs: Audio Pre-Training with Acoustic Tokenizers",
      "summary": "The massive growth of self-supervised learning (SSL) has been witnessed in language, vision, speech, and audio domains over the past few years. While discrete label prediction is widely adopted for other modalities, the state-of-the-art audio SSL models still employ reconstruction loss for pre-training. Compared with reconstruction loss, semantic-rich discrete label prediction encourages the SSL model to abstract the high-level audio semantics and discard the redundant details as in human perception. However, a semantic-rich acoustic tokenizer for general audio pre-training is usually not straightforward to obtain, due to the continuous property of audio and unavailable phoneme sequences like speech. To tackle this challenge, we propose BEATs, an iterative audio pre-training framework to learn Bidirectional Encoder representation from Audio Transformers, where an acoustic tokenizer and an audio SSL model are optimized by iterations. In the first iteration, we use random projection as the acoustic tokenizer to train an audio SSL model in a mask and label prediction manner. Then, we train an acoustic tokenizer for the next iteration by distilling the semantic knowledge from the pre-trained or fine-tuned audio SSL model. The iteration is repeated with the hope of mutual promotion of the acoustic tokenizer and audio SSL model. The experimental results demonstrate our acoustic tokenizers can generate discrete labels with rich audio semantics and our audio SSL models achieve state-of-the-art results across various audio classification benchmarks, even outperforming previous models that use more training data and model parameters significantly. Specifically, we set a new state-of-the-art mAP 50.6% on AudioSet-2M for audio-only models without using any external data, and 98.1% accuracy on ESC-50. The code and pre-trained models are available at https://aka.ms/beats.",
      "authors": [
        "Sanyuan Chen",
        "Yu Wu",
        "Chengyi Wang",
        "Shujie Liu",
        "Daniel Tompkins",
        "Zhuo Chen",
        "Furu Wei"
      ],
      "published": "2022-12-18T10:41:55Z",
      "updated": "2022-12-18T10:41:55Z",
      "categories": [
        "eess.AS",
        "cs.AI",
        "cs.CL",
        "cs.LG",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2212.09058v1",
      "landing_url": "https://arxiv.org/abs/2212.09058v1",
      "doi": "https://doi.org/10.48550/arXiv.2212.09058"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The paper introduces BEATs, an iterative framework that creates and uses an acoustic tokenizer to generate discrete tokens for audio pre-training, analyzes their semantic usefulness and demonstrates state-of-the-art results, thus directly addressing discrete audio tokenization as the core object of study.",
      "evaluation": 5
    },
    "round-A_JuniorNano_reasoning": "The paper introduces BEATs, an iterative framework that creates and uses an acoustic tokenizer to generate discrete tokens for audio pre-training, analyzes their semantic usefulness and demonstrates state-of-the-art results, thus directly addressing discrete audio tokenization as the core object of study.",
    "round-A_JuniorNano_evaluation": 5,
    "round-A_JuniorMini_output": {
      "reasoning": "The study proposes an iterative audio pre-training framework involving an acoustic tokenizer that generates discrete labels representing audio semantics, fulfilling the core inclusion criterion of discrete audio tokenization for understanding tasks; it provides experimental validation with state-of-the-art results, is in English, and actively analyses discrete tokens rather than only using continuous features or being an engineering report, aligning well with inclusion criteria and avoiding all listed exclusions.",
      "evaluation": 5
    },
    "round-A_JuniorMini_reasoning": "The study proposes an iterative audio pre-training framework involving an acoustic tokenizer that generates discrete labels representing audio semantics, fulfilling the core inclusion criterion of discrete audio tokenization for understanding tasks; it provides experimental validation with state-of-the-art results, is in English, and actively analyses discrete tokens rather than only using continuous features or being an engineering report, aligning well with inclusion criteria and avoiding all listed exclusions.",
    "round-A_JuniorMini_evaluation": 5,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "include (junior:5)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "FileDAG: A Multi-Version Decentralized Storage Network Built on DAG-based Blockchain",
    "abstract": "Decentralized Storage Networks (DSNs) can gather storage resources from mutually untrusted providers and form worldwide decentralized file systems. Compared to traditional storage networks, DSNs are built on top of blockchains, which can incentivize service providers and ensure strong security. However, existing DSNs face two major challenges. First, deduplication can only be achieved at the directory-level. Missing file-level deduplication leads to unavoidable extra storage and bandwidth cost. Second, current DSNs realize file indexing by storing extra metadata while blockchain ledgers are not fully exploited. To overcome these problems, we propose FileDAG, a DSN built on DAG-based blockchain to support file-level deduplication in storing multi-versioned files. When updating files, we adopt an increment generation method to calculate and store only the increments instead of the entire updated files. Besides, we introduce a two-layer DAG-based blockchain ledger, by which FileDAG can provide flexible and storage-saving file indexing by directly using the blockchain database without incurring extra storage overhead. We implement FileDAG and evaluate its performance with extensive experiments. The results demonstrate that FileDAG outperforms the state-of-the-art industrial DSNs considering storage cost and latency.",
    "metadata": {
      "arxiv_id": "2212.09096",
      "title": "FileDAG: A Multi-Version Decentralized Storage Network Built on DAG-based Blockchain",
      "summary": "Decentralized Storage Networks (DSNs) can gather storage resources from mutually untrusted providers and form worldwide decentralized file systems. Compared to traditional storage networks, DSNs are built on top of blockchains, which can incentivize service providers and ensure strong security. However, existing DSNs face two major challenges. First, deduplication can only be achieved at the directory-level. Missing file-level deduplication leads to unavoidable extra storage and bandwidth cost. Second, current DSNs realize file indexing by storing extra metadata while blockchain ledgers are not fully exploited. To overcome these problems, we propose FileDAG, a DSN built on DAG-based blockchain to support file-level deduplication in storing multi-versioned files. When updating files, we adopt an increment generation method to calculate and store only the increments instead of the entire updated files. Besides, we introduce a two-layer DAG-based blockchain ledger, by which FileDAG can provide flexible and storage-saving file indexing by directly using the blockchain database without incurring extra storage overhead. We implement FileDAG and evaluate its performance with extensive experiments. The results demonstrate that FileDAG outperforms the state-of-the-art industrial DSNs considering storage cost and latency.",
      "authors": [
        "Hechuan Guo",
        "Minghui Xu",
        "Jiahao Zhang",
        "Chunchi Liu",
        "Dongxiao Yu",
        "Schahram Dustdar",
        "Xiuzhen Cheng"
      ],
      "published": "2022-12-18T14:40:52Z",
      "updated": "2022-12-18T14:40:52Z",
      "categories": [
        "cs.DC"
      ],
      "pdf_url": "https://arxiv.org/pdf/2212.09096v1",
      "landing_url": "https://arxiv.org/abs/2212.09096v1",
      "doi": "https://doi.org/10.48550/arXiv.2212.09096"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The manuscript focuses on decentralized storage networks and blockchain-based file indexing; it does not involve discrete audio/speech tokenization or token designs for language tasks.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The manuscript focuses on decentralized storage networks and blockchain-based file indexing; it does not involve discrete audio/speech tokenization or token designs for language tasks.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study proposes FileDAG, a decentralized storage network focused on blockchain-based file storage optimization, without any mention of audio or speech tokenization, discrete audio symbols, or related audio processing tasks, and thus does not meet the inclusion criteria for discrete audio/speech tokenization studies.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study proposes FileDAG, a decentralized storage network focused on blockchain-based file storage optimization, without any mention of audio or speech tokenization, discrete audio symbols, or related audio processing tasks, and thus does not meet the inclusion criteria for discrete audio/speech tokenization studies.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Similarity Contrastive Estimation for Image and Video Soft Contrastive Self-Supervised Learning",
    "abstract": "Contrastive representation learning has proven to be an effective self-supervised learning method for images and videos. Most successful approaches are based on Noise Contrastive Estimation (NCE) and use different views of an instance as positives that should be contrasted with other instances, called negatives, that are considered as noise. However, several instances in a dataset are drawn from the same distribution and share underlying semantic information. A good data representation should contain relations between the instances, or semantic similarity and dissimilarity, that contrastive learning harms by considering all negatives as noise. To circumvent this issue, we propose a novel formulation of contrastive learning using semantic similarity between instances called Similarity Contrastive Estimation (SCE). Our training objective is a soft contrastive one that brings the positives closer and estimates a continuous distribution to push or pull negative instances based on their learned similarities. We validate empirically our approach on both image and video representation learning. We show that SCE performs competitively with the state of the art on the ImageNet linear evaluation protocol for fewer pretraining epochs and that it generalizes to several downstream image tasks. We also show that SCE reaches state-of-the-art results for pretraining video representation and that the learned representation can generalize to video downstream tasks.",
    "metadata": {
      "arxiv_id": "2212.11187",
      "title": "Similarity Contrastive Estimation for Image and Video Soft Contrastive Self-Supervised Learning",
      "summary": "Contrastive representation learning has proven to be an effective self-supervised learning method for images and videos. Most successful approaches are based on Noise Contrastive Estimation (NCE) and use different views of an instance as positives that should be contrasted with other instances, called negatives, that are considered as noise. However, several instances in a dataset are drawn from the same distribution and share underlying semantic information. A good data representation should contain relations between the instances, or semantic similarity and dissimilarity, that contrastive learning harms by considering all negatives as noise. To circumvent this issue, we propose a novel formulation of contrastive learning using semantic similarity between instances called Similarity Contrastive Estimation (SCE). Our training objective is a soft contrastive one that brings the positives closer and estimates a continuous distribution to push or pull negative instances based on their learned similarities. We validate empirically our approach on both image and video representation learning. We show that SCE performs competitively with the state of the art on the ImageNet linear evaluation protocol for fewer pretraining epochs and that it generalizes to several downstream image tasks. We also show that SCE reaches state-of-the-art results for pretraining video representation and that the learned representation can generalize to video downstream tasks.",
      "authors": [
        "Julien Denize",
        "Jaonary Rabarisoa",
        "Astrid Orcesi",
        "Romain Hérault"
      ],
      "published": "2022-12-21T16:56:55Z",
      "updated": "2022-12-21T16:56:55Z",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2212.11187v1",
      "landing_url": "https://arxiv.org/abs/2212.11187v1",
      "doi": "https://doi.org/10.1007/s00138-023-01444-9"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "Reasoning: The article focuses on image and video self-supervised learning with no discrete audio tokenization, so it does not meet the discrete-audio inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "Reasoning: The article focuses on image and video self-supervised learning with no discrete audio tokenization, so it does not meet the discrete-audio inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on contrastive learning methods for image and video representation learning and does not involve audio or speech tokenization, nor does it address discrete tokenization of continuous audio signals as required by the inclusion criteria. Therefore, it does not meet the inclusion criteria surrounding discrete audio tokenization.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on contrastive learning methods for image and video representation learning and does not involve audio or speech tokenization, nor does it address discrete tokenization of continuous audio signals as required by the inclusion criteria. Therefore, it does not meet the inclusion criteria surrounding discrete audio tokenization.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Offline Clustering Approach to Self-supervised Learning for Class-imbalanced Image Data",
    "abstract": "Class-imbalanced datasets are known to cause the problem of model being biased towards the majority classes. In this project, we set up two research questions: 1) when is the class-imbalance problem more prevalent in self-supervised pre-training? and 2) can offline clustering of feature representations help pre-training on class-imbalanced data? Our experiments investigate the former question by adjusting the degree of {\\it class-imbalance} when training the baseline models, namely SimCLR and SimSiam on CIFAR-10 database. To answer the latter question, we train each expert model on each subset of the feature clusters. We then distill the knowledge of expert models into a single model, so that we will be able to compare the performance of this model to our baselines.",
    "metadata": {
      "arxiv_id": "2212.11444",
      "title": "Offline Clustering Approach to Self-supervised Learning for Class-imbalanced Image Data",
      "summary": "Class-imbalanced datasets are known to cause the problem of model being biased towards the majority classes. In this project, we set up two research questions: 1) when is the class-imbalance problem more prevalent in self-supervised pre-training? and 2) can offline clustering of feature representations help pre-training on class-imbalanced data? Our experiments investigate the former question by adjusting the degree of {\\it class-imbalance} when training the baseline models, namely SimCLR and SimSiam on CIFAR-10 database. To answer the latter question, we train each expert model on each subset of the feature clusters. We then distill the knowledge of expert models into a single model, so that we will be able to compare the performance of this model to our baselines.",
      "authors": [
        "Hye-min Chang",
        "Sungkyun Chang"
      ],
      "published": "2022-12-22T01:26:38Z",
      "updated": "2022-12-22T01:26:38Z",
      "categories": [
        "cs.LG",
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2212.11444v1",
      "landing_url": "https://arxiv.org/abs/2212.11444v1",
      "doi": "https://doi.org/10.48550/arXiv.2212.11444"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The article concerns image-based self-supervised learning with clustering for class-imbalanced data and does not involve discrete audio/speech tokenization or any discrete token design related to audio; it fails the audio/tokenization inclusion and is not applicable.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The article concerns image-based self-supervised learning with clustering for class-imbalanced data and does not involve discrete audio/speech tokenization or any discrete token design related to audio; it fails the audio/tokenization inclusion and is not applicable.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on self-supervised learning and clustering approaches for class-imbalanced image data, without any mention of audio or speech tokenization, discrete audio tokens, or related audio signal processing tasks, which are the core criteria for inclusion.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on self-supervised learning and clustering approaches for class-imbalanced image data, without any mention of audio or speech tokenization, discrete audio tokens, or related audio signal processing tasks, which are the core criteria for inclusion.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "GEDI: GEnerative and DIscriminative Training for Self-Supervised Learning",
    "abstract": "Self-supervised learning is a popular and powerful method for utilizing large amounts of unlabeled data, for which a wide variety of training objectives have been proposed in the literature. In this study, we perform a Bayesian analysis of state-of-the-art self-supervised learning objectives and propose a unified formulation based on likelihood learning. Our analysis suggests a simple method for integrating self-supervised learning with generative models, allowing for the joint training of these two seemingly distinct approaches. We refer to this combined framework as GEDI, which stands for GEnerative and DIscriminative training. Additionally, we demonstrate an instantiation of the GEDI framework by integrating an energy-based model with a cluster-based self-supervised learning model. Through experiments on synthetic and real-world data, including SVHN, CIFAR10, and CIFAR100, we show that GEDI outperforms existing self-supervised learning strategies in terms of clustering performance by a wide margin. We also demonstrate that GEDI can be integrated into a neural-symbolic framework to address tasks in the small data regime, where it can use logical constraints to further improve clustering and classification performance.",
    "metadata": {
      "arxiv_id": "2212.13425",
      "title": "GEDI: GEnerative and DIscriminative Training for Self-Supervised Learning",
      "summary": "Self-supervised learning is a popular and powerful method for utilizing large amounts of unlabeled data, for which a wide variety of training objectives have been proposed in the literature. In this study, we perform a Bayesian analysis of state-of-the-art self-supervised learning objectives and propose a unified formulation based on likelihood learning. Our analysis suggests a simple method for integrating self-supervised learning with generative models, allowing for the joint training of these two seemingly distinct approaches. We refer to this combined framework as GEDI, which stands for GEnerative and DIscriminative training. Additionally, we demonstrate an instantiation of the GEDI framework by integrating an energy-based model with a cluster-based self-supervised learning model. Through experiments on synthetic and real-world data, including SVHN, CIFAR10, and CIFAR100, we show that GEDI outperforms existing self-supervised learning strategies in terms of clustering performance by a wide margin. We also demonstrate that GEDI can be integrated into a neural-symbolic framework to address tasks in the small data regime, where it can use logical constraints to further improve clustering and classification performance.",
      "authors": [
        "Emanuele Sansone",
        "Robin Manhaeve"
      ],
      "published": "2022-12-27T09:33:50Z",
      "updated": "2023-02-07T04:19:54Z",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2212.13425v4",
      "landing_url": "https://arxiv.org/abs/2212.13425v4",
      "doi": "https://doi.org/10.48550/arXiv.2212.13425"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The article investigates generative and discriminative training for self-supervised learning in vision with no discrete audio/speech tokenization analyzed or proposed.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The article investigates generative and discriminative training for self-supervised learning in vision with no discrete audio/speech tokenization analyzed or proposed.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on self-supervised learning methods integrating generative and discriminative training, evaluated on visual datasets (SVHN, CIFAR10, CIFAR100) with no mention of audio or speech tokenization, discrete audio symbols, or quantization-based token representations; thus, it does not meet the inclusion criteria focused on discrete audio/speech tokenization nor address the exclusion criteria relevant to audio tokenization studies.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on self-supervised learning methods integrating generative and discriminative training, evaluated on visual datasets (SVHN, CIFAR10, CIFAR100) with no mention of audio or speech tokenization, discrete audio symbols, or quantization-based token representations; thus, it does not meet the inclusion criteria focused on discrete audio/speech tokenization nor address the exclusion criteria relevant to audio tokenization studies.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Neural Codec Language Models are Zero-Shot Text to Speech Synthesizers",
    "abstract": "We introduce a language modeling approach for text to speech synthesis (TTS). Specifically, we train a neural codec language model (called Vall-E) using discrete codes derived from an off-the-shelf neural audio codec model, and regard TTS as a conditional language modeling task rather than continuous signal regression as in previous work. During the pre-training stage, we scale up the TTS training data to 60K hours of English speech which is hundreds of times larger than existing systems. Vall-E emerges in-context learning capabilities and can be used to synthesize high-quality personalized speech with only a 3-second enrolled recording of an unseen speaker as an acoustic prompt. Experiment results show that Vall-E significantly outperforms the state-of-the-art zero-shot TTS system in terms of speech naturalness and speaker similarity. In addition, we find Vall-E could preserve the speaker's emotion and acoustic environment of the acoustic prompt in synthesis. See https://aka.ms/valle for demos of our work.",
    "metadata": {
      "arxiv_id": "2301.02111",
      "title": "Neural Codec Language Models are Zero-Shot Text to Speech Synthesizers",
      "summary": "We introduce a language modeling approach for text to speech synthesis (TTS). Specifically, we train a neural codec language model (called Vall-E) using discrete codes derived from an off-the-shelf neural audio codec model, and regard TTS as a conditional language modeling task rather than continuous signal regression as in previous work. During the pre-training stage, we scale up the TTS training data to 60K hours of English speech which is hundreds of times larger than existing systems. Vall-E emerges in-context learning capabilities and can be used to synthesize high-quality personalized speech with only a 3-second enrolled recording of an unseen speaker as an acoustic prompt. Experiment results show that Vall-E significantly outperforms the state-of-the-art zero-shot TTS system in terms of speech naturalness and speaker similarity. In addition, we find Vall-E could preserve the speaker's emotion and acoustic environment of the acoustic prompt in synthesis. See https://aka.ms/valle for demos of our work.",
      "authors": [
        "Chengyi Wang",
        "Sanyuan Chen",
        "Yu Wu",
        "Ziqiang Zhang",
        "Long Zhou",
        "Shujie Liu",
        "Zhuo Chen",
        "Yanqing Liu",
        "Huaming Wang",
        "Jinyu Li",
        "Lei He",
        "Sheng Zhao",
        "Furu Wei"
      ],
      "published": "2023-01-05T15:37:15Z",
      "updated": "2023-01-05T15:37:15Z",
      "categories": [
        "cs.CL",
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2301.02111v1",
      "landing_url": "https://arxiv.org/abs/2301.02111v1",
      "doi": "https://doi.org/10.48550/arXiv.2301.02111"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The paper explicitly utilizes discrete audio codes derived from a neural codec and models TTS as a conditional language model over those discrete tokens, meeting the discrete-tokenization inclusion and not falling into exclusion categories (continuous features or non-speech tokens).",
      "evaluation": 5
    },
    "round-A_JuniorNano_reasoning": "The paper explicitly utilizes discrete audio codes derived from a neural codec and models TTS as a conditional language model over those discrete tokens, meeting the discrete-tokenization inclusion and not falling into exclusion categories (continuous features or non-speech tokens).",
    "round-A_JuniorNano_evaluation": 5,
    "round-A_JuniorMini_output": {
      "reasoning": "The study explicitly proposes a neural codec language model that uses discrete codes derived from a neural audio codec for text-to-speech synthesis, which aligns with the inclusion criteria requiring discrete audio tokenization as a core object of study; it also involves modern neural quantization-based discrete token representations rather than traditional codecs, thus meeting the inclusion criteria and avoiding the exclusion criteria.",
      "evaluation": 5
    },
    "round-A_JuniorMini_reasoning": "The study explicitly proposes a neural codec language model that uses discrete codes derived from a neural audio codec for text-to-speech synthesis, which aligns with the inclusion criteria requiring discrete audio tokenization as a core object of study; it also involves modern neural quantization-based discrete token representations rather than traditional codecs, thus meeting the inclusion criteria and avoiding the exclusion criteria.",
    "round-A_JuniorMini_evaluation": 5,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "include (junior:5)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Multi-Modal and Multi-Resolution Data Fusion for High-Resolution Cloud Removal: A Novel Baseline and Benchmark",
    "abstract": "Cloud removal is a significant and challenging problem in remote sensing, and in recent years, there have been notable advancements in this area. However, two major issues remain hindering the development of cloud removal: the unavailability of high-resolution imagery for existing datasets and the absence of evaluation regarding the semantic meaningfulness of the generated structures. In this paper, we introduce M3R-CR, a benchmark dataset for high-resolution Cloud Removal with Multi-Modal and Multi-Resolution data fusion. With this dataset, we consider the problem of cloud removal in high-resolution optical remote sensing imagery by integrating multi-modal and multi-resolution information. In this context, we have to take into account the alignment errors caused by the multi-resolution nature, along with the more pronounced misalignment issues in high-resolution images due to inherent imaging mechanism differences and other factors. Existing multi-modal data fusion based methods, which assume the image pairs are aligned accurately at pixel-level, are thus not appropriate for this problem. To this end, we design a new baseline named Align-CR to perform the low-resolution SAR image guided high-resolution optical image cloud removal. It gradually warps and fuses the features of the multi-modal and multi-resolution data during the reconstruction process, effectively mitigating concerns associated with misalignment. In the experiments, we evaluate the performance of cloud removal by analyzing the quality of visually pleasing textures using image reconstruction metrics and further analyze the generation of semantically meaningful structures using a well-established semantic segmentation task. The proposed Align-CR method is superior to other baseline methods in both areas.",
    "metadata": {
      "arxiv_id": "2301.03432",
      "title": "Multi-Modal and Multi-Resolution Data Fusion for High-Resolution Cloud Removal: A Novel Baseline and Benchmark",
      "summary": "Cloud removal is a significant and challenging problem in remote sensing, and in recent years, there have been notable advancements in this area. However, two major issues remain hindering the development of cloud removal: the unavailability of high-resolution imagery for existing datasets and the absence of evaluation regarding the semantic meaningfulness of the generated structures. In this paper, we introduce M3R-CR, a benchmark dataset for high-resolution Cloud Removal with Multi-Modal and Multi-Resolution data fusion. With this dataset, we consider the problem of cloud removal in high-resolution optical remote sensing imagery by integrating multi-modal and multi-resolution information. In this context, we have to take into account the alignment errors caused by the multi-resolution nature, along with the more pronounced misalignment issues in high-resolution images due to inherent imaging mechanism differences and other factors. Existing multi-modal data fusion based methods, which assume the image pairs are aligned accurately at pixel-level, are thus not appropriate for this problem. To this end, we design a new baseline named Align-CR to perform the low-resolution SAR image guided high-resolution optical image cloud removal. It gradually warps and fuses the features of the multi-modal and multi-resolution data during the reconstruction process, effectively mitigating concerns associated with misalignment. In the experiments, we evaluate the performance of cloud removal by analyzing the quality of visually pleasing textures using image reconstruction metrics and further analyze the generation of semantically meaningful structures using a well-established semantic segmentation task. The proposed Align-CR method is superior to other baseline methods in both areas.",
      "authors": [
        "Fang Xu",
        "Yilei Shi",
        "Patrick Ebel",
        "Wen Yang",
        "Xiao Xiang Zhu"
      ],
      "published": "2023-01-09T15:31:28Z",
      "updated": "2024-10-11T05:43:05Z",
      "categories": [
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2301.03432v2",
      "landing_url": "https://arxiv.org/abs/2301.03432v2",
      "doi": "https://doi.org/10.1109/TGRS.2023.3337845"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The article focuses on high-resolution cloud removal in remote sensing using multi-modal/multi-resolution data fusion; there is no discrete audio/speech tokenization analysis, so it should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The article focuses on high-resolution cloud removal in remote sensing using multi-modal/multi-resolution data fusion; there is no discrete audio/speech tokenization analysis, so it should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on cloud removal in high-resolution optical remote sensing imagery using multi-modal data fusion, without any mention or focus on discrete audio/speech tokenization or related concepts. It does not address converting continuous audio signals into sequences of discrete symbols, nor does it involve any audio tokenization or speech analysis relevant to the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on cloud removal in high-resolution optical remote sensing imagery using multi-modal data fusion, without any mention or focus on discrete audio/speech tokenization or related concepts. It does not address converting continuous audio signals into sequences of discrete symbols, nor does it involve any audio tokenization or speech analysis relevant to the inclusion criteria.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "A Residual Diffusion Model for High Perceptual Quality Codec Augmentation",
    "abstract": "Diffusion probabilistic models have recently achieved remarkable success in generating high quality image and video data. In this work, we build on this class of generative models and introduce a method for lossy compression of high resolution images. The resulting codec, which we call DIffuson-based Residual Augmentation Codec (DIRAC), is the first neural codec to allow smooth traversal of the rate-distortion-perception tradeoff at test time, while obtaining competitive performance with GAN-based methods in perceptual quality. Furthermore, while sampling from diffusion probabilistic models is notoriously expensive, we show that in the compression setting the number of steps can be drastically reduced.",
    "metadata": {
      "arxiv_id": "2301.05489",
      "title": "A Residual Diffusion Model for High Perceptual Quality Codec Augmentation",
      "summary": "Diffusion probabilistic models have recently achieved remarkable success in generating high quality image and video data. In this work, we build on this class of generative models and introduce a method for lossy compression of high resolution images. The resulting codec, which we call DIffuson-based Residual Augmentation Codec (DIRAC), is the first neural codec to allow smooth traversal of the rate-distortion-perception tradeoff at test time, while obtaining competitive performance with GAN-based methods in perceptual quality. Furthermore, while sampling from diffusion probabilistic models is notoriously expensive, we show that in the compression setting the number of steps can be drastically reduced.",
      "authors": [
        "Noor Fathima Ghouse",
        "Jens Petersen",
        "Auke Wiggers",
        "Tianlin Xu",
        "Guillaume Sautière"
      ],
      "published": "2023-01-13T11:27:26Z",
      "updated": "2023-03-29T16:13:22Z",
      "categories": [
        "cs.CV",
        "eess.IV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2301.05489v3",
      "landing_url": "https://arxiv.org/abs/2301.05489v3",
      "doi": "https://doi.org/10.48550/arXiv.2301.05489"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This item targets image/video diffusion-based neural codecs and does not involve discrete audio/speech tokenization, so it fails the inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "This item targets image/video diffusion-based neural codecs and does not involve discrete audio/speech tokenization, so it fails the inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on a diffusion model for image compression and does not address discrete audio/speech tokenization or discrete audio token representations, which are required by the inclusion criteria. Therefore, it does not meet the inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on a diffusion model for image compression and does not address discrete audio/speech tokenization or discrete audio token representations, which are required by the inclusion criteria. Therefore, it does not meet the inclusion criteria and should be excluded.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Masked Vector Quantization",
    "abstract": "Generative models with discrete latent representations have recently demonstrated an impressive ability to learn complex high-dimensional data distributions. However, their performance relies on a long sequence of tokens per instance and a large number of codebook entries, resulting in long sampling times and considerable computation to fit the categorical posterior. To address these issues, we propose the Masked Vector Quantization (MVQ) framework which increases the representational capacity of each code vector by learning mask configurations via a stochastic winner-takes-all training regime called Multiple Hypothese Dropout (MH-Dropout). On ImageNet 64$\\times$64, MVQ reduces FID in existing vector quantization architectures by up to $68\\%$ at 2 tokens per instance and $57\\%$ at 5 tokens. These improvements widen as codebook entries is reduced and allows for $7\\textit{--}45\\times$ speed-up in token sampling during inference. As an additional benefit, we find that smaller latent spaces lead to MVQ identifying transferable visual representations where multiple can be smoothly combined.",
    "metadata": {
      "arxiv_id": "2301.06626",
      "title": "Masked Vector Quantization",
      "summary": "Generative models with discrete latent representations have recently demonstrated an impressive ability to learn complex high-dimensional data distributions. However, their performance relies on a long sequence of tokens per instance and a large number of codebook entries, resulting in long sampling times and considerable computation to fit the categorical posterior. To address these issues, we propose the Masked Vector Quantization (MVQ) framework which increases the representational capacity of each code vector by learning mask configurations via a stochastic winner-takes-all training regime called Multiple Hypothese Dropout (MH-Dropout). On ImageNet 64$\\times$64, MVQ reduces FID in existing vector quantization architectures by up to $68\\%$ at 2 tokens per instance and $57\\%$ at 5 tokens. These improvements widen as codebook entries is reduced and allows for $7\\textit{--}45\\times$ speed-up in token sampling during inference. As an additional benefit, we find that smaller latent spaces lead to MVQ identifying transferable visual representations where multiple can be smoothly combined.",
      "authors": [
        "David D. Nguyen",
        "David Leibowitz",
        "Surya Nepal",
        "Salil S. Kanhere"
      ],
      "published": "2023-01-16T22:30:53Z",
      "updated": "2024-03-25T00:45:30Z",
      "categories": [
        "cs.LG",
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2301.06626v2",
      "landing_url": "https://arxiv.org/abs/2301.06626v2",
      "doi": "https://doi.org/10.48550/arXiv.2301.06626"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The abstract focuses on discrete latent representations and vector quantization for image data rather than discrete audio/speech tokenization; it does not analyze or evaluate discrete audio tokens as a core object, so it should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The abstract focuses on discrete latent representations and vector quantization for image data rather than discrete audio/speech tokenization; it does not analyze or evaluate discrete audio tokens as a core object, so it should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on discrete latent representations and vector quantization in the context of image data (ImageNet) rather than audio or speech signals; it does not explicitly address discrete audio/speech tokenization or analyze discrete audio tokenization methods as required by the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on discrete latent representations and vector quantization in the context of image data (ImageNet) rather than audio or speech signals; it does not explicitly address discrete audio/speech tokenization or analyze discrete audio tokenization methods as required by the inclusion criteria.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Cellular Network Speech Enhancement: Removing Background and Transmission Noise",
    "abstract": "The primary objective of speech enhancement is to reduce background noise while preserving the target's speech. A common dilemma occurs when a speaker is confined to a noisy environment and receives a call with high background and transmission noise. To address this problem, the Deep Noise Suppression (DNS) Challenge focuses on removing the background noise with the next-generation deep learning models to enhance the target's speech; however, researchers fail to consider Voice Over IP (VoIP) applications their transmission noise. Focusing on Google Meet and its cellular application, our work achieves state-of-the-art performance on the Google Meet To Phone Track of the VoIP DNS Challenge. This paper demonstrates how to beat industrial performance and achieve 1.92 PESQ and 0.88 STOI, as well as superior acoustic fidelity, perceptual quality, and intelligibility in various metrics.",
    "metadata": {
      "arxiv_id": "2301.09027",
      "title": "Cellular Network Speech Enhancement: Removing Background and Transmission Noise",
      "summary": "The primary objective of speech enhancement is to reduce background noise while preserving the target's speech. A common dilemma occurs when a speaker is confined to a noisy environment and receives a call with high background and transmission noise. To address this problem, the Deep Noise Suppression (DNS) Challenge focuses on removing the background noise with the next-generation deep learning models to enhance the target's speech; however, researchers fail to consider Voice Over IP (VoIP) applications their transmission noise. Focusing on Google Meet and its cellular application, our work achieves state-of-the-art performance on the Google Meet To Phone Track of the VoIP DNS Challenge. This paper demonstrates how to beat industrial performance and achieve 1.92 PESQ and 0.88 STOI, as well as superior acoustic fidelity, perceptual quality, and intelligibility in various metrics.",
      "authors": [
        "Amanda Shu",
        "Hamza Khalid",
        "Haohui Liu",
        "Shikhar Agnihotri",
        "Joseph Konan",
        "Ojas Bhargave"
      ],
      "published": "2023-01-22T00:18:10Z",
      "updated": "2023-01-22T00:18:10Z",
      "categories": [
        "cs.SD",
        "cs.LG",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2301.09027v1",
      "landing_url": "https://arxiv.org/abs/2301.09027v1",
      "doi": "https://doi.org/10.48550/arXiv.2301.09027"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The abstract discusses deep learning-based noise suppression for VoIP and does not address discrete audio tokenization or token-based representations, so it does not meet the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The abstract discusses deep learning-based noise suppression for VoIP and does not address discrete audio tokenization or token-based representations, so it does not meet the inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on speech enhancement and noise suppression for cellular networks and VoIP applications, but it does not explicitly propose, analyze, or evaluate discrete audio/speech tokenization; rather, it emphasizes deep learning models for noise suppression and acoustic fidelity without addressing discrete tokenization aspects required by the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on speech enhancement and noise suppression for cellular networks and VoIP applications, but it does not explicitly propose, analyze, or evaluate discrete audio/speech tokenization; rather, it emphasizes deep learning models for noise suppression and acoustic fidelity without addressing discrete tokenization aspects required by the inclusion criteria.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "InstructTTS: Modelling Expressive TTS in Discrete Latent Space with Natural Language Style Prompt",
    "abstract": "Expressive text-to-speech (TTS) aims to synthesize different speaking style speech according to human's demands. Nowadays, there are two common ways to control speaking styles: (1) Pre-defining a group of speaking style and using categorical index to denote different speaking style. However, there are limitations in the diversity of expressiveness, as these models can only generate the pre-defined styles. (2) Using reference speech as style input, which results in a problem that the extracted style information is not intuitive or interpretable. In this study, we attempt to use natural language as style prompt to control the styles in the synthetic speech, e.g., \"Sigh tone in full of sad mood with some helpless feeling\". Considering that there is no existing TTS corpus which is proper to benchmark this novel task, we first construct a speech corpus, whose speech samples are annotated with not only content transcriptions but also style descriptions in natural language. Then we propose an expressive TTS model, named as InstructTTS, which is novel in the sense of following aspects: (1) We fully take the advantage of self-supervised learning and cross-modal metric learning, and propose a novel three-stage training procedure to obtain a robust sentence embedding model, which can effectively capture semantic information from the style prompts and control the speaking style in the generated speech. (2) We propose to model acoustic features in discrete latent space and train a novel discrete diffusion probabilistic model to generate vector-quantized (VQ) acoustic tokens rather than the commonly-used mel spectrogram. (3) We jointly apply mutual information (MI) estimation and minimization during acoustic model training to minimize style-speaker and style-content MI, avoiding possible content and speaker information leakage from the style prompt.",
    "metadata": {
      "arxiv_id": "2301.13662",
      "title": "InstructTTS: Modelling Expressive TTS in Discrete Latent Space with Natural Language Style Prompt",
      "summary": "Expressive text-to-speech (TTS) aims to synthesize different speaking style speech according to human's demands. Nowadays, there are two common ways to control speaking styles: (1) Pre-defining a group of speaking style and using categorical index to denote different speaking style. However, there are limitations in the diversity of expressiveness, as these models can only generate the pre-defined styles. (2) Using reference speech as style input, which results in a problem that the extracted style information is not intuitive or interpretable. In this study, we attempt to use natural language as style prompt to control the styles in the synthetic speech, e.g., \"Sigh tone in full of sad mood with some helpless feeling\". Considering that there is no existing TTS corpus which is proper to benchmark this novel task, we first construct a speech corpus, whose speech samples are annotated with not only content transcriptions but also style descriptions in natural language. Then we propose an expressive TTS model, named as InstructTTS, which is novel in the sense of following aspects: (1) We fully take the advantage of self-supervised learning and cross-modal metric learning, and propose a novel three-stage training procedure to obtain a robust sentence embedding model, which can effectively capture semantic information from the style prompts and control the speaking style in the generated speech. (2) We propose to model acoustic features in discrete latent space and train a novel discrete diffusion probabilistic model to generate vector-quantized (VQ) acoustic tokens rather than the commonly-used mel spectrogram. (3) We jointly apply mutual information (MI) estimation and minimization during acoustic model training to minimize style-speaker and style-content MI, avoiding possible content and speaker information leakage from the style prompt.",
      "authors": [
        "Dongchao Yang",
        "Songxiang Liu",
        "Rongjie Huang",
        "Chao Weng",
        "Helen Meng"
      ],
      "published": "2023-01-31T14:26:52Z",
      "updated": "2023-06-25T11:42:52Z",
      "categories": [
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2301.13662v2",
      "landing_url": "https://arxiv.org/abs/2301.13662v2",
      "doi": "https://doi.org/10.48550/arXiv.2301.13662"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The abstract centers on a discrete-token-based audio representation (vector-quantized acoustic tokens) and a discrete diffusion model for generation, addressing discrete audio/tokenization as the core focus and aligning with inclusion criteria without falling into exclusion categories.",
      "evaluation": 5
    },
    "round-A_JuniorNano_reasoning": "The abstract centers on a discrete-token-based audio representation (vector-quantized acoustic tokens) and a discrete diffusion model for generation, addressing discrete audio/tokenization as the core focus and aligning with inclusion criteria without falling into exclusion categories.",
    "round-A_JuniorNano_evaluation": 5,
    "round-A_JuniorMini_output": {
      "reasoning": "The study explicitly proposes modeling acoustic features in a discrete latent space using a discrete diffusion probabilistic model to generate vector-quantized acoustic tokens, directly addressing discrete audio tokenization for speech synthesis; it also includes methodological contributions and experimental validation relevant to discrete audio representations in text-to-speech tasks, meeting all inclusion criteria and none of the exclusion criteria.",
      "evaluation": 5
    },
    "round-A_JuniorMini_reasoning": "The study explicitly proposes modeling acoustic features in a discrete latent space using a discrete diffusion probabilistic model to generate vector-quantized acoustic tokens, directly addressing discrete audio tokenization for speech synthesis; it also includes methodological contributions and experimental validation relevant to discrete audio representations in text-to-speech tasks, meeting all inclusion criteria and none of the exclusion criteria.",
    "round-A_JuniorMini_evaluation": 5,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "include (junior:5)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "No One Left Behind: Real-World Federated Class-Incremental Learning",
    "abstract": "Federated learning (FL) is a hot collaborative training framework via aggregating model parameters of decentralized local clients. However, most FL methods unreasonably assume data categories of FL framework are known and fixed in advance. Moreover, some new local clients that collect novel categories unseen by other clients may be introduced to FL training irregularly. These issues render global model to undergo catastrophic forgetting on old categories, when local clients receive new categories consecutively under limited memory of storing old categories. To tackle the above issues, we propose a novel Local-Global Anti-forgetting (LGA) model. It ensures no local clients are left behind as they learn new classes continually, by addressing local and global catastrophic forgetting. Specifically, considering tackling class imbalance of local client to surmount local forgetting, we develop a category-balanced gradient-adaptive compensation loss and a category gradient-induced semantic distillation loss. They can balance heterogeneous forgetting speeds of hard-to-forget and easy-to-forget old categories, while ensure consistent class-relations within different tasks. Moreover, a proxy server is designed to tackle global forgetting caused by Non-IID class imbalance between different clients. It augments perturbed prototype images of new categories collected from local clients via self-supervised prototype augmentation, thus improving robustness to choose the best old global model for local-side semantic distillation loss. Experiments on representative datasets verify superior performance of our model against comparison methods. The code is available at https://github.com/JiahuaDong/LGA.",
    "metadata": {
      "arxiv_id": "2302.00903",
      "title": "No One Left Behind: Real-World Federated Class-Incremental Learning",
      "summary": "Federated learning (FL) is a hot collaborative training framework via aggregating model parameters of decentralized local clients. However, most FL methods unreasonably assume data categories of FL framework are known and fixed in advance. Moreover, some new local clients that collect novel categories unseen by other clients may be introduced to FL training irregularly. These issues render global model to undergo catastrophic forgetting on old categories, when local clients receive new categories consecutively under limited memory of storing old categories. To tackle the above issues, we propose a novel Local-Global Anti-forgetting (LGA) model. It ensures no local clients are left behind as they learn new classes continually, by addressing local and global catastrophic forgetting. Specifically, considering tackling class imbalance of local client to surmount local forgetting, we develop a category-balanced gradient-adaptive compensation loss and a category gradient-induced semantic distillation loss. They can balance heterogeneous forgetting speeds of hard-to-forget and easy-to-forget old categories, while ensure consistent class-relations within different tasks. Moreover, a proxy server is designed to tackle global forgetting caused by Non-IID class imbalance between different clients. It augments perturbed prototype images of new categories collected from local clients via self-supervised prototype augmentation, thus improving robustness to choose the best old global model for local-side semantic distillation loss. Experiments on representative datasets verify superior performance of our model against comparison methods. The code is available at https://github.com/JiahuaDong/LGA.",
      "authors": [
        "Jiahua Dong",
        "Hongliu Li",
        "Yang Cong",
        "Gan Sun",
        "Yulun Zhang",
        "Luc Van Gool"
      ],
      "published": "2023-02-02T06:41:02Z",
      "updated": "2023-11-16T03:53:45Z",
      "categories": [
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2302.00903v3",
      "landing_url": "https://arxiv.org/abs/2302.00903v3",
      "doi": "https://doi.org/10.48550/arXiv.2302.00903"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item discusses federated/continual learning for class-incremental tasks with no discrete audio tokenization or analysis of discrete speech tokens, so it does not meet the inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item discusses federated/continual learning for class-incremental tasks with no discrete audio tokenization or analysis of discrete speech tokens, so it does not meet the inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on federated learning for class-incremental learning in decentralized settings and does not mention discrete audio or speech tokenization methods or analysis, thus failing to meet the core inclusion criteria related to discrete tokenization of audio signals.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on federated learning for class-incremental learning in decentralized settings and does not mention discrete audio or speech tokenization methods or analysis, thus failing to meet the core inclusion criteria related to discrete tokenization of audio signals.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Blockwise Self-Supervised Learning at Scale",
    "abstract": "Current state-of-the-art deep networks are all powered by backpropagation. In this paper, we explore alternatives to full backpropagation in the form of blockwise learning rules, leveraging the latest developments in self-supervised learning. We show that a blockwise pretraining procedure consisting of training independently the 4 main blocks of layers of a ResNet-50 with Barlow Twins' loss function at each block performs almost as well as end-to-end backpropagation on ImageNet: a linear probe trained on top of our blockwise pretrained model obtains a top-1 classification accuracy of 70.48%, only 1.1% below the accuracy of an end-to-end pretrained network (71.57% accuracy). We perform extensive experiments to understand the impact of different components within our method and explore a variety of adaptations of self-supervised learning to the blockwise paradigm, building an exhaustive understanding of the critical avenues for scaling local learning rules to large networks, with implications ranging from hardware design to neuroscience.",
    "metadata": {
      "arxiv_id": "2302.01647",
      "title": "Blockwise Self-Supervised Learning at Scale",
      "summary": "Current state-of-the-art deep networks are all powered by backpropagation. In this paper, we explore alternatives to full backpropagation in the form of blockwise learning rules, leveraging the latest developments in self-supervised learning. We show that a blockwise pretraining procedure consisting of training independently the 4 main blocks of layers of a ResNet-50 with Barlow Twins' loss function at each block performs almost as well as end-to-end backpropagation on ImageNet: a linear probe trained on top of our blockwise pretrained model obtains a top-1 classification accuracy of 70.48%, only 1.1% below the accuracy of an end-to-end pretrained network (71.57% accuracy). We perform extensive experiments to understand the impact of different components within our method and explore a variety of adaptations of self-supervised learning to the blockwise paradigm, building an exhaustive understanding of the critical avenues for scaling local learning rules to large networks, with implications ranging from hardware design to neuroscience.",
      "authors": [
        "Shoaib Ahmed Siddiqui",
        "David Krueger",
        "Yann LeCun",
        "Stéphane Deny"
      ],
      "published": "2023-02-03T10:48:24Z",
      "updated": "2024-08-11T15:59:30Z",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2302.01647v2",
      "landing_url": "https://arxiv.org/abs/2302.01647v2",
      "doi": "https://doi.org/10.48550/arXiv.2302.01647"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This article concerns blockwise self-supervised learning in computer vision and does not involve discrete audio/speech tokenization, therefore it does not meet the audio-tokenization inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "This article concerns blockwise self-supervised learning in computer vision and does not involve discrete audio/speech tokenization, therefore it does not meet the audio-tokenization inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper focuses on blockwise self-supervised learning methods applied to ResNet-50 for image classification tasks and does not address discrete audio/speech tokenization or audio signal processing in any manner relevant to the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper focuses on blockwise self-supervised learning methods applied to ResNet-50 for image classification tasks and does not address discrete audio/speech tokenization or audio signal processing in any manner relevant to the inclusion criteria.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Speak, Read and Prompt: High-Fidelity Text-to-Speech with Minimal Supervision",
    "abstract": "We introduce SPEAR-TTS, a multi-speaker text-to-speech (TTS) system that can be trained with minimal supervision. By combining two types of discrete speech representations, we cast TTS as a composition of two sequence-to-sequence tasks: from text to high-level semantic tokens (akin to \"reading\") and from semantic tokens to low-level acoustic tokens (\"speaking\"). Decoupling these two tasks enables training of the \"speaking\" module using abundant audio-only data, and unlocks the highly efficient combination of pretraining and backtranslation to reduce the need for parallel data when training the \"reading\" component. To control the speaker identity, we adopt example prompting, which allows SPEAR-TTS to generalize to unseen speakers using only a short sample of 3 seconds, without any explicit speaker representation or speaker-id labels. Our experiments demonstrate that SPEAR-TTS achieves a character error rate that is competitive with state-of-the-art methods using only 15 minutes of parallel data, while matching ground-truth speech in terms of naturalness and acoustic quality, as measured in subjective tests.",
    "metadata": {
      "arxiv_id": "2302.03540",
      "title": "Speak, Read and Prompt: High-Fidelity Text-to-Speech with Minimal Supervision",
      "summary": "We introduce SPEAR-TTS, a multi-speaker text-to-speech (TTS) system that can be trained with minimal supervision. By combining two types of discrete speech representations, we cast TTS as a composition of two sequence-to-sequence tasks: from text to high-level semantic tokens (akin to \"reading\") and from semantic tokens to low-level acoustic tokens (\"speaking\"). Decoupling these two tasks enables training of the \"speaking\" module using abundant audio-only data, and unlocks the highly efficient combination of pretraining and backtranslation to reduce the need for parallel data when training the \"reading\" component. To control the speaker identity, we adopt example prompting, which allows SPEAR-TTS to generalize to unseen speakers using only a short sample of 3 seconds, without any explicit speaker representation or speaker-id labels. Our experiments demonstrate that SPEAR-TTS achieves a character error rate that is competitive with state-of-the-art methods using only 15 minutes of parallel data, while matching ground-truth speech in terms of naturalness and acoustic quality, as measured in subjective tests.",
      "authors": [
        "Eugene Kharitonov",
        "Damien Vincent",
        "Zalán Borsos",
        "Raphaël Marinier",
        "Sertan Girgin",
        "Olivier Pietquin",
        "Matt Sharifi",
        "Marco Tagliasacchi",
        "Neil Zeghidour"
      ],
      "published": "2023-02-07T15:48:31Z",
      "updated": "2023-02-07T15:48:31Z",
      "categories": [
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2302.03540v1",
      "landing_url": "https://arxiv.org/abs/2302.03540v1",
      "doi": "https://doi.org/10.48550/arXiv.2302.03540"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The study explicitly uses discrete speech representations and tokenization (semantic tokens to acoustic tokens) as central to its TTS pipeline, aligning with the inclusion criteria and not meeting any exclusion criteria.",
      "evaluation": 5
    },
    "round-A_JuniorNano_reasoning": "The study explicitly uses discrete speech representations and tokenization (semantic tokens to acoustic tokens) as central to its TTS pipeline, aligning with the inclusion criteria and not meeting any exclusion criteria.",
    "round-A_JuniorNano_evaluation": 5,
    "round-A_JuniorMini_output": {
      "reasoning": "The study explicitly involves discrete speech representations by converting continuous audio signals into sequences of discrete semantic and acoustic tokens to enable text-to-speech synthesis, fulfilling the inclusion criteria of analyzing discrete audio tokenization; it is not excluded under any criteria as it uses modern neural quantization-based methods and presents experimental validation relevant to speech tasks.",
      "evaluation": 5
    },
    "round-A_JuniorMini_reasoning": "The study explicitly involves discrete speech representations by converting continuous audio signals into sequences of discrete semantic and acoustic tokens to enable text-to-speech synthesis, fulfilling the inclusion criteria of analyzing discrete audio tokenization; it is not excluded under any criteria as it uses modern neural quantization-based methods and presents experimental validation relevant to speech tasks.",
    "round-A_JuniorMini_evaluation": 5,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "include (junior:5)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Multi-Prototypes Convex Merging Based K-Means Clustering Algorithm",
    "abstract": "K-Means algorithm is a popular clustering method. However, it has two limitations: 1) it gets stuck easily in spurious local minima, and 2) the number of clusters k has to be given a priori. To solve these two issues, a multi-prototypes convex merging based K-Means clustering algorithm (MCKM) is presented. First, based on the structure of the spurious local minima of the K-Means problem, a multi-prototypes sampling (MPS) is designed to select the appropriate number of multi-prototypes for data with arbitrary shapes. A theoretical proof is given to guarantee that the multi-prototypes selected by MPS can achieve a constant factor approximation to the optimal cost of the K-Means problem. Then, a merging technique, called convex merging (CM), merges the multi-prototypes to get a better local minima without k being given a priori. Specifically, CM can obtain the optimal merging and estimate the correct k. By integrating these two techniques with K-Means algorithm, the proposed MCKM is an efficient and explainable clustering algorithm for escaping the undesirable local minima of K-Means problem without given k first. Experimental results performed on synthetic and real-world data sets have verified the effectiveness of the proposed algorithm.",
    "metadata": {
      "arxiv_id": "2302.07045",
      "title": "Multi-Prototypes Convex Merging Based K-Means Clustering Algorithm",
      "summary": "K-Means algorithm is a popular clustering method. However, it has two limitations: 1) it gets stuck easily in spurious local minima, and 2) the number of clusters k has to be given a priori. To solve these two issues, a multi-prototypes convex merging based K-Means clustering algorithm (MCKM) is presented. First, based on the structure of the spurious local minima of the K-Means problem, a multi-prototypes sampling (MPS) is designed to select the appropriate number of multi-prototypes for data with arbitrary shapes. A theoretical proof is given to guarantee that the multi-prototypes selected by MPS can achieve a constant factor approximation to the optimal cost of the K-Means problem. Then, a merging technique, called convex merging (CM), merges the multi-prototypes to get a better local minima without k being given a priori. Specifically, CM can obtain the optimal merging and estimate the correct k. By integrating these two techniques with K-Means algorithm, the proposed MCKM is an efficient and explainable clustering algorithm for escaping the undesirable local minima of K-Means problem without given k first. Experimental results performed on synthetic and real-world data sets have verified the effectiveness of the proposed algorithm.",
      "authors": [
        "Dong Li",
        "Shuisheng Zhou",
        "Tieyong Zeng",
        "Raymond H. Chan"
      ],
      "published": "2023-02-14T13:57:33Z",
      "updated": "2023-02-14T13:57:33Z",
      "categories": [
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2302.07045v1",
      "landing_url": "https://arxiv.org/abs/2302.07045v1",
      "doi": "https://doi.org/10.48550/arXiv.2302.07045"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The article describes a clustering algorithm (MCKM) without any discrete audio/speech tokenization or analysis of discrete tokens; thus it does not meet the inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The article describes a clustering algorithm (MCKM) without any discrete audio/speech tokenization or analysis of discrete tokens; thus it does not meet the inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study proposes a K-Means clustering algorithm focused on overcoming issues in clustering such as local minima and determining the number of clusters. However, it does not address discrete audio or speech tokenization, nor does it discuss converting continuous audio signals into discrete tokens for reconstruction, generation, or understanding tasks, which is central to the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study proposes a K-Means clustering algorithm focused on overcoming issues in clustering such as local minima and determining the number of clusters. However, it does not address discrete audio or speech tokenization, nor does it discuss converting continuous audio signals into discrete tokens for reconstruction, generation, or understanding tasks, which is central to the inclusion criteria.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Speech Enhancement with Multi-granularity Vector Quantization",
    "abstract": "With advances in deep learning, neural network based speech enhancement (SE) has developed rapidly in the last decade. Meanwhile, the self-supervised pre-trained model and vector quantization (VQ) have achieved excellent performance on many speech-related tasks, while they are less explored on SE. As it was shown in our previous work that utilizing a VQ module to discretize noisy speech representations is beneficial for speech denoising, in this work we therefore study the impact of using VQ at different layers with different number of codebooks. Different VQ modules indeed enable to extract multiple-granularity speech features. Following an attention mechanism, the contextual features extracted by a pre-trained model are fused with the local features extracted by the encoder, such that both global and local information are preserved to reconstruct the enhanced speech. Experimental results on the Valentini dataset show that the proposed model can improve the SE performance, where the impact of choosing pre-trained models is also revealed.",
    "metadata": {
      "arxiv_id": "2302.08342",
      "title": "Speech Enhancement with Multi-granularity Vector Quantization",
      "summary": "With advances in deep learning, neural network based speech enhancement (SE) has developed rapidly in the last decade. Meanwhile, the self-supervised pre-trained model and vector quantization (VQ) have achieved excellent performance on many speech-related tasks, while they are less explored on SE. As it was shown in our previous work that utilizing a VQ module to discretize noisy speech representations is beneficial for speech denoising, in this work we therefore study the impact of using VQ at different layers with different number of codebooks. Different VQ modules indeed enable to extract multiple-granularity speech features. Following an attention mechanism, the contextual features extracted by a pre-trained model are fused with the local features extracted by the encoder, such that both global and local information are preserved to reconstruct the enhanced speech. Experimental results on the Valentini dataset show that the proposed model can improve the SE performance, where the impact of choosing pre-trained models is also revealed.",
      "authors": [
        "Xiao-Ying Zhao",
        "Qiu-Shi Zhu",
        "Jie Zhang"
      ],
      "published": "2023-02-16T14:53:41Z",
      "updated": "2023-02-16T14:53:41Z",
      "categories": [
        "eess.AS",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2302.08342v1",
      "landing_url": "https://arxiv.org/abs/2302.08342v1",
      "doi": "https://doi.org/10.48550/arXiv.2302.08342"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The abstract centers on using vector quantization with multiple codebooks to discretize speech representations for enhancement, explicitly addressing discrete audio tokenization.",
      "evaluation": 5
    },
    "round-A_JuniorNano_reasoning": "The abstract centers on using vector quantization with multiple codebooks to discretize speech representations for enhancement, explicitly addressing discrete audio tokenization.",
    "round-A_JuniorNano_evaluation": 5,
    "round-A_JuniorMini_output": {
      "reasoning": "The study explicitly focuses on using vector quantization (VQ) as a discretization method to convert noisy speech representations into discrete token-like features for speech enhancement, fulfilling the inclusion criteria of analyzing discrete audio tokenization; it also involves experimental validation on speech tasks, thus meeting all inclusion and none of the exclusion criteria.",
      "evaluation": 5
    },
    "round-A_JuniorMini_reasoning": "The study explicitly focuses on using vector quantization (VQ) as a discretization method to convert noisy speech representations into discrete token-like features for speech enhancement, fulfilling the inclusion criteria of analyzing discrete audio tokenization; it also involves experimental validation on speech tasks, thus meeting all inclusion and none of the exclusion criteria.",
    "round-A_JuniorMini_evaluation": 5,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "include (junior:5)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Giant Magneto-Optical Schäfer-Hubert Effect in Two-Dimensional van der Waals Antiferromagnets \\textit{M}PS$_3$ (\\textit{M}=Mn, Fe, Ni)",
    "abstract": "The recent discovery of long-range magnetic order in atomically thin films has triggered particular interest in two-dimensional (2D) van der Waals (vdW) magnetic materials. In this paper, we perform a systematic theoretical study of the magneto-optical Schäfer-Hubert effect (MOSHE) in 2D vdW antiferromagnetic \\textit{M}PS$_3$ (\\textit{M} = Mn, Fe, Ni) with multifold intralayer and interlayer magnetic orders. The formula for evaluating the MOSHE in 2D magnets is derived by considering the influence of a non-magnetic substrate. The MOSHE of monolayer and bilayer \\textit{M}PS$_3$ are considerably large ($>2^{\\circ}$), originating from the strong anisotropy of in-plane optical conductivity. The Schäfer-Hubert rotation angles are surprisingly insensitive to the orientations of the Néel vector, while the Schäfer-Hubert ellipticities are identified to be a good criterion to distinguish different interlayer magnetic orders. Our work establishes a theoretical framework for exploring novel 2D vdW magnets and facilitates the promising applications of the 2D \\textit{M}PS$_3$ family in antiferromagnetic nanophotonic devices.",
    "metadata": {
      "arxiv_id": "2302.10606",
      "title": "Giant Magneto-Optical Schäfer-Hubert Effect in Two-Dimensional van der Waals Antiferromagnets \\textit{M}PS$_3$ (\\textit{M}=Mn, Fe, Ni)",
      "summary": "The recent discovery of long-range magnetic order in atomically thin films has triggered particular interest in two-dimensional (2D) van der Waals (vdW) magnetic materials. In this paper, we perform a systematic theoretical study of the magneto-optical Schäfer-Hubert effect (MOSHE) in 2D vdW antiferromagnetic \\textit{M}PS$_3$ (\\textit{M} = Mn, Fe, Ni) with multifold intralayer and interlayer magnetic orders. The formula for evaluating the MOSHE in 2D magnets is derived by considering the influence of a non-magnetic substrate. The MOSHE of monolayer and bilayer \\textit{M}PS$_3$ are considerably large ($>2^{\\circ}$), originating from the strong anisotropy of in-plane optical conductivity. The Schäfer-Hubert rotation angles are surprisingly insensitive to the orientations of the Néel vector, while the Schäfer-Hubert ellipticities are identified to be a good criterion to distinguish different interlayer magnetic orders. Our work establishes a theoretical framework for exploring novel 2D vdW magnets and facilitates the promising applications of the 2D \\textit{M}PS$_3$ family in antiferromagnetic nanophotonic devices.",
      "authors": [
        "Ping Yang",
        "Wanxiang Feng",
        "Gui-Bin Liu",
        "Guang-Yu Guo",
        "Yugui Yao"
      ],
      "published": "2023-02-21T11:28:41Z",
      "updated": "2023-02-21T11:28:41Z",
      "categories": [
        "cond-mat.mtrl-sci"
      ],
      "pdf_url": "https://arxiv.org/pdf/2302.10606v1",
      "landing_url": "https://arxiv.org/abs/2302.10606v1",
      "doi": "https://doi.org/10.1103/PhysRevB.107.214437"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This article investigates magneto-optical effects in 2D magnetic materials and contains no discussion of discrete audio/speech tokenization, thus it does not meet the inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "This article investigates magneto-optical effects in 2D magnetic materials and contains no discussion of discrete audio/speech tokenization, thus it does not meet the inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study investigates the magneto-optical properties of 2D van der Waals antiferromagnetic materials and does not involve any aspect of audio or speech tokenization, nor does it address discrete audio symbol representations or tokenization methods. It focuses purely on physical and material science phenomena, so it does not meet the inclusion criteria related to discrete audio/speech tokenization.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study investigates the magneto-optical properties of 2D van der Waals antiferromagnetic materials and does not involve any aspect of audio or speech tokenization, nor does it address discrete audio symbol representations or tokenization methods. It focuses purely on physical and material science phenomena, so it does not meet the inclusion criteria related to discrete audio/speech tokenization.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "PixCUE: Joint Uncertainty Estimation and Image Reconstruction in MRI using Deep Pixel Classification",
    "abstract": "Deep learning (DL) models are capable of successfully exploiting latent representations in MR data and have become state-of-the-art for accelerated MRI reconstruction. However, undersampling the measurements in k-space as well as the over- or under-parameterized and non-transparent nature of DL make these models exposed to uncertainty. Consequently, uncertainty estimation has become a major issue in DL MRI reconstruction. To estimate uncertainty, Monte Carlo (MC) inference techniques have become a common practice where multiple reconstructions are utilized to compute the variance in reconstruction as a measurement of uncertainty. However, these methods demand high computational costs as they require multiple inferences through the DL model. To this end, we introduce a method to estimate uncertainty during MRI reconstruction using a pixel classification framework. The proposed method, PixCUE (stands for Pixel Classification Uncertainty Estimation) produces the reconstructed image along with an uncertainty map during a single forward pass through the DL model. We demonstrate that this approach generates uncertainty maps that highly correlate with the reconstruction errors with respect to various MR imaging sequences and under numerous adversarial conditions. We also show that the estimated uncertainties are correlated to that of the conventional MC method. We further provide an empirical relationship between the uncertainty estimations using PixCUE and well-established reconstruction metrics such as NMSE, PSNR, and SSIM. We conclude that PixCUE is capable of reliably estimating the uncertainty in MRI reconstruction with a minimum additional computational cost.",
    "metadata": {
      "arxiv_id": "2303.00111",
      "title": "PixCUE: Joint Uncertainty Estimation and Image Reconstruction in MRI using Deep Pixel Classification",
      "summary": "Deep learning (DL) models are capable of successfully exploiting latent representations in MR data and have become state-of-the-art for accelerated MRI reconstruction. However, undersampling the measurements in k-space as well as the over- or under-parameterized and non-transparent nature of DL make these models exposed to uncertainty. Consequently, uncertainty estimation has become a major issue in DL MRI reconstruction. To estimate uncertainty, Monte Carlo (MC) inference techniques have become a common practice where multiple reconstructions are utilized to compute the variance in reconstruction as a measurement of uncertainty. However, these methods demand high computational costs as they require multiple inferences through the DL model. To this end, we introduce a method to estimate uncertainty during MRI reconstruction using a pixel classification framework. The proposed method, PixCUE (stands for Pixel Classification Uncertainty Estimation) produces the reconstructed image along with an uncertainty map during a single forward pass through the DL model. We demonstrate that this approach generates uncertainty maps that highly correlate with the reconstruction errors with respect to various MR imaging sequences and under numerous adversarial conditions. We also show that the estimated uncertainties are correlated to that of the conventional MC method. We further provide an empirical relationship between the uncertainty estimations using PixCUE and well-established reconstruction metrics such as NMSE, PSNR, and SSIM. We conclude that PixCUE is capable of reliably estimating the uncertainty in MRI reconstruction with a minimum additional computational cost.",
      "authors": [
        "Mevan Ekanayake",
        "Kamlesh Pawar",
        "Gary Egan",
        "Zhaolin Chen"
      ],
      "published": "2023-02-28T22:26:18Z",
      "updated": "2023-03-08T07:55:37Z",
      "categories": [
        "eess.IV",
        "cs.AI",
        "cs.CV",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2303.00111v2",
      "landing_url": "https://arxiv.org/abs/2303.00111v2",
      "doi": "https://doi.org/10.48550/arXiv.2303.00111"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The article concerns MRI image reconstruction and uncertainty estimation (PixCUE) and does not involve discrete audio tokenization, so it fails the required audio-tokenization inclusion criterion and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The article concerns MRI image reconstruction and uncertainty estimation (PixCUE) and does not involve discrete audio tokenization, so it fails the required audio-tokenization inclusion criterion and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on MRI image reconstruction and uncertainty estimation using deep learning, without any mention or analysis of discrete audio or speech tokenization as required by the inclusion criteria. It neither addresses tokenization of audio or speech signals nor relates to audio discrete tokens, making it irrelevant to the stated scope.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on MRI image reconstruction and uncertainty estimation using deep learning, without any mention or analysis of discrete audio or speech tokenization as required by the inclusion criteria. It neither addresses tokenization of audio or speech signals nor relates to audio discrete tokens, making it irrelevant to the stated scope.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Evolutionary Augmentation Policy Optimization for Self-supervised Learning",
    "abstract": "Self-supervised Learning (SSL) is a machine learning algorithm for pretraining Deep Neural Networks (DNNs) without requiring manually labeled data. The central idea of this learning technique is based on an auxiliary stage aka pretext task in which labeled data are created automatically through data augmentation and exploited for pretraining the DNN. However, the effect of each pretext task is not well studied or compared in the literature. In this paper, we study the contribution of augmentation operators on the performance of self supervised learning algorithms in a constrained settings. We propose an evolutionary search method for optimization of data augmentation pipeline in pretext tasks and measure the impact of augmentation operators in several SOTA SSL algorithms. By encoding different combination of augmentation operators in chromosomes we seek the optimal augmentation policies through an evolutionary optimization mechanism. We further introduce methods for analyzing and explaining the performance of optimized SSL algorithms. Our results indicate that our proposed method can find solutions that outperform the accuracy of classification of SSL algorithms which confirms the influence of augmentation policy choice on the overall performance of SSL algorithms. We also compare optimal SSL solutions found by our evolutionary search mechanism and show the effect of batch size in the pretext task on two visual datasets.",
    "metadata": {
      "arxiv_id": "2303.01584",
      "title": "Evolutionary Augmentation Policy Optimization for Self-supervised Learning",
      "summary": "Self-supervised Learning (SSL) is a machine learning algorithm for pretraining Deep Neural Networks (DNNs) without requiring manually labeled data. The central idea of this learning technique is based on an auxiliary stage aka pretext task in which labeled data are created automatically through data augmentation and exploited for pretraining the DNN. However, the effect of each pretext task is not well studied or compared in the literature. In this paper, we study the contribution of augmentation operators on the performance of self supervised learning algorithms in a constrained settings. We propose an evolutionary search method for optimization of data augmentation pipeline in pretext tasks and measure the impact of augmentation operators in several SOTA SSL algorithms. By encoding different combination of augmentation operators in chromosomes we seek the optimal augmentation policies through an evolutionary optimization mechanism. We further introduce methods for analyzing and explaining the performance of optimized SSL algorithms. Our results indicate that our proposed method can find solutions that outperform the accuracy of classification of SSL algorithms which confirms the influence of augmentation policy choice on the overall performance of SSL algorithms. We also compare optimal SSL solutions found by our evolutionary search mechanism and show the effect of batch size in the pretext task on two visual datasets.",
      "authors": [
        "Noah Barrett",
        "Zahra Sadeghi",
        "Stan Matwin"
      ],
      "published": "2023-03-02T21:16:53Z",
      "updated": "2023-08-02T15:38:37Z",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG",
        "cs.NE"
      ],
      "pdf_url": "https://arxiv.org/pdf/2303.01584v2",
      "landing_url": "https://arxiv.org/abs/2303.01584v2",
      "doi": "https://doi.org/10.54364/AAIML.2023.1167"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The article investigates augmentation policies for self-supervised learning without any discrete audio tokenization; thus it does not meet the audio-tokenization inclusion criterion and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The article investigates augmentation policies for self-supervised learning without any discrete audio tokenization; thus it does not meet the audio-tokenization inclusion criterion and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on self-supervised learning optimization via evolutionary search for data augmentation policies in visual datasets, without addressing discrete audio/speech tokenization or any audio-related discrete token representations; thus, it does not meet the inclusion criteria which require explicit analysis or proposal of discrete audio tokenization.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on self-supervised learning optimization via evolutionary search for data augmentation policies in visual datasets, without addressing discrete audio/speech tokenization or any audio-related discrete token representations; thus, it does not meet the inclusion criteria which require explicit analysis or proposal of discrete audio tokenization.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "FoundationTTS: Text-to-Speech for ASR Customization with Generative Language Model",
    "abstract": "Neural text-to-speech (TTS) generally consists of cascaded architecture with separately optimized acoustic model and vocoder, or end-to-end architecture with continuous mel-spectrograms or self-extracted speech frames as the intermediate representations to bridge acoustic model and vocoder, which suffers from two limitations: 1) the continuous acoustic frames are hard to predict with phoneme only, and acoustic information like duration or pitch is also needed to solve the one-to-many problem, which is not easy to scale on large scale and noise datasets; 2) to achieve diverse speech output based on continuous speech features, complex VAE or flow-based models are usually required. In this paper, we propose FoundationTTS, a new speech synthesis system with a neural audio codec for discrete speech token extraction and waveform reconstruction and a large language model for discrete token generation from linguistic (phoneme) tokens. Specifically, 1) we propose a hierarchical codec network based on vector-quantized auto-encoders with adversarial training (VQ-GAN), which first extracts continuous frame-level speech representations with fine-grained codec, and extracts a discrete token from each continuous speech frame with coarse-grained codec; 2) we jointly optimize speech token, linguistic tokens, speaker token together with a large language model and predict the discrete speech tokens autoregressively. Experiments show that FoundationTTS achieves a MOS gain of +0.14 compared to the baseline system. In ASR customization tasks, our method achieves 7.09\\% and 10.35\\% WERR respectively over two strong customized ASR baselines.",
    "metadata": {
      "arxiv_id": "2303.02939",
      "title": "FoundationTTS: Text-to-Speech for ASR Customization with Generative Language Model",
      "summary": "Neural text-to-speech (TTS) generally consists of cascaded architecture with separately optimized acoustic model and vocoder, or end-to-end architecture with continuous mel-spectrograms or self-extracted speech frames as the intermediate representations to bridge acoustic model and vocoder, which suffers from two limitations: 1) the continuous acoustic frames are hard to predict with phoneme only, and acoustic information like duration or pitch is also needed to solve the one-to-many problem, which is not easy to scale on large scale and noise datasets; 2) to achieve diverse speech output based on continuous speech features, complex VAE or flow-based models are usually required. In this paper, we propose FoundationTTS, a new speech synthesis system with a neural audio codec for discrete speech token extraction and waveform reconstruction and a large language model for discrete token generation from linguistic (phoneme) tokens. Specifically, 1) we propose a hierarchical codec network based on vector-quantized auto-encoders with adversarial training (VQ-GAN), which first extracts continuous frame-level speech representations with fine-grained codec, and extracts a discrete token from each continuous speech frame with coarse-grained codec; 2) we jointly optimize speech token, linguistic tokens, speaker token together with a large language model and predict the discrete speech tokens autoregressively. Experiments show that FoundationTTS achieves a MOS gain of +0.14 compared to the baseline system. In ASR customization tasks, our method achieves 7.09\\% and 10.35\\% WERR respectively over two strong customized ASR baselines.",
      "authors": [
        "Ruiqing Xue",
        "Yanqing Liu",
        "Lei He",
        "Xu Tan",
        "Linquan Liu",
        "Edward Lin",
        "Sheng Zhao"
      ],
      "published": "2023-03-06T07:17:15Z",
      "updated": "2023-03-08T03:06:47Z",
      "categories": [
        "eess.AS",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2303.02939v3",
      "landing_url": "https://arxiv.org/abs/2303.02939v3",
      "doi": "https://doi.org/10.48550/arXiv.2303.02939"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This work explicitly uses a neural speech system with discrete speech tokens (via a hierarchical VQ-GAN codec) and token generation with an LLM, addressing discrete audio tokenization as a core object.",
      "evaluation": 5
    },
    "round-A_JuniorNano_reasoning": "This work explicitly uses a neural speech system with discrete speech tokens (via a hierarchical VQ-GAN codec) and token generation with an LLM, addressing discrete audio tokenization as a core object.",
    "round-A_JuniorNano_evaluation": 5,
    "round-A_JuniorMini_output": {
      "reasoning": "The study explicitly proposes a discrete speech token extraction method using a hierarchical codec and vector-quantized auto-encoders, jointly optimized with a large language model, focusing on discrete token generation for speech synthesis and ASR customization. It includes methodological innovation and experimental validation of discrete audio tokens relevant to speech, meeting all inclusion criteria and none of the exclusion criteria.",
      "evaluation": 5
    },
    "round-A_JuniorMini_reasoning": "The study explicitly proposes a discrete speech token extraction method using a hierarchical codec and vector-quantized auto-encoders, jointly optimized with a large language model, focusing on discrete token generation for speech synthesis and ASR customization. It includes methodological innovation and experimental validation of discrete audio tokens relevant to speech, meeting all inclusion criteria and none of the exclusion criteria.",
    "round-A_JuniorMini_evaluation": 5,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "include (junior:5)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Speak Foreign Languages with Your Own Voice: Cross-Lingual Neural Codec Language Modeling",
    "abstract": "We propose a cross-lingual neural codec language model, VALL-E X, for cross-lingual speech synthesis. Specifically, we extend VALL-E and train a multi-lingual conditional codec language model to predict the acoustic token sequences of the target language speech by using both the source language speech and the target language text as prompts. VALL-E X inherits strong in-context learning capabilities and can be applied for zero-shot cross-lingual text-to-speech synthesis and zero-shot speech-to-speech translation tasks. Experimental results show that it can generate high-quality speech in the target language via just one speech utterance in the source language as a prompt while preserving the unseen speaker's voice, emotion, and acoustic environment. Moreover, VALL-E X effectively alleviates the foreign accent problems, which can be controlled by a language ID. Audio samples are available at \\url{https://aka.ms/vallex}.",
    "metadata": {
      "arxiv_id": "2303.03926",
      "title": "Speak Foreign Languages with Your Own Voice: Cross-Lingual Neural Codec Language Modeling",
      "summary": "We propose a cross-lingual neural codec language model, VALL-E X, for cross-lingual speech synthesis. Specifically, we extend VALL-E and train a multi-lingual conditional codec language model to predict the acoustic token sequences of the target language speech by using both the source language speech and the target language text as prompts. VALL-E X inherits strong in-context learning capabilities and can be applied for zero-shot cross-lingual text-to-speech synthesis and zero-shot speech-to-speech translation tasks. Experimental results show that it can generate high-quality speech in the target language via just one speech utterance in the source language as a prompt while preserving the unseen speaker's voice, emotion, and acoustic environment. Moreover, VALL-E X effectively alleviates the foreign accent problems, which can be controlled by a language ID. Audio samples are available at \\url{https://aka.ms/vallex}.",
      "authors": [
        "Ziqiang Zhang",
        "Long Zhou",
        "Chengyi Wang",
        "Sanyuan Chen",
        "Yu Wu",
        "Shujie Liu",
        "Zhuo Chen",
        "Yanqing Liu",
        "Huaming Wang",
        "Jinyu Li",
        "Lei He",
        "Sheng Zhao",
        "Furu Wei"
      ],
      "published": "2023-03-07T14:31:55Z",
      "updated": "2023-03-07T14:31:55Z",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2303.03926v1",
      "landing_url": "https://arxiv.org/abs/2303.03926v1",
      "doi": "https://doi.org/10.48550/arXiv.2303.03926"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The abstract explicitly centers on using acoustic token sequences (discrete tokens) in a neural codec language model for cross-lingual speech synthesis, indicating a core focus on discrete audio tokenization; therefore it meets the inclusion criteria and is suitable for evaluation.",
      "evaluation": 5
    },
    "round-A_JuniorNano_reasoning": "The abstract explicitly centers on using acoustic token sequences (discrete tokens) in a neural codec language model for cross-lingual speech synthesis, indicating a core focus on discrete audio tokenization; therefore it meets the inclusion criteria and is suitable for evaluation.",
    "round-A_JuniorNano_evaluation": 5,
    "round-A_JuniorMini_output": {
      "reasoning": "The study proposes VALL-E X, a neural codec language model for cross-lingual speech synthesis that predicts acoustic token sequences, implying discrete audio tokenization is a core component. It includes methodological contributions, experimental validation, and focuses on discrete token sequences rather than continuous audio features alone. Thus, it meets the inclusion criteria and none of the exclusion criteria.",
      "evaluation": 5
    },
    "round-A_JuniorMini_reasoning": "The study proposes VALL-E X, a neural codec language model for cross-lingual speech synthesis that predicts acoustic token sequences, implying discrete audio tokenization is a core component. It includes methodological contributions, experimental validation, and focuses on discrete token sequences rather than continuous audio features alone. Thus, it meets the inclusion criteria and none of the exclusion criteria.",
    "round-A_JuniorMini_evaluation": 5,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "include (junior:5)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Regularized Vector Quantization for Tokenized Image Synthesis",
    "abstract": "Quantizing images into discrete representations has been a fundamental problem in unified generative modeling. Predominant approaches learn the discrete representation either in a deterministic manner by selecting the best-matching token or in a stochastic manner by sampling from a predicted distribution. However, deterministic quantization suffers from severe codebook collapse and misalignment with inference stage while stochastic quantization suffers from low codebook utilization and perturbed reconstruction objective. This paper presents a regularized vector quantization framework that allows to mitigate above issues effectively by applying regularization from two perspectives. The first is a prior distribution regularization which measures the discrepancy between a prior token distribution and the predicted token distribution to avoid codebook collapse and low codebook utilization. The second is a stochastic mask regularization that introduces stochasticity during quantization to strike a good balance between inference stage misalignment and unperturbed reconstruction objective. In addition, we design a probabilistic contrastive loss which serves as a calibrated metric to further mitigate the perturbed reconstruction objective. Extensive experiments show that the proposed quantization framework outperforms prevailing vector quantization methods consistently across different generative models including auto-regressive models and diffusion models.",
    "metadata": {
      "arxiv_id": "2303.06424",
      "title": "Regularized Vector Quantization for Tokenized Image Synthesis",
      "summary": "Quantizing images into discrete representations has been a fundamental problem in unified generative modeling. Predominant approaches learn the discrete representation either in a deterministic manner by selecting the best-matching token or in a stochastic manner by sampling from a predicted distribution. However, deterministic quantization suffers from severe codebook collapse and misalignment with inference stage while stochastic quantization suffers from low codebook utilization and perturbed reconstruction objective. This paper presents a regularized vector quantization framework that allows to mitigate above issues effectively by applying regularization from two perspectives. The first is a prior distribution regularization which measures the discrepancy between a prior token distribution and the predicted token distribution to avoid codebook collapse and low codebook utilization. The second is a stochastic mask regularization that introduces stochasticity during quantization to strike a good balance between inference stage misalignment and unperturbed reconstruction objective. In addition, we design a probabilistic contrastive loss which serves as a calibrated metric to further mitigate the perturbed reconstruction objective. Extensive experiments show that the proposed quantization framework outperforms prevailing vector quantization methods consistently across different generative models including auto-regressive models and diffusion models.",
      "authors": [
        "Jiahui Zhang",
        "Fangneng Zhan",
        "Christian Theobalt",
        "Shijian Lu"
      ],
      "published": "2023-03-11T15:20:54Z",
      "updated": "2023-10-14T06:17:12Z",
      "categories": [
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2303.06424v2",
      "landing_url": "https://arxiv.org/abs/2303.06424v2",
      "doi": "https://doi.org/10.48550/arXiv.2303.06424"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The study addresses discrete tokenization for images (tokenized image synthesis) rather than discrete audio/speech tokenization; it does not meet the inclusion criterion focused on discrete audio tokens, so it should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The study addresses discrete tokenization for images (tokenized image synthesis) rather than discrete audio/speech tokenization; it does not meet the inclusion criterion focused on discrete audio tokens, so it should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on quantizing images into discrete representations for image synthesis tasks and does not address discrete audio/speech tokenization or relate to audio signal processing as required by the inclusion criteria; therefore, it does not meet the inclusion criteria nor violate any exclusion criteria specifically about audio, but the core subject is unrelated to audio tokenization.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on quantizing images into discrete representations for image synthesis tasks and does not address discrete audio/speech tokenization or relate to audio signal processing as required by the inclusion criteria; therefore, it does not meet the inclusion criteria nor violate any exclusion criteria specifically about audio, but the core subject is unrelated to audio tokenization.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Making Vision Transformers Efficient from A Token Sparsification View",
    "abstract": "The quadratic computational complexity to the number of tokens limits the practical applications of Vision Transformers (ViTs). Several works propose to prune redundant tokens to achieve efficient ViTs. However, these methods generally suffer from (i) dramatic accuracy drops, (ii) application difficulty in the local vision transformer, and (iii) non-general-purpose networks for downstream tasks. In this work, we propose a novel Semantic Token ViT (STViT), for efficient global and local vision transformers, which can also be revised to serve as backbone for downstream tasks. The semantic tokens represent cluster centers, and they are initialized by pooling image tokens in space and recovered by attention, which can adaptively represent global or local semantic information. Due to the cluster properties, a few semantic tokens can attain the same effect as vast image tokens, for both global and local vision transformers. For instance, only 16 semantic tokens on DeiT-(Tiny,Small,Base) can achieve the same accuracy with more than 100% inference speed improvement and nearly 60% FLOPs reduction; on Swin-(Tiny,Small,Base), we can employ 16 semantic tokens in each window to further speed it up by around 20% with slight accuracy increase. Besides great success in image classification, we also extend our method to video recognition. In addition, we design a STViT-R(ecover) network to restore the detailed spatial information based on the STViT, making it work for downstream tasks, which is powerless for previous token sparsification methods. Experiments demonstrate that our method can achieve competitive results compared to the original networks in object detection and instance segmentation, with over 30% FLOPs reduction for backbone. Code is available at http://github.com/changsn/STViT-R",
    "metadata": {
      "arxiv_id": "2303.08685",
      "title": "Making Vision Transformers Efficient from A Token Sparsification View",
      "summary": "The quadratic computational complexity to the number of tokens limits the practical applications of Vision Transformers (ViTs). Several works propose to prune redundant tokens to achieve efficient ViTs. However, these methods generally suffer from (i) dramatic accuracy drops, (ii) application difficulty in the local vision transformer, and (iii) non-general-purpose networks for downstream tasks. In this work, we propose a novel Semantic Token ViT (STViT), for efficient global and local vision transformers, which can also be revised to serve as backbone for downstream tasks. The semantic tokens represent cluster centers, and they are initialized by pooling image tokens in space and recovered by attention, which can adaptively represent global or local semantic information. Due to the cluster properties, a few semantic tokens can attain the same effect as vast image tokens, for both global and local vision transformers. For instance, only 16 semantic tokens on DeiT-(Tiny,Small,Base) can achieve the same accuracy with more than 100% inference speed improvement and nearly 60% FLOPs reduction; on Swin-(Tiny,Small,Base), we can employ 16 semantic tokens in each window to further speed it up by around 20% with slight accuracy increase. Besides great success in image classification, we also extend our method to video recognition. In addition, we design a STViT-R(ecover) network to restore the detailed spatial information based on the STViT, making it work for downstream tasks, which is powerless for previous token sparsification methods. Experiments demonstrate that our method can achieve competitive results compared to the original networks in object detection and instance segmentation, with over 30% FLOPs reduction for backbone. Code is available at http://github.com/changsn/STViT-R",
      "authors": [
        "Shuning Chang",
        "Pichao Wang",
        "Ming Lin",
        "Fan Wang",
        "David Junhao Zhang",
        "Rong Jin",
        "Mike Zheng Shou"
      ],
      "published": "2023-03-15T15:12:36Z",
      "updated": "2023-03-30T11:56:29Z",
      "categories": [
        "cs.CV",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2303.08685v2",
      "landing_url": "https://arxiv.org/abs/2303.08685v2",
      "doi": "https://doi.org/10.48550/arXiv.2303.08685"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The paper focuses on semantic token sparsification for vision transformers (images/videos) and does not propose or evaluate discrete audio/speech tokenization; thus it does not meet audio-token criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The paper focuses on semantic token sparsification for vision transformers (images/videos) and does not propose or evaluate discrete audio/speech tokenization; thus it does not meet audio-token criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on Vision Transformers for image and video tasks, involving token sparsification of visual tokens rather than discrete tokenization of audio or speech signals. It does not analyze or propose audio/speech discrete tokenization, nor does it focus on speech-relevant tasks, which fails the inclusion criteria and fits the exclusion criteria about non-audio/vision tokenization focus.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on Vision Transformers for image and video tasks, involving token sparsification of visual tokens rather than discrete tokenization of audio or speech signals. It does not analyze or propose audio/speech discrete tokenization, nor does it focus on speech-relevant tasks, which fails the inclusion criteria and fits the exclusion criteria about non-audio/vision tokenization focus.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Cocktail HuBERT: Generalized Self-Supervised Pre-training for Mixture and Single-Source Speech",
    "abstract": "Self-supervised learning leverages unlabeled data effectively, improving label efficiency and generalization to domains without labeled data. While recent work has studied generalization to more acoustic/linguistic domains, languages, and modalities, these investigations are limited to single-source speech with one primary speaker in the recording. This paper presents Cocktail HuBERT, a self-supervised learning framework that generalizes to mixture speech using a masked pseudo source separation objective. This objective encourages the model to identify the number of sources, separate and understand the context, and infer the content of masked regions represented as discovered units. Cocktail HuBERT outperforms state-of-the-art results with 69% lower WER on multi-speaker ASR, 31% lower DER on diarization, and is competitive on single- and multi-speaker tasks from SUPERB.",
    "metadata": {
      "arxiv_id": "2303.11131",
      "title": "Cocktail HuBERT: Generalized Self-Supervised Pre-training for Mixture and Single-Source Speech",
      "summary": "Self-supervised learning leverages unlabeled data effectively, improving label efficiency and generalization to domains without labeled data. While recent work has studied generalization to more acoustic/linguistic domains, languages, and modalities, these investigations are limited to single-source speech with one primary speaker in the recording. This paper presents Cocktail HuBERT, a self-supervised learning framework that generalizes to mixture speech using a masked pseudo source separation objective. This objective encourages the model to identify the number of sources, separate and understand the context, and infer the content of masked regions represented as discovered units. Cocktail HuBERT outperforms state-of-the-art results with 69% lower WER on multi-speaker ASR, 31% lower DER on diarization, and is competitive on single- and multi-speaker tasks from SUPERB.",
      "authors": [
        "Maryam Fazel-Zarandi",
        "Wei-Ning Hsu"
      ],
      "published": "2023-03-20T14:07:13Z",
      "updated": "2023-03-20T14:07:13Z",
      "categories": [
        "cs.CL",
        "cs.LG",
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2303.11131v1",
      "landing_url": "https://arxiv.org/abs/2303.11131v1",
      "doi": "https://doi.org/10.48550/arXiv.2303.11131"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The paper explicitly uses discrete speech units (discovered tokens) as targets for a self-supervised pretraining objective, i.e., tokenization of audio into a discrete vocabulary for understanding tasks in mixture and single-source speech, which aligns with the inclusion criteria and does not match the exclusions.",
      "evaluation": 5
    },
    "round-A_JuniorNano_reasoning": "The paper explicitly uses discrete speech units (discovered tokens) as targets for a self-supervised pretraining objective, i.e., tokenization of audio into a discrete vocabulary for understanding tasks in mixture and single-source speech, which aligns with the inclusion criteria and does not match the exclusions.",
    "round-A_JuniorNano_evaluation": 5,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper proposes Cocktail HuBERT, a self-supervised learning framework for speech mixtures that leverages a masked pseudo source separation objective and discovered units, implying discrete token units are central to the model's approach for speech representation and understanding, which aligns with the inclusion criteria requiring discrete audio tokenization as a core study object. The abstract also indicates competitive results on speech tasks, suggesting experimental validation. There is no indication the work is merely engineering or focused on traditional codecs, and it involves speech audio, not only non-speech audio. Therefore, this study meets the inclusion criteria and does not fall under any exclusion criteria based on the information given.",
      "evaluation": 5
    },
    "round-A_JuniorMini_reasoning": "The paper proposes Cocktail HuBERT, a self-supervised learning framework for speech mixtures that leverages a masked pseudo source separation objective and discovered units, implying discrete token units are central to the model's approach for speech representation and understanding, which aligns with the inclusion criteria requiring discrete audio tokenization as a core study object. The abstract also indicates competitive results on speech tasks, suggesting experimental validation. There is no indication the work is merely engineering or focused on traditional codecs, and it involves speech audio, not only non-speech audio. Therefore, this study meets the inclusion criteria and does not fall under any exclusion criteria based on the information given.",
    "round-A_JuniorMini_evaluation": 5,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "include (junior:5)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Practice of the conformer enhanced AUDIO-VISUAL HUBERT on Mandarin and English",
    "abstract": "Considering the bimodal nature of human speech perception, lips, and teeth movement has a pivotal role in automatic speech recognition. Benefiting from the correlated and noise-invariant visual information, audio-visual recognition systems enhance robustness in multiple scenarios. In previous work, audio-visual HuBERT appears to be the finest practice incorporating modality knowledge. This paper outlines a mixed methodology, named conformer enhanced AV-HuBERT, boosting the AV-HuBERT system's performance a step further. Compared with baseline AV-HuBERT, our method in the one-phase evaluation of clean and noisy conditions achieves 7% and 16% relative WER reduction on the English AVSR benchmark dataset LRS3. Furthermore, we establish a novel 1000h Mandarin AVSR dataset CSTS. On top of the baseline AV-HuBERT, we exceed the WeNet ASR system by 14% and 18% relatively on MISP and CMLR by pre-training with this dataset. The conformer-enhanced AV-HuBERT we proposed brings 7% on MISP and 6% CER reduction on CMLR, compared with the baseline AV-HuBERT system.",
    "metadata": {
      "arxiv_id": "2303.12187",
      "title": "Practice of the conformer enhanced AUDIO-VISUAL HUBERT on Mandarin and English",
      "summary": "Considering the bimodal nature of human speech perception, lips, and teeth movement has a pivotal role in automatic speech recognition. Benefiting from the correlated and noise-invariant visual information, audio-visual recognition systems enhance robustness in multiple scenarios. In previous work, audio-visual HuBERT appears to be the finest practice incorporating modality knowledge. This paper outlines a mixed methodology, named conformer enhanced AV-HuBERT, boosting the AV-HuBERT system's performance a step further. Compared with baseline AV-HuBERT, our method in the one-phase evaluation of clean and noisy conditions achieves 7% and 16% relative WER reduction on the English AVSR benchmark dataset LRS3. Furthermore, we establish a novel 1000h Mandarin AVSR dataset CSTS. On top of the baseline AV-HuBERT, we exceed the WeNet ASR system by 14% and 18% relatively on MISP and CMLR by pre-training with this dataset. The conformer-enhanced AV-HuBERT we proposed brings 7% on MISP and 6% CER reduction on CMLR, compared with the baseline AV-HuBERT system.",
      "authors": [
        "Xiaoming Ren",
        "Chao Li",
        "Shenjian Wang",
        "Biao Li"
      ],
      "published": "2023-02-28T02:10:13Z",
      "updated": "2023-02-28T02:10:13Z",
      "categories": [
        "eess.AS",
        "cs.AI",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2303.12187v1",
      "landing_url": "https://arxiv.org/abs/2303.12187v1",
      "doi": "https://doi.org/10.48550/arXiv.2303.12187"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The abstract discusses conformer-enhanced AV-HuBERT and performance improvements in audio-visual speech recognition but does not explicitly propose, analyze, or evaluate discrete audio/tokenization into a finite vocabulary; thus it does not meet the explicit core criterion for tokenization-focused studies.",
      "evaluation": 2
    },
    "round-A_JuniorNano_reasoning": "The abstract discusses conformer-enhanced AV-HuBERT and performance improvements in audio-visual speech recognition but does not explicitly propose, analyze, or evaluate discrete audio/tokenization into a finite vocabulary; thus it does not meet the explicit core criterion for tokenization-focused studies.",
    "round-A_JuniorNano_evaluation": 2,
    "round-A_JuniorMini_output": {
      "reasoning": "The study primarily focuses on enhancing an audio-visual speech recognition system (AV-HuBERT) by integrating a conformer module and evaluating its performance on English and Mandarin datasets; however, the abstract does not explicitly mention proposing, analyzing, or evaluating discrete audio/speech tokenization, nor does it discuss discrete token representations or quantization methods central to discrete tokenization; it seems centered on system performance improvements without clear focus on discrete token design or analysis, thus it does not fully satisfy the inclusion criteria nor fully align with exclusion criteria based on the abstract alone.",
      "evaluation": 2
    },
    "round-A_JuniorMini_reasoning": "The study primarily focuses on enhancing an audio-visual speech recognition system (AV-HuBERT) by integrating a conformer module and evaluating its performance on English and Mandarin datasets; however, the abstract does not explicitly mention proposing, analyzing, or evaluating discrete audio/speech tokenization, nor does it discuss discrete token representations or quantization methods central to discrete tokenization; it seems centered on system performance improvements without clear focus on discrete token design or analysis, thus it does not fully satisfy the inclusion criteria nor fully align with exclusion criteria based on the abstract alone.",
    "round-A_JuniorMini_evaluation": 2,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:2)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:2)"
  },
  {
    "title": "Time-domain Speech Enhancement Assisted by Multi-resolution Frequency Encoder and Decoder",
    "abstract": "Time-domain speech enhancement (SE) has recently been intensively investigated. Among recent works, DEMUCS introduces multi-resolution STFT loss to enhance performance. However, some resolutions used for STFT contain non-stationary signals, and it is challenging to learn multi-resolution frequency losses simultaneously with only one output. For better use of multi-resolution frequency information, we supplement multiple spectrograms in different frame lengths into the time-domain encoders. They extract stationary frequency information in both narrowband and wideband. We also adopt multiple decoder outputs, each of which computes its corresponding resolution frequency loss. Experimental results show that (1) it is more effective to fuse stationary frequency features than non-stationary features in the encoder, and (2) the multiple outputs consistent with the frequency loss improve performance. Experiments on the Voice-Bank dataset show that the proposed method obtained a 0.14 PESQ improvement.",
    "metadata": {
      "arxiv_id": "2303.14593",
      "title": "Time-domain Speech Enhancement Assisted by Multi-resolution Frequency Encoder and Decoder",
      "summary": "Time-domain speech enhancement (SE) has recently been intensively investigated. Among recent works, DEMUCS introduces multi-resolution STFT loss to enhance performance. However, some resolutions used for STFT contain non-stationary signals, and it is challenging to learn multi-resolution frequency losses simultaneously with only one output. For better use of multi-resolution frequency information, we supplement multiple spectrograms in different frame lengths into the time-domain encoders. They extract stationary frequency information in both narrowband and wideband. We also adopt multiple decoder outputs, each of which computes its corresponding resolution frequency loss. Experimental results show that (1) it is more effective to fuse stationary frequency features than non-stationary features in the encoder, and (2) the multiple outputs consistent with the frequency loss improve performance. Experiments on the Voice-Bank dataset show that the proposed method obtained a 0.14 PESQ improvement.",
      "authors": [
        "Hao Shi",
        "Masato Mimura",
        "Longbiao Wang",
        "Jianwu Dang",
        "Tatsuya Kawahara"
      ],
      "published": "2023-03-26T00:30:06Z",
      "updated": "2023-03-26T00:30:06Z",
      "categories": [
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2303.14593v1",
      "landing_url": "https://arxiv.org/abs/2303.14593v1",
      "doi": "https://doi.org/10.48550/arXiv.2303.14593"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The work concerns time-domain speech enhancement and multiple frequency representations, but it does not propose, analyze, or evaluate discrete audio tokenization into discrete symbols; it relies on continuous speech representations (e.g., spectrogram-like features) rather than discrete tokenization suited to language-model-style token sequences.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The work concerns time-domain speech enhancement and multiple frequency representations, but it does not propose, analyze, or evaluate discrete audio tokenization into discrete symbols; it relies on continuous speech representations (e.g., spectrogram-like features) rather than discrete tokenization suited to language-model-style token sequences.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on time-domain speech enhancement using multi-resolution frequency encoders and decoders to improve speech quality, but it does not mention any discrete audio/speech tokenization or discrete quantization/token design; instead, it operates on continuous audio features and spectrograms with no evidence of converting continuous audio into discrete tokens for reconstruction or generation purposes.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on time-domain speech enhancement using multi-resolution frequency encoders and decoders to improve speech quality, but it does not mention any discrete audio/speech tokenization or discrete quantization/token design; instead, it operates on continuous audio features and spectrograms with no evidence of converting continuous audio into discrete tokens for reconstruction or generation purposes.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "On the Stepwise Nature of Self-Supervised Learning",
    "abstract": "We present a simple picture of the training process of joint embedding self-supervised learning methods. We find that these methods learn their high-dimensional embeddings one dimension at a time in a sequence of discrete, well-separated steps. We arrive at this conclusion via the study of a linearized model of Barlow Twins applicable to the case in which the trained network is infinitely wide. We solve the training dynamics of this model from small initialization, finding that the model learns the top eigenmodes of a certain contrastive kernel in a stepwise fashion, and obtain a closed-form expression for the final learned representations. Remarkably, we then see the same stepwise learning phenomenon when training deep ResNets using the Barlow Twins, SimCLR, and VICReg losses. Our theory suggests that, just as kernel regression can be thought of as a model of supervised learning, kernel PCA may serve as a useful model of self-supervised learning.",
    "metadata": {
      "arxiv_id": "2303.15438",
      "title": "On the Stepwise Nature of Self-Supervised Learning",
      "summary": "We present a simple picture of the training process of joint embedding self-supervised learning methods. We find that these methods learn their high-dimensional embeddings one dimension at a time in a sequence of discrete, well-separated steps. We arrive at this conclusion via the study of a linearized model of Barlow Twins applicable to the case in which the trained network is infinitely wide. We solve the training dynamics of this model from small initialization, finding that the model learns the top eigenmodes of a certain contrastive kernel in a stepwise fashion, and obtain a closed-form expression for the final learned representations. Remarkably, we then see the same stepwise learning phenomenon when training deep ResNets using the Barlow Twins, SimCLR, and VICReg losses. Our theory suggests that, just as kernel regression can be thought of as a model of supervised learning, kernel PCA may serve as a useful model of self-supervised learning.",
      "authors": [
        "James B. Simon",
        "Maksis Knutins",
        "Liu Ziyin",
        "Daniel Geisz",
        "Abraham J. Fetterman",
        "Joshua Albrecht"
      ],
      "published": "2023-03-27T17:59:20Z",
      "updated": "2023-05-30T17:25:42Z",
      "categories": [
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2303.15438v2",
      "landing_url": "https://arxiv.org/abs/2303.15438v2",
      "doi": "https://doi.org/10.48550/arXiv.2303.15438"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The abstract concerns stepwise learning dynamics in self-supervised vision-language embedding methods and kernel interpretations, with no discrete audio/speech tokenization or analysis of discrete audio tokens, thus failing the inclusion criteria and offering no relevant exclusion-based justification for inclusion.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The abstract concerns stepwise learning dynamics in self-supervised vision-language embedding methods and kernel interpretations, with no discrete audio/speech tokenization or analysis of discrete audio tokens, thus failing the inclusion criteria and offering no relevant exclusion-based justification for inclusion.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on the training process of self-supervised learning methods analyzing embedding dimensions and learning dynamics but does not discuss discrete audio or speech tokenization, nor does it address converting continuous audio signals into discrete tokens for audio tasks, thus failing to meet the inclusion criteria and fitting the exclusion criteria related to absence of discrete tokenization analysis.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on the training process of self-supervised learning methods analyzing embedding dimensions and learning dynamics but does not discuss discrete audio or speech tokenization, nor does it address converting continuous audio signals into discrete tokens for audio tasks, thus failing to meet the inclusion criteria and fitting the exclusion criteria related to absence of discrete tokenization analysis.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "TorchAudio-Squim: Reference-less Speech Quality and Intelligibility measures in TorchAudio",
    "abstract": "Measuring quality and intelligibility of a speech signal is usually a critical step in development of speech processing systems. To enable this, a variety of metrics to measure quality and intelligibility under different assumptions have been developed. Through this paper, we introduce tools and a set of models to estimate such known metrics using deep neural networks. These models are made available in the well-established TorchAudio library, the core audio and speech processing library within the PyTorch deep learning framework. We refer to it as TorchAudio-Squim, TorchAudio-Speech QUality and Intelligibility Measures. More specifically, in the current version of TorchAudio-squim, we establish and release models for estimating PESQ, STOI and SI-SDR among objective metrics and MOS among subjective metrics. We develop a novel approach for objective metric estimation and use a recently developed approach for subjective metric estimation. These models operate in a ``reference-less\" manner, that is they do not require the corresponding clean speech as reference for speech assessment. Given the unavailability of clean speech and the effortful process of subjective evaluation in real-world situations, such easy-to-use tools would greatly benefit speech processing research and development.",
    "metadata": {
      "arxiv_id": "2304.01448",
      "title": "TorchAudio-Squim: Reference-less Speech Quality and Intelligibility measures in TorchAudio",
      "summary": "Measuring quality and intelligibility of a speech signal is usually a critical step in development of speech processing systems. To enable this, a variety of metrics to measure quality and intelligibility under different assumptions have been developed. Through this paper, we introduce tools and a set of models to estimate such known metrics using deep neural networks. These models are made available in the well-established TorchAudio library, the core audio and speech processing library within the PyTorch deep learning framework. We refer to it as TorchAudio-Squim, TorchAudio-Speech QUality and Intelligibility Measures. More specifically, in the current version of TorchAudio-squim, we establish and release models for estimating PESQ, STOI and SI-SDR among objective metrics and MOS among subjective metrics. We develop a novel approach for objective metric estimation and use a recently developed approach for subjective metric estimation. These models operate in a ``reference-less\" manner, that is they do not require the corresponding clean speech as reference for speech assessment. Given the unavailability of clean speech and the effortful process of subjective evaluation in real-world situations, such easy-to-use tools would greatly benefit speech processing research and development.",
      "authors": [
        "Anurag Kumar",
        "Ke Tan",
        "Zhaoheng Ni",
        "Pranay Manocha",
        "Xiaohui Zhang",
        "Ethan Henderson",
        "Buye Xu"
      ],
      "published": "2023-04-04T01:44:24Z",
      "updated": "2023-04-04T01:44:24Z",
      "categories": [
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2304.01448v1",
      "landing_url": "https://arxiv.org/abs/2304.01448v1",
      "doi": "https://doi.org/10.48550/arXiv.2304.01448"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The paper presents reference-less estimation of speech quality metrics using neural networks within TorchAudio, with no discrete audio tokenization or analysis of converting continuous speech into discrete tokens.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The paper presents reference-less estimation of speech quality metrics using neural networks within TorchAudio, with no discrete audio tokenization or analysis of converting continuous speech into discrete tokens.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper introduces TorchAudio-Squim, a set of reference-less speech quality and intelligibility estimation models integrated into TorchAudio, focusing on predicting metrics like PESQ, STOI, SI-SDR, and MOS without requiring clean references. However, the study does not propose or analyze any discrete audio/speech tokenization methods; it rather presents tools for speech quality estimation using continuous signal processing and neural model-based assessment. Additionally, it appears more like an engineering/tool description without a core methodological contribution related to discrete tokenization. Therefore, it does not meet the inclusion criteria and aligns with exclusion criteria regarding a lack of discrete tokenization focus and being a library/tool description without discrete tokenization analysis.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper introduces TorchAudio-Squim, a set of reference-less speech quality and intelligibility estimation models integrated into TorchAudio, focusing on predicting metrics like PESQ, STOI, SI-SDR, and MOS without requiring clean references. However, the study does not propose or analyze any discrete audio/speech tokenization methods; it rather presents tools for speech quality estimation using continuous signal processing and neural model-based assessment. Additionally, it appears more like an engineering/tool description without a core methodological contribution related to discrete tokenization. Therefore, it does not meet the inclusion criteria and aligns with exclusion criteria regarding a lack of discrete tokenization focus and being a library/tool description without discrete tokenization analysis.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "FineRecon: Depth-aware Feed-forward Network for Detailed 3D Reconstruction",
    "abstract": "Recent works on 3D reconstruction from posed images have demonstrated that direct inference of scene-level 3D geometry without test-time optimization is feasible using deep neural networks, showing remarkable promise and high efficiency. However, the reconstructed geometry, typically represented as a 3D truncated signed distance function (TSDF), is often coarse without fine geometric details. To address this problem, we propose three effective solutions for improving the fidelity of inference-based 3D reconstructions. We first present a resolution-agnostic TSDF supervision strategy to provide the network with a more accurate learning signal during training, avoiding the pitfalls of TSDF interpolation seen in previous work. We then introduce a depth guidance strategy using multi-view depth estimates to enhance the scene representation and recover more accurate surfaces. Finally, we develop a novel architecture for the final layers of the network, conditioning the output TSDF prediction on high-resolution image features in addition to coarse voxel features, enabling sharper reconstruction of fine details. Our method, FineRecon, produces smooth and highly accurate reconstructions, showing significant improvements across multiple depth and 3D reconstruction metrics.",
    "metadata": {
      "arxiv_id": "2304.01480",
      "title": "FineRecon: Depth-aware Feed-forward Network for Detailed 3D Reconstruction",
      "summary": "Recent works on 3D reconstruction from posed images have demonstrated that direct inference of scene-level 3D geometry without test-time optimization is feasible using deep neural networks, showing remarkable promise and high efficiency. However, the reconstructed geometry, typically represented as a 3D truncated signed distance function (TSDF), is often coarse without fine geometric details. To address this problem, we propose three effective solutions for improving the fidelity of inference-based 3D reconstructions. We first present a resolution-agnostic TSDF supervision strategy to provide the network with a more accurate learning signal during training, avoiding the pitfalls of TSDF interpolation seen in previous work. We then introduce a depth guidance strategy using multi-view depth estimates to enhance the scene representation and recover more accurate surfaces. Finally, we develop a novel architecture for the final layers of the network, conditioning the output TSDF prediction on high-resolution image features in addition to coarse voxel features, enabling sharper reconstruction of fine details. Our method, FineRecon, produces smooth and highly accurate reconstructions, showing significant improvements across multiple depth and 3D reconstruction metrics.",
      "authors": [
        "Noah Stier",
        "Anurag Ranjan",
        "Alex Colburn",
        "Yajie Yan",
        "Liang Yang",
        "Fangchang Ma",
        "Baptiste Angles"
      ],
      "published": "2023-04-04T02:50:29Z",
      "updated": "2023-08-18T22:35:08Z",
      "categories": [
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2304.01480v2",
      "landing_url": "https://arxiv.org/abs/2304.01480v2",
      "doi": "https://doi.org/10.48550/arXiv.2304.01480"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item focuses on 3D reconstruction from images with TSDF representations and depth-guided networks; there is no discrete audio/speech tokenization or analysis of token-based representations, so it does not meet the specified inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item focuses on 3D reconstruction from images with TSDF representations and depth-guided networks; there is no discrete audio/speech tokenization or analysis of token-based representations, so it does not meet the specified inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on 3D reconstruction from posed images using neural networks, without any mention or focus on audio or speech tokenization, discrete audio symbols, or related tasks; it thus does not meet the inclusion criteria related to discrete audio tokenization, and is irrelevant to the target domain described.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on 3D reconstruction from posed images using neural networks, without any mention or focus on audio or speech tokenization, discrete audio symbols, or related tasks; it thus does not meet the inclusion criteria related to discrete audio tokenization, and is irrelevant to the target domain described.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Pac-HuBERT: Self-Supervised Music Source Separation via Primitive Auditory Clustering and Hidden-Unit BERT",
    "abstract": "In spite of the progress in music source separation research, the small amount of publicly-available clean source data remains a constant limiting factor for performance. Thus, recent advances in self-supervised learning present a largely-unexplored opportunity for improving separation models by leveraging unlabelled music data. In this paper, we propose a self-supervised learning framework for music source separation inspired by the HuBERT speech representation model. We first investigate the potential impact of the original HuBERT model by inserting an adapted version of it into the well-known Demucs V2 time-domain separation model architecture. We then propose a time-frequency-domain self-supervised model, Pac-HuBERT (for primitive auditory clustering HuBERT), that we later use in combination with a Res-U-Net decoder for source separation. Pac-HuBERT uses primitive auditory features of music as unsupervised clustering labels to initialize the self-supervised pretraining process using the Free Music Archive (FMA) dataset. The resulting framework achieves better source-to-distortion ratio (SDR) performance on the MusDB18 test set than the original Demucs V2 and Res-U-Net models. We further demonstrate that it can boost performance with small amounts of supervised data. Ultimately, our proposed framework is an effective solution to the challenge of limited clean source data for music source separation.",
    "metadata": {
      "arxiv_id": "2304.02160",
      "title": "Pac-HuBERT: Self-Supervised Music Source Separation via Primitive Auditory Clustering and Hidden-Unit BERT",
      "summary": "In spite of the progress in music source separation research, the small amount of publicly-available clean source data remains a constant limiting factor for performance. Thus, recent advances in self-supervised learning present a largely-unexplored opportunity for improving separation models by leveraging unlabelled music data. In this paper, we propose a self-supervised learning framework for music source separation inspired by the HuBERT speech representation model. We first investigate the potential impact of the original HuBERT model by inserting an adapted version of it into the well-known Demucs V2 time-domain separation model architecture. We then propose a time-frequency-domain self-supervised model, Pac-HuBERT (for primitive auditory clustering HuBERT), that we later use in combination with a Res-U-Net decoder for source separation. Pac-HuBERT uses primitive auditory features of music as unsupervised clustering labels to initialize the self-supervised pretraining process using the Free Music Archive (FMA) dataset. The resulting framework achieves better source-to-distortion ratio (SDR) performance on the MusDB18 test set than the original Demucs V2 and Res-U-Net models. We further demonstrate that it can boost performance with small amounts of supervised data. Ultimately, our proposed framework is an effective solution to the challenge of limited clean source data for music source separation.",
      "authors": [
        "Ke Chen",
        "Gordon Wichern",
        "François G. Germain",
        "Jonathan Le Roux"
      ],
      "published": "2023-04-04T23:19:53Z",
      "updated": "2023-04-04T23:19:53Z",
      "categories": [
        "cs.SD",
        "cs.LG",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2304.02160v1",
      "landing_url": "https://arxiv.org/abs/2304.02160v1",
      "doi": "https://doi.org/10.48550/arXiv.2304.02160"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The paper explicitly uses discrete tokenization via primitive auditory clustering as self-supervised targets (aligned with HuBERT-style tokenization) for music source separation, meeting the inclusion criterion, but it focuses on non-speech music audio and does not address applying or evaluating the discrete tokenization in speech tasks, triggering exclusion under the non-speech tokenization criterion.",
      "evaluation": 2
    },
    "round-A_JuniorNano_reasoning": "The paper explicitly uses discrete tokenization via primitive auditory clustering as self-supervised targets (aligned with HuBERT-style tokenization) for music source separation, meeting the inclusion criterion, but it focuses on non-speech music audio and does not address applying or evaluating the discrete tokenization in speech tasks, triggering exclusion under the non-speech tokenization criterion.",
    "round-A_JuniorNano_evaluation": 2,
    "round-A_JuniorMini_output": {
      "reasoning": "The study proposes Pac-HuBERT, a self-supervised learning model that involves clustering primitive auditory features as unsupervised labels and is inspired by HuBERT, which is known for discrete audio tokenization; however, the focus of this work is on music source separation and it is not explicit whether it converts continuous audio signals into discrete token sequences used for audio reconstruction or generation as defined in the inclusion criteria. Since the tokenization aspect as core to the study is not clearly analyzed or evaluated for discrete tokens in the abstract, it is unclear if the work meets the strict inclusion criteria, leading to uncertainty about inclusion.",
      "evaluation": 3
    },
    "round-A_JuniorMini_reasoning": "The study proposes Pac-HuBERT, a self-supervised learning model that involves clustering primitive auditory features as unsupervised labels and is inspired by HuBERT, which is known for discrete audio tokenization; however, the focus of this work is on music source separation and it is not explicit whether it converts continuous audio signals into discrete token sequences used for audio reconstruction or generation as defined in the inclusion criteria. Since the tokenization aspect as core to the study is not clearly analyzed or evaluated for discrete tokens in the abstract, it is unclear if the work meets the strict inclusion criteria, leading to uncertainty about inclusion.",
    "round-A_JuniorMini_evaluation": 3,
    "round-B_SeniorLead_output": {
      "reasoning": "I reviewed the two junior reviewers' comments: JuniorNano argued the work uses HuBERT-style discrete tokenization but focuses on non-speech music and thus triggers the non-speech-tokenization exclusion; JuniorMini noted the paper is inspired by HuBERT and uses clustering labels but the abstract does not clearly state conversion to discrete token sequences for reconstruction/generation or any speech evaluation. Because the study applies primitive auditory clustering only for music source separation and does not analyze or evaluate discrete audio/speech tokenization with speech-relevant implications (nor clearly frame tokens for reconstruction/generation), it fails the inclusion requirements and matches the non-speech-tokenization exclusion. Final one-sentence explanation: Exclude — the paper uses clustering-based self-supervision for music separation but does not explicitly propose, analyze, or evaluate discrete audio/speech tokenization applicable to speech or token-based reconstruction/generation as required.",
      "evaluation": 2
    },
    "round-B_SeniorLead_reasoning": "I reviewed the two junior reviewers' comments: JuniorNano argued the work uses HuBERT-style discrete tokenization but focuses on non-speech music and thus triggers the non-speech-tokenization exclusion; JuniorMini noted the paper is inspired by HuBERT and uses clustering labels but the abstract does not clearly state conversion to discrete token sequences for reconstruction/generation or any speech evaluation. Because the study applies primitive auditory clustering only for music source separation and does not analyze or evaluate discrete audio/speech tokenization with speech-relevant implications (nor clearly frame tokens for reconstruction/generation), it fails the inclusion requirements and matches the non-speech-tokenization exclusion. Final one-sentence explanation: Exclude — the paper uses clustering-based self-supervision for music separation but does not explicitly propose, analyze, or evaluate discrete audio/speech tokenization applicable to speech or token-based reconstruction/generation as required.",
    "round-B_SeniorLead_evaluation": 2,
    "final_verdict": "exclude (senior:2)",
    "review_skipped": false,
    "discard_reason": "exclude (senior:2)"
  },
  {
    "title": "Unsupervised Speech Representation Pooling Using Vector Quantization",
    "abstract": "With the advent of general-purpose speech representations from large-scale self-supervised models, applying a single model to multiple downstream tasks is becoming a de-facto approach. However, the pooling problem remains; the length of speech representations is inherently variable. The naive average pooling is often used, even though it ignores the characteristics of speech, such as differently lengthed phonemes. Hence, we design a novel pooling method to squash acoustically similar representations via vector quantization, which does not require additional training, unlike attention-based pooling. Further, we evaluate various unsupervised pooling methods on various self-supervised models. We gather diverse methods scattered around speech and text to evaluate on various tasks: keyword spotting, speaker identification, intent classification, and emotion recognition. Finally, we quantitatively and qualitatively analyze our method, comparing it with supervised pooling methods.",
    "metadata": {
      "arxiv_id": "2304.03940",
      "title": "Unsupervised Speech Representation Pooling Using Vector Quantization",
      "summary": "With the advent of general-purpose speech representations from large-scale self-supervised models, applying a single model to multiple downstream tasks is becoming a de-facto approach. However, the pooling problem remains; the length of speech representations is inherently variable. The naive average pooling is often used, even though it ignores the characteristics of speech, such as differently lengthed phonemes. Hence, we design a novel pooling method to squash acoustically similar representations via vector quantization, which does not require additional training, unlike attention-based pooling. Further, we evaluate various unsupervised pooling methods on various self-supervised models. We gather diverse methods scattered around speech and text to evaluate on various tasks: keyword spotting, speaker identification, intent classification, and emotion recognition. Finally, we quantitatively and qualitatively analyze our method, comparing it with supervised pooling methods.",
      "authors": [
        "Jeongkyun Park",
        "Kwanghee Choi",
        "Hyunjun Heo",
        "Hyung-Min Park"
      ],
      "published": "2023-04-08T07:03:01Z",
      "updated": "2023-04-08T07:03:01Z",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2304.03940v1",
      "landing_url": "https://arxiv.org/abs/2304.03940v1",
      "doi": "https://doi.org/10.48550/arXiv.2304.03940"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "Although the paper centers on unsupervised pooling, it relies on vector quantization to produce discrete speech tokens from continuous representations, effectively analyzing a discrete tokenization mechanism for speech and evaluating it across tasks; thus it meets inclusion criteria and should be included.",
      "evaluation": 4
    },
    "round-A_JuniorNano_reasoning": "Although the paper centers on unsupervised pooling, it relies on vector quantization to produce discrete speech tokens from continuous representations, effectively analyzing a discrete tokenization mechanism for speech and evaluating it across tasks; thus it meets inclusion criteria and should be included.",
    "round-A_JuniorNano_evaluation": 4,
    "round-A_JuniorMini_output": {
      "reasoning": "The study proposes a novel unsupervised pooling method using vector quantization for speech representations, directly involving discrete tokenization of audio data by converting continuous speech features into discrete symbols for various downstream tasks like keyword spotting and speaker identification, fulfilling the inclusion criteria and avoiding exclusion criteria based on its methodological contribution and focus on discrete speech tokens rather than traditional continuous features.",
      "evaluation": 5
    },
    "round-A_JuniorMini_reasoning": "The study proposes a novel unsupervised pooling method using vector quantization for speech representations, directly involving discrete tokenization of audio data by converting continuous speech features into discrete symbols for various downstream tasks like keyword spotting and speaker identification, fulfilling the inclusion criteria and avoiding exclusion criteria based on its methodological contribution and focus on discrete speech tokens rather than traditional continuous features.",
    "round-A_JuniorMini_evaluation": 5,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "include (junior:4)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "GreedyGD: Enhanced Generalized Deduplication for Direct Analytics in IoT",
    "abstract": "Exponential growth in the amount of data generated by the Internet of Things currently pose significant challenges for data communication, storage and analytics and leads to high costs for organisations hoping to leverage their data. Novel techniques are therefore needed to holistically improve the efficiency of data storage and analytics in IoT systems. The emerging compression technique Generalized Deduplication (GD) has been shown to deliver high compression and enable direct compressed data analytics with low storage and memory requirements. In this paper, we propose a new GD-based data compression algorithm called GreedyGD that is designed for analytics. Compared to existing versions of GD, GreedyGD enables more reliable analytics with less data, while running 11.2x faster and delivering even better compression.",
    "metadata": {
      "arxiv_id": "2304.07240",
      "title": "GreedyGD: Enhanced Generalized Deduplication for Direct Analytics in IoT",
      "summary": "Exponential growth in the amount of data generated by the Internet of Things currently pose significant challenges for data communication, storage and analytics and leads to high costs for organisations hoping to leverage their data. Novel techniques are therefore needed to holistically improve the efficiency of data storage and analytics in IoT systems. The emerging compression technique Generalized Deduplication (GD) has been shown to deliver high compression and enable direct compressed data analytics with low storage and memory requirements. In this paper, we propose a new GD-based data compression algorithm called GreedyGD that is designed for analytics. Compared to existing versions of GD, GreedyGD enables more reliable analytics with less data, while running 11.2x faster and delivering even better compression.",
      "authors": [
        "Aaron Hurst",
        "Daniel E. Lucani",
        "Qi Zhang"
      ],
      "published": "2023-04-14T16:43:31Z",
      "updated": "2023-04-14T16:43:31Z",
      "categories": [
        "cs.DB"
      ],
      "pdf_url": "https://arxiv.org/pdf/2304.07240v1",
      "landing_url": "https://arxiv.org/abs/2304.07240v1",
      "doi": "https://doi.org/10.48550/arXiv.2304.07240"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The abstract focuses on a generalized deduplication compression algorithm for IoT analytics and does not involve discrete audio/speech tokenization, token design, or evaluation of discrete audio tokens, so it does not meet the required inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The abstract focuses on a generalized deduplication compression algorithm for IoT analytics and does not involve discrete audio/speech tokenization, token design, or evaluation of discrete audio tokens, so it does not meet the required inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study proposes a data compression algorithm for IoT data analytics without any mention of audio or speech tokenization, discrete audio symbol sequences, or evaluation related to audio or speech tasks, thus failing to meet the inclusion criteria focused on discrete audio/speech tokenization and discrete tokenization relevance; it also does not touch on neural encoding or language-model-style discrete tokens, and it is not about audio data but IoT general data.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study proposes a data compression algorithm for IoT data analytics without any mention of audio or speech tokenization, discrete audio symbol sequences, or evaluation related to audio or speech tasks, thus failing to meet the inclusion criteria focused on discrete audio/speech tokenization and discrete tokenization relevance; it also does not touch on neural encoding or language-model-style discrete tokens, and it is not about audio data but IoT general data.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Evaluating Strong Idempotence of Image Codec",
    "abstract": "In this paper, we first propose the concept of strong idempotent codec based on idempotent codec. The idempotence of codec refers to the stability of codec to re-compression. Similarly, we define the strong idempotence of codec as the stability of codec to multiple quality re-compression, which is an important feature of codec in the context of cloud transcoding. We provide a detailed example of strong idempotent codec with known source distribution. Further, we formalize a testing protocol to evaluate the strong idempotence of image codec. And finally, we evaluate the strong idempotence of current image codecs, including traditional codec and recent neural codec. Experimental results show that current image codecs are not stable to multi-round re-compression with different qualities, even if they are close to idempotent.",
    "metadata": {
      "arxiv_id": "2304.08269",
      "title": "Evaluating Strong Idempotence of Image Codec",
      "summary": "In this paper, we first propose the concept of strong idempotent codec based on idempotent codec. The idempotence of codec refers to the stability of codec to re-compression. Similarly, we define the strong idempotence of codec as the stability of codec to multiple quality re-compression, which is an important feature of codec in the context of cloud transcoding. We provide a detailed example of strong idempotent codec with known source distribution. Further, we formalize a testing protocol to evaluate the strong idempotence of image codec. And finally, we evaluate the strong idempotence of current image codecs, including traditional codec and recent neural codec. Experimental results show that current image codecs are not stable to multi-round re-compression with different qualities, even if they are close to idempotent.",
      "authors": [
        "Qian Zhang",
        "Tongda Xu",
        "Yanghao Li",
        "Yan Wang"
      ],
      "published": "2023-04-17T13:26:31Z",
      "updated": "2023-04-17T13:26:31Z",
      "categories": [
        "cs.MM"
      ],
      "pdf_url": "https://arxiv.org/pdf/2304.08269v1",
      "landing_url": "https://arxiv.org/abs/2304.08269v1",
      "doi": "https://doi.org/10.48550/arXiv.2304.08269"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This item focuses on image codecs and their idempotence, not on discrete audio/speech tokenization, so it does not meet the inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "This item focuses on image codecs and their idempotence, not on discrete audio/speech tokenization, so it does not meet the inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on evaluating strong idempotence of image codecs, dealing with image compression and stability upon re-compression, without any indication of discrete audio/speech tokenization or related audio signal processing. Therefore, it does not meet any of the inclusion criteria related to audio discrete tokenization, nor is it relevant to the exclusion of continuous audio features or audio tokenization methods.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on evaluating strong idempotence of image codecs, dealing with image compression and stability upon re-compression, without any indication of discrete audio/speech tokenization or related audio signal processing. Therefore, it does not meet any of the inclusion criteria related to audio discrete tokenization, nor is it relevant to the exclusion of continuous audio features or audio tokenization methods.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Coded Speech Quality Measurement by a Non-Intrusive PESQ-DNN",
    "abstract": "Wideband codecs such as AMR-WB or EVS are widely used in (mobile) speech communication. Evaluation of coded speech quality is often performed subjectively by an absolute category rating (ACR) listening test. However, the ACR test is impractical for online monitoring of speech communication networks. Perceptual evaluation of speech quality (PESQ) is one of the widely used metrics instrumentally predicting the results of an ACR test. However, the PESQ algorithm requires an original reference signal, which is usually unavailable in network monitoring, thus limiting its applicability. NISQA is a new non-intrusive neural-network-based speech quality measure, focusing on super-wideband speech signals. In this work, however, we aim at predicting the well-known PESQ metric using a non-intrusive PESQ-DNN model. We illustrate the potential of this model by predicting the PESQ scores of wideband-coded speech obtained from AMR-WB or EVS codecs operating at different bitrates in noisy, tandeming, and error-prone transmission conditions. We compare our methods with the state-of-the-art network topologies of QualityNet, WaweNet, and DNSMOS -- all applied to PESQ prediction -- by measuring the mean absolute error (MAE) and the linear correlation coefficient (LCC). The proposed PESQ-DNN offers the best total MAE and LCC of 0.11 and 0.92, respectively, in conditions without frame loss, and still is best when including frame loss. Note that our model could be similarly used to non-intrusively predict POLQA or other (intrusive) metrics. Upon article acceptance, code will be provided at GitHub.",
    "metadata": {
      "arxiv_id": "2304.09226",
      "title": "Coded Speech Quality Measurement by a Non-Intrusive PESQ-DNN",
      "summary": "Wideband codecs such as AMR-WB or EVS are widely used in (mobile) speech communication. Evaluation of coded speech quality is often performed subjectively by an absolute category rating (ACR) listening test. However, the ACR test is impractical for online monitoring of speech communication networks. Perceptual evaluation of speech quality (PESQ) is one of the widely used metrics instrumentally predicting the results of an ACR test. However, the PESQ algorithm requires an original reference signal, which is usually unavailable in network monitoring, thus limiting its applicability. NISQA is a new non-intrusive neural-network-based speech quality measure, focusing on super-wideband speech signals. In this work, however, we aim at predicting the well-known PESQ metric using a non-intrusive PESQ-DNN model. We illustrate the potential of this model by predicting the PESQ scores of wideband-coded speech obtained from AMR-WB or EVS codecs operating at different bitrates in noisy, tandeming, and error-prone transmission conditions. We compare our methods with the state-of-the-art network topologies of QualityNet, WaweNet, and DNSMOS -- all applied to PESQ prediction -- by measuring the mean absolute error (MAE) and the linear correlation coefficient (LCC). The proposed PESQ-DNN offers the best total MAE and LCC of 0.11 and 0.92, respectively, in conditions without frame loss, and still is best when including frame loss. Note that our model could be similarly used to non-intrusively predict POLQA or other (intrusive) metrics. Upon article acceptance, code will be provided at GitHub.",
      "authors": [
        "Ziyi Xu",
        "Ziyue Zhao",
        "Tim Fingscheidt"
      ],
      "published": "2023-04-18T18:26:56Z",
      "updated": "2023-04-18T18:26:56Z",
      "categories": [
        "eess.AS",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2304.09226v1",
      "landing_url": "https://arxiv.org/abs/2304.09226v1",
      "doi": "https://doi.org/10.48550/arXiv.2304.09226"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item discusses non-intrusive neural-network based prediction of PESQ scores for coded speech, with no focus on converting continuous audio into discrete tokens; it does not analyze or evaluate discrete audio tokenization, thus failing the inclusion criterion for tokenization as a core object of study.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item discusses non-intrusive neural-network based prediction of PESQ scores for coded speech, with no focus on converting continuous audio into discrete tokens; it does not analyze or evaluate discrete audio tokenization, thus failing the inclusion criterion for tokenization as a core object of study.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on non-intrusive prediction of speech quality metrics (PESQ) using a neural network model but does not propose, analyze, or evaluate discrete audio/speech tokenization methods converting continuous audio into discrete tokens; instead, it deals with continuous quality prediction without discrete token representation or analysis relevant to language-model-style token sequences.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on non-intrusive prediction of speech quality metrics (PESQ) using a neural network model but does not propose, analyze, or evaluate discrete audio/speech tokenization methods converting continuous audio into discrete tokens; instead, it deals with continuous quality prediction without discrete token representation or analysis relevant to language-model-style token sequences.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "To Compress or Not to Compress- Self-Supervised Learning and Information Theory: A Review",
    "abstract": "Deep neural networks excel in supervised learning tasks but are constrained by the need for extensive labeled data. Self-supervised learning emerges as a promising alternative, allowing models to learn without explicit labels. Information theory, and notably the information bottleneck principle, has been pivotal in shaping deep neural networks. This principle focuses on optimizing the trade-off between compression and preserving relevant information, providing a foundation for efficient network design in supervised contexts. However, its precise role and adaptation in self-supervised learning remain unclear. In this work, we scrutinize various self-supervised learning approaches from an information-theoretic perspective, introducing a unified framework that encapsulates the \\textit{self-supervised information-theoretic learning problem}. We weave together existing research into a cohesive narrative, delve into contemporary self-supervised methodologies, and spotlight potential research avenues and inherent challenges. Additionally, we discuss the empirical evaluation of information-theoretic quantities and their estimation methods. Overall, this paper furnishes an exhaustive review of the intersection of information theory, self-supervised learning, and deep neural networks.",
    "metadata": {
      "arxiv_id": "2304.09355",
      "title": "To Compress or Not to Compress- Self-Supervised Learning and Information Theory: A Review",
      "summary": "Deep neural networks excel in supervised learning tasks but are constrained by the need for extensive labeled data. Self-supervised learning emerges as a promising alternative, allowing models to learn without explicit labels. Information theory, and notably the information bottleneck principle, has been pivotal in shaping deep neural networks. This principle focuses on optimizing the trade-off between compression and preserving relevant information, providing a foundation for efficient network design in supervised contexts. However, its precise role and adaptation in self-supervised learning remain unclear. In this work, we scrutinize various self-supervised learning approaches from an information-theoretic perspective, introducing a unified framework that encapsulates the \\textit{self-supervised information-theoretic learning problem}. We weave together existing research into a cohesive narrative, delve into contemporary self-supervised methodologies, and spotlight potential research avenues and inherent challenges. Additionally, we discuss the empirical evaluation of information-theoretic quantities and their estimation methods. Overall, this paper furnishes an exhaustive review of the intersection of information theory, self-supervised learning, and deep neural networks.",
      "authors": [
        "Ravid Shwartz-Ziv",
        "Yann LeCun"
      ],
      "published": "2023-04-19T00:33:59Z",
      "updated": "2023-11-21T13:12:21Z",
      "categories": [
        "cs.LG",
        "cs.IT"
      ],
      "pdf_url": "https://arxiv.org/pdf/2304.09355v5",
      "landing_url": "https://arxiv.org/abs/2304.09355v5",
      "doi": "https://doi.org/10.48550/arXiv.2304.09355"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "Does not address discrete audio/speech tokenization; focuses on self-supervised learning and information theory with no analysis of discrete tokenization or discrete speech representations.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "Does not address discrete audio/speech tokenization; focuses on self-supervised learning and information theory with no analysis of discrete tokenization or discrete speech representations.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper reviews the intersection of information theory and self-supervised learning for deep neural networks but does not explicitly mention discrete audio or speech tokenization as a core focus, nor does it specify analysis or evaluation of discrete tokenization methods for audio signals; therefore, it does not meet the inclusion criteria related to discrete audio tokenization.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper reviews the intersection of information theory and self-supervised learning for deep neural networks but does not explicitly mention discrete audio or speech tokenization as a core focus, nor does it specify analysis or evaluation of discrete tokenization methods for audio signals; therefore, it does not meet the inclusion criteria related to discrete audio tokenization.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Semantic Tokenizer for Enhanced Natural Language Processing",
    "abstract": "Traditionally, NLP performance improvement has been focused on improving models and increasing the number of model parameters. NLP vocabulary construction has remained focused on maximizing the number of words represented through subword regularization. We present a novel tokenizer that uses semantics to drive vocabulary construction. The tokenizer includes a trainer that uses stemming to enhance subword formation. Further optimizations and adaptations are implemented to minimize the number of words that cannot be encoded. The encoder is updated to integrate with the trainer. The tokenizer is implemented as a drop-in replacement for the SentencePiece tokenizer. The new tokenizer more than doubles the number of wordforms represented in the vocabulary. The enhanced vocabulary significantly improves NLP model convergence, and improves quality of word and sentence embeddings. Our experimental results show top performance on two Glue tasks using BERT-base, improving on models more than 50X in size.",
    "metadata": {
      "arxiv_id": "2304.12404",
      "title": "Semantic Tokenizer for Enhanced Natural Language Processing",
      "summary": "Traditionally, NLP performance improvement has been focused on improving models and increasing the number of model parameters. NLP vocabulary construction has remained focused on maximizing the number of words represented through subword regularization. We present a novel tokenizer that uses semantics to drive vocabulary construction. The tokenizer includes a trainer that uses stemming to enhance subword formation. Further optimizations and adaptations are implemented to minimize the number of words that cannot be encoded. The encoder is updated to integrate with the trainer. The tokenizer is implemented as a drop-in replacement for the SentencePiece tokenizer. The new tokenizer more than doubles the number of wordforms represented in the vocabulary. The enhanced vocabulary significantly improves NLP model convergence, and improves quality of word and sentence embeddings. Our experimental results show top performance on two Glue tasks using BERT-base, improving on models more than 50X in size.",
      "authors": [
        "Sandeep Mehta",
        "Darpan Shah",
        "Ravindra Kulkarni",
        "Cornelia Caragea"
      ],
      "published": "2023-04-24T19:33:41Z",
      "updated": "2023-04-24T19:33:41Z",
      "categories": [
        "cs.CL"
      ],
      "pdf_url": "https://arxiv.org/pdf/2304.12404v1",
      "landing_url": "https://arxiv.org/abs/2304.12404v1",
      "doi": "https://doi.org/10.48550/arXiv.2304.12404"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This abstract addresses a text-based tokenizer for NLP and does not involve discrete audio/tokenization of speech signals, so it fails the inclusion criteria requiring discrete audio tokens for speech tasks.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "This abstract addresses a text-based tokenizer for NLP and does not involve discrete audio/tokenization of speech signals, so it fails the inclusion criteria requiring discrete audio tokens for speech tasks.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on a semantic tokenizer designed for natural language processing of text, with no mention of audio or speech tokenization, discrete audio symbolization, or any evaluation related to audio signals; thus, it does not meet the inclusion criteria regarding discrete audio/speech tokenization and instead solely addresses text tokenization methods.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on a semantic tokenizer designed for natural language processing of text, with no mention of audio or speech tokenization, discrete audio symbolization, or any evaluation related to audio signals; thus, it does not meet the inclusion criteria regarding discrete audio/speech tokenization and instead solely addresses text tokenization methods.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Self-Supervised 3D Scene Flow Estimation Guided by Superpoints",
    "abstract": "3D scene flow estimation aims to estimate point-wise motions between two consecutive frames of point clouds. Superpoints, i.e., points with similar geometric features, are usually employed to capture similar motions of local regions in 3D scenes for scene flow estimation. However, in existing methods, superpoints are generated with the offline clustering methods, which cannot characterize local regions with similar motions for complex 3D scenes well, leading to inaccurate scene flow estimation. To this end, we propose an iterative end-to-end superpoint based scene flow estimation framework, where the superpoints can be dynamically updated to guide the point-level flow prediction. Specifically, our framework consists of a flow guided superpoint generation module and a superpoint guided flow refinement module. In our superpoint generation module, we utilize the bidirectional flow information at the previous iteration to obtain the matching points of points and superpoint centers for soft point-to-superpoint association construction, in which the superpoints are generated for pairwise point clouds. With the generated superpoints, we first reconstruct the flow for each point by adaptively aggregating the superpoint-level flow, and then encode the consistency between the reconstructed flow of pairwise point clouds. Finally, we feed the consistency encoding along with the reconstructed flow into GRU to refine point-level flow. Extensive experiments on several different datasets show that our method can achieve promising performance.",
    "metadata": {
      "arxiv_id": "2305.02528",
      "title": "Self-Supervised 3D Scene Flow Estimation Guided by Superpoints",
      "summary": "3D scene flow estimation aims to estimate point-wise motions between two consecutive frames of point clouds. Superpoints, i.e., points with similar geometric features, are usually employed to capture similar motions of local regions in 3D scenes for scene flow estimation. However, in existing methods, superpoints are generated with the offline clustering methods, which cannot characterize local regions with similar motions for complex 3D scenes well, leading to inaccurate scene flow estimation. To this end, we propose an iterative end-to-end superpoint based scene flow estimation framework, where the superpoints can be dynamically updated to guide the point-level flow prediction. Specifically, our framework consists of a flow guided superpoint generation module and a superpoint guided flow refinement module. In our superpoint generation module, we utilize the bidirectional flow information at the previous iteration to obtain the matching points of points and superpoint centers for soft point-to-superpoint association construction, in which the superpoints are generated for pairwise point clouds. With the generated superpoints, we first reconstruct the flow for each point by adaptively aggregating the superpoint-level flow, and then encode the consistency between the reconstructed flow of pairwise point clouds. Finally, we feed the consistency encoding along with the reconstructed flow into GRU to refine point-level flow. Extensive experiments on several different datasets show that our method can achieve promising performance.",
      "authors": [
        "Yaqi Shen",
        "Le Hui",
        "Jin Xie",
        "Jian Yang"
      ],
      "published": "2023-05-04T03:33:40Z",
      "updated": "2023-05-04T03:33:40Z",
      "categories": [
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2305.02528v1",
      "landing_url": "https://arxiv.org/abs/2305.02528v1",
      "doi": "https://doi.org/10.48550/arXiv.2305.02528"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The article discusses self-supervised 3D scene flow estimation for point clouds and superpoints; there is no discrete audio tokenization or speech/token design analyzed, so it does not meet the audio-focused inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The article discusses self-supervised 3D scene flow estimation for point clouds and superpoints; there is no discrete audio tokenization or speech/token design analyzed, so it does not meet the audio-focused inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on 3D scene flow estimation using superpoints for point cloud data and does not address discrete audio or speech tokenization. There is no mention or indication of converting continuous audio signals into discrete tokens or analyzing discrete audio representations. Therefore, it does not meet the inclusion criteria related to discrete audio tokenization and is irrelevant to the specified topic.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on 3D scene flow estimation using superpoints for point cloud data and does not address discrete audio or speech tokenization. There is no mention or indication of converting continuous audio signals into discrete tokens or analyzing discrete audio representations. Therefore, it does not meet the inclusion criteria related to discrete audio tokenization and is irrelevant to the specified topic.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "A vector quantized masked autoencoder for audiovisual speech emotion recognition",
    "abstract": "An important challenge in emotion recognition is to develop methods that can leverage unlabeled training data. In this paper, we propose the VQ-MAE-AV model, a self-supervised multimodal model that leverages masked autoencoders to learn representations of audiovisual speech without labels. The model includes vector quantized variational autoencoders that compress raw audio and visual speech data into discrete tokens. The audiovisual speech tokens are used to train a multimodal masked autoencoder that consists of an encoder-decoder architecture with attention mechanisms. The model is designed to extract both local (i.e., at the frame level) and global (i.e., at the sequence level) representations of audiovisual speech. During self-supervised pre-training, the VQ-MAE-AV model is trained on a large-scale unlabeled dataset of audiovisual speech, for the task of reconstructing randomly masked audiovisual speech tokens and with a contrastive learning strategy. During this pre-training, the encoder learns to extract a representation of audiovisual speech that can be subsequently leveraged for emotion recognition. During the supervised fine-tuning stage, a small classification model is trained on top of the VQ-MAE-AV encoder for an emotion recognition task. The proposed approach achieves state-of-the-art emotion recognition results across several datasets in both controlled and in-the-wild conditions.",
    "metadata": {
      "arxiv_id": "2305.03568",
      "title": "A vector quantized masked autoencoder for audiovisual speech emotion recognition",
      "summary": "An important challenge in emotion recognition is to develop methods that can leverage unlabeled training data. In this paper, we propose the VQ-MAE-AV model, a self-supervised multimodal model that leverages masked autoencoders to learn representations of audiovisual speech without labels. The model includes vector quantized variational autoencoders that compress raw audio and visual speech data into discrete tokens. The audiovisual speech tokens are used to train a multimodal masked autoencoder that consists of an encoder-decoder architecture with attention mechanisms. The model is designed to extract both local (i.e., at the frame level) and global (i.e., at the sequence level) representations of audiovisual speech. During self-supervised pre-training, the VQ-MAE-AV model is trained on a large-scale unlabeled dataset of audiovisual speech, for the task of reconstructing randomly masked audiovisual speech tokens and with a contrastive learning strategy. During this pre-training, the encoder learns to extract a representation of audiovisual speech that can be subsequently leveraged for emotion recognition. During the supervised fine-tuning stage, a small classification model is trained on top of the VQ-MAE-AV encoder for an emotion recognition task. The proposed approach achieves state-of-the-art emotion recognition results across several datasets in both controlled and in-the-wild conditions.",
      "authors": [
        "Samir Sadok",
        "Simon Leglaive",
        "Renaud Séguier"
      ],
      "published": "2023-05-05T14:19:46Z",
      "updated": "2025-05-09T08:19:45Z",
      "categories": [
        "cs.SD",
        "cs.LG",
        "cs.MM",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2305.03568v3",
      "landing_url": "https://arxiv.org/abs/2305.03568v3",
      "doi": "https://doi.org/10.1016/j.cviu.2025.104362"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The abstract explicitly uses vector-quantized discrete tokens to represent audio/visual speech and trains a masked autoencoder on these discrete tokens, indicating discrete audio tokenization as the core object of study.",
      "evaluation": 5
    },
    "round-A_JuniorNano_reasoning": "The abstract explicitly uses vector-quantized discrete tokens to represent audio/visual speech and trains a masked autoencoder on these discrete tokens, indicating discrete audio tokenization as the core object of study.",
    "round-A_JuniorNano_evaluation": 5,
    "round-A_JuniorMini_output": {
      "reasoning": "The study clearly proposes a discrete tokenization approach for audiovisual speech using vector quantized variational autoencoders that convert raw audio and visual data into discrete tokens, which are then used for self-supervised learning and emotion recognition tasks. This directly addresses the inclusion criteria by treating discrete audio tokenization as a core component and involves methodological contribution and experimental validation, with the full text presumably available in English. Therefore, it meets all inclusion criteria and none of the exclusion criteria.",
      "evaluation": 5
    },
    "round-A_JuniorMini_reasoning": "The study clearly proposes a discrete tokenization approach for audiovisual speech using vector quantized variational autoencoders that convert raw audio and visual data into discrete tokens, which are then used for self-supervised learning and emotion recognition tasks. This directly addresses the inclusion criteria by treating discrete audio tokenization as a core component and involves methodological contribution and experimental validation, with the full text presumably available in English. Therefore, it meets all inclusion criteria and none of the exclusion criteria.",
    "round-A_JuniorMini_evaluation": 5,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "include (junior:5)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Vector Quantization with Error Uniformly Distributed over an Arbitrary Set",
    "abstract": "For uniform scalar quantization, the error distribution is approximately a uniform distribution over an interval (which is also a 1-dimensional ball). Nevertheless, for lattice vector quantization, the error distribution is uniform not over a ball, but over the basic cell of the quantization lattice. In this paper, we construct vector quantizers with periodic properties, where the error is uniformly distributed over the n-ball, or any other prescribed set. We then prove upper and lower bounds on the entropy of the quantized signals. We also discuss how our construction can be applied to give a randomized quantization scheme with a nonuniform error distribution.",
    "metadata": {
      "arxiv_id": "2305.06788",
      "title": "Vector Quantization with Error Uniformly Distributed over an Arbitrary Set",
      "summary": "For uniform scalar quantization, the error distribution is approximately a uniform distribution over an interval (which is also a 1-dimensional ball). Nevertheless, for lattice vector quantization, the error distribution is uniform not over a ball, but over the basic cell of the quantization lattice. In this paper, we construct vector quantizers with periodic properties, where the error is uniformly distributed over the n-ball, or any other prescribed set. We then prove upper and lower bounds on the entropy of the quantized signals. We also discuss how our construction can be applied to give a randomized quantization scheme with a nonuniform error distribution.",
      "authors": [
        "Chih Wei Ling",
        "Cheuk Ting Li"
      ],
      "published": "2023-05-11T13:23:42Z",
      "updated": "2024-01-24T13:44:44Z",
      "categories": [
        "cs.IT"
      ],
      "pdf_url": "https://arxiv.org/pdf/2305.06788v4",
      "landing_url": "https://arxiv.org/abs/2305.06788v4",
      "doi": "https://doi.org/10.48550/arXiv.2305.06788"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The paper focuses on vector quantization theory and error distribution bounds, not on discrete audio/speech tokenization as a core object for speech tasks.",
      "evaluation": 2
    },
    "round-A_JuniorNano_reasoning": "The paper focuses on vector quantization theory and error distribution bounds, not on discrete audio/speech tokenization as a core object for speech tasks.",
    "round-A_JuniorNano_evaluation": 2,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper focuses on vector quantization methods with error distributions over arbitrary sets and discusses entropy bounds and quantization schemes; however, it does not explicitly address discrete audio or speech tokenization, nor does it focus on converting continuous audio signals into discrete token sequences for speech-related tasks. Hence, it does not meet the inclusion criteria which require explicit focus on discrete audio/speech tokenization for generation or understanding tasks.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper focuses on vector quantization methods with error distributions over arbitrary sets and discusses entropy bounds and quantization schemes; however, it does not explicitly address discrete audio or speech tokenization, nor does it focus on converting continuous audio signals into discrete token sequences for speech-related tasks. Hence, it does not meet the inclusion criteria which require explicit focus on discrete audio/speech tokenization for generation or understanding tasks.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:2)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:2)"
  },
  {
    "title": "A Language Model of Java Methods with Train/Test Deduplication",
    "abstract": "This tool demonstration presents a research toolkit for a language model of Java source code. The target audience includes researchers studying problems at the granularity level of subroutines, statements, or variables in Java. In contrast to many existing language models, we prioritize features for researchers including an open and easily-searchable training set, a held out test set with different levels of deduplication from the training set, infrastructure for deduplicating new examples, and an implementation platform suitable for execution on equipment accessible to a relatively modest budget. Our model is a GPT2-like architecture with 350m parameters. Our training set includes 52m Java methods (9b tokens) and 13m StackOverflow threads (10.5b tokens). To improve accessibility of research to more members of the community, we limit local resource requirements to GPUs with 16GB video memory. We provide a test set of held out Java methods that include descriptive comments, including the entire Java projects for those methods. We also provide deduplication tools using precomputed hash tables at various similarity thresholds to help researchers ensure that their own test examples are not in the training set. We make all our tools and data open source and available via Huggingface and Github.",
    "metadata": {
      "arxiv_id": "2305.08286",
      "title": "A Language Model of Java Methods with Train/Test Deduplication",
      "summary": "This tool demonstration presents a research toolkit for a language model of Java source code. The target audience includes researchers studying problems at the granularity level of subroutines, statements, or variables in Java. In contrast to many existing language models, we prioritize features for researchers including an open and easily-searchable training set, a held out test set with different levels of deduplication from the training set, infrastructure for deduplicating new examples, and an implementation platform suitable for execution on equipment accessible to a relatively modest budget. Our model is a GPT2-like architecture with 350m parameters. Our training set includes 52m Java methods (9b tokens) and 13m StackOverflow threads (10.5b tokens). To improve accessibility of research to more members of the community, we limit local resource requirements to GPUs with 16GB video memory. We provide a test set of held out Java methods that include descriptive comments, including the entire Java projects for those methods. We also provide deduplication tools using precomputed hash tables at various similarity thresholds to help researchers ensure that their own test examples are not in the training set. We make all our tools and data open source and available via Huggingface and Github.",
      "authors": [
        "Chia-Yi Su",
        "Aakash Bansal",
        "Vijayanta Jain",
        "Sepideh Ghanavati",
        "Collin McMillan"
      ],
      "published": "2023-05-15T00:22:02Z",
      "updated": "2023-05-15T00:22:02Z",
      "categories": [
        "cs.SE",
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2305.08286v1",
      "landing_url": "https://arxiv.org/abs/2305.08286v1",
      "doi": "https://doi.org/10.48550/arXiv.2305.08286"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "Reasoning: The item concerns a language model for Java code with deduplication and does not involve discrete audio tokenization or analysis of discrete audio representations, so it should be excluded under the audio-tokenization criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "Reasoning: The item concerns a language model for Java code with deduplication and does not involve discrete audio tokenization or analysis of discrete audio representations, so it should be excluded under the audio-tokenization criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on a language model for Java source code, detailing data sets, architecture, and deduplication tools for Java methods, without any mention or analysis of audio or speech tokenization or discrete audio symbol representations required by the inclusion criteria. It is clearly an engineering/tool demonstration related to source code, not audio or speech tokenization.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on a language model for Java source code, detailing data sets, architecture, and deduplication tools for Java methods, without any mention or analysis of audio or speech tokenization or discrete audio symbol representations required by the inclusion criteria. It is clearly an engineering/tool demonstration related to source code, not audio or speech tokenization.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "SoundStorm: Efficient Parallel Audio Generation",
    "abstract": "We present SoundStorm, a model for efficient, non-autoregressive audio generation. SoundStorm receives as input the semantic tokens of AudioLM, and relies on bidirectional attention and confidence-based parallel decoding to generate the tokens of a neural audio codec. Compared to the autoregressive generation approach of AudioLM, our model produces audio of the same quality and with higher consistency in voice and acoustic conditions, while being two orders of magnitude faster. SoundStorm generates 30 seconds of audio in 0.5 seconds on a TPU-v4. We demonstrate the ability of our model to scale audio generation to longer sequences by synthesizing high-quality, natural dialogue segments, given a transcript annotated with speaker turns and a short prompt with the speakers' voices.",
    "metadata": {
      "arxiv_id": "2305.09636",
      "title": "SoundStorm: Efficient Parallel Audio Generation",
      "summary": "We present SoundStorm, a model for efficient, non-autoregressive audio generation. SoundStorm receives as input the semantic tokens of AudioLM, and relies on bidirectional attention and confidence-based parallel decoding to generate the tokens of a neural audio codec. Compared to the autoregressive generation approach of AudioLM, our model produces audio of the same quality and with higher consistency in voice and acoustic conditions, while being two orders of magnitude faster. SoundStorm generates 30 seconds of audio in 0.5 seconds on a TPU-v4. We demonstrate the ability of our model to scale audio generation to longer sequences by synthesizing high-quality, natural dialogue segments, given a transcript annotated with speaker turns and a short prompt with the speakers' voices.",
      "authors": [
        "Zalán Borsos",
        "Matt Sharifi",
        "Damien Vincent",
        "Eugene Kharitonov",
        "Neil Zeghidour",
        "Marco Tagliasacchi"
      ],
      "published": "2023-05-16T17:41:25Z",
      "updated": "2023-05-16T17:41:25Z",
      "categories": [
        "cs.SD",
        "cs.LG",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2305.09636v1",
      "landing_url": "https://arxiv.org/abs/2305.09636v1",
      "doi": "https://doi.org/10.48550/arXiv.2305.09636"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "SoundStorm centers on discrete semantic tokens for audio generation using a neural codec, i.e., explicit discrete tokenization of audio, satisfying the inclusion criteria and not matching any exclusion criteria.",
      "evaluation": 5
    },
    "round-A_JuniorNano_reasoning": "SoundStorm centers on discrete semantic tokens for audio generation using a neural codec, i.e., explicit discrete tokenization of audio, satisfying the inclusion criteria and not matching any exclusion criteria.",
    "round-A_JuniorNano_evaluation": 5,
    "round-A_JuniorMini_output": {
      "reasoning": "The study presents SoundStorm, a non-autoregressive model for audio generation using semantic tokens and neural audio codec tokens, implying that discrete tokenization of audio signals is a core element. It contrasts itself with autoregressive models and highlights use of tokens derived from neural quantization, rather than traditional codecs or only continuous features, meeting the inclusion criteria and avoiding exclusion criteria. Therefore, it is appropriate to include this study.",
      "evaluation": 5
    },
    "round-A_JuniorMini_reasoning": "The study presents SoundStorm, a non-autoregressive model for audio generation using semantic tokens and neural audio codec tokens, implying that discrete tokenization of audio signals is a core element. It contrasts itself with autoregressive models and highlights use of tokens derived from neural quantization, rather than traditional codecs or only continuous features, meeting the inclusion criteria and avoiding exclusion criteria. Therefore, it is appropriate to include this study.",
    "round-A_JuniorMini_evaluation": 5,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "include (junior:5)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Adversarial Defenses via Vector Quantization",
    "abstract": "Adversarial attacks pose significant challenges to the robustness of modern deep neural networks in computer vision, and defending these networks against adversarial attacks has attracted intense research efforts. Among various defense strategies, preprocessing-based defenses are practically appealing since there is no need to train the network under protection. However, such approaches typically do not achieve comparable robustness as other methods such as adversarial training. In this paper, we propose a novel framework for preprocessing-based defenses, where a vector quantizer is used as a preprocessor. This framework, inspired by and extended from Randomized Discretization (RandDisc), is theoretically principled by rate-distortion theory: indeed, RandDisc may be viewed as a scalar quantizer, and rate-distortion theory suggests that such quantization schemes are inferior to vector quantization. In our framework, the preprocessing vector quantizer treats the input image as a collection of patches and finds a set of representative patches based on the patch distributions; each original patch is then modified according to the representative patches close to it. We present two lightweight defenses in this framework, referred to as patched RandDisc (pRD) and sliding-window RandDisc (swRD), where the patches are disjoint in the former and overlapping in the latter. We show that vector-quantization-based defenses have certifiable robust accuracy and that pRD and swRD demonstrate state-of-the-art performances, surpassing RandDisc by a large margin. Notably, the proposed defenses possess the obfuscated gradients property. Our experiments however show that pRD and swRD remain effective under the STE and EOT attacks, which are designed specifically for defenses with gradient obfuscation. ...",
    "metadata": {
      "arxiv_id": "2305.13651",
      "title": "Adversarial Defenses via Vector Quantization",
      "summary": "Adversarial attacks pose significant challenges to the robustness of modern deep neural networks in computer vision, and defending these networks against adversarial attacks has attracted intense research efforts. Among various defense strategies, preprocessing-based defenses are practically appealing since there is no need to train the network under protection. However, such approaches typically do not achieve comparable robustness as other methods such as adversarial training. In this paper, we propose a novel framework for preprocessing-based defenses, where a vector quantizer is used as a preprocessor. This framework, inspired by and extended from Randomized Discretization (RandDisc), is theoretically principled by rate-distortion theory: indeed, RandDisc may be viewed as a scalar quantizer, and rate-distortion theory suggests that such quantization schemes are inferior to vector quantization. In our framework, the preprocessing vector quantizer treats the input image as a collection of patches and finds a set of representative patches based on the patch distributions; each original patch is then modified according to the representative patches close to it. We present two lightweight defenses in this framework, referred to as patched RandDisc (pRD) and sliding-window RandDisc (swRD), where the patches are disjoint in the former and overlapping in the latter. We show that vector-quantization-based defenses have certifiable robust accuracy and that pRD and swRD demonstrate state-of-the-art performances, surpassing RandDisc by a large margin. Notably, the proposed defenses possess the obfuscated gradients property. Our experiments however show that pRD and swRD remain effective under the STE and EOT attacks, which are designed specifically for defenses with gradient obfuscation. ...",
      "authors": [
        "Zhiyi Dong",
        "Yongyi Mao"
      ],
      "published": "2023-05-23T03:49:41Z",
      "updated": "2025-07-09T23:51:43Z",
      "categories": [
        "cs.LG",
        "cs.CR",
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2305.13651v2",
      "landing_url": "https://arxiv.org/abs/2305.13651v2",
      "doi": "https://doi.org/10.1016/j.neucom.2025.130703"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The paper discusses image adversarial defenses using a vector quantization framework and patch-based preprocessing, with no discrete audio tokenization or speech token sequence analysis, which is required by the inclusion criteria for discrete audio/speech tokenization.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The paper discusses image adversarial defenses using a vector quantization framework and patch-based preprocessing, with no discrete audio tokenization or speech token sequence analysis, which is required by the inclusion criteria for discrete audio/speech tokenization.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on adversarial defenses for deep neural networks in computer vision by applying vector quantization to image patches, without any mention of discrete audio or speech tokenization, audio signal processing, or evaluation on audio/speech tasks; therefore, it does not meet the inclusion criteria related to discrete audio tokenization.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on adversarial defenses for deep neural networks in computer vision by applying vector quantization to image patches, without any mention of discrete audio or speech tokenization, audio signal processing, or evaluation on audio/speech tasks; therefore, it does not meet the inclusion criteria related to discrete audio tokenization.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "MP-SENet: A Speech Enhancement Model with Parallel Denoising of Magnitude and Phase Spectra",
    "abstract": "This paper proposes MP-SENet, a novel Speech Enhancement Network which directly denoises Magnitude and Phase spectra in parallel. The proposed MP-SENet adopts a codec architecture in which the encoder and decoder are bridged by convolution-augmented transformers. The encoder aims to encode time-frequency representations from the input noisy magnitude and phase spectra. The decoder is composed of parallel magnitude mask decoder and phase decoder, directly recovering clean magnitude spectra and clean-wrapped phase spectra by incorporating learnable sigmoid activation and parallel phase estimation architecture, respectively. Multi-level losses defined on magnitude spectra, phase spectra, short-time complex spectra, and time-domain waveforms are used to train the MP-SENet model jointly. Experimental results show that our proposed MP-SENet achieves a PESQ of 3.50 on the public VoiceBank+DEMAND dataset and outperforms existing advanced speech enhancement methods.",
    "metadata": {
      "arxiv_id": "2305.13686",
      "title": "MP-SENet: A Speech Enhancement Model with Parallel Denoising of Magnitude and Phase Spectra",
      "summary": "This paper proposes MP-SENet, a novel Speech Enhancement Network which directly denoises Magnitude and Phase spectra in parallel. The proposed MP-SENet adopts a codec architecture in which the encoder and decoder are bridged by convolution-augmented transformers. The encoder aims to encode time-frequency representations from the input noisy magnitude and phase spectra. The decoder is composed of parallel magnitude mask decoder and phase decoder, directly recovering clean magnitude spectra and clean-wrapped phase spectra by incorporating learnable sigmoid activation and parallel phase estimation architecture, respectively. Multi-level losses defined on magnitude spectra, phase spectra, short-time complex spectra, and time-domain waveforms are used to train the MP-SENet model jointly. Experimental results show that our proposed MP-SENet achieves a PESQ of 3.50 on the public VoiceBank+DEMAND dataset and outperforms existing advanced speech enhancement methods.",
      "authors": [
        "Ye-Xin Lu",
        "Yang Ai",
        "Zhen-Hua Ling"
      ],
      "published": "2023-05-23T04:48:51Z",
      "updated": "2023-05-23T04:48:51Z",
      "categories": [
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2305.13686v1",
      "landing_url": "https://arxiv.org/abs/2305.13686v1",
      "doi": "https://doi.org/10.21437/Interspeech.2023-1441"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This work focuses on denoising continuous magnitude and phase spectra using a neural network and does not involve discrete audio tokenization into a finite vocabulary, so it does not meet the tokenization criterion; thus it should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "This work focuses on denoising continuous magnitude and phase spectra using a neural network and does not involve discrete audio tokenization into a finite vocabulary, so it does not meet the tokenization criterion; thus it should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study proposes a speech enhancement model that operates on continuous magnitude and phase spectra without mentioning any discrete audio/speech tokenization or quantization to discrete symbols from a finite vocabulary; thus, it does not satisfy the inclusion criteria focused on discrete tokenization and is more aligned with continuous feature processing.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study proposes a speech enhancement model that operates on continuous magnitude and phase spectra without mentioning any discrete audio/speech tokenization or quantization to discrete symbols from a finite vocabulary; thus, it does not satisfy the inclusion criteria focused on discrete tokenization and is more aligned with continuous feature processing.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Efficient Neural Music Generation",
    "abstract": "Recent progress in music generation has been remarkably advanced by the state-of-the-art MusicLM, which comprises a hierarchy of three LMs, respectively, for semantic, coarse acoustic, and fine acoustic modelings. Yet, sampling with the MusicLM requires processing through these LMs one by one to obtain the fine-grained acoustic tokens, making it computationally expensive and prohibitive for a real-time generation. Efficient music generation with a quality on par with MusicLM remains a significant challenge. In this paper, we present MeLoDy (M for music; L for LM; D for diffusion), an LM-guided diffusion model that generates music audios of state-of-the-art quality meanwhile reducing 95.7% or 99.6% forward passes in MusicLM, respectively, for sampling 10s or 30s music. MeLoDy inherits the highest-level LM from MusicLM for semantic modeling, and applies a novel dual-path diffusion (DPD) model and an audio VAE-GAN to efficiently decode the conditioning semantic tokens into waveform. DPD is proposed to simultaneously model the coarse and fine acoustics by incorporating the semantic information into segments of latents effectively via cross-attention at each denoising step. Our experimental results suggest the superiority of MeLoDy, not only in its practical advantages on sampling speed and infinitely continuable generation, but also in its state-of-the-art musicality, audio quality, and text correlation. Our samples are available at https://Efficient-MeLoDy.github.io/.",
    "metadata": {
      "arxiv_id": "2305.15719",
      "title": "Efficient Neural Music Generation",
      "summary": "Recent progress in music generation has been remarkably advanced by the state-of-the-art MusicLM, which comprises a hierarchy of three LMs, respectively, for semantic, coarse acoustic, and fine acoustic modelings. Yet, sampling with the MusicLM requires processing through these LMs one by one to obtain the fine-grained acoustic tokens, making it computationally expensive and prohibitive for a real-time generation. Efficient music generation with a quality on par with MusicLM remains a significant challenge. In this paper, we present MeLoDy (M for music; L for LM; D for diffusion), an LM-guided diffusion model that generates music audios of state-of-the-art quality meanwhile reducing 95.7% or 99.6% forward passes in MusicLM, respectively, for sampling 10s or 30s music. MeLoDy inherits the highest-level LM from MusicLM for semantic modeling, and applies a novel dual-path diffusion (DPD) model and an audio VAE-GAN to efficiently decode the conditioning semantic tokens into waveform. DPD is proposed to simultaneously model the coarse and fine acoustics by incorporating the semantic information into segments of latents effectively via cross-attention at each denoising step. Our experimental results suggest the superiority of MeLoDy, not only in its practical advantages on sampling speed and infinitely continuable generation, but also in its state-of-the-art musicality, audio quality, and text correlation.\n  Our samples are available at https://Efficient-MeLoDy.github.io/.",
      "authors": [
        "Max W. Y. Lam",
        "Qiao Tian",
        "Tang Li",
        "Zongyu Yin",
        "Siyuan Feng",
        "Ming Tu",
        "Yuliang Ji",
        "Rui Xia",
        "Mingbo Ma",
        "Xuchen Song",
        "Jitong Chen",
        "Yuping Wang",
        "Yuxuan Wang"
      ],
      "published": "2023-05-25T05:02:35Z",
      "updated": "2023-05-25T05:02:35Z",
      "categories": [
        "cs.SD",
        "cs.AI",
        "cs.LG",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2305.15719v1",
      "landing_url": "https://arxiv.org/abs/2305.15719v1",
      "doi": "https://doi.org/10.48550/arXiv.2305.15719"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The abstract centers on using discrete semantic tokens to condition a diffusion model for music generation, indicating tokenization is a core component, though the paper emphasizes efficiency and generation quality rather than a deep analysis of tokenization methods.",
      "evaluation": 4
    },
    "round-A_JuniorNano_reasoning": "The abstract centers on using discrete semantic tokens to condition a diffusion model for music generation, indicating tokenization is a core component, though the paper emphasizes efficiency and generation quality rather than a deep analysis of tokenization methods.",
    "round-A_JuniorNano_evaluation": 4,
    "round-A_JuniorMini_output": {
      "reasoning": "The study proposes MeLoDy, which uses discrete semantic tokens derived from MusicLM's highest-level language model for semantic modeling and includes a novel diffusion model to decode these tokens into audio waveforms, indicating that discrete audio tokenization is a core element of the methodology. Additionally, it addresses efficient neural music generation and evaluates the quality of generated discrete token-based audio, fitting the inclusion criteria of analyzing discrete audio tokenization for audio generation tasks. The study is in English and provides experimental validation, and it goes beyond continuous features by involving discrete semantic tokens and novel token decoding schemes.",
      "evaluation": 5
    },
    "round-A_JuniorMini_reasoning": "The study proposes MeLoDy, which uses discrete semantic tokens derived from MusicLM's highest-level language model for semantic modeling and includes a novel diffusion model to decode these tokens into audio waveforms, indicating that discrete audio tokenization is a core element of the methodology. Additionally, it addresses efficient neural music generation and evaluates the quality of generated discrete token-based audio, fitting the inclusion criteria of analyzing discrete audio tokenization for audio generation tasks. The study is in English and provides experimental validation, and it goes beyond continuous features by involving discrete semantic tokens and novel token decoding schemes.",
    "round-A_JuniorMini_evaluation": 5,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "include (junior:4)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "ElectrodeNet -- A Deep Learning Based Sound Coding Strategy for Cochlear Implants",
    "abstract": "ElectrodeNet, a deep learning based sound coding strategy for the cochlear implant (CI), is proposed to emulate the advanced combination encoder (ACE) strategy by replacing the conventional envelope detection using various artificial neural networks. The extended ElectrodeNet-CS strategy further incorporates the channel selection (CS). Network models of deep neural network (DNN), convolutional neural network (CNN), and long short-term memory (LSTM) were trained using the Fast Fourier Transformed bins and channel envelopes obtained from the processing of clean speech by the ACE strategy. Objective speech understanding using short-time objective intelligibility (STOI) and normalized covariance metric (NCM) was estimated for ElectrodeNet using CI simulations. Sentence recognition tests for vocoded Mandarin speech were conducted with normal-hearing listeners. DNN, CNN, and LSTM based ElectrodeNets exhibited strong correlations to ACE in objective and subjective scores using mean squared error (MSE), linear correlation coefficient (LCC) and Spearman's rank correlation coefficient (SRCC). The ElectrodeNet-CS strategy was capable of producing N-of-M compatible electrode patterns using a modified DNN network to embed maxima selection, and to perform in similar or even slightly higher average in STOI and sentence recognition compared to ACE. The methods and findings demonstrated the feasibility and potential of using deep learning in CI coding strategy.",
    "metadata": {
      "arxiv_id": "2305.16753",
      "title": "ElectrodeNet -- A Deep Learning Based Sound Coding Strategy for Cochlear Implants",
      "summary": "ElectrodeNet, a deep learning based sound coding strategy for the cochlear implant (CI), is proposed to emulate the advanced combination encoder (ACE) strategy by replacing the conventional envelope detection using various artificial neural networks. The extended ElectrodeNet-CS strategy further incorporates the channel selection (CS). Network models of deep neural network (DNN), convolutional neural network (CNN), and long short-term memory (LSTM) were trained using the Fast Fourier Transformed bins and channel envelopes obtained from the processing of clean speech by the ACE strategy. Objective speech understanding using short-time objective intelligibility (STOI) and normalized covariance metric (NCM) was estimated for ElectrodeNet using CI simulations. Sentence recognition tests for vocoded Mandarin speech were conducted with normal-hearing listeners. DNN, CNN, and LSTM based ElectrodeNets exhibited strong correlations to ACE in objective and subjective scores using mean squared error (MSE), linear correlation coefficient (LCC) and Spearman's rank correlation coefficient (SRCC). The ElectrodeNet-CS strategy was capable of producing N-of-M compatible electrode patterns using a modified DNN network to embed maxima selection, and to perform in similar or even slightly higher average in STOI and sentence recognition compared to ACE. The methods and findings demonstrated the feasibility and potential of using deep learning in CI coding strategy.",
      "authors": [
        "Enoch Hsin-Ho Huang",
        "Rong Chao",
        "Yu Tsao",
        "Chao-Min Wu"
      ],
      "published": "2023-05-26T09:06:04Z",
      "updated": "2023-05-26T09:06:04Z",
      "categories": [
        "eess.AS",
        "cs.AI",
        "eess.SP"
      ],
      "pdf_url": "https://arxiv.org/pdf/2305.16753v1",
      "landing_url": "https://arxiv.org/abs/2305.16753v1",
      "doi": "https://doi.org/10.1109/TCDS.2023.3275587"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The article describes a deep-learning based sound coding strategy for cochlear implants with discrete electrode activation patterns (N-of-M) and token-like selection, but does not explicitly propose, analyze, or evaluate discrete audio/speech tokenization as the core object of study. The primary focus is on neural coding and electrode pattern generation rather than design/analysis of discrete speech tokens or a token vocabulary for audio reconstruction/generation/understanding. Therefore it should not be included under the stated tokenization criteria.",
      "evaluation": 2
    },
    "round-A_JuniorNano_reasoning": "The article describes a deep-learning based sound coding strategy for cochlear implants with discrete electrode activation patterns (N-of-M) and token-like selection, but does not explicitly propose, analyze, or evaluate discrete audio/speech tokenization as the core object of study. The primary focus is on neural coding and electrode pattern generation rather than design/analysis of discrete speech tokens or a token vocabulary for audio reconstruction/generation/understanding. Therefore it should not be included under the stated tokenization criteria.",
    "round-A_JuniorNano_evaluation": 2,
    "round-A_JuniorMini_output": {
      "reasoning": "The study proposes ElectrodeNet, a deep learning-based sound coding strategy for cochlear implants, which appears to focus on encoding audio signals for cochlear implant processing but does not explicitly describe converting continuous audio signals into discrete tokens or symbols for reconstruction or generation; the focus is on emulating ACE strategy and neural network-based sound coding rather than discrete audio/speech tokenization, thus not meeting all inclusion criteria.",
      "evaluation": 2
    },
    "round-A_JuniorMini_reasoning": "The study proposes ElectrodeNet, a deep learning-based sound coding strategy for cochlear implants, which appears to focus on encoding audio signals for cochlear implant processing but does not explicitly describe converting continuous audio signals into discrete tokens or symbols for reconstruction or generation; the focus is on emulating ACE strategy and neural network-based sound coding rather than discrete audio/speech tokenization, thus not meeting all inclusion criteria.",
    "round-A_JuniorMini_evaluation": 2,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:2)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:2)"
  },
  {
    "title": "DotHash: Estimating Set Similarity Metrics for Link Prediction and Document Deduplication",
    "abstract": "Metrics for set similarity are a core aspect of several data mining tasks. To remove duplicate results in a Web search, for example, a common approach looks at the Jaccard index between all pairs of pages. In social network analysis, a much-celebrated metric is the Adamic-Adar index, widely used to compare node neighborhood sets in the important problem of predicting links. However, with the increasing amount of data to be processed, calculating the exact similarity between all pairs can be intractable. The challenge of working at this scale has motivated research into efficient estimators for set similarity metrics. The two most popular estimators, MinHash and SimHash, are indeed used in applications such as document deduplication and recommender systems where large volumes of data need to be processed. Given the importance of these tasks, the demand for advancing estimators is evident. We propose DotHash, an unbiased estimator for the intersection size of two sets. DotHash can be used to estimate the Jaccard index and, to the best of our knowledge, is the first method that can also estimate the Adamic-Adar index and a family of related metrics. We formally define this family of metrics, provide theoretical bounds on the probability of estimate errors, and analyze its empirical performance. Our experimental results indicate that DotHash is more accurate than the other estimators in link prediction and detecting duplicate documents with the same complexity and similar comparison time.",
    "metadata": {
      "arxiv_id": "2305.17310",
      "title": "DotHash: Estimating Set Similarity Metrics for Link Prediction and Document Deduplication",
      "summary": "Metrics for set similarity are a core aspect of several data mining tasks. To remove duplicate results in a Web search, for example, a common approach looks at the Jaccard index between all pairs of pages. In social network analysis, a much-celebrated metric is the Adamic-Adar index, widely used to compare node neighborhood sets in the important problem of predicting links. However, with the increasing amount of data to be processed, calculating the exact similarity between all pairs can be intractable. The challenge of working at this scale has motivated research into efficient estimators for set similarity metrics. The two most popular estimators, MinHash and SimHash, are indeed used in applications such as document deduplication and recommender systems where large volumes of data need to be processed. Given the importance of these tasks, the demand for advancing estimators is evident. We propose DotHash, an unbiased estimator for the intersection size of two sets. DotHash can be used to estimate the Jaccard index and, to the best of our knowledge, is the first method that can also estimate the Adamic-Adar index and a family of related metrics. We formally define this family of metrics, provide theoretical bounds on the probability of estimate errors, and analyze its empirical performance. Our experimental results indicate that DotHash is more accurate than the other estimators in link prediction and detecting duplicate documents with the same complexity and similar comparison time.",
      "authors": [
        "Igor Nunes",
        "Mike Heddes",
        "Pere Vergés",
        "Danny Abraham",
        "Alexander Veidenbaum",
        "Alexandru Nicolau",
        "Tony Givargis"
      ],
      "published": "2023-05-27T00:05:39Z",
      "updated": "2023-05-27T00:05:39Z",
      "categories": [
        "cs.SI",
        "cs.DS",
        "cs.IR"
      ],
      "pdf_url": "https://arxiv.org/pdf/2305.17310v1",
      "landing_url": "https://arxiv.org/abs/2305.17310v1",
      "doi": "https://doi.org/10.1145/3580305.3599314"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The paper focuses on set similarity estimators (DotHash, MinHash, SimHash) for tasks like link prediction and document deduplication and contains no discrete audio/speech tokenization, so it does not meet the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The paper focuses on set similarity estimators (DotHash, MinHash, SimHash) for tasks like link prediction and document deduplication and contains no discrete audio/speech tokenization, so it does not meet the inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study proposes and evaluates a novel estimator for set similarity metrics useful in link prediction and document deduplication, focusing on data mining and network analysis tasks without any mention of audio or speech tokenization, discrete audio token design, or audio-related applications. Thus, it neither meets the inclusion criteria of discrete audio/speech tokenization nor addresses any audio-related tasks or representations.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study proposes and evaluates a novel estimator for set similarity metrics useful in link prediction and document deduplication, focusing on data mining and network analysis tasks without any mention of audio or speech tokenization, discrete audio token design, or audio-related applications. Thus, it neither meets the inclusion criteria of discrete audio/speech tokenization nor addresses any audio-related tasks or representations.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Multi-behavior Self-supervised Learning for Recommendation",
    "abstract": "Modern recommender systems often deal with a variety of user interactions, e.g., click, forward, purchase, etc., which requires the underlying recommender engines to fully understand and leverage multi-behavior data from users. Despite recent efforts towards making use of heterogeneous data, multi-behavior recommendation still faces great challenges. Firstly, sparse target signals and noisy auxiliary interactions remain an issue. Secondly, existing methods utilizing self-supervised learning (SSL) to tackle the data sparsity neglect the serious optimization imbalance between the SSL task and the target task. Hence, we propose a Multi-Behavior Self-Supervised Learning (MBSSL) framework together with an adaptive optimization method. Specifically, we devise a behavior-aware graph neural network incorporating the self-attention mechanism to capture behavior multiplicity and dependencies. To increase the robustness to data sparsity under the target behavior and noisy interactions from auxiliary behaviors, we propose a novel self-supervised learning paradigm to conduct node self-discrimination at both inter-behavior and intra-behavior levels. In addition, we develop a customized optimization strategy through hybrid manipulation on gradients to adaptively balance the self-supervised learning task and the main supervised recommendation task. Extensive experiments on five real-world datasets demonstrate the consistent improvements obtained by MBSSL over ten state-of-the art (SOTA) baselines. We release our model implementation at: https://github.com/Scofield666/MBSSL.git.",
    "metadata": {
      "arxiv_id": "2305.18238",
      "title": "Multi-behavior Self-supervised Learning for Recommendation",
      "summary": "Modern recommender systems often deal with a variety of user interactions, e.g., click, forward, purchase, etc., which requires the underlying recommender engines to fully understand and leverage multi-behavior data from users. Despite recent efforts towards making use of heterogeneous data, multi-behavior recommendation still faces great challenges. Firstly, sparse target signals and noisy auxiliary interactions remain an issue. Secondly, existing methods utilizing self-supervised learning (SSL) to tackle the data sparsity neglect the serious optimization imbalance between the SSL task and the target task. Hence, we propose a Multi-Behavior Self-Supervised Learning (MBSSL) framework together with an adaptive optimization method. Specifically, we devise a behavior-aware graph neural network incorporating the self-attention mechanism to capture behavior multiplicity and dependencies. To increase the robustness to data sparsity under the target behavior and noisy interactions from auxiliary behaviors, we propose a novel self-supervised learning paradigm to conduct node self-discrimination at both inter-behavior and intra-behavior levels. In addition, we develop a customized optimization strategy through hybrid manipulation on gradients to adaptively balance the self-supervised learning task and the main supervised recommendation task. Extensive experiments on five real-world datasets demonstrate the consistent improvements obtained by MBSSL over ten state-of-the art (SOTA) baselines. We release our model implementation at: https://github.com/Scofield666/MBSSL.git.",
      "authors": [
        "Jingcao Xu",
        "Chaokun Wang",
        "Cheng Wu",
        "Yang Song",
        "Kai Zheng",
        "Xiaowei Wang",
        "Changping Wang",
        "Guorui Zhou",
        "Kun Gai"
      ],
      "published": "2023-05-22T15:57:32Z",
      "updated": "2023-05-22T15:57:32Z",
      "categories": [
        "cs.IR",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2305.18238v1",
      "landing_url": "https://arxiv.org/abs/2305.18238v1",
      "doi": "https://doi.org/10.1145/3539618.3591734"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This item describes a multi-behavior self-supervised learning framework for recommendation using graphs; there is no discrete audio/speech tokenization and no analysis of discrete representations, so it does not meet the audio-tokenization inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "This item describes a multi-behavior self-supervised learning framework for recommendation using graphs; there is no discrete audio/speech tokenization and no analysis of discrete representations, so it does not meet the audio-tokenization inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The title and abstract focus on a multi-behavior self-supervised learning framework for recommendation systems involving user interactions, without any mention of discrete audio/speech tokenization or converting continuous audio signals into discrete symbols; therefore, it does not meet the inclusion criteria related to audio tokenization.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The title and abstract focus on a multi-behavior self-supervised learning framework for recommendation systems involving user interactions, without any mention of discrete audio/speech tokenization or converting continuous audio signals into discrete symbols; therefore, it does not meet the inclusion criteria related to audio tokenization.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Make-A-Voice: Unified Voice Synthesis With Discrete Representation",
    "abstract": "Various applications of voice synthesis have been developed independently despite the fact that they generate \"voice\" as output in common. In addition, the majority of voice synthesis models currently rely on annotated audio data, but it is crucial to scale them to self-supervised datasets in order to effectively capture the wide range of acoustic variations present in human voice, including speaker identity, emotion, and prosody. In this work, we propose Make-A-Voice, a unified framework for synthesizing and manipulating voice signals from discrete representations. Make-A-Voice leverages a \"coarse-to-fine\" approach to model the human voice, which involves three stages: 1) semantic stage: model high-level transformation between linguistic content and self-supervised semantic tokens, 2) acoustic stage: introduce varying control signals as acoustic conditions for semantic-to-acoustic modeling, and 3) generation stage: synthesize high-fidelity waveforms from acoustic tokens. Make-A-Voice offers notable benefits as a unified voice synthesis framework: 1) Data scalability: the major backbone (i.e., acoustic and generation stage) does not require any annotations, and thus the training data could be scaled up. 2) Controllability and conditioning flexibility: we investigate different conditioning mechanisms and effectively handle three voice synthesis applications, including text-to-speech (TTS), voice conversion (VC), and singing voice synthesis (SVS) by re-synthesizing the discrete voice representations with prompt guidance. Experimental results demonstrate that Make-A-Voice exhibits superior audio quality and style similarity compared with competitive baseline models. Audio samples are available at https://Make-A-Voice.github.io",
    "metadata": {
      "arxiv_id": "2305.19269",
      "title": "Make-A-Voice: Unified Voice Synthesis With Discrete Representation",
      "summary": "Various applications of voice synthesis have been developed independently despite the fact that they generate \"voice\" as output in common. In addition, the majority of voice synthesis models currently rely on annotated audio data, but it is crucial to scale them to self-supervised datasets in order to effectively capture the wide range of acoustic variations present in human voice, including speaker identity, emotion, and prosody. In this work, we propose Make-A-Voice, a unified framework for synthesizing and manipulating voice signals from discrete representations. Make-A-Voice leverages a \"coarse-to-fine\" approach to model the human voice, which involves three stages: 1) semantic stage: model high-level transformation between linguistic content and self-supervised semantic tokens, 2) acoustic stage: introduce varying control signals as acoustic conditions for semantic-to-acoustic modeling, and 3) generation stage: synthesize high-fidelity waveforms from acoustic tokens. Make-A-Voice offers notable benefits as a unified voice synthesis framework: 1) Data scalability: the major backbone (i.e., acoustic and generation stage) does not require any annotations, and thus the training data could be scaled up. 2) Controllability and conditioning flexibility: we investigate different conditioning mechanisms and effectively handle three voice synthesis applications, including text-to-speech (TTS), voice conversion (VC), and singing voice synthesis (SVS) by re-synthesizing the discrete voice representations with prompt guidance. Experimental results demonstrate that Make-A-Voice exhibits superior audio quality and style similarity compared with competitive baseline models. Audio samples are available at https://Make-A-Voice.github.io",
      "authors": [
        "Rongjie Huang",
        "Chunlei Zhang",
        "Yongqi Wang",
        "Dongchao Yang",
        "Luping Liu",
        "Zhenhui Ye",
        "Ziyue Jiang",
        "Chao Weng",
        "Zhou Zhao",
        "Dong Yu"
      ],
      "published": "2023-05-30T17:59:26Z",
      "updated": "2023-05-30T17:59:26Z",
      "categories": [
        "eess.AS",
        "cs.AI",
        "cs.CL",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2305.19269v1",
      "landing_url": "https://arxiv.org/abs/2305.19269v1",
      "doi": "https://doi.org/10.48550/arXiv.2305.19269"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The abstract centers on modeling and generating voice via discrete representations and token-based (semantic tokens) stages, which directly targets discrete audio tokenization; there are no evident exclusions (e.g., purely continuous features or non-speech tokenization).",
      "evaluation": 5
    },
    "round-A_JuniorNano_reasoning": "The abstract centers on modeling and generating voice via discrete representations and token-based (semantic tokens) stages, which directly targets discrete audio tokenization; there are no evident exclusions (e.g., purely continuous features or non-speech tokenization).",
    "round-A_JuniorNano_evaluation": 5,
    "round-A_JuniorMini_output": {
      "reasoning": "The study explicitly proposes a unified voice synthesis framework that relies on discrete representations and self-supervised semantic tokens as a core aspect of the synthesis pipeline, which aligns well with the inclusion criteria focusing on discrete audio/speech tokenization for generation and reconstruction; it also involves experimental validation and is presented in English, thus meeting accessibility requirements, and it is not limited to continuous audio features nor is it merely engineering or non-speech focused.",
      "evaluation": 5
    },
    "round-A_JuniorMini_reasoning": "The study explicitly proposes a unified voice synthesis framework that relies on discrete representations and self-supervised semantic tokens as a core aspect of the synthesis pipeline, which aligns well with the inclusion criteria focusing on discrete audio/speech tokenization for generation and reconstruction; it also involves experimental validation and is presented in English, thus meeting accessibility requirements, and it is not limited to continuous audio features nor is it merely engineering or non-speech focused.",
    "round-A_JuniorMini_evaluation": 5,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "include (junior:5)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "A Multi-dimensional Deep Structured State Space Approach to Speech Enhancement Using Small-footprint Models",
    "abstract": "We propose a multi-dimensional structured state space (S4) approach to speech enhancement. To better capture the spectral dependencies across the frequency axis, we focus on modifying the multi-dimensional S4 layer with whitening transformation to build new small-footprint models that also achieve good performance. We explore several S4-based deep architectures in time (T) and time-frequency (TF) domains. The 2-D S4 layer can be considered a particular convolutional layer with an infinite receptive field although it utilizes fewer parameters than a conventional convolutional layer. Evaluated on the VoiceBank-DEMAND data set, when compared with the conventional U-net model based on convolutional layers, the proposed TF-domain S4-based model is 78.6% smaller in size, yet it still achieves competitive results with a PESQ score of 3.15 with data augmentation. By increasing the model size, we can even reach a PESQ score of 3.18.",
    "metadata": {
      "arxiv_id": "2306.00331",
      "title": "A Multi-dimensional Deep Structured State Space Approach to Speech Enhancement Using Small-footprint Models",
      "summary": "We propose a multi-dimensional structured state space (S4) approach to speech enhancement. To better capture the spectral dependencies across the frequency axis, we focus on modifying the multi-dimensional S4 layer with whitening transformation to build new small-footprint models that also achieve good performance. We explore several S4-based deep architectures in time (T) and time-frequency (TF) domains. The 2-D S4 layer can be considered a particular convolutional layer with an infinite receptive field although it utilizes fewer parameters than a conventional convolutional layer. Evaluated on the VoiceBank-DEMAND data set, when compared with the conventional U-net model based on convolutional layers, the proposed TF-domain S4-based model is 78.6% smaller in size, yet it still achieves competitive results with a PESQ score of 3.15 with data augmentation. By increasing the model size, we can even reach a PESQ score of 3.18.",
      "authors": [
        "Pin-Jui Ku",
        "Chao-Han Huck Yang",
        "Sabato Marco Siniscalchi",
        "Chin-Hui Lee"
      ],
      "published": "2023-06-01T04:19:57Z",
      "updated": "2023-06-01T04:19:57Z",
      "categories": [
        "eess.AS",
        "cs.AI",
        "cs.SD",
        "eess.SP",
        "eess.SY"
      ],
      "pdf_url": "https://arxiv.org/pdf/2306.00331v1",
      "landing_url": "https://arxiv.org/abs/2306.00331v1",
      "doi": "https://doi.org/10.21437/Interspeech.2023-1084"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The abstract discusses continuous-signal speech enhancement using S4-based models and does not involve discrete audio tokenization or quantization into finite vocabularies, thus failing the inclusion criterion and meeting an exclusion criterion.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The abstract discusses continuous-signal speech enhancement using S4-based models and does not involve discrete audio tokenization or quantization into finite vocabularies, thus failing the inclusion criterion and meeting an exclusion criterion.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on speech enhancement using a multi-dimensional structured state space model but does not mention discrete audio/speech tokenization or the conversion of continuous audio into sequences of discrete symbols, which is a strict inclusion criterion; it mainly involves continuous features and model architectures without discrete token design or analysis.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on speech enhancement using a multi-dimensional structured state space model but does not mention discrete audio/speech tokenization or the conversion of continuous audio into sequences of discrete symbols, which is a strict inclusion criterion; it mainly involves continuous features and model architectures without discrete token design or analysis.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Exploration on HuBERT with Multiple Resolutions",
    "abstract": "Hidden-unit BERT (HuBERT) is a widely-used self-supervised learning (SSL) model in speech processing. However, we argue that its fixed 20ms resolution for hidden representations would not be optimal for various speech-processing tasks since their attributes (e.g., speaker characteristics and semantics) are based on different time scales. To address this limitation, we propose utilizing HuBERT representations at multiple resolutions for downstream tasks. We explore two approaches, namely the parallel and hierarchical approaches, for integrating HuBERT features with different resolutions. Through experiments, we demonstrate that HuBERT with multiple resolutions outperforms the original model. This highlights the potential of utilizing multiple resolutions in SSL models like HuBERT to capture diverse information from speech signals.",
    "metadata": {
      "arxiv_id": "2306.01084",
      "title": "Exploration on HuBERT with Multiple Resolutions",
      "summary": "Hidden-unit BERT (HuBERT) is a widely-used self-supervised learning (SSL) model in speech processing. However, we argue that its fixed 20ms resolution for hidden representations would not be optimal for various speech-processing tasks since their attributes (e.g., speaker characteristics and semantics) are based on different time scales. To address this limitation, we propose utilizing HuBERT representations at multiple resolutions for downstream tasks. We explore two approaches, namely the parallel and hierarchical approaches, for integrating HuBERT features with different resolutions. Through experiments, we demonstrate that HuBERT with multiple resolutions outperforms the original model. This highlights the potential of utilizing multiple resolutions in SSL models like HuBERT to capture diverse information from speech signals.",
      "authors": [
        "Jiatong Shi",
        "Yun Tang",
        "Hirofumi Inaguma",
        "Hongyu GOng",
        "Juan Pino",
        "Shinji Watanabe"
      ],
      "published": "2023-06-01T18:51:34Z",
      "updated": "2023-06-22T18:34:22Z",
      "categories": [
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2306.01084v2",
      "landing_url": "https://arxiv.org/abs/2306.01084v2",
      "doi": "https://doi.org/10.48550/arXiv.2306.01084"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The work focuses on using HuBERT representations at multiple resolutions for downstream tasks, but does not explicitly propose, analyze, or evaluate discrete audio tokenization as the core object of study.",
      "evaluation": 2
    },
    "round-A_JuniorNano_reasoning": "The work focuses on using HuBERT representations at multiple resolutions for downstream tasks, but does not explicitly propose, analyze, or evaluate discrete audio tokenization as the core object of study.",
    "round-A_JuniorNano_evaluation": 2,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on using HuBERT representations at multiple temporal resolutions to improve speech processing tasks, but it does not explicitly propose or evaluate discrete audio tokenization methods converting continuous audio signals into discrete tokens; rather, it deals with continuous hidden representations and their resolutions without discrete quantization or token design.",
      "evaluation": 2
    },
    "round-A_JuniorMini_reasoning": "The study focuses on using HuBERT representations at multiple temporal resolutions to improve speech processing tasks, but it does not explicitly propose or evaluate discrete audio tokenization methods converting continuous audio signals into discrete tokens; rather, it deals with continuous hidden representations and their resolutions without discrete quantization or token design.",
    "round-A_JuniorMini_evaluation": 2,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:2)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:2)"
  },
  {
    "title": "Augmentation-aware Self-supervised Learning with Conditioned Projector",
    "abstract": "Self-supervised learning (SSL) is a powerful technique for learning from unlabeled data. By learning to remain invariant to applied data augmentations, methods such as SimCLR and MoCo can reach quality on par with supervised approaches. However, this invariance may be detrimental for solving downstream tasks that depend on traits affected by augmentations used during pretraining, such as color. In this paper, we propose to foster sensitivity to such characteristics in the representation space by modifying the projector network, a common component of self-supervised architectures. Specifically, we supplement the projector with information about augmentations applied to images. For the projector to take advantage of this auxiliary conditioning when solving the SSL task, the feature extractor learns to preserve the augmentation information in its representations. Our approach, coined Conditional Augmentation-aware Self-supervised Learning (CASSLE), is directly applicable to typical joint-embedding SSL methods regardless of their objective functions. Moreover, it does not require major changes in the network architecture or prior knowledge of downstream tasks. In addition to an analysis of sensitivity towards different data augmentations, we conduct a series of experiments, which show that CASSLE improves over various SSL methods, reaching state-of-the-art performance in multiple downstream tasks.",
    "metadata": {
      "arxiv_id": "2306.06082",
      "title": "Augmentation-aware Self-supervised Learning with Conditioned Projector",
      "summary": "Self-supervised learning (SSL) is a powerful technique for learning from unlabeled data. By learning to remain invariant to applied data augmentations, methods such as SimCLR and MoCo can reach quality on par with supervised approaches. However, this invariance may be detrimental for solving downstream tasks that depend on traits affected by augmentations used during pretraining, such as color. In this paper, we propose to foster sensitivity to such characteristics in the representation space by modifying the projector network, a common component of self-supervised architectures. Specifically, we supplement the projector with information about augmentations applied to images. For the projector to take advantage of this auxiliary conditioning when solving the SSL task, the feature extractor learns to preserve the augmentation information in its representations. Our approach, coined Conditional Augmentation-aware Self-supervised Learning (CASSLE), is directly applicable to typical joint-embedding SSL methods regardless of their objective functions. Moreover, it does not require major changes in the network architecture or prior knowledge of downstream tasks. In addition to an analysis of sensitivity towards different data augmentations, we conduct a series of experiments, which show that CASSLE improves over various SSL methods, reaching state-of-the-art performance in multiple downstream tasks.",
      "authors": [
        "Marcin Przewięźlikowski",
        "Mateusz Pyla",
        "Bartosz Zieliński",
        "Bartłomiej Twardowski",
        "Jacek Tabor",
        "Marek Śmieja"
      ],
      "published": "2023-05-31T12:24:06Z",
      "updated": "2024-10-19T08:00:13Z",
      "categories": [
        "cs.CV",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2306.06082v4",
      "landing_url": "https://arxiv.org/abs/2306.06082v4",
      "doi": "https://doi.org/10.1016/j.knosys.2024.112572"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The study discusses augmentation-aware self-supervised learning for image representations and a conditioned projector, with no focus on discrete audio/speech tokenization; thus it does not meet the inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The study discusses augmentation-aware self-supervised learning for image representations and a conditioned projector, with no focus on discrete audio/speech tokenization; thus it does not meet the inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on self-supervised learning for image data augmentation awareness and does not discuss discrete audio or speech tokenization, nor does it address audio signal processing or audio-specific token representations relevant to language modeling; therefore, it does not meet the inclusion criteria focused on discrete audio tokenization.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on self-supervised learning for image data augmentation awareness and does not discuss discrete audio or speech tokenization, nor does it address audio signal processing or audio-specific token representations relevant to language modeling; therefore, it does not meet the inclusion criteria focused on discrete audio tokenization.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Record Deduplication for Entity Distribution Modeling in ASR Transcripts",
    "abstract": "Voice digital assistants must keep up with trending search queries. We rely on a speech recognition model using contextual biasing with a rapidly updated set of entities, instead of frequent model retraining, to keep up with trends. There are several challenges with this approach: (1) the entity set must be frequently reconstructed, (2) the entity set is of limited size due to latency and accuracy trade-offs, and (3) finding the true entity distribution for biasing is complicated by ASR misrecognition. We address these challenges and define an entity set by modeling customers true requested entity distribution from ASR output in production using record deduplication, a technique from the field of entity resolution. Record deduplication resolves or deduplicates coreferences, including misrecognitions, of the same latent entity. Our method successfully retrieves 95% of misrecognized entities and when used for contextual biasing shows an estimated 5% relative word error rate reduction.",
    "metadata": {
      "arxiv_id": "2306.06246",
      "title": "Record Deduplication for Entity Distribution Modeling in ASR Transcripts",
      "summary": "Voice digital assistants must keep up with trending search queries. We rely on a speech recognition model using contextual biasing with a rapidly updated set of entities, instead of frequent model retraining, to keep up with trends. There are several challenges with this approach: (1) the entity set must be frequently reconstructed, (2) the entity set is of limited size due to latency and accuracy trade-offs, and (3) finding the true entity distribution for biasing is complicated by ASR misrecognition. We address these challenges and define an entity set by modeling customers true requested entity distribution from ASR output in production using record deduplication, a technique from the field of entity resolution. Record deduplication resolves or deduplicates coreferences, including misrecognitions, of the same latent entity. Our method successfully retrieves 95% of misrecognized entities and when used for contextual biasing shows an estimated 5% relative word error rate reduction.",
      "authors": [
        "Tianyu Huang",
        "Chung Hoon Hong",
        "Carl Wivagg",
        "Kanna Shimizu"
      ],
      "published": "2023-06-09T20:42:11Z",
      "updated": "2023-06-09T20:42:11Z",
      "categories": [
        "cs.CL"
      ],
      "pdf_url": "https://arxiv.org/pdf/2306.06246v1",
      "landing_url": "https://arxiv.org/abs/2306.06246v1",
      "doi": "https://doi.org/10.48550/arXiv.2306.06246"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The abstract does not study discrete audio tokenization or token-level representations; it focuses on deduplication of ASR entity mentions for biasing, not on analyzing or proposing discrete speech tokens for language tasks.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The abstract does not study discrete audio tokenization or token-level representations; it focuses on deduplication of ASR entity mentions for biasing, not on analyzing or proposing discrete speech tokens for language tasks.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on record deduplication and entity distribution modeling in ASR transcripts but does not explicitly propose or analyze discrete audio/speech tokenization methods where continuous audio signals are converted into discrete tokens; thus, it does not meet the inclusion criteria related to discrete tokenization as a core object of study.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on record deduplication and entity distribution modeling in ASR transcripts but does not explicitly propose or analyze discrete audio/speech tokenization methods where continuous audio signals are converted into discrete tokens; thus, it does not meet the inclusion criteria related to discrete tokenization as a core object of study.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Reducing Barriers to Self-Supervised Learning: HuBERT Pre-training with Academic Compute",
    "abstract": "Self-supervised learning (SSL) has led to great strides in speech processing. However, the resources needed to train these models has become prohibitively large as they continue to scale. Currently, only a few groups with substantial resources are capable of creating SSL models, which harms reproducibility. In this work, we optimize HuBERT SSL to fit in academic constraints. We reproduce HuBERT independently from the original implementation, with no performance loss. Our code and training optimizations make SSL feasible with only 8 GPUs, instead of the 32 used in the original work. We also explore a semi-supervised route, using an ASR model to skip the first pre-training iteration. Within one iteration of pre-training, our models improve over HuBERT on several tasks. Furthermore, our HuBERT Large variant requires only 8 GPUs, achieving similar performance to the original trained on 128. As our contribution to the community, all models, configurations, and code are made open-source in ESPnet.",
    "metadata": {
      "arxiv_id": "2306.06672",
      "title": "Reducing Barriers to Self-Supervised Learning: HuBERT Pre-training with Academic Compute",
      "summary": "Self-supervised learning (SSL) has led to great strides in speech processing. However, the resources needed to train these models has become prohibitively large as they continue to scale. Currently, only a few groups with substantial resources are capable of creating SSL models, which harms reproducibility. In this work, we optimize HuBERT SSL to fit in academic constraints. We reproduce HuBERT independently from the original implementation, with no performance loss. Our code and training optimizations make SSL feasible with only 8 GPUs, instead of the 32 used in the original work. We also explore a semi-supervised route, using an ASR model to skip the first pre-training iteration. Within one iteration of pre-training, our models improve over HuBERT on several tasks. Furthermore, our HuBERT Large variant requires only 8 GPUs, achieving similar performance to the original trained on 128. As our contribution to the community, all models, configurations, and code are made open-source in ESPnet.",
      "authors": [
        "William Chen",
        "Xuankai Chang",
        "Yifan Peng",
        "Zhaoheng Ni",
        "Soumi Maiti",
        "Shinji Watanabe"
      ],
      "published": "2023-06-11T12:53:46Z",
      "updated": "2023-06-11T12:53:46Z",
      "categories": [
        "cs.CL",
        "cs.AI",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2306.06672v1",
      "landing_url": "https://arxiv.org/abs/2306.06672v1",
      "doi": "https://doi.org/10.48550/arXiv.2306.06672"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The abstract centers on making HuBERT self-supervised learning feasible under academic compute through replication and optimizations, not on proposing or analyzing discrete audio tokenization or a discrete token vocabulary for audio reconstruction/understanding, thus failing the core inclusion criterion for tokenization analysis.",
      "evaluation": 2
    },
    "round-A_JuniorNano_reasoning": "The abstract centers on making HuBERT self-supervised learning feasible under academic compute through replication and optimizations, not on proposing or analyzing discrete audio tokenization or a discrete token vocabulary for audio reconstruction/understanding, thus failing the core inclusion criterion for tokenization analysis.",
    "round-A_JuniorNano_evaluation": 2,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on optimizing HuBERT self-supervised learning for speech processing but does not explicitly discuss proposing, analyzing, or evaluating discrete audio tokenization methods, which is a core inclusion criterion; it mainly addresses training efficiency and resource constraints without details on discrete token representations or quantization methods.",
      "evaluation": 2
    },
    "round-A_JuniorMini_reasoning": "The study focuses on optimizing HuBERT self-supervised learning for speech processing but does not explicitly discuss proposing, analyzing, or evaluating discrete audio tokenization methods, which is a core inclusion criterion; it mainly addresses training efficiency and resource constraints without details on discrete token representations or quantization methods.",
    "round-A_JuniorMini_evaluation": 2,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:2)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:2)"
  },
  {
    "title": "UniCATS: A Unified Context-Aware Text-to-Speech Framework with Contextual VQ-Diffusion and Vocoding",
    "abstract": "The utilization of discrete speech tokens, divided into semantic tokens and acoustic tokens, has been proven superior to traditional acoustic feature mel-spectrograms in terms of naturalness and robustness for text-to-speech (TTS) synthesis. Recent popular models, such as VALL-E and SPEAR-TTS, allow zero-shot speaker adaptation through auto-regressive (AR) continuation of acoustic tokens extracted from a short speech prompt. However, these AR models are restricted to generate speech only in a left-to-right direction, making them unsuitable for speech editing where both preceding and following contexts are provided. Furthermore, these models rely on acoustic tokens, which have audio quality limitations imposed by the performance of audio codec models. In this study, we propose a unified context-aware TTS framework called UniCATS, which is capable of both speech continuation and editing. UniCATS comprises two components, an acoustic model CTX-txt2vec and a vocoder CTX-vec2wav. CTX-txt2vec employs contextual VQ-diffusion to predict semantic tokens from the input text, enabling it to incorporate the semantic context and maintain seamless concatenation with the surrounding context. Following that, CTX-vec2wav utilizes contextual vocoding to convert these semantic tokens into waveforms, taking into consideration the acoustic context. Our experimental results demonstrate that CTX-vec2wav outperforms HifiGAN and AudioLM in terms of speech resynthesis from semantic tokens. Moreover, we show that UniCATS achieves state-of-the-art performance in both speech continuation and editing.",
    "metadata": {
      "arxiv_id": "2306.07547",
      "title": "UniCATS: A Unified Context-Aware Text-to-Speech Framework with Contextual VQ-Diffusion and Vocoding",
      "summary": "The utilization of discrete speech tokens, divided into semantic tokens and acoustic tokens, has been proven superior to traditional acoustic feature mel-spectrograms in terms of naturalness and robustness for text-to-speech (TTS) synthesis. Recent popular models, such as VALL-E and SPEAR-TTS, allow zero-shot speaker adaptation through auto-regressive (AR) continuation of acoustic tokens extracted from a short speech prompt. However, these AR models are restricted to generate speech only in a left-to-right direction, making them unsuitable for speech editing where both preceding and following contexts are provided. Furthermore, these models rely on acoustic tokens, which have audio quality limitations imposed by the performance of audio codec models. In this study, we propose a unified context-aware TTS framework called UniCATS, which is capable of both speech continuation and editing. UniCATS comprises two components, an acoustic model CTX-txt2vec and a vocoder CTX-vec2wav. CTX-txt2vec employs contextual VQ-diffusion to predict semantic tokens from the input text, enabling it to incorporate the semantic context and maintain seamless concatenation with the surrounding context. Following that, CTX-vec2wav utilizes contextual vocoding to convert these semantic tokens into waveforms, taking into consideration the acoustic context. Our experimental results demonstrate that CTX-vec2wav outperforms HifiGAN and AudioLM in terms of speech resynthesis from semantic tokens. Moreover, we show that UniCATS achieves state-of-the-art performance in both speech continuation and editing.",
      "authors": [
        "Chenpeng Du",
        "Yiwei Guo",
        "Feiyu Shen",
        "Zhijun Liu",
        "Zheng Liang",
        "Xie Chen",
        "Shuai Wang",
        "Hui Zhang",
        "Kai Yu"
      ],
      "published": "2023-06-13T05:38:34Z",
      "updated": "2024-03-28T13:56:33Z",
      "categories": [
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2306.07547v6",
      "landing_url": "https://arxiv.org/abs/2306.07547v6",
      "doi": "https://doi.org/10.1609/aaai.v38i16.29747"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The paper centers on discrete speech tokenization (semantic and acoustic tokens) with VQ-diffusion for TTS and a vocoding stage, meeting the inclusion criteria and not falling into any exclusion (continuous features, non-speech tokenization, or purely engineering descriptions).",
      "evaluation": 5
    },
    "round-A_JuniorNano_reasoning": "The paper centers on discrete speech tokenization (semantic and acoustic tokens) with VQ-diffusion for TTS and a vocoding stage, meeting the inclusion criteria and not falling into any exclusion (continuous features, non-speech tokenization, or purely engineering descriptions).",
    "round-A_JuniorNano_evaluation": 5,
    "round-A_JuniorMini_output": {
      "reasoning": "The study explicitly proposes and evaluates a discrete speech tokenization approach as a core component of a text-to-speech framework (UniCATS), involving semantic and acoustic tokens for speech reconstruction and editing, meeting all inclusion criteria and none of the exclusion criteria.",
      "evaluation": 5
    },
    "round-A_JuniorMini_reasoning": "The study explicitly proposes and evaluates a discrete speech tokenization approach as a core component of a text-to-speech framework (UniCATS), involving semantic and acoustic tokens for speech reconstruction and editing, meeting all inclusion criteria and none of the exclusion criteria.",
    "round-A_JuniorMini_evaluation": 5,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "include (junior:5)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Pushing the Limits of Unsupervised Unit Discovery for SSL Speech Representation",
    "abstract": "The excellent generalization ability of self-supervised learning (SSL) for speech foundation models has garnered significant attention. HuBERT is a successful example that utilizes offline clustering to convert speech features into discrete units for a masked language modeling pretext task. However, simply clustering features as targets by k-means does not fully inspire the model's performance. In this work, we present an unsupervised method to improve SSL targets. Two models are proposed, MonoBERT and PolyBERT, which leverage context-independent and context-dependent phoneme-based units for pre-training. Our models outperform other SSL models significantly on the LibriSpeech benchmark without the need for iterative re-clustering and re-training. Furthermore, our models equipped with context-dependent units even outperform target-improvement models that use labeled data during pre-training. How we progressively improve the unit discovery process is demonstrated through experiments.",
    "metadata": {
      "arxiv_id": "2306.08920",
      "title": "Pushing the Limits of Unsupervised Unit Discovery for SSL Speech Representation",
      "summary": "The excellent generalization ability of self-supervised learning (SSL) for speech foundation models has garnered significant attention. HuBERT is a successful example that utilizes offline clustering to convert speech features into discrete units for a masked language modeling pretext task. However, simply clustering features as targets by k-means does not fully inspire the model's performance. In this work, we present an unsupervised method to improve SSL targets. Two models are proposed, MonoBERT and PolyBERT, which leverage context-independent and context-dependent phoneme-based units for pre-training. Our models outperform other SSL models significantly on the LibriSpeech benchmark without the need for iterative re-clustering and re-training. Furthermore, our models equipped with context-dependent units even outperform target-improvement models that use labeled data during pre-training. How we progressively improve the unit discovery process is demonstrated through experiments.",
      "authors": [
        "Ziyang Ma",
        "Zhisheng Zheng",
        "Guanrou Yang",
        "Yu Wang",
        "Chao Zhang",
        "Xie Chen"
      ],
      "published": "2023-06-15T07:45:12Z",
      "updated": "2023-06-15T07:45:12Z",
      "categories": [
        "cs.CL",
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2306.08920v1",
      "landing_url": "https://arxiv.org/abs/2306.08920v1",
      "doi": "https://doi.org/10.48550/arXiv.2306.08920"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The article explicitly investigates discrete phoneme-based units learned unsupervised for SSL speech pre-training, i.e., converting continuous speech into discrete tokens for modeling, meeting the inclusion criteria and not fitting any exclusion criteria.",
      "evaluation": 5
    },
    "round-A_JuniorNano_reasoning": "The article explicitly investigates discrete phoneme-based units learned unsupervised for SSL speech pre-training, i.e., converting continuous speech into discrete tokens for modeling, meeting the inclusion criteria and not fitting any exclusion criteria.",
    "round-A_JuniorNano_evaluation": 5,
    "round-A_JuniorMini_output": {
      "reasoning": "The study explicitly proposes and analyzes discrete speech tokenization methods (MonoBERT and PolyBERT) that convert continuous speech features into discrete units for self-supervised learning, fulfilling the inclusion criteria on discrete audio tokenization for speech tasks; it also demonstrates methodological contributions with experimental validation on the LibriSpeech benchmark and does not fall into any exclusion criteria such as focusing solely on continuous features or traditional codecs.",
      "evaluation": 5
    },
    "round-A_JuniorMini_reasoning": "The study explicitly proposes and analyzes discrete speech tokenization methods (MonoBERT and PolyBERT) that convert continuous speech features into discrete units for self-supervised learning, fulfilling the inclusion criteria on discrete audio tokenization for speech tasks; it also demonstrates methodological contributions with experimental validation on the LibriSpeech benchmark and does not fall into any exclusion criteria such as focusing solely on continuous features or traditional codecs.",
    "round-A_JuniorMini_evaluation": 5,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "include (junior:5)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Self-Supervised Learning for Time Series Analysis: Taxonomy, Progress, and Prospects",
    "abstract": "Self-supervised learning (SSL) has recently achieved impressive performance on various time series tasks. The most prominent advantage of SSL is that it reduces the dependence on labeled data. Based on the pre-training and fine-tuning strategy, even a small amount of labeled data can achieve high performance. Compared with many published self-supervised surveys on computer vision and natural language processing, a comprehensive survey for time series SSL is still missing. To fill this gap, we review current state-of-the-art SSL methods for time series data in this article. To this end, we first comprehensively review existing surveys related to SSL and time series, and then provide a new taxonomy of existing time series SSL methods by summarizing them from three perspectives: generative-based, contrastive-based, and adversarial-based. These methods are further divided into ten subcategories with detailed reviews and discussions about their key intuitions, main frameworks, advantages and disadvantages. To facilitate the experiments and validation of time series SSL methods, we also summarize datasets commonly used in time series forecasting, classification, anomaly detection, and clustering tasks. Finally, we present the future directions of SSL for time series analysis.",
    "metadata": {
      "arxiv_id": "2306.10125",
      "title": "Self-Supervised Learning for Time Series Analysis: Taxonomy, Progress, and Prospects",
      "summary": "Self-supervised learning (SSL) has recently achieved impressive performance on various time series tasks. The most prominent advantage of SSL is that it reduces the dependence on labeled data. Based on the pre-training and fine-tuning strategy, even a small amount of labeled data can achieve high performance. Compared with many published self-supervised surveys on computer vision and natural language processing, a comprehensive survey for time series SSL is still missing. To fill this gap, we review current state-of-the-art SSL methods for time series data in this article. To this end, we first comprehensively review existing surveys related to SSL and time series, and then provide a new taxonomy of existing time series SSL methods by summarizing them from three perspectives: generative-based, contrastive-based, and adversarial-based. These methods are further divided into ten subcategories with detailed reviews and discussions about their key intuitions, main frameworks, advantages and disadvantages. To facilitate the experiments and validation of time series SSL methods, we also summarize datasets commonly used in time series forecasting, classification, anomaly detection, and clustering tasks. Finally, we present the future directions of SSL for time series analysis.",
      "authors": [
        "Kexin Zhang",
        "Qingsong Wen",
        "Chaoli Zhang",
        "Rongyao Cai",
        "Ming Jin",
        "Yong Liu",
        "James Zhang",
        "Yuxuan Liang",
        "Guansong Pang",
        "Dongjin Song",
        "Shirui Pan"
      ],
      "published": "2023-06-16T18:23:10Z",
      "updated": "2024-04-08T15:38:59Z",
      "categories": [
        "cs.LG",
        "cs.AI",
        "eess.SP",
        "stat.AP"
      ],
      "pdf_url": "https://arxiv.org/pdf/2306.10125v4",
      "landing_url": "https://arxiv.org/abs/2306.10125v4",
      "doi": "https://doi.org/10.48550/arXiv.2306.10125"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The abstract covers self-supervised learning for time series broadly and a taxonomy; it does not discuss discrete audio/speech tokenization or tokenization-based representations, so it fails the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The abstract covers self-supervised learning for time series broadly and a taxonomy; it does not discuss discrete audio/speech tokenization or tokenization-based representations, so it fails the inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The article is a survey on self-supervised learning for time series analysis and does not specifically propose, analyze, or evaluate discrete audio/speech tokenization methods; moreover, it does not mention discrete token representations in the context of audio or speech, failing the inclusion criteria and not addressing the exclusion criteria specifically but lacking relevance to discrete audio tokenization.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The article is a survey on self-supervised learning for time series analysis and does not specifically propose, analyze, or evaluate discrete audio/speech tokenization methods; moreover, it does not mention discrete token representations in the context of audio or speech, failing the inclusion criteria and not addressing the exclusion criteria specifically but lacking relevance to discrete audio tokenization.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "LM-VC: Zero-shot Voice Conversion via Speech Generation based on Language Models",
    "abstract": "Language model (LM) based audio generation frameworks, e.g., AudioLM, have recently achieved new state-of-the-art performance in zero-shot audio generation. In this paper, we explore the feasibility of LMs for zero-shot voice conversion. An intuitive approach is to follow AudioLM - Tokenizing speech into semantic and acoustic tokens respectively by HuBERT and SoundStream, and converting source semantic tokens to target acoustic tokens conditioned on acoustic tokens of the target speaker. However, such an approach encounters several issues: 1) the linguistic content contained in semantic tokens may get dispersed during multi-layer modeling while the lengthy speech input in the voice conversion task makes contextual learning even harder; 2) the semantic tokens still contain speaker-related information, which may be leaked to the target speech, lowering the target speaker similarity; 3) the generation diversity in the sampling of the LM can lead to unexpected outcomes during inference, leading to unnatural pronunciation and speech quality degradation. To mitigate these problems, we propose LM-VC, a two-stage language modeling approach that generates coarse acoustic tokens for recovering the source linguistic content and target speaker's timbre, and then reconstructs the fine for acoustic details as converted speech. Specifically, to enhance content preservation and facilitates better disentanglement, a masked prefix LM with a mask prediction strategy is used for coarse acoustic modeling. This model is encouraged to recover the masked content from the surrounding context and generate target speech based on the target speaker's utterance and corrupted semantic tokens. Besides, to further alleviate the sampling error in the generation, an external LM, which employs window attention to capture the local acoustic relations, is introduced to participate in the coarse acoustic modeling.",
    "metadata": {
      "arxiv_id": "2306.10521",
      "title": "LM-VC: Zero-shot Voice Conversion via Speech Generation based on Language Models",
      "summary": "Language model (LM) based audio generation frameworks, e.g., AudioLM, have recently achieved new state-of-the-art performance in zero-shot audio generation. In this paper, we explore the feasibility of LMs for zero-shot voice conversion. An intuitive approach is to follow AudioLM - Tokenizing speech into semantic and acoustic tokens respectively by HuBERT and SoundStream, and converting source semantic tokens to target acoustic tokens conditioned on acoustic tokens of the target speaker. However, such an approach encounters several issues: 1) the linguistic content contained in semantic tokens may get dispersed during multi-layer modeling while the lengthy speech input in the voice conversion task makes contextual learning even harder; 2) the semantic tokens still contain speaker-related information, which may be leaked to the target speech, lowering the target speaker similarity; 3) the generation diversity in the sampling of the LM can lead to unexpected outcomes during inference, leading to unnatural pronunciation and speech quality degradation. To mitigate these problems, we propose LM-VC, a two-stage language modeling approach that generates coarse acoustic tokens for recovering the source linguistic content and target speaker's timbre, and then reconstructs the fine for acoustic details as converted speech. Specifically, to enhance content preservation and facilitates better disentanglement, a masked prefix LM with a mask prediction strategy is used for coarse acoustic modeling. This model is encouraged to recover the masked content from the surrounding context and generate target speech based on the target speaker's utterance and corrupted semantic tokens. Besides, to further alleviate the sampling error in the generation, an external LM, which employs window attention to capture the local acoustic relations, is introduced to participate in the coarse acoustic modeling.",
      "authors": [
        "Zhichao Wang",
        "Yuanzhe Chen",
        "Lei Xie",
        "Qiao Tian",
        "Yuping Wang"
      ],
      "published": "2023-06-18T10:59:06Z",
      "updated": "2023-08-21T02:21:06Z",
      "categories": [
        "eess.AS",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2306.10521v2",
      "landing_url": "https://arxiv.org/abs/2306.10521v2",
      "doi": "https://doi.org/10.48550/arXiv.2306.10521"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "Abstract centers on discrete speech tokenization into semantic and acoustic tokens and uses those tokens in LM-based generation for voice conversion, aligning with the inclusion criterion and not falling under excluded categories.",
      "evaluation": 5
    },
    "round-A_JuniorNano_reasoning": "Abstract centers on discrete speech tokenization into semantic and acoustic tokens and uses those tokens in LM-based generation for voice conversion, aligning with the inclusion criterion and not falling under excluded categories.",
    "round-A_JuniorNano_evaluation": 5,
    "round-A_JuniorMini_output": {
      "reasoning": "The study proposes and analyzes discrete audio/speech tokenization methods using HuBERT and SoundStream to convert speech into semantic and acoustic tokens, which are central to the zero-shot voice conversion task explored, satisfying the inclusion criteria of methodological contribution to discrete audio tokens. Moreover, the study is in English and includes experimental validation related to discrete tokens through language model-based generation approaches. Hence, it meets all inclusion criteria and none of the exclusion ones.",
      "evaluation": 5
    },
    "round-A_JuniorMini_reasoning": "The study proposes and analyzes discrete audio/speech tokenization methods using HuBERT and SoundStream to convert speech into semantic and acoustic tokens, which are central to the zero-shot voice conversion task explored, satisfying the inclusion criteria of methodological contribution to discrete audio tokens. Moreover, the study is in English and includes experimental validation related to discrete tokens through language model-based generation approaches. Hence, it meets all inclusion criteria and none of the exclusion ones.",
    "round-A_JuniorMini_evaluation": 5,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "include (junior:5)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "MFCCGAN: A Novel MFCC-Based Speech Synthesizer Using Adversarial Learning",
    "abstract": "In this paper, we introduce MFCCGAN as a novel speech synthesizer based on adversarial learning that adopts MFCCs as input and generates raw speech waveforms. Benefiting the GAN model capabilities, it produces speech with higher intelligibility than a rule-based MFCC-based speech synthesizer WORLD. We evaluated the model based on a popular intrusive objective speech intelligibility measure (STOI) and quality (NISQA score). Experimental results show that our proposed system outperforms Librosa MFCC- inversion (by an increase of about 26% up to 53% in STOI and 16% up to 78% in NISQA score) and a rise of about 10% in intelligibility and about 4% in naturalness in comparison with conventional rule-based vocoder WORLD that used in the CycleGAN-VC family. However, WORLD needs additional data like F0. Finally, using perceptual loss in discriminators based on STOI could improve the quality more. WebMUSHRA-based subjective tests also show the quality of the proposed approach.",
    "metadata": {
      "arxiv_id": "2306.12785",
      "title": "MFCCGAN: A Novel MFCC-Based Speech Synthesizer Using Adversarial Learning",
      "summary": "In this paper, we introduce MFCCGAN as a novel speech synthesizer based on adversarial learning that adopts MFCCs as input and generates raw speech waveforms. Benefiting the GAN model capabilities, it produces speech with higher intelligibility than a rule-based MFCC-based speech synthesizer WORLD. We evaluated the model based on a popular intrusive objective speech intelligibility measure (STOI) and quality (NISQA score). Experimental results show that our proposed system outperforms Librosa MFCC- inversion (by an increase of about 26% up to 53% in STOI and 16% up to 78% in NISQA score) and a rise of about 10% in intelligibility and about 4% in naturalness in comparison with conventional rule-based vocoder WORLD that used in the CycleGAN-VC family. However, WORLD needs additional data like F0. Finally, using perceptual loss in discriminators based on STOI could improve the quality more. WebMUSHRA-based subjective tests also show the quality of the proposed approach.",
      "authors": [
        "Mohammad Reza Hasanabadi Majid Behdad Davood Gharavian"
      ],
      "published": "2023-06-22T10:29:24Z",
      "updated": "2023-06-22T10:29:24Z",
      "categories": [
        "cs.SD",
        "cs.AI",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2306.12785v1",
      "landing_url": "https://arxiv.org/abs/2306.12785v1",
      "doi": "https://doi.org/10.1109/ICASSP49357.2023.10095873"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This item uses MFCCs as input features for a GAN-based speech synthesizer and does not involve discretization/tokenization of audio into discrete tokens; thus it does not meet the discrete tokenization criterion and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "This item uses MFCCs as input features for a GAN-based speech synthesizer and does not involve discretization/tokenization of audio into discrete tokens; thus it does not meet the discrete tokenization criterion and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on speech synthesis using MFCC features and GAN-based modeling, but it does not mention proposing, analyzing, or evaluating any discrete audio/speech tokenization or converting continuous audio signals into discrete token sequences; it mainly uses continuous MFCC features without discrete quantization, which falls under the exclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on speech synthesis using MFCC features and GAN-based modeling, but it does not mention proposing, analyzing, or evaluating any discrete audio/speech tokenization or converting continuous audio signals into discrete token sequences; it mainly uses continuous MFCC features without discrete quantization, which falls under the exclusion criteria.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Evaluation of Virtual Acoustic Environments with Different Acoustic Level of Detail",
    "abstract": "Virtual acoustic environments enable the creation and simulation of realistic and ecologically valid daily-life situations with applications in hearing research and audiology. Hereby, reverberant indoor environments play an important role. For real-time applications, simplifications in the room acoustics simulation are required, however, it remains unclear what acoustic level of detail (ALOD) is necessary to capture all perceptually relevant effects. This study investigates the effect of varying ALOD in the simulation of three different real environments, a living room with a coupled kitchen, a pub, and an underground station. ALOD was varied by generating different numbers of image sources for early reflections, or by excluding geometrical room details specific for each environment. The simulations were perceptually evaluated using headphones in comparison to binaural room impulse responses measured with a dummy head in the corresponding real environments, and partly using loudspeakers. The study assessed the perceived overall difference for a pulse, and a speech token. Furthermore, plausibility and externalization were evaluated. The results show that a strong reduction in ALOD is possible while obtaining similar plausibility and externalization as with the dummy head recordings. The number and accuracy of early reflections appear less relevant, provided diffuse late reverberation is appropriately accounted for.",
    "metadata": {
      "arxiv_id": "2306.17012",
      "title": "Evaluation of Virtual Acoustic Environments with Different Acoustic Level of Detail",
      "summary": "Virtual acoustic environments enable the creation and simulation of realistic and ecologically valid daily-life situations with applications in hearing research and audiology. Hereby, reverberant indoor environments play an important role. For real-time applications, simplifications in the room acoustics simulation are required, however, it remains unclear what acoustic level of detail (ALOD) is necessary to capture all perceptually relevant effects. This study investigates the effect of varying ALOD in the simulation of three different real environments, a living room with a coupled kitchen, a pub, and an underground station. ALOD was varied by generating different numbers of image sources for early reflections, or by excluding geometrical room details specific for each environment. The simulations were perceptually evaluated using headphones in comparison to binaural room impulse responses measured with a dummy head in the corresponding real environments, and partly using loudspeakers. The study assessed the perceived overall difference for a pulse, and a speech token. Furthermore, plausibility and externalization were evaluated. The results show that a strong reduction in ALOD is possible while obtaining similar plausibility and externalization as with the dummy head recordings. The number and accuracy of early reflections appear less relevant, provided diffuse late reverberation is appropriately accounted for.",
      "authors": [
        "Stefan Fichna",
        "Steven van de Par",
        "Stephan D. Ewert"
      ],
      "published": "2023-06-29T15:07:13Z",
      "updated": "2023-08-10T14:39:50Z",
      "categories": [
        "eess.AS",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2306.17012v2",
      "landing_url": "https://arxiv.org/abs/2306.17012v2",
      "doi": "https://doi.org/10.1109/I3DA57090.2023.10289496"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The study focuses on acoustic level of detail in virtual environments and perceptual evaluation, with no discrete audio tokenization or token-based analysis for speech; thus it does not meet the inclusion criterion and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The study focuses on acoustic level of detail in virtual environments and perceptual evaluation, with no discrete audio tokenization or token-based analysis for speech; thus it does not meet the inclusion criterion and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on evaluating virtual acoustic environments and the perceptual effects of different acoustic levels of detail in room acoustics simulations, without discussing discrete audio or speech tokenization methods where continuous signals are converted into sequences of discrete symbols. The core content is about acoustic simulation and perceptual evaluation rather than discrete token representation or quantization of audio signals relevant to speech tasks, thus failing to meet the inclusion criteria and falling within the exclusion criteria related to lack of discrete token design or analysis.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on evaluating virtual acoustic environments and the perceptual effects of different acoustic levels of detail in room acoustics simulations, without discussing discrete audio or speech tokenization methods where continuous signals are converted into sequences of discrete symbols. The core content is about acoustic simulation and perceptual evaluation rather than discrete token representation or quantization of audio signals relevant to speech tasks, thus failing to meet the inclusion criteria and falling within the exclusion criteria related to lack of discrete token design or analysis.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Joint Hierarchical Priors and Adaptive Spatial Resolution for Efficient Neural Image Compression",
    "abstract": "Recently, the performance of neural image compression (NIC) has steadily improved thanks to the last line of study, reaching or outperforming state-of-the-art conventional codecs. Despite significant progress, current NIC methods still rely on ConvNet-based entropy coding, limited in modeling long-range dependencies due to their local connectivity and the increasing number of architectural biases and priors, resulting in complex underperforming models with high decoding latency. Motivated by the efficiency investigation of the Tranformer-based transform coding framework, namely SwinT-ChARM, we propose to enhance the latter, as first, with a more straightforward yet effective Tranformer-based channel-wise auto-regressive prior model, resulting in an absolute image compression transformer (ICT). Through the proposed ICT, we can capture both global and local contexts from the latent representations and better parameterize the distribution of the quantized latents. Further, we leverage a learnable scaling module with a sandwich ConvNeXt-based pre-/post-processor to accurately extract more compact latent codes while reconstructing higher-quality images. Extensive experimental results on benchmark datasets showed that the proposed framework significantly improves the trade-off between coding efficiency and decoder complexity over the versatile video coding (VVC) reference encoder (VTM-18.0) and the neural codec SwinT-ChARM. Moreover, we provide model scaling studies to verify the computational efficiency of our approach and conduct several objective and subjective analyses to bring to the fore the performance gap between the adaptive image compression transformer (AICT) and the neural codec SwinT-ChARM.",
    "metadata": {
      "arxiv_id": "2307.02273",
      "title": "Joint Hierarchical Priors and Adaptive Spatial Resolution for Efficient Neural Image Compression",
      "summary": "Recently, the performance of neural image compression (NIC) has steadily improved thanks to the last line of study, reaching or outperforming state-of-the-art conventional codecs. Despite significant progress, current NIC methods still rely on ConvNet-based entropy coding, limited in modeling long-range dependencies due to their local connectivity and the increasing number of architectural biases and priors, resulting in complex underperforming models with high decoding latency. Motivated by the efficiency investigation of the Tranformer-based transform coding framework, namely SwinT-ChARM, we propose to enhance the latter, as first, with a more straightforward yet effective Tranformer-based channel-wise auto-regressive prior model, resulting in an absolute image compression transformer (ICT). Through the proposed ICT, we can capture both global and local contexts from the latent representations and better parameterize the distribution of the quantized latents. Further, we leverage a learnable scaling module with a sandwich ConvNeXt-based pre-/post-processor to accurately extract more compact latent codes while reconstructing higher-quality images. Extensive experimental results on benchmark datasets showed that the proposed framework significantly improves the trade-off between coding efficiency and decoder complexity over the versatile video coding (VVC) reference encoder (VTM-18.0) and the neural codec SwinT-ChARM. Moreover, we provide model scaling studies to verify the computational efficiency of our approach and conduct several objective and subjective analyses to bring to the fore the performance gap between the adaptive image compression transformer (AICT) and the neural codec SwinT-ChARM.",
      "authors": [
        "Ahmed Ghorbel",
        "Wassim Hamidouche",
        "Luce Morin"
      ],
      "published": "2023-07-05T13:17:14Z",
      "updated": "2024-01-22T17:37:03Z",
      "categories": [
        "cs.CV",
        "eess.IV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2307.02273v4",
      "landing_url": "https://arxiv.org/abs/2307.02273v4",
      "doi": "https://doi.org/10.48550/arXiv.2307.02273"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This item focuses on neural image compression and image latent representations, not on discrete audio/speech tokenization; it does not meet the audio tokenization criterion and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "This item focuses on neural image compression and image latent representations, not on discrete audio/speech tokenization; it does not meet the audio tokenization criterion and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses entirely on neural image compression using transformer-based models and convolutional networks, with no mention or analysis of discrete audio or speech tokenization, quantization, or audio-related tasks; thus, it does not meet any inclusion criteria and falls outside the scope regarding audio tokenization.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses entirely on neural image compression using transformer-based models and convolutional networks, with no mention or analysis of discrete audio or speech tokenization, quantization, or audio-related tasks; thus, it does not meet any inclusion criteria and falls outside the scope regarding audio tokenization.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "IANS: Intelligibility-aware Null-steering Beamforming for Dual-Microphone Arrays",
    "abstract": "Beamforming techniques are popular in speech-related applications due to their effective spatial filtering capabilities. Nonetheless, conventional beamforming techniques generally depend heavily on either the target's direction-of-arrival (DOA), relative transfer function (RTF) or covariance matrix. This paper presents a new approach, the intelligibility-aware null-steering (IANS) beamforming framework, which uses the STOI-Net intelligibility prediction model to improve speech intelligibility without prior knowledge of the speech signal parameters mentioned earlier. The IANS framework combines a null-steering beamformer (NSBF) to generate a set of beamformed outputs, and STOI-Net, to determine the optimal result. Experimental results indicate that IANS can produce intelligibility-enhanced signals using a small dual-microphone array. The results are comparable to those obtained by null-steering beamformers with given knowledge of DOAs.",
    "metadata": {
      "arxiv_id": "2307.04179",
      "title": "IANS: Intelligibility-aware Null-steering Beamforming for Dual-Microphone Arrays",
      "summary": "Beamforming techniques are popular in speech-related applications due to their effective spatial filtering capabilities. Nonetheless, conventional beamforming techniques generally depend heavily on either the target's direction-of-arrival (DOA), relative transfer function (RTF) or covariance matrix. This paper presents a new approach, the intelligibility-aware null-steering (IANS) beamforming framework, which uses the STOI-Net intelligibility prediction model to improve speech intelligibility without prior knowledge of the speech signal parameters mentioned earlier. The IANS framework combines a null-steering beamformer (NSBF) to generate a set of beamformed outputs, and STOI-Net, to determine the optimal result. Experimental results indicate that IANS can produce intelligibility-enhanced signals using a small dual-microphone array. The results are comparable to those obtained by null-steering beamformers with given knowledge of DOAs.",
      "authors": [
        "Wen-Yuan Ting",
        "Syu-Siang Wang",
        "Yu Tsao",
        "Borching Su"
      ],
      "published": "2023-07-09T14:04:58Z",
      "updated": "2023-07-09T14:04:58Z",
      "categories": [
        "eess.AS",
        "eess.SP"
      ],
      "pdf_url": "https://arxiv.org/pdf/2307.04179v1",
      "landing_url": "https://arxiv.org/abs/2307.04179v1",
      "doi": "https://doi.org/10.48550/arXiv.2307.04179"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The work focuses on intelligibility-aware beamforming using STOI-Net and does not introduce or analyze discrete speech tokenization into finite vocabularies, thus it fails the inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The work focuses on intelligibility-aware beamforming using STOI-Net and does not introduce or analyze discrete speech tokenization into finite vocabularies, thus it fails the inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper focuses on a beamforming technique to enhance speech intelligibility using STOI-Net without involving discrete audio/speech tokenization or any conversion of continuous signals into discrete token sequences; thus, it does not meet the inclusion criteria centered around discrete tokenization.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper focuses on a beamforming technique to enhance speech intelligibility using STOI-Net without involving discrete audio/speech tokenization or any conversion of continuous signals into discrete token sequences; thus, it does not meet the inclusion criteria centered around discrete tokenization.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "VampNet: Music Generation via Masked Acoustic Token Modeling",
    "abstract": "We introduce VampNet, a masked acoustic token modeling approach to music synthesis, compression, inpainting, and variation. We use a variable masking schedule during training which allows us to sample coherent music from the model by applying a variety of masking approaches (called prompts) during inference. VampNet is non-autoregressive, leveraging a bidirectional transformer architecture that attends to all tokens in a forward pass. With just 36 sampling passes, VampNet can generate coherent high-fidelity musical waveforms. We show that by prompting VampNet in various ways, we can apply it to tasks like music compression, inpainting, outpainting, continuation, and looping with variation (vamping). Appropriately prompted, VampNet is capable of maintaining style, genre, instrumentation, and other high-level aspects of the music. This flexible prompting capability makes VampNet a powerful music co-creation tool. Code and audio samples are available online.",
    "metadata": {
      "arxiv_id": "2307.04686",
      "title": "VampNet: Music Generation via Masked Acoustic Token Modeling",
      "summary": "We introduce VampNet, a masked acoustic token modeling approach to music synthesis, compression, inpainting, and variation. We use a variable masking schedule during training which allows us to sample coherent music from the model by applying a variety of masking approaches (called prompts) during inference. VampNet is non-autoregressive, leveraging a bidirectional transformer architecture that attends to all tokens in a forward pass. With just 36 sampling passes, VampNet can generate coherent high-fidelity musical waveforms. We show that by prompting VampNet in various ways, we can apply it to tasks like music compression, inpainting, outpainting, continuation, and looping with variation (vamping). Appropriately prompted, VampNet is capable of maintaining style, genre, instrumentation, and other high-level aspects of the music. This flexible prompting capability makes VampNet a powerful music co-creation tool. Code and audio samples are available online.",
      "authors": [
        "Hugo Flores Garcia",
        "Prem Seetharaman",
        "Rithesh Kumar",
        "Bryan Pardo"
      ],
      "published": "2023-07-10T16:42:03Z",
      "updated": "2023-07-12T17:06:41Z",
      "categories": [
        "cs.SD",
        "cs.AI",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2307.04686v2",
      "landing_url": "https://arxiv.org/abs/2307.04686v2",
      "doi": "https://doi.org/10.48550/arXiv.2307.04686"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "VampNet explicitly uses masked acoustic token modeling with discrete tokens and token-based generation for music synthesis, compression, and inpainting, satisfying the requirement of discrete audio tokenization as a core object without triggering the exclusions.",
      "evaluation": 5
    },
    "round-A_JuniorNano_reasoning": "VampNet explicitly uses masked acoustic token modeling with discrete tokens and token-based generation for music synthesis, compression, and inpainting, satisfying the requirement of discrete audio tokenization as a core object without triggering the exclusions.",
    "round-A_JuniorNano_evaluation": 5,
    "round-A_JuniorMini_output": {
      "reasoning": "The study presents VampNet, focusing on masked acoustic token modeling for music generation, compression, and other audio tasks; however, it centers on music (non-speech) and does not explicitly discuss discrete tokenization applicable to speech or analyze speech-relevant implications, which is a key inclusion criterion. Therefore, it does not meet all inclusion criteria and partially falls under the exclusion criteria regarding tokenization methods for non-speech audio without speech relevance.",
      "evaluation": 2
    },
    "round-A_JuniorMini_reasoning": "The study presents VampNet, focusing on masked acoustic token modeling for music generation, compression, and other audio tasks; however, it centers on music (non-speech) and does not explicitly discuss discrete tokenization applicable to speech or analyze speech-relevant implications, which is a key inclusion criterion. Therefore, it does not meet all inclusion criteria and partially falls under the exclusion criteria regarding tokenization methods for non-speech audio without speech relevance.",
    "round-A_JuniorMini_evaluation": 2,
    "round-B_SeniorLead_output": {
      "reasoning": "I reviewed the two junior assessments and the abstract: while VampNet repeatedly mentions “masked acoustic token modeling” and token-based generation for music, the abstract centers on music-generation tasks and does not explicitly describe or analyze the discrete quantization/tokenization design nor discuss applicability to speech or speech-relevant implications — a required inclusion criterion; therefore it fails the requirement that discrete audio/speech tokenization be a core, explicitly analyzed object and that the approach be applicable to or evaluated for speech. One-sentence justification: Score 2 — better to exclude, because the work appears focused on music-only token modeling without explicit discrete-token design/analysis or speech applicability.",
      "evaluation": 2
    },
    "round-B_SeniorLead_reasoning": "I reviewed the two junior assessments and the abstract: while VampNet repeatedly mentions “masked acoustic token modeling” and token-based generation for music, the abstract centers on music-generation tasks and does not explicitly describe or analyze the discrete quantization/tokenization design nor discuss applicability to speech or speech-relevant implications — a required inclusion criterion; therefore it fails the requirement that discrete audio/speech tokenization be a core, explicitly analyzed object and that the approach be applicable to or evaluated for speech. One-sentence justification: Score 2 — better to exclude, because the work appears focused on music-only token modeling without explicit discrete-token design/analysis or speech applicability.",
    "round-B_SeniorLead_evaluation": 2,
    "final_verdict": "exclude (senior:2)",
    "review_skipped": false,
    "discard_reason": "exclude (senior:2)"
  },
  {
    "title": "Circularity in Finite Fields and Solutions of the Equations $\\boldsymbol{x^{m}+y^{m}-z^{m}=1}$",
    "abstract": "An explicit formula for the number of solutions of the equation in the title is given when a certain condition, depending only on the exponent and the characteristic of the field, holds. This formula improves the one given by the authors in an earlier paper.",
    "metadata": {
      "arxiv_id": "2307.05586",
      "title": "Circularity in Finite Fields and Solutions of the Equations $\\boldsymbol{x^{m}+y^{m}-z^{m}=1}$",
      "summary": "An explicit formula for the number of solutions of the equation in the title is given when a certain condition, depending only on the exponent and the characteristic of the field, holds. This formula improves the one given by the authors in an earlier paper.",
      "authors": [
        "Wen-Fong Ke",
        "Hubert Kiechle"
      ],
      "published": "2023-07-10T15:46:40Z",
      "updated": "2023-07-13T10:40:18Z",
      "categories": [
        "math.NT",
        "math.CO"
      ],
      "pdf_url": "https://arxiv.org/pdf/2307.05586v2",
      "landing_url": "https://arxiv.org/abs/2307.05586v2",
      "doi": "https://doi.org/10.1016/j.ffa.2024.102467"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "Not about discrete audio/speech tokenization: it's a number-theory/finite-field equation paper with no analysis or evaluation of discrete audio tokens or tokenization; therefore it does not meet inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "Not about discrete audio/speech tokenization: it's a number-theory/finite-field equation paper with no analysis or evaluation of discrete audio tokens or tokenization; therefore it does not meet inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on explicit formulas for solutions to a mathematical equation in finite fields and does not address any aspect of discrete audio or speech tokenization, nor does it involve neural encoding or audio signal processing relevant to the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on explicit formulas for solutions to a mathematical equation in finite fields and does not address any aspect of discrete audio or speech tokenization, nor does it involve neural encoding or audio signal processing relevant to the inclusion criteria.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "AICT: An Adaptive Image Compression Transformer",
    "abstract": "Motivated by the efficiency investigation of the Tranformer-based transform coding framework, namely SwinT-ChARM, we propose to enhance the latter, as first, with a more straightforward yet effective Tranformer-based channel-wise auto-regressive prior model, resulting in an absolute image compression transformer (ICT). Current methods that still rely on ConvNet-based entropy coding are limited in long-range modeling dependencies due to their local connectivity and an increasing number of architectural biases and priors. On the contrary, the proposed ICT can capture both global and local contexts from the latent representations and better parameterize the distribution of the quantized latents. Further, we leverage a learnable scaling module with a sandwich ConvNeXt-based pre/post-processor to accurately extract more compact latent representation while reconstructing higher-quality images. Extensive experimental results on benchmark datasets showed that the proposed adaptive image compression transformer (AICT) framework significantly improves the trade-off between coding efficiency and decoder complexity over the versatile video coding (VVC) reference encoder (VTM-18.0) and the neural codec SwinT-ChARM.",
    "metadata": {
      "arxiv_id": "2307.06091",
      "title": "AICT: An Adaptive Image Compression Transformer",
      "summary": "Motivated by the efficiency investigation of the Tranformer-based transform coding framework, namely SwinT-ChARM, we propose to enhance the latter, as first, with a more straightforward yet effective Tranformer-based channel-wise auto-regressive prior model, resulting in an absolute image compression transformer (ICT). Current methods that still rely on ConvNet-based entropy coding are limited in long-range modeling dependencies due to their local connectivity and an increasing number of architectural biases and priors. On the contrary, the proposed ICT can capture both global and local contexts from the latent representations and better parameterize the distribution of the quantized latents. Further, we leverage a learnable scaling module with a sandwich ConvNeXt-based pre/post-processor to accurately extract more compact latent representation while reconstructing higher-quality images. Extensive experimental results on benchmark datasets showed that the proposed adaptive image compression transformer (AICT) framework significantly improves the trade-off between coding efficiency and decoder complexity over the versatile video coding (VVC) reference encoder (VTM-18.0) and the neural codec SwinT-ChARM.",
      "authors": [
        "Ahmed Ghorbel",
        "Wassim Hamidouche",
        "Luce Morin"
      ],
      "published": "2023-07-12T11:32:02Z",
      "updated": "2023-07-12T11:32:02Z",
      "categories": [
        "cs.CV",
        "eess.IV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2307.06091v1",
      "landing_url": "https://arxiv.org/abs/2307.06091v1",
      "doi": "https://doi.org/10.48550/arXiv.2307.06091"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item analyzes an adaptive image compression transformer without any discrete audio/tokenization focus; it does not meet the audio tokenization inclusion criteria and aligns more with image compression, thus should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item analyzes an adaptive image compression transformer without any discrete audio/tokenization focus; it does not meet the audio tokenization inclusion criteria and aligns more with image compression, thus should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on image compression using Transformer-based methods and does not address discrete audio/speech tokenization or any audio modality-related analysis, which is required by the inclusion criteria. Therefore, it does not meet the inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on image compression using Transformer-based methods and does not address discrete audio/speech tokenization or any audio modality-related analysis, which is required by the inclusion criteria. Therefore, it does not meet the inclusion criteria and should be excluded.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Overlaps in Field Generated Circular Planar Nearrings",
    "abstract": "We investigate circular planar nearrings constructed from finite fields as well the complex number field using a multiplicative subgroup of order $k$, and characterize the overlaps of the basic graphs which arise in the associated $2$-designs.",
    "metadata": {
      "arxiv_id": "2307.07303",
      "title": "Overlaps in Field Generated Circular Planar Nearrings",
      "summary": "We investigate circular planar nearrings constructed from finite fields as well the complex number field using a multiplicative subgroup of order $k$, and characterize the overlaps of the basic graphs which arise in the associated $2$-designs.",
      "authors": [
        "Wen-Fong Ke",
        "Hubert Kiechle"
      ],
      "published": "2023-07-14T12:28:55Z",
      "updated": "2023-08-05T02:33:41Z",
      "categories": [
        "math.CO",
        "math.NT"
      ],
      "pdf_url": "https://arxiv.org/pdf/2307.07303v2",
      "landing_url": "https://arxiv.org/abs/2307.07303v2",
      "doi": "https://doi.org/10.12958/adm2130"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The title/abstract describe mathematical structures (fields, nearrings, 2-designs) with no discrete audio/speech tokenization analysis, so it does not meet the inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The title/abstract describe mathematical structures (fields, nearrings, 2-designs) with no discrete audio/speech tokenization analysis, so it does not meet the inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on mathematical structures related to circular planar nearrings and their associated graphs, which do not relate to discrete audio/speech tokenization or any audio signal processing, and therefore do not meet the inclusion criteria nor avoid the exclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on mathematical structures related to circular planar nearrings and their associated graphs, which do not relate to discrete audio/speech tokenization or any audio signal processing, and therefore do not meet the inclusion criteria nor avoid the exclusion criteria.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Deduplicating and Ranking Solution Programs for Suggesting Reference Solutions",
    "abstract": "Referring to solution programs written by other users is helpful for learners in programming education. However, current online judge systems just list all solution programs submitted by users for references, and the programs are sorted based on the submission date and time, execution time, or user rating, ignoring to what extent the programs can be helpful to be referenced. In addition, users struggle to refer to a variety of solution approaches since there are too many duplicated and near-duplicated programs. To motivate learners to refer to various solutions to learn better solution approaches, in this paper, we propose an approach to deduplicate and rank common solution programs in each programming problem. Inspired by the nature that the many-duplicated program adopts a more common approach and can be a general reference, we remove the near-duplicated solution programs and rank the unique programs based on the duplicate count. The experiments on the solution programs submitted to a real-world online judge system demonstrate that the number of programs is reduced by 60.20%, whereas the baseline only reduces by 29.59% after the deduplication, meaning that users only need to refer to 39.80% of programs on average. Furthermore, our analysis shows that top-10 ranked programs cover 29.95% of programs on average, indicating that users can grasp 29.95% of solution approaches by referring to only 10 programs. The proposed approach shows the potential of reducing the learners' burden of referring to too many solutions and motivating them to learn a variety of solution approaches.",
    "metadata": {
      "arxiv_id": "2307.07940",
      "title": "Deduplicating and Ranking Solution Programs for Suggesting Reference Solutions",
      "summary": "Referring to solution programs written by other users is helpful for learners in programming education. However, current online judge systems just list all solution programs submitted by users for references, and the programs are sorted based on the submission date and time, execution time, or user rating, ignoring to what extent the programs can be helpful to be referenced. In addition, users struggle to refer to a variety of solution approaches since there are too many duplicated and near-duplicated programs. To motivate learners to refer to various solutions to learn better solution approaches, in this paper, we propose an approach to deduplicate and rank common solution programs in each programming problem. Inspired by the nature that the many-duplicated program adopts a more common approach and can be a general reference, we remove the near-duplicated solution programs and rank the unique programs based on the duplicate count. The experiments on the solution programs submitted to a real-world online judge system demonstrate that the number of programs is reduced by 60.20%, whereas the baseline only reduces by 29.59% after the deduplication, meaning that users only need to refer to 39.80% of programs on average. Furthermore, our analysis shows that top-10 ranked programs cover 29.95% of programs on average, indicating that users can grasp 29.95% of solution approaches by referring to only 10 programs. The proposed approach shows the potential of reducing the learners' burden of referring to too many solutions and motivating them to learn a variety of solution approaches.",
      "authors": [
        "Atsushi Shirafuji",
        "Yutaka Watanobe"
      ],
      "published": "2023-07-16T04:20:26Z",
      "updated": "2023-09-11T09:42:37Z",
      "categories": [
        "cs.SE",
        "cs.CL",
        "cs.PL"
      ],
      "pdf_url": "https://arxiv.org/pdf/2307.07940v2",
      "landing_url": "https://arxiv.org/abs/2307.07940v2",
      "doi": "https://doi.org/10.1145/3634814.3634828"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The article discusses deduplication and ranking of programming solution programs, with no focus on discrete audio/speech tokenization or tokenization analysis.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The article discusses deduplication and ranking of programming solution programs, with no focus on discrete audio/speech tokenization or tokenization analysis.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper focuses on deduplicating and ranking solution programs in programming education and does not address discrete audio/speech tokenization or any related audio signal processing tasks as required by the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper focuses on deduplicating and ranking solution programs in programming education and does not address discrete audio/speech tokenization or any related audio signal processing tasks as required by the inclusion criteria.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "The History of Moral Certainty as the Pre-History of Typicality",
    "abstract": "This paper investigates the historical origin and ancestors of typicality, which is now a central concept in Boltzmannian Statistical Mechanics and Bohmian Mechanics. Although Ludwig Boltzmann did not use the word typicality, its main idea, namely, that something happens almost always or is valid for almost all cases, plays a crucial role for his explanation of how thermodynamic systems approach equilibrium. At the beginning of the 20th century, the focus on almost always or almost everywhere was fruitful for developing measure theory and probability theory. It was apparently Hugh Everett III who first mentioned typicality in physics in 1957 while searching for a justification of the Born rule in his interpretation of quantum mechanics. The historically closest concept before these developments is moral certainty, which was invented by the medieval French theologian Jean Gerson, and it became a standard concept at least until the Age of Enlightenment, when Jakob Bernoulli proved the Law of Large numbers.",
    "metadata": {
      "arxiv_id": "2307.09899",
      "title": "The History of Moral Certainty as the Pre-History of Typicality",
      "summary": "This paper investigates the historical origin and ancestors of typicality, which is now a central concept in Boltzmannian Statistical Mechanics and Bohmian Mechanics. Although Ludwig Boltzmann did not use the word typicality, its main idea, namely, that something happens almost always or is valid for almost all cases, plays a crucial role for his explanation of how thermodynamic systems approach equilibrium. At the beginning of the 20th century, the focus on almost always or almost everywhere was fruitful for developing measure theory and probability theory. It was apparently Hugh Everett III who first mentioned typicality in physics in 1957 while searching for a justification of the Born rule in his interpretation of quantum mechanics. The historically closest concept before these developments is moral certainty, which was invented by the medieval French theologian Jean Gerson, and it became a standard concept at least until the Age of Enlightenment, when Jakob Bernoulli proved the Law of Large numbers.",
      "authors": [
        "Mario Hubert"
      ],
      "published": "2023-07-19T10:58:01Z",
      "updated": "2023-07-19T10:58:01Z",
      "categories": [
        "physics.hist-ph"
      ],
      "pdf_url": "https://arxiv.org/pdf/2307.09899v1",
      "landing_url": "https://arxiv.org/abs/2307.09899v1",
      "doi": "https://doi.org/10.48550/arXiv.2307.09899"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item is a historical/philosophical treatise on typicality in physics and has no focus on discrete audio/speech tokenization, so it fails the inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item is a historical/philosophical treatise on typicality in physics and has no focus on discrete audio/speech tokenization, so it fails the inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper focuses on the historical and philosophical origins of the concept of typicality in statistical and quantum mechanics and does not propose, analyze, or evaluate any discrete audio/speech tokenization methods or related representations; it does not meet any of the inclusion criteria related to audio tokenization and is not relevant to the evaluation tasks specified.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper focuses on the historical and philosophical origins of the concept of typicality in statistical and quantum mechanics and does not propose, analyze, or evaluate any discrete audio/speech tokenization methods or related representations; it does not meet any of the inclusion criteria related to audio tokenization and is not relevant to the evaluation tasks specified.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Artificial Intelligence-Generated Terahertz Multi-Resonant Metasurfaces via Improved Transformer and CGAN Neural Networks",
    "abstract": "It is well known that the inverse design of terahertz (THz) multi-resonant graphene metasurfaces by using traditional deep neural networks (DNNs) has limited generalization ability. In this paper, we propose improved Transformer and conditional generative adversarial neural networks (CGAN) for the inverse design of graphene metasurfaces based upon THz multi-resonant absorption spectra. The improved Transformer can obtain higher accuracy and generalization performance in the StoV (Spectrum to Vector) design compared to traditional multilayer perceptron (MLP) neural networks, while the StoI (Spectrum to Image) design achieved through CGAN can provide more comprehensive information and higher accuracy than the StoV design obtained by MLP. Moreover, the improved CGAN can achieve the inverse design of graphene metasurface images directly from the desired multi-resonant absorption spectra. It is turned out that this work can finish facilitating the design process of artificial intelligence-generated metasurfaces (AIGM), and even provide a useful guide for developing complex THz metasurfaces based on 2D materials using generative neural networks.",
    "metadata": {
      "arxiv_id": "2307.11794",
      "title": "Artificial Intelligence-Generated Terahertz Multi-Resonant Metasurfaces via Improved Transformer and CGAN Neural Networks",
      "summary": "It is well known that the inverse design of terahertz (THz) multi-resonant graphene metasurfaces by using traditional deep neural networks (DNNs) has limited generalization ability. In this paper, we propose improved Transformer and conditional generative adversarial neural networks (CGAN) for the inverse design of graphene metasurfaces based upon THz multi-resonant absorption spectra. The improved Transformer can obtain higher accuracy and generalization performance in the StoV (Spectrum to Vector) design compared to traditional multilayer perceptron (MLP) neural networks, while the StoI (Spectrum to Image) design achieved through CGAN can provide more comprehensive information and higher accuracy than the StoV design obtained by MLP. Moreover, the improved CGAN can achieve the inverse design of graphene metasurface images directly from the desired multi-resonant absorption spectra. It is turned out that this work can finish facilitating the design process of artificial intelligence-generated metasurfaces (AIGM), and even provide a useful guide for developing complex THz metasurfaces based on 2D materials using generative neural networks.",
      "authors": [
        "Yangpeng Huang",
        "Naixing Feng",
        "Yijun Cai"
      ],
      "published": "2023-07-21T02:49:03Z",
      "updated": "2023-07-21T02:49:03Z",
      "categories": [
        "physics.optics",
        "cs.LG",
        "physics.app-ph"
      ],
      "pdf_url": "https://arxiv.org/pdf/2307.11794v1",
      "landing_url": "https://arxiv.org/abs/2307.11794v1",
      "doi": "https://doi.org/10.48550/arXiv.2307.11794"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The article discusses AI-driven design of terahertz metasurfaces using Transformer and CGAN networks, with no discrete audio/speech tokenization or analysis of discrete tokens; thus it does not meet the audio-tokenization inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The article discusses AI-driven design of terahertz metasurfaces using Transformer and CGAN networks, with no discrete audio/speech tokenization or analysis of discrete tokens; thus it does not meet the audio-tokenization inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on the inverse design of terahertz multi-resonant graphene metasurfaces using improved Transformer and CGAN neural networks, which relates to electromagnetic material design rather than any discrete audio or speech tokenization techniques; it does not address the tokenization of continuous audio signals into discrete tokens, nor does it engage with audio reconstruction or generation tasks involving discrete token vocabularies. Therefore, it does not meet the inclusion criteria focusing on discrete audio/speech tokenization.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on the inverse design of terahertz multi-resonant graphene metasurfaces using improved Transformer and CGAN neural networks, which relates to electromagnetic material design rather than any discrete audio or speech tokenization techniques; it does not address the tokenization of continuous audio signals into discrete tokens, nor does it engage with audio reconstruction or generation tasks involving discrete token vocabularies. Therefore, it does not meet the inclusion criteria focusing on discrete audio/speech tokenization.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Blockchain-based Cloud Data Deduplication Scheme with Fair Incentives",
    "abstract": "With the rapid development of cloud computing, vast amounts of duplicated data are being uploaded to the cloud, wasting storage resources. Deduplication (dedup) is an efficient solution to save storage costs of cloud storage providers (CSPs) by storing only one copy of the uploaded data. However, cloud users do not benefit directly from dedup and may be reluctant to dedup their data. To motivate the cloud users towards dedup, CSPs offer incentives on storage fees. The problems with the existing dedup schemes are that they do not consider: (1) correctness - the incentive offered to a cloud user should be computed correctly without any prejudice. (2) fairness - the cloud user receives the file link and access rights of the uploaded data if and only if the CSP receives the storage fee. Meeting these requirements without a trusted party is non-trivial, and most of the existing dedup schemes do not apply. Another drawback is that most of the existing schemes emphasize incentives to cloud users but failed to provide a reliable incentive mechanism. As public Blockchain networks emulate the properties of trusted parties, in this paper, we propose a new Blockchain-based dedup scheme to meet the above requirements. In our scheme, a smart contract computes the incentives on storage fee, and the fairness rules are encoded into the smart contract for facilitating fair payments between the CSPs and cloud users. We prove the correctness and fairness of the proposed scheme. We also design a new incentive mechanism and show that the scheme is individually rational and incentive compatible. Furthermore, we conduct experiments by implementing the designed smart contract on Ethereum local Blockchain network and list the transactional and financial costs of interacting with the designed smart contract.",
    "metadata": {
      "arxiv_id": "2307.12052",
      "title": "Blockchain-based Cloud Data Deduplication Scheme with Fair Incentives",
      "summary": "With the rapid development of cloud computing, vast amounts of duplicated data are being uploaded to the cloud, wasting storage resources. Deduplication (dedup) is an efficient solution to save storage costs of cloud storage providers (CSPs) by storing only one copy of the uploaded data. However, cloud users do not benefit directly from dedup and may be reluctant to dedup their data. To motivate the cloud users towards dedup, CSPs offer incentives on storage fees. The problems with the existing dedup schemes are that they do not consider: (1) correctness - the incentive offered to a cloud user should be computed correctly without any prejudice. (2) fairness - the cloud user receives the file link and access rights of the uploaded data if and only if the CSP receives the storage fee. Meeting these requirements without a trusted party is non-trivial, and most of the existing dedup schemes do not apply. Another drawback is that most of the existing schemes emphasize incentives to cloud users but failed to provide a reliable incentive mechanism.\n  As public Blockchain networks emulate the properties of trusted parties, in this paper, we propose a new Blockchain-based dedup scheme to meet the above requirements. In our scheme, a smart contract computes the incentives on storage fee, and the fairness rules are encoded into the smart contract for facilitating fair payments between the CSPs and cloud users. We prove the correctness and fairness of the proposed scheme. We also design a new incentive mechanism and show that the scheme is individually rational and incentive compatible. Furthermore, we conduct experiments by implementing the designed smart contract on Ethereum local Blockchain network and list the transactional and financial costs of interacting with the designed smart contract.",
      "authors": [
        "Mallikarjun Reddy Dorsala",
        "V. N. Sastry",
        "Sudhakar Chapram"
      ],
      "published": "2023-07-22T11:27:05Z",
      "updated": "2023-07-22T11:27:05Z",
      "categories": [
        "cs.CR"
      ],
      "pdf_url": "https://arxiv.org/pdf/2307.12052v1",
      "landing_url": "https://arxiv.org/abs/2307.12052v1",
      "doi": "https://doi.org/10.48550/arXiv.2307.12052"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The article discusses blockchain-based cloud data deduplication and incentive mechanisms, with no focus on discrete audio/speech tokenization; thus it does not meet the inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The article discusses blockchain-based cloud data deduplication and incentive mechanisms, with no focus on discrete audio/speech tokenization; thus it does not meet the inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on blockchain-based data deduplication and incentive mechanisms for cloud storage, with no mention or analysis of discrete audio/speech tokenization, quantization, or representation, which is the core inclusion criteria; thus, it does not meet the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on blockchain-based data deduplication and incentive mechanisms for cloud storage, with no mention or analysis of discrete audio/speech tokenization, quantization, or representation, which is the core inclusion criteria; thus, it does not meet the inclusion criteria.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Self-Supervised Learning for Audio-Based Emotion Recognition",
    "abstract": "Emotion recognition models using audio input data can enable the development of interactive systems with applications in mental healthcare, marketing, gaming, and social media analysis. While the field of affective computing using audio data is rich, a major barrier to achieve consistently high-performance models is the paucity of available training labels. Self-supervised learning (SSL) is a family of methods which can learn despite a scarcity of supervised labels by predicting properties of the data itself. To understand the utility of self-supervised learning for audio-based emotion recognition, we have applied self-supervised learning pre-training to the classification of emotions from the CMU- MOSEI's acoustic modality. Unlike prior papers that have experimented with raw acoustic data, our technique has been applied to encoded acoustic data. Our model is first pretrained to uncover the randomly-masked timestamps of the acoustic data. The pre-trained model is then fine-tuned using a small sample of annotated data. The performance of the final model is then evaluated via several evaluation metrics against a baseline deep learning model with an identical backbone architecture. We find that self-supervised learning consistently improves the performance of the model across all metrics. This work shows the utility of self-supervised learning for affective computing, demonstrating that self-supervised learning is most useful when the number of training examples is small, and that the effect is most pronounced for emotions which are easier to classify such as happy, sad and anger. This work further demonstrates that self-supervised learning works when applied to embedded feature representations rather than the traditional approach of pre-training on the raw input space.",
    "metadata": {
      "arxiv_id": "2307.12343",
      "title": "Self-Supervised Learning for Audio-Based Emotion Recognition",
      "summary": "Emotion recognition models using audio input data can enable the development of interactive systems with applications in mental healthcare, marketing, gaming, and social media analysis. While the field of affective computing using audio data is rich, a major barrier to achieve consistently high-performance models is the paucity of available training labels. Self-supervised learning (SSL) is a family of methods which can learn despite a scarcity of supervised labels by predicting properties of the data itself. To understand the utility of self-supervised learning for audio-based emotion recognition, we have applied self-supervised learning pre-training to the classification of emotions from the CMU- MOSEI's acoustic modality. Unlike prior papers that have experimented with raw acoustic data, our technique has been applied to encoded acoustic data. Our model is first pretrained to uncover the randomly-masked timestamps of the acoustic data. The pre-trained model is then fine-tuned using a small sample of annotated data. The performance of the final model is then evaluated via several evaluation metrics against a baseline deep learning model with an identical backbone architecture. We find that self-supervised learning consistently improves the performance of the model across all metrics. This work shows the utility of self-supervised learning for affective computing, demonstrating that self-supervised learning is most useful when the number of training examples is small, and that the effect is most pronounced for emotions which are easier to classify such as happy, sad and anger. This work further demonstrates that self-supervised learning works when applied to embedded feature representations rather than the traditional approach of pre-training on the raw input space.",
      "authors": [
        "Peranut Nimitsurachat",
        "Peter Washington"
      ],
      "published": "2023-07-23T14:40:50Z",
      "updated": "2023-07-23T14:40:50Z",
      "categories": [
        "cs.SD",
        "cs.LG",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2307.12343v1",
      "landing_url": "https://arxiv.org/abs/2307.12343v1",
      "doi": "https://doi.org/10.48550/arXiv.2307.12343"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The abstract discusses self-supervised pre-training on encoded acoustic data but does not involve discrete audio tokenization or a finite vocabulary of tokens that would be used for audio reconstruction/understanding.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The abstract discusses self-supervised pre-training on encoded acoustic data but does not involve discrete audio tokenization or a finite vocabulary of tokens that would be used for audio reconstruction/understanding.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on self-supervised learning for audio-based emotion recognition using encoded acoustic data and does not explicitly propose or analyze discrete audio/speech tokenization methods converting continuous audio into discrete symbols; it emphasizes masked timestamp prediction on encoded features rather than a discrete tokenization scheme with a finite vocabulary as required by the inclusion criteria, thus failing to meet the core inclusion requirement.",
      "evaluation": 2
    },
    "round-A_JuniorMini_reasoning": "The study focuses on self-supervised learning for audio-based emotion recognition using encoded acoustic data and does not explicitly propose or analyze discrete audio/speech tokenization methods converting continuous audio into discrete symbols; it emphasizes masked timestamp prediction on encoded features rather than a discrete tokenization scheme with a finite vocabulary as required by the inclusion criteria, thus failing to meet the core inclusion requirement.",
    "round-A_JuniorMini_evaluation": 2,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:2)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:2)"
  },
  {
    "title": "A Pre-trained Data Deduplication Model based on Active Learning",
    "abstract": "In the era of big data, the issue of data quality has become increasingly prominent. One of the main challenges is the problem of duplicate data, which can arise from repeated entry or the merging of multiple data sources. These \"dirty data\" problems can significantly limit the effective application of big data. To address the issue of data deduplication, we propose a pre-trained deduplication model based on active learning, which is the first work that utilizes active learning to address the problem of deduplication at the semantic level. The model is built on a pre-trained Transformer and fine-tuned to solve the deduplication problem as a sequence to classification task, which firstly integrate the transformer with active learning into an end-to-end architecture to select the most valuable data for deduplication model training, and also firstly employ the R-Drop method to perform data augmentation on each round of labeled data, which can reduce the cost of manual labeling and improve the model's performance. Experimental results demonstrate that our proposed model outperforms previous state-of-the-art (SOTA) for deduplicated data identification, achieving up to a 28% improvement in Recall score on benchmark datasets.",
    "metadata": {
      "arxiv_id": "2308.00721",
      "title": "A Pre-trained Data Deduplication Model based on Active Learning",
      "summary": "In the era of big data, the issue of data quality has become increasingly prominent. One of the main challenges is the problem of duplicate data, which can arise from repeated entry or the merging of multiple data sources. These \"dirty data\" problems can significantly limit the effective application of big data. To address the issue of data deduplication, we propose a pre-trained deduplication model based on active learning, which is the first work that utilizes active learning to address the problem of deduplication at the semantic level. The model is built on a pre-trained Transformer and fine-tuned to solve the deduplication problem as a sequence to classification task, which firstly integrate the transformer with active learning into an end-to-end architecture to select the most valuable data for deduplication model training, and also firstly employ the R-Drop method to perform data augmentation on each round of labeled data, which can reduce the cost of manual labeling and improve the model's performance. Experimental results demonstrate that our proposed model outperforms previous state-of-the-art (SOTA) for deduplicated data identification, achieving up to a 28% improvement in Recall score on benchmark datasets.",
      "authors": [
        "Haochen Shi",
        "Xinyao Liu",
        "Fengmao Lv",
        "Hongtao Xue",
        "Jie Hu",
        "Shengdong Du",
        "Tianrui Li"
      ],
      "published": "2023-07-31T03:56:46Z",
      "updated": "2025-01-10T09:35:20Z",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2308.00721v4",
      "landing_url": "https://arxiv.org/abs/2308.00721v4",
      "doi": "https://doi.org/10.48550/arXiv.2308.00721"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The article discusses a pre-trained deduplication model using active learning and transformers; it does not address discrete audio/speech tokenization, thus it fails the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The article discusses a pre-trained deduplication model using active learning and transformers; it does not address discrete audio/speech tokenization, thus it fails the inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on data deduplication of data entries using a Transformer-based active learning model and does not address discrete audio/speech tokenization, nor the conversion of continuous audio signals to discrete tokens for audio tasks as specified in the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on data deduplication of data entries using a Transformer-based active learning model and does not address discrete audio/speech tokenization, nor the conversion of continuous audio signals to discrete tokens for audio tasks as specified in the inclusion criteria.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Latent-Shift: Gradient of Entropy Helps Neural Codecs",
    "abstract": "End-to-end image/video codecs are getting competitive compared to traditional compression techniques that have been developed through decades of manual engineering efforts. These trainable codecs have many advantages over traditional techniques such as easy adaptation on perceptual distortion metrics and high performance on specific domains thanks to their learning ability. However, state of the art neural codecs does not take advantage of the existence of gradient of entropy in decoding device. In this paper, we theoretically show that gradient of entropy (available at decoder side) is correlated with the gradient of the reconstruction error (which is not available at decoder side). We then demonstrate experimentally that this gradient can be used on various compression methods, leading to a $1-2\\%$ rate savings for the same quality. Our method is orthogonal to other improvements and brings independent rate savings.",
    "metadata": {
      "arxiv_id": "2308.00725",
      "title": "Latent-Shift: Gradient of Entropy Helps Neural Codecs",
      "summary": "End-to-end image/video codecs are getting competitive compared to traditional compression techniques that have been developed through decades of manual engineering efforts. These trainable codecs have many advantages over traditional techniques such as easy adaptation on perceptual distortion metrics and high performance on specific domains thanks to their learning ability. However, state of the art neural codecs does not take advantage of the existence of gradient of entropy in decoding device. In this paper, we theoretically show that gradient of entropy (available at decoder side) is correlated with the gradient of the reconstruction error (which is not available at decoder side). We then demonstrate experimentally that this gradient can be used on various compression methods, leading to a $1-2\\%$ rate savings for the same quality. Our method is orthogonal to other improvements and brings independent rate savings.",
      "authors": [
        "Muhammet Balcilar",
        "Bharath Bhushan Damodaran",
        "Karam Naser",
        "Franck Galpin",
        "Pierre Hellier"
      ],
      "published": "2023-08-01T15:12:36Z",
      "updated": "2023-08-01T15:12:36Z",
      "categories": [
        "eess.IV",
        "cs.CV",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2308.00725v1",
      "landing_url": "https://arxiv.org/abs/2308.00725v1",
      "doi": "https://doi.org/10.48550/arXiv.2308.00725"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The abstract focuses on neural image/video codecs and the gradient of entropy, not on discrete audio/speech tokenization or discrete token representations for speech; thus it does not meet the inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The abstract focuses on neural image/video codecs and the gradient of entropy, not on discrete audio/speech tokenization or discrete token representations for speech; thus it does not meet the inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on neural image/video codecs and discusses gradient of entropy used for compression rate savings, without any mention or analysis of discrete audio or speech tokenization, which is the core inclusion criterion.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on neural image/video codecs and discusses gradient of entropy used for compression rate savings, without any mention or analysis of discrete audio or speech tokenization, which is the core inclusion criterion.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "A Probabilistic Approach to Self-Supervised Learning using Cyclical Stochastic Gradient MCMC",
    "abstract": "In this paper we present a practical Bayesian self-supervised learning method with Cyclical Stochastic Gradient Hamiltonian Monte Carlo (cSGHMC). Within this framework, we place a prior over the parameters of a self-supervised learning model and use cSGHMC to approximate the high dimensional and multimodal posterior distribution over the embeddings. By exploring an expressive posterior over the embeddings, Bayesian self-supervised learning produces interpretable and diverse representations. Marginalizing over these representations yields a significant gain in performance, calibration and out-of-distribution detection on a variety of downstream classification tasks. We provide experimental results on multiple classification tasks on four challenging datasets. Moreover, we demonstrate the effectiveness of the proposed method in out-of-distribution detection using the SVHN and CIFAR-10 datasets.",
    "metadata": {
      "arxiv_id": "2308.01271",
      "title": "A Probabilistic Approach to Self-Supervised Learning using Cyclical Stochastic Gradient MCMC",
      "summary": "In this paper we present a practical Bayesian self-supervised learning method with Cyclical Stochastic Gradient Hamiltonian Monte Carlo (cSGHMC). Within this framework, we place a prior over the parameters of a self-supervised learning model and use cSGHMC to approximate the high dimensional and multimodal posterior distribution over the embeddings. By exploring an expressive posterior over the embeddings, Bayesian self-supervised learning produces interpretable and diverse representations. Marginalizing over these representations yields a significant gain in performance, calibration and out-of-distribution detection on a variety of downstream classification tasks. We provide experimental results on multiple classification tasks on four challenging datasets. Moreover, we demonstrate the effectiveness of the proposed method in out-of-distribution detection using the SVHN and CIFAR-10 datasets.",
      "authors": [
        "Masoumeh Javanbakhat",
        "Christoph Lippert"
      ],
      "published": "2023-08-02T16:52:56Z",
      "updated": "2023-08-02T16:52:56Z",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2308.01271v1",
      "landing_url": "https://arxiv.org/abs/2308.01271v1",
      "doi": "https://doi.org/10.48550/arXiv.2308.01271"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The work presents a probabilistic Bayesian self-supervised learning method for embeddings with cyclical SGHMC; it does not address discrete audio/speech tokenization or token-based representations for speech.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The work presents a probabilistic Bayesian self-supervised learning method for embeddings with cyclical SGHMC; it does not address discrete audio/speech tokenization or token-based representations for speech.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on a Bayesian self-supervised learning method with cyclical stochastic gradient MCMC to improve representations for classification and out-of-distribution detection; however, it does not explicitly discuss or analyze discrete audio/speech tokenization or the conversion of continuous audio signals into discrete tokens, nor does it focus on discrete token design or quantization relevant to language-model-style token sequences as required by the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on a Bayesian self-supervised learning method with cyclical stochastic gradient MCMC to improve representations for classification and out-of-distribution detection; however, it does not explicitly discuss or analyze discrete audio/speech tokenization or the conversion of continuous audio signals into discrete tokens, nor does it focus on discrete token design or quantization relevant to language-model-style token sequences as required by the inclusion criteria.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Improving Deep Attractor Network by BGRU and GMM for Speech Separation",
    "abstract": "Deep Attractor Network (DANet) is the state-of-the-art technique in speech separation field, which uses Bidirectional Long Short-Term Memory (BLSTM), but the complexity of the DANet model is very high. In this paper, a simplified and powerful DANet model is proposed using Bidirectional Gated neural network (BGRU) instead of BLSTM. The Gaussian Mixture Model (GMM) other than the k-means was applied in DANet as a clustering algorithm to reduce the complexity and increase the learning speed and accuracy. The metrics used in this paper are Signal to Distortion Ratio (SDR), Signal to Interference Ratio (SIR), Signal to Artifact Ratio (SAR), and Perceptual Evaluation Speech Quality (PESQ) score. Two speaker mixture datasets from TIMIT corpus were prepared to evaluate the proposed model, and the system achieved 12.3 dB and 2.94 for SDR and PESQ scores respectively, which were better than the original DANet model. Other improvements were 20.7% and 17.9% in the number of parameters and time training, respectively. The model was applied on mixed Arabic speech signals and the results were better than that in English.",
    "metadata": {
      "arxiv_id": "2308.03332",
      "title": "Improving Deep Attractor Network by BGRU and GMM for Speech Separation",
      "summary": "Deep Attractor Network (DANet) is the state-of-the-art technique in speech separation field, which uses Bidirectional Long Short-Term Memory (BLSTM), but the complexity of the DANet model is very high. In this paper, a simplified and powerful DANet model is proposed using Bidirectional Gated neural network (BGRU) instead of BLSTM. The Gaussian Mixture Model (GMM) other than the k-means was applied in DANet as a clustering algorithm to reduce the complexity and increase the learning speed and accuracy. The metrics used in this paper are Signal to Distortion Ratio (SDR), Signal to Interference Ratio (SIR), Signal to Artifact Ratio (SAR), and Perceptual Evaluation Speech Quality (PESQ) score. Two speaker mixture datasets from TIMIT corpus were prepared to evaluate the proposed model, and the system achieved 12.3 dB and 2.94 for SDR and PESQ scores respectively, which were better than the original DANet model. Other improvements were 20.7% and 17.9% in the number of parameters and time training, respectively. The model was applied on mixed Arabic speech signals and the results were better than that in English.",
      "authors": [
        "Rawad Melhem",
        "Assef Jafar",
        "Riad Hamadeh"
      ],
      "published": "2023-08-07T06:26:53Z",
      "updated": "2023-08-07T06:26:53Z",
      "categories": [
        "cs.SD",
        "cs.AI",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2308.03332v1",
      "landing_url": "https://arxiv.org/abs/2308.03332v1",
      "doi": "https://doi.org/10.11916/j.issn.1005-9113.2019044"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The study targets improving speech separation models (DANet) with BGRU and GMM and discusses continuous speech processing, with no discrete audio tokenization or token-based representations addressed, thus failing the inclusion criterion requiring discrete tokenization analysis.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The study targets improving speech separation models (DANet) with BGRU and GMM and discusses continuous speech processing, with no discrete audio tokenization or token-based representations addressed, thus failing the inclusion criterion requiring discrete tokenization analysis.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on improving Deep Attractor Network models for speech separation using BGRU and GMM clustering methods, but does not explicitly address discrete audio/speech tokenization or conversion of continuous audio signals into sequences of discrete tokens, which is a core aspect of the inclusion criteria; instead, it primarily works with continuous features and clustering methods without analysis or proposal of discrete token representations.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on improving Deep Attractor Network models for speech separation using BGRU and GMM clustering methods, but does not explicitly address discrete audio/speech tokenization or conversion of continuous audio signals into sequences of discrete tokens, which is a core aspect of the inclusion criteria; instead, it primarily works with continuous features and clustering methods without analysis or proposal of discrete token representations.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "SpeechX: Neural Codec Language Model as a Versatile Speech Transformer",
    "abstract": "Recent advancements in generative speech models based on audio-text prompts have enabled remarkable innovations like high-quality zero-shot text-to-speech. However, existing models still face limitations in handling diverse audio-text speech generation tasks involving transforming input speech and processing audio captured in adverse acoustic conditions. This paper introduces SpeechX, a versatile speech generation model capable of zero-shot TTS and various speech transformation tasks, dealing with both clean and noisy signals. SpeechX combines neural codec language modeling with multi-task learning using task-dependent prompting, enabling unified and extensible modeling and providing a consistent way for leveraging textual input in speech enhancement and transformation tasks. Experimental results show SpeechX's efficacy in various tasks, including zero-shot TTS, noise suppression, target speaker extraction, speech removal, and speech editing with or without background noise, achieving comparable or superior performance to specialized models across tasks. See https://aka.ms/speechx for demo samples.",
    "metadata": {
      "arxiv_id": "2308.06873",
      "title": "SpeechX: Neural Codec Language Model as a Versatile Speech Transformer",
      "summary": "Recent advancements in generative speech models based on audio-text prompts have enabled remarkable innovations like high-quality zero-shot text-to-speech. However, existing models still face limitations in handling diverse audio-text speech generation tasks involving transforming input speech and processing audio captured in adverse acoustic conditions. This paper introduces SpeechX, a versatile speech generation model capable of zero-shot TTS and various speech transformation tasks, dealing with both clean and noisy signals. SpeechX combines neural codec language modeling with multi-task learning using task-dependent prompting, enabling unified and extensible modeling and providing a consistent way for leveraging textual input in speech enhancement and transformation tasks. Experimental results show SpeechX's efficacy in various tasks, including zero-shot TTS, noise suppression, target speaker extraction, speech removal, and speech editing with or without background noise, achieving comparable or superior performance to specialized models across tasks. See https://aka.ms/speechx for demo samples.",
      "authors": [
        "Xiaofei Wang",
        "Manthan Thakker",
        "Zhuo Chen",
        "Naoyuki Kanda",
        "Sefik Emre Eskimez",
        "Sanyuan Chen",
        "Min Tang",
        "Shujie Liu",
        "Jinyu Li",
        "Takuya Yoshioka"
      ],
      "published": "2023-08-14T01:01:19Z",
      "updated": "2024-06-25T18:38:28Z",
      "categories": [
        "eess.AS",
        "cs.CL",
        "cs.LG",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2308.06873v2",
      "landing_url": "https://arxiv.org/abs/2308.06873v2",
      "doi": "https://doi.org/10.48550/arXiv.2308.06873"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The abstract mentions a neural codec language model for speech transformation but does not explicitly frame discrete audio tokenization into a finite vocabulary as the core object of study, thus failing the explicit tokenization criterion.",
      "evaluation": 2
    },
    "round-A_JuniorNano_reasoning": "The abstract mentions a neural codec language model for speech transformation but does not explicitly frame discrete audio tokenization into a finite vocabulary as the core object of study, thus failing the explicit tokenization criterion.",
    "round-A_JuniorNano_evaluation": 2,
    "round-A_JuniorMini_output": {
      "reasoning": "The abstract describes a model named SpeechX that integrates neural codec language modeling, which implies the use of discrete audio tokenization derived from neural encoding techniques for audio generation and transformation tasks. This aligns well with the inclusion criterion focusing on discrete audio tokenization as a core object of study, and it explicitly addresses multiple speech tasks involving this approach. There is no indication that the model uses only continuous features or traditional codecs, and the work presents experimental validation. Thus, it meets the inclusion criteria and avoids the exclusion criteria.",
      "evaluation": 5
    },
    "round-A_JuniorMini_reasoning": "The abstract describes a model named SpeechX that integrates neural codec language modeling, which implies the use of discrete audio tokenization derived from neural encoding techniques for audio generation and transformation tasks. This aligns well with the inclusion criterion focusing on discrete audio tokenization as a core object of study, and it explicitly addresses multiple speech tasks involving this approach. There is no indication that the model uses only continuous features or traditional codecs, and the work presents experimental validation. Thus, it meets the inclusion criteria and avoids the exclusion criteria.",
    "round-A_JuniorMini_evaluation": 5,
    "round-B_SeniorLead_output": {
      "reasoning": "Junior reviewers disagreed (one said the abstract lacked explicit mention of discrete audio tokens; the other noted 'neural codec language modeling' implies token sequences). The abstract explicitly centers a 'neural codec language model' and reports multi-task experimental validation (TTS, enhancement, editing), which strongly indicates discrete neural codec tokenization is a core object of study rather than only continuous features or a traditional codec — therefore it meets the inclusion criteria and none of the exclusions.",
      "evaluation": 5
    },
    "round-B_SeniorLead_reasoning": "Junior reviewers disagreed (one said the abstract lacked explicit mention of discrete audio tokens; the other noted 'neural codec language modeling' implies token sequences). The abstract explicitly centers a 'neural codec language model' and reports multi-task experimental validation (TTS, enhancement, editing), which strongly indicates discrete neural codec tokenization is a core object of study rather than only continuous features or a traditional codec — therefore it meets the inclusion criteria and none of the exclusions.",
    "round-B_SeniorLead_evaluation": 5,
    "final_verdict": "include (senior:5)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "TokenSplit: Using Discrete Speech Representations for Direct, Refined, and Transcript-Conditioned Speech Separation and Recognition",
    "abstract": "We present TokenSplit, a speech separation model that acts on discrete token sequences. The model is trained on multiple tasks simultaneously: separate and transcribe each speech source, and generate speech from text. The model operates on transcripts and audio token sequences and achieves multiple tasks through masking of inputs. The model is a sequence-to-sequence encoder-decoder model that uses the Transformer architecture. We also present a \"refinement\" version of the model that predicts enhanced audio tokens from the audio tokens of speech separated by a conventional separation model. Using both objective metrics and subjective MUSHRA listening tests, we show that our model achieves excellent performance in terms of separation, both with or without transcript conditioning. We also measure the automatic speech recognition (ASR) performance and provide audio samples of speech synthesis to demonstrate the additional utility of our model.",
    "metadata": {
      "arxiv_id": "2308.10415",
      "title": "TokenSplit: Using Discrete Speech Representations for Direct, Refined, and Transcript-Conditioned Speech Separation and Recognition",
      "summary": "We present TokenSplit, a speech separation model that acts on discrete token sequences. The model is trained on multiple tasks simultaneously: separate and transcribe each speech source, and generate speech from text. The model operates on transcripts and audio token sequences and achieves multiple tasks through masking of inputs. The model is a sequence-to-sequence encoder-decoder model that uses the Transformer architecture. We also present a \"refinement\" version of the model that predicts enhanced audio tokens from the audio tokens of speech separated by a conventional separation model. Using both objective metrics and subjective MUSHRA listening tests, we show that our model achieves excellent performance in terms of separation, both with or without transcript conditioning. We also measure the automatic speech recognition (ASR) performance and provide audio samples of speech synthesis to demonstrate the additional utility of our model.",
      "authors": [
        "Hakan Erdogan",
        "Scott Wisdom",
        "Xuankai Chang",
        "Zalán Borsos",
        "Marco Tagliasacchi",
        "Neil Zeghidour",
        "John R. Hershey"
      ],
      "published": "2023-08-21T01:52:01Z",
      "updated": "2023-08-21T01:52:01Z",
      "categories": [
        "cs.SD",
        "cs.LG",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2308.10415v1",
      "landing_url": "https://arxiv.org/abs/2308.10415v1",
      "doi": "https://doi.org/10.48550/arXiv.2308.10415"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The work explicitly uses discrete speech representations and token sequences for separation, transcription, and synthesis, satisfying the core inclusion criterion and not aligning with the exclusion categories.",
      "evaluation": 5
    },
    "round-A_JuniorNano_reasoning": "The work explicitly uses discrete speech representations and token sequences for separation, transcription, and synthesis, satisfying the core inclusion criterion and not aligning with the exclusion categories.",
    "round-A_JuniorNano_evaluation": 5,
    "round-A_JuniorMini_output": {
      "reasoning": "The study TokenSplit explicitly deals with discrete speech token sequences for speech separation, transcription, and synthesis tasks, indicating a focus on discrete tokenization of audio signals rather than continuous features. It uses Transformer-based models and analyzes multiple tasks involving discrete audio tokens, aligning well with the inclusion criteria and avoiding exclusion criteria related to continuous feature use or engineering-only reports. Therefore, the study meets all inclusion requirements and none of the exclusions.",
      "evaluation": 5
    },
    "round-A_JuniorMini_reasoning": "The study TokenSplit explicitly deals with discrete speech token sequences for speech separation, transcription, and synthesis tasks, indicating a focus on discrete tokenization of audio signals rather than continuous features. It uses Transformer-based models and analyzes multiple tasks involving discrete audio tokens, aligning well with the inclusion criteria and avoiding exclusion criteria related to continuous feature use or engineering-only reports. Therefore, the study meets all inclusion requirements and none of the exclusions.",
    "round-A_JuniorMini_evaluation": 5,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "include (junior:5)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Unleash Model Potential: Bootstrapped Meta Self-supervised Learning",
    "abstract": "The long-term goal of machine learning is to learn general visual representations from a small amount of data without supervision, mimicking three advantages of human cognition: i) no need for labels, ii) robustness to data scarcity, and iii) learning from experience. Self-supervised learning and meta-learning are two promising techniques to achieve this goal, but they both only partially capture the advantages and fail to address all the problems. Self-supervised learning struggles to overcome the drawbacks of data scarcity, while ignoring prior knowledge that can facilitate learning and generalization. Meta-learning relies on supervised information and suffers from a bottleneck of insufficient learning. To address these issues, we propose a novel Bootstrapped Meta Self-Supervised Learning (BMSSL) framework that aims to simulate the human learning process. We first analyze the close relationship between meta-learning and self-supervised learning. Based on this insight, we reconstruct tasks to leverage the strengths of both paradigms, achieving advantages i and ii. Moreover, we employ a bi-level optimization framework that alternates between solving specific tasks with a learned ability (first level) and improving this ability (second level), attaining advantage iii. To fully harness its power, we introduce a bootstrapped target based on meta-gradient to make the model its own teacher. We validate the effectiveness of our approach with comprehensive theoretical and empirical study.",
    "metadata": {
      "arxiv_id": "2308.14267",
      "title": "Unleash Model Potential: Bootstrapped Meta Self-supervised Learning",
      "summary": "The long-term goal of machine learning is to learn general visual representations from a small amount of data without supervision, mimicking three advantages of human cognition: i) no need for labels, ii) robustness to data scarcity, and iii) learning from experience. Self-supervised learning and meta-learning are two promising techniques to achieve this goal, but they both only partially capture the advantages and fail to address all the problems. Self-supervised learning struggles to overcome the drawbacks of data scarcity, while ignoring prior knowledge that can facilitate learning and generalization. Meta-learning relies on supervised information and suffers from a bottleneck of insufficient learning. To address these issues, we propose a novel Bootstrapped Meta Self-Supervised Learning (BMSSL) framework that aims to simulate the human learning process. We first analyze the close relationship between meta-learning and self-supervised learning. Based on this insight, we reconstruct tasks to leverage the strengths of both paradigms, achieving advantages i and ii. Moreover, we employ a bi-level optimization framework that alternates between solving specific tasks with a learned ability (first level) and improving this ability (second level), attaining advantage iii. To fully harness its power, we introduce a bootstrapped target based on meta-gradient to make the model its own teacher. We validate the effectiveness of our approach with comprehensive theoretical and empirical study.",
      "authors": [
        "Jingyao Wang",
        "Zeen Song",
        "Wenwen Qiang",
        "Changwen Zheng"
      ],
      "published": "2023-08-28T02:49:07Z",
      "updated": "2023-08-28T02:49:07Z",
      "categories": [
        "cs.LG",
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2308.14267v1",
      "landing_url": "https://arxiv.org/abs/2308.14267v1",
      "doi": "https://doi.org/10.48550/arXiv.2308.14267"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The study focuses on bootstrapped meta self-supervised learning for general visual representations and does not propose or analyze discrete audio/speech tokenization or discrete token representations for speech tasks.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The study focuses on bootstrapped meta self-supervised learning for general visual representations and does not propose or analyze discrete audio/speech tokenization or discrete token representations for speech tasks.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper proposes a novel Bootstrapped Meta Self-Supervised Learning framework for general visual representation learning without supervision, focusing on meta-learning and self-supervised learning paradigms. However, the abstract does not mention any discrete audio/speech tokenization, nor does it discuss conversion of continuous audio signals into discrete symbols. It centers around visual data and learning robustness, which does not meet the inclusion criteria of discrete audio/speech tokenization studies. Therefore, it does not fulfill the required criteria for inclusion.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper proposes a novel Bootstrapped Meta Self-Supervised Learning framework for general visual representation learning without supervision, focusing on meta-learning and self-supervised learning paradigms. However, the abstract does not mention any discrete audio/speech tokenization, nor does it discuss conversion of continuous audio signals into discrete symbols. It centers around visual data and learning robustness, which does not meet the inclusion criteria of discrete audio/speech tokenization studies. Therefore, it does not fulfill the required criteria for inclusion.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "SpeechTokenizer: Unified Speech Tokenizer for Speech Large Language Models",
    "abstract": "Current speech large language models build upon discrete speech representations, which can be categorized into semantic tokens and acoustic tokens. However, existing speech tokens are not specifically designed for speech language modeling. To assess the suitability of speech tokens for building speech language models, we established the first benchmark, SLMTokBench. Our results indicate that neither semantic nor acoustic tokens are ideal for this purpose. Therefore, we propose SpeechTokenizer, a unified speech tokenizer for speech large language models. SpeechTokenizer adopts the Encoder-Decoder architecture with residual vector quantization (RVQ). Unifying semantic and acoustic tokens, SpeechTokenizer disentangles different aspects of speech information hierarchically across different RVQ layers. Furthermore, We construct a Unified Speech Language Model (USLM) leveraging SpeechTokenizer. Experiments show that SpeechTokenizer performs comparably to EnCodec in speech reconstruction and demonstrates strong performance on the SLMTokBench benchmark. Also, USLM outperforms VALL-E in zero-shot Text-to-Speech tasks. Code and models are available at https://github.com/ZhangXInFD/SpeechTokenizer/.",
    "metadata": {
      "arxiv_id": "2308.16692",
      "title": "SpeechTokenizer: Unified Speech Tokenizer for Speech Large Language Models",
      "summary": "Current speech large language models build upon discrete speech representations, which can be categorized into semantic tokens and acoustic tokens. However, existing speech tokens are not specifically designed for speech language modeling. To assess the suitability of speech tokens for building speech language models, we established the first benchmark, SLMTokBench. Our results indicate that neither semantic nor acoustic tokens are ideal for this purpose. Therefore, we propose SpeechTokenizer, a unified speech tokenizer for speech large language models. SpeechTokenizer adopts the Encoder-Decoder architecture with residual vector quantization (RVQ). Unifying semantic and acoustic tokens, SpeechTokenizer disentangles different aspects of speech information hierarchically across different RVQ layers. Furthermore, We construct a Unified Speech Language Model (USLM) leveraging SpeechTokenizer. Experiments show that SpeechTokenizer performs comparably to EnCodec in speech reconstruction and demonstrates strong performance on the SLMTokBench benchmark. Also, USLM outperforms VALL-E in zero-shot Text-to-Speech tasks. Code and models are available at https://github.com/ZhangXInFD/SpeechTokenizer/.",
      "authors": [
        "Xin Zhang",
        "Dong Zhang",
        "Shimin Li",
        "Yaqian Zhou",
        "Xipeng Qiu"
      ],
      "published": "2023-08-31T12:53:09Z",
      "updated": "2024-01-23T01:56:57Z",
      "categories": [
        "cs.CL",
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2308.16692v2",
      "landing_url": "https://arxiv.org/abs/2308.16692v2",
      "doi": "https://doi.org/10.48550/arXiv.2308.16692"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item explicitly proposes a unified discrete speech tokenizer (SpeechTokenizer) using residual vector quantization to produce discrete tokens for speech, and evaluates it for speech reconstruction and speech-language modeling tasks, meeting the inclusion criteria and not falling under any exclusion criteria.",
      "evaluation": 5
    },
    "round-A_JuniorNano_reasoning": "The item explicitly proposes a unified discrete speech tokenizer (SpeechTokenizer) using residual vector quantization to produce discrete tokens for speech, and evaluates it for speech reconstruction and speech-language modeling tasks, meeting the inclusion criteria and not falling under any exclusion criteria.",
    "round-A_JuniorNano_evaluation": 5,
    "round-A_JuniorMini_output": {
      "reasoning": "The study explicitly proposes and evaluates a unified discrete speech tokenizer (SpeechTokenizer) designed for speech large language models, including benchmarks and comparisons, thus meeting all inclusion criteria and avoiding exclusion criteria related to continuous features or engineering-only contributions.",
      "evaluation": 5
    },
    "round-A_JuniorMini_reasoning": "The study explicitly proposes and evaluates a unified discrete speech tokenizer (SpeechTokenizer) designed for speech large language models, including benchmarks and comparisons, thus meeting all inclusion criteria and avoiding exclusion criteria related to continuous features or engineering-only contributions.",
    "round-A_JuniorMini_evaluation": 5,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "include (junior:5)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "RepCodec: A Speech Representation Codec for Speech Tokenization",
    "abstract": "With recent rapid growth of large language models (LLMs), discrete speech tokenization has played an important role for injecting speech into LLMs. However, this discretization gives rise to a loss of information, consequently impairing overall performance. To improve the performance of these discrete speech tokens, we present RepCodec, a novel speech representation codec for semantic speech tokenization. In contrast to audio codecs which reconstruct the raw audio, RepCodec learns a vector quantization codebook through reconstructing speech representations from speech encoders like HuBERT or data2vec. Together, the speech encoder, the codec encoder and the vector quantization codebook form a pipeline for converting speech waveforms into semantic tokens. The extensive experiments illustrate that RepCodec, by virtue of its enhanced information retention capacity, significantly outperforms the widely used k-means clustering approach in both speech understanding and generation. Furthermore, this superiority extends across various speech encoders and languages, affirming the robustness of RepCodec. We believe our method can facilitate large language modeling research on speech processing.",
    "metadata": {
      "arxiv_id": "2309.00169",
      "title": "RepCodec: A Speech Representation Codec for Speech Tokenization",
      "summary": "With recent rapid growth of large language models (LLMs), discrete speech tokenization has played an important role for injecting speech into LLMs. However, this discretization gives rise to a loss of information, consequently impairing overall performance. To improve the performance of these discrete speech tokens, we present RepCodec, a novel speech representation codec for semantic speech tokenization. In contrast to audio codecs which reconstruct the raw audio, RepCodec learns a vector quantization codebook through reconstructing speech representations from speech encoders like HuBERT or data2vec. Together, the speech encoder, the codec encoder and the vector quantization codebook form a pipeline for converting speech waveforms into semantic tokens. The extensive experiments illustrate that RepCodec, by virtue of its enhanced information retention capacity, significantly outperforms the widely used k-means clustering approach in both speech understanding and generation. Furthermore, this superiority extends across various speech encoders and languages, affirming the robustness of RepCodec. We believe our method can facilitate large language modeling research on speech processing.",
      "authors": [
        "Zhichao Huang",
        "Chutong Meng",
        "Tom Ko"
      ],
      "published": "2023-08-31T23:26:10Z",
      "updated": "2024-07-22T09:53:44Z",
      "categories": [
        "eess.AS",
        "cs.LG",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2309.00169v3",
      "landing_url": "https://arxiv.org/abs/2309.00169v3",
      "doi": "https://doi.org/10.48550/arXiv.2309.00169"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This work explicitly develops and evaluates a discrete speech tokenization method (semantic tokens via a vector-quantized codec) and demonstrates improvements in speech understanding/generation, aligning with the inclusion criteria and not fitting any exclusion criteria.",
      "evaluation": 5
    },
    "round-A_JuniorNano_reasoning": "This work explicitly develops and evaluates a discrete speech tokenization method (semantic tokens via a vector-quantized codec) and demonstrates improvements in speech understanding/generation, aligning with the inclusion criteria and not fitting any exclusion criteria.",
    "round-A_JuniorNano_evaluation": 5,
    "round-A_JuniorMini_output": {
      "reasoning": "The study proposes RepCodec, a novel method for semantic speech tokenization converting continuous speech into discrete tokens via vector quantization, directly addressing discrete audio tokenization for speech understanding and generation; it includes methodological contribution, systematic analysis, and validation across multiple languages, meeting all inclusion criteria and none of the exclusion criteria.",
      "evaluation": 5
    },
    "round-A_JuniorMini_reasoning": "The study proposes RepCodec, a novel method for semantic speech tokenization converting continuous speech into discrete tokens via vector quantization, directly addressing discrete audio tokenization for speech understanding and generation; it includes methodological contribution, systematic analysis, and validation across multiple languages, meeting all inclusion criteria and none of the exclusion criteria.",
    "round-A_JuniorMini_evaluation": 5,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "include (junior:5)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Employing Real Training Data for Deep Noise Suppression",
    "abstract": "Most deep noise suppression (DNS) models are trained with reference-based losses requiring access to clean speech. However, sometimes an additive microphone model is insufficient for real-world applications. Accordingly, ways to use real training data in supervised learning for DNS models promise to reduce a potential training/inference mismatch. Employing real data for DNS training requires either generative approaches or a reference-free loss without access to the corresponding clean speech. In this work, we propose to employ an end-to-end non-intrusive deep neural network (DNN), named PESQ-DNN, to estimate perceptual evaluation of speech quality (PESQ) scores of enhanced real data. It provides a reference-free perceptual loss for employing real data during DNS training, maximizing the PESQ scores. Furthermore, we use an epoch-wise alternating training protocol, updating the DNS model on real data, followed by PESQ-DNN updating on synthetic data. The DNS model trained with the PESQ-DNN employing real data outperforms all reference methods employing only synthetic training data. On synthetic test data, our proposed method excels the Interspeech 2021 DNS Challenge baseline by a significant 0.32 PESQ points. Both on synthetic and real test data, the proposed method beats the baseline by 0.05 DNSMOS points - although PESQ-DNN optimizes for a different perceptual metric.",
    "metadata": {
      "arxiv_id": "2309.02432",
      "title": "Employing Real Training Data for Deep Noise Suppression",
      "summary": "Most deep noise suppression (DNS) models are trained with reference-based losses requiring access to clean speech. However, sometimes an additive microphone model is insufficient for real-world applications. Accordingly, ways to use real training data in supervised learning for DNS models promise to reduce a potential training/inference mismatch. Employing real data for DNS training requires either generative approaches or a reference-free loss without access to the corresponding clean speech. In this work, we propose to employ an end-to-end non-intrusive deep neural network (DNN), named PESQ-DNN, to estimate perceptual evaluation of speech quality (PESQ) scores of enhanced real data. It provides a reference-free perceptual loss for employing real data during DNS training, maximizing the PESQ scores. Furthermore, we use an epoch-wise alternating training protocol, updating the DNS model on real data, followed by PESQ-DNN updating on synthetic data. The DNS model trained with the PESQ-DNN employing real data outperforms all reference methods employing only synthetic training data. On synthetic test data, our proposed method excels the Interspeech 2021 DNS Challenge baseline by a significant 0.32 PESQ points. Both on synthetic and real test data, the proposed method beats the baseline by 0.05 DNSMOS points - although PESQ-DNN optimizes for a different perceptual metric.",
      "authors": [
        "Ziyi Xu",
        "Marvin Sach",
        "Jan Pirklbauer",
        "Tim Fingscheidt"
      ],
      "published": "2023-09-05T17:58:58Z",
      "updated": "2023-09-05T17:58:58Z",
      "categories": [
        "eess.AS",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2309.02432v1",
      "landing_url": "https://arxiv.org/abs/2309.02432v1",
      "doi": "https://doi.org/10.48550/arXiv.2309.02432"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This work centers on using real data and a reference-free perceptual loss (PESQ-DNN) for training deep noise suppression and does not study discrete audio tokenization or token-based representations, so it does not meet the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "This work centers on using real data and a reference-free perceptual loss (PESQ-DNN) for training deep noise suppression and does not study discrete audio tokenization or token-based representations, so it does not meet the inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on deep noise suppression using real training data and non-intrusive perceptual loss functions, but it does not mention any discrete audio/speech tokenization or conversion of continuous audio into discrete tokens; hence, it does not meet the inclusion criteria related to discrete tokenization and instead relies on continuous audio features and neural network training for enhancement.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on deep noise suppression using real training data and non-intrusive perceptual loss functions, but it does not mention any discrete audio/speech tokenization or conversion of continuous audio into discrete tokens; hence, it does not meet the inclusion criteria related to discrete tokenization and instead relies on continuous audio features and neural network training for enhancement.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Differentiable Modelling of Percussive Audio with Transient and Spectral Synthesis",
    "abstract": "Differentiable digital signal processing (DDSP) techniques, including methods for audio synthesis, have gained attention in recent years and lend themselves to interpretability in the parameter space. However, current differentiable synthesis methods have not explicitly sought to model the transient portion of signals, which is important for percussive sounds. In this work, we present a unified synthesis framework aiming to address transient generation and percussive synthesis within a DDSP framework. To this end, we propose a model for percussive synthesis that builds on sinusoidal modeling synthesis and incorporates a modulated temporal convolutional network for transient generation. We use a modified sinusoidal peak picking algorithm to generate time-varying non-harmonic sinusoids and pair it with differentiable noise and transient encoders that are jointly trained to reconstruct drumset sounds. We compute a set of reconstruction metrics using a large dataset of acoustic and electronic percussion samples that show that our method leads to improved onset signal reconstruction for membranophone percussion instruments.",
    "metadata": {
      "arxiv_id": "2309.06649",
      "title": "Differentiable Modelling of Percussive Audio with Transient and Spectral Synthesis",
      "summary": "Differentiable digital signal processing (DDSP) techniques, including methods for audio synthesis, have gained attention in recent years and lend themselves to interpretability in the parameter space. However, current differentiable synthesis methods have not explicitly sought to model the transient portion of signals, which is important for percussive sounds. In this work, we present a unified synthesis framework aiming to address transient generation and percussive synthesis within a DDSP framework. To this end, we propose a model for percussive synthesis that builds on sinusoidal modeling synthesis and incorporates a modulated temporal convolutional network for transient generation. We use a modified sinusoidal peak picking algorithm to generate time-varying non-harmonic sinusoids and pair it with differentiable noise and transient encoders that are jointly trained to reconstruct drumset sounds. We compute a set of reconstruction metrics using a large dataset of acoustic and electronic percussion samples that show that our method leads to improved onset signal reconstruction for membranophone percussion instruments.",
      "authors": [
        "Jordie Shier",
        "Franco Caspe",
        "Andrew Robertson",
        "Mark Sandler",
        "Charalampos Saitis",
        "Andrew McPherson"
      ],
      "published": "2023-09-13T00:21:04Z",
      "updated": "2023-09-13T00:21:04Z",
      "categories": [
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2309.06649v1",
      "landing_url": "https://arxiv.org/abs/2309.06649v1",
      "doi": "https://doi.org/10.48550/arXiv.2309.06649"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The abstract covers differentiable percussion synthesis and transient/spatial modeling but does not mention discrete audio tokenization or token-based representations; it centers on continuous synthesis parameters rather than quantized/discrete tokens, so it does not meet the inclusion criteria for tokenization-based studies and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The abstract covers differentiable percussion synthesis and transient/spatial modeling but does not mention discrete audio tokenization or token-based representations; it centers on continuous synthesis parameters rather than quantized/discrete tokens, so it does not meet the inclusion criteria for tokenization-based studies and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on differentiable synthesis of percussive audio signals using transient and spectral synthesis methods, but it does not explicitly propose or analyze discrete audio/speech tokenization nor convert continuous audio signals into discrete sequences for reconstruction or understanding; furthermore, it centers on music/percussive sounds without evaluating speech-relevant implications, which falls outside the inclusion criteria and within the exclusion criteria related to non-speech audio tokenization without speech applicability.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on differentiable synthesis of percussive audio signals using transient and spectral synthesis methods, but it does not explicitly propose or analyze discrete audio/speech tokenization nor convert continuous audio signals into discrete sequences for reconstruction or understanding; furthermore, it centers on music/percussive sounds without evaluating speech-relevant implications, which falls outside the inclusion criteria and within the exclusion criteria related to non-speech audio tokenization without speech applicability.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "BANC: Towards Efficient Binaural Audio Neural Codec for Overlapping Speech",
    "abstract": "We introduce BANC, a neural binaural audio codec designed for efficient speech compression in single and two-speaker scenarios while preserving the spatial location information of each speaker. Our key contributions are as follows: 1) The ability of our proposed model to compress and decode overlapping speech. 2) A novel architecture that compresses speech content and spatial cues separately, ensuring the preservation of each speaker's spatial context after decoding. 3) BANC's proficiency in reducing the bandwidth required for compressing binaural speech by 48% compared to compressing individual binaural channels. In our evaluation, we employed speech enhancement, room acoustics, and perceptual metrics to assess the accuracy of BANC's clean speech and spatial cue estimates.",
    "metadata": {
      "arxiv_id": "2309.07416",
      "title": "BANC: Towards Efficient Binaural Audio Neural Codec for Overlapping Speech",
      "summary": "We introduce BANC, a neural binaural audio codec designed for efficient speech compression in single and two-speaker scenarios while preserving the spatial location information of each speaker. Our key contributions are as follows: 1) The ability of our proposed model to compress and decode overlapping speech. 2) A novel architecture that compresses speech content and spatial cues separately, ensuring the preservation of each speaker's spatial context after decoding. 3) BANC's proficiency in reducing the bandwidth required for compressing binaural speech by 48% compared to compressing individual binaural channels. In our evaluation, we employed speech enhancement, room acoustics, and perceptual metrics to assess the accuracy of BANC's clean speech and spatial cue estimates.",
      "authors": [
        "Anton Ratnarajah",
        "Shi-Xiong Zhang",
        "Dong Yu"
      ],
      "published": "2023-09-14T04:04:50Z",
      "updated": "2024-11-25T03:50:32Z",
      "categories": [
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2309.07416v4",
      "landing_url": "https://arxiv.org/abs/2309.07416v4",
      "doi": "https://doi.org/10.48550/arXiv.2309.07416"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "Does not explicitly propose/analyze discrete audio/tokenization as a core object; focuses on neural binaural speech compression/codec rather than discrete token quantization or token-based representations.",
      "evaluation": 2
    },
    "round-A_JuniorNano_reasoning": "Does not explicitly propose/analyze discrete audio/tokenization as a core object; focuses on neural binaural speech compression/codec rather than discrete token quantization or token-based representations.",
    "round-A_JuniorNano_evaluation": 2,
    "round-A_JuniorMini_output": {
      "reasoning": "The study introduces BANC, a neural binaural audio codec focused on speech compression and spatial cue preservation, but the abstract does not explicitly mention any discrete audio tokenization or quantization methods used as core objects of study, nor does it analyze or evaluate discrete token sequences; therefore, it is not clear if it meets the inclusion criteria.",
      "evaluation": 2
    },
    "round-A_JuniorMini_reasoning": "The study introduces BANC, a neural binaural audio codec focused on speech compression and spatial cue preservation, but the abstract does not explicitly mention any discrete audio tokenization or quantization methods used as core objects of study, nor does it analyze or evaluate discrete token sequences; therefore, it is not clear if it meets the inclusion criteria.",
    "round-A_JuniorMini_evaluation": 2,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:2)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:2)"
  },
  {
    "title": "Voxtlm: unified decoder-only models for consolidating speech recognition/synthesis and speech/text continuation tasks",
    "abstract": "We propose a decoder-only language model, VoxtLM, that can perform four tasks: speech recognition, speech synthesis, text generation, and speech continuation. VoxtLM integrates text vocabulary with discrete speech tokens from self-supervised speech features and uses special tokens to enable multitask learning. Compared to a single-task model, VoxtLM exhibits a significant improvement in speech synthesis, with improvements in both speech intelligibility from 28.9 to 5.6 and objective quality from 2.68 to 3.90. VoxtLM also improves speech generation and speech recognition performance over the single-task counterpart. Further, VoxtLM is trained with publicly available data and training recipes and model checkpoints are open-sourced to make fully reproducible work.",
    "metadata": {
      "arxiv_id": "2309.07937",
      "title": "Voxtlm: unified decoder-only models for consolidating speech recognition/synthesis and speech/text continuation tasks",
      "summary": "We propose a decoder-only language model, VoxtLM, that can perform four tasks: speech recognition, speech synthesis, text generation, and speech continuation. VoxtLM integrates text vocabulary with discrete speech tokens from self-supervised speech features and uses special tokens to enable multitask learning. Compared to a single-task model, VoxtLM exhibits a significant improvement in speech synthesis, with improvements in both speech intelligibility from 28.9 to 5.6 and objective quality from 2.68 to 3.90. VoxtLM also improves speech generation and speech recognition performance over the single-task counterpart. Further, VoxtLM is trained with publicly available data and training recipes and model checkpoints are open-sourced to make fully reproducible work.",
      "authors": [
        "Soumi Maiti",
        "Yifan Peng",
        "Shukjae Choi",
        "Jee-weon Jung",
        "Xuankai Chang",
        "Shinji Watanabe"
      ],
      "published": "2023-09-14T03:13:18Z",
      "updated": "2024-01-24T15:36:31Z",
      "categories": [
        "eess.AS",
        "cs.LG",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2309.07937v3",
      "landing_url": "https://arxiv.org/abs/2309.07937v3",
      "doi": "https://doi.org/10.48550/arXiv.2309.07937"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The abstract explicitly uses discrete speech tokens derived from self-supervised features and integrates them with text tokens for a multitask decoder model, indicating a core focus on discrete audio tokenization suitable for speech reconstruction/generation, and it reports evaluation on speech-related tasks without relying solely on continuous features.",
      "evaluation": 5
    },
    "round-A_JuniorNano_reasoning": "The abstract explicitly uses discrete speech tokens derived from self-supervised features and integrates them with text tokens for a multitask decoder model, indicating a core focus on discrete audio tokenization suitable for speech reconstruction/generation, and it reports evaluation on speech-related tasks without relying solely on continuous features.",
    "round-A_JuniorNano_evaluation": 5,
    "round-A_JuniorMini_output": {
      "reasoning": "The study proposes VoxtLM, a decoder-only model that integrates discrete speech tokens derived from self-supervised speech features with text vocabulary, which indicates the use and analysis of discrete audio tokenization as a core part of the model; it includes experimental validation showing improvements in speech tasks. This aligns with the inclusion criteria and avoids the exclusion criteria, as it focuses on discrete tokenization beyond traditional codecs and includes methodological contribution with reproducible experiments.",
      "evaluation": 5
    },
    "round-A_JuniorMini_reasoning": "The study proposes VoxtLM, a decoder-only model that integrates discrete speech tokens derived from self-supervised speech features with text vocabulary, which indicates the use and analysis of discrete audio tokenization as a core part of the model; it includes experimental validation showing improvements in speech tasks. This aligns with the inclusion criteria and avoids the exclusion criteria, as it focuses on discrete tokenization beyond traditional codecs and includes methodological contribution with reproducible experiments.",
    "round-A_JuniorMini_evaluation": 5,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "include (junior:5)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Refining DNN-based Mask Estimation using CGMM-based EM Algorithm for Multi-channel Noise Reduction",
    "abstract": "In this paper, we present a method that allows to further improve speech enhancement obtained with recently introduced Deep Neural Network (DNN) models. We propose a multi-channel refinement method of time-frequency masks obtained with single-channel DNNs, which consists of an iterative Complex Gaussian Mixture Model (CGMM) based algorithm, followed by optimum spatial filtration. We validate our approach on time-frequency masks estimated with three recent deep learning models, namely DCUnet, DCCRN, and FullSubNet. We show that our method with the proposed mask refinement procedure allows to improve the accuracy of estimated masks, in terms of the Area Under the ROC Curve (AUC) measure, and as a consequence the overall speech quality of the enhanced speech signal, as measured by PESQ improvement, and that the improvement is consistent across all three DNN models.",
    "metadata": {
      "arxiv_id": "2309.09630",
      "title": "Refining DNN-based Mask Estimation using CGMM-based EM Algorithm for Multi-channel Noise Reduction",
      "summary": "In this paper, we present a method that allows to further improve speech enhancement obtained with recently introduced Deep Neural Network (DNN) models. We propose a multi-channel refinement method of time-frequency masks obtained with single-channel DNNs, which consists of an iterative Complex Gaussian Mixture Model (CGMM) based algorithm, followed by optimum spatial filtration. We validate our approach on time-frequency masks estimated with three recent deep learning models, namely DCUnet, DCCRN, and FullSubNet. We show that our method with the proposed mask refinement procedure allows to improve the accuracy of estimated masks, in terms of the Area Under the ROC Curve (AUC) measure, and as a consequence the overall speech quality of the enhanced speech signal, as measured by PESQ improvement, and that the improvement is consistent across all three DNN models.",
      "authors": [
        "Julitta Bartolewska",
        "Stanisław Kacprzak",
        "Konrad Kowalczyk"
      ],
      "published": "2023-09-18T10:05:41Z",
      "updated": "2023-09-18T10:05:41Z",
      "categories": [
        "eess.AS",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2309.09630v1",
      "landing_url": "https://arxiv.org/abs/2309.09630v1",
      "doi": "https://doi.org/10.21437/Interspeech.2022-10632"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The paper focuses on refining time-frequency masks for speech enhancement using a CGMM-based EM algorithm and does not address discrete audio tokenization or token-based representations, so it does not meet the inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The paper focuses on refining time-frequency masks for speech enhancement using a CGMM-based EM algorithm and does not address discrete audio tokenization or token-based representations, so it does not meet the inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on multi-channel noise reduction through refinement of time-frequency masks estimated by DNNs but does not address discrete audio/speech tokenization or the conversion of continuous audio signals into sequences of discrete symbols; it instead deals with continuous mask estimation and enhancement, so it does not meet the inclusion criteria related to discrete tokenization nor discusses quantization or token design relevant to language-model-style token sequences.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on multi-channel noise reduction through refinement of time-frequency masks estimated by DNNs but does not address discrete audio/speech tokenization or the conversion of continuous audio signals into sequences of discrete symbols; it instead deals with continuous mask estimation and enhancement, so it does not meet the inclusion criteria related to discrete tokenization nor discusses quantization or token design relevant to language-model-style token sequences.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Distilling HuBERT with LSTMs via Decoupled Knowledge Distillation",
    "abstract": "Much research effort is being applied to the task of compressing the knowledge of self-supervised models, which are powerful, yet large and memory consuming. In this work, we show that the original method of knowledge distillation (and its more recently proposed extension, decoupled knowledge distillation) can be applied to the task of distilling HuBERT. In contrast to methods that focus on distilling internal features, this allows for more freedom in the network architecture of the compressed model. We thus propose to distill HuBERT's Transformer layers into an LSTM-based distilled model that reduces the number of parameters even below DistilHuBERT and at the same time shows improved performance in automatic speech recognition.",
    "metadata": {
      "arxiv_id": "2309.09920",
      "title": "Distilling HuBERT with LSTMs via Decoupled Knowledge Distillation",
      "summary": "Much research effort is being applied to the task of compressing the knowledge of self-supervised models, which are powerful, yet large and memory consuming. In this work, we show that the original method of knowledge distillation (and its more recently proposed extension, decoupled knowledge distillation) can be applied to the task of distilling HuBERT. In contrast to methods that focus on distilling internal features, this allows for more freedom in the network architecture of the compressed model. We thus propose to distill HuBERT's Transformer layers into an LSTM-based distilled model that reduces the number of parameters even below DistilHuBERT and at the same time shows improved performance in automatic speech recognition.",
      "authors": [
        "Danilo de Oliveira",
        "Timo Gerkmann"
      ],
      "published": "2023-09-18T16:34:40Z",
      "updated": "2023-09-18T16:34:40Z",
      "categories": [
        "eess.AS",
        "cs.LG",
        "cs.SD",
        "eess.SP"
      ],
      "pdf_url": "https://arxiv.org/pdf/2309.09920v1",
      "landing_url": "https://arxiv.org/abs/2309.09920v1",
      "doi": "https://doi.org/10.48550/arXiv.2309.09920"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The work centers on distilling a self-supervised speech model (HuBERT) into an LSTM-based distilled model using knowledge distillation, focusing on model compression rather than proposing or evaluating discrete audio tokenization into a finite vocabulary; thus it does not meet the inclusion criteria for discrete audio/speech tokenization and should be excluded under the stated criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The work centers on distilling a self-supervised speech model (HuBERT) into an LSTM-based distilled model using knowledge distillation, focusing on model compression rather than proposing or evaluating discrete audio tokenization into a finite vocabulary; thus it does not meet the inclusion criteria for discrete audio/speech tokenization and should be excluded under the stated criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on distillation of HuBERT models into an LSTM-based architecture to improve performance in automatic speech recognition, but it does not explicitly discuss or propose discrete audio/speech tokenization methods or analyze discrete token representations central to tokenization as per the inclusion criteria.",
      "evaluation": 2
    },
    "round-A_JuniorMini_reasoning": "The study focuses on distillation of HuBERT models into an LSTM-based architecture to improve performance in automatic speech recognition, but it does not explicitly discuss or propose discrete audio/speech tokenization methods or analyze discrete token representations central to tokenization as per the inclusion criteria.",
    "round-A_JuniorMini_evaluation": 2,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:2)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:2)"
  },
  {
    "title": "PDPCRN: Parallel Dual-Path CRN with Bi-directional Inter-Branch Interactions for Multi-Channel Speech Enhancement",
    "abstract": "Multi-channel speech enhancement seeks to utilize spatial information to distinguish target speech from interfering signals. While deep learning approaches like the dual-path convolutional recurrent network (DPCRN) have made strides, challenges persist in effectively modeling inter-channel correlations and amalgamating multi-level information. In response, we introduce the Parallel Dual-Path Convolutional Recurrent Network (PDPCRN). This acoustic modeling architecture has two key innovations. First, a parallel design with separate branches extracts complementary features. Second, bi-directional modules enable cross-branch communication. Together, these facilitate diverse representation fusion and enhanced modeling. Experimental validation on TIMIT datasets underscores the prowess of PDPCRN. Notably, against baseline models like the standard DPCRN, PDPCRN not only outperforms in PESQ and STOI metrics but also boasts a leaner computational footprint with reduced parameters.",
    "metadata": {
      "arxiv_id": "2309.10379",
      "title": "PDPCRN: Parallel Dual-Path CRN with Bi-directional Inter-Branch Interactions for Multi-Channel Speech Enhancement",
      "summary": "Multi-channel speech enhancement seeks to utilize spatial information to distinguish target speech from interfering signals. While deep learning approaches like the dual-path convolutional recurrent network (DPCRN) have made strides, challenges persist in effectively modeling inter-channel correlations and amalgamating multi-level information. In response, we introduce the Parallel Dual-Path Convolutional Recurrent Network (PDPCRN). This acoustic modeling architecture has two key innovations. First, a parallel design with separate branches extracts complementary features. Second, bi-directional modules enable cross-branch communication. Together, these facilitate diverse representation fusion and enhanced modeling. Experimental validation on TIMIT datasets underscores the prowess of PDPCRN. Notably, against baseline models like the standard DPCRN, PDPCRN not only outperforms in PESQ and STOI metrics but also boasts a leaner computational footprint with reduced parameters.",
      "authors": [
        "Jiahui Pan",
        "Shulin He",
        "Tianci Wu",
        "Hui Zhang",
        "Xueliang Zhang"
      ],
      "published": "2023-09-19T07:27:38Z",
      "updated": "2023-09-19T07:27:38Z",
      "categories": [
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2309.10379v1",
      "landing_url": "https://arxiv.org/abs/2309.10379v1",
      "doi": "https://doi.org/10.48550/arXiv.2309.10379"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The work focuses on a neural network architecture for multi-channel speech enhancement using continuous representations; there is no discrete audio tokenization or token-based analysis, so it does not meet the inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The work focuses on a neural network architecture for multi-channel speech enhancement using continuous representations; there is no discrete audio tokenization or token-based analysis, so it does not meet the inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on multi-channel speech enhancement using a novel neural network architecture (PDPCRN) to improve representation fusion and speech modeling, but there is no indication that it involves discrete audio/speech tokenization or quantization as required by the inclusion criteria. Instead, it appears to work with continuous features for enhancement without discrete token design or analysis.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on multi-channel speech enhancement using a novel neural network architecture (PDPCRN) to improve representation fusion and speech modeling, but there is no indication that it involves discrete audio/speech tokenization or quantization as required by the inclusion criteria. Instead, it appears to work with continuous features for enhancement without discrete token design or analysis.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "SlimPajama-DC: Understanding Data Combinations for LLM Training",
    "abstract": "This paper aims to understand the impacts of various data combinations (e.g., web text, Wikipedia, GitHub, books) on the pretraining of large language models using SlimPajama. SlimPajama is a rigorously deduplicated, multi-source dataset, which has been refined and further deduplicated to 627B tokens from the extensive 1.2T token RedPajama dataset contributed by Together. We have termed our research as SlimPajama-DC, an empirical analysis designed to uncover fundamental characteristics and best practices associated with employing SlimPajama in the training of large language models. During our research with SlimPajama, two pivotal observations emerged: (1) Global deduplication vs. local deduplication. We analyze and discuss how global (across different sources of datasets) and local (within the single source of dataset) deduplications affect the performance of trained models. (2) Proportions of highly-deduplicated multi-source datasets in the combination. To study this, we construct six configurations on SlimPajama dataset and train individual ones using 1.3B Cerebras-GPT model with Alibi and SwiGLU. Our best configuration outperforms the 1.3B model trained on RedPajama using the same number of training tokens by a significant margin. All our 1.3B models are trained on Cerebras 16$\\times$ CS-2 cluster with a total of 80 PFLOP/s in bf16 mixed precision. We further extend our discoveries (such as increasing data diversity is crucial after global deduplication) on a 7B model with large batch-size training. Our SlimPajama-DC models are available at: https://huggingface.co/MBZUAI-LLM/SlimPajama-DC and the separate SlimPajama-DC datasets are available at: https://huggingface.co/datasets/MBZUAI-LLM/SlimPajama-627B-DC.",
    "metadata": {
      "arxiv_id": "2309.10818",
      "title": "SlimPajama-DC: Understanding Data Combinations for LLM Training",
      "summary": "This paper aims to understand the impacts of various data combinations (e.g., web text, Wikipedia, GitHub, books) on the pretraining of large language models using SlimPajama. SlimPajama is a rigorously deduplicated, multi-source dataset, which has been refined and further deduplicated to 627B tokens from the extensive 1.2T token RedPajama dataset contributed by Together. We have termed our research as SlimPajama-DC, an empirical analysis designed to uncover fundamental characteristics and best practices associated with employing SlimPajama in the training of large language models. During our research with SlimPajama, two pivotal observations emerged: (1) Global deduplication vs. local deduplication. We analyze and discuss how global (across different sources of datasets) and local (within the single source of dataset) deduplications affect the performance of trained models. (2) Proportions of highly-deduplicated multi-source datasets in the combination. To study this, we construct six configurations on SlimPajama dataset and train individual ones using 1.3B Cerebras-GPT model with Alibi and SwiGLU. Our best configuration outperforms the 1.3B model trained on RedPajama using the same number of training tokens by a significant margin. All our 1.3B models are trained on Cerebras 16$\\times$ CS-2 cluster with a total of 80 PFLOP/s in bf16 mixed precision. We further extend our discoveries (such as increasing data diversity is crucial after global deduplication) on a 7B model with large batch-size training. Our SlimPajama-DC models are available at: https://huggingface.co/MBZUAI-LLM/SlimPajama-DC and the separate SlimPajama-DC datasets are available at: https://huggingface.co/datasets/MBZUAI-LLM/SlimPajama-627B-DC.",
      "authors": [
        "Zhiqiang Shen",
        "Tianhua Tao",
        "Liqun Ma",
        "Willie Neiswanger",
        "Zhengzhong Liu",
        "Hongyi Wang",
        "Bowen Tan",
        "Joel Hestness",
        "Natalia Vassilieva",
        "Daria Soboleva",
        "Eric Xing"
      ],
      "published": "2023-09-19T17:59:54Z",
      "updated": "2024-05-09T13:56:06Z",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2309.10818v3",
      "landing_url": "https://arxiv.org/abs/2309.10818v3",
      "doi": "https://doi.org/10.48550/arXiv.2309.10818"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The abstract concerns data sources, deduplication, and training configurations for large language models with SlimPajama-DC; there is no discussion of discrete audio/speech tokenization or token quantization relevant to audio, so it does not meet the audio tokenization inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The abstract concerns data sources, deduplication, and training configurations for large language models with SlimPajama-DC; there is no discussion of discrete audio/speech tokenization or token quantization relevant to audio, so it does not meet the audio tokenization inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper focuses on understanding data combinations and deduplication strategies for training large language models using the SlimPajama dataset, which is a text-based dataset; it does not propose or analyze discrete audio/speech tokenization or any related audio representation or tokenization methods, nor does it deal with audio signals or their discrete conversion.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper focuses on understanding data combinations and deduplication strategies for training large language models using the SlimPajama dataset, which is a text-based dataset; it does not propose or analyze discrete audio/speech tokenization or any related audio representation or tokenization methods, nor does it deal with audio signals or their discrete conversion.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Improving Language Model-Based Zero-Shot Text-to-Speech Synthesis with Multi-Scale Acoustic Prompts",
    "abstract": "Zero-shot text-to-speech (TTS) synthesis aims to clone any unseen speaker's voice without adaptation parameters. By quantizing speech waveform into discrete acoustic tokens and modeling these tokens with the language model, recent language model-based TTS models show zero-shot speaker adaptation capabilities with only a 3-second acoustic prompt of an unseen speaker. However, they are limited by the length of the acoustic prompt, which makes it difficult to clone personal speaking style. In this paper, we propose a novel zero-shot TTS model with the multi-scale acoustic prompts based on a neural codec language model VALL-E. A speaker-aware text encoder is proposed to learn the personal speaking style at the phoneme-level from the style prompt consisting of multiple sentences. Following that, a VALL-E based acoustic decoder is utilized to model the timbre from the timbre prompt at the frame-level and generate speech. The experimental results show that our proposed method outperforms baselines in terms of naturalness and speaker similarity, and can achieve better performance by scaling out to a longer style prompt.",
    "metadata": {
      "arxiv_id": "2309.11977",
      "title": "Improving Language Model-Based Zero-Shot Text-to-Speech Synthesis with Multi-Scale Acoustic Prompts",
      "summary": "Zero-shot text-to-speech (TTS) synthesis aims to clone any unseen speaker's voice without adaptation parameters. By quantizing speech waveform into discrete acoustic tokens and modeling these tokens with the language model, recent language model-based TTS models show zero-shot speaker adaptation capabilities with only a 3-second acoustic prompt of an unseen speaker. However, they are limited by the length of the acoustic prompt, which makes it difficult to clone personal speaking style. In this paper, we propose a novel zero-shot TTS model with the multi-scale acoustic prompts based on a neural codec language model VALL-E. A speaker-aware text encoder is proposed to learn the personal speaking style at the phoneme-level from the style prompt consisting of multiple sentences. Following that, a VALL-E based acoustic decoder is utilized to model the timbre from the timbre prompt at the frame-level and generate speech. The experimental results show that our proposed method outperforms baselines in terms of naturalness and speaker similarity, and can achieve better performance by scaling out to a longer style prompt.",
      "authors": [
        "Shun Lei",
        "Yixuan Zhou",
        "Liyang Chen",
        "Dan Luo",
        "Zhiyong Wu",
        "Xixin Wu",
        "Shiyin Kang",
        "Tao Jiang",
        "Yahui Zhou",
        "Yuxing Han",
        "Helen Meng"
      ],
      "published": "2023-09-21T11:22:22Z",
      "updated": "2024-04-09T08:39:52Z",
      "categories": [
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2309.11977v3",
      "landing_url": "https://arxiv.org/abs/2309.11977v3",
      "doi": "https://doi.org/10.48550/arXiv.2309.11977"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The abstract explicitly discusses quantizing speech waveform into discrete acoustic tokens and modeling them with a language-model-based TTS framework (VALL-E), which is a core discrete tokenization approach for speech generation.",
      "evaluation": 5
    },
    "round-A_JuniorNano_reasoning": "The abstract explicitly discusses quantizing speech waveform into discrete acoustic tokens and modeling them with a language-model-based TTS framework (VALL-E), which is a core discrete tokenization approach for speech generation.",
    "round-A_JuniorNano_evaluation": 5,
    "round-A_JuniorMini_output": {
      "reasoning": "The study proposes a zero-shot TTS model that utilizes discrete acoustic tokens derived from speech waveform quantization and modeled with a language model, explicitly focusing on discrete audio tokenization for speech synthesis; it also presents novel methodological contributions and experimental validation, fulfilling the inclusion criteria and not falling under any exclusion criteria.",
      "evaluation": 5
    },
    "round-A_JuniorMini_reasoning": "The study proposes a zero-shot TTS model that utilizes discrete acoustic tokens derived from speech waveform quantization and modeled with a language model, explicitly focusing on discrete audio tokenization for speech synthesis; it also presents novel methodological contributions and experimental validation, fulfilling the inclusion criteria and not falling under any exclusion criteria.",
    "round-A_JuniorMini_evaluation": 5,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "include (junior:5)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Fast-HuBERT: An Efficient Training Framework for Self-Supervised Speech Representation Learning",
    "abstract": "Recent years have witnessed significant advancements in self-supervised learning (SSL) methods for speech-processing tasks. Various speech-based SSL models have been developed and present promising performance on a range of downstream tasks including speech recognition. However, existing speech-based SSL models face a common dilemma in terms of computational cost, which might hinder their potential application and in-depth academic research. To address this issue, we first analyze the computational cost of different modules during HuBERT pre-training and then introduce a stack of efficiency optimizations, which is named Fast-HuBERT in this paper. The proposed Fast-HuBERT can be trained in 1.1 days with 8 V100 GPUs on the Librispeech 960h benchmark, without performance degradation, resulting in a 5.2x speedup, compared to the original implementation. Moreover, we explore two well-studied techniques in the Fast-HuBERT and demonstrate consistent improvements as reported in previous work.",
    "metadata": {
      "arxiv_id": "2309.13860",
      "title": "Fast-HuBERT: An Efficient Training Framework for Self-Supervised Speech Representation Learning",
      "summary": "Recent years have witnessed significant advancements in self-supervised learning (SSL) methods for speech-processing tasks. Various speech-based SSL models have been developed and present promising performance on a range of downstream tasks including speech recognition. However, existing speech-based SSL models face a common dilemma in terms of computational cost, which might hinder their potential application and in-depth academic research. To address this issue, we first analyze the computational cost of different modules during HuBERT pre-training and then introduce a stack of efficiency optimizations, which is named Fast-HuBERT in this paper. The proposed Fast-HuBERT can be trained in 1.1 days with 8 V100 GPUs on the Librispeech 960h benchmark, without performance degradation, resulting in a 5.2x speedup, compared to the original implementation. Moreover, we explore two well-studied techniques in the Fast-HuBERT and demonstrate consistent improvements as reported in previous work.",
      "authors": [
        "Guanrou Yang",
        "Ziyang Ma",
        "Zhisheng Zheng",
        "Yakun Song",
        "Zhikang Niu",
        "Xie Chen"
      ],
      "published": "2023-09-25T04:07:34Z",
      "updated": "2023-09-29T06:48:11Z",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG",
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2309.13860v2",
      "landing_url": "https://arxiv.org/abs/2309.13860v2",
      "doi": "https://doi.org/10.48550/arXiv.2309.13860"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The study focuses on efficiency of self-supervised speech representation learning and HuBERT pre-training but does not analyze or propose discrete speech tokenization (discrete tokens from a finite vocabulary) as a core object of study, so it does not meet the inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The study focuses on efficiency of self-supervised speech representation learning and HuBERT pre-training but does not analyze or propose discrete speech tokenization (discrete tokens from a finite vocabulary) as a core object of study, so it does not meet the inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on improving training efficiency for a self-supervised speech representation model (HuBERT) but does not explicitly address discrete audio/speech tokenization as a core subject—there is no clear analysis or proposal of discrete tokenization methods converting continuous audio signals into discrete tokens; instead, it focuses on computational cost and training speedup without discrete token design.",
      "evaluation": 2
    },
    "round-A_JuniorMini_reasoning": "The study focuses on improving training efficiency for a self-supervised speech representation model (HuBERT) but does not explicitly address discrete audio/speech tokenization as a core subject—there is no clear analysis or proposal of discrete tokenization methods converting continuous audio signals into discrete tokens; instead, it focuses on computational cost and training speedup without discrete token design.",
    "round-A_JuniorMini_evaluation": 2,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:2)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:2)"
  },
  {
    "title": "Towards General-Purpose Text-Instruction-Guided Voice Conversion",
    "abstract": "This paper introduces a novel voice conversion (VC) model, guided by text instructions such as \"articulate slowly with a deep tone\" or \"speak in a cheerful boyish voice\". Unlike traditional methods that rely on reference utterances to determine the attributes of the converted speech, our model adds versatility and specificity to voice conversion. The proposed VC model is a neural codec language model which processes a sequence of discrete codes, resulting in the code sequence of converted speech. It utilizes text instructions as style prompts to modify the prosody and emotional information of the given speech. In contrast to previous approaches, which often rely on employing separate encoders like prosody and content encoders to handle different aspects of the source speech, our model handles various information of speech in an end-to-end manner. Experiments have demonstrated the impressive capabilities of our model in comprehending instructions and delivering reasonable results.",
    "metadata": {
      "arxiv_id": "2309.14324",
      "title": "Towards General-Purpose Text-Instruction-Guided Voice Conversion",
      "summary": "This paper introduces a novel voice conversion (VC) model, guided by text instructions such as \"articulate slowly with a deep tone\" or \"speak in a cheerful boyish voice\". Unlike traditional methods that rely on reference utterances to determine the attributes of the converted speech, our model adds versatility and specificity to voice conversion. The proposed VC model is a neural codec language model which processes a sequence of discrete codes, resulting in the code sequence of converted speech. It utilizes text instructions as style prompts to modify the prosody and emotional information of the given speech. In contrast to previous approaches, which often rely on employing separate encoders like prosody and content encoders to handle different aspects of the source speech, our model handles various information of speech in an end-to-end manner. Experiments have demonstrated the impressive capabilities of our model in comprehending instructions and delivering reasonable results.",
      "authors": [
        "Chun-Yi Kuan",
        "Chen An Li",
        "Tsu-Yuan Hsu",
        "Tse-Yang Lin",
        "Ho-Lam Chung",
        "Kai-Wei Chang",
        "Shuo-yiin Chang",
        "Hung-yi Lee"
      ],
      "published": "2023-09-25T17:52:09Z",
      "updated": "2024-01-16T13:53:56Z",
      "categories": [
        "eess.AS",
        "cs.CL",
        "cs.LG",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2309.14324v2",
      "landing_url": "https://arxiv.org/abs/2309.14324v2",
      "doi": "https://doi.org/10.48550/arXiv.2309.14324"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The abstract describes a neural codec language model that processes a sequence of discrete codes to represent converted speech, i.e., discrete audio tokenization as part of the core method. This aligns with the inclusion criterion that the study explicitly proposes/analyzes discrete audio/speech tokenization for reconstruction/generation. It goes beyond engineering-only description and applies tokenization to text-instruction-guided voice conversion, including prosody/emotion control. There is no evidence in the abstract that the work uses only continuous features, focuses on non-speech tokenization, or is merely a data-modality description for text/images.",
      "evaluation": 4
    },
    "round-A_JuniorNano_reasoning": "The abstract describes a neural codec language model that processes a sequence of discrete codes to represent converted speech, i.e., discrete audio tokenization as part of the core method. This aligns with the inclusion criterion that the study explicitly proposes/analyzes discrete audio/speech tokenization for reconstruction/generation. It goes beyond engineering-only description and applies tokenization to text-instruction-guided voice conversion, including prosody/emotion control. There is no evidence in the abstract that the work uses only continuous features, focuses on non-speech tokenization, or is merely a data-modality description for text/images.",
    "round-A_JuniorNano_evaluation": 4,
    "round-A_JuniorMini_output": {
      "reasoning": "The study proposes a neural codec language model that processes sequences of discrete codes for voice conversion, implying the use of discrete audio tokenization as a core component; thus, it meets the inclusion criteria of analyzing discrete audio tokenization for speech tasks and does not fall under any exclusion criteria.",
      "evaluation": 5
    },
    "round-A_JuniorMini_reasoning": "The study proposes a neural codec language model that processes sequences of discrete codes for voice conversion, implying the use of discrete audio tokenization as a core component; thus, it meets the inclusion criteria of analyzing discrete audio tokenization for speech tasks and does not fall under any exclusion criteria.",
    "round-A_JuniorMini_evaluation": 5,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "include (junior:4)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Unveiling Fairness Biases in Deep Learning-Based Brain MRI Reconstruction",
    "abstract": "Deep learning (DL) reconstruction particularly of MRI has led to improvements in image fidelity and reduction of acquisition time. In neuroimaging, DL methods can reconstruct high-quality images from undersampled data. However, it is essential to consider fairness in DL algorithms, particularly in terms of demographic characteristics. This study presents the first fairness analysis in a DL-based brain MRI reconstruction model. The model utilises the U-Net architecture for image reconstruction and explores the presence and sources of unfairness by implementing baseline Empirical Risk Minimisation (ERM) and rebalancing strategies. Model performance is evaluated using image reconstruction metrics. Our findings reveal statistically significant performance biases between the gender and age subgroups. Surprisingly, data imbalance and training discrimination are not the main sources of bias. This analysis provides insights of fairness in DL-based image reconstruction and aims to improve equity in medical AI applications.",
    "metadata": {
      "arxiv_id": "2309.14392",
      "title": "Unveiling Fairness Biases in Deep Learning-Based Brain MRI Reconstruction",
      "summary": "Deep learning (DL) reconstruction particularly of MRI has led to improvements in image fidelity and reduction of acquisition time. In neuroimaging, DL methods can reconstruct high-quality images from undersampled data. However, it is essential to consider fairness in DL algorithms, particularly in terms of demographic characteristics. This study presents the first fairness analysis in a DL-based brain MRI reconstruction model. The model utilises the U-Net architecture for image reconstruction and explores the presence and sources of unfairness by implementing baseline Empirical Risk Minimisation (ERM) and rebalancing strategies. Model performance is evaluated using image reconstruction metrics. Our findings reveal statistically significant performance biases between the gender and age subgroups. Surprisingly, data imbalance and training discrimination are not the main sources of bias. This analysis provides insights of fairness in DL-based image reconstruction and aims to improve equity in medical AI applications.",
      "authors": [
        "Yuning Du",
        "Yuyang Xue",
        "Rohan Dharmakumar",
        "Sotirios A. Tsaftaris"
      ],
      "published": "2023-09-25T11:07:25Z",
      "updated": "2023-09-25T11:07:25Z",
      "categories": [
        "eess.IV",
        "cs.CV",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2309.14392v1",
      "landing_url": "https://arxiv.org/abs/2309.14392v1",
      "doi": "https://doi.org/10.48550/arXiv.2309.14392"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This item focuses on fairness biases in DL-based brain MRI reconstruction and contains no discrete audio tokenization analysis, so it does not meet the inclusion criteria requiring discrete audio tokens.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "This item focuses on fairness biases in DL-based brain MRI reconstruction and contains no discrete audio tokenization analysis, so it does not meet the inclusion criteria requiring discrete audio tokens.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on fairness biases in deep learning-based brain MRI reconstruction, which involves medical imaging rather than discrete audio or speech tokenization; it does not address the conversion of continuous audio signals into discrete tokens, nor does it analyze discrete audio tokenization relevant to the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on fairness biases in deep learning-based brain MRI reconstruction, which involves medical imaging rather than discrete audio or speech tokenization; it does not address the conversion of continuous audio signals into discrete tokens, nor does it analyze discrete audio tokenization relevant to the inclusion criteria.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "CPIPS: Learning to Preserve Perceptual Distances in End-to-End Image Compression",
    "abstract": "Lossy image coding standards such as JPEG and MPEG have successfully achieved high compression rates for human consumption of multimedia data. However, with the increasing prevalence of IoT devices, drones, and self-driving cars, machines rather than humans are processing a greater portion of captured visual content. Consequently, it is crucial to pursue an efficient compressed representation that caters not only to human vision but also to image processing and machine vision tasks. Drawing inspiration from the efficient coding hypothesis in biological systems and the modeling of the sensory cortex in neural science, we repurpose the compressed latent representation to prioritize semantic relevance while preserving perceptual distance. Our proposed method, Compressed Perceptual Image Patch Similarity (CPIPS), can be derived at a minimal cost from a learned neural codec and computed significantly faster than DNN-based perceptual metrics such as LPIPS and DISTS.",
    "metadata": {
      "arxiv_id": "2310.00559",
      "title": "CPIPS: Learning to Preserve Perceptual Distances in End-to-End Image Compression",
      "summary": "Lossy image coding standards such as JPEG and MPEG have successfully achieved high compression rates for human consumption of multimedia data. However, with the increasing prevalence of IoT devices, drones, and self-driving cars, machines rather than humans are processing a greater portion of captured visual content. Consequently, it is crucial to pursue an efficient compressed representation that caters not only to human vision but also to image processing and machine vision tasks. Drawing inspiration from the efficient coding hypothesis in biological systems and the modeling of the sensory cortex in neural science, we repurpose the compressed latent representation to prioritize semantic relevance while preserving perceptual distance. Our proposed method, Compressed Perceptual Image Patch Similarity (CPIPS), can be derived at a minimal cost from a learned neural codec and computed significantly faster than DNN-based perceptual metrics such as LPIPS and DISTS.",
      "authors": [
        "Chen-Hsiu Huang",
        "Ja-Ling Wu"
      ],
      "published": "2023-10-01T03:29:21Z",
      "updated": "2023-10-01T03:29:21Z",
      "categories": [
        "cs.CV",
        "cs.MM",
        "eess.IV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2310.00559v1",
      "landing_url": "https://arxiv.org/abs/2310.00559v1",
      "doi": "https://doi.org/10.48550/arXiv.2310.00559"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This item focuses on image compression and perceptual similarity (CPIPS) and has no discrete audio/speech tokenization; therefore it does not meet the audio-token criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "This item focuses on image compression and perceptual similarity (CPIPS) and has no discrete audio/speech tokenization; therefore it does not meet the audio-token criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on an image compression method that preserves perceptual distances and is targeted at image processing and machine vision tasks; it does not address audio or speech tokenization, discrete audio symbol representations, or related audio reconstruction or understanding tasks as required by the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on an image compression method that preserves perceptual distances and is targeted at image processing and machine vision tasks; it does not address audio or speech tokenization, discrete audio symbol representations, or related audio reconstruction or understanding tasks as required by the inclusion criteria.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Multi-resolution HuBERT: Multi-resolution Speech Self-Supervised Learning with Masked Unit Prediction",
    "abstract": "Existing Self-Supervised Learning (SSL) models for speech typically process speech signals at a fixed resolution of 20 milliseconds. This approach overlooks the varying informational content present at different resolutions in speech signals. In contrast, this paper aims to incorporate multi-resolution information into speech self-supervised representation learning. We introduce a SSL model that leverages a hierarchical Transformer architecture, complemented by HuBERT-style masked prediction objectives, to process speech at multiple resolutions. Experimental results indicate that the proposed model not only achieves more efficient inference but also exhibits superior or comparable performance to the original HuBERT model over various tasks. Specifically, significant performance improvements over the original HuBERT have been observed in fine-tuning experiments on the LibriSpeech speech recognition benchmark as well as in evaluations using the Speech Universal PERformance Benchmark (SUPERB) and Multilingual SUPERB (ML-SUPERB).",
    "metadata": {
      "arxiv_id": "2310.02720",
      "title": "Multi-resolution HuBERT: Multi-resolution Speech Self-Supervised Learning with Masked Unit Prediction",
      "summary": "Existing Self-Supervised Learning (SSL) models for speech typically process speech signals at a fixed resolution of 20 milliseconds. This approach overlooks the varying informational content present at different resolutions in speech signals. In contrast, this paper aims to incorporate multi-resolution information into speech self-supervised representation learning. We introduce a SSL model that leverages a hierarchical Transformer architecture, complemented by HuBERT-style masked prediction objectives, to process speech at multiple resolutions. Experimental results indicate that the proposed model not only achieves more efficient inference but also exhibits superior or comparable performance to the original HuBERT model over various tasks. Specifically, significant performance improvements over the original HuBERT have been observed in fine-tuning experiments on the LibriSpeech speech recognition benchmark as well as in evaluations using the Speech Universal PERformance Benchmark (SUPERB) and Multilingual SUPERB (ML-SUPERB).",
      "authors": [
        "Jiatong Shi",
        "Hirofumi Inaguma",
        "Xutai Ma",
        "Ilia Kulikov",
        "Anna Sun"
      ],
      "published": "2023-10-04T10:52:13Z",
      "updated": "2024-01-30T08:52:12Z",
      "categories": [
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2310.02720v2",
      "landing_url": "https://arxiv.org/abs/2310.02720v2",
      "doi": "https://doi.org/10.48550/arXiv.2310.02720"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The paper introduces a multi-resolution HuBERT model that uses masked unit prediction, implying discrete token representations (units) derived from clustering and used as targets. This centers on discrete audio tokenization as part of a self-supervised learning objective for speech, aligning with inclusion criteria and not falling under exclusion (which would be continuous features or tokenization only for non-speech). Therefore it should be included.",
      "evaluation": 5
    },
    "round-A_JuniorNano_reasoning": "The paper introduces a multi-resolution HuBERT model that uses masked unit prediction, implying discrete token representations (units) derived from clustering and used as targets. This centers on discrete audio tokenization as part of a self-supervised learning objective for speech, aligning with inclusion criteria and not falling under exclusion (which would be continuous features or tokenization only for non-speech). Therefore it should be included.",
    "round-A_JuniorNano_evaluation": 5,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on a multi-resolution speech self-supervised model using masked unit prediction, which implies learning discrete units or tokens from speech for representation; however, the abstract does not explicitly describe the process of discrete audio tokenization or conversion of continuous signals into discrete token sequences, leaving uncertainty if discrete tokenization is a core object of study as per inclusion criteria.",
      "evaluation": 3
    },
    "round-A_JuniorMini_reasoning": "The study focuses on a multi-resolution speech self-supervised model using masked unit prediction, which implies learning discrete units or tokens from speech for representation; however, the abstract does not explicitly describe the process of discrete audio tokenization or conversion of continuous signals into discrete token sequences, leaving uncertainty if discrete tokenization is a core object of study as per inclusion criteria.",
    "round-A_JuniorMini_evaluation": 3,
    "round-B_SeniorLead_output": {
      "reasoning": "The abstract mentions “masked unit prediction” and HuBERT-style objectives, which typically use discrete units derived from clustering, so the work likely uses discrete token targets; however the abstract does not explicitly describe proposing, analyzing, or evaluating a discrete audio/speech tokenization method or quantization design as a core contribution (it focuses on multi-resolution SSL and downstream performance). The two junior reviews disagree (one incl., one uncertain); given the abstract’s implication but lack of explicit tokenization-method focus, I cannot confirm the paper meets the inclusion criterion that discrete tokenization is explicitly proposed/analyzed/evaluated as the core object.",
      "evaluation": 3
    },
    "round-B_SeniorLead_reasoning": "The abstract mentions “masked unit prediction” and HuBERT-style objectives, which typically use discrete units derived from clustering, so the work likely uses discrete token targets; however the abstract does not explicitly describe proposing, analyzing, or evaluating a discrete audio/speech tokenization method or quantization design as a core contribution (it focuses on multi-resolution SSL and downstream performance). The two junior reviews disagree (one incl., one uncertain); given the abstract’s implication but lack of explicit tokenization-method focus, I cannot confirm the paper meets the inclusion criterion that discrete tokenization is explicitly proposed/analyzed/evaluated as the core object.",
    "round-B_SeniorLead_evaluation": 3,
    "final_verdict": "maybe (senior:3)",
    "review_skipped": false,
    "discard_reason": "review_needs_followup"
  },
  {
    "title": "HuBERTopic: Enhancing Semantic Representation of HuBERT through Self-supervision Utilizing Topic Model",
    "abstract": "Recently, the usefulness of self-supervised representation learning (SSRL) methods has been confirmed in various downstream tasks. Many of these models, as exemplified by HuBERT and WavLM, use pseudo-labels generated from spectral features or the model's own representation features. From previous studies, it is known that the pseudo-labels contain semantic information. However, the masked prediction task, the learning criterion of HuBERT, focuses on local contextual information and may not make effective use of global semantic information such as speaker, theme of speech, and so on. In this paper, we propose a new approach to enrich the semantic representation of HuBERT. We apply topic model to pseudo-labels to generate a topic label for each utterance. An auxiliary topic classification task is added to HuBERT by using topic labels as teachers. This allows additional global semantic information to be incorporated in an unsupervised manner. Experimental results demonstrate that our method achieves comparable or better performance than the baseline in most tasks, including automatic speech recognition and five out of the eight SUPERB tasks. Moreover, we find that topic labels include various information about utterance, such as gender, speaker, and its theme. This highlights the effectiveness of our approach in capturing multifaceted semantic nuances.",
    "metadata": {
      "arxiv_id": "2310.03975",
      "title": "HuBERTopic: Enhancing Semantic Representation of HuBERT through Self-supervision Utilizing Topic Model",
      "summary": "Recently, the usefulness of self-supervised representation learning (SSRL) methods has been confirmed in various downstream tasks. Many of these models, as exemplified by HuBERT and WavLM, use pseudo-labels generated from spectral features or the model's own representation features. From previous studies, it is known that the pseudo-labels contain semantic information. However, the masked prediction task, the learning criterion of HuBERT, focuses on local contextual information and may not make effective use of global semantic information such as speaker, theme of speech, and so on. In this paper, we propose a new approach to enrich the semantic representation of HuBERT. We apply topic model to pseudo-labels to generate a topic label for each utterance. An auxiliary topic classification task is added to HuBERT by using topic labels as teachers. This allows additional global semantic information to be incorporated in an unsupervised manner. Experimental results demonstrate that our method achieves comparable or better performance than the baseline in most tasks, including automatic speech recognition and five out of the eight SUPERB tasks. Moreover, we find that topic labels include various information about utterance, such as gender, speaker, and its theme. This highlights the effectiveness of our approach in capturing multifaceted semantic nuances.",
      "authors": [
        "Takashi Maekaku",
        "Jiatong Shi",
        "Xuankai Chang",
        "Yuya Fujita",
        "Shinji Watanabe"
      ],
      "published": "2023-10-06T02:19:09Z",
      "updated": "2023-10-06T02:19:09Z",
      "categories": [
        "cs.SD",
        "cs.CL"
      ],
      "pdf_url": "https://arxiv.org/pdf/2310.03975v1",
      "landing_url": "https://arxiv.org/abs/2310.03975v1",
      "doi": "https://doi.org/10.48550/arXiv.2310.03975"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The paper aims to enrich semantic representations of HuBERT by applying a topic model to pseudo-labels and using topic labels as teachers, but it does not treat discrete audio tokenization into a finite vocabulary as a core object of study, so it does not meet the stated inclusion criteria and should be excluded.",
      "evaluation": 2
    },
    "round-A_JuniorNano_reasoning": "The paper aims to enrich semantic representations of HuBERT by applying a topic model to pseudo-labels and using topic labels as teachers, but it does not treat discrete audio tokenization into a finite vocabulary as a core object of study, so it does not meet the stated inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 2,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on enhancing the semantic representation of HuBERT through self-supervision using topic models but does not explicitly propose or analyze discrete audio/speech tokenization methods where continuous audio signals are converted into sequences of discrete symbols; instead, it builds on pseudo-labels generated by existing features, without a clear focus on discrete token design or analysis, thus not meeting the inclusion criteria regarding discrete tokenization as a core study object.",
      "evaluation": 2
    },
    "round-A_JuniorMini_reasoning": "The study focuses on enhancing the semantic representation of HuBERT through self-supervision using topic models but does not explicitly propose or analyze discrete audio/speech tokenization methods where continuous audio signals are converted into sequences of discrete symbols; instead, it builds on pseudo-labels generated by existing features, without a clear focus on discrete token design or analysis, thus not meeting the inclusion criteria regarding discrete tokenization as a core study object.",
    "round-A_JuniorMini_evaluation": 2,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:2)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:2)"
  },
  {
    "title": "Psychoacoustic Challenges Of Speech Enhancement On VoIP Platforms",
    "abstract": "Within the ambit of VoIP (Voice over Internet Protocol) telecommunications, the complexities introduced by acoustic transformations merit rigorous analysis. This research, rooted in the exploration of proprietary sender-side denoising effects, meticulously evaluates platforms such as Google Meets and Zoom. The study draws upon the Deep Noise Suppression (DNS) 2020 dataset, ensuring a structured examination tailored to various denoising settings and receiver interfaces. A methodological novelty is introduced via Blinder-Oaxaca decomposition, traditionally an econometric tool, repurposed herein to analyze acoustic-phonetic perturbations within VoIP systems. To further ground the implications of these transformations, psychoacoustic metrics, specifically PESQ and STOI, were used to explain of perceptual quality and intelligibility. Cumulatively, the insights garnered underscore the intricate landscape of VoIP-influenced acoustic dynamics. In addition to the primary findings, a multitude of metrics are reported, extending the research purview. Moreover, out-of-domain benchmarking for both time and time-frequency domain speech enhancement models is included, thereby enhancing the depth and applicability of this inquiry.",
    "metadata": {
      "arxiv_id": "2310.07161",
      "title": "Psychoacoustic Challenges Of Speech Enhancement On VoIP Platforms",
      "summary": "Within the ambit of VoIP (Voice over Internet Protocol) telecommunications, the complexities introduced by acoustic transformations merit rigorous analysis. This research, rooted in the exploration of proprietary sender-side denoising effects, meticulously evaluates platforms such as Google Meets and Zoom. The study draws upon the Deep Noise Suppression (DNS) 2020 dataset, ensuring a structured examination tailored to various denoising settings and receiver interfaces. A methodological novelty is introduced via Blinder-Oaxaca decomposition, traditionally an econometric tool, repurposed herein to analyze acoustic-phonetic perturbations within VoIP systems. To further ground the implications of these transformations, psychoacoustic metrics, specifically PESQ and STOI, were used to explain of perceptual quality and intelligibility. Cumulatively, the insights garnered underscore the intricate landscape of VoIP-influenced acoustic dynamics. In addition to the primary findings, a multitude of metrics are reported, extending the research purview. Moreover, out-of-domain benchmarking for both time and time-frequency domain speech enhancement models is included, thereby enhancing the depth and applicability of this inquiry.",
      "authors": [
        "Joseph Konan",
        "Shikhar Agnihotri",
        "Ojas Bhargave",
        "Shuo Han",
        "Yunyang Zeng",
        "Ankit Shah",
        "Bhiksha Raj"
      ],
      "published": "2023-10-11T03:19:22Z",
      "updated": "2024-08-01T11:37:16Z",
      "categories": [
        "cs.SD",
        "cs.CL",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2310.07161v3",
      "landing_url": "https://arxiv.org/abs/2310.07161v3",
      "doi": "https://doi.org/10.48550/arXiv.2310.07161"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The abstract centers on psychoacoustic quality of speech enhancement on VoIP using continuous speech representations and metrics (PESQ/STOI) with no discrete tokenization or token-based representations analyzed as a core object.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The abstract centers on psychoacoustic quality of speech enhancement on VoIP using continuous speech representations and metrics (PESQ/STOI) with no discrete tokenization or token-based representations analyzed as a core object.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on psychoacoustic challenges and denoising effects in VoIP platforms, analyzing acoustic-phonetic perturbations using metrics for perceptual quality and intelligibility; however, it does not explicitly propose, analyze, or evaluate discrete audio/speech tokenization (i.e., conversion of continuous audio into finite discrete tokens) as a core object of study. The emphasis is more on continuous audio feature evaluation and speech enhancement rather than discrete token-based methods, thus failing to meet the inclusion criteria and falling into exclusion criteria related to continuous audio feature analysis without discrete tokenization.",
      "evaluation": 2
    },
    "round-A_JuniorMini_reasoning": "The study focuses on psychoacoustic challenges and denoising effects in VoIP platforms, analyzing acoustic-phonetic perturbations using metrics for perceptual quality and intelligibility; however, it does not explicitly propose, analyze, or evaluate discrete audio/speech tokenization (i.e., conversion of continuous audio into finite discrete tokens) as a core object of study. The emphasis is more on continuous audio feature evaluation and speech enhancement rather than discrete token-based methods, thus failing to meet the inclusion criteria and falling into exclusion criteria related to continuous audio feature analysis without discrete tokenization.",
    "round-A_JuniorMini_evaluation": 2,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:2)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:2)"
  },
  {
    "title": "Vec-Tok Speech: speech vectorization and tokenization for neural speech generation",
    "abstract": "Language models (LMs) have recently flourished in natural language processing and computer vision, generating high-fidelity texts or images in various tasks. In contrast, the current speech generative models are still struggling regarding speech quality and task generalization. This paper presents Vec-Tok Speech, an extensible framework that resembles multiple speech generation tasks, generating expressive and high-fidelity speech. Specifically, we propose a novel speech codec based on speech vectors and semantic tokens. Speech vectors contain acoustic details contributing to high-fidelity speech reconstruction, while semantic tokens focus on the linguistic content of speech, facilitating language modeling. Based on the proposed speech codec, Vec-Tok Speech leverages an LM to undertake the core of speech generation. Moreover, Byte-Pair Encoding (BPE) is introduced to reduce the token length and bit rate for lower exposure bias and longer context coverage, improving the performance of LMs. Vec-Tok Speech can be used for intra- and cross-lingual zero-shot voice conversion (VC), zero-shot speaking style transfer text-to-speech (TTS), speech-to-speech translation (S2ST), speech denoising, and speaker de-identification and anonymization. Experiments show that Vec-Tok Speech, built on 50k hours of speech, performs better than other SOTA models. Code will be available at https://github.com/BakerBunker/VecTok .",
    "metadata": {
      "arxiv_id": "2310.07246",
      "title": "Vec-Tok Speech: speech vectorization and tokenization for neural speech generation",
      "summary": "Language models (LMs) have recently flourished in natural language processing and computer vision, generating high-fidelity texts or images in various tasks. In contrast, the current speech generative models are still struggling regarding speech quality and task generalization. This paper presents Vec-Tok Speech, an extensible framework that resembles multiple speech generation tasks, generating expressive and high-fidelity speech. Specifically, we propose a novel speech codec based on speech vectors and semantic tokens. Speech vectors contain acoustic details contributing to high-fidelity speech reconstruction, while semantic tokens focus on the linguistic content of speech, facilitating language modeling. Based on the proposed speech codec, Vec-Tok Speech leverages an LM to undertake the core of speech generation. Moreover, Byte-Pair Encoding (BPE) is introduced to reduce the token length and bit rate for lower exposure bias and longer context coverage, improving the performance of LMs. Vec-Tok Speech can be used for intra- and cross-lingual zero-shot voice conversion (VC), zero-shot speaking style transfer text-to-speech (TTS), speech-to-speech translation (S2ST), speech denoising, and speaker de-identification and anonymization. Experiments show that Vec-Tok Speech, built on 50k hours of speech, performs better than other SOTA models. Code will be available at https://github.com/BakerBunker/VecTok .",
      "authors": [
        "Xinfa Zhu",
        "Yuanjun Lv",
        "Yi Lei",
        "Tao Li",
        "Wendi He",
        "Hongbin Zhou",
        "Heng Lu",
        "Lei Xie"
      ],
      "published": "2023-10-11T07:23:27Z",
      "updated": "2023-10-12T05:49:52Z",
      "categories": [
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2310.07246v2",
      "landing_url": "https://arxiv.org/abs/2310.07246v2",
      "doi": "https://doi.org/10.48550/arXiv.2310.07246"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The paper proposes a discrete speech tokenization framework with speech vectors and semantic tokens, using BPE to build a discrete token vocabulary for speech reconstruction and generation, thus matching inclusion criteria and not falling into exclusions.",
      "evaluation": 5
    },
    "round-A_JuniorNano_reasoning": "The paper proposes a discrete speech tokenization framework with speech vectors and semantic tokens, using BPE to build a discrete token vocabulary for speech reconstruction and generation, thus matching inclusion criteria and not falling into exclusions.",
    "round-A_JuniorNano_evaluation": 5,
    "round-A_JuniorMini_output": {
      "reasoning": "The study introduces Vec-Tok Speech, which explicitly proposes a novel speech codec involving discrete speech vectors and semantic tokens, aligning with inclusion criteria focusing on discrete audio tokenization for speech generation tasks, and it provides experimental validation surpassing SOTA models. It does not fall under exclusion criteria as it involves neural encoding and discrete token representations, not traditional codecs or engineering-only reports, and focuses on speech-related discrete tokenization.",
      "evaluation": 5
    },
    "round-A_JuniorMini_reasoning": "The study introduces Vec-Tok Speech, which explicitly proposes a novel speech codec involving discrete speech vectors and semantic tokens, aligning with inclusion criteria focusing on discrete audio tokenization for speech generation tasks, and it provides experimental validation surpassing SOTA models. It does not fall under exclusion criteria as it involves neural encoding and discrete token representations, not traditional codecs or engineering-only reports, and focuses on speech-related discrete tokenization.",
    "round-A_JuniorMini_evaluation": 5,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "include (junior:5)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "End-to-end Online Speaker Diarization with Target Speaker Tracking",
    "abstract": "This paper proposes an online target speaker voice activity detection system for speaker diarization tasks, which does not require a priori knowledge from the clustering-based diarization system to obtain the target speaker embeddings. By adapting the conventional target speaker voice activity detection for real-time operation, this framework can identify speaker activities using self-generated embeddings, resulting in consistent performance without permutation inconsistencies in the inference phase. During the inference process, we employ a front-end model to extract the frame-level speaker embeddings for each coming block of a signal. Next, we predict the detection state of each speaker based on these frame-level speaker embeddings and the previously estimated target speaker embedding. Then, the target speaker embeddings are updated by aggregating these frame-level speaker embeddings according to the predictions in the current block. Our model predicts the results for each block and updates the target speakers' embeddings until reaching the end of the signal. Experimental results show that the proposed method outperforms the offline clustering-based diarization system on the DIHARD III and AliMeeting datasets. The proposed method is further extended to multi-channel data, which achieves similar performance with the state-of-the-art offline diarization systems.",
    "metadata": {
      "arxiv_id": "2310.08696",
      "title": "End-to-end Online Speaker Diarization with Target Speaker Tracking",
      "summary": "This paper proposes an online target speaker voice activity detection system for speaker diarization tasks, which does not require a priori knowledge from the clustering-based diarization system to obtain the target speaker embeddings. By adapting the conventional target speaker voice activity detection for real-time operation, this framework can identify speaker activities using self-generated embeddings, resulting in consistent performance without permutation inconsistencies in the inference phase. During the inference process, we employ a front-end model to extract the frame-level speaker embeddings for each coming block of a signal. Next, we predict the detection state of each speaker based on these frame-level speaker embeddings and the previously estimated target speaker embedding. Then, the target speaker embeddings are updated by aggregating these frame-level speaker embeddings according to the predictions in the current block. Our model predicts the results for each block and updates the target speakers' embeddings until reaching the end of the signal. Experimental results show that the proposed method outperforms the offline clustering-based diarization system on the DIHARD III and AliMeeting datasets. The proposed method is further extended to multi-channel data, which achieves similar performance with the state-of-the-art offline diarization systems.",
      "authors": [
        "Weiqing Wang",
        "Ming Li"
      ],
      "published": "2023-10-12T20:02:07Z",
      "updated": "2023-10-12T20:02:07Z",
      "categories": [
        "eess.AS",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2310.08696v1",
      "landing_url": "https://arxiv.org/abs/2310.08696v1",
      "doi": "https://doi.org/10.48550/arXiv.2310.08696"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The paper focuses on online target speaker voice activity detection and embeddings for diarization, with no discrete tokenization or quantization of audio into a finite vocabulary, so it does not meet the inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The paper focuses on online target speaker voice activity detection and embeddings for diarization, with no discrete tokenization or quantization of audio into a finite vocabulary, so it does not meet the inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on speaker diarization using continuous speaker embeddings and voice activity detection without discussing any discrete audio tokenization, quantization, or conversion of audio signals into discrete token sequences, which is a core inclusion criterion.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on speaker diarization using continuous speaker embeddings and voice activity detection without discussing any discrete audio tokenization, quantization, or conversion of audio signals into discrete token sequences, which is a core inclusion criterion.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Low-latency Speech Enhancement via Speech Token Generation",
    "abstract": "Existing deep learning based speech enhancement mainly employ a data-driven approach, which leverage large amounts of data with a variety of noise types to achieve noise removal from noisy signal. However, the high dependence on the data limits its generalization on the unseen complex noises in real-life environment. In this paper, we focus on the low-latency scenario and regard speech enhancement as a speech generation problem conditioned on the noisy signal, where we generate clean speech instead of identifying and removing noises. Specifically, we propose a conditional generative framework for speech enhancement, which models clean speech by acoustic codes of a neural speech codec and generates the speech codes conditioned on past noisy frames in an auto-regressive way. Moreover, we propose an explicit-alignment approach to align noisy frames with the generated speech tokens to improve the robustness and scalability to different input lengths. Different from other methods that leverage multiple stages to generate speech codes, we leverage a single-stage speech generation approach based on the TF-Codec neural codec to achieve high speech quality with low latency. Extensive results on both synthetic and real-recorded test set show its superiority over data-driven approaches in terms of noise robustness and temporal speech coherence.",
    "metadata": {
      "arxiv_id": "2310.08981",
      "title": "Low-latency Speech Enhancement via Speech Token Generation",
      "summary": "Existing deep learning based speech enhancement mainly employ a data-driven approach, which leverage large amounts of data with a variety of noise types to achieve noise removal from noisy signal. However, the high dependence on the data limits its generalization on the unseen complex noises in real-life environment. In this paper, we focus on the low-latency scenario and regard speech enhancement as a speech generation problem conditioned on the noisy signal, where we generate clean speech instead of identifying and removing noises. Specifically, we propose a conditional generative framework for speech enhancement, which models clean speech by acoustic codes of a neural speech codec and generates the speech codes conditioned on past noisy frames in an auto-regressive way. Moreover, we propose an explicit-alignment approach to align noisy frames with the generated speech tokens to improve the robustness and scalability to different input lengths. Different from other methods that leverage multiple stages to generate speech codes, we leverage a single-stage speech generation approach based on the TF-Codec neural codec to achieve high speech quality with low latency. Extensive results on both synthetic and real-recorded test set show its superiority over data-driven approaches in terms of noise robustness and temporal speech coherence.",
      "authors": [
        "Huaying Xue",
        "Xiulian Peng",
        "Yan Lu"
      ],
      "published": "2023-10-13T09:57:09Z",
      "updated": "2024-01-23T06:13:04Z",
      "categories": [
        "cs.SD",
        "cs.MM",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2310.08981v3",
      "landing_url": "https://arxiv.org/abs/2310.08981v3",
      "doi": "https://doi.org/10.48550/arXiv.2310.08981"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The abstract centers on generating and manipulating discrete speech tokens/codes via a neural speech codec, treating tokenized speech representations as the core mechanism for reconstruction, which aligns with the discrete audio tokenization inclusion criterion and excludes outcomes based only on continuous features.",
      "evaluation": 5
    },
    "round-A_JuniorNano_reasoning": "The abstract centers on generating and manipulating discrete speech tokens/codes via a neural speech codec, treating tokenized speech representations as the core mechanism for reconstruction, which aligns with the discrete audio tokenization inclusion criterion and excludes outcomes based only on continuous features.",
    "round-A_JuniorNano_evaluation": 5,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on speech enhancement by generating clean speech through acoustic codes of a neural speech codec, explicitly modeling clean speech with discrete-style speech tokens within a neural codec framework; this aligns with the inclusion criteria emphasizing discrete audio/speech tokenization for audio generation, and it avoids exclusion criteria as it does not rely solely on continuous features or traditional codecs but uses a modern TF-Codec neural codec generating discrete speech tokens in an autoregressive manner while providing experimental validation.",
      "evaluation": 5
    },
    "round-A_JuniorMini_reasoning": "The study focuses on speech enhancement by generating clean speech through acoustic codes of a neural speech codec, explicitly modeling clean speech with discrete-style speech tokens within a neural codec framework; this aligns with the inclusion criteria emphasizing discrete audio/speech tokenization for audio generation, and it avoids exclusion criteria as it does not rely solely on continuous features or traditional codecs but uses a modern TF-Codec neural codec generating discrete speech tokens in an autoregressive manner while providing experimental validation.",
    "round-A_JuniorMini_evaluation": 5,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "include (junior:5)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "LL-VQ-VAE: Learnable Lattice Vector-Quantization For Efficient Representations",
    "abstract": "In this paper we introduce learnable lattice vector quantization and demonstrate its effectiveness for learning discrete representations. Our method, termed LL-VQ-VAE, replaces the vector quantization layer in VQ-VAE with lattice-based discretization. The learnable lattice imposes a structure over all discrete embeddings, acting as a deterrent against codebook collapse, leading to high codebook utilization. Compared to VQ-VAE, our method obtains lower reconstruction errors under the same training conditions, trains in a fraction of the time, and with a constant number of parameters (equal to the embedding dimension $D$), making it a very scalable approach. We demonstrate these results on the FFHQ-1024 dataset and include FashionMNIST and Celeb-A.",
    "metadata": {
      "arxiv_id": "2310.09382",
      "title": "LL-VQ-VAE: Learnable Lattice Vector-Quantization For Efficient Representations",
      "summary": "In this paper we introduce learnable lattice vector quantization and demonstrate its effectiveness for learning discrete representations. Our method, termed LL-VQ-VAE, replaces the vector quantization layer in VQ-VAE with lattice-based discretization. The learnable lattice imposes a structure over all discrete embeddings, acting as a deterrent against codebook collapse, leading to high codebook utilization. Compared to VQ-VAE, our method obtains lower reconstruction errors under the same training conditions, trains in a fraction of the time, and with a constant number of parameters (equal to the embedding dimension $D$), making it a very scalable approach. We demonstrate these results on the FFHQ-1024 dataset and include FashionMNIST and Celeb-A.",
      "authors": [
        "Ahmed Khalil",
        "Robert Piechocki",
        "Raul Santos-Rodriguez"
      ],
      "published": "2023-10-13T20:03:18Z",
      "updated": "2023-10-13T20:03:18Z",
      "categories": [
        "cs.LG",
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2310.09382v1",
      "landing_url": "https://arxiv.org/abs/2310.09382v1",
      "doi": "https://doi.org/10.48550/arXiv.2310.09382"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The abstract focuses on learnable lattice vector quantization for image-related data (e.g., FFHQ-1024, FashionMNIST, Celeb-A) and does not discuss discrete audio tokenization or speech-specific discrete representations, so it does not meet the inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The abstract focuses on learnable lattice vector quantization for image-related data (e.g., FFHQ-1024, FashionMNIST, Celeb-A) and does not discuss discrete audio tokenization or speech-specific discrete representations, so it does not meet the inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper proposes a learnable lattice vector quantization method applied primarily to visual datasets such as FFHQ-1024, FashionMNIST, and Celeb-A, without any mention of discrete audio or speech tokenization, which is a core inclusion criterion; therefore, the study does not meet the inclusion criteria related to audio/speech discrete tokenization.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper proposes a learnable lattice vector quantization method applied primarily to visual datasets such as FFHQ-1024, FashionMNIST, and Celeb-A, without any mention of discrete audio or speech tokenization, which is a core inclusion criterion; therefore, the study does not meet the inclusion criteria related to audio/speech discrete tokenization.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "SD-HuBERT: Sentence-Level Self-Distillation Induces Syllabic Organization in HuBERT",
    "abstract": "Data-driven unit discovery in self-supervised learning (SSL) of speech has embarked on a new era of spoken language processing. Yet, the discovered units often remain in phonetic space and the units beyond phonemes are largely underexplored. Here, we demonstrate that a syllabic organization emerges in learning sentence-level representation of speech. In particular, we adopt \"self-distillation\" objective to fine-tune the pretrained HuBERT with an aggregator token that summarizes the entire sentence. Without any supervision, the resulting model draws definite boundaries in speech, and the representations across frames exhibit salient syllabic structures. We demonstrate that this emergent structure largely corresponds to the ground truth syllables. Furthermore, we propose a new benchmark task, Spoken Speech ABX, for evaluating sentence-level representation of speech. When compared to previous models, our model outperforms in both unsupervised syllable discovery and learning sentence-level representation. Together, we demonstrate that the self-distillation of HuBERT gives rise to syllabic organization without relying on external labels or modalities, and potentially provides novel data-driven units for spoken language modeling.",
    "metadata": {
      "arxiv_id": "2310.10803",
      "title": "SD-HuBERT: Sentence-Level Self-Distillation Induces Syllabic Organization in HuBERT",
      "summary": "Data-driven unit discovery in self-supervised learning (SSL) of speech has embarked on a new era of spoken language processing. Yet, the discovered units often remain in phonetic space and the units beyond phonemes are largely underexplored. Here, we demonstrate that a syllabic organization emerges in learning sentence-level representation of speech. In particular, we adopt \"self-distillation\" objective to fine-tune the pretrained HuBERT with an aggregator token that summarizes the entire sentence. Without any supervision, the resulting model draws definite boundaries in speech, and the representations across frames exhibit salient syllabic structures. We demonstrate that this emergent structure largely corresponds to the ground truth syllables. Furthermore, we propose a new benchmark task, Spoken Speech ABX, for evaluating sentence-level representation of speech. When compared to previous models, our model outperforms in both unsupervised syllable discovery and learning sentence-level representation. Together, we demonstrate that the self-distillation of HuBERT gives rise to syllabic organization without relying on external labels or modalities, and potentially provides novel data-driven units for spoken language modeling.",
      "authors": [
        "Cheol Jun Cho",
        "Abdelrahman Mohamed",
        "Shang-Wen Li",
        "Alan W Black",
        "Gopala K. Anumanchipalli"
      ],
      "published": "2023-10-16T20:05:36Z",
      "updated": "2025-04-10T11:20:55Z",
      "categories": [
        "cs.CL",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2310.10803v3",
      "landing_url": "https://arxiv.org/abs/2310.10803v3",
      "doi": "https://doi.org/10.48550/arXiv.2310.10803"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The abstract centers on emergent syllabic structures and sentence-level representations via self-distillation and does not clearly propose, analyze, or evaluate discrete audio tokens/quantization as a core object.",
      "evaluation": 3
    },
    "round-A_JuniorNano_reasoning": "The abstract centers on emergent syllabic structures and sentence-level representations via self-distillation and does not clearly propose, analyze, or evaluate discrete audio tokens/quantization as a core object.",
    "round-A_JuniorNano_evaluation": 3,
    "round-A_JuniorMini_output": {
      "reasoning": "The study proposes and analyzes an unsupervised method ('self-distillation' on HuBERT) that effectively induces syllabic organization and discrete syllable-like units from continuous speech data, thereby addressing discrete audio/speech tokenization as a core object; it demonstrates emergent discrete units corresponding to syllables without using external labels, which aligns with inclusion criteria and avoids the exclusion criteria related to continuous features with no discrete quantization or engineering-only reports.",
      "evaluation": 5
    },
    "round-A_JuniorMini_reasoning": "The study proposes and analyzes an unsupervised method ('self-distillation' on HuBERT) that effectively induces syllabic organization and discrete syllable-like units from continuous speech data, thereby addressing discrete audio/speech tokenization as a core object; it demonstrates emergent discrete units corresponding to syllables without using external labels, which aligns with inclusion criteria and avoids the exclusion criteria related to continuous features with no discrete quantization or engineering-only reports.",
    "round-A_JuniorMini_evaluation": 5,
    "round-B_SeniorLead_output": {
      "reasoning": "Junior reviews: JuniorNano (3) flagged lack of clear discrete-token/quantization focus; JuniorMini (5) found the paper does induce and evaluate unsupervised syllable-like discrete units. My reading: the abstract explicitly describes emergence of data-driven syllabic units, unsupervised syllable discovery, and proposes a benchmark for sentence-level representation, which indicates the paper proposes and evaluates discrete audio units (syllable-like tokens) as a core object—though it is slightly ambiguous whether they formalize a finite-vocabulary tokenization pipeline for generation, the work clearly analyzes discrete unit discovery and evaluation. Given the inclusion criteria require explicit proposal/analysis/evaluation of discrete audio tokenization and the abstract meets that in the context of syllable/unit discovery, I recommend inclusion but with slight caution about full-text confirmation.",
      "evaluation": 4
    },
    "round-B_SeniorLead_reasoning": "Junior reviews: JuniorNano (3) flagged lack of clear discrete-token/quantization focus; JuniorMini (5) found the paper does induce and evaluate unsupervised syllable-like discrete units. My reading: the abstract explicitly describes emergence of data-driven syllabic units, unsupervised syllable discovery, and proposes a benchmark for sentence-level representation, which indicates the paper proposes and evaluates discrete audio units (syllable-like tokens) as a core object—though it is slightly ambiguous whether they formalize a finite-vocabulary tokenization pipeline for generation, the work clearly analyzes discrete unit discovery and evaluation. Given the inclusion criteria require explicit proposal/analysis/evaluation of discrete audio tokenization and the abstract meets that in the context of syllable/unit discovery, I recommend inclusion but with slight caution about full-text confirmation.",
    "round-B_SeniorLead_evaluation": 4,
    "final_verdict": "include (senior:4)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Spatial HuBERT: Self-supervised Spatial Speech Representation Learning for a Single Talker from Multi-channel Audio",
    "abstract": "Self-supervised learning has been used to leverage unlabelled data, improving accuracy and generalisation of speech systems through the training of representation models. While many recent works have sought to produce effective representations across a variety of acoustic domains, languages, modalities and even simultaneous speakers, these studies have all been limited to single-channel audio recordings. This paper presents Spatial HuBERT, a self-supervised speech representation model that learns both acoustic and spatial information pertaining to a single speaker in a potentially noisy environment by using multi-channel audio inputs. Spatial HuBERT learns representations that outperform state-of-the-art single-channel speech representations on a variety of spatial downstream tasks, particularly in reverberant and noisy environments. We also demonstrate the utility of the representations learned by Spatial HuBERT on a speech localisation downstream task. Along with this paper, we publicly release a new dataset of 100 000 simulated first-order ambisonics room impulse responses.",
    "metadata": {
      "arxiv_id": "2310.10922",
      "title": "Spatial HuBERT: Self-supervised Spatial Speech Representation Learning for a Single Talker from Multi-channel Audio",
      "summary": "Self-supervised learning has been used to leverage unlabelled data, improving accuracy and generalisation of speech systems through the training of representation models. While many recent works have sought to produce effective representations across a variety of acoustic domains, languages, modalities and even simultaneous speakers, these studies have all been limited to single-channel audio recordings. This paper presents Spatial HuBERT, a self-supervised speech representation model that learns both acoustic and spatial information pertaining to a single speaker in a potentially noisy environment by using multi-channel audio inputs. Spatial HuBERT learns representations that outperform state-of-the-art single-channel speech representations on a variety of spatial downstream tasks, particularly in reverberant and noisy environments. We also demonstrate the utility of the representations learned by Spatial HuBERT on a speech localisation downstream task. Along with this paper, we publicly release a new dataset of 100 000 simulated first-order ambisonics room impulse responses.",
      "authors": [
        "Antoni Dimitriadis",
        "Siqi Pan",
        "Vidhyasaharan Sethu",
        "Beena Ahmed"
      ],
      "published": "2023-10-17T01:31:59Z",
      "updated": "2023-10-17T01:31:59Z",
      "categories": [
        "cs.CL",
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2310.10922v1",
      "landing_url": "https://arxiv.org/abs/2310.10922v1",
      "doi": "https://doi.org/10.48550/arXiv.2310.10922"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The abstract discusses self-supervised spatial speech representations using multi-channel audio but does not address discrete audio tokenization or quantization as a core object of study.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The abstract discusses self-supervised spatial speech representations using multi-channel audio but does not address discrete audio tokenization or quantization as a core object of study.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper proposes Spatial HuBERT, a self-supervised speech representation model that learns acoustic and spatial information from multi-channel audio but does not explicitly mention proposing or analyzing discrete audio/speech tokenization as a core focus; rather, it emphasizes representation learning without clear evidence of discrete token sequence modeling or discrete quantization design, which is crucial per inclusion criteria.",
      "evaluation": 2
    },
    "round-A_JuniorMini_reasoning": "The paper proposes Spatial HuBERT, a self-supervised speech representation model that learns acoustic and spatial information from multi-channel audio but does not explicitly mention proposing or analyzing discrete audio/speech tokenization as a core focus; rather, it emphasizes representation learning without clear evidence of discrete token sequence modeling or discrete quantization design, which is crucial per inclusion criteria.",
    "round-A_JuniorMini_evaluation": 2,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:2)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:2)"
  },
  {
    "title": "Acoustic BPE for Speech Generation with Discrete Tokens",
    "abstract": "Discrete audio tokens derived from self-supervised learning models have gained widespread usage in speech generation. However, current practice of directly utilizing audio tokens poses challenges for sequence modeling due to the length of the token sequence. Additionally, this approach places the burden on the model to establish correlations between tokens, further complicating the modeling process. To address this issue, we propose acoustic BPE which encodes frequent audio token patterns by utilizing byte-pair encoding. Acoustic BPE effectively reduces the sequence length and leverages the prior morphological information present in token sequence, which alleviates the modeling challenges of token correlation. Through comprehensive investigations on a speech language model trained with acoustic BPE, we confirm the notable advantages it offers, including faster inference and improved syntax capturing capabilities. In addition, we propose a novel rescore method to select the optimal synthetic speech among multiple candidates generated by rich-diversity TTS system. Experiments prove that rescore selection aligns closely with human preference, which highlights acoustic BPE's potential to other speech generation tasks.",
    "metadata": {
      "arxiv_id": "2310.14580",
      "title": "Acoustic BPE for Speech Generation with Discrete Tokens",
      "summary": "Discrete audio tokens derived from self-supervised learning models have gained widespread usage in speech generation. However, current practice of directly utilizing audio tokens poses challenges for sequence modeling due to the length of the token sequence. Additionally, this approach places the burden on the model to establish correlations between tokens, further complicating the modeling process. To address this issue, we propose acoustic BPE which encodes frequent audio token patterns by utilizing byte-pair encoding. Acoustic BPE effectively reduces the sequence length and leverages the prior morphological information present in token sequence, which alleviates the modeling challenges of token correlation. Through comprehensive investigations on a speech language model trained with acoustic BPE, we confirm the notable advantages it offers, including faster inference and improved syntax capturing capabilities. In addition, we propose a novel rescore method to select the optimal synthetic speech among multiple candidates generated by rich-diversity TTS system. Experiments prove that rescore selection aligns closely with human preference, which highlights acoustic BPE's potential to other speech generation tasks.",
      "authors": [
        "Feiyu Shen",
        "Yiwei Guo",
        "Chenpeng Du",
        "Xie Chen",
        "Kai Yu"
      ],
      "published": "2023-10-23T05:38:41Z",
      "updated": "2024-01-15T05:53:31Z",
      "categories": [
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2310.14580v4",
      "landing_url": "https://arxiv.org/abs/2310.14580v4",
      "doi": "https://doi.org/10.48550/arXiv.2310.14580"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The study explicitly proposes and evaluates a discrete audio tokenization scheme (acoustic BPE) for speech generation and analyzes its effect on sequence modeling, inference efficiency, and candidate rescore methods, aligning with the inclusion criteria and not matching any exclusion criteria.",
      "evaluation": 5
    },
    "round-A_JuniorNano_reasoning": "The study explicitly proposes and evaluates a discrete audio tokenization scheme (acoustic BPE) for speech generation and analyzes its effect on sequence modeling, inference efficiency, and candidate rescore methods, aligning with the inclusion criteria and not matching any exclusion criteria.",
    "round-A_JuniorNano_evaluation": 5,
    "round-A_JuniorMini_output": {
      "reasoning": "The study explicitly proposes acoustic BPE, a discrete tokenization method for speech generation, analyzing its effect on sequence length and token modeling, fulfilling the inclusion criteria and avoiding all exclusion criteria.",
      "evaluation": 5
    },
    "round-A_JuniorMini_reasoning": "The study explicitly proposes acoustic BPE, a discrete tokenization method for speech generation, analyzing its effect on sequence length and token modeling, fulfilling the inclusion criteria and avoiding all exclusion criteria.",
    "round-A_JuniorMini_evaluation": 5,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "include (junior:5)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Dynamically Weighted Federated k-Means",
    "abstract": "Federated clustering, an integral aspect of federated machine learning, enables multiple data sources to collaboratively cluster their data, maintaining decentralization and preserving privacy. In this paper, we introduce a novel federated clustering algorithm named Dynamically Weighted Federated k-means (DWF k-means) based on Lloyd's method for k-means clustering, to address the challenges associated with distributed data sources and heterogeneous data. Our proposed algorithm combines the benefits of traditional clustering techniques with the privacy and scalability benefits offered by federated learning. The algorithm facilitates collaborative clustering among multiple data owners, allowing them to cluster their local data collectively while exchanging minimal information with the central coordinator. The algorithm optimizes the clustering process by adaptively aggregating cluster assignments and centroids from each data source, thereby learning a global clustering solution that reflects the collective knowledge of the entire federated network. We address the issue of empty clusters, which commonly arises in the context of federated clustering. We conduct experiments on multiple datasets and data distribution settings to evaluate the performance of our algorithm in terms of clustering score, accuracy, and v-measure. The results demonstrate that our approach can match the performance of the centralized classical k-means baseline, and outperform existing federated clustering methods like k-FED in realistic scenarios.",
    "metadata": {
      "arxiv_id": "2310.14858",
      "title": "Dynamically Weighted Federated k-Means",
      "summary": "Federated clustering, an integral aspect of federated machine learning, enables multiple data sources to collaboratively cluster their data, maintaining decentralization and preserving privacy. In this paper, we introduce a novel federated clustering algorithm named Dynamically Weighted Federated k-means (DWF k-means) based on Lloyd's method for k-means clustering, to address the challenges associated with distributed data sources and heterogeneous data. Our proposed algorithm combines the benefits of traditional clustering techniques with the privacy and scalability benefits offered by federated learning. The algorithm facilitates collaborative clustering among multiple data owners, allowing them to cluster their local data collectively while exchanging minimal information with the central coordinator. The algorithm optimizes the clustering process by adaptively aggregating cluster assignments and centroids from each data source, thereby learning a global clustering solution that reflects the collective knowledge of the entire federated network. We address the issue of empty clusters, which commonly arises in the context of federated clustering. We conduct experiments on multiple datasets and data distribution settings to evaluate the performance of our algorithm in terms of clustering score, accuracy, and v-measure. The results demonstrate that our approach can match the performance of the centralized classical k-means baseline, and outperform existing federated clustering methods like k-FED in realistic scenarios.",
      "authors": [
        "Patrick Holzer",
        "Tania Jacob",
        "Shubham Kavane"
      ],
      "published": "2023-10-23T12:28:21Z",
      "updated": "2023-11-17T10:35:48Z",
      "categories": [
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2310.14858v2",
      "landing_url": "https://arxiv.org/abs/2310.14858v2",
      "doi": "https://doi.org/10.48550/arXiv.2310.14858"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The abstract discusses a federated clustering algorithm (DWF k-means) with no mention of discrete audio/speech tokenization or discrete token representations for audio.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The abstract discusses a federated clustering algorithm (DWF k-means) with no mention of discrete audio/speech tokenization or discrete token representations for audio.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study proposes a federated clustering algorithm for distributed data without any mention or focus on discrete audio or speech tokenization, discrete symbol sequences, or audio reconstruction and generation tasks, thus it does not meet the inclusion criteria related to discrete audio/speech tokenization and is unrelated to the specified tokenization focus.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study proposes a federated clustering algorithm for distributed data without any mention or focus on discrete audio or speech tokenization, discrete symbol sequences, or audio reconstruction and generation tasks, thus it does not meet the inclusion criteria related to discrete audio/speech tokenization and is unrelated to the specified tokenization focus.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "GESI: Gammachirp Envelope Similarity Index for Predicting Intelligibility of Simulated Hearing Loss Sounds",
    "abstract": "We propose an objective intelligibility measure (OIM), called the Gammachirp Envelope Similarity Index (GESI), which can predict the speech intelligibility (SI) of simulated hearing loss (HL) sounds for normal hearing (NH) listeners. GESI is an intrusive method that computes the SI metric using the gammachirp filterbank (GCFB), the modulation filterbank, and the extended cosine similarity measure. The unique features of GESI are that i) it reflects the hearing impaired (HI) listener's HL that appears in the audiogram and is caused by active and passive cochlear dysfunction, ii) it provides a single goodness metric, as in the widely used STOI and ESTOI, that can be used immediately to evaluate SE algorithms, and iii) it provides a simple control parameter to accept the level asymmetry of the reference and test sounds and to deal with individual listening conditions and environments. We evaluated GESI and the conventional OIMs, STOI, ESTOI, MBSTOI, and HASPI versions 1 and 2 by using four SI experiments on words of male and female speech sounds in both laboratory and remote environments. GESI was shown to outperform the other OIMs in the evaluations. GESI could be used to improve SE algorithms in assistive listening devices for individual HI listeners.",
    "metadata": {
      "arxiv_id": "2310.15399",
      "title": "GESI: Gammachirp Envelope Similarity Index for Predicting Intelligibility of Simulated Hearing Loss Sounds",
      "summary": "We propose an objective intelligibility measure (OIM), called the Gammachirp Envelope Similarity Index (GESI), which can predict the speech intelligibility (SI) of simulated hearing loss (HL) sounds for normal hearing (NH) listeners. GESI is an intrusive method that computes the SI metric using the gammachirp filterbank (GCFB), the modulation filterbank, and the extended cosine similarity measure. The unique features of GESI are that i) it reflects the hearing impaired (HI) listener's HL that appears in the audiogram and is caused by active and passive cochlear dysfunction, ii) it provides a single goodness metric, as in the widely used STOI and ESTOI, that can be used immediately to evaluate SE algorithms, and iii) it provides a simple control parameter to accept the level asymmetry of the reference and test sounds and to deal with individual listening conditions and environments. We evaluated GESI and the conventional OIMs, STOI, ESTOI, MBSTOI, and HASPI versions 1 and 2 by using four SI experiments on words of male and female speech sounds in both laboratory and remote environments. GESI was shown to outperform the other OIMs in the evaluations. GESI could be used to improve SE algorithms in assistive listening devices for individual HI listeners.",
      "authors": [
        "Ayako Yamamoto",
        "Toshio Irino",
        "Fuki Miyazaki",
        "Honoka Tamaru"
      ],
      "published": "2023-10-23T23:01:33Z",
      "updated": "2024-03-14T02:14:04Z",
      "categories": [
        "eess.AS",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2310.15399v3",
      "landing_url": "https://arxiv.org/abs/2310.15399v3",
      "doi": "https://doi.org/10.48550/arXiv.2310.15399"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The work proposes and evaluates continuous-valued intelligibility metrics (GESI) based on filterbanks and cosine similarity, with no discrete audio tokenization or token sequence design analyzed or developed as a core object of study.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The work proposes and evaluates continuous-valued intelligibility metrics (GESI) based on filterbanks and cosine similarity, with no discrete audio tokenization or token sequence design analyzed or developed as a core object of study.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study proposes and evaluates an objective intelligibility measure (GESI) based on continuous audio processing techniques (gammachirp filterbank, modulation filterbank, and similarity measures) to predict speech intelligibility for simulated hearing loss sounds; however, it does not address discrete audio/speech tokenization or any discrete symbol sequence representation as per the inclusion criteria. Therefore, it focuses on continuous audio features without introducing or analyzing discrete tokenization or quantization methods relevant to language-model-style token sequences.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study proposes and evaluates an objective intelligibility measure (GESI) based on continuous audio processing techniques (gammachirp filterbank, modulation filterbank, and similarity measures) to predict speech intelligibility for simulated hearing loss sounds; however, it does not address discrete audio/speech tokenization or any discrete symbol sequence representation as per the inclusion criteria. Therefore, it focuses on continuous audio features without introducing or analyzing discrete tokenization or quantization methods relevant to language-model-style token sequences.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Dynamic Processing Neural Network Architecture For Hearing Loss Compensation",
    "abstract": "This paper proposes neural networks for compensating sensorineural hearing loss. The aim of the hearing loss compensation task is to transform a speech signal to increase speech intelligibility after further processing by a person with a hearing impairment, which is modeled by a hearing loss model. We propose an interpretable model called dynamic processing network, which has a structure similar to band-wise dynamic compressor. The network is differentiable, and therefore allows to learn its parameters to maximize speech intelligibility. More generic models based on convolutional layers were tested as well. The performance of the tested architectures was assessed using spectro-temporal objective index (STOI) with hearing-threshold noise and hearing aid speech intelligibility (HASPI) metrics. The dynamic processing network gave a significant improvement of STOI and HASPI in comparison to popular compressive gain prescription rule Camfit. A large enough convolutional network could outperform the interpretable model with the cost of larger computational load. Finally, a combination of the dynamic processing network with convolutional neural network gave the best results in terms of STOI and HASPI.",
    "metadata": {
      "arxiv_id": "2310.16550",
      "title": "Dynamic Processing Neural Network Architecture For Hearing Loss Compensation",
      "summary": "This paper proposes neural networks for compensating sensorineural hearing loss. The aim of the hearing loss compensation task is to transform a speech signal to increase speech intelligibility after further processing by a person with a hearing impairment, which is modeled by a hearing loss model. We propose an interpretable model called dynamic processing network, which has a structure similar to band-wise dynamic compressor. The network is differentiable, and therefore allows to learn its parameters to maximize speech intelligibility. More generic models based on convolutional layers were tested as well. The performance of the tested architectures was assessed using spectro-temporal objective index (STOI) with hearing-threshold noise and hearing aid speech intelligibility (HASPI) metrics. The dynamic processing network gave a significant improvement of STOI and HASPI in comparison to popular compressive gain prescription rule Camfit. A large enough convolutional network could outperform the interpretable model with the cost of larger computational load. Finally, a combination of the dynamic processing network with convolutional neural network gave the best results in terms of STOI and HASPI.",
      "authors": [
        "Szymon Drgas",
        "Lars Bramsløw",
        "Archontis Politis",
        "Gaurav Naithani",
        "Tuomas Virtanen"
      ],
      "published": "2023-10-25T11:04:32Z",
      "updated": "2023-10-25T11:04:32Z",
      "categories": [
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2310.16550v1",
      "landing_url": "https://arxiv.org/abs/2310.16550v1",
      "doi": "https://doi.org/10.48550/arXiv.2310.16550"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The article presents a neural network-based hearing loss compensation method focusing on continuous speech enhancement and does not involve discrete audio tokenization into symbols for tokens; hence it fails the inclusion criteria requiring explicit discrete tokenization of audio.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The article presents a neural network-based hearing loss compensation method focusing on continuous speech enhancement and does not involve discrete audio tokenization into symbols for tokens; hence it fails the inclusion criteria requiring explicit discrete tokenization of audio.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper focuses on neural network architectures for compensating hearing loss, specifically transforming speech signals to improve intelligibility, but does not discuss discrete audio/speech tokenization or the conversion of continuous audio signals into discrete tokens; thus, it does not meet the inclusion criteria and aligns with the exclusion criteria regarding continuous audio feature usage without discrete tokenization components.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper focuses on neural network architectures for compensating hearing loss, specifically transforming speech signals to improve intelligibility, but does not discuss discrete audio/speech tokenization or the conversion of continuous audio signals into discrete tokens; thus, it does not meet the inclusion criteria and aligns with the exclusion criteria regarding continuous audio feature usage without discrete tokenization components.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "4D-Editor: Interactive Object-level Editing in Dynamic Neural Radiance Fields via Semantic Distillation",
    "abstract": "This paper targets interactive object-level editing (e.g., deletion, recoloring, transformation, composition) in dynamic scenes. Recently, some methods aiming for flexible editing static scenes represented by neural radiance field (NeRF) have shown impressive synthesis quality, while similar capabilities in time-variant dynamic scenes remain limited. To solve this problem, we propose 4D-Editor, an interactive semantic-driven editing framework, allowing editing multiple objects in a dynamic NeRF with user strokes on a single frame. We propose an extension to the original dynamic NeRF by incorporating a hybrid semantic feature distillation to maintain spatial-temporal consistency after editing. In addition, we design Recursive Selection Refinement that significantly boosts object segmentation accuracy within a dynamic NeRF to aid the editing process. Moreover, we develop Multi-view Reprojection Inpainting to fill holes caused by incomplete scene capture after editing. Extensive experiments and editing examples on real-world demonstrate that 4D-Editor achieves photo-realistic editing on dynamic NeRFs. Project page: https://patrickddj.github.io/4D-Editor",
    "metadata": {
      "arxiv_id": "2310.16858",
      "title": "4D-Editor: Interactive Object-level Editing in Dynamic Neural Radiance Fields via Semantic Distillation",
      "summary": "This paper targets interactive object-level editing (e.g., deletion, recoloring, transformation, composition) in dynamic scenes. Recently, some methods aiming for flexible editing static scenes represented by neural radiance field (NeRF) have shown impressive synthesis quality, while similar capabilities in time-variant dynamic scenes remain limited. To solve this problem, we propose 4D-Editor, an interactive semantic-driven editing framework, allowing editing multiple objects in a dynamic NeRF with user strokes on a single frame. We propose an extension to the original dynamic NeRF by incorporating a hybrid semantic feature distillation to maintain spatial-temporal consistency after editing. In addition, we design Recursive Selection Refinement that significantly boosts object segmentation accuracy within a dynamic NeRF to aid the editing process. Moreover, we develop Multi-view Reprojection Inpainting to fill holes caused by incomplete scene capture after editing. Extensive experiments and editing examples on real-world demonstrate that 4D-Editor achieves photo-realistic editing on dynamic NeRFs. Project page: https://patrickddj.github.io/4D-Editor",
      "authors": [
        "Dadong Jiang",
        "Zhihui Ke",
        "Xiaobo Zhou",
        "Xidong Shi"
      ],
      "published": "2023-10-25T02:20:03Z",
      "updated": "2023-11-06T03:38:44Z",
      "categories": [
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2310.16858v2",
      "landing_url": "https://arxiv.org/abs/2310.16858v2",
      "doi": "https://doi.org/10.48550/arXiv.2310.16858"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This item discusses 4D-Editor for dynamic NeRF editing and semantic distillation and does not mention discrete audio/speech tokenization, so it does not meet the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "This item discusses 4D-Editor for dynamic NeRF editing and semantic distillation and does not mention discrete audio/speech tokenization, so it does not meet the inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper focuses on interactive object-level editing in dynamic neural radiance fields for visual scenes and does not address discrete audio or speech tokenization, which is the core inclusion criterion.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper focuses on interactive object-level editing in dynamic neural radiance fields for visual scenes and does not address discrete audio or speech tokenization, which is the core inclusion criterion.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Single channel speech enhancement by colored spectrograms",
    "abstract": "Speech enhancement concerns the processes required to remove unwanted background sounds from the target speech to improve its quality and intelligibility. In this paper, a novel approach for single-channel speech enhancement is presented, using colored spectrograms. We propose the use of a deep neural network (DNN) architecture adapted from the pix2pix generative adversarial network (GAN) and train it over colored spectrograms of speech to denoise them. After denoising, the colors of spectrograms are translated to magnitudes of short-time Fourier transform (STFT) using a shallow regression neural network. These estimated STFT magnitudes are later combined with the noisy phases to obtain an enhanced speech. The results show an improvement of almost 0.84 points in the perceptual evaluation of speech quality (PESQ) and 1% in the short-term objective intelligibility (STOI) over the unprocessed noisy data. The gain in quality and intelligibility over the unprocessed signal is almost equal to the gain achieved by the baseline methods used for comparison with the proposed model, but at a much reduced computational cost. The proposed solution offers a comparative PESQ score at almost 10 times reduced computational cost than a similar baseline model that has generated the highest PESQ score trained on grayscaled spectrograms, while it provides only a 1% deficit in STOI at 28 times reduced computational cost when compared to another baseline system based on convolutional neural network-GAN (CNN-GAN) that produces the most intelligible speech.",
    "metadata": {
      "arxiv_id": "2310.17142",
      "title": "Single channel speech enhancement by colored spectrograms",
      "summary": "Speech enhancement concerns the processes required to remove unwanted background sounds from the target speech to improve its quality and intelligibility. In this paper, a novel approach for single-channel speech enhancement is presented, using colored spectrograms. We propose the use of a deep neural network (DNN) architecture adapted from the pix2pix generative adversarial network (GAN) and train it over colored spectrograms of speech to denoise them. After denoising, the colors of spectrograms are translated to magnitudes of short-time Fourier transform (STFT) using a shallow regression neural network. These estimated STFT magnitudes are later combined with the noisy phases to obtain an enhanced speech. The results show an improvement of almost 0.84 points in the perceptual evaluation of speech quality (PESQ) and 1% in the short-term objective intelligibility (STOI) over the unprocessed noisy data. The gain in quality and intelligibility over the unprocessed signal is almost equal to the gain achieved by the baseline methods used for comparison with the proposed model, but at a much reduced computational cost. The proposed solution offers a comparative PESQ score at almost 10 times reduced computational cost than a similar baseline model that has generated the highest PESQ score trained on grayscaled spectrograms, while it provides only a 1% deficit in STOI at 28 times reduced computational cost when compared to another baseline system based on convolutional neural network-GAN (CNN-GAN) that produces the most intelligible speech.",
      "authors": [
        "Sania Gul",
        "Muhammad Salman Khan",
        "Muhammad Fazeel"
      ],
      "published": "2023-10-26T04:29:27Z",
      "updated": "2023-10-26T04:29:27Z",
      "categories": [
        "eess.AS",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2310.17142v1",
      "landing_url": "https://arxiv.org/abs/2310.17142v1",
      "doi": "https://doi.org/10.48550/arXiv.2310.17142"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The paper uses colored spectrograms and neural nets to denoise speech, converting colors to STFT magnitudes, with no discrete tokenization of audio into a finite vocabulary; therefore it does not address discrete audio tokens as core, violating the inclusion criterion and aligning with exclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The paper uses colored spectrograms and neural nets to denoise speech, converting colors to STFT magnitudes, with no discrete tokenization of audio into a finite vocabulary; therefore it does not address discrete audio tokens as core, violating the inclusion criterion and aligning with exclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on single-channel speech enhancement using deep neural networks applied to colored spectrograms, improving speech quality and intelligibility; however, it does not propose or analyze any discrete audio/speech tokenization method converting continuous audio into discrete symbols, instead it operates directly on continuous spectrogram data without discrete quantization relevant for tokenization tasks as specified in the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on single-channel speech enhancement using deep neural networks applied to colored spectrograms, improving speech quality and intelligibility; however, it does not propose or analyze any discrete audio/speech tokenization method converting continuous audio into discrete symbols, instead it operates directly on continuous spectrogram data without discrete quantization relevant for tokenization tasks as specified in the inclusion criteria.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "RTP: Rethinking Tensor Parallelism with Memory Deduplication",
    "abstract": "In the evolving landscape of neural network models, one prominent challenge stand out: the significant memory overheads associated with training expansive models. Addressing this challenge, this study delves deep into the Rotated Tensor Parallelism (RTP). RTP is an innovative approach that strategically focuses on memory deduplication in distributed training environments. It boasts of unique features like a customized communication primitive and the Flyweight Pattern initialization. Furthermore, RTP ensures a seamless overlap between partition computation and partition weight communication, optimizing the training process. Our empirical evaluations underscore RTP's efficiency, revealing that its memory consumption during distributed system training is remarkably close to the optimal - distributing the memory overhead of a single machine equitably among multiple machines. The experimental results demonstrate that RTP is capable of achieving comparable performance to Distributed Data Parallel while providing support for significantly larger models with near-linear scalability in terms of memory. Code of RTP is available at https://github.com/wdlctc/rtp.",
    "metadata": {
      "arxiv_id": "2311.01635",
      "title": "RTP: Rethinking Tensor Parallelism with Memory Deduplication",
      "summary": "In the evolving landscape of neural network models, one prominent challenge stand out: the significant memory overheads associated with training expansive models. Addressing this challenge, this study delves deep into the Rotated Tensor Parallelism (RTP). RTP is an innovative approach that strategically focuses on memory deduplication in distributed training environments. It boasts of unique features like a customized communication primitive and the Flyweight Pattern initialization. Furthermore, RTP ensures a seamless overlap between partition computation and partition weight communication, optimizing the training process. Our empirical evaluations underscore RTP's efficiency, revealing that its memory consumption during distributed system training is remarkably close to the optimal - distributing the memory overhead of a single machine equitably among multiple machines. The experimental results demonstrate that RTP is capable of achieving comparable performance to Distributed Data Parallel while providing support for significantly larger models with near-linear scalability in terms of memory. Code of RTP is available at https://github.com/wdlctc/rtp.",
      "authors": [
        "Cheng Luo",
        "Tianle Zhong",
        "Geoffrey Fox"
      ],
      "published": "2023-11-02T23:12:42Z",
      "updated": "2023-11-02T23:12:42Z",
      "categories": [
        "cs.DC",
        "cs.AI",
        "cs.NI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2311.01635v1",
      "landing_url": "https://arxiv.org/abs/2311.01635v1",
      "doi": "https://doi.org/10.48550/arXiv.2311.01635"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This item discusses Rotated Tensor Parallelism and memory deduplication for distributed training; there is no discrete audio/speech tokenization, no analysis of discrete audio tokens, and it is not about speech tasks, so it fails the inclusion criteria and does not trigger any exclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "This item discusses Rotated Tensor Parallelism and memory deduplication for distributed training; there is no discrete audio/speech tokenization, no analysis of discrete audio tokens, and it is not about speech tasks, so it fails the inclusion criteria and does not trigger any exclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on a new parallelism technique to optimize memory usage in neural network training but does not discuss or analyze discrete audio/speech tokenization or discrete token sequences for audio processing, which are required by the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on a new parallelism technique to optimize memory usage in neural network training but does not discuss or analyze discrete audio/speech tokenization or discrete token sequences for audio processing, which are required by the inclusion criteria.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "AV-Lip-Sync+: Leveraging AV-HuBERT to Exploit Multimodal Inconsistency for Deepfake Detection of Frontal Face Videos",
    "abstract": "Multimodal manipulations (also known as audio-visual deepfakes) make it difficult for unimodal deepfake detectors to detect forgeries in multimedia content. To avoid the spread of false propaganda and fake news, timely detection is crucial. The damage to either modality (i.e., visual or audio) can only be discovered through multimodal models that can exploit both pieces of information simultaneously. However, previous methods mainly adopt unimodal video forensics and use supervised pre-training for forgery detection. This study proposes a new method based on a multimodal self-supervised-learning (SSL) feature extractor to exploit inconsistency between audio and visual modalities for multimodal video forgery detection. We use the transformer-based SSL pre-trained Audio-Visual HuBERT (AV-HuBERT) model as a visual and acoustic feature extractor and a multi-scale temporal convolutional neural network to capture the temporal correlation between the audio and visual modalities. Since AV-HuBERT only extracts visual features from the lip region, we also adopt another transformer-based video model to exploit facial features and capture spatial and temporal artifacts caused during the deepfake generation process. Experimental results show that our model outperforms all existing models and achieves new state-of-the-art performance on the FakeAVCeleb and DeepfakeTIMIT datasets.",
    "metadata": {
      "arxiv_id": "2311.02733",
      "title": "AV-Lip-Sync+: Leveraging AV-HuBERT to Exploit Multimodal Inconsistency for Deepfake Detection of Frontal Face Videos",
      "summary": "Multimodal manipulations (also known as audio-visual deepfakes) make it difficult for unimodal deepfake detectors to detect forgeries in multimedia content. To avoid the spread of false propaganda and fake news, timely detection is crucial. The damage to either modality (i.e., visual or audio) can only be discovered through multimodal models that can exploit both pieces of information simultaneously. However, previous methods mainly adopt unimodal video forensics and use supervised pre-training for forgery detection. This study proposes a new method based on a multimodal self-supervised-learning (SSL) feature extractor to exploit inconsistency between audio and visual modalities for multimodal video forgery detection. We use the transformer-based SSL pre-trained Audio-Visual HuBERT (AV-HuBERT) model as a visual and acoustic feature extractor and a multi-scale temporal convolutional neural network to capture the temporal correlation between the audio and visual modalities. Since AV-HuBERT only extracts visual features from the lip region, we also adopt another transformer-based video model to exploit facial features and capture spatial and temporal artifacts caused during the deepfake generation process. Experimental results show that our model outperforms all existing models and achieves new state-of-the-art performance on the FakeAVCeleb and DeepfakeTIMIT datasets.",
      "authors": [
        "Sahibzada Adil Shahzad",
        "Ammarah Hashmi",
        "Yan-Tsung Peng",
        "Yu Tsao",
        "Hsin-Min Wang"
      ],
      "published": "2023-11-05T18:35:03Z",
      "updated": "2025-11-21T05:23:01Z",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG",
        "cs.MM",
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2311.02733v2",
      "landing_url": "https://arxiv.org/abs/2311.02733v2",
      "doi": "https://doi.org/10.1109/THMS.2025.3618409"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The study uses AV-HuBERT as a feature extractor for multimodal deepfake detection and does not explicitly propose, analyze, or evaluate discrete audio tokenization as a core object.",
      "evaluation": 2
    },
    "round-A_JuniorNano_reasoning": "The study uses AV-HuBERT as a feature extractor for multimodal deepfake detection and does not explicitly propose, analyze, or evaluate discrete audio tokenization as a core object.",
    "round-A_JuniorNano_evaluation": 2,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on using the AV-HuBERT model as a feature extractor for deepfake detection leveraging multimodal inconsistency, but it does not explicitly propose, analyze, or evaluate discrete audio/speech tokenization methods as a core subject, nor does it discuss discrete token design or analysis; therefore, it does not meet the inclusion criteria centered on discrete audio tokenization for audio reconstruction, generation, or understanding.",
      "evaluation": 2
    },
    "round-A_JuniorMini_reasoning": "The study focuses on using the AV-HuBERT model as a feature extractor for deepfake detection leveraging multimodal inconsistency, but it does not explicitly propose, analyze, or evaluate discrete audio/speech tokenization methods as a core subject, nor does it discuss discrete token design or analysis; therefore, it does not meet the inclusion criteria centered on discrete audio tokenization for audio reconstruction, generation, or understanding.",
    "round-A_JuniorMini_evaluation": 2,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:2)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:2)"
  },
  {
    "title": "Transduce and Speak: Neural Transducer for Text-to-Speech with Semantic Token Prediction",
    "abstract": "We introduce a text-to-speech(TTS) framework based on a neural transducer. We use discretized semantic tokens acquired from wav2vec2.0 embeddings, which makes it easy to adopt a neural transducer for the TTS framework enjoying its monotonic alignment constraints. The proposed model first generates aligned semantic tokens using the neural transducer, then synthesizes a speech sample from the semantic tokens using a non-autoregressive(NAR) speech generator. This decoupled framework alleviates the training complexity of TTS and allows each stage to focus on 1) linguistic and alignment modeling and 2) fine-grained acoustic modeling, respectively. Experimental results on the zero-shot adaptive TTS show that the proposed model exceeds the baselines in speech quality and speaker similarity via objective and subjective measures. We also investigate the inference speed and prosody controllability of our proposed model, showing the potential of the neural transducer for TTS frameworks.",
    "metadata": {
      "arxiv_id": "2311.02898",
      "title": "Transduce and Speak: Neural Transducer for Text-to-Speech with Semantic Token Prediction",
      "summary": "We introduce a text-to-speech(TTS) framework based on a neural transducer. We use discretized semantic tokens acquired from wav2vec2.0 embeddings, which makes it easy to adopt a neural transducer for the TTS framework enjoying its monotonic alignment constraints. The proposed model first generates aligned semantic tokens using the neural transducer, then synthesizes a speech sample from the semantic tokens using a non-autoregressive(NAR) speech generator. This decoupled framework alleviates the training complexity of TTS and allows each stage to focus on 1) linguistic and alignment modeling and 2) fine-grained acoustic modeling, respectively. Experimental results on the zero-shot adaptive TTS show that the proposed model exceeds the baselines in speech quality and speaker similarity via objective and subjective measures. We also investigate the inference speed and prosody controllability of our proposed model, showing the potential of the neural transducer for TTS frameworks.",
      "authors": [
        "Minchan Kim",
        "Myeonghun Jeong",
        "Byoung Jin Choi",
        "Dongjune Lee",
        "Nam Soo Kim"
      ],
      "published": "2023-11-06T06:13:39Z",
      "updated": "2023-11-08T05:52:39Z",
      "categories": [
        "eess.AS",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2311.02898v2",
      "landing_url": "https://arxiv.org/abs/2311.02898v2",
      "doi": "https://doi.org/10.48550/arXiv.2311.02898"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The work explicitly employs discretized semantic tokens as a core component (semantic token prediction from wav2vec2.0 embeddings) for text-to-speech, aligning with the discrete-tokenization inclusion criterion and not relying solely on continuous features.",
      "evaluation": 5
    },
    "round-A_JuniorNano_reasoning": "The work explicitly employs discretized semantic tokens as a core component (semantic token prediction from wav2vec2.0 embeddings) for text-to-speech, aligning with the discrete-tokenization inclusion criterion and not relying solely on continuous features.",
    "round-A_JuniorNano_evaluation": 5,
    "round-A_JuniorMini_output": {
      "reasoning": "The study explicitly proposes and evaluates discrete semantic tokens derived from wav2vec2.0 embeddings as a core part of a text-to-speech framework, satisfying the inclusion criteria of analyzing discrete audio tokenization; it does not fall into any exclusion categories, making it a suitable candidate for inclusion.",
      "evaluation": 5
    },
    "round-A_JuniorMini_reasoning": "The study explicitly proposes and evaluates discrete semantic tokens derived from wav2vec2.0 embeddings as a core part of a text-to-speech framework, satisfying the inclusion criteria of analyzing discrete audio tokenization; it does not fall into any exclusion categories, making it a suitable candidate for inclusion.",
    "round-A_JuniorMini_evaluation": 5,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "include (junior:5)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Selective HuBERT: Self-Supervised Pre-Training for Target Speaker in Clean and Mixture Speech",
    "abstract": "Self-supervised pre-trained speech models were shown effective for various downstream speech processing tasks. Since they are mainly pre-trained to map input speech to pseudo-labels, the resulting representations are only effective for the type of pre-train data used, either clean or mixture speech. With the idea of selective auditory attention, we propose a novel pre-training solution called Selective-HuBERT, or SHuBERT, which learns the selective extraction of target speech representations from either clean or mixture speech. Specifically, SHuBERT is trained to predict pseudo labels of a target speaker, conditioned on an enrolled speech from the target speaker. By doing so, SHuBERT is expected to selectively attend to the target speaker in a complex acoustic environment, thus benefiting various downstream tasks. We further introduce a dual-path training strategy and use the cross-correlation constraint between the two branches to encourage the model to generate noise-invariant representation. Experiments on SUPERB benchmark and LibriMix dataset demonstrate the universality and noise-robustness of SHuBERT. Furthermore, we find that our high-quality representation can be easily integrated with conventional supervised learning methods to achieve significant performance, even under extremely low-resource labeled data.",
    "metadata": {
      "arxiv_id": "2311.04526",
      "title": "Selective HuBERT: Self-Supervised Pre-Training for Target Speaker in Clean and Mixture Speech",
      "summary": "Self-supervised pre-trained speech models were shown effective for various downstream speech processing tasks. Since they are mainly pre-trained to map input speech to pseudo-labels, the resulting representations are only effective for the type of pre-train data used, either clean or mixture speech. With the idea of selective auditory attention, we propose a novel pre-training solution called Selective-HuBERT, or SHuBERT, which learns the selective extraction of target speech representations from either clean or mixture speech. Specifically, SHuBERT is trained to predict pseudo labels of a target speaker, conditioned on an enrolled speech from the target speaker. By doing so, SHuBERT is expected to selectively attend to the target speaker in a complex acoustic environment, thus benefiting various downstream tasks. We further introduce a dual-path training strategy and use the cross-correlation constraint between the two branches to encourage the model to generate noise-invariant representation. Experiments on SUPERB benchmark and LibriMix dataset demonstrate the universality and noise-robustness of SHuBERT. Furthermore, we find that our high-quality representation can be easily integrated with conventional supervised learning methods to achieve significant performance, even under extremely low-resource labeled data.",
      "authors": [
        "Jingru Lin",
        "Meng Ge",
        "Wupeng Wang",
        "Haizhou Li",
        "Mengling Feng"
      ],
      "published": "2023-11-08T08:28:25Z",
      "updated": "2023-11-08T08:28:25Z",
      "categories": [
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2311.04526v1",
      "landing_url": "https://arxiv.org/abs/2311.04526v1",
      "doi": "https://doi.org/10.48550/arXiv.2311.04526"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "Because the item centers on self-supervised target-speaker representations with pseudo labels but does not discuss discrete audio tokenization or a quantized token vocabulary, it does not meet the inclusion criteria for discrete tokenization and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "Because the item centers on self-supervised target-speaker representations with pseudo labels but does not discuss discrete audio tokenization or a quantized token vocabulary, it does not meet the inclusion criteria for discrete tokenization and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on self-supervised pre-training of speech representations conditioned on target speaker speech, aiming at selective extraction in clean and mixture speech. However, the abstract does not explicitly mention the design, analysis, or evaluation of discrete audio/speech tokenization or discrete quantization to convert continuous audio into discrete token sequences, which is a required inclusion criterion. It mostly discusses representation learning and noise-robustness rather than discrete tokenization methods. Therefore, it does not meet the core inclusion criterion.",
      "evaluation": 2
    },
    "round-A_JuniorMini_reasoning": "The study focuses on self-supervised pre-training of speech representations conditioned on target speaker speech, aiming at selective extraction in clean and mixture speech. However, the abstract does not explicitly mention the design, analysis, or evaluation of discrete audio/speech tokenization or discrete quantization to convert continuous audio into discrete token sequences, which is a required inclusion criterion. It mostly discusses representation learning and noise-robustness rather than discrete tokenization methods. Therefore, it does not meet the core inclusion criterion.",
    "round-A_JuniorMini_evaluation": 2,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:2)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:2)"
  },
  {
    "title": "Loss Masking Is Not Needed in Decoder-only Transformer for Discrete-token-based ASR",
    "abstract": "Recently, unified speech-text models, such as SpeechGPT, VioLA, and AudioPaLM, have achieved remarkable performance on various speech tasks. These models discretize speech signals into tokens (speech discretization) and use a shared vocabulary for both text and speech tokens. Then they train a single decoder-only Transformer on a mixture of speech tasks. However, these models rely on the Loss Masking strategy for the ASR task, which ignores the dependency among speech tokens. In this paper, we propose to model speech tokens in an autoregressive way, similar to text. We find that applying the conventional cross-entropy loss on input speech tokens does not consistently improve the ASR performance over the Loss Masking approach. To address this issue, we propose a novel approach denoted Smoothed Label Distillation (SLD), which applies a KL divergence loss with smoothed labels on speech tokens. Our experiments show that SLD effectively models speech tokens and outperforms Loss Masking for decoder-only Transformers in ASR tasks with different speech discretization methods. The source code can be found here: https://github.com/alibaba-damo-academy/SpokenNLP/tree/main/sld",
    "metadata": {
      "arxiv_id": "2311.04534",
      "title": "Loss Masking Is Not Needed in Decoder-only Transformer for Discrete-token-based ASR",
      "summary": "Recently, unified speech-text models, such as SpeechGPT, VioLA, and AudioPaLM, have achieved remarkable performance on various speech tasks. These models discretize speech signals into tokens (speech discretization) and use a shared vocabulary for both text and speech tokens. Then they train a single decoder-only Transformer on a mixture of speech tasks. However, these models rely on the Loss Masking strategy for the ASR task, which ignores the dependency among speech tokens. In this paper, we propose to model speech tokens in an autoregressive way, similar to text. We find that applying the conventional cross-entropy loss on input speech tokens does not consistently improve the ASR performance over the Loss Masking approach. To address this issue, we propose a novel approach denoted Smoothed Label Distillation (SLD), which applies a KL divergence loss with smoothed labels on speech tokens. Our experiments show that SLD effectively models speech tokens and outperforms Loss Masking for decoder-only Transformers in ASR tasks with different speech discretization methods. The source code can be found here: https://github.com/alibaba-damo-academy/SpokenNLP/tree/main/sld",
      "authors": [
        "Qian Chen",
        "Wen Wang",
        "Qinglin Zhang",
        "Siqi Zheng",
        "Shiliang Zhang",
        "Chong Deng",
        "Yukun Ma",
        "Hai Yu",
        "Jiaqing Liu",
        "Chong Zhang"
      ],
      "published": "2023-11-08T08:45:14Z",
      "updated": "2024-02-05T02:42:57Z",
      "categories": [
        "cs.CL",
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2311.04534v2",
      "landing_url": "https://arxiv.org/abs/2311.04534v2",
      "doi": "https://doi.org/10.48550/arXiv.2311.04534"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The work centers on discrete speech tokenization and token-based modeling (SLD for speech tokens) in ASR, meeting inclusion and not falling under the exclusion criteria.",
      "evaluation": 5
    },
    "round-A_JuniorNano_reasoning": "The work centers on discrete speech tokenization and token-based modeling (SLD for speech tokens) in ASR, meeting inclusion and not falling under the exclusion criteria.",
    "round-A_JuniorNano_evaluation": 5,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on discrete tokenization of speech for ASR tasks and proposes a novel methodological contribution (Smoothed Label Distillation) to improve modeling speech tokens in decoder-only Transformers, meeting all inclusion criteria and none of the exclusion criteria.",
      "evaluation": 5
    },
    "round-A_JuniorMini_reasoning": "The study focuses on discrete tokenization of speech for ASR tasks and proposes a novel methodological contribution (Smoothed Label Distillation) to improve modeling speech tokens in decoder-only Transformers, meeting all inclusion criteria and none of the exclusion criteria.",
    "round-A_JuniorMini_evaluation": 5,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "include (junior:5)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Shifting to Machine Supervision: Annotation-Efficient Semi and Self-Supervised Learning for Automatic Medical Image Segmentation and Classification",
    "abstract": "Advancements in clinical treatment are increasingly constrained by the limitations of supervised learning techniques, which depend heavily on large volumes of annotated data. The annotation process is not only costly but also demands substantial time from clinical specialists. Addressing this issue, we introduce the S4MI (Self-Supervision and Semi-Supervision for Medical Imaging) pipeline, a novel approach that leverages advancements in self-supervised and semi-supervised learning. These techniques engage in auxiliary tasks that do not require labeling, thus simplifying the scaling of machine supervision compared to fully-supervised methods. Our study benchmarks these techniques on three distinct medical imaging datasets to evaluate their effectiveness in classification and segmentation tasks. Notably, we observed that self supervised learning significantly surpassed the performance of supervised methods in the classification of all evaluated datasets. Remarkably, the semi-supervised approach demonstrated superior outcomes in segmentation, outperforming fully-supervised methods while using 50% fewer labels across all datasets. In line with our commitment to contributing to the scientific community, we have made the S4MI code openly accessible, allowing for broader application and further development of these methods.",
    "metadata": {
      "arxiv_id": "2311.10319",
      "title": "Shifting to Machine Supervision: Annotation-Efficient Semi and Self-Supervised Learning for Automatic Medical Image Segmentation and Classification",
      "summary": "Advancements in clinical treatment are increasingly constrained by the limitations of supervised learning techniques, which depend heavily on large volumes of annotated data. The annotation process is not only costly but also demands substantial time from clinical specialists. Addressing this issue, we introduce the S4MI (Self-Supervision and Semi-Supervision for Medical Imaging) pipeline, a novel approach that leverages advancements in self-supervised and semi-supervised learning. These techniques engage in auxiliary tasks that do not require labeling, thus simplifying the scaling of machine supervision compared to fully-supervised methods. Our study benchmarks these techniques on three distinct medical imaging datasets to evaluate their effectiveness in classification and segmentation tasks. Notably, we observed that self supervised learning significantly surpassed the performance of supervised methods in the classification of all evaluated datasets. Remarkably, the semi-supervised approach demonstrated superior outcomes in segmentation, outperforming fully-supervised methods while using 50% fewer labels across all datasets. In line with our commitment to contributing to the scientific community, we have made the S4MI code openly accessible, allowing for broader application and further development of these methods.",
      "authors": [
        "Pranav Singh",
        "Raviteja Chukkapalli",
        "Shravan Chaudhari",
        "Luoyao Chen",
        "Mei Chen",
        "Jinqian Pan",
        "Craig Smuda",
        "Jacopo Cirrone"
      ],
      "published": "2023-11-17T04:04:29Z",
      "updated": "2024-05-17T17:42:30Z",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2311.10319v6",
      "landing_url": "https://arxiv.org/abs/2311.10319v6",
      "doi": "https://doi.org/10.1038/s41598-024-61822-9"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item is about self-supervised and semi-supervised learning for medical image segmentation with no discrete audio tokenization; inclusion criteria require explicit discrete audio/speech tokenization as core focus, so this item should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item is about self-supervised and semi-supervised learning for medical image segmentation with no discrete audio tokenization; inclusion criteria require explicit discrete audio/speech tokenization as core focus, so this item should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on medical image segmentation and classification using self-supervised and semi-supervised learning techniques, without any mention of audio or speech tokenization, discrete audio token analysis, or related tasks as outlined in the inclusion criteria. Therefore, it does not meet the core requirement of analyzing discrete audio/speech tokenization.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on medical image segmentation and classification using self-supervised and semi-supervised learning techniques, without any mention of audio or speech tokenization, discrete audio token analysis, or related tasks as outlined in the inclusion criteria. Therefore, it does not meet the core requirement of analyzing discrete audio/speech tokenization.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "LOSTU: Fast, Scalable, and Uncertainty-Aware Triangulation",
    "abstract": "This work proposes a non-iterative, scalable, and statistically optimal way to triangulate called \\texttt{LOSTU}. Unlike triangulation algorithms that minimize the reprojection ($L_2$) error, LOSTU will still provide the maximum likelihood estimate when there are errors in camera pose or parameters. This generic framework is used to contextualize other triangulation methods like the direct linear transform (DLT) or the midpoint. Synthetic experiments show that LOSTU can be substantially faster than using uncertainty-aware Levenberg-Marquardt (or similar) optimization schemes, while providing results of comparable precision. Finally, LOSTU is implemented in sequential reconstruction in conjunction with uncertainty-aware pose estimation, where it yields better reconstruction metrics.",
    "metadata": {
      "arxiv_id": "2311.11171",
      "title": "LOSTU: Fast, Scalable, and Uncertainty-Aware Triangulation",
      "summary": "This work proposes a non-iterative, scalable, and statistically optimal way to triangulate called \\texttt{LOSTU}. Unlike triangulation algorithms that minimize the reprojection ($L_2$) error, LOSTU will still provide the maximum likelihood estimate when there are errors in camera pose or parameters. This generic framework is used to contextualize other triangulation methods like the direct linear transform (DLT) or the midpoint. Synthetic experiments show that LOSTU can be substantially faster than using uncertainty-aware Levenberg-Marquardt (or similar) optimization schemes, while providing results of comparable precision. Finally, LOSTU is implemented in sequential reconstruction in conjunction with uncertainty-aware pose estimation, where it yields better reconstruction metrics.",
      "authors": [
        "Sébastien Henry",
        "John A. Christian"
      ],
      "published": "2023-11-18T21:27:04Z",
      "updated": "2024-03-18T02:12:44Z",
      "categories": [
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2311.11171v2",
      "landing_url": "https://arxiv.org/abs/2311.11171v2",
      "doi": "https://doi.org/10.48550/arXiv.2311.11171"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This item is about fast triangulation in computer vision and has no discrete audio/speech tokenization; it does not meet the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "This item is about fast triangulation in computer vision and has no discrete audio/speech tokenization; it does not meet the inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study LOSTU focuses on triangulation methods for 3D reconstruction and does not address discrete audio/speech tokenization or any related audio processing tasks; thus, it does not meet the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study LOSTU focuses on triangulation methods for 3D reconstruction and does not address discrete audio/speech tokenization or any related audio processing tasks; thus, it does not meet the inclusion criteria.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Self-Distilled Representation Learning for Time Series",
    "abstract": "Self-supervised learning for time-series data holds potential similar to that recently unleashed in Natural Language Processing and Computer Vision. While most existing works in this area focus on contrastive learning, we propose a conceptually simple yet powerful non-contrastive approach, based on the data2vec self-distillation framework. The core of our method is a student-teacher scheme that predicts the latent representation of an input time series from masked views of the same time series. This strategy avoids strong modality-specific assumptions and biases typically introduced by the design of contrastive sample pairs. We demonstrate the competitiveness of our approach for classification and forecasting as downstream tasks, comparing with state-of-the-art self-supervised learning methods on the UCR and UEA archives as well as the ETT and Electricity datasets.",
    "metadata": {
      "arxiv_id": "2311.11335",
      "title": "Self-Distilled Representation Learning for Time Series",
      "summary": "Self-supervised learning for time-series data holds potential similar to that recently unleashed in Natural Language Processing and Computer Vision. While most existing works in this area focus on contrastive learning, we propose a conceptually simple yet powerful non-contrastive approach, based on the data2vec self-distillation framework. The core of our method is a student-teacher scheme that predicts the latent representation of an input time series from masked views of the same time series. This strategy avoids strong modality-specific assumptions and biases typically introduced by the design of contrastive sample pairs. We demonstrate the competitiveness of our approach for classification and forecasting as downstream tasks, comparing with state-of-the-art self-supervised learning methods on the UCR and UEA archives as well as the ETT and Electricity datasets.",
      "authors": [
        "Felix Pieper",
        "Konstantin Ditschuneit",
        "Martin Genzel",
        "Alexandra Lindt",
        "Johannes Otterbach"
      ],
      "published": "2023-11-19T14:34:01Z",
      "updated": "2023-11-19T14:34:01Z",
      "categories": [
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2311.11335v1",
      "landing_url": "https://arxiv.org/abs/2311.11335v1",
      "doi": "https://doi.org/10.48550/arXiv.2311.11335"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item focuses on self-supervised learning for time-series with a student–teacher distillation framework and does not discuss discrete audio/speech tokenization or discrete token representations; since inclusion requires explicit discrete audio tokenization as the core object, this item does not meet the criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item focuses on self-supervised learning for time-series with a student–teacher distillation framework and does not discuss discrete audio/speech tokenization or discrete token representations; since inclusion requires explicit discrete audio tokenization as the core object, this item does not meet the criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on self-supervised learning for time-series data using a self-distillation framework, with applications on time series classification and forecasting datasets, and does not mention any discrete audio/speech tokenization or related quantization-based discrete token representations; therefore, it does not meet the inclusion criteria related to discrete audio/speech tokenization.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on self-supervised learning for time-series data using a self-distillation framework, with applications on time series classification and forecasting datasets, and does not mention any discrete audio/speech tokenization or related quantization-based discrete token representations; therefore, it does not meet the inclusion criteria related to discrete audio/speech tokenization.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "GP-NeRF: Generalized Perception NeRF for Context-Aware 3D Scene Understanding",
    "abstract": "Applying NeRF to downstream perception tasks for scene understanding and representation is becoming increasingly popular. Most existing methods treat semantic prediction as an additional rendering task, \\textit{i.e.}, the \"label rendering\" task, to build semantic NeRFs. However, by rendering semantic/instance labels per pixel without considering the contextual information of the rendered image, these methods usually suffer from unclear boundary segmentation and abnormal segmentation of pixels within an object. To solve this problem, we propose Generalized Perception NeRF (GP-NeRF), a novel pipeline that makes the widely used segmentation model and NeRF work compatibly under a unified framework, for facilitating context-aware 3D scene perception. To accomplish this goal, we introduce transformers to aggregate radiance as well as semantic embedding fields jointly for novel views and facilitate the joint volumetric rendering of both fields. In addition, we propose two self-distillation mechanisms, i.e., the Semantic Distill Loss and the Depth-Guided Semantic Distill Loss, to enhance the discrimination and quality of the semantic field and the maintenance of geometric consistency. In evaluation, we conduct experimental comparisons under two perception tasks (\\textit{i.e.} semantic and instance segmentation) using both synthetic and real-world datasets. Notably, our method outperforms SOTA approaches by 6.94\\%, 11.76\\%, and 8.47\\% on generalized semantic segmentation, finetuning semantic segmentation, and instance segmentation, respectively.",
    "metadata": {
      "arxiv_id": "2311.11863",
      "title": "GP-NeRF: Generalized Perception NeRF for Context-Aware 3D Scene Understanding",
      "summary": "Applying NeRF to downstream perception tasks for scene understanding and representation is becoming increasingly popular. Most existing methods treat semantic prediction as an additional rendering task, \\textit{i.e.}, the \"label rendering\" task, to build semantic NeRFs. However, by rendering semantic/instance labels per pixel without considering the contextual information of the rendered image, these methods usually suffer from unclear boundary segmentation and abnormal segmentation of pixels within an object. To solve this problem, we propose Generalized Perception NeRF (GP-NeRF), a novel pipeline that makes the widely used segmentation model and NeRF work compatibly under a unified framework, for facilitating context-aware 3D scene perception. To accomplish this goal, we introduce transformers to aggregate radiance as well as semantic embedding fields jointly for novel views and facilitate the joint volumetric rendering of both fields. In addition, we propose two self-distillation mechanisms, i.e., the Semantic Distill Loss and the Depth-Guided Semantic Distill Loss, to enhance the discrimination and quality of the semantic field and the maintenance of geometric consistency. In evaluation, we conduct experimental comparisons under two perception tasks (\\textit{i.e.} semantic and instance segmentation) using both synthetic and real-world datasets. Notably, our method outperforms SOTA approaches by 6.94\\%, 11.76\\%, and 8.47\\% on generalized semantic segmentation, finetuning semantic segmentation, and instance segmentation, respectively.",
      "authors": [
        "Hao Li",
        "Dingwen Zhang",
        "Yalun Dai",
        "Nian Liu",
        "Lechao Cheng",
        "Jingfeng Li",
        "Jingdong Wang",
        "Junwei Han"
      ],
      "published": "2023-11-20T15:59:41Z",
      "updated": "2024-04-07T07:37:15Z",
      "categories": [
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2311.11863v2",
      "landing_url": "https://arxiv.org/abs/2311.11863v2",
      "doi": "https://doi.org/10.48550/arXiv.2311.11863"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item is about GP-NeRF for context-aware 3D scene understanding with no discrete audio tokenization or speech-token analysis, hence it does not meet the audio tokenization inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item is about GP-NeRF for context-aware 3D scene understanding with no discrete audio tokenization or speech-token analysis, hence it does not meet the audio tokenization inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on a NeRF-based approach for 3D scene understanding involving semantic and instance segmentation, with no mention of discrete audio or speech tokenization, discrete symbol conversion from audio signals, nor related neural encoding for audio reconstruction or generation; thus, it does not meet the inclusion criteria related to audio tokenization and instead falls outside the scope.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on a NeRF-based approach for 3D scene understanding involving semantic and instance segmentation, with no mention of discrete audio or speech tokenization, discrete symbol conversion from audio signals, nor related neural encoding for audio reconstruction or generation; thus, it does not meet the inclusion criteria related to audio tokenization and instead falls outside the scope.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "User-guided Page Merging for Memory Deduplication in Serverless Systems",
    "abstract": "Serverless computing is an emerging cloud paradigm that offers an elastic and scalable allocation of computing resources with pay-as-you-go billing. In the Function-as-a-Service (FaaS) programming model, applications comprise short-lived and stateless serverless functions executed in isolated containers or microVMs, which can quickly scale to thousands of instances and process terabytes of data. This flexibility comes at the cost of duplicated runtimes, libraries, and user data spread across many function instances, and cloud providers do not utilize this redundancy. The memory footprint of serverless forces removing idle containers to make space for new ones, which decreases performance through more cold starts and fewer data caching opportunities. We address this issue by proposing deduplicating memory pages of serverless workers with identical content, based on the content-based page-sharing concept of Linux Kernel Same-page Merging (KSM). We replace the background memory scanning process of KSM, as it is too slow to locate sharing candidates in short-lived functions. Instead, we design User-Guided Page Merging (UPM), a built-in Linux kernel module that leverages the madvise system call: we enable users to advise the kernel of memory areas that can be shared with others. We show that UPM reduces memory consumption by up to 55% on 16 concurrent containers executing a typical image recognition function, more than doubling the density for containers of the same function that can run on a system.",
    "metadata": {
      "arxiv_id": "2311.13588",
      "title": "User-guided Page Merging for Memory Deduplication in Serverless Systems",
      "summary": "Serverless computing is an emerging cloud paradigm that offers an elastic and scalable allocation of computing resources with pay-as-you-go billing. In the Function-as-a-Service (FaaS) programming model, applications comprise short-lived and stateless serverless functions executed in isolated containers or microVMs, which can quickly scale to thousands of instances and process terabytes of data. This flexibility comes at the cost of duplicated runtimes, libraries, and user data spread across many function instances, and cloud providers do not utilize this redundancy. The memory footprint of serverless forces removing idle containers to make space for new ones, which decreases performance through more cold starts and fewer data caching opportunities. We address this issue by proposing deduplicating memory pages of serverless workers with identical content, based on the content-based page-sharing concept of Linux Kernel Same-page Merging (KSM). We replace the background memory scanning process of KSM, as it is too slow to locate sharing candidates in short-lived functions. Instead, we design User-Guided Page Merging (UPM), a built-in Linux kernel module that leverages the madvise system call: we enable users to advise the kernel of memory areas that can be shared with others. We show that UPM reduces memory consumption by up to 55% on 16 concurrent containers executing a typical image recognition function, more than doubling the density for containers of the same function that can run on a system.",
      "authors": [
        "Wei Qiu",
        "Marcin Copik",
        "Yun Wang",
        "Alexandru Calotoiu",
        "Torsten Hoefler"
      ],
      "published": "2023-11-22T18:49:00Z",
      "updated": "2023-11-22T18:49:00Z",
      "categories": [
        "cs.DC"
      ],
      "pdf_url": "https://arxiv.org/pdf/2311.13588v1",
      "landing_url": "https://arxiv.org/abs/2311.13588v1",
      "doi": "https://doi.org/10.48550/arXiv.2311.13588"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This paper addresses kernel-level memory deduplication for serverless systems, not discrete audio/speech tokenization or token-based speech representations, so it fails all inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "This paper addresses kernel-level memory deduplication for serverless systems, not discrete audio/speech tokenization or token-based speech representations, so it fails all inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on memory deduplication techniques in serverless computing, specifically addressing memory usage optimization in containerized environments. It does not discuss or propose any discrete audio or speech tokenization methods, nor does it analyze audio signal conversion into discrete tokens. Therefore, it does not meet the inclusion criteria related to audio tokenization and is unrelated to the evaluation focus.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on memory deduplication techniques in serverless computing, specifically addressing memory usage optimization in containerized environments. It does not discuss or propose any discrete audio or speech tokenization methods, nor does it analyze audio signal conversion into discrete tokens. Therefore, it does not meet the inclusion criteria related to audio tokenization and is unrelated to the evaluation focus.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Making Self-supervised Learning Robust to Spurious Correlation via Learning-speed Aware Sampling",
    "abstract": "Self-supervised learning (SSL) has emerged as a powerful technique for learning rich representations from unlabeled data. The data representations are able to capture many underlying attributes of data, and be useful in downstream prediction tasks. In real-world settings, spurious correlations between some attributes (e.g. race, gender and age) and labels for downstream tasks often exist, e.g. cancer is usually more prevalent among elderly patients. In this paper, we investigate SSL in the presence of spurious correlations and show that the SSL training loss can be minimized by capturing only a subset of the conspicuous features relevant to those sensitive attributes, despite the presence of other important predictive features for the downstream tasks. To address this issue, we investigate the learning dynamics of SSL and observe that the learning is slower for samples that conflict with such correlations (e.g. elder patients without cancer). Motivated by these findings, we propose a learning-speed aware SSL (LA-SSL) approach, in which we sample each training data with a probability that is inversely related to its learning speed. We evaluate LA-SSL on three datasets that exhibit spurious correlations between different attributes, demonstrating that it improves the robustness of pretrained representations on downstream classification tasks.",
    "metadata": {
      "arxiv_id": "2311.16361",
      "title": "Making Self-supervised Learning Robust to Spurious Correlation via Learning-speed Aware Sampling",
      "summary": "Self-supervised learning (SSL) has emerged as a powerful technique for learning rich representations from unlabeled data. The data representations are able to capture many underlying attributes of data, and be useful in downstream prediction tasks. In real-world settings, spurious correlations between some attributes (e.g. race, gender and age) and labels for downstream tasks often exist, e.g. cancer is usually more prevalent among elderly patients. In this paper, we investigate SSL in the presence of spurious correlations and show that the SSL training loss can be minimized by capturing only a subset of the conspicuous features relevant to those sensitive attributes, despite the presence of other important predictive features for the downstream tasks. To address this issue, we investigate the learning dynamics of SSL and observe that the learning is slower for samples that conflict with such correlations (e.g. elder patients without cancer). Motivated by these findings, we propose a learning-speed aware SSL (LA-SSL) approach, in which we sample each training data with a probability that is inversely related to its learning speed. We evaluate LA-SSL on three datasets that exhibit spurious correlations between different attributes, demonstrating that it improves the robustness of pretrained representations on downstream classification tasks.",
      "authors": [
        "Weicheng Zhu",
        "Sheng Liu",
        "Carlos Fernandez-Granda",
        "Narges Razavian"
      ],
      "published": "2023-11-27T22:52:45Z",
      "updated": "2023-11-29T23:19:30Z",
      "categories": [
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2311.16361v2",
      "landing_url": "https://arxiv.org/abs/2311.16361v2",
      "doi": "https://doi.org/10.48550/arXiv.2311.16361"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item focuses on self-supervised learning robustness to spurious correlations and data sampling, with no emphasis on discrete audio/speech tokenization or a system that converts audio signals into discrete tokens for reconstruction, generation, or understanding; thus it does not meet the discrete audio tokenization inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item focuses on self-supervised learning robustness to spurious correlations and data sampling, with no emphasis on discrete audio/speech tokenization or a system that converts audio signals into discrete tokens for reconstruction, generation, or understanding; thus it does not meet the discrete audio tokenization inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on improving self-supervised learning robustness to spurious correlations through a learning-speed aware sampling method, but it does not discuss or analyze discrete audio/speech tokenization, nor does it address any aspects related to converting continuous audio signals into discrete tokens for reconstruction or generation tasks, which are core criteria for inclusion.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on improving self-supervised learning robustness to spurious correlations through a learning-speed aware sampling method, but it does not discuss or analyze discrete audio/speech tokenization, nor does it address any aspects related to converting continuous audio signals into discrete tokens for reconstruction or generation tasks, which are core criteria for inclusion.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "RETSim: Resilient and Efficient Text Similarity",
    "abstract": "This paper introduces RETSim (Resilient and Efficient Text Similarity), a lightweight, multilingual deep learning model trained to produce robust metric embeddings for near-duplicate text retrieval, clustering, and dataset deduplication tasks. We demonstrate that RETSim is significantly more robust and accurate than MinHash and neural text embeddings, achieving new state-of-the-art performance on dataset deduplication, adversarial text retrieval benchmarks, and spam clustering tasks. We also introduce the W4NT3D benchmark (Wiki-40B 4dversarial Near-T3xt Dataset) for evaluating multilingual, near-duplicate text retrieval capabilities under adversarial settings. RETSim and the W4NT3D benchmark are open-sourced under the MIT License at https://github.com/google/unisim.",
    "metadata": {
      "arxiv_id": "2311.17264",
      "title": "RETSim: Resilient and Efficient Text Similarity",
      "summary": "This paper introduces RETSim (Resilient and Efficient Text Similarity), a lightweight, multilingual deep learning model trained to produce robust metric embeddings for near-duplicate text retrieval, clustering, and dataset deduplication tasks. We demonstrate that RETSim is significantly more robust and accurate than MinHash and neural text embeddings, achieving new state-of-the-art performance on dataset deduplication, adversarial text retrieval benchmarks, and spam clustering tasks. We also introduce the W4NT3D benchmark (Wiki-40B 4dversarial Near-T3xt Dataset) for evaluating multilingual, near-duplicate text retrieval capabilities under adversarial settings. RETSim and the W4NT3D benchmark are open-sourced under the MIT License at https://github.com/google/unisim.",
      "authors": [
        "Marina Zhang",
        "Owen Vallis",
        "Aysegul Bumin",
        "Tanay Vakharia",
        "Elie Bursztein"
      ],
      "published": "2023-11-28T22:54:33Z",
      "updated": "2023-11-28T22:54:33Z",
      "categories": [
        "cs.CL"
      ],
      "pdf_url": "https://arxiv.org/pdf/2311.17264v1",
      "landing_url": "https://arxiv.org/abs/2311.17264v1",
      "doi": "https://doi.org/10.48550/arXiv.2311.17264"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This item is about text similarity models and dataset deduplication, with no discrete audio tokenization or analysis of discrete audio representations, so it should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "This item is about text similarity models and dataset deduplication, with no discrete audio tokenization or analysis of discrete audio representations, so it should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on text similarity using deep learning models for textual data tasks such as duplicate detection, clustering, and retrieval, with no mention or analysis of discrete audio/speech tokenization or audio-related discrete token representations, which is the core inclusion criterion.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on text similarity using deep learning models for textual data tasks such as duplicate detection, clustering, and retrieval, with no mention or analysis of discrete audio/speech tokenization or audio-related discrete token representations, which is the core inclusion criterion.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "FAT-HuBERT: Front-end Adaptive Training of Hidden-unit BERT for Distortion-Invariant Robust Speech Recognition",
    "abstract": "Advancements in monaural speech enhancement (SE) techniques have greatly improved the perceptual quality of speech. However, integrating these techniques into automatic speech recognition (ASR) systems has not yielded the expected performance gains, primarily due to the introduction of distortions during the SE process. In this paper, we propose a novel approach called FAT-HuBERT, which leverages distortion-invariant self-supervised learning (SSL) to enhance the robustness of ASR. To address the distortions introduced by the SE frontends, we introduce layer-wise fusion modules that incorporate features extracted from both observed noisy signals and enhanced signals. During training, the SE frontend is randomly selected from a pool of models. We evaluate the performance of FAT-HuBERT on simulated noisy speech generated from LibriSpeech as well as real-world noisy speech from the CHiME-4 1-channel dataset. The experimental results demonstrate a significant relative reduction in word error rate (WER).",
    "metadata": {
      "arxiv_id": "2311.17790",
      "title": "FAT-HuBERT: Front-end Adaptive Training of Hidden-unit BERT for Distortion-Invariant Robust Speech Recognition",
      "summary": "Advancements in monaural speech enhancement (SE) techniques have greatly improved the perceptual quality of speech. However, integrating these techniques into automatic speech recognition (ASR) systems has not yielded the expected performance gains, primarily due to the introduction of distortions during the SE process. In this paper, we propose a novel approach called FAT-HuBERT, which leverages distortion-invariant self-supervised learning (SSL) to enhance the robustness of ASR. To address the distortions introduced by the SE frontends, we introduce layer-wise fusion modules that incorporate features extracted from both observed noisy signals and enhanced signals. During training, the SE frontend is randomly selected from a pool of models. We evaluate the performance of FAT-HuBERT on simulated noisy speech generated from LibriSpeech as well as real-world noisy speech from the CHiME-4 1-channel dataset. The experimental results demonstrate a significant relative reduction in word error rate (WER).",
      "authors": [
        "Dongning Yang",
        "Wei Wang",
        "Yanmin Qian"
      ],
      "published": "2023-11-29T16:35:13Z",
      "updated": "2023-11-29T16:35:13Z",
      "categories": [
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2311.17790v1",
      "landing_url": "https://arxiv.org/abs/2311.17790v1",
      "doi": "https://doi.org/10.48550/arXiv.2311.17790"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The abstract describes distortion-invariant self-supervised learning for robust ASR and front-end adaptive training, but it does not explicitly propose, analyze, or evaluate discrete audio-tokenization as a core object.",
      "evaluation": 2
    },
    "round-A_JuniorNano_reasoning": "The abstract describes distortion-invariant self-supervised learning for robust ASR and front-end adaptive training, but it does not explicitly propose, analyze, or evaluate discrete audio-tokenization as a core object.",
    "round-A_JuniorNano_evaluation": 2,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on a robust speech recognition approach leveraging distortion-invariant self-supervised learning and fusion of noisy and enhanced speech features, but it does not explicitly propose, analyze, or evaluate discrete audio tokenization (i.e., conversion of continuous audio into discrete symbols) as required by the inclusion criteria. Instead, it appears to work primarily with continuous audio features without discrete token design or analysis relevant to tokenization-based speech tasks.",
      "evaluation": 2
    },
    "round-A_JuniorMini_reasoning": "The study focuses on a robust speech recognition approach leveraging distortion-invariant self-supervised learning and fusion of noisy and enhanced speech features, but it does not explicitly propose, analyze, or evaluate discrete audio tokenization (i.e., conversion of continuous audio into discrete symbols) as required by the inclusion criteria. Instead, it appears to work primarily with continuous audio features without discrete token design or analysis relevant to tokenization-based speech tasks.",
    "round-A_JuniorMini_evaluation": 2,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:2)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:2)"
  },
  {
    "title": "SASSL: Enhancing Self-Supervised Learning via Neural Style Transfer",
    "abstract": "Existing data augmentation in self-supervised learning, while diverse, fails to preserve the inherent structure of natural images. This results in distorted augmented samples with compromised semantic information, ultimately impacting downstream performance. To overcome this limitation, we propose SASSL: Style Augmentations for Self Supervised Learning, a novel data augmentation technique based on Neural Style Transfer. SASSL decouples semantic and stylistic attributes in images and applies transformations exclusively to their style while preserving content, generating diverse samples that better retain semantic information. SASSL boosts top-1 image classification accuracy on ImageNet by up to 2 percentage points compared to established self-supervised methods like MoCo, SimCLR, and BYOL, while achieving superior transfer learning performance across various datasets. Because SASSL can be performed asynchronously as part of the data augmentation pipeline, these performance impacts can be obtained with no change in pretraining throughput.",
    "metadata": {
      "arxiv_id": "2312.01187",
      "title": "SASSL: Enhancing Self-Supervised Learning via Neural Style Transfer",
      "summary": "Existing data augmentation in self-supervised learning, while diverse, fails to preserve the inherent structure of natural images. This results in distorted augmented samples with compromised semantic information, ultimately impacting downstream performance. To overcome this limitation, we propose SASSL: Style Augmentations for Self Supervised Learning, a novel data augmentation technique based on Neural Style Transfer. SASSL decouples semantic and stylistic attributes in images and applies transformations exclusively to their style while preserving content, generating diverse samples that better retain semantic information. SASSL boosts top-1 image classification accuracy on ImageNet by up to 2 percentage points compared to established self-supervised methods like MoCo, SimCLR, and BYOL, while achieving superior transfer learning performance across various datasets. Because SASSL can be performed asynchronously as part of the data augmentation pipeline, these performance impacts can be obtained with no change in pretraining throughput.",
      "authors": [
        "Renan A. Rojas-Gomez",
        "Karan Singhal",
        "Ali Etemad",
        "Alex Bijamov",
        "Warren R. Morningstar",
        "Philip Andrew Mansfield"
      ],
      "published": "2023-12-02T17:25:30Z",
      "updated": "2024-11-02T17:08:45Z",
      "categories": [
        "cs.CV",
        "cs.LG",
        "stat.ML"
      ],
      "pdf_url": "https://arxiv.org/pdf/2312.01187v4",
      "landing_url": "https://arxiv.org/abs/2312.01187v4",
      "doi": "https://doi.org/10.48550/arXiv.2312.01187"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item focuses on image data augmentation via Neural Style Transfer for self-supervised learning and has no discrete audio tokenization or speech-related analysis, so it does not meet the audio/tokenization inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item focuses on image data augmentation via Neural Style Transfer for self-supervised learning and has no discrete audio tokenization or speech-related analysis, so it does not meet the audio/tokenization inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on self-supervised learning and proposes a data augmentation method based on neural style transfer for image data, with no mention or analysis of discrete audio or speech tokenization, which is required by the inclusion criteria. Therefore, it does not meet the inclusion criteria related to audio tokenization and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on self-supervised learning and proposes a data augmentation method based on neural style transfer for image data, with no mention or analysis of discrete audio or speech tokenization, which is required by the inclusion criteria. Therefore, it does not meet the inclusion criteria related to audio tokenization and should be excluded.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Rejuvenating image-GPT as Strong Visual Representation Learners",
    "abstract": "This paper enhances image-GPT (iGPT), one of the pioneering works that introduce autoregressive pretraining to predict the next pixels for visual representation learning. Two simple yet essential changes are made. First, we shift the prediction target from raw pixels to semantic tokens, enabling a higher-level understanding of visual content. Second, we supplement the autoregressive modeling by instructing the model to predict not only the next tokens but also the visible tokens. This pipeline is particularly effective when semantic tokens are encoded by discriminatively trained models, such as CLIP. We introduce this novel approach as D-iGPT. Extensive experiments showcase that D-iGPT excels as a strong learner of visual representations: A notable achievement is its compelling performance on the ImageNet-1K dataset -- by training on publicly available datasets, D-iGPT unprecedentedly achieves \\textbf{90.0\\%} top-1 accuracy with a vanilla ViT-H. Additionally, D-iGPT shows strong generalization on the downstream task. Code is available at https://github.com/OliverRensu/D-iGPT.",
    "metadata": {
      "arxiv_id": "2312.02147",
      "title": "Rejuvenating image-GPT as Strong Visual Representation Learners",
      "summary": "This paper enhances image-GPT (iGPT), one of the pioneering works that introduce autoregressive pretraining to predict the next pixels for visual representation learning. Two simple yet essential changes are made. First, we shift the prediction target from raw pixels to semantic tokens, enabling a higher-level understanding of visual content. Second, we supplement the autoregressive modeling by instructing the model to predict not only the next tokens but also the visible tokens. This pipeline is particularly effective when semantic tokens are encoded by discriminatively trained models, such as CLIP. We introduce this novel approach as D-iGPT. Extensive experiments showcase that D-iGPT excels as a strong learner of visual representations: A notable achievement is its compelling performance on the ImageNet-1K dataset -- by training on publicly available datasets, D-iGPT unprecedentedly achieves \\textbf{90.0\\%} top-1 accuracy with a vanilla ViT-H. Additionally, D-iGPT shows strong generalization on the downstream task. Code is available at https://github.com/OliverRensu/D-iGPT.",
      "authors": [
        "Sucheng Ren",
        "Zeyu Wang",
        "Hongru Zhu",
        "Junfei Xiao",
        "Alan Yuille",
        "Cihang Xie"
      ],
      "published": "2023-12-04T18:59:20Z",
      "updated": "2024-07-05T05:07:08Z",
      "categories": [
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2312.02147v2",
      "landing_url": "https://arxiv.org/abs/2312.02147v2",
      "doi": "https://doi.org/10.48550/arXiv.2312.02147"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item centers on discrete visual semantic tokens for image modeling, not on discrete audio/speech tokenization, so it does not meet the inclusion criteria focused on audio tokenization.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item centers on discrete visual semantic tokens for image modeling, not on discrete audio/speech tokenization, so it does not meet the inclusion criteria focused on audio tokenization.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper focuses on visual representation learning by enhancing image-GPT with semantic token prediction for images and does not discuss discrete audio/speech tokenization or audio signal processing, thus it does not meet the inclusion criteria and falls under the exclusion criteria where audio is not the focus.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper focuses on visual representation learning by enhancing image-GPT with semantic token prediction for images and does not discuss discrete audio/speech tokenization or audio signal processing, thus it does not meet the inclusion criteria and falls under the exclusion criteria where audio is not the focus.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Does Vector Quantization Fail in Spatio-Temporal Forecasting? Exploring a Differentiable Sparse Soft-Vector Quantization Approach",
    "abstract": "Spatio-temporal forecasting is crucial in various fields and requires a careful balance between identifying subtle patterns and filtering out noise. Vector quantization (VQ) appears well-suited for this purpose, as it quantizes input vectors into a set of codebook vectors or patterns. Although VQ has shown promise in various computer vision tasks, it surprisingly falls short in enhancing the accuracy of spatio-temporal forecasting. We attribute this to two main issues: inaccurate optimization due to non-differentiability and limited representation power in hard-VQ. To tackle these challenges, we introduce Differentiable Sparse Soft-Vector Quantization (SVQ), the first VQ method to enhance spatio-temporal forecasting. SVQ balances detail preservation with noise reduction, offering full differentiability and a solid foundation in sparse regression. Our approach employs a two-layer MLP and an extensive codebook to streamline the sparse regression process, significantly cutting computational costs while simplifying training and improving performance. Empirical studies on five spatio-temporal benchmark datasets show SVQ achieves state-of-the-art results, including a 7.9% improvement on the WeatherBench-S temperature dataset and an average mean absolute error reduction of 9.4% in video prediction benchmarks (Human3.6M, KTH, and KittiCaltech), along with a 17.3% enhancement in image quality (LPIPS). Code is publicly available at https://github.com/Pachark/SVQ-Forecasting.",
    "metadata": {
      "arxiv_id": "2312.03406",
      "title": "Does Vector Quantization Fail in Spatio-Temporal Forecasting? Exploring a Differentiable Sparse Soft-Vector Quantization Approach",
      "summary": "Spatio-temporal forecasting is crucial in various fields and requires a careful balance between identifying subtle patterns and filtering out noise. Vector quantization (VQ) appears well-suited for this purpose, as it quantizes input vectors into a set of codebook vectors or patterns. Although VQ has shown promise in various computer vision tasks, it surprisingly falls short in enhancing the accuracy of spatio-temporal forecasting. We attribute this to two main issues: inaccurate optimization due to non-differentiability and limited representation power in hard-VQ. To tackle these challenges, we introduce Differentiable Sparse Soft-Vector Quantization (SVQ), the first VQ method to enhance spatio-temporal forecasting. SVQ balances detail preservation with noise reduction, offering full differentiability and a solid foundation in sparse regression. Our approach employs a two-layer MLP and an extensive codebook to streamline the sparse regression process, significantly cutting computational costs while simplifying training and improving performance. Empirical studies on five spatio-temporal benchmark datasets show SVQ achieves state-of-the-art results, including a 7.9% improvement on the WeatherBench-S temperature dataset and an average mean absolute error reduction of 9.4% in video prediction benchmarks (Human3.6M, KTH, and KittiCaltech), along with a 17.3% enhancement in image quality (LPIPS). Code is publicly available at https://github.com/Pachark/SVQ-Forecasting.",
      "authors": [
        "Chao Chen",
        "Tian Zhou",
        "Yanjun Zhao",
        "Hui Liu",
        "Liang Sun",
        "Rong Jin"
      ],
      "published": "2023-12-06T10:42:40Z",
      "updated": "2025-05-18T09:11:15Z",
      "categories": [
        "cs.CV",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2312.03406v4",
      "landing_url": "https://arxiv.org/abs/2312.03406v4",
      "doi": "https://doi.org/10.48550/arXiv.2312.03406"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item focuses on vector quantization for spatio-temporal forecasting and does not involve discrete audio/speech tokenization as a core object; therefore it does not meet the inclusion criteria and should be excluded (score 2).",
      "evaluation": 2
    },
    "round-A_JuniorNano_reasoning": "The item focuses on vector quantization for spatio-temporal forecasting and does not involve discrete audio/speech tokenization as a core object; therefore it does not meet the inclusion criteria and should be excluded (score 2).",
    "round-A_JuniorNano_evaluation": 2,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on vector quantization methods for spatio-temporal forecasting, with applications in temperature and video prediction, not discrete audio/speech tokenization or related tasks; it lacks any mention of audio tokenization or discrete representation of audio signals, and thus does not meet the inclusion criteria concerning discrete audio tokenization as a core subject.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on vector quantization methods for spatio-temporal forecasting, with applications in temperature and video prediction, not discrete audio/speech tokenization or related tasks; it lacks any mention of audio tokenization or discrete representation of audio signals, and thus does not meet the inclusion criteria concerning discrete audio tokenization as a core subject.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:2)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:2)"
  },
  {
    "title": "Novel class discovery meets foundation models for 3D semantic segmentation",
    "abstract": "The task of Novel Class Discovery (NCD) in semantic segmentation entails training a model able to accurately segment unlabelled (novel) classes, relying on the available supervision from annotated (base) classes. Although extensively investigated in 2D image data, the extension of the NCD task to the domain of 3D point clouds represents a pioneering effort, characterized by assumptions and challenges that are not present in the 2D case. This paper represents an advancement in the analysis of point cloud data in four directions. Firstly, it introduces the novel task of NCD for point cloud semantic segmentation. Secondly, it demonstrates that directly transposing the only existing NCD method for 2D image semantic segmentation to 3D data yields suboptimal results. Thirdly, a new NCD approach based on online clustering, uncertainty estimation, and semantic distillation is presented. Lastly, a novel evaluation protocol is proposed to rigorously assess the performance of NCD in point cloud semantic segmentation. Through comprehensive evaluations on the SemanticKITTI, SemanticPOSS, and S3DIS datasets, the paper demonstrates substantial superiority of the proposed method over the considered baselines.",
    "metadata": {
      "arxiv_id": "2312.03782",
      "title": "Novel class discovery meets foundation models for 3D semantic segmentation",
      "summary": "The task of Novel Class Discovery (NCD) in semantic segmentation entails training a model able to accurately segment unlabelled (novel) classes, relying on the available supervision from annotated (base) classes. Although extensively investigated in 2D image data, the extension of the NCD task to the domain of 3D point clouds represents a pioneering effort, characterized by assumptions and challenges that are not present in the 2D case. This paper represents an advancement in the analysis of point cloud data in four directions. Firstly, it introduces the novel task of NCD for point cloud semantic segmentation. Secondly, it demonstrates that directly transposing the only existing NCD method for 2D image semantic segmentation to 3D data yields suboptimal results. Thirdly, a new NCD approach based on online clustering, uncertainty estimation, and semantic distillation is presented. Lastly, a novel evaluation protocol is proposed to rigorously assess the performance of NCD in point cloud semantic segmentation. Through comprehensive evaluations on the SemanticKITTI, SemanticPOSS, and S3DIS datasets, the paper demonstrates substantial superiority of the proposed method over the considered baselines.",
      "authors": [
        "Luigi Riz",
        "Cristiano Saltori",
        "Yiming Wang",
        "Elisa Ricci",
        "Fabio Poiesi"
      ],
      "published": "2023-12-06T09:59:30Z",
      "updated": "2024-08-20T09:13:35Z",
      "categories": [
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2312.03782v2",
      "landing_url": "https://arxiv.org/abs/2312.03782v2",
      "doi": "https://doi.org/10.48550/arXiv.2312.03782"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item focuses on novel class discovery for 3D semantic segmentation and foundation models, with no discrete audio tokenization or analysis of discrete audio tokens, thus not meeting the inclusion criteria for discrete audio/speech tokenization.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item focuses on novel class discovery for 3D semantic segmentation and foundation models, with no discrete audio tokenization or analysis of discrete audio tokens, thus not meeting the inclusion criteria for discrete audio/speech tokenization.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on Novel Class Discovery for 3D semantic segmentation of point cloud data, without any mention or analysis of discrete audio or speech tokenization. It does not deal with audio signals, discrete tokenization of audio, or related speech tasks, thus failing the inclusion criteria and not violating the exclusion criteria as it is simply unrelated.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on Novel Class Discovery for 3D semantic segmentation of point cloud data, without any mention or analysis of discrete audio or speech tokenization. It does not deal with audio signals, discrete tokenization of audio, or related speech tasks, thus failing the inclusion criteria and not violating the exclusion criteria as it is simply unrelated.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "FASTEN: Towards a FAult-tolerant and STorage EfficieNt Cloud: Balancing Between Replication and Deduplication",
    "abstract": "With the surge in cloud storage adoption, enterprises face challenges managing data duplication and exponential data growth. Deduplication mitigates redundancy, yet maintaining redundancy ensures high availability, incurring storage costs. Balancing these aspects is a significant research concern. We propose FASTEN, a distributed cloud storage scheme ensuring efficiency, security, and high availability. FASTEN achieves fault tolerance by dispersing data subsets optimally across servers and maintains redundancy for high availability. Experimental results show FASTEN's effectiveness in fault tolerance, cost reduction, batch auditing, and file and block-level deduplication. It outperforms existing systems with low time complexity, strong fault tolerance, and commendable deduplication performance.",
    "metadata": {
      "arxiv_id": "2312.08309",
      "title": "FASTEN: Towards a FAult-tolerant and STorage EfficieNt Cloud: Balancing Between Replication and Deduplication",
      "summary": "With the surge in cloud storage adoption, enterprises face challenges managing data duplication and exponential data growth. Deduplication mitigates redundancy, yet maintaining redundancy ensures high availability, incurring storage costs. Balancing these aspects is a significant research concern. We propose FASTEN, a distributed cloud storage scheme ensuring efficiency, security, and high availability. FASTEN achieves fault tolerance by dispersing data subsets optimally across servers and maintains redundancy for high availability. Experimental results show FASTEN's effectiveness in fault tolerance, cost reduction, batch auditing, and file and block-level deduplication. It outperforms existing systems with low time complexity, strong fault tolerance, and commendable deduplication performance.",
      "authors": [
        "Sabbir Ahmed",
        "Md Nahiduzzaman",
        "Tariqul Islam",
        "Faisal Haque Bappy",
        "Tarannum Shaila Zaman",
        "Raiful Hasan"
      ],
      "published": "2023-12-13T17:27:17Z",
      "updated": "2023-12-13T17:27:17Z",
      "categories": [
        "cs.DC"
      ],
      "pdf_url": "https://arxiv.org/pdf/2312.08309v1",
      "landing_url": "https://arxiv.org/abs/2312.08309v1",
      "doi": "https://doi.org/10.1109/CCNC51664.2024.10454894"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item focuses on fault-tolerant cloud storage, replication, and deduplication, with no discrete audio/speech tokenization as a core object, so it does not meet the inclusion criteria for tokenization studies.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item focuses on fault-tolerant cloud storage, replication, and deduplication, with no discrete audio/speech tokenization as a core object, so it does not meet the inclusion criteria for tokenization studies.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The title and abstract focus on cloud storage optimization through fault tolerance and deduplication strategies, without any mention or analysis of discrete audio/speech tokenization or related audio signal processing methods required by the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The title and abstract focus on cloud storage optimization through fault tolerance and deduplication strategies, without any mention or analysis of discrete audio/speech tokenization or related audio signal processing methods required by the inclusion criteria.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "SEF-VC: Speaker Embedding Free Zero-Shot Voice Conversion with Cross Attention",
    "abstract": "Zero-shot voice conversion (VC) aims to transfer the source speaker timbre to arbitrary unseen target speaker timbre, while keeping the linguistic content unchanged. Although the voice of generated speech can be controlled by providing the speaker embedding of the target speaker, the speaker similarity still lags behind the ground truth recordings. In this paper, we propose SEF-VC, a speaker embedding free voice conversion model, which is designed to learn and incorporate speaker timbre from reference speech via a powerful position-agnostic cross-attention mechanism, and then reconstruct waveform from HuBERT semantic tokens in a non-autoregressive manner. The concise design of SEF-VC enhances its training stability and voice conversion performance. Objective and subjective evaluations demonstrate the superiority of SEF-VC to generate high-quality speech with better similarity to target reference than strong zero-shot VC baselines, even for very short reference speeches.",
    "metadata": {
      "arxiv_id": "2312.08676",
      "title": "SEF-VC: Speaker Embedding Free Zero-Shot Voice Conversion with Cross Attention",
      "summary": "Zero-shot voice conversion (VC) aims to transfer the source speaker timbre to arbitrary unseen target speaker timbre, while keeping the linguistic content unchanged. Although the voice of generated speech can be controlled by providing the speaker embedding of the target speaker, the speaker similarity still lags behind the ground truth recordings. In this paper, we propose SEF-VC, a speaker embedding free voice conversion model, which is designed to learn and incorporate speaker timbre from reference speech via a powerful position-agnostic cross-attention mechanism, and then reconstruct waveform from HuBERT semantic tokens in a non-autoregressive manner. The concise design of SEF-VC enhances its training stability and voice conversion performance. Objective and subjective evaluations demonstrate the superiority of SEF-VC to generate high-quality speech with better similarity to target reference than strong zero-shot VC baselines, even for very short reference speeches.",
      "authors": [
        "Junjie Li",
        "Yiwei Guo",
        "Xie Chen",
        "Kai Yu"
      ],
      "published": "2023-12-14T06:26:55Z",
      "updated": "2024-01-30T14:11:29Z",
      "categories": [
        "cs.SD",
        "cs.CL",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2312.08676v2",
      "landing_url": "https://arxiv.org/abs/2312.08676v2",
      "doi": "https://doi.org/10.48550/arXiv.2312.08676"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The work uses HuBERT semantic tokens for waveform reconstruction, indicating discrete tokenization is involved, but the abstract does not frame tokenization as the core research object; inclusion is thus uncertain.",
      "evaluation": 3
    },
    "round-A_JuniorNano_reasoning": "The work uses HuBERT semantic tokens for waveform reconstruction, indicating discrete tokenization is involved, but the abstract does not frame tokenization as the core research object; inclusion is thus uncertain.",
    "round-A_JuniorNano_evaluation": 3,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on zero-shot voice conversion using a speaker embedding free model that reconstructs waveforms from HuBERT semantic tokens, indicating use of discrete audio tokenization integral to the method; it also provides experimental validation, thus meeting inclusion criteria and no exclusion criteria are apparent.",
      "evaluation": 5
    },
    "round-A_JuniorMini_reasoning": "The study focuses on zero-shot voice conversion using a speaker embedding free model that reconstructs waveforms from HuBERT semantic tokens, indicating use of discrete audio tokenization integral to the method; it also provides experimental validation, thus meeting inclusion criteria and no exclusion criteria are apparent.",
    "round-A_JuniorMini_evaluation": 5,
    "round-B_SeniorLead_output": {
      "reasoning": "After reviewing both junior reviewers (one uncertain, one favorable), I conclude the paper merely uses pre-existing HuBERT semantic tokens for waveform reconstruction without proposing, analyzing, or evaluating discrete audio tokenization as a core contribution, so it fails the inclusion criterion and should be excluded.",
      "evaluation": 2
    },
    "round-B_SeniorLead_reasoning": "After reviewing both junior reviewers (one uncertain, one favorable), I conclude the paper merely uses pre-existing HuBERT semantic tokens for waveform reconstruction without proposing, analyzing, or evaluating discrete audio tokenization as a core contribution, so it fails the inclusion criterion and should be excluded.",
    "round-B_SeniorLead_evaluation": 2,
    "final_verdict": "exclude (senior:2)",
    "review_skipped": false,
    "discard_reason": "exclude (senior:2)"
  },
  {
    "title": "Tokenize Anything via Prompting",
    "abstract": "We present a unified, promptable model capable of simultaneously segmenting, recognizing, and captioning anything. Unlike SAM, we aim to build a versatile region representation in the wild via visual prompting. To achieve this, we train a generalizable model with massive segmentation masks, \\eg, SA-1B masks, and semantic priors from a pre-trained CLIP model with 5 billion parameters. Specifically, we construct a promptable image decoder by adding a semantic token to each mask token. The semantic token is responsible for learning the semantic priors in a predefined concept space. Through joint optimization of segmentation on mask tokens and concept prediction on semantic tokens, our model exhibits strong regional recognition and localization capabilities. For example, an additional 38M-parameter causal text decoder trained from scratch sets a new record with a CIDEr score of 164.7 on the Visual Genome region captioning task. We believe this model can be a versatile region-level image tokenizer, capable of encoding general-purpose region context for a broad range of visual perception tasks. Code and models are available at {\\footnotesize \\url{https://github.com/baaivision/tokenize-anything}}.",
    "metadata": {
      "arxiv_id": "2312.09128",
      "title": "Tokenize Anything via Prompting",
      "summary": "We present a unified, promptable model capable of simultaneously segmenting, recognizing, and captioning anything. Unlike SAM, we aim to build a versatile region representation in the wild via visual prompting. To achieve this, we train a generalizable model with massive segmentation masks, \\eg, SA-1B masks, and semantic priors from a pre-trained CLIP model with 5 billion parameters. Specifically, we construct a promptable image decoder by adding a semantic token to each mask token. The semantic token is responsible for learning the semantic priors in a predefined concept space. Through joint optimization of segmentation on mask tokens and concept prediction on semantic tokens, our model exhibits strong regional recognition and localization capabilities. For example, an additional 38M-parameter causal text decoder trained from scratch sets a new record with a CIDEr score of 164.7 on the Visual Genome region captioning task. We believe this model can be a versatile region-level image tokenizer, capable of encoding general-purpose region context for a broad range of visual perception tasks. Code and models are available at {\\footnotesize \\url{https://github.com/baaivision/tokenize-anything}}.",
      "authors": [
        "Ting Pan",
        "Lulu Tang",
        "Xinlong Wang",
        "Shiguang Shan"
      ],
      "published": "2023-12-14T17:01:02Z",
      "updated": "2024-07-17T04:34:37Z",
      "categories": [
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2312.09128v2",
      "landing_url": "https://arxiv.org/abs/2312.09128v2",
      "doi": "https://doi.org/10.48550/arXiv.2312.09128"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item focuses on vision-language prompting and image segmentation/tokenization, with no discussion of discrete audio/speech tokenization; the inclusion criterion requires a core study on discrete audio tokens, so this should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item focuses on vision-language prompting and image segmentation/tokenization, with no discussion of discrete audio/speech tokenization; the inclusion criterion requires a core study on discrete audio tokens, so this should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on visual tokenization involving image segmentation and captioning, with no mention of audio or speech tokenization, discrete audio symbol sequences, or evaluation on speech-related tasks, thus it does not meet the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on visual tokenization involving image segmentation and captioning, with no mention of audio or speech tokenization, discrete audio symbol sequences, or evaluation on speech-related tasks, thus it does not meet the inclusion criteria.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Clinical Text Deduplication Practices for Efficient Pretraining and Improved Clinical Tasks",
    "abstract": "Despite being a unique source of information on patients' status and disease progression, clinical notes are characterized by high levels of duplication and information redundancy. In general domain text, it has been shown that deduplication does not harm language model (LM) pretraining, thus helping reduce the training cost. Although large LMs have proven to learn medical knowledge, they still require specialized domain adaptation for improved downstream clinical tasks. By leveraging large real-world clinical corpora, we first provided a fine-grained characterization of duplicates stemming from common writing practices and clinical relevancy. Second, we demonstrated that deduplicating clinical text can help clinical LMs encode less redundant information in a more efficient manner and do not harm classification tasks via prompt-based learning.",
    "metadata": {
      "arxiv_id": "2312.09469",
      "title": "Clinical Text Deduplication Practices for Efficient Pretraining and Improved Clinical Tasks",
      "summary": "Despite being a unique source of information on patients' status and disease progression, clinical notes are characterized by high levels of duplication and information redundancy. In general domain text, it has been shown that deduplication does not harm language model (LM) pretraining, thus helping reduce the training cost. Although large LMs have proven to learn medical knowledge, they still require specialized domain adaptation for improved downstream clinical tasks. By leveraging large real-world clinical corpora, we first provided a fine-grained characterization of duplicates stemming from common writing practices and clinical relevancy. Second, we demonstrated that deduplicating clinical text can help clinical LMs encode less redundant information in a more efficient manner and do not harm classification tasks via prompt-based learning.",
      "authors": [
        "Isotta Landi",
        "Eugenia Alleva",
        "Alissa A. Valentine",
        "Lauren A. Lepow",
        "Alexander W. Charney"
      ],
      "published": "2023-09-29T18:35:52Z",
      "updated": "2023-09-29T18:35:52Z",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2312.09469v1",
      "landing_url": "https://arxiv.org/abs/2312.09469v1",
      "doi": "https://doi.org/10.48550/arXiv.2312.09469"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The article addresses deduplication of clinical text for language-model pretraining and evaluation on text tasks, with no discrete audio/speech tokenization analysis or application to speech tasks.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The article addresses deduplication of clinical text for language-model pretraining and evaluation on text tasks, with no discrete audio/speech tokenization analysis or application to speech tasks.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on deduplication of clinical text data to improve language models in clinical tasks and does not address discrete audio/speech tokenization or any related audio token analysis, which is the core inclusion criterion.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on deduplication of clinical text data to improve language models in clinical tasks and does not address discrete audio/speech tokenization or any related audio token analysis, which is the core inclusion criterion.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Kernel Heterogeneity Improves Sparseness of Natural Images Representations",
    "abstract": "Both biological and artificial neural networks inherently balance their performance with their operational cost, which balances their computational abilities. Typically, an efficient neuromorphic neural network is one that learns representations that reduce the redundancies and dimensionality of its input. This is for instance achieved in sparse coding, and sparse representations derived from natural images yield representations that are heterogeneous, both in their sampling of input features and in the variance of those features. Here, we investigated the connection between natural images' structure, particularly oriented features, and their corresponding sparse codes. We showed that representations of input features scattered across multiple levels of variance substantially improve the sparseness and resilience of sparse codes, at the cost of reconstruction performance. This echoes the structure of the model's input, allowing to account for the heterogeneously aleatoric structures of natural images. We demonstrate that learning kernel from natural images produces heterogeneity by balancing between approximate and dense representations, which improves all reconstruction metrics. Using a parametrized control of the kernels' heterogeneity used by a convolutional sparse coding algorithm, we show that heterogeneity emphasizes sparseness, while homogeneity improves representation granularity. In a broader context, these encoding strategy can serve as inputs to deep convolutional neural networks. We prove that such variance-encoded sparse image datasets enhance computational efficiency, emphasizing the benefits of kernel heterogeneity to leverage naturalistic and variant input structures and possible applications to improve the throughput of neuromorphic hardware.",
    "metadata": {
      "arxiv_id": "2312.14685",
      "title": "Kernel Heterogeneity Improves Sparseness of Natural Images Representations",
      "summary": "Both biological and artificial neural networks inherently balance their performance with their operational cost, which balances their computational abilities. Typically, an efficient neuromorphic neural network is one that learns representations that reduce the redundancies and dimensionality of its input. This is for instance achieved in sparse coding, and sparse representations derived from natural images yield representations that are heterogeneous, both in their sampling of input features and in the variance of those features. Here, we investigated the connection between natural images' structure, particularly oriented features, and their corresponding sparse codes. We showed that representations of input features scattered across multiple levels of variance substantially improve the sparseness and resilience of sparse codes, at the cost of reconstruction performance. This echoes the structure of the model's input, allowing to account for the heterogeneously aleatoric structures of natural images. We demonstrate that learning kernel from natural images produces heterogeneity by balancing between approximate and dense representations, which improves all reconstruction metrics. Using a parametrized control of the kernels' heterogeneity used by a convolutional sparse coding algorithm, we show that heterogeneity emphasizes sparseness, while homogeneity improves representation granularity. In a broader context, these encoding strategy can serve as inputs to deep convolutional neural networks. We prove that such variance-encoded sparse image datasets enhance computational efficiency, emphasizing the benefits of kernel heterogeneity to leverage naturalistic and variant input structures and possible applications to improve the throughput of neuromorphic hardware.",
      "authors": [
        "Hugo J. Ladret",
        "Christian Casanova",
        "Laurent Udo Perrinet"
      ],
      "published": "2023-12-22T13:36:27Z",
      "updated": "2023-12-22T13:36:27Z",
      "categories": [
        "q-bio.NC",
        "cs.NE"
      ],
      "pdf_url": "https://arxiv.org/pdf/2312.14685v1",
      "landing_url": "https://arxiv.org/abs/2312.14685v1",
      "doi": "https://doi.org/10.48550/arXiv.2312.14685"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The study investigates kernel heterogeneity and sparse coding for natural images; it does not propose or analyze discrete audio/speech tokenization or any discrete audio token representations, so it fails the inclusion criteria and aligns with exclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The study investigates kernel heterogeneity and sparse coding for natural images; it does not propose or analyze discrete audio/speech tokenization or any discrete audio token representations, so it fails the inclusion criteria and aligns with exclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on sparse representations of natural images and kernel heterogeneity in convolutional sparse coding, with no mention of audio or speech tokenization or discrete quantization of audio signals, making it irrelevant to the inclusion criteria centered on discrete audio/speech tokenization.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on sparse representations of natural images and kernel heterogeneity in convolutional sparse coding, with no mention of audio or speech tokenization or discrete quantization of audio signals, making it irrelevant to the inclusion criteria centered on discrete audio/speech tokenization.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Single-channel speech enhancement using learnable loss mixup",
    "abstract": "Generalization remains a major problem in supervised learning of single-channel speech enhancement. In this work, we propose learnable loss mixup (LLM), a simple and effortless training diagram, to improve the generalization of deep learning-based speech enhancement models. Loss mixup, of which learnable loss mixup is a special variant, optimizes a mixture of the loss functions of random sample pairs to train a model on virtual training data constructed from these pairs of samples. In learnable loss mixup, by conditioning on the mixed data, the loss functions are mixed using a non-linear mixing function automatically learned via neural parameterization. Our experimental results on the VCTK benchmark show that learnable loss mixup achieves 3.26 PESQ, outperforming the state-of-the-art.",
    "metadata": {
      "arxiv_id": "2312.17255",
      "title": "Single-channel speech enhancement using learnable loss mixup",
      "summary": "Generalization remains a major problem in supervised learning of single-channel speech enhancement. In this work, we propose learnable loss mixup (LLM), a simple and effortless training diagram, to improve the generalization of deep learning-based speech enhancement models. Loss mixup, of which learnable loss mixup is a special variant, optimizes a mixture of the loss functions of random sample pairs to train a model on virtual training data constructed from these pairs of samples. In learnable loss mixup, by conditioning on the mixed data, the loss functions are mixed using a non-linear mixing function automatically learned via neural parameterization. Our experimental results on the VCTK benchmark show that learnable loss mixup achieves 3.26 PESQ, outperforming the state-of-the-art.",
      "authors": [
        "Oscar Chang",
        "Dung N. Tran",
        "Kazuhito Koishida"
      ],
      "published": "2023-12-20T00:25:55Z",
      "updated": "2023-12-20T00:25:55Z",
      "categories": [
        "eess.AS",
        "cs.LG",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2312.17255v1",
      "landing_url": "https://arxiv.org/abs/2312.17255v1",
      "doi": "https://doi.org/10.48550/arXiv.2312.17255"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The paper focuses on a loss-mixing technique for single-channel speech enhancement using continuous representations and does not propose or analyze discrete audio tokenization or token-based representations, so it does not meet the inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The paper focuses on a loss-mixing technique for single-channel speech enhancement using continuous representations and does not propose or analyze discrete audio tokenization or token-based representations, so it does not meet the inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on improving generalization in supervised single-channel speech enhancement via a novel training method called learnable loss mixup, but it does not mention or analyze discrete audio/speech tokenization or the conversion of continuous audio signals into discrete token sequences from a finite vocabulary, which is the core inclusion criterion.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on improving generalization in supervised single-channel speech enhancement via a novel training method called learnable loss mixup, but it does not mention or analyze discrete audio/speech tokenization or the conversion of continuous audio signals into discrete token sequences from a finite vocabulary, which is the core inclusion criterion.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Utilizing Neural Transducers for Two-Stage Text-to-Speech via Semantic Token Prediction",
    "abstract": "We propose a novel text-to-speech (TTS) framework centered around a neural transducer. Our approach divides the whole TTS pipeline into semantic-level sequence-to-sequence (seq2seq) modeling and fine-grained acoustic modeling stages, utilizing discrete semantic tokens obtained from wav2vec2.0 embeddings. For a robust and efficient alignment modeling, we employ a neural transducer named token transducer for the semantic token prediction, benefiting from its hard monotonic alignment constraints. Subsequently, a non-autoregressive (NAR) speech generator efficiently synthesizes waveforms from these semantic tokens. Additionally, a reference speech controls temporal dynamics and acoustic conditions at each stage. This decoupled framework reduces the training complexity of TTS while allowing each stage to focus on semantic and acoustic modeling. Our experimental results on zero-shot adaptive TTS demonstrate that our model surpasses the baseline in terms of speech quality and speaker similarity, both objectively and subjectively. We also delve into the inference speed and prosody control capabilities of our approach, highlighting the potential of neural transducers in TTS frameworks.",
    "metadata": {
      "arxiv_id": "2401.01498",
      "title": "Utilizing Neural Transducers for Two-Stage Text-to-Speech via Semantic Token Prediction",
      "summary": "We propose a novel text-to-speech (TTS) framework centered around a neural transducer. Our approach divides the whole TTS pipeline into semantic-level sequence-to-sequence (seq2seq) modeling and fine-grained acoustic modeling stages, utilizing discrete semantic tokens obtained from wav2vec2.0 embeddings. For a robust and efficient alignment modeling, we employ a neural transducer named token transducer for the semantic token prediction, benefiting from its hard monotonic alignment constraints. Subsequently, a non-autoregressive (NAR) speech generator efficiently synthesizes waveforms from these semantic tokens. Additionally, a reference speech controls temporal dynamics and acoustic conditions at each stage. This decoupled framework reduces the training complexity of TTS while allowing each stage to focus on semantic and acoustic modeling. Our experimental results on zero-shot adaptive TTS demonstrate that our model surpasses the baseline in terms of speech quality and speaker similarity, both objectively and subjectively. We also delve into the inference speed and prosody control capabilities of our approach, highlighting the potential of neural transducers in TTS frameworks.",
      "authors": [
        "Minchan Kim",
        "Myeonghun Jeong",
        "Byoung Jin Choi",
        "Semin Kim",
        "Joun Yeop Lee",
        "Nam Soo Kim"
      ],
      "published": "2024-01-03T02:03:36Z",
      "updated": "2024-01-03T02:03:36Z",
      "categories": [
        "eess.AS",
        "cs.CL",
        "cs.LG",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2401.01498v1",
      "landing_url": "https://arxiv.org/abs/2401.01498v1",
      "doi": "https://doi.org/10.48550/arXiv.2401.01498"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The paper explicitly uses discrete semantic tokens (from wav2vec2.0) and a token transducer for semantic-token prediction as core components, satisfying the inclusion criterion for discrete audio tokenization without triggering any exclusion.",
      "evaluation": 5
    },
    "round-A_JuniorNano_reasoning": "The paper explicitly uses discrete semantic tokens (from wav2vec2.0) and a token transducer for semantic-token prediction as core components, satisfying the inclusion criterion for discrete audio tokenization without triggering any exclusion.",
    "round-A_JuniorNano_evaluation": 5,
    "round-A_JuniorMini_output": {
      "reasoning": "The study explicitly focuses on discrete semantic tokens obtained from wav2vec2.0 embeddings as the core object for text-to-speech synthesis, analyzing and evaluating discrete audio tokenization within a novel TTS framework. The approach involves neural transducers for semantic token prediction as well as non-autoregressive speech generation, demonstrating experimental validation related to discrete audio tokens in speech tasks, which satisfies all inclusion criteria without falling into any exclusion categories.",
      "evaluation": 5
    },
    "round-A_JuniorMini_reasoning": "The study explicitly focuses on discrete semantic tokens obtained from wav2vec2.0 embeddings as the core object for text-to-speech synthesis, analyzing and evaluating discrete audio tokenization within a novel TTS framework. The approach involves neural transducers for semantic token prediction as well as non-autoregressive speech generation, demonstrating experimental validation related to discrete audio tokens in speech tasks, which satisfies all inclusion criteria without falling into any exclusion categories.",
    "round-A_JuniorMini_evaluation": 5,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "include (junior:5)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Zero-shot Active Learning Using Self Supervised Learning",
    "abstract": "Deep learning algorithms are often said to be data hungry. The performance of such algorithms generally improve as more and more annotated data is fed into the model. While collecting unlabelled data is easier (as they can be scraped easily from the internet), annotating them is a tedious and expensive task. Given a fixed budget available for data annotation, Active Learning helps selecting the best subset of data for annotation, such that the deep learning model when trained over that subset will have maximum generalization performance under this budget. In this work, we aim to propose a new Active Learning approach which is model agnostic as well as one doesn't require an iterative process. We aim to leverage self-supervised learnt features for the task of Active Learning. The benefit of self-supervised learning, is that one can get useful feature representation of the input data, without having any annotation.",
    "metadata": {
      "arxiv_id": "2401.01690",
      "title": "Zero-shot Active Learning Using Self Supervised Learning",
      "summary": "Deep learning algorithms are often said to be data hungry. The performance of such algorithms generally improve as more and more annotated data is fed into the model. While collecting unlabelled data is easier (as they can be scraped easily from the internet), annotating them is a tedious and expensive task. Given a fixed budget available for data annotation, Active Learning helps selecting the best subset of data for annotation, such that the deep learning model when trained over that subset will have maximum generalization performance under this budget. In this work, we aim to propose a new Active Learning approach which is model agnostic as well as one doesn't require an iterative process. We aim to leverage self-supervised learnt features for the task of Active Learning. The benefit of self-supervised learning, is that one can get useful feature representation of the input data, without having any annotation.",
      "authors": [
        "Abhishek Sinha",
        "Shreya Singh"
      ],
      "published": "2024-01-03T11:49:07Z",
      "updated": "2024-01-03T11:49:07Z",
      "categories": [
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2401.01690v1",
      "landing_url": "https://arxiv.org/abs/2401.01690v1",
      "doi": "https://doi.org/10.48550/arXiv.2401.01690"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item discusses zero-shot/active learning using self-supervised learning and does not involve discrete audio/speech tokenization or analysis of discrete audio tokens, thus it does not meet the specified audio/tokenization criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item discusses zero-shot/active learning using self-supervised learning and does not involve discrete audio/speech tokenization or analysis of discrete audio tokens, thus it does not meet the specified audio/tokenization criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The title and abstract describe a study on zero-shot active learning using self-supervised learning for deep learning models, focusing mainly on data annotation efficiency; there is no mention of discrete audio/speech tokenization or converting continuous audio signals into discrete tokens, nor analysis or evaluation of such tokenization, which is the core inclusion criterion.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The title and abstract describe a study on zero-shot active learning using self-supervised learning for deep learning models, focusing mainly on data annotation efficiency; there is no mention of discrete audio/speech tokenization or converting continuous audio signals into discrete tokens, nor analysis or evaluation of such tokenization, which is the core inclusion criterion.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Zero Shot Audio to Audio Emotion Transfer With Speaker Disentanglement",
    "abstract": "The problem of audio-to-audio (A2A) style transfer involves replacing the style features of the source audio with those from the target audio while preserving the content related attributes of the source audio. In this paper, we propose an efficient approach, termed as Zero-shot Emotion Style Transfer (ZEST), that allows the transfer of emotional content present in the given source audio with the one embedded in the target audio while retaining the speaker and speech content from the source. The proposed system builds upon decomposing speech into semantic tokens, speaker representations and emotion embeddings. Using these factors, we propose a framework to reconstruct the pitch contour of the given speech signal and train a decoder that reconstructs the speech signal. The model is trained using a self-supervision based reconstruction loss. During conversion, the emotion embedding is alone derived from the target audio, while rest of the factors are derived from the source audio. In our experiments, we show that, even without using parallel training data or labels from the source or target audio, we illustrate zero shot emotion transfer capabilities of the proposed ZEST model using objective and subjective quality evaluations.",
    "metadata": {
      "arxiv_id": "2401.04511",
      "title": "Zero Shot Audio to Audio Emotion Transfer With Speaker Disentanglement",
      "summary": "The problem of audio-to-audio (A2A) style transfer involves replacing the style features of the source audio with those from the target audio while preserving the content related attributes of the source audio. In this paper, we propose an efficient approach, termed as Zero-shot Emotion Style Transfer (ZEST), that allows the transfer of emotional content present in the given source audio with the one embedded in the target audio while retaining the speaker and speech content from the source. The proposed system builds upon decomposing speech into semantic tokens, speaker representations and emotion embeddings. Using these factors, we propose a framework to reconstruct the pitch contour of the given speech signal and train a decoder that reconstructs the speech signal. The model is trained using a self-supervision based reconstruction loss. During conversion, the emotion embedding is alone derived from the target audio, while rest of the factors are derived from the source audio. In our experiments, we show that, even without using parallel training data or labels from the source or target audio, we illustrate zero shot emotion transfer capabilities of the proposed ZEST model using objective and subjective quality evaluations.",
      "authors": [
        "Soumya Dutta",
        "Sriram Ganapathy"
      ],
      "published": "2024-01-09T12:10:04Z",
      "updated": "2024-01-09T12:10:04Z",
      "categories": [
        "eess.AS",
        "cs.LG",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2401.04511v1",
      "landing_url": "https://arxiv.org/abs/2401.04511v1",
      "doi": "https://doi.org/10.48550/arXiv.2401.04511"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The abstract mentions decomposing speech into semantic tokens and a decoder for reconstruction, but it does not explicitly state the use of discrete, quantized tokens from a finite vocabulary as a core tokenization mechanism; without explicit discrete tokenization analysis, it may not meet the strict inclusion criterion.",
      "evaluation": 3
    },
    "round-A_JuniorNano_reasoning": "The abstract mentions decomposing speech into semantic tokens and a decoder for reconstruction, but it does not explicitly state the use of discrete, quantized tokens from a finite vocabulary as a core tokenization mechanism; without explicit discrete tokenization analysis, it may not meet the strict inclusion criterion.",
    "round-A_JuniorNano_evaluation": 3,
    "round-A_JuniorMini_output": {
      "reasoning": "The study discusses decomposing speech into semantic tokens, speaker representations, and emotion embeddings, suggesting the use of discrete audio tokenization as a core part of the method. It focuses on zero-shot emotion style transfer and includes both objective and subjective evaluations, indicating methodological contribution and experimental validation. However, the abstract does not provide detailed specifics on the tokenization process or explicit analysis of discrete tokenization, which makes the inclusion uncertain but leaning towards inclusion based on the information given.",
      "evaluation": 4
    },
    "round-A_JuniorMini_reasoning": "The study discusses decomposing speech into semantic tokens, speaker representations, and emotion embeddings, suggesting the use of discrete audio tokenization as a core part of the method. It focuses on zero-shot emotion style transfer and includes both objective and subjective evaluations, indicating methodological contribution and experimental validation. However, the abstract does not provide detailed specifics on the tokenization process or explicit analysis of discrete tokenization, which makes the inclusion uncertain but leaning towards inclusion based on the information given.",
    "round-A_JuniorMini_evaluation": 4,
    "round-B_SeniorLead_output": {
      "reasoning": "I reviewed both junior reviewers' feedback (one scored 3, the other 4) and the abstract: it mentions decomposing speech into “semantic tokens” but does not explicitly describe a discrete quantization/token vocabulary, nor any explicit analysis/evaluation of discrete tokenization or confirmation that the full text is available in English—therefore it is ambiguous whether the paper meets the strict inclusion criterion and inclusion is uncertain. (Score explanation: ambiguous mention of tokens without explicit discrete-token design or analysis, and full-text availability is unknown.)",
      "evaluation": 3
    },
    "round-B_SeniorLead_reasoning": "I reviewed both junior reviewers' feedback (one scored 3, the other 4) and the abstract: it mentions decomposing speech into “semantic tokens” but does not explicitly describe a discrete quantization/token vocabulary, nor any explicit analysis/evaluation of discrete tokenization or confirmation that the full text is available in English—therefore it is ambiguous whether the paper meets the strict inclusion criterion and inclusion is uncertain. (Score explanation: ambiguous mention of tokens without explicit discrete-token design or analysis, and full-text availability is unknown.)",
    "round-B_SeniorLead_evaluation": 3,
    "final_verdict": "maybe (senior:3)",
    "review_skipped": false,
    "discard_reason": "review_needs_followup"
  },
  {
    "title": "Generative Deduplication For Socia Media Data Selection",
    "abstract": "Social media data exhibits severe redundancy caused by its noisy nature. It leads to increased training time and model bias in its processing. To address this issue, we propose a novel Generative Deduplication framework for social media data selection by removing semantically duplicate data. While related work involves data selection in task-specific training, our model acts as an efficient pre-processing method to universally enhance social media NLP pipelines. Specifically, we train a generative model via self-supervised learning to predict a keyword to capture the semantics of noisy social media text for deduplication. Meanwhile, time-dimensional Gaussian noise is added to improve training complexity and avoid learning trivial features. Extensive experiments suggest that our model can better reduce training samples while improving performance than baselines. The results show our model's potential to broadly advance social media language understanding in effectiveness and efficiency.",
    "metadata": {
      "arxiv_id": "2401.05883",
      "title": "Generative Deduplication For Socia Media Data Selection",
      "summary": "Social media data exhibits severe redundancy caused by its noisy nature. It leads to increased training time and model bias in its processing. To address this issue, we propose a novel Generative Deduplication framework for social media data selection by removing semantically duplicate data. While related work involves data selection in task-specific training, our model acts as an efficient pre-processing method to universally enhance social media NLP pipelines. Specifically, we train a generative model via self-supervised learning to predict a keyword to capture the semantics of noisy social media text for deduplication. Meanwhile, time-dimensional Gaussian noise is added to improve training complexity and avoid learning trivial features. Extensive experiments suggest that our model can better reduce training samples while improving performance than baselines. The results show our model's potential to broadly advance social media language understanding in effectiveness and efficiency.",
      "authors": [
        "Xianming Li",
        "Jing Li"
      ],
      "published": "2024-01-11T12:43:26Z",
      "updated": "2024-10-03T03:34:34Z",
      "categories": [
        "cs.CL"
      ],
      "pdf_url": "https://arxiv.org/pdf/2401.05883v3",
      "landing_url": "https://arxiv.org/abs/2401.05883v3",
      "doi": "https://doi.org/10.48550/arXiv.2401.05883"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item focuses on text data deduplication for social media and does not involve discrete audio tokenization or analysis of discrete audio representations.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item focuses on text data deduplication for social media and does not involve discrete audio tokenization or analysis of discrete audio representations.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on generative deduplication for social media data selection and discusses methods for removing semantically duplicate textual data, but it does not propose, analyze, or evaluate discrete audio/speech tokenization or discrete tokenization of audio signals, which is the core inclusion criterion.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on generative deduplication for social media data selection and discusses methods for removing semantically duplicate textual data, but it does not propose, analyze, or evaluate discrete audio/speech tokenization or discrete tokenization of audio signals, which is the core inclusion criterion.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "ELLA-V: Stable Neural Codec Language Modeling with Alignment-guided Sequence Reordering",
    "abstract": "The language model (LM) approach based on acoustic and linguistic prompts, such as VALL-E, has achieved remarkable progress in the field of zero-shot audio generation. However, existing methods still have some limitations: 1) repetitions, transpositions, and omissions in the output synthesized speech due to limited alignment constraints between audio and phoneme tokens; 2) challenges of fine-grained control over the synthesized speech with autoregressive (AR) language model; 3) infinite silence generation due to the nature of AR-based decoding, especially under the greedy strategy. To alleviate these issues, we propose ELLA-V, a simple but efficient LM-based zero-shot text-to-speech (TTS) framework, which enables fine-grained control over synthesized audio at the phoneme level. The key to ELLA-V is interleaving sequences of acoustic and phoneme tokens, where phoneme tokens appear ahead of the corresponding acoustic tokens. The experimental findings reveal that our model outperforms VALL-E in terms of accuracy and delivers more stable results using both greedy and sampling-based decoding strategies. The code of ELLA-V will be open-sourced after cleanups. Audio samples are available at https://ereboas.github.io/ELLAV/.",
    "metadata": {
      "arxiv_id": "2401.07333",
      "title": "ELLA-V: Stable Neural Codec Language Modeling with Alignment-guided Sequence Reordering",
      "summary": "The language model (LM) approach based on acoustic and linguistic prompts, such as VALL-E, has achieved remarkable progress in the field of zero-shot audio generation. However, existing methods still have some limitations: 1) repetitions, transpositions, and omissions in the output synthesized speech due to limited alignment constraints between audio and phoneme tokens; 2) challenges of fine-grained control over the synthesized speech with autoregressive (AR) language model; 3) infinite silence generation due to the nature of AR-based decoding, especially under the greedy strategy. To alleviate these issues, we propose ELLA-V, a simple but efficient LM-based zero-shot text-to-speech (TTS) framework, which enables fine-grained control over synthesized audio at the phoneme level. The key to ELLA-V is interleaving sequences of acoustic and phoneme tokens, where phoneme tokens appear ahead of the corresponding acoustic tokens. The experimental findings reveal that our model outperforms VALL-E in terms of accuracy and delivers more stable results using both greedy and sampling-based decoding strategies. The code of ELLA-V will be open-sourced after cleanups. Audio samples are available at https://ereboas.github.io/ELLAV/.",
      "authors": [
        "Yakun Song",
        "Zhuo Chen",
        "Xiaofei Wang",
        "Ziyang Ma",
        "Xie Chen"
      ],
      "published": "2024-01-14T17:43:55Z",
      "updated": "2024-01-14T17:43:55Z",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2401.07333v1",
      "landing_url": "https://arxiv.org/abs/2401.07333v1",
      "doi": "https://doi.org/10.48550/arXiv.2401.07333"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The article explicitly uses discrete speech tokens (phoneme and acoustic tokens) and interleaves them to enable TTS, treating discrete tokenization as a core design, which meets inclusion and does not fit any exclusion criteria.",
      "evaluation": 5
    },
    "round-A_JuniorNano_reasoning": "The article explicitly uses discrete speech tokens (phoneme and acoustic tokens) and interleaves them to enable TTS, treating discrete tokenization as a core design, which meets inclusion and does not fit any exclusion criteria.",
    "round-A_JuniorNano_evaluation": 5,
    "round-A_JuniorMini_output": {
      "reasoning": "The study proposes ELLA-V, a neural language model framework for zero-shot text-to-speech synthesis which interleaves sequences of acoustic and phoneme tokens, indicating discrete audio tokenization as a core object; it analyzes and evaluates these tokens in relation to audio reconstruction and synthesis tasks, meeting the inclusion criteria and avoiding exclusion conditions.",
      "evaluation": 5
    },
    "round-A_JuniorMini_reasoning": "The study proposes ELLA-V, a neural language model framework for zero-shot text-to-speech synthesis which interleaves sequences of acoustic and phoneme tokens, indicating discrete audio tokenization as a core object; it analyzes and evaluates these tokens in relation to audio reconstruction and synthesis tasks, meeting the inclusion criteria and avoiding exclusion conditions.",
    "round-A_JuniorMini_evaluation": 5,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "include (junior:5)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Resource-efficient Layer-wise Federated Self-supervised Learning",
    "abstract": "Many studies integrate federated learning (FL) with self-supervised learning (SSL) to take advantage of raw data distributed across edge devices. However, edge devices often struggle with high computational and communication costs imposed by SSL and FL algorithms. With the deployment of more complex and large-scale models, these challenges are exacerbated. To tackle this, we propose Layer-Wise Federated Self-Supervised Learning (LW-FedSSL), which allows edge devices to incrementally train a small part of the model at a time. Specifically, in LW-FedSSL, training is decomposed into multiple stages, with each stage responsible for only a specific layer of the model. Since only a portion of the model is active for training at any given time, LW-FedSSL significantly reduces computational requirements. Additionally, only the active model portion needs to be exchanged between the FL server and clients, reducing communication overhead. This enables LW-FedSSL to jointly address both computational and communication challenges of FL client devices. It can achieve up to a $3.34 \\times$ reduction in memory usage, $4.20 \\times$ fewer computational operations (giga floating point operations, GFLOPs), and a $5.07 \\times$ lower communication cost while maintaining performance comparable to its end-to-end training counterpart. Furthermore, we explore a progressive training strategy called Progressive Federated Self-Supervised Learning (Prog-FedSSL), which offers a $1.84\\times$ reduction in GFLOPs and a $1.67\\times$ reduction in communication costs while maintaining the same memory requirements as end-to-end training. Although the resource efficiency of Prog-FedSSL is lower than that of LW-FedSSL, its performance improvements make it a viable candidate for FL environments with more lenient resource constraints.",
    "metadata": {
      "arxiv_id": "2401.11647",
      "title": "Resource-efficient Layer-wise Federated Self-supervised Learning",
      "summary": "Many studies integrate federated learning (FL) with self-supervised learning (SSL) to take advantage of raw data distributed across edge devices. However, edge devices often struggle with high computational and communication costs imposed by SSL and FL algorithms. With the deployment of more complex and large-scale models, these challenges are exacerbated. To tackle this, we propose Layer-Wise Federated Self-Supervised Learning (LW-FedSSL), which allows edge devices to incrementally train a small part of the model at a time. Specifically, in LW-FedSSL, training is decomposed into multiple stages, with each stage responsible for only a specific layer of the model. Since only a portion of the model is active for training at any given time, LW-FedSSL significantly reduces computational requirements. Additionally, only the active model portion needs to be exchanged between the FL server and clients, reducing communication overhead. This enables LW-FedSSL to jointly address both computational and communication challenges of FL client devices. It can achieve up to a $3.34 \\times$ reduction in memory usage, $4.20 \\times$ fewer computational operations (giga floating point operations, GFLOPs), and a $5.07 \\times$ lower communication cost while maintaining performance comparable to its end-to-end training counterpart. Furthermore, we explore a progressive training strategy called Progressive Federated Self-Supervised Learning (Prog-FedSSL), which offers a $1.84\\times$ reduction in GFLOPs and a $1.67\\times$ reduction in communication costs while maintaining the same memory requirements as end-to-end training. Although the resource efficiency of Prog-FedSSL is lower than that of LW-FedSSL, its performance improvements make it a viable candidate for FL environments with more lenient resource constraints.",
      "authors": [
        "Ye Lin Tun",
        "Chu Myaet Thwal",
        "Huy Q. Le",
        "Minh N. H. Nguyen",
        "Eui-Nam Huh",
        "Choong Seon Hong"
      ],
      "published": "2024-01-22T01:57:31Z",
      "updated": "2025-12-01T06:49:40Z",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2401.11647v5",
      "landing_url": "https://arxiv.org/abs/2401.11647v5",
      "doi": "https://doi.org/10.48550/arXiv.2401.11647"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item presents resource-efficient FL/SSL for general models and does not involve discrete audio/speech tokenization, nor any analysis of discrete audio tokens, so it does not meet the inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item presents resource-efficient FL/SSL for general models and does not involve discrete audio/speech tokenization, nor any analysis of discrete audio tokens, so it does not meet the inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on providing a resource-efficient method for layer-wise federated self-supervised learning, but it does not explicitly address discrete audio/speech tokenization or analyze discrete tokenization methods; it mainly targets computational and communication efficiency in federated learning, without mention of converting continuous audio signals into discrete token sequences.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on providing a resource-efficient method for layer-wise federated self-supervised learning, but it does not explicitly address discrete audio/speech tokenization or analyze discrete tokenization methods; it mainly targets computational and communication efficiency in federated learning, without mention of converting continuous audio signals into discrete token sequences.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Two-View Topogram-Based Anatomy-Guided CT Reconstruction for Prospective Risk Minimization",
    "abstract": "To facilitate a prospective estimation of CT effective dose and risk minimization process, a prospective spatial dose estimation and the known anatomical structures are expected. To this end, a CT reconstruction method is required to reconstruct CT volumes from as few projections as possible, i.e. by using the topograms, with anatomical structures as correct as possible. In this work, an optimized CT reconstruction model based on a generative adversarial network (GAN) is proposed. The GAN is trained to reconstruct 3D volumes from an anterior-posterior and a lateral CT projection. To enhance anatomical structures, a pre-trained organ segmentation network and the 3D perceptual loss are applied during the training phase, so that the model can then generate both organ-enhanced CT volume and the organ segmentation mask. The proposed method can reconstruct CT volumes with PSNR of 26.49, RMSE of 196.17, and SSIM of 0.64, compared to 26.21, 201.55 and 0.63 using the baseline method. In terms of the anatomical structure, the proposed method effectively enhances the organ shape and boundary and allows for a straight-forward identification of the relevant anatomical structures. We note that conventional reconstruction metrics fail to indicate the enhancement of anatomical structures. In addition to such metrics, the evaluation is expanded with assessing the organ segmentation performance. The average organ dice of the proposed method is 0.71 compared with 0.63 in baseline model, indicating the enhancement of anatomical structures.",
    "metadata": {
      "arxiv_id": "2401.12725",
      "title": "Two-View Topogram-Based Anatomy-Guided CT Reconstruction for Prospective Risk Minimization",
      "summary": "To facilitate a prospective estimation of CT effective dose and risk minimization process, a prospective spatial dose estimation and the known anatomical structures are expected. To this end, a CT reconstruction method is required to reconstruct CT volumes from as few projections as possible, i.e. by using the topograms, with anatomical structures as correct as possible. In this work, an optimized CT reconstruction model based on a generative adversarial network (GAN) is proposed. The GAN is trained to reconstruct 3D volumes from an anterior-posterior and a lateral CT projection. To enhance anatomical structures, a pre-trained organ segmentation network and the 3D perceptual loss are applied during the training phase, so that the model can then generate both organ-enhanced CT volume and the organ segmentation mask. The proposed method can reconstruct CT volumes with PSNR of 26.49, RMSE of 196.17, and SSIM of 0.64, compared to 26.21, 201.55 and 0.63 using the baseline method. In terms of the anatomical structure, the proposed method effectively enhances the organ shape and boundary and allows for a straight-forward identification of the relevant anatomical structures. We note that conventional reconstruction metrics fail to indicate the enhancement of anatomical structures. In addition to such metrics, the evaluation is expanded with assessing the organ segmentation performance. The average organ dice of the proposed method is 0.71 compared with 0.63 in baseline model, indicating the enhancement of anatomical structures.",
      "authors": [
        "Chang Liu",
        "Laura Klein",
        "Yixing Huang",
        "Edith Baader",
        "Michael Lell",
        "Marc Kachelrieß",
        "Andreas Maier"
      ],
      "published": "2024-01-23T12:53:37Z",
      "updated": "2024-01-23T12:53:37Z",
      "categories": [
        "eess.IV",
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2401.12725v1",
      "landing_url": "https://arxiv.org/abs/2401.12725v1",
      "doi": "https://doi.org/10.48550/arXiv.2401.12725"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This item examines CT reconstruction from topograms with GANs and organ segmentation; it does not propose or evaluate discrete audio/speech tokenization, thus fails the audio-tokenization inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "This item examines CT reconstruction from topograms with GANs and organ segmentation; it does not propose or evaluate discrete audio/speech tokenization, thus fails the audio-tokenization inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on CT reconstruction using GANs based on anatomical structures from limited CT projections, with no mention of audio, speech tokenization, or discrete audio symbols; thus, it does not meet any inclusion criteria related to audio tokenization.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on CT reconstruction using GANs based on anatomical structures from limited CT projections, with no mention of audio, speech tokenization, or discrete audio symbols; thus, it does not meet any inclusion criteria related to audio tokenization.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "A Survey on Self-Supervised Learning for Non-Sequential Tabular Data",
    "abstract": "Self-supervised learning (SSL) has been incorporated into many state-of-the-art models in various domains, where SSL defines pretext tasks based on unlabeled datasets to learn contextualized and robust representations. Recently, SSL has become a new trend in exploring the representation learning capability in the realm of tabular data, which is more challenging due to not having explicit relations for learning descriptive representations. This survey aims to systematically review and summarize the recent progress and challenges of SSL for non-sequential tabular data (SSL4NS-TD). We first present a formal definition of NS-TD and clarify its correlation to related studies. Then, these approaches are categorized into three groups - predictive learning, contrastive learning, and hybrid learning, with their motivations and strengths of representative methods in each direction. Moreover, application issues of SSL4NS-TD are presented, including automatic data engineering, cross-table transferability, and domain knowledge integration. In addition, we elaborate on existing benchmarks and datasets for NS-TD applications to analyze the performance of existing tabular models. Finally, we discuss the challenges of SSL4NS-TD and provide potential directions for future research. We expect our work to be useful in terms of encouraging more research on lowering the barrier to entry SSL for the tabular domain, and of improving the foundations for implicit tabular data.",
    "metadata": {
      "arxiv_id": "2402.01204",
      "title": "A Survey on Self-Supervised Learning for Non-Sequential Tabular Data",
      "summary": "Self-supervised learning (SSL) has been incorporated into many state-of-the-art models in various domains, where SSL defines pretext tasks based on unlabeled datasets to learn contextualized and robust representations. Recently, SSL has become a new trend in exploring the representation learning capability in the realm of tabular data, which is more challenging due to not having explicit relations for learning descriptive representations. This survey aims to systematically review and summarize the recent progress and challenges of SSL for non-sequential tabular data (SSL4NS-TD). We first present a formal definition of NS-TD and clarify its correlation to related studies. Then, these approaches are categorized into three groups - predictive learning, contrastive learning, and hybrid learning, with their motivations and strengths of representative methods in each direction. Moreover, application issues of SSL4NS-TD are presented, including automatic data engineering, cross-table transferability, and domain knowledge integration. In addition, we elaborate on existing benchmarks and datasets for NS-TD applications to analyze the performance of existing tabular models. Finally, we discuss the challenges of SSL4NS-TD and provide potential directions for future research. We expect our work to be useful in terms of encouraging more research on lowering the barrier to entry SSL for the tabular domain, and of improving the foundations for implicit tabular data.",
      "authors": [
        "Wei-Yao Wang",
        "Wei-Wei Du",
        "Derek Xu",
        "Wei Wang",
        "Wen-Chih Peng"
      ],
      "published": "2024-02-02T08:17:41Z",
      "updated": "2024-09-10T07:02:47Z",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2402.01204v4",
      "landing_url": "https://arxiv.org/abs/2402.01204v4",
      "doi": "https://doi.org/10.48550/arXiv.2402.01204"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The paper focuses on self-supervised learning for non-sequential tabular data and does not analyze discrete audio/speech tokenization, thus it should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The paper focuses on self-supervised learning for non-sequential tabular data and does not analyze discrete audio/speech tokenization, thus it should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The article is a survey on self-supervised learning for non-sequential tabular data, with no mention of audio or speech tokenization, discrete audio signal representations, or any related topics matching the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The article is a survey on self-supervised learning for non-sequential tabular data, with no mention of audio or speech tokenization, discrete audio signal representations, or any related topics matching the inclusion criteria.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "A Probabilistic Model Behind Self-Supervised Learning",
    "abstract": "In self-supervised learning (SSL), representations are learned via an auxiliary task without annotated labels. A common task is to classify augmentations or different modalities of the data, which share semantic content (e.g. an object in an image) but differ in style (e.g. the object's location). Many approaches to self-supervised learning have been proposed, e.g. SimCLR, CLIP, and DINO, which have recently gained much attention for their representations achieving downstream performance comparable to supervised learning. However, a theoretical understanding of self-supervised methods eludes. Addressing this, we present a generative latent variable model for self-supervised learning and show that several families of discriminative SSL, including contrastive methods, induce a comparable distribution over representations, providing a unifying theoretical framework for these methods. The proposed model also justifies connections drawn to mutual information and the use of a ''projection head''. Learning representations by fitting the model generatively (termed SimVAE) improves performance over discriminative and other VAE-based methods on simple image benchmarks and significantly narrows the gap between generative and discriminative representation learning in more complex settings. Importantly, as our analysis predicts, SimVAE outperforms self-supervised learning where style information is required, taking an important step toward understanding self-supervised methods and achieving task-agnostic representations.",
    "metadata": {
      "arxiv_id": "2402.01399",
      "title": "A Probabilistic Model Behind Self-Supervised Learning",
      "summary": "In self-supervised learning (SSL), representations are learned via an auxiliary task without annotated labels. A common task is to classify augmentations or different modalities of the data, which share semantic content (e.g. an object in an image) but differ in style (e.g. the object's location). Many approaches to self-supervised learning have been proposed, e.g. SimCLR, CLIP, and DINO, which have recently gained much attention for their representations achieving downstream performance comparable to supervised learning. However, a theoretical understanding of self-supervised methods eludes. Addressing this, we present a generative latent variable model for self-supervised learning and show that several families of discriminative SSL, including contrastive methods, induce a comparable distribution over representations, providing a unifying theoretical framework for these methods. The proposed model also justifies connections drawn to mutual information and the use of a ''projection head''. Learning representations by fitting the model generatively (termed SimVAE) improves performance over discriminative and other VAE-based methods on simple image benchmarks and significantly narrows the gap between generative and discriminative representation learning in more complex settings. Importantly, as our analysis predicts, SimVAE outperforms self-supervised learning where style information is required, taking an important step toward understanding self-supervised methods and achieving task-agnostic representations.",
      "authors": [
        "Alice Bizeul",
        "Bernhard Schölkopf",
        "Carl Allen"
      ],
      "published": "2024-02-02T13:31:17Z",
      "updated": "2024-10-15T13:16:13Z",
      "categories": [
        "cs.LG",
        "cs.AI",
        "stat.ML"
      ],
      "pdf_url": "https://arxiv.org/pdf/2402.01399v3",
      "landing_url": "https://arxiv.org/abs/2402.01399v3",
      "doi": "https://doi.org/10.48550/arXiv.2402.01399"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item discusses a probabilistic model for self-supervised learning and generative representations (SimVAE) in image contexts, with no focus on discrete audio/speech tokenization or discrete token design; thus it fails inclusion criteria and aligns with exclusion criteria for discrete audio token studies.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item discusses a probabilistic model for self-supervised learning and generative representations (SimVAE) in image contexts, with no focus on discrete audio/speech tokenization or discrete token design; thus it fails inclusion criteria and aligns with exclusion criteria for discrete audio token studies.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on a theoretical framework for self-supervised learning in image modalities and does not address discrete audio/speech tokenization, nor does it analyze discrete tokenization of audio signals, which is the core inclusion criterion; thus, it does not meet the inclusion criteria and falls under exclusion criteria related to modalities and tokenization focus.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on a theoretical framework for self-supervised learning in image modalities and does not address discrete audio/speech tokenization, nor does it analyze discrete tokenization of audio signals, which is the core inclusion criterion; thus, it does not meet the inclusion criteria and falls under exclusion criteria related to modalities and tokenization focus.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Predicting positive transfer for improved low-resource speech recognition using acoustic pseudo-tokens",
    "abstract": "While massively multilingual speech models like wav2vec 2.0 XLSR-128 can be directly fine-tuned for automatic speech recognition (ASR), downstream performance can still be relatively poor on languages that are under-represented in the pre-training data. Continued pre-training on 70-200 hours of untranscribed speech in these languages can help -- but what about languages without that much recorded data? For such cases, we show that supplementing the target language with data from a similar, higher-resource 'donor' language can help. For example, continued pre-training on only 10 hours of low-resource Punjabi supplemented with 60 hours of donor Hindi is almost as good as continued pretraining on 70 hours of Punjabi. By contrast, sourcing data from less similar donors like Bengali does not improve ASR performance. To inform donor language selection, we propose a novel similarity metric based on the sequence distribution of induced acoustic units: the Acoustic Token Distribution Similarity (ATDS). Across a set of typologically different target languages (Punjabi, Galician, Iban, Setswana), we show that the ATDS between the target language and its candidate donors precisely predicts target language ASR performance.",
    "metadata": {
      "arxiv_id": "2402.02302",
      "title": "Predicting positive transfer for improved low-resource speech recognition using acoustic pseudo-tokens",
      "summary": "While massively multilingual speech models like wav2vec 2.0 XLSR-128 can be directly fine-tuned for automatic speech recognition (ASR), downstream performance can still be relatively poor on languages that are under-represented in the pre-training data. Continued pre-training on 70-200 hours of untranscribed speech in these languages can help -- but what about languages without that much recorded data? For such cases, we show that supplementing the target language with data from a similar, higher-resource 'donor' language can help. For example, continued pre-training on only 10 hours of low-resource Punjabi supplemented with 60 hours of donor Hindi is almost as good as continued pretraining on 70 hours of Punjabi. By contrast, sourcing data from less similar donors like Bengali does not improve ASR performance. To inform donor language selection, we propose a novel similarity metric based on the sequence distribution of induced acoustic units: the Acoustic Token Distribution Similarity (ATDS). Across a set of typologically different target languages (Punjabi, Galician, Iban, Setswana), we show that the ATDS between the target language and its candidate donors precisely predicts target language ASR performance.",
      "authors": [
        "Nay San",
        "Georgios Paraskevopoulos",
        "Aryaman Arora",
        "Xiluo He",
        "Prabhjot Kaur",
        "Oliver Adams",
        "Dan Jurafsky"
      ],
      "published": "2024-02-03T23:54:03Z",
      "updated": "2024-02-03T23:54:03Z",
      "categories": [
        "eess.AS",
        "cs.CL"
      ],
      "pdf_url": "https://arxiv.org/pdf/2402.02302v1",
      "landing_url": "https://arxiv.org/abs/2402.02302v1",
      "doi": "https://doi.org/10.48550/arXiv.2402.02302"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The abstract explicitly focuses on discrete acoustic tokens (acoustic tokens/induced acoustic units) and a token-distribution similarity metric (ATDS) for ASR, meeting the inclusion criterion and not triggering any exclusion.",
      "evaluation": 5
    },
    "round-A_JuniorNano_reasoning": "The abstract explicitly focuses on discrete acoustic tokens (acoustic tokens/induced acoustic units) and a token-distribution similarity metric (ATDS) for ASR, meeting the inclusion criterion and not triggering any exclusion.",
    "round-A_JuniorNano_evaluation": 5,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on selecting donor languages for transfer learning in low-resource speech recognition by using a novel similarity metric (ATDS) based on the distribution of induced acoustic units, which implies discrete tokenization of speech signals. This aligns with the inclusion criterion requiring explicit work on discrete audio tokenization. There is methodological contribution and experimental validation related to discrete acoustic unit sequences, not just continuous features or traditional codecs. Therefore, it meets the inclusion criteria and does not fall under exclusion criteria.",
      "evaluation": 5
    },
    "round-A_JuniorMini_reasoning": "The study focuses on selecting donor languages for transfer learning in low-resource speech recognition by using a novel similarity metric (ATDS) based on the distribution of induced acoustic units, which implies discrete tokenization of speech signals. This aligns with the inclusion criterion requiring explicit work on discrete audio tokenization. There is methodological contribution and experimental validation related to discrete acoustic unit sequences, not just continuous features or traditional codecs. Therefore, it meets the inclusion criteria and does not fall under exclusion criteria.",
    "round-A_JuniorMini_evaluation": 5,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "include (junior:5)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Optimal and Near-Optimal Adaptive Vector Quantization",
    "abstract": "Quantization is a fundamental optimization for many machine-learning use cases, including compressing gradients, model weights and activations, and datasets. The most accurate form of quantization is \\emph{adaptive}, where the error is minimized with respect to a given input, rather than optimizing for the worst case. However, optimal adaptive quantization methods are considered infeasible in terms of both their runtime and memory requirements. We revisit the Adaptive Vector Quantization (AVQ) problem and present algorithms that find optimal solutions with asymptotically improved time and space complexity. We also present an even faster near-optimal algorithm for large inputs. Our experiments show our algorithms may open the door to using AVQ more extensively in a variety of machine learning applications.",
    "metadata": {
      "arxiv_id": "2402.03158",
      "title": "Optimal and Near-Optimal Adaptive Vector Quantization",
      "summary": "Quantization is a fundamental optimization for many machine-learning use cases, including compressing gradients, model weights and activations, and datasets. The most accurate form of quantization is \\emph{adaptive}, where the error is minimized with respect to a given input, rather than optimizing for the worst case. However, optimal adaptive quantization methods are considered infeasible in terms of both their runtime and memory requirements.\n  We revisit the Adaptive Vector Quantization (AVQ) problem and present algorithms that find optimal solutions with asymptotically improved time and space complexity. We also present an even faster near-optimal algorithm for large inputs. Our experiments show our algorithms may open the door to using AVQ more extensively in a variety of machine learning applications.",
      "authors": [
        "Ran Ben-Basat",
        "Yaniv Ben-Itzhak",
        "Michael Mitzenmacher",
        "Shay Vargaftik"
      ],
      "published": "2024-02-05T16:27:59Z",
      "updated": "2025-07-31T13:53:50Z",
      "categories": [
        "cs.LG",
        "cs.DS",
        "cs.IT",
        "cs.NI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2402.03158v2",
      "landing_url": "https://arxiv.org/abs/2402.03158v2",
      "doi": "https://doi.org/10.48550/arXiv.2402.03158"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The abstract discusses adaptive vector quantization for general ML tasks (gradients/weights) rather than discrete audio/speech tokenization, so it does not meet the core inclusion criterion.",
      "evaluation": 2
    },
    "round-A_JuniorNano_reasoning": "The abstract discusses adaptive vector quantization for general ML tasks (gradients/weights) rather than discrete audio/speech tokenization, so it does not meet the core inclusion criterion.",
    "round-A_JuniorNano_evaluation": 2,
    "round-A_JuniorMini_output": {
      "reasoning": "The study proposes algorithms for optimal and near-optimal adaptive vector quantization, which is a general quantization optimization, but does not explicitly address discrete audio or speech tokenization, nor does it focus on converting continuous audio signals into discrete token sequences for audio tasks; therefore, it does not meet the inclusion criteria focused specifically on discrete audio/speech tokenization.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study proposes algorithms for optimal and near-optimal adaptive vector quantization, which is a general quantization optimization, but does not explicitly address discrete audio or speech tokenization, nor does it focus on converting continuous audio signals into discrete token sequences for audio tasks; therefore, it does not meet the inclusion criteria focused specifically on discrete audio/speech tokenization.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:2)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:2)"
  },
  {
    "title": "BASE TTS: Lessons from building a billion-parameter Text-to-Speech model on 100K hours of data",
    "abstract": "We introduce a text-to-speech (TTS) model called BASE TTS, which stands for $\\textbf{B}$ig $\\textbf{A}$daptive $\\textbf{S}$treamable TTS with $\\textbf{E}$mergent abilities. BASE TTS is the largest TTS model to-date, trained on 100K hours of public domain speech data, achieving a new state-of-the-art in speech naturalness. It deploys a 1-billion-parameter autoregressive Transformer that converts raw texts into discrete codes (\"speechcodes\") followed by a convolution-based decoder which converts these speechcodes into waveforms in an incremental, streamable manner. Further, our speechcodes are built using a novel speech tokenization technique that features speaker ID disentanglement and compression with byte-pair encoding. Echoing the widely-reported \"emergent abilities\" of large language models when trained on increasing volume of data, we show that BASE TTS variants built with 10K+ hours and 500M+ parameters begin to demonstrate natural prosody on textually complex sentences. We design and share a specialized dataset to measure these emergent abilities for text-to-speech. We showcase state-of-the-art naturalness of BASE TTS by evaluating against baselines that include publicly available large-scale text-to-speech systems: YourTTS, Bark and TortoiseTTS. Audio samples generated by the model can be heard at https://amazon-ltts-paper.com/.",
    "metadata": {
      "arxiv_id": "2402.08093",
      "title": "BASE TTS: Lessons from building a billion-parameter Text-to-Speech model on 100K hours of data",
      "summary": "We introduce a text-to-speech (TTS) model called BASE TTS, which stands for $\\textbf{B}$ig $\\textbf{A}$daptive $\\textbf{S}$treamable TTS with $\\textbf{E}$mergent abilities. BASE TTS is the largest TTS model to-date, trained on 100K hours of public domain speech data, achieving a new state-of-the-art in speech naturalness. It deploys a 1-billion-parameter autoregressive Transformer that converts raw texts into discrete codes (\"speechcodes\") followed by a convolution-based decoder which converts these speechcodes into waveforms in an incremental, streamable manner. Further, our speechcodes are built using a novel speech tokenization technique that features speaker ID disentanglement and compression with byte-pair encoding. Echoing the widely-reported \"emergent abilities\" of large language models when trained on increasing volume of data, we show that BASE TTS variants built with 10K+ hours and 500M+ parameters begin to demonstrate natural prosody on textually complex sentences. We design and share a specialized dataset to measure these emergent abilities for text-to-speech. We showcase state-of-the-art naturalness of BASE TTS by evaluating against baselines that include publicly available large-scale text-to-speech systems: YourTTS, Bark and TortoiseTTS. Audio samples generated by the model can be heard at https://amazon-ltts-paper.com/.",
      "authors": [
        "Mateusz Łajszczak",
        "Guillermo Cámbara",
        "Yang Li",
        "Fatih Beyhan",
        "Arent van Korlaar",
        "Fan Yang",
        "Arnaud Joly",
        "Álvaro Martín-Cortinas",
        "Ammar Abbas",
        "Adam Michalski",
        "Alexis Moinet",
        "Sri Karlapati",
        "Ewa Muszyńska",
        "Haohan Guo",
        "Bartosz Putrycz",
        "Soledad López Gambino",
        "Kayeon Yoo",
        "Elena Sokolova",
        "Thomas Drugman"
      ],
      "published": "2024-02-12T22:21:30Z",
      "updated": "2024-02-15T18:57:26Z",
      "categories": [
        "cs.LG",
        "cs.CL",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2402.08093v2",
      "landing_url": "https://arxiv.org/abs/2402.08093v2",
      "doi": "https://doi.org/10.48550/arXiv.2402.08093"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This work explicitly introduces and evaluates discrete speech tokenization (speechcodes) as core to TTS, using a finite vocabulary for speech reconstruction/generation, thus meeting the inclusion criteria and not falling under the exclusions.",
      "evaluation": 5
    },
    "round-A_JuniorNano_reasoning": "This work explicitly introduces and evaluates discrete speech tokenization (speechcodes) as core to TTS, using a finite vocabulary for speech reconstruction/generation, thus meeting the inclusion criteria and not falling under the exclusions.",
    "round-A_JuniorNano_evaluation": 5,
    "round-A_JuniorMini_output": {
      "reasoning": "The study presents a large-scale text-to-speech model using a novel speech tokenization technique that involves converting continuous speech into discrete codes (speechcodes) with speaker ID disentanglement and byte-pair encoding, thus explicitly focusing on discrete audio tokenization as a core part of the methodology for speech generation; it is written in English and includes experimental evaluation, meeting all inclusion criteria and none of the exclusion criteria.",
      "evaluation": 5
    },
    "round-A_JuniorMini_reasoning": "The study presents a large-scale text-to-speech model using a novel speech tokenization technique that involves converting continuous speech into discrete codes (speechcodes) with speaker ID disentanglement and byte-pair encoding, thus explicitly focusing on discrete audio tokenization as a core part of the methodology for speech generation; it is written in English and includes experimental evaluation, meeting all inclusion criteria and none of the exclusion criteria.",
    "round-A_JuniorMini_evaluation": 5,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "include (junior:5)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Language-Codec: Bridging Discrete Codec Representations and Speech Language Models",
    "abstract": "In recent years, large language models have achieved significant success in generative tasks related to speech, audio, music, and other signal domains. A crucial element of these models is the discrete acoustic codecs, which serve as an intermediate representation replacing the mel-spectrogram. However, there exist several gaps between discrete codecs and downstream speech language models. Specifically, 1) Due to the reconstruction paradigm of the Codec model and the structure of residual vector quantization, the initial channel of the codebooks contains excessive information, making it challenging to directly generate acoustic tokens from weakly supervised signals such as text in downstream tasks. 2) numerous codebooks increases the burden on downstream speech language models. Consequently, leveraging the characteristics of speech language models, we propose Language-Codec. In the Language-Codec, we introduce a Masked Channel Residual Vector Quantization (MCRVQ) mechanism along with improved fourier transform structures and attention blocks, refined discriminator design to address the aforementioned gaps. We compare our method with competing audio compression algorithms and observe significant outperformance across extensive evaluations. Furthermore, we also validate the efficiency of the Language-Codec on downstream speech language models. The source code and pre-trained models can be accessed at https://github.com/jishengpeng/languagecodec .",
    "metadata": {
      "arxiv_id": "2402.12208",
      "title": "Language-Codec: Bridging Discrete Codec Representations and Speech Language Models",
      "summary": "In recent years, large language models have achieved significant success in generative tasks related to speech, audio, music, and other signal domains. A crucial element of these models is the discrete acoustic codecs, which serve as an intermediate representation replacing the mel-spectrogram. However, there exist several gaps between discrete codecs and downstream speech language models. Specifically, 1) Due to the reconstruction paradigm of the Codec model and the structure of residual vector quantization, the initial channel of the codebooks contains excessive information, making it challenging to directly generate acoustic tokens from weakly supervised signals such as text in downstream tasks. 2) numerous codebooks increases the burden on downstream speech language models. Consequently, leveraging the characteristics of speech language models, we propose Language-Codec. In the Language-Codec, we introduce a Masked Channel Residual Vector Quantization (MCRVQ) mechanism along with improved fourier transform structures and attention blocks, refined discriminator design to address the aforementioned gaps. We compare our method with competing audio compression algorithms and observe significant outperformance across extensive evaluations. Furthermore, we also validate the efficiency of the Language-Codec on downstream speech language models. The source code and pre-trained models can be accessed at https://github.com/jishengpeng/languagecodec .",
      "authors": [
        "Shengpeng Ji",
        "Minghui Fang",
        "Jialong Zuo",
        "Ziyue Jiang",
        "Dingdong Wang",
        "Hanting Wang",
        "Hai Huang",
        "Zhou Zhao"
      ],
      "published": "2024-02-19T15:12:12Z",
      "updated": "2025-06-04T05:50:15Z",
      "categories": [
        "eess.AS",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2402.12208v4",
      "landing_url": "https://arxiv.org/abs/2402.12208v4",
      "doi": "https://doi.org/10.48550/arXiv.2402.12208"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item explicitly focuses on discrete audio tokenization (discrete codec representations and codebooks) as the core object, proposing a discrete-token-based method (MCRVQ) and evaluating its impact on downstream speech language models, which aligns with the inclusion criteria and does not fit the exclusion categories.",
      "evaluation": 5
    },
    "round-A_JuniorNano_reasoning": "The item explicitly focuses on discrete audio tokenization (discrete codec representations and codebooks) as the core object, proposing a discrete-token-based method (MCRVQ) and evaluating its impact on downstream speech language models, which aligns with the inclusion criteria and does not fit the exclusion categories.",
    "round-A_JuniorNano_evaluation": 5,
    "round-A_JuniorMini_output": {
      "reasoning": "The study explicitly proposes and evaluates a novel discrete audio codec representation (Language-Codec) with a focus on bridging discrete codec representations and speech language models, discussing vector quantization and discrete token generation for speech tasks, thus meeting the inclusion criteria; it is not focused on traditional codecs, continuous features only, or non-speech audio alone, and provides experimental validation beyond an engineering report.",
      "evaluation": 5
    },
    "round-A_JuniorMini_reasoning": "The study explicitly proposes and evaluates a novel discrete audio codec representation (Language-Codec) with a focus on bridging discrete codec representations and speech language models, discussing vector quantization and discrete token generation for speech tasks, thus meeting the inclusion criteria; it is not focused on traditional codecs, continuous features only, or non-speech audio alone, and provides experimental validation beyond an engineering report.",
    "round-A_JuniorMini_evaluation": 5,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "include (junior:5)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "MVDiffusion++: A Dense High-resolution Multi-view Diffusion Model for Single or Sparse-view 3D Object Reconstruction",
    "abstract": "This paper presents a neural architecture MVDiffusion++ for 3D object reconstruction that synthesizes dense and high-resolution views of an object given one or a few images without camera poses. MVDiffusion++ achieves superior flexibility and scalability with two surprisingly simple ideas: 1) A ``pose-free architecture'' where standard self-attention among 2D latent features learns 3D consistency across an arbitrary number of conditional and generation views without explicitly using camera pose information; and 2) A ``view dropout strategy'' that discards a substantial number of output views during training, which reduces the training-time memory footprint and enables dense and high-resolution view synthesis at test time. We use the Objaverse for training and the Google Scanned Objects for evaluation with standard novel view synthesis and 3D reconstruction metrics, where MVDiffusion++ significantly outperforms the current state of the arts. We also demonstrate a text-to-3D application example by combining MVDiffusion++ with a text-to-image generative model. The project page is at https://mvdiffusion-plusplus.github.io.",
    "metadata": {
      "arxiv_id": "2402.12712",
      "title": "MVDiffusion++: A Dense High-resolution Multi-view Diffusion Model for Single or Sparse-view 3D Object Reconstruction",
      "summary": "This paper presents a neural architecture MVDiffusion++ for 3D object reconstruction that synthesizes dense and high-resolution views of an object given one or a few images without camera poses. MVDiffusion++ achieves superior flexibility and scalability with two surprisingly simple ideas: 1) A ``pose-free architecture'' where standard self-attention among 2D latent features learns 3D consistency across an arbitrary number of conditional and generation views without explicitly using camera pose information; and 2) A ``view dropout strategy'' that discards a substantial number of output views during training, which reduces the training-time memory footprint and enables dense and high-resolution view synthesis at test time. We use the Objaverse for training and the Google Scanned Objects for evaluation with standard novel view synthesis and 3D reconstruction metrics, where MVDiffusion++ significantly outperforms the current state of the arts. We also demonstrate a text-to-3D application example by combining MVDiffusion++ with a text-to-image generative model. The project page is at https://mvdiffusion-plusplus.github.io.",
      "authors": [
        "Shitao Tang",
        "Jiacheng Chen",
        "Dilin Wang",
        "Chengzhou Tang",
        "Fuyang Zhang",
        "Yuchen Fan",
        "Vikas Chandra",
        "Yasutaka Furukawa",
        "Rakesh Ranjan"
      ],
      "published": "2024-02-20T04:25:57Z",
      "updated": "2024-04-30T04:11:58Z",
      "categories": [
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2402.12712v3",
      "landing_url": "https://arxiv.org/abs/2402.12712v3",
      "doi": "https://doi.org/10.48550/arXiv.2402.12712"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This paper focuses on dense 3D view synthesis using diffusion models and has no discrete audio/speech tokenization, so it does not meet the inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "This paper focuses on dense 3D view synthesis using diffusion models and has no discrete audio/speech tokenization, so it does not meet the inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on a neural architecture for 3D object reconstruction from sparse views and does not address discrete audio/speech tokenization or any related audio processing tasks; thus, it does not meet the inclusion criteria and falls outside the domain of discrete audio tokenization research.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on a neural architecture for 3D object reconstruction from sparse views and does not address discrete audio/speech tokenization or any related audio processing tasks; thus, it does not meet the inclusion criteria and falls outside the domain of discrete audio tokenization research.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Identifying Unnecessary 3D Gaussians using Clustering for Fast Rendering of 3D Gaussian Splatting",
    "abstract": "3D Gaussian splatting (3D-GS) is a new rendering approach that outperforms the neural radiance field (NeRF) in terms of both speed and image quality. 3D-GS represents 3D scenes by utilizing millions of 3D Gaussians and projects these Gaussians onto the 2D image plane for rendering. However, during the rendering process, a substantial number of unnecessary 3D Gaussians exist for the current view direction, resulting in significant computation costs associated with their identification. In this paper, we propose a computational reduction technique that quickly identifies unnecessary 3D Gaussians in real-time for rendering the current view without compromising image quality. This is accomplished through the offline clustering of 3D Gaussians that are close in distance, followed by the projection of these clusters onto a 2D image plane during runtime. Additionally, we analyze the bottleneck associated with the proposed technique when executed on GPUs and propose an efficient hardware architecture that seamlessly supports the proposed scheme. For the Mip-NeRF360 dataset, the proposed technique excludes 63% of 3D Gaussians on average before the 2D image projection, which reduces the overall rendering computation by almost 38.3% without sacrificing peak-signal-to-noise-ratio (PSNR). The proposed accelerator also achieves a speedup of 10.7x compared to a GPU.",
    "metadata": {
      "arxiv_id": "2402.13827",
      "title": "Identifying Unnecessary 3D Gaussians using Clustering for Fast Rendering of 3D Gaussian Splatting",
      "summary": "3D Gaussian splatting (3D-GS) is a new rendering approach that outperforms the neural radiance field (NeRF) in terms of both speed and image quality. 3D-GS represents 3D scenes by utilizing millions of 3D Gaussians and projects these Gaussians onto the 2D image plane for rendering. However, during the rendering process, a substantial number of unnecessary 3D Gaussians exist for the current view direction, resulting in significant computation costs associated with their identification. In this paper, we propose a computational reduction technique that quickly identifies unnecessary 3D Gaussians in real-time for rendering the current view without compromising image quality. This is accomplished through the offline clustering of 3D Gaussians that are close in distance, followed by the projection of these clusters onto a 2D image plane during runtime. Additionally, we analyze the bottleneck associated with the proposed technique when executed on GPUs and propose an efficient hardware architecture that seamlessly supports the proposed scheme. For the Mip-NeRF360 dataset, the proposed technique excludes 63% of 3D Gaussians on average before the 2D image projection, which reduces the overall rendering computation by almost 38.3% without sacrificing peak-signal-to-noise-ratio (PSNR). The proposed accelerator also achieves a speedup of 10.7x compared to a GPU.",
      "authors": [
        "Joongho Jo",
        "Hyeongwon Kim",
        "Jongsun Park"
      ],
      "published": "2024-02-21T14:16:49Z",
      "updated": "2024-09-25T01:29:37Z",
      "categories": [
        "cs.CV",
        "cs.AR"
      ],
      "pdf_url": "https://arxiv.org/pdf/2402.13827v2",
      "landing_url": "https://arxiv.org/abs/2402.13827v2",
      "doi": "https://doi.org/10.48550/arXiv.2402.13827"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This paper concerns 3D Gaussian splatting and hardware acceleration for rendering; it does not address discrete audio/speech tokenization, so it should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "This paper concerns 3D Gaussian splatting and hardware acceleration for rendering; it does not address discrete audio/speech tokenization, so it should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on 3D Gaussian splatting for rendering scenes, involving spatial clustering and hardware acceleration techniques, without addressing audio or speech tokenization, discrete audio symbolization, or any related neural audio processing tasks; thus, it fails to meet the inclusion criteria centered on discrete audio/speech tokenization.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on 3D Gaussian splatting for rendering scenes, involving spatial clustering and hardware acceleration techniques, without addressing audio or speech tokenization, discrete audio symbolization, or any related neural audio processing tasks; thus, it fails to meet the inclusion criteria centered on discrete audio/speech tokenization.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Imbalanced Data Clustering using Equilibrium K-Means",
    "abstract": "Centroid-based clustering algorithms, such as hard K-means (HKM) and fuzzy K-means (FKM), have suffered from learning bias towards large clusters. Their centroids tend to be crowded in large clusters, compromising performance when the true underlying data groups vary in size (i.e., imbalanced data). To address this, we propose a new clustering objective function based on the Boltzmann operator, which introduces a novel centroid repulsion mechanism, where data points surrounding the centroids repel other centroids. Larger clusters repel more, effectively mitigating the issue of large cluster learning bias. The proposed new algorithm, called equilibrium K-means (EKM), is simple, alternating between two steps; resource-saving, with the same time and space complexity as FKM; and scalable to large datasets via batch learning. We substantially evaluate the performance of EKM on synthetic and real-world datasets. The results show that EKM performs competitively on balanced data and significantly outperforms benchmark algorithms on imbalanced data. Deep clustering experiments demonstrate that EKM is a better alternative to HKM and FKM on imbalanced data as more discriminative representation can be obtained. Additionally, we reformulate HKM, FKM, and EKM in a general form of gradient descent and demonstrate how this general form facilitates a uniform study of K-means algorithms.",
    "metadata": {
      "arxiv_id": "2402.14490",
      "title": "Imbalanced Data Clustering using Equilibrium K-Means",
      "summary": "Centroid-based clustering algorithms, such as hard K-means (HKM) and fuzzy K-means (FKM), have suffered from learning bias towards large clusters. Their centroids tend to be crowded in large clusters, compromising performance when the true underlying data groups vary in size (i.e., imbalanced data). To address this, we propose a new clustering objective function based on the Boltzmann operator, which introduces a novel centroid repulsion mechanism, where data points surrounding the centroids repel other centroids. Larger clusters repel more, effectively mitigating the issue of large cluster learning bias. The proposed new algorithm, called equilibrium K-means (EKM), is simple, alternating between two steps; resource-saving, with the same time and space complexity as FKM; and scalable to large datasets via batch learning. We substantially evaluate the performance of EKM on synthetic and real-world datasets. The results show that EKM performs competitively on balanced data and significantly outperforms benchmark algorithms on imbalanced data. Deep clustering experiments demonstrate that EKM is a better alternative to HKM and FKM on imbalanced data as more discriminative representation can be obtained. Additionally, we reformulate HKM, FKM, and EKM in a general form of gradient descent and demonstrate how this general form facilitates a uniform study of K-means algorithms.",
      "authors": [
        "Yudong He"
      ],
      "published": "2024-02-22T12:27:38Z",
      "updated": "2024-06-06T15:51:21Z",
      "categories": [
        "cs.LG",
        "stat.ML"
      ],
      "pdf_url": "https://arxiv.org/pdf/2402.14490v3",
      "landing_url": "https://arxiv.org/abs/2402.14490v3",
      "doi": "https://doi.org/10.48550/arXiv.2402.14490"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The abstract presents a clustering method for imbalanced data (equilibrium K-means) and does not involve discrete audio/speech tokenization or analysis of discrete token representations for speech tasks; it therefore does not meet the inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The abstract presents a clustering method for imbalanced data (equilibrium K-means) and does not involve discrete audio/speech tokenization or analysis of discrete token representations for speech tasks; it therefore does not meet the inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on a novel clustering algorithm for imbalanced data without mentioning any discrete audio/speech tokenization, neural quantization, or analysis of discrete token representations relevant to audio signal processing or speech tasks, failing to meet the core inclusion criteria and matching the exclusion criteria related to lack of discrete audio token study.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on a novel clustering algorithm for imbalanced data without mentioning any discrete audio/speech tokenization, neural quantization, or analysis of discrete token representations relevant to audio signal processing or speech tasks, failing to meet the core inclusion criteria and matching the exclusion criteria related to lack of discrete audio token study.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Self-Guided Masked Autoencoders for Domain-Agnostic Self-Supervised Learning",
    "abstract": "Self-supervised learning excels in learning representations from large amounts of unlabeled data, demonstrating success across multiple data modalities. Yet, extending self-supervised learning to new modalities is non-trivial because the specifics of existing methods are tailored to each domain, such as domain-specific augmentations which reflect the invariances in the target task. While masked modeling is promising as a domain-agnostic framework for self-supervised learning because it does not rely on input augmentations, its mask sampling procedure remains domain-specific. We present Self-guided Masked Autoencoders (SMA), a fully domain-agnostic masked modeling method. SMA trains an attention based model using a masked modeling objective, by learning masks to sample without any domain-specific assumptions. We evaluate SMA on three self-supervised learning benchmarks in protein biology, chemical property prediction, and particle physics. We find SMA is capable of learning representations without domain-specific knowledge and achieves state-of-the-art performance on these three benchmarks.",
    "metadata": {
      "arxiv_id": "2402.14789",
      "title": "Self-Guided Masked Autoencoders for Domain-Agnostic Self-Supervised Learning",
      "summary": "Self-supervised learning excels in learning representations from large amounts of unlabeled data, demonstrating success across multiple data modalities. Yet, extending self-supervised learning to new modalities is non-trivial because the specifics of existing methods are tailored to each domain, such as domain-specific augmentations which reflect the invariances in the target task. While masked modeling is promising as a domain-agnostic framework for self-supervised learning because it does not rely on input augmentations, its mask sampling procedure remains domain-specific. We present Self-guided Masked Autoencoders (SMA), a fully domain-agnostic masked modeling method. SMA trains an attention based model using a masked modeling objective, by learning masks to sample without any domain-specific assumptions. We evaluate SMA on three self-supervised learning benchmarks in protein biology, chemical property prediction, and particle physics. We find SMA is capable of learning representations without domain-specific knowledge and achieves state-of-the-art performance on these three benchmarks.",
      "authors": [
        "Johnathan Xie",
        "Yoonho Lee",
        "Annie S. Chen",
        "Chelsea Finn"
      ],
      "published": "2024-02-22T18:46:22Z",
      "updated": "2024-02-22T18:46:22Z",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2402.14789v1",
      "landing_url": "https://arxiv.org/abs/2402.14789v1",
      "doi": "https://doi.org/10.48550/arXiv.2402.14789"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item concerns domain-agnostic masked autoencoders for self-supervised learning across modalities (protein biology, chemistry, physics) and does not discuss discrete audio/speech tokenization or any discrete token design; thus it fails the inclusion criteria and does not trigger exclusion criteria related to discrete audio tokens.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item concerns domain-agnostic masked autoencoders for self-supervised learning across modalities (protein biology, chemistry, physics) and does not discuss discrete audio/speech tokenization or any discrete token design; thus it fails the inclusion criteria and does not trigger exclusion criteria related to discrete audio tokens.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on domain-agnostic self-supervised learning across protein biology, chemical property prediction, and particle physics, without explicit mention of audio or speech data, nor any discussion of discrete audio/speech tokenization as per the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on domain-agnostic self-supervised learning across protein biology, chemical property prediction, and particle physics, without explicit mention of audio or speech data, nor any discussion of discrete audio/speech tokenization as per the inclusion criteria.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Text-guided HuBERT: Self-Supervised Speech Pre-training via Generative Adversarial Networks",
    "abstract": "Human language can be expressed in either written or spoken form, i.e. text or speech. Humans can acquire knowledge from text to improve speaking and listening. However, the quest for speech pre-trained models to leverage unpaired text has just started. In this paper, we investigate a new way to pre-train such a joint speech-text model to learn enhanced speech representations and benefit various speech-related downstream tasks. Specifically, we propose a novel pre-training method, text-guided HuBERT, or T-HuBERT, which performs self-supervised learning over speech to derive phoneme-like discrete representations. And these phoneme-like pseudo-label sequences are firstly derived from speech via the generative adversarial networks (GAN) to be statistically similar to those from additional unpaired textual data. In this way, we build a bridge between unpaired speech and text in an unsupervised manner. Extensive experiments demonstrate the significant superiority of our proposed method over various strong baselines, which achieves up to 15.3% relative Word Error Rate (WER) reduction on the LibriSpeech dataset.",
    "metadata": {
      "arxiv_id": "2402.15725",
      "title": "Text-guided HuBERT: Self-Supervised Speech Pre-training via Generative Adversarial Networks",
      "summary": "Human language can be expressed in either written or spoken form, i.e. text or speech. Humans can acquire knowledge from text to improve speaking and listening. However, the quest for speech pre-trained models to leverage unpaired text has just started. In this paper, we investigate a new way to pre-train such a joint speech-text model to learn enhanced speech representations and benefit various speech-related downstream tasks. Specifically, we propose a novel pre-training method, text-guided HuBERT, or T-HuBERT, which performs self-supervised learning over speech to derive phoneme-like discrete representations. And these phoneme-like pseudo-label sequences are firstly derived from speech via the generative adversarial networks (GAN) to be statistically similar to those from additional unpaired textual data. In this way, we build a bridge between unpaired speech and text in an unsupervised manner. Extensive experiments demonstrate the significant superiority of our proposed method over various strong baselines, which achieves up to 15.3% relative Word Error Rate (WER) reduction on the LibriSpeech dataset.",
      "authors": [
        "Duo Ma",
        "Xianghu Yue",
        "Junyi Ao",
        "Xiaoxue Gao",
        "Haizhou Li"
      ],
      "published": "2024-02-24T05:30:23Z",
      "updated": "2024-08-03T12:58:45Z",
      "categories": [
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2402.15725v5",
      "landing_url": "https://arxiv.org/abs/2402.15725v5",
      "doi": "https://doi.org/10.48550/arXiv.2402.15725"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The paper explicitly derives phoneme-like discrete representations (tokenization) for speech and uses them to guide self-supervised pre-training, meeting the discrete-tokenization inclusion criterion and addressing speech tokens rather than only continuous features.",
      "evaluation": 5
    },
    "round-A_JuniorNano_reasoning": "The paper explicitly derives phoneme-like discrete representations (tokenization) for speech and uses them to guide self-supervised pre-training, meeting the discrete-tokenization inclusion criterion and addressing speech tokens rather than only continuous features.",
    "round-A_JuniorNano_evaluation": 5,
    "round-A_JuniorMini_output": {
      "reasoning": "The study proposes a self-supervised speech pre-training approach that derives phoneme-like discrete representations from continuous speech signals via generative adversarial networks, explicitly addressing discrete audio tokenization as a core component with experimental validation on speech-related datasets, thus meeting all inclusion criteria without falling into any exclusion criteria.",
      "evaluation": 5
    },
    "round-A_JuniorMini_reasoning": "The study proposes a self-supervised speech pre-training approach that derives phoneme-like discrete representations from continuous speech signals via generative adversarial networks, explicitly addressing discrete audio tokenization as a core component with experimental validation on speech-related datasets, thus meeting all inclusion criteria without falling into any exclusion criteria.",
    "round-A_JuniorMini_evaluation": 5,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "include (junior:5)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Phonetic and Lexical Discovery of a Canine Language using HuBERT",
    "abstract": "This paper delves into the pioneering exploration of potential communication patterns within dog vocalizations and transcends traditional linguistic analysis barriers, which heavily relies on human priori knowledge on limited datasets to find sound units in dog vocalization. We present a self-supervised approach with HuBERT, enabling the accurate classification of phoneme labels and the identification of vocal patterns that suggest a rudimentary vocabulary within dog vocalizations. Our findings indicate a significant acoustic consistency in these identified canine vocabulary, covering the entirety of observed dog vocalization sequences. We further develop a web-based dog vocalization labeling system. This system can highlight phoneme n-grams, present in the vocabulary, in the dog audio uploaded by users.",
    "metadata": {
      "arxiv_id": "2402.15985",
      "title": "Phonetic and Lexical Discovery of a Canine Language using HuBERT",
      "summary": "This paper delves into the pioneering exploration of potential communication patterns within dog vocalizations and transcends traditional linguistic analysis barriers, which heavily relies on human priori knowledge on limited datasets to find sound units in dog vocalization. We present a self-supervised approach with HuBERT, enabling the accurate classification of phoneme labels and the identification of vocal patterns that suggest a rudimentary vocabulary within dog vocalizations. Our findings indicate a significant acoustic consistency in these identified canine vocabulary, covering the entirety of observed dog vocalization sequences. We further develop a web-based dog vocalization labeling system. This system can highlight phoneme n-grams, present in the vocabulary, in the dog audio uploaded by users.",
      "authors": [
        "Xingyuan Li",
        "Sinong Wang",
        "Zeyu Xie",
        "Mengyue Wu",
        "Kenny Q. Zhu"
      ],
      "published": "2024-02-25T04:35:45Z",
      "updated": "2024-02-25T04:35:45Z",
      "categories": [
        "cs.SD",
        "cs.CL",
        "cs.LG",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2402.15985v1",
      "landing_url": "https://arxiv.org/abs/2402.15985v1",
      "doi": "https://doi.org/10.48550/arXiv.2402.15985"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The abstract describes a self-supervised HuBERT-based approach to classify phoneme labels and discover a canine vocabulary, i.e., converting continuous dog vocalizations into discrete phoneme-like tokens for linguistic-level understanding, which aligns with the inclusion criteria and goes beyond purely continuous features; while the English abstract is present, we cannot confirm full-text availability from the abstract alone, so it should be included but not classified as absolutely certain.",
      "evaluation": 4
    },
    "round-A_JuniorNano_reasoning": "The abstract describes a self-supervised HuBERT-based approach to classify phoneme labels and discover a canine vocabulary, i.e., converting continuous dog vocalizations into discrete phoneme-like tokens for linguistic-level understanding, which aligns with the inclusion criteria and goes beyond purely continuous features; while the English abstract is present, we cannot confirm full-text availability from the abstract alone, so it should be included but not classified as absolutely certain.",
    "round-A_JuniorNano_evaluation": 4,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on discovering phonetic units and lexical patterns in dog vocalizations using HuBERT, indicating discrete tokenization of audio signals into phoneme labels; it applies a self-supervised method to derive a finite vocabulary from continuous canine audio, meeting the core inclusion criterion of analyzing discrete audio/speech tokenization. Although the study is innovative in animal vocalization rather than human speech, it involves discrete quantization/token design and systematic analysis, not merely continuous features or engineering without methodological contribution. Given the abstract, it appears to fulfill the inclusion criteria and avoids all exclusion criteria.",
      "evaluation": 5
    },
    "round-A_JuniorMini_reasoning": "The study focuses on discovering phonetic units and lexical patterns in dog vocalizations using HuBERT, indicating discrete tokenization of audio signals into phoneme labels; it applies a self-supervised method to derive a finite vocabulary from continuous canine audio, meeting the core inclusion criterion of analyzing discrete audio/speech tokenization. Although the study is innovative in animal vocalization rather than human speech, it involves discrete quantization/token design and systematic analysis, not merely continuous features or engineering without methodological contribution. Given the abstract, it appears to fulfill the inclusion criteria and avoids all exclusion criteria.",
    "round-A_JuniorMini_evaluation": 5,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "include (junior:4)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Vision Transformers with Natural Language Semantics",
    "abstract": "Tokens or patches within Vision Transformers (ViT) lack essential semantic information, unlike their counterparts in natural language processing (NLP). Typically, ViT tokens are associated with rectangular image patches that lack specific semantic context, making interpretation difficult and failing to effectively encapsulate information. We introduce a novel transformer model, Semantic Vision Transformers (sViT), which leverages recent progress on segmentation models to design novel tokenizer strategies. sViT effectively harnesses semantic information, creating an inductive bias reminiscent of convolutional neural networks while capturing global dependencies and contextual information within images that are characteristic of transformers. Through validation using real datasets, sViT demonstrates superiority over ViT, requiring less training data while maintaining similar or superior performance. Furthermore, sViT demonstrates significant superiority in out-of-distribution generalization and robustness to natural distribution shifts, attributed to its scale invariance semantic characteristic. Notably, the use of semantic tokens significantly enhances the model's interpretability. Lastly, the proposed paradigm facilitates the introduction of new and powerful augmentation techniques at the token (or segment) level, increasing training data diversity and generalization capabilities. Just as sentences are made of words, images are formed by semantic objects; our proposed methodology leverages recent progress in object segmentation and takes an important and natural step toward interpretable and robust vision transformers.",
    "metadata": {
      "arxiv_id": "2402.17863",
      "title": "Vision Transformers with Natural Language Semantics",
      "summary": "Tokens or patches within Vision Transformers (ViT) lack essential semantic information, unlike their counterparts in natural language processing (NLP). Typically, ViT tokens are associated with rectangular image patches that lack specific semantic context, making interpretation difficult and failing to effectively encapsulate information. We introduce a novel transformer model, Semantic Vision Transformers (sViT), which leverages recent progress on segmentation models to design novel tokenizer strategies. sViT effectively harnesses semantic information, creating an inductive bias reminiscent of convolutional neural networks while capturing global dependencies and contextual information within images that are characteristic of transformers. Through validation using real datasets, sViT demonstrates superiority over ViT, requiring less training data while maintaining similar or superior performance. Furthermore, sViT demonstrates significant superiority in out-of-distribution generalization and robustness to natural distribution shifts, attributed to its scale invariance semantic characteristic. Notably, the use of semantic tokens significantly enhances the model's interpretability. Lastly, the proposed paradigm facilitates the introduction of new and powerful augmentation techniques at the token (or segment) level, increasing training data diversity and generalization capabilities. Just as sentences are made of words, images are formed by semantic objects; our proposed methodology leverages recent progress in object segmentation and takes an important and natural step toward interpretable and robust vision transformers.",
      "authors": [
        "Young Kyung Kim",
        "J. Matías Di Martino",
        "Guillermo Sapiro"
      ],
      "published": "2024-02-27T19:54:42Z",
      "updated": "2024-02-27T19:54:42Z",
      "categories": [
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2402.17863v1",
      "landing_url": "https://arxiv.org/abs/2402.17863v1",
      "doi": "https://doi.org/10.48550/arXiv.2402.17863"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This item discusses vision transformers and semantic tokens for images, not discrete audio/speech tokenization, so it does not meet the inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "This item discusses vision transformers and semantic tokens for images, not discrete audio/speech tokenization, so it does not meet the inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on vision transformers and semantic tokens derived from image patches, without involving audio or speech tokenization; therefore, it does not meet the inclusion criteria related to discrete audio/speech tokenization and instead falls outside the scope due to its focus on image data rather than audio or speech signals.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on vision transformers and semantic tokens derived from image patches, without involving audio or speech tokenization; therefore, it does not meet the inclusion criteria related to discrete audio/speech tokenization and instead falls outside the scope due to its focus on image data rather than audio or speech signals.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "The Author of a Quotation Goethe Adduced Against Newton",
    "abstract": "The hitherto unknown author of a citation by Goethe in his History of Colours is identified as J. E. Montucla and the context of Montucla's quotation is discussed.",
    "metadata": {
      "arxiv_id": "2402.19104",
      "title": "The Author of a Quotation Goethe Adduced Against Newton",
      "summary": "The hitherto unknown author of a citation by Goethe in his History of Colours is identified as J. E. Montucla and the context of Montucla's quotation is discussed.",
      "authors": [
        "Hubert Kalf"
      ],
      "published": "2024-02-29T12:36:04Z",
      "updated": "2024-02-29T12:36:04Z",
      "categories": [
        "math.HO",
        "physics.hist-ph"
      ],
      "pdf_url": "https://arxiv.org/pdf/2402.19104v1",
      "landing_url": "https://arxiv.org/abs/2402.19104v1",
      "doi": "https://doi.org/10.48550/arXiv.2402.19104"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This item is a historical/literary analysis about Goethe and Montucla, with no discussion or evaluation of discrete audio/speech tokenization or token-based representations.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "This item is a historical/literary analysis about Goethe and Montucla, with no discussion or evaluation of discrete audio/speech tokenization or token-based representations.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on identifying the author of a historical quotation by Goethe and does not discuss any aspect of discrete audio/speech tokenization, continuous audio signal processing, or relevant audio representation techniques, hence it does not meet the inclusion criteria and falls outside the scope of the review.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on identifying the author of a historical quotation by Goethe and does not discuss any aspect of discrete audio/speech tokenization, continuous audio signal processing, or relevant audio representation techniques, hence it does not meet the inclusion criteria and falls outside the scope of the review.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Rethinking The Uniformity Metric in Self-Supervised Learning",
    "abstract": "Uniformity plays an important role in evaluating learned representations, providing insights into self-supervised learning. In our quest for effective uniformity metrics, we pinpoint four principled properties that such metrics should possess. Namely, an effective uniformity metric should remain invariant to instance permutations and sample replications while accurately capturing feature redundancy and dimensional collapse. Surprisingly, we find that the uniformity metric proposed by \\citet{Wang2020UnderstandingCR} fails to satisfy the majority of these properties. Specifically, their metric is sensitive to sample replications, and can not account for feature redundancy and dimensional collapse correctly. To overcome these limitations, we introduce a new uniformity metric based on the Wasserstein distance, which satisfies all the aforementioned properties. Integrating this new metric in existing self-supervised learning methods effectively mitigates dimensional collapse and consistently improves their performance on downstream tasks involving CIFAR-10 and CIFAR-100 datasets. Code is available at \\url{https://github.com/statsle/WassersteinSSL}.",
    "metadata": {
      "arxiv_id": "2403.00642",
      "title": "Rethinking The Uniformity Metric in Self-Supervised Learning",
      "summary": "Uniformity plays an important role in evaluating learned representations, providing insights into self-supervised learning. In our quest for effective uniformity metrics, we pinpoint four principled properties that such metrics should possess. Namely, an effective uniformity metric should remain invariant to instance permutations and sample replications while accurately capturing feature redundancy and dimensional collapse. Surprisingly, we find that the uniformity metric proposed by \\citet{Wang2020UnderstandingCR} fails to satisfy the majority of these properties. Specifically, their metric is sensitive to sample replications, and can not account for feature redundancy and dimensional collapse correctly. To overcome these limitations, we introduce a new uniformity metric based on the Wasserstein distance, which satisfies all the aforementioned properties. Integrating this new metric in existing self-supervised learning methods effectively mitigates dimensional collapse and consistently improves their performance on downstream tasks involving CIFAR-10 and CIFAR-100 datasets. Code is available at \\url{https://github.com/statsle/WassersteinSSL}.",
      "authors": [
        "Xianghong Fang",
        "Jian Li",
        "Qiang Sun",
        "Benyou Wang"
      ],
      "published": "2024-03-01T16:22:05Z",
      "updated": "2024-04-26T08:24:11Z",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2403.00642v2",
      "landing_url": "https://arxiv.org/abs/2403.00642v2",
      "doi": "https://doi.org/10.48550/arXiv.2403.00642"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The article discusses uniformity metrics in self-supervised learning for vision datasets and does not involve discrete audio/speech tokenization or token-based representations, so it fails the required inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The article discusses uniformity metrics in self-supervised learning for vision datasets and does not involve discrete audio/speech tokenization or token-based representations, so it fails the required inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on evaluating and proposing a new uniformity metric in self-supervised learning related to representations, specifically discussing improvements on CIFAR-10 and CIFAR-100 image datasets, with no mention of audio/speech tokenization or discrete audio token representations; thus, it does not meet the inclusion criteria related to discrete audio/speech tokenization.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on evaluating and proposing a new uniformity metric in self-supervised learning related to representations, specifically discussing improvements on CIFAR-10 and CIFAR-100 image datasets, with no mention of audio/speech tokenization or discrete audio token representations; thus, it does not meet the inclusion criteria related to discrete audio/speech tokenization.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "RFWave: Multi-band Rectified Flow for Audio Waveform Reconstruction",
    "abstract": "Recent advancements in generative modeling have significantly enhanced the reconstruction of audio waveforms from various representations. While diffusion models are adept at this task, they are hindered by latency issues due to their operation at the individual sample point level and the need for numerous sampling steps. In this study, we introduce RFWave, a cutting-edge multi-band Rectified Flow approach designed to reconstruct high-fidelity audio waveforms from Mel-spectrograms or discrete acoustic tokens. RFWave uniquely generates complex spectrograms and operates at the frame level, processing all subbands simultaneously to boost efficiency. Leveraging Rectified Flow, which targets a straight transport trajectory, RFWave achieves reconstruction with just 10 sampling steps. Our empirical evaluations show that RFWave not only provides outstanding reconstruction quality but also offers vastly superior computational efficiency, enabling audio generation at speeds up to 160 times faster than real-time on a GPU. An online demonstration is available at: https://rfwave-demo.github.io/rfwave/.",
    "metadata": {
      "arxiv_id": "2403.05010",
      "title": "RFWave: Multi-band Rectified Flow for Audio Waveform Reconstruction",
      "summary": "Recent advancements in generative modeling have significantly enhanced the reconstruction of audio waveforms from various representations. While diffusion models are adept at this task, they are hindered by latency issues due to their operation at the individual sample point level and the need for numerous sampling steps. In this study, we introduce RFWave, a cutting-edge multi-band Rectified Flow approach designed to reconstruct high-fidelity audio waveforms from Mel-spectrograms or discrete acoustic tokens. RFWave uniquely generates complex spectrograms and operates at the frame level, processing all subbands simultaneously to boost efficiency. Leveraging Rectified Flow, which targets a straight transport trajectory, RFWave achieves reconstruction with just 10 sampling steps. Our empirical evaluations show that RFWave not only provides outstanding reconstruction quality but also offers vastly superior computational efficiency, enabling audio generation at speeds up to 160 times faster than real-time on a GPU. An online demonstration is available at: https://rfwave-demo.github.io/rfwave/.",
      "authors": [
        "Peng Liu",
        "Dongyang Dai",
        "Zhiyong Wu"
      ],
      "published": "2024-03-08T03:16:47Z",
      "updated": "2024-10-07T02:08:05Z",
      "categories": [
        "cs.SD",
        "cs.AI",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2403.05010v3",
      "landing_url": "https://arxiv.org/abs/2403.05010v3",
      "doi": "https://doi.org/10.48550/arXiv.2403.05010"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The abstract mentions discrete acoustic tokens as a possible input but does not analyze or propose tokenization as the core object of study; the work focuses on waveform reconstruction and efficiency rather than tokenization design or evaluation, so it should be excluded.",
      "evaluation": 2
    },
    "round-A_JuniorNano_reasoning": "The abstract mentions discrete acoustic tokens as a possible input but does not analyze or propose tokenization as the core object of study; the work focuses on waveform reconstruction and efficiency rather than tokenization design or evaluation, so it should be excluded.",
    "round-A_JuniorNano_evaluation": 2,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on reconstructing audio waveforms from Mel-spectrograms or discrete acoustic tokens, but it does not explicitly state that discrete audio/speech tokenization (conversion of continuous audio to discrete symbolic sequences) is a core object of study, analysis, or evaluation; thus, it is unclear if the discrete tokens are a primary focus rather than continuous features or engineering improvements in flow-based models.",
      "evaluation": 3
    },
    "round-A_JuniorMini_reasoning": "The study focuses on reconstructing audio waveforms from Mel-spectrograms or discrete acoustic tokens, but it does not explicitly state that discrete audio/speech tokenization (conversion of continuous audio to discrete symbolic sequences) is a core object of study, analysis, or evaluation; thus, it is unclear if the discrete tokens are a primary focus rather than continuous features or engineering improvements in flow-based models.",
    "round-A_JuniorMini_evaluation": 3,
    "round-B_SeniorLead_output": {
      "reasoning": "I reviewed the two junior reviewers' feedback (they flagged that the paper mentions discrete acoustic tokens but does not treat tokenization as the core study) and the abstract: RFWave focuses on multi-band Rectified Flow for waveform reconstruction and efficiency, citing Mel-spectrograms or discrete acoustic tokens as inputs but without proposing, analyzing, or evaluating a discrete audio/speech tokenization method. Therefore it does not meet the inclusion criterion requiring discrete-tokenization to be a core object of study. Score explanation: Mentions discrete tokens as optional inputs but provides no tokenization design or analysis, so better to exclude.",
      "evaluation": 2
    },
    "round-B_SeniorLead_reasoning": "I reviewed the two junior reviewers' feedback (they flagged that the paper mentions discrete acoustic tokens but does not treat tokenization as the core study) and the abstract: RFWave focuses on multi-band Rectified Flow for waveform reconstruction and efficiency, citing Mel-spectrograms or discrete acoustic tokens as inputs but without proposing, analyzing, or evaluating a discrete audio/speech tokenization method. Therefore it does not meet the inclusion criterion requiring discrete-tokenization to be a core object of study. Score explanation: Mentions discrete tokens as optional inputs but provides no tokenization design or analysis, so better to exclude.",
    "round-B_SeniorLead_evaluation": 2,
    "final_verdict": "exclude (senior:2)",
    "review_skipped": false,
    "discard_reason": "exclude (senior:2)"
  },
  {
    "title": "Low-dose CT Denoising with Language-engaged Dual-space Alignment",
    "abstract": "While various deep learning methods were proposed for low-dose computed tomography (CT) denoising, they often suffer from over-smoothing, blurring, and lack of explainability. To alleviate these issues, we propose a plug-and-play Language-Engaged Dual-space Alignment loss (LEDA) to optimize low-dose CT denoising models. Our idea is to leverage large language models (LLMs) to align denoised CT and normal dose CT images in both the continuous perceptual space and discrete semantic space, which is the first LLM-based scheme for low-dose CT denoising. LEDA involves two steps: the first is to pretrain an LLM-guided CT autoencoder, which can encode a CT image into continuous high-level features and quantize them into a token space to produce semantic tokens derived from the LLM's vocabulary; and the second is to minimize the discrepancy between the denoised CT images and normal dose CT in terms of both encoded high-level features and quantized token embeddings derived by the LLM-guided CT autoencoder. Extensive experimental results on two public LDCT denoising datasets demonstrate that our LEDA can enhance existing denoising models in terms of quantitative metrics and qualitative evaluation, and also provide explainability through language-level image understanding. Source code is available at https://github.com/hao1635/LEDA.",
    "metadata": {
      "arxiv_id": "2403.06128",
      "title": "Low-dose CT Denoising with Language-engaged Dual-space Alignment",
      "summary": "While various deep learning methods were proposed for low-dose computed tomography (CT) denoising, they often suffer from over-smoothing, blurring, and lack of explainability. To alleviate these issues, we propose a plug-and-play Language-Engaged Dual-space Alignment loss (LEDA) to optimize low-dose CT denoising models. Our idea is to leverage large language models (LLMs) to align denoised CT and normal dose CT images in both the continuous perceptual space and discrete semantic space, which is the first LLM-based scheme for low-dose CT denoising. LEDA involves two steps: the first is to pretrain an LLM-guided CT autoencoder, which can encode a CT image into continuous high-level features and quantize them into a token space to produce semantic tokens derived from the LLM's vocabulary; and the second is to minimize the discrepancy between the denoised CT images and normal dose CT in terms of both encoded high-level features and quantized token embeddings derived by the LLM-guided CT autoencoder. Extensive experimental results on two public LDCT denoising datasets demonstrate that our LEDA can enhance existing denoising models in terms of quantitative metrics and qualitative evaluation, and also provide explainability through language-level image understanding. Source code is available at https://github.com/hao1635/LEDA.",
      "authors": [
        "Zhihao Chen",
        "Tao Chen",
        "Chenhui Wang",
        "Chuang Niu",
        "Ge Wang",
        "Hongming Shan"
      ],
      "published": "2024-03-10T08:21:50Z",
      "updated": "2024-03-10T08:21:50Z",
      "categories": [
        "eess.IV",
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2403.06128v1",
      "landing_url": "https://arxiv.org/abs/2403.06128v1",
      "doi": "https://doi.org/10.48550/arXiv.2403.06128"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item addresses CT image denoising and discrete semantic tokens derived from an LLM in the image domain, not discrete audio/speech tokenization; thus it does not meet the inclusion criterion (discrete audio tokenization) and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item addresses CT image denoising and discrete semantic tokens derived from an LLM in the image domain, not discrete audio/speech tokenization; thus it does not meet the inclusion criterion (discrete audio tokenization) and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on low-dose CT image denoising using a novel method involving large language models and discrete token embeddings derived from CT images, which are visual medical images, not audio or speech signals. The inclusion criteria require discrete audio/speech tokenization and continuous audio signal conversion, which this paper does not address as it pertains to medical imaging and not audio or speech. Therefore, the study does not meet the inclusion criteria related to audio/speech tokenization and is irrelevant to the scope of the discrete audio tokenization evaluation.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on low-dose CT image denoising using a novel method involving large language models and discrete token embeddings derived from CT images, which are visual medical images, not audio or speech signals. The inclusion criteria require discrete audio/speech tokenization and continuous audio signal conversion, which this paper does not address as it pertains to medical imaging and not audio or speech. Therefore, the study does not meet the inclusion criteria related to audio/speech tokenization and is irrelevant to the scope of the discrete audio tokenization evaluation.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "See Through Their Minds: Learning Transferable Neural Representation from Cross-Subject fMRI",
    "abstract": "Deciphering visual content from functional Magnetic Resonance Imaging (fMRI) helps illuminate the human vision system. However, the scarcity of fMRI data and noise hamper brain decoding model performance. Previous approaches primarily employ subject-specific models, sensitive to training sample size. In this paper, we explore a straightforward but overlooked solution to address data scarcity. We propose shallow subject-specific adapters to map cross-subject fMRI data into unified representations. Subsequently, a shared deeper decoding model decodes cross-subject features into the target feature space. During training, we leverage both visual and textual supervision for multi-modal brain decoding. Our model integrates a high-level perception decoding pipeline and a pixel-wise reconstruction pipeline guided by high-level perceptions, simulating bottom-up and top-down processes in neuroscience. Empirical experiments demonstrate robust neural representation learning across subjects for both pipelines. Moreover, merging high-level and low-level information improves both low-level and high-level reconstruction metrics. Additionally, we successfully transfer learned general knowledge to new subjects by training new adapters with limited training data. Compared to previous state-of-the-art methods, notably pre-training-based methods (Mind-Vis and fMRI-PTE), our approach achieves comparable or superior results across diverse tasks, showing promise as an alternative method for cross-subject fMRI data pre-training. Our code and pre-trained weights will be publicly released at https://github.com/YulongBonjour/See_Through_Their_Minds.",
    "metadata": {
      "arxiv_id": "2403.06361",
      "title": "See Through Their Minds: Learning Transferable Neural Representation from Cross-Subject fMRI",
      "summary": "Deciphering visual content from functional Magnetic Resonance Imaging (fMRI) helps illuminate the human vision system. However, the scarcity of fMRI data and noise hamper brain decoding model performance. Previous approaches primarily employ subject-specific models, sensitive to training sample size. In this paper, we explore a straightforward but overlooked solution to address data scarcity. We propose shallow subject-specific adapters to map cross-subject fMRI data into unified representations. Subsequently, a shared deeper decoding model decodes cross-subject features into the target feature space. During training, we leverage both visual and textual supervision for multi-modal brain decoding. Our model integrates a high-level perception decoding pipeline and a pixel-wise reconstruction pipeline guided by high-level perceptions, simulating bottom-up and top-down processes in neuroscience. Empirical experiments demonstrate robust neural representation learning across subjects for both pipelines. Moreover, merging high-level and low-level information improves both low-level and high-level reconstruction metrics. Additionally, we successfully transfer learned general knowledge to new subjects by training new adapters with limited training data. Compared to previous state-of-the-art methods, notably pre-training-based methods (Mind-Vis and fMRI-PTE), our approach achieves comparable or superior results across diverse tasks, showing promise as an alternative method for cross-subject fMRI data pre-training. Our code and pre-trained weights will be publicly released at https://github.com/YulongBonjour/See_Through_Their_Minds.",
      "authors": [
        "Yulong Liu",
        "Yongqiang Ma",
        "Guibo Zhu",
        "Haodong Jing",
        "Nanning Zheng"
      ],
      "published": "2024-03-11T01:18:49Z",
      "updated": "2024-06-13T14:17:04Z",
      "categories": [
        "cs.CV",
        "cs.HC"
      ],
      "pdf_url": "https://arxiv.org/pdf/2403.06361v2",
      "landing_url": "https://arxiv.org/abs/2403.06361v2",
      "doi": "https://doi.org/10.48550/arXiv.2403.06361"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The paper focuses on cross-subject fMRI neural representation learning and multimodal decoding for brain activity; it does not involve discrete audio/speech tokenization, so it should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The paper focuses on cross-subject fMRI neural representation learning and multimodal decoding for brain activity; it does not involve discrete audio/speech tokenization, so it should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on neural decoding of fMRI data related to vision and brain activity, with no mention of audio or speech tokenization, discrete audio symbol sequences, or quantization-based discrete token representations for audio reconstruction or understanding.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on neural decoding of fMRI data related to vision and brain activity, with no mention of audio or speech tokenization, discrete audio symbol sequences, or quantization-based discrete token representations for audio reconstruction or understanding.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Vector Quantization for Deep-Learning-Based CSI Feedback in Massive MIMO Systems",
    "abstract": "This paper presents a finite-rate deep-learning (DL)-based channel state information (CSI) feedback method for massive multiple-input multiple-output (MIMO) systems. The presented method provides a finite-bit representation of the latent vector based on a vector-quantized variational autoencoder (VQ-VAE) framework while reducing its computational complexity based on shape-gain vector quantization. In this method, the magnitude of the latent vector is quantized using a non-uniform scalar codebook with a proper transformation function, while the direction of the latent vector is quantized using a trainable Grassmannian codebook. A multi-rate codebook design strategy is also developed by introducing a codeword selection rule for a nested codebook along with the design of a loss function. Simulation results demonstrate that the proposed method reduces the computational complexity associated with VQ-VAE while improving CSI reconstruction performance under a given feedback overhead.",
    "metadata": {
      "arxiv_id": "2403.07355",
      "title": "Vector Quantization for Deep-Learning-Based CSI Feedback in Massive MIMO Systems",
      "summary": "This paper presents a finite-rate deep-learning (DL)-based channel state information (CSI) feedback method for massive multiple-input multiple-output (MIMO) systems. The presented method provides a finite-bit representation of the latent vector based on a vector-quantized variational autoencoder (VQ-VAE) framework while reducing its computational complexity based on shape-gain vector quantization. In this method, the magnitude of the latent vector is quantized using a non-uniform scalar codebook with a proper transformation function, while the direction of the latent vector is quantized using a trainable Grassmannian codebook. A multi-rate codebook design strategy is also developed by introducing a codeword selection rule for a nested codebook along with the design of a loss function. Simulation results demonstrate that the proposed method reduces the computational complexity associated with VQ-VAE while improving CSI reconstruction performance under a given feedback overhead.",
      "authors": [
        "Junyong Shin",
        "Yujin Kang",
        "Yo-Seb Jeon"
      ],
      "published": "2024-03-12T06:28:41Z",
      "updated": "2024-03-13T02:29:29Z",
      "categories": [
        "eess.SP",
        "cs.AI",
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2403.07355v2",
      "landing_url": "https://arxiv.org/abs/2403.07355v2",
      "doi": "https://doi.org/10.48550/arXiv.2403.07355"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The paper focuses on vector quantization for DL-based CSI feedback in wireless MIMO systems and does not address discrete audio/speech tokenization or token-level representations for speech tasks.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The paper focuses on vector quantization for DL-based CSI feedback in wireless MIMO systems and does not address discrete audio/speech tokenization or token-level representations for speech tasks.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on vector quantization techniques applied to channel state information feedback in massive MIMO systems and does not mention audio or speech tokenization, nor does it involve converting continuous audio signals into discrete tokens for audio reconstruction or related tasks; thus, it does not meet the inclusion criteria and instead falls outside the scope of discrete audio/speech tokenization research.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on vector quantization techniques applied to channel state information feedback in massive MIMO systems and does not mention audio or speech tokenization, nor does it involve converting continuous audio signals into discrete tokens for audio reconstruction or related tasks; thus, it does not meet the inclusion criteria and instead falls outside the scope of discrete audio/speech tokenization research.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Discrete Semantic Tokenization for Deep CTR Prediction",
    "abstract": "Incorporating item content information into click-through rate (CTR) prediction models remains a challenge, especially with the time and space constraints of industrial scenarios. The content-encoding paradigm, which integrates user and item encoders directly into CTR models, prioritizes space over time. In contrast, the embedding-based paradigm transforms item and user semantics into latent embeddings, subsequently caching them to optimize processing time at the expense of space. In this paper, we introduce a new semantic-token paradigm and propose a discrete semantic tokenization approach, namely UIST, for user and item representation. UIST facilitates swift training and inference while maintaining a conservative memory footprint. Specifically, UIST quantizes dense embedding vectors into discrete tokens with shorter lengths and employs a hierarchical mixture inference module to weigh the contribution of each user--item token pair. Our experimental results on news recommendation showcase the effectiveness and efficiency (about 200-fold space compression) of UIST for CTR prediction.",
    "metadata": {
      "arxiv_id": "2403.08206",
      "title": "Discrete Semantic Tokenization for Deep CTR Prediction",
      "summary": "Incorporating item content information into click-through rate (CTR) prediction models remains a challenge, especially with the time and space constraints of industrial scenarios. The content-encoding paradigm, which integrates user and item encoders directly into CTR models, prioritizes space over time. In contrast, the embedding-based paradigm transforms item and user semantics into latent embeddings, subsequently caching them to optimize processing time at the expense of space. In this paper, we introduce a new semantic-token paradigm and propose a discrete semantic tokenization approach, namely UIST, for user and item representation. UIST facilitates swift training and inference while maintaining a conservative memory footprint. Specifically, UIST quantizes dense embedding vectors into discrete tokens with shorter lengths and employs a hierarchical mixture inference module to weigh the contribution of each user--item token pair. Our experimental results on news recommendation showcase the effectiveness and efficiency (about 200-fold space compression) of UIST for CTR prediction.",
      "authors": [
        "Qijiong Liu",
        "Hengchang Hu",
        "Jiahao Wu",
        "Jieming Zhu",
        "Min-Yen Kan",
        "Xiao-Ming Wu"
      ],
      "published": "2024-03-13T03:03:15Z",
      "updated": "2024-03-21T15:17:46Z",
      "categories": [
        "cs.IR"
      ],
      "pdf_url": "https://arxiv.org/pdf/2403.08206v2",
      "landing_url": "https://arxiv.org/abs/2403.08206v2",
      "doi": "https://doi.org/10.48550/arXiv.2403.08206"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The paper focuses on discrete semantic tokenization for user/item representations in CTR prediction, not discrete audio/speech tokenization, so it does not meet the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The paper focuses on discrete semantic tokenization for user/item representations in CTR prediction, not discrete audio/speech tokenization, so it does not meet the inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on discrete semantic tokenization in the context of click-through rate prediction for user and item representations, with no indication of dealing with audio or speech signal tokenization as defined by the inclusion criteria; thus, it does not meet the core inclusion requirements related to discrete audio/speech tokenization.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on discrete semantic tokenization in the context of click-through rate prediction for user and item representations, with no indication of dealing with audio or speech signal tokenization as defined by the inclusion criteria; thus, it does not meet the core inclusion requirements related to discrete audio/speech tokenization.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "FoldToken: Learning Protein Language via Vector Quantization and Beyond",
    "abstract": "Is there a foreign language describing protein sequences and structures simultaneously? Protein structures, represented by continuous 3D points, have long posed a challenge due to the contrasting modeling paradigms of discrete sequences. We introduce \\textbf{FoldTokenizer} to represent protein sequence-structure as discrete symbols. This innovative approach involves projecting residue types and structures into a discrete space, guided by a reconstruction loss for information preservation. We refer to the learned discrete symbols as \\textbf{FoldToken}, and the sequence of FoldTokens serves as a new protein language, transforming the protein sequence-structure into a unified modality. We apply the created protein language on general backbone inpainting and antibody design tasks, building the first GPT-style model (\\textbf{FoldGPT}) for sequence-structure co-generation with promising results. Key to our success is the substantial enhancement of the vector quantization module, Soft Conditional Vector Quantization (\\textbf{SoftCVQ}).",
    "metadata": {
      "arxiv_id": "2403.09673",
      "title": "FoldToken: Learning Protein Language via Vector Quantization and Beyond",
      "summary": "Is there a foreign language describing protein sequences and structures simultaneously? Protein structures, represented by continuous 3D points, have long posed a challenge due to the contrasting modeling paradigms of discrete sequences. We introduce \\textbf{FoldTokenizer} to represent protein sequence-structure as discrete symbols. This innovative approach involves projecting residue types and structures into a discrete space, guided by a reconstruction loss for information preservation. We refer to the learned discrete symbols as \\textbf{FoldToken}, and the sequence of FoldTokens serves as a new protein language, transforming the protein sequence-structure into a unified modality. We apply the created protein language on general backbone inpainting and antibody design tasks, building the first GPT-style model (\\textbf{FoldGPT}) for sequence-structure co-generation with promising results. Key to our success is the substantial enhancement of the vector quantization module, Soft Conditional Vector Quantization (\\textbf{SoftCVQ}).",
      "authors": [
        "Zhangyang Gao",
        "Cheng Tan",
        "Jue Wang",
        "Yufei Huang",
        "Lirong Wu",
        "Stan Z. Li"
      ],
      "published": "2024-02-04T12:18:51Z",
      "updated": "2024-03-19T05:29:23Z",
      "categories": [
        "q-bio.BM",
        "cs.AI",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2403.09673v2",
      "landing_url": "https://arxiv.org/abs/2403.09673v2",
      "doi": "https://doi.org/10.48550/arXiv.2403.09673"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item centers on discrete tokenization of protein sequence-structure (FoldToken) for bioinformatics, not discrete audio/speech tokenization or speech-relevant tasks, so it does not meet the inclusion criteria focused on audio tokenization.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item centers on discrete tokenization of protein sequence-structure (FoldToken) for bioinformatics, not discrete audio/speech tokenization or speech-relevant tasks, so it does not meet the inclusion criteria focused on audio tokenization.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on discrete tokenization of protein sequences and structures rather than audio or speech signals; it does not address audio tokenization nor audio reconstruction, generation, or understanding tasks as required by the inclusion criteria, therefore it does not meet the inclusion criteria and falls outside the scope of the task.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on discrete tokenization of protein sequences and structures rather than audio or speech signals; it does not address audio tokenization nor audio reconstruction, generation, or understanding tasks as required by the inclusion criteria, therefore it does not meet the inclusion criteria and falls outside the scope of the task.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "MindEye2: Shared-Subject Models Enable fMRI-To-Image With 1 Hour of Data",
    "abstract": "Reconstructions of visual perception from brain activity have improved tremendously, but the practical utility of such methods has been limited. This is because such models are trained independently per subject where each subject requires dozens of hours of expensive fMRI training data to attain high-quality results. The present work showcases high-quality reconstructions using only 1 hour of fMRI training data. We pretrain our model across 7 subjects and then fine-tune on minimal data from a new subject. Our novel functional alignment procedure linearly maps all brain data to a shared-subject latent space, followed by a shared non-linear mapping to CLIP image space. We then map from CLIP space to pixel space by fine-tuning Stable Diffusion XL to accept CLIP latents as inputs instead of text. This approach improves out-of-subject generalization with limited training data and also attains state-of-the-art image retrieval and reconstruction metrics compared to single-subject approaches. MindEye2 demonstrates how accurate reconstructions of perception are possible from a single visit to the MRI facility. All code is available on GitHub.",
    "metadata": {
      "arxiv_id": "2403.11207",
      "title": "MindEye2: Shared-Subject Models Enable fMRI-To-Image With 1 Hour of Data",
      "summary": "Reconstructions of visual perception from brain activity have improved tremendously, but the practical utility of such methods has been limited. This is because such models are trained independently per subject where each subject requires dozens of hours of expensive fMRI training data to attain high-quality results. The present work showcases high-quality reconstructions using only 1 hour of fMRI training data. We pretrain our model across 7 subjects and then fine-tune on minimal data from a new subject. Our novel functional alignment procedure linearly maps all brain data to a shared-subject latent space, followed by a shared non-linear mapping to CLIP image space. We then map from CLIP space to pixel space by fine-tuning Stable Diffusion XL to accept CLIP latents as inputs instead of text. This approach improves out-of-subject generalization with limited training data and also attains state-of-the-art image retrieval and reconstruction metrics compared to single-subject approaches. MindEye2 demonstrates how accurate reconstructions of perception are possible from a single visit to the MRI facility. All code is available on GitHub.",
      "authors": [
        "Paul S. Scotti",
        "Mihir Tripathy",
        "Cesar Kadir Torrico Villanueva",
        "Reese Kneeland",
        "Tong Chen",
        "Ashutosh Narang",
        "Charan Santhirasegaran",
        "Jonathan Xu",
        "Thomas Naselaris",
        "Kenneth A. Norman",
        "Tanishq Mathew Abraham"
      ],
      "published": "2024-03-17T13:15:22Z",
      "updated": "2024-06-15T23:07:17Z",
      "categories": [
        "cs.CV",
        "cs.AI",
        "q-bio.NC"
      ],
      "pdf_url": "https://arxiv.org/pdf/2403.11207v2",
      "landing_url": "https://arxiv.org/abs/2403.11207v2",
      "doi": "https://doi.org/10.48550/arXiv.2403.11207"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The paper focuses on fMRI-to-image reconstruction and cross-subject alignment using CLIP and Stable Diffusion, with no discrete audio/speech tokenization analyzed or proposed.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The paper focuses on fMRI-to-image reconstruction and cross-subject alignment using CLIP and Stable Diffusion, with no discrete audio/speech tokenization analyzed or proposed.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on reconstructing images from fMRI brain data and does not address discrete audio/speech tokenization or sequence conversion of audio signals; thus, it does not meet the inclusion criteria related to discrete audio tokenization or analysis.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on reconstructing images from fMRI brain data and does not address discrete audio/speech tokenization or sequence conversion of audio signals; thus, it does not meet the inclusion criteria related to discrete audio tokenization or analysis.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Agentic AI: The Era of Semantic Decoding",
    "abstract": "Recent work demonstrated great promise in the idea of orchestrating collaborations between LLMs, human input, and various tools to address the inherent limitations of LLMs. We propose a novel perspective called semantic decoding, which frames these collaborative processes as optimization procedures in semantic space. Specifically, we conceptualize LLMs as semantic processors that manipulate meaningful pieces of information that we call semantic tokens (known thoughts). LLMs are among a large pool of other semantic processors, including humans and tools, such as search engines or code executors. Collectively, semantic processors engage in dynamic exchanges of semantic tokens to progressively construct high-utility outputs. We refer to these orchestrated interactions among semantic processors, optimizing and searching in semantic space, as semantic decoding algorithms. This concept draws a direct parallel to the well-studied problem of syntactic decoding, which involves crafting algorithms to best exploit auto-regressive language models for extracting high-utility sequences of syntactic tokens. By focusing on the semantic level and disregarding syntactic details, we gain a fresh perspective on the engineering of AI systems, enabling us to imagine systems with much greater complexity and capabilities. In this position paper, we formalize the transition from syntactic to semantic tokens as well as the analogy between syntactic and semantic decoding. Subsequently, we explore the possibilities of optimizing within the space of semantic tokens via semantic decoding algorithms. We conclude with a list of research opportunities and questions arising from this fresh perspective. The semantic decoding perspective offers a powerful abstraction for search and optimization directly in the space of meaningful concepts, with semantic tokens as the fundamental units of a new type of computation.",
    "metadata": {
      "arxiv_id": "2403.14562",
      "title": "Agentic AI: The Era of Semantic Decoding",
      "summary": "Recent work demonstrated great promise in the idea of orchestrating collaborations between LLMs, human input, and various tools to address the inherent limitations of LLMs. We propose a novel perspective called semantic decoding, which frames these collaborative processes as optimization procedures in semantic space. Specifically, we conceptualize LLMs as semantic processors that manipulate meaningful pieces of information that we call semantic tokens (known thoughts). LLMs are among a large pool of other semantic processors, including humans and tools, such as search engines or code executors. Collectively, semantic processors engage in dynamic exchanges of semantic tokens to progressively construct high-utility outputs. We refer to these orchestrated interactions among semantic processors, optimizing and searching in semantic space, as semantic decoding algorithms. This concept draws a direct parallel to the well-studied problem of syntactic decoding, which involves crafting algorithms to best exploit auto-regressive language models for extracting high-utility sequences of syntactic tokens. By focusing on the semantic level and disregarding syntactic details, we gain a fresh perspective on the engineering of AI systems, enabling us to imagine systems with much greater complexity and capabilities. In this position paper, we formalize the transition from syntactic to semantic tokens as well as the analogy between syntactic and semantic decoding. Subsequently, we explore the possibilities of optimizing within the space of semantic tokens via semantic decoding algorithms. We conclude with a list of research opportunities and questions arising from this fresh perspective. The semantic decoding perspective offers a powerful abstraction for search and optimization directly in the space of meaningful concepts, with semantic tokens as the fundamental units of a new type of computation.",
      "authors": [
        "Maxime Peyrard",
        "Martin Josifoski",
        "Robert West"
      ],
      "published": "2024-03-21T17:06:17Z",
      "updated": "2025-04-29T15:24:52Z",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.HC",
        "cs.MA"
      ],
      "pdf_url": "https://arxiv.org/pdf/2403.14562v2",
      "landing_url": "https://arxiv.org/abs/2403.14562v2",
      "doi": "https://doi.org/10.48550/arXiv.2403.14562"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The abstract presents semantic decoding and semantic tokens in AI collaboration but does not address discrete audio/speech tokenization or converting continuous audio signals into a discrete token vocabulary, so it does not meet the inclusion criteria and is not a match for the tokenization-focused exclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The abstract presents semantic decoding and semantic tokens in AI collaboration but does not address discrete audio/speech tokenization or converting continuous audio signals into a discrete token vocabulary, so it does not meet the inclusion criteria and is not a match for the tokenization-focused exclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study introduces the concept of semantic decoding and semantic tokens in the context of LLMs and AI collaboration, but it does not discuss discrete audio or speech tokenization, discrete quantization of audio signals, or related methodologies pertinent to audio reconstruction or understanding tasks; thus, it fails to meet the inclusion criteria focused on discrete audio/speech tokenization and also does not fall under any acceptable related domain based on the abstract provided.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study introduces the concept of semantic decoding and semantic tokens in the context of LLMs and AI collaboration, but it does not discuss discrete audio or speech tokenization, discrete quantization of audio signals, or related methodologies pertinent to audio reconstruction or understanding tasks; thus, it fails to meet the inclusion criteria focused on discrete audio/speech tokenization and also does not fall under any acceptable related domain based on the abstract provided.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Target Speech Extraction with Pre-trained AV-HuBERT and Mask-And-Recover Strategy",
    "abstract": "Audio-visual target speech extraction (AV-TSE) is one of the enabling technologies in robotics and many audio-visual applications. One of the challenges of AV-TSE is how to effectively utilize audio-visual synchronization information in the process. AV-HuBERT can be a useful pre-trained model for lip-reading, which has not been adopted by AV-TSE. In this paper, we would like to explore the way to integrate a pre-trained AV-HuBERT into our AV-TSE system. We have good reasons to expect an improved performance. To benefit from the inter and intra-modality correlations, we also propose a novel Mask-And-Recover (MAR) strategy for self-supervised learning. The experimental results on the VoxCeleb2 dataset show that our proposed model outperforms the baselines both in terms of subjective and objective metrics, suggesting that the pre-trained AV-HuBERT model provides more informative visual cues for target speech extraction. Furthermore, through a comparative study, we confirm that the proposed Mask-And-Recover strategy is significantly effective.",
    "metadata": {
      "arxiv_id": "2403.16078",
      "title": "Target Speech Extraction with Pre-trained AV-HuBERT and Mask-And-Recover Strategy",
      "summary": "Audio-visual target speech extraction (AV-TSE) is one of the enabling technologies in robotics and many audio-visual applications. One of the challenges of AV-TSE is how to effectively utilize audio-visual synchronization information in the process. AV-HuBERT can be a useful pre-trained model for lip-reading, which has not been adopted by AV-TSE. In this paper, we would like to explore the way to integrate a pre-trained AV-HuBERT into our AV-TSE system. We have good reasons to expect an improved performance. To benefit from the inter and intra-modality correlations, we also propose a novel Mask-And-Recover (MAR) strategy for self-supervised learning. The experimental results on the VoxCeleb2 dataset show that our proposed model outperforms the baselines both in terms of subjective and objective metrics, suggesting that the pre-trained AV-HuBERT model provides more informative visual cues for target speech extraction. Furthermore, through a comparative study, we confirm that the proposed Mask-And-Recover strategy is significantly effective.",
      "authors": [
        "Wenxuan Wu",
        "Xueyuan Chen",
        "Xixin Wu",
        "Haizhou Li",
        "Helen Meng"
      ],
      "published": "2024-03-24T09:42:05Z",
      "updated": "2024-03-24T09:42:05Z",
      "categories": [
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2403.16078v1",
      "landing_url": "https://arxiv.org/abs/2403.16078v1",
      "doi": "https://doi.org/10.48550/arXiv.2403.16078"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The abstract focuses on integrating a pre-trained AV-HuBERT model and a Mask-And-Recover self-supervised strategy for audio-visual target speech extraction; it does not explicitly propose, analyze, or evaluate discrete audio/speech tokenization (discrete tokens/vocabulary) as a core object of study, so it does not meet the tokenization-centric inclusion criteria.",
      "evaluation": 2
    },
    "round-A_JuniorNano_reasoning": "The abstract focuses on integrating a pre-trained AV-HuBERT model and a Mask-And-Recover self-supervised strategy for audio-visual target speech extraction; it does not explicitly propose, analyze, or evaluate discrete audio/speech tokenization (discrete tokens/vocabulary) as a core object of study, so it does not meet the tokenization-centric inclusion criteria.",
    "round-A_JuniorNano_evaluation": 2,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on audio-visual target speech extraction using a pre-trained AV-HuBERT model and a novel Mask-And-Recover strategy to utilize audio-visual synchronization, but it does not explicitly propose, analyze, or evaluate discrete audio/speech tokenization as a core object of study; rather, it centers on speech extraction performance improvements without detailing discrete token design or quantization relevant to language-model-style token sequences.",
      "evaluation": 2
    },
    "round-A_JuniorMini_reasoning": "The study focuses on audio-visual target speech extraction using a pre-trained AV-HuBERT model and a novel Mask-And-Recover strategy to utilize audio-visual synchronization, but it does not explicitly propose, analyze, or evaluate discrete audio/speech tokenization as a core object of study; rather, it centers on speech extraction performance improvements without detailing discrete token design or quantization relevant to language-model-style token sequences.",
    "round-A_JuniorMini_evaluation": 2,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:2)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:2)"
  },
  {
    "title": "Are NeRFs ready for autonomous driving? Towards closing the real-to-simulation gap",
    "abstract": "Neural Radiance Fields (NeRFs) have emerged as promising tools for advancing autonomous driving (AD) research, offering scalable closed-loop simulation and data augmentation capabilities. However, to trust the results achieved in simulation, one needs to ensure that AD systems perceive real and rendered data in the same way. Although the performance of rendering methods is increasing, many scenarios will remain inherently challenging to reconstruct faithfully. To this end, we propose a novel perspective for addressing the real-to-simulated data gap. Rather than solely focusing on improving rendering fidelity, we explore simple yet effective methods to enhance perception model robustness to NeRF artifacts without compromising performance on real data. Moreover, we conduct the first large-scale investigation into the real-to-simulated data gap in an AD setting using a state-of-the-art neural rendering technique. Specifically, we evaluate object detectors and an online mapping model on real and simulated data, and study the effects of different fine-tuning strategies.Our results show notable improvements in model robustness to simulated data, even improving real-world performance in some cases. Last, we delve into the correlation between the real-to-simulated gap and image reconstruction metrics, identifying FID and LPIPS as strong indicators. See https://research.zenseact.com/publications/closing-real2sim-gap for our project page.",
    "metadata": {
      "arxiv_id": "2403.16092",
      "title": "Are NeRFs ready for autonomous driving? Towards closing the real-to-simulation gap",
      "summary": "Neural Radiance Fields (NeRFs) have emerged as promising tools for advancing autonomous driving (AD) research, offering scalable closed-loop simulation and data augmentation capabilities. However, to trust the results achieved in simulation, one needs to ensure that AD systems perceive real and rendered data in the same way. Although the performance of rendering methods is increasing, many scenarios will remain inherently challenging to reconstruct faithfully. To this end, we propose a novel perspective for addressing the real-to-simulated data gap. Rather than solely focusing on improving rendering fidelity, we explore simple yet effective methods to enhance perception model robustness to NeRF artifacts without compromising performance on real data. Moreover, we conduct the first large-scale investigation into the real-to-simulated data gap in an AD setting using a state-of-the-art neural rendering technique. Specifically, we evaluate object detectors and an online mapping model on real and simulated data, and study the effects of different fine-tuning strategies.Our results show notable improvements in model robustness to simulated data, even improving real-world performance in some cases. Last, we delve into the correlation between the real-to-simulated gap and image reconstruction metrics, identifying FID and LPIPS as strong indicators. See https://research.zenseact.com/publications/closing-real2sim-gap for our project page.",
      "authors": [
        "Carl Lindström",
        "Georg Hess",
        "Adam Lilja",
        "Maryam Fatemi",
        "Lars Hammarstrand",
        "Christoffer Petersson",
        "Lennart Svensson"
      ],
      "published": "2024-03-24T11:09:41Z",
      "updated": "2024-04-15T10:06:41Z",
      "categories": [
        "cs.CV",
        "cs.RO"
      ],
      "pdf_url": "https://arxiv.org/pdf/2403.16092v2",
      "landing_url": "https://arxiv.org/abs/2403.16092v2",
      "doi": "https://doi.org/10.48550/arXiv.2403.16092"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item concerns NeRF-based real-to-sim data gap in autonomous driving and robustness to NeRF artifacts; there is no discrete audio tokenization or analysis of discrete audio representations, so it does not meet the required inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item concerns NeRF-based real-to-sim data gap in autonomous driving and robustness to NeRF artifacts; there is no discrete audio tokenization or analysis of discrete audio representations, so it does not meet the required inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on Neural Radiance Fields (NeRFs) and their application to autonomous driving simulation, addressing gaps between real and simulated data for perception models; it does not propose, analyze, or evaluate discrete audio/speech tokenization methods, nor does it include any focus on audio signals or token sequences relevant to speech or audio reconstruction tasks.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on Neural Radiance Fields (NeRFs) and their application to autonomous driving simulation, addressing gaps between real and simulated data for perception models; it does not propose, analyze, or evaluate discrete audio/speech tokenization methods, nor does it include any focus on audio signals or token sequences relevant to speech or audio reconstruction tasks.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Laplacian-guided Entropy Model in Neural Codec with Blur-dissipated Synthesis",
    "abstract": "While replacing Gaussian decoders with a conditional diffusion model enhances the perceptual quality of reconstructions in neural image compression, their lack of inductive bias for image data restricts their ability to achieve state-of-the-art perceptual levels. To address this limitation, we adopt a non-isotropic diffusion model at the decoder side. This model imposes an inductive bias aimed at distinguishing between frequency contents, thereby facilitating the generation of high-quality images. Moreover, our framework is equipped with a novel entropy model that accurately models the probability distribution of latent representation by exploiting spatio-channel correlations in latent space, while accelerating the entropy decoding step. This channel-wise entropy model leverages both local and global spatial contexts within each channel chunk. The global spatial context is built upon the Transformer, which is specifically designed for image compression tasks. The designed Transformer employs a Laplacian-shaped positional encoding, the learnable parameters of which are adaptively adjusted for each channel cluster. Our experiments demonstrate that our proposed framework yields better perceptual quality compared to cutting-edge generative-based codecs, and the proposed entropy model contributes to notable bitrate savings.",
    "metadata": {
      "arxiv_id": "2403.16258",
      "title": "Laplacian-guided Entropy Model in Neural Codec with Blur-dissipated Synthesis",
      "summary": "While replacing Gaussian decoders with a conditional diffusion model enhances the perceptual quality of reconstructions in neural image compression, their lack of inductive bias for image data restricts their ability to achieve state-of-the-art perceptual levels. To address this limitation, we adopt a non-isotropic diffusion model at the decoder side. This model imposes an inductive bias aimed at distinguishing between frequency contents, thereby facilitating the generation of high-quality images. Moreover, our framework is equipped with a novel entropy model that accurately models the probability distribution of latent representation by exploiting spatio-channel correlations in latent space, while accelerating the entropy decoding step. This channel-wise entropy model leverages both local and global spatial contexts within each channel chunk. The global spatial context is built upon the Transformer, which is specifically designed for image compression tasks. The designed Transformer employs a Laplacian-shaped positional encoding, the learnable parameters of which are adaptively adjusted for each channel cluster. Our experiments demonstrate that our proposed framework yields better perceptual quality compared to cutting-edge generative-based codecs, and the proposed entropy model contributes to notable bitrate savings.",
      "authors": [
        "Atefeh Khoshkhahtinat",
        "Ali Zafari",
        "Piyush M. Mehta",
        "Nasser M. Nasrabadi"
      ],
      "published": "2024-03-24T18:33:16Z",
      "updated": "2024-03-24T18:33:16Z",
      "categories": [
        "eess.IV",
        "cs.CV",
        "cs.IT",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2403.16258v1",
      "landing_url": "https://arxiv.org/abs/2403.16258v1",
      "doi": "https://doi.org/10.48550/arXiv.2403.16258"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The title/abstract describe a neural image codec with diffusion-based generation and a channel-wise entropy model for latent representations; there is no discrete audio/speech tokenization or analysis of discrete audio tokens, so it does not meet the inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The title/abstract describe a neural image codec with diffusion-based generation and a channel-wise entropy model for latent representations; there is no discrete audio/speech tokenization or analysis of discrete audio tokens, so it does not meet the inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on neural image compression and introduces an entropy model and diffusion decoder tailored for image data, without any mention of audio or speech tokenization or discrete audio token analysis, thus not meeting the inclusion criteria related to discrete audio/speech tokenization.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on neural image compression and introduces an entropy model and diffusion decoder tailored for image data, without any mention of audio or speech tokenization or discrete audio token analysis, thus not meeting the inclusion criteria related to discrete audio/speech tokenization.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "VoiceCraft: Zero-Shot Speech Editing and Text-to-Speech in the Wild",
    "abstract": "We introduce VoiceCraft, a token infilling neural codec language model, that achieves state-of-the-art performance on both speech editing and zero-shot text-to-speech (TTS) on audiobooks, internet videos, and podcasts. VoiceCraft employs a Transformer decoder architecture and introduces a token rearrangement procedure that combines causal masking and delayed stacking to enable generation within an existing sequence. On speech editing tasks, VoiceCraft produces edited speech that is nearly indistinguishable from unedited recordings in terms of naturalness, as evaluated by humans; for zero-shot TTS, our model outperforms prior SotA models including VALLE and the popular commercial model XTTS-v2. Crucially, the models are evaluated on challenging and realistic datasets, that consist of diverse accents, speaking styles, recording conditions, and background noise and music, and our model performs consistently well compared to other models and real recordings. In particular, for speech editing evaluation, we introduce a high quality, challenging, and realistic dataset named RealEdit. We encourage readers to listen to the demos at https://jasonppy.github.io/VoiceCraft_web.",
    "metadata": {
      "arxiv_id": "2403.16973",
      "title": "VoiceCraft: Zero-Shot Speech Editing and Text-to-Speech in the Wild",
      "summary": "We introduce VoiceCraft, a token infilling neural codec language model, that achieves state-of-the-art performance on both speech editing and zero-shot text-to-speech (TTS) on audiobooks, internet videos, and podcasts. VoiceCraft employs a Transformer decoder architecture and introduces a token rearrangement procedure that combines causal masking and delayed stacking to enable generation within an existing sequence. On speech editing tasks, VoiceCraft produces edited speech that is nearly indistinguishable from unedited recordings in terms of naturalness, as evaluated by humans; for zero-shot TTS, our model outperforms prior SotA models including VALLE and the popular commercial model XTTS-v2. Crucially, the models are evaluated on challenging and realistic datasets, that consist of diverse accents, speaking styles, recording conditions, and background noise and music, and our model performs consistently well compared to other models and real recordings. In particular, for speech editing evaluation, we introduce a high quality, challenging, and realistic dataset named RealEdit. We encourage readers to listen to the demos at https://jasonppy.github.io/VoiceCraft_web.",
      "authors": [
        "Puyuan Peng",
        "Po-Yao Huang",
        "Shang-Wen Li",
        "Abdelrahman Mohamed",
        "David Harwath"
      ],
      "published": "2024-03-25T17:38:32Z",
      "updated": "2024-06-14T00:29:46Z",
      "categories": [
        "eess.AS",
        "cs.AI",
        "cs.CL",
        "cs.LG",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2403.16973v3",
      "landing_url": "https://arxiv.org/abs/2403.16973v3",
      "doi": "https://doi.org/10.48550/arXiv.2403.16973"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The work describes a token-based neural codec for speech editing and zero-shot TTS, implying discrete audio/token representations as a core mechanism, which aligns with the inclusion criterion for discrete tokenization in speech.",
      "evaluation": 4
    },
    "round-A_JuniorNano_reasoning": "The work describes a token-based neural codec for speech editing and zero-shot TTS, implying discrete audio/token representations as a core mechanism, which aligns with the inclusion criterion for discrete tokenization in speech.",
    "round-A_JuniorNano_evaluation": 4,
    "round-A_JuniorMini_output": {
      "reasoning": "The study introduces VoiceCraft, a neural codec language model that performs zero-shot speech editing and TTS using discrete token-based representations, implying it analyzes discrete audio tokenization as core to its method. It presents novel methodology, systematic evaluation on diverse realistic datasets, and comparative results with prior state-of-the-art models. Thus, it meets all inclusion criteria and none of the exclusion criteria.",
      "evaluation": 5
    },
    "round-A_JuniorMini_reasoning": "The study introduces VoiceCraft, a neural codec language model that performs zero-shot speech editing and TTS using discrete token-based representations, implying it analyzes discrete audio tokenization as core to its method. It presents novel methodology, systematic evaluation on diverse realistic datasets, and comparative results with prior state-of-the-art models. Thus, it meets all inclusion criteria and none of the exclusion criteria.",
    "round-A_JuniorMini_evaluation": 5,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "include (junior:4)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Low-Latency Neural Stereo Streaming",
    "abstract": "The rise of new video modalities like virtual reality or autonomous driving has increased the demand for efficient multi-view video compression methods, both in terms of rate-distortion (R-D) performance and in terms of delay and runtime. While most recent stereo video compression approaches have shown promising performance, they compress left and right views sequentially, leading to poor parallelization and runtime performance. This work presents Low-Latency neural codec for Stereo video Streaming (LLSS), a novel parallel stereo video coding method designed for fast and efficient low-latency stereo video streaming. Instead of using a sequential cross-view motion compensation like existing methods, LLSS introduces a bidirectional feature shifting module to directly exploit mutual information among views and encode them effectively with a joint cross-view prior model for entropy coding. Thanks to this design, LLSS processes left and right views in parallel, minimizing latency; all while substantially improving R-D performance compared to both existing neural and conventional codecs.",
    "metadata": {
      "arxiv_id": "2403.17879",
      "title": "Low-Latency Neural Stereo Streaming",
      "summary": "The rise of new video modalities like virtual reality or autonomous driving has increased the demand for efficient multi-view video compression methods, both in terms of rate-distortion (R-D) performance and in terms of delay and runtime. While most recent stereo video compression approaches have shown promising performance, they compress left and right views sequentially, leading to poor parallelization and runtime performance. This work presents Low-Latency neural codec for Stereo video Streaming (LLSS), a novel parallel stereo video coding method designed for fast and efficient low-latency stereo video streaming. Instead of using a sequential cross-view motion compensation like existing methods, LLSS introduces a bidirectional feature shifting module to directly exploit mutual information among views and encode them effectively with a joint cross-view prior model for entropy coding. Thanks to this design, LLSS processes left and right views in parallel, minimizing latency; all while substantially improving R-D performance compared to both existing neural and conventional codecs.",
      "authors": [
        "Qiqi Hou",
        "Farzad Farhadzadeh",
        "Amir Said",
        "Guillaume Sautiere",
        "Hoang Le"
      ],
      "published": "2024-03-26T17:11:51Z",
      "updated": "2024-03-26T17:11:51Z",
      "categories": [
        "cs.CV",
        "eess.IV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2403.17879v1",
      "landing_url": "https://arxiv.org/abs/2403.17879v1",
      "doi": "https://doi.org/10.48550/arXiv.2403.17879"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item is about a neural stereo video codec with no discrete audio tokenization or speech representation; it does not meet inclusion criteria focused on discrete audio tokenization.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item is about a neural stereo video codec with no discrete audio tokenization or speech representation; it does not meet inclusion criteria focused on discrete audio tokenization.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on neural stereo video compression methods for streaming, explicitly dealing with video data and not audio or speech tokenization. It does not address discrete audio tokenization or any audio-related tokenization tasks as per the inclusion criteria, nor does it discuss discrete audio symbols or quantization for speech or audio signals. Therefore, it does not meet the inclusion criteria related to audio/speech tokenization and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on neural stereo video compression methods for streaming, explicitly dealing with video data and not audio or speech tokenization. It does not address discrete audio tokenization or any audio-related tokenization tasks as per the inclusion criteria, nor does it discuss discrete audio symbols or quantization for speech or audio signals. Therefore, it does not meet the inclusion criteria related to audio/speech tokenization and should be excluded.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Scaling Properties of Speech Language Models",
    "abstract": "Speech Language Models (SLMs) aim to learn language from raw audio, without textual resources. Despite significant advances, our current models exhibit weak syntax and semantic abilities. However, if the scaling properties of neural language models hold for the speech modality, these abilities will improve as the amount of compute used for training increases. In this paper, we use models of this scaling behavior to estimate the scale at which our current methods will yield a SLM with the English proficiency of text-based Large Language Models (LLMs). We establish a strong correlation between pre-training loss and downstream syntactic and semantic performance in SLMs and LLMs, which results in predictable scaling of linguistic performance. We show that the linguistic performance of SLMs scales up to three orders of magnitude more slowly than that of text-based LLMs. Additionally, we study the benefits of synthetic data designed to boost semantic understanding and the effects of coarser speech tokenization.",
    "metadata": {
      "arxiv_id": "2404.00685",
      "title": "Scaling Properties of Speech Language Models",
      "summary": "Speech Language Models (SLMs) aim to learn language from raw audio, without textual resources. Despite significant advances, our current models exhibit weak syntax and semantic abilities. However, if the scaling properties of neural language models hold for the speech modality, these abilities will improve as the amount of compute used for training increases. In this paper, we use models of this scaling behavior to estimate the scale at which our current methods will yield a SLM with the English proficiency of text-based Large Language Models (LLMs). We establish a strong correlation between pre-training loss and downstream syntactic and semantic performance in SLMs and LLMs, which results in predictable scaling of linguistic performance. We show that the linguistic performance of SLMs scales up to three orders of magnitude more slowly than that of text-based LLMs. Additionally, we study the benefits of synthetic data designed to boost semantic understanding and the effects of coarser speech tokenization.",
      "authors": [
        "Santiago Cuervo",
        "Ricard Marxer"
      ],
      "published": "2024-03-31T13:30:12Z",
      "updated": "2024-04-16T06:46:18Z",
      "categories": [
        "eess.AS",
        "cs.AI",
        "cs.CL",
        "cs.NE"
      ],
      "pdf_url": "https://arxiv.org/pdf/2404.00685v2",
      "landing_url": "https://arxiv.org/abs/2404.00685v2",
      "doi": "https://doi.org/10.18653/v1/2024.emnlp-main.21"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The abstract explicitly discusses discrete speech tokenization (e.g., coarser speech tokenization) as part of the study, indicating analysis of discrete audio tokens rather than relying solely on continuous features, which aligns with the inclusion criteria while not being obviously exclusionary.",
      "evaluation": 4
    },
    "round-A_JuniorNano_reasoning": "The abstract explicitly discusses discrete speech tokenization (e.g., coarser speech tokenization) as part of the study, indicating analysis of discrete audio tokens rather than relying solely on continuous features, which aligns with the inclusion criteria while not being obviously exclusionary.",
    "round-A_JuniorNano_evaluation": 4,
    "round-A_JuniorMini_output": {
      "reasoning": "The study discusses the scaling properties of speech language models, including aspects related to speech tokenization such as the effects of coarser speech tokenization, indicating an analysis relevant to discrete audio tokenization. However, the core focus appears to be on linguistic performance scaling rather than proposing or deeply analyzing discrete tokenization methods themselves, making it ambiguous whether the paper meets the inclusion criteria fully.",
      "evaluation": 3
    },
    "round-A_JuniorMini_reasoning": "The study discusses the scaling properties of speech language models, including aspects related to speech tokenization such as the effects of coarser speech tokenization, indicating an analysis relevant to discrete audio tokenization. However, the core focus appears to be on linguistic performance scaling rather than proposing or deeply analyzing discrete tokenization methods themselves, making it ambiguous whether the paper meets the inclusion criteria fully.",
    "round-A_JuniorMini_evaluation": 3,
    "round-B_SeniorLead_output": {
      "reasoning": "I reviewed the two junior reviewers (scores 4 and 3) and the abstract: it explicitly reports studying \"the effects of coarser speech tokenization,\" which indicates analysis of discrete audio tokenization, though tokenization appears secondary to the paper's scaling focus and the full-text availability is not confirmed. Therefore I assign 4 — better to include: the abstract explicitly analyzes discrete speech tokenization (meeting the inclusion criterion) but the tokenization aspect may not be a primary/core focus, so some uncertainty remains.",
      "evaluation": 4
    },
    "round-B_SeniorLead_reasoning": "I reviewed the two junior reviewers (scores 4 and 3) and the abstract: it explicitly reports studying \"the effects of coarser speech tokenization,\" which indicates analysis of discrete audio tokenization, though tokenization appears secondary to the paper's scaling focus and the full-text availability is not confirmed. Therefore I assign 4 — better to include: the abstract explicitly analyzes discrete speech tokenization (meeting the inclusion criterion) but the tokenization aspect may not be a primary/core focus, so some uncertainty remains.",
    "round-B_SeniorLead_evaluation": 4,
    "final_verdict": "include (senior:4)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Large Motion Model for Unified Multi-Modal Motion Generation",
    "abstract": "Human motion generation, a cornerstone technique in animation and video production, has widespread applications in various tasks like text-to-motion and music-to-dance. Previous works focus on developing specialist models tailored for each task without scalability. In this work, we present Large Motion Model (LMM), a motion-centric, multi-modal framework that unifies mainstream motion generation tasks into a generalist model. A unified motion model is appealing since it can leverage a wide range of motion data to achieve broad generalization beyond a single task. However, it is also challenging due to the heterogeneous nature of substantially different motion data and tasks. LMM tackles these challenges from three principled aspects: 1) Data: We consolidate datasets with different modalities, formats and tasks into a comprehensive yet unified motion generation dataset, MotionVerse, comprising 10 tasks, 16 datasets, a total of 320k sequences, and 100 million frames. 2) Architecture: We design an articulated attention mechanism ArtAttention that incorporates body part-aware modeling into Diffusion Transformer backbone. 3) Pre-Training: We propose a novel pre-training strategy for LMM, which employs variable frame rates and masking forms, to better exploit knowledge from diverse training data. Extensive experiments demonstrate that our generalist LMM achieves competitive performance across various standard motion generation tasks over state-of-the-art specialist models. Notably, LMM exhibits strong generalization capabilities and emerging properties across many unseen tasks. Additionally, our ablation studies reveal valuable insights about training and scaling up large motion models for future research.",
    "metadata": {
      "arxiv_id": "2404.01284",
      "title": "Large Motion Model for Unified Multi-Modal Motion Generation",
      "summary": "Human motion generation, a cornerstone technique in animation and video production, has widespread applications in various tasks like text-to-motion and music-to-dance. Previous works focus on developing specialist models tailored for each task without scalability. In this work, we present Large Motion Model (LMM), a motion-centric, multi-modal framework that unifies mainstream motion generation tasks into a generalist model. A unified motion model is appealing since it can leverage a wide range of motion data to achieve broad generalization beyond a single task. However, it is also challenging due to the heterogeneous nature of substantially different motion data and tasks. LMM tackles these challenges from three principled aspects: 1) Data: We consolidate datasets with different modalities, formats and tasks into a comprehensive yet unified motion generation dataset, MotionVerse, comprising 10 tasks, 16 datasets, a total of 320k sequences, and 100 million frames. 2) Architecture: We design an articulated attention mechanism ArtAttention that incorporates body part-aware modeling into Diffusion Transformer backbone. 3) Pre-Training: We propose a novel pre-training strategy for LMM, which employs variable frame rates and masking forms, to better exploit knowledge from diverse training data. Extensive experiments demonstrate that our generalist LMM achieves competitive performance across various standard motion generation tasks over state-of-the-art specialist models. Notably, LMM exhibits strong generalization capabilities and emerging properties across many unseen tasks. Additionally, our ablation studies reveal valuable insights about training and scaling up large motion models for future research.",
      "authors": [
        "Mingyuan Zhang",
        "Daisheng Jin",
        "Chenyang Gu",
        "Fangzhou Hong",
        "Zhongang Cai",
        "Jingfang Huang",
        "Chongzhi Zhang",
        "Xinying Guo",
        "Lei Yang",
        "Ying He",
        "Ziwei Liu"
      ],
      "published": "2024-04-01T17:55:11Z",
      "updated": "2024-04-01T17:55:11Z",
      "categories": [
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2404.01284v1",
      "landing_url": "https://arxiv.org/abs/2404.01284v1",
      "doi": "https://doi.org/10.48550/arXiv.2404.01284"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The work centers on a generalist model for motion generation across modalities and does not involve discrete audio/speech tokenization or analysis of discrete audio representations.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The work centers on a generalist model for motion generation across modalities and does not involve discrete audio/speech tokenization or analysis of discrete audio representations.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on a large motion model unifying multi-modal motion generation tasks, emphasizing motion data consolidation, architecture, and pre-training strategies; however, it does not address discrete audio/speech tokenization nor analyze any discrete tokenization related to audio, making it irrelevant to the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on a large motion model unifying multi-modal motion generation tasks, emphasizing motion data consolidation, architecture, and pre-training strategies; however, it does not address discrete audio/speech tokenization nor analyze any discrete tokenization related to audio, making it irrelevant to the inclusion criteria.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "CLaM-TTS: Improving Neural Codec Language Model for Zero-Shot Text-to-Speech",
    "abstract": "With the emergence of neural audio codecs, which encode multiple streams of discrete tokens from audio, large language models have recently gained attention as a promising approach for zero-shot Text-to-Speech (TTS) synthesis. Despite the ongoing rush towards scaling paradigms, audio tokenization ironically amplifies the scalability challenge, stemming from its long sequence length and the complexity of modelling the multiple sequences. To mitigate these issues, we present CLaM-TTS that employs a probabilistic residual vector quantization to (1) achieve superior compression in the token length, and (2) allow a language model to generate multiple tokens at once, thereby eliminating the need for cascaded modeling to handle the number of token streams. Our experimental results demonstrate that CLaM-TTS is better than or comparable to state-of-the-art neural codec-based TTS models regarding naturalness, intelligibility, speaker similarity, and inference speed. In addition, we examine the impact of the pretraining extent of the language models and their text tokenization strategies on performances.",
    "metadata": {
      "arxiv_id": "2404.02781",
      "title": "CLaM-TTS: Improving Neural Codec Language Model for Zero-Shot Text-to-Speech",
      "summary": "With the emergence of neural audio codecs, which encode multiple streams of discrete tokens from audio, large language models have recently gained attention as a promising approach for zero-shot Text-to-Speech (TTS) synthesis. Despite the ongoing rush towards scaling paradigms, audio tokenization ironically amplifies the scalability challenge, stemming from its long sequence length and the complexity of modelling the multiple sequences. To mitigate these issues, we present CLaM-TTS that employs a probabilistic residual vector quantization to (1) achieve superior compression in the token length, and (2) allow a language model to generate multiple tokens at once, thereby eliminating the need for cascaded modeling to handle the number of token streams. Our experimental results demonstrate that CLaM-TTS is better than or comparable to state-of-the-art neural codec-based TTS models regarding naturalness, intelligibility, speaker similarity, and inference speed. In addition, we examine the impact of the pretraining extent of the language models and their text tokenization strategies on performances.",
      "authors": [
        "Jaehyeon Kim",
        "Keon Lee",
        "Seungjun Chung",
        "Jaewoong Cho"
      ],
      "published": "2024-04-03T14:52:20Z",
      "updated": "2024-04-03T14:52:20Z",
      "categories": [
        "eess.AS",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2404.02781v1",
      "landing_url": "https://arxiv.org/abs/2404.02781v1",
      "doi": "https://doi.org/10.48550/arXiv.2404.02781"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The abstract centers on discrete audio tokenization via neural codecs and vector quantization for zero-shot TTS, analyzing tokenization strategies and token-length compression, which directly matches the inclusion criteria and does not fall into any exclusion category.",
      "evaluation": 5
    },
    "round-A_JuniorNano_reasoning": "The abstract centers on discrete audio tokenization via neural codecs and vector quantization for zero-shot TTS, analyzing tokenization strategies and token-length compression, which directly matches the inclusion criteria and does not fall into any exclusion category.",
    "round-A_JuniorNano_evaluation": 5,
    "round-A_JuniorMini_output": {
      "reasoning": "The study explicitly proposes and evaluates a neural codec language model focusing on discrete audio tokenization, employing probabilistic residual vector quantization to compress token length and improve multiple token generation, which aligns directly with the inclusion criteria requiring analysis of discrete audio/speech tokenization; it involves neural encoding techniques rather than traditional codecs and includes experimental validation, with the full text presumably in English for academic evaluation, thus meeting all inclusion and none of the exclusion criteria.",
      "evaluation": 5
    },
    "round-A_JuniorMini_reasoning": "The study explicitly proposes and evaluates a neural codec language model focusing on discrete audio tokenization, employing probabilistic residual vector quantization to compress token length and improve multiple token generation, which aligns directly with the inclusion criteria requiring analysis of discrete audio/speech tokenization; it involves neural encoding techniques rather than traditional codecs and includes experimental validation, with the full text presumably in English for academic evaluation, thus meeting all inclusion and none of the exclusion criteria.",
    "round-A_JuniorMini_evaluation": 5,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "include (junior:5)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "RALL-E: Robust Codec Language Modeling with Chain-of-Thought Prompting for Text-to-Speech Synthesis",
    "abstract": "We present RALL-E, a robust language modeling method for text-to-speech (TTS) synthesis. While previous work based on large language models (LLMs) shows impressive performance on zero-shot TTS, such methods often suffer from poor robustness, such as unstable prosody (weird pitch and rhythm/duration) and a high word error rate (WER), due to the autoregressive prediction style of language models. The core idea behind RALL-E is chain-of-thought (CoT) prompting, which decomposes the task into simpler steps to enhance the robustness of LLM-based TTS. To accomplish this idea, RALL-E first predicts prosody features (pitch and duration) of the input text and uses them as intermediate conditions to predict speech tokens in a CoT style. Second, RALL-E utilizes the predicted duration prompt to guide the computing of self-attention weights in Transformer to enforce the model to focus on the corresponding phonemes and prosody features when predicting speech tokens. Results of comprehensive objective and subjective evaluations demonstrate that, compared to a powerful baseline method VALL-E, RALL-E significantly improves the WER of zero-shot TTS from $5.6\\%$ (without reranking) and $1.7\\%$ (with reranking) to $2.5\\%$ and $1.0\\%$, respectively. Furthermore, we demonstrate that RALL-E correctly synthesizes sentences that are hard for VALL-E and reduces the error rate from $68\\%$ to $4\\%$.",
    "metadata": {
      "arxiv_id": "2404.03204",
      "title": "RALL-E: Robust Codec Language Modeling with Chain-of-Thought Prompting for Text-to-Speech Synthesis",
      "summary": "We present RALL-E, a robust language modeling method for text-to-speech (TTS) synthesis. While previous work based on large language models (LLMs) shows impressive performance on zero-shot TTS, such methods often suffer from poor robustness, such as unstable prosody (weird pitch and rhythm/duration) and a high word error rate (WER), due to the autoregressive prediction style of language models. The core idea behind RALL-E is chain-of-thought (CoT) prompting, which decomposes the task into simpler steps to enhance the robustness of LLM-based TTS. To accomplish this idea, RALL-E first predicts prosody features (pitch and duration) of the input text and uses them as intermediate conditions to predict speech tokens in a CoT style. Second, RALL-E utilizes the predicted duration prompt to guide the computing of self-attention weights in Transformer to enforce the model to focus on the corresponding phonemes and prosody features when predicting speech tokens. Results of comprehensive objective and subjective evaluations demonstrate that, compared to a powerful baseline method VALL-E, RALL-E significantly improves the WER of zero-shot TTS from $5.6\\%$ (without reranking) and $1.7\\%$ (with reranking) to $2.5\\%$ and $1.0\\%$, respectively. Furthermore, we demonstrate that RALL-E correctly synthesizes sentences that are hard for VALL-E and reduces the error rate from $68\\%$ to $4\\%$.",
      "authors": [
        "Detai Xin",
        "Xu Tan",
        "Kai Shen",
        "Zeqian Ju",
        "Dongchao Yang",
        "Yuancheng Wang",
        "Shinnosuke Takamichi",
        "Hiroshi Saruwatari",
        "Shujie Liu",
        "Jinyu Li",
        "Sheng Zhao"
      ],
      "published": "2024-04-04T05:15:07Z",
      "updated": "2024-05-19T21:34:28Z",
      "categories": [
        "eess.AS",
        "cs.AI",
        "cs.CL",
        "cs.LG",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2404.03204v3",
      "landing_url": "https://arxiv.org/abs/2404.03204v3",
      "doi": "https://doi.org/10.48550/arXiv.2404.03204"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "Abstract describes using discrete 'speech tokens' via a neural codec/tokenization approach for TTS, aligning with discrete audio tokenization as a core concept; however, full-text availability cannot be confirmed.",
      "evaluation": 4
    },
    "round-A_JuniorNano_reasoning": "Abstract describes using discrete 'speech tokens' via a neural codec/tokenization approach for TTS, aligning with discrete audio tokenization as a core concept; however, full-text availability cannot be confirmed.",
    "round-A_JuniorNano_evaluation": 4,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on improving text-to-speech synthesis robustness using large language models and chain-of-thought prompting, emphasizing prosody features and speech token prediction; however, it does not explicitly mention proposing, analyzing, or evaluating discrete audio/speech tokenization as a core object of study, nor does it detail any novel discrete tokenization method or quantization-based representation central to the work.",
      "evaluation": 2
    },
    "round-A_JuniorMini_reasoning": "The study focuses on improving text-to-speech synthesis robustness using large language models and chain-of-thought prompting, emphasizing prosody features and speech token prediction; however, it does not explicitly mention proposing, analyzing, or evaluating discrete audio/speech tokenization as a core object of study, nor does it detail any novel discrete tokenization method or quantization-based representation central to the work.",
    "round-A_JuniorMini_evaluation": 2,
    "round-B_SeniorLead_output": {
      "reasoning": "Both junior reviewers noted the abstract mentions predicting \"speech tokens\" (one leaning include) but does not explicitly propose, analyze, or evaluate discrete audio/speech tokenization as a core contribution (the other leaning exclude); because the abstract centers on CoT prompting and prosody prediction without clear discrete-tokenization design or analysis, it should be excluded.",
      "evaluation": 2
    },
    "round-B_SeniorLead_reasoning": "Both junior reviewers noted the abstract mentions predicting \"speech tokens\" (one leaning include) but does not explicitly propose, analyze, or evaluate discrete audio/speech tokenization as a core contribution (the other leaning exclude); because the abstract centers on CoT prompting and prosody prediction without clear discrete-tokenization design or analysis, it should be excluded.",
    "round-B_SeniorLead_evaluation": 2,
    "final_verdict": "exclude (senior:2)",
    "review_skipped": false,
    "discard_reason": "exclude (senior:2)"
  },
  {
    "title": "Cross-Domain Audio Deepfake Detection: Dataset and Analysis",
    "abstract": "Audio deepfake detection (ADD) is essential for preventing the misuse of synthetic voices that may infringe on personal rights and privacy. Recent zero-shot text-to-speech (TTS) models pose higher risks as they can clone voices with a single utterance. However, the existing ADD datasets are outdated, leading to suboptimal generalization of detection models. In this paper, we construct a new cross-domain ADD dataset comprising over 300 hours of speech data that is generated by five advanced zero-shot TTS models. To simulate real-world scenarios, we employ diverse attack methods and audio prompts from different datasets. Experiments show that, through novel attack-augmented training, the Wav2Vec2-large and Whisper-medium models achieve equal error rates of 4.1\\% and 6.5\\% respectively. Additionally, we demonstrate our models' outstanding few-shot ADD ability by fine-tuning with just one minute of target-domain data. Nonetheless, neural codec compressors greatly affect the detection accuracy, necessitating further research.",
    "metadata": {
      "arxiv_id": "2404.04904",
      "title": "Cross-Domain Audio Deepfake Detection: Dataset and Analysis",
      "summary": "Audio deepfake detection (ADD) is essential for preventing the misuse of synthetic voices that may infringe on personal rights and privacy. Recent zero-shot text-to-speech (TTS) models pose higher risks as they can clone voices with a single utterance. However, the existing ADD datasets are outdated, leading to suboptimal generalization of detection models. In this paper, we construct a new cross-domain ADD dataset comprising over 300 hours of speech data that is generated by five advanced zero-shot TTS models. To simulate real-world scenarios, we employ diverse attack methods and audio prompts from different datasets. Experiments show that, through novel attack-augmented training, the Wav2Vec2-large and Whisper-medium models achieve equal error rates of 4.1\\% and 6.5\\% respectively. Additionally, we demonstrate our models' outstanding few-shot ADD ability by fine-tuning with just one minute of target-domain data. Nonetheless, neural codec compressors greatly affect the detection accuracy, necessitating further research.",
      "authors": [
        "Yuang Li",
        "Min Zhang",
        "Mengxin Ren",
        "Miaomiao Ma",
        "Daimeng Wei",
        "Hao Yang"
      ],
      "published": "2024-04-07T10:10:15Z",
      "updated": "2024-09-20T08:08:53Z",
      "categories": [
        "cs.SD",
        "cs.AI",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2404.04904v2",
      "landing_url": "https://arxiv.org/abs/2404.04904v2",
      "doi": "https://doi.org/10.48550/arXiv.2404.04904"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item centers on cross-domain audio deepfake detection, dataset construction, and model performance; it does not propose or analyze discrete audio tokenization or a token-based discrete representation as a core object, thus it fails the inclusion criteria and aligns with exclusion criteria focused on tokenization-focused work.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item centers on cross-domain audio deepfake detection, dataset construction, and model performance; it does not propose or analyze discrete audio tokenization or a token-based discrete representation as a core object, thus it fails the inclusion criteria and aligns with exclusion criteria focused on tokenization-focused work.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on audio deepfake detection using models like Wav2Vec2 and Whisper, and discusses audio data and detection performance, but it does not explicitly propose or analyze discrete audio/speech tokenization methods converting continuous audio into discrete token sequences; thus, it does not meet the inclusion criterion of studying discrete tokenization as the core subject.",
      "evaluation": 2
    },
    "round-A_JuniorMini_reasoning": "The study focuses on audio deepfake detection using models like Wav2Vec2 and Whisper, and discusses audio data and detection performance, but it does not explicitly propose or analyze discrete audio/speech tokenization methods converting continuous audio into discrete token sequences; thus, it does not meet the inclusion criterion of studying discrete tokenization as the core subject.",
    "round-A_JuniorMini_evaluation": 2,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:2)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:2)"
  },
  {
    "title": "CodecNeRF: Toward Fast Encoding and Decoding, Compact, and High-quality Novel-view Synthesis",
    "abstract": "Neural Radiance Fields (NeRF) have achieved huge success in effectively capturing and representing 3D objects and scenes. However, to establish a ubiquitous presence in everyday media formats, such as images and videos, we need to fulfill three key objectives: 1. fast encoding and decoding time, 2. compact model sizes, and 3. high-quality renderings. Despite recent advancements, a comprehensive algorithm that adequately addresses all objectives has yet to be fully realized. In this work, we present CodecNeRF, a neural codec for NeRF representations, consisting of an encoder and decoder architecture that can generate a NeRF representation in a single forward pass. Furthermore, inspired by the recent parameter-efficient finetuning approaches, we propose a finetuning method to efficiently adapt the generated NeRF representations to a new test instance, leading to high-quality image renderings and compact code sizes. The proposed CodecNeRF, a newly suggested encoding-decoding-finetuning pipeline for NeRF, achieved unprecedented compression performance of more than 100x and remarkable reduction in encoding time while maintaining (or improving) the image quality on widely used 3D object datasets.",
    "metadata": {
      "arxiv_id": "2404.04913",
      "title": "CodecNeRF: Toward Fast Encoding and Decoding, Compact, and High-quality Novel-view Synthesis",
      "summary": "Neural Radiance Fields (NeRF) have achieved huge success in effectively capturing and representing 3D objects and scenes. However, to establish a ubiquitous presence in everyday media formats, such as images and videos, we need to fulfill three key objectives: 1. fast encoding and decoding time, 2. compact model sizes, and 3. high-quality renderings. Despite recent advancements, a comprehensive algorithm that adequately addresses all objectives has yet to be fully realized. In this work, we present CodecNeRF, a neural codec for NeRF representations, consisting of an encoder and decoder architecture that can generate a NeRF representation in a single forward pass. Furthermore, inspired by the recent parameter-efficient finetuning approaches, we propose a finetuning method to efficiently adapt the generated NeRF representations to a new test instance, leading to high-quality image renderings and compact code sizes. The proposed CodecNeRF, a newly suggested encoding-decoding-finetuning pipeline for NeRF, achieved unprecedented compression performance of more than 100x and remarkable reduction in encoding time while maintaining (or improving) the image quality on widely used 3D object datasets.",
      "authors": [
        "Gyeongjin Kang",
        "Younggeun Lee",
        "Seungjun Oh",
        "Eunbyung Park"
      ],
      "published": "2024-04-07T10:49:59Z",
      "updated": "2024-09-25T07:16:41Z",
      "categories": [
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2404.04913v3",
      "landing_url": "https://arxiv.org/abs/2404.04913v3",
      "doi": "https://doi.org/10.48550/arXiv.2404.04913"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The article focuses on CodecNeRF for fast encoding/decoding of NeRF representations for 3D scenes, with no discussion of discrete audio/speech tokenization; thus it does not meet the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The article focuses on CodecNeRF for fast encoding/decoding of NeRF representations for 3D scenes, with no discussion of discrete audio/speech tokenization; thus it does not meet the inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on neural radiance fields and novel-view synthesis of 3D objects and scenes, proposing a codec for NeRF representations to achieve fast encoding and decoding, compact model size, and high-quality rendering. It does not discuss the discrete tokenization of audio or speech signals, nor does it analyze discrete audio representations. Thus, it does not meet the inclusion criteria concerning discrete audio/speech tokenization and is not relevant to the specified audio tokenization tasks.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on neural radiance fields and novel-view synthesis of 3D objects and scenes, proposing a codec for NeRF representations to achieve fast encoding and decoding, compact model size, and high-quality rendering. It does not discuss the discrete tokenization of audio or speech signals, nor does it analyze discrete audio representations. Thus, it does not meet the inclusion criteria concerning discrete audio/speech tokenization and is not relevant to the specified audio tokenization tasks.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Fuzzy K-Means Clustering without Cluster Centroids",
    "abstract": "Fuzzy K-Means clustering is a critical technique in unsupervised data analysis. Unlike traditional hard clustering algorithms such as K-Means, it allows data points to belong to multiple clusters with varying degrees of membership, determined through iterative optimization to establish optimal cluster centers and memberships, thereby achieving fuzzy partitioning of data. However, the performance of popular Fuzzy K-Means algorithms is sensitive to the selection of initial cluster centroids and is also affected by noise when updating mean cluster centroids. To address these challenges, this paper proposes a novel Fuzzy \\textit{K}-Means clustering algorithm that entirely eliminates the reliance on cluster centroids, obtaining membership metrics solely through distance matrix computation. This innovation enhances flexibility in distance measurement between sample points, thus improving the algorithm's performance and robustness. The paper also establishes theoretical connections between the proposed model and popular Fuzzy K-Means clustering techniques. Experimental results on several real datasets demonstrate the effectiveness of the algorithm.",
    "metadata": {
      "arxiv_id": "2404.04940",
      "title": "Fuzzy K-Means Clustering without Cluster Centroids",
      "summary": "Fuzzy K-Means clustering is a critical technique in unsupervised data analysis. Unlike traditional hard clustering algorithms such as K-Means, it allows data points to belong to multiple clusters with varying degrees of membership, determined through iterative optimization to establish optimal cluster centers and memberships, thereby achieving fuzzy partitioning of data. However, the performance of popular Fuzzy K-Means algorithms is sensitive to the selection of initial cluster centroids and is also affected by noise when updating mean cluster centroids. To address these challenges, this paper proposes a novel Fuzzy \\textit{K}-Means clustering algorithm that entirely eliminates the reliance on cluster centroids, obtaining membership metrics solely through distance matrix computation. This innovation enhances flexibility in distance measurement between sample points, thus improving the algorithm's performance and robustness. The paper also establishes theoretical connections between the proposed model and popular Fuzzy K-Means clustering techniques. Experimental results on several real datasets demonstrate the effectiveness of the algorithm.",
      "authors": [
        "Yichen Bao",
        "Han Lu",
        "Quanxue Gao"
      ],
      "published": "2024-04-07T12:25:03Z",
      "updated": "2024-11-07T08:59:23Z",
      "categories": [
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2404.04940v2",
      "landing_url": "https://arxiv.org/abs/2404.04940v2",
      "doi": "https://doi.org/10.48550/arXiv.2404.04940"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item discusses a centroid-free fuzzy K-Means clustering algorithm with a focus on distance matrices, and does not address discrete audio/speech tokenization, token design, or token-based speech tasks; thus it does not meet the inclusion criteria for discrete audio tokenization analysis and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item discusses a centroid-free fuzzy K-Means clustering algorithm with a focus on distance matrices, and does not address discrete audio/speech tokenization, token design, or token-based speech tasks; thus it does not meet the inclusion criteria for discrete audio tokenization analysis and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on a novel fuzzy K-means clustering algorithm without cluster centroids, primarily addressing unsupervised data analysis and clustering methods; however, it does not explicitly propose, analyze, or evaluate discrete audio/speech tokenization processes converting continuous audio signals into discrete symbols. There is no mention of audio or speech data, discrete tokenization, or related neural or quantization-based token design as per the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on a novel fuzzy K-means clustering algorithm without cluster centroids, primarily addressing unsupervised data analysis and clustering methods; however, it does not explicitly propose, analyze, or evaluate discrete audio/speech tokenization processes converting continuous audio signals into discrete symbols. There is no mention of audio or speech data, discrete tokenization, or related neural or quantization-based token design as per the inclusion criteria.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "The X-LANCE Technical Report for Interspeech 2024 Speech Processing Using Discrete Speech Unit Challenge",
    "abstract": "Discrete speech tokens have been more and more popular in multiple speech processing fields, including automatic speech recognition (ASR), text-to-speech (TTS) and singing voice synthesis (SVS). In this paper, we describe the systems developed by the SJTU X-LANCE group for the TTS (acoustic + vocoder), SVS, and ASR tracks in the Interspeech 2024 Speech Processing Using Discrete Speech Unit Challenge. Notably, we achieved 1st rank on the leaderboard in the TTS track both with the whole training set and only 1h training data, with the highest UTMOS score and lowest bitrate among all submissions.",
    "metadata": {
      "arxiv_id": "2404.06079",
      "title": "The X-LANCE Technical Report for Interspeech 2024 Speech Processing Using Discrete Speech Unit Challenge",
      "summary": "Discrete speech tokens have been more and more popular in multiple speech processing fields, including automatic speech recognition (ASR), text-to-speech (TTS) and singing voice synthesis (SVS). In this paper, we describe the systems developed by the SJTU X-LANCE group for the TTS (acoustic + vocoder), SVS, and ASR tracks in the Interspeech 2024 Speech Processing Using Discrete Speech Unit Challenge. Notably, we achieved 1st rank on the leaderboard in the TTS track both with the whole training set and only 1h training data, with the highest UTMOS score and lowest bitrate among all submissions.",
      "authors": [
        "Yiwei Guo",
        "Chenrun Wang",
        "Yifan Yang",
        "Hankun Wang",
        "Ziyang Ma",
        "Chenpeng Du",
        "Shuai Wang",
        "Hanzheng Li",
        "Shuai Fan",
        "Hui Zhang",
        "Xie Chen",
        "Kai Yu"
      ],
      "published": "2024-04-09T07:37:41Z",
      "updated": "2024-04-10T00:33:25Z",
      "categories": [
        "eess.AS",
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2404.06079v2",
      "landing_url": "https://arxiv.org/abs/2404.06079v2",
      "doi": "https://doi.org/10.48550/arXiv.2404.06079"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The paper explicitly centers on discrete speech tokens/units used for TTS/ASR/SVS within a discrete speech unit challenge, indicating discrete tokenization as the core object of study, and it is an English technical report suitable for academic evaluation, meeting the inclusion criteria and not falling under the exclusions.",
      "evaluation": 5
    },
    "round-A_JuniorNano_reasoning": "The paper explicitly centers on discrete speech tokens/units used for TTS/ASR/SVS within a discrete speech unit challenge, indicating discrete tokenization as the core object of study, and it is an English technical report suitable for academic evaluation, meeting the inclusion criteria and not falling under the exclusions.",
    "round-A_JuniorNano_evaluation": 5,
    "round-A_JuniorMini_output": {
      "reasoning": "The study explicitly discusses the use and evaluation of discrete speech tokens as a core object for multiple speech processing tasks including ASR, TTS, and SVS, with experimental validation and top leaderboard ranking, thus meeting all inclusion criteria and none of the exclusion criteria.",
      "evaluation": 5
    },
    "round-A_JuniorMini_reasoning": "The study explicitly discusses the use and evaluation of discrete speech tokens as a core object for multiple speech processing tasks including ASR, TTS, and SVS, with experimental validation and top leaderboard ranking, thus meeting all inclusion criteria and none of the exclusion criteria.",
    "round-A_JuniorMini_evaluation": 5,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "include (junior:5)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "CoST: Contrastive Quantization based Semantic Tokenization for Generative Recommendation",
    "abstract": "Embedding-based retrieval serves as a dominant approach to candidate item matching for industrial recommender systems. With the success of generative AI, generative retrieval has recently emerged as a new retrieval paradigm for recommendation, which casts item retrieval as a generation problem. Its model consists of two stages: semantic tokenization and autoregressive generation. The first stage involves item tokenization that constructs discrete semantic tokens to index items, while the second stage autoregressively generates semantic tokens of candidate items. Therefore, semantic tokenization serves as a crucial preliminary step for training generative recommendation models. Existing research usually employs a vector quantizier with reconstruction loss (e.g., RQ-VAE) to obtain semantic tokens of items, but this method fails to capture the essential neighborhood relationships that are vital for effective item modeling in recommender systems. In this paper, we propose a contrastive quantization-based semantic tokenization approach, named CoST, which harnesses both item relationships and semantic information to learn semantic tokens. Our experimental results highlight the significant impact of semantic tokenization on generative recommendation performance, with CoST achieving up to a 43% improvement in Recall@5 and 44% improvement in NDCG@5 on the MIND dataset over previous baselines.",
    "metadata": {
      "arxiv_id": "2404.14774",
      "title": "CoST: Contrastive Quantization based Semantic Tokenization for Generative Recommendation",
      "summary": "Embedding-based retrieval serves as a dominant approach to candidate item matching for industrial recommender systems. With the success of generative AI, generative retrieval has recently emerged as a new retrieval paradigm for recommendation, which casts item retrieval as a generation problem. Its model consists of two stages: semantic tokenization and autoregressive generation. The first stage involves item tokenization that constructs discrete semantic tokens to index items, while the second stage autoregressively generates semantic tokens of candidate items. Therefore, semantic tokenization serves as a crucial preliminary step for training generative recommendation models. Existing research usually employs a vector quantizier with reconstruction loss (e.g., RQ-VAE) to obtain semantic tokens of items, but this method fails to capture the essential neighborhood relationships that are vital for effective item modeling in recommender systems. In this paper, we propose a contrastive quantization-based semantic tokenization approach, named CoST, which harnesses both item relationships and semantic information to learn semantic tokens. Our experimental results highlight the significant impact of semantic tokenization on generative recommendation performance, with CoST achieving up to a 43% improvement in Recall@5 and 44% improvement in NDCG@5 on the MIND dataset over previous baselines.",
      "authors": [
        "Jieming Zhu",
        "Mengqun Jin",
        "Qijiong Liu",
        "Zexuan Qiu",
        "Zhenhua Dong",
        "Xiu Li"
      ],
      "published": "2024-04-23T06:29:48Z",
      "updated": "2024-09-07T16:11:36Z",
      "categories": [
        "cs.IR"
      ],
      "pdf_url": "https://arxiv.org/pdf/2404.14774v2",
      "landing_url": "https://arxiv.org/abs/2404.14774v2",
      "doi": "https://doi.org/10.48550/arXiv.2404.14774"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "Abstract discusses semantic tokenization of items for a generative recommender system, not discrete audio/speech tokenization into a finite vocabulary for audio reconstruction or understanding, hence it does not meet the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "Abstract discusses semantic tokenization of items for a generative recommender system, not discrete audio/speech tokenization into a finite vocabulary for audio reconstruction or understanding, hence it does not meet the inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on semantic tokenization for generative recommendation systems using item tokens and does not involve audio or speech signal tokenization; hence, it does not meet the inclusion criteria related to discrete audio/speech tokenization.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on semantic tokenization for generative recommendation systems using item tokens and does not involve audio or speech signal tokenization; hence, it does not meet the inclusion criteria related to discrete audio/speech tokenization.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Self-Supervised Learning for User Localization",
    "abstract": "Machine learning techniques have shown remarkable accuracy in localization tasks, but their dependency on vast amounts of labeled data, particularly Channel State Information (CSI) and corresponding coordinates, remains a bottleneck. Self-supervised learning techniques alleviate the need for labeled data, a potential that remains largely untapped and underexplored in existing research. Addressing this gap, we propose a pioneering approach that leverages self-supervised pretraining on unlabeled data to boost the performance of supervised learning for user localization based on CSI. We introduce two pretraining Auto Encoder (AE) models employing Multi Layer Perceptrons (MLPs) and Convolutional Neural Networks (CNNs) to glean representations from unlabeled data via self-supervised learning. Following this, we utilize the encoder portion of the AE models to extract relevant features from labeled data, and finetune an MLP-based Position Estimation Model to accurately deduce user locations. Our experimentation on the CTW-2020 dataset, which features a substantial volume of unlabeled data but limited labeled samples, demonstrates the viability of our approach. Notably, the dataset covers a vast area spanning over 646x943x41 meters, and our approach demonstrates promising results even for such expansive localization tasks.",
    "metadata": {
      "arxiv_id": "2404.15370",
      "title": "Self-Supervised Learning for User Localization",
      "summary": "Machine learning techniques have shown remarkable accuracy in localization tasks, but their dependency on vast amounts of labeled data, particularly Channel State Information (CSI) and corresponding coordinates, remains a bottleneck. Self-supervised learning techniques alleviate the need for labeled data, a potential that remains largely untapped and underexplored in existing research. Addressing this gap, we propose a pioneering approach that leverages self-supervised pretraining on unlabeled data to boost the performance of supervised learning for user localization based on CSI. We introduce two pretraining Auto Encoder (AE) models employing Multi Layer Perceptrons (MLPs) and Convolutional Neural Networks (CNNs) to glean representations from unlabeled data via self-supervised learning. Following this, we utilize the encoder portion of the AE models to extract relevant features from labeled data, and finetune an MLP-based Position Estimation Model to accurately deduce user locations. Our experimentation on the CTW-2020 dataset, which features a substantial volume of unlabeled data but limited labeled samples, demonstrates the viability of our approach. Notably, the dataset covers a vast area spanning over 646x943x41 meters, and our approach demonstrates promising results even for such expansive localization tasks.",
      "authors": [
        "Ankan Dash",
        "Jingyi Gu",
        "Guiling Wang",
        "Nirwan Ansari"
      ],
      "published": "2024-04-19T21:49:10Z",
      "updated": "2024-04-19T21:49:10Z",
      "categories": [
        "eess.SP",
        "cs.AI",
        "cs.LG",
        "cs.NI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2404.15370v1",
      "landing_url": "https://arxiv.org/abs/2404.15370v1",
      "doi": "https://doi.org/10.48550/arXiv.2404.15370"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The manuscript centers on self-supervised learning for user localization using Channel State Information and unlabeled data; it does not propose, analyze, or evaluate discrete audio/speech tokenization as a core object of study, so it does not meet the inclusion criteria targeting discrete audio tokenization.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The manuscript centers on self-supervised learning for user localization using Channel State Information and unlabeled data; it does not propose, analyze, or evaluate discrete audio/speech tokenization as a core object of study, so it does not meet the inclusion criteria targeting discrete audio tokenization.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on self-supervised learning for user localization based on Channel State Information (CSI), which does not pertain to discrete audio or speech tokenization involving conversion of continuous audio signals into sequences of discrete symbols; thus, it does not meet the inclusion criteria related to discrete audio tokenization and also does not address any of the exclusion criteria related to audio tokenization.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on self-supervised learning for user localization based on Channel State Information (CSI), which does not pertain to discrete audio or speech tokenization involving conversion of continuous audio signals into sequences of discrete symbols; thus, it does not meet the inclusion criteria related to discrete audio tokenization and also does not address any of the exclusion criteria related to audio tokenization.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "FairDeDup: Detecting and Mitigating Vision-Language Fairness Disparities in Semantic Dataset Deduplication",
    "abstract": "Recent dataset deduplication techniques have demonstrated that content-aware dataset pruning can dramatically reduce the cost of training Vision-Language Pretrained (VLP) models without significant performance losses compared to training on the original dataset. These results have been based on pruning commonly used image-caption datasets collected from the web -- datasets that are known to harbor harmful social biases that may then be codified in trained models. In this work, we evaluate how deduplication affects the prevalence of these biases in the resulting trained models and introduce an easy-to-implement modification to the recent SemDeDup algorithm that can reduce the negative effects that we observe. When examining CLIP-style models trained on deduplicated variants of LAION-400M, we find our proposed FairDeDup algorithm consistently leads to improved fairness metrics over SemDeDup on the FairFace and FACET datasets while maintaining zero-shot performance on CLIP benchmarks.",
    "metadata": {
      "arxiv_id": "2404.16123",
      "title": "FairDeDup: Detecting and Mitigating Vision-Language Fairness Disparities in Semantic Dataset Deduplication",
      "summary": "Recent dataset deduplication techniques have demonstrated that content-aware dataset pruning can dramatically reduce the cost of training Vision-Language Pretrained (VLP) models without significant performance losses compared to training on the original dataset. These results have been based on pruning commonly used image-caption datasets collected from the web -- datasets that are known to harbor harmful social biases that may then be codified in trained models. In this work, we evaluate how deduplication affects the prevalence of these biases in the resulting trained models and introduce an easy-to-implement modification to the recent SemDeDup algorithm that can reduce the negative effects that we observe. When examining CLIP-style models trained on deduplicated variants of LAION-400M, we find our proposed FairDeDup algorithm consistently leads to improved fairness metrics over SemDeDup on the FairFace and FACET datasets while maintaining zero-shot performance on CLIP benchmarks.",
      "authors": [
        "Eric Slyman",
        "Stefan Lee",
        "Scott Cohen",
        "Kushal Kafle"
      ],
      "published": "2024-04-24T18:28:17Z",
      "updated": "2024-04-24T18:28:17Z",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CL"
      ],
      "pdf_url": "https://arxiv.org/pdf/2404.16123v1",
      "landing_url": "https://arxiv.org/abs/2404.16123v1",
      "doi": "https://doi.org/10.48550/arXiv.2404.16123"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "Does not involve discrete audio/speech tokenization; there is no core analysis of discrete audio representations, so it fails to meet the inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "Does not involve discrete audio/speech tokenization; there is no core analysis of discrete audio representations, so it fails to meet the inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on fairness in vision-language dataset deduplication and model training, specifically for image-caption datasets, and does not address discrete audio/speech tokenization. Hence, it does not meet the inclusion criteria centered on discrete audio tokenization and instead addresses vision-language fairness issues.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on fairness in vision-language dataset deduplication and model training, specifically for image-caption datasets, and does not address discrete audio/speech tokenization. Hence, it does not meet the inclusion criteria centered on discrete audio tokenization and instead addresses vision-language fairness issues.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Semantic-aware Next-Best-View for Multi-DoFs Mobile System in Search-and-Acquisition based Visual Perception",
    "abstract": "Efficient visual perception using mobile systems is crucial, particularly in unknown environments such as search and rescue operations, where swift and comprehensive perception of objects of interest is essential. In such real-world applications, objects of interest are often situated in complex environments, making the selection of the 'Next Best' view based solely on maximizing visibility gain suboptimal. Semantics, providing a higher-level interpretation of perception, should significantly contribute to the selection of the next viewpoint for various perception tasks. In this study, we formulate a novel information gain that integrates both visibility gain and semantic gain in a unified form to select the semantic-aware Next-Best-View. Additionally, we design an adaptive strategy with termination criterion to support a two-stage search-and-acquisition manoeuvre on multiple objects of interest aided by a multi-degree-of-freedoms (Multi-DoFs) mobile system. Several semantically relevant reconstruction metrics, including perspective directivity and region of interest (ROI)-to-full reconstruction volume ratio, are introduced to evaluate the performance of the proposed approach. Simulation experiments demonstrate the advantages of the proposed approach over existing methods, achieving improvements of up to 27.13% for the ROI-to-full reconstruction volume ratio and a 0.88234 average perspective directivity. Furthermore, the planned motion trajectory exhibits better perceiving coverage toward the target.",
    "metadata": {
      "arxiv_id": "2404.16507",
      "title": "Semantic-aware Next-Best-View for Multi-DoFs Mobile System in Search-and-Acquisition based Visual Perception",
      "summary": "Efficient visual perception using mobile systems is crucial, particularly in unknown environments such as search and rescue operations, where swift and comprehensive perception of objects of interest is essential. In such real-world applications, objects of interest are often situated in complex environments, making the selection of the 'Next Best' view based solely on maximizing visibility gain suboptimal. Semantics, providing a higher-level interpretation of perception, should significantly contribute to the selection of the next viewpoint for various perception tasks. In this study, we formulate a novel information gain that integrates both visibility gain and semantic gain in a unified form to select the semantic-aware Next-Best-View. Additionally, we design an adaptive strategy with termination criterion to support a two-stage search-and-acquisition manoeuvre on multiple objects of interest aided by a multi-degree-of-freedoms (Multi-DoFs) mobile system. Several semantically relevant reconstruction metrics, including perspective directivity and region of interest (ROI)-to-full reconstruction volume ratio, are introduced to evaluate the performance of the proposed approach. Simulation experiments demonstrate the advantages of the proposed approach over existing methods, achieving improvements of up to 27.13% for the ROI-to-full reconstruction volume ratio and a 0.88234 average perspective directivity. Furthermore, the planned motion trajectory exhibits better perceiving coverage toward the target.",
      "authors": [
        "Xiaotong Yu",
        "Chang-Wen Chen"
      ],
      "published": "2024-04-25T11:01:40Z",
      "updated": "2024-04-25T11:01:40Z",
      "categories": [
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2404.16507v1",
      "landing_url": "https://arxiv.org/abs/2404.16507v1",
      "doi": "https://doi.org/10.48550/arXiv.2404.16507"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item is about semantic-aware next-best-view planning for a mobile system in visual perception; it does not involve discrete audio/speech tokenization as a core object of study, so it does not meet the required inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item is about semantic-aware next-best-view planning for a mobile system in visual perception; it does not involve discrete audio/speech tokenization as a core object of study, so it does not meet the required inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on next-best-view selection for visual perception in mobile systems, emphasizing semantic and visibility gain integration, without any mention or analysis of discrete audio or speech tokenization or related audio signal processing, making it irrelevant to the inclusion criteria focused on audio tokenization.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on next-best-view selection for visual perception in mobile systems, emphasizing semantic and visibility gain integration, without any mention or analysis of discrete audio or speech tokenization or related audio signal processing, making it irrelevant to the inclusion criteria focused on audio tokenization.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "ESC: Efficient Speech Coding with Cross-Scale Residual Vector Quantized Transformers",
    "abstract": "Neural speech codecs aim to compress input signals into minimal bits while maintaining content quality in a low-latency manner. However, existing neural codecs often trade model complexity for reconstruction performance. These codecs primarily use convolutional blocks for feature transformation, which are not inherently suited for capturing the local redundancies in speech signals. To compensate, they require either adversarial discriminators or a large number of model parameters to enhance audio quality. In response to these challenges, we introduce the Efficient Speech Codec (ESC), a lightweight, parameter-efficient speech codec based on a cross-scale residual vector quantization scheme and transformers. Our model employs mirrored hierarchical window transformer blocks and performs step-wise decoding from coarse-to-fine feature representations. To enhance bitrate efficiency, we propose a novel combination of vector quantization techniques along with a pre-training paradigm. Extensive experiments demonstrate that ESC can achieve high-fidelity speech reconstruction with significantly lower model complexity, making it a promising alternative to existing convolutional audio codecs.",
    "metadata": {
      "arxiv_id": "2404.19441",
      "title": "ESC: Efficient Speech Coding with Cross-Scale Residual Vector Quantized Transformers",
      "summary": "Neural speech codecs aim to compress input signals into minimal bits while maintaining content quality in a low-latency manner. However, existing neural codecs often trade model complexity for reconstruction performance. These codecs primarily use convolutional blocks for feature transformation, which are not inherently suited for capturing the local redundancies in speech signals. To compensate, they require either adversarial discriminators or a large number of model parameters to enhance audio quality. In response to these challenges, we introduce the Efficient Speech Codec (ESC), a lightweight, parameter-efficient speech codec based on a cross-scale residual vector quantization scheme and transformers. Our model employs mirrored hierarchical window transformer blocks and performs step-wise decoding from coarse-to-fine feature representations. To enhance bitrate efficiency, we propose a novel combination of vector quantization techniques along with a pre-training paradigm. Extensive experiments demonstrate that ESC can achieve high-fidelity speech reconstruction with significantly lower model complexity, making it a promising alternative to existing convolutional audio codecs.",
      "authors": [
        "Yuzhe Gu",
        "Enmao Diao"
      ],
      "published": "2024-04-30T10:44:33Z",
      "updated": "2024-10-03T12:23:26Z",
      "categories": [
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2404.19441v3",
      "landing_url": "https://arxiv.org/abs/2404.19441v3",
      "doi": "https://doi.org/10.48550/arXiv.2404.19441"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This work centers on a neural speech codec that employs vector quantization to produce discrete speech tokens for reconstruction, making discrete audio tokenization the core focus rather than just continuous features.",
      "evaluation": 5
    },
    "round-A_JuniorNano_reasoning": "This work centers on a neural speech codec that employs vector quantization to produce discrete speech tokens for reconstruction, making discrete audio tokenization the core focus rather than just continuous features.",
    "round-A_JuniorNano_evaluation": 5,
    "round-A_JuniorMini_output": {
      "reasoning": "The study proposes a neural speech codec using a cross-scale residual vector quantization scheme to compress speech signals into discrete tokens for reconstruction, which aligns with the inclusion criteria of analyzing discrete audio/speech tokenization for reconstruction tasks. The abstract indicates methodological innovation and experimental validation focused on discrete tokenization relevant to speech, rather than continuous features alone or traditional signal-processing codecs. Therefore, it meets all inclusion criteria and none of the exclusions.",
      "evaluation": 5
    },
    "round-A_JuniorMini_reasoning": "The study proposes a neural speech codec using a cross-scale residual vector quantization scheme to compress speech signals into discrete tokens for reconstruction, which aligns with the inclusion criteria of analyzing discrete audio/speech tokenization for reconstruction tasks. The abstract indicates methodological innovation and experimental validation focused on discrete tokenization relevant to speech, rather than continuous features alone or traditional signal-processing codecs. Therefore, it meets all inclusion criteria and none of the exclusions.",
    "round-A_JuniorMini_evaluation": 5,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "include (junior:5)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "TRAMBA: A Hybrid Transformer and Mamba Architecture for Practical Audio and Bone Conduction Speech Super Resolution and Enhancement on Mobile and Wearable Platforms",
    "abstract": "We propose TRAMBA, a hybrid transformer and Mamba architecture for acoustic and bone conduction speech enhancement, suitable for mobile and wearable platforms. Bone conduction speech enhancement has been impractical to adopt in mobile and wearable platforms for several reasons: (i) data collection is labor-intensive, resulting in scarcity; (ii) there exists a performance gap between state of-art models with memory footprints of hundreds of MBs and methods better suited for resource-constrained systems. To adapt TRAMBA to vibration-based sensing modalities, we pre-train TRAMBA with audio speech datasets that are widely available. Then, users fine-tune with a small amount of bone conduction data. TRAMBA outperforms state-of-art GANs by up to 7.3% in PESQ and 1.8% in STOI, with an order of magnitude smaller memory footprint and an inference speed up of up to 465 times. We integrate TRAMBA into real systems and show that TRAMBA (i) improves battery life of wearables by up to 160% by requiring less data sampling and transmission; (ii) generates higher quality voice in noisy environments than over-the-air speech; (iii) requires a memory footprint of less than 20.0 MB.",
    "metadata": {
      "arxiv_id": "2405.01242",
      "title": "TRAMBA: A Hybrid Transformer and Mamba Architecture for Practical Audio and Bone Conduction Speech Super Resolution and Enhancement on Mobile and Wearable Platforms",
      "summary": "We propose TRAMBA, a hybrid transformer and Mamba architecture for acoustic and bone conduction speech enhancement, suitable for mobile and wearable platforms. Bone conduction speech enhancement has been impractical to adopt in mobile and wearable platforms for several reasons: (i) data collection is labor-intensive, resulting in scarcity; (ii) there exists a performance gap between state of-art models with memory footprints of hundreds of MBs and methods better suited for resource-constrained systems. To adapt TRAMBA to vibration-based sensing modalities, we pre-train TRAMBA with audio speech datasets that are widely available. Then, users fine-tune with a small amount of bone conduction data. TRAMBA outperforms state-of-art GANs by up to 7.3% in PESQ and 1.8% in STOI, with an order of magnitude smaller memory footprint and an inference speed up of up to 465 times. We integrate TRAMBA into real systems and show that TRAMBA (i) improves battery life of wearables by up to 160% by requiring less data sampling and transmission; (ii) generates higher quality voice in noisy environments than over-the-air speech; (iii) requires a memory footprint of less than 20.0 MB.",
      "authors": [
        "Yueyuan Sui",
        "Minghui Zhao",
        "Junxi Xia",
        "Xiaofan Jiang",
        "Stephen Xia"
      ],
      "published": "2024-05-02T12:45:48Z",
      "updated": "2024-05-29T15:46:57Z",
      "categories": [
        "cs.SD",
        "cs.AI",
        "cs.LG",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2405.01242v3",
      "landing_url": "https://arxiv.org/abs/2405.01242v3",
      "doi": "https://doi.org/10.48550/arXiv.2405.01242"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The work discusses a hybrid transformer architecture for continuous audio and bone-conduction speech enhancement with metrics like PESQ/STOI and device memory, but does not address discrete audio tokenization or quantization as a core contribution, so it fails the tokenization criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The work discusses a hybrid transformer architecture for continuous audio and bone-conduction speech enhancement with metrics like PESQ/STOI and device memory, but does not address discrete audio tokenization or quantization as a core contribution, so it fails the tokenization criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study proposes a hybrid transformer architecture for speech enhancement on mobile and wearable platforms, focusing on improving acoustic and bone conduction speech signals; however, there is no mention of discrete audio/speech tokenization or converting continuous audio signals into sequences of discrete symbols as part of the core methodological contribution. Thus, it does not satisfy the inclusion criteria related to discrete tokenization, nor does it fall into the exclusion categories involving only traditional continuous features or codecs. Given the absence of explicit discrete tokenization analysis, this work should likely be excluded.",
      "evaluation": 2
    },
    "round-A_JuniorMini_reasoning": "The study proposes a hybrid transformer architecture for speech enhancement on mobile and wearable platforms, focusing on improving acoustic and bone conduction speech signals; however, there is no mention of discrete audio/speech tokenization or converting continuous audio signals into sequences of discrete symbols as part of the core methodological contribution. Thus, it does not satisfy the inclusion criteria related to discrete tokenization, nor does it fall into the exclusion categories involving only traditional continuous features or codecs. Given the absence of explicit discrete tokenization analysis, this work should likely be excluded.",
    "round-A_JuniorMini_evaluation": 2,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:2)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:2)"
  },
  {
    "title": "Self-Supervised Learning for Real-World Super-Resolution from Dual and Multiple Zoomed Observations",
    "abstract": "In this paper, we consider two challenging issues in reference-based super-resolution (RefSR) for smartphone, (i) how to choose a proper reference image, and (ii) how to learn RefSR in a self-supervised manner. Particularly, we propose a novel self-supervised learning approach for real-world RefSR from observations at dual and multiple camera zooms. Firstly, considering the popularity of multiple cameras in modern smartphones, the more zoomed (telephoto) image can be naturally leveraged as the reference to guide the super-resolution (SR) of the lesser zoomed (ultra-wide) image, which gives us a chance to learn a deep network that performs SR from the dual zoomed observations (DZSR). Secondly, for self-supervised learning of DZSR, we take the telephoto image instead of an additional high-resolution image as the supervision information, and select a center patch from it as the reference to super-resolve the corresponding ultra-wide image patch. To mitigate the effect of the misalignment between ultra-wide low-resolution (LR) patch and telephoto ground-truth (GT) image during training, we first adopt patch-based optical flow alignment and then design an auxiliary-LR to guide the deforming of the warped LR features. To generate visually pleasing results, we present local overlapped sliced Wasserstein loss to better represent the perceptual difference between GT and output in the feature space. During testing, DZSR can be directly deployed to super-solve the whole ultra-wide image with the reference of the telephoto image. In addition, we further take multiple zoomed observations to explore self-supervised RefSR, and present a progressive fusion scheme for the effective utilization of reference images. Experiments show that our methods achieve better quantitative and qualitative performance against state-of-the-arts. Codes are available at https://github.com/cszhilu1998/SelfDZSR_PlusPlus.",
    "metadata": {
      "arxiv_id": "2405.02171",
      "title": "Self-Supervised Learning for Real-World Super-Resolution from Dual and Multiple Zoomed Observations",
      "summary": "In this paper, we consider two challenging issues in reference-based super-resolution (RefSR) for smartphone, (i) how to choose a proper reference image, and (ii) how to learn RefSR in a self-supervised manner. Particularly, we propose a novel self-supervised learning approach for real-world RefSR from observations at dual and multiple camera zooms. Firstly, considering the popularity of multiple cameras in modern smartphones, the more zoomed (telephoto) image can be naturally leveraged as the reference to guide the super-resolution (SR) of the lesser zoomed (ultra-wide) image, which gives us a chance to learn a deep network that performs SR from the dual zoomed observations (DZSR). Secondly, for self-supervised learning of DZSR, we take the telephoto image instead of an additional high-resolution image as the supervision information, and select a center patch from it as the reference to super-resolve the corresponding ultra-wide image patch. To mitigate the effect of the misalignment between ultra-wide low-resolution (LR) patch and telephoto ground-truth (GT) image during training, we first adopt patch-based optical flow alignment and then design an auxiliary-LR to guide the deforming of the warped LR features. To generate visually pleasing results, we present local overlapped sliced Wasserstein loss to better represent the perceptual difference between GT and output in the feature space. During testing, DZSR can be directly deployed to super-solve the whole ultra-wide image with the reference of the telephoto image. In addition, we further take multiple zoomed observations to explore self-supervised RefSR, and present a progressive fusion scheme for the effective utilization of reference images. Experiments show that our methods achieve better quantitative and qualitative performance against state-of-the-arts. Codes are available at https://github.com/cszhilu1998/SelfDZSR_PlusPlus.",
      "authors": [
        "Zhilu Zhang",
        "Ruohao Wang",
        "Hongzhi Zhang",
        "Wangmeng Zuo"
      ],
      "published": "2024-05-03T15:20:30Z",
      "updated": "2024-05-03T15:20:30Z",
      "categories": [
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2405.02171v1",
      "landing_url": "https://arxiv.org/abs/2405.02171v1",
      "doi": "https://doi.org/10.48550/arXiv.2405.02171"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This item addresses image SR with dual/multiple zoom observations and self-supervision; there is no discrete_audio tokenization or discrete token-based analysis, so it does not meet the audio-token criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "This item addresses image SR with dual/multiple zoom observations and self-supervision; there is no discrete_audio tokenization or discrete token-based analysis, so it does not meet the audio-token criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on self-supervised learning for image super-resolution using multiple zoomed observations from smartphone cameras, without any discussion or analysis of discrete audio or speech tokenization; hence, it does not meet the inclusion criteria related to audio tokenization nor address any relevant token quantization or discrete audio representations as required.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on self-supervised learning for image super-resolution using multiple zoomed observations from smartphone cameras, without any discussion or analysis of discrete audio or speech tokenization; hence, it does not meet the inclusion criteria related to audio tokenization nor address any relevant token quantization or discrete audio representations as required.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Adaptive Semantic Token Selection for AI-native Goal-oriented Communications",
    "abstract": "In this paper, we propose a novel design for AI-native goal-oriented communications, exploiting transformer neural networks under dynamic inference constraints on bandwidth and computation. Transformers have become the standard architecture for pretraining large-scale vision and text models, and preliminary results have shown promising performance also in deep joint source-channel coding (JSCC). Here, we consider a dynamic model where communication happens over a channel with variable latency and bandwidth constraints. Leveraging recent works on conditional computation, we exploit the structure of the transformer blocks and the multihead attention operator to design a trainable semantic token selection mechanism that learns to select relevant tokens (e.g., image patches) from the input signal. This is done dynamically, on a per-input basis, with a rate that can be chosen as an additional input by the user. We show that our model improves over state-of-the-art token selection mechanisms, exhibiting high accuracy for a wide range of latency and bandwidth constraints, without the need for deploying multiple architectures tailored to each constraint. Last, but not least, the proposed token selection mechanism helps extract powerful semantics that are easy to understand and explain, paving the way for interpretable-by-design models for the next generation of AI-native communication systems.",
    "metadata": {
      "arxiv_id": "2405.02330",
      "title": "Adaptive Semantic Token Selection for AI-native Goal-oriented Communications",
      "summary": "In this paper, we propose a novel design for AI-native goal-oriented communications, exploiting transformer neural networks under dynamic inference constraints on bandwidth and computation. Transformers have become the standard architecture for pretraining large-scale vision and text models, and preliminary results have shown promising performance also in deep joint source-channel coding (JSCC). Here, we consider a dynamic model where communication happens over a channel with variable latency and bandwidth constraints. Leveraging recent works on conditional computation, we exploit the structure of the transformer blocks and the multihead attention operator to design a trainable semantic token selection mechanism that learns to select relevant tokens (e.g., image patches) from the input signal. This is done dynamically, on a per-input basis, with a rate that can be chosen as an additional input by the user. We show that our model improves over state-of-the-art token selection mechanisms, exhibiting high accuracy for a wide range of latency and bandwidth constraints, without the need for deploying multiple architectures tailored to each constraint. Last, but not least, the proposed token selection mechanism helps extract powerful semantics that are easy to understand and explain, paving the way for interpretable-by-design models for the next generation of AI-native communication systems.",
      "authors": [
        "Alessio Devoto",
        "Simone Petruzzi",
        "Jary Pomponi",
        "Paolo Di Lorenzo",
        "Simone Scardapane"
      ],
      "published": "2024-04-25T13:49:50Z",
      "updated": "2024-04-25T13:49:50Z",
      "categories": [
        "cs.IT",
        "cs.AI",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2405.02330v1",
      "landing_url": "https://arxiv.org/abs/2405.02330v1",
      "doi": "https://doi.org/10.48550/arXiv.2405.02330"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The abstract centers on dynamic token selection in transformer-based AI-native communications, using tokens like image patches, and does not focus on discrete audio/speech tokenization or discrete audio representations, so it does not meet the inclusion criteria.",
      "evaluation": 2
    },
    "round-A_JuniorNano_reasoning": "The abstract centers on dynamic token selection in transformer-based AI-native communications, using tokens like image patches, and does not focus on discrete audio/speech tokenization or discrete audio representations, so it does not meet the inclusion criteria.",
    "round-A_JuniorNano_evaluation": 2,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on a novel transformer-based token selection mechanism for AI-native goal-oriented communications, involving semantic token selection from inputs such as image patches; however, it does not pertain to discrete audio or speech tokenization, nor does it analyze or propose discrete tokenization of continuous audio signals as required by the inclusion criteria. The abstract does not mention audio, speech tokenization, or related discrete audio tokens, and thus it does not meet the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on a novel transformer-based token selection mechanism for AI-native goal-oriented communications, involving semantic token selection from inputs such as image patches; however, it does not pertain to discrete audio or speech tokenization, nor does it analyze or propose discrete tokenization of continuous audio signals as required by the inclusion criteria. The abstract does not mention audio, speech tokenization, or related discrete audio tokens, and thus it does not meet the inclusion criteria.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:2)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:2)"
  },
  {
    "title": "Deduplicator: When Computation Reuse Meets Load Balancing at the Network Edge",
    "abstract": "Load balancing has been a fundamental building block of cloud and, more recently, edge computing environments. At the same time, in edge computing environments, prior research has highlighted that applications operate on similar (correlated) data. Based on this observation, prior research has advocated for the direction of \"computation reuse\", where the results of previously executed computational tasks are stored at the edge and are reused (if possible) to satisfy incoming tasks with similar input data, instead of executing incoming tasks from scratch. Both load balancing and computation reuse are critical to the deployment of scalable edge computing environments, yet they are contradictory in nature. In this paper, we propose the Deduplicator, a middlebox that aims to facilitate both load balancing and computation reuse at the edge. The Deduplicator features mechanisms to identify and deduplicate similar tasks offloaded by user devices, collect information about the usage of edge servers' resources, manage the addition of new edge servers and the failures of existing edge servers, and ultimately balance the load imposed on edge servers. Our evaluation results demonstrate that the Deduplicator achieves up to 20% higher percentages of computation reuse compared to several other load balancing approaches, while also effectively balancing the distribution of tasks among edge servers at line rate.",
    "metadata": {
      "arxiv_id": "2405.02682",
      "title": "Deduplicator: When Computation Reuse Meets Load Balancing at the Network Edge",
      "summary": "Load balancing has been a fundamental building block of cloud and, more recently, edge computing environments. At the same time, in edge computing environments, prior research has highlighted that applications operate on similar (correlated) data. Based on this observation, prior research has advocated for the direction of \"computation reuse\", where the results of previously executed computational tasks are stored at the edge and are reused (if possible) to satisfy incoming tasks with similar input data, instead of executing incoming tasks from scratch. Both load balancing and computation reuse are critical to the deployment of scalable edge computing environments, yet they are contradictory in nature. In this paper, we propose the Deduplicator, a middlebox that aims to facilitate both load balancing and computation reuse at the edge. The Deduplicator features mechanisms to identify and deduplicate similar tasks offloaded by user devices, collect information about the usage of edge servers' resources, manage the addition of new edge servers and the failures of existing edge servers, and ultimately balance the load imposed on edge servers. Our evaluation results demonstrate that the Deduplicator achieves up to 20% higher percentages of computation reuse compared to several other load balancing approaches, while also effectively balancing the distribution of tasks among edge servers at line rate.",
      "authors": [
        "Md Washik Al Azad",
        "Spyridon Mastorakis"
      ],
      "published": "2024-05-04T14:48:19Z",
      "updated": "2024-05-04T14:48:19Z",
      "categories": [
        "cs.DC",
        "cs.NI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2405.02682v1",
      "landing_url": "https://arxiv.org/abs/2405.02682v1",
      "doi": "https://doi.org/10.48550/arXiv.2405.02682"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The title/abstract describe edge computing techniques for computation reuse and load balancing, with no discussion of discrete audio/speech tokenization; thus it does not meet the inclusion criteria focused on discrete audio tokenization, and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The title/abstract describe edge computing techniques for computation reuse and load balancing, with no discussion of discrete audio/speech tokenization; thus it does not meet the inclusion criteria focused on discrete audio tokenization, and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on load balancing and computation reuse at the network edge in computing environments and does not propose, analyze, or evaluate discrete audio/speech tokenization, nor does it address any aspect of discrete tokenization for audio reconstruction, generation, or understanding tasks as required by the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on load balancing and computation reuse at the network edge in computing environments and does not propose, analyze, or evaluate discrete audio/speech tokenization, nor does it address any aspect of discrete tokenization for audio reconstruction, generation, or understanding tasks as required by the inclusion criteria.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Vector Quantization for Recommender Systems: A Review and Outlook",
    "abstract": "Vector quantization, renowned for its unparalleled feature compression capabilities, has been a prominent topic in signal processing and machine learning research for several decades and remains widely utilized today. With the emergence of large models and generative AI, vector quantization has gained popularity in recommender systems, establishing itself as a preferred solution. This paper starts with a comprehensive review of vector quantization techniques. It then explores systematic taxonomies of vector quantization methods for recommender systems (VQ4Rec), examining their applications from multiple perspectives. Further, it provides a thorough introduction to research efforts in diverse recommendation scenarios, including efficiency-oriented approaches and quality-oriented approaches. Finally, the survey analyzes the remaining challenges and anticipates future trends in VQ4Rec, including the challenges associated with the training of vector quantization, the opportunities presented by large language models, and emerging trends in multimodal recommender systems. We hope this survey can pave the way for future researchers in the recommendation community and accelerate their exploration in this promising field.",
    "metadata": {
      "arxiv_id": "2405.03110",
      "title": "Vector Quantization for Recommender Systems: A Review and Outlook",
      "summary": "Vector quantization, renowned for its unparalleled feature compression capabilities, has been a prominent topic in signal processing and machine learning research for several decades and remains widely utilized today. With the emergence of large models and generative AI, vector quantization has gained popularity in recommender systems, establishing itself as a preferred solution. This paper starts with a comprehensive review of vector quantization techniques. It then explores systematic taxonomies of vector quantization methods for recommender systems (VQ4Rec), examining their applications from multiple perspectives. Further, it provides a thorough introduction to research efforts in diverse recommendation scenarios, including efficiency-oriented approaches and quality-oriented approaches. Finally, the survey analyzes the remaining challenges and anticipates future trends in VQ4Rec, including the challenges associated with the training of vector quantization, the opportunities presented by large language models, and emerging trends in multimodal recommender systems. We hope this survey can pave the way for future researchers in the recommendation community and accelerate their exploration in this promising field.",
      "authors": [
        "Qijiong Liu",
        "Xiaoyu Dong",
        "Jiaren Xiao",
        "Nuo Chen",
        "Hengchang Hu",
        "Jieming Zhu",
        "Chenxu Zhu",
        "Tetsuya Sakai",
        "Xiao-Ming Wu"
      ],
      "published": "2024-05-06T02:06:26Z",
      "updated": "2024-05-06T02:06:26Z",
      "categories": [
        "cs.IR"
      ],
      "pdf_url": "https://arxiv.org/pdf/2405.03110v1",
      "landing_url": "https://arxiv.org/abs/2405.03110v1",
      "doi": "https://doi.org/10.48550/arXiv.2405.03110"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "Although the title/abstract discuss vector quantization, it concerns recommender systems in general rather than discrete audio/speech tokenization, and does not treat discrete audio tokens as its core subject.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "Although the title/abstract discuss vector quantization, it concerns recommender systems in general rather than discrete audio/speech tokenization, and does not treat discrete audio tokens as its core subject.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on vector quantization techniques within recommender systems, without any indication of discrete audio/speech tokenization or analysis of audio signals, thus it does not meet the inclusion criteria related to audio/speech tokenization.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on vector quantization techniques within recommender systems, without any indication of discrete audio/speech tokenization or analysis of audio signals, thus it does not meet the inclusion criteria related to audio/speech tokenization.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "CRA5: Extreme Compression of ERA5 for Portable Global Climate and Weather Research via an Efficient Variational Transformer",
    "abstract": "The advent of data-driven weather forecasting models, which learn from hundreds of terabytes (TB) of reanalysis data, has significantly advanced forecasting capabilities. However, the substantial costs associated with data storage and transmission present a major challenge for data providers and users, affecting resource-constrained researchers and limiting their accessibility to participate in AI-based meteorological research. To mitigate this issue, we introduce an efficient neural codec, the Variational Autoencoder Transformer (VAEformer), for extreme compression of climate data to significantly reduce data storage cost, making AI-based meteorological research portable to researchers. Our approach diverges from recent complex neural codecs by utilizing a low-complexity Auto-Encoder transformer. This encoder produces a quantized latent representation through variance inference, which reparameterizes the latent space as a Gaussian distribution. This method improves the estimation of distributions for cross-entropy coding. Extensive experiments demonstrate that our VAEformer outperforms existing state-of-the-art compression methods in the context of climate data. By applying our VAEformer, we compressed the most popular ERA5 climate dataset (226 TB) into a new dataset, CRA5 (0.7 TB). This translates to a compression ratio of over 300 while retaining the dataset's utility for accurate scientific analysis. Further, downstream experiments show that global weather forecasting models trained on the compact CRA5 dataset achieve forecasting accuracy comparable to the model trained on the original dataset. Code, the CRA5 dataset, and the pre-trained model are available at https://github.com/taohan10200/CRA5.",
    "metadata": {
      "arxiv_id": "2405.03376",
      "title": "CRA5: Extreme Compression of ERA5 for Portable Global Climate and Weather Research via an Efficient Variational Transformer",
      "summary": "The advent of data-driven weather forecasting models, which learn from hundreds of terabytes (TB) of reanalysis data, has significantly advanced forecasting capabilities. However, the substantial costs associated with data storage and transmission present a major challenge for data providers and users, affecting resource-constrained researchers and limiting their accessibility to participate in AI-based meteorological research. To mitigate this issue, we introduce an efficient neural codec, the Variational Autoencoder Transformer (VAEformer), for extreme compression of climate data to significantly reduce data storage cost, making AI-based meteorological research portable to researchers. Our approach diverges from recent complex neural codecs by utilizing a low-complexity Auto-Encoder transformer. This encoder produces a quantized latent representation through variance inference, which reparameterizes the latent space as a Gaussian distribution. This method improves the estimation of distributions for cross-entropy coding. Extensive experiments demonstrate that our VAEformer outperforms existing state-of-the-art compression methods in the context of climate data. By applying our VAEformer, we compressed the most popular ERA5 climate dataset (226 TB) into a new dataset, CRA5 (0.7 TB). This translates to a compression ratio of over 300 while retaining the dataset's utility for accurate scientific analysis. Further, downstream experiments show that global weather forecasting models trained on the compact CRA5 dataset achieve forecasting accuracy comparable to the model trained on the original dataset. Code, the CRA5 dataset, and the pre-trained model are available at https://github.com/taohan10200/CRA5.",
      "authors": [
        "Tao Han",
        "Zhenghao Chen",
        "Song Guo",
        "Wanghan Xu",
        "Lei Bai"
      ],
      "published": "2024-05-06T11:30:55Z",
      "updated": "2024-05-08T03:27:04Z",
      "categories": [
        "cs.LG",
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2405.03376v2",
      "landing_url": "https://arxiv.org/abs/2405.03376v2",
      "doi": "https://doi.org/10.48550/arXiv.2405.03376"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item describes compression of ERA5 climate data using a neural codec, with no discrete audio/speech tokenization or token-based analysis of audio signals, so it does not meet the inclusion criteria for discrete audio tokens.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item describes compression of ERA5 climate data using a neural codec, with no discrete audio/speech tokenization or token-based analysis of audio signals, so it does not meet the inclusion criteria for discrete audio tokens.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on efficient compression of climate/weather data (ERA5) using a neural variational transformer, not on discrete audio or speech tokenization for audio reconstruction, generation, or understanding tasks. It neither addresses audio signal quantization nor discrete tokenization of speech audio as defined by the inclusion criteria, and it is unrelated to audio data or tokenization methods for speech or general audio processing.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on efficient compression of climate/weather data (ERA5) using a neural variational transformer, not on discrete audio or speech tokenization for audio reconstruction, generation, or understanding tasks. It neither addresses audio signal quantization nor discrete tokenization of speech audio as defined by the inclusion criteria, and it is unrelated to audio data or tokenization methods for speech or general audio processing.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "An Investigation of Incorporating Mamba for Speech Enhancement",
    "abstract": "This work aims to investigate the use of a recently proposed, attention-free, scalable state-space model (SSM), Mamba, for the speech enhancement (SE) task. In particular, we employ Mamba to deploy different regression-based SE models (SEMamba) with different configurations, namely basic, advanced, causal, and non-causal. Furthermore, loss functions either based on signal-level distances or metric-oriented are considered. Experimental evidence shows that SEMamba attains a competitive PESQ of 3.55 on the VoiceBank-DEMAND dataset with the advanced, non-causal configuration. A new state-of-the-art PESQ of 3.69 is also reported when SEMamba is combined with Perceptual Contrast Stretching (PCS). Compared against Transformed-based equivalent SE solutions, a noticeable FLOPs reduction up to ~12% is observed with the advanced non-causal configurations. Finally, SEMamba can be used as a pre-processing step before automatic speech recognition (ASR), showing competitive performance against recent SE solutions.",
    "metadata": {
      "arxiv_id": "2405.06573",
      "title": "An Investigation of Incorporating Mamba for Speech Enhancement",
      "summary": "This work aims to investigate the use of a recently proposed, attention-free, scalable state-space model (SSM), Mamba, for the speech enhancement (SE) task. In particular, we employ Mamba to deploy different regression-based SE models (SEMamba) with different configurations, namely basic, advanced, causal, and non-causal. Furthermore, loss functions either based on signal-level distances or metric-oriented are considered. Experimental evidence shows that SEMamba attains a competitive PESQ of 3.55 on the VoiceBank-DEMAND dataset with the advanced, non-causal configuration. A new state-of-the-art PESQ of 3.69 is also reported when SEMamba is combined with Perceptual Contrast Stretching (PCS). Compared against Transformed-based equivalent SE solutions, a noticeable FLOPs reduction up to ~12% is observed with the advanced non-causal configurations. Finally, SEMamba can be used as a pre-processing step before automatic speech recognition (ASR), showing competitive performance against recent SE solutions.",
      "authors": [
        "Rong Chao",
        "Wen-Huang Cheng",
        "Moreno La Quatra",
        "Sabato Marco Siniscalchi",
        "Chao-Han Huck Yang",
        "Szu-Wei Fu",
        "Yu Tsao"
      ],
      "published": "2024-05-10T16:18:49Z",
      "updated": "2025-10-07T07:07:34Z",
      "categories": [
        "cs.SD",
        "cs.AI",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2405.06573v2",
      "landing_url": "https://arxiv.org/abs/2405.06573v2",
      "doi": "https://doi.org/10.48550/arXiv.2405.06573"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The work analyzes continuous speech enhancement models without any discrete audio tokenization, thus it does not meet the inclusion criteria and falls under the exclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The work analyzes continuous speech enhancement models without any discrete audio tokenization, thus it does not meet the inclusion criteria and falls under the exclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on speech enhancement using a state-space model called Mamba and evaluates regression-based models without any mention of discrete audio/speech tokenization or quantization into discrete symbols; thus, it does not meet the inclusion criterion of analyzing or proposing discrete tokenization methods for speech.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on speech enhancement using a state-space model called Mamba and evaluates regression-based models without any mention of discrete audio/speech tokenization or quantization into discrete symbols; thus, it does not meet the inclusion criterion of analyzing or proposing discrete tokenization methods for speech.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "FastSAG: Towards Fast Non-Autoregressive Singing Accompaniment Generation",
    "abstract": "Singing Accompaniment Generation (SAG), which generates instrumental music to accompany input vocals, is crucial to developing human-AI symbiotic art creation systems. The state-of-the-art method, SingSong, utilizes a multi-stage autoregressive (AR) model for SAG, however, this method is extremely slow as it generates semantic and acoustic tokens recursively, and this makes it impossible for real-time applications. In this paper, we aim to develop a Fast SAG method that can create high-quality and coherent accompaniments. A non-AR diffusion-based framework is developed, which by carefully designing the conditions inferred from the vocal signals, generates the Mel spectrogram of the target accompaniment directly. With diffusion and Mel spectrogram modeling, the proposed method significantly simplifies the AR token-based SingSong framework, and largely accelerates the generation. We also design semantic projection, prior projection blocks as well as a set of loss functions, to ensure the generated accompaniment has semantic and rhythm coherence with the vocal signal. By intensive experimental studies, we demonstrate that the proposed method can generate better samples than SingSong, and accelerate the generation by at least 30 times. Audio samples and code are available at https://fastsag.github.io/.",
    "metadata": {
      "arxiv_id": "2405.07682",
      "title": "FastSAG: Towards Fast Non-Autoregressive Singing Accompaniment Generation",
      "summary": "Singing Accompaniment Generation (SAG), which generates instrumental music to accompany input vocals, is crucial to developing human-AI symbiotic art creation systems. The state-of-the-art method, SingSong, utilizes a multi-stage autoregressive (AR) model for SAG, however, this method is extremely slow as it generates semantic and acoustic tokens recursively, and this makes it impossible for real-time applications. In this paper, we aim to develop a Fast SAG method that can create high-quality and coherent accompaniments. A non-AR diffusion-based framework is developed, which by carefully designing the conditions inferred from the vocal signals, generates the Mel spectrogram of the target accompaniment directly. With diffusion and Mel spectrogram modeling, the proposed method significantly simplifies the AR token-based SingSong framework, and largely accelerates the generation. We also design semantic projection, prior projection blocks as well as a set of loss functions, to ensure the generated accompaniment has semantic and rhythm coherence with the vocal signal. By intensive experimental studies, we demonstrate that the proposed method can generate better samples than SingSong, and accelerate the generation by at least 30 times. Audio samples and code are available at https://fastsag.github.io/.",
      "authors": [
        "Jianyi Chen",
        "Wei Xue",
        "Xu Tan",
        "Zhen Ye",
        "Qifeng Liu",
        "Yike Guo"
      ],
      "published": "2024-05-13T12:14:54Z",
      "updated": "2024-05-13T12:14:54Z",
      "categories": [
        "cs.SD",
        "cs.AI",
        "cs.CL",
        "cs.MM",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2405.07682v1",
      "landing_url": "https://arxiv.org/abs/2405.07682v1",
      "doi": "https://doi.org/10.48550/arXiv.2405.07682"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The method models Mel-spectrograms directly (continuous features) and does not introduce or analyze discrete audio tokens or tokenization; therefore it does not meet the discrete-token inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The method models Mel-spectrograms directly (continuous features) and does not introduce or analyze discrete audio tokens or tokenization; therefore it does not meet the discrete-token inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on generating singing accompaniment using a non-autoregressive diffusion-based framework that directly generates Mel spectrograms, which are continuous audio features. There is no mention or analysis of discrete audio/speech tokenization or quantization methods, and the approach seems to use continuous representations rather than discrete token sequences. Therefore, it does not meet the inclusion criteria requiring discrete tokenization as a core object of study.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on generating singing accompaniment using a non-autoregressive diffusion-based framework that directly generates Mel spectrograms, which are continuous audio features. There is no mention or analysis of discrete audio/speech tokenization or quantization methods, and the approach seems to use continuous representations rather than discrete token sequences. Therefore, it does not meet the inclusion criteria requiring discrete tokenization as a core object of study.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Investigating the 'Autoencoder Behavior' in Speech Self-Supervised Models: a focus on HuBERT's Pretraining",
    "abstract": "Self-supervised learning has shown great success in Speech Recognition. However, it has been observed that finetuning all layers of the learned model leads to lower performance compared to resetting top layers. This phenomenon is attributed to the ''autoencoder'' behavior: top layers contain information closer to the input and are less suitable for tasks that require linguistic information, such as Speech Recognition.To better our understanding of this behavior, we propose to study the evolution of high-level information within the model during pretraining. We focus on the HuBERT model, which exhibits a less pronounced ''autoencoder'' behavior. By experimentally exploring various factors that may have an impact, we aim to improve the training procedure and enhance the top layers of HuBERT for high-level tasks.Furthermore, our experiments demonstrate that these improvements in the training procedure result in faster convergence and competitive performance on downstream tasks.",
    "metadata": {
      "arxiv_id": "2405.08402",
      "title": "Investigating the 'Autoencoder Behavior' in Speech Self-Supervised Models: a focus on HuBERT's Pretraining",
      "summary": "Self-supervised learning has shown great success in Speech Recognition. However, it has been observed that finetuning all layers of the learned model leads to lower performance compared to resetting top layers. This phenomenon is attributed to the ''autoencoder'' behavior: top layers contain information closer to the input and are less suitable for tasks that require linguistic information, such as Speech Recognition.To better our understanding of this behavior, we propose to study the evolution of high-level information within the model during pretraining. We focus on the HuBERT model, which exhibits a less pronounced ''autoencoder'' behavior. By experimentally exploring various factors that may have an impact, we aim to improve the training procedure and enhance the top layers of HuBERT for high-level tasks.Furthermore, our experiments demonstrate that these improvements in the training procedure result in faster convergence and competitive performance on downstream tasks.",
      "authors": [
        "Valentin Vielzeuf"
      ],
      "published": "2024-05-14T07:55:37Z",
      "updated": "2024-05-14T07:55:37Z",
      "categories": [
        "cs.CL"
      ],
      "pdf_url": "https://arxiv.org/pdf/2405.08402v1",
      "landing_url": "https://arxiv.org/abs/2405.08402v1",
      "doi": "https://doi.org/10.48550/arXiv.2405.08402"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The abstract discusses HuBERT pretraining and ‘autoencoder behavior’ but does not explicitly propose, analyze, or evaluate discrete speech tokenization as a core object of study; it centers on representation dynamics and training procedures rather than discrete token representations, so it should be excluded under tokenization-focused criteria.",
      "evaluation": 2
    },
    "round-A_JuniorNano_reasoning": "The abstract discusses HuBERT pretraining and ‘autoencoder behavior’ but does not explicitly propose, analyze, or evaluate discrete speech tokenization as a core object of study; it centers on representation dynamics and training procedures rather than discrete token representations, so it should be excluded under tokenization-focused criteria.",
    "round-A_JuniorNano_evaluation": 2,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on the 'autoencoder' behavior in speech self-supervised models (HuBERT), aiming to improve training and understand high-level information evolution during pretraining. However, the abstract does not explicitly mention proposing, analyzing, or evaluating discrete audio/speech tokenization as the core study object, nor does it discuss discrete token sequences or quantization-based token design as per the inclusion criteria. It seems more focused on model behavior and training procedures rather than discrete tokenization, which is central to the inclusion criteria. Therefore, it does not meet the core inclusion requirement.",
      "evaluation": 2
    },
    "round-A_JuniorMini_reasoning": "The study focuses on the 'autoencoder' behavior in speech self-supervised models (HuBERT), aiming to improve training and understand high-level information evolution during pretraining. However, the abstract does not explicitly mention proposing, analyzing, or evaluating discrete audio/speech tokenization as the core study object, nor does it discuss discrete token sequences or quantization-based token design as per the inclusion criteria. It seems more focused on model behavior and training procedures rather than discrete tokenization, which is central to the inclusion criteria. Therefore, it does not meet the core inclusion requirement.",
    "round-A_JuniorMini_evaluation": 2,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:2)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:2)"
  },
  {
    "title": "Evaluating Text-to-Speech Synthesis from a Large Discrete Token-based Speech Language Model",
    "abstract": "Recent advances in generative language modeling applied to discrete speech tokens presented a new avenue for text-to-speech (TTS) synthesis. These speech language models (SLMs), similarly to their textual counterparts, are scalable, probabilistic, and context-aware. While they can produce diverse and natural outputs, they sometimes face issues such as unintelligibility and the inclusion of non-speech noises or hallucination. As the adoption of this innovative paradigm in speech synthesis increases, there is a clear need for an in-depth evaluation of its capabilities and limitations. In this paper, we evaluate TTS from a discrete token-based SLM, through both automatic metrics and listening tests. We examine five key dimensions: speaking style, intelligibility, speaker consistency, prosodic variation, spontaneous behaviour. Our results highlight the model's strength in generating varied prosody and spontaneous outputs. It is also rated higher in naturalness and context appropriateness in listening tests compared to a conventional TTS. However, the model's performance in intelligibility and speaker consistency lags behind traditional TTS. Additionally, we show that increasing the scale of SLMs offers a modest boost in robustness. Our findings aim to serve as a benchmark for future advancements in generative SLMs for speech synthesis.",
    "metadata": {
      "arxiv_id": "2405.09768",
      "title": "Evaluating Text-to-Speech Synthesis from a Large Discrete Token-based Speech Language Model",
      "summary": "Recent advances in generative language modeling applied to discrete speech tokens presented a new avenue for text-to-speech (TTS) synthesis. These speech language models (SLMs), similarly to their textual counterparts, are scalable, probabilistic, and context-aware. While they can produce diverse and natural outputs, they sometimes face issues such as unintelligibility and the inclusion of non-speech noises or hallucination. As the adoption of this innovative paradigm in speech synthesis increases, there is a clear need for an in-depth evaluation of its capabilities and limitations. In this paper, we evaluate TTS from a discrete token-based SLM, through both automatic metrics and listening tests. We examine five key dimensions: speaking style, intelligibility, speaker consistency, prosodic variation, spontaneous behaviour. Our results highlight the model's strength in generating varied prosody and spontaneous outputs. It is also rated higher in naturalness and context appropriateness in listening tests compared to a conventional TTS. However, the model's performance in intelligibility and speaker consistency lags behind traditional TTS. Additionally, we show that increasing the scale of SLMs offers a modest boost in robustness. Our findings aim to serve as a benchmark for future advancements in generative SLMs for speech synthesis.",
      "authors": [
        "Siyang Wang",
        "Éva Székely"
      ],
      "published": "2024-05-16T02:18:41Z",
      "updated": "2024-05-16T02:18:41Z",
      "categories": [
        "eess.AS",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2405.09768v1",
      "landing_url": "https://arxiv.org/abs/2405.09768v1",
      "doi": "https://doi.org/10.48550/arXiv.2405.09768"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The work centers on discrete token-based speech representations for TTS and provides evaluation of their intelligibility, prosody, and other discrete-token metrics, satisfying the inclusion criteria and not falling under the stated exclusions.",
      "evaluation": 5
    },
    "round-A_JuniorNano_reasoning": "The work centers on discrete token-based speech representations for TTS and provides evaluation of their intelligibility, prosody, and other discrete-token metrics, satisfying the inclusion criteria and not falling under the stated exclusions.",
    "round-A_JuniorNano_evaluation": 5,
    "round-A_JuniorMini_output": {
      "reasoning": "The study explicitly evaluates text-to-speech synthesis using discrete token-based speech language models, which aligns directly with the inclusion criteria of analyzing discrete audio/speech tokenization for generation tasks; it presents systematic evaluation and analysis of discrete token representations rather than focusing solely on continuous features or engineering tools, and the abstract suggests the full paper is likely in English and suitable for academic review.",
      "evaluation": 5
    },
    "round-A_JuniorMini_reasoning": "The study explicitly evaluates text-to-speech synthesis using discrete token-based speech language models, which aligns directly with the inclusion criteria of analyzing discrete audio/speech tokenization for generation tasks; it presents systematic evaluation and analysis of discrete token representations rather than focusing solely on continuous features or engineering tools, and the abstract suggests the full paper is likely in English and suitable for academic review.",
    "round-A_JuniorMini_evaluation": 5,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "include (junior:5)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Stochastic super-resolution for Gaussian microtextures",
    "abstract": "Super-Resolution (SR) is the problem that consists in reconstructing images that have been degraded by a zoom-out operator. This is an ill-posed problem that does not have a unique solution, and numerical approaches rely on a prior on high-resolution images. While optimization-based methods are generally deterministic, with the rise of image generative models more and more interest has been given to stochastic SR, that is, sampling among all possible SR images associated with a given low-resolution input. In this paper, we construct an efficient, stable and provably exact sampler for the stochastic SR of Gaussian microtextures. Even though our approach is limited regarding the scope of images it encompasses, our algorithm is competitive with deep learning state-of-the-art methods both in terms of perceptual metric and execution time when applied to microtextures. The framework of Gaussian microtextures also allows us to rigorously discuss the limitations of various reconstruction metrics to evaluate the efficiency of SR routines.",
    "metadata": {
      "arxiv_id": "2405.15399",
      "title": "Stochastic super-resolution for Gaussian microtextures",
      "summary": "Super-Resolution (SR) is the problem that consists in reconstructing images that have been degraded by a zoom-out operator. This is an ill-posed problem that does not have a unique solution, and numerical approaches rely on a prior on high-resolution images. While optimization-based methods are generally deterministic, with the rise of image generative models more and more interest has been given to stochastic SR, that is, sampling among all possible SR images associated with a given low-resolution input. In this paper, we construct an efficient, stable and provably exact sampler for the stochastic SR of Gaussian microtextures. Even though our approach is limited regarding the scope of images it encompasses, our algorithm is competitive with deep learning state-of-the-art methods both in terms of perceptual metric and execution time when applied to microtextures. The framework of Gaussian microtextures also allows us to rigorously discuss the limitations of various reconstruction metrics to evaluate the efficiency of SR routines.",
      "authors": [
        "Emile Pierret",
        "Bruno Galerne"
      ],
      "published": "2024-05-24T09:53:24Z",
      "updated": "2024-05-29T06:45:58Z",
      "categories": [
        "eess.IV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2405.15399v2",
      "landing_url": "https://arxiv.org/abs/2405.15399v2",
      "doi": "https://doi.org/10.48550/arXiv.2405.15399"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The paper concerns stochastic super-resolution for Gaussian microtextures in the image domain; it does not propose or analyze discrete audio/speech tokenization, so it does not meet the inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The paper concerns stochastic super-resolution for Gaussian microtextures in the image domain; it does not propose or analyze discrete audio/speech tokenization, so it does not meet the inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on stochastic super-resolution of Gaussian microtextures in images, with no mention of discrete audio/speech tokenization or any analyses related to converting continuous audio signals into discrete tokens; thus, it does not meet the inclusion criteria related to discrete audio tokenization and instead focuses on image-based methods.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on stochastic super-resolution of Gaussian microtextures in images, with no mention of discrete audio/speech tokenization or any analyses related to converting continuous audio signals into discrete tokens; thus, it does not meet the inclusion criteria related to discrete audio tokenization and instead focuses on image-based methods.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "C3LLM: Conditional Multimodal Content Generation Using Large Language Models",
    "abstract": "We introduce C3LLM (Conditioned-on-Three-Modalities Large Language Models), a novel framework combining three tasks of video-to-audio, audio-to-text, and text-to-audio together. C3LLM adapts the Large Language Model (LLM) structure as a bridge for aligning different modalities, synthesizing the given conditional information, and making multimodal generation in a discrete manner. Our contributions are as follows. First, we adapt a hierarchical structure for audio generation tasks with pre-trained audio codebooks. Specifically, we train the LLM to generate audio semantic tokens from the given conditions, and further use a non-autoregressive transformer to generate different levels of acoustic tokens in layers to better enhance the fidelity of the generated audio. Second, based on the intuition that LLMs were originally designed for discrete tasks with the next-word prediction method, we use the discrete representation for audio generation and compress their semantic meanings into acoustic tokens, similar to adding \"acoustic vocabulary\" to LLM. Third, our method combines the previous tasks of audio understanding, video-to-audio generation, and text-to-audio generation together into one unified model, providing more versatility in an end-to-end fashion. Our C3LLM achieves improved results through various automated evaluation metrics, providing better semantic alignment compared to previous methods.",
    "metadata": {
      "arxiv_id": "2405.16136",
      "title": "C3LLM: Conditional Multimodal Content Generation Using Large Language Models",
      "summary": "We introduce C3LLM (Conditioned-on-Three-Modalities Large Language Models), a novel framework combining three tasks of video-to-audio, audio-to-text, and text-to-audio together. C3LLM adapts the Large Language Model (LLM) structure as a bridge for aligning different modalities, synthesizing the given conditional information, and making multimodal generation in a discrete manner. Our contributions are as follows. First, we adapt a hierarchical structure for audio generation tasks with pre-trained audio codebooks. Specifically, we train the LLM to generate audio semantic tokens from the given conditions, and further use a non-autoregressive transformer to generate different levels of acoustic tokens in layers to better enhance the fidelity of the generated audio. Second, based on the intuition that LLMs were originally designed for discrete tasks with the next-word prediction method, we use the discrete representation for audio generation and compress their semantic meanings into acoustic tokens, similar to adding \"acoustic vocabulary\" to LLM. Third, our method combines the previous tasks of audio understanding, video-to-audio generation, and text-to-audio generation together into one unified model, providing more versatility in an end-to-end fashion. Our C3LLM achieves improved results through various automated evaluation metrics, providing better semantic alignment compared to previous methods.",
      "authors": [
        "Zixuan Wang",
        "Qinkai Duan",
        "Yu-Wing Tai",
        "Chi-Keung Tang"
      ],
      "published": "2024-05-25T09:10:12Z",
      "updated": "2024-05-25T09:10:12Z",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.LG",
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2405.16136v1",
      "landing_url": "https://arxiv.org/abs/2405.16136v1",
      "doi": "https://doi.org/10.48550/arXiv.2405.16136"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The work explicitly uses discrete audio tokens (semantic tokens and acoustic tokens) via audio codebooks as core mechanisms for generation and alignment across modalities, which matches the inclusion criteria for discrete tokenization in audio/speech; thus it should be included.",
      "evaluation": 5
    },
    "round-A_JuniorNano_reasoning": "The work explicitly uses discrete audio tokens (semantic tokens and acoustic tokens) via audio codebooks as core mechanisms for generation and alignment across modalities, which matches the inclusion criteria for discrete tokenization in audio/speech; thus it should be included.",
    "round-A_JuniorNano_evaluation": 5,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper introduces C3LLM which explicitly involves generating audio semantic tokens and acoustic tokens, thus focusing on discrete audio tokenization as a core element. It describes a neural approach to compressing continuous audio into discrete tokens for audio reconstruction and generation tasks, meeting the inclusion criteria of studying discrete audio/speech tokenization and avoiding exclusion criteria such as relying solely on continuous features or traditional codecs. Hence, it clearly aligns with the required criteria for inclusion.",
      "evaluation": 5
    },
    "round-A_JuniorMini_reasoning": "The paper introduces C3LLM which explicitly involves generating audio semantic tokens and acoustic tokens, thus focusing on discrete audio tokenization as a core element. It describes a neural approach to compressing continuous audio into discrete tokens for audio reconstruction and generation tasks, meeting the inclusion criteria of studying discrete audio/speech tokenization and avoiding exclusion criteria such as relying solely on continuous features or traditional codecs. Hence, it clearly aligns with the required criteria for inclusion.",
    "round-A_JuniorMini_evaluation": 5,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "include (junior:5)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Understanding the Effect of using Semantically Meaningful Tokens for Visual Representation Learning",
    "abstract": "Vision transformers have established a precedent of patchifying images into uniformly-sized chunks before processing. We hypothesize that this design choice may limit models in learning comprehensive and compositional representations from visual data. This paper explores the notion of providing semantically-meaningful visual tokens to transformer encoders within a vision-language pre-training framework. Leveraging off-the-shelf segmentation and scene-graph models, we extract representations of instance segmentation masks (referred to as tangible tokens) and relationships and actions (referred to as intangible tokens). Subsequently, we pre-train a vision-side transformer by incorporating these newly extracted tokens and aligning the resultant embeddings with caption embeddings from a text-side encoder. To capture the structural and semantic relationships among visual tokens, we introduce additive attention weights, which are used to compute self-attention scores. Our experiments on COCO demonstrate notable improvements over ViTs in learned representation quality across text-to-image (+47%) and image-to-text retrieval (+44%) tasks. Furthermore, we showcase the advantages on compositionality benchmarks such as ARO (+18%) and Winoground (+10%).",
    "metadata": {
      "arxiv_id": "2405.16401",
      "title": "Understanding the Effect of using Semantically Meaningful Tokens for Visual Representation Learning",
      "summary": "Vision transformers have established a precedent of patchifying images into uniformly-sized chunks before processing. We hypothesize that this design choice may limit models in learning comprehensive and compositional representations from visual data. This paper explores the notion of providing semantically-meaningful visual tokens to transformer encoders within a vision-language pre-training framework. Leveraging off-the-shelf segmentation and scene-graph models, we extract representations of instance segmentation masks (referred to as tangible tokens) and relationships and actions (referred to as intangible tokens). Subsequently, we pre-train a vision-side transformer by incorporating these newly extracted tokens and aligning the resultant embeddings with caption embeddings from a text-side encoder. To capture the structural and semantic relationships among visual tokens, we introduce additive attention weights, which are used to compute self-attention scores. Our experiments on COCO demonstrate notable improvements over ViTs in learned representation quality across text-to-image (+47%) and image-to-text retrieval (+44%) tasks. Furthermore, we showcase the advantages on compositionality benchmarks such as ARO (+18%) and Winoground (+10%).",
      "authors": [
        "Neha Kalibhat",
        "Priyatham Kattakinda",
        "Sumit Nawathe",
        "Arman Zarei",
        "Nikita Seleznev",
        "Samuel Sharpe",
        "Senthil Kumar",
        "Soheil Feizi"
      ],
      "published": "2024-05-26T01:46:22Z",
      "updated": "2025-05-19T16:00:51Z",
      "categories": [
        "cs.CV",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2405.16401v2",
      "landing_url": "https://arxiv.org/abs/2405.16401v2",
      "doi": "https://doi.org/10.48550/arXiv.2405.16401"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item studies semantically meaningful visual tokens for vision transformers and does not involve discrete audio/speech tokenization, thus it fails the audio-token inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item studies semantically meaningful visual tokens for vision transformers and does not involve discrete audio/speech tokenization, thus it fails the audio-token inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on visual representation learning using semantically meaningful tokens derived from image segmentation and scene-graph models, and does not discuss discrete audio/speech tokenization, nor does it involve audio signal conversion or related audio reconstruction, generation, or understanding tasks.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on visual representation learning using semantically meaningful tokens derived from image segmentation and scene-graph models, and does not discuss discrete audio/speech tokenization, nor does it involve audio signal conversion or related audio reconstruction, generation, or understanding tasks.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Rethinking Spectral Augmentation for Contrast-based Graph Self-Supervised Learning",
    "abstract": "The recent surge in contrast-based graph self-supervised learning has prominently featured an intensified exploration of spectral cues. Spectral augmentation, which involves modifying a graph's spectral properties such as eigenvalues or eigenvectors, is widely believed to enhance model performance. However, an intriguing paradox emerges, as methods grounded in seemingly conflicting assumptions regarding the spectral domain demonstrate notable enhancements in learning performance. Through extensive empirical studies, we find that simple edge perturbations - random edge dropping for node-level and random edge adding for graph-level self-supervised learning - consistently yield comparable or superior performance while being significantly more computationally efficient. This suggests that the computational overhead of sophisticated spectral augmentations may not justify their practical benefits. Our theoretical analysis of the InfoNCE loss bounds for shallow GNNs further supports this observation. The proposed insights represent a significant leap forward in the field, potentially refining the understanding and implementation of graph self-supervised learning.",
    "metadata": {
      "arxiv_id": "2405.19600",
      "title": "Rethinking Spectral Augmentation for Contrast-based Graph Self-Supervised Learning",
      "summary": "The recent surge in contrast-based graph self-supervised learning has prominently featured an intensified exploration of spectral cues. Spectral augmentation, which involves modifying a graph's spectral properties such as eigenvalues or eigenvectors, is widely believed to enhance model performance. However, an intriguing paradox emerges, as methods grounded in seemingly conflicting assumptions regarding the spectral domain demonstrate notable enhancements in learning performance. Through extensive empirical studies, we find that simple edge perturbations - random edge dropping for node-level and random edge adding for graph-level self-supervised learning - consistently yield comparable or superior performance while being significantly more computationally efficient. This suggests that the computational overhead of sophisticated spectral augmentations may not justify their practical benefits. Our theoretical analysis of the InfoNCE loss bounds for shallow GNNs further supports this observation. The proposed insights represent a significant leap forward in the field, potentially refining the understanding and implementation of graph self-supervised learning.",
      "authors": [
        "Xiangru Jian",
        "Xinjian Zhao",
        "Wei Pang",
        "Chaolong Ying",
        "Yimu Wang",
        "Yaoyao Xu",
        "Tianshu Yu"
      ],
      "published": "2024-05-30T01:30:34Z",
      "updated": "2024-12-04T04:41:49Z",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2405.19600v2",
      "landing_url": "https://arxiv.org/abs/2405.19600v2",
      "doi": "https://doi.org/10.48550/arXiv.2405.19600"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The title/abstract concern spectral augmentation in graph self-supervised learning and do not involve discrete audio tokenization or speech tasks; it fails the inclusion criteria for discrete audio/tokenization analysis and does not meet any exclusion-based necessity for tokenized audio representations.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The title/abstract concern spectral augmentation in graph self-supervised learning and do not involve discrete audio tokenization or speech tasks; it fails the inclusion criteria for discrete audio/tokenization analysis and does not meet any exclusion-based necessity for tokenized audio representations.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on graph self-supervised learning using spectral augmentation without any mention or analysis of discrete audio/speech tokenization or audio signal processing; it is unrelated to the inclusion criteria centered on discrete audio tokenization.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on graph self-supervised learning using spectral augmentation without any mention or analysis of discrete audio/speech tokenization or audio signal processing; it is unrelated to the inclusion criteria centered on discrete audio tokenization.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "SeamlessExpressiveLM: Speech Language Model for Expressive Speech-to-Speech Translation with Chain-of-Thought",
    "abstract": "Expressive speech-to-speech translation (S2ST) is a key research topic in seamless communication, which focuses on the preservation of semantics and speaker vocal style in translated speech. Early works synthesized speaker style aligned speech in order to directly learn the mapping from speech to target speech spectrogram. Without reliance on style aligned data, recent studies leverage the advances of language modeling (LM) and build cascaded LMs on semantic and acoustic tokens. This work proposes SeamlessExpressiveLM, a single speech language model for expressive S2ST. We decompose the complex source-to-target speech mapping into intermediate generation steps with chain-of-thought prompting. The model is first guided to translate target semantic content and then transfer the speaker style to multi-stream acoustic units. Evaluated on Spanish-to-English and Hungarian-to-English translations, SeamlessExpressiveLM outperforms cascaded LMs in both semantic quality and style transfer, meanwhile achieving better parameter efficiency.",
    "metadata": {
      "arxiv_id": "2405.20410",
      "title": "SeamlessExpressiveLM: Speech Language Model for Expressive Speech-to-Speech Translation with Chain-of-Thought",
      "summary": "Expressive speech-to-speech translation (S2ST) is a key research topic in seamless communication, which focuses on the preservation of semantics and speaker vocal style in translated speech. Early works synthesized speaker style aligned speech in order to directly learn the mapping from speech to target speech spectrogram. Without reliance on style aligned data, recent studies leverage the advances of language modeling (LM) and build cascaded LMs on semantic and acoustic tokens. This work proposes SeamlessExpressiveLM, a single speech language model for expressive S2ST. We decompose the complex source-to-target speech mapping into intermediate generation steps with chain-of-thought prompting. The model is first guided to translate target semantic content and then transfer the speaker style to multi-stream acoustic units. Evaluated on Spanish-to-English and Hungarian-to-English translations, SeamlessExpressiveLM outperforms cascaded LMs in both semantic quality and style transfer, meanwhile achieving better parameter efficiency.",
      "authors": [
        "Hongyu Gong",
        "Bandhav Veluri"
      ],
      "published": "2024-05-30T18:28:31Z",
      "updated": "2024-05-30T18:28:31Z",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2405.20410v1",
      "landing_url": "https://arxiv.org/abs/2405.20410v1",
      "doi": "https://doi.org/10.48550/arXiv.2405.20410"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "Although the abstract references discrete acoustic tokens and acoustic units, the work centers on a single speech language model for expressive S2ST and chain-of-thought prompting rather than explicitly proposing or evaluating discrete audio tokenization as the core object.",
      "evaluation": 3
    },
    "round-A_JuniorNano_reasoning": "Although the abstract references discrete acoustic tokens and acoustic units, the work centers on a single speech language model for expressive S2ST and chain-of-thought prompting rather than explicitly proposing or evaluating discrete audio tokenization as the core object.",
    "round-A_JuniorNano_evaluation": 3,
    "round-A_JuniorMini_output": {
      "reasoning": "The study proposes a speech language model that involves discrete semantic and acoustic tokens for expressive speech-to-speech translation, indicating an explicit focus on discrete audio tokenization as a core part of the methodology rather than just continuous features; thus, it meets the inclusion criteria and does not fall into any exclusion category.",
      "evaluation": 5
    },
    "round-A_JuniorMini_reasoning": "The study proposes a speech language model that involves discrete semantic and acoustic tokens for expressive speech-to-speech translation, indicating an explicit focus on discrete audio tokenization as a core part of the methodology rather than just continuous features; thus, it meets the inclusion criteria and does not fall into any exclusion category.",
    "round-A_JuniorMini_evaluation": 5,
    "round-B_SeniorLead_output": {
      "reasoning": "Both junior reviewers noted the presence of \"acoustic/semantic tokens\" and \"multi-stream acoustic units,\" but their conclusions diverged: one viewed the paper as centered on a speech LM/chain-of-thought (not on discrete token design) while the other interpreted the use of discrete tokens as a core methodological element. The abstract does mention discrete semantic and acoustic tokens and multi-stream acoustic units, but it does not explicitly propose, analyze, or evaluate a discrete audio tokenization method as the primary contribution (it focuses on a single speech LM and chain-of-thought pipeline for S2ST), so from the abstract alone I cannot confirm the paper meets the inclusion criterion that discrete tokenization is a core object of study; the full text would be needed to decide. Therefore I rate it as uncertain rather than included or excluded.",
      "evaluation": 3
    },
    "round-B_SeniorLead_reasoning": "Both junior reviewers noted the presence of \"acoustic/semantic tokens\" and \"multi-stream acoustic units,\" but their conclusions diverged: one viewed the paper as centered on a speech LM/chain-of-thought (not on discrete token design) while the other interpreted the use of discrete tokens as a core methodological element. The abstract does mention discrete semantic and acoustic tokens and multi-stream acoustic units, but it does not explicitly propose, analyze, or evaluate a discrete audio tokenization method as the primary contribution (it focuses on a single speech LM and chain-of-thought pipeline for S2ST), so from the abstract alone I cannot confirm the paper meets the inclusion criterion that discrete tokenization is a core object of study; the full text would be needed to decide. Therefore I rate it as uncertain rather than included or excluded.",
    "round-B_SeniorLead_evaluation": 3,
    "final_verdict": "maybe (senior:3)",
    "review_skipped": false,
    "discard_reason": "review_needs_followup"
  },
  {
    "title": "Generative Pre-trained Speech Language Model with Efficient Hierarchical Transformer",
    "abstract": "While recent advancements in speech language models have achieved significant progress, they face remarkable challenges in modeling the long acoustic sequences of neural audio codecs. In this paper, we introduce \\textbf{G}enerative \\textbf{P}re-trained \\textbf{S}peech \\textbf{T}ransformer (GPST), a hierarchical transformer designed for efficient speech language modeling. GPST quantizes audio waveforms into two distinct types of discrete speech representations and integrates them within a hierarchical transformer architecture, allowing for a unified one-stage generation process and enhancing Hi-Res audio generation capabilities. By training on large corpora of speeches in an end-to-end unsupervised manner, GPST can generate syntactically consistent speech with diverse speaker identities. Given a brief 3-second prompt, GPST can produce natural and coherent personalized speech, demonstrating in-context learning abilities. Moreover, our approach can be easily extended to spoken cross-lingual speech generation by incorporating multi-lingual semantic tokens and universal acoustic tokens. Experimental results indicate that GPST significantly outperforms the existing speech language models in terms of word error rate, speech quality, and speaker similarity. The code is available at \\url{https://github.com/youngsheen/GPST}.",
    "metadata": {
      "arxiv_id": "2406.00976",
      "title": "Generative Pre-trained Speech Language Model with Efficient Hierarchical Transformer",
      "summary": "While recent advancements in speech language models have achieved significant progress, they face remarkable challenges in modeling the long acoustic sequences of neural audio codecs. In this paper, we introduce \\textbf{G}enerative \\textbf{P}re-trained \\textbf{S}peech \\textbf{T}ransformer (GPST), a hierarchical transformer designed for efficient speech language modeling. GPST quantizes audio waveforms into two distinct types of discrete speech representations and integrates them within a hierarchical transformer architecture, allowing for a unified one-stage generation process and enhancing Hi-Res audio generation capabilities. By training on large corpora of speeches in an end-to-end unsupervised manner, GPST can generate syntactically consistent speech with diverse speaker identities. Given a brief 3-second prompt, GPST can produce natural and coherent personalized speech, demonstrating in-context learning abilities. Moreover, our approach can be easily extended to spoken cross-lingual speech generation by incorporating multi-lingual semantic tokens and universal acoustic tokens. Experimental results indicate that GPST significantly outperforms the existing speech language models in terms of word error rate, speech quality, and speaker similarity. The code is available at \\url{https://github.com/youngsheen/GPST}.",
      "authors": [
        "Yongxin Zhu",
        "Dan Su",
        "Liqiang He",
        "Linli Xu",
        "Dong Yu"
      ],
      "published": "2024-06-03T04:16:30Z",
      "updated": "2024-11-01T13:54:48Z",
      "categories": [
        "cs.CL",
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2406.00976v2",
      "landing_url": "https://arxiv.org/abs/2406.00976v2",
      "doi": "https://doi.org/10.48550/arXiv.2406.00976"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "GPST explicitly quantizes audio waveforms into discrete speech representations (two token types) for end-to-end generation, fulfilling the discrete audio tokenization requirement and thus should be included.",
      "evaluation": 5
    },
    "round-A_JuniorNano_reasoning": "GPST explicitly quantizes audio waveforms into discrete speech representations (two token types) for end-to-end generation, fulfilling the discrete audio tokenization requirement and thus should be included.",
    "round-A_JuniorNano_evaluation": 5,
    "round-A_JuniorMini_output": {
      "reasoning": "The study explicitly proposes a hierarchical transformer model that quantizes audio waveforms into discrete speech representations, focusing on discrete tokenization of speech audio; it includes experimental validation and methodological contributions related to discrete audio tokenization in speech language modeling and generation tasks, meeting all inclusion criteria and none of the exclusion criteria.",
      "evaluation": 5
    },
    "round-A_JuniorMini_reasoning": "The study explicitly proposes a hierarchical transformer model that quantizes audio waveforms into discrete speech representations, focusing on discrete tokenization of speech audio; it includes experimental validation and methodological contributions related to discrete audio tokenization in speech language modeling and generation tasks, meeting all inclusion criteria and none of the exclusion criteria.",
    "round-A_JuniorMini_evaluation": 5,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "include (junior:5)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "MaskSR: Masked Language Model for Full-band Speech Restoration",
    "abstract": "Speech restoration aims at restoring high quality speech in the presence of a diverse set of distortions. Although several deep learning paradigms have been studied for this task, the power of the recently emerging language models has not been fully explored. In this paper, we propose MaskSR, a masked language model capable of restoring full-band 44.1 kHz speech jointly considering noise, reverb, clipping, and low bandwidth. MaskSR works with discrete acoustic tokens extracted using a pre-trained neural codec. During training, MaskSR is optimized to predict randomly masked tokens extracted from the high quality target speech, conditioned on the corrupted speech with various distortions. During inference, MaskSR reconstructs the target speech tokens with efficient iterative sampling. Extensive experiments show that MaskSR obtains competitive results on both the full-band speech restoration task and also on sub-tasks compared with a wide range of models.",
    "metadata": {
      "arxiv_id": "2406.02092",
      "title": "MaskSR: Masked Language Model for Full-band Speech Restoration",
      "summary": "Speech restoration aims at restoring high quality speech in the presence of a diverse set of distortions. Although several deep learning paradigms have been studied for this task, the power of the recently emerging language models has not been fully explored. In this paper, we propose MaskSR, a masked language model capable of restoring full-band 44.1 kHz speech jointly considering noise, reverb, clipping, and low bandwidth. MaskSR works with discrete acoustic tokens extracted using a pre-trained neural codec. During training, MaskSR is optimized to predict randomly masked tokens extracted from the high quality target speech, conditioned on the corrupted speech with various distortions. During inference, MaskSR reconstructs the target speech tokens with efficient iterative sampling. Extensive experiments show that MaskSR obtains competitive results on both the full-band speech restoration task and also on sub-tasks compared with a wide range of models.",
      "authors": [
        "Xu Li",
        "Qirui Wang",
        "Xiaoyu Liu"
      ],
      "published": "2024-06-04T08:23:57Z",
      "updated": "2024-06-04T08:23:57Z",
      "categories": [
        "cs.SD",
        "cs.AI",
        "cs.LG",
        "eess.AS",
        "eess.SP"
      ],
      "pdf_url": "https://arxiv.org/pdf/2406.02092v1",
      "landing_url": "https://arxiv.org/abs/2406.02092v1",
      "doi": "https://doi.org/10.48550/arXiv.2406.02092"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The study explicitly uses discrete acoustic tokens from a neural codec for speech restoration, satisfying the discrete tokenization inclusion criterion and not falling under any exclusion criteria.",
      "evaluation": 5
    },
    "round-A_JuniorNano_reasoning": "The study explicitly uses discrete acoustic tokens from a neural codec for speech restoration, satisfying the discrete tokenization inclusion criterion and not falling under any exclusion criteria.",
    "round-A_JuniorNano_evaluation": 5,
    "round-A_JuniorMini_output": {
      "reasoning": "The study explicitly proposes MaskSR, a masked language model that operates on discrete acoustic tokens extracted by a pre-trained neural codec for full-band speech restoration, which directly involves discrete audio tokenization for audio reconstruction, fulfilling the inclusion criteria and not falling under any exclusion criteria.",
      "evaluation": 5
    },
    "round-A_JuniorMini_reasoning": "The study explicitly proposes MaskSR, a masked language model that operates on discrete acoustic tokens extracted by a pre-trained neural codec for full-band speech restoration, which directly involves discrete audio tokenization for audio reconstruction, fulfilling the inclusion criteria and not falling under any exclusion criteria.",
    "round-A_JuniorMini_evaluation": 5,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "include (junior:5)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Addressing Index Collapse of Large-Codebook Speech Tokenizer with Dual-Decoding Product-Quantized Variational Auto-Encoder",
    "abstract": "VQ-VAE, as a mainstream approach of speech tokenizer, has been troubled by ``index collapse'', where only a small number of codewords are activated in large codebooks. This work proposes product-quantized (PQ) VAE with more codebooks but fewer codewords to address this problem and build large-codebook speech tokenizers. It encodes speech features into multiple VQ subspaces and composes them into codewords in a larger codebook. Besides, to utilize each VQ subspace well, we also enhance PQ-VAE via a dual-decoding training strategy with the encoding and quantized sequences. The experimental results demonstrate that PQ-VAE addresses ``index collapse\" effectively, especially for larger codebooks. The model with the proposed training strategy further improves codebook perplexity and reconstruction quality, outperforming other multi-codebook VQ approaches. Finally, PQ-VAE demonstrates its effectiveness in language-model-based TTS, supporting higher-quality speech generation with larger codebooks.",
    "metadata": {
      "arxiv_id": "2406.02940",
      "title": "Addressing Index Collapse of Large-Codebook Speech Tokenizer with Dual-Decoding Product-Quantized Variational Auto-Encoder",
      "summary": "VQ-VAE, as a mainstream approach of speech tokenizer, has been troubled by ``index collapse'', where only a small number of codewords are activated in large codebooks. This work proposes product-quantized (PQ) VAE with more codebooks but fewer codewords to address this problem and build large-codebook speech tokenizers. It encodes speech features into multiple VQ subspaces and composes them into codewords in a larger codebook. Besides, to utilize each VQ subspace well, we also enhance PQ-VAE via a dual-decoding training strategy with the encoding and quantized sequences. The experimental results demonstrate that PQ-VAE addresses ``index collapse\" effectively, especially for larger codebooks. The model with the proposed training strategy further improves codebook perplexity and reconstruction quality, outperforming other multi-codebook VQ approaches. Finally, PQ-VAE demonstrates its effectiveness in language-model-based TTS, supporting higher-quality speech generation with larger codebooks.",
      "authors": [
        "Haohan Guo",
        "Fenglong Xie",
        "Dongchao Yang",
        "Hui Lu",
        "Xixin Wu",
        "Helen Meng"
      ],
      "published": "2024-06-05T04:54:49Z",
      "updated": "2024-06-05T04:54:49Z",
      "categories": [
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2406.02940v1",
      "landing_url": "https://arxiv.org/abs/2406.02940v1",
      "doi": "https://doi.org/10.48550/arXiv.2406.02940"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This work explicitly targets discrete speech tokenization via VQ-VAE with product-quantized codebooks and a dual-decoding strategy, addressing how continuous speech is represented as discrete tokens for reconstruction and generation, which aligns with the inclusion criteria and does not fit the exclusion categories.",
      "evaluation": 5
    },
    "round-A_JuniorNano_reasoning": "This work explicitly targets discrete speech tokenization via VQ-VAE with product-quantized codebooks and a dual-decoding strategy, addressing how continuous speech is represented as discrete tokens for reconstruction and generation, which aligns with the inclusion criteria and does not fit the exclusion categories.",
    "round-A_JuniorNano_evaluation": 5,
    "round-A_JuniorMini_output": {
      "reasoning": "The study explicitly proposes a novel discrete audio tokenization method using product-quantized VAE to address index collapse in large codebook speech tokenizers, which aligns well with the inclusion criteria, while not falling into any exclusion criteria regarding continuous features only or non-speech tokenization.",
      "evaluation": 5
    },
    "round-A_JuniorMini_reasoning": "The study explicitly proposes a novel discrete audio tokenization method using product-quantized VAE to address index collapse in large codebook speech tokenizers, which aligns well with the inclusion criteria, while not falling into any exclusion criteria regarding continuous features only or non-speech tokenization.",
    "round-A_JuniorMini_evaluation": 5,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "include (junior:5)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "The PESQetarian: On the Relevance of Goodhart's Law for Speech Enhancement",
    "abstract": "To obtain improved speech enhancement models, researchers often focus on increasing performance according to specific instrumental metrics. However, when the same metric is used in a loss function to optimize models, it may be detrimental to aspects that the given metric does not see. The goal of this paper is to illustrate the risk of overfitting a speech enhancement model to the metric used for evaluation. For this, we introduce enhancement models that exploit the widely used PESQ measure. Our \"PESQetarian\" model achieves 3.82 PESQ on VB-DMD while scoring very poorly in a listening experiment. While the obtained PESQ value of 3.82 would imply \"state-of-the-art\" PESQ-performance on the VB-DMD benchmark, our examples show that when optimizing w.r.t. a metric, an isolated evaluation on the same metric may be misleading. Instead, other metrics should be included in the evaluation and the resulting performance predictions should be confirmed by listening.",
    "metadata": {
      "arxiv_id": "2406.03460",
      "title": "The PESQetarian: On the Relevance of Goodhart's Law for Speech Enhancement",
      "summary": "To obtain improved speech enhancement models, researchers often focus on increasing performance according to specific instrumental metrics. However, when the same metric is used in a loss function to optimize models, it may be detrimental to aspects that the given metric does not see. The goal of this paper is to illustrate the risk of overfitting a speech enhancement model to the metric used for evaluation. For this, we introduce enhancement models that exploit the widely used PESQ measure. Our \"PESQetarian\" model achieves 3.82 PESQ on VB-DMD while scoring very poorly in a listening experiment. While the obtained PESQ value of 3.82 would imply \"state-of-the-art\" PESQ-performance on the VB-DMD benchmark, our examples show that when optimizing w.r.t. a metric, an isolated evaluation on the same metric may be misleading. Instead, other metrics should be included in the evaluation and the resulting performance predictions should be confirmed by listening.",
      "authors": [
        "Danilo de Oliveira",
        "Simon Welker",
        "Julius Richter",
        "Timo Gerkmann"
      ],
      "published": "2024-06-05T17:07:39Z",
      "updated": "2024-06-05T17:07:39Z",
      "categories": [
        "eess.AS",
        "cs.LG",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2406.03460v1",
      "landing_url": "https://arxiv.org/abs/2406.03460v1",
      "doi": "https://doi.org/10.48550/arXiv.2406.03460"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The abstract critiques evaluation metrics (PESQ) in speech enhancement but contains no discrete audio tokenization or token-based representations, so it does not meet the tokenization-focused inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The abstract critiques evaluation metrics (PESQ) in speech enhancement but contains no discrete audio tokenization or token-based representations, so it does not meet the tokenization-focused inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on speech enhancement by optimizing metrics such as PESQ and illustrates problems of metric overfitting; however, it does not propose, analyze, or evaluate discrete audio/speech tokenization or discrete symbol representations, which is central to the inclusion criteria. The work uses continuous metrics and enhancement models but not discrete tokenization or quantization-based discrete token designs, hence it does not meet the core inclusion requirement.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on speech enhancement by optimizing metrics such as PESQ and illustrates problems of metric overfitting; however, it does not propose, analyze, or evaluate discrete audio/speech tokenization or discrete symbol representations, which is central to the inclusion criteria. The work uses continuous metrics and enhancement models but not discrete tokenization or quantization-based discrete token designs, hence it does not meet the core inclusion requirement.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Improving Audio Codec-based Zero-Shot Text-to-Speech Synthesis with Multi-Modal Context and Large Language Model",
    "abstract": "Recent advances in large language models (LLMs) and development of audio codecs greatly propel the zero-shot TTS. They can synthesize personalized speech with only a 3-second speech of an unseen speaker as acoustic prompt. However, they only support short speech prompts and cannot leverage longer context information, as required in audiobook and conversational TTS scenarios. In this paper, we introduce a novel audio codec-based TTS model to adapt context features with multiple enhancements. Inspired by the success of Qformer, we propose a multi-modal context-enhanced Qformer (MMCE-Qformer) to utilize additional multi-modal context information. Besides, we adapt a pretrained LLM to leverage its understanding ability to predict semantic tokens, and use a SoundStorm to generate acoustic tokens thereby enhancing audio quality and speaker similarity. The extensive objective and subjective evaluations show that our proposed method outperforms baselines across various context TTS scenarios.",
    "metadata": {
      "arxiv_id": "2406.03706",
      "title": "Improving Audio Codec-based Zero-Shot Text-to-Speech Synthesis with Multi-Modal Context and Large Language Model",
      "summary": "Recent advances in large language models (LLMs) and development of audio codecs greatly propel the zero-shot TTS. They can synthesize personalized speech with only a 3-second speech of an unseen speaker as acoustic prompt. However, they only support short speech prompts and cannot leverage longer context information, as required in audiobook and conversational TTS scenarios. In this paper, we introduce a novel audio codec-based TTS model to adapt context features with multiple enhancements. Inspired by the success of Qformer, we propose a multi-modal context-enhanced Qformer (MMCE-Qformer) to utilize additional multi-modal context information. Besides, we adapt a pretrained LLM to leverage its understanding ability to predict semantic tokens, and use a SoundStorm to generate acoustic tokens thereby enhancing audio quality and speaker similarity. The extensive objective and subjective evaluations show that our proposed method outperforms baselines across various context TTS scenarios.",
      "authors": [
        "Jinlong Xue",
        "Yayue Deng",
        "Yicheng Han",
        "Yingming Gao",
        "Ya Li"
      ],
      "published": "2024-06-06T03:06:45Z",
      "updated": "2024-06-06T03:06:45Z",
      "categories": [
        "cs.SD",
        "cs.CL",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2406.03706v1",
      "landing_url": "https://arxiv.org/abs/2406.03706v1",
      "doi": "https://doi.org/10.48550/arXiv.2406.03706"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The work explicitly centers on discrete audio tokenization (semantic and acoustic tokens) within an audio codec–based TTS framework, meeting the inclusion criterion and not falling under any exclusion.",
      "evaluation": 4
    },
    "round-A_JuniorNano_reasoning": "The work explicitly centers on discrete audio tokenization (semantic and acoustic tokens) within an audio codec–based TTS framework, meeting the inclusion criterion and not falling under any exclusion.",
    "round-A_JuniorNano_evaluation": 4,
    "round-A_JuniorMini_output": {
      "reasoning": "The study proposes an audio codec-based zero-shot TTS model using discrete audio tokens (acoustic tokens generated by SoundStorm and semantic tokens predicted by a pretrained LLM) to improve speech synthesis, indicating a clear focus on discrete audio tokenization for speech generation, which satisfies the inclusion criteria and does not violate any exclusion criteria.",
      "evaluation": 5
    },
    "round-A_JuniorMini_reasoning": "The study proposes an audio codec-based zero-shot TTS model using discrete audio tokens (acoustic tokens generated by SoundStorm and semantic tokens predicted by a pretrained LLM) to improve speech synthesis, indicating a clear focus on discrete audio tokenization for speech generation, which satisfies the inclusion criteria and does not violate any exclusion criteria.",
    "round-A_JuniorMini_evaluation": 5,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "include (junior:4)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Neural Codec-based Adversarial Sample Detection for Speaker Verification",
    "abstract": "Automatic Speaker Verification (ASV), increasingly used in security-critical applications, faces vulnerabilities from rising adversarial attacks, with few effective defenses available. In this paper, we propose a neural codec-based adversarial sample detection method for ASV. The approach leverages the codec's ability to discard redundant perturbations and retain essential information. Specifically, we distinguish between genuine and adversarial samples by comparing ASV score differences between original and re-synthesized audio (by codec models). This comprehensive study explores all open-source neural codecs and their variant models for experiments. The Descript-audio-codec model stands out by delivering the highest detection rate among 15 neural codecs and surpassing seven prior state-of-the-art (SOTA) detection methods. Note that, our single-model method even outperforms a SOTA ensemble method by a large margin.",
    "metadata": {
      "arxiv_id": "2406.04582",
      "title": "Neural Codec-based Adversarial Sample Detection for Speaker Verification",
      "summary": "Automatic Speaker Verification (ASV), increasingly used in security-critical applications, faces vulnerabilities from rising adversarial attacks, with few effective defenses available. In this paper, we propose a neural codec-based adversarial sample detection method for ASV. The approach leverages the codec's ability to discard redundant perturbations and retain essential information. Specifically, we distinguish between genuine and adversarial samples by comparing ASV score differences between original and re-synthesized audio (by codec models). This comprehensive study explores all open-source neural codecs and their variant models for experiments. The Descript-audio-codec model stands out by delivering the highest detection rate among 15 neural codecs and surpassing seven prior state-of-the-art (SOTA) detection methods. Note that, our single-model method even outperforms a SOTA ensemble method by a large margin.",
      "authors": [
        "Xuanjun Chen",
        "Jiawei Du",
        "Haibin Wu",
        "Jyh-Shing Roger Jang",
        "Hung-yi Lee"
      ],
      "published": "2024-06-07T02:03:27Z",
      "updated": "2024-06-07T02:03:27Z",
      "categories": [
        "eess.AS",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2406.04582v1",
      "landing_url": "https://arxiv.org/abs/2406.04582v1",
      "doi": "https://doi.org/10.48550/arXiv.2406.04582"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The paper uses neural codecs to detect adversarial samples in speaker verification, which implies discrete tokenization via encoding/quantization and aligns with the inclusion criterion, though tokenization is not explicitly framed as the core object in the abstract.",
      "evaluation": 4
    },
    "round-A_JuniorNano_reasoning": "The paper uses neural codecs to detect adversarial samples in speaker verification, which implies discrete tokenization via encoding/quantization and aligns with the inclusion criterion, though tokenization is not explicitly framed as the core object in the abstract.",
    "round-A_JuniorNano_evaluation": 4,
    "round-A_JuniorMini_output": {
      "reasoning": "The study proposes a neural codec-based method for adversarial sample detection in speaker verification, leveraging neural codecs to process audio signals. However, the abstract does not explicitly discuss discrete audio/speech tokenization as a core object of study, nor does it analyze or evaluate the conversion of continuous audio into discrete token sequences for reconstruction or understanding tasks, which is a key inclusion criterion. The work focuses on adversarial detection using neural codecs but lacks clear emphasis on discrete tokenization design or analysis, thus not fully meeting the inclusion requirements.",
      "evaluation": 2
    },
    "round-A_JuniorMini_reasoning": "The study proposes a neural codec-based method for adversarial sample detection in speaker verification, leveraging neural codecs to process audio signals. However, the abstract does not explicitly discuss discrete audio/speech tokenization as a core object of study, nor does it analyze or evaluate the conversion of continuous audio into discrete token sequences for reconstruction or understanding tasks, which is a key inclusion criterion. The work focuses on adversarial detection using neural codecs but lacks clear emphasis on discrete tokenization design or analysis, thus not fully meeting the inclusion requirements.",
    "round-A_JuniorMini_evaluation": 2,
    "round-B_SeniorLead_output": {
      "reasoning": "Junior reviews: JuniorNano argued the use of neural codecs implies discrete encoding/quantization and leaned toward inclusion (score 4); JuniorMini argued the abstract does not explicitly treat discrete tokenization as the core object and recommended exclusion (score 2). My assessment: the abstract describes using neural codecs to resynthesize audio and detect adversarial samples and reports experiments across many open-source neural codecs, but it does not explicitly propose, analyze, or evaluate discrete audio/speech tokenization (i.e., conversion to discrete symbol sequences) as a core contribution—there is no mention of discrete token/quantization design, token sequences, or evaluation of tokenization for reconstruction/generation/understanding tasks. Because the inclusion criteria require explicit focus on discrete tokenization, and the abstract only describes using codecs for detection (which could rely on continuous latents or off-the-shelf models) this study does not meet the required inclusion condition. One-sentence explanation: The abstract lacks an explicit proposal or analysis of discrete audio tokenization, so it should be excluded (score 2).",
      "evaluation": 2
    },
    "round-B_SeniorLead_reasoning": "Junior reviews: JuniorNano argued the use of neural codecs implies discrete encoding/quantization and leaned toward inclusion (score 4); JuniorMini argued the abstract does not explicitly treat discrete tokenization as the core object and recommended exclusion (score 2). My assessment: the abstract describes using neural codecs to resynthesize audio and detect adversarial samples and reports experiments across many open-source neural codecs, but it does not explicitly propose, analyze, or evaluate discrete audio/speech tokenization (i.e., conversion to discrete symbol sequences) as a core contribution—there is no mention of discrete token/quantization design, token sequences, or evaluation of tokenization for reconstruction/generation/understanding tasks. Because the inclusion criteria require explicit focus on discrete tokenization, and the abstract only describes using codecs for detection (which could rely on continuous latents or off-the-shelf models) this study does not meet the required inclusion condition. One-sentence explanation: The abstract lacks an explicit proposal or analysis of discrete audio tokenization, so it should be excluded (score 2).",
    "round-B_SeniorLead_evaluation": 2,
    "final_verdict": "exclude (senior:2)",
    "review_skipped": false,
    "discard_reason": "exclude (senior:2)"
  },
  {
    "title": "Boosting Diffusion Model for Spectrogram Up-sampling in Text-to-speech: An Empirical Study",
    "abstract": "Scaling text-to-speech (TTS) with autoregressive language model (LM) to large-scale datasets by quantizing waveform into discrete speech tokens is making great progress to capture the diversity and expressiveness in human speech, but the speech reconstruction quality from discrete speech token is far from satisfaction depending on the compressed speech token compression ratio. Generative diffusion models trained with score-matching loss and continuous normalized flow trained with flow-matching loss have become prominent in generation of images as well as speech. LM based TTS systems usually quantize speech into discrete tokens and generate these tokens autoregressively, and finally use a diffusion model to up sample coarse-grained speech tokens into fine-grained codec features or mel-spectrograms before reconstructing into waveforms with vocoder, which has a high latency and is not realistic for real time speech applications. In this paper, we systematically investigate varied diffusion models for up sampling stage, which is the main bottleneck for streaming synthesis of LM and diffusion-based architecture, we present the model architecture, objective and subjective metrics to show quality and efficiency improvement.",
    "metadata": {
      "arxiv_id": "2406.04633",
      "title": "Boosting Diffusion Model for Spectrogram Up-sampling in Text-to-speech: An Empirical Study",
      "summary": "Scaling text-to-speech (TTS) with autoregressive language model (LM) to large-scale datasets by quantizing waveform into discrete speech tokens is making great progress to capture the diversity and expressiveness in human speech, but the speech reconstruction quality from discrete speech token is far from satisfaction depending on the compressed speech token compression ratio. Generative diffusion models trained with score-matching loss and continuous normalized flow trained with flow-matching loss have become prominent in generation of images as well as speech. LM based TTS systems usually quantize speech into discrete tokens and generate these tokens autoregressively, and finally use a diffusion model to up sample coarse-grained speech tokens into fine-grained codec features or mel-spectrograms before reconstructing into waveforms with vocoder, which has a high latency and is not realistic for real time speech applications. In this paper, we systematically investigate varied diffusion models for up sampling stage, which is the main bottleneck for streaming synthesis of LM and diffusion-based architecture, we present the model architecture, objective and subjective metrics to show quality and efficiency improvement.",
      "authors": [
        "Chong Zhang",
        "Yanqing Liu",
        "Yang Zheng",
        "Sheng Zhao"
      ],
      "published": "2024-06-07T04:34:03Z",
      "updated": "2024-06-07T04:34:03Z",
      "categories": [
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2406.04633v1",
      "landing_url": "https://arxiv.org/abs/2406.04633v1",
      "doi": "https://doi.org/10.48550/arXiv.2406.04633"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The paper explicitly discusses quantizing speech into discrete tokens and uses diffusion-based up-sampling of those discrete tokens, addressing discrete audio tokenization as a core object of study.",
      "evaluation": 5
    },
    "round-A_JuniorNano_reasoning": "The paper explicitly discusses quantizing speech into discrete tokens and uses diffusion-based up-sampling of those discrete tokens, addressing discrete audio tokenization as a core object of study.",
    "round-A_JuniorNano_evaluation": 5,
    "round-A_JuniorMini_output": {
      "reasoning": "The study investigates the use of diffusion models for up-sampling discrete speech tokens in a text-to-speech system, explicitly dealing with the conversion of continuous audio to discrete speech tokens and analyzing the token-based reconstruction and generation process. Therefore, it meets the inclusion criteria of focusing on discrete audio tokenization and provides systematic analysis relevant to that area without falling into any exclusion criteria.",
      "evaluation": 5
    },
    "round-A_JuniorMini_reasoning": "The study investigates the use of diffusion models for up-sampling discrete speech tokens in a text-to-speech system, explicitly dealing with the conversion of continuous audio to discrete speech tokens and analyzing the token-based reconstruction and generation process. Therefore, it meets the inclusion criteria of focusing on discrete audio tokenization and provides systematic analysis relevant to that area without falling into any exclusion criteria.",
    "round-A_JuniorMini_evaluation": 5,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "include (junior:5)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Activation Map-based Vector Quantization for 360-degree Image Semantic Communication",
    "abstract": "In virtual reality (VR) applications, 360-degree images play a pivotal role in crafting immersive experiences and offering panoramic views, thus improving user Quality of Experience (QoE). However, the voluminous data generated by 360-degree images poses challenges in network storage and bandwidth. To address these challenges, we propose a novel Activation Map-based Vector Quantization (AM-VQ) framework, which is designed to reduce communication overhead for wireless transmission. The proposed AM-VQ scheme uses the Deep Neural Networks (DNNs) with vector quantization (VQ) to extract and compress semantic features. Particularly, the AM-VQ framework utilizes activation map to adaptively quantize semantic features, thus reducing data distortion caused by quantization operation. To further enhance the reconstruction quality of the 360-degree image, adversarial training with a Generative Adversarial Networks (GANs) discriminator is incorporated. Numerical results show that our proposed AM-VQ scheme achieves better performance than the existing Deep Learning (DL) based coding and the traditional coding schemes under the same transmission symbols.",
    "metadata": {
      "arxiv_id": "2406.04740",
      "title": "Activation Map-based Vector Quantization for 360-degree Image Semantic Communication",
      "summary": "In virtual reality (VR) applications, 360-degree images play a pivotal role in crafting immersive experiences and offering panoramic views, thus improving user Quality of Experience (QoE). However, the voluminous data generated by 360-degree images poses challenges in network storage and bandwidth. To address these challenges, we propose a novel Activation Map-based Vector Quantization (AM-VQ) framework, which is designed to reduce communication overhead for wireless transmission. The proposed AM-VQ scheme uses the Deep Neural Networks (DNNs) with vector quantization (VQ) to extract and compress semantic features. Particularly, the AM-VQ framework utilizes activation map to adaptively quantize semantic features, thus reducing data distortion caused by quantization operation. To further enhance the reconstruction quality of the 360-degree image, adversarial training with a Generative Adversarial Networks (GANs) discriminator is incorporated. Numerical results show that our proposed AM-VQ scheme achieves better performance than the existing Deep Learning (DL) based coding and the traditional coding schemes under the same transmission symbols.",
      "authors": [
        "Yang Ma",
        "Wenchi Cheng",
        "Jingqing Wang",
        "Wei Zhang"
      ],
      "published": "2024-06-07T08:40:53Z",
      "updated": "2024-06-07T08:40:53Z",
      "categories": [
        "eess.IV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2406.04740v1",
      "landing_url": "https://arxiv.org/abs/2406.04740v1",
      "doi": "https://doi.org/10.48550/arXiv.2406.04740"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The title/abstract describe a 360-degree image semantic communication framework using activation-map based vector quantization and GANs for image data, not discrete audio/speech tokenization, thus it does not meet the inclusion criteria which require discrete audio tokenization as the core focus.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The title/abstract describe a 360-degree image semantic communication framework using activation-map based vector quantization and GANs for image data, not discrete audio/speech tokenization, thus it does not meet the inclusion criteria which require discrete audio tokenization as the core focus.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on a novel method for compressing and transmitting 360-degree images using vector quantization of semantic features and does not address discrete audio/speech tokenization or the conversion of continuous audio signals into discrete tokens; thus, it does not meet the inclusion criteria related to audio tokenization and is outside the scope defined by the criteria.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on a novel method for compressing and transmitting 360-degree images using vector quantization of semantic features and does not address discrete audio/speech tokenization or the conversion of continuous audio signals into discrete tokens; thus, it does not meet the inclusion criteria related to audio tokenization and is outside the scope defined by the criteria.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "VALL-E 2: Neural Codec Language Models are Human Parity Zero-Shot Text to Speech Synthesizers",
    "abstract": "This paper introduces VALL-E 2, the latest advancement in neural codec language models that marks a milestone in zero-shot text-to-speech synthesis (TTS), achieving human parity for the first time. Based on its predecessor, VALL-E, the new iteration introduces two significant enhancements: Repetition Aware Sampling refines the original nucleus sampling process by accounting for token repetition in the decoding history. It not only stabilizes the decoding but also circumvents the infinite loop issue. Grouped Code Modeling organizes codec codes into groups to effectively shorten the sequence length, which not only boosts inference speed but also addresses the challenges of long sequence modeling. Our experiments on the LibriSpeech and VCTK datasets show that VALL-E 2 surpasses previous systems in speech robustness, naturalness, and speaker similarity. It is the first of its kind to reach human parity on these benchmarks. Moreover, VALL-E 2 consistently synthesizes high-quality speech, even for sentences that are traditionally challenging due to their complexity or repetitive phrases. The advantages of this work could contribute to valuable endeavors, such as generating speech for individuals with aphasia or people with amyotrophic lateral sclerosis. See https://aka.ms/valle2 for demos of VALL-E 2.",
    "metadata": {
      "arxiv_id": "2406.05370",
      "title": "VALL-E 2: Neural Codec Language Models are Human Parity Zero-Shot Text to Speech Synthesizers",
      "summary": "This paper introduces VALL-E 2, the latest advancement in neural codec language models that marks a milestone in zero-shot text-to-speech synthesis (TTS), achieving human parity for the first time. Based on its predecessor, VALL-E, the new iteration introduces two significant enhancements: Repetition Aware Sampling refines the original nucleus sampling process by accounting for token repetition in the decoding history. It not only stabilizes the decoding but also circumvents the infinite loop issue. Grouped Code Modeling organizes codec codes into groups to effectively shorten the sequence length, which not only boosts inference speed but also addresses the challenges of long sequence modeling. Our experiments on the LibriSpeech and VCTK datasets show that VALL-E 2 surpasses previous systems in speech robustness, naturalness, and speaker similarity. It is the first of its kind to reach human parity on these benchmarks. Moreover, VALL-E 2 consistently synthesizes high-quality speech, even for sentences that are traditionally challenging due to their complexity or repetitive phrases. The advantages of this work could contribute to valuable endeavors, such as generating speech for individuals with aphasia or people with amyotrophic lateral sclerosis. See https://aka.ms/valle2 for demos of VALL-E 2.",
      "authors": [
        "Sanyuan Chen",
        "Shujie Liu",
        "Long Zhou",
        "Yanqing Liu",
        "Xu Tan",
        "Jinyu Li",
        "Sheng Zhao",
        "Yao Qian",
        "Furu Wei"
      ],
      "published": "2024-06-08T06:31:03Z",
      "updated": "2024-06-17T04:39:08Z",
      "categories": [
        "cs.CL",
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2406.05370v2",
      "landing_url": "https://arxiv.org/abs/2406.05370v2",
      "doi": "https://doi.org/10.48550/arXiv.2406.05370"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The abstract centers on discrete audio tokenization via neural codec codes and grouped code modeling for TTS, treating audio as sequences of discrete tokens, thus meeting the inclusion criteria and not falling into exclusion categories.",
      "evaluation": 5
    },
    "round-A_JuniorNano_reasoning": "The abstract centers on discrete audio tokenization via neural codec codes and grouped code modeling for TTS, treating audio as sequences of discrete tokens, thus meeting the inclusion criteria and not falling into exclusion categories.",
    "round-A_JuniorNano_evaluation": 5,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper presents VALL-E 2, a neural codec language model for zero-shot text-to-speech that involves grouped code modeling to shorten codec code sequences, which indicates discrete audio tokenization; it also includes experimental validation surpassing previous systems, satisfying the inclusion criteria and not falling under any exclusion criteria.",
      "evaluation": 5
    },
    "round-A_JuniorMini_reasoning": "The paper presents VALL-E 2, a neural codec language model for zero-shot text-to-speech that involves grouped code modeling to shorten codec code sequences, which indicates discrete audio tokenization; it also includes experimental validation surpassing previous systems, satisfying the inclusion criteria and not falling under any exclusion criteria.",
    "round-A_JuniorMini_evaluation": 5,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "include (junior:5)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "MS-HuBERT: Mitigating Pre-training and Inference Mismatch in Masked Language Modelling methods for learning Speech Representations",
    "abstract": "In recent years, self-supervised pre-training methods have gained significant traction in learning high-level information from raw speech. Among these methods, HuBERT has demonstrated SOTA performance in automatic speech recognition (ASR). However, HuBERT's performance lags behind data2vec due to disparities in pre-training strategies. In this paper, we propose (i) a Swap method to address pre-training and inference mismatch observed in HuBERT and (ii) incorporates Multicluster masked prediction loss for more effective utilization of the models capacity. The resulting method is, MS-HuBERT, an end-to-end self-supervised pre-training method for learning robust speech representations. It beats vanilla HuBERT on the ASR Librispeech benchmark on average by a 5% margin when evaluated on different finetuning splits. Additionally, we demonstrate that the learned embeddings obtained during pre-training encode essential information for improving performance of content based tasks such as ASR.",
    "metadata": {
      "arxiv_id": "2406.05661",
      "title": "MS-HuBERT: Mitigating Pre-training and Inference Mismatch in Masked Language Modelling methods for learning Speech Representations",
      "summary": "In recent years, self-supervised pre-training methods have gained significant traction in learning high-level information from raw speech. Among these methods, HuBERT has demonstrated SOTA performance in automatic speech recognition (ASR). However, HuBERT's performance lags behind data2vec due to disparities in pre-training strategies. In this paper, we propose (i) a Swap method to address pre-training and inference mismatch observed in HuBERT and (ii) incorporates Multicluster masked prediction loss for more effective utilization of the models capacity. The resulting method is, MS-HuBERT, an end-to-end self-supervised pre-training method for learning robust speech representations. It beats vanilla HuBERT on the ASR Librispeech benchmark on average by a 5% margin when evaluated on different finetuning splits. Additionally, we demonstrate that the learned embeddings obtained during pre-training encode essential information for improving performance of content based tasks such as ASR.",
      "authors": [
        "Hemant Yadav",
        "Sunayana Sitaram",
        "Rajiv Ratn Shah"
      ],
      "published": "2024-06-09T06:30:28Z",
      "updated": "2025-02-18T10:07:36Z",
      "categories": [
        "cs.CL"
      ],
      "pdf_url": "https://arxiv.org/pdf/2406.05661v4",
      "landing_url": "https://arxiv.org/abs/2406.05661v4",
      "doi": "https://doi.org/10.48550/arXiv.2406.05661"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The study discusses MS-HuBERT with a swap method and multicluster masked prediction for speech representations; while it uses discrete cluster targets (implying tokenization), the abstract does not explicitly treat discrete speech tokenization as a core object of investigation, making inclusion uncertain.",
      "evaluation": 3
    },
    "round-A_JuniorNano_reasoning": "The study discusses MS-HuBERT with a swap method and multicluster masked prediction for speech representations; while it uses discrete cluster targets (implying tokenization), the abstract does not explicitly treat discrete speech tokenization as a core object of investigation, making inclusion uncertain.",
    "round-A_JuniorNano_evaluation": 3,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on self-supervised pre-training methods (MS-HuBERT) for learning robust speech representations and improving ASR performance but does not explicitly propose, analyze, or evaluate discrete audio/speech tokenization methods converting continuous signals into discrete token sequences; the abstract lacks mention of discrete tokenization or quantization design relevant to language-model-style tokens, hence it likely uses continuous features without discrete token analysis, failing the inclusion criteria while meeting exclusion criteria.",
      "evaluation": 2
    },
    "round-A_JuniorMini_reasoning": "The study focuses on self-supervised pre-training methods (MS-HuBERT) for learning robust speech representations and improving ASR performance but does not explicitly propose, analyze, or evaluate discrete audio/speech tokenization methods converting continuous signals into discrete token sequences; the abstract lacks mention of discrete tokenization or quantization design relevant to language-model-style tokens, hence it likely uses continuous features without discrete token analysis, failing the inclusion criteria while meeting exclusion criteria.",
    "round-A_JuniorMini_evaluation": 2,
    "round-B_SeniorLead_output": {
      "reasoning": "Both junior reviewers flagged uncertainty: JuniorNano (3) noted the paper uses clustering-like ideas but the abstract does not explicitly frame discrete tokenization as a core contribution; JuniorMini (2) judged it likely uses continuous features and does not propose or analyze discrete tokenization. Integrating their points and the abstract text: MS-HuBERT describes a Swap method and a multicluster masked prediction loss to improve HuBERT for ASR, but it does not explicitly propose, analyze, or evaluate discrete audio/speech tokenization (i.e., converting continuous audio into a finite vocabulary of discrete symbols) as a primary object of study; any use of clustered targets appears incidental to improving pre-training rather than a discrete-tokenization method aimed at generation/reconstruction or systematic analysis. Therefore it does not meet the inclusion criterion and falls under the exclusion criterion of studies that do not meaningfully address discrete tokenization design or analysis.",
      "evaluation": 2
    },
    "round-B_SeniorLead_reasoning": "Both junior reviewers flagged uncertainty: JuniorNano (3) noted the paper uses clustering-like ideas but the abstract does not explicitly frame discrete tokenization as a core contribution; JuniorMini (2) judged it likely uses continuous features and does not propose or analyze discrete tokenization. Integrating their points and the abstract text: MS-HuBERT describes a Swap method and a multicluster masked prediction loss to improve HuBERT for ASR, but it does not explicitly propose, analyze, or evaluate discrete audio/speech tokenization (i.e., converting continuous audio into a finite vocabulary of discrete symbols) as a primary object of study; any use of clustered targets appears incidental to improving pre-training rather than a discrete-tokenization method aimed at generation/reconstruction or systematic analysis. Therefore it does not meet the inclusion criterion and falls under the exclusion criterion of studies that do not meaningfully address discrete tokenization design or analysis.",
    "round-B_SeniorLead_evaluation": 2,
    "final_verdict": "exclude (senior:2)",
    "review_skipped": false,
    "discard_reason": "exclude (senior:2)"
  },
  {
    "title": "mHuBERT-147: A Compact Multilingual HuBERT Model",
    "abstract": "We present mHuBERT-147, the first general-purpose massively multilingual HuBERT speech representation model trained on 90K hours of clean, open-license data. To scale up the multi-iteration HuBERT approach, we use faiss-based clustering, achieving 5.2x faster label assignment than the original method. We also apply a new multilingual batching up-sampling strategy, leveraging both language and dataset diversity. After 3 training iterations, our compact 95M parameter mHuBERT-147 outperforms larger models trained on substantially more data. We rank second and first on the ML-SUPERB 10min and 1h leaderboards, with SOTA scores for 3 tasks. Across ASR/LID tasks, our model consistently surpasses XLS-R (300M params; 436K hours) and demonstrates strong competitiveness against the much larger MMS (1B params; 491K hours). Our findings indicate that mHuBERT-147 is a promising model for multilingual speech tasks, offering an unprecedented balance between high performance and parameter efficiency.",
    "metadata": {
      "arxiv_id": "2406.06371",
      "title": "mHuBERT-147: A Compact Multilingual HuBERT Model",
      "summary": "We present mHuBERT-147, the first general-purpose massively multilingual HuBERT speech representation model trained on 90K hours of clean, open-license data. To scale up the multi-iteration HuBERT approach, we use faiss-based clustering, achieving 5.2x faster label assignment than the original method. We also apply a new multilingual batching up-sampling strategy, leveraging both language and dataset diversity. After 3 training iterations, our compact 95M parameter mHuBERT-147 outperforms larger models trained on substantially more data. We rank second and first on the ML-SUPERB 10min and 1h leaderboards, with SOTA scores for 3 tasks. Across ASR/LID tasks, our model consistently surpasses XLS-R (300M params; 436K hours) and demonstrates strong competitiveness against the much larger MMS (1B params; 491K hours). Our findings indicate that mHuBERT-147 is a promising model for multilingual speech tasks, offering an unprecedented balance between high performance and parameter efficiency.",
      "authors": [
        "Marcely Zanon Boito",
        "Vivek Iyer",
        "Nikolaos Lagos",
        "Laurent Besacier",
        "Ioan Calapodescu"
      ],
      "published": "2024-06-10T15:32:42Z",
      "updated": "2024-11-21T10:45:39Z",
      "categories": [
        "cs.CL",
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2406.06371v5",
      "landing_url": "https://arxiv.org/abs/2406.06371v5",
      "doi": "https://doi.org/10.48550/arXiv.2406.06371"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The abstract centers on a compact multilingual HuBERT model and uses clustering to assign discrete labels, but it does not explicitly analyze discrete audio tokenization as a core research objective.",
      "evaluation": 3
    },
    "round-A_JuniorNano_reasoning": "The abstract centers on a compact multilingual HuBERT model and uses clustering to assign discrete labels, but it does not explicitly analyze discrete audio tokenization as a core research objective.",
    "round-A_JuniorNano_evaluation": 3,
    "round-A_JuniorMini_output": {
      "reasoning": "The study presents mHuBERT-147, a multilingual HuBERT speech representation model, emphasizing a new clustering approach and multilingual batching; however, although it deals with speech representations and clustering, there is no explicit mention or clear analysis of discrete audio/speech tokenization converting continuous signals into discrete symbol sequences for downstream tasks, which is required by the inclusion criteria. Therefore, it is unclear whether the core focus is on discrete tokenization itself rather than speech representation or model scaling techniques.",
      "evaluation": 3
    },
    "round-A_JuniorMini_reasoning": "The study presents mHuBERT-147, a multilingual HuBERT speech representation model, emphasizing a new clustering approach and multilingual batching; however, although it deals with speech representations and clustering, there is no explicit mention or clear analysis of discrete audio/speech tokenization converting continuous signals into discrete symbol sequences for downstream tasks, which is required by the inclusion criteria. Therefore, it is unclear whether the core focus is on discrete tokenization itself rather than speech representation or model scaling techniques.",
    "round-A_JuniorMini_evaluation": 3,
    "round-B_SeniorLead_output": {
      "reasoning": "The abstract describes a compact multilingual HuBERT model that uses faiss-based clustering for faster label assignment across multi-iteration training, but it frames the work primarily as a representation/model-scaling and multilingual-training contribution with downstream ASR/LID benchmark results rather than explicitly proposing, analyzing, or evaluating discrete audio/speech tokenization (i.e., treating discrete tokenization as the core object for reconstruction/generation/understanding). Both junior reviewers also flagged this ambiguity: they noted clustering/label assignment is present but found no clear, explicit focus on discrete tokenization in the abstract. Because the inclusion criteria require an explicit, central study of discrete audio tokenization and the abstract does not make that clear (and we do not have the full text to confirm), inclusion is uncertain.",
      "evaluation": 3
    },
    "round-B_SeniorLead_reasoning": "The abstract describes a compact multilingual HuBERT model that uses faiss-based clustering for faster label assignment across multi-iteration training, but it frames the work primarily as a representation/model-scaling and multilingual-training contribution with downstream ASR/LID benchmark results rather than explicitly proposing, analyzing, or evaluating discrete audio/speech tokenization (i.e., treating discrete tokenization as the core object for reconstruction/generation/understanding). Both junior reviewers also flagged this ambiguity: they noted clustering/label assignment is present but found no clear, explicit focus on discrete tokenization in the abstract. Because the inclusion criteria require an explicit, central study of discrete audio tokenization and the abstract does not make that clear (and we do not have the full text to confirm), inclusion is uncertain.",
    "round-B_SeniorLead_evaluation": 3,
    "final_verdict": "maybe (senior:3)",
    "review_skipped": false,
    "discard_reason": "review_needs_followup"
  },
  {
    "title": "Discrete Multimodal Transformers with a Pretrained Large Language Model for Mixed-Supervision Speech Processing",
    "abstract": "Recent work on discrete speech tokenization has paved the way for models that can seamlessly perform multiple tasks across modalities, e.g., speech recognition, text to speech, speech to speech translation. Moreover, large language models (LLMs) pretrained from vast text corpora contain rich linguistic information that can improve accuracy in a variety of tasks. In this paper, we present a decoder-only Discrete Multimodal Language Model (DMLM), which can be flexibly applied to multiple tasks (ASR, T2S, S2TT, etc.) and modalities (text, speech, vision). We explore several critical aspects of discrete multi-modal models, including the loss function, weight initialization, mixed training supervision, and codebook. Our results show that DMLM benefits significantly, across multiple tasks and datasets, from a combination of supervised and unsupervised training. Moreover, for ASR, it benefits from initializing DMLM from a pretrained LLM, and from a codebook derived from Whisper activations.",
    "metadata": {
      "arxiv_id": "2406.06582",
      "title": "Discrete Multimodal Transformers with a Pretrained Large Language Model for Mixed-Supervision Speech Processing",
      "summary": "Recent work on discrete speech tokenization has paved the way for models that can seamlessly perform multiple tasks across modalities, e.g., speech recognition, text to speech, speech to speech translation. Moreover, large language models (LLMs) pretrained from vast text corpora contain rich linguistic information that can improve accuracy in a variety of tasks. In this paper, we present a decoder-only Discrete Multimodal Language Model (DMLM), which can be flexibly applied to multiple tasks (ASR, T2S, S2TT, etc.) and modalities (text, speech, vision). We explore several critical aspects of discrete multi-modal models, including the loss function, weight initialization, mixed training supervision, and codebook. Our results show that DMLM benefits significantly, across multiple tasks and datasets, from a combination of supervised and unsupervised training. Moreover, for ASR, it benefits from initializing DMLM from a pretrained LLM, and from a codebook derived from Whisper activations.",
      "authors": [
        "Viet Anh Trinh",
        "Rosy Southwell",
        "Yiwen Guan",
        "Xinlu He",
        "Zhiyong Wang",
        "Jacob Whitehill"
      ],
      "published": "2024-06-04T20:08:25Z",
      "updated": "2024-06-25T17:44:00Z",
      "categories": [
        "cs.CL",
        "cs.LG",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2406.06582v2",
      "landing_url": "https://arxiv.org/abs/2406.06582v2",
      "doi": "https://doi.org/10.48550/arXiv.2406.06582"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The abstract explicitly studies discrete speech tokenization (codebook, discrete tokens) and a discrete multimodal language model applied to ASR and other speech-related tasks, aligning with the inclusion criteria and addressing discrete token representations rather than purely continuous audio features.",
      "evaluation": 5
    },
    "round-A_JuniorNano_reasoning": "The abstract explicitly studies discrete speech tokenization (codebook, discrete tokens) and a discrete multimodal language model applied to ASR and other speech-related tasks, aligning with the inclusion criteria and addressing discrete token representations rather than purely continuous audio features.",
    "round-A_JuniorNano_evaluation": 5,
    "round-A_JuniorMini_output": {
      "reasoning": "The study explicitly focuses on discrete speech tokenization, converting continuous audio signals into discrete token sequences for multiple speech tasks like ASR, text-to-speech, and speech-to-speech translation. It also explores methodological aspects such as loss functions, weight initialization, and codebooks related to discrete tokens and employs pretrained large language models, indicating a solid methodological contribution beyond engineering-only reports. The abstract is in English and suggests an academic paper rather than a tool/library report. Therefore, it satisfies all inclusion criteria and none of the exclusion criteria.",
      "evaluation": 5
    },
    "round-A_JuniorMini_reasoning": "The study explicitly focuses on discrete speech tokenization, converting continuous audio signals into discrete token sequences for multiple speech tasks like ASR, text-to-speech, and speech-to-speech translation. It also explores methodological aspects such as loss functions, weight initialization, and codebooks related to discrete tokens and employs pretrained large language models, indicating a solid methodological contribution beyond engineering-only reports. The abstract is in English and suggests an academic paper rather than a tool/library report. Therefore, it satisfies all inclusion criteria and none of the exclusion criteria.",
    "round-A_JuniorMini_evaluation": 5,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "include (junior:5)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "T2S-GPT: Dynamic Vector Quantization for Autoregressive Sign Language Production from Text",
    "abstract": "In this work, we propose a two-stage sign language production (SLP) paradigm that first encodes sign language sequences into discrete codes and then autoregressively generates sign language from text based on the learned codebook. However, existing vector quantization (VQ) methods are fixed-length encodings, overlooking the uneven information density in sign language, which leads to under-encoding of important regions and over-encoding of unimportant regions. To address this issue, we propose a novel dynamic vector quantization (DVA-VAE) model that can dynamically adjust the encoding length based on the information density in sign language to achieve accurate and compact encoding. Then, a GPT-like model learns to generate code sequences and their corresponding durations from spoken language text. Extensive experiments conducted on the PHOENIX14T dataset demonstrate the effectiveness of our proposed method. To promote sign language research, we propose a new large German sign language dataset, PHOENIX-News, which contains 486 hours of sign language videos, audio, and transcription texts.Experimental analysis on PHOENIX-News shows that the performance of our model can be further improved by increasing the size of the training data. Our project homepage is https://t2sgpt-demo.yinaoxiong.cn.",
    "metadata": {
      "arxiv_id": "2406.07119",
      "title": "T2S-GPT: Dynamic Vector Quantization for Autoregressive Sign Language Production from Text",
      "summary": "In this work, we propose a two-stage sign language production (SLP) paradigm that first encodes sign language sequences into discrete codes and then autoregressively generates sign language from text based on the learned codebook. However, existing vector quantization (VQ) methods are fixed-length encodings, overlooking the uneven information density in sign language, which leads to under-encoding of important regions and over-encoding of unimportant regions. To address this issue, we propose a novel dynamic vector quantization (DVA-VAE) model that can dynamically adjust the encoding length based on the information density in sign language to achieve accurate and compact encoding. Then, a GPT-like model learns to generate code sequences and their corresponding durations from spoken language text. Extensive experiments conducted on the PHOENIX14T dataset demonstrate the effectiveness of our proposed method. To promote sign language research, we propose a new large German sign language dataset, PHOENIX-News, which contains 486 hours of sign language videos, audio, and transcription texts.Experimental analysis on PHOENIX-News shows that the performance of our model can be further improved by increasing the size of the training data. Our project homepage is https://t2sgpt-demo.yinaoxiong.cn.",
      "authors": [
        "Aoxiong Yin",
        "Haoyuan Li",
        "Kai Shen",
        "Siliang Tang",
        "Yueting Zhuang"
      ],
      "published": "2024-06-11T10:06:53Z",
      "updated": "2024-06-11T10:06:53Z",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2406.07119v1",
      "landing_url": "https://arxiv.org/abs/2406.07119v1",
      "doi": "https://doi.org/10.48550/arXiv.2406.07119"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The study focuses on tokenizing sign language sequences into discrete codes, not discrete audio/speech tokens; the inclusion criteria require discrete audio/speech tokenization as a core object of study, which this does not provide.",
      "evaluation": 2
    },
    "round-A_JuniorNano_reasoning": "The study focuses on tokenizing sign language sequences into discrete codes, not discrete audio/speech tokens; the inclusion criteria require discrete audio/speech tokenization as a core object of study, which this does not provide.",
    "round-A_JuniorNano_evaluation": 2,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on dynamic vector quantization for sign language production from text, encoding sign language into discrete codes; however, it does not explicitly address discrete audio/speech tokenization nor involve converting continuous audio signals into discrete audio tokens as required by the inclusion criteria. Instead, it deals primarily with sign language video and text representations without discrete audio tokenization analysis. Therefore, it does not meet the core inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on dynamic vector quantization for sign language production from text, encoding sign language into discrete codes; however, it does not explicitly address discrete audio/speech tokenization nor involve converting continuous audio signals into discrete audio tokens as required by the inclusion criteria. Instead, it deals primarily with sign language video and text representations without discrete audio tokenization analysis. Therefore, it does not meet the core inclusion criteria.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:2)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:2)"
  },
  {
    "title": "DualVC 3: Leveraging Language Model Generated Pseudo Context for End-to-end Low Latency Streaming Voice Conversion",
    "abstract": "Streaming voice conversion has become increasingly popular for its potential in real-time applications. The recently proposed DualVC 2 has achieved robust and high-quality streaming voice conversion with a latency of about 180ms. Nonetheless, the recognition-synthesis framework hinders end-to-end optimization, and the instability of automatic speech recognition (ASR) model with short chunks makes it challenging to further reduce latency. To address these issues, we propose an end-to-end model, DualVC 3. With speaker-independent semantic tokens to guide the training of the content encoder, the dependency on ASR is removed and the model can operate under extremely small chunks, with cascading errors eliminated. A language model is trained on the content encoder output to produce pseudo context by iteratively predicting future frames, providing more contextual information for the decoder to improve conversion quality. Experimental results demonstrate that DualVC 3 achieves comparable performance to DualVC 2 in subjective and objective metrics, with a latency of only 50 ms.",
    "metadata": {
      "arxiv_id": "2406.07846",
      "title": "DualVC 3: Leveraging Language Model Generated Pseudo Context for End-to-end Low Latency Streaming Voice Conversion",
      "summary": "Streaming voice conversion has become increasingly popular for its potential in real-time applications. The recently proposed DualVC 2 has achieved robust and high-quality streaming voice conversion with a latency of about 180ms. Nonetheless, the recognition-synthesis framework hinders end-to-end optimization, and the instability of automatic speech recognition (ASR) model with short chunks makes it challenging to further reduce latency. To address these issues, we propose an end-to-end model, DualVC 3. With speaker-independent semantic tokens to guide the training of the content encoder, the dependency on ASR is removed and the model can operate under extremely small chunks, with cascading errors eliminated. A language model is trained on the content encoder output to produce pseudo context by iteratively predicting future frames, providing more contextual information for the decoder to improve conversion quality. Experimental results demonstrate that DualVC 3 achieves comparable performance to DualVC 2 in subjective and objective metrics, with a latency of only 50 ms.",
      "authors": [
        "Ziqian Ning",
        "Shuai Wang",
        "Pengcheng Zhu",
        "Zhichao Wang",
        "Jixun Yao",
        "Lei Xie",
        "Mengxiao Bi"
      ],
      "published": "2024-06-12T03:25:18Z",
      "updated": "2024-06-12T03:25:18Z",
      "categories": [
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2406.07846v1",
      "landing_url": "https://arxiv.org/abs/2406.07846v1",
      "doi": "https://doi.org/10.48550/arXiv.2406.07846"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The abstract mentions 'semantic tokens' and language-model generated context, but it does not clearly describe a discrete, finite-tokenization of audio (no explicit quantization or token vocabulary), so it does not meet the discrete-token criterion.",
      "evaluation": 2
    },
    "round-A_JuniorNano_reasoning": "The abstract mentions 'semantic tokens' and language-model generated context, but it does not clearly describe a discrete, finite-tokenization of audio (no explicit quantization or token vocabulary), so it does not meet the discrete-token criterion.",
    "round-A_JuniorNano_evaluation": 2,
    "round-A_JuniorMini_output": {
      "reasoning": "The study introduces DualVC 3, which leverages speaker-independent semantic tokens as a core part of its model, indicating the use of discrete tokenization of audio for voice conversion; it also removes dependency on traditional ASR and reports both methodological contribution and experimental validation. Therefore, it meets the inclusion criteria requiring discrete audio tokenization and excludes reliance solely on continuous features or unrelated tokenizations.",
      "evaluation": 5
    },
    "round-A_JuniorMini_reasoning": "The study introduces DualVC 3, which leverages speaker-independent semantic tokens as a core part of its model, indicating the use of discrete tokenization of audio for voice conversion; it also removes dependency on traditional ASR and reports both methodological contribution and experimental validation. Therefore, it meets the inclusion criteria requiring discrete audio tokenization and excludes reliance solely on continuous features or unrelated tokenizations.",
    "round-A_JuniorMini_evaluation": 5,
    "round-B_SeniorLead_output": {
      "reasoning": "I reviewed the abstract and the two junior reviewers' comments. The abstract mentions \"speaker-independent semantic tokens\" and a language model trained on the content-encoder outputs to predict future frames, which suggests a token-based representation, but it does not explicitly state that the audio is quantized into a sequence of discrete symbols from a finite vocabulary or describe an explicit tokenization/quantization method; given the inclusion criterion requires explicit discrete-token design/analysis, the item is ambiguous from the abstract alone. Therefore I rate this a 3 (uncertain): the work hints at discrete semantic tokens but lacks explicit confirmation of finite discrete-tokenization required for definite inclusion.",
      "evaluation": 3
    },
    "round-B_SeniorLead_reasoning": "I reviewed the abstract and the two junior reviewers' comments. The abstract mentions \"speaker-independent semantic tokens\" and a language model trained on the content-encoder outputs to predict future frames, which suggests a token-based representation, but it does not explicitly state that the audio is quantized into a sequence of discrete symbols from a finite vocabulary or describe an explicit tokenization/quantization method; given the inclusion criterion requires explicit discrete-token design/analysis, the item is ambiguous from the abstract alone. Therefore I rate this a 3 (uncertain): the work hints at discrete semantic tokens but lacks explicit confirmation of finite discrete-tokenization required for definite inclusion.",
    "round-B_SeniorLead_evaluation": 3,
    "final_verdict": "maybe (senior:3)",
    "review_skipped": false,
    "discard_reason": "review_needs_followup"
  },
  {
    "title": "VALL-E R: Robust and Efficient Zero-Shot Text-to-Speech Synthesis via Monotonic Alignment",
    "abstract": "With the help of discrete neural audio codecs, large language models (LLM) have increasingly been recognized as a promising methodology for zero-shot Text-to-Speech (TTS) synthesis. However, sampling based decoding strategies bring astonishing diversity to generation, but also pose robustness issues such as typos, omissions and repetition. In addition, the high sampling rate of audio also brings huge computational overhead to the inference process of autoregression. To address these issues, we propose VALL-E R, a robust and efficient zero-shot TTS system, building upon the foundation of VALL-E. Specifically, we introduce a phoneme monotonic alignment strategy to strengthen the connection between phonemes and acoustic sequence, ensuring a more precise alignment by constraining the acoustic tokens to match their associated phonemes. Furthermore, we employ a codec-merging approach to downsample the discrete codes in shallow quantization layer, thereby accelerating the decoding speed while preserving the high quality of speech output. Benefiting from these strategies, VALL-E R obtains controllablity over phonemes and demonstrates its strong robustness by approaching the WER of ground truth. In addition, it requires fewer autoregressive steps, with over 60% time reduction during inference. This research has the potential to be applied to meaningful projects, including the creation of speech for those affected by aphasia. Audio samples will be available at: https://aka.ms/valler.",
    "metadata": {
      "arxiv_id": "2406.07855",
      "title": "VALL-E R: Robust and Efficient Zero-Shot Text-to-Speech Synthesis via Monotonic Alignment",
      "summary": "With the help of discrete neural audio codecs, large language models (LLM) have increasingly been recognized as a promising methodology for zero-shot Text-to-Speech (TTS) synthesis. However, sampling based decoding strategies bring astonishing diversity to generation, but also pose robustness issues such as typos, omissions and repetition. In addition, the high sampling rate of audio also brings huge computational overhead to the inference process of autoregression. To address these issues, we propose VALL-E R, a robust and efficient zero-shot TTS system, building upon the foundation of VALL-E. Specifically, we introduce a phoneme monotonic alignment strategy to strengthen the connection between phonemes and acoustic sequence, ensuring a more precise alignment by constraining the acoustic tokens to match their associated phonemes. Furthermore, we employ a codec-merging approach to downsample the discrete codes in shallow quantization layer, thereby accelerating the decoding speed while preserving the high quality of speech output. Benefiting from these strategies, VALL-E R obtains controllablity over phonemes and demonstrates its strong robustness by approaching the WER of ground truth. In addition, it requires fewer autoregressive steps, with over 60% time reduction during inference. This research has the potential to be applied to meaningful projects, including the creation of speech for those affected by aphasia. Audio samples will be available at: https://aka.ms/valler.",
      "authors": [
        "Bing Han",
        "Long Zhou",
        "Shujie Liu",
        "Sanyuan Chen",
        "Lingwei Meng",
        "Yanming Qian",
        "Yanqing Liu",
        "Sheng Zhao",
        "Jinyu Li",
        "Furu Wei"
      ],
      "published": "2024-06-12T04:09:44Z",
      "updated": "2024-06-12T04:09:44Z",
      "categories": [
        "cs.CL",
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2406.07855v1",
      "landing_url": "https://arxiv.org/abs/2406.07855v1",
      "doi": "https://doi.org/10.48550/arXiv.2406.07855"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "It explicitly centers on discrete neural audio codecs and acoustic tokens (discrete tokenization) as core components of the TTS system.",
      "evaluation": 5
    },
    "round-A_JuniorNano_reasoning": "It explicitly centers on discrete neural audio codecs and acoustic tokens (discrete tokenization) as core components of the TTS system.",
    "round-A_JuniorNano_evaluation": 5,
    "round-A_JuniorMini_output": {
      "reasoning": "The study explicitly addresses discrete neural audio codecs and proposes a methodology involving phoneme monotonic alignment and codec-merging approaches to improve zero-shot text-to-speech synthesis, focusing on discrete tokenization of audio signals rather than continuous features or traditional codecs. It includes methodological contributions, robustness analysis, and efficiency improvements related to neural discrete audio token representations relevant for language-model-style token sequences, meeting all inclusion criteria with no exclusion criteria triggered.",
      "evaluation": 5
    },
    "round-A_JuniorMini_reasoning": "The study explicitly addresses discrete neural audio codecs and proposes a methodology involving phoneme monotonic alignment and codec-merging approaches to improve zero-shot text-to-speech synthesis, focusing on discrete tokenization of audio signals rather than continuous features or traditional codecs. It includes methodological contributions, robustness analysis, and efficiency improvements related to neural discrete audio token representations relevant for language-model-style token sequences, meeting all inclusion criteria with no exclusion criteria triggered.",
    "round-A_JuniorMini_evaluation": 5,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "include (junior:5)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Codecfake: An Initial Dataset for Detecting LLM-based Deepfake Audio",
    "abstract": "With the proliferation of Large Language Model (LLM) based deepfake audio, there is an urgent need for effective detection methods. Previous deepfake audio generation methods typically involve a multi-step generation process, with the final step using a vocoder to predict the waveform from handcrafted features. However, LLM-based audio is directly generated from discrete neural codecs in an end-to-end generation process, skipping the final step of vocoder processing. This poses a significant challenge for current audio deepfake detection (ADD) models based on vocoder artifacts. To effectively detect LLM-based deepfake audio, we focus on the core of the generation process, the conversion from neural codec to waveform. We propose Codecfake dataset, which is generated by seven representative neural codec methods. Experiment results show that codec-trained ADD models exhibit a 41.406% reduction in average equal error rate compared to vocoder-trained ADD models on the Codecfake test set.",
    "metadata": {
      "arxiv_id": "2406.08112",
      "title": "Codecfake: An Initial Dataset for Detecting LLM-based Deepfake Audio",
      "summary": "With the proliferation of Large Language Model (LLM) based deepfake audio, there is an urgent need for effective detection methods. Previous deepfake audio generation methods typically involve a multi-step generation process, with the final step using a vocoder to predict the waveform from handcrafted features. However, LLM-based audio is directly generated from discrete neural codecs in an end-to-end generation process, skipping the final step of vocoder processing. This poses a significant challenge for current audio deepfake detection (ADD) models based on vocoder artifacts. To effectively detect LLM-based deepfake audio, we focus on the core of the generation process, the conversion from neural codec to waveform. We propose Codecfake dataset, which is generated by seven representative neural codec methods. Experiment results show that codec-trained ADD models exhibit a 41.406% reduction in average equal error rate compared to vocoder-trained ADD models on the Codecfake test set.",
      "authors": [
        "Yi Lu",
        "Yuankun Xie",
        "Ruibo Fu",
        "Zhengqi Wen",
        "Jianhua Tao",
        "Zhiyong Wang",
        "Xin Qi",
        "Xuefei Liu",
        "Yongwei Li",
        "Yukun Liu",
        "Xiaopeng Wang",
        "Shuchen Shi"
      ],
      "published": "2024-06-12T11:47:23Z",
      "updated": "2024-06-12T11:47:23Z",
      "categories": [
        "cs.SD",
        "cs.AI",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2406.08112v1",
      "landing_url": "https://arxiv.org/abs/2406.08112v1",
      "doi": "https://doi.org/10.48550/arXiv.2406.08112"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The work focuses on detecting LLM-based deepfake audio and uses discrete neural codecs, but it does not explicitly study or evaluate discrete audio tokenization as a core object of study for speech reconstruction/generation.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The work focuses on detecting LLM-based deepfake audio and uses discrete neural codecs, but it does not explicitly study or evaluate discrete audio tokenization as a core object of study for speech reconstruction/generation.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study presents the Codecfake dataset focusing on neural codec-based deepfake audio generation and detection; it centers on the conversion from discrete neural codec representations to audio waveforms, implying an analysis of discrete tokenization as part of the generation process. The work contrasts neural codec methods with vocoder-based methods, indicating methodological contribution and experimental validation relevant to discrete audio tokens. Hence, it meets the inclusion criteria of analyzing discrete audio tokenization and involves neural encoding-based discrete token representations rather than traditional signal codecs, making it relevant for inclusion.",
      "evaluation": 5
    },
    "round-A_JuniorMini_reasoning": "The study presents the Codecfake dataset focusing on neural codec-based deepfake audio generation and detection; it centers on the conversion from discrete neural codec representations to audio waveforms, implying an analysis of discrete tokenization as part of the generation process. The work contrasts neural codec methods with vocoder-based methods, indicating methodological contribution and experimental validation relevant to discrete audio tokens. Hence, it meets the inclusion criteria of analyzing discrete audio tokenization and involves neural encoding-based discrete token representations rather than traditional signal codecs, making it relevant for inclusion.",
    "round-A_JuniorMini_evaluation": 5,
    "round-B_SeniorLead_output": {
      "reasoning": "Junior reviewers' comments: JuniorNano says the paper does not explicitly study or evaluate discrete audio tokenization; JuniorMini argues it does because it focuses on neural codec-based generation and codec-to-waveform conversion. My assessment: the abstract clearly states the work targets LLM-generated audio produced from discrete neural codecs and builds a dataset from seven neural codec methods, focusing on the codec-to-waveform conversion, which implies the involvement of discrete codec representations—but it does not explicitly state that it proposes, analyzes, or evaluates discrete tokenization/quantization as a core methodological contribution. Score: 3 — The study uses discrete neural codecs and examines codec-related artifacts, but the abstract lacks an explicit focus on tokenization design or analysis, so inclusion is uncertain without the full text.",
      "evaluation": 3
    },
    "round-B_SeniorLead_reasoning": "Junior reviewers' comments: JuniorNano says the paper does not explicitly study or evaluate discrete audio tokenization; JuniorMini argues it does because it focuses on neural codec-based generation and codec-to-waveform conversion. My assessment: the abstract clearly states the work targets LLM-generated audio produced from discrete neural codecs and builds a dataset from seven neural codec methods, focusing on the codec-to-waveform conversion, which implies the involvement of discrete codec representations—but it does not explicitly state that it proposes, analyzes, or evaluates discrete tokenization/quantization as a core methodological contribution. Score: 3 — The study uses discrete neural codecs and examines codec-related artifacts, but the abstract lacks an explicit focus on tokenization design or analysis, so inclusion is uncertain without the full text.",
    "round-B_SeniorLead_evaluation": 3,
    "final_verdict": "maybe (senior:3)",
    "review_skipped": false,
    "discard_reason": "review_needs_followup"
  },
  {
    "title": "CoLM-DSR: Leveraging Neural Codec Language Modeling for Multi-Modal Dysarthric Speech Reconstruction",
    "abstract": "Dysarthric speech reconstruction (DSR) aims to transform dysarthric speech into normal speech. It still suffers from low speaker similarity and poor prosody naturalness. In this paper, we propose a multi-modal DSR model by leveraging neural codec language modeling to improve the reconstruction results, especially for the speaker similarity and prosody naturalness. Our proposed model consists of: (i) a multi-modal content encoder to extract robust phoneme embeddings from dysarthric speech with auxiliary visual inputs; (ii) a speaker codec encoder to extract and normalize the speaker-aware codecs from the dysarthric speech, in order to provide original timbre and normal prosody; (iii) a codec language model based speech decoder to reconstruct the speech based on the extracted phoneme embeddings and normalized codecs. Evaluations on the commonly used UASpeech corpus show that our proposed model can achieve significant improvements in terms of speaker similarity and prosody naturalness.",
    "metadata": {
      "arxiv_id": "2406.08336",
      "title": "CoLM-DSR: Leveraging Neural Codec Language Modeling for Multi-Modal Dysarthric Speech Reconstruction",
      "summary": "Dysarthric speech reconstruction (DSR) aims to transform dysarthric speech into normal speech. It still suffers from low speaker similarity and poor prosody naturalness. In this paper, we propose a multi-modal DSR model by leveraging neural codec language modeling to improve the reconstruction results, especially for the speaker similarity and prosody naturalness. Our proposed model consists of: (i) a multi-modal content encoder to extract robust phoneme embeddings from dysarthric speech with auxiliary visual inputs; (ii) a speaker codec encoder to extract and normalize the speaker-aware codecs from the dysarthric speech, in order to provide original timbre and normal prosody; (iii) a codec language model based speech decoder to reconstruct the speech based on the extracted phoneme embeddings and normalized codecs. Evaluations on the commonly used UASpeech corpus show that our proposed model can achieve significant improvements in terms of speaker similarity and prosody naturalness.",
      "authors": [
        "Xueyuan Chen",
        "Dongchao Yang",
        "Dingdong Wang",
        "Xixin Wu",
        "Zhiyong Wu",
        "Helen Meng"
      ],
      "published": "2024-06-12T15:42:21Z",
      "updated": "2024-06-24T06:09:42Z",
      "categories": [
        "cs.SD",
        "cs.CV",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2406.08336v2",
      "landing_url": "https://arxiv.org/abs/2406.08336v2",
      "doi": "https://doi.org/10.48550/arXiv.2406.08336"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The abstract mentions a 'codec language model' and 'codecs' in the context of dysarthric speech reconstruction, which could imply discrete tokenization, but it does not explicitly state that discrete audio tokenization is the core object of study, and availability of the full text for evaluation is not indicated.",
      "evaluation": 3
    },
    "round-A_JuniorNano_reasoning": "The abstract mentions a 'codec language model' and 'codecs' in the context of dysarthric speech reconstruction, which could imply discrete tokenization, but it does not explicitly state that discrete audio tokenization is the core object of study, and availability of the full text for evaluation is not indicated.",
    "round-A_JuniorNano_evaluation": 3,
    "round-A_JuniorMini_output": {
      "reasoning": "The study proposes a neural codec language modeling approach for dysarthric speech reconstruction, leveraging discrete codec tokens extracted from speech and auxiliary inputs to reconstruct normal speech, indicating a core focus on discrete audio tokenization for speech reconstruction tasks; thus, it meets all inclusion criteria without falling under any exclusion criteria.",
      "evaluation": 5
    },
    "round-A_JuniorMini_reasoning": "The study proposes a neural codec language modeling approach for dysarthric speech reconstruction, leveraging discrete codec tokens extracted from speech and auxiliary inputs to reconstruct normal speech, indicating a core focus on discrete audio tokenization for speech reconstruction tasks; thus, it meets all inclusion criteria without falling under any exclusion criteria.",
    "round-A_JuniorMini_evaluation": 5,
    "round-B_SeniorLead_output": {
      "reasoning": "Junior reviewers: JuniorNano flagged uncertainty because the abstract does not explicitly state 'discrete tokens', while JuniorMini judged it meets inclusion. My assessment: the abstract repeatedly refers to 'neural codec language modeling', a 'speaker codec encoder' and a 'codec language model based speech decoder', which strongly implies the use of neural codec (quantized/discrete) tokens and sequence modeling of those codecs as a core component for dysarthric speech reconstruction — aligning with the inclusion criterion that discrete audio tokenization is the object of study. It does not read like traditional signal-processing codecs nor only continuous-feature usage. However, the abstract does not literally say 'discrete tokens' or 'quantization/codebook' and the availability of the full text for academic evaluation is not confirmed, introducing some uncertainty. One-sentence explanation: I give a 4 because the work very likely centers on discrete neural codec tokens for language-model-style speech reconstruction and thus meets the content criterion, but the lack of an explicit mention of discrete tokenization and unclear full-text availability prevent an absolute include.",
      "evaluation": 4
    },
    "round-B_SeniorLead_reasoning": "Junior reviewers: JuniorNano flagged uncertainty because the abstract does not explicitly state 'discrete tokens', while JuniorMini judged it meets inclusion. My assessment: the abstract repeatedly refers to 'neural codec language modeling', a 'speaker codec encoder' and a 'codec language model based speech decoder', which strongly implies the use of neural codec (quantized/discrete) tokens and sequence modeling of those codecs as a core component for dysarthric speech reconstruction — aligning with the inclusion criterion that discrete audio tokenization is the object of study. It does not read like traditional signal-processing codecs nor only continuous-feature usage. However, the abstract does not literally say 'discrete tokens' or 'quantization/codebook' and the availability of the full text for academic evaluation is not confirmed, introducing some uncertainty. One-sentence explanation: I give a 4 because the work very likely centers on discrete neural codec tokens for language-model-style speech reconstruction and thus meets the content criterion, but the lack of an explicit mention of discrete tokenization and unclear full-text availability prevent an absolute include.",
    "round-B_SeniorLead_evaluation": 4,
    "final_verdict": "include (senior:4)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Diffusion Synthesizer for Efficient Multilingual Speech to Speech Translation",
    "abstract": "We introduce DiffuseST, a low-latency, direct speech-to-speech translation system capable of preserving the input speaker's voice zero-shot while translating from multiple source languages into English. We experiment with the synthesizer component of the architecture, comparing a Tacotron-based synthesizer to a novel diffusion-based synthesizer. We find the diffusion-based synthesizer to improve MOS and PESQ audio quality metrics by 23\\% each and speaker similarity by 5\\% while maintaining comparable BLEU scores. Despite having more than double the parameter count, the diffusion synthesizer has lower latency, allowing the entire model to run more than 5$\\times$ faster than real-time.",
    "metadata": {
      "arxiv_id": "2406.10223",
      "title": "Diffusion Synthesizer for Efficient Multilingual Speech to Speech Translation",
      "summary": "We introduce DiffuseST, a low-latency, direct speech-to-speech translation system capable of preserving the input speaker's voice zero-shot while translating from multiple source languages into English. We experiment with the synthesizer component of the architecture, comparing a Tacotron-based synthesizer to a novel diffusion-based synthesizer. We find the diffusion-based synthesizer to improve MOS and PESQ audio quality metrics by 23\\% each and speaker similarity by 5\\% while maintaining comparable BLEU scores. Despite having more than double the parameter count, the diffusion synthesizer has lower latency, allowing the entire model to run more than 5$\\times$ faster than real-time.",
      "authors": [
        "Nameer Hirschkind",
        "Xiao Yu",
        "Mahesh Kumar Nandwana",
        "Joseph Liu",
        "Eloi DuBois",
        "Dao Le",
        "Nicolas Thiebaut",
        "Colin Sinclair",
        "Kyle Spence",
        "Charles Shang",
        "Zoe Abrams",
        "Morgan McGuire"
      ],
      "published": "2024-06-14T17:55:55Z",
      "updated": "2024-06-14T17:55:55Z",
      "categories": [
        "cs.LG",
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2406.10223v1",
      "landing_url": "https://arxiv.org/abs/2406.10223v1",
      "doi": "https://doi.org/10.48550/arXiv.2406.10223"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "Abstract discusses diffusion-based speech synthesizer and speech-to-speech translation, but does not analyze or propose discrete audio tokenization into a finite vocabulary; thus it does not meet the tokenization criterion.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "Abstract discusses diffusion-based speech synthesizer and speech-to-speech translation, but does not analyze or propose discrete audio tokenization into a finite vocabulary; thus it does not meet the tokenization criterion.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The abstract describes a speech-to-speech translation system comparing a Tacotron-based synthesizer with a diffusion-based synthesizer focusing on audio quality, speaker similarity, latency, and translation quality metrics. However, it does not mention discrete audio/speech tokenization or conversion of continuous audio signals into discrete symbols, which is the core inclusion criterion. Instead, it appears to focus on continuous neural synthesis techniques without discussing discrete tokenization or quantization methods explicitly. Therefore, the study does not meet the inclusion criteria related to discrete audio tokenization.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The abstract describes a speech-to-speech translation system comparing a Tacotron-based synthesizer with a diffusion-based synthesizer focusing on audio quality, speaker similarity, latency, and translation quality metrics. However, it does not mention discrete audio/speech tokenization or conversion of continuous audio signals into discrete symbols, which is the core inclusion criterion. Instead, it appears to focus on continuous neural synthesis techniques without discussing discrete tokenization or quantization methods explicitly. Therefore, the study does not meet the inclusion criteria related to discrete audio tokenization.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "ExHuBERT: Enhancing HuBERT Through Block Extension and Fine-Tuning on 37 Emotion Datasets",
    "abstract": "Foundation models have shown great promise in speech emotion recognition (SER) by leveraging their pre-trained representations to capture emotion patterns in speech signals. To further enhance SER performance across various languages and domains, we propose a novel twofold approach. First, we gather EmoSet++, a comprehensive multi-lingual, multi-cultural speech emotion corpus with 37 datasets, 150,907 samples, and a total duration of 119.5 hours. Second, we introduce ExHuBERT, an enhanced version of HuBERT achieved by backbone extension and fine-tuning on EmoSet++. We duplicate each encoder layer and its weights, then freeze the first duplicate, integrating an extra zero-initialized linear layer and skip connections to preserve functionality and ensure its adaptability for subsequent fine-tuning. Our evaluation on unseen datasets shows the efficacy of ExHuBERT, setting a new benchmark for various SER tasks. Model and details on EmoSet++: https://huggingface.co/amiriparian/ExHuBERT.",
    "metadata": {
      "arxiv_id": "2406.10275",
      "title": "ExHuBERT: Enhancing HuBERT Through Block Extension and Fine-Tuning on 37 Emotion Datasets",
      "summary": "Foundation models have shown great promise in speech emotion recognition (SER) by leveraging their pre-trained representations to capture emotion patterns in speech signals. To further enhance SER performance across various languages and domains, we propose a novel twofold approach. First, we gather EmoSet++, a comprehensive multi-lingual, multi-cultural speech emotion corpus with 37 datasets, 150,907 samples, and a total duration of 119.5 hours. Second, we introduce ExHuBERT, an enhanced version of HuBERT achieved by backbone extension and fine-tuning on EmoSet++. We duplicate each encoder layer and its weights, then freeze the first duplicate, integrating an extra zero-initialized linear layer and skip connections to preserve functionality and ensure its adaptability for subsequent fine-tuning. Our evaluation on unseen datasets shows the efficacy of ExHuBERT, setting a new benchmark for various SER tasks. Model and details on EmoSet++: https://huggingface.co/amiriparian/ExHuBERT.",
      "authors": [
        "Shahin Amiriparian",
        "Filip Packań",
        "Maurice Gerczuk",
        "Björn W. Schuller"
      ],
      "published": "2024-06-11T21:30:15Z",
      "updated": "2024-06-11T21:30:15Z",
      "categories": [
        "cs.CL"
      ],
      "pdf_url": "https://arxiv.org/pdf/2406.10275v1",
      "landing_url": "https://arxiv.org/abs/2406.10275v1",
      "doi": "https://doi.org/10.48550/arXiv.2406.10275"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The abstract describes improving a speech emotion recognition model (ExHuBERT) using EmoSet++ but does not explicitly analyze or evaluate discrete audio tokenization as a core object of study (no discussion of converting continuous audio to discrete token sequences).",
      "evaluation": 2
    },
    "round-A_JuniorNano_reasoning": "The abstract describes improving a speech emotion recognition model (ExHuBERT) using EmoSet++ but does not explicitly analyze or evaluate discrete audio tokenization as a core object of study (no discussion of converting continuous audio to discrete token sequences).",
    "round-A_JuniorNano_evaluation": 2,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on enhancing a speech emotion recognition model (ExHuBERT) through backbone extension and fine-tuning on a large multi-lingual speech emotion corpus. However, there is no explicit mention of proposing, analyzing, or evaluating discrete audio/speech tokenization methods as a core study objective, nor is there discussion of converting continuous audio signals into discrete token sequences. The work seems centered on improving emotion recognition performance leveraging pre-trained representations rather than discrete tokenization per se, thus it does not satisfy the inclusion criteria fully.",
      "evaluation": 2
    },
    "round-A_JuniorMini_reasoning": "The study focuses on enhancing a speech emotion recognition model (ExHuBERT) through backbone extension and fine-tuning on a large multi-lingual speech emotion corpus. However, there is no explicit mention of proposing, analyzing, or evaluating discrete audio/speech tokenization methods as a core study objective, nor is there discussion of converting continuous audio signals into discrete token sequences. The work seems centered on improving emotion recognition performance leveraging pre-trained representations rather than discrete tokenization per se, thus it does not satisfy the inclusion criteria fully.",
    "round-A_JuniorMini_evaluation": 2,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:2)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:2)"
  },
  {
    "title": "How Should We Extract Discrete Audio Tokens from Self-Supervised Models?",
    "abstract": "Discrete audio tokens have recently gained attention for their potential to bridge the gap between audio and language processing. Ideal audio tokens must preserve content, paralinguistic elements, speaker identity, and many other audio details. Current audio tokenization methods fall into two categories: Semantic tokens, acquired through quantization of Self-Supervised Learning (SSL) models, and Neural compression-based tokens (codecs). Although previous studies have benchmarked codec models to identify optimal configurations, the ideal setup for quantizing pretrained SSL models remains unclear. This paper explores the optimal configuration of semantic tokens across discriminative and generative tasks. We propose a scalable solution to train a universal vocoder across multiple SSL layers. Furthermore, an attention mechanism is employed to identify task-specific influential layers, enhancing the adaptability and performance of semantic tokens in diverse audio applications.",
    "metadata": {
      "arxiv_id": "2406.10735",
      "title": "How Should We Extract Discrete Audio Tokens from Self-Supervised Models?",
      "summary": "Discrete audio tokens have recently gained attention for their potential to bridge the gap between audio and language processing. Ideal audio tokens must preserve content, paralinguistic elements, speaker identity, and many other audio details. Current audio tokenization methods fall into two categories: Semantic tokens, acquired through quantization of Self-Supervised Learning (SSL) models, and Neural compression-based tokens (codecs). Although previous studies have benchmarked codec models to identify optimal configurations, the ideal setup for quantizing pretrained SSL models remains unclear. This paper explores the optimal configuration of semantic tokens across discriminative and generative tasks. We propose a scalable solution to train a universal vocoder across multiple SSL layers. Furthermore, an attention mechanism is employed to identify task-specific influential layers, enhancing the adaptability and performance of semantic tokens in diverse audio applications.",
      "authors": [
        "Pooneh Mousavi",
        "Jarod Duret",
        "Salah Zaiem",
        "Luca Della Libera",
        "Artem Ploujnikov",
        "Cem Subakan",
        "Mirco Ravanelli"
      ],
      "published": "2024-06-15T20:43:07Z",
      "updated": "2024-06-15T20:43:07Z",
      "categories": [
        "cs.SD",
        "cs.AI",
        "cs.CL",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2406.10735v1",
      "landing_url": "https://arxiv.org/abs/2406.10735v1",
      "doi": "https://doi.org/10.48550/arXiv.2406.10735"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The title/abstract clearly centers on extracting discrete audio tokens by quantizing self-supervised models, a direct study of discrete tokenization for audio reconstruction/generation, and it discusses semantic tokens and token-based vocoding rather than conventional continuous features or non-discrete approaches; this aligns with inclusion criteria for discrete audio tokenization in speech/audio tasks, assuming full English text is accessible for evaluation.",
      "evaluation": 5
    },
    "round-A_JuniorNano_reasoning": "The title/abstract clearly centers on extracting discrete audio tokens by quantizing self-supervised models, a direct study of discrete tokenization for audio reconstruction/generation, and it discusses semantic tokens and token-based vocoding rather than conventional continuous features or non-discrete approaches; this aligns with inclusion criteria for discrete audio tokenization in speech/audio tasks, assuming full English text is accessible for evaluation.",
    "round-A_JuniorNano_evaluation": 5,
    "round-A_JuniorMini_output": {
      "reasoning": "The study explicitly focuses on discrete audio tokens derived from self-supervised learning models and evaluates optimal configurations for semantic tokens, which aligns directly with the inclusion criteria of analyzing discrete audio tokenization for audio reconstruction and understanding tasks; it also involves modern quantization-based token representations rather than traditional codecs, and provides methodological analysis and experimental validation.",
      "evaluation": 5
    },
    "round-A_JuniorMini_reasoning": "The study explicitly focuses on discrete audio tokens derived from self-supervised learning models and evaluates optimal configurations for semantic tokens, which aligns directly with the inclusion criteria of analyzing discrete audio tokenization for audio reconstruction and understanding tasks; it also involves modern quantization-based token representations rather than traditional codecs, and provides methodological analysis and experimental validation.",
    "round-A_JuniorMini_evaluation": 5,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "include (junior:5)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Occam's Razor for Self Supervised Learning: What is Sufficient to Learn Good Representations?",
    "abstract": "Deep Learning is often depicted as a trio of data-architecture-loss. Yet, recent Self Supervised Learning (SSL) solutions have introduced numerous additional design choices, e.g., a projector network, positive views, or teacher-student networks. These additions pose two challenges. First, they limit the impact of theoretical studies that often fail to incorporate all those intertwined designs. Second, they slow-down the deployment of SSL methods to new domains as numerous hyper-parameters need to be carefully tuned. In this study, we bring forward the surprising observation that--at least for pretraining datasets of up to a few hundred thousands samples--the additional designs introduced by SSL do not contribute to the quality of the learned representations. That finding not only provides legitimacy to existing theoretical studies, but also simplifies the practitioner's path to SSL deployment in numerous small and medium scale settings. Our finding answers a long-lasting question: the often-experienced sensitivity to training settings and hyper-parameters encountered in SSL come from their design, rather than the absence of supervised guidance.",
    "metadata": {
      "arxiv_id": "2406.10743",
      "title": "Occam's Razor for Self Supervised Learning: What is Sufficient to Learn Good Representations?",
      "summary": "Deep Learning is often depicted as a trio of data-architecture-loss. Yet, recent Self Supervised Learning (SSL) solutions have introduced numerous additional design choices, e.g., a projector network, positive views, or teacher-student networks. These additions pose two challenges. First, they limit the impact of theoretical studies that often fail to incorporate all those intertwined designs. Second, they slow-down the deployment of SSL methods to new domains as numerous hyper-parameters need to be carefully tuned. In this study, we bring forward the surprising observation that--at least for pretraining datasets of up to a few hundred thousands samples--the additional designs introduced by SSL do not contribute to the quality of the learned representations. That finding not only provides legitimacy to existing theoretical studies, but also simplifies the practitioner's path to SSL deployment in numerous small and medium scale settings. Our finding answers a long-lasting question: the often-experienced sensitivity to training settings and hyper-parameters encountered in SSL come from their design, rather than the absence of supervised guidance.",
      "authors": [
        "Mark Ibrahim",
        "David Klindt",
        "Randall Balestriero"
      ],
      "published": "2024-06-15T21:42:15Z",
      "updated": "2024-06-15T21:42:15Z",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2406.10743v1",
      "landing_url": "https://arxiv.org/abs/2406.10743v1",
      "doi": "https://doi.org/10.48550/arXiv.2406.10743"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The paper focuses on self-supervised learning design choices and architectures rather than discrete audio/speech tokenization; there is no evaluation or proposal of generating/using discrete audio tokens, so it fails inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The paper focuses on self-supervised learning design choices and architectures rather than discrete audio/speech tokenization; there is no evaluation or proposal of generating/using discrete audio tokens, so it fails inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on self-supervised learning methods for representation learning but does not mention discrete audio/speech tokenization, discrete symbol sequences, or quantization-based token designs; it also does not appear to analyze or propose any specific discrete audio tokenization methods, nor does it focus on speech or audio tokenization relevant to language modeling, which are required by the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on self-supervised learning methods for representation learning but does not mention discrete audio/speech tokenization, discrete symbol sequences, or quantization-based token designs; it also does not appear to analyze or propose any specific discrete audio tokenization methods, nor does it focus on speech or audio tokenization relevant to language modeling, which are required by the inclusion criteria.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "NAST: Noise Aware Speech Tokenization for Speech Language Models",
    "abstract": "Speech tokenization is the task of representing speech signals as a sequence of discrete units. Such representations can be later used for various downstream tasks including automatic speech recognition, text-to-speech, etc. More relevant to this study, such representation serves as the basis of Speech Language Models. In this work, we tackle the task of speech tokenization under the noisy setup and present NAST: Noise Aware Speech Tokenization for Speech Language Models. NAST is composed of three main components: (i) a predictor; (ii) a residual encoder; and (iii) a decoder. We evaluate the efficiency of NAST considering several spoken language modeling tasks and show that NAST is superior to the evaluated baselines across all setups. Lastly, we analyze NAST and show its disentanglement properties and robustness to signal variations in the form of noise, reverberation, pitch-shift, and time-stretch. Code and pre-trained models are available at https://github.com/ShovalMessica/NAST.",
    "metadata": {
      "arxiv_id": "2406.11037",
      "title": "NAST: Noise Aware Speech Tokenization for Speech Language Models",
      "summary": "Speech tokenization is the task of representing speech signals as a sequence of discrete units. Such representations can be later used for various downstream tasks including automatic speech recognition, text-to-speech, etc. More relevant to this study, such representation serves as the basis of Speech Language Models. In this work, we tackle the task of speech tokenization under the noisy setup and present NAST: Noise Aware Speech Tokenization for Speech Language Models. NAST is composed of three main components: (i) a predictor; (ii) a residual encoder; and (iii) a decoder. We evaluate the efficiency of NAST considering several spoken language modeling tasks and show that NAST is superior to the evaluated baselines across all setups. Lastly, we analyze NAST and show its disentanglement properties and robustness to signal variations in the form of noise, reverberation, pitch-shift, and time-stretch. Code and pre-trained models are available at https://github.com/ShovalMessica/NAST.",
      "authors": [
        "Shoval Messica",
        "Yossi Adi"
      ],
      "published": "2024-06-16T18:20:45Z",
      "updated": "2024-06-16T18:20:45Z",
      "categories": [
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2406.11037v1",
      "landing_url": "https://arxiv.org/abs/2406.11037v1",
      "doi": "https://doi.org/10.48550/arXiv.2406.11037"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The work centers on transforming speech into discrete tokens (tokenization) for speech language models and evaluates its robustness to noise and other signal variations, aligning with the discrete-tokenization inclusion criterion.",
      "evaluation": 5
    },
    "round-A_JuniorNano_reasoning": "The work centers on transforming speech into discrete tokens (tokenization) for speech language models and evaluates its robustness to noise and other signal variations, aligning with the discrete-tokenization inclusion criterion.",
    "round-A_JuniorNano_evaluation": 5,
    "round-A_JuniorMini_output": {
      "reasoning": "The study explicitly focuses on discrete audio/speech tokenization, proposing and evaluating a novel method (NAST) for noise-aware speech tokenization aimed at speech language models, with systematic analysis and experimental validation demonstrating advantages over baselines; thus, it satisfies all inclusion criteria and none of the exclusion criteria.",
      "evaluation": 5
    },
    "round-A_JuniorMini_reasoning": "The study explicitly focuses on discrete audio/speech tokenization, proposing and evaluating a novel method (NAST) for noise-aware speech tokenization aimed at speech language models, with systematic analysis and experimental validation demonstrating advantages over baselines; thus, it satisfies all inclusion criteria and none of the exclusion criteria.",
    "round-A_JuniorMini_evaluation": 5,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "include (junior:5)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "SMRU: Split-and-Merge Recurrent-based UNet for Acoustic Echo Cancellation and Noise Suppression",
    "abstract": "The proliferation of deep neural networks has spawned the rapid development of acoustic echo cancellation and noise suppression, and plenty of prior arts have been proposed, which yield promising performance. Nevertheless, they rarely consider the deployment generality in different processing scenarios, such as edge devices, and cloud processing. To this end, this paper proposes a general model, termed SMRU, to cover different application scenarios. The novelty lies in two-fold. First, a multi-scale band split layer and band merge layer are proposed to effectively fuse local frequency bands for lower complexity modeling. Besides, by simulating the multi-resolution feature modeling characteristic of the classical UNet structure, a novel recurrent-dominated UNet is devised. It consists of multiple variable frame rate blocks, each of which involves the causal time down-/up-sampling layer with varying compression ratios and the dual-path structure for inter- and intra-band modeling. The model is configured from 50 M/s to 6.8 G/s in terms of MACs, and the experimental results show that the proposed approach yields competitive or even better performance over existing baselines, and has the full potential to adapt to more general scenarios with varying complexity requirements.",
    "metadata": {
      "arxiv_id": "2406.11175",
      "title": "SMRU: Split-and-Merge Recurrent-based UNet for Acoustic Echo Cancellation and Noise Suppression",
      "summary": "The proliferation of deep neural networks has spawned the rapid development of acoustic echo cancellation and noise suppression, and plenty of prior arts have been proposed, which yield promising performance. Nevertheless, they rarely consider the deployment generality in different processing scenarios, such as edge devices, and cloud processing. To this end, this paper proposes a general model, termed SMRU, to cover different application scenarios. The novelty lies in two-fold. First, a multi-scale band split layer and band merge layer are proposed to effectively fuse local frequency bands for lower complexity modeling. Besides, by simulating the multi-resolution feature modeling characteristic of the classical UNet structure, a novel recurrent-dominated UNet is devised. It consists of multiple variable frame rate blocks, each of which involves the causal time down-/up-sampling layer with varying compression ratios and the dual-path structure for inter- and intra-band modeling. The model is configured from 50 M/s to 6.8 G/s in terms of MACs, and the experimental results show that the proposed approach yields competitive or even better performance over existing baselines, and has the full potential to adapt to more general scenarios with varying complexity requirements.",
      "authors": [
        "Zhihang Sun",
        "Andong Li",
        "Rilin Chen",
        "Hao Zhang",
        "Meng Yu",
        "Yi Zhou",
        "Dong Yu"
      ],
      "published": "2024-06-17T03:28:08Z",
      "updated": "2025-01-24T12:41:06Z",
      "categories": [
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2406.11175v2",
      "landing_url": "https://arxiv.org/abs/2406.11175v2",
      "doi": "https://doi.org/10.1109/SLT61566.2024.10832279"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The abstract focuses on AEC/NS with continuous signal processing and novel UNet-based architectures, and does not propose or analyze discrete audio tokenization or token-based representations.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The abstract focuses on AEC/NS with continuous signal processing and novel UNet-based architectures, and does not propose or analyze discrete audio tokenization or token-based representations.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study proposes a novel neural network model for acoustic echo cancellation and noise suppression but does not mention any discrete audio/speech tokenization or the conversion of continuous audio signals into discrete token sequences, which is required by the inclusion criteria. The focus is on signal processing via neural networks for noise and echo suppression rather than discrete token design or analysis suitable for audio reconstruction or generation tasks.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study proposes a novel neural network model for acoustic echo cancellation and noise suppression but does not mention any discrete audio/speech tokenization or the conversion of continuous audio signals into discrete token sequences, which is required by the inclusion criteria. The focus is on signal processing via neural networks for noise and echo suppression rather than discrete token design or analysis suitable for audio reconstruction or generation tasks.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Autoregressive Image Generation without Vector Quantization",
    "abstract": "Conventional wisdom holds that autoregressive models for image generation are typically accompanied by vector-quantized tokens. We observe that while a discrete-valued space can facilitate representing a categorical distribution, it is not a necessity for autoregressive modeling. In this work, we propose to model the per-token probability distribution using a diffusion procedure, which allows us to apply autoregressive models in a continuous-valued space. Rather than using categorical cross-entropy loss, we define a Diffusion Loss function to model the per-token probability. This approach eliminates the need for discrete-valued tokenizers. We evaluate its effectiveness across a wide range of cases, including standard autoregressive models and generalized masked autoregressive (MAR) variants. By removing vector quantization, our image generator achieves strong results while enjoying the speed advantage of sequence modeling. We hope this work will motivate the use of autoregressive generation in other continuous-valued domains and applications. Code is available at: https://github.com/LTH14/mar.",
    "metadata": {
      "arxiv_id": "2406.11838",
      "title": "Autoregressive Image Generation without Vector Quantization",
      "summary": "Conventional wisdom holds that autoregressive models for image generation are typically accompanied by vector-quantized tokens. We observe that while a discrete-valued space can facilitate representing a categorical distribution, it is not a necessity for autoregressive modeling. In this work, we propose to model the per-token probability distribution using a diffusion procedure, which allows us to apply autoregressive models in a continuous-valued space. Rather than using categorical cross-entropy loss, we define a Diffusion Loss function to model the per-token probability. This approach eliminates the need for discrete-valued tokenizers. We evaluate its effectiveness across a wide range of cases, including standard autoregressive models and generalized masked autoregressive (MAR) variants. By removing vector quantization, our image generator achieves strong results while enjoying the speed advantage of sequence modeling. We hope this work will motivate the use of autoregressive generation in other continuous-valued domains and applications. Code is available at: https://github.com/LTH14/mar.",
      "authors": [
        "Tianhong Li",
        "Yonglong Tian",
        "He Li",
        "Mingyang Deng",
        "Kaiming He"
      ],
      "published": "2024-06-17T17:59:58Z",
      "updated": "2024-11-01T14:45:36Z",
      "categories": [
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2406.11838v3",
      "landing_url": "https://arxiv.org/abs/2406.11838v3",
      "doi": "https://doi.org/10.48550/arXiv.2406.11838"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This item concerns image generation with continuous autoregressive modeling and diffusion loss, not discrete audio tokenization or speech token quantization, so it does not meet the core inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "This item concerns image generation with continuous autoregressive modeling and diffusion loss, not discrete audio tokenization or speech token quantization, so it does not meet the core inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on autoregressive image generation without vector quantization and does not discuss discrete audio or speech tokenization, nor does it analyze tokenization methods for audio signals; hence, it does not meet the inclusion criteria related to audio/speech tokenization and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on autoregressive image generation without vector quantization and does not discuss discrete audio or speech tokenization, nor does it analyze tokenization methods for audio signals; hence, it does not meet the inclusion criteria related to audio/speech tokenization and should be excluded.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Binaural Selective Attention Model for Target Speaker Extraction",
    "abstract": "The remarkable ability of humans to selectively focus on a target speaker in cocktail party scenarios is facilitated by binaural audio processing. In this paper, we present a binaural time-domain Target Speaker Extraction model based on the Filter-and-Sum Network (FaSNet). Inspired by human selective hearing, our proposed model introduces target speaker embedding into separators using a multi-head attention-based selective attention block. We also compared two binaural interaction approaches -- the cosine similarity of time-domain signals and inter-channel correlation in learned spectral representations. Our experimental results show that our proposed model outperforms monaural configurations and state-of-the-art multi-channel target speaker extraction models, achieving best-in-class performance with 18.52 dB SI-SDR, 19.12 dB SDR, and 3.05 PESQ scores under anechoic two-speaker test configurations.",
    "metadata": {
      "arxiv_id": "2406.12236",
      "title": "Binaural Selective Attention Model for Target Speaker Extraction",
      "summary": "The remarkable ability of humans to selectively focus on a target speaker in cocktail party scenarios is facilitated by binaural audio processing. In this paper, we present a binaural time-domain Target Speaker Extraction model based on the Filter-and-Sum Network (FaSNet). Inspired by human selective hearing, our proposed model introduces target speaker embedding into separators using a multi-head attention-based selective attention block. We also compared two binaural interaction approaches -- the cosine similarity of time-domain signals and inter-channel correlation in learned spectral representations. Our experimental results show that our proposed model outperforms monaural configurations and state-of-the-art multi-channel target speaker extraction models, achieving best-in-class performance with 18.52 dB SI-SDR, 19.12 dB SDR, and 3.05 PESQ scores under anechoic two-speaker test configurations.",
      "authors": [
        "Hanyu Meng",
        "Qiquan Zhang",
        "Xiangyu Zhang",
        "Vidhyasaharan Sethu",
        "Eliathamby Ambikairajah"
      ],
      "published": "2024-06-18T03:24:52Z",
      "updated": "2024-06-18T03:24:52Z",
      "categories": [
        "eess.AS",
        "cs.SD",
        "eess.SP"
      ],
      "pdf_url": "https://arxiv.org/pdf/2406.12236v1",
      "landing_url": "https://arxiv.org/abs/2406.12236v1",
      "doi": "https://doi.org/10.48550/arXiv.2406.12236"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The work focuses on time-domain speech separation and does not address discrete audio tokenization or quantization, thus it fails the inclusion criteria requiring discrete token representations.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The work focuses on time-domain speech separation and does not address discrete audio tokenization or quantization, thus it fails the inclusion criteria requiring discrete token representations.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on binaural selective attention and target speaker extraction using continuous audio processing techniques like time-domain signals and spectral representations; it does not explicitly mention discrete audio/speech tokenization or the conversion of continuous audio into discrete tokens, which is the core requirement of the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on binaural selective attention and target speaker extraction using continuous audio processing techniques like time-domain signals and spectral representations; it does not explicitly mention discrete audio/speech tokenization or the conversion of continuous audio into discrete tokens, which is the core requirement of the inclusion criteria.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Enhancing Automated Audio Captioning via Large Language Models with Optimized Audio Encoding",
    "abstract": "Automated audio captioning (AAC) is an audio-to-text task to describe audio contents in natural language. Recently, the advancements in large language models (LLMs), with improvements in training approaches for audio encoders, have opened up possibilities for improving AAC. Thus, we explore enhancing AAC from three aspects: 1) a pre-trained audio encoder via consistent ensemble distillation (CED) is used to improve the effectivity of acoustic tokens, with a querying transformer (Q-Former) bridging the modality gap to LLM and compress acoustic tokens; 2) we investigate the advantages of using a Llama 2 with 7B parameters as the decoder; 3) another pre-trained LLM corrects text errors caused by insufficient training data and annotation ambiguities. Both the audio encoder and text decoder are optimized by low-rank adaptation (LoRA). Experiments show that each of these enhancements is effective. Our method obtains a 33.0 SPIDEr-FL score, outperforming the winner of DCASE 2023 Task 6A.",
    "metadata": {
      "arxiv_id": "2406.13275",
      "title": "Enhancing Automated Audio Captioning via Large Language Models with Optimized Audio Encoding",
      "summary": "Automated audio captioning (AAC) is an audio-to-text task to describe audio contents in natural language. Recently, the advancements in large language models (LLMs), with improvements in training approaches for audio encoders, have opened up possibilities for improving AAC. Thus, we explore enhancing AAC from three aspects: 1) a pre-trained audio encoder via consistent ensemble distillation (CED) is used to improve the effectivity of acoustic tokens, with a querying transformer (Q-Former) bridging the modality gap to LLM and compress acoustic tokens; 2) we investigate the advantages of using a Llama 2 with 7B parameters as the decoder; 3) another pre-trained LLM corrects text errors caused by insufficient training data and annotation ambiguities. Both the audio encoder and text decoder are optimized by low-rank adaptation (LoRA). Experiments show that each of these enhancements is effective. Our method obtains a 33.0 SPIDEr-FL score, outperforming the winner of DCASE 2023 Task 6A.",
      "authors": [
        "Jizhong Liu",
        "Gang Li",
        "Junbo Zhang",
        "Heinrich Dinkel",
        "Yongqing Wang",
        "Zhiyong Yan",
        "Yujun Wang",
        "Bin Wang"
      ],
      "published": "2024-06-19T07:09:46Z",
      "updated": "2024-06-25T08:07:36Z",
      "categories": [
        "cs.SD",
        "cs.CL",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2406.13275v2",
      "landing_url": "https://arxiv.org/abs/2406.13275v2",
      "doi": "https://doi.org/10.48550/arXiv.2406.13275"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The abstract centers on using discrete acoustic tokens and tokenization-based encoding to connect audio with an LLM for AAC, indicating explicit interest in discrete token representations as part of the method.",
      "evaluation": 4
    },
    "round-A_JuniorNano_reasoning": "The abstract centers on using discrete acoustic tokens and tokenization-based encoding to connect audio with an LLM for AAC, indicating explicit interest in discrete token representations as part of the method.",
    "round-A_JuniorNano_evaluation": 4,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on enhancing automated audio captioning (AAC) by improving audio encoders and leveraging large language models, but it does not explicitly state the use, analysis, or proposal of discrete audio/speech tokenization turning continuous audio into discrete symbols; instead, it emphasizes optimized acoustic tokens and encoder-decoder architectures without clear evidence of discrete token quantization or discrete vocabulary design relevant to the inclusion criteria.",
      "evaluation": 2
    },
    "round-A_JuniorMini_reasoning": "The study focuses on enhancing automated audio captioning (AAC) by improving audio encoders and leveraging large language models, but it does not explicitly state the use, analysis, or proposal of discrete audio/speech tokenization turning continuous audio into discrete symbols; instead, it emphasizes optimized acoustic tokens and encoder-decoder architectures without clear evidence of discrete token quantization or discrete vocabulary design relevant to the inclusion criteria.",
    "round-A_JuniorMini_evaluation": 2,
    "round-B_SeniorLead_output": {
      "reasoning": "Both junior reviewers disagreed: although the abstract mentions \"acoustic tokens\" and compression, it does not explicitly describe discrete quantization, a finite-token vocabulary, or an analysis/proposal of discrete audio tokenization as required by the inclusion criteria, so it should be excluded.",
      "evaluation": 2
    },
    "round-B_SeniorLead_reasoning": "Both junior reviewers disagreed: although the abstract mentions \"acoustic tokens\" and compression, it does not explicitly describe discrete quantization, a finite-token vocabulary, or an analysis/proposal of discrete audio tokenization as required by the inclusion criteria, so it should be excluded.",
    "round-B_SeniorLead_evaluation": 2,
    "final_verdict": "exclude (senior:2)",
    "review_skipped": false,
    "discard_reason": "exclude (senior:2)"
  },
  {
    "title": "Children's Speech Recognition through Discrete Token Enhancement",
    "abstract": "Children's speech recognition is considered a low-resource task mainly due to the lack of publicly available data. There are several reasons for such data scarcity, including expensive data collection and annotation processes, and data privacy, among others. Transforming speech signals into discrete tokens that do not carry sensitive information but capture both linguistic and acoustic information could be a solution for privacy concerns. In this study, we investigate the integration of discrete speech tokens into children's speech recognition systems as input without significantly degrading the ASR performance. Additionally, we explored single-view and multi-view strategies for creating these discrete labels. Furthermore, we tested the models for generalization capabilities with unseen domain and nativity dataset. Results reveal that the discrete token ASR for children achieves nearly equivalent performance with an approximate 83% reduction in parameters.",
    "metadata": {
      "arxiv_id": "2406.13431",
      "title": "Children's Speech Recognition through Discrete Token Enhancement",
      "summary": "Children's speech recognition is considered a low-resource task mainly due to the lack of publicly available data. There are several reasons for such data scarcity, including expensive data collection and annotation processes, and data privacy, among others. Transforming speech signals into discrete tokens that do not carry sensitive information but capture both linguistic and acoustic information could be a solution for privacy concerns. In this study, we investigate the integration of discrete speech tokens into children's speech recognition systems as input without significantly degrading the ASR performance. Additionally, we explored single-view and multi-view strategies for creating these discrete labels. Furthermore, we tested the models for generalization capabilities with unseen domain and nativity dataset. Results reveal that the discrete token ASR for children achieves nearly equivalent performance with an approximate 83% reduction in parameters.",
      "authors": [
        "Vrunda N. Sukhadia",
        "Shammur Absar Chowdhury"
      ],
      "published": "2024-06-19T10:45:12Z",
      "updated": "2024-06-24T15:31:59Z",
      "categories": [
        "cs.CL",
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2406.13431v2",
      "landing_url": "https://arxiv.org/abs/2406.13431v2",
      "doi": "https://doi.org/10.48550/arXiv.2406.13431"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The abstract explicitly studies discrete speech tokens as core input for children's ASR, including methods for creating discrete labels and evaluating their impact, which meets the inclusion criteria and does not match any exclusion criteria.",
      "evaluation": 5
    },
    "round-A_JuniorNano_reasoning": "The abstract explicitly studies discrete speech tokens as core input for children's ASR, including methods for creating discrete labels and evaluating their impact, which meets the inclusion criteria and does not match any exclusion criteria.",
    "round-A_JuniorNano_evaluation": 5,
    "round-A_JuniorMini_output": {
      "reasoning": "The study explicitly investigates the use of discrete speech tokens as inputs for children's speech recognition, analyzing different tokenization strategies and evaluating their performance and generalization, which aligns with the inclusion criteria of discrete audio tokenization for speech-related tasks; the abstract suggests methodological contributions and experimental validation beyond continuous features and traditional codecs, thus meeting the criteria for inclusion.",
      "evaluation": 5
    },
    "round-A_JuniorMini_reasoning": "The study explicitly investigates the use of discrete speech tokens as inputs for children's speech recognition, analyzing different tokenization strategies and evaluating their performance and generalization, which aligns with the inclusion criteria of discrete audio tokenization for speech-related tasks; the abstract suggests methodological contributions and experimental validation beyond continuous features and traditional codecs, thus meeting the criteria for inclusion.",
    "round-A_JuniorMini_evaluation": 5,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "include (junior:5)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Multilingual De-Duplication Strategies: Applying scalable similarity search with monolingual & multilingual embedding models",
    "abstract": "This paper addresses the deduplication of multilingual textual data using advanced NLP tools. We compare a two-step method involving translation to English followed by embedding with mpnet, and a multilingual embedding model (distiluse). The two-step approach achieved a higher F1 score (82% vs. 60%), particularly with less widely used languages, which can be increased up to 89% by leveraging expert rules based on domain knowledge. We also highlight limitations related to token length constraints and computational efficiency. Our methodology suggests improvements for future multilingual deduplication tasks.",
    "metadata": {
      "arxiv_id": "2406.13695",
      "title": "Multilingual De-Duplication Strategies: Applying scalable similarity search with monolingual & multilingual embedding models",
      "summary": "This paper addresses the deduplication of multilingual textual data using advanced NLP tools. We compare a two-step method involving translation to English followed by embedding with mpnet, and a multilingual embedding model (distiluse). The two-step approach achieved a higher F1 score (82% vs. 60%), particularly with less widely used languages, which can be increased up to 89% by leveraging expert rules based on domain knowledge. We also highlight limitations related to token length constraints and computational efficiency. Our methodology suggests improvements for future multilingual deduplication tasks.",
      "authors": [
        "Stefan Pasch",
        "Dimitirios Petridis",
        "Jannic Cutura"
      ],
      "published": "2024-06-19T16:48:14Z",
      "updated": "2024-06-19T16:48:14Z",
      "categories": [
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2406.13695v1",
      "landing_url": "https://arxiv.org/abs/2406.13695v1",
      "doi": "https://doi.org/10.48550/arXiv.2406.13695"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item is about multilingual text deduplication and embeddings, with no discrete audio tokenization or speech token design; thus it does not satisfy the inclusion criteria focused on discrete audio tokenization for speech tasks and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item is about multilingual text deduplication and embeddings, with no discrete audio tokenization or speech token design; thus it does not satisfy the inclusion criteria focused on discrete audio tokenization for speech tasks and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on multilingual textual data deduplication using embedding models and translation strategies, without addressing discrete audio/speech tokenization or any neural discrete token representation of audio signals, thus failing to meet the inclusion criteria and conflicting with the exclusion criteria related to discrete audio tokenization.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on multilingual textual data deduplication using embedding models and translation strategies, without addressing discrete audio/speech tokenization or any neural discrete token representation of audio signals, thus failing to meet the inclusion criteria and conflicting with the exclusion criteria related to discrete audio tokenization.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "EAGER: Two-Stream Generative Recommender with Behavior-Semantic Collaboration",
    "abstract": "Generative retrieval has recently emerged as a promising approach to sequential recommendation, framing candidate item retrieval as an autoregressive sequence generation problem. However, existing generative methods typically focus solely on either behavioral or semantic aspects of item information, neglecting their complementary nature and thus resulting in limited effectiveness. To address this limitation, we introduce EAGER, a novel generative recommendation framework that seamlessly integrates both behavioral and semantic information. Specifically, we identify three key challenges in combining these two types of information: a unified generative architecture capable of handling two feature types, ensuring sufficient and independent learning for each type, and fostering subtle interactions that enhance collaborative information utilization. To achieve these goals, we propose (1) a two-stream generation architecture leveraging a shared encoder and two separate decoders to decode behavior tokens and semantic tokens with a confidence-based ranking strategy; (2) a global contrastive task with summary tokens to achieve discriminative decoding for each type of information; and (3) a semantic-guided transfer task designed to implicitly promote cross-interactions through reconstruction and estimation objectives. We validate the effectiveness of EAGER on four public benchmarks, demonstrating its superior performance compared to existing methods.",
    "metadata": {
      "arxiv_id": "2406.14017",
      "title": "EAGER: Two-Stream Generative Recommender with Behavior-Semantic Collaboration",
      "summary": "Generative retrieval has recently emerged as a promising approach to sequential recommendation, framing candidate item retrieval as an autoregressive sequence generation problem. However, existing generative methods typically focus solely on either behavioral or semantic aspects of item information, neglecting their complementary nature and thus resulting in limited effectiveness. To address this limitation, we introduce EAGER, a novel generative recommendation framework that seamlessly integrates both behavioral and semantic information. Specifically, we identify three key challenges in combining these two types of information: a unified generative architecture capable of handling two feature types, ensuring sufficient and independent learning for each type, and fostering subtle interactions that enhance collaborative information utilization. To achieve these goals, we propose (1) a two-stream generation architecture leveraging a shared encoder and two separate decoders to decode behavior tokens and semantic tokens with a confidence-based ranking strategy; (2) a global contrastive task with summary tokens to achieve discriminative decoding for each type of information; and (3) a semantic-guided transfer task designed to implicitly promote cross-interactions through reconstruction and estimation objectives. We validate the effectiveness of EAGER on four public benchmarks, demonstrating its superior performance compared to existing methods.",
      "authors": [
        "Ye Wang",
        "Jiahao Xun",
        "Minjie Hong",
        "Jieming Zhu",
        "Tao Jin",
        "Wang Lin",
        "Haoyuan Li",
        "Linjun Li",
        "Yan Xia",
        "Zhou Zhao",
        "Zhenhua Dong"
      ],
      "published": "2024-06-20T06:21:56Z",
      "updated": "2024-07-03T10:00:26Z",
      "categories": [
        "cs.IR"
      ],
      "pdf_url": "https://arxiv.org/pdf/2406.14017v2",
      "landing_url": "https://arxiv.org/abs/2406.14017v2",
      "doi": "https://doi.org/10.48550/arXiv.2406.14017"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item targets a generative recommender system with behavioral/semantic information; it does not discuss discrete audio/speech tokenization or discrete token representations for audio, so it does not meet the audio-tokenization inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item targets a generative recommender system with behavioral/semantic information; it does not discuss discrete audio/speech tokenization or discrete token representations for audio, so it does not meet the audio-tokenization inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on a generative recommender system integrating behavioral and semantic information for item recommendation, with no mention of discrete audio/speech tokenization, nor analysis or evaluation of audio token sequences; it does not meet the inclusion criteria related to audio tokenization and falls under exclusion for lacking any relevance to discrete audio tokens.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on a generative recommender system integrating behavioral and semantic information for item recommendation, with no mention of discrete audio/speech tokenization, nor analysis or evaluation of audio token sequences; it does not meet the inclusion criteria related to audio tokenization and falls under exclusion for lacking any relevance to discrete audio tokens.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "DASB - Discrete Audio and Speech Benchmark",
    "abstract": "Discrete audio tokens have recently gained considerable attention for their potential to connect audio and language processing, enabling the creation of modern multimodal large language models. Ideal audio tokens must effectively preserve phonetic and semantic content along with paralinguistic information, speaker identity, and other details. While several types of audio tokens have been recently proposed, identifying the optimal tokenizer for various tasks is challenging due to the inconsistent evaluation settings in existing studies. To address this gap, we release the Discrete Audio and Speech Benchmark (DASB), a comprehensive leaderboard for benchmarking discrete audio tokens across a wide range of discriminative tasks, including speech recognition, speaker identification and verification, emotion recognition, keyword spotting, and intent classification, as well as generative tasks such as speech enhancement, separation, and text-to-speech. Our results show that, on average, semantic tokens outperform compression tokens across most discriminative and generative tasks. However, the performance gap between semantic tokens and standard continuous representations remains substantial, highlighting the need for further research in this field.",
    "metadata": {
      "arxiv_id": "2406.14294",
      "title": "DASB - Discrete Audio and Speech Benchmark",
      "summary": "Discrete audio tokens have recently gained considerable attention for their potential to connect audio and language processing, enabling the creation of modern multimodal large language models. Ideal audio tokens must effectively preserve phonetic and semantic content along with paralinguistic information, speaker identity, and other details. While several types of audio tokens have been recently proposed, identifying the optimal tokenizer for various tasks is challenging due to the inconsistent evaluation settings in existing studies. To address this gap, we release the Discrete Audio and Speech Benchmark (DASB), a comprehensive leaderboard for benchmarking discrete audio tokens across a wide range of discriminative tasks, including speech recognition, speaker identification and verification, emotion recognition, keyword spotting, and intent classification, as well as generative tasks such as speech enhancement, separation, and text-to-speech. Our results show that, on average, semantic tokens outperform compression tokens across most discriminative and generative tasks. However, the performance gap between semantic tokens and standard continuous representations remains substantial, highlighting the need for further research in this field.",
      "authors": [
        "Pooneh Mousavi",
        "Luca Della Libera",
        "Jarod Duret",
        "Artem Ploujnikov",
        "Cem Subakan",
        "Mirco Ravanelli"
      ],
      "published": "2024-06-20T13:23:27Z",
      "updated": "2024-06-21T17:07:17Z",
      "categories": [
        "cs.SD",
        "cs.AI",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2406.14294v2",
      "landing_url": "https://arxiv.org/abs/2406.14294v2",
      "doi": "https://doi.org/10.48550/arXiv.2406.14294"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The abstract centers on discrete audio tokens and benchmarks their use across speech-related tasks, addressing discrete tokenization as the core object, and the full text is in English, meeting inclusion criteria and not falling under exclusions.",
      "evaluation": 5
    },
    "round-A_JuniorNano_reasoning": "The abstract centers on discrete audio tokens and benchmarks their use across speech-related tasks, addressing discrete tokenization as the core object, and the full text is in English, meeting inclusion criteria and not falling under exclusions.",
    "round-A_JuniorNano_evaluation": 5,
    "round-A_JuniorMini_output": {
      "reasoning": "The study explicitly proposes and evaluates discrete audio tokens as its core focus, covering both discriminative and generative tasks relevant to speech, and it involves systematic analysis across multiple benchmarks, thus meeting all inclusion criteria without triggering any exclusion criteria.",
      "evaluation": 5
    },
    "round-A_JuniorMini_reasoning": "The study explicitly proposes and evaluates discrete audio tokens as its core focus, covering both discriminative and generative tasks relevant to speech, and it involves systematic analysis across multiple benchmarks, thus meeting all inclusion criteria without triggering any exclusion criteria.",
    "round-A_JuniorMini_evaluation": 5,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "include (junior:5)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Learning Binary Color Filter Arrays with Trainable Hard Thresholding",
    "abstract": "Color Filter Arrays (CFA) are optical filters in digital cameras that capture specific color channels. Current commercial CFAs are hand-crafted patterns with different physical and application-specific considerations. This study proposes a binary CFA learning module based on hard thresholding with a deep learning-based demosaicing network in a joint architecture. Unlike most existing learnable CFAs that learn a channel from the whole color spectrum or linearly combine available digital colors, this method learns a binary channel selection, resulting in CFAs that are practical and physically implementable to digital cameras. The binary selection is based on adapting the hard thresholding operation into neural networks via a straight-through estimator, and therefore it is named HardMax. This paper includes the background on the CFA design problem, the description of the HardMax method, and the performance evaluation results. The evaluation of the proposed method includes tests for different demosaicing models, color configurations, filter sizes, and a comparison with existing methods in various reconstruction metrics. The proposed approach is tested with Kodak and BSDS500 datasets and provides higher reconstruction performance than hand-crafted or alternative learned binary filters.",
    "metadata": {
      "arxiv_id": "2406.14421",
      "title": "Learning Binary Color Filter Arrays with Trainable Hard Thresholding",
      "summary": "Color Filter Arrays (CFA) are optical filters in digital cameras that capture specific color channels. Current commercial CFAs are hand-crafted patterns with different physical and application-specific considerations. This study proposes a binary CFA learning module based on hard thresholding with a deep learning-based demosaicing network in a joint architecture. Unlike most existing learnable CFAs that learn a channel from the whole color spectrum or linearly combine available digital colors, this method learns a binary channel selection, resulting in CFAs that are practical and physically implementable to digital cameras. The binary selection is based on adapting the hard thresholding operation into neural networks via a straight-through estimator, and therefore it is named HardMax. This paper includes the background on the CFA design problem, the description of the HardMax method, and the performance evaluation results. The evaluation of the proposed method includes tests for different demosaicing models, color configurations, filter sizes, and a comparison with existing methods in various reconstruction metrics. The proposed approach is tested with Kodak and BSDS500 datasets and provides higher reconstruction performance than hand-crafted or alternative learned binary filters.",
      "authors": [
        "Cemre Omer Ayna",
        "Bahadir Kursat Gunturk",
        "Ali Cafer Gurbuz"
      ],
      "published": "2024-06-20T15:41:22Z",
      "updated": "2024-06-20T15:41:22Z",
      "categories": [
        "eess.IV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2406.14421v1",
      "landing_url": "https://arxiv.org/abs/2406.14421v1",
      "doi": "https://doi.org/10.48550/arXiv.2406.14421"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This item addresses learnable binary color filter arrays for camera demosaicing; there is no discrete audio/speech tokenization or token design, so it does not meet the audio-token inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "This item addresses learnable binary color filter arrays for camera demosaicing; there is no discrete audio/speech tokenization or token design, so it does not meet the audio-token inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on learning binary color filter arrays (CFAs) for digital cameras, dealing with optical filter design and image reconstruction; it does not address discrete audio/speech tokenization, audio signals, or speech-related tasks, thus not meeting any inclusion criteria related to audio tokenization or analysis.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on learning binary color filter arrays (CFAs) for digital cameras, dealing with optical filter design and image reconstruction; it does not address discrete audio/speech tokenization, audio signals, or speech-related tasks, thus not meeting any inclusion criteria related to audio tokenization or analysis.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "TacoLM: GaTed Attention Equipped Codec Language Model are Efficient Zero-Shot Text to Speech Synthesizers",
    "abstract": "Neural codec language model (LM) has demonstrated strong capability in zero-shot text-to-speech (TTS) synthesis. However, the codec LM often suffers from limitations in inference speed and stability, due to its auto-regressive nature and implicit alignment between text and audio. In this work, to handle these challenges, we introduce a new variant of neural codec LM, namely TacoLM. Specifically, TacoLM introduces a gated attention mechanism to improve the training and inference efficiency and reduce the model size. Meanwhile, an additional gated cross-attention layer is included for each decoder layer, which improves the efficiency and content accuracy of the synthesized speech. In the evaluation of the Librispeech corpus, the proposed TacoLM achieves a better word error rate, speaker similarity, and mean opinion score, with 90% fewer parameters and 5.2 times speed up, compared with VALL-E. Demo and code is available at https://ereboas.github.io/TacoLM/.",
    "metadata": {
      "arxiv_id": "2406.15752",
      "title": "TacoLM: GaTed Attention Equipped Codec Language Model are Efficient Zero-Shot Text to Speech Synthesizers",
      "summary": "Neural codec language model (LM) has demonstrated strong capability in zero-shot text-to-speech (TTS) synthesis. However, the codec LM often suffers from limitations in inference speed and stability, due to its auto-regressive nature and implicit alignment between text and audio. In this work, to handle these challenges, we introduce a new variant of neural codec LM, namely TacoLM. Specifically, TacoLM introduces a gated attention mechanism to improve the training and inference efficiency and reduce the model size. Meanwhile, an additional gated cross-attention layer is included for each decoder layer, which improves the efficiency and content accuracy of the synthesized speech. In the evaluation of the Librispeech corpus, the proposed TacoLM achieves a better word error rate, speaker similarity, and mean opinion score, with 90% fewer parameters and 5.2 times speed up, compared with VALL-E. Demo and code is available at https://ereboas.github.io/TacoLM/.",
      "authors": [
        "Yakun Song",
        "Zhuo Chen",
        "Xiaofei Wang",
        "Ziyang Ma",
        "Guanrou Yang",
        "Xie Chen"
      ],
      "published": "2024-06-22T06:39:52Z",
      "updated": "2024-06-22T06:39:52Z",
      "categories": [
        "eess.AS",
        "cs.AI",
        "cs.CL"
      ],
      "pdf_url": "https://arxiv.org/pdf/2406.15752v1",
      "landing_url": "https://arxiv.org/abs/2406.15752v1",
      "doi": "https://doi.org/10.48550/arXiv.2406.15752"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The abstract presents a gated-attention neural codec language model for zero-shot TTS with efficiency gains, but it does not explicitly discuss or analyze discrete audio/speech tokenization as a core object of study, which is required by the inclusion criteria.",
      "evaluation": 2
    },
    "round-A_JuniorNano_reasoning": "The abstract presents a gated-attention neural codec language model for zero-shot TTS with efficiency gains, but it does not explicitly discuss or analyze discrete audio/speech tokenization as a core object of study, which is required by the inclusion criteria.",
    "round-A_JuniorNano_evaluation": 2,
    "round-A_JuniorMini_output": {
      "reasoning": "The study introduces TacoLM, a neural codec language model for zero-shot text-to-speech synthesis that involves discrete audio tokenization as part of its model design and evaluation, demonstrating improvements over prior models with clear experimental validation and methodological contributions relevant to discrete audio tokens.",
      "evaluation": 5
    },
    "round-A_JuniorMini_reasoning": "The study introduces TacoLM, a neural codec language model for zero-shot text-to-speech synthesis that involves discrete audio tokenization as part of its model design and evaluation, demonstrating improvements over prior models with clear experimental validation and methodological contributions relevant to discrete audio tokens.",
    "round-A_JuniorMini_evaluation": 5,
    "round-B_SeniorLead_output": {
      "reasoning": "I reviewed both junior reviewers: JuniorNano notes the abstract does not explicitly discuss discrete audio/speech tokenization as a core object, while JuniorMini infers token usage from the term 'neural codec LM'. The inclusion criteria require explicit proposal/analysis/evaluation of discrete audio tokenization, but this abstract focuses on gated-attention architecture and efficiency gains versus VALL-E without explicitly proposing or analyzing a discrete tokenization method—therefore it does not meet the inclusion criteria. Explanation (one sentence): Better to exclude (2) — the abstract lacks explicit treatment or analysis of discrete audio/speech tokenization as a core contribution.",
      "evaluation": 2
    },
    "round-B_SeniorLead_reasoning": "I reviewed both junior reviewers: JuniorNano notes the abstract does not explicitly discuss discrete audio/speech tokenization as a core object, while JuniorMini infers token usage from the term 'neural codec LM'. The inclusion criteria require explicit proposal/analysis/evaluation of discrete audio tokenization, but this abstract focuses on gated-attention architecture and efficiency gains versus VALL-E without explicitly proposing or analyzing a discrete tokenization method—therefore it does not meet the inclusion criteria. Explanation (one sentence): Better to exclude (2) — the abstract lacks explicit treatment or analysis of discrete audio/speech tokenization as a core contribution.",
    "round-B_SeniorLead_evaluation": 2,
    "final_verdict": "exclude (senior:2)",
    "review_skipped": false,
    "discard_reason": "exclude (senior:2)"
  },
  {
    "title": "High Fidelity Text-to-Speech Via Discrete Tokens Using Token Transducer and Group Masked Language Model",
    "abstract": "We propose a novel two-stage text-to-speech (TTS) framework with two types of discrete tokens, i.e., semantic and acoustic tokens, for high-fidelity speech synthesis. It features two core components: the Interpreting module, which processes text and a speech prompt into semantic tokens focusing on linguistic contents and alignment, and the Speaking module, which captures the timbre of the target voice to generate acoustic tokens from semantic tokens, enriching speech reconstruction. The Interpreting stage employs a transducer for its robustness in aligning text to speech. In contrast, the Speaking stage utilizes a Conformer-based architecture integrated with a Grouped Masked Language Model (G-MLM) to boost computational efficiency. Our experiments verify that this innovative structure surpasses the conventional models in the zero-shot scenario in terms of speech quality and speaker similarity.",
    "metadata": {
      "arxiv_id": "2406.17310",
      "title": "High Fidelity Text-to-Speech Via Discrete Tokens Using Token Transducer and Group Masked Language Model",
      "summary": "We propose a novel two-stage text-to-speech (TTS) framework with two types of discrete tokens, i.e., semantic and acoustic tokens, for high-fidelity speech synthesis. It features two core components: the Interpreting module, which processes text and a speech prompt into semantic tokens focusing on linguistic contents and alignment, and the Speaking module, which captures the timbre of the target voice to generate acoustic tokens from semantic tokens, enriching speech reconstruction. The Interpreting stage employs a transducer for its robustness in aligning text to speech. In contrast, the Speaking stage utilizes a Conformer-based architecture integrated with a Grouped Masked Language Model (G-MLM) to boost computational efficiency. Our experiments verify that this innovative structure surpasses the conventional models in the zero-shot scenario in terms of speech quality and speaker similarity.",
      "authors": [
        "Joun Yeop Lee",
        "Myeonghun Jeong",
        "Minchan Kim",
        "Ji-Hyun Lee",
        "Hoon-Young Cho",
        "Nam Soo Kim"
      ],
      "published": "2024-06-25T06:46:47Z",
      "updated": "2024-06-25T06:46:47Z",
      "categories": [
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2406.17310v1",
      "landing_url": "https://arxiv.org/abs/2406.17310v1",
      "doi": "https://doi.org/10.48550/arXiv.2406.17310"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The abstract explicitly proposes and analyzes discrete audio tokens (semantic and acoustic) for TTS, meeting inclusion criteria and not falling under any exclusion criteria.",
      "evaluation": 5
    },
    "round-A_JuniorNano_reasoning": "The abstract explicitly proposes and analyzes discrete audio tokens (semantic and acoustic) for TTS, meeting inclusion criteria and not falling under any exclusion criteria.",
    "round-A_JuniorNano_evaluation": 5,
    "round-A_JuniorMini_output": {
      "reasoning": "The study explicitly proposes and evaluates discrete tokens (semantic and acoustic) as a core object for text-to-speech synthesis, converting continuous audio into discrete sequences for high-fidelity speech reconstruction, fulfilling all inclusion criteria and none of the exclusion criteria.",
      "evaluation": 5
    },
    "round-A_JuniorMini_reasoning": "The study explicitly proposes and evaluates discrete tokens (semantic and acoustic) as a core object for text-to-speech synthesis, converting continuous audio into discrete sequences for high-fidelity speech reconstruction, fulfilling all inclusion criteria and none of the exclusion criteria.",
    "round-A_JuniorMini_evaluation": 5,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "include (junior:5)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "On the Effectiveness of Acoustic BPE in Decoder-Only TTS",
    "abstract": "Discretizing speech into tokens and generating them by a decoder-only model have been a promising direction for text-to-speech (TTS) and spoken language modeling (SLM). To shorten the sequence length of speech tokens, acoustic byte-pair encoding (BPE) has emerged in SLM that treats speech tokens from self-supervised semantic representations as characters to further compress the token sequence. But the gain in TTS has not been fully investigated, and the proper choice of acoustic BPE remains unclear. In this work, we conduct a comprehensive study on various settings of acoustic BPE to explore its effectiveness in decoder-only TTS models with semantic speech tokens. Experiments on LibriTTS verify that acoustic BPE uniformly increases the intelligibility and diversity of synthesized speech, while showing different features across BPE settings. Hence, acoustic BPE is a favorable tool for decoder-only TTS.",
    "metadata": {
      "arxiv_id": "2407.03892",
      "title": "On the Effectiveness of Acoustic BPE in Decoder-Only TTS",
      "summary": "Discretizing speech into tokens and generating them by a decoder-only model have been a promising direction for text-to-speech (TTS) and spoken language modeling (SLM). To shorten the sequence length of speech tokens, acoustic byte-pair encoding (BPE) has emerged in SLM that treats speech tokens from self-supervised semantic representations as characters to further compress the token sequence. But the gain in TTS has not been fully investigated, and the proper choice of acoustic BPE remains unclear. In this work, we conduct a comprehensive study on various settings of acoustic BPE to explore its effectiveness in decoder-only TTS models with semantic speech tokens. Experiments on LibriTTS verify that acoustic BPE uniformly increases the intelligibility and diversity of synthesized speech, while showing different features across BPE settings. Hence, acoustic BPE is a favorable tool for decoder-only TTS.",
      "authors": [
        "Bohan Li",
        "Feiyu Shen",
        "Yiwei Guo",
        "Shuai Wang",
        "Xie Chen",
        "Kai Yu"
      ],
      "published": "2024-07-04T12:35:32Z",
      "updated": "2024-07-04T12:35:32Z",
      "categories": [
        "cs.SD",
        "cs.AI",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2407.03892v1",
      "landing_url": "https://arxiv.org/abs/2407.03892v1",
      "doi": "https://doi.org/10.48550/arXiv.2407.03892"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The study directly investigates discrete acoustic tokenization using acoustic BPE in a decoder-only TTS model and provides empirical evaluation, matching the core tokenization criterion and not aligning with the exclusion criteria.",
      "evaluation": 5
    },
    "round-A_JuniorNano_reasoning": "The study directly investigates discrete acoustic tokenization using acoustic BPE in a decoder-only TTS model and provides empirical evaluation, matching the core tokenization criterion and not aligning with the exclusion criteria.",
    "round-A_JuniorNano_evaluation": 5,
    "round-A_JuniorMini_output": {
      "reasoning": "The study explicitly focuses on evaluating acoustic byte-pair encoding (BPE) as a discrete tokenization technique for speech signals in decoder-only text-to-speech models, analyzing its effects on synthesized speech intelligibility and diversity, which aligns with the inclusion criteria of analyzing discrete audio tokenization. The abstract indicates a systematic experimental evaluation with semantic speech tokens, excludes pure continuous feature usage and traditional codecs, and presents a methodological contribution rather than an engineering-only report.",
      "evaluation": 5
    },
    "round-A_JuniorMini_reasoning": "The study explicitly focuses on evaluating acoustic byte-pair encoding (BPE) as a discrete tokenization technique for speech signals in decoder-only text-to-speech models, analyzing its effects on synthesized speech intelligibility and diversity, which aligns with the inclusion criteria of analyzing discrete audio tokenization. The abstract indicates a systematic experimental evaluation with semantic speech tokens, excludes pure continuous feature usage and traditional codecs, and presents a methodological contribution rather than an engineering-only report.",
    "round-A_JuniorMini_evaluation": 5,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "include (junior:5)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "CosyVoice: A Scalable Multilingual Zero-shot Text-to-speech Synthesizer based on Supervised Semantic Tokens",
    "abstract": "Recent years have witnessed a trend that large language model (LLM) based text-to-speech (TTS) emerges into the mainstream due to their high naturalness and zero-shot capacity. In this paradigm, speech signals are discretized into token sequences, which are modeled by an LLM with text as prompts and reconstructed by a token-based vocoder to waveforms. Obviously, speech tokens play a critical role in LLM-based TTS models. Current speech tokens are learned in an unsupervised manner, which lacks explicit semantic information and alignment to the text. In this paper, we propose to represent speech with supervised semantic tokens, which are derived from a multilingual speech recognition model by inserting vector quantization into the encoder. Based on the tokens, we further propose a scalable zero-shot TTS synthesizer, CosyVoice, which consists of an LLM for text-to-token generation and a conditional flow matching model for token-to-speech synthesis. Experimental results show that supervised semantic tokens significantly outperform existing unsupervised tokens in terms of content consistency and speaker similarity for zero-shot voice cloning. Moreover, we find that utilizing large-scale data further improves the synthesis performance, indicating the scalable capacity of CosyVoice. To the best of our knowledge, this is the first attempt to involve supervised speech tokens into TTS models.",
    "metadata": {
      "arxiv_id": "2407.05407",
      "title": "CosyVoice: A Scalable Multilingual Zero-shot Text-to-speech Synthesizer based on Supervised Semantic Tokens",
      "summary": "Recent years have witnessed a trend that large language model (LLM) based text-to-speech (TTS) emerges into the mainstream due to their high naturalness and zero-shot capacity. In this paradigm, speech signals are discretized into token sequences, which are modeled by an LLM with text as prompts and reconstructed by a token-based vocoder to waveforms. Obviously, speech tokens play a critical role in LLM-based TTS models. Current speech tokens are learned in an unsupervised manner, which lacks explicit semantic information and alignment to the text. In this paper, we propose to represent speech with supervised semantic tokens, which are derived from a multilingual speech recognition model by inserting vector quantization into the encoder. Based on the tokens, we further propose a scalable zero-shot TTS synthesizer, CosyVoice, which consists of an LLM for text-to-token generation and a conditional flow matching model for token-to-speech synthesis. Experimental results show that supervised semantic tokens significantly outperform existing unsupervised tokens in terms of content consistency and speaker similarity for zero-shot voice cloning. Moreover, we find that utilizing large-scale data further improves the synthesis performance, indicating the scalable capacity of CosyVoice. To the best of our knowledge, this is the first attempt to involve supervised speech tokens into TTS models.",
      "authors": [
        "Zhihao Du",
        "Qian Chen",
        "Shiliang Zhang",
        "Kai Hu",
        "Heng Lu",
        "Yexin Yang",
        "Hangrui Hu",
        "Siqi Zheng",
        "Yue Gu",
        "Ziyang Ma",
        "Zhifu Gao",
        "Zhijie Yan"
      ],
      "published": "2024-07-07T15:16:19Z",
      "updated": "2024-07-09T07:42:51Z",
      "categories": [
        "cs.SD",
        "cs.AI",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2407.05407v2",
      "landing_url": "https://arxiv.org/abs/2407.05407v2",
      "doi": "https://doi.org/10.48550/arXiv.2407.05407"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The paper explicitly centers on representing speech with supervised semantic tokens that are discrete and derived via vector quantization, and builds a TTS pipeline (text-to-token generation and token-to-speech synthesis) around these tokens, addressing discrete audio tokenization as the core object of study; this directly satisfies the inclusion criteria and addresses the exclusion criteria by not relying solely on continuous features or non-speech tokenization.",
      "evaluation": 5
    },
    "round-A_JuniorNano_reasoning": "The paper explicitly centers on representing speech with supervised semantic tokens that are discrete and derived via vector quantization, and builds a TTS pipeline (text-to-token generation and token-to-speech synthesis) around these tokens, addressing discrete audio tokenization as the core object of study; this directly satisfies the inclusion criteria and addresses the exclusion criteria by not relying solely on continuous features or non-speech tokenization.",
    "round-A_JuniorNano_evaluation": 5,
    "round-A_JuniorMini_output": {
      "reasoning": "The study explicitly proposes and evaluates supervised semantic tokens as discrete representations of speech for zero-shot text-to-speech synthesis, and thoroughly analyzes the token design and its effects on synthesis performance, meeting the inclusion criteria. It is not focused on continuous features alone, traditional codecs, nor non-speech audio, and includes methodological contributions with experimental validations relevant to discrete audio tokenization.",
      "evaluation": 5
    },
    "round-A_JuniorMini_reasoning": "The study explicitly proposes and evaluates supervised semantic tokens as discrete representations of speech for zero-shot text-to-speech synthesis, and thoroughly analyzes the token design and its effects on synthesis performance, meeting the inclusion criteria. It is not focused on continuous features alone, traditional codecs, nor non-speech audio, and includes methodological contributions with experimental validations relevant to discrete audio tokenization.",
    "round-A_JuniorMini_evaluation": 5,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "include (junior:5)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Local zeta functions for a class of p-adic symmetric spaces (II)",
    "abstract": "In this paper we study the zeta functions associated to the minimal spherical principal series of representations for a class of reductive p-adic symmetric spaces, which are realized as open orbits of some prehomogeneous spaces. These symmetric spaces have been studied in the paper arXiv: 2003.05764. We prove that the zeta functions satisfy a functional equation which is given explicitly (see Theorem 4.3.9 and Theorem 4.4.5). Moreover, for a subclass of these spaces, we define L-functions and epsilon-factors associated to the representations.",
    "metadata": {
      "arxiv_id": "2407.06667",
      "title": "Local zeta functions for a class of p-adic symmetric spaces (II)",
      "summary": "In this paper we study the zeta functions associated to the minimal spherical principal series of representations for a class of reductive p-adic symmetric spaces, which are realized as open orbits of some prehomogeneous spaces. These symmetric spaces have been studied in the paper arXiv: 2003.05764. We prove that the zeta functions satisfy a functional equation which is given explicitly (see Theorem 4.3.9 and Theorem 4.4.5). Moreover, for a subclass of these spaces, we define L-functions and epsilon-factors associated to the representations.",
      "authors": [
        "Pascale Harinck",
        "Hubert Rubenthaler"
      ],
      "published": "2024-07-09T08:43:27Z",
      "updated": "2025-03-17T21:18:09Z",
      "categories": [
        "math.RT"
      ],
      "pdf_url": "https://arxiv.org/pdf/2407.06667v2",
      "landing_url": "https://arxiv.org/abs/2407.06667v2",
      "doi": "https://doi.org/10.48550/arXiv.2407.06667"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This article is a pure mathematics study on local zeta functions for p-adic symmetric spaces, with no discrete audio/tokenization analysis, so it does not meet the inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "This article is a pure mathematics study on local zeta functions for p-adic symmetric spaces, with no discrete audio/tokenization analysis, so it does not meet the inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper focuses on mathematical properties of zeta functions for p-adic symmetric spaces, without any mention or analysis of discrete audio/speech tokenization or related tasks, thus not meeting any inclusion criteria and clearly falling outside the scope.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper focuses on mathematical properties of zeta functions for p-adic symmetric spaces, without any mention or analysis of discrete audio/speech tokenization or related tasks, thus not meeting any inclusion criteria and clearly falling outside the scope.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Adaptively Robust and Sparse K-means Clustering",
    "abstract": "While K-means is known to be a standard clustering algorithm, its performance may be compromised due to the presence of outliers and high-dimensional noisy variables. This paper proposes adaptively robust and sparse K-means clustering (ARSK) to address these practical limitations of the standard K-means algorithm. For robustness, we introduce a redundant error component for each observation, and this additional parameter is penalized using a group sparse penalty. To accommodate the impact of high-dimensional noisy variables, the objective function is modified by incorporating weights and implementing a penalty to control the sparsity of the weight vector. The tuning parameters to control the robustness and sparsity are selected by Gap statistics. Through simulation experiments and real data analysis, we demonstrate the proposed method's superiority to existing algorithms in identifying clusters without outliers and informative variables simultaneously.",
    "metadata": {
      "arxiv_id": "2407.06945",
      "title": "Adaptively Robust and Sparse K-means Clustering",
      "summary": "While K-means is known to be a standard clustering algorithm, its performance may be compromised due to the presence of outliers and high-dimensional noisy variables. This paper proposes adaptively robust and sparse K-means clustering (ARSK) to address these practical limitations of the standard K-means algorithm. For robustness, we introduce a redundant error component for each observation, and this additional parameter is penalized using a group sparse penalty. To accommodate the impact of high-dimensional noisy variables, the objective function is modified by incorporating weights and implementing a penalty to control the sparsity of the weight vector. The tuning parameters to control the robustness and sparsity are selected by Gap statistics. Through simulation experiments and real data analysis, we demonstrate the proposed method's superiority to existing algorithms in identifying clusters without outliers and informative variables simultaneously.",
      "authors": [
        "Hao Li",
        "Shonosuke Sugasawa",
        "Shota Katayama"
      ],
      "published": "2024-07-09T15:20:41Z",
      "updated": "2024-11-07T15:02:32Z",
      "categories": [
        "stat.CO",
        "stat.ML"
      ],
      "pdf_url": "https://arxiv.org/pdf/2407.06945v2",
      "landing_url": "https://arxiv.org/abs/2407.06945v2",
      "doi": "https://doi.org/10.48550/arXiv.2407.06945"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The article proposes adaptively robust and sparse K-means clustering for outlier robustness and feature sparsity; it does not involve discrete audio tokenization or token-based representations, so it fails the inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The article proposes adaptively robust and sparse K-means clustering for outlier robustness and feature sparsity; it does not involve discrete audio tokenization or token-based representations, so it fails the inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper focuses on a clustering algorithm enhancement (Robust and Sparse K-means) and does not discuss discrete audio/speech tokenization, nor does it address converting continuous audio signals into discrete token representations relevant to audio reconstruction, generation, or understanding tasks.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper focuses on a clustering algorithm enhancement (Robust and Sparse K-means) and does not discuss discrete audio/speech tokenization, nor does it address converting continuous audio signals into discrete token representations relevant to audio reconstruction, generation, or understanding tasks.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Privacy-Preserving Data Deduplication for Enhancing Federated Learning of Language Models (Extended Version)",
    "abstract": "Deduplication is a vital preprocessing step that enhances machine learning model performance and saves training time and energy. However, enhancing federated learning through deduplication poses challenges, especially regarding scalability and potential privacy violations if deduplication involves sharing all clients' data. In this paper, we address the problem of deduplication in a federated setup by introducing a pioneering protocol, Efficient Privacy-Preserving Multi-Party Deduplication (EP-MPD). It efficiently removes duplicates from multiple clients' datasets without compromising data privacy. EP-MPD is constructed in a modular fashion, utilizing two novel variants of the Private Set Intersection protocol. Our extensive experiments demonstrate the significant benefits of deduplication in federated learning of large language models. For instance, we observe up to 19.62\\% improvement in perplexity and up to 27.95\\% reduction in running time while varying the duplication level between 10\\% and 30\\%. EP-MPD effectively balances privacy and performance in federated learning, making it a valuable solution for large-scale applications.",
    "metadata": {
      "arxiv_id": "2407.08152",
      "title": "Privacy-Preserving Data Deduplication for Enhancing Federated Learning of Language Models (Extended Version)",
      "summary": "Deduplication is a vital preprocessing step that enhances machine learning model performance and saves training time and energy. However, enhancing federated learning through deduplication poses challenges, especially regarding scalability and potential privacy violations if deduplication involves sharing all clients' data. In this paper, we address the problem of deduplication in a federated setup by introducing a pioneering protocol, Efficient Privacy-Preserving Multi-Party Deduplication (EP-MPD). It efficiently removes duplicates from multiple clients' datasets without compromising data privacy. EP-MPD is constructed in a modular fashion, utilizing two novel variants of the Private Set Intersection protocol. Our extensive experiments demonstrate the significant benefits of deduplication in federated learning of large language models. For instance, we observe up to 19.62\\% improvement in perplexity and up to 27.95\\% reduction in running time while varying the duplication level between 10\\% and 30\\%. EP-MPD effectively balances privacy and performance in federated learning, making it a valuable solution for large-scale applications.",
      "authors": [
        "Aydin Abadi",
        "Vishnu Asutosh Dasu",
        "Sumanta Sarkar"
      ],
      "published": "2024-07-11T03:10:27Z",
      "updated": "2024-12-04T17:56:57Z",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2407.08152v2",
      "landing_url": "https://arxiv.org/abs/2407.08152v2",
      "doi": "https://doi.org/10.48550/arXiv.2407.08152"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item centers on privacy-preserving deduplication in federated learning for language models and does not involve discrete audio/speech tokenization or analysis of discrete tokens.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item centers on privacy-preserving deduplication in federated learning for language models and does not involve discrete audio/speech tokenization or analysis of discrete tokens.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on privacy-preserving data deduplication methods for federated learning of language models, without addressing discrete audio/speech tokenization or converting continuous audio signals into discrete symbols, which is the core requirement in the inclusion criteria. It does not analyze tokenization of discrete audio representations, nor does it focus on speech-related discrete tokenization methods.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on privacy-preserving data deduplication methods for federated learning of language models, without addressing discrete audio/speech tokenization or converting continuous audio signals into discrete symbols, which is the core requirement in the inclusion criteria. It does not analyze tokenization of discrete audio representations, nor does it focus on speech-related discrete tokenization methods.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Autoregressive Speech Synthesis without Vector Quantization",
    "abstract": "We present MELLE, a novel continuous-valued token based language modeling approach for text-to-speech synthesis (TTS). MELLE autoregressively generates continuous mel-spectrogram frames directly from text condition, bypassing the need for vector quantization, which is typically designed for audio compression and sacrifices fidelity compared to continuous representations. Specifically, (i) instead of cross-entropy loss, we apply regression loss with a proposed spectrogram flux loss function to model the probability distribution of the continuous-valued tokens; (ii) we have incorporated variational inference into MELLE to facilitate sampling mechanisms, thereby enhancing the output diversity and model robustness. Experiments demonstrate that, compared to the two-stage codec language model VALL-E and its variants, the single-stage MELLE mitigates robustness issues by avoiding the inherent flaws of sampling vector-quantized codes, achieves superior performance across multiple metrics, and, most importantly, offers a more streamlined paradigm. The demos of our work are provided at https://aka.ms/melle.",
    "metadata": {
      "arxiv_id": "2407.08551",
      "title": "Autoregressive Speech Synthesis without Vector Quantization",
      "summary": "We present MELLE, a novel continuous-valued token based language modeling approach for text-to-speech synthesis (TTS). MELLE autoregressively generates continuous mel-spectrogram frames directly from text condition, bypassing the need for vector quantization, which is typically designed for audio compression and sacrifices fidelity compared to continuous representations. Specifically, (i) instead of cross-entropy loss, we apply regression loss with a proposed spectrogram flux loss function to model the probability distribution of the continuous-valued tokens; (ii) we have incorporated variational inference into MELLE to facilitate sampling mechanisms, thereby enhancing the output diversity and model robustness. Experiments demonstrate that, compared to the two-stage codec language model VALL-E and its variants, the single-stage MELLE mitigates robustness issues by avoiding the inherent flaws of sampling vector-quantized codes, achieves superior performance across multiple metrics, and, most importantly, offers a more streamlined paradigm. The demos of our work are provided at https://aka.ms/melle.",
      "authors": [
        "Lingwei Meng",
        "Long Zhou",
        "Shujie Liu",
        "Sanyuan Chen",
        "Bing Han",
        "Shujie Hu",
        "Yanqing Liu",
        "Jinyu Li",
        "Sheng Zhao",
        "Xixin Wu",
        "Helen Meng",
        "Furu Wei"
      ],
      "published": "2024-07-11T14:36:53Z",
      "updated": "2025-05-27T05:07:56Z",
      "categories": [
        "cs.CL",
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2407.08551v2",
      "landing_url": "https://arxiv.org/abs/2407.08551v2",
      "doi": "https://doi.org/10.48550/arXiv.2407.08551"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The work uses continuous-valued tokens and bypasses discrete tokenization (no discrete audio tokens are proposed or analyzed), so it does not meet the inclusion criteria for discrete tokenization.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The work uses continuous-valued tokens and bypasses discrete tokenization (no discrete audio tokens are proposed or analyzed), so it does not meet the inclusion criteria for discrete tokenization.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study proposes a text-to-speech synthesis method (MELLE) that explicitly avoids vector quantization and instead uses continuous mel-spectrogram frames, indicating it does not analyze or propose discrete audio/speech tokenization but rather continuous representations; thus, it does not meet the inclusion criteria and falls under exclusion for using continuous features without discrete tokenization analysis.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study proposes a text-to-speech synthesis method (MELLE) that explicitly avoids vector quantization and instead uses continuous mel-spectrogram frames, indicating it does not analyze or propose discrete audio/speech tokenization but rather continuous representations; thus, it does not meet the inclusion criteria and falls under exclusion for using continuous features without discrete tokenization analysis.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Speech Slytherin: Examining the Performance and Efficiency of Mamba for Speech Separation, Recognition, and Synthesis",
    "abstract": "It is too early to conclude that Mamba is a better alternative to transformers for speech before comparing Mamba with transformers in terms of both performance and efficiency in multiple speech-related tasks. To reach this conclusion, we propose and evaluate three models for three tasks: Mamba-TasNet for speech separation, ConMamba for speech recognition, and VALL-M for speech synthesis. We compare them with transformers of similar sizes in performance, memory, and speed. Our Mamba or Mamba-transformer hybrid models show comparable or higher performance than their transformer counterparts: Sepformer, Conformer, and VALL-E. They are more efficient than transformers in memory and speed for speech longer than a threshold duration, inversely related to the resolution of a speech token. Mamba for separation is the most efficient, and Mamba for recognition is the least. Further, we show that Mamba is not more efficient than transformer for speech shorter than the threshold duration and performs worse in models that require joint modeling of text and speech, such as cross or masked attention of two inputs. Therefore, we argue that the superiority of Mamba or transformer depends on particular problems and models. Code available at https://github.com/xi-j/Mamba-TasNet and https://github.com/xi-j/Mamba-ASR.",
    "metadata": {
      "arxiv_id": "2407.09732",
      "title": "Speech Slytherin: Examining the Performance and Efficiency of Mamba for Speech Separation, Recognition, and Synthesis",
      "summary": "It is too early to conclude that Mamba is a better alternative to transformers for speech before comparing Mamba with transformers in terms of both performance and efficiency in multiple speech-related tasks. To reach this conclusion, we propose and evaluate three models for three tasks: Mamba-TasNet for speech separation, ConMamba for speech recognition, and VALL-M for speech synthesis. We compare them with transformers of similar sizes in performance, memory, and speed. Our Mamba or Mamba-transformer hybrid models show comparable or higher performance than their transformer counterparts: Sepformer, Conformer, and VALL-E. They are more efficient than transformers in memory and speed for speech longer than a threshold duration, inversely related to the resolution of a speech token. Mamba for separation is the most efficient, and Mamba for recognition is the least. Further, we show that Mamba is not more efficient than transformer for speech shorter than the threshold duration and performs worse in models that require joint modeling of text and speech, such as cross or masked attention of two inputs. Therefore, we argue that the superiority of Mamba or transformer depends on particular problems and models. Code available at https://github.com/xi-j/Mamba-TasNet and https://github.com/xi-j/Mamba-ASR.",
      "authors": [
        "Xilin Jiang",
        "Yinghao Aaron Li",
        "Adrian Nicolas Florea",
        "Cong Han",
        "Nima Mesgarani"
      ],
      "published": "2024-07-13T00:35:21Z",
      "updated": "2024-07-13T00:35:21Z",
      "categories": [
        "eess.AS",
        "cs.LG",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2407.09732v1",
      "landing_url": "https://arxiv.org/abs/2407.09732v1",
      "doi": "https://doi.org/10.48550/arXiv.2407.09732"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The abstract focuses on evaluating Mamba-based models for speech separation, recognition, and synthesis and their comparison to transformers; it does not explicitly propose, analyze, or evaluate discrete audio/speech tokenization as the core object of study, making it unlikely to meet the stated inclusion criteria for discrete tokenization analysis.",
      "evaluation": 2
    },
    "round-A_JuniorNano_reasoning": "The abstract focuses on evaluating Mamba-based models for speech separation, recognition, and synthesis and their comparison to transformers; it does not explicitly propose, analyze, or evaluate discrete audio/speech tokenization as the core object of study, making it unlikely to meet the stated inclusion criteria for discrete tokenization analysis.",
    "round-A_JuniorNano_evaluation": 2,
    "round-A_JuniorMini_output": {
      "reasoning": "The study compares and evaluates Mamba models with transformers for speech tasks focusing on performance and efficiency but does not explicitly propose, analyze, or evaluate discrete audio/speech tokenization as a core object of study; it appears focused on model architectures rather than discrete token design or quantization, thus not meeting the core inclusion criterion.",
      "evaluation": 2
    },
    "round-A_JuniorMini_reasoning": "The study compares and evaluates Mamba models with transformers for speech tasks focusing on performance and efficiency but does not explicitly propose, analyze, or evaluate discrete audio/speech tokenization as a core object of study; it appears focused on model architectures rather than discrete token design or quantization, thus not meeting the core inclusion criterion.",
    "round-A_JuniorMini_evaluation": 2,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:2)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:2)"
  },
  {
    "title": "MUSA: Multi-lingual Speaker Anonymization via Serial Disentanglement",
    "abstract": "Speaker anonymization is an effective privacy protection solution designed to conceal the speaker's identity while preserving the linguistic content and para-linguistic information of the original speech. While most prior studies focus solely on a single language, an ideal speaker anonymization system should be capable of handling multiple languages. This paper proposes MUSA, a Multi-lingual Speaker Anonymization approach that employs a serial disentanglement strategy to perform a step-by-step disentanglement from a global time-invariant representation to a temporal time-variant representation. By utilizing semantic distillation and self-supervised speaker distillation, the serial disentanglement strategy can avoid strong inductive biases and exhibit superior generalization performance across different languages. Meanwhile, we propose a straightforward anonymization strategy that employs empty embedding with zero values to simulate the speaker identity concealment process, eliminating the need for conversion to a pseudo-speaker identity and thereby reducing the complexity of speaker anonymization process. Experimental results on VoicePrivacy official datasets and multi-lingual datasets demonstrate that MUSA can effectively protect speaker privacy while preserving linguistic content and para-linguistic information.",
    "metadata": {
      "arxiv_id": "2407.11629",
      "title": "MUSA: Multi-lingual Speaker Anonymization via Serial Disentanglement",
      "summary": "Speaker anonymization is an effective privacy protection solution designed to conceal the speaker's identity while preserving the linguistic content and para-linguistic information of the original speech. While most prior studies focus solely on a single language, an ideal speaker anonymization system should be capable of handling multiple languages. This paper proposes MUSA, a Multi-lingual Speaker Anonymization approach that employs a serial disentanglement strategy to perform a step-by-step disentanglement from a global time-invariant representation to a temporal time-variant representation. By utilizing semantic distillation and self-supervised speaker distillation, the serial disentanglement strategy can avoid strong inductive biases and exhibit superior generalization performance across different languages. Meanwhile, we propose a straightforward anonymization strategy that employs empty embedding with zero values to simulate the speaker identity concealment process, eliminating the need for conversion to a pseudo-speaker identity and thereby reducing the complexity of speaker anonymization process. Experimental results on VoicePrivacy official datasets and multi-lingual datasets demonstrate that MUSA can effectively protect speaker privacy while preserving linguistic content and para-linguistic information.",
      "authors": [
        "Jixun Yao",
        "Qing Wang",
        "Pengcheng Guo",
        "Ziqian Ning",
        "Yuguang Yang",
        "Yu Pan",
        "Lei Xie"
      ],
      "published": "2024-07-16T11:48:00Z",
      "updated": "2024-07-16T11:48:00Z",
      "categories": [
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2407.11629v1",
      "landing_url": "https://arxiv.org/abs/2407.11629v1",
      "doi": "https://doi.org/10.48550/arXiv.2407.11629"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The abstract does not involve discrete audio tokenization or token-level representations; it focuses on speaker anonymization via representations and disentanglement rather than discrete audio tokens.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The abstract does not involve discrete audio tokenization or token-level representations; it focuses on speaker anonymization via representations and disentanglement rather than discrete audio tokens.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on speaker anonymization through a serial disentanglement strategy for multiple languages but does not explicitly mention proposing, analyzing, or evaluating discrete audio/speech tokenization where continuous audio is converted into discrete tokens; it appears to use continuous representations and embeddings rather than discrete token sequences as required by the inclusion criteria.",
      "evaluation": 2
    },
    "round-A_JuniorMini_reasoning": "The study focuses on speaker anonymization through a serial disentanglement strategy for multiple languages but does not explicitly mention proposing, analyzing, or evaluating discrete audio/speech tokenization where continuous audio is converted into discrete tokens; it appears to use continuous representations and embeddings rather than discrete token sequences as required by the inclusion criteria.",
    "round-A_JuniorMini_evaluation": 2,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:2)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:2)"
  },
  {
    "title": "The object detection method aids in image reconstruction evaluation and clinical interpretation of meniscal abnormalities",
    "abstract": "This study investigates the relationship between deep learning (DL) image reconstruction quality and anomaly detection performance, and evaluates the efficacy of an artificial intelligence (AI) assistant in enhancing radiologists' interpretation of meniscal anomalies on reconstructed images. A retrospective study was conducted using an in-house reconstruction and anomaly detection pipeline to assess knee MR images from 896 patients. The original and 14 sets of DL-reconstructed images were evaluated using standard reconstruction and object detection metrics, alongside newly developed box-based reconstruction metrics. Two clinical radiologists reviewed a subset of 50 patients' images, both original and AI-assisted reconstructed, with subsequent assessment of their accuracy and performance characteristics. Results indicated that the structural similarity index (SSIM) showed a weaker correlation with anomaly detection metrics (mAP, r=0.64, p=0.01; F1 score, r=0.38, p=0.18), while box-based SSIM had a stronger association with detection performance (mAP, r=0.81, p<0.01; F1 score, r=0.65, p=0.01). Minor SSIM fluctuations did not affect detection outcomes, but significant changes reduced performance. Radiologists' AI-assisted evaluations demonstrated improved accuracy (86.0% without assistance vs. 88.3% with assistance, p<0.05) and interrater agreement (Cohen's kappa, 0.39 without assistance vs. 0.57 with assistance). An additional review led to the incorporation of 17 more lesions into the dataset. The proposed anomaly detection method shows promise in evaluating reconstruction algorithms for automated tasks and aiding radiologists in interpreting DL-reconstructed MR images.",
    "metadata": {
      "arxiv_id": "2407.12184",
      "title": "The object detection method aids in image reconstruction evaluation and clinical interpretation of meniscal abnormalities",
      "summary": "This study investigates the relationship between deep learning (DL) image reconstruction quality and anomaly detection performance, and evaluates the efficacy of an artificial intelligence (AI) assistant in enhancing radiologists' interpretation of meniscal anomalies on reconstructed images. A retrospective study was conducted using an in-house reconstruction and anomaly detection pipeline to assess knee MR images from 896 patients. The original and 14 sets of DL-reconstructed images were evaluated using standard reconstruction and object detection metrics, alongside newly developed box-based reconstruction metrics. Two clinical radiologists reviewed a subset of 50 patients' images, both original and AI-assisted reconstructed, with subsequent assessment of their accuracy and performance characteristics. Results indicated that the structural similarity index (SSIM) showed a weaker correlation with anomaly detection metrics (mAP, r=0.64, p=0.01; F1 score, r=0.38, p=0.18), while box-based SSIM had a stronger association with detection performance (mAP, r=0.81, p<0.01; F1 score, r=0.65, p=0.01). Minor SSIM fluctuations did not affect detection outcomes, but significant changes reduced performance. Radiologists' AI-assisted evaluations demonstrated improved accuracy (86.0% without assistance vs. 88.3% with assistance, p<0.05) and interrater agreement (Cohen's kappa, 0.39 without assistance vs. 0.57 with assistance). An additional review led to the incorporation of 17 more lesions into the dataset. The proposed anomaly detection method shows promise in evaluating reconstruction algorithms for automated tasks and aiding radiologists in interpreting DL-reconstructed MR images.",
      "authors": [
        "Natalia Konovalova",
        "Aniket Tolpadi",
        "Felix Liu",
        "Zehra Akkaya",
        "Felix Gassert",
        "Paula Giesler",
        "Johanna Luitjens",
        "Misung Han",
        "Emma Bahroos",
        "Sharmila Majumdar",
        "Valentina Pedoia"
      ],
      "published": "2024-07-16T21:25:28Z",
      "updated": "2024-07-16T21:25:28Z",
      "categories": [
        "eess.IV",
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2407.12184v1",
      "landing_url": "https://arxiv.org/abs/2407.12184v1",
      "doi": "https://doi.org/10.48550/arXiv.2407.12184"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This item addresses AI-assisted interpretation of medical images rather than discrete audio/speech tokenization; it does not propose or analyze any discrete audio tokens or tokenization relevant to audio reconstruction or language-model-style tasks, so it should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "This item addresses AI-assisted interpretation of medical images rather than discrete audio/speech tokenization; it does not propose or analyze any discrete audio tokens or tokenization relevant to audio reconstruction or language-model-style tasks, so it should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on evaluating image reconstruction and anomaly detection in knee MR images using deep learning and AI assistance, with no mention of discrete audio/speech tokenization or related audio tasks as per the inclusion criteria. Therefore, it does not meet the core inclusion requirement of discrete audio tokenization and is irrelevant to the audio/speech tokenization scope.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on evaluating image reconstruction and anomaly detection in knee MR images using deep learning and AI assistance, with no mention of discrete audio/speech tokenization or related audio tasks as per the inclusion criteria. Therefore, it does not meet the core inclusion requirement of discrete audio tokenization and is irrelevant to the audio/speech tokenization scope.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Using Speech Foundational Models in Loss Functions for Hearing Aid Speech Enhancement",
    "abstract": "Machine learning techniques are an active area of research for speech enhancement for hearing aids, with one particular focus on improving the intelligibility of a noisy speech signal. Recent work has shown that feature encodings from self-supervised speech representation models can effectively capture speech intelligibility. In this work, it is shown that the distance between self-supervised speech representations of clean and noisy speech correlates more strongly with human intelligibility ratings than other signal-based metrics. Experiments show that training a speech enhancement model using this distance as part of a loss function improves the performance over using an SNR-based loss function, demonstrated by an increase in HASPI, STOI, PESQ and SI-SNR scores. This method takes inference of a high parameter count model only at training time, meaning the speech enhancement model can remain smaller, as is required for hearing aids.",
    "metadata": {
      "arxiv_id": "2407.13333",
      "title": "Using Speech Foundational Models in Loss Functions for Hearing Aid Speech Enhancement",
      "summary": "Machine learning techniques are an active area of research for speech enhancement for hearing aids, with one particular focus on improving the intelligibility of a noisy speech signal. Recent work has shown that feature encodings from self-supervised speech representation models can effectively capture speech intelligibility. In this work, it is shown that the distance between self-supervised speech representations of clean and noisy speech correlates more strongly with human intelligibility ratings than other signal-based metrics. Experiments show that training a speech enhancement model using this distance as part of a loss function improves the performance over using an SNR-based loss function, demonstrated by an increase in HASPI, STOI, PESQ and SI-SNR scores. This method takes inference of a high parameter count model only at training time, meaning the speech enhancement model can remain smaller, as is required for hearing aids.",
      "authors": [
        "Robert Sutherland",
        "George Close",
        "Thomas Hain",
        "Stefan Goetze",
        "Jon Barker"
      ],
      "published": "2024-07-18T09:32:57Z",
      "updated": "2024-07-18T09:32:57Z",
      "categories": [
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2407.13333v1",
      "landing_url": "https://arxiv.org/abs/2407.13333v1",
      "doi": "https://doi.org/10.48550/arXiv.2407.13333"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The article discusses using distances between self-supervised speech representations as a loss for speech enhancement, but does not propose or analyze discrete audio tokenization (quantized tokens) as the core object of study.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The article discusses using distances between self-supervised speech representations as a loss for speech enhancement, but does not propose or analyze discrete audio tokenization (quantized tokens) as the core object of study.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on using self-supervised speech representation models within loss functions for speech enhancement in hearing aids and evaluates improvements in intelligibility metrics. However, the work does not explicitly propose, analyze, or evaluate discrete audio/speech tokenization where a continuous audio signal is converted into discrete tokens for reconstruction or generation; rather, it uses continuous speech representations as features. Therefore, it does not meet the inclusion criteria related to discrete tokenization nor violate the exclusion criteria specifically but lacks the core tokenization focus.",
      "evaluation": 2
    },
    "round-A_JuniorMini_reasoning": "The study focuses on using self-supervised speech representation models within loss functions for speech enhancement in hearing aids and evaluates improvements in intelligibility metrics. However, the work does not explicitly propose, analyze, or evaluate discrete audio/speech tokenization where a continuous audio signal is converted into discrete tokens for reconstruction or generation; rather, it uses continuous speech representations as features. Therefore, it does not meet the inclusion criteria related to discrete tokenization nor violate the exclusion criteria specifically but lacks the core tokenization focus.",
    "round-A_JuniorMini_evaluation": 2,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:2)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:2)"
  },
  {
    "title": "X-Recon: Learning-based Patient-specific High-Resolution CT Reconstruction from Orthogonal X-Ray Images",
    "abstract": "Rapid and accurate diagnosis of pneumothorax, utilizing chest X-ray and computed tomography (CT), is crucial for assisted diagnosis. Chest X-ray is commonly used for initial localization of pneumothorax, while CT ensures accurate quantification. However, CT scans involve high radiation doses and can be costly. To achieve precise quantitative diagnosis while minimizing radiation exposure, we proposed X-Recon, a CT ultra-sparse reconstruction network based on ortho-lateral chest X-ray images. X-Recon integrates generative adversarial networks (GANs), including a generator with a multi-scale fusion rendering module and a discriminator enhanced by 3D coordinate convolutional layers, designed to facilitate CT reconstruction. To improve precision, a projective spatial transformer is utilized to incorporate multi-angle projection loss. Additionally, we proposed PTX-Seg, a zero-shot pneumothorax segmentation algorithm, combining image processing techniques with deep-learning models for the segmentation of air-accumulated regions and lung structures. Experiments on a large-scale dataset demonstrate its superiority over existing approaches. X-Recon achieved a significantly higher reconstruction resolution with a higher average spatial resolution and a lower average slice thickness. The reconstruction metrics achieved state-of-the-art performance in terms of several metrics including peak signal-to-noise ratio. The zero-shot segmentation algorithm, PTX-Seg, also demonstrated high segmentation precision for the air-accumulated region, the left lung, and the right lung. Moreover, the consistency analysis for the pneumothorax chest occupancy ratio between reconstructed CT and original CT obtained a high correlation coefficient. Code will be available at: https://github.com/wangyunpengbio/X-Recon",
    "metadata": {
      "arxiv_id": "2407.15356",
      "title": "X-Recon: Learning-based Patient-specific High-Resolution CT Reconstruction from Orthogonal X-Ray Images",
      "summary": "Rapid and accurate diagnosis of pneumothorax, utilizing chest X-ray and computed tomography (CT), is crucial for assisted diagnosis. Chest X-ray is commonly used for initial localization of pneumothorax, while CT ensures accurate quantification. However, CT scans involve high radiation doses and can be costly. To achieve precise quantitative diagnosis while minimizing radiation exposure, we proposed X-Recon, a CT ultra-sparse reconstruction network based on ortho-lateral chest X-ray images. X-Recon integrates generative adversarial networks (GANs), including a generator with a multi-scale fusion rendering module and a discriminator enhanced by 3D coordinate convolutional layers, designed to facilitate CT reconstruction. To improve precision, a projective spatial transformer is utilized to incorporate multi-angle projection loss. Additionally, we proposed PTX-Seg, a zero-shot pneumothorax segmentation algorithm, combining image processing techniques with deep-learning models for the segmentation of air-accumulated regions and lung structures. Experiments on a large-scale dataset demonstrate its superiority over existing approaches. X-Recon achieved a significantly higher reconstruction resolution with a higher average spatial resolution and a lower average slice thickness. The reconstruction metrics achieved state-of-the-art performance in terms of several metrics including peak signal-to-noise ratio. The zero-shot segmentation algorithm, PTX-Seg, also demonstrated high segmentation precision for the air-accumulated region, the left lung, and the right lung. Moreover, the consistency analysis for the pneumothorax chest occupancy ratio between reconstructed CT and original CT obtained a high correlation coefficient. Code will be available at: https://github.com/wangyunpengbio/X-Recon",
      "authors": [
        "Yunpeng Wang",
        "Kang Wang",
        "Yaoyao Zhuo",
        "Weiya Shi",
        "Fei Shan",
        "Lei Liu"
      ],
      "published": "2024-07-22T03:55:36Z",
      "updated": "2024-07-22T03:55:36Z",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2407.15356v1",
      "landing_url": "https://arxiv.org/abs/2407.15356v1",
      "doi": "https://doi.org/10.48550/arXiv.2407.15356"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This item focuses on CT reconstruction from orthogonal X-ray images with GAN-based methods and does not involve discrete audio/speech tokenization; it fails the specified inclusion criterion (discrete audio tokenization) and does not meet it or any exclusion related to audio tokens, so it should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "This item focuses on CT reconstruction from orthogonal X-ray images with GAN-based methods and does not involve discrete audio/speech tokenization; it fails the specified inclusion criterion (discrete audio tokenization) and does not meet it or any exclusion related to audio tokens, so it should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on CT image reconstruction from X-ray images using deep learning methods and segmentation for pneumothorax diagnosis; it does not mention any audio or speech signal processing, nor does it address discrete audio/speech tokenization, continuous audio features, or tokenization-related topics specified in the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on CT image reconstruction from X-ray images using deep learning methods and segmentation for pneumothorax diagnosis; it does not mention any audio or speech signal processing, nor does it address discrete audio/speech tokenization, continuous audio features, or tokenization-related topics specified in the inclusion criteria.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "EMO-Codec: An In-Depth Look at Emotion Preservation capacity of Legacy and Neural Codec Models With Subjective and Objective Evaluations",
    "abstract": "The neural codec model reduces speech data transmission delay and serves as the foundational tokenizer for speech language models (speech LMs). Preserving emotional information in codecs is crucial for effective communication and context understanding. However, there is a lack of studies on emotion loss in existing codecs. This paper evaluates neural and legacy codecs using subjective and objective methods on emotion datasets like IEMOCAP. Our study identifies which codecs best preserve emotional information under various bitrate scenarios. We found that training codec models with both English and Chinese data had limited success in retaining emotional information in Chinese. Additionally, resynthesizing speech through these codecs degrades the performance of speech emotion recognition (SER), particularly for emotions like sadness, depression, fear, and disgust. Human listening tests confirmed these findings. This work guides future speech technology developments to ensure new codecs maintain the integrity of emotional information in speech.",
    "metadata": {
      "arxiv_id": "2407.15458",
      "title": "EMO-Codec: An In-Depth Look at Emotion Preservation capacity of Legacy and Neural Codec Models With Subjective and Objective Evaluations",
      "summary": "The neural codec model reduces speech data transmission delay and serves as the foundational tokenizer for speech language models (speech LMs). Preserving emotional information in codecs is crucial for effective communication and context understanding. However, there is a lack of studies on emotion loss in existing codecs. This paper evaluates neural and legacy codecs using subjective and objective methods on emotion datasets like IEMOCAP. Our study identifies which codecs best preserve emotional information under various bitrate scenarios. We found that training codec models with both English and Chinese data had limited success in retaining emotional information in Chinese. Additionally, resynthesizing speech through these codecs degrades the performance of speech emotion recognition (SER), particularly for emotions like sadness, depression, fear, and disgust. Human listening tests confirmed these findings. This work guides future speech technology developments to ensure new codecs maintain the integrity of emotional information in speech.",
      "authors": [
        "Wenze Ren",
        "Yi-Cheng Lin",
        "Huang-Cheng Chou",
        "Haibin Wu",
        "Yi-Chiao Wu",
        "Chi-Chun Lee",
        "Hung-yi Lee",
        "Yu Tsao"
      ],
      "published": "2024-07-22T08:14:16Z",
      "updated": "2024-07-30T12:37:35Z",
      "categories": [
        "eess.AS",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2407.15458v4",
      "landing_url": "https://arxiv.org/abs/2407.15458v4",
      "doi": "https://doi.org/10.48550/arXiv.2407.15458"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The work focuses on evaluating emotion preservation in neural vs legacy codecs and does not explicitly analyze or evaluate discrete speech tokenization into a sequence of discrete symbols; it centers on codec performance rather than token-based representations, thus failing the core inclusion criterion for discrete audio tokenization.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The work focuses on evaluating emotion preservation in neural vs legacy codecs and does not explicitly analyze or evaluate discrete speech tokenization into a sequence of discrete symbols; it centers on codec performance rather than token-based representations, thus failing the core inclusion criterion for discrete audio tokenization.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on evaluating both neural and legacy codec models for their capacity to preserve emotional information in speech, which involves analyzing discrete audio tokenization methods as core objects of study, including quantization and discrete token representations relevant for speech language models; it also includes subjective and objective assessments, meeting the inclusion criteria and none of the exclusion criteria.",
      "evaluation": 5
    },
    "round-A_JuniorMini_reasoning": "The study focuses on evaluating both neural and legacy codec models for their capacity to preserve emotional information in speech, which involves analyzing discrete audio tokenization methods as core objects of study, including quantization and discrete token representations relevant for speech language models; it also includes subjective and objective assessments, meeting the inclusion criteria and none of the exclusion criteria.",
    "round-A_JuniorMini_evaluation": 5,
    "round-B_SeniorLead_output": {
      "reasoning": "JuniorNano judged exclusion, noting the paper centers on codec emotion preservation rather than discrete tokenization; JuniorMini judged inclusion, interpreting codecs as tokenizers for speech LMs. I reviewed the title and abstract: while they evaluate neural and legacy codecs and mention that neural codecs serve as \"foundational tokenizer for speech LMs,\" the abstract does not explicitly propose, analyze, or evaluate discrete audio/speech tokenization (e.g., quantization into symbol sequences, vocabulary design, token-level representations) as a core object—rather it focuses on emotion preservation, resynthesis effects, and subjective/objective emotion evaluations. Because the manuscript, as described in the abstract, lacks explicit analysis or methodological treatment of discrete tokenization (quantized token sequences) required by the inclusion criteria, it should be excluded. (One-sentence explanation: The abstract does not explicitly evaluate or propose discrete tokenization mechanisms—quantization into discrete symbols or token-sequence analysis—so it fails the inclusion criteria.)",
      "evaluation": 2
    },
    "round-B_SeniorLead_reasoning": "JuniorNano judged exclusion, noting the paper centers on codec emotion preservation rather than discrete tokenization; JuniorMini judged inclusion, interpreting codecs as tokenizers for speech LMs. I reviewed the title and abstract: while they evaluate neural and legacy codecs and mention that neural codecs serve as \"foundational tokenizer for speech LMs,\" the abstract does not explicitly propose, analyze, or evaluate discrete audio/speech tokenization (e.g., quantization into symbol sequences, vocabulary design, token-level representations) as a core object—rather it focuses on emotion preservation, resynthesis effects, and subjective/objective emotion evaluations. Because the manuscript, as described in the abstract, lacks explicit analysis or methodological treatment of discrete tokenization (quantized token sequences) required by the inclusion criteria, it should be excluded. (One-sentence explanation: The abstract does not explicitly evaluate or propose discrete tokenization mechanisms—quantization into discrete symbols or token-sequence analysis—so it fails the inclusion criteria.)",
    "round-B_SeniorLead_evaluation": 2,
    "final_verdict": "exclude (senior:2)",
    "review_skipped": false,
    "discard_reason": "exclude (senior:2)"
  },
  {
    "title": "dMel: Speech Tokenization made Simple",
    "abstract": "Large language models have revolutionized natural language processing by leveraging self-supervised pretraining on vast textual data. Inspired by this success, researchers have investigated various compression-based speech tokenization methods to discretize continuous speech signals, enabling the application of language modeling techniques to discrete tokens. However, audio compressor introduces additional complexity and computational cost, and often fail on out-of-domain audio signals. In this work, we introduce a novel speech representation (dmel) that discretizes mel-filterbank channels into intensity bins, creating a simpler yet more effective representation compared to existing speech tokenization methods. Our approach demonstrates superior performance in preserving audio content, robustness to out-of-domain data, and offers a training-free, natural, and streamable representation. To address the high-dimensional nature of log-mel spectrograms, we propose an efficient parallel encoding and decoding method for high-dimensional tokens using an LM-style transformer architecture. This innovation enables us to develop RichTTS and RichASR, two models sharing the same architecture while achieving comparable or better results than specialized existing methods. Our results demonstrate the effectiveness of dmel in achieving high performance on both speech synthesis and recognition tasks within a unified framework, paving the way for efficient and effective joint modeling of speech and text.",
    "metadata": {
      "arxiv_id": "2407.15835",
      "title": "dMel: Speech Tokenization made Simple",
      "summary": "Large language models have revolutionized natural language processing by leveraging self-supervised pretraining on vast textual data. Inspired by this success, researchers have investigated various compression-based speech tokenization methods to discretize continuous speech signals, enabling the application of language modeling techniques to discrete tokens. However, audio compressor introduces additional complexity and computational cost, and often fail on out-of-domain audio signals. In this work, we introduce a novel speech representation (dmel) that discretizes mel-filterbank channels into intensity bins, creating a simpler yet more effective representation compared to existing speech tokenization methods. Our approach demonstrates superior performance in preserving audio content, robustness to out-of-domain data, and offers a training-free, natural, and streamable representation. To address the high-dimensional nature of log-mel spectrograms, we propose an efficient parallel encoding and decoding method for high-dimensional tokens using an LM-style transformer architecture. This innovation enables us to develop RichTTS and RichASR, two models sharing the same architecture while achieving comparable or better results than specialized existing methods. Our results demonstrate the effectiveness of dmel in achieving high performance on both speech synthesis and recognition tasks within a unified framework, paving the way for efficient and effective joint modeling of speech and text.",
      "authors": [
        "Richard He Bai",
        "Tatiana Likhomanenko",
        "Ruixiang Zhang",
        "Zijin Gu",
        "Zakaria Aldeneh",
        "Navdeep Jaitly"
      ],
      "published": "2024-07-22T17:51:53Z",
      "updated": "2025-05-21T16:55:34Z",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2407.15835v3",
      "landing_url": "https://arxiv.org/abs/2407.15835v3",
      "doi": "https://doi.org/10.48550/arXiv.2407.15835"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The abstract presents a novel discrete speech tokenization (dmel) that discretizes continuous speech into discrete tokens and demonstrates its effectiveness for speech synthesis and recognition, meeting the inclusion criteria and not aligning with any exclusion criteria.",
      "evaluation": 5
    },
    "round-A_JuniorNano_reasoning": "The abstract presents a novel discrete speech tokenization (dmel) that discretizes continuous speech into discrete tokens and demonstrates its effectiveness for speech synthesis and recognition, meeting the inclusion criteria and not aligning with any exclusion criteria.",
    "round-A_JuniorNano_evaluation": 5,
    "round-A_JuniorMini_output": {
      "reasoning": "The study explicitly proposes a novel discrete speech tokenization method named dMel that discretizes mel-filterbank channels into intensity bins, aligns with the inclusion criteria focused on discrete audio tokenization for speech tasks, includes methodological innovations and experimental validation, and does not fall into any exclusion criteria such as purely continuous feature usage or engineering-only reports.",
      "evaluation": 5
    },
    "round-A_JuniorMini_reasoning": "The study explicitly proposes a novel discrete speech tokenization method named dMel that discretizes mel-filterbank channels into intensity bins, aligns with the inclusion criteria focused on discrete audio tokenization for speech tasks, includes methodological innovations and experimental validation, and does not fall into any exclusion criteria such as purely continuous feature usage or engineering-only reports.",
    "round-A_JuniorMini_evaluation": 5,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "include (junior:5)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "High Efficiency Image Compression for Large Visual-Language Models",
    "abstract": "In recent years, large visual language models (LVLMs) have shown impressive performance and promising generalization capability in multi-modal tasks, thus replacing humans as receivers of visual information in various application scenarios. In this paper, we pioneer to propose a variable bitrate image compression framework consisting of a pre-editing module and an end-to-end codec to achieve promising rate-accuracy performance for different LVLMs. In particular, instead of optimizing an adaptive pre-editing network towards a particular task or several representative tasks, we propose a new optimization strategy tailored for LVLMs, which is designed based on the representation and discrimination capability with token-level distortion and rank. The pre-editing module and the variable bitrate end-to-end image codec are jointly trained by the losses based on semantic tokens of the large model, which introduce enhanced generalization capability for various data and tasks. {Experimental results demonstrate that the proposed framework could efficiently achieve much better rate-accuracy performance compared to the state-of-the-art coding standard, Versatile Video Coding.} Meanwhile, experiments with multi-modal tasks have revealed the robustness and generalization capability of the proposed framework.",
    "metadata": {
      "arxiv_id": "2407.17060",
      "title": "High Efficiency Image Compression for Large Visual-Language Models",
      "summary": "In recent years, large visual language models (LVLMs) have shown impressive performance and promising generalization capability in multi-modal tasks, thus replacing humans as receivers of visual information in various application scenarios. In this paper, we pioneer to propose a variable bitrate image compression framework consisting of a pre-editing module and an end-to-end codec to achieve promising rate-accuracy performance for different LVLMs. In particular, instead of optimizing an adaptive pre-editing network towards a particular task or several representative tasks, we propose a new optimization strategy tailored for LVLMs, which is designed based on the representation and discrimination capability with token-level distortion and rank. The pre-editing module and the variable bitrate end-to-end image codec are jointly trained by the losses based on semantic tokens of the large model, which introduce enhanced generalization capability for various data and tasks. {Experimental results demonstrate that the proposed framework could efficiently achieve much better rate-accuracy performance compared to the state-of-the-art coding standard, Versatile Video Coding.} Meanwhile, experiments with multi-modal tasks have revealed the robustness and generalization capability of the proposed framework.",
      "authors": [
        "Binzhe Li",
        "Shurun Wang",
        "Shiqi Wang",
        "Yan Ye"
      ],
      "published": "2024-07-24T07:37:12Z",
      "updated": "2024-07-24T07:37:12Z",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CL",
        "eess.IV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2407.17060v1",
      "landing_url": "https://arxiv.org/abs/2407.17060v1",
      "doi": "https://doi.org/10.48550/arXiv.2407.17060"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The work centers on image compression and LVLM-specific semantic tokens, not on discrete audio/speech tokenization or phoneme/word-like discrete audio representations for speech tasks.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The work centers on image compression and LVLM-specific semantic tokens, not on discrete audio/speech tokenization or phoneme/word-like discrete audio representations for speech tasks.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on image compression for large visual-language models and does not address audio or speech tokenization; it neither proposes nor evaluates discrete audio tokens, which is a core inclusion criterion, thus it does not meet the inclusion criteria and falls under exclusion.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on image compression for large visual-language models and does not address audio or speech tokenization; it neither proposes nor evaluates discrete audio tokens, which is a core inclusion criterion, thus it does not meet the inclusion criteria and falls under exclusion.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Bailing-TTS: Chinese Dialectal Speech Synthesis Towards Human-like Spontaneous Representation",
    "abstract": "Large-scale text-to-speech (TTS) models have made significant progress recently.However, they still fall short in the generation of Chinese dialectal speech. Toaddress this, we propose Bailing-TTS, a family of large-scale TTS models capable of generating high-quality Chinese dialectal speech. Bailing-TTS serves as a foundation model for Chinese dialectal speech generation. First, continual semi-supervised learning is proposed to facilitate the alignment of text tokens and speech tokens. Second, the Chinese dialectal representation learning is developed using a specific transformer architecture and multi-stage training processes. With the proposed design of novel network architecture and corresponding strategy, Bailing-TTS is able to generate Chinese dialectal speech from text effectively and efficiently. Experiments demonstrate that Bailing-TTS generates Chinese dialectal speech towards human-like spontaneous representation. Readers are encouraged to listen to demos at \\url{https://c9412600.github.io/bltts_tech_report/index.html}.",
    "metadata": {
      "arxiv_id": "2408.00284",
      "title": "Bailing-TTS: Chinese Dialectal Speech Synthesis Towards Human-like Spontaneous Representation",
      "summary": "Large-scale text-to-speech (TTS) models have made significant progress recently.However, they still fall short in the generation of Chinese dialectal speech. Toaddress this, we propose Bailing-TTS, a family of large-scale TTS models capable of generating high-quality Chinese dialectal speech. Bailing-TTS serves as a foundation model for Chinese dialectal speech generation. First, continual semi-supervised learning is proposed to facilitate the alignment of text tokens and speech tokens. Second, the Chinese dialectal representation learning is developed using a specific transformer architecture and multi-stage training processes. With the proposed design of novel network architecture and corresponding strategy, Bailing-TTS is able to generate Chinese dialectal speech from text effectively and efficiently. Experiments demonstrate that Bailing-TTS generates Chinese dialectal speech towards human-like spontaneous representation. Readers are encouraged to listen to demos at \\url{https://c9412600.github.io/bltts_tech_report/index.html}.",
      "authors": [
        "Xinhan Di",
        "Zihao Chen",
        "Yunming Liang",
        "Junjie Zheng",
        "Yihua Wang",
        "Chaofan Ding"
      ],
      "published": "2024-08-01T04:57:31Z",
      "updated": "2024-08-01T04:57:31Z",
      "categories": [
        "cs.CL",
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2408.00284v1",
      "landing_url": "https://arxiv.org/abs/2408.00284v1",
      "doi": "https://doi.org/10.48550/arXiv.2408.00284"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The abstract mentions text and speech tokens and their alignment, but it does not explicitly analyze or evaluate discrete audio/speech tokenization as a core contribution, so it is not clearly meeting the inclusion criteria.",
      "evaluation": 3
    },
    "round-A_JuniorNano_reasoning": "The abstract mentions text and speech tokens and their alignment, but it does not explicitly analyze or evaluate discrete audio/speech tokenization as a core contribution, so it is not clearly meeting the inclusion criteria.",
    "round-A_JuniorNano_evaluation": 3,
    "round-A_JuniorMini_output": {
      "reasoning": "The abstract describes Bailing-TTS, a TTS model for Chinese dialectal speech generation, focusing on continuous text-to-speech synthesis improvements without explicit mention or analysis of discrete audio/speech tokenization or discrete symbol sequences as a core study object, which is required by the inclusion criteria.",
      "evaluation": 2
    },
    "round-A_JuniorMini_reasoning": "The abstract describes Bailing-TTS, a TTS model for Chinese dialectal speech generation, focusing on continuous text-to-speech synthesis improvements without explicit mention or analysis of discrete audio/speech tokenization or discrete symbol sequences as a core study object, which is required by the inclusion criteria.",
    "round-A_JuniorMini_evaluation": 2,
    "round-B_SeniorLead_output": {
      "reasoning": "I reviewed the two junior reviewers' comments (scores 3 and 2) and the abstract: although it mentions aligning \"text tokens and speech tokens,\" the paper appears focused on TTS architecture and training for Chinese dialects and does not explicitly propose, analyze, or evaluate a discrete audio/speech tokenization scheme (no description of quantization, discrete vocabularies, or tokenization methodology); therefore it does not meet the inclusion criteria. Score justification (one sentence): the abstract lacks explicit design or analysis of discrete audio tokenization, so it should be excluded.",
      "evaluation": 2
    },
    "round-B_SeniorLead_reasoning": "I reviewed the two junior reviewers' comments (scores 3 and 2) and the abstract: although it mentions aligning \"text tokens and speech tokens,\" the paper appears focused on TTS architecture and training for Chinese dialects and does not explicitly propose, analyze, or evaluate a discrete audio/speech tokenization scheme (no description of quantization, discrete vocabularies, or tokenization methodology); therefore it does not meet the inclusion criteria. Score justification (one sentence): the abstract lacks explicit design or analysis of discrete audio tokenization, so it should be excluded.",
    "round-B_SeniorLead_evaluation": 2,
    "final_verdict": "exclude (senior:2)",
    "review_skipped": false,
    "discard_reason": "exclude (senior:2)"
  },
  {
    "title": "FT K-means: A High-Performance K-means on GPU with Fault Tolerance",
    "abstract": "K-means is a widely used algorithm in clustering, however, its efficiency is primarily constrained by the computational cost of distance computing. Existing implementations suffer from suboptimal utilization of computational units and lack resilience against soft errors. To address these challenges, we introduce FT K-means, a high-performance GPU-accelerated implementation of K-means with online fault tolerance. We first present a stepwise optimization strategy that achieves competitive performance compared to NVIDIA's cuML library. We further improve FT K-means with a template-based code generation framework that supports different data types and adapts to different input shapes. A novel warp-level tensor-core error correction scheme is proposed to address the failure of existing fault tolerance methods due to memory asynchronization during copy operations. Our experimental evaluations on NVIDIA T4 GPU and A100 GPU demonstrate that FT K-means without fault tolerance outperforms cuML's K-means implementation, showing a performance increase of 10\\%-300\\% in scenarios involving irregular data shapes. Moreover, the fault tolerance feature of FT K-means introduces only an overhead of 11\\%, maintaining robust performance even with tens of errors injected per second.",
    "metadata": {
      "arxiv_id": "2408.01391",
      "title": "FT K-means: A High-Performance K-means on GPU with Fault Tolerance",
      "summary": "K-means is a widely used algorithm in clustering, however, its efficiency is primarily constrained by the computational cost of distance computing. Existing implementations suffer from suboptimal utilization of computational units and lack resilience against soft errors. To address these challenges, we introduce FT K-means, a high-performance GPU-accelerated implementation of K-means with online fault tolerance. We first present a stepwise optimization strategy that achieves competitive performance compared to NVIDIA's cuML library. We further improve FT K-means with a template-based code generation framework that supports different data types and adapts to different input shapes. A novel warp-level tensor-core error correction scheme is proposed to address the failure of existing fault tolerance methods due to memory asynchronization during copy operations. Our experimental evaluations on NVIDIA T4 GPU and A100 GPU demonstrate that FT K-means without fault tolerance outperforms cuML's K-means implementation, showing a performance increase of 10\\%-300\\% in scenarios involving irregular data shapes. Moreover, the fault tolerance feature of FT K-means introduces only an overhead of 11\\%, maintaining robust performance even with tens of errors injected per second.",
      "authors": [
        "Shixun Wu",
        "Yitong Ding",
        "Yujia Zhai",
        "Jinyang Liu",
        "Jiajun Huang",
        "Zizhe Jian",
        "Huangliang Dai",
        "Sheng Di",
        "Bryan M. Wong",
        "Zizhong Chen",
        "Franck Cappello"
      ],
      "published": "2024-08-02T17:01:36Z",
      "updated": "2024-08-07T21:55:08Z",
      "categories": [
        "cs.DC",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2408.01391v2",
      "landing_url": "https://arxiv.org/abs/2408.01391v2",
      "doi": "https://doi.org/10.48550/arXiv.2408.01391"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The title/abstract describes a GPU-accelerated K-means algorithm with fault tolerance and does not address discrete audio/speech tokenization, so it does not meet the inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The title/abstract describes a GPU-accelerated K-means algorithm with fault tolerance and does not address discrete audio/speech tokenization, so it does not meet the inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on a high-performance K-means implementation on GPUs with fault tolerance, but it does not address discrete audio/speech tokenization or discrete token design related to audio; instead, it is an engineering optimization work on clustering algorithms without relevance to audio tokenization.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on a high-performance K-means implementation on GPUs with fault tolerance, but it does not address discrete audio/speech tokenization or discrete token design related to audio; instead, it is an engineering optimization work on clustering algorithms without relevance to audio tokenization.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "High-Efficiency Urban 3D Radio Map Estimation Based on Sparse Measurements",
    "abstract": "Recent widespread applications for unmanned aerial vehicles (UAVs) -- from infrastructure inspection to urban logistics -- have prompted an urgent need for high-accuracy three-dimensional (3D) radio maps. However, existing methods designed for two-dimensional radio maps face challenges of high measurement costs and limited data availability when extended to 3D scenarios. To tackle these challenges, we first build a real-world large-scale 3D radio map dataset, covering over 4.2 million m^3 and over 4 thousand data points in complex urban environments. We propose a Gaussian Process Regression-based scheme for 3D radio map estimation, allowing us to realize more accurate map recovery with a lower RMSE than state-of-the-art schemes by over 2.5 dB. To further enhance data efficiency, we propose two methods for training point selection, including an offline clustering-based method and an online maximum a posterior (MAP)-based method. Extensive experiments demonstrate that the proposed scheme not only achieves full-map recovery with only 2% of UAV measurements, but also sheds light on future studies on 3D radio maps.",
    "metadata": {
      "arxiv_id": "2408.04205",
      "title": "High-Efficiency Urban 3D Radio Map Estimation Based on Sparse Measurements",
      "summary": "Recent widespread applications for unmanned aerial vehicles (UAVs) -- from infrastructure inspection to urban logistics -- have prompted an urgent need for high-accuracy three-dimensional (3D) radio maps. However, existing methods designed for two-dimensional radio maps face challenges of high measurement costs and limited data availability when extended to 3D scenarios. To tackle these challenges, we first build a real-world large-scale 3D radio map dataset, covering over 4.2 million m^3 and over 4 thousand data points in complex urban environments. We propose a Gaussian Process Regression-based scheme for 3D radio map estimation, allowing us to realize more accurate map recovery with a lower RMSE than state-of-the-art schemes by over 2.5 dB. To further enhance data efficiency, we propose two methods for training point selection, including an offline clustering-based method and an online maximum a posterior (MAP)-based method. Extensive experiments demonstrate that the proposed scheme not only achieves full-map recovery with only 2% of UAV measurements, but also sheds light on future studies on 3D radio maps.",
      "authors": [
        "Xinwei Chen",
        "Xiaofeng Zhong",
        "Zijian Zhang",
        "Linglong Dai",
        "Shidong Zhou"
      ],
      "published": "2024-08-08T04:05:18Z",
      "updated": "2024-08-08T04:05:18Z",
      "categories": [
        "cs.IT"
      ],
      "pdf_url": "https://arxiv.org/pdf/2408.04205v1",
      "landing_url": "https://arxiv.org/abs/2408.04205v1",
      "doi": "https://doi.org/10.48550/arXiv.2408.04205"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item concerns 3D radio map estimation using UAVs and Gaussian Process regression; there is no discussion of discrete audio/speech tokenization or token-level representation, so it fails the audio-tokenization inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item concerns 3D radio map estimation using UAVs and Gaussian Process regression; there is no discussion of discrete audio/speech tokenization or token-level representation, so it fails the audio-tokenization inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on 3D radio map estimation using UAV measurements and Gaussian Process Regression methods, which is unrelated to audio or speech tokenization; it does not address converting continuous audio signals into discrete tokens or any discrete audio tokenization methods, hence it does not meet the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on 3D radio map estimation using UAV measurements and Gaussian Process Regression methods, which is unrelated to audio or speech tokenization; it does not address converting continuous audio signals into discrete tokens or any discrete audio tokenization methods, hence it does not meet the inclusion criteria.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "CMD: A Cache-assisted GPU Memory Deduplication Architecture",
    "abstract": "Massive off-chip accesses in GPUs are the main performance bottleneck, and we divided these accesses into three types: (1) Write, (2) Data-Read, and (3) Read-Only. Besides, We find that many writes are duplicate, and the duplication can be inter-dup and intra-dup. While inter-dup means different memory blocks are identical, and intra-dup means all the 4B elements in a line are the same. In this work, we propose a cache-assisted GPU memory deduplication architecture named CMD to reduce the off-chip accesses via utilizing the data duplication in GPU applications. CMD includes three key design contributions which aim to reduce the three kinds of accesses: (1) A novel GPU memory deduplication architecture that removes the inter-dup and inter-dup lines. As for the inter-dup detection, we reduce the extra read requests caused by the traditional read-verify hash process. Besides, we design several techniques to manage duplicate blocks. (2) We propose a cache-assisted read scheme to reduce the reads to duplicate data. When an L2 cache miss wants to read the duplicate block, if the reference block has been fetched to L2 and it is clean, we can copy it to the L2 missed block without accessing off-chip DRAM. As for the reads to intra-dup data, CMD uses the on-chip metadata cache to get the data. (3) When a cache line is evicted, the clean sectors in the line are invalidated while the dirty sectors are written back. However, most read-only victims are re-referenced from DRAM more than twice. Therefore, we add a full-associate FIFO to accommodate the read-only (it is also clean) victims to reduce the re-reference counts. Experiments show that CMD can decrease the off-chip accesses by 31.01%, reduce the energy by 32.78% and improve performance by 37.79%. Besides, CMD can improve the performance of memory-intensive workloads by 50.18%.",
    "metadata": {
      "arxiv_id": "2408.09483",
      "title": "CMD: A Cache-assisted GPU Memory Deduplication Architecture",
      "summary": "Massive off-chip accesses in GPUs are the main performance bottleneck, and we divided these accesses into three types: (1) Write, (2) Data-Read, and (3) Read-Only. Besides, We find that many writes are duplicate, and the duplication can be inter-dup and intra-dup. While inter-dup means different memory blocks are identical, and intra-dup means all the 4B elements in a line are the same. In this work, we propose a cache-assisted GPU memory deduplication architecture named CMD to reduce the off-chip accesses via utilizing the data duplication in GPU applications. CMD includes three key design contributions which aim to reduce the three kinds of accesses: (1) A novel GPU memory deduplication architecture that removes the inter-dup and inter-dup lines. As for the inter-dup detection, we reduce the extra read requests caused by the traditional read-verify hash process. Besides, we design several techniques to manage duplicate blocks. (2) We propose a cache-assisted read scheme to reduce the reads to duplicate data. When an L2 cache miss wants to read the duplicate block, if the reference block has been fetched to L2 and it is clean, we can copy it to the L2 missed block without accessing off-chip DRAM. As for the reads to intra-dup data, CMD uses the on-chip metadata cache to get the data. (3) When a cache line is evicted, the clean sectors in the line are invalidated while the dirty sectors are written back. However, most read-only victims are re-referenced from DRAM more than twice. Therefore, we add a full-associate FIFO to accommodate the read-only (it is also clean) victims to reduce the re-reference counts. Experiments show that CMD can decrease the off-chip accesses by 31.01%, reduce the energy by 32.78% and improve performance by 37.79%. Besides, CMD can improve the performance of memory-intensive workloads by 50.18%.",
      "authors": [
        "Wei Zhao",
        "Dan Feng",
        "Wei Tong",
        "Xueliang Wei",
        "Bing Wu"
      ],
      "published": "2024-08-18T13:54:46Z",
      "updated": "2024-08-18T13:54:46Z",
      "categories": [
        "cs.AR"
      ],
      "pdf_url": "https://arxiv.org/pdf/2408.09483v1",
      "landing_url": "https://arxiv.org/abs/2408.09483v1",
      "doi": "https://doi.org/10.48550/arXiv.2408.09483"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item describes a cache-assisted GPU memory deduplication architecture and off-chip access reduction, which is a hardware/memory systems topic, not a study of discrete audio/speech tokenization or token-based audio representations; thus it does not meet the inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item describes a cache-assisted GPU memory deduplication architecture and off-chip access reduction, which is a hardware/memory systems topic, not a study of discrete audio/speech tokenization or token-based audio representations; thus it does not meet the inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on a GPU memory deduplication architecture to reduce off-chip memory accesses and improve performance, with no mention of discrete audio/speech tokenization, quantization, or audio signal processing; it does not meet any inclusion criteria related to audio tokenization and falls outside the scope defined by the inclusion/exclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on a GPU memory deduplication architecture to reduce off-chip memory accesses and improve performance, with no mention of discrete audio/speech tokenization, quantization, or audio signal processing; it does not meet any inclusion criteria related to audio tokenization and falls outside the scope defined by the inclusion/exclusion criteria.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Non-Causal to Causal SSL-Supported Transfer Learning: Towards a High-Performance Low-Latency Speech Vocoder",
    "abstract": "Recently, BigVGAN has emerged as high-performance speech vocoder. Its sequence-to-sequence-based synthesis, however, prohibits usage in low-latency conversational applications. Our work addresses this shortcoming in three steps. First, we introduce low latency into BigVGAN via implementing causal convolutions, yielding decreased performance. Second, to regain performance, we propose a teacher-student transfer learning scheme to distill the high-delay non-causal BigVGAN into our low-latency causal vocoder. Third, taking advantage of a self-supervised learning (SSL) model, in our case wav2vec 2.0, we align its encoder speech representations extracted from our low-latency causal vocoder to the ground truth ones. In speaker-independent settings, both proposed training schemes notably elevate the performance of our low-latency vocoder, closing up to the original high-delay BigVGAN. At only 21% higher complexity, our best small causal vocoder achieves 3.96 PESQ and 1.25 MCD, excelling even the original small non-causal BigVGAN (3.64 PESQ) by 0.32 PESQ and 0.1 MCD points, respectively.",
    "metadata": {
      "arxiv_id": "2408.11842",
      "title": "Non-Causal to Causal SSL-Supported Transfer Learning: Towards a High-Performance Low-Latency Speech Vocoder",
      "summary": "Recently, BigVGAN has emerged as high-performance speech vocoder. Its sequence-to-sequence-based synthesis, however, prohibits usage in low-latency conversational applications. Our work addresses this shortcoming in three steps. First, we introduce low latency into BigVGAN via implementing causal convolutions, yielding decreased performance. Second, to regain performance, we propose a teacher-student transfer learning scheme to distill the high-delay non-causal BigVGAN into our low-latency causal vocoder. Third, taking advantage of a self-supervised learning (SSL) model, in our case wav2vec 2.0, we align its encoder speech representations extracted from our low-latency causal vocoder to the ground truth ones. In speaker-independent settings, both proposed training schemes notably elevate the performance of our low-latency vocoder, closing up to the original high-delay BigVGAN. At only 21% higher complexity, our best small causal vocoder achieves 3.96 PESQ and 1.25 MCD, excelling even the original small non-causal BigVGAN (3.64 PESQ) by 0.32 PESQ and 0.1 MCD points, respectively.",
      "authors": [
        "Renzheng Shi",
        "Andreas Bär",
        "Marvin Sach",
        "Wouter Tirry",
        "Tim Fingscheidt"
      ],
      "published": "2024-08-07T12:49:40Z",
      "updated": "2024-08-26T12:01:07Z",
      "categories": [
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2408.11842v2",
      "landing_url": "https://arxiv.org/abs/2408.11842v2",
      "doi": "https://doi.org/10.48550/arXiv.2408.11842"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The work is about causal vocoders, transfer learning, and SSL alignment for waveform generation, with no discrete audio tokenization or token-based representations, so it does not meet the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The work is about causal vocoders, transfer learning, and SSL alignment for waveform generation, with no discrete audio tokenization or token-based representations, so it does not meet the inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on improving a speech vocoder's latency and performance through causal convolutions and transfer learning, but it does not explicitly propose, analyze, or evaluate discrete audio/speech tokenization or discrete token representations as a core object; instead, it seems to operate mainly on continuous representations and vocoder architectures without discrete quantization analysis, thus it does not meet the core inclusion criteria requiring discrete tokenization focus.",
      "evaluation": 2
    },
    "round-A_JuniorMini_reasoning": "The study focuses on improving a speech vocoder's latency and performance through causal convolutions and transfer learning, but it does not explicitly propose, analyze, or evaluate discrete audio/speech tokenization or discrete token representations as a core object; instead, it seems to operate mainly on continuous representations and vocoder architectures without discrete quantization analysis, thus it does not meet the core inclusion criteria requiring discrete tokenization focus.",
    "round-A_JuniorMini_evaluation": 2,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:2)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:2)"
  },
  {
    "title": "Enabling Beam Search for Language Model-Based Text-to-Speech Synthesis",
    "abstract": "Tokenising continuous speech into sequences of discrete tokens and modelling them with language models (LMs) has led to significant success in text-to-speech (TTS) synthesis. Although these models can generate speech with high quality and naturalness, their synthesised samples can still suffer from artefacts, mispronunciation, word repeating, etc. In this paper, we argue these undesirable properties could partly be caused by the randomness of sampling-based strategies during the autoregressive decoding of LMs. Therefore, we look at maximisation-based decoding approaches and propose Temporal Repetition Aware Diverse Beam Search (TRAD-BS) to find the most probable sequences of the generated speech tokens. Experiments with two state-of-the-art LM-based TTS models demonstrate that our proposed maximisation-based decoding strategy generates speech with fewer mispronunciations and improved speaker consistency.",
    "metadata": {
      "arxiv_id": "2408.16373",
      "title": "Enabling Beam Search for Language Model-Based Text-to-Speech Synthesis",
      "summary": "Tokenising continuous speech into sequences of discrete tokens and modelling them with language models (LMs) has led to significant success in text-to-speech (TTS) synthesis. Although these models can generate speech with high quality and naturalness, their synthesised samples can still suffer from artefacts, mispronunciation, word repeating, etc. In this paper, we argue these undesirable properties could partly be caused by the randomness of sampling-based strategies during the autoregressive decoding of LMs. Therefore, we look at maximisation-based decoding approaches and propose Temporal Repetition Aware Diverse Beam Search (TRAD-BS) to find the most probable sequences of the generated speech tokens. Experiments with two state-of-the-art LM-based TTS models demonstrate that our proposed maximisation-based decoding strategy generates speech with fewer mispronunciations and improved speaker consistency.",
      "authors": [
        "Zehai Tu",
        "Guangyan Zhang",
        "Yiting Lu",
        "Adaeze Adigwe",
        "Simon King",
        "Yiwen Guo"
      ],
      "published": "2024-08-29T09:31:06Z",
      "updated": "2024-08-29T09:31:06Z",
      "categories": [
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2408.16373v1",
      "landing_url": "https://arxiv.org/abs/2408.16373v1",
      "doi": "https://doi.org/10.48550/arXiv.2408.16373"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The abstract explicitly discusses tokenising continuous speech into discrete tokens for LM-based TTS and proposes a decoding strategy over these discrete token sequences, meeting the discrete-tokenization inclusion criterion and not focusing on continuous features alone.",
      "evaluation": 5
    },
    "round-A_JuniorNano_reasoning": "The abstract explicitly discusses tokenising continuous speech into discrete tokens for LM-based TTS and proposes a decoding strategy over these discrete token sequences, meeting the discrete-tokenization inclusion criterion and not focusing on continuous features alone.",
    "round-A_JuniorNano_evaluation": 5,
    "round-A_JuniorMini_output": {
      "reasoning": "The study explicitly discusses tokenizing continuous speech into sequences of discrete tokens and models them with language models for TTS, which directly aligns with the inclusion criterion of discrete audio/speech tokenization as a core object of study; the focus is on addressing issues in autoregressive decoding strategies for LM-based TTS models, which shows methodological contribution and experimental evaluation, and there is no indication of exclusion criteria being met.",
      "evaluation": 5
    },
    "round-A_JuniorMini_reasoning": "The study explicitly discusses tokenizing continuous speech into sequences of discrete tokens and models them with language models for TTS, which directly aligns with the inclusion criterion of discrete audio/speech tokenization as a core object of study; the focus is on addressing issues in autoregressive decoding strategies for LM-based TTS models, which shows methodological contribution and experimental evaluation, and there is no indication of exclusion criteria being met.",
    "round-A_JuniorMini_evaluation": 5,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "include (junior:5)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "VQ4DiT: Efficient Post-Training Vector Quantization for Diffusion Transformers",
    "abstract": "The Diffusion Transformers Models (DiTs) have transitioned the network architecture from traditional UNets to transformers, demonstrating exceptional capabilities in image generation. Although DiTs have been widely applied to high-definition video generation tasks, their large parameter size hinders inference on edge devices. Vector quantization (VQ) can decompose model weight into a codebook and assignments, allowing extreme weight quantization and significantly reducing memory usage. In this paper, we propose VQ4DiT, a fast post-training vector quantization method for DiTs. We found that traditional VQ methods calibrate only the codebook without calibrating the assignments. This leads to weight sub-vectors being incorrectly assigned to the same assignment, providing inconsistent gradients to the codebook and resulting in a suboptimal result. To address this challenge, VQ4DiT calculates the candidate assignment set for each weight sub-vector based on Euclidean distance and reconstructs the sub-vector based on the weighted average. Then, using the zero-data and block-wise calibration method, the optimal assignment from the set is efficiently selected while calibrating the codebook. VQ4DiT quantizes a DiT XL/2 model on a single NVIDIA A100 GPU within 20 minutes to 5 hours depending on the different quantization settings. Experiments show that VQ4DiT establishes a new state-of-the-art in model size and performance trade-offs, quantizing weights to 2-bit precision while retaining acceptable image generation quality.",
    "metadata": {
      "arxiv_id": "2408.17131",
      "title": "VQ4DiT: Efficient Post-Training Vector Quantization for Diffusion Transformers",
      "summary": "The Diffusion Transformers Models (DiTs) have transitioned the network architecture from traditional UNets to transformers, demonstrating exceptional capabilities in image generation. Although DiTs have been widely applied to high-definition video generation tasks, their large parameter size hinders inference on edge devices. Vector quantization (VQ) can decompose model weight into a codebook and assignments, allowing extreme weight quantization and significantly reducing memory usage. In this paper, we propose VQ4DiT, a fast post-training vector quantization method for DiTs. We found that traditional VQ methods calibrate only the codebook without calibrating the assignments. This leads to weight sub-vectors being incorrectly assigned to the same assignment, providing inconsistent gradients to the codebook and resulting in a suboptimal result. To address this challenge, VQ4DiT calculates the candidate assignment set for each weight sub-vector based on Euclidean distance and reconstructs the sub-vector based on the weighted average. Then, using the zero-data and block-wise calibration method, the optimal assignment from the set is efficiently selected while calibrating the codebook. VQ4DiT quantizes a DiT XL/2 model on a single NVIDIA A100 GPU within 20 minutes to 5 hours depending on the different quantization settings. Experiments show that VQ4DiT establishes a new state-of-the-art in model size and performance trade-offs, quantizing weights to 2-bit precision while retaining acceptable image generation quality.",
      "authors": [
        "Juncan Deng",
        "Shuaiting Li",
        "Zeyu Wang",
        "Hong Gu",
        "Kedong Xu",
        "Kejie Huang"
      ],
      "published": "2024-08-30T09:15:54Z",
      "updated": "2024-08-30T09:15:54Z",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2408.17131v1",
      "landing_url": "https://arxiv.org/abs/2408.17131v1",
      "doi": "https://doi.org/10.48550/arXiv.2408.17131"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The work targets discrete weight quantization for diffusion transformer models in image/video domains, not any discrete audio/speech tokenization or analysis.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The work targets discrete weight quantization for diffusion transformer models in image/video domains, not any discrete audio/speech tokenization or analysis.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on vector quantization methods for Diffusion Transformers in image and video generation, without addressing discrete audio or speech tokenization or proposing any method relevant to audio signal processing or discrete audio tokens, thus it does not meet the inclusion criteria related to audio/speech tokenization.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on vector quantization methods for Diffusion Transformers in image and video generation, without addressing discrete audio or speech tokenization or proposing any method relevant to audio signal processing or discrete audio tokens, thus it does not meet the inclusion criteria related to audio/speech tokenization.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Codec Does Matter: Exploring the Semantic Shortcoming of Codec for Audio Language Model",
    "abstract": "Recent advancements in audio generation have been significantly propelled by the capabilities of Large Language Models (LLMs). The existing research on audio LLM has primarily focused on enhancing the architecture and scale of audio language models, as well as leveraging larger datasets, and generally, acoustic codecs, such as EnCodec, are used for audio tokenization. However, these codecs were originally designed for audio compression, which may lead to suboptimal performance in the context of audio LLM. Our research aims to address the shortcomings of current audio LLM codecs, particularly their challenges in maintaining semantic integrity in generated audio. For instance, existing methods like VALL-E, which condition acoustic token generation on text transcriptions, often suffer from content inaccuracies and elevated word error rates (WER) due to semantic misinterpretations of acoustic tokens, resulting in word skipping and errors. To overcome these issues, we propose a straightforward yet effective approach called X-Codec. X-Codec incorporates semantic features from a pre-trained semantic encoder before the Residual Vector Quantization (RVQ) stage and introduces a semantic reconstruction loss after RVQ. By enhancing the semantic ability of the codec, X-Codec significantly reduces WER in speech synthesis tasks and extends these benefits to non-speech applications, including music and sound generation. Our experiments in text-to-speech, music continuation, and text-to-sound tasks demonstrate that integrating semantic information substantially improves the overall performance of language models in audio generation. Our code and demo are available (Demo: https://x-codec-audio.github.io Code: https://github.com/zhenye234/xcodec)",
    "metadata": {
      "arxiv_id": "2408.17175",
      "title": "Codec Does Matter: Exploring the Semantic Shortcoming of Codec for Audio Language Model",
      "summary": "Recent advancements in audio generation have been significantly propelled by the capabilities of Large Language Models (LLMs). The existing research on audio LLM has primarily focused on enhancing the architecture and scale of audio language models, as well as leveraging larger datasets, and generally, acoustic codecs, such as EnCodec, are used for audio tokenization. However, these codecs were originally designed for audio compression, which may lead to suboptimal performance in the context of audio LLM. Our research aims to address the shortcomings of current audio LLM codecs, particularly their challenges in maintaining semantic integrity in generated audio. For instance, existing methods like VALL-E, which condition acoustic token generation on text transcriptions, often suffer from content inaccuracies and elevated word error rates (WER) due to semantic misinterpretations of acoustic tokens, resulting in word skipping and errors. To overcome these issues, we propose a straightforward yet effective approach called X-Codec. X-Codec incorporates semantic features from a pre-trained semantic encoder before the Residual Vector Quantization (RVQ) stage and introduces a semantic reconstruction loss after RVQ. By enhancing the semantic ability of the codec, X-Codec significantly reduces WER in speech synthesis tasks and extends these benefits to non-speech applications, including music and sound generation. Our experiments in text-to-speech, music continuation, and text-to-sound tasks demonstrate that integrating semantic information substantially improves the overall performance of language models in audio generation. Our code and demo are available (Demo: https://x-codec-audio.github.io Code: https://github.com/zhenye234/xcodec)",
      "authors": [
        "Zhen Ye",
        "Peiwen Sun",
        "Jiahe Lei",
        "Hongzhan Lin",
        "Xu Tan",
        "Zheqi Dai",
        "Qiuqiang Kong",
        "Jianyi Chen",
        "Jiahao Pan",
        "Qifeng Liu",
        "Yike Guo",
        "Wei Xue"
      ],
      "published": "2024-08-30T10:24:07Z",
      "updated": "2024-11-27T11:47:45Z",
      "categories": [
        "eess.AS",
        "cs.AI",
        "cs.CL",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2408.17175v3",
      "landing_url": "https://arxiv.org/abs/2408.17175v3",
      "doi": "https://doi.org/10.48550/arXiv.2408.17175"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "Explicitly focuses on discrete audio tokenization via a semantic-aware codec (X-Codec) with RVQ and semantic reconstruction, evaluates on speech and non-speech tasks, and discusses improvements in token-based audio generation; meets inclusion and not excluded.",
      "evaluation": 5
    },
    "round-A_JuniorNano_reasoning": "Explicitly focuses on discrete audio tokenization via a semantic-aware codec (X-Codec) with RVQ and semantic reconstruction, evaluates on speech and non-speech tasks, and discusses improvements in token-based audio generation; meets inclusion and not excluded.",
    "round-A_JuniorNano_evaluation": 5,
    "round-A_JuniorMini_output": {
      "reasoning": "The study explicitly proposes and evaluates a novel codec, X-Codec, which integrates semantic features into the discrete tokenization process for audio, focusing on speech synthesis and other audio generation tasks; it addresses the semantic shortcomings of current discrete audio tokenizations and provides experimental validation, fully meeting the inclusion criteria and not falling into any exclusion criteria.",
      "evaluation": 5
    },
    "round-A_JuniorMini_reasoning": "The study explicitly proposes and evaluates a novel codec, X-Codec, which integrates semantic features into the discrete tokenization process for audio, focusing on speech synthesis and other audio generation tasks; it addresses the semantic shortcomings of current discrete audio tokenizations and provides experimental validation, fully meeting the inclusion criteria and not falling into any exclusion criteria.",
    "round-A_JuniorMini_evaluation": 5,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "include (junior:5)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Self-supervised learning for crystal property prediction via denoising",
    "abstract": "Accurate prediction of the properties of crystalline materials is crucial for targeted discovery, and this prediction is increasingly done with data-driven models. However, for many properties of interest, the number of materials for which a specific property has been determined is much smaller than the number of known materials. To overcome this disparity, we propose a novel self-supervised learning (SSL) strategy for material property prediction. Our approach, crystal denoising self-supervised learning (CDSSL), pretrains predictive models (e.g., graph networks) with a pretext task based on recovering valid material structures when given perturbed versions of these structures. We demonstrate that CDSSL models out-perform models trained without SSL, across material types, properties, and dataset sizes.",
    "metadata": {
      "arxiv_id": "2408.17255",
      "title": "Self-supervised learning for crystal property prediction via denoising",
      "summary": "Accurate prediction of the properties of crystalline materials is crucial for targeted discovery, and this prediction is increasingly done with data-driven models. However, for many properties of interest, the number of materials for which a specific property has been determined is much smaller than the number of known materials. To overcome this disparity, we propose a novel self-supervised learning (SSL) strategy for material property prediction. Our approach, crystal denoising self-supervised learning (CDSSL), pretrains predictive models (e.g., graph networks) with a pretext task based on recovering valid material structures when given perturbed versions of these structures. We demonstrate that CDSSL models out-perform models trained without SSL, across material types, properties, and dataset sizes.",
      "authors": [
        "Alexander New",
        "Nam Q. Le",
        "Michael J. Pekala",
        "Christopher D. Stiles"
      ],
      "published": "2024-08-30T12:53:40Z",
      "updated": "2024-08-30T12:53:40Z",
      "categories": [
        "cs.LG",
        "cond-mat.mtrl-sci"
      ],
      "pdf_url": "https://arxiv.org/pdf/2408.17255v1",
      "landing_url": "https://arxiv.org/abs/2408.17255v1",
      "doi": "https://doi.org/10.48550/arXiv.2408.17255"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item is about self-supervised learning for crystal property prediction; there is no discrete audio/speech tokenization core; thus it does not meet the audio tokenization inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item is about self-supervised learning for crystal property prediction; there is no discrete audio/speech tokenization core; thus it does not meet the audio tokenization inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on self-supervised learning for crystal property prediction, which is unrelated to discrete audio/speech tokenization or any form of audio signal processing; it does not address any inclusion criteria and clearly falls under exclusion criteria as it does not deal with audio tokenization or speech signals.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on self-supervised learning for crystal property prediction, which is unrelated to discrete audio/speech tokenization or any form of audio signal processing; it does not address any inclusion criteria and clearly falls under exclusion criteria as it does not deal with audio tokenization or speech signals.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "MaskGCT: Zero-Shot Text-to-Speech with Masked Generative Codec Transformer",
    "abstract": "The recent large-scale text-to-speech (TTS) systems are usually grouped as autoregressive and non-autoregressive systems. The autoregressive systems implicitly model duration but exhibit certain deficiencies in robustness and lack of duration controllability. Non-autoregressive systems require explicit alignment information between text and speech during training and predict durations for linguistic units (e.g. phone), which may compromise their naturalness. In this paper, we introduce Masked Generative Codec Transformer (MaskGCT), a fully non-autoregressive TTS model that eliminates the need for explicit alignment information between text and speech supervision, as well as phone-level duration prediction. MaskGCT is a two-stage model: in the first stage, the model uses text to predict semantic tokens extracted from a speech self-supervised learning (SSL) model, and in the second stage, the model predicts acoustic tokens conditioned on these semantic tokens. MaskGCT follows the mask-and-predict learning paradigm. During training, MaskGCT learns to predict masked semantic or acoustic tokens based on given conditions and prompts. During inference, the model generates tokens of a specified length in a parallel manner. Experiments with 100K hours of in-the-wild speech demonstrate that MaskGCT outperforms the current state-of-the-art zero-shot TTS systems in terms of quality, similarity, and intelligibility. Audio samples are available at https://maskgct.github.io/. We release our code and model checkpoints at https://github.com/open-mmlab/Amphion/blob/main/models/tts/maskgct.",
    "metadata": {
      "arxiv_id": "2409.00750",
      "title": "MaskGCT: Zero-Shot Text-to-Speech with Masked Generative Codec Transformer",
      "summary": "The recent large-scale text-to-speech (TTS) systems are usually grouped as autoregressive and non-autoregressive systems. The autoregressive systems implicitly model duration but exhibit certain deficiencies in robustness and lack of duration controllability. Non-autoregressive systems require explicit alignment information between text and speech during training and predict durations for linguistic units (e.g. phone), which may compromise their naturalness. In this paper, we introduce Masked Generative Codec Transformer (MaskGCT), a fully non-autoregressive TTS model that eliminates the need for explicit alignment information between text and speech supervision, as well as phone-level duration prediction. MaskGCT is a two-stage model: in the first stage, the model uses text to predict semantic tokens extracted from a speech self-supervised learning (SSL) model, and in the second stage, the model predicts acoustic tokens conditioned on these semantic tokens. MaskGCT follows the mask-and-predict learning paradigm. During training, MaskGCT learns to predict masked semantic or acoustic tokens based on given conditions and prompts. During inference, the model generates tokens of a specified length in a parallel manner. Experiments with 100K hours of in-the-wild speech demonstrate that MaskGCT outperforms the current state-of-the-art zero-shot TTS systems in terms of quality, similarity, and intelligibility. Audio samples are available at https://maskgct.github.io/. We release our code and model checkpoints at https://github.com/open-mmlab/Amphion/blob/main/models/tts/maskgct.",
      "authors": [
        "Yuancheng Wang",
        "Haoyue Zhan",
        "Liwei Liu",
        "Ruihong Zeng",
        "Haotian Guo",
        "Jiachen Zheng",
        "Qiang Zhang",
        "Xueyao Zhang",
        "Shunsi Zhang",
        "Zhizheng Wu"
      ],
      "published": "2024-09-01T15:26:30Z",
      "updated": "2024-10-20T14:25:49Z",
      "categories": [
        "cs.SD",
        "cs.AI",
        "cs.LG",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2409.00750v3",
      "landing_url": "https://arxiv.org/abs/2409.00750v3",
      "doi": "https://doi.org/10.48550/arXiv.2409.00750"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The paper centers on discrete speech tokens (semantic and acoustic tokens) and token-based generation using a mask-and-predict approach, aligning with the core discrete audio/tokenization analysis criterion.",
      "evaluation": 5
    },
    "round-A_JuniorNano_reasoning": "The paper centers on discrete speech tokens (semantic and acoustic tokens) and token-based generation using a mask-and-predict approach, aligning with the core discrete audio/tokenization analysis criterion.",
    "round-A_JuniorNano_evaluation": 5,
    "round-A_JuniorMini_output": {
      "reasoning": "The study proposes a non-autoregressive TTS model called MaskGCT that converts text to semantic tokens and then to acoustic tokens derived from self-supervised speech models, explicitly involving discrete audio token sequences as core objects for speech generation, and provides methodological and experimental validation on large-scale data, meeting all inclusion criteria without falling into any exclusion criteria.",
      "evaluation": 5
    },
    "round-A_JuniorMini_reasoning": "The study proposes a non-autoregressive TTS model called MaskGCT that converts text to semantic tokens and then to acoustic tokens derived from self-supervised speech models, explicitly involving discrete audio token sequences as core objects for speech generation, and provides methodological and experimental validation on large-scale data, meeting all inclusion criteria without falling into any exclusion criteria.",
    "round-A_JuniorMini_evaluation": 5,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "include (junior:5)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "VQ-Flow: Taming Normalizing Flows for Multi-Class Anomaly Detection via Hierarchical Vector Quantization",
    "abstract": "Normalizing flows, a category of probabilistic models famed for their capabilities in modeling complex data distributions, have exhibited remarkable efficacy in unsupervised anomaly detection. This paper explores the potential of normalizing flows in multi-class anomaly detection, wherein the normal data is compounded with multiple classes without providing class labels. Through the integration of vector quantization (VQ), we empower the flow models to distinguish different concepts of multi-class normal data in an unsupervised manner, resulting in a novel flow-based unified method, named VQ-Flow. Specifically, our VQ-Flow leverages hierarchical vector quantization to estimate two relative codebooks: a Conceptual Prototype Codebook (CPC) for concept distinction and its concomitant Concept-Specific Pattern Codebook (CSPC) to capture concept-specific normal patterns. The flow models in VQ-Flow are conditioned on the concept-specific patterns captured in CSPC, capable of modeling specific normal patterns associated with different concepts. Moreover, CPC further enables our VQ-Flow for concept-aware distribution modeling, faithfully mimicking the intricate multi-class normal distribution through a mixed Gaussian distribution reparametrized on the conceptual prototypes. Through the introduction of vector quantization, the proposed VQ-Flow advances the state-of-the-art in multi-class anomaly detection within a unified training scheme, yielding the Det./Loc. AUROC of 99.5%/98.3% on MVTec AD. The codebase is publicly available at https://github.com/cool-xuan/vqflow.",
    "metadata": {
      "arxiv_id": "2409.00942",
      "title": "VQ-Flow: Taming Normalizing Flows for Multi-Class Anomaly Detection via Hierarchical Vector Quantization",
      "summary": "Normalizing flows, a category of probabilistic models famed for their capabilities in modeling complex data distributions, have exhibited remarkable efficacy in unsupervised anomaly detection. This paper explores the potential of normalizing flows in multi-class anomaly detection, wherein the normal data is compounded with multiple classes without providing class labels. Through the integration of vector quantization (VQ), we empower the flow models to distinguish different concepts of multi-class normal data in an unsupervised manner, resulting in a novel flow-based unified method, named VQ-Flow. Specifically, our VQ-Flow leverages hierarchical vector quantization to estimate two relative codebooks: a Conceptual Prototype Codebook (CPC) for concept distinction and its concomitant Concept-Specific Pattern Codebook (CSPC) to capture concept-specific normal patterns. The flow models in VQ-Flow are conditioned on the concept-specific patterns captured in CSPC, capable of modeling specific normal patterns associated with different concepts. Moreover, CPC further enables our VQ-Flow for concept-aware distribution modeling, faithfully mimicking the intricate multi-class normal distribution through a mixed Gaussian distribution reparametrized on the conceptual prototypes. Through the introduction of vector quantization, the proposed VQ-Flow advances the state-of-the-art in multi-class anomaly detection within a unified training scheme, yielding the Det./Loc. AUROC of 99.5%/98.3% on MVTec AD. The codebase is publicly available at https://github.com/cool-xuan/vqflow.",
      "authors": [
        "Yixuan Zhou",
        "Xing Xu",
        "Zhe Sun",
        "Jingkuan Song",
        "Andrzej Cichocki",
        "Heng Tao Shen"
      ],
      "published": "2024-09-02T05:01:41Z",
      "updated": "2024-09-02T05:01:41Z",
      "categories": [
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2409.00942v1",
      "landing_url": "https://arxiv.org/abs/2409.00942v1",
      "doi": "https://doi.org/10.48550/arXiv.2409.00942"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The paper focuses on normalizing flows with vector quantization for unsupervised multi-class anomaly detection and does not involve discrete audio/speech tokenization or analysis of discrete audio tokens, so it does not meet the inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The paper focuses on normalizing flows with vector quantization for unsupervised multi-class anomaly detection and does not involve discrete audio/speech tokenization or analysis of discrete audio tokens, so it does not meet the inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study proposes a novel method for multi-class anomaly detection using normalizing flows with hierarchical vector quantization, but it does not involve discrete audio or speech tokenization, nor does it focus on converting continuous audio signals into discrete tokens for reconstruction, generation, or understanding tasks; the domain is anomaly detection in visual or general data, unrelated to audio tokenization.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study proposes a novel method for multi-class anomaly detection using normalizing flows with hierarchical vector quantization, but it does not involve discrete audio or speech tokenization, nor does it focus on converting continuous audio signals into discrete tokens for reconstruction, generation, or understanding tasks; the domain is anomaly detection in visual or general data, unrelated to audio tokenization.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "vec2wav 2.0: Advancing Voice Conversion via Discrete Token Vocoders",
    "abstract": "We propose a new speech discrete token vocoder, vec2wav 2.0, which advances voice conversion (VC). We use discrete tokens from speech self-supervised models as the content features of source speech, and treat VC as a prompted vocoding task. To amend the loss of speaker timbre in the content tokens, vec2wav 2.0 utilizes the WavLM features to provide strong timbre-dependent information. A novel adaptive Snake activation function is proposed to better incorporate timbre into the waveform reconstruction process. In this way, vec2wav 2.0 learns to alter the speaker timbre appropriately given different reference prompts. Also, no supervised data is required for vec2wav 2.0 to be effectively trained. Experimental results demonstrate that vec2wav 2.0 outperforms all other baselines to a considerable margin in terms of audio quality and speaker similarity in any-to-any VC. Ablation studies verify the effects made by the proposed techniques. Moreover, vec2wav 2.0 achieves competitive cross-lingual VC even only trained on monolingual corpus. Thus, vec2wav 2.0 shows timbre can potentially be manipulated only by speech token vocoders, pushing the frontiers of VC and speech synthesis.",
    "metadata": {
      "arxiv_id": "2409.01995",
      "title": "vec2wav 2.0: Advancing Voice Conversion via Discrete Token Vocoders",
      "summary": "We propose a new speech discrete token vocoder, vec2wav 2.0, which advances voice conversion (VC). We use discrete tokens from speech self-supervised models as the content features of source speech, and treat VC as a prompted vocoding task. To amend the loss of speaker timbre in the content tokens, vec2wav 2.0 utilizes the WavLM features to provide strong timbre-dependent information. A novel adaptive Snake activation function is proposed to better incorporate timbre into the waveform reconstruction process. In this way, vec2wav 2.0 learns to alter the speaker timbre appropriately given different reference prompts. Also, no supervised data is required for vec2wav 2.0 to be effectively trained. Experimental results demonstrate that vec2wav 2.0 outperforms all other baselines to a considerable margin in terms of audio quality and speaker similarity in any-to-any VC. Ablation studies verify the effects made by the proposed techniques. Moreover, vec2wav 2.0 achieves competitive cross-lingual VC even only trained on monolingual corpus. Thus, vec2wav 2.0 shows timbre can potentially be manipulated only by speech token vocoders, pushing the frontiers of VC and speech synthesis.",
      "authors": [
        "Yiwei Guo",
        "Zhihan Li",
        "Junjie Li",
        "Chenpeng Du",
        "Hankun Wang",
        "Shuai Wang",
        "Xie Chen",
        "Kai Yu"
      ],
      "published": "2024-09-03T15:41:07Z",
      "updated": "2025-05-24T13:50:34Z",
      "categories": [
        "eess.AS",
        "cs.AI",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2409.01995v4",
      "landing_url": "https://arxiv.org/abs/2409.01995v4",
      "doi": "https://doi.org/10.48550/arXiv.2409.01995"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The abstract centers on a discrete-token vocoder for voice conversion, explicitly using discrete tokens from speech models as content features and evaluating their impact on reconstruction and speaker timbre, which satisfies the inclusion criteria and does not trigger the exclusion criteria.",
      "evaluation": 5
    },
    "round-A_JuniorNano_reasoning": "The abstract centers on a discrete-token vocoder for voice conversion, explicitly using discrete tokens from speech models as content features and evaluating their impact on reconstruction and speaker timbre, which satisfies the inclusion criteria and does not trigger the exclusion criteria.",
    "round-A_JuniorNano_evaluation": 5,
    "round-A_JuniorMini_output": {
      "reasoning": "The study introduces vec2wav 2.0, a speech discrete token vocoder that converts continuous speech into discrete token sequences for voice conversion, explicitly focusing on discrete tokenization and its use in speech synthesis and manipulation tasks, with experimental validation and methodological advances, all aligning with the inclusion criteria and having no violation of the exclusion criteria.",
      "evaluation": 5
    },
    "round-A_JuniorMini_reasoning": "The study introduces vec2wav 2.0, a speech discrete token vocoder that converts continuous speech into discrete token sequences for voice conversion, explicitly focusing on discrete tokenization and its use in speech synthesis and manipulation tasks, with experimental validation and methodological advances, all aligning with the inclusion criteria and having no violation of the exclusion criteria.",
    "round-A_JuniorMini_evaluation": 5,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "include (junior:5)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "STAB: Speech Tokenizer Assessment Benchmark",
    "abstract": "Representing speech as discrete tokens provides a framework for transforming speech into a format that closely resembles text, thus enabling the use of speech as an input to the widely successful large language models (LLMs). Currently, while several speech tokenizers have been proposed, there is ambiguity regarding the properties that are desired from a tokenizer for specific downstream tasks and its overall generalizability. Evaluating the performance of tokenizers across different downstream tasks is a computationally intensive effort that poses challenges for scalability. To circumvent this requirement, we present STAB (Speech Tokenizer Assessment Benchmark), a systematic evaluation framework designed to assess speech tokenizers comprehensively and shed light on their inherent characteristics. This framework provides a deeper understanding of the underlying mechanisms of speech tokenization, thereby offering a valuable resource for expediting the advancement of future tokenizer models and enabling comparative analysis using a standardized benchmark. We evaluate the STAB metrics and correlate this with downstream task performance across a range of speech tasks and tokenizer choices.",
    "metadata": {
      "arxiv_id": "2409.02384",
      "title": "STAB: Speech Tokenizer Assessment Benchmark",
      "summary": "Representing speech as discrete tokens provides a framework for transforming speech into a format that closely resembles text, thus enabling the use of speech as an input to the widely successful large language models (LLMs). Currently, while several speech tokenizers have been proposed, there is ambiguity regarding the properties that are desired from a tokenizer for specific downstream tasks and its overall generalizability. Evaluating the performance of tokenizers across different downstream tasks is a computationally intensive effort that poses challenges for scalability. To circumvent this requirement, we present STAB (Speech Tokenizer Assessment Benchmark), a systematic evaluation framework designed to assess speech tokenizers comprehensively and shed light on their inherent characteristics. This framework provides a deeper understanding of the underlying mechanisms of speech tokenization, thereby offering a valuable resource for expediting the advancement of future tokenizer models and enabling comparative analysis using a standardized benchmark. We evaluate the STAB metrics and correlate this with downstream task performance across a range of speech tasks and tokenizer choices.",
      "authors": [
        "Shikhar Vashishth",
        "Harman Singh",
        "Shikhar Bharadwaj",
        "Sriram Ganapathy",
        "Chulayuth Asawaroengchai",
        "Kartik Audhkhasi",
        "Andrew Rosenberg",
        "Ankur Bapna",
        "Bhuvana Ramabhadran"
      ],
      "published": "2024-09-04T02:20:59Z",
      "updated": "2024-09-04T02:20:59Z",
      "categories": [
        "cs.CL",
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2409.02384v1",
      "landing_url": "https://arxiv.org/abs/2409.02384v1",
      "doi": "https://doi.org/10.48550/arXiv.2409.02384"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The abstract presents STAB, a benchmark framework explicitly evaluating discrete speech tokenizers that convert audio into token sequences, with analysis of tokenizer properties and correlation to downstream tasks, meeting the inclusion criteria and not falling under exclusions.",
      "evaluation": 5
    },
    "round-A_JuniorNano_reasoning": "The abstract presents STAB, a benchmark framework explicitly evaluating discrete speech tokenizers that convert audio into token sequences, with analysis of tokenizer properties and correlation to downstream tasks, meeting the inclusion criteria and not falling under exclusions.",
    "round-A_JuniorNano_evaluation": 5,
    "round-A_JuniorMini_output": {
      "reasoning": "The study explicitly proposes and evaluates a method for assessing discrete speech tokenizers, where speech is converted into discrete tokens for downstream tasks, with systematic analysis and experimental validation; it focuses on neural quantization-based discrete tokens relevant to language-model-style sequences, meeting all inclusion criteria and no exclusion criteria.",
      "evaluation": 5
    },
    "round-A_JuniorMini_reasoning": "The study explicitly proposes and evaluates a method for assessing discrete speech tokenizers, where speech is converted into discrete tokens for downstream tasks, with systematic analysis and experimental validation; it focuses on neural quantization-based discrete tokens relevant to language-model-style sequences, meeting all inclusion criteria and no exclusion criteria.",
    "round-A_JuniorMini_evaluation": 5,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "include (junior:5)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "FireRedTTS: A Foundation Text-To-Speech Framework for Industry-Level Generative Speech Applications",
    "abstract": "This work proposes FireRedTTS, a foundation text-to-speech framework, to meet the growing demands for personalized and diverse generative speech applications. The framework comprises three parts: data processing, foundation system, and downstream applications. First, we comprehensively present our data processing pipeline, which transforms massive raw audio into a large-scale high-quality TTS dataset with rich annotations and a wide coverage of content, speaking style, and timbre. Then, we propose a language-model-based foundation TTS system. The speech signal is compressed into discrete semantic tokens via a semantic-aware speech tokenizer, and can be generated by a language model from the prompt text and audio. Then, a two-stage waveform generator is proposed to decode them to the high-fidelity waveform. We present two applications of this system: voice cloning for dubbing and human-like speech generation for chatbots. The experimental results demonstrate the solid in-context learning capability of FireRedTTS, which can stably synthesize high-quality speech consistent with the prompt text and audio. For dubbing, FireRedTTS can clone target voices in a zero-shot way for the UGC scenario and adapt to studio-level expressive voice characters in the PUGC scenario via few-shot fine-tuning with 1-hour recording. Moreover, FireRedTTS achieves controllable human-like speech generation in a casual style with paralinguistic behaviors and emotions via instruction tuning, to better serve spoken chatbots.",
    "metadata": {
      "arxiv_id": "2409.03283",
      "title": "FireRedTTS: A Foundation Text-To-Speech Framework for Industry-Level Generative Speech Applications",
      "summary": "This work proposes FireRedTTS, a foundation text-to-speech framework, to meet the growing demands for personalized and diverse generative speech applications. The framework comprises three parts: data processing, foundation system, and downstream applications. First, we comprehensively present our data processing pipeline, which transforms massive raw audio into a large-scale high-quality TTS dataset with rich annotations and a wide coverage of content, speaking style, and timbre. Then, we propose a language-model-based foundation TTS system. The speech signal is compressed into discrete semantic tokens via a semantic-aware speech tokenizer, and can be generated by a language model from the prompt text and audio. Then, a two-stage waveform generator is proposed to decode them to the high-fidelity waveform. We present two applications of this system: voice cloning for dubbing and human-like speech generation for chatbots. The experimental results demonstrate the solid in-context learning capability of FireRedTTS, which can stably synthesize high-quality speech consistent with the prompt text and audio. For dubbing, FireRedTTS can clone target voices in a zero-shot way for the UGC scenario and adapt to studio-level expressive voice characters in the PUGC scenario via few-shot fine-tuning with 1-hour recording. Moreover, FireRedTTS achieves controllable human-like speech generation in a casual style with paralinguistic behaviors and emotions via instruction tuning, to better serve spoken chatbots.",
      "authors": [
        "Hao-Han Guo",
        "Yao Hu",
        "Kun Liu",
        "Fei-Yu Shen",
        "Xu Tang",
        "Yi-Chen Wu",
        "Feng-Long Xie",
        "Kun Xie",
        "Kai-Tuo Xu"
      ],
      "published": "2024-09-05T06:48:02Z",
      "updated": "2025-04-11T07:36:53Z",
      "categories": [
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2409.03283v2",
      "landing_url": "https://arxiv.org/abs/2409.03283v2",
      "doi": "https://doi.org/10.48550/arXiv.2409.03283"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The abstract explicitly centers on compressing speech into discrete semantic tokens via a tokenizer, a core discrete tokenization approach for generation/reconstruction, aligning with the inclusion criteria and not triggering any exclusion criteria.",
      "evaluation": 5
    },
    "round-A_JuniorNano_reasoning": "The abstract explicitly centers on compressing speech into discrete semantic tokens via a tokenizer, a core discrete tokenization approach for generation/reconstruction, aligning with the inclusion criteria and not triggering any exclusion criteria.",
    "round-A_JuniorNano_evaluation": 5,
    "round-A_JuniorMini_output": {
      "reasoning": "The study explicitly proposes a language-model-based foundation TTS system where the speech signal is compressed into discrete semantic tokens via a semantic-aware speech tokenizer, which aligns with the inclusion criterion of analyzing discrete audio/speech tokenization for generative tasks. The abstract also details experimental validation and methodological contributions beyond an engineering report, and there is no indication that it uses only continuous features or traditional codecs, nor that the tokenization is unrelated to speech tasks.",
      "evaluation": 5
    },
    "round-A_JuniorMini_reasoning": "The study explicitly proposes a language-model-based foundation TTS system where the speech signal is compressed into discrete semantic tokens via a semantic-aware speech tokenizer, which aligns with the inclusion criterion of analyzing discrete audio/speech tokenization for generative tasks. The abstract also details experimental validation and methodological contributions beyond an engineering report, and there is no indication that it uses only continuous features or traditional codecs, nor that the tokenization is unrelated to speech tasks.",
    "round-A_JuniorMini_evaluation": 5,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "include (junior:5)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "aTENNuate: Optimized Real-time Speech Enhancement with Deep SSMs on Raw Audio",
    "abstract": "We present aTENNuate, a simple deep state-space autoencoder configured for efficient online raw speech enhancement in an end-to-end fashion. The network's performance is primarily evaluated on raw speech denoising, with additional assessments on tasks such as super-resolution and de-quantization. We benchmark aTENNuate on the VoiceBank + DEMAND and the Microsoft DNS1 synthetic test sets. The network outperforms previous real-time denoising models in terms of PESQ score, parameter count, MACs, and latency. Even as a raw waveform processing model, the model maintains high fidelity to the clean signal with minimal audible artifacts. In addition, the model remains performant even when the noisy input is compressed down to 4000Hz and 4 bits, suggesting general speech enhancement capabilities in low-resource environments. Try it out by pip install attenuate",
    "metadata": {
      "arxiv_id": "2409.03377",
      "title": "aTENNuate: Optimized Real-time Speech Enhancement with Deep SSMs on Raw Audio",
      "summary": "We present aTENNuate, a simple deep state-space autoencoder configured for efficient online raw speech enhancement in an end-to-end fashion. The network's performance is primarily evaluated on raw speech denoising, with additional assessments on tasks such as super-resolution and de-quantization. We benchmark aTENNuate on the VoiceBank + DEMAND and the Microsoft DNS1 synthetic test sets. The network outperforms previous real-time denoising models in terms of PESQ score, parameter count, MACs, and latency. Even as a raw waveform processing model, the model maintains high fidelity to the clean signal with minimal audible artifacts. In addition, the model remains performant even when the noisy input is compressed down to 4000Hz and 4 bits, suggesting general speech enhancement capabilities in low-resource environments. Try it out by pip install attenuate",
      "authors": [
        "Yan Ru Pei",
        "Ritik Shrivastava",
        "FNU Sidharth"
      ],
      "published": "2024-09-05T09:28:56Z",
      "updated": "2025-05-20T02:23:19Z",
      "categories": [
        "cs.SD",
        "cs.AI",
        "cs.LG",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2409.03377v4",
      "landing_url": "https://arxiv.org/abs/2409.03377v4",
      "doi": "https://doi.org/10.48550/arXiv.2409.03377"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The study focuses on real-time speech enhancement on raw audio using a deep state-space autoencoder, with no discrete tokenization or token-level representations analyzed, thus it does not meet the inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The study focuses on real-time speech enhancement on raw audio using a deep state-space autoencoder, with no discrete tokenization or token-level representations analyzed, thus it does not meet the inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study presents a deep state-space autoencoder for raw speech enhancement focusing on speech denoising and related tasks, but it does not explicitly propose, analyze, or evaluate discrete audio/speech tokenization methods; instead, it processes raw continuous waveform signals without discrete tokenization as a core subject, and it appears to be more of a method for real-time enhancement rather than a study on discrete speech tokens.",
      "evaluation": 2
    },
    "round-A_JuniorMini_reasoning": "The study presents a deep state-space autoencoder for raw speech enhancement focusing on speech denoising and related tasks, but it does not explicitly propose, analyze, or evaluate discrete audio/speech tokenization methods; instead, it processes raw continuous waveform signals without discrete tokenization as a core subject, and it appears to be more of a method for real-time enhancement rather than a study on discrete speech tokens.",
    "round-A_JuniorMini_evaluation": 2,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:2)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:2)"
  },
  {
    "title": "VQ-DeepVSC: A Dual-Stage Vector Quantization Framework for Video Semantic Communication",
    "abstract": "In response to the rapid growth of global videomtraffic and the limitations of traditional wireless transmission systems, we propose a novel dual-stage vector quantization framework, VQ-DeepVSC, tailored to enhance video transmission over wireless channels. In the first stage, we design the adaptive keyframe extractor and interpolator, deployed respectively at the transmitter and receiver, which intelligently select key frames to minimize inter-frame redundancy and mitigate the cliff-effect under challenging channel conditions. In the second stage, we propose the semantic vector quantization encoder and decoder, placed respectively at the transmitter and receiver, which efficiently compress key frames using advanced indexing and spatial normalization modules to reduce redundancy. Additionally, we propose adjustable index selection and recovery modules, enhancing compression efficiency and enabling flexible compression ratio adjustment. Compared to the joint source-channel coding (JSCC) framework, the proposed framework exhibits superior compatibility with current digital communication systems. Experimental results demonstrate that VQ-DeepVSC achieves substantial improvements in both Multi-Scale Structural Similarity (MS-SSIM) and Learned Perceptual Image Patch Similarity (LPIPS) metrics than the H.265 standard, particularly under low channel signal-to-noise ratio (SNR) or multi-path channels, highlighting the significantly enhanced transmission capabilities of our approach.",
    "metadata": {
      "arxiv_id": "2409.03393",
      "title": "VQ-DeepVSC: A Dual-Stage Vector Quantization Framework for Video Semantic Communication",
      "summary": "In response to the rapid growth of global videomtraffic and the limitations of traditional wireless transmission systems, we propose a novel dual-stage vector quantization framework, VQ-DeepVSC, tailored to enhance video transmission over wireless channels. In the first stage, we design the adaptive keyframe extractor and interpolator, deployed respectively at the transmitter and receiver, which intelligently select key frames to minimize inter-frame redundancy and mitigate the cliff-effect under challenging channel conditions. In the second stage, we propose the semantic vector quantization encoder and decoder, placed respectively at the transmitter and receiver, which efficiently compress key frames using advanced indexing and spatial normalization modules to reduce redundancy. Additionally, we propose adjustable index selection and recovery modules, enhancing compression efficiency and enabling flexible compression ratio adjustment. Compared to the joint source-channel coding (JSCC) framework, the proposed framework exhibits superior compatibility with current digital communication systems. Experimental results demonstrate that VQ-DeepVSC achieves substantial improvements in both Multi-Scale Structural Similarity (MS-SSIM) and Learned Perceptual Image Patch Similarity (LPIPS) metrics than the H.265 standard, particularly under low channel signal-to-noise ratio (SNR) or multi-path channels, highlighting the significantly enhanced transmission capabilities of our approach.",
      "authors": [
        "Yongyi Miao",
        "Zhongdang Li",
        "Yang Wang",
        "Die Hu",
        "Jun Yan",
        "Youfang Wang"
      ],
      "published": "2024-09-05T09:53:53Z",
      "updated": "2024-09-05T09:53:53Z",
      "categories": [
        "cs.NI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2409.03393v1",
      "landing_url": "https://arxiv.org/abs/2409.03393v1",
      "doi": "https://doi.org/10.48550/arXiv.2409.03393"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The article centers on a video semantic communication framework and discrete vector quantization for video frames, not on discrete audio/speech tokenization or analysis of discrete audio tokens, so it fails the audio-tokenization inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The article centers on a video semantic communication framework and discrete vector quantization for video frames, not on discrete audio/speech tokenization or analysis of discrete audio tokens, so it fails the audio-tokenization inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on a dual-stage vector quantization framework for video semantic communication, addressing video transmission over wireless channels with no mention of discrete audio/speech tokenization as a core object of study; the tokenization discussed pertains to video key frames and semantic vectors, not audio signals.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on a dual-stage vector quantization framework for video semantic communication, addressing video transmission over wireless channels with no mention of discrete audio/speech tokenization as a core object of study; the tokenization discussed pertains to video key frames and semantic vectors, not audio signals.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "LAST: Language Model Aware Speech Tokenization",
    "abstract": "Speech tokenization serves as the foundation of speech language model (LM), enabling them to perform various tasks such as spoken language modeling, text-to-speech, speech-to-text, etc. Most speech tokenizers are trained independently of the LM training process, relying on separate acoustic models and quantization methods. Following such an approach may create a mismatch between the tokenization process and its usage afterward. In this study, we propose a novel approach to training a speech tokenizer by leveraging objectives from pre-trained textual LMs. We advocate for the integration of this objective into the process of learning discrete speech representations. Our aim is to transform features from a pre-trained speech model into a new feature space that enables better clustering for speech LMs. We empirically investigate the impact of various model design choices, including speech vocabulary size and text LM size. Our results demonstrate the proposed tokenization method outperforms the evaluated baselines considering both spoken language modeling and speech-to-text. More importantly, unlike prior work, the proposed method allows the utilization of a single pre-trained LM for processing both speech and text inputs, setting it apart from conventional tokenization approaches.",
    "metadata": {
      "arxiv_id": "2409.03701",
      "title": "LAST: Language Model Aware Speech Tokenization",
      "summary": "Speech tokenization serves as the foundation of speech language model (LM), enabling them to perform various tasks such as spoken language modeling, text-to-speech, speech-to-text, etc. Most speech tokenizers are trained independently of the LM training process, relying on separate acoustic models and quantization methods. Following such an approach may create a mismatch between the tokenization process and its usage afterward. In this study, we propose a novel approach to training a speech tokenizer by leveraging objectives from pre-trained textual LMs. We advocate for the integration of this objective into the process of learning discrete speech representations. Our aim is to transform features from a pre-trained speech model into a new feature space that enables better clustering for speech LMs. We empirically investigate the impact of various model design choices, including speech vocabulary size and text LM size. Our results demonstrate the proposed tokenization method outperforms the evaluated baselines considering both spoken language modeling and speech-to-text. More importantly, unlike prior work, the proposed method allows the utilization of a single pre-trained LM for processing both speech and text inputs, setting it apart from conventional tokenization approaches.",
      "authors": [
        "Arnon Turetzky",
        "Yossi Adi"
      ],
      "published": "2024-09-05T16:57:39Z",
      "updated": "2024-09-10T14:45:15Z",
      "categories": [
        "cs.CL",
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2409.03701v2",
      "landing_url": "https://arxiv.org/abs/2409.03701v2",
      "doi": "https://doi.org/10.48550/arXiv.2409.03701"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The work explicitly targets discrete speech/tokenization by learning a discrete speech representation (speech tokenizer) via alignment with pre-trained text LMs, investigates vocabulary size and LM size, and reports improvements on spoken language modeling and speech-to-text, which fits the inclusion criteria and does not appear to be about continuous features or codec engineering; therefore should be included.",
      "evaluation": 5
    },
    "round-A_JuniorNano_reasoning": "The work explicitly targets discrete speech/tokenization by learning a discrete speech representation (speech tokenizer) via alignment with pre-trained text LMs, investigates vocabulary size and LM size, and reports improvements on spoken language modeling and speech-to-text, which fits the inclusion criteria and does not appear to be about continuous features or codec engineering; therefore should be included.",
    "round-A_JuniorNano_evaluation": 5,
    "round-A_JuniorMini_output": {
      "reasoning": "The study explicitly focuses on discrete speech tokenization by proposing a novel tokenization method that transforms continuous speech features into a discrete representation optimized for language models, meeting the inclusion criteria and avoiding all exclusion criteria.",
      "evaluation": 5
    },
    "round-A_JuniorMini_reasoning": "The study explicitly focuses on discrete speech tokenization by proposing a novel tokenization method that transforms continuous speech features into a discrete representation optimized for language models, meeting the inclusion criteria and avoiding all exclusion criteria.",
    "round-A_JuniorMini_evaluation": 5,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "include (junior:5)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Investigating Neural Audio Codecs for Speech Language Model-Based Speech Generation",
    "abstract": "Neural audio codec tokens serve as the fundamental building blocks for speech language model (SLM)-based speech generation. However, there is no systematic understanding on how the codec system affects the speech generation performance of the SLM. In this work, we examine codec tokens within SLM framework for speech generation to provide insights for effective codec design. We retrain existing high-performing neural codec models on the same data set and loss functions to compare their performance in a uniform setting. We integrate codec tokens into two SLM systems: masked-based parallel speech generation system and an auto-regressive (AR) plus non-auto-regressive (NAR) model-based system. Our findings indicate that better speech reconstruction in codec systems does not guarantee improved speech generation in SLM. A high-quality codec decoder is crucial for natural speech production in SLM, while speech intelligibility depends more on quantization mechanism.",
    "metadata": {
      "arxiv_id": "2409.04016",
      "title": "Investigating Neural Audio Codecs for Speech Language Model-Based Speech Generation",
      "summary": "Neural audio codec tokens serve as the fundamental building blocks for speech language model (SLM)-based speech generation. However, there is no systematic understanding on how the codec system affects the speech generation performance of the SLM. In this work, we examine codec tokens within SLM framework for speech generation to provide insights for effective codec design. We retrain existing high-performing neural codec models on the same data set and loss functions to compare their performance in a uniform setting. We integrate codec tokens into two SLM systems: masked-based parallel speech generation system and an auto-regressive (AR) plus non-auto-regressive (NAR) model-based system. Our findings indicate that better speech reconstruction in codec systems does not guarantee improved speech generation in SLM. A high-quality codec decoder is crucial for natural speech production in SLM, while speech intelligibility depends more on quantization mechanism.",
      "authors": [
        "Jiaqi Li",
        "Dongmei Wang",
        "Xiaofei Wang",
        "Yao Qian",
        "Long Zhou",
        "Shujie Liu",
        "Midia Yousefi",
        "Canrun Li",
        "Chung-Hsien Tsai",
        "Zhen Xiao",
        "Yanqing Liu",
        "Junkun Chen",
        "Sheng Zhao",
        "Jinyu Li",
        "Zhizheng Wu",
        "Michael Zeng"
      ],
      "published": "2024-09-06T04:06:50Z",
      "updated": "2024-09-06T04:06:50Z",
      "categories": [
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2409.04016v1",
      "landing_url": "https://arxiv.org/abs/2409.04016v1",
      "doi": "https://doi.org/10.48550/arXiv.2409.04016"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item analyzes neural audio codec tokens as discrete tokens for speech generation with speech-language models, explicitly focusing on discrete tokenization for audio reconstruction/generation and evaluating its impact, which matches the inclusion criteria and does not fit any exclusion category.",
      "evaluation": 5
    },
    "round-A_JuniorNano_reasoning": "The item analyzes neural audio codec tokens as discrete tokens for speech generation with speech-language models, explicitly focusing on discrete tokenization for audio reconstruction/generation and evaluating its impact, which matches the inclusion criteria and does not fit any exclusion category.",
    "round-A_JuniorNano_evaluation": 5,
    "round-A_JuniorMini_output": {
      "reasoning": "The study investigates neural audio codec tokens as discrete audio representations within speech language models for speech generation, systematically analyzing how different codec designs impact performance; this aligns well with the inclusion criteria requiring discrete audio tokenization analysis and excludes traditional codecs or engineering-only reports.",
      "evaluation": 5
    },
    "round-A_JuniorMini_reasoning": "The study investigates neural audio codec tokens as discrete audio representations within speech language models for speech generation, systematically analyzing how different codec designs impact performance; this aligns well with the inclusion criteria requiring discrete audio tokenization analysis and excludes traditional codecs or engineering-only reports.",
    "round-A_JuniorMini_evaluation": 5,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "include (junior:5)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "NPU-NTU System for Voice Privacy 2024 Challenge",
    "abstract": "Speaker anonymization is an effective privacy protection solution that conceals the speaker's identity while preserving the linguistic content and paralinguistic information of the original speech. To establish a fair benchmark and facilitate comparison of speaker anonymization systems, the VoicePrivacy Challenge (VPC) was held in 2020 and 2022, with a new edition planned for 2024. In this paper, we describe our proposed speaker anonymization system for VPC 2024. Our system employs a disentangled neural codec architecture and a serial disentanglement strategy to gradually disentangle the global speaker identity and time-variant linguistic content and paralinguistic information. We introduce multiple distillation methods to disentangle linguistic content, speaker identity, and emotion. These methods include semantic distillation, supervised speaker distillation, and frame-level emotion distillation. Based on these distillations, we anonymize the original speaker identity using a weighted sum of a set of candidate speaker identities and a randomly generated speaker identity. Our system achieves the best trade-off of privacy protection and emotion preservation in VPC 2024.",
    "metadata": {
      "arxiv_id": "2409.04173",
      "title": "NPU-NTU System for Voice Privacy 2024 Challenge",
      "summary": "Speaker anonymization is an effective privacy protection solution that conceals the speaker's identity while preserving the linguistic content and paralinguistic information of the original speech. To establish a fair benchmark and facilitate comparison of speaker anonymization systems, the VoicePrivacy Challenge (VPC) was held in 2020 and 2022, with a new edition planned for 2024. In this paper, we describe our proposed speaker anonymization system for VPC 2024. Our system employs a disentangled neural codec architecture and a serial disentanglement strategy to gradually disentangle the global speaker identity and time-variant linguistic content and paralinguistic information. We introduce multiple distillation methods to disentangle linguistic content, speaker identity, and emotion. These methods include semantic distillation, supervised speaker distillation, and frame-level emotion distillation. Based on these distillations, we anonymize the original speaker identity using a weighted sum of a set of candidate speaker identities and a randomly generated speaker identity. Our system achieves the best trade-off of privacy protection and emotion preservation in VPC 2024.",
      "authors": [
        "Jixun Yao",
        "Nikita Kuzmin",
        "Qing Wang",
        "Pengcheng Guo",
        "Ziqian Ning",
        "Dake Guo",
        "Kong Aik Lee",
        "Eng-Siong Chng",
        "Lei Xie"
      ],
      "published": "2024-09-06T10:32:42Z",
      "updated": "2025-02-04T08:43:31Z",
      "categories": [
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2409.04173v2",
      "landing_url": "https://arxiv.org/abs/2409.04173v2",
      "doi": "https://doi.org/10.48550/arXiv.2409.04173"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The paper describes speaker anonymization with a neural codec architecture and disentanglement/distillation methods, but there is no reference to converting audio into a discrete token sequence or finite vocabulary, so it does not meet the discrete-tokenization inclusion and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The paper describes speaker anonymization with a neural codec architecture and disentanglement/distillation methods, but there is no reference to converting audio into a discrete token sequence or finite vocabulary, so it does not meet the discrete-tokenization inclusion and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study proposes a speaker anonymization system using a disentangled neural codec architecture and multiple distillation methods to separate linguistic content, speaker identity, and emotion. However, the abstract does not explicitly mention the use or analysis of discrete audio/speech tokenization where continuous signals are converted into discrete tokens for audio reconstruction or generation. The focus is on anonymization and disentanglement but lacks clear evidence of discrete token design or quantization relevant to language-model-style token sequences, nor does it emphasize discrete token methodology as a core contribution. Therefore, it does not fully meet the inclusion criteria, nor does it clearly fall under any specific exclusion criteria based on the provided abstract, which warrants some uncertainty.",
      "evaluation": 3
    },
    "round-A_JuniorMini_reasoning": "The study proposes a speaker anonymization system using a disentangled neural codec architecture and multiple distillation methods to separate linguistic content, speaker identity, and emotion. However, the abstract does not explicitly mention the use or analysis of discrete audio/speech tokenization where continuous signals are converted into discrete tokens for audio reconstruction or generation. The focus is on anonymization and disentanglement but lacks clear evidence of discrete token design or quantization relevant to language-model-style token sequences, nor does it emphasize discrete token methodology as a core contribution. Therefore, it does not fully meet the inclusion criteria, nor does it clearly fall under any specific exclusion criteria based on the provided abstract, which warrants some uncertainty.",
    "round-A_JuniorMini_evaluation": 3,
    "round-B_SeniorLead_output": {
      "reasoning": "Both junior reviewers flagged the lack of explicit discrete-token work (one recommended exclusion, one was unsure). The abstract discusses a neural codec and disentanglement for speaker anonymization but does not explicitly propose, analyze, or evaluate discrete audio/speech tokenization or quantization into a finite vocabulary required by the inclusion criteria—thus it should be excluded. Score: 1 (absolutely exclude).",
      "evaluation": 1
    },
    "round-B_SeniorLead_reasoning": "Both junior reviewers flagged the lack of explicit discrete-token work (one recommended exclusion, one was unsure). The abstract discusses a neural codec and disentanglement for speaker anonymization but does not explicitly propose, analyze, or evaluate discrete audio/speech tokenization or quantization into a finite vocabulary required by the inclusion criteria—thus it should be excluded. Score: 1 (absolutely exclude).",
    "round-B_SeniorLead_evaluation": 1,
    "final_verdict": "exclude (senior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (senior:1)"
  },
  {
    "title": "Disentangling the Prosody and Semantic Information with Pre-trained Model for In-Context Learning based Zero-Shot Voice Conversion",
    "abstract": "Voice conversion (VC) aims to modify the speaker's timbre while retaining speech content. Previous approaches have tokenized the outputs from self-supervised into semantic tokens, facilitating disentanglement of speech content information. Recently, in-context learning (ICL) has emerged in text-to-speech (TTS) systems for effectively modeling specific characteristics such as timbre through context conditioning. This paper proposes an ICL capability enhanced VC system (ICL-VC) employing a mask and reconstruction training strategy based on flow-matching generative models. Augmented with semantic tokens, our experiments on the LibriTTS dataset demonstrate that ICL-VC improves speaker similarity. Additionally, we find that k-means is a versatile tokenization method applicable to various pre-trained models. However, the ICL-VC system faces challenges in preserving the prosody of the source speech. To mitigate this issue, we propose incorporating prosody embeddings extracted from a pre-trained emotion recognition model into our system. Integration of prosody embeddings notably enhances the system's capability to preserve source speech prosody, as validated on the Emotional Speech Database.",
    "metadata": {
      "arxiv_id": "2409.05004",
      "title": "Disentangling the Prosody and Semantic Information with Pre-trained Model for In-Context Learning based Zero-Shot Voice Conversion",
      "summary": "Voice conversion (VC) aims to modify the speaker's timbre while retaining speech content. Previous approaches have tokenized the outputs from self-supervised into semantic tokens, facilitating disentanglement of speech content information. Recently, in-context learning (ICL) has emerged in text-to-speech (TTS) systems for effectively modeling specific characteristics such as timbre through context conditioning. This paper proposes an ICL capability enhanced VC system (ICL-VC) employing a mask and reconstruction training strategy based on flow-matching generative models. Augmented with semantic tokens, our experiments on the LibriTTS dataset demonstrate that ICL-VC improves speaker similarity. Additionally, we find that k-means is a versatile tokenization method applicable to various pre-trained models. However, the ICL-VC system faces challenges in preserving the prosody of the source speech. To mitigate this issue, we propose incorporating prosody embeddings extracted from a pre-trained emotion recognition model into our system. Integration of prosody embeddings notably enhances the system's capability to preserve source speech prosody, as validated on the Emotional Speech Database.",
      "authors": [
        "Zhengyang Chen",
        "Shuai Wang",
        "Mingyang Zhang",
        "Xuechen Liu",
        "Junichi Yamagishi",
        "Yanmin Qian"
      ],
      "published": "2024-09-08T07:24:03Z",
      "updated": "2024-09-10T07:36:03Z",
      "categories": [
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2409.05004v2",
      "landing_url": "https://arxiv.org/abs/2409.05004v2",
      "doi": "https://doi.org/10.48550/arXiv.2409.05004"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The abstract explicitly discusses tokenizing audio into semantic tokens and using discrete tokenization methods (e.g., k-means) as part of a voice-conversion framework, indicating a focus on discrete audio tokenization within a speech task, which meets the inclusion criterion and aligns with the exclusion criteria since it is not merely a continuous-feature study or non-speech token analysis.",
      "evaluation": 4
    },
    "round-A_JuniorNano_reasoning": "The abstract explicitly discusses tokenizing audio into semantic tokens and using discrete tokenization methods (e.g., k-means) as part of a voice-conversion framework, indicating a focus on discrete audio tokenization within a speech task, which meets the inclusion criterion and aligns with the exclusion criteria since it is not merely a continuous-feature study or non-speech token analysis.",
    "round-A_JuniorNano_evaluation": 4,
    "round-A_JuniorMini_output": {
      "reasoning": "The study explicitly investigates discrete semantic tokenization methods derived from self-supervised outputs and applies k-means tokenization to various pre-trained models for voice conversion, meeting the core inclusion criterion of analyzing discrete audio tokenization; it also includes experimental validation and focuses on speech tasks, without relying solely on continuous features or traditional codecs, thus fulfilling the inclusion criteria and avoiding exclusion criteria.",
      "evaluation": 5
    },
    "round-A_JuniorMini_reasoning": "The study explicitly investigates discrete semantic tokenization methods derived from self-supervised outputs and applies k-means tokenization to various pre-trained models for voice conversion, meeting the core inclusion criterion of analyzing discrete audio tokenization; it also includes experimental validation and focuses on speech tasks, without relying solely on continuous features or traditional codecs, thus fulfilling the inclusion criteria and avoiding exclusion criteria.",
    "round-A_JuniorMini_evaluation": 5,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "include (junior:4)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "A Thorough Investigation of Content-Defined Chunking Algorithms for Data Deduplication",
    "abstract": "Data deduplication emerged as a powerful solution for reducing storage and bandwidth costs in cloud settings by eliminating redundancies at the level of chunks. This has spurred the development of numerous Content-Defined Chunking (CDC) algorithms over the past two decades. Despite advancements, the current state-of-the-art remains obscure, as a thorough and impartial analysis and comparison is lacking. We conduct a rigorous theoretical analysis and impartial experimental comparison of several leading CDC algorithms. Using four realistic datasets, we evaluate these algorithms against four key metrics: throughput, deduplication ratio, average chunk size, and chunk-size variance. Our analyses, in many instances, extend the findings of their original publications by reporting new results and putting existing ones into context. Moreover, we highlight limitations that have previously gone unnoticed. Our findings provide valuable insights that inform the selection and optimization of CDC algorithms for practical applications in data deduplication.",
    "metadata": {
      "arxiv_id": "2409.06066",
      "title": "A Thorough Investigation of Content-Defined Chunking Algorithms for Data Deduplication",
      "summary": "Data deduplication emerged as a powerful solution for reducing storage and bandwidth costs in cloud settings by eliminating redundancies at the level of chunks. This has spurred the development of numerous Content-Defined Chunking (CDC) algorithms over the past two decades. Despite advancements, the current state-of-the-art remains obscure, as a thorough and impartial analysis and comparison is lacking. We conduct a rigorous theoretical analysis and impartial experimental comparison of several leading CDC algorithms. Using four realistic datasets, we evaluate these algorithms against four key metrics: throughput, deduplication ratio, average chunk size, and chunk-size variance. Our analyses, in many instances, extend the findings of their original publications by reporting new results and putting existing ones into context. Moreover, we highlight limitations that have previously gone unnoticed. Our findings provide valuable insights that inform the selection and optimization of CDC algorithms for practical applications in data deduplication.",
      "authors": [
        "Marcel Gregoriadis",
        "Leonhard Balduf",
        "Björn Scheuermann",
        "Johan Pouwelse"
      ],
      "published": "2024-09-09T20:58:40Z",
      "updated": "2024-09-28T19:05:45Z",
      "categories": [
        "cs.DC"
      ],
      "pdf_url": "https://arxiv.org/pdf/2409.06066v3",
      "landing_url": "https://arxiv.org/abs/2409.06066v3",
      "doi": "https://doi.org/10.48550/arXiv.2409.06066"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item analyzes Content-Defined Chunking for data deduplication and contains no discrete audio/speech tokenization analysis, so it fails the inclusion criteria for discrete audio tokens and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item analyzes Content-Defined Chunking for data deduplication and contains no discrete audio/speech tokenization analysis, so it fails the inclusion criteria for discrete audio tokens and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on analyzing content-defined chunking algorithms for data deduplication in cloud storage, and does not involve discrete audio/speech tokenization or address audio-specific tokenization tasks as required by the inclusion criteria; thus, it does not meet the core requirements.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on analyzing content-defined chunking algorithms for data deduplication in cloud storage, and does not involve discrete audio/speech tokenization or address audio-specific tokenization tasks as required by the inclusion criteria; thus, it does not meet the core requirements.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "RobustSVC: HuBERT-based Melody Extractor and Adversarial Learning for Robust Singing Voice Conversion",
    "abstract": "Singing voice conversion (SVC) is hindered by noise sensitivity due to the use of non-robust methods for extracting pitch and energy during the inference. As clean signals are key for the source audio in SVC, music source separation preprocessing offers a viable solution for handling noisy audio, like singing with background music (BGM). However, current separating methods struggle to fully remove noise or excessively suppress signal components, affecting the naturalness and similarity of the processed audio. To tackle this, our study introduces RobustSVC, a novel any-to-one SVC framework that converts noisy vocals into clean vocals sung by the target singer. We replace the non-robust feature with a HuBERT-based melody extractor and use adversarial training mechanisms with three discriminators to reduce information leakage in self-supervised representations. Experimental results show that RobustSVC is noise-robust and achieves higher similarity and naturalness than baseline methods in both noisy and clean vocal conditions.",
    "metadata": {
      "arxiv_id": "2409.06237",
      "title": "RobustSVC: HuBERT-based Melody Extractor and Adversarial Learning for Robust Singing Voice Conversion",
      "summary": "Singing voice conversion (SVC) is hindered by noise sensitivity due to the use of non-robust methods for extracting pitch and energy during the inference. As clean signals are key for the source audio in SVC, music source separation preprocessing offers a viable solution for handling noisy audio, like singing with background music (BGM). However, current separating methods struggle to fully remove noise or excessively suppress signal components, affecting the naturalness and similarity of the processed audio. To tackle this, our study introduces RobustSVC, a novel any-to-one SVC framework that converts noisy vocals into clean vocals sung by the target singer. We replace the non-robust feature with a HuBERT-based melody extractor and use adversarial training mechanisms with three discriminators to reduce information leakage in self-supervised representations. Experimental results show that RobustSVC is noise-robust and achieves higher similarity and naturalness than baseline methods in both noisy and clean vocal conditions.",
      "authors": [
        "Wei Chen",
        "Xintao Zhao",
        "Jun Chen",
        "Binzhu Sha",
        "Zhiwei Lin",
        "Zhiyong Wu"
      ],
      "published": "2024-09-10T06:10:33Z",
      "updated": "2024-09-10T06:10:33Z",
      "categories": [
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2409.06237v1",
      "landing_url": "https://arxiv.org/abs/2409.06237v1",
      "doi": "https://doi.org/10.48550/arXiv.2409.06237"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item centers on an SVC system using a HuBERT-based melody extractor and adversarial learning; it does not explicitly propose, analyze, or evaluate discrete audio tokenization or a discrete token vocabulary for audio reconstruction/generation, which is required by the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item centers on an SVC system using a HuBERT-based melody extractor and adversarial learning; it does not explicitly propose, analyze, or evaluate discrete audio tokenization or a discrete token vocabulary for audio reconstruction/generation, which is required by the inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on singing voice conversion using a HuBERT-based melody extractor, which is a self-supervised model often used for feature extraction, but the abstract does not mention converting continuous audio into discrete tokens or analyzing discrete audio tokenization explicitly. It mainly discusses noise robustness and adversarial learning without evidence of discrete token design or analysis, nor does it emphasize tokenization as a core contribution. Therefore, it likely does not meet the inclusion criteria emphasizing discrete audio/speech tokenization as a core object of study.",
      "evaluation": 2
    },
    "round-A_JuniorMini_reasoning": "The study focuses on singing voice conversion using a HuBERT-based melody extractor, which is a self-supervised model often used for feature extraction, but the abstract does not mention converting continuous audio into discrete tokens or analyzing discrete audio tokenization explicitly. It mainly discusses noise robustness and adversarial learning without evidence of discrete token design or analysis, nor does it emphasize tokenization as a core contribution. Therefore, it likely does not meet the inclusion criteria emphasizing discrete audio/speech tokenization as a core object of study.",
    "round-A_JuniorMini_evaluation": 2,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:2)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:2)"
  },
  {
    "title": "Learning Multi-Aspect Item Palette: A Semantic Tokenization Framework for Generative Recommendation",
    "abstract": "Traditional recommendation models often rely on unique item identifiers (IDs) to distinguish between items, which can hinder their ability to effectively leverage item content information and generalize to long-tailed or cold-start items. Recently, semantic tokenization has been proposed as a promising solution that aims to tokenize each item's semantic representation into a sequence of discrete tokens. These semantic tokens have become fundamental in training generative recommendation models. However, existing methods typically rely on RQ-VAE, a residual vector quantizer, for semantic tokenization. This reliance introduces several key limitations, including challenges in embedding extraction, hierarchical coarse-to-fine quantization, and training stability. To address these issues, we introduce LAMIA, a novel approach for multi-aspect semantic tokenization. Unlike RQ-VAE, which uses a single embedding, LAMIA learns an ``item palette''--a collection of independent and semantically parallel embeddings that capture multiple aspects of items. Additionally, LAMIA enhances the semantic encoders through domain-specific tuning using text-based reconstruction tasks, resulting in more representative item palette embeddings. We have conducted extensive experiments to validate the effectiveness of the LAMIA framework across various recommendation tasks and datasets. Our results demonstrate significant improvements in recommendation accuracy over existing methods. To facilitate reproducible research, we will release the source code, data, and configurations.",
    "metadata": {
      "arxiv_id": "2409.07276",
      "title": "Learning Multi-Aspect Item Palette: A Semantic Tokenization Framework for Generative Recommendation",
      "summary": "Traditional recommendation models often rely on unique item identifiers (IDs) to distinguish between items, which can hinder their ability to effectively leverage item content information and generalize to long-tailed or cold-start items. Recently, semantic tokenization has been proposed as a promising solution that aims to tokenize each item's semantic representation into a sequence of discrete tokens. These semantic tokens have become fundamental in training generative recommendation models. However, existing methods typically rely on RQ-VAE, a residual vector quantizer, for semantic tokenization. This reliance introduces several key limitations, including challenges in embedding extraction, hierarchical coarse-to-fine quantization, and training stability. To address these issues, we introduce LAMIA, a novel approach for multi-aspect semantic tokenization. Unlike RQ-VAE, which uses a single embedding, LAMIA learns an ``item palette''--a collection of independent and semantically parallel embeddings that capture multiple aspects of items. Additionally, LAMIA enhances the semantic encoders through domain-specific tuning using text-based reconstruction tasks, resulting in more representative item palette embeddings. We have conducted extensive experiments to validate the effectiveness of the LAMIA framework across various recommendation tasks and datasets. Our results demonstrate significant improvements in recommendation accuracy over existing methods. To facilitate reproducible research, we will release the source code, data, and configurations.",
      "authors": [
        "Qijiong Liu",
        "Jieming Zhu",
        "Zhaocheng Du",
        "Lu Fan",
        "Zhou Zhao",
        "Xiao-Ming Wu"
      ],
      "published": "2024-09-11T13:49:48Z",
      "updated": "2025-08-05T11:07:31Z",
      "categories": [
        "cs.IR"
      ],
      "pdf_url": "https://arxiv.org/pdf/2409.07276v3",
      "landing_url": "https://arxiv.org/abs/2409.07276v3",
      "doi": "https://doi.org/10.48550/arXiv.2409.07276"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The paper focuses on semantic tokenization of item content for recommender systems, not on discrete audio/speech tokenization, so it does not meet the inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The paper focuses on semantic tokenization of item content for recommender systems, not on discrete audio/speech tokenization, so it does not meet the inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on semantic tokenization for recommendation systems involving item representations, with no mention or indication of tokenization of continuous audio signals into discrete audio tokens for speech or audio-specific tasks; it does not meet the inclusion criterion related to audio/speech tokenization, nor does it address discrete audio tokenization methodologies relevant to speech or audio reconstruction/generation.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on semantic tokenization for recommendation systems involving item representations, with no mention or indication of tokenization of continuous audio signals into discrete audio tokens for speech or audio-specific tasks; it does not meet the inclusion criterion related to audio/speech tokenization, nor does it address discrete audio tokenization methodologies relevant to speech or audio reconstruction/generation.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "SSR-Speech: Towards Stable, Safe and Robust Zero-shot Text-based Speech Editing and Synthesis",
    "abstract": "In this paper, we introduce SSR-Speech, a neural codec autoregressive model designed for stable, safe, and robust zero-shot textbased speech editing and text-to-speech synthesis. SSR-Speech is built on a Transformer decoder and incorporates classifier-free guidance to enhance the stability of the generation process. A watermark Encodec is proposed to embed frame-level watermarks into the edited regions of the speech so that which parts were edited can be detected. In addition, the waveform reconstruction leverages the original unedited speech segments, providing superior recovery compared to the Encodec model. Our approach achieves state-of-the-art performance in the RealEdit speech editing task and the LibriTTS text-to-speech task, surpassing previous methods. Furthermore, SSR-Speech excels in multi-span speech editing and also demonstrates remarkable robustness to background sounds. The source code and demos are released.",
    "metadata": {
      "arxiv_id": "2409.07556",
      "title": "SSR-Speech: Towards Stable, Safe and Robust Zero-shot Text-based Speech Editing and Synthesis",
      "summary": "In this paper, we introduce SSR-Speech, a neural codec autoregressive model designed for stable, safe, and robust zero-shot textbased speech editing and text-to-speech synthesis. SSR-Speech is built on a Transformer decoder and incorporates classifier-free guidance to enhance the stability of the generation process. A watermark Encodec is proposed to embed frame-level watermarks into the edited regions of the speech so that which parts were edited can be detected. In addition, the waveform reconstruction leverages the original unedited speech segments, providing superior recovery compared to the Encodec model. Our approach achieves state-of-the-art performance in the RealEdit speech editing task and the LibriTTS text-to-speech task, surpassing previous methods. Furthermore, SSR-Speech excels in multi-span speech editing and also demonstrates remarkable robustness to background sounds. The source code and demos are released.",
      "authors": [
        "Helin Wang",
        "Meng Yu",
        "Jiarui Hai",
        "Chen Chen",
        "Yuchen Hu",
        "Rilin Chen",
        "Najim Dehak",
        "Dong Yu"
      ],
      "published": "2024-09-11T18:24:07Z",
      "updated": "2025-01-02T03:07:29Z",
      "categories": [
        "eess.AS",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2409.07556v2",
      "landing_url": "https://arxiv.org/abs/2409.07556v2",
      "doi": "https://doi.org/10.48550/arXiv.2409.07556"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The abstract does not explicitly address discrete audio tokenization as a core object of study; it focuses on stable/robust speech editing, synthesis, and watermarking rather than analyzing or evaluating discrete tokenization.",
      "evaluation": 2
    },
    "round-A_JuniorNano_reasoning": "The abstract does not explicitly address discrete audio tokenization as a core object of study; it focuses on stable/robust speech editing, synthesis, and watermarking rather than analyzing or evaluating discrete tokenization.",
    "round-A_JuniorNano_evaluation": 2,
    "round-A_JuniorMini_output": {
      "reasoning": "The study presents SSR-Speech, a neural codec autoregressive model that involves discrete token-based speech editing and synthesis, indicating a focus on discrete audio tokenization, and provides state-of-the-art results with methodological contributions rather than just engineering reports, fitting all inclusion and no exclusion criteria.",
      "evaluation": 5
    },
    "round-A_JuniorMini_reasoning": "The study presents SSR-Speech, a neural codec autoregressive model that involves discrete token-based speech editing and synthesis, indicating a focus on discrete audio tokenization, and provides state-of-the-art results with methodological contributions rather than just engineering reports, fitting all inclusion and no exclusion criteria.",
    "round-A_JuniorMini_evaluation": 5,
    "round-B_SeniorLead_output": {
      "reasoning": "I reviewed both junior reviewers' comments (JuniorNano: argues the abstract does not explicitly address discrete audio tokenization; JuniorMini: interprets the neural codec/autoregressive framing as involving discrete tokens) and the inclusion rule that the study must explicitly propose, analyze, or evaluate discrete audio/speech tokenization as a core object of study. The abstract describes a neural codec autoregressive model, watermarking, and improved editing/synthesis performance, but it does not explicitly state that discrete tokenization design, evaluation, or analysis is a primary contribution—only implies use of a neural codec (Encodec) without clear focus on discrete-token methods. Because the inclusion criteria require an explicit focus on discrete audio tokenization and the abstract lacks that explicit emphasis, I recommend exclusion.",
      "evaluation": 2
    },
    "round-B_SeniorLead_reasoning": "I reviewed both junior reviewers' comments (JuniorNano: argues the abstract does not explicitly address discrete audio tokenization; JuniorMini: interprets the neural codec/autoregressive framing as involving discrete tokens) and the inclusion rule that the study must explicitly propose, analyze, or evaluate discrete audio/speech tokenization as a core object of study. The abstract describes a neural codec autoregressive model, watermarking, and improved editing/synthesis performance, but it does not explicitly state that discrete tokenization design, evaluation, or analysis is a primary contribution—only implies use of a neural codec (Encodec) without clear focus on discrete-token methods. Because the inclusion criteria require an explicit focus on discrete audio tokenization and the abstract lacks that explicit emphasis, I recommend exclusion.",
    "round-B_SeniorLead_evaluation": 2,
    "final_verdict": "exclude (senior:2)",
    "review_skipped": false,
    "discard_reason": "exclude (senior:2)"
  },
  {
    "title": "Unleash LLMs Potential for Recommendation by Coordinating Twin-Tower Dynamic Semantic Token Generator",
    "abstract": "Owing to the unprecedented capability in semantic understanding and logical reasoning, the pre-trained large language models (LLMs) have shown fantastic potential in developing the next-generation recommender systems (RSs). However, the static index paradigm adopted by current methods greatly restricts the utilization of LLMs capacity for recommendation, leading to not only the insufficient alignment between semantic and collaborative knowledge, but also the neglect of high-order user-item interaction patterns. In this paper, we propose Twin-Tower Dynamic Semantic Recommender (TTDS), the first generative RS which adopts dynamic semantic index paradigm, targeting at resolving the above problems simultaneously. To be more specific, we for the first time contrive a dynamic knowledge fusion framework which integrates a twin-tower semantic token generator into the LLM-based recommender, hierarchically allocating meaningful semantic index for items and users, and accordingly predicting the semantic index of target item. Furthermore, a dual-modality variational auto-encoder is proposed to facilitate multi-grained alignment between semantic and collaborative knowledge. Eventually, a series of novel tuning tasks specially customized for capturing high-order user-item interaction patterns are proposed to take advantages of user historical behavior. Extensive experiments across three public datasets demonstrate the superiority of the proposed methodology in developing LLM-based generative RSs. The proposed TTDS recommender achieves an average improvement of 19.41% in Hit-Rate and 20.84% in NDCG metric, compared with the leading baseline methods.",
    "metadata": {
      "arxiv_id": "2409.09253",
      "title": "Unleash LLMs Potential for Recommendation by Coordinating Twin-Tower Dynamic Semantic Token Generator",
      "summary": "Owing to the unprecedented capability in semantic understanding and logical reasoning, the pre-trained large language models (LLMs) have shown fantastic potential in developing the next-generation recommender systems (RSs). However, the static index paradigm adopted by current methods greatly restricts the utilization of LLMs capacity for recommendation, leading to not only the insufficient alignment between semantic and collaborative knowledge, but also the neglect of high-order user-item interaction patterns. In this paper, we propose Twin-Tower Dynamic Semantic Recommender (TTDS), the first generative RS which adopts dynamic semantic index paradigm, targeting at resolving the above problems simultaneously. To be more specific, we for the first time contrive a dynamic knowledge fusion framework which integrates a twin-tower semantic token generator into the LLM-based recommender, hierarchically allocating meaningful semantic index for items and users, and accordingly predicting the semantic index of target item. Furthermore, a dual-modality variational auto-encoder is proposed to facilitate multi-grained alignment between semantic and collaborative knowledge. Eventually, a series of novel tuning tasks specially customized for capturing high-order user-item interaction patterns are proposed to take advantages of user historical behavior. Extensive experiments across three public datasets demonstrate the superiority of the proposed methodology in developing LLM-based generative RSs. The proposed TTDS recommender achieves an average improvement of 19.41% in Hit-Rate and 20.84% in NDCG metric, compared with the leading baseline methods.",
      "authors": [
        "Jun Yin",
        "Zhengxin Zeng",
        "Mingzheng Li",
        "Hao Yan",
        "Chaozhuo Li",
        "Weihao Han",
        "Jianjin Zhang",
        "Ruochen Liu",
        "Allen Sun",
        "Denvy Deng",
        "Feng Sun",
        "Qi Zhang",
        "Shirui Pan",
        "Senzhang Wang"
      ],
      "published": "2024-09-14T01:45:04Z",
      "updated": "2024-09-14T01:45:04Z",
      "categories": [
        "cs.IR",
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2409.09253v1",
      "landing_url": "https://arxiv.org/abs/2409.09253v1",
      "doi": "https://doi.org/10.48550/arXiv.2409.09253"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The manuscript centers on LLM-based recommender systems and a dynamic semantic token framework for recommendations; it does not address discrete audio/speech tokenization or discrete audio token representations, so it does not meet the inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The manuscript centers on LLM-based recommender systems and a dynamic semantic token framework for recommendations; it does not address discrete audio/speech tokenization or discrete audio token representations, so it does not meet the inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on developing a large language model-based recommender system using dynamic semantic token generation, but it does not discuss or analyze discrete audio or speech tokenization, nor does it involve converting continuous audio signals into discrete tokens for audio tasks as required by the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on developing a large language model-based recommender system using dynamic semantic token generation, but it does not discuss or analyze discrete audio or speech tokenization, nor does it involve converting continuous audio signals into discrete tokens for audio tasks as required by the inclusion criteria.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Joint Semantic Knowledge Distillation and Masked Acoustic Modeling for Full-band Speech Restoration with Improved Intelligibility",
    "abstract": "Speech restoration aims at restoring full-band speech with high quality and intelligibility, considering a diverse set of distortions. MaskSR is a recently proposed generative model for this task. As other models of its kind, MaskSR attains high quality but, as we show, intelligibility can be substantially improved. We do so by boosting the speech encoder component of MaskSR with predictions of semantic representations of the target speech, using a pre-trained self-supervised teacher model. Then, a masked language model is conditioned on the learned semantic features to predict acoustic tokens that encode low level spectral details of the target speech. We show that, with the same MaskSR model capacity and inference time, the proposed model, MaskSR2, significantly reduces the word error rate, a typical metric for intelligibility. MaskSR2 also achieves competitive word error rate among other models, while providing superior quality. An ablation study shows the effectiveness of various semantic representations.",
    "metadata": {
      "arxiv_id": "2409.09357",
      "title": "Joint Semantic Knowledge Distillation and Masked Acoustic Modeling for Full-band Speech Restoration with Improved Intelligibility",
      "summary": "Speech restoration aims at restoring full-band speech with high quality and intelligibility, considering a diverse set of distortions. MaskSR is a recently proposed generative model for this task. As other models of its kind, MaskSR attains high quality but, as we show, intelligibility can be substantially improved. We do so by boosting the speech encoder component of MaskSR with predictions of semantic representations of the target speech, using a pre-trained self-supervised teacher model. Then, a masked language model is conditioned on the learned semantic features to predict acoustic tokens that encode low level spectral details of the target speech. We show that, with the same MaskSR model capacity and inference time, the proposed model, MaskSR2, significantly reduces the word error rate, a typical metric for intelligibility. MaskSR2 also achieves competitive word error rate among other models, while providing superior quality. An ablation study shows the effectiveness of various semantic representations.",
      "authors": [
        "Xiaoyu Liu",
        "Xu Li",
        "Joan Serrà",
        "Santiago Pascual"
      ],
      "published": "2024-09-14T08:09:55Z",
      "updated": "2024-09-14T08:09:55Z",
      "categories": [
        "cs.SD",
        "cs.AI",
        "eess.AS",
        "eess.SP"
      ],
      "pdf_url": "https://arxiv.org/pdf/2409.09357v1",
      "landing_url": "https://arxiv.org/abs/2409.09357v1",
      "doi": "https://doi.org/10.48550/arXiv.2409.09357"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The abstract centers on discrete acoustic tokenization and token-based modeling to reconstruct speech, i.e., converting continuous audio into discrete tokens for restoration, which aligns with the inclusion criteria; the abstract is in English, but full-text availability is not stated.",
      "evaluation": 4
    },
    "round-A_JuniorNano_reasoning": "The abstract centers on discrete acoustic tokenization and token-based modeling to reconstruct speech, i.e., converting continuous audio into discrete tokens for restoration, which aligns with the inclusion criteria; the abstract is in English, but full-text availability is not stated.",
    "round-A_JuniorNano_evaluation": 4,
    "round-A_JuniorMini_output": {
      "reasoning": "The study proposes and analyzes a model (MaskSR2) that predicts acoustic tokens encoding spectral details, conditioned on semantic features; this implies discrete audio tokenization plays a core role in their speech restoration method. The abstract suggests use of discrete acoustic tokens for audio reconstruction with evaluation on intelligibility metrics, aligning well with the inclusion criteria and without triggers of exclusion criteria.",
      "evaluation": 5
    },
    "round-A_JuniorMini_reasoning": "The study proposes and analyzes a model (MaskSR2) that predicts acoustic tokens encoding spectral details, conditioned on semantic features; this implies discrete audio tokenization plays a core role in their speech restoration method. The abstract suggests use of discrete acoustic tokens for audio reconstruction with evaluation on intelligibility metrics, aligning well with the inclusion criteria and without triggers of exclusion criteria.",
    "round-A_JuniorMini_evaluation": 5,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "include (junior:4)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Towards Real-Time Generation of Delay-Compensated Video Feeds for Outdoor Mobile Robot Teleoperation",
    "abstract": "Teleoperation is an important technology to enable supervisors to control agricultural robots remotely. However, environmental factors in dense crop rows and limitations in network infrastructure hinder the reliability of data streamed to teleoperators. These issues result in delayed and variable frame rate video feeds that often deviate significantly from the robot's actual viewpoint. We propose a modular learning-based vision pipeline to generate delay-compensated images in real-time for supervisors. Our extensive offline evaluations demonstrate that our method generates more accurate images compared to state-of-the-art approaches in our setting. Additionally, ours is one of the few works to evaluate a delay-compensation method in outdoor field environments with complex terrain on data from a real robot in real-time. Resulting videos and code are provided at https://sites.google.com/illinois.edu/comp-teleop.",
    "metadata": {
      "arxiv_id": "2409.09921",
      "title": "Towards Real-Time Generation of Delay-Compensated Video Feeds for Outdoor Mobile Robot Teleoperation",
      "summary": "Teleoperation is an important technology to enable supervisors to control agricultural robots remotely. However, environmental factors in dense crop rows and limitations in network infrastructure hinder the reliability of data streamed to teleoperators. These issues result in delayed and variable frame rate video feeds that often deviate significantly from the robot's actual viewpoint. We propose a modular learning-based vision pipeline to generate delay-compensated images in real-time for supervisors. Our extensive offline evaluations demonstrate that our method generates more accurate images compared to state-of-the-art approaches in our setting. Additionally, ours is one of the few works to evaluate a delay-compensation method in outdoor field environments with complex terrain on data from a real robot in real-time. Resulting videos and code are provided at https://sites.google.com/illinois.edu/comp-teleop.",
      "authors": [
        "Neeloy Chakraborty",
        "Yixiao Fang",
        "Andre Schreiber",
        "Tianchen Ji",
        "Zhe Huang",
        "Aganze Mihigo",
        "Cassidy Wall",
        "Abdulrahman Almana",
        "Katherine Driggs-Campbell"
      ],
      "published": "2024-09-16T01:39:50Z",
      "updated": "2025-02-16T23:43:14Z",
      "categories": [
        "cs.RO",
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2409.09921v2",
      "landing_url": "https://arxiv.org/abs/2409.09921v2",
      "doi": "https://doi.org/10.48550/arXiv.2409.09921"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item targets real-time delay-compensated video generation for outdoor robot teleoperation and provides no discrete audio/speech tokenization analysis or token-based representations, so it fails the inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item targets real-time delay-compensated video generation for outdoor robot teleoperation and provides no discrete audio/speech tokenization analysis or token-based representations, so it fails the inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on delay-compensated video feeds for mobile robot teleoperation and does not address discrete audio/speech tokenization, continuous audio signal conversion, or neural discrete token representations; thus, it does not meet the inclusion criteria related to audio tokenization.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on delay-compensated video feeds for mobile robot teleoperation and does not address discrete audio/speech tokenization, continuous audio signal conversion, or neural discrete token representations; thus, it does not meet the inclusion criteria related to audio tokenization.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Self-Supervised Syllable Discovery Based on Speaker-Disentangled HuBERT",
    "abstract": "Self-supervised speech representation learning has become essential for extracting meaningful features from untranscribed audio. Recent advances highlight the potential of deriving discrete symbols from the features correlated with linguistic units, which enables text-less training across diverse tasks. In particular, sentence-level Self-Distillation of the pretrained HuBERT (SD-HuBERT) induces syllabic structures within latent speech frame representations extracted from an intermediate Transformer layer. In SD-HuBERT, sentence-level representation is accumulated from speech frame features through self-attention layers using a special CLS token. However, we observe that the information aggregated in the CLS token correlates more with speaker identity than with linguistic content. To address this, we propose a speech-only self-supervised fine-tuning approach that separates syllabic units from speaker information. Our method introduces speaker perturbation as data augmentation and adopts a frame-level training objective to prevent the CLS token from aggregating paralinguistic information. Experimental results show that our approach surpasses the current state-of-the-art method in most syllable segmentation and syllabic unit quality metrics on Librispeech, underscoring its effectiveness in promoting syllabic organization within speech-only models.",
    "metadata": {
      "arxiv_id": "2409.10103",
      "title": "Self-Supervised Syllable Discovery Based on Speaker-Disentangled HuBERT",
      "summary": "Self-supervised speech representation learning has become essential for extracting meaningful features from untranscribed audio. Recent advances highlight the potential of deriving discrete symbols from the features correlated with linguistic units, which enables text-less training across diverse tasks. In particular, sentence-level Self-Distillation of the pretrained HuBERT (SD-HuBERT) induces syllabic structures within latent speech frame representations extracted from an intermediate Transformer layer. In SD-HuBERT, sentence-level representation is accumulated from speech frame features through self-attention layers using a special CLS token. However, we observe that the information aggregated in the CLS token correlates more with speaker identity than with linguistic content. To address this, we propose a speech-only self-supervised fine-tuning approach that separates syllabic units from speaker information. Our method introduces speaker perturbation as data augmentation and adopts a frame-level training objective to prevent the CLS token from aggregating paralinguistic information. Experimental results show that our approach surpasses the current state-of-the-art method in most syllable segmentation and syllabic unit quality metrics on Librispeech, underscoring its effectiveness in promoting syllabic organization within speech-only models.",
      "authors": [
        "Ryota Komatsu",
        "Takahiro Shinozaki"
      ],
      "published": "2024-09-16T09:07:08Z",
      "updated": "2024-09-16T09:07:08Z",
      "categories": [
        "cs.CL",
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2409.10103v1",
      "landing_url": "https://arxiv.org/abs/2409.10103v1",
      "doi": "https://doi.org/10.48550/arXiv.2409.10103"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The work explicitly targets discrete speech tokenization by discovering syllabic units (discrete symbols) from continuous speech representations, addressing tokenization for speech tasks, and reporting improvements in syllable-related metrics, which aligns with the inclusion criteria and excludes only non-tokenization-focused studies.",
      "evaluation": 4
    },
    "round-A_JuniorNano_reasoning": "The work explicitly targets discrete speech tokenization by discovering syllabic units (discrete symbols) from continuous speech representations, addressing tokenization for speech tasks, and reporting improvements in syllable-related metrics, which aligns with the inclusion criteria and excludes only non-tokenization-focused studies.",
    "round-A_JuniorNano_evaluation": 4,
    "round-A_JuniorMini_output": {
      "reasoning": "The study explicitly focuses on self-supervised speech representation learning, deriving discrete symbols (syllabic units) from continuous speech features, effectively addressing discrete tokenization in speech; it proposes and evaluates a method that separates speaker identity from syllabic units, with experimental validation on syllable segmentation and syllabic unit quality, thus meeting the inclusion criteria without violating any exclusion criteria.",
      "evaluation": 5
    },
    "round-A_JuniorMini_reasoning": "The study explicitly focuses on self-supervised speech representation learning, deriving discrete symbols (syllabic units) from continuous speech features, effectively addressing discrete tokenization in speech; it proposes and evaluates a method that separates speaker identity from syllabic units, with experimental validation on syllable segmentation and syllabic unit quality, thus meeting the inclusion criteria without violating any exclusion criteria.",
    "round-A_JuniorMini_evaluation": 5,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "include (junior:4)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Adaptive Large Language Models By Layerwise Attention Shortcuts",
    "abstract": "Transformer architectures are the backbone of the modern AI revolution. However, they are based on simply stacking the same blocks in dozens of layers and processing information sequentially from one block to another. In this paper, we propose to challenge this and introduce adaptive computations for LLM-like setups, which allow the final layer to attend to all of the intermediate layers as it deems fit through the attention mechanism, thereby introducing computational \\textbf{attention shortcuts}. These shortcuts can thus make the architecture depth and context adaptive. We showcase four different datasets, namely acoustic tokens, natural language, and symbolic music, and we achieve superior performance for GPT-like architecture. We give evidence via attention maps that the models learn complex dependencies across layers that are adaptive in context and depth depending on the input tokens.",
    "metadata": {
      "arxiv_id": "2409.10870",
      "title": "Adaptive Large Language Models By Layerwise Attention Shortcuts",
      "summary": "Transformer architectures are the backbone of the modern AI revolution. However, they are based on simply stacking the same blocks in dozens of layers and processing information sequentially from one block to another. In this paper, we propose to challenge this and introduce adaptive computations for LLM-like setups, which allow the final layer to attend to all of the intermediate layers as it deems fit through the attention mechanism, thereby introducing computational \\textbf{attention shortcuts}. These shortcuts can thus make the architecture depth and context adaptive. We showcase four different datasets, namely acoustic tokens, natural language, and symbolic music, and we achieve superior performance for GPT-like architecture. We give evidence via attention maps that the models learn complex dependencies across layers that are adaptive in context and depth depending on the input tokens.",
      "authors": [
        "Prateek Verma",
        "Mert Pilanci"
      ],
      "published": "2024-09-17T03:46:01Z",
      "updated": "2024-09-17T03:46:01Z",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG",
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2409.10870v1",
      "landing_url": "https://arxiv.org/abs/2409.10870v1",
      "doi": "https://doi.org/10.48550/arXiv.2409.10870"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The paper focuses on adaptive layerwise attention in LLMs and does not explicitly analyze or propose discrete audio/speech tokenization as a core object, despite mentioning 'acoustic tokens' as a dataset.",
      "evaluation": 2
    },
    "round-A_JuniorNano_reasoning": "The paper focuses on adaptive layerwise attention in LLMs and does not explicitly analyze or propose discrete audio/speech tokenization as a core object, despite mentioning 'acoustic tokens' as a dataset.",
    "round-A_JuniorNano_evaluation": 2,
    "round-A_JuniorMini_output": {
      "reasoning": "The abstract mentions acoustic tokens among the datasets used, but it does not explicitly focus on discrete audio/speech tokenization as a core object of study nor analyze or evaluate discrete tokenization in depth; rather, the study centers on adaptive large language models and architectural innovations involving attention shortcuts across layers, with no clear evidence of methodological contributions or analysis specific to discrete audio token representations relevant to speech.",
      "evaluation": 2
    },
    "round-A_JuniorMini_reasoning": "The abstract mentions acoustic tokens among the datasets used, but it does not explicitly focus on discrete audio/speech tokenization as a core object of study nor analyze or evaluate discrete tokenization in depth; rather, the study centers on adaptive large language models and architectural innovations involving attention shortcuts across layers, with no clear evidence of methodological contributions or analysis specific to discrete audio token representations relevant to speech.",
    "round-A_JuniorMini_evaluation": 2,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:2)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:2)"
  },
  {
    "title": "Single-stage TTS with Masked Audio Token Modeling and Semantic Knowledge Distillation",
    "abstract": "Audio token modeling has become a powerful framework for speech synthesis, with two-stage approaches employing semantic tokens remaining prevalent. In this paper, we aim to simplify this process by introducing a semantic knowledge distillation method that enables high-quality speech generation in a single stage. Our proposed model improves speech quality, intelligibility, and speaker similarity compared to a single-stage baseline. Although two-stage systems still lead in intelligibility, our model significantly narrows the gap while delivering comparable speech quality. These findings showcase the potential of single-stage models to achieve efficient, high-quality TTS with a more compact and streamlined architecture.",
    "metadata": {
      "arxiv_id": "2409.11003",
      "title": "Single-stage TTS with Masked Audio Token Modeling and Semantic Knowledge Distillation",
      "summary": "Audio token modeling has become a powerful framework for speech synthesis, with two-stage approaches employing semantic tokens remaining prevalent. In this paper, we aim to simplify this process by introducing a semantic knowledge distillation method that enables high-quality speech generation in a single stage. Our proposed model improves speech quality, intelligibility, and speaker similarity compared to a single-stage baseline. Although two-stage systems still lead in intelligibility, our model significantly narrows the gap while delivering comparable speech quality. These findings showcase the potential of single-stage models to achieve efficient, high-quality TTS with a more compact and streamlined architecture.",
      "authors": [
        "Gerard I. Gállego",
        "Roy Fejgin",
        "Chunghsin Yeh",
        "Xiaoyu Liu",
        "Gautam Bhattacharya"
      ],
      "published": "2024-09-17T09:08:43Z",
      "updated": "2024-09-17T09:08:43Z",
      "categories": [
        "cs.SD",
        "cs.AI",
        "eess.AS",
        "eess.SP"
      ],
      "pdf_url": "https://arxiv.org/pdf/2409.11003v1",
      "landing_url": "https://arxiv.org/abs/2409.11003v1",
      "doi": "https://doi.org/10.48550/arXiv.2409.11003"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The abstract centers on discrete audio tokenization (masked audio tokens/semantic tokens) as the core framework for a single-stage TTS, aligning with the inclusion criteria.",
      "evaluation": 4
    },
    "round-A_JuniorNano_reasoning": "The abstract centers on discrete audio tokenization (masked audio tokens/semantic tokens) as the core framework for a single-stage TTS, aligning with the inclusion criteria.",
    "round-A_JuniorNano_evaluation": 4,
    "round-A_JuniorMini_output": {
      "reasoning": "The study explicitly proposes a single-stage TTS model with masked audio token modeling and semantic knowledge distillation, indicating that discrete audio tokenization is central to the approach; the abstract highlights improving speech quality and intelligibility through these tokens, fitting the inclusion criteria without contradicting any exclusion criteria.",
      "evaluation": 5
    },
    "round-A_JuniorMini_reasoning": "The study explicitly proposes a single-stage TTS model with masked audio token modeling and semantic knowledge distillation, indicating that discrete audio tokenization is central to the approach; the abstract highlights improving speech quality and intelligibility through these tokens, fitting the inclusion criteria without contradicting any exclusion criteria.",
    "round-A_JuniorMini_evaluation": 5,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "include (junior:4)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Learning Source Disentanglement in Neural Audio Codec",
    "abstract": "Neural audio codecs have significantly advanced audio compression by efficiently converting continuous audio signals into discrete tokens. These codecs preserve high-quality sound and enable sophisticated sound generation through generative models trained on these tokens. However, existing neural codec models are typically trained on large, undifferentiated audio datasets, neglecting the essential discrepancies between sound domains like speech, music, and environmental sound effects. This oversight complicates data modeling and poses additional challenges to the controllability of sound generation. To tackle these issues, we introduce the Source-Disentangled Neural Audio Codec (SD-Codec), a novel approach that combines audio coding and source separation. By jointly learning audio resynthesis and separation, SD-Codec explicitly assigns audio signals from different domains to distinct codebooks, sets of discrete representations. Experimental results indicate that SD-Codec not only maintains competitive resynthesis quality but also, supported by the separation results, demonstrates successful disentanglement of different sources in the latent space, thereby enhancing interpretability in audio codec and providing potential finer control over the audio generation process.",
    "metadata": {
      "arxiv_id": "2409.11228",
      "title": "Learning Source Disentanglement in Neural Audio Codec",
      "summary": "Neural audio codecs have significantly advanced audio compression by efficiently converting continuous audio signals into discrete tokens. These codecs preserve high-quality sound and enable sophisticated sound generation through generative models trained on these tokens. However, existing neural codec models are typically trained on large, undifferentiated audio datasets, neglecting the essential discrepancies between sound domains like speech, music, and environmental sound effects. This oversight complicates data modeling and poses additional challenges to the controllability of sound generation. To tackle these issues, we introduce the Source-Disentangled Neural Audio Codec (SD-Codec), a novel approach that combines audio coding and source separation. By jointly learning audio resynthesis and separation, SD-Codec explicitly assigns audio signals from different domains to distinct codebooks, sets of discrete representations. Experimental results indicate that SD-Codec not only maintains competitive resynthesis quality but also, supported by the separation results, demonstrates successful disentanglement of different sources in the latent space, thereby enhancing interpretability in audio codec and providing potential finer control over the audio generation process.",
      "authors": [
        "Xiaoyu Bie",
        "Xubo Liu",
        "Gaël Richard"
      ],
      "published": "2024-09-17T14:21:02Z",
      "updated": "2025-02-11T10:35:04Z",
      "categories": [
        "cs.SD",
        "cs.AI",
        "cs.LG",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2409.11228v2",
      "landing_url": "https://arxiv.org/abs/2409.11228v2",
      "doi": "https://doi.org/10.48550/arXiv.2409.11228"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "It explicitly proposes and evaluates a neural audio codec that uses discrete codebooks (tokens) for audio resynthesis and source separation, i.e., discrete audio tokenization as the core object of study, meeting inclusion and not falling under exclusion.",
      "evaluation": 5
    },
    "round-A_JuniorNano_reasoning": "It explicitly proposes and evaluates a neural audio codec that uses discrete codebooks (tokens) for audio resynthesis and source separation, i.e., discrete audio tokenization as the core object of study, meeting inclusion and not falling under exclusion.",
    "round-A_JuniorNano_evaluation": 5,
    "round-A_JuniorMini_output": {
      "reasoning": "The study introduces a Source-Disentangled Neural Audio Codec that discretizes audio signals into distinct codebooks for various sound domains, explicitly addressing discrete tokenization of continuous audio for reconstruction, generation, and enhanced interpretability; it uses neural codec models rather than traditional codecs and provides experimental validation, satisfying all inclusion criteria and none of the exclusions.",
      "evaluation": 5
    },
    "round-A_JuniorMini_reasoning": "The study introduces a Source-Disentangled Neural Audio Codec that discretizes audio signals into distinct codebooks for various sound domains, explicitly addressing discrete tokenization of continuous audio for reconstruction, generation, and enhanced interpretability; it uses neural codec models rather than traditional codecs and provides experimental validation, satisfying all inclusion criteria and none of the exclusions.",
    "round-A_JuniorMini_evaluation": 5,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "include (junior:5)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Speaking from Coarse to Fine: Improving Neural Codec Language Model via Multi-Scale Speech Coding and Generation",
    "abstract": "The neural codec language model (CLM) has demonstrated remarkable performance in text-to-speech (TTS) synthesis. However, troubled by ``recency bias\", CLM lacks sufficient attention to coarse-grained information at a higher temporal scale, often producing unnatural or even unintelligible speech. This work proposes CoFi-Speech, a coarse-to-fine CLM-TTS approach, employing multi-scale speech coding and generation to address this issue. We train a multi-scale neural codec, CoFi-Codec, to encode speech into a multi-scale discrete representation, comprising multiple token sequences with different time resolutions. Then, we propose CoFi-LM that can generate this representation in two modes: the single-LM-based chain-of-scale generation and the multiple-LM-based stack-of-scale generation. In experiments, CoFi-Speech significantly outperforms single-scale baseline systems on naturalness and speaker similarity in zero-shot TTS. The analysis of multi-scale coding demonstrates the effectiveness of CoFi-Codec in learning multi-scale discrete speech representations while keeping high-quality speech reconstruction. The coarse-to-fine multi-scale generation, especially for the stack-of-scale approach, is also validated as a crucial approach in pursuing a high-quality neural codec language model for TTS.",
    "metadata": {
      "arxiv_id": "2409.11630",
      "title": "Speaking from Coarse to Fine: Improving Neural Codec Language Model via Multi-Scale Speech Coding and Generation",
      "summary": "The neural codec language model (CLM) has demonstrated remarkable performance in text-to-speech (TTS) synthesis. However, troubled by ``recency bias\", CLM lacks sufficient attention to coarse-grained information at a higher temporal scale, often producing unnatural or even unintelligible speech. This work proposes CoFi-Speech, a coarse-to-fine CLM-TTS approach, employing multi-scale speech coding and generation to address this issue. We train a multi-scale neural codec, CoFi-Codec, to encode speech into a multi-scale discrete representation, comprising multiple token sequences with different time resolutions. Then, we propose CoFi-LM that can generate this representation in two modes: the single-LM-based chain-of-scale generation and the multiple-LM-based stack-of-scale generation. In experiments, CoFi-Speech significantly outperforms single-scale baseline systems on naturalness and speaker similarity in zero-shot TTS. The analysis of multi-scale coding demonstrates the effectiveness of CoFi-Codec in learning multi-scale discrete speech representations while keeping high-quality speech reconstruction. The coarse-to-fine multi-scale generation, especially for the stack-of-scale approach, is also validated as a crucial approach in pursuing a high-quality neural codec language model for TTS.",
      "authors": [
        "Haohan Guo",
        "Fenglong Xie",
        "Dongchao Yang",
        "Xixin Wu",
        "Helen Meng"
      ],
      "published": "2024-09-18T01:31:19Z",
      "updated": "2024-09-18T01:31:19Z",
      "categories": [
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2409.11630v1",
      "landing_url": "https://arxiv.org/abs/2409.11630v1",
      "doi": "https://doi.org/10.48550/arXiv.2409.11630"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The paper explicitly studies discrete speech tokenization by encoding speech into multi-scale discrete representations and building token-based generation models for TTS, meeting inclusion criteria and not falling under exclusions.",
      "evaluation": 5
    },
    "round-A_JuniorNano_reasoning": "The paper explicitly studies discrete speech tokenization by encoding speech into multi-scale discrete representations and building token-based generation models for TTS, meeting inclusion criteria and not falling under exclusions.",
    "round-A_JuniorNano_evaluation": 5,
    "round-A_JuniorMini_output": {
      "reasoning": "The study explicitly proposes and evaluates a multi-scale neural codec language model that encodes speech into discrete token sequences for speech reconstruction and generation tasks, addressing issues in text-to-speech synthesis. It focuses on discrete speech tokenization using a neural codec, includes methodological contributions with experimental validation, and is clearly in English. Therefore, it meets all inclusion criteria and none of the exclusion criteria.",
      "evaluation": 5
    },
    "round-A_JuniorMini_reasoning": "The study explicitly proposes and evaluates a multi-scale neural codec language model that encodes speech into discrete token sequences for speech reconstruction and generation tasks, addressing issues in text-to-speech synthesis. It focuses on discrete speech tokenization using a neural codec, includes methodological contributions with experimental validation, and is clearly in English. Therefore, it meets all inclusion criteria and none of the exclusion criteria.",
    "round-A_JuniorMini_evaluation": 5,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "include (junior:5)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "NDVQ: Robust Neural Audio Codec with Normal Distribution-Based Vector Quantization",
    "abstract": "Built upon vector quantization (VQ), discrete audio codec models have achieved great success in audio compression and auto-regressive audio generation. However, existing models face substantial challenges in perceptual quality and signal distortion, especially when operating in extremely low bandwidth, rooted in the sensitivity of the VQ codebook to noise. This degradation poses significant challenges for several downstream tasks, such as codec-based speech synthesis. To address this issue, we propose a novel VQ method, Normal Distribution-based Vector Quantization (NDVQ), by introducing an explicit margin between the VQ codes via learning a variance. Specifically, our approach involves mapping the waveform to a latent space and quantizing it by selecting the most likely normal distribution, with each codebook entry representing a unique normal distribution defined by its mean and variance. Using these distribution-based VQ codec codes, a decoder reconstructs the input waveform. NDVQ is trained with additional distribution-related losses, alongside reconstruction and discrimination losses. Experiments demonstrate that NDVQ outperforms existing audio compression baselines, such as EnCodec, in terms of audio quality and zero-shot TTS, particularly in very low bandwidth scenarios.",
    "metadata": {
      "arxiv_id": "2409.12717",
      "title": "NDVQ: Robust Neural Audio Codec with Normal Distribution-Based Vector Quantization",
      "summary": "Built upon vector quantization (VQ), discrete audio codec models have achieved great success in audio compression and auto-regressive audio generation. However, existing models face substantial challenges in perceptual quality and signal distortion, especially when operating in extremely low bandwidth, rooted in the sensitivity of the VQ codebook to noise. This degradation poses significant challenges for several downstream tasks, such as codec-based speech synthesis. To address this issue, we propose a novel VQ method, Normal Distribution-based Vector Quantization (NDVQ), by introducing an explicit margin between the VQ codes via learning a variance. Specifically, our approach involves mapping the waveform to a latent space and quantizing it by selecting the most likely normal distribution, with each codebook entry representing a unique normal distribution defined by its mean and variance. Using these distribution-based VQ codec codes, a decoder reconstructs the input waveform. NDVQ is trained with additional distribution-related losses, alongside reconstruction and discrimination losses. Experiments demonstrate that NDVQ outperforms existing audio compression baselines, such as EnCodec, in terms of audio quality and zero-shot TTS, particularly in very low bandwidth scenarios.",
      "authors": [
        "Zhikang Niu",
        "Sanyuan Chen",
        "Long Zhou",
        "Ziyang Ma",
        "Xie Chen",
        "Shujie Liu"
      ],
      "published": "2024-09-19T12:41:30Z",
      "updated": "2024-09-19T12:41:30Z",
      "categories": [
        "eess.AS",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2409.12717v1",
      "landing_url": "https://arxiv.org/abs/2409.12717v1",
      "doi": "https://doi.org/10.48550/arXiv.2409.12717"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The abstract presents a neural audio codec that uses discrete vector quantization, with codebook entries representing distributions (mean and variance) to quantize waveforms into discrete tokens for reconstruction and generation, directly addressing discrete audio tokenization and not aligning with any exclusion criteria.",
      "evaluation": 5
    },
    "round-A_JuniorNano_reasoning": "The abstract presents a neural audio codec that uses discrete vector quantization, with codebook entries representing distributions (mean and variance) to quantize waveforms into discrete tokens for reconstruction and generation, directly addressing discrete audio tokenization and not aligning with any exclusion criteria.",
    "round-A_JuniorNano_evaluation": 5,
    "round-A_JuniorMini_output": {
      "reasoning": "The study clearly proposes a novel vector quantization method (NDVQ) for discrete audio codec models, focusing on converting continuous audio signals into discrete codes and evaluating their effectiveness in audio compression and synthesis tasks, meeting both the inclusion criteria and none of the exclusion criteria.",
      "evaluation": 5
    },
    "round-A_JuniorMini_reasoning": "The study clearly proposes a novel vector quantization method (NDVQ) for discrete audio codec models, focusing on converting continuous audio signals into discrete codes and evaluating their effectiveness in audio compression and synthesis tasks, meeting both the inclusion criteria and none of the exclusion criteria.",
    "round-A_JuniorMini_evaluation": 5,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "include (junior:5)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Codec-SUPERB @ SLT 2024: A lightweight benchmark for neural audio codec models",
    "abstract": "Neural audio codec models are becoming increasingly important as they serve as tokenizers for audio, enabling efficient transmission or facilitating speech language modeling. The ideal neural audio codec should maintain content, paralinguistics, speaker characteristics, and audio information even at low bitrates. Recently, numerous advanced neural codec models have been proposed. However, codec models are often tested under varying experimental conditions. As a result, we introduce the Codec-SUPERB challenge at SLT 2024, designed to facilitate fair and lightweight comparisons among existing codec models and inspire advancements in the field. This challenge brings together representative speech applications and objective metrics, and carefully selects license-free datasets, sampling them into small sets to reduce evaluation computation costs. This paper presents the challenge's rules, datasets, five participant systems, results, and findings.",
    "metadata": {
      "arxiv_id": "2409.14085",
      "title": "Codec-SUPERB @ SLT 2024: A lightweight benchmark for neural audio codec models",
      "summary": "Neural audio codec models are becoming increasingly important as they serve as tokenizers for audio, enabling efficient transmission or facilitating speech language modeling. The ideal neural audio codec should maintain content, paralinguistics, speaker characteristics, and audio information even at low bitrates. Recently, numerous advanced neural codec models have been proposed. However, codec models are often tested under varying experimental conditions. As a result, we introduce the Codec-SUPERB challenge at SLT 2024, designed to facilitate fair and lightweight comparisons among existing codec models and inspire advancements in the field. This challenge brings together representative speech applications and objective metrics, and carefully selects license-free datasets, sampling them into small sets to reduce evaluation computation costs. This paper presents the challenge's rules, datasets, five participant systems, results, and findings.",
      "authors": [
        "Haibin Wu",
        "Xuanjun Chen",
        "Yi-Cheng Lin",
        "Kaiwei Chang",
        "Jiawei Du",
        "Ke-Han Lu",
        "Alexander H. Liu",
        "Ho-Lam Chung",
        "Yuan-Kuei Wu",
        "Dongchao Yang",
        "Songxiang Liu",
        "Yi-Chiao Wu",
        "Xu Tan",
        "James Glass",
        "Shinji Watanabe",
        "Hung-yi Lee"
      ],
      "published": "2024-09-21T09:39:36Z",
      "updated": "2024-09-21T09:39:36Z",
      "categories": [
        "eess.AS",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2409.14085v1",
      "landing_url": "https://arxiv.org/abs/2409.14085v1",
      "doi": "https://doi.org/10.48550/arXiv.2409.14085"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The abstract describes a lightweight benchmark for neural audio codecs and discusses tokenizers for audio, but it does not explicitly analyze or propose discrete audio/speech tokenization (quantized discrete tokens), which is required for inclusion.",
      "evaluation": 2
    },
    "round-A_JuniorNano_reasoning": "The abstract describes a lightweight benchmark for neural audio codecs and discusses tokenizers for audio, but it does not explicitly analyze or propose discrete audio/speech tokenization (quantized discrete tokens), which is required for inclusion.",
    "round-A_JuniorNano_evaluation": 2,
    "round-A_JuniorMini_output": {
      "reasoning": "The study introduces the Codec-SUPERB challenge, which focuses on benchmarking neural audio codec models that serve as tokenizers for audio, converting continuous audio into discrete tokens for reconstruction and modeling, aligning directly with the inclusion criteria; it also discusses methodological contributions, evaluation, and results, and does not fall into any exclusion categories such as traditional codecs or engineering-only reports.",
      "evaluation": 5
    },
    "round-A_JuniorMini_reasoning": "The study introduces the Codec-SUPERB challenge, which focuses on benchmarking neural audio codec models that serve as tokenizers for audio, converting continuous audio into discrete tokens for reconstruction and modeling, aligning directly with the inclusion criteria; it also discusses methodological contributions, evaluation, and results, and does not fall into any exclusion categories such as traditional codecs or engineering-only reports.",
    "round-A_JuniorMini_evaluation": 5,
    "round-B_SeniorLead_output": {
      "reasoning": "The paper clearly evaluates neural audio codec models described as “tokenizers” and presents systematic benchmark results, but the abstract does not explicitly state that these codecs produce or are evaluated as discrete, quantized token sequences (finite-vocabulary symbols) rather than continuous representations; junior reviewers are split (one excluded for lack of explicit discrete-token mention, one included noting tokenizer framing). Because the inclusion criteria require an explicit focus on discrete tokenization/quantization, the item's eligibility is ambiguous from the abstract alone.",
      "evaluation": 3
    },
    "round-B_SeniorLead_reasoning": "The paper clearly evaluates neural audio codec models described as “tokenizers” and presents systematic benchmark results, but the abstract does not explicitly state that these codecs produce or are evaluated as discrete, quantized token sequences (finite-vocabulary symbols) rather than continuous representations; junior reviewers are split (one excluded for lack of explicit discrete-token mention, one included noting tokenizer framing). Because the inclusion criteria require an explicit focus on discrete tokenization/quantization, the item's eligibility is ambiguous from the abstract alone.",
    "round-B_SeniorLead_evaluation": 3,
    "final_verdict": "maybe (senior:3)",
    "review_skipped": false,
    "discard_reason": "review_needs_followup"
  },
  {
    "title": "ESPnet-Codec: Comprehensive Training and Evaluation of Neural Codecs for Audio, Music, and Speech",
    "abstract": "Neural codecs have become crucial to recent speech and audio generation research. In addition to signal compression capabilities, discrete codecs have also been found to enhance downstream training efficiency and compatibility with autoregressive language models. However, as extensive downstream applications are investigated, challenges have arisen in ensuring fair comparisons across diverse applications. To address these issues, we present a new open-source platform ESPnet-Codec, which is built on ESPnet and focuses on neural codec training and evaluation. ESPnet-Codec offers various recipes in audio, music, and speech for training and evaluation using several widely adopted codec models. Together with ESPnet-Codec, we present VERSA, a standalone evaluation toolkit, which provides a comprehensive evaluation of codec performance over 20 audio evaluation metrics. Notably, we demonstrate that ESPnet-Codec can be integrated into six ESPnet tasks, supporting diverse applications.",
    "metadata": {
      "arxiv_id": "2409.15897",
      "title": "ESPnet-Codec: Comprehensive Training and Evaluation of Neural Codecs for Audio, Music, and Speech",
      "summary": "Neural codecs have become crucial to recent speech and audio generation research. In addition to signal compression capabilities, discrete codecs have also been found to enhance downstream training efficiency and compatibility with autoregressive language models. However, as extensive downstream applications are investigated, challenges have arisen in ensuring fair comparisons across diverse applications. To address these issues, we present a new open-source platform ESPnet-Codec, which is built on ESPnet and focuses on neural codec training and evaluation. ESPnet-Codec offers various recipes in audio, music, and speech for training and evaluation using several widely adopted codec models. Together with ESPnet-Codec, we present VERSA, a standalone evaluation toolkit, which provides a comprehensive evaluation of codec performance over 20 audio evaluation metrics. Notably, we demonstrate that ESPnet-Codec can be integrated into six ESPnet tasks, supporting diverse applications.",
      "authors": [
        "Jiatong Shi",
        "Jinchuan Tian",
        "Yihan Wu",
        "Jee-weon Jung",
        "Jia Qi Yip",
        "Yoshiki Masuyama",
        "William Chen",
        "Yuning Wu",
        "Yuxun Tang",
        "Massa Baali",
        "Dareen Alharhi",
        "Dong Zhang",
        "Ruifan Deng",
        "Tejes Srivastava",
        "Haibin Wu",
        "Alexander H. Liu",
        "Bhiksha Raj",
        "Qin Jin",
        "Ruihua Song",
        "Shinji Watanabe"
      ],
      "published": "2024-09-24T09:16:11Z",
      "updated": "2025-02-24T18:34:41Z",
      "categories": [
        "eess.AS",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2409.15897v2",
      "landing_url": "https://arxiv.org/abs/2409.15897v2",
      "doi": "https://doi.org/10.48550/arXiv.2409.15897"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "Although the paper discusses neural codecs and discrete codecs, the abstract does not clearly present discrete audio/speech tokenization into a finite vocabulary as the core object of study, so it does not meet the inclusion criteria and should be excluded.",
      "evaluation": 2
    },
    "round-A_JuniorNano_reasoning": "Although the paper discusses neural codecs and discrete codecs, the abstract does not clearly present discrete audio/speech tokenization into a finite vocabulary as the core object of study, so it does not meet the inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 2,
    "round-A_JuniorMini_output": {
      "reasoning": "The study proposes and evaluates neural discrete codecs that convert continuous audio signals into discrete tokens for use in various tasks including audio, music, and speech reconstruction and understanding, while providing systematic evaluation across many metrics and supporting multiple speech-related applications, which aligns well with the inclusion criteria and does not fall under any exclusion category.",
      "evaluation": 5
    },
    "round-A_JuniorMini_reasoning": "The study proposes and evaluates neural discrete codecs that convert continuous audio signals into discrete tokens for use in various tasks including audio, music, and speech reconstruction and understanding, while providing systematic evaluation across many metrics and supporting multiple speech-related applications, which aligns well with the inclusion criteria and does not fall under any exclusion category.",
    "round-A_JuniorMini_evaluation": 5,
    "round-B_SeniorLead_output": {
      "reasoning": "The abstract explicitly focuses on neural codecs including discrete codecs, describes training and systematic evaluation of codec models (VERSA) across audio, music, and speech, and demonstrates integration into multiple speech-related tasks—meeting the inclusion requirement that discrete audio/speech tokenization and its evaluation are a core object of study; JuniorNano erred in claiming no clear discrete-token focus, while JuniorMini correctly recognized the codec/token evaluation. Score justification (1-sentence): The work explicitly evaluates neural discrete codecs (tokenization) for audio/music/speech with comprehensive metrics and task integration, so it should be included.",
      "evaluation": 5
    },
    "round-B_SeniorLead_reasoning": "The abstract explicitly focuses on neural codecs including discrete codecs, describes training and systematic evaluation of codec models (VERSA) across audio, music, and speech, and demonstrates integration into multiple speech-related tasks—meeting the inclusion requirement that discrete audio/speech tokenization and its evaluation are a core object of study; JuniorNano erred in claiming no clear discrete-token focus, while JuniorMini correctly recognized the codec/token evaluation. Score justification (1-sentence): The work explicitly evaluates neural discrete codecs (tokenization) for audio/music/speech with comprehensive metrics and task integration, so it should be included.",
    "round-B_SeniorLead_evaluation": 5,
    "final_verdict": "include (senior:5)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Subjective and Objective Quality-of-Experience Evaluation Study for Live Video Streaming",
    "abstract": "In recent years, live video streaming has gained widespread popularity across various social media platforms. Quality of experience (QoE), which reflects end-users' satisfaction and overall experience, plays a critical role for media service providers to optimize large-scale live compression and transmission strategies to achieve perceptually optimal rate-distortion trade-off. Although many QoE metrics for video-on-demand (VoD) have been proposed, there remain significant challenges in developing QoE metrics for live video streaming. To bridge this gap, we conduct a comprehensive study of subjective and objective QoE evaluations for live video streaming. For the subjective QoE study, we introduce the first live video streaming QoE dataset, TaoLive QoE, which consists of $42$ source videos collected from real live broadcasts and $1,155$ corresponding distorted ones degraded due to a variety of streaming distortions, including conventional streaming distortions such as compression, stalling, as well as live streaming-specific distortions like frame skipping, variable frame rate, etc. Subsequently, a human study was conducted to derive subjective QoE scores of videos in the TaoLive QoE dataset. For the objective QoE study, we benchmark existing QoE models on the TaoLive QoE dataset as well as publicly available QoE datasets for VoD scenarios, highlighting that current models struggle to accurately assess video QoE, particularly for live content. Hence, we propose an end-to-end QoE evaluation model, Tao-QoE, which integrates multi-scale semantic features and optical flow-based motion features to predicting a retrospective QoE score, eliminating reliance on statistical quality of service (QoS) features.",
    "metadata": {
      "arxiv_id": "2409.17596",
      "title": "Subjective and Objective Quality-of-Experience Evaluation Study for Live Video Streaming",
      "summary": "In recent years, live video streaming has gained widespread popularity across various social media platforms. Quality of experience (QoE), which reflects end-users' satisfaction and overall experience, plays a critical role for media service providers to optimize large-scale live compression and transmission strategies to achieve perceptually optimal rate-distortion trade-off. Although many QoE metrics for video-on-demand (VoD) have been proposed, there remain significant challenges in developing QoE metrics for live video streaming. To bridge this gap, we conduct a comprehensive study of subjective and objective QoE evaluations for live video streaming. For the subjective QoE study, we introduce the first live video streaming QoE dataset, TaoLive QoE, which consists of $42$ source videos collected from real live broadcasts and $1,155$ corresponding distorted ones degraded due to a variety of streaming distortions, including conventional streaming distortions such as compression, stalling, as well as live streaming-specific distortions like frame skipping, variable frame rate, etc. Subsequently, a human study was conducted to derive subjective QoE scores of videos in the TaoLive QoE dataset. For the objective QoE study, we benchmark existing QoE models on the TaoLive QoE dataset as well as publicly available QoE datasets for VoD scenarios, highlighting that current models struggle to accurately assess video QoE, particularly for live content. Hence, we propose an end-to-end QoE evaluation model, Tao-QoE, which integrates multi-scale semantic features and optical flow-based motion features to predicting a retrospective QoE score, eliminating reliance on statistical quality of service (QoS) features.",
      "authors": [
        "Zehao Zhu",
        "Wei Sun",
        "Jun Jia",
        "Wei Wu",
        "Sibin Deng",
        "Kai Li",
        "Ying Chen",
        "Xiongkuo Min",
        "Jia Wang",
        "Guangtao Zhai"
      ],
      "published": "2024-09-26T07:22:38Z",
      "updated": "2024-09-26T07:22:38Z",
      "categories": [
        "cs.MM",
        "cs.AI",
        "eess.IV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2409.17596v1",
      "landing_url": "https://arxiv.org/abs/2409.17596v1",
      "doi": "https://doi.org/10.48550/arXiv.2409.17596"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This item focuses on subjective/objective QoE for live video streaming and does not discuss discrete audio/speech tokenization or any tokenization methods; it does not meet the inclusion criteria requiring analysis of discrete audio tokens.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "This item focuses on subjective/objective QoE for live video streaming and does not discuss discrete audio/speech tokenization or any tokenization methods; it does not meet the inclusion criteria requiring analysis of discrete audio tokens.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on subjective and objective evaluations of video streaming Quality-of-Experience (QoE) and does not address discrete audio/speech tokenization or the conversion of continuous audio signals into discrete token sequences, which is the core inclusion criterion. It primarily deals with video QoE metrics and models, and there is no indication of discrete audio token design, analysis, or application relevant to audio reconstruction or understanding tasks mentioned in the abstract.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on subjective and objective evaluations of video streaming Quality-of-Experience (QoE) and does not address discrete audio/speech tokenization or the conversion of continuous audio signals into discrete token sequences, which is the core inclusion criterion. It primarily deals with video QoE metrics and models, and there is no indication of discrete audio token design, analysis, or application relevant to audio reconstruction or understanding tasks mentioned in the abstract.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Analyzing and Mitigating Inconsistency in Discrete Audio Tokens for Neural Codec Language Models",
    "abstract": "Building upon advancements in Large Language Models (LLMs), the field of audio processing has seen increased interest in training audio generation tasks with discrete audio token sequences. However, directly discretizing audio by neural audio codecs often results in sequences that fundamentally differ from text sequences. Unlike text, where text token sequences are deterministic, discrete audio tokens can exhibit significant variability based on contextual factors, while still producing perceptually identical audio segments. We refer to this phenomenon as \\textbf{Discrete Representation Inconsistency (DRI)}. This inconsistency can lead to a single audio segment being represented by multiple divergent sequences, which creates confusion in neural codec language models and results in omissions and repetitions during speech generation. In this paper, we quantitatively analyze the DRI phenomenon within popular audio tokenizers such as EnCodec. Our approach effectively mitigates the DRI phenomenon of the neural audio codec. Furthermore, extensive experiments on the neural codec language model over LibriTTS and large-scale MLS datases (44,000 hours) demonstrate the effectiveness and generality of our method. The demo of audio samples is available online~\\footnote{\\url{https://consistencyinneuralcodec.github.io}}.",
    "metadata": {
      "arxiv_id": "2409.19283",
      "title": "Analyzing and Mitigating Inconsistency in Discrete Audio Tokens for Neural Codec Language Models",
      "summary": "Building upon advancements in Large Language Models (LLMs), the field of audio processing has seen increased interest in training audio generation tasks with discrete audio token sequences. However, directly discretizing audio by neural audio codecs often results in sequences that fundamentally differ from text sequences. Unlike text, where text token sequences are deterministic, discrete audio tokens can exhibit significant variability based on contextual factors, while still producing perceptually identical audio segments. We refer to this phenomenon as \\textbf{Discrete Representation Inconsistency (DRI)}. This inconsistency can lead to a single audio segment being represented by multiple divergent sequences, which creates confusion in neural codec language models and results in omissions and repetitions during speech generation. In this paper, we quantitatively analyze the DRI phenomenon within popular audio tokenizers such as EnCodec. Our approach effectively mitigates the DRI phenomenon of the neural audio codec. Furthermore, extensive experiments on the neural codec language model over LibriTTS and large-scale MLS datases (44,000 hours) demonstrate the effectiveness and generality of our method. The demo of audio samples is available online~\\footnote{\\url{https://consistencyinneuralcodec.github.io}}.",
      "authors": [
        "Wenrui Liu",
        "Zhifang Guo",
        "Jin Xu",
        "Yuanjun Lv",
        "Yunfei Chu",
        "Zhou Zhao",
        "Junyang Lin"
      ],
      "published": "2024-09-28T08:36:44Z",
      "updated": "2024-10-04T22:34:38Z",
      "categories": [
        "eess.AS",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2409.19283v2",
      "landing_url": "https://arxiv.org/abs/2409.19283v2",
      "doi": "https://doi.org/10.48550/arXiv.2409.19283"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item directly studies discrete audio tokenization and inconsistencies, analyzes a neural codec tokenization (EnCodec), and provides experimental evaluation on speech data (LibriTTS), aligning with inclusion criteria and not falling under exclusions.",
      "evaluation": 5
    },
    "round-A_JuniorNano_reasoning": "The item directly studies discrete audio tokenization and inconsistencies, analyzes a neural codec tokenization (EnCodec), and provides experimental evaluation on speech data (LibriTTS), aligning with inclusion criteria and not falling under exclusions.",
    "round-A_JuniorNano_evaluation": 5,
    "round-A_JuniorMini_output": {
      "reasoning": "The study explicitly proposes, analyzes, and mitigates discrete audio token representation inconsistency in neural audio codec language models, focusing on discrete audio tokenization from continuous audio signals for speech generation tasks, and includes quantitative analysis and experiments over large-scale speech datasets, thus meeting all inclusion criteria and none of the exclusion criteria.",
      "evaluation": 5
    },
    "round-A_JuniorMini_reasoning": "The study explicitly proposes, analyzes, and mitigates discrete audio token representation inconsistency in neural audio codec language models, focusing on discrete audio tokenization from continuous audio signals for speech generation tasks, and includes quantitative analysis and experiments over large-scale speech datasets, thus meeting all inclusion criteria and none of the exclusion criteria.",
    "round-A_JuniorMini_evaluation": 5,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "include (junior:5)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Moshi: a speech-text foundation model for real-time dialogue",
    "abstract": "We introduce Moshi, a speech-text foundation model and full-duplex spoken dialogue framework. Current systems for spoken dialogue rely on pipelines of independent components, namely voice activity detection, speech recognition, textual dialogue and text-to-speech. Such frameworks cannot emulate the experience of real conversations. First, their complexity induces a latency of several seconds between interactions. Second, text being the intermediate modality for dialogue, non-linguistic information that modifies meaning -- such as emotion or non-speech sounds -- is lost in the interaction. Finally, they rely on a segmentation into speaker turns, which does not take into account overlapping speech, interruptions and interjections. Moshi solves these independent issues altogether by casting spoken dialogue as speech-to-speech generation. Starting from a text language model backbone, Moshi generates speech as tokens from the residual quantizer of a neural audio codec, while modeling separately its own speech and that of the user into parallel streams. This allows for the removal of explicit speaker turns, and the modeling of arbitrary conversational dynamics. We moreover extend the hierarchical semantic-to-acoustic token generation of previous work to first predict time-aligned text tokens as a prefix to audio tokens. Not only this \"Inner Monologue\" method significantly improves the linguistic quality of generated speech, but we also illustrate how it can provide streaming speech recognition and text-to-speech. Our resulting model is the first real-time full-duplex spoken large language model, with a theoretical latency of 160ms, 200ms in practice, and is available at https://github.com/kyutai-labs/moshi.",
    "metadata": {
      "arxiv_id": "2410.00037",
      "title": "Moshi: a speech-text foundation model for real-time dialogue",
      "summary": "We introduce Moshi, a speech-text foundation model and full-duplex spoken dialogue framework. Current systems for spoken dialogue rely on pipelines of independent components, namely voice activity detection, speech recognition, textual dialogue and text-to-speech. Such frameworks cannot emulate the experience of real conversations. First, their complexity induces a latency of several seconds between interactions. Second, text being the intermediate modality for dialogue, non-linguistic information that modifies meaning -- such as emotion or non-speech sounds -- is lost in the interaction. Finally, they rely on a segmentation into speaker turns, which does not take into account overlapping speech, interruptions and interjections. Moshi solves these independent issues altogether by casting spoken dialogue as speech-to-speech generation. Starting from a text language model backbone, Moshi generates speech as tokens from the residual quantizer of a neural audio codec, while modeling separately its own speech and that of the user into parallel streams. This allows for the removal of explicit speaker turns, and the modeling of arbitrary conversational dynamics. We moreover extend the hierarchical semantic-to-acoustic token generation of previous work to first predict time-aligned text tokens as a prefix to audio tokens. Not only this \"Inner Monologue\" method significantly improves the linguistic quality of generated speech, but we also illustrate how it can provide streaming speech recognition and text-to-speech. Our resulting model is the first real-time full-duplex spoken large language model, with a theoretical latency of 160ms, 200ms in practice, and is available at https://github.com/kyutai-labs/moshi.",
      "authors": [
        "Alexandre Défossez",
        "Laurent Mazaré",
        "Manu Orsini",
        "Amélie Royer",
        "Patrick Pérez",
        "Hervé Jégou",
        "Edouard Grave",
        "Neil Zeghidour"
      ],
      "published": "2024-09-17T17:55:39Z",
      "updated": "2024-10-02T09:11:45Z",
      "categories": [
        "eess.AS",
        "cs.AI",
        "cs.CL",
        "cs.LG",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2410.00037v2",
      "landing_url": "https://arxiv.org/abs/2410.00037v2",
      "doi": "https://doi.org/10.48550/arXiv.2410.00037"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The work explicitly treats speech as discrete tokens (audio tokens from a neural codec residual quantizer) and introduces time-aligned text tokens as a prefix to audio tokens, addressing generation and recognition within a discrete-token framework.",
      "evaluation": 5
    },
    "round-A_JuniorNano_reasoning": "The work explicitly treats speech as discrete tokens (audio tokens from a neural codec residual quantizer) and introduces time-aligned text tokens as a prefix to audio tokens, addressing generation and recognition within a discrete-token framework.",
    "round-A_JuniorNano_evaluation": 5,
    "round-A_JuniorMini_output": {
      "reasoning": "The study introduces Moshi, which centers on generating speech as tokens from the residual quantizer of a neural audio codec, indicating a discrete audio tokenization approach central to its methodology. It explicitly addresses discrete token generation for speech reconstruction and generation tasks, involves neural encoding, and includes methodological contributions beyond just engineering implementation. Therefore, it satisfies the inclusion criteria and avoids all exclusion criteria.",
      "evaluation": 5
    },
    "round-A_JuniorMini_reasoning": "The study introduces Moshi, which centers on generating speech as tokens from the residual quantizer of a neural audio codec, indicating a discrete audio tokenization approach central to its methodology. It explicitly addresses discrete token generation for speech reconstruction and generation tasks, involves neural encoding, and includes methodological contributions beyond just engineering implementation. Therefore, it satisfies the inclusion criteria and avoids all exclusion criteria.",
    "round-A_JuniorMini_evaluation": 5,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "include (junior:5)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Evaluating Deduplication Techniques for Economic Research Paper Titles with a Focus on Semantic Similarity using NLP and LLMs",
    "abstract": "This study investigates efficient deduplication techniques for a large NLP dataset of economic research paper titles. We explore various pairing methods alongside established distance measures (Levenshtein distance, cosine similarity) and a sBERT model for semantic evaluation. Our findings suggest a potentially low prevalence of duplicates based on the observed semantic similarity across different methods. Further exploration with a human-annotated ground truth set is completed for a more conclusive assessment. The result supports findings from the NLP, LLM based distance metrics.",
    "metadata": {
      "arxiv_id": "2410.01141",
      "title": "Evaluating Deduplication Techniques for Economic Research Paper Titles with a Focus on Semantic Similarity using NLP and LLMs",
      "summary": "This study investigates efficient deduplication techniques for a large NLP dataset of economic research paper titles. We explore various pairing methods alongside established distance measures (Levenshtein distance, cosine similarity) and a sBERT model for semantic evaluation. Our findings suggest a potentially low prevalence of duplicates based on the observed semantic similarity across different methods. Further exploration with a human-annotated ground truth set is completed for a more conclusive assessment. The result supports findings from the NLP, LLM based distance metrics.",
      "authors": [
        "Doohee You",
        "S Fraiberger"
      ],
      "published": "2024-10-02T00:43:10Z",
      "updated": "2025-06-30T18:26:08Z",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2410.01141v3",
      "landing_url": "https://arxiv.org/abs/2410.01141v3",
      "doi": "https://doi.org/10.48550/arXiv.2410.01141"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item focuses on text-based deduplication using NLP/LLMs for economic paper titles and does not involve discrete audio/speech tokenization or analysis of discrete tokens; thus it does not meet inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item focuses on text-based deduplication using NLP/LLMs for economic paper titles and does not involve discrete audio/speech tokenization or analysis of discrete tokens; thus it does not meet inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on deduplication techniques for economic research paper titles using NLP and semantic similarity methods; it does not involve discrete audio or speech tokenization nor discuss audio signal processing or discrete token design related to audio or speech tasks, thus it fails to meet any of the inclusion criteria and is irrelevant to the specified topic.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on deduplication techniques for economic research paper titles using NLP and semantic similarity methods; it does not involve discrete audio or speech tokenization nor discuss audio signal processing or discrete token design related to audio or speech tasks, thus it fails to meet any of the inclusion criteria and is irrelevant to the specified topic.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Textless Streaming Speech-to-Speech Translation using Semantic Speech Tokens",
    "abstract": "Cascaded speech-to-speech translation systems often suffer from the error accumulation problem and high latency, which is a result of cascaded modules whose inference delays accumulate. In this paper, we propose a transducer-based speech translation model that outputs discrete speech tokens in a low-latency streaming fashion. This approach eliminates the need for generating text output first, followed by machine translation (MT) and text-to-speech (TTS) systems. The produced speech tokens can be directly used to generate a speech signal with low latency by utilizing an acoustic language model (LM) to obtain acoustic tokens and an audio codec model to retrieve the waveform. Experimental results show that the proposed method outperforms other existing approaches and achieves state-of-the-art results for streaming translation in terms of BLEU, average latency, and BLASER 2.0 scores for multiple language pairs using the CVSS-C dataset as a benchmark.",
    "metadata": {
      "arxiv_id": "2410.03298",
      "title": "Textless Streaming Speech-to-Speech Translation using Semantic Speech Tokens",
      "summary": "Cascaded speech-to-speech translation systems often suffer from the error accumulation problem and high latency, which is a result of cascaded modules whose inference delays accumulate. In this paper, we propose a transducer-based speech translation model that outputs discrete speech tokens in a low-latency streaming fashion. This approach eliminates the need for generating text output first, followed by machine translation (MT) and text-to-speech (TTS) systems. The produced speech tokens can be directly used to generate a speech signal with low latency by utilizing an acoustic language model (LM) to obtain acoustic tokens and an audio codec model to retrieve the waveform. Experimental results show that the proposed method outperforms other existing approaches and achieves state-of-the-art results for streaming translation in terms of BLEU, average latency, and BLASER 2.0 scores for multiple language pairs using the CVSS-C dataset as a benchmark.",
      "authors": [
        "Jinzheng Zhao",
        "Niko Moritz",
        "Egor Lakomkin",
        "Ruiming Xie",
        "Zhiping Xiu",
        "Katerina Zmolikova",
        "Zeeshan Ahmed",
        "Yashesh Gaur",
        "Duc Le",
        "Christian Fuegen"
      ],
      "published": "2024-10-04T10:21:15Z",
      "updated": "2024-10-04T10:21:15Z",
      "categories": [
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2410.03298v1",
      "landing_url": "https://arxiv.org/abs/2410.03298v1",
      "doi": "https://doi.org/10.48550/arXiv.2410.03298"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The paper explicitly studies discrete speech tokens as the core object (textless streaming speech-to-speech translation using discrete tokens for generation), converting continuous audio into discrete tokens for speech reconstruction, which satisfies the inclusion criteria and does not fall under any exclusion criterion.",
      "evaluation": 5
    },
    "round-A_JuniorNano_reasoning": "The paper explicitly studies discrete speech tokens as the core object (textless streaming speech-to-speech translation using discrete tokens for generation), converting continuous audio into discrete tokens for speech reconstruction, which satisfies the inclusion criteria and does not fall under any exclusion criterion.",
    "round-A_JuniorNano_evaluation": 5,
    "round-A_JuniorMini_output": {
      "reasoning": "The study directly proposes and experimentally validates a transducer-based speech translation model that outputs discrete speech tokens, explicitly addressing the conversion of continuous audio into discrete symbols for speech generation tasks, which aligns with the inclusion criteria; it does not fall under any exclusion criteria as it features neural discrete token representations rather than traditional codecs and includes systematic experimental validation.",
      "evaluation": 5
    },
    "round-A_JuniorMini_reasoning": "The study directly proposes and experimentally validates a transducer-based speech translation model that outputs discrete speech tokens, explicitly addressing the conversion of continuous audio into discrete symbols for speech generation tasks, which aligns with the inclusion criteria; it does not fall under any exclusion criteria as it features neural discrete token representations rather than traditional codecs and includes systematic experimental validation.",
    "round-A_JuniorMini_evaluation": 5,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "include (junior:5)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "HALL-E: Hierarchical Neural Codec Language Model for Minute-Long Zero-Shot Text-to-Speech Synthesis",
    "abstract": "Recently, Text-to-speech (TTS) models based on large language models (LLMs) that translate natural language text into sequences of discrete audio tokens have gained great research attention, with advances in neural audio codec (NAC) models using residual vector quantization (RVQ). However, long-form speech synthesis remains a significant challenge due to the high frame rate, which increases the length of audio tokens and makes it difficult for autoregressive language models to generate audio tokens for even a minute of speech. To address this challenge, this paper introduces two novel post-training approaches: 1) Multi-Resolution Requantization (MReQ) and 2) HALL-E. MReQ is a framework to reduce the frame rate of pre-trained NAC models. Specifically, it incorporates multi-resolution residual vector quantization (MRVQ) module that hierarchically reorganizes discrete audio tokens through teacher-student distillation. HALL-E is an LLM-based TTS model designed to predict hierarchical tokens of MReQ. Specifically, it incorporates the technique of using MRVQ sub-modules and continues training from a pre-trained LLM-based TTS model. Furthermore, to promote TTS research, we create MinutesSpeech, a new benchmark dataset consisting of 40k hours of filtered speech data for training and evaluating speech synthesis ranging from 3s up to 180s. In experiments, we demonstrated the effectiveness of our approaches by applying our post-training framework to VALL-E. We achieved the frame rate down to as low as 8 Hz, enabling the stable minitue-long speech synthesis in a single inference step. Audio samples, dataset, codes and pre-trained models are available at https://yutonishimura-v2.github.io/HALL-E_DEMO/.",
    "metadata": {
      "arxiv_id": "2410.04380",
      "title": "HALL-E: Hierarchical Neural Codec Language Model for Minute-Long Zero-Shot Text-to-Speech Synthesis",
      "summary": "Recently, Text-to-speech (TTS) models based on large language models (LLMs) that translate natural language text into sequences of discrete audio tokens have gained great research attention, with advances in neural audio codec (NAC) models using residual vector quantization (RVQ). However, long-form speech synthesis remains a significant challenge due to the high frame rate, which increases the length of audio tokens and makes it difficult for autoregressive language models to generate audio tokens for even a minute of speech. To address this challenge, this paper introduces two novel post-training approaches: 1) Multi-Resolution Requantization (MReQ) and 2) HALL-E. MReQ is a framework to reduce the frame rate of pre-trained NAC models. Specifically, it incorporates multi-resolution residual vector quantization (MRVQ) module that hierarchically reorganizes discrete audio tokens through teacher-student distillation. HALL-E is an LLM-based TTS model designed to predict hierarchical tokens of MReQ. Specifically, it incorporates the technique of using MRVQ sub-modules and continues training from a pre-trained LLM-based TTS model. Furthermore, to promote TTS research, we create MinutesSpeech, a new benchmark dataset consisting of 40k hours of filtered speech data for training and evaluating speech synthesis ranging from 3s up to 180s. In experiments, we demonstrated the effectiveness of our approaches by applying our post-training framework to VALL-E. We achieved the frame rate down to as low as 8 Hz, enabling the stable minitue-long speech synthesis in a single inference step. Audio samples, dataset, codes and pre-trained models are available at https://yutonishimura-v2.github.io/HALL-E_DEMO/.",
      "authors": [
        "Yuto Nishimura",
        "Takumi Hirose",
        "Masanari Ohi",
        "Hideki Nakayama",
        "Nakamasa Inoue"
      ],
      "published": "2024-10-06T07:20:58Z",
      "updated": "2024-10-06T07:20:58Z",
      "categories": [
        "eess.AS",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2410.04380v1",
      "landing_url": "https://arxiv.org/abs/2410.04380v1",
      "doi": "https://doi.org/10.48550/arXiv.2410.04380"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The abstract explicitly centers on discrete audio tokenization via neural codecs (MRVQ/HALL-E) for TTS, converting audio into discrete tokens for generation, meeting the inclusion criteria.",
      "evaluation": 5
    },
    "round-A_JuniorNano_reasoning": "The abstract explicitly centers on discrete audio tokenization via neural codecs (MRVQ/HALL-E) for TTS, converting audio into discrete tokens for generation, meeting the inclusion criteria.",
    "round-A_JuniorNano_evaluation": 5,
    "round-A_JuniorMini_output": {
      "reasoning": "The study explicitly focuses on discrete audio tokenization by proposing a novel hierarchical neural codec model that reorganizes discrete audio tokens through residual vector quantization and applies this method to zero-shot text-to-speech synthesis. The approach is rooted in neural quantization-based discrete token representations instead of traditional continuous audio features or signal-processing codecs. Additionally, the paper includes methodological contributions and experimental validation relevant to discrete audio tokenization, meeting the inclusion criteria without falling into any exclusion categories.",
      "evaluation": 5
    },
    "round-A_JuniorMini_reasoning": "The study explicitly focuses on discrete audio tokenization by proposing a novel hierarchical neural codec model that reorganizes discrete audio tokens through residual vector quantization and applies this method to zero-shot text-to-speech synthesis. The approach is rooted in neural quantization-based discrete token representations instead of traditional continuous audio features or signal-processing codecs. Additionally, the paper includes methodological contributions and experimental validation relevant to discrete audio tokenization, meeting the inclusion criteria without falling into any exclusion categories.",
    "round-A_JuniorMini_evaluation": 5,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "include (junior:5)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Collapse-Proof Non-Contrastive Self-Supervised Learning",
    "abstract": "We present a principled and simplified design of the projector and loss function for non-contrastive self-supervised learning based on hyperdimensional computing. We theoretically demonstrate that this design introduces an inductive bias that encourages representations to be simultaneously decorrelated and clustered, without explicitly enforcing these properties. This bias provably enhances generalization and suffices to avoid known training failure modes, such as representation, dimensional, cluster, and intracluster collapses. We validate our theoretical findings on image datasets, including SVHN, CIFAR-10, CIFAR-100, and ImageNet-100. Our approach effectively combines the strengths of feature decorrelation and cluster-based self-supervised learning methods, overcoming training failure modes while achieving strong generalization in clustering and linear classification tasks.",
    "metadata": {
      "arxiv_id": "2410.04959",
      "title": "Collapse-Proof Non-Contrastive Self-Supervised Learning",
      "summary": "We present a principled and simplified design of the projector and loss function for non-contrastive self-supervised learning based on hyperdimensional computing. We theoretically demonstrate that this design introduces an inductive bias that encourages representations to be simultaneously decorrelated and clustered, without explicitly enforcing these properties. This bias provably enhances generalization and suffices to avoid known training failure modes, such as representation, dimensional, cluster, and intracluster collapses. We validate our theoretical findings on image datasets, including SVHN, CIFAR-10, CIFAR-100, and ImageNet-100. Our approach effectively combines the strengths of feature decorrelation and cluster-based self-supervised learning methods, overcoming training failure modes while achieving strong generalization in clustering and linear classification tasks.",
      "authors": [
        "Emanuele Sansone",
        "Tim Lebailly",
        "Tinne Tuytelaars"
      ],
      "published": "2024-10-07T11:58:56Z",
      "updated": "2025-07-06T19:11:03Z",
      "categories": [
        "cs.LG",
        "stat.ML"
      ],
      "pdf_url": "https://arxiv.org/pdf/2410.04959v4",
      "landing_url": "https://arxiv.org/abs/2410.04959v4",
      "doi": "https://doi.org/10.48550/arXiv.2410.04959"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This item does not involve discrete audio tokenization; it's an image-focused self-supervised learning paper, so it does not meet the audio/token criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "This item does not involve discrete audio tokenization; it's an image-focused self-supervised learning paper, so it does not meet the audio/token criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on non-contrastive self-supervised learning with applications to image datasets and theoretical contributions related to representation learning, but it does not address or analyze discrete audio/speech tokenization or relevant audio tasks as required by the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on non-contrastive self-supervised learning with applications to image datasets and theoretical contributions related to representation learning, but it does not address or analyze discrete audio/speech tokenization or relevant audio tasks as required by the inclusion criteria.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "SeeClear: Semantic Distillation Enhances Pixel Condensation for Video Super-Resolution",
    "abstract": "Diffusion-based Video Super-Resolution (VSR) is renowned for generating perceptually realistic videos, yet it grapples with maintaining detail consistency across frames due to stochastic fluctuations. The traditional approach of pixel-level alignment is ineffective for diffusion-processed frames because of iterative disruptions. To overcome this, we introduce SeeClear--a novel VSR framework leveraging conditional video generation, orchestrated by instance-centric and channel-wise semantic controls. This framework integrates a Semantic Distiller and a Pixel Condenser, which synergize to extract and upscale semantic details from low-resolution frames. The Instance-Centric Alignment Module (InCAM) utilizes video-clip-wise tokens to dynamically relate pixels within and across frames, enhancing coherency. Additionally, the Channel-wise Texture Aggregation Memory (CaTeGory) infuses extrinsic knowledge, capitalizing on long-standing semantic textures. Our method also innovates the blurring diffusion process with the ResShift mechanism, finely balancing between sharpness and diffusion effects. Comprehensive experiments confirm our framework's advantage over state-of-the-art diffusion-based VSR techniques. The code is available: https://github.com/Tang1705/SeeClear-NeurIPS24.",
    "metadata": {
      "arxiv_id": "2410.05799",
      "title": "SeeClear: Semantic Distillation Enhances Pixel Condensation for Video Super-Resolution",
      "summary": "Diffusion-based Video Super-Resolution (VSR) is renowned for generating perceptually realistic videos, yet it grapples with maintaining detail consistency across frames due to stochastic fluctuations. The traditional approach of pixel-level alignment is ineffective for diffusion-processed frames because of iterative disruptions. To overcome this, we introduce SeeClear--a novel VSR framework leveraging conditional video generation, orchestrated by instance-centric and channel-wise semantic controls. This framework integrates a Semantic Distiller and a Pixel Condenser, which synergize to extract and upscale semantic details from low-resolution frames. The Instance-Centric Alignment Module (InCAM) utilizes video-clip-wise tokens to dynamically relate pixels within and across frames, enhancing coherency. Additionally, the Channel-wise Texture Aggregation Memory (CaTeGory) infuses extrinsic knowledge, capitalizing on long-standing semantic textures. Our method also innovates the blurring diffusion process with the ResShift mechanism, finely balancing between sharpness and diffusion effects. Comprehensive experiments confirm our framework's advantage over state-of-the-art diffusion-based VSR techniques. The code is available: https://github.com/Tang1705/SeeClear-NeurIPS24.",
      "authors": [
        "Qi Tang",
        "Yao Zhao",
        "Meiqin Liu",
        "Chao Yao"
      ],
      "published": "2024-10-08T08:33:47Z",
      "updated": "2024-10-26T06:11:30Z",
      "categories": [
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2410.05799v4",
      "landing_url": "https://arxiv.org/abs/2410.05799v4",
      "doi": "https://doi.org/10.48550/arXiv.2410.05799"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item addresses diffusion-based video super-resolution and semantic modules; there is no discrete audio/speech tokenization or analysis of discrete audio token representations, so it does not meet the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item addresses diffusion-based video super-resolution and semantic modules; there is no discrete audio/speech tokenization or analysis of discrete audio token representations, so it does not meet the inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The title and abstract describe a novel framework for video super-resolution using semantic distillation and pixel condensation techniques, focusing on image/video processing; there is no mention or indication of discrete audio/speech tokenization or converting continuous audio signals into discrete tokens, nor any involvement of audio or speech-related tasks as per the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The title and abstract describe a novel framework for video super-resolution using semantic distillation and pixel condensation techniques, focusing on image/video processing; there is no mention or indication of discrete audio/speech tokenization or converting continuous audio signals into discrete tokens, nor any involvement of audio or speech-related tasks as per the inclusion criteria.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Variable Bitrate Residual Vector Quantization for Audio Coding",
    "abstract": "Recent state-of-the-art neural audio compression models have progressively adopted residual vector quantization (RVQ). Despite this success, these models employ a fixed number of codebooks per frame, which can be suboptimal in terms of rate-distortion tradeoff, particularly in scenarios with simple input audio, such as silence. To address this limitation, we propose variable bitrate RVQ (VRVQ) for audio codecs, which allows for more efficient coding by adapting the number of codebooks used per frame. Furthermore, we propose a gradient estimation method for the non-differentiable masking operation that transforms from the importance map to the binary importance mask, improving model training via a straight-through estimator. We demonstrate that the proposed training framework achieves superior results compared to the baseline method and shows further improvement when applied to the current state-of-the-art codec.",
    "metadata": {
      "arxiv_id": "2410.06016",
      "title": "Variable Bitrate Residual Vector Quantization for Audio Coding",
      "summary": "Recent state-of-the-art neural audio compression models have progressively adopted residual vector quantization (RVQ). Despite this success, these models employ a fixed number of codebooks per frame, which can be suboptimal in terms of rate-distortion tradeoff, particularly in scenarios with simple input audio, such as silence. To address this limitation, we propose variable bitrate RVQ (VRVQ) for audio codecs, which allows for more efficient coding by adapting the number of codebooks used per frame. Furthermore, we propose a gradient estimation method for the non-differentiable masking operation that transforms from the importance map to the binary importance mask, improving model training via a straight-through estimator. We demonstrate that the proposed training framework achieves superior results compared to the baseline method and shows further improvement when applied to the current state-of-the-art codec.",
      "authors": [
        "Yunkee Chae",
        "Woosung Choi",
        "Yuhta Takida",
        "Junghyun Koo",
        "Yukara Ikemiya",
        "Zhi Zhong",
        "Kin Wai Cheuk",
        "Marco A. Martínez-Ramírez",
        "Kyogu Lee",
        "Wei-Hsiang Liao",
        "Yuki Mitsufuji"
      ],
      "published": "2024-10-08T13:18:24Z",
      "updated": "2025-04-27T15:10:16Z",
      "categories": [
        "cs.SD",
        "cs.LG",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2410.06016v3",
      "landing_url": "https://arxiv.org/abs/2410.06016v3",
      "doi": "https://doi.org/10.48550/arXiv.2410.06016"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The article uses residual vector quantization with discrete codebooks and a variable-number per frame, constituting a discrete tokenization approach for neural audio coding, and discusses a gradient method for non-differentiable masking tied to token selection; this aligns with the inclusion criteria and does not fit the exclusion criteria.",
      "evaluation": 5
    },
    "round-A_JuniorNano_reasoning": "The article uses residual vector quantization with discrete codebooks and a variable-number per frame, constituting a discrete tokenization approach for neural audio coding, and discusses a gradient method for non-differentiable masking tied to token selection; this aligns with the inclusion criteria and does not fit the exclusion criteria.",
    "round-A_JuniorNano_evaluation": 5,
    "round-A_JuniorMini_output": {
      "reasoning": "The study proposes variable bitrate residual vector quantization (VRVQ) for neural audio codecs, which inherently involves discrete quantization of audio signals, a core component of discrete audio tokenization for compression and reconstruction tasks. It also includes methodological contributions and experimental validation. Thus, it meets the inclusion criteria and none of the exclusion criteria.",
      "evaluation": 5
    },
    "round-A_JuniorMini_reasoning": "The study proposes variable bitrate residual vector quantization (VRVQ) for neural audio codecs, which inherently involves discrete quantization of audio signals, a core component of discrete audio tokenization for compression and reconstruction tasks. It also includes methodological contributions and experimental validation. Thus, it meets the inclusion criteria and none of the exclusion criteria.",
    "round-A_JuniorMini_evaluation": 5,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "include (junior:5)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Restructuring Vector Quantization with the Rotation Trick",
    "abstract": "Vector Quantized Variational AutoEncoders (VQ-VAEs) are designed to compress a continuous input to a discrete latent space and reconstruct it with minimal distortion. They operate by maintaining a set of vectors -- often referred to as the codebook -- and quantizing each encoder output to the nearest vector in the codebook. However, as vector quantization is non-differentiable, the gradient to the encoder flows around the vector quantization layer rather than through it in a straight-through approximation. This approximation may be undesirable as all information from the vector quantization operation is lost. In this work, we propose a way to propagate gradients through the vector quantization layer of VQ-VAEs. We smoothly transform each encoder output into its corresponding codebook vector via a rotation and rescaling linear transformation that is treated as a constant during backpropagation. As a result, the relative magnitude and angle between encoder output and codebook vector becomes encoded into the gradient as it propagates through the vector quantization layer and back to the encoder. Across 11 different VQ-VAE training paradigms, we find this restructuring improves reconstruction metrics, codebook utilization, and quantization error. Our code is available at https://github.com/cfifty/rotation_trick.",
    "metadata": {
      "arxiv_id": "2410.06424",
      "title": "Restructuring Vector Quantization with the Rotation Trick",
      "summary": "Vector Quantized Variational AutoEncoders (VQ-VAEs) are designed to compress a continuous input to a discrete latent space and reconstruct it with minimal distortion. They operate by maintaining a set of vectors -- often referred to as the codebook -- and quantizing each encoder output to the nearest vector in the codebook. However, as vector quantization is non-differentiable, the gradient to the encoder flows around the vector quantization layer rather than through it in a straight-through approximation. This approximation may be undesirable as all information from the vector quantization operation is lost. In this work, we propose a way to propagate gradients through the vector quantization layer of VQ-VAEs. We smoothly transform each encoder output into its corresponding codebook vector via a rotation and rescaling linear transformation that is treated as a constant during backpropagation. As a result, the relative magnitude and angle between encoder output and codebook vector becomes encoded into the gradient as it propagates through the vector quantization layer and back to the encoder. Across 11 different VQ-VAE training paradigms, we find this restructuring improves reconstruction metrics, codebook utilization, and quantization error. Our code is available at https://github.com/cfifty/rotation_trick.",
      "authors": [
        "Christopher Fifty",
        "Ronald G. Junkins",
        "Dennis Duan",
        "Aniketh Iyengar",
        "Jerry W. Liu",
        "Ehsan Amid",
        "Sebastian Thrun",
        "Christopher Ré"
      ],
      "published": "2024-10-08T23:39:34Z",
      "updated": "2025-03-16T03:30:10Z",
      "categories": [
        "cs.LG",
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2410.06424v2",
      "landing_url": "https://arxiv.org/abs/2410.06424v2",
      "doi": "https://doi.org/10.48550/arXiv.2410.06424"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "Although the work uses discrete codebooks in a VQ-VAE framework, it does not explicitly address discrete audio/speech tokenization as its core object, nor evaluate speech-related token representations, so it should be excluded.",
      "evaluation": 2
    },
    "round-A_JuniorNano_reasoning": "Although the work uses discrete codebooks in a VQ-VAE framework, it does not explicitly address discrete audio/speech tokenization as its core object, nor evaluate speech-related token representations, so it should be excluded.",
    "round-A_JuniorNano_evaluation": 2,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on Vector Quantized Variational AutoEncoders (VQ-VAEs) and proposes an improvement in the vector quantization process. However, the abstract does not specify that the tokenization is applied to audio or speech signals, nor does it explicitly state that discrete audio/speech tokenization is a core object of study. Therefore, it is unclear if the proposed method is relevant to discrete audio/speech tokenization as required by the inclusion criteria, making it difficult to confirm its relevance for inclusion.",
      "evaluation": 3
    },
    "round-A_JuniorMini_reasoning": "The study focuses on Vector Quantized Variational AutoEncoders (VQ-VAEs) and proposes an improvement in the vector quantization process. However, the abstract does not specify that the tokenization is applied to audio or speech signals, nor does it explicitly state that discrete audio/speech tokenization is a core object of study. Therefore, it is unclear if the proposed method is relevant to discrete audio/speech tokenization as required by the inclusion criteria, making it difficult to confirm its relevance for inclusion.",
    "round-A_JuniorMini_evaluation": 3,
    "round-B_SeniorLead_output": {
      "reasoning": "This paper proposes a gradient-propagation method for VQ-VAEs via a rotation/rescaling trick, but the abstract contains no mention of audio or speech data, discrete audio/speech tokenization, or evaluation on speech reconstruction/generation/understanding tasks; both junior reviews noted the same lack of speech-specific tokenization (one recommended exclusion, the other was unsure). Therefore it does not meet the inclusion criterion requiring explicit focus on discrete audio/speech tokenization and should be excluded. (Score rationale: the study is ML-methods focused and not about discrete audio/speech tokens.)",
      "evaluation": 1
    },
    "round-B_SeniorLead_reasoning": "This paper proposes a gradient-propagation method for VQ-VAEs via a rotation/rescaling trick, but the abstract contains no mention of audio or speech data, discrete audio/speech tokenization, or evaluation on speech reconstruction/generation/understanding tasks; both junior reviews noted the same lack of speech-specific tokenization (one recommended exclusion, the other was unsure). Therefore it does not meet the inclusion criterion requiring explicit focus on discrete audio/speech tokenization and should be excluded. (Score rationale: the study is ML-methods focused and not about discrete audio/speech tokens.)",
    "round-B_SeniorLead_evaluation": 1,
    "final_verdict": "exclude (senior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (senior:1)"
  },
  {
    "title": "A short proof of diffusivity for the directed polymers in the weak disorder phase",
    "abstract": "We provide a new short and elementary proof of diffusivity for directed polymer in a random environment in the weak disorder phase.",
    "metadata": {
      "arxiv_id": "2410.07068",
      "title": "A short proof of diffusivity for the directed polymers in the weak disorder phase",
      "summary": "We provide a new short and elementary proof of diffusivity for directed polymer in a random environment in the weak disorder phase.",
      "authors": [
        "Hubert Lacoin"
      ],
      "published": "2024-10-09T17:14:13Z",
      "updated": "2025-05-06T14:27:28Z",
      "categories": [
        "math.PR"
      ],
      "pdf_url": "https://arxiv.org/pdf/2410.07068v2",
      "landing_url": "https://arxiv.org/abs/2410.07068v2",
      "doi": "https://doi.org/10.48550/arXiv.2410.07068"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item is a mathematical paper on diffusivity of directed polymers in random environments, with no discrete audio/speech tokenization or token-level analysis, so it fails the inclusion criteria and is not salvaged by any exclusion.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item is a mathematical paper on diffusivity of directed polymers in random environments, with no discrete audio/speech tokenization or token-level analysis, so it fails the inclusion criteria and is not salvaged by any exclusion.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The title and abstract focus on a mathematical proof related to diffusivity in directed polymers in a random environment, which is unrelated to discrete audio/speech tokenization or any audio representation tasks specified in the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The title and abstract focus on a mathematical proof related to diffusivity in directed polymers in a random environment, which is unrelated to discrete audio/speech tokenization or any audio representation tasks specified in the inclusion criteria.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Sylber: Syllabic Embedding Representation of Speech from Raw Audio",
    "abstract": "Syllables are compositional units of spoken language that efficiently structure human speech perception and production. However, current neural speech representations lack such structure, resulting in dense token sequences that are costly to process. To bridge this gap, we propose a new model, Sylber, that produces speech representations with clean and robust syllabic structure. Specifically, we propose a self-supervised learning (SSL) framework that bootstraps syllabic embeddings by distilling from its own initial unsupervised syllabic segmentation. This results in a highly structured representation of speech features, offering three key benefits: 1) a fast, linear-time syllable segmentation algorithm, 2) efficient syllabic tokenization with an average of 4.27 tokens per second, and 3) novel phonological units suited for efficient spoken language modeling. Our proposed segmentation method is highly robust and generalizes to out-of-domain data and unseen languages without any tuning. By training token-to-speech generative models, fully intelligible speech can be reconstructed from Sylber tokens with a significantly lower bitrate than baseline SSL tokens. This suggests that our model effectively compresses speech into a compact sequence of tokens with minimal information loss. Lastly, we demonstrate that categorical perception-a linguistic phenomenon in speech perception-emerges naturally in Sylber, making the embedding space more categorical and sparse than previous speech features and thus supporting the high efficiency of our tokenization. Together, we present a novel SSL approach for representing speech as syllables, with significant potential for efficient speech tokenization and spoken language modeling.",
    "metadata": {
      "arxiv_id": "2410.07168",
      "title": "Sylber: Syllabic Embedding Representation of Speech from Raw Audio",
      "summary": "Syllables are compositional units of spoken language that efficiently structure human speech perception and production. However, current neural speech representations lack such structure, resulting in dense token sequences that are costly to process. To bridge this gap, we propose a new model, Sylber, that produces speech representations with clean and robust syllabic structure. Specifically, we propose a self-supervised learning (SSL) framework that bootstraps syllabic embeddings by distilling from its own initial unsupervised syllabic segmentation. This results in a highly structured representation of speech features, offering three key benefits: 1) a fast, linear-time syllable segmentation algorithm, 2) efficient syllabic tokenization with an average of 4.27 tokens per second, and 3) novel phonological units suited for efficient spoken language modeling. Our proposed segmentation method is highly robust and generalizes to out-of-domain data and unseen languages without any tuning. By training token-to-speech generative models, fully intelligible speech can be reconstructed from Sylber tokens with a significantly lower bitrate than baseline SSL tokens. This suggests that our model effectively compresses speech into a compact sequence of tokens with minimal information loss. Lastly, we demonstrate that categorical perception-a linguistic phenomenon in speech perception-emerges naturally in Sylber, making the embedding space more categorical and sparse than previous speech features and thus supporting the high efficiency of our tokenization. Together, we present a novel SSL approach for representing speech as syllables, with significant potential for efficient speech tokenization and spoken language modeling.",
      "authors": [
        "Cheol Jun Cho",
        "Nicholas Lee",
        "Akshat Gupta",
        "Dhruv Agarwal",
        "Ethan Chen",
        "Alan W Black",
        "Gopala K. Anumanchipalli"
      ],
      "published": "2024-10-09T17:59:04Z",
      "updated": "2025-03-02T09:16:05Z",
      "categories": [
        "cs.CL",
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2410.07168v2",
      "landing_url": "https://arxiv.org/abs/2410.07168v2",
      "doi": "https://doi.org/10.48550/arXiv.2410.07168"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The paper explicitly constructs and evaluates a discrete syllabic tokenization of speech from raw audio, demonstrating reconstruction from token sequences and benefits in bitrate and modeling, which satisfies the inclusion criteria and does not fit the exclusion categories.",
      "evaluation": 5
    },
    "round-A_JuniorNano_reasoning": "The paper explicitly constructs and evaluates a discrete syllabic tokenization of speech from raw audio, demonstrating reconstruction from token sequences and benefits in bitrate and modeling, which satisfies the inclusion criteria and does not fit the exclusion categories.",
    "round-A_JuniorNano_evaluation": 5,
    "round-A_JuniorMini_output": {
      "reasoning": "The study proposes Sylber, a self-supervised learning model to produce discrete syllabic embeddings from raw audio, effectively converting continuous speech signals into discrete tokens with robust syllabic structure. It explicitly analyzes and evaluates discrete audio tokenization aimed at speech reconstruction and modeling, aligns well with inclusion criteria, and does not fall under any exclusion criteria such as continuous feature-only representation, traditional codecs, or engineering-only reports. Therefore, it meets all the inclusion criteria and none of the exclusion criteria.",
      "evaluation": 5
    },
    "round-A_JuniorMini_reasoning": "The study proposes Sylber, a self-supervised learning model to produce discrete syllabic embeddings from raw audio, effectively converting continuous speech signals into discrete tokens with robust syllabic structure. It explicitly analyzes and evaluates discrete audio tokenization aimed at speech reconstruction and modeling, aligns well with inclusion criteria, and does not fall under any exclusion criteria such as continuous feature-only representation, traditional codecs, or engineering-only reports. Therefore, it meets all the inclusion criteria and none of the exclusion criteria.",
    "round-A_JuniorMini_evaluation": 5,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "include (junior:5)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Low Bitrate High-Quality RVQGAN-based Discrete Speech Tokenizer",
    "abstract": "Discrete Audio codecs (or audio tokenizers) have recently regained interest due to the ability of Large Language Models (LLMs) to learn their compressed acoustic representations. Various publicly available trainable discrete tokenizers recently demonstrated impressive results for audio tokenization, yet they mostly require high token rates to gain high-quality reconstruction. In this study, we fine-tuned an open-source general audio RVQGAN model using diverse open-source speech data, considering various recording conditions and quality levels. The resulting wideband (24kHz) speech-only model achieves speech reconstruction, which is nearly indistinguishable from PCM (pulse-code modulation) with a rate of 150-300 tokens per second (1500-3000 bps). The evaluation used comprehensive English speech data encompassing different recording conditions, including studio settings. Speech samples are made publicly available in http://ibm.biz/IS24SpeechRVQ . The model is officially released in https://huggingface.co/ibm/DAC.speech.v1.0",
    "metadata": {
      "arxiv_id": "2410.08325",
      "title": "Low Bitrate High-Quality RVQGAN-based Discrete Speech Tokenizer",
      "summary": "Discrete Audio codecs (or audio tokenizers) have recently regained interest due to the ability of Large Language Models (LLMs) to learn their compressed acoustic representations. Various publicly available trainable discrete tokenizers recently demonstrated impressive results for audio tokenization, yet they mostly require high token rates to gain high-quality reconstruction. In this study, we fine-tuned an open-source general audio RVQGAN model using diverse open-source speech data, considering various recording conditions and quality levels. The resulting wideband (24kHz) speech-only model achieves speech reconstruction, which is nearly indistinguishable from PCM (pulse-code modulation) with a rate of 150-300 tokens per second (1500-3000 bps). The evaluation used comprehensive English speech data encompassing different recording conditions, including studio settings. Speech samples are made publicly available in http://ibm.biz/IS24SpeechRVQ . The model is officially released in https://huggingface.co/ibm/DAC.speech.v1.0",
      "authors": [
        "Slava Shechtman",
        "Avihu Dekel"
      ],
      "published": "2024-10-10T19:29:05Z",
      "updated": "2024-10-10T19:29:05Z",
      "categories": [
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2410.08325v1",
      "landing_url": "https://arxiv.org/abs/2410.08325v1",
      "doi": "https://doi.org/10.21437/Interspeech.2024-2366"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The work explicitly develops and evaluates a neural discrete speech tokenizer (RVQGAN-based) with quantized speech tokens for reconstruction at low bitrate, aligning with the discrete-tokenization inclusion criteria and not falling under exclusion categories.",
      "evaluation": 5
    },
    "round-A_JuniorNano_reasoning": "The work explicitly develops and evaluates a neural discrete speech tokenizer (RVQGAN-based) with quantized speech tokens for reconstruction at low bitrate, aligning with the discrete-tokenization inclusion criteria and not falling under exclusion categories.",
    "round-A_JuniorNano_evaluation": 5,
    "round-A_JuniorMini_output": {
      "reasoning": "The study explicitly proposes and evaluates a discrete speech tokenizer based on RVQGAN, converting continuous speech audio into discrete tokens and demonstrating high-quality speech reconstruction at low token rates; it also provides methodological contributions and experimental validation relevant to discrete audio tokenization for speech, thereby meeting all inclusion criteria and none of the exclusion criteria.",
      "evaluation": 5
    },
    "round-A_JuniorMini_reasoning": "The study explicitly proposes and evaluates a discrete speech tokenizer based on RVQGAN, converting continuous speech audio into discrete tokens and demonstrating high-quality speech reconstruction at low token rates; it also provides methodological contributions and experimental validation relevant to discrete audio tokenization for speech, thereby meeting all inclusion criteria and none of the exclusion criteria.",
    "round-A_JuniorMini_evaluation": 5,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "include (junior:5)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Semantic Token Reweighting for Interpretable and Controllable Text Embeddings in CLIP",
    "abstract": "A text encoder within Vision-Language Models (VLMs) like CLIP plays a crucial role in translating textual input into an embedding space shared with images, thereby facilitating the interpretative analysis of vision tasks through natural language. Despite the varying significance of different textual elements within a sentence depending on the context, efforts to account for variation of importance in constructing text embeddings have been lacking. We propose a framework of Semantic Token Reweighting to build Interpretable text embeddings (SToRI), which incorporates controllability as well. SToRI refines the text encoding process in CLIP by differentially weighting semantic elements based on contextual importance, enabling finer control over emphasis responsive to data-driven insights and user preferences. The efficacy of SToRI is demonstrated through comprehensive experiments on few-shot image classification and image retrieval tailored to user preferences.",
    "metadata": {
      "arxiv_id": "2410.08469",
      "title": "Semantic Token Reweighting for Interpretable and Controllable Text Embeddings in CLIP",
      "summary": "A text encoder within Vision-Language Models (VLMs) like CLIP plays a crucial role in translating textual input into an embedding space shared with images, thereby facilitating the interpretative analysis of vision tasks through natural language. Despite the varying significance of different textual elements within a sentence depending on the context, efforts to account for variation of importance in constructing text embeddings have been lacking. We propose a framework of Semantic Token Reweighting to build Interpretable text embeddings (SToRI), which incorporates controllability as well. SToRI refines the text encoding process in CLIP by differentially weighting semantic elements based on contextual importance, enabling finer control over emphasis responsive to data-driven insights and user preferences. The efficacy of SToRI is demonstrated through comprehensive experiments on few-shot image classification and image retrieval tailored to user preferences.",
      "authors": [
        "Eunji Kim",
        "Kyuhong Shim",
        "Simyung Chang",
        "Sungroh Yoon"
      ],
      "published": "2024-10-11T02:42:13Z",
      "updated": "2024-10-16T14:09:14Z",
      "categories": [
        "cs.LG",
        "cs.CL",
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2410.08469v2",
      "landing_url": "https://arxiv.org/abs/2410.08469v2",
      "doi": "https://doi.org/10.48550/arXiv.2410.08469"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "Does not address discrete audio/speech tokenization; it's about reweighting text embeddings in a vision-language model, so it fails the audio-tokenization inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "Does not address discrete audio/speech tokenization; it's about reweighting text embeddings in a vision-language model, so it fails the audio-tokenization inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on semantic token reweighting for text embeddings in CLIP, a vision-language model, without addressing discrete audio/speech tokenization or discrete audio representations; thus, it does not meet the inclusion criteria related to audio tokenization nor does it violate exclusion criteria, but it is irrelevant for inclusion based on the given criteria.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on semantic token reweighting for text embeddings in CLIP, a vision-language model, without addressing discrete audio/speech tokenization or discrete audio representations; thus, it does not meet the inclusion criteria related to audio tokenization nor does it violate exclusion criteria, but it is irrelevant for inclusion based on the given criteria.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Gaussian Mixture Vector Quantization with Aggregated Categorical Posterior",
    "abstract": "The vector quantization is a widely used method to map continuous representation to discrete space and has important application in tokenization for generative mode, bottlenecking information and many other tasks in machine learning. Vector Quantized Variational Autoencoder (VQ-VAE) is a type of variational autoencoder using discrete embedding as latent. We generalize the technique further, enriching the probabilistic framework with a Gaussian mixture as the underlying generative model. This framework leverages a codebook of latent means and adaptive variances to capture complex data distributions. This principled framework avoids various heuristics and strong assumptions that are needed with the VQ-VAE to address training instability and to improve codebook utilization. This approach integrates the benefits of both discrete and continuous representations within a variational Bayesian framework. Furthermore, by introducing the \\textit{Aggregated Categorical Posterior Evidence Lower Bound} (ALBO), we offer a principled alternative optimization objective that aligns variational distributions with the generative model. Our experiments demonstrate that GM-VQ improves codebook utilization and reduces information loss without relying on handcrafted heuristics.",
    "metadata": {
      "arxiv_id": "2410.10180",
      "title": "Gaussian Mixture Vector Quantization with Aggregated Categorical Posterior",
      "summary": "The vector quantization is a widely used method to map continuous representation to discrete space and has important application in tokenization for generative mode, bottlenecking information and many other tasks in machine learning. Vector Quantized Variational Autoencoder (VQ-VAE) is a type of variational autoencoder using discrete embedding as latent. We generalize the technique further, enriching the probabilistic framework with a Gaussian mixture as the underlying generative model. This framework leverages a codebook of latent means and adaptive variances to capture complex data distributions. This principled framework avoids various heuristics and strong assumptions that are needed with the VQ-VAE to address training instability and to improve codebook utilization. This approach integrates the benefits of both discrete and continuous representations within a variational Bayesian framework. Furthermore, by introducing the \\textit{Aggregated Categorical Posterior Evidence Lower Bound} (ALBO), we offer a principled alternative optimization objective that aligns variational distributions with the generative model. Our experiments demonstrate that GM-VQ improves codebook utilization and reduces information loss without relying on handcrafted heuristics.",
      "authors": [
        "Mingyuan Yan",
        "Jiawei Wu",
        "Rushi Shah",
        "Dianbo Liu"
      ],
      "published": "2024-10-14T05:58:11Z",
      "updated": "2024-10-14T05:58:11Z",
      "categories": [
        "cs.LG",
        "stat.ML"
      ],
      "pdf_url": "https://arxiv.org/pdf/2410.10180v1",
      "landing_url": "https://arxiv.org/abs/2410.10180v1",
      "doi": "https://doi.org/10.48550/arXiv.2410.10180"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The abstract discusses discrete latent quantization and VQ-VAE but does not explicitly target audio/speech tokenization as the core object; thus it does not clearly meet the explicit inclusion criterion for discrete audio/speech tokenization.",
      "evaluation": 2
    },
    "round-A_JuniorNano_reasoning": "The abstract discusses discrete latent quantization and VQ-VAE but does not explicitly target audio/speech tokenization as the core object; thus it does not clearly meet the explicit inclusion criterion for discrete audio/speech tokenization.",
    "round-A_JuniorNano_evaluation": 2,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper proposes a generalized vector quantization method using Gaussian mixture models for discrete latent representations, which could be applicable to tokenization; however, the abstract lacks explicit mention of application to discrete audio or speech tokenization, or evaluation on audio/speech tasks, making it unclear if it meets the inclusion criteria related to discrete audio tokenization.",
      "evaluation": 3
    },
    "round-A_JuniorMini_reasoning": "The paper proposes a generalized vector quantization method using Gaussian mixture models for discrete latent representations, which could be applicable to tokenization; however, the abstract lacks explicit mention of application to discrete audio or speech tokenization, or evaluation on audio/speech tasks, making it unclear if it meets the inclusion criteria related to discrete audio tokenization.",
    "round-A_JuniorMini_evaluation": 3,
    "round-B_SeniorLead_output": {
      "reasoning": "Junior reviewers: round-A_JuniorNano recommended exclusion (2) noting no explicit audio/speech focus; round-A_JuniorMini was uncertain (3) noting possible applicability to tokenization but lacking explicit audio/speech evaluation. My assessment: the abstract describes a general Gaussian-mixture-based vector quantization method for discrete latent representations (VQ-VAE generalization) but does not explicitly propose, analyze, or evaluate discrete audio/speech tokenization nor report audio/speech experiments; it therefore fails the inclusion criterion that the study must explicitly target discrete audio/speech tokenization as a core object. Score rationale (one sentence): The paper is methodological and not explicitly about discrete audio/speech tokenization or evaluated on speech/audio tasks, so it should be excluded.",
      "evaluation": 2
    },
    "round-B_SeniorLead_reasoning": "Junior reviewers: round-A_JuniorNano recommended exclusion (2) noting no explicit audio/speech focus; round-A_JuniorMini was uncertain (3) noting possible applicability to tokenization but lacking explicit audio/speech evaluation. My assessment: the abstract describes a general Gaussian-mixture-based vector quantization method for discrete latent representations (VQ-VAE generalization) but does not explicitly propose, analyze, or evaluate discrete audio/speech tokenization nor report audio/speech experiments; it therefore fails the inclusion criterion that the study must explicitly target discrete audio/speech tokenization as a core object. Score rationale (one sentence): The paper is methodological and not explicitly about discrete audio/speech tokenization or evaluated on speech/audio tasks, so it should be excluded.",
    "round-B_SeniorLead_evaluation": 2,
    "final_verdict": "exclude (senior:2)",
    "review_skipped": false,
    "discard_reason": "exclude (senior:2)"
  },
  {
    "title": "Code Drift: Towards Idempotent Neural Audio Codecs",
    "abstract": "Neural codecs have demonstrated strong performance in high-fidelity compression of audio signals at low bitrates. The token-based representations produced by these codecs have proven particularly useful for generative modeling. While much research has focused on improvements in compression ratio and perceptual transparency, recent works have largely overlooked another desirable codec property -- idempotence, the stability of compressed outputs under multiple rounds of encoding. We find that state-of-the-art neural codecs exhibit varied degrees of idempotence, with some degrading audio outputs significantly after as few as three encodings. We investigate possible causes of low idempotence and devise a method for improving idempotence through fine-tuning a codec model. We then examine the effect of idempotence on a simple conditional generative modeling task, and find that increased idempotence can be achieved without negatively impacting downstream modeling performance -- potentially extending the usefulness of neural codecs for practical file compression and iterative generative modeling workflows.",
    "metadata": {
      "arxiv_id": "2410.11025",
      "title": "Code Drift: Towards Idempotent Neural Audio Codecs",
      "summary": "Neural codecs have demonstrated strong performance in high-fidelity compression of audio signals at low bitrates. The token-based representations produced by these codecs have proven particularly useful for generative modeling. While much research has focused on improvements in compression ratio and perceptual transparency, recent works have largely overlooked another desirable codec property -- idempotence, the stability of compressed outputs under multiple rounds of encoding. We find that state-of-the-art neural codecs exhibit varied degrees of idempotence, with some degrading audio outputs significantly after as few as three encodings. We investigate possible causes of low idempotence and devise a method for improving idempotence through fine-tuning a codec model. We then examine the effect of idempotence on a simple conditional generative modeling task, and find that increased idempotence can be achieved without negatively impacting downstream modeling performance -- potentially extending the usefulness of neural codecs for practical file compression and iterative generative modeling workflows.",
      "authors": [
        "Patrick O'Reilly",
        "Prem Seetharaman",
        "Jiaqi Su",
        "Zeyu Jin",
        "Bryan Pardo"
      ],
      "published": "2024-10-14T19:21:28Z",
      "updated": "2025-04-14T23:07:19Z",
      "categories": [
        "eess.AS",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2410.11025v2",
      "landing_url": "https://arxiv.org/abs/2410.11025v2",
      "doi": "https://doi.org/10.1109/ICASSP49660.2025.10890096"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The title/abstract focus on discrete token-based neural audio codecs and their idempotence, i.e., analysis and optimization of discrete audio token representations used for reconstruction and generative tasks, which aligns with the inclusion criteria and does not appear to fall under any exclusion.",
      "evaluation": 5
    },
    "round-A_JuniorNano_reasoning": "The title/abstract focus on discrete token-based neural audio codecs and their idempotence, i.e., analysis and optimization of discrete audio token representations used for reconstruction and generative tasks, which aligns with the inclusion criteria and does not appear to fall under any exclusion.",
    "round-A_JuniorNano_evaluation": 5,
    "round-A_JuniorMini_output": {
      "reasoning": "The study explicitly analyzes neural audio codecs that produce token-based representations, which corresponds to discrete audio tokenization for compression and generative modeling; it also focuses on methodological contributions related to idempotence improving codec stability, meeting the inclusion criteria and none of the exclusion criteria.",
      "evaluation": 5
    },
    "round-A_JuniorMini_reasoning": "The study explicitly analyzes neural audio codecs that produce token-based representations, which corresponds to discrete audio tokenization for compression and generative modeling; it also focuses on methodological contributions related to idempotence improving codec stability, meeting the inclusion criteria and none of the exclusion criteria.",
    "round-A_JuniorMini_evaluation": 5,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "include (junior:5)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "CleanUMamba: A Compact Mamba Network for Speech Denoising using Channel Pruning",
    "abstract": "This paper presents CleanUMamba, a time-domain neural network architecture designed for real-time causal audio denoising directly applied to raw waveforms. CleanUMamba leverages a U-Net encoder-decoder structure, incorporating the Mamba state-space model in the bottleneck layer. By replacing conventional self-attention and LSTM mechanisms with Mamba, our architecture offers superior denoising performance while maintaining a constant memory footprint, enabling streaming operation. To enhance efficiency, we applied structured channel pruning, achieving an 8X reduction in model size without compromising audio quality. Our model demonstrates strong results in the Interspeech 2020 Deep Noise Suppression challenge. Specifically, CleanUMamba achieves a PESQ score of 2.42 and STOI of 95.1% with only 442K parameters and 468M MACs, matching or outperforming larger models in real-time performance. Code will be available at: https://github.com/lab-emi/CleanUMamba",
    "metadata": {
      "arxiv_id": "2410.11062",
      "title": "CleanUMamba: A Compact Mamba Network for Speech Denoising using Channel Pruning",
      "summary": "This paper presents CleanUMamba, a time-domain neural network architecture designed for real-time causal audio denoising directly applied to raw waveforms. CleanUMamba leverages a U-Net encoder-decoder structure, incorporating the Mamba state-space model in the bottleneck layer. By replacing conventional self-attention and LSTM mechanisms with Mamba, our architecture offers superior denoising performance while maintaining a constant memory footprint, enabling streaming operation. To enhance efficiency, we applied structured channel pruning, achieving an 8X reduction in model size without compromising audio quality. Our model demonstrates strong results in the Interspeech 2020 Deep Noise Suppression challenge. Specifically, CleanUMamba achieves a PESQ score of 2.42 and STOI of 95.1% with only 442K parameters and 468M MACs, matching or outperforming larger models in real-time performance. Code will be available at: https://github.com/lab-emi/CleanUMamba",
      "authors": [
        "Sjoerd Groot",
        "Qinyu Chen",
        "Jan C. van Gemert",
        "Chang Gao"
      ],
      "published": "2024-10-14T20:18:03Z",
      "updated": "2025-02-10T18:07:54Z",
      "categories": [
        "cs.SD",
        "cs.AI",
        "cs.CV",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2410.11062v2",
      "landing_url": "https://arxiv.org/abs/2410.11062v2",
      "doi": "https://doi.org/10.1109/ISCAS56072.2025.11043389"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The work focuses on continuous-time audio denoising of raw waveforms and neural architectural improvements; it does not propose or analyze discrete audio tokenization or token-based representations, so it fails the inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The work focuses on continuous-time audio denoising of raw waveforms and neural architectural improvements; it does not propose or analyze discrete audio tokenization or token-based representations, so it fails the inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on a neural network architecture for real-time speech denoising using raw waveforms and channel pruning, but it does not mention any discrete audio or speech tokenization, quantization, or token design, which are required by the inclusion criteria. Therefore, it does not meet the core inclusion condition of discrete tokenization analysis or proposal.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on a neural network architecture for real-time speech denoising using raw waveforms and channel pruning, but it does not mention any discrete audio or speech tokenization, quantization, or token design, which are required by the inclusion criteria. Therefore, it does not meet the core inclusion condition of discrete tokenization analysis or proposal.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "ERVQ: Enhanced Residual Vector Quantization with Intra-and-Inter-Codebook Optimization for Neural Audio Codecs",
    "abstract": "Current neural audio codecs typically use residual vector quantization (RVQ) to discretize speech signals. However, they often experience codebook collapse, which reduces the effective codebook size and leads to suboptimal performance. To address this problem, we introduce ERVQ, Enhanced Residual Vector Quantization, a novel enhancement strategy for the RVQ framework in neural audio codecs. ERVQ mitigates codebook collapse and boosts codec performance through both intra- and inter-codebook optimization. Intra-codebook optimization incorporates an online clustering strategy and a code balancing loss to ensure balanced and efficient codebook utilization. Inter-codebook optimization improves the diversity of quantized features by minimizing the similarity between successive quantizations. Our experiments show that ERVQ significantly enhances audio codec performance across different models, sampling rates, and bitrates, achieving superior quality and generalization capabilities. It also achieves 100% codebook utilization on one of the most advanced neural audio codecs. Further experiments indicate that audio codecs improved by the ERVQ strategy can improve unified speech-and-text large language models (LLMs). Specifically, there is a notable improvement in the naturalness of generated speech in downstream zero-shot text-to-speech tasks. Audio samples are available here.",
    "metadata": {
      "arxiv_id": "2410.12359",
      "title": "ERVQ: Enhanced Residual Vector Quantization with Intra-and-Inter-Codebook Optimization for Neural Audio Codecs",
      "summary": "Current neural audio codecs typically use residual vector quantization (RVQ) to discretize speech signals. However, they often experience codebook collapse, which reduces the effective codebook size and leads to suboptimal performance. To address this problem, we introduce ERVQ, Enhanced Residual Vector Quantization, a novel enhancement strategy for the RVQ framework in neural audio codecs. ERVQ mitigates codebook collapse and boosts codec performance through both intra- and inter-codebook optimization. Intra-codebook optimization incorporates an online clustering strategy and a code balancing loss to ensure balanced and efficient codebook utilization. Inter-codebook optimization improves the diversity of quantized features by minimizing the similarity between successive quantizations. Our experiments show that ERVQ significantly enhances audio codec performance across different models, sampling rates, and bitrates, achieving superior quality and generalization capabilities. It also achieves 100% codebook utilization on one of the most advanced neural audio codecs. Further experiments indicate that audio codecs improved by the ERVQ strategy can improve unified speech-and-text large language models (LLMs). Specifically, there is a notable improvement in the naturalness of generated speech in downstream zero-shot text-to-speech tasks. Audio samples are available here.",
      "authors": [
        "Rui-Chen Zheng",
        "Hui-Peng Du",
        "Xiao-Hang Jiang",
        "Yang Ai",
        "Zhen-Hua Ling"
      ],
      "published": "2024-10-16T08:21:37Z",
      "updated": "2025-06-11T08:43:47Z",
      "categories": [
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2410.12359v2",
      "landing_url": "https://arxiv.org/abs/2410.12359v2",
      "doi": "https://doi.org/10.48550/arXiv.2410.12359"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The work explicitly focuses on discrete audio tokenization via residual vector quantization with intra- and inter-codebook optimization, addressing codebook utilization and quantized-token diversity, which aligns with the inclusion criteria.",
      "evaluation": 5
    },
    "round-A_JuniorNano_reasoning": "The work explicitly focuses on discrete audio tokenization via residual vector quantization with intra- and inter-codebook optimization, addressing codebook utilization and quantized-token diversity, which aligns with the inclusion criteria.",
    "round-A_JuniorNano_evaluation": 5,
    "round-A_JuniorMini_output": {
      "reasoning": "The study explicitly proposes and analyzes a novel discrete tokenization method (ERVQ) for neural audio codecs, focusing on residual vector quantization to convert continuous speech signals into discrete codes, which aligns directly with the inclusion criteria. It thoroughly evaluates the discrete tokenization method's impact on codec performance and downstream speech tasks, includes experimental validation, and is presented in English. It is not related to traditional codecs, does not solely use continuous features, and contributes methodologically beyond engineering reports; hence, it meets all inclusion criteria and none of the exclusion criteria.",
      "evaluation": 5
    },
    "round-A_JuniorMini_reasoning": "The study explicitly proposes and analyzes a novel discrete tokenization method (ERVQ) for neural audio codecs, focusing on residual vector quantization to convert continuous speech signals into discrete codes, which aligns directly with the inclusion criteria. It thoroughly evaluates the discrete tokenization method's impact on codec performance and downstream speech tasks, includes experimental validation, and is presented in English. It is not related to traditional codecs, does not solely use continuous features, and contributes methodologically beyond engineering reports; hence, it meets all inclusion criteria and none of the exclusion criteria.",
    "round-A_JuniorMini_evaluation": 5,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "include (junior:5)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "On uniqueness of the set of k-means",
    "abstract": "We provide necessary and sufficient conditions for the uniqueness of the k-means set of a probability distribution. This uniqueness problem is related to the choice of k: depending on the underlying distribution, some values of this parameter could lead to multiple sets of k-means, which hampers the interpretation of the results and/or the stability of the algorithms. We give a general assessment on consistency of the empirical k-means adapted to the setting of non-uniqueness and determine the asymptotic distribution of the within cluster sum of squares (WCSS). We also provide statistical characterizations of k-means uniqueness in terms of the asymptotic behavior of the empirical WCSS. As a consequence, we derive a bootstrap test for uniqueness of the set of k-means. The results are illustrated with examples of different types of non-uniqueness and we check by simulations the performance of the proposed methodology.",
    "metadata": {
      "arxiv_id": "2410.13495",
      "title": "On uniqueness of the set of k-means",
      "summary": "We provide necessary and sufficient conditions for the uniqueness of the k-means set of a probability distribution. This uniqueness problem is related to the choice of k: depending on the underlying distribution, some values of this parameter could lead to multiple sets of k-means, which hampers the interpretation of the results and/or the stability of the algorithms. We give a general assessment on consistency of the empirical k-means adapted to the setting of non-uniqueness and determine the asymptotic distribution of the within cluster sum of squares (WCSS). We also provide statistical characterizations of k-means uniqueness in terms of the asymptotic behavior of the empirical WCSS. As a consequence, we derive a bootstrap test for uniqueness of the set of k-means. The results are illustrated with examples of different types of non-uniqueness and we check by simulations the performance of the proposed methodology.",
      "authors": [
        "Javier Cárcamo",
        "Antonio Cuevas",
        "Luis A. Rodríguez"
      ],
      "published": "2024-10-17T12:40:56Z",
      "updated": "2024-10-17T12:40:56Z",
      "categories": [
        "math.ST",
        "stat.ME",
        "stat.ML"
      ],
      "pdf_url": "https://arxiv.org/pdf/2410.13495v1",
      "landing_url": "https://arxiv.org/abs/2410.13495v1",
      "doi": "https://doi.org/10.48550/arXiv.2410.13495"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item addresses theoretical properties of k-means clustering for distributions and bootstrap testing, with no explicit discrete speech/audio tokenization or evaluation of discrete token representations, thus not meeting the inclusion criteria for discrete audio tokenization.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item addresses theoretical properties of k-means clustering for distributions and bootstrap testing, with no explicit discrete speech/audio tokenization or evaluation of discrete token representations, thus not meeting the inclusion criteria for discrete audio tokenization.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on the mathematical and statistical properties of the uniqueness of k-means clustering sets in probability distributions, without any mention or relevance to discrete audio/speech tokenization, nor to converting continuous audio signals into discrete tokens for reconstruction or understanding tasks. It does not involve audio or speech tokenization but instead concerns clustering theory and methodology more generally.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on the mathematical and statistical properties of the uniqueness of k-means clustering sets in probability distributions, without any mention or relevance to discrete audio/speech tokenization, nor to converting continuous audio signals into discrete tokens for reconstruction or understanding tasks. It does not involve audio or speech tokenization but instead concerns clustering theory and methodology more generally.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "On time series clustering with k-means",
    "abstract": "There is a long history of research into time series clustering using distance-based partitional clustering. Many of the most popular algorithms adapt k-means (also known as Lloyd's algorithm) to exploit time dependencies in the data by specifying a time series distance function. However, these algorithms are often presented with k-means configured in various ways, altering key parameters such as the initialisation strategy. This variability makes it difficult to compare studies because k-means is known to be highly sensitive to its configuration. To address this, we propose a standard Lloyd's-based model for TSCL that adopts an end-to-end approach, incorporating a specialised distance function not only in the assignment step but also in the initialisation and stopping criteria. By doing so, we create a unified structure for comparing seven popular Lloyd's-based TSCL algorithms. This common framework enables us to more easily attribute differences in clustering performance to the distance function itself, rather than variations in the k-means configuration.",
    "metadata": {
      "arxiv_id": "2410.14269",
      "title": "On time series clustering with k-means",
      "summary": "There is a long history of research into time series clustering using distance-based partitional clustering. Many of the most popular algorithms adapt k-means (also known as Lloyd's algorithm) to exploit time dependencies in the data by specifying a time series distance function. However, these algorithms are often presented with k-means configured in various ways, altering key parameters such as the initialisation strategy. This variability makes it difficult to compare studies because k-means is known to be highly sensitive to its configuration. To address this, we propose a standard Lloyd's-based model for TSCL that adopts an end-to-end approach, incorporating a specialised distance function not only in the assignment step but also in the initialisation and stopping criteria. By doing so, we create a unified structure for comparing seven popular Lloyd's-based TSCL algorithms. This common framework enables us to more easily attribute differences in clustering performance to the distance function itself, rather than variations in the k-means configuration.",
      "authors": [
        "Christopher Holder",
        "Anthony Bagnall",
        "Jason Lines"
      ],
      "published": "2024-10-18T08:24:07Z",
      "updated": "2024-10-18T08:24:07Z",
      "categories": [
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2410.14269v1",
      "landing_url": "https://arxiv.org/abs/2410.14269v1",
      "doi": "https://doi.org/10.48550/arXiv.2410.14269"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "Focuses on time-series clustering with k-means and a unified framework for TSCL; it does not propose or analyze discrete audio/speech tokenization or token-based representations.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "Focuses on time-series clustering with k-means and a unified framework for TSCL; it does not propose or analyze discrete audio/speech tokenization or token-based representations.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on time series clustering using k-means algorithms applied to time series data generally, without any mention or focus on discrete audio/speech tokenization or token design relevant to language-model-style token sequences; therefore, it does not meet the inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on time series clustering using k-means algorithms applied to time series data generally, without any mention or focus on discrete audio/speech tokenization or token design relevant to language-model-style token sequences; therefore, it does not meet the inclusion criteria and should be excluded.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "SNAC: Multi-Scale Neural Audio Codec",
    "abstract": "Neural audio codecs have recently gained popularity because they can represent audio signals with high fidelity at very low bitrates, making it feasible to use language modeling approaches for audio generation and understanding. Residual Vector Quantization (RVQ) has become the standard technique for neural audio compression using a cascade of VQ codebooks. This paper proposes the Multi-Scale Neural Audio Codec, a simple extension of RVQ where the quantizers can operate at different temporal resolutions. By applying a hierarchy of quantizers at variable frame rates, the codec adapts to the audio structure across multiple timescales. This leads to more efficient compression, as demonstrated by extensive objective and subjective evaluations. The code and model weights are open-sourced at https://github.com/hubertsiuzdak/snac.",
    "metadata": {
      "arxiv_id": "2410.14411",
      "title": "SNAC: Multi-Scale Neural Audio Codec",
      "summary": "Neural audio codecs have recently gained popularity because they can represent audio signals with high fidelity at very low bitrates, making it feasible to use language modeling approaches for audio generation and understanding. Residual Vector Quantization (RVQ) has become the standard technique for neural audio compression using a cascade of VQ codebooks. This paper proposes the Multi-Scale Neural Audio Codec, a simple extension of RVQ where the quantizers can operate at different temporal resolutions. By applying a hierarchy of quantizers at variable frame rates, the codec adapts to the audio structure across multiple timescales. This leads to more efficient compression, as demonstrated by extensive objective and subjective evaluations. The code and model weights are open-sourced at https://github.com/hubertsiuzdak/snac.",
      "authors": [
        "Hubert Siuzdak",
        "Florian Grötschla",
        "Luca A. Lanzendörfer"
      ],
      "published": "2024-10-18T12:24:05Z",
      "updated": "2024-10-18T12:24:05Z",
      "categories": [
        "cs.SD",
        "cs.LG",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2410.14411v1",
      "landing_url": "https://arxiv.org/abs/2410.14411v1",
      "doi": "https://doi.org/10.48550/arXiv.2410.14411"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The paper explicitly builds a discrete-token representation via vector quantization codebooks (RVQ) across multi-scale temporal resolutions, treating discrete tokens as the basis for audio reconstruction at multiple timescales, which aligns with the inclusion criteria and avoids exclusion criteria.",
      "evaluation": 5
    },
    "round-A_JuniorNano_reasoning": "The paper explicitly builds a discrete-token representation via vector quantization codebooks (RVQ) across multi-scale temporal resolutions, treating discrete tokens as the basis for audio reconstruction at multiple timescales, which aligns with the inclusion criteria and avoids exclusion criteria.",
    "round-A_JuniorNano_evaluation": 5,
    "round-A_JuniorMini_output": {
      "reasoning": "The study proposes a Multi-Scale Neural Audio Codec utilizing Residual Vector Quantization, which inherently involves discrete audio tokenization by converting continuous audio into sequences of discrete symbols for compression and reconstruction. The focus on hierarchical quantizers operating at different temporal resolutions indicates methodological contributions relevant to discrete tokenization, and it explicitly targets neural audio compression with evaluation metrics, aligning with the inclusion criteria. Therefore, it meets all inclusion criteria and does not fall under any exclusion criteria.",
      "evaluation": 5
    },
    "round-A_JuniorMini_reasoning": "The study proposes a Multi-Scale Neural Audio Codec utilizing Residual Vector Quantization, which inherently involves discrete audio tokenization by converting continuous audio into sequences of discrete symbols for compression and reconstruction. The focus on hierarchical quantizers operating at different temporal resolutions indicates methodological contributions relevant to discrete tokenization, and it explicitly targets neural audio compression with evaluation metrics, aligning with the inclusion criteria. Therefore, it meets all inclusion criteria and does not fall under any exclusion criteria.",
    "round-A_JuniorMini_evaluation": 5,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "include (junior:5)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "DM-Codec: Distilling Multimodal Representations for Speech Tokenization",
    "abstract": "Recent advancements in speech-language models have yielded significant improvements in speech tokenization and synthesis. However, effectively mapping the complex, multidimensional attributes of speech into discrete tokens remains challenging. This process demands acoustic, semantic, and contextual information for precise speech representations. Existing speech representations generally fall into two categories: acoustic tokens from audio codecs and semantic tokens from speech self-supervised learning models. Although recent efforts have unified acoustic and semantic tokens for improved performance, they overlook the crucial role of contextual representation in comprehensive speech modeling. Our empirical investigations reveal that the absence of contextual representations results in elevated Word Error Rate (WER) and Word Information Lost (WIL) scores in speech transcriptions. To address these limitations, we propose two novel distillation approaches: (1) a language model (LM)-guided distillation method that incorporates contextual information, and (2) a combined LM and self-supervised speech model (SM)-guided distillation technique that effectively distills multimodal representations (acoustic, semantic, and contextual) into a comprehensive speech tokenizer, termed DM-Codec. The DM-Codec architecture adopts a streamlined encoder-decoder framework with a Residual Vector Quantizer (RVQ) and incorporates the LM and SM during the training process. Experiments show DM-Codec significantly outperforms state-of-the-art speech tokenization models, reducing WER by up to 13.46%, WIL by 9.82%, and improving speech quality by 5.84% and intelligibility by 1.85% on the LibriSpeech benchmark dataset. Code, samples, and checkpoints are available at https://github.com/mubtasimahasan/DM-Codec.",
    "metadata": {
      "arxiv_id": "2410.15017",
      "title": "DM-Codec: Distilling Multimodal Representations for Speech Tokenization",
      "summary": "Recent advancements in speech-language models have yielded significant improvements in speech tokenization and synthesis. However, effectively mapping the complex, multidimensional attributes of speech into discrete tokens remains challenging. This process demands acoustic, semantic, and contextual information for precise speech representations. Existing speech representations generally fall into two categories: acoustic tokens from audio codecs and semantic tokens from speech self-supervised learning models. Although recent efforts have unified acoustic and semantic tokens for improved performance, they overlook the crucial role of contextual representation in comprehensive speech modeling. Our empirical investigations reveal that the absence of contextual representations results in elevated Word Error Rate (WER) and Word Information Lost (WIL) scores in speech transcriptions. To address these limitations, we propose two novel distillation approaches: (1) a language model (LM)-guided distillation method that incorporates contextual information, and (2) a combined LM and self-supervised speech model (SM)-guided distillation technique that effectively distills multimodal representations (acoustic, semantic, and contextual) into a comprehensive speech tokenizer, termed DM-Codec. The DM-Codec architecture adopts a streamlined encoder-decoder framework with a Residual Vector Quantizer (RVQ) and incorporates the LM and SM during the training process. Experiments show DM-Codec significantly outperforms state-of-the-art speech tokenization models, reducing WER by up to 13.46%, WIL by 9.82%, and improving speech quality by 5.84% and intelligibility by 1.85% on the LibriSpeech benchmark dataset. Code, samples, and checkpoints are available at https://github.com/mubtasimahasan/DM-Codec.",
      "authors": [
        "Md Mubtasim Ahasan",
        "Md Fahim",
        "Tasnim Mohiuddin",
        "A K M Mahbubur Rahman",
        "Aman Chadha",
        "Tariq Iqbal",
        "M Ashraful Amin",
        "Md Mofijul Islam",
        "Amin Ahsan Ali"
      ],
      "published": "2024-10-19T07:14:14Z",
      "updated": "2025-09-29T08:08:40Z",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2410.15017v2",
      "landing_url": "https://arxiv.org/abs/2410.15017v2",
      "doi": "https://doi.org/10.48550/arXiv.2410.15017"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The work explicitly targets discrete speech tokenization by distilling multimodal representations into a tokenized speech codec (RVQ) and reports quantitative gains in WER/WIL, fitting the inclusion criteria and not matching any exclusion criteria.",
      "evaluation": 5
    },
    "round-A_JuniorNano_reasoning": "The work explicitly targets discrete speech tokenization by distilling multimodal representations into a tokenized speech codec (RVQ) and reports quantitative gains in WER/WIL, fitting the inclusion criteria and not matching any exclusion criteria.",
    "round-A_JuniorNano_evaluation": 5,
    "round-A_JuniorMini_output": {
      "reasoning": "The study proposes DM-Codec, a novel model that explicitly focuses on discrete speech tokenization by distilling multimodal representations (acoustic, semantic, contextual) into discrete tokens for improved speech transcription and synthesis. It includes systematic analysis and experimental validation showing significant improvements over state-of-the-art speech tokenizers, with accessible code and datasets for academic evaluation. The work goes beyond traditional codecs with neural quantization and focuses on speech, not other modalities or purely text/image tokenization.",
      "evaluation": 5
    },
    "round-A_JuniorMini_reasoning": "The study proposes DM-Codec, a novel model that explicitly focuses on discrete speech tokenization by distilling multimodal representations (acoustic, semantic, contextual) into discrete tokens for improved speech transcription and synthesis. It includes systematic analysis and experimental validation showing significant improvements over state-of-the-art speech tokenizers, with accessible code and datasets for academic evaluation. The work goes beyond traditional codecs with neural quantization and focuses on speech, not other modalities or purely text/image tokenization.",
    "round-A_JuniorMini_evaluation": 5,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "include (junior:5)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Residual vector quantization for KV cache compression in large language model",
    "abstract": "KV cache compression methods have mainly relied on scalar quantization techniques to reduce the memory requirements during decoding. In this work, we apply residual vector quantization, which has been widely used for high fidelity audio compression, to compress KV cache in large language models (LLM). We adapt the standard recipe with minimal changes to compress the output of any key or value projection matrix in a pretrained LLM: we scale the vector by its standard deviation, divide channels into groups and then quantize each group with the same residual vector quantizer. We learn the codebook using exponential moving average and there are no other learnable parameters including the input and output projections normally used in a vector quantization set up. We find that a residual depth of 8 recovers most of the performance of the unquantized model. We also find that grouping non-contiguous channels together works better than grouping contiguous channels for compressing key matrix and the method further benefits from a light weight finetuning of LLM together with the quantization. Overall, the proposed technique is competitive with existing quantization methods while being much simpler and results in 5.5x compression compared to half precision.",
    "metadata": {
      "arxiv_id": "2410.15704",
      "title": "Residual vector quantization for KV cache compression in large language model",
      "summary": "KV cache compression methods have mainly relied on scalar quantization techniques to reduce the memory requirements during decoding. In this work, we apply residual vector quantization, which has been widely used for high fidelity audio compression, to compress KV cache in large language models (LLM). We adapt the standard recipe with minimal changes to compress the output of any key or value projection matrix in a pretrained LLM: we scale the vector by its standard deviation, divide channels into groups and then quantize each group with the same residual vector quantizer. We learn the codebook using exponential moving average and there are no other learnable parameters including the input and output projections normally used in a vector quantization set up. We find that a residual depth of 8 recovers most of the performance of the unquantized model. We also find that grouping non-contiguous channels together works better than grouping contiguous channels for compressing key matrix and the method further benefits from a light weight finetuning of LLM together with the quantization. Overall, the proposed technique is competitive with existing quantization methods while being much simpler and results in 5.5x compression compared to half precision.",
      "authors": [
        "Ankur Kumar"
      ],
      "published": "2024-10-21T07:20:41Z",
      "updated": "2024-10-21T07:20:41Z",
      "categories": [
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2410.15704v1",
      "landing_url": "https://arxiv.org/abs/2410.15704v1",
      "doi": "https://doi.org/10.48550/arXiv.2410.15704"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The study focuses on residual vector quantization for compressing KV caches in large language models, a model-compression technique that does not analyze or propose discrete audio/speech tokenization, thus failing the inclusion criteria for discrete audio tokenization studies.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The study focuses on residual vector quantization for compressing KV caches in large language models, a model-compression technique that does not analyze or propose discrete audio/speech tokenization, thus failing the inclusion criteria for discrete audio tokenization studies.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on residual vector quantization for KV cache compression in large language models and does not propose, analyze, or evaluate discrete audio or speech tokenization methods; it centers on model compression techniques rather than audio token discretization as required by the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on residual vector quantization for KV cache compression in large language models and does not propose, analyze, or evaluate discrete audio or speech tokenization methods; it centers on model compression techniques rather than audio token discretization as required by the inclusion criteria.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "LSCodec: Low-Bitrate and Speaker-Decoupled Discrete Speech Codec",
    "abstract": "Although discrete speech tokens have exhibited strong potential for language model-based speech generation, their high bitrates and redundant timbre information restrict the development of such models. In this work, we propose LSCodec, a discrete speech codec that has both low bitrate and speaker decoupling ability. LSCodec adopts a multi-stage unsupervised training framework with a speaker perturbation technique. A continuous information bottleneck is first established, followed by vector quantization that produces a discrete speaker-decoupled space. A discrete token vocoder finally refines acoustic details from LSCodec. By reconstruction evaluations, LSCodec demonstrates superior intelligibility and audio quality with only a single codebook and smaller vocabulary size than baselines. Voice conversion and speaker probing experiments prove the excellent speaker disentanglement of LSCodec, and ablation study verifies the effectiveness of the proposed training framework.",
    "metadata": {
      "arxiv_id": "2410.15764",
      "title": "LSCodec: Low-Bitrate and Speaker-Decoupled Discrete Speech Codec",
      "summary": "Although discrete speech tokens have exhibited strong potential for language model-based speech generation, their high bitrates and redundant timbre information restrict the development of such models. In this work, we propose LSCodec, a discrete speech codec that has both low bitrate and speaker decoupling ability. LSCodec adopts a multi-stage unsupervised training framework with a speaker perturbation technique. A continuous information bottleneck is first established, followed by vector quantization that produces a discrete speaker-decoupled space. A discrete token vocoder finally refines acoustic details from LSCodec. By reconstruction evaluations, LSCodec demonstrates superior intelligibility and audio quality with only a single codebook and smaller vocabulary size than baselines. Voice conversion and speaker probing experiments prove the excellent speaker disentanglement of LSCodec, and ablation study verifies the effectiveness of the proposed training framework.",
      "authors": [
        "Yiwei Guo",
        "Zhihan Li",
        "Chenpeng Du",
        "Hankun Wang",
        "Xie Chen",
        "Kai Yu"
      ],
      "published": "2024-10-21T08:23:31Z",
      "updated": "2025-05-21T16:46:32Z",
      "categories": [
        "eess.AS",
        "cs.AI",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2410.15764v3",
      "landing_url": "https://arxiv.org/abs/2410.15764v3",
      "doi": "https://doi.org/10.48550/arXiv.2410.15764"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The abstract directly describes a discrete speech codec that uses vector quantization to produce a discrete token space for speech reconstruction and generation, with experiments validating intelligibility and speaker disentanglement, meeting the inclusion criteria and not aligning with exclusion criteria.",
      "evaluation": 5
    },
    "round-A_JuniorNano_reasoning": "The abstract directly describes a discrete speech codec that uses vector quantization to produce a discrete token space for speech reconstruction and generation, with experiments validating intelligibility and speaker disentanglement, meeting the inclusion criteria and not aligning with exclusion criteria.",
    "round-A_JuniorNano_evaluation": 5,
    "round-A_JuniorMini_output": {
      "reasoning": "The study explicitly proposes and evaluates a discrete speech codec, LSCodec, that converts continuous speech signals into discrete tokens with a finite vocabulary, and includes methodological contributions such as a multi-stage unsupervised training framework and speaker decoupling—fulfilling all inclusion criteria and none of the exclusion criteria.",
      "evaluation": 5
    },
    "round-A_JuniorMini_reasoning": "The study explicitly proposes and evaluates a discrete speech codec, LSCodec, that converts continuous speech signals into discrete tokens with a finite vocabulary, and includes methodological contributions such as a multi-stage unsupervised training framework and speaker decoupling—fulfilling all inclusion criteria and none of the exclusion criteria.",
    "round-A_JuniorMini_evaluation": 5,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "include (junior:5)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Pyramid Vector Quantization for LLMs",
    "abstract": "Recent works on compression of large language models (LLM) using quantization considered reparameterizing the architecture such that weights are distributed on the sphere. This demonstratively improves the ability to quantize by increasing the mathematical notion of coherence, resulting in fewer weight outliers without affecting the network output. In this work, we aim to further exploit this spherical geometry of the weights when performing quantization by considering Pyramid Vector Quantization (PVQ) for large language models. Arranging points evenly on the sphere is notoriously difficult, especially in high dimensions, and in case approximate solutions exists, representing points explicitly in a codebook is typically not feasible due to its additional memory cost. Instead, PVQ uses a fixed integer lattice on the sphere by projecting points onto the 1-sphere, which allows for efficient encoding and decoding without requiring an explicit codebook in memory. To obtain a practical algorithm, we propose to combine PVQ with scale quantization for which we derive theoretically optimal quantizations, under empirically verified assumptions. Further, we extend pyramid vector quantization to use Hessian information to minimize quantization error under expected feature activations, instead of only relying on weight magnitudes. Experimentally, we achieves state-of-the-art quantization performance with pareto-optimal trade-off between performance and bits per weight and bits per activation, compared to compared methods. On weight-only, we find that we can quantize a Llama-3 70B model to 3.25 bits per weight and retain 98\\% accuracy on downstream tasks.",
    "metadata": {
      "arxiv_id": "2410.16926",
      "title": "Pyramid Vector Quantization for LLMs",
      "summary": "Recent works on compression of large language models (LLM) using quantization considered reparameterizing the architecture such that weights are distributed on the sphere. This demonstratively improves the ability to quantize by increasing the mathematical notion of coherence, resulting in fewer weight outliers without affecting the network output. In this work, we aim to further exploit this spherical geometry of the weights when performing quantization by considering Pyramid Vector Quantization (PVQ) for large language models. Arranging points evenly on the sphere is notoriously difficult, especially in high dimensions, and in case approximate solutions exists, representing points explicitly in a codebook is typically not feasible due to its additional memory cost. Instead, PVQ uses a fixed integer lattice on the sphere by projecting points onto the 1-sphere, which allows for efficient encoding and decoding without requiring an explicit codebook in memory. To obtain a practical algorithm, we propose to combine PVQ with scale quantization for which we derive theoretically optimal quantizations, under empirically verified assumptions. Further, we extend pyramid vector quantization to use Hessian information to minimize quantization error under expected feature activations, instead of only relying on weight magnitudes. Experimentally, we achieves state-of-the-art quantization performance with pareto-optimal trade-off between performance and bits per weight and bits per activation, compared to compared methods. On weight-only, we find that we can quantize a Llama-3 70B model to 3.25 bits per weight and retain 98\\% accuracy on downstream tasks.",
      "authors": [
        "Tycho F. A. van der Ouderaa",
        "Maximilian L. Croci",
        "Agrin Hilmkil",
        "James Hensman"
      ],
      "published": "2024-10-22T11:57:32Z",
      "updated": "2024-12-04T10:52:04Z",
      "categories": [
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2410.16926v2",
      "landing_url": "https://arxiv.org/abs/2410.16926v2",
      "doi": "https://doi.org/10.48550/arXiv.2410.16926"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This article investigates quantization of LLM weights (Pyramid Vector Quantization) for model compression, with no discrete audio/speech tokenization; it does not address converting audio into discrete tokens.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "This article investigates quantization of LLM weights (Pyramid Vector Quantization) for model compression, with no discrete audio/speech tokenization; it does not address converting audio into discrete tokens.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on quantization techniques for compressing large language models weights and does not involve discrete audio or speech tokenization, nor does it deal with converting continuous audio signals into discrete tokens for audio reconstruction, generation, or understanding tasks, thereby failing to meet the inclusion criteria and aligning with exclusion criteria related to lack of discrete audio token analysis.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on quantization techniques for compressing large language models weights and does not involve discrete audio or speech tokenization, nor does it deal with converting continuous audio signals into discrete tokens for audio reconstruction, generation, or understanding tasks, thereby failing to meet the inclusion criteria and aligning with exclusion criteria related to lack of discrete audio token analysis.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Continuous Speech Tokenizer in Text To Speech",
    "abstract": "The fusion of speech and language in the era of large language models has garnered significant attention. Discrete speech token is often utilized in text-to-speech tasks for speech compression and portability, which is convenient for joint training with text and have good compression efficiency. However, we found that the discrete speech tokenizer still suffers from information loss. Therefore, we propose a simple yet effective continuous speech tokenizer named Cont-SPT, and a text-to-speech model based on continuous speech tokens. Our results show that the speech language model based on the continuous speech tokenizer has better continuity and higher estimated Mean Opinion Scores (MoS). This enhancement is attributed to better information preservation rate of the continuous speech tokenizer across both low and high frequencies in the frequency domain. The code and resources for Cont-SPT can be found in https://github.com/Yixing-Li/Continuous-Speech-Tokenizer",
    "metadata": {
      "arxiv_id": "2410.17081",
      "title": "Continuous Speech Tokenizer in Text To Speech",
      "summary": "The fusion of speech and language in the era of large language models has garnered significant attention. Discrete speech token is often utilized in text-to-speech tasks for speech compression and portability, which is convenient for joint training with text and have good compression efficiency. However, we found that the discrete speech tokenizer still suffers from information loss. Therefore, we propose a simple yet effective continuous speech tokenizer named Cont-SPT, and a text-to-speech model based on continuous speech tokens. Our results show that the speech language model based on the continuous speech tokenizer has better continuity and higher estimated Mean Opinion Scores (MoS). This enhancement is attributed to better information preservation rate of the continuous speech tokenizer across both low and high frequencies in the frequency domain. The code and resources for Cont-SPT can be found in https://github.com/Yixing-Li/Continuous-Speech-Tokenizer",
      "authors": [
        "Yixing Li",
        "Ruobing Xie",
        "Xingwu Sun",
        "Yu Cheng",
        "Zhanhui Kang"
      ],
      "published": "2024-10-22T15:02:37Z",
      "updated": "2025-03-31T13:57:49Z",
      "categories": [
        "cs.SD",
        "cs.CL",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2410.17081v2",
      "landing_url": "https://arxiv.org/abs/2410.17081v2",
      "doi": "https://doi.org/10.48550/arXiv.2410.17081"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The study centers on a continuous speech tokenizer (Cont-SPT) and continuous tokens, not discrete tokenization; it does not meet the inclusion criterion requiring explicit analysis/evaluation of discrete audio/speech tokens.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The study centers on a continuous speech tokenizer (Cont-SPT) and continuous tokens, not discrete tokenization; it does not meet the inclusion criterion requiring explicit analysis/evaluation of discrete audio/speech tokens.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study proposes a continuous speech tokenizer instead of discrete tokens, and emphasizes that discrete speech tokenization suffers from information loss; it does not analyze or evaluate discrete audio tokenization as the main focus, but rather proposes a continuous alternative, which falls outside the inclusion criterion requiring discrete tokenization as the core object. Therefore, it does not meet the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study proposes a continuous speech tokenizer instead of discrete tokens, and emphasizes that discrete speech tokenization suffers from information loss; it does not analyze or evaluate discrete audio tokenization as the main focus, but rather proposes a continuous alternative, which falls outside the inclusion criterion requiring discrete tokenization as the core object. Therefore, it does not meet the inclusion criteria.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Inference with K-means",
    "abstract": "This thesis aims to invent new approaches for making inferences with the k-means algorithm. k-means is an iterative clustering algorithm that randomly assigns k centroids, then assigns data points to the nearest centroid, and updates centroids based on the mean of assigned points. This process continues until convergence, forming k clusters where each point belongs to the closest centroid. This research investigates the prediction of the last component of data points obtained from a distribution of clustered data using the online balanced k-means approach. Through extensive experimentation and analysis, key findings have emerged. It is observed that a larger number of clusters or partitions tends to yield lower errors while increasing the number of assigned data points does not significantly improve inference errors. Reducing losses in the learning process does not significantly impact overall inference errors. Indicating that as learning is going on inference errors remain unchanged. Recommendations include the need for specialized inference techniques to estimate better data points derived from multi-clustered data and exploring methods that yield improved results with larger assigned datasets. By addressing these recommendations, this research advances the accuracy and reliability of inferences made with the k-means algorithm, bridging the gap between clustering and non-parametric density estimation and inference.",
    "metadata": {
      "arxiv_id": "2410.17256",
      "title": "Inference with K-means",
      "summary": "This thesis aims to invent new approaches for making inferences with the k-means algorithm. k-means is an iterative clustering algorithm that randomly assigns k centroids, then assigns data points to the nearest centroid, and updates centroids based on the mean of assigned points. This process continues until convergence, forming k clusters where each point belongs to the closest centroid. This research investigates the prediction of the last component of data points obtained from a distribution of clustered data using the online balanced k-means approach. Through extensive experimentation and analysis, key findings have emerged. It is observed that a larger number of clusters or partitions tends to yield lower errors while increasing the number of assigned data points does not significantly improve inference errors. Reducing losses in the learning process does not significantly impact overall inference errors. Indicating that as learning is going on inference errors remain unchanged. Recommendations include the need for specialized inference techniques to estimate better data points derived from multi-clustered data and exploring methods that yield improved results with larger assigned datasets. By addressing these recommendations, this research advances the accuracy and reliability of inferences made with the k-means algorithm, bridging the gap between clustering and non-parametric density estimation and inference.",
      "authors": [
        "Alfred K. Adzika",
        "Prudence Djagba"
      ],
      "published": "2024-10-04T06:51:58Z",
      "updated": "2024-10-04T06:51:58Z",
      "categories": [
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2410.17256v1",
      "landing_url": "https://arxiv.org/abs/2410.17256v1",
      "doi": "https://doi.org/10.48550/arXiv.2410.17256"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "Reasoning: The item focuses on general k-means inference and clustering techniques without any discrete audio/speech tokenization or analysis of token representations, hence it does not meet the inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "Reasoning: The item focuses on general k-means inference and clustering techniques without any discrete audio/speech tokenization or analysis of token representations, hence it does not meet the inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on k-means clustering and inference methods without any mention of discrete audio or speech tokenization, nor does it address audio signal processing or discrete token representations relevant to audio reconstruction or understanding tasks, thus failing to meet the inclusion criteria and aligning with the exclusion criteria regarding the lack of discrete audio tokenization analysis.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on k-means clustering and inference methods without any mention of discrete audio or speech tokenization, nor does it address audio signal processing or discrete token representations relevant to audio reconstruction or understanding tasks, thus failing to meet the inclusion criteria and aligning with the exclusion criteria regarding the lack of discrete audio tokenization analysis.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Unsupervised Panoptic Interpretation of Latent Spaces in GANs Using Space-Filling Vector Quantization",
    "abstract": "Generative adversarial networks (GANs) learn a latent space whose samples can be mapped to real-world images. Such latent spaces are difficult to interpret. Some earlier supervised methods aim to create an interpretable latent space or discover interpretable directions, which requires exploiting data labels or annotated synthesized samples for training. However, we propose using a modification of vector quantization called space-filling vector quantization (SFVQ), which quantizes the data on a piece-wise linear curve. SFVQ can capture the underlying morphological structure of the latent space, making it interpretable. We apply this technique to model the latent space of pre-trained StyleGAN2 and BigGAN networks on various datasets. Our experiments show that the SFVQ curve yields a general interpretable model of the latent space such that it determines which parts of the latent space correspond to specific generative factors. Furthermore, we demonstrate that each line of the SFVQ curve can potentially refer to an interpretable direction for applying intelligible image transformations. We also demonstrate that the points located on an SFVQ line can be used for controllable data augmentation.",
    "metadata": {
      "arxiv_id": "2410.20573",
      "title": "Unsupervised Panoptic Interpretation of Latent Spaces in GANs Using Space-Filling Vector Quantization",
      "summary": "Generative adversarial networks (GANs) learn a latent space whose samples can be mapped to real-world images. Such latent spaces are difficult to interpret. Some earlier supervised methods aim to create an interpretable latent space or discover interpretable directions, which requires exploiting data labels or annotated synthesized samples for training. However, we propose using a modification of vector quantization called space-filling vector quantization (SFVQ), which quantizes the data on a piece-wise linear curve. SFVQ can capture the underlying morphological structure of the latent space, making it interpretable. We apply this technique to model the latent space of pre-trained StyleGAN2 and BigGAN networks on various datasets. Our experiments show that the SFVQ curve yields a general interpretable model of the latent space such that it determines which parts of the latent space correspond to specific generative factors. Furthermore, we demonstrate that each line of the SFVQ curve can potentially refer to an interpretable direction for applying intelligible image transformations. We also demonstrate that the points located on an SFVQ line can be used for controllable data augmentation.",
      "authors": [
        "Mohammad Hassan Vali",
        "Tom Bäckström"
      ],
      "published": "2024-10-27T19:56:02Z",
      "updated": "2025-07-02T10:27:13Z",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2410.20573v2",
      "landing_url": "https://arxiv.org/abs/2410.20573v2",
      "doi": "https://doi.org/10.48550/arXiv.2410.20573"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item discusses unsupervised interpretation of GAN latent spaces using space-filling vector quantization for image generation, not involving discrete audio/speech tokenization; thus it does not meet the inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item discusses unsupervised interpretation of GAN latent spaces using space-filling vector quantization for image generation, not involving discrete audio/speech tokenization; thus it does not meet the inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on unsupervised interpretation of latent spaces in GANs using space-filling vector quantization, applied to image generation models (StyleGAN2 and BigGAN), without addressing audio/speech tokenization or discrete audio token representation relevant to speech tasks, which is the core inclusion criterion.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on unsupervised interpretation of latent spaces in GANs using space-filling vector quantization, applied to image generation models (StyleGAN2 and BigGAN), without addressing audio/speech tokenization or discrete audio token representation relevant to speech tasks, which is the core inclusion criterion.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Fast and High-Quality Auto-Regressive Speech Synthesis via Speculative Decoding",
    "abstract": "The auto-regressive architecture, like GPTs, is widely used in modern Text-to-Speech (TTS) systems. However, it incurs substantial inference time, particularly due to the challenges in the next-token prediction posed by lengthy sequences of speech tokens. In this work, we introduce VADUSA, one of the first approaches to accelerate auto-regressive TTS through speculative decoding. Our results show that VADUSA not only significantly improves inference speed but also enhances performance by incorporating draft heads to predict future speech content auto-regressively. Furthermore, the inclusion of a tolerance mechanism during sampling accelerates inference without compromising quality. Our approach demonstrates strong generalization across large datasets and various types of speech tokens.",
    "metadata": {
      "arxiv_id": "2410.21951",
      "title": "Fast and High-Quality Auto-Regressive Speech Synthesis via Speculative Decoding",
      "summary": "The auto-regressive architecture, like GPTs, is widely used in modern Text-to-Speech (TTS) systems. However, it incurs substantial inference time, particularly due to the challenges in the next-token prediction posed by lengthy sequences of speech tokens. In this work, we introduce VADUSA, one of the first approaches to accelerate auto-regressive TTS through speculative decoding. Our results show that VADUSA not only significantly improves inference speed but also enhances performance by incorporating draft heads to predict future speech content auto-regressively. Furthermore, the inclusion of a tolerance mechanism during sampling accelerates inference without compromising quality. Our approach demonstrates strong generalization across large datasets and various types of speech tokens.",
      "authors": [
        "Bohan Li",
        "Hankun Wang",
        "Situo Zhang",
        "Yiwei Guo",
        "Kai Yu"
      ],
      "published": "2024-10-29T11:12:01Z",
      "updated": "2025-02-10T04:22:08Z",
      "categories": [
        "eess.AS",
        "cs.AI",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2410.21951v2",
      "landing_url": "https://arxiv.org/abs/2410.21951v2",
      "doi": "https://doi.org/10.48550/arXiv.2410.21951"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The abstract centers on auto-regressive speech generation using discrete speech tokens and speculative decoding, addressing token-level representations; thus it aligns with tokenization-focused studies, though the tokenization method is not deeply detailed.",
      "evaluation": 4
    },
    "round-A_JuniorNano_reasoning": "The abstract centers on auto-regressive speech generation using discrete speech tokens and speculative decoding, addressing token-level representations; thus it aligns with tokenization-focused studies, though the tokenization method is not deeply detailed.",
    "round-A_JuniorNano_evaluation": 4,
    "round-A_JuniorMini_output": {
      "reasoning": "The abstract discusses accelerating auto-regressive text-to-speech synthesis with speculative decoding, focusing on improving inference speed and maintaining quality, but it does not explicitly mention discrete audio/speech tokenization or analyzing discrete token representations as the core study object, which is a strict requirement in the inclusion criteria.",
      "evaluation": 2
    },
    "round-A_JuniorMini_reasoning": "The abstract discusses accelerating auto-regressive text-to-speech synthesis with speculative decoding, focusing on improving inference speed and maintaining quality, but it does not explicitly mention discrete audio/speech tokenization or analyzing discrete token representations as the core study object, which is a strict requirement in the inclusion criteria.",
    "round-A_JuniorMini_evaluation": 2,
    "round-B_SeniorLead_output": {
      "reasoning": "Junior reviewers disagreed: one leaned to include because the abstract mentions ‘‘speech tokens’’, while the other recommended exclusion because the work does not explicitly propose, analyze, or evaluate a discrete audio tokenization method. The abstract focuses on accelerating auto-regressive TTS via speculative decoding and draft heads and only references ‘‘speech tokens’’ at a high level without describing tokenization, quantization, or treating tokenization as the core object of study—therefore it does not meet the inclusion criterion requiring explicit proposal/analysis/evaluation of discrete audio/speech tokenization. Recommendation: exclude (score 2).",
      "evaluation": 2
    },
    "round-B_SeniorLead_reasoning": "Junior reviewers disagreed: one leaned to include because the abstract mentions ‘‘speech tokens’’, while the other recommended exclusion because the work does not explicitly propose, analyze, or evaluate a discrete audio tokenization method. The abstract focuses on accelerating auto-regressive TTS via speculative decoding and draft heads and only references ‘‘speech tokens’’ at a high level without describing tokenization, quantization, or treating tokenization as the core object of study—therefore it does not meet the inclusion criterion requiring explicit proposal/analysis/evaluation of discrete audio/speech tokenization. Recommendation: exclude (score 2).",
    "round-B_SeniorLead_evaluation": 2,
    "final_verdict": "exclude (senior:2)",
    "review_skipped": false,
    "discard_reason": "exclude (senior:2)"
  },
  {
    "title": "A Closer Look at Neural Codec Resynthesis: Bridging the Gap between Codec and Waveform Generation",
    "abstract": "Neural Audio Codecs, initially designed as a compression technique, have gained more attention recently for speech generation. Codec models represent each audio frame as a sequence of tokens, i.e., discrete embeddings. The discrete and low-frequency nature of neural codecs introduced a new way to generate speech with token-based models. As these tokens encode information at various levels of granularity, from coarse to fine, most existing works focus on how to better generate the coarse tokens. In this paper, we focus on an equally important but often overlooked question: How can we better resynthesize the waveform from coarse tokens? We point out that both the choice of learning target and resynthesis approach have a dramatic impact on the generated audio quality. Specifically, we study two different strategies based on token prediction and regression, and introduce a new method based on Schrödinger Bridge. We examine how different design choices affect machine and human perception.",
    "metadata": {
      "arxiv_id": "2410.22448",
      "title": "A Closer Look at Neural Codec Resynthesis: Bridging the Gap between Codec and Waveform Generation",
      "summary": "Neural Audio Codecs, initially designed as a compression technique, have gained more attention recently for speech generation. Codec models represent each audio frame as a sequence of tokens, i.e., discrete embeddings. The discrete and low-frequency nature of neural codecs introduced a new way to generate speech with token-based models. As these tokens encode information at various levels of granularity, from coarse to fine, most existing works focus on how to better generate the coarse tokens. In this paper, we focus on an equally important but often overlooked question: How can we better resynthesize the waveform from coarse tokens? We point out that both the choice of learning target and resynthesis approach have a dramatic impact on the generated audio quality. Specifically, we study two different strategies based on token prediction and regression, and introduce a new method based on Schrödinger Bridge. We examine how different design choices affect machine and human perception.",
      "authors": [
        "Alexander H. Liu",
        "Qirui Wang",
        "Yuan Gong",
        "James Glass"
      ],
      "published": "2024-10-29T18:29:39Z",
      "updated": "2024-10-29T18:29:39Z",
      "categories": [
        "eess.AS",
        "cs.CL",
        "cs.LG",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2410.22448v1",
      "landing_url": "https://arxiv.org/abs/2410.22448v1",
      "doi": "https://doi.org/10.48550/arXiv.2410.22448"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The abstract centers on neural audio codecs that tokenize audio into discrete symbols and studies how to reconstruct waveforms from those tokens, addressing discrete tokenization as a core object of study.",
      "evaluation": 5
    },
    "round-A_JuniorNano_reasoning": "The abstract centers on neural audio codecs that tokenize audio into discrete symbols and studies how to reconstruct waveforms from those tokens, addressing discrete tokenization as a core object of study.",
    "round-A_JuniorNano_evaluation": 5,
    "round-A_JuniorMini_output": {
      "reasoning": "The study explicitly focuses on neural audio codecs that convert continuous audio signals into discrete token sequences for waveform generation, aligning well with the inclusion criteria of analyzing discrete audio tokenization for speech generation, and it does not fall under any exclusion criteria such as using only continuous features, traditional codecs, or being an engineering-only report.",
      "evaluation": 5
    },
    "round-A_JuniorMini_reasoning": "The study explicitly focuses on neural audio codecs that convert continuous audio signals into discrete token sequences for waveform generation, aligning well with the inclusion criteria of analyzing discrete audio tokenization for speech generation, and it does not fall under any exclusion criteria such as using only continuous features, traditional codecs, or being an engineering-only report.",
    "round-A_JuniorMini_evaluation": 5,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "include (junior:5)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "DC-Spin: A Speaker-invariant Speech Tokenizer for Spoken Language Models",
    "abstract": "Spoken language models (SLMs) have gained increasing attention with advancements in text-based, decoder-only language models. SLMs process text and speech, enabling simultaneous speech understanding and generation. This paper presents Double-Codebook Speaker-invariant Clustering (DC-Spin), which aims to improve speech tokenization by bridging audio signals and SLM tokens. DC-Spin extracts speaker-invariant tokens rich in phonetic information and resilient to input variations, enhancing zero-shot SLM tasks and speech resynthesis. We propose a chunk-wise approach to enable streamable DC-Spin without retraining and degradation. Comparisons of tokenization methods (self-supervised and neural audio codecs), model scalability, and downstream task proxies show that tokens easily modeled by an n-gram LM or aligned with phonemes offer strong performance, providing insights for designing speech tokenizers for SLMs.",
    "metadata": {
      "arxiv_id": "2410.24177",
      "title": "DC-Spin: A Speaker-invariant Speech Tokenizer for Spoken Language Models",
      "summary": "Spoken language models (SLMs) have gained increasing attention with advancements in text-based, decoder-only language models. SLMs process text and speech, enabling simultaneous speech understanding and generation. This paper presents Double-Codebook Speaker-invariant Clustering (DC-Spin), which aims to improve speech tokenization by bridging audio signals and SLM tokens. DC-Spin extracts speaker-invariant tokens rich in phonetic information and resilient to input variations, enhancing zero-shot SLM tasks and speech resynthesis. We propose a chunk-wise approach to enable streamable DC-Spin without retraining and degradation. Comparisons of tokenization methods (self-supervised and neural audio codecs), model scalability, and downstream task proxies show that tokens easily modeled by an n-gram LM or aligned with phonemes offer strong performance, providing insights for designing speech tokenizers for SLMs.",
      "authors": [
        "Heng-Jui Chang",
        "Hongyu Gong",
        "Changhan Wang",
        "James Glass",
        "Yu-An Chung"
      ],
      "published": "2024-10-31T17:43:13Z",
      "updated": "2024-10-31T17:43:13Z",
      "categories": [
        "eess.AS",
        "cs.CL",
        "cs.LG",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2410.24177v1",
      "landing_url": "https://arxiv.org/abs/2410.24177v1",
      "doi": "https://doi.org/10.48550/arXiv.2410.24177"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The paper explicitly develops and evaluates a discrete speech tokenizer that maps audio to a finite-token vocabulary for speech understanding/generation, addressing token design and downstream evaluation, thereby meeting the inclusion criteria and not triggering the exclusions.",
      "evaluation": 5
    },
    "round-A_JuniorNano_reasoning": "The paper explicitly develops and evaluates a discrete speech tokenizer that maps audio to a finite-token vocabulary for speech understanding/generation, addressing token design and downstream evaluation, thereby meeting the inclusion criteria and not triggering the exclusions.",
    "round-A_JuniorNano_evaluation": 5,
    "round-A_JuniorMini_output": {
      "reasoning": "The study explicitly proposes and evaluates a discrete audio/speech tokenization method (DC-Spin) aimed at converting continuous speech signals into speaker-invariant discrete tokens rich in phonetic information, designed for spoken language models. It includes methodological contributions, systematic analysis, and experimental validation, focusing on discrete tokenization rather than continuous audio features, and is relevant to speech tasks. Therefore, it meets all inclusion criteria and none of the exclusion criteria.",
      "evaluation": 5
    },
    "round-A_JuniorMini_reasoning": "The study explicitly proposes and evaluates a discrete audio/speech tokenization method (DC-Spin) aimed at converting continuous speech signals into speaker-invariant discrete tokens rich in phonetic information, designed for spoken language models. It includes methodological contributions, systematic analysis, and experimental validation, focusing on discrete tokenization rather than continuous audio features, and is relevant to speech tasks. Therefore, it meets all inclusion criteria and none of the exclusion criteria.",
    "round-A_JuniorMini_evaluation": 5,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "include (junior:5)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Reducing Data Fragmentation in Data Deduplication Systems via Partial Repetition and Coding",
    "abstract": "Data deduplication, one of the key features of modern Big Data storage devices, is the process of removing replicas of data chunks stored by different users. Despite the importance of deduplication, several drawbacks of the method, such as storage robustness and file fragmentation, have not been previously analyzed from a theoretical point of view. Storage robustness pertains to ensuring that deduplicated data can be used to reconstruct the original files without service disruptions and data loss. Fragmentation pertains to the problems of placing deduplicated data chunks of different user files in a proximity-preserving linear order, since neighboring chunks of the same file may be stored in sectors far apart on the server. This work proposes a new theoretical model for data fragmentation and introduces novel graph- and coding-theoretic approaches for reducing fragmentation via limited duplication (repetition coding) and coded deduplication (e.g., linear coding). In addition to alleviating issues with fragmentation, limited duplication and coded deduplication can also serve the dual purpose of increasing the robusteness of the system design. The contributions of our work are three-fold. First, we describe a new model for file structures in the form of self-avoiding (simple) paths in specialized graphs. Second, we introduce several new metrics for measuring the fragmentation level in deduplication systems on graph-structured files, including the stretch metric that captures the worst-case \"spread\" of adjacent data chunks within a file when deduplicated and placed on the server; and, the jump metric that captures the worst-case number of times during the reconstruction process of a file that one has to change the readout location on the server. For the stretch metric, we establish a connection between the level of fragmentation and the bandwidth of the file-graph. In particular, ...",
    "metadata": {
      "arxiv_id": "2411.01407",
      "title": "Reducing Data Fragmentation in Data Deduplication Systems via Partial Repetition and Coding",
      "summary": "Data deduplication, one of the key features of modern Big Data storage devices, is the process of removing replicas of data chunks stored by different users. Despite the importance of deduplication, several drawbacks of the method, such as storage robustness and file fragmentation, have not been previously analyzed from a theoretical point of view. Storage robustness pertains to ensuring that deduplicated data can be used to reconstruct the original files without service disruptions and data loss. Fragmentation pertains to the problems of placing deduplicated data chunks of different user files in a proximity-preserving linear order, since neighboring chunks of the same file may be stored in sectors far apart on the server. This work proposes a new theoretical model for data fragmentation and introduces novel graph- and coding-theoretic approaches for reducing fragmentation via limited duplication (repetition coding) and coded deduplication (e.g., linear coding). In addition to alleviating issues with fragmentation, limited duplication and coded deduplication can also serve the dual purpose of increasing the robusteness of the system design. The contributions of our work are three-fold. First, we describe a new model for file structures in the form of self-avoiding (simple) paths in specialized graphs. Second, we introduce several new metrics for measuring the fragmentation level in deduplication systems on graph-structured files, including the stretch metric that captures the worst-case \"spread\" of adjacent data chunks within a file when deduplicated and placed on the server; and, the jump metric that captures the worst-case number of times during the reconstruction process of a file that one has to change the readout location on the server. For the stretch metric, we establish a connection between the level of fragmentation and the bandwidth of the file-graph. In particular, ...",
      "authors": [
        "Yun-Han Li",
        "Jin Sima",
        "Ilan Shomorony",
        "Olgica Milenkovic"
      ],
      "published": "2024-11-03T02:14:03Z",
      "updated": "2024-11-03T02:14:03Z",
      "categories": [
        "cs.IT",
        "cs.DM"
      ],
      "pdf_url": "https://arxiv.org/pdf/2411.01407v1",
      "landing_url": "https://arxiv.org/abs/2411.01407v1",
      "doi": "https://doi.org/10.48550/arXiv.2411.01407"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The work focuses on data deduplication, fragmentation, and coding in storage systems and does not propose or analyze discrete audio/speech tokenization as the core object; it is not about converting continuous audio signals into discrete tokens for speech tasks.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The work focuses on data deduplication, fragmentation, and coding in storage systems and does not propose or analyze discrete audio/speech tokenization as the core object; it is not about converting continuous audio signals into discrete tokens for speech tasks.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on data deduplication systems addressing storage robustness and data fragmentation using graph and coding-theoretic methods; it does not address discrete audio/speech tokenization or the conversion of continuous audio signals into discrete tokens, nor does it mention audio or speech data at all. Hence, it clearly does not meet any of the inclusion criteria relating to audio tokenization, and it does not relate to speech/audio tokenization tasks.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on data deduplication systems addressing storage robustness and data fragmentation using graph and coding-theoretic methods; it does not address discrete audio/speech tokenization or the conversion of continuous audio signals into discrete tokens, nor does it mention audio or speech data at all. Hence, it clearly does not meet any of the inclusion criteria relating to audio tokenization, and it does not relate to speech/audio tokenization tasks.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Speaker Emotion Recognition: Leveraging Self-Supervised Models for Feature Extraction Using Wav2Vec2 and HuBERT",
    "abstract": "Speech is the most natural way of expressing ourselves as humans. Identifying emotion from speech is a nontrivial task due to the ambiguous definition of emotion itself. Speaker Emotion Recognition (SER) is essential for understanding human emotional behavior. The SER task is challenging due to the variety of speakers, background noise, complexity of emotions, and speaking styles. It has many applications in education, healthcare, customer service, and Human-Computer Interaction (HCI). Previously, conventional machine learning methods such as SVM, HMM, and KNN have been used for the SER task. In recent years, deep learning methods have become popular, with convolutional neural networks and recurrent neural networks being used for SER tasks. The input of these methods is mostly spectrograms and hand-crafted features. In this work, we study the use of self-supervised transformer-based models, Wav2Vec2 and HuBERT, to determine the emotion of speakers from their voice. The models automatically extract features from raw audio signals, which are then used for the classification task. The proposed solution is evaluated on reputable datasets, including RAVDESS, SHEMO, SAVEE, AESDD, and Emo-DB. The results show the effectiveness of the proposed method on different datasets. Moreover, the model has been used for real-world applications like call center conversations, and the results demonstrate that the model accurately predicts emotions.",
    "metadata": {
      "arxiv_id": "2411.02964",
      "title": "Speaker Emotion Recognition: Leveraging Self-Supervised Models for Feature Extraction Using Wav2Vec2 and HuBERT",
      "summary": "Speech is the most natural way of expressing ourselves as humans. Identifying emotion from speech is a nontrivial task due to the ambiguous definition of emotion itself. Speaker Emotion Recognition (SER) is essential for understanding human emotional behavior. The SER task is challenging due to the variety of speakers, background noise, complexity of emotions, and speaking styles. It has many applications in education, healthcare, customer service, and Human-Computer Interaction (HCI). Previously, conventional machine learning methods such as SVM, HMM, and KNN have been used for the SER task. In recent years, deep learning methods have become popular, with convolutional neural networks and recurrent neural networks being used for SER tasks. The input of these methods is mostly spectrograms and hand-crafted features. In this work, we study the use of self-supervised transformer-based models, Wav2Vec2 and HuBERT, to determine the emotion of speakers from their voice. The models automatically extract features from raw audio signals, which are then used for the classification task. The proposed solution is evaluated on reputable datasets, including RAVDESS, SHEMO, SAVEE, AESDD, and Emo-DB. The results show the effectiveness of the proposed method on different datasets. Moreover, the model has been used for real-world applications like call center conversations, and the results demonstrate that the model accurately predicts emotions.",
      "authors": [
        "Pourya Jafarzadeh",
        "Amir Mohammad Rostami",
        "Padideh Choobdar"
      ],
      "published": "2024-11-05T10:06:40Z",
      "updated": "2024-11-06T14:18:15Z",
      "categories": [
        "cs.SD",
        "cs.AI",
        "cs.LG",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2411.02964v2",
      "landing_url": "https://arxiv.org/abs/2411.02964v2",
      "doi": "https://doi.org/10.48550/arXiv.2411.02964"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The paper targets speaker emotion recognition using self-supervised models for feature extraction; it does not explicitly propose, analyze, or evaluate discrete audio tokenization (quantized/discrete tokens) as the core object of study, so it does not meet the inclusion criteria and should be excluded.",
      "evaluation": 2
    },
    "round-A_JuniorNano_reasoning": "The paper targets speaker emotion recognition using self-supervised models for feature extraction; it does not explicitly propose, analyze, or evaluate discrete audio tokenization (quantized/discrete tokens) as the core object of study, so it does not meet the inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 2,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on using self-supervised transformer-based models (Wav2Vec2 and HuBERT) for Speaker Emotion Recognition by extracting features from raw audio. However, based on the abstract, the primary focus is on emotion classification performance rather than explicitly proposing, analyzing, or evaluating discrete audio tokenization as the core object of study. There is no clear indication that the study systematically analyzes discrete tokenization or quantization methods as required by the inclusion criteria, and it appears to use the models mainly for feature extraction without a discrete tokenization methodological contribution.",
      "evaluation": 2
    },
    "round-A_JuniorMini_reasoning": "The study focuses on using self-supervised transformer-based models (Wav2Vec2 and HuBERT) for Speaker Emotion Recognition by extracting features from raw audio. However, based on the abstract, the primary focus is on emotion classification performance rather than explicitly proposing, analyzing, or evaluating discrete audio tokenization as the core object of study. There is no clear indication that the study systematically analyzes discrete tokenization or quantization methods as required by the inclusion criteria, and it appears to use the models mainly for feature extraction without a discrete tokenization methodological contribution.",
    "round-A_JuniorMini_evaluation": 2,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:2)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:2)"
  },
  {
    "title": "Understanding Self-Supervised Learning via Gaussian Mixture Models",
    "abstract": "Self-supervised learning attempts to learn representations from un-labeled data; it does so via a loss function that encourages the embedding of a point to be close to that of its augmentations. This simple idea performs remarkably well, yet it is not precisely theoretically understood why this is the case. In this paper we analyze self-supervised learning in a natural context: dimensionality reduction in Gaussian Mixture Models. Crucially, we define an augmentation of a data point as being another independent draw from the same underlying mixture component. We show that vanilla contrastive learning (specifically, the InfoNCE loss) is able to find the optimal lower-dimensional subspace even when the Gaussians are not isotropic -- something that vanilla spectral techniques cannot do. We also prove a similar result for \"non-contrastive\" self-supervised learning (i.e., SimSiam loss). We further extend our analyses to multi-modal contrastive learning algorithms (e.g., CLIP). In this setting we show that contrastive learning learns the subset of fisher-optimal subspace, effectively filtering out all the noise from the learnt representations. Finally, we corroborate our theoretical finding through synthetic data experiments.",
    "metadata": {
      "arxiv_id": "2411.03517",
      "title": "Understanding Self-Supervised Learning via Gaussian Mixture Models",
      "summary": "Self-supervised learning attempts to learn representations from un-labeled data; it does so via a loss function that encourages the embedding of a point to be close to that of its augmentations. This simple idea performs remarkably well, yet it is not precisely theoretically understood why this is the case. In this paper we analyze self-supervised learning in a natural context: dimensionality reduction in Gaussian Mixture Models. Crucially, we define an augmentation of a data point as being another independent draw from the same underlying mixture component. We show that vanilla contrastive learning (specifically, the InfoNCE loss) is able to find the optimal lower-dimensional subspace even when the Gaussians are not isotropic -- something that vanilla spectral techniques cannot do. We also prove a similar result for \"non-contrastive\" self-supervised learning (i.e., SimSiam loss). We further extend our analyses to multi-modal contrastive learning algorithms (e.g., CLIP). In this setting we show that contrastive learning learns the subset of fisher-optimal subspace, effectively filtering out all the noise from the learnt representations. Finally, we corroborate our theoretical finding through synthetic data experiments.",
      "authors": [
        "Parikshit Bansal",
        "Ali Kavis",
        "Sujay Sanghavi"
      ],
      "published": "2024-11-05T21:43:05Z",
      "updated": "2025-02-06T18:48:27Z",
      "categories": [
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2411.03517v2",
      "landing_url": "https://arxiv.org/abs/2411.03517v2",
      "doi": "https://doi.org/10.48550/arXiv.2411.03517"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The paper discusses self-supervised learning via Gaussian mixture models and does not involve discrete audio tokenization or speech token sequences, so it does not meet the audio/tokenization inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The paper discusses self-supervised learning via Gaussian mixture models and does not involve discrete audio tokenization or speech token sequences, so it does not meet the audio/tokenization inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on the theoretical analysis of self-supervised learning methods in the context of Gaussian Mixture Models for dimensionality reduction, without any discussion or proposal related to discrete audio/speech tokenization or quantization of audio signals into discrete symbols; thus, it does not meet the inclusion criteria and falls under exclusion criteria concerning lack of discrete audio tokenization analysis.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on the theoretical analysis of self-supervised learning methods in the context of Gaussian Mixture Models for dimensionality reduction, without any discussion or proposal related to discrete audio/speech tokenization or quantization of audio signals into discrete symbols; thus, it does not meet the inclusion criteria and falls under exclusion criteria concerning lack of discrete audio tokenization analysis.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "LSHBloom: Memory-efficient, Extreme-scale Document Deduplication",
    "abstract": "Contemporary large language model (LLM) training pipelines require the assembly of internet-scale databases full of text data from a variety of sources (e.g., web, academic, and publishers). Preprocessing these datasets via deduplication -- detecting and eliminating additional instances of the same content -- is a major focus for assembling and curating training datasets for LLMs. Unrestrained, duplicates in the training dataset increase training costs and lead to undesirable properties such as memorization in trained models or cheating on evaluation. Unfortunately, contemporary approaches to document-level deduplication are either unreliable at accurately identifying duplicate documents or extremely expensive in terms of both runtime and memory. We propose LSHBloom, an extension to MinhashLSH, which replaces the expensive LSHIndex with lightweight Bloom filters. LSHBloom demonstrates the same state-of-the-art deduplication performance as MinhashLSH, with only a marginal increase in false positives (near zero in our experiments), while boasting competitive runtime (12$\\times$ faster than MinhashLSH on peS2o) and, crucially, using 18$\\times$ less disk space than MinhashLSH (as measured on peS2o). Based on extrapolation, we show that this advantage in space and runtime remains even at the extreme scale of several billion documents. LSHBloom allows practitioners to access the deduplication quality of MinHashLSH at scales that are normally only tractable for less sophisticated, heuristic solutions. As a result, LSHBloom promises to enable scaling high-quality document deduplication to internet-scale text datasets.",
    "metadata": {
      "arxiv_id": "2411.04257",
      "title": "LSHBloom: Memory-efficient, Extreme-scale Document Deduplication",
      "summary": "Contemporary large language model (LLM) training pipelines require the assembly of internet-scale databases full of text data from a variety of sources (e.g., web, academic, and publishers). Preprocessing these datasets via deduplication -- detecting and eliminating additional instances of the same content -- is a major focus for assembling and curating training datasets for LLMs. Unrestrained, duplicates in the training dataset increase training costs and lead to undesirable properties such as memorization in trained models or cheating on evaluation. Unfortunately, contemporary approaches to document-level deduplication are either unreliable at accurately identifying duplicate documents or extremely expensive in terms of both runtime and memory. We propose LSHBloom, an extension to MinhashLSH, which replaces the expensive LSHIndex with lightweight Bloom filters. LSHBloom demonstrates the same state-of-the-art deduplication performance as MinhashLSH, with only a marginal increase in false positives (near zero in our experiments), while boasting competitive runtime (12$\\times$ faster than MinhashLSH on peS2o) and, crucially, using 18$\\times$ less disk space than MinhashLSH (as measured on peS2o). Based on extrapolation, we show that this advantage in space and runtime remains even at the extreme scale of several billion documents. LSHBloom allows practitioners to access the deduplication quality of MinHashLSH at scales that are normally only tractable for less sophisticated, heuristic solutions. As a result, LSHBloom promises to enable scaling high-quality document deduplication to internet-scale text datasets.",
      "authors": [
        "Arham Khan",
        "Robert Underwood",
        "Carlo Siebenschuh",
        "Yadu Babuji",
        "Aswathy Ajith",
        "Kyle Hippe",
        "Ozan Gokdemir",
        "Alexander Brace",
        "Kyle Chard",
        "Ian Foster"
      ],
      "published": "2024-11-06T21:00:45Z",
      "updated": "2025-12-02T12:52:27Z",
      "categories": [
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2411.04257v3",
      "landing_url": "https://arxiv.org/abs/2411.04257v3",
      "doi": "https://doi.org/10.48550/arXiv.2411.04257"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The article focuses on document deduplication using LSHBloom and Bloom filters for LLM training and does not involve discrete audio/speech tokenization or analysis of discrete audio representations.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The article focuses on document deduplication using LSHBloom and Bloom filters for LLM training and does not involve discrete audio/speech tokenization or analysis of discrete audio representations.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on document deduplication for large text datasets using a novel hashing technique, with no mention or analysis of discrete audio/speech tokenization or related audio signal processing, thus it does not meet the inclusion criteria centered around discrete audio tokenization.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on document deduplication for large text datasets using a novel hashing technique, with no mention or analysis of discrete audio/speech tokenization or related audio signal processing, thus it does not meet the inclusion criteria centered around discrete audio tokenization.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Tomato, Tomahto, Tomate: Do Multilingual Language Models Understand Based on Subword-Level Semantic Concepts?",
    "abstract": "Human understanding of text depends on general semantic concepts of words rather than their superficial forms. To what extent does our human intuition transfer to language models? In this work, we study the degree to which current multilingual language models (mLMs) understand based on subword-level semantic concepts. To this end, we form \"semantic tokens\" by merging the semantically similar subwords and their embeddings, and evaluate the updated mLMs on five heterogeneous multilingual downstream tasks. Results show that the general shared semantics could get the models a long way in making the predictions on mLMs with different tokenizers and model sizes. Inspections of the grouped subwords show that they exhibit a wide range of semantic similarities, including synonyms and translations across many languages and scripts. Lastly, we find that the zero-shot results with semantic tokens are on par with or even better than the original models on certain classification tasks, suggesting that the shared subword-level semantics may serve as the anchors for cross-lingual transfer.",
    "metadata": {
      "arxiv_id": "2411.04530",
      "title": "Tomato, Tomahto, Tomate: Do Multilingual Language Models Understand Based on Subword-Level Semantic Concepts?",
      "summary": "Human understanding of text depends on general semantic concepts of words rather than their superficial forms. To what extent does our human intuition transfer to language models? In this work, we study the degree to which current multilingual language models (mLMs) understand based on subword-level semantic concepts. To this end, we form \"semantic tokens\" by merging the semantically similar subwords and their embeddings, and evaluate the updated mLMs on five heterogeneous multilingual downstream tasks. Results show that the general shared semantics could get the models a long way in making the predictions on mLMs with different tokenizers and model sizes. Inspections of the grouped subwords show that they exhibit a wide range of semantic similarities, including synonyms and translations across many languages and scripts. Lastly, we find that the zero-shot results with semantic tokens are on par with or even better than the original models on certain classification tasks, suggesting that the shared subword-level semantics may serve as the anchors for cross-lingual transfer.",
      "authors": [
        "Crystina Zhang",
        "Jing Lu",
        "Vinh Q. Tran",
        "Tal Schuster",
        "Donald Metzler",
        "Jimmy Lin"
      ],
      "published": "2024-11-07T08:38:32Z",
      "updated": "2025-11-19T00:30:00Z",
      "categories": [
        "cs.CL"
      ],
      "pdf_url": "https://arxiv.org/pdf/2411.04530v2",
      "landing_url": "https://arxiv.org/abs/2411.04530v2",
      "doi": "https://doi.org/10.48550/arXiv.2411.04530"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This item analyzes subword-level semantics in multilingual language models for text tasks and does not involve discrete audio tokenization or speech token representations, so it should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "This item analyzes subword-level semantics in multilingual language models for text tasks and does not involve discrete audio tokenization or speech token representations, so it should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on multilingual language models analyzing subword-level semantic concepts in text, without involving audio or speech tokenization; it does not propose or evaluate discrete audio/speech tokenization methods as required by the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on multilingual language models analyzing subword-level semantic concepts in text, without involving audio or speech tokenization; it does not propose or evaluate discrete audio/speech tokenization methods as required by the inclusion criteria.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Understanding the Role of Equivariance in Self-supervised Learning",
    "abstract": "Contrastive learning has been a leading paradigm for self-supervised learning, but it is widely observed that it comes at the price of sacrificing useful features (\\eg colors) by being invariant to data augmentations. Given this limitation, there has been a surge of interest in equivariant self-supervised learning (E-SSL) that learns features to be augmentation-aware. However, even for the simplest rotation prediction method, there is a lack of rigorous understanding of why, when, and how E-SSL learns useful features for downstream tasks. To bridge this gap between practice and theory, we establish an information-theoretic perspective to understand the generalization ability of E-SSL. In particular, we identify a critical explaining-away effect in E-SSL that creates a synergy between the equivariant and classification tasks. This synergy effect encourages models to extract class-relevant features to improve its equivariant prediction, which, in turn, benefits downstream tasks requiring semantic features. Based on this perspective, we theoretically analyze the influence of data transformations and reveal several principles for practical designs of E-SSL. Our theory not only aligns well with existing E-SSL methods but also sheds light on new directions by exploring the benefits of model equivariance. We believe that a theoretically grounded understanding on the role of equivariance would inspire more principled and advanced designs in this field. Code is available at https://github.com/kaotty/Understanding-ESSL.",
    "metadata": {
      "arxiv_id": "2411.06508",
      "title": "Understanding the Role of Equivariance in Self-supervised Learning",
      "summary": "Contrastive learning has been a leading paradigm for self-supervised learning, but it is widely observed that it comes at the price of sacrificing useful features (\\eg colors) by being invariant to data augmentations. Given this limitation, there has been a surge of interest in equivariant self-supervised learning (E-SSL) that learns features to be augmentation-aware. However, even for the simplest rotation prediction method, there is a lack of rigorous understanding of why, when, and how E-SSL learns useful features for downstream tasks. To bridge this gap between practice and theory, we establish an information-theoretic perspective to understand the generalization ability of E-SSL. In particular, we identify a critical explaining-away effect in E-SSL that creates a synergy between the equivariant and classification tasks. This synergy effect encourages models to extract class-relevant features to improve its equivariant prediction, which, in turn, benefits downstream tasks requiring semantic features. Based on this perspective, we theoretically analyze the influence of data transformations and reveal several principles for practical designs of E-SSL. Our theory not only aligns well with existing E-SSL methods but also sheds light on new directions by exploring the benefits of model equivariance. We believe that a theoretically grounded understanding on the role of equivariance would inspire more principled and advanced designs in this field. Code is available at https://github.com/kaotty/Understanding-ESSL.",
      "authors": [
        "Yifei Wang",
        "Kaiwen Hu",
        "Sharut Gupta",
        "Ziyu Ye",
        "Yisen Wang",
        "Stefanie Jegelka"
      ],
      "published": "2024-11-10T16:09:47Z",
      "updated": "2024-11-10T16:09:47Z",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CV",
        "cs.IT",
        "stat.ML"
      ],
      "pdf_url": "https://arxiv.org/pdf/2411.06508v1",
      "landing_url": "https://arxiv.org/abs/2411.06508v1",
      "doi": "https://doi.org/10.48550/arXiv.2411.06508"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The title/abstract address equivariant self-supervised learning theory and do not involve discrete audio/speech tokenization or discrete token representations, so they do not meet the audio-tokenization inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The title/abstract address equivariant self-supervised learning theory and do not involve discrete audio/speech tokenization or discrete token representations, so they do not meet the audio-tokenization inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on equivariant self-supervised learning in general machine learning contexts without mention of audio or speech tokenization, discrete token representations, or neural quantization for audio; thus, it does not meet the inclusion criteria regarding discrete audio/speech tokenization as a core object of study.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on equivariant self-supervised learning in general machine learning contexts without mention of audio or speech tokenization, discrete token representations, or neural quantization for audio; thus, it does not meet the inclusion criteria regarding discrete audio/speech tokenization as a core object of study.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Mamba-based Decoder-Only Approach with Bidirectional Speech Modeling for Speech Recognition",
    "abstract": "Selective state space models (SSMs) represented by Mamba have demonstrated their computational efficiency and promising outcomes in various tasks, including automatic speech recognition (ASR). Mamba has been applied to ASR task with the attention-based encoder-decoder framework, where the cross-attention mechanism between encoder and decoder remains. This paper explores the capability of Mamba as the decoder-only architecture in ASR task. Our MAmba-based DEcoder-ONly approach (MADEON) consists of a single decoder that takes speech tokens as a condition and predicts text tokens in an autoregressive manner. To enhance MADEON, we further propose speech prefixing that performs bidirectional processing on speech tokens, which enriches the contextual information in the hidden states. Our experiments show that MADEON significantly outperforms a non-selective SSM. The combination of speech prefixing and the recently proposed Mamba-2 yields comparable performance to Transformer-based models on large datasets.",
    "metadata": {
      "arxiv_id": "2411.06968",
      "title": "Mamba-based Decoder-Only Approach with Bidirectional Speech Modeling for Speech Recognition",
      "summary": "Selective state space models (SSMs) represented by Mamba have demonstrated their computational efficiency and promising outcomes in various tasks, including automatic speech recognition (ASR). Mamba has been applied to ASR task with the attention-based encoder-decoder framework, where the cross-attention mechanism between encoder and decoder remains. This paper explores the capability of Mamba as the decoder-only architecture in ASR task. Our MAmba-based DEcoder-ONly approach (MADEON) consists of a single decoder that takes speech tokens as a condition and predicts text tokens in an autoregressive manner. To enhance MADEON, we further propose speech prefixing that performs bidirectional processing on speech tokens, which enriches the contextual information in the hidden states. Our experiments show that MADEON significantly outperforms a non-selective SSM. The combination of speech prefixing and the recently proposed Mamba-2 yields comparable performance to Transformer-based models on large datasets.",
      "authors": [
        "Yoshiki Masuyama",
        "Koichi Miyazaki",
        "Masato Murata"
      ],
      "published": "2024-11-11T13:17:24Z",
      "updated": "2024-11-11T13:17:24Z",
      "categories": [
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2411.06968v1",
      "landing_url": "https://arxiv.org/abs/2411.06968v1",
      "doi": "https://doi.org/10.48550/arXiv.2411.06968"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The abstract describes a decoder-only ASR model that uses speech tokens and speech prefixing, implying discrete speech tokens are involved, but it does not clearly state an explicit analysis or evaluation of discrete speech tokenization itself as a core object, so inclusion is uncertain.",
      "evaluation": 3
    },
    "round-A_JuniorNano_reasoning": "The abstract describes a decoder-only ASR model that uses speech tokens and speech prefixing, implying discrete speech tokens are involved, but it does not clearly state an explicit analysis or evaluation of discrete speech tokenization itself as a core object, so inclusion is uncertain.",
    "round-A_JuniorNano_evaluation": 3,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on automatic speech recognition using a Mamba-based decoder-only approach with bidirectional speech modeling, but it does not explicitly propose, analyze, or evaluate discrete audio/speech tokenization where continuous audio signals are converted into discrete tokens; instead, it seems to work on speech tokens and text tokens without detailed discrete tokenization design or analysis as a core object of study, thus it does not meet the inclusion criteria and partially aligns with exclusion criteria concerning lack of discrete tokenization analysis.",
      "evaluation": 2
    },
    "round-A_JuniorMini_reasoning": "The study focuses on automatic speech recognition using a Mamba-based decoder-only approach with bidirectional speech modeling, but it does not explicitly propose, analyze, or evaluate discrete audio/speech tokenization where continuous audio signals are converted into discrete tokens; instead, it seems to work on speech tokens and text tokens without detailed discrete tokenization design or analysis as a core object of study, thus it does not meet the inclusion criteria and partially aligns with exclusion criteria concerning lack of discrete tokenization analysis.",
    "round-A_JuniorMini_evaluation": 2,
    "round-B_SeniorLead_output": {
      "reasoning": "Both junior reviewers leaned toward exclusion (scores 3 and 2) and noted the abstract does not explicitly treat discrete audio tokenization as a core contribution. The paper describes a Mamba-based decoder-only ASR model that conditions on “speech tokens,” but the abstract gives no details of any discrete quantization/tokenization design, no analysis of converting continuous audio into a finite vocabulary, nor any evaluation for audio reconstruction/generation using discrete tokens—only ASR modeling—so it does not meet the inclusion criterion.  Therefore I rate this as better to exclude.",
      "evaluation": 2
    },
    "round-B_SeniorLead_reasoning": "Both junior reviewers leaned toward exclusion (scores 3 and 2) and noted the abstract does not explicitly treat discrete audio tokenization as a core contribution. The paper describes a Mamba-based decoder-only ASR model that conditions on “speech tokens,” but the abstract gives no details of any discrete quantization/tokenization design, no analysis of converting continuous audio into a finite vocabulary, nor any evaluation for audio reconstruction/generation using discrete tokens—only ASR modeling—so it does not meet the inclusion criterion.  Therefore I rate this as better to exclude.",
    "round-B_SeniorLead_evaluation": 2,
    "final_verdict": "exclude (senior:2)",
    "review_skipped": false,
    "discard_reason": "exclude (senior:2)"
  },
  {
    "title": "A Comparative Study of Discrete Speech Tokens for Semantic-Related Tasks with Large Language Models",
    "abstract": "With the rise of Speech Large Language Models (Speech LLMs), there has been growing interest in discrete speech tokens for their ability to integrate with text-based tokens seamlessly. Compared to most studies that focus on continuous speech features, although discrete-token based LLMs have shown promising results on certain tasks, the performance gap between these two paradigms is rarely explored. In this paper, we present a fair and thorough comparison between discrete and continuous features across a variety of semantic-related tasks using a light-weight LLM (Qwen1.5-0.5B). Our findings reveal that continuous features generally outperform discrete tokens, particularly in tasks requiring fine-grained semantic understanding. Moreover, this study goes beyond surface-level comparison by identifying key factors behind the under-performance of discrete tokens, such as limited token granularity and inefficient information retention. To enhance the performance of discrete tokens, we explore potential aspects based on our analysis. We hope our results can offer new insights into the opportunities for advancing discrete speech tokens in Speech LLMs.",
    "metadata": {
      "arxiv_id": "2411.08742",
      "title": "A Comparative Study of Discrete Speech Tokens for Semantic-Related Tasks with Large Language Models",
      "summary": "With the rise of Speech Large Language Models (Speech LLMs), there has been growing interest in discrete speech tokens for their ability to integrate with text-based tokens seamlessly. Compared to most studies that focus on continuous speech features, although discrete-token based LLMs have shown promising results on certain tasks, the performance gap between these two paradigms is rarely explored. In this paper, we present a fair and thorough comparison between discrete and continuous features across a variety of semantic-related tasks using a light-weight LLM (Qwen1.5-0.5B). Our findings reveal that continuous features generally outperform discrete tokens, particularly in tasks requiring fine-grained semantic understanding. Moreover, this study goes beyond surface-level comparison by identifying key factors behind the under-performance of discrete tokens, such as limited token granularity and inefficient information retention. To enhance the performance of discrete tokens, we explore potential aspects based on our analysis. We hope our results can offer new insights into the opportunities for advancing discrete speech tokens in Speech LLMs.",
      "authors": [
        "Dingdong Wang",
        "Mingyu Cui",
        "Dongchao Yang",
        "Xueyuan Chen",
        "Helen Meng"
      ],
      "published": "2024-11-13T16:20:20Z",
      "updated": "2024-11-13T16:20:20Z",
      "categories": [
        "cs.CL",
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2411.08742v1",
      "landing_url": "https://arxiv.org/abs/2411.08742v1",
      "doi": "https://doi.org/10.48550/arXiv.2411.08742"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The abstract centers on discrete speech tokens as the main representation and analyzes their performance relative to continuous features in semantic tasks with an LLM, aligning with the inclusion criteria and not falling into the exclusion categories.",
      "evaluation": 4
    },
    "round-A_JuniorNano_reasoning": "The abstract centers on discrete speech tokens as the main representation and analyzes their performance relative to continuous features in semantic tasks with an LLM, aligning with the inclusion criteria and not falling into the exclusion categories.",
    "round-A_JuniorNano_evaluation": 4,
    "round-A_JuniorMini_output": {
      "reasoning": "The study explicitly focuses on discrete speech tokens and provides a thorough comparison and analysis between discrete and continuous speech feature representations within semantic-related tasks using language models, fulfilling the inclusion criteria while avoiding all exclusion criteria.",
      "evaluation": 5
    },
    "round-A_JuniorMini_reasoning": "The study explicitly focuses on discrete speech tokens and provides a thorough comparison and analysis between discrete and continuous speech feature representations within semantic-related tasks using language models, fulfilling the inclusion criteria while avoiding all exclusion criteria.",
    "round-A_JuniorMini_evaluation": 5,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "include (junior:4)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Pitch-and-Spectrum-Aware Singing Quality Assessment with Bias Correction and Model Fusion",
    "abstract": "We participated in track 2 of the VoiceMOS Challenge 2024, which aimed to predict the mean opinion score (MOS) of singing samples. Our submission secured the first place among all participating teams, excluding the official baseline. In this paper, we further improve our submission and propose a novel Pitch-and-Spectrum-aware Singing Quality Assessment (PS-SQA) method. The PS-SQA is designed based on the self-supervised-learning (SSL) MOS predictor, incorporating singing pitch and spectral information, which are extracted using pitch histogram and non-quantized neural codec, respectively. Additionally, the PS-SQA introduces a bias correction strategy to address prediction biases caused by low-resource training samples, and employs model fusion technology to further enhance prediction accuracy. Experimental results confirm that our proposed PS-SQA significantly outperforms all competing systems across all system-level metrics, confirming its strong sing quality assessment capabilities.",
    "metadata": {
      "arxiv_id": "2411.11123",
      "title": "Pitch-and-Spectrum-Aware Singing Quality Assessment with Bias Correction and Model Fusion",
      "summary": "We participated in track 2 of the VoiceMOS Challenge 2024, which aimed to predict the mean opinion score (MOS) of singing samples. Our submission secured the first place among all participating teams, excluding the official baseline. In this paper, we further improve our submission and propose a novel Pitch-and-Spectrum-aware Singing Quality Assessment (PS-SQA) method. The PS-SQA is designed based on the self-supervised-learning (SSL) MOS predictor, incorporating singing pitch and spectral information, which are extracted using pitch histogram and non-quantized neural codec, respectively. Additionally, the PS-SQA introduces a bias correction strategy to address prediction biases caused by low-resource training samples, and employs model fusion technology to further enhance prediction accuracy. Experimental results confirm that our proposed PS-SQA significantly outperforms all competing systems across all system-level metrics, confirming its strong sing quality assessment capabilities.",
      "authors": [
        "Yu-Fei Shi",
        "Yang Ai",
        "Ye-Xin Lu",
        "Hui-Peng Du",
        "Zhen-Hua Ling"
      ],
      "published": "2024-11-17T16:53:39Z",
      "updated": "2024-12-23T12:42:03Z",
      "categories": [
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2411.11123v3",
      "landing_url": "https://arxiv.org/abs/2411.11123v3",
      "doi": "https://doi.org/10.48550/arXiv.2411.11123"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "No discrete audio tokenization is proposed or analyzed; the work uses continuous features (pitch histogram, non-quantized neural codec) and focuses on MOS prediction rather than discrete token representations.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "No discrete audio tokenization is proposed or analyzed; the work uses continuous features (pitch histogram, non-quantized neural codec) and focuses on MOS prediction rather than discrete token representations.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on a singing quality assessment method using self-supervised learning and incorporates pitch and spectral features; however, it does not mention any discrete audio/speech tokenization or conversion of continuous audio signals into discrete symbol sequences, which is a core inclusion criterion. The abstract describes the use of continuous features like pitch histograms and neural codec spectral information but does not analyze or design discrete token representations, and hence does not meet the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on a singing quality assessment method using self-supervised learning and incorporates pitch and spectral features; however, it does not mention any discrete audio/speech tokenization or conversion of continuous audio signals into discrete symbol sequences, which is a core inclusion criterion. The abstract describes the use of continuous features like pitch histograms and neural codec spectral information but does not analyze or design discrete token representations, and hence does not meet the inclusion criteria.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Anomalous Spectroscopical Effects in an Antiferromagnetic Semiconductor",
    "abstract": "Following the recent observation of anomalous Hall effect in antiferromagnetic hexagonal MnTe thin films, related phenomena at finite frequencies have come into focus. Magnetic circular dichroism (MCD) is the key material property here. In the x-ray range, the XMCD has already been demonstrated and used to visualise domains via photoemission electron microscopy (PEEM). Here we report on MCD in optical range and discuss its microscopic mechanism.",
    "metadata": {
      "arxiv_id": "2411.11673",
      "title": "Anomalous Spectroscopical Effects in an Antiferromagnetic Semiconductor",
      "summary": "Following the recent observation of anomalous Hall effect in antiferromagnetic hexagonal MnTe thin films, related phenomena at finite frequencies have come into focus. Magnetic circular dichroism (MCD) is the key material property here. In the x-ray range, the XMCD has already been demonstrated and used to visualise domains via photoemission electron microscopy (PEEM). Here we report on MCD in optical range and discuss its microscopic mechanism.",
      "authors": [
        "Michal Hubert",
        "Tomáš Maleček",
        "Kyo-Hoon Ahn",
        "Martin Míšek",
        "Jakub Železný",
        "František Máca",
        "Gunther Springholz",
        "Martin Veis",
        "Karel Výborný"
      ],
      "published": "2024-11-18T15:51:50Z",
      "updated": "2024-11-19T20:18:00Z",
      "categories": [
        "cond-mat.mtrl-sci"
      ],
      "pdf_url": "https://arxiv.org/pdf/2411.11673v2",
      "landing_url": "https://arxiv.org/abs/2411.11673v2",
      "doi": "https://doi.org/10.1002/pssb.202400541"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The article discusses magneto-optical spectroscopy in an antiferromagnetic semiconductor and contains no discrete audio/speech tokenization or related tokenization analysis, so it does not meet the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The article discusses magneto-optical spectroscopy in an antiferromagnetic semiconductor and contains no discrete audio/speech tokenization or related tokenization analysis, so it does not meet the inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on magnetic circular dichroism and spectroscopical effects in antiferromagnetic semiconductors and does not address discrete audio/speech tokenization or related neural encoding for audio tasks, thus failing the inclusion criteria and fitting the exclusion conditions.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on magnetic circular dichroism and spectroscopical effects in antiferromagnetic semiconductors and does not address discrete audio/speech tokenization or related neural encoding for audio tasks, thus failing the inclusion criteria and fitting the exclusion conditions.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "FAST-Splat: Fast, Ambiguity-Free Semantics Transfer in Gaussian Splatting",
    "abstract": "We present FAST-Splat for fast, ambiguity-free semantic Gaussian Splatting, which seeks to address the main limitations of existing semantic Gaussian Splatting methods, namely: slow training and rendering speeds; high memory usage; and ambiguous semantic object localization. We take a bottom-up approach in deriving FAST-Splat, dismantling the limitations of closed-set semantic distillation to enable open-set (open-vocabulary) semantic distillation. Ultimately, this key approach enables FAST-Splat to provide precise semantic object localization results, even when prompted with ambiguous user-provided natural-language queries. Further, by exploiting the explicit form of the Gaussian Splatting scene representation to the fullest extent, FAST-Splat retains the remarkable training and rendering speeds of Gaussian Splatting. Precisely, while existing semantic Gaussian Splatting methods distill semantics into a separate neural field or utilize neural models for dimensionality reduction, FAST-Splat directly augments each Gaussian with specific semantic codes, preserving the training, rendering, and memory-usage advantages of Gaussian Splatting over neural field methods. These Gaussian-specific semantic codes, together with a hash-table, enable semantic similarity to be measured with open-vocabulary user prompts and further enable FAST-Splat to respond with unambiguous semantic object labels and $3$D masks, unlike prior methods. In experiments, we demonstrate that FAST-Splat is 6x to 8x faster to train, achieves between 18x to 51x faster rendering speeds, and requires about 6x smaller GPU memory, compared to the best-competing semantic Gaussian Splatting methods. Further, FAST-Splat achieves relatively similar or better semantic segmentation performance compared to existing methods. After the review period, we will provide links to the project website and the codebase.",
    "metadata": {
      "arxiv_id": "2411.13753",
      "title": "FAST-Splat: Fast, Ambiguity-Free Semantics Transfer in Gaussian Splatting",
      "summary": "We present FAST-Splat for fast, ambiguity-free semantic Gaussian Splatting, which seeks to address the main limitations of existing semantic Gaussian Splatting methods, namely: slow training and rendering speeds; high memory usage; and ambiguous semantic object localization. We take a bottom-up approach in deriving FAST-Splat, dismantling the limitations of closed-set semantic distillation to enable open-set (open-vocabulary) semantic distillation. Ultimately, this key approach enables FAST-Splat to provide precise semantic object localization results, even when prompted with ambiguous user-provided natural-language queries. Further, by exploiting the explicit form of the Gaussian Splatting scene representation to the fullest extent, FAST-Splat retains the remarkable training and rendering speeds of Gaussian Splatting. Precisely, while existing semantic Gaussian Splatting methods distill semantics into a separate neural field or utilize neural models for dimensionality reduction, FAST-Splat directly augments each Gaussian with specific semantic codes, preserving the training, rendering, and memory-usage advantages of Gaussian Splatting over neural field methods. These Gaussian-specific semantic codes, together with a hash-table, enable semantic similarity to be measured with open-vocabulary user prompts and further enable FAST-Splat to respond with unambiguous semantic object labels and $3$D masks, unlike prior methods. In experiments, we demonstrate that FAST-Splat is 6x to 8x faster to train, achieves between 18x to 51x faster rendering speeds, and requires about 6x smaller GPU memory, compared to the best-competing semantic Gaussian Splatting methods. Further, FAST-Splat achieves relatively similar or better semantic segmentation performance compared to existing methods. After the review period, we will provide links to the project website and the codebase.",
      "authors": [
        "Ola Shorinwa",
        "Jiankai Sun",
        "Mac Schwager"
      ],
      "published": "2024-11-20T23:36:46Z",
      "updated": "2025-03-12T02:17:03Z",
      "categories": [
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2411.13753v2",
      "landing_url": "https://arxiv.org/abs/2411.13753v2",
      "doi": "https://doi.org/10.48550/arXiv.2411.13753"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This item does not address discrete audio/speech tokenization; it focuses on Gaussian Splatting for 3D semantics rather than tokenization of audio, so it should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "This item does not address discrete audio/speech tokenization; it focuses on Gaussian Splatting for 3D semantics rather than tokenization of audio, so it should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study FAST-Splat focuses on semantic Gaussian Splatting for visual 3D scene representation and semantic segmentation; it does not propose, analyze, or evaluate any discrete audio or speech tokenization methods. There is no mention of audio signal processing, discrete audio tokens, or related analysis, which means it does not meet the inclusion criteria and falls under exclusion criteria involving lack of discrete audio tokenization focus.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study FAST-Splat focuses on semantic Gaussian Splatting for visual 3D scene representation and semantic segmentation; it does not propose, analyze, or evaluate any discrete audio or speech tokenization methods. There is no mention of audio signal processing, discrete audio tokens, or related analysis, which means it does not meet the inclusion criteria and falls under exclusion criteria involving lack of discrete audio tokenization focus.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "BEST-STD: Bidirectional Mamba-Enhanced Speech Tokenization for Spoken Term Detection",
    "abstract": "Spoken term detection (STD) is often hindered by reliance on frame-level features and the computationally intensive DTW-based template matching, limiting its practicality. To address these challenges, we propose a novel approach that encodes speech into discrete, speaker-agnostic semantic tokens. This facilitates fast retrieval using text-based search algorithms and effectively handles out-of-vocabulary terms. Our approach focuses on generating consistent token sequences across varying utterances of the same term. We also propose a bidirectional state space modeling within the Mamba encoder, trained in a self-supervised learning framework, to learn contextual frame-level features that are further encoded into discrete tokens. Our analysis shows that our speech tokens exhibit greater speaker invariance than those from existing tokenizers, making them more suitable for STD tasks. Empirical evaluation on LibriSpeech and TIMIT databases indicates that our method outperforms existing STD baselines while being more efficient.",
    "metadata": {
      "arxiv_id": "2411.14100",
      "title": "BEST-STD: Bidirectional Mamba-Enhanced Speech Tokenization for Spoken Term Detection",
      "summary": "Spoken term detection (STD) is often hindered by reliance on frame-level features and the computationally intensive DTW-based template matching, limiting its practicality. To address these challenges, we propose a novel approach that encodes speech into discrete, speaker-agnostic semantic tokens. This facilitates fast retrieval using text-based search algorithms and effectively handles out-of-vocabulary terms. Our approach focuses on generating consistent token sequences across varying utterances of the same term. We also propose a bidirectional state space modeling within the Mamba encoder, trained in a self-supervised learning framework, to learn contextual frame-level features that are further encoded into discrete tokens. Our analysis shows that our speech tokens exhibit greater speaker invariance than those from existing tokenizers, making them more suitable for STD tasks. Empirical evaluation on LibriSpeech and TIMIT databases indicates that our method outperforms existing STD baselines while being more efficient.",
      "authors": [
        "Anup Singh",
        "Kris Demuynck",
        "Vipul Arora"
      ],
      "published": "2024-11-21T13:05:18Z",
      "updated": "2024-12-21T19:15:27Z",
      "categories": [
        "eess.AS",
        "cs.CL",
        "cs.IR"
      ],
      "pdf_url": "https://arxiv.org/pdf/2411.14100v2",
      "landing_url": "https://arxiv.org/abs/2411.14100v2",
      "doi": "https://doi.org/10.48550/arXiv.2411.14100"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The work explicitly proposes and evaluates discrete speech tokenization into a finite vocabulary for spoken term detection, with empirical validation, and does not focus solely on continuous features, non-speech codecs, or tokenization of text/images.",
      "evaluation": 5
    },
    "round-A_JuniorNano_reasoning": "The work explicitly proposes and evaluates discrete speech tokenization into a finite vocabulary for spoken term detection, with empirical validation, and does not focus solely on continuous features, non-speech codecs, or tokenization of text/images.",
    "round-A_JuniorNano_evaluation": 5,
    "round-A_JuniorMini_output": {
      "reasoning": "The study explicitly proposes and evaluates a novel discrete speech tokenization method (Bidirectional Mamba-Enhanced Speech Tokenization) that converts continuous speech signals into discrete semantic tokens, with analysis of speaker invariance and application to spoken term detection, thus fitting all inclusion criteria and avoiding any exclusion criteria.",
      "evaluation": 5
    },
    "round-A_JuniorMini_reasoning": "The study explicitly proposes and evaluates a novel discrete speech tokenization method (Bidirectional Mamba-Enhanced Speech Tokenization) that converts continuous speech signals into discrete semantic tokens, with analysis of speaker invariance and application to spoken term detection, thus fitting all inclusion criteria and avoiding any exclusion criteria.",
    "round-A_JuniorMini_evaluation": 5,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "include (junior:5)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "K-means Derived Unsupervised Feature Selection using Improved ADMM",
    "abstract": "Feature selection is important for high-dimensional data analysis and is non-trivial in unsupervised learning problems such as dimensionality reduction and clustering. The goal of unsupervised feature selection is finding a subset of features such that the data points from different clusters are well separated. This paper presents a novel method called K-means Derived Unsupervised Feature Selection (K-means UFS). Unlike most existing spectral analysis based unsupervised feature selection methods, we select features using the objective of K-means. We develop an alternating direction method of multipliers (ADMM) to solve the NP-hard optimization problem of our K-means UFS model. Extensive experiments on real datasets show that our K-means UFS is more effective than the baselines in selecting features for clustering.",
    "metadata": {
      "arxiv_id": "2411.15197",
      "title": "K-means Derived Unsupervised Feature Selection using Improved ADMM",
      "summary": "Feature selection is important for high-dimensional data analysis and is non-trivial in unsupervised learning problems such as dimensionality reduction and clustering. The goal of unsupervised feature selection is finding a subset of features such that the data points from different clusters are well separated. This paper presents a novel method called K-means Derived Unsupervised Feature Selection (K-means UFS). Unlike most existing spectral analysis based unsupervised feature selection methods, we select features using the objective of K-means. We develop an alternating direction method of multipliers (ADMM) to solve the NP-hard optimization problem of our K-means UFS model. Extensive experiments on real datasets show that our K-means UFS is more effective than the baselines in selecting features for clustering.",
      "authors": [
        "Ziheng Sun",
        "Chris Ding",
        "Jicong Fan"
      ],
      "published": "2024-11-19T18:05:02Z",
      "updated": "2024-11-19T18:05:02Z",
      "categories": [
        "cs.LG",
        "cs.AI",
        "stat.ML"
      ],
      "pdf_url": "https://arxiv.org/pdf/2411.15197v1",
      "landing_url": "https://arxiv.org/abs/2411.15197v1",
      "doi": "https://doi.org/10.48550/arXiv.2411.15197"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This item does not address discrete audio/speech tokenization; it focuses on unsupervised feature selection for general data rather than tokenizing audio into a discrete vocabulary, so it should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "This item does not address discrete audio/speech tokenization; it focuses on unsupervised feature selection for general data rather than tokenizing audio into a discrete vocabulary, so it should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on unsupervised feature selection for clustering using K-means and ADMM optimization, without any mention of audio/speech tokenization or discrete token sequences for audio reconstruction or generation; thus, it does not meet the inclusion criteria related to discrete audio tokenization.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on unsupervised feature selection for clustering using K-means and ADMM optimization, without any mention of audio/speech tokenization or discrete token sequences for audio reconstruction or generation; thus, it does not meet the inclusion criteria related to discrete audio tokenization.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Learning Optimal Lattice Vector Quantizers for End-to-end Neural Image Compression",
    "abstract": "It is customary to deploy uniform scalar quantization in the end-to-end optimized Neural image compression methods, instead of more powerful vector quantization, due to the high complexity of the latter. Lattice vector quantization (LVQ), on the other hand, presents a compelling alternative, which can exploit inter-feature dependencies more effectively while keeping computational efficiency almost the same as scalar quantization. However, traditional LVQ structures are designed/optimized for uniform source distributions, hence nonadaptive and suboptimal for real source distributions of latent code space for Neural image compression tasks. In this paper, we propose a novel learning method to overcome this weakness by designing the rate-distortion optimal lattice vector quantization (OLVQ) codebooks with respect to the sample statistics of the latent features to be compressed. By being able to better fit the LVQ structures to any given latent sample distribution, the proposed OLVQ method improves the rate-distortion performances of the existing quantization schemes in neural image compression significantly, while retaining the amenability of uniform scalar quantization.",
    "metadata": {
      "arxiv_id": "2411.16119",
      "title": "Learning Optimal Lattice Vector Quantizers for End-to-end Neural Image Compression",
      "summary": "It is customary to deploy uniform scalar quantization in the end-to-end optimized Neural image compression methods, instead of more powerful vector quantization, due to the high complexity of the latter. Lattice vector quantization (LVQ), on the other hand, presents a compelling alternative, which can exploit inter-feature dependencies more effectively while keeping computational efficiency almost the same as scalar quantization. However, traditional LVQ structures are designed/optimized for uniform source distributions, hence nonadaptive and suboptimal for real source distributions of latent code space for Neural image compression tasks. In this paper, we propose a novel learning method to overcome this weakness by designing the rate-distortion optimal lattice vector quantization (OLVQ) codebooks with respect to the sample statistics of the latent features to be compressed. By being able to better fit the LVQ structures to any given latent sample distribution, the proposed OLVQ method improves the rate-distortion performances of the existing quantization schemes in neural image compression significantly, while retaining the amenability of uniform scalar quantization.",
      "authors": [
        "Xi Zhang",
        "Xiaolin Wu"
      ],
      "published": "2024-11-25T06:05:08Z",
      "updated": "2024-11-25T06:05:08Z",
      "categories": [
        "eess.IV",
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2411.16119v1",
      "landing_url": "https://arxiv.org/abs/2411.16119v1",
      "doi": "https://doi.org/10.48550/arXiv.2411.16119"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item focuses on image compression and lattice vector quantizers, not on discrete audio/speech tokenization, so it does not meet the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item focuses on image compression and lattice vector quantizers, not on discrete audio/speech tokenization, so it does not meet the inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on learning optimal lattice vector quantizers for neural image compression, addressing quantization methods in image data and latent features, with no mention of discrete audio or speech tokenization; hence, it does not meet the inclusion criteria requiring discrete audio/speech tokenization as the core study object.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on learning optimal lattice vector quantizers for neural image compression, addressing quantization methods in image data and latent features, with no mention of discrete audio or speech tokenization; hence, it does not meet the inclusion criteria requiring discrete audio/speech tokenization as the core study object.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "VideoOrion: Tokenizing Object Dynamics in Videos",
    "abstract": "We present VideoOrion, a Video Large Language Model (Video-LLM) that explicitly captures the key semantic information in videos - the spatial-temporal dynamics of objects throughout the videos. VideoOrion employs expert vision models to extract object dynamics through a detect-segment-track pipeline, encoding them into a set of object tokens by aggregating spatial-temporal object features. Our method addresses the persistent challenge in Video-LLMs of efficiently compressing high-dimensional video data into semantic tokens that are comprehensible to LLMs. Compared to prior methods which resort to downsampling the original video or aggregating visual tokens using resamplers, leading to information loss and entangled semantics, VideoOrion not only offers a more natural and efficient way to derive compact, disentangled semantic representations but also enables explicit object modeling of video content with minimal computational cost. Moreover, the introduced object tokens naturally allow VideoOrion to accomplish video-based referring tasks. Experimental results show that VideoOrion can learn to make good use of the object tokens, and achieves competitive results on both general video question answering and video-based referring benchmarks.",
    "metadata": {
      "arxiv_id": "2411.16156",
      "title": "VideoOrion: Tokenizing Object Dynamics in Videos",
      "summary": "We present VideoOrion, a Video Large Language Model (Video-LLM) that explicitly captures the key semantic information in videos - the spatial-temporal dynamics of objects throughout the videos. VideoOrion employs expert vision models to extract object dynamics through a detect-segment-track pipeline, encoding them into a set of object tokens by aggregating spatial-temporal object features. Our method addresses the persistent challenge in Video-LLMs of efficiently compressing high-dimensional video data into semantic tokens that are comprehensible to LLMs. Compared to prior methods which resort to downsampling the original video or aggregating visual tokens using resamplers, leading to information loss and entangled semantics, VideoOrion not only offers a more natural and efficient way to derive compact, disentangled semantic representations but also enables explicit object modeling of video content with minimal computational cost. Moreover, the introduced object tokens naturally allow VideoOrion to accomplish video-based referring tasks. Experimental results show that VideoOrion can learn to make good use of the object tokens, and achieves competitive results on both general video question answering and video-based referring benchmarks.",
      "authors": [
        "Yicheng Feng",
        "Yijiang Li",
        "Wanpeng Zhang",
        "Hao Luo",
        "Zihao Yue",
        "Sipeng Zheng",
        "Zongqing Lu"
      ],
      "published": "2024-11-25T07:32:02Z",
      "updated": "2025-03-18T08:15:28Z",
      "categories": [
        "cs.CV",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2411.16156v2",
      "landing_url": "https://arxiv.org/abs/2411.16156v2",
      "doi": "https://doi.org/10.48550/arXiv.2411.16156"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The paper focuses on tokenizing visual object dynamics in videos (video tokens), not discrete audio/speech tokenization as required by the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The paper focuses on tokenizing visual object dynamics in videos (video tokens), not discrete audio/speech tokenization as required by the inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on tokenizing object dynamics in videos using a vision-language model, without addressing audio or speech tokenization. The tokenization discussed is related to visual object features, not discrete audio tokens, and thus does not meet the inclusion criteria centered on audio/speech tokenization.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on tokenizing object dynamics in videos using a vision-language model, without addressing audio or speech tokenization. The tokenization discussed is related to visual object features, not discrete audio tokens, and thus does not meet the inclusion criteria centered on audio/speech tokenization.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Representation Collapsing Problems in Vector Quantization",
    "abstract": "Vector quantization is a technique in machine learning that discretizes continuous representations into a set of discrete vectors. It is widely employed in tokenizing data representations for large language models, diffusion models, and other generative models. Despite its prevalence, the characteristics and behaviors of vector quantization in generative models remain largely underexplored. In this study, we investigate representation collapse in vector quantization - a critical degradation where codebook tokens or latent embeddings lose their discriminative power by converging to a limited subset of values. This collapse fundamentally compromises the model's ability to capture diverse data patterns. By leveraging both synthetic and real datasets, we identify the severity of each type of collapses and triggering conditions. Our analysis reveals that restricted initialization and limited encoder capacity result in tokens collapse and embeddings collapse. Building on these findings, we propose potential solutions aimed at mitigating each collapse. To the best of our knowledge, this is the first comprehensive study examining representation collapsing problems in vector quantization.",
    "metadata": {
      "arxiv_id": "2411.16550",
      "title": "Representation Collapsing Problems in Vector Quantization",
      "summary": "Vector quantization is a technique in machine learning that discretizes continuous representations into a set of discrete vectors. It is widely employed in tokenizing data representations for large language models, diffusion models, and other generative models. Despite its prevalence, the characteristics and behaviors of vector quantization in generative models remain largely underexplored. In this study, we investigate representation collapse in vector quantization - a critical degradation where codebook tokens or latent embeddings lose their discriminative power by converging to a limited subset of values. This collapse fundamentally compromises the model's ability to capture diverse data patterns. By leveraging both synthetic and real datasets, we identify the severity of each type of collapses and triggering conditions. Our analysis reveals that restricted initialization and limited encoder capacity result in tokens collapse and embeddings collapse. Building on these findings, we propose potential solutions aimed at mitigating each collapse. To the best of our knowledge, this is the first comprehensive study examining representation collapsing problems in vector quantization.",
      "authors": [
        "Wenhao Zhao",
        "Qiran Zou",
        "Rushi Shah",
        "Dianbo Liu"
      ],
      "published": "2024-11-25T16:32:29Z",
      "updated": "2024-11-25T16:32:29Z",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2411.16550v1",
      "landing_url": "https://arxiv.org/abs/2411.16550v1",
      "doi": "https://doi.org/10.48550/arXiv.2411.16550"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The abstract addresses representation collapse in vector quantization in generative models but does not explicitly study discrete audio/speech tokenization as the core object, nor evaluate speech-relevant implications, so it does not meet the inclusion criteria.",
      "evaluation": 2
    },
    "round-A_JuniorNano_reasoning": "The abstract addresses representation collapse in vector quantization in generative models but does not explicitly study discrete audio/speech tokenization as the core object, nor evaluate speech-relevant implications, so it does not meet the inclusion criteria.",
    "round-A_JuniorNano_evaluation": 2,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on vector quantization and representation collapse in generative models without explicitly addressing discrete audio or speech tokenization, nor is there mention of continuous audio signal discretization into token sequences for speech tasks; hence it does not meet the core inclusion criteria related to audio/speech tokenization.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on vector quantization and representation collapse in generative models without explicitly addressing discrete audio or speech tokenization, nor is there mention of continuous audio signal discretization into token sequences for speech tasks; hence it does not meet the core inclusion criteria related to audio/speech tokenization.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:2)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:2)"
  },
  {
    "title": "Learning Robust Anymodal Segmentor with Unimodal and Cross-modal Distillation",
    "abstract": "Simultaneously using multimodal inputs from multiple sensors to train segmentors is intuitively advantageous but practically challenging. A key challenge is unimodal bias, where multimodal segmentors over rely on certain modalities, causing performance drops when others are missing, common in real world applications. To this end, we develop the first framework for learning robust segmentor that can handle any combinations of visual modalities. Specifically, we first introduce a parallel multimodal learning strategy for learning a strong teacher. The cross-modal and unimodal distillation is then achieved in the multi scale representation space by transferring the feature level knowledge from multimodal to anymodal segmentors, aiming at addressing the unimodal bias and avoiding over-reliance on specific modalities. Moreover, a prediction level modality agnostic semantic distillation is proposed to achieve semantic knowledge transferring for segmentation. Extensive experiments on both synthetic and real-world multi-sensor benchmarks demonstrate that our method achieves superior performance.",
    "metadata": {
      "arxiv_id": "2411.17141",
      "title": "Learning Robust Anymodal Segmentor with Unimodal and Cross-modal Distillation",
      "summary": "Simultaneously using multimodal inputs from multiple sensors to train segmentors is intuitively advantageous but practically challenging. A key challenge is unimodal bias, where multimodal segmentors over rely on certain modalities, causing performance drops when others are missing, common in real world applications. To this end, we develop the first framework for learning robust segmentor that can handle any combinations of visual modalities. Specifically, we first introduce a parallel multimodal learning strategy for learning a strong teacher. The cross-modal and unimodal distillation is then achieved in the multi scale representation space by transferring the feature level knowledge from multimodal to anymodal segmentors, aiming at addressing the unimodal bias and avoiding over-reliance on specific modalities. Moreover, a prediction level modality agnostic semantic distillation is proposed to achieve semantic knowledge transferring for segmentation. Extensive experiments on both synthetic and real-world multi-sensor benchmarks demonstrate that our method achieves superior performance.",
      "authors": [
        "Xu Zheng",
        "Haiwei Xue",
        "Jialei Chen",
        "Yibo Yan",
        "Lutao Jiang",
        "Yuanhuiyi Lyu",
        "Kailun Yang",
        "Linfeng Zhang",
        "Xuming Hu"
      ],
      "published": "2024-11-26T06:15:27Z",
      "updated": "2025-05-15T18:54:21Z",
      "categories": [
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2411.17141v2",
      "landing_url": "https://arxiv.org/abs/2411.17141v2",
      "doi": "https://doi.org/10.48550/arXiv.2411.17141"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The work centers on multimodal visual segmentation with unimodal and cross-modal distillation and does not involve discrete audio tokenization or any discrete audio token representations.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The work centers on multimodal visual segmentation with unimodal and cross-modal distillation and does not involve discrete audio tokenization or any discrete audio token representations.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on multimodal segmentors and addresses unimodal bias in visual modalities, with no mention of discrete audio/speech tokenization or conversion of continuous audio signals into discrete tokens as a core object of study, hence it does not meet the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on multimodal segmentors and addresses unimodal bias in visual modalities, with no mention of discrete audio/speech tokenization or conversion of continuous audio signals into discrete tokens as a core object of study, hence it does not meet the inclusion criteria.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Scaling Speech-Text Pre-training with Synthetic Interleaved Data",
    "abstract": "Speech language models (SpeechLMs) accept speech input and produce speech output, allowing for more natural human-computer interaction compared to text-based large language models (LLMs). Traditional approaches for developing SpeechLMs are constrained by the limited availability of unsupervised speech data and parallel speech-text data, which are significantly less abundant than text pre-training data, thereby limiting their scalability as LLMs. We propose a novel approach to scaling speech-text pre-training by leveraging large-scale synthetic interleaved data derived from text corpora, eliminating the need for parallel speech-text datasets. Our method efficiently constructs speech-text interleaved data by sampling text spans from existing text corpora and synthesizing corresponding speech spans using a text-to-token model, bypassing the need to generate actual speech. We also employ a supervised speech tokenizer derived from an automatic speech recognition (ASR) model by incorporating a vector-quantized bottleneck into the encoder. This supervised training approach results in discrete speech tokens with strong semantic preservation even at lower frame rates (e.g. 12.5Hz), while still maintaining speech reconstruction quality. Starting from a pre-trained language model and scaling our pre-training to 1 trillion tokens (with 600B synthetic interleaved speech-text data), we achieve state-of-the-art performance in speech language modeling and spoken question answering, improving performance on spoken questions tasks from the previous SOTA of 13% (Moshi) to 31%. We further demonstrate that by fine-tuning the pre-trained model with speech dialogue data, we can develop an end-to-end spoken chatbot that achieves competitive performance comparable to existing baselines in both conversational abilities and speech quality, even operating exclusively in the speech domain.",
    "metadata": {
      "arxiv_id": "2411.17607",
      "title": "Scaling Speech-Text Pre-training with Synthetic Interleaved Data",
      "summary": "Speech language models (SpeechLMs) accept speech input and produce speech output, allowing for more natural human-computer interaction compared to text-based large language models (LLMs). Traditional approaches for developing SpeechLMs are constrained by the limited availability of unsupervised speech data and parallel speech-text data, which are significantly less abundant than text pre-training data, thereby limiting their scalability as LLMs. We propose a novel approach to scaling speech-text pre-training by leveraging large-scale synthetic interleaved data derived from text corpora, eliminating the need for parallel speech-text datasets. Our method efficiently constructs speech-text interleaved data by sampling text spans from existing text corpora and synthesizing corresponding speech spans using a text-to-token model, bypassing the need to generate actual speech. We also employ a supervised speech tokenizer derived from an automatic speech recognition (ASR) model by incorporating a vector-quantized bottleneck into the encoder. This supervised training approach results in discrete speech tokens with strong semantic preservation even at lower frame rates (e.g. 12.5Hz), while still maintaining speech reconstruction quality. Starting from a pre-trained language model and scaling our pre-training to 1 trillion tokens (with 600B synthetic interleaved speech-text data), we achieve state-of-the-art performance in speech language modeling and spoken question answering, improving performance on spoken questions tasks from the previous SOTA of 13% (Moshi) to 31%. We further demonstrate that by fine-tuning the pre-trained model with speech dialogue data, we can develop an end-to-end spoken chatbot that achieves competitive performance comparable to existing baselines in both conversational abilities and speech quality, even operating exclusively in the speech domain.",
      "authors": [
        "Aohan Zeng",
        "Zhengxiao Du",
        "Mingdao Liu",
        "Lei Zhang",
        "Shengmin Jiang",
        "Yuxiao Dong",
        "Jie Tang"
      ],
      "published": "2024-11-26T17:19:09Z",
      "updated": "2024-12-02T16:13:24Z",
      "categories": [
        "cs.CL",
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2411.17607v2",
      "landing_url": "https://arxiv.org/abs/2411.17607v2",
      "doi": "https://doi.org/10.48550/arXiv.2411.17607"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "Explicitly centers discrete speech tokenization (vector-quantized bottleneck, discrete speech tokens) as a core component for scalable speech-text pre-training, satisfying the inclusion criteria.",
      "evaluation": 5
    },
    "round-A_JuniorNano_reasoning": "Explicitly centers discrete speech tokenization (vector-quantized bottleneck, discrete speech tokens) as a core component for scalable speech-text pre-training, satisfying the inclusion criteria.",
    "round-A_JuniorNano_evaluation": 5,
    "round-A_JuniorMini_output": {
      "reasoning": "The study explicitly proposes and analyzes a supervised speech tokenizer derived from an ASR model that converts continuous speech into discrete tokens, with a focus on semantic preservation and enabling speech language modeling and generation tasks, which directly meets the inclusion criteria; it is not excluded by any of the provided criteria.",
      "evaluation": 5
    },
    "round-A_JuniorMini_reasoning": "The study explicitly proposes and analyzes a supervised speech tokenizer derived from an ASR model that converts continuous speech into discrete tokens, with a focus on semantic preservation and enabling speech language modeling and generation tasks, which directly meets the inclusion criteria; it is not excluded by any of the provided criteria.",
    "round-A_JuniorMini_evaluation": 5,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "include (junior:5)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Efficient Multi-modal Large Language Models via Visual Token Grouping",
    "abstract": "The development of Multi-modal Large Language Models (MLLMs) enhances Large Language Models (LLMs) with the ability to perceive data formats beyond text, significantly advancing a range of downstream applications, such as visual question answering and image captioning. However, the substantial computational costs associated with processing high-resolution images and videos pose a barrier to their broader adoption. To address this challenge, compressing vision tokens in MLLMs has emerged as a promising approach to reduce inference costs. While existing methods conduct token reduction in the feature alignment phase. In this paper, we introduce VisToG, a novel grouping mechanism that leverages the capabilities of pre-trained vision encoders to group similar image segments without the need for segmentation masks. Specifically, we concatenate semantic tokens to represent image semantic segments after the linear projection layer before feeding into the vision encoder. Besides, with the isolated attention we adopt, VisToG can identify and eliminate redundant visual tokens utilizing the prior knowledge in the pre-trained vision encoder, which effectively reduces computational demands. Extensive experiments demonstrate the effectiveness of VisToG, maintaining 98.1% of the original performance while achieving a reduction of over 27\\% inference time.",
    "metadata": {
      "arxiv_id": "2411.17773",
      "title": "Efficient Multi-modal Large Language Models via Visual Token Grouping",
      "summary": "The development of Multi-modal Large Language Models (MLLMs) enhances Large Language Models (LLMs) with the ability to perceive data formats beyond text, significantly advancing a range of downstream applications, such as visual question answering and image captioning. However, the substantial computational costs associated with processing high-resolution images and videos pose a barrier to their broader adoption. To address this challenge, compressing vision tokens in MLLMs has emerged as a promising approach to reduce inference costs. While existing methods conduct token reduction in the feature alignment phase. In this paper, we introduce VisToG, a novel grouping mechanism that leverages the capabilities of pre-trained vision encoders to group similar image segments without the need for segmentation masks. Specifically, we concatenate semantic tokens to represent image semantic segments after the linear projection layer before feeding into the vision encoder. Besides, with the isolated attention we adopt, VisToG can identify and eliminate redundant visual tokens utilizing the prior knowledge in the pre-trained vision encoder, which effectively reduces computational demands. Extensive experiments demonstrate the effectiveness of VisToG, maintaining 98.1% of the original performance while achieving a reduction of over 27\\% inference time.",
      "authors": [
        "Minbin Huang",
        "Runhui Huang",
        "Han Shi",
        "Yimeng Chen",
        "Chuanyang Zheng",
        "Xiangguo Sun",
        "Xin Jiang",
        "Zhenguo Li",
        "Hong Cheng"
      ],
      "published": "2024-11-26T09:36:02Z",
      "updated": "2024-12-02T14:55:49Z",
      "categories": [
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2411.17773v2",
      "landing_url": "https://arxiv.org/abs/2411.17773v2",
      "doi": "https://doi.org/10.48550/arXiv.2411.17773"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The abstract focuses on visual token grouping for multimodal LLMs and does not address discrete audio/speech tokenization, so it does not meet the audio-tokenization criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The abstract focuses on visual token grouping for multimodal LLMs and does not address discrete audio/speech tokenization, so it does not meet the audio-tokenization criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on visual token grouping within multi-modal large language models to reduce computational costs related to image and video processing, without addressing discrete audio/speech tokenization or quantization, which is the required inclusion criterion.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on visual token grouping within multi-modal large language models to reduce computational costs related to image and video processing, without addressing discrete audio/speech tokenization or quantization, which is the required inclusion criterion.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Speech Separation using Neural Audio Codecs with Embedding Loss",
    "abstract": "Neural audio codecs have revolutionized audio processing by enabling speech tasks to be performed on highly compressed representations. Recent work has shown that speech separation can be achieved within these compressed domains, offering faster training and reduced inference costs. However, current approaches still rely on waveform-based loss functions, necessitating unnecessary decoding steps during training. We propose a novel embedding loss for neural audio codec-based speech separation that operates directly on compressed audio representations, eliminating the need for decoding during training. To validate our approach, we conduct comprehensive evaluations using both objective metrics and perceptual assessment techniques, including intrusive and non-intrusive methods. Our results demonstrate that embedding loss can be used to train codec-based speech separation models with a 2x improvement in training speed and computational cost while achieving better DNSMOS and STOI performance on the WSJ0-2mix dataset across 3 different pre-trained codecs.",
    "metadata": {
      "arxiv_id": "2411.17998",
      "title": "Speech Separation using Neural Audio Codecs with Embedding Loss",
      "summary": "Neural audio codecs have revolutionized audio processing by enabling speech tasks to be performed on highly compressed representations. Recent work has shown that speech separation can be achieved within these compressed domains, offering faster training and reduced inference costs. However, current approaches still rely on waveform-based loss functions, necessitating unnecessary decoding steps during training. We propose a novel embedding loss for neural audio codec-based speech separation that operates directly on compressed audio representations, eliminating the need for decoding during training. To validate our approach, we conduct comprehensive evaluations using both objective metrics and perceptual assessment techniques, including intrusive and non-intrusive methods. Our results demonstrate that embedding loss can be used to train codec-based speech separation models with a 2x improvement in training speed and computational cost while achieving better DNSMOS and STOI performance on the WSJ0-2mix dataset across 3 different pre-trained codecs.",
      "authors": [
        "Jia Qi Yip",
        "Chin Yuen Kwok",
        "Bin Ma",
        "Eng Siong Chng"
      ],
      "published": "2024-11-27T02:31:52Z",
      "updated": "2024-11-27T02:31:52Z",
      "categories": [
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2411.17998v1",
      "landing_url": "https://arxiv.org/abs/2411.17998v1",
      "doi": "https://doi.org/10.48550/arXiv.2411.17998"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The work centers on training speech separation in compressed representations produced by neural audio codecs, without explicitly analyzing or proposing discrete audio tokenization (finite-vocabulary symbols) as the core object of study, so it does not meet the inclusion criteria.",
      "evaluation": 2
    },
    "round-A_JuniorNano_reasoning": "The work centers on training speech separation in compressed representations produced by neural audio codecs, without explicitly analyzing or proposing discrete audio tokenization (finite-vocabulary symbols) as the core object of study, so it does not meet the inclusion criteria.",
    "round-A_JuniorNano_evaluation": 2,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on neural audio codecs and proposes an embedding loss operating on compressed audio representations, which suggests processing discrete representations; however, the abstract does not explicitly describe discrete audio/speech tokenization as converting continuous audio into sequences of discrete symbols from a finite vocabulary, nor does it analyze or evaluate discrete tokenization methods in detail as the core subject. Therefore, it is unclear whether the core focus aligns fully with the inclusion criteria on discrete tokenization.",
      "evaluation": 3
    },
    "round-A_JuniorMini_reasoning": "The study focuses on neural audio codecs and proposes an embedding loss operating on compressed audio representations, which suggests processing discrete representations; however, the abstract does not explicitly describe discrete audio/speech tokenization as converting continuous audio into sequences of discrete symbols from a finite vocabulary, nor does it analyze or evaluate discrete tokenization methods in detail as the core subject. Therefore, it is unclear whether the core focus aligns fully with the inclusion criteria on discrete tokenization.",
    "round-A_JuniorMini_evaluation": 3,
    "round-B_SeniorLead_output": {
      "reasoning": "Junior reviewers: Round-A_JuniorNano recommended exclusion (2), noting the work focuses on compressed representations without explicit discrete tokenization; Round-A_JuniorMini was uncertain (3) because the abstract implies use of compressed representations but does not explicitly describe discrete tokenization. My assessment: the abstract describes training speech-separation models on neural-audio-codec compressed representations and an embedding loss, but it never explicitly proposes, analyzes, or evaluates discrete audio/speech tokenization (i.e., conversion of continuous audio into sequences of discrete symbols from a finite vocabulary) or quantization/token-sequence design; references to pre-trained codecs and compressed embeddings alone do not satisfy the inclusion criterion. Therefore it does not meet the required inclusion and should be excluded. One-sentence explanation: I score 2 (better to exclude) because the study focuses on codec embeddings and training efficiency without explicitly proposing or analyzing discrete audio/speech tokenization as required.",
      "evaluation": 2
    },
    "round-B_SeniorLead_reasoning": "Junior reviewers: Round-A_JuniorNano recommended exclusion (2), noting the work focuses on compressed representations without explicit discrete tokenization; Round-A_JuniorMini was uncertain (3) because the abstract implies use of compressed representations but does not explicitly describe discrete tokenization. My assessment: the abstract describes training speech-separation models on neural-audio-codec compressed representations and an embedding loss, but it never explicitly proposes, analyzes, or evaluates discrete audio/speech tokenization (i.e., conversion of continuous audio into sequences of discrete symbols from a finite vocabulary) or quantization/token-sequence design; references to pre-trained codecs and compressed embeddings alone do not satisfy the inclusion criterion. Therefore it does not meet the required inclusion and should be excluded. One-sentence explanation: I score 2 (better to exclude) because the study focuses on codec embeddings and training efficiency without explicitly proposing or analyzing discrete audio/speech tokenization as required.",
    "round-B_SeniorLead_evaluation": 2,
    "final_verdict": "exclude (senior:2)",
    "review_skipped": false,
    "discard_reason": "exclude (senior:2)"
  },
  {
    "title": "Semantic Tokens in Retrieval Augmented Generation",
    "abstract": "Retrieval-Augmented Generation (RAG) architectures have recently garnered significant attention for their ability to improve truth grounding and coherence in natural language processing tasks. However, the reliability of RAG systems in producing accurate answers diminishes as the volume of data they access increases. Even with smaller datasets, these systems occasionally fail to address simple queries. This issue arises from their dependence on state-of-the-art large language models (LLMs), which can introduce uncertainty into the system's outputs. In this work, I propose a novel Comparative RAG system that introduces an evaluator module to bridge the gap between probabilistic RAG systems and deterministically verifiable responses. The evaluator compares external recommendations with the retrieved document chunks, adding a decision-making layer that enhances the system's reliability. This approach ensures that the chunks retrieved are both semantically relevant and logically consistent with deterministic insights, thereby improving the accuracy and overall efficiency of RAG systems. This framework paves the way for more reliable and scalable question-answering applications in domains requiring high precision and verifiability.",
    "metadata": {
      "arxiv_id": "2412.02563",
      "title": "Semantic Tokens in Retrieval Augmented Generation",
      "summary": "Retrieval-Augmented Generation (RAG) architectures have recently garnered significant attention for their ability to improve truth grounding and coherence in natural language processing tasks. However, the reliability of RAG systems in producing accurate answers diminishes as the volume of data they access increases. Even with smaller datasets, these systems occasionally fail to address simple queries. This issue arises from their dependence on state-of-the-art large language models (LLMs), which can introduce uncertainty into the system's outputs. In this work, I propose a novel Comparative RAG system that introduces an evaluator module to bridge the gap between probabilistic RAG systems and deterministically verifiable responses. The evaluator compares external recommendations with the retrieved document chunks, adding a decision-making layer that enhances the system's reliability. This approach ensures that the chunks retrieved are both semantically relevant and logically consistent with deterministic insights, thereby improving the accuracy and overall efficiency of RAG systems. This framework paves the way for more reliable and scalable question-answering applications in domains requiring high precision and verifiability.",
      "authors": [
        "Joel Suro"
      ],
      "published": "2024-12-03T16:52:06Z",
      "updated": "2024-12-03T16:52:06Z",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2412.02563v1",
      "landing_url": "https://arxiv.org/abs/2412.02563v1",
      "doi": "https://doi.org/10.48550/arXiv.2412.02563"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The work focuses on semantic tokens within RAG and a verifier module for text-based retrieval, with no analysis or proposal of discrete audio/speech tokenization or discrete audio representations for speech tasks.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The work focuses on semantic tokens within RAG and a verifier module for text-based retrieval, with no analysis or proposal of discrete audio/speech tokenization or discrete audio representations for speech tasks.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on a novel Retrieval-Augmented Generation system involving an evaluator module to improve information retrieval and answer accuracy, but it does not discuss discrete audio/speech tokenization, nor does it mention any related tokenization of continuous audio signals into discrete tokens as required by the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on a novel Retrieval-Augmented Generation system involving an evaluator module to improve information retrieval and answer accuracy, but it does not discuss discrete audio/speech tokenization, nor does it mention any related tokenization of continuous audio signals into discrete tokens as required by the inclusion criteria.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Continuous Speech Tokens Makes LLMs Robust Multi-Modality Learners",
    "abstract": "Recent advances in GPT-4o like multi-modality models have demonstrated remarkable progress for direct speech-to-speech conversation, with real-time speech interaction experience and strong speech understanding ability. However, current research focuses on discrete speech tokens to align with discrete text tokens for language modelling, which depends on an audio codec with residual connections or independent group tokens, such a codec usually leverages large scale and diverse datasets training to ensure that the discrete speech codes have good representation for varied domain, noise, style data reconstruction as well as a well-designed codec quantizer and encoder-decoder architecture for discrete token language modelling. This paper introduces Flow-Omni, a continuous speech token based GPT-4o like model, capable of real-time speech interaction and low streaming latency. Specifically, first, instead of cross-entropy loss only, we combine flow matching loss with a pretrained autoregressive LLM and a small MLP network to predict the probability distribution of the continuous-valued speech tokens from speech prompt. second, we incorporated the continuous speech tokens to Flow-Omni multi-modality training, thereby achieving robust speech-to-speech performance with discrete text tokens and continuous speech tokens together. Experiments demonstrate that, compared to discrete text and speech multi-modality training and its variants, the continuous speech tokens mitigate robustness issues by avoiding the inherent flaws of discrete speech code's representation loss for LLM.",
    "metadata": {
      "arxiv_id": "2412.04917",
      "title": "Continuous Speech Tokens Makes LLMs Robust Multi-Modality Learners",
      "summary": "Recent advances in GPT-4o like multi-modality models have demonstrated remarkable progress for direct speech-to-speech conversation, with real-time speech interaction experience and strong speech understanding ability. However, current research focuses on discrete speech tokens to align with discrete text tokens for language modelling, which depends on an audio codec with residual connections or independent group tokens, such a codec usually leverages large scale and diverse datasets training to ensure that the discrete speech codes have good representation for varied domain, noise, style data reconstruction as well as a well-designed codec quantizer and encoder-decoder architecture for discrete token language modelling. This paper introduces Flow-Omni, a continuous speech token based GPT-4o like model, capable of real-time speech interaction and low streaming latency. Specifically, first, instead of cross-entropy loss only, we combine flow matching loss with a pretrained autoregressive LLM and a small MLP network to predict the probability distribution of the continuous-valued speech tokens from speech prompt. second, we incorporated the continuous speech tokens to Flow-Omni multi-modality training, thereby achieving robust speech-to-speech performance with discrete text tokens and continuous speech tokens together. Experiments demonstrate that, compared to discrete text and speech multi-modality training and its variants, the continuous speech tokens mitigate robustness issues by avoiding the inherent flaws of discrete speech code's representation loss for LLM.",
      "authors": [
        "Ze Yuan",
        "Yanqing Liu",
        "Shujie Liu",
        "Sheng Zhao"
      ],
      "published": "2024-12-06T10:16:04Z",
      "updated": "2024-12-06T10:16:04Z",
      "categories": [
        "cs.SD",
        "eess.AS",
        "eess.SP"
      ],
      "pdf_url": "https://arxiv.org/pdf/2412.04917v1",
      "landing_url": "https://arxiv.org/abs/2412.04917v1",
      "doi": "https://doi.org/10.48550/arXiv.2412.04917"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The abstract centers on continuous speech tokens as the core representation and treats discrete tokens as a comparison, so it does not explicitly analyze or evaluate discrete audio/speech tokenization as the primary object of study.",
      "evaluation": 2
    },
    "round-A_JuniorNano_reasoning": "The abstract centers on continuous speech tokens as the core representation and treats discrete tokens as a comparison, so it does not explicitly analyze or evaluate discrete audio/speech tokenization as the primary object of study.",
    "round-A_JuniorNano_evaluation": 2,
    "round-A_JuniorMini_output": {
      "reasoning": "The study introduces Flow-Omni, a model using continuous speech tokens rather than discrete speech tokens, and contrasts its approach with prior discrete token methods; however, the core focus is on continuous speech token representations, not on discrete audio/speech tokenization as a primary subject of analysis or proposal, which is required by the inclusion criteria.",
      "evaluation": 2
    },
    "round-A_JuniorMini_reasoning": "The study introduces Flow-Omni, a model using continuous speech tokens rather than discrete speech tokens, and contrasts its approach with prior discrete token methods; however, the core focus is on continuous speech token representations, not on discrete audio/speech tokenization as a primary subject of analysis or proposal, which is required by the inclusion criteria.",
    "round-A_JuniorMini_evaluation": 2,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:2)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:2)"
  },
  {
    "title": "Spectral Image Tokenizer",
    "abstract": "Image tokenizers map images to sequences of discrete tokens, and are a crucial component of autoregressive transformer-based image generation. The tokens are typically associated with spatial locations in the input image, arranged in raster scan order, which is not ideal for autoregressive modeling. In this paper, we propose to tokenize the image spectrum instead, obtained from a discrete wavelet transform (DWT), such that the sequence of tokens represents the image in a coarse-to-fine fashion. Our tokenizer brings several advantages: 1) it leverages that natural images are more compressible at high frequencies, 2) it can take and reconstruct images of different resolutions without retraining, 3) it improves the conditioning for next-token prediction -- instead of conditioning on a partial line-by-line reconstruction of the image, it takes a coarse reconstruction of the full image, 4) it enables partial decoding where the first few generated tokens can reconstruct a coarse version of the image, 5) it enables autoregressive models to be used for image upsampling. We evaluate the tokenizer reconstruction metrics as well as multiscale image generation, text-guided image upsampling and editing.",
    "metadata": {
      "arxiv_id": "2412.09607",
      "title": "Spectral Image Tokenizer",
      "summary": "Image tokenizers map images to sequences of discrete tokens, and are a crucial component of autoregressive transformer-based image generation. The tokens are typically associated with spatial locations in the input image, arranged in raster scan order, which is not ideal for autoregressive modeling. In this paper, we propose to tokenize the image spectrum instead, obtained from a discrete wavelet transform (DWT), such that the sequence of tokens represents the image in a coarse-to-fine fashion. Our tokenizer brings several advantages: 1) it leverages that natural images are more compressible at high frequencies, 2) it can take and reconstruct images of different resolutions without retraining, 3) it improves the conditioning for next-token prediction -- instead of conditioning on a partial line-by-line reconstruction of the image, it takes a coarse reconstruction of the full image, 4) it enables partial decoding where the first few generated tokens can reconstruct a coarse version of the image, 5) it enables autoregressive models to be used for image upsampling. We evaluate the tokenizer reconstruction metrics as well as multiscale image generation, text-guided image upsampling and editing.",
      "authors": [
        "Carlos Esteves",
        "Mohammed Suhail",
        "Ameesh Makadia"
      ],
      "published": "2024-12-12T18:59:31Z",
      "updated": "2025-06-11T17:58:34Z",
      "categories": [
        "cs.CV",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2412.09607v2",
      "landing_url": "https://arxiv.org/abs/2412.09607v2",
      "doi": "https://doi.org/10.48550/arXiv.2412.09607"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This item focuses on spectral image tokenization using DWT for images, not discrete audio/speech tokenization, thus does not meet the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "This item focuses on spectral image tokenization using DWT for images, not discrete audio/speech tokenization, thus does not meet the inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on image tokenization using spectral representations, with no mention or analysis of discrete audio/speech tokenization, which is the required inclusion criterion.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on image tokenization using spectral representations, with no mention or analysis of discrete audio/speech tokenization, which is the required inclusion criterion.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "CosyVoice 2: Scalable Streaming Speech Synthesis with Large Language Models",
    "abstract": "In our previous work, we introduced CosyVoice, a multilingual speech synthesis model based on supervised discrete speech tokens. By employing progressive semantic decoding with two popular generative models, language models (LMs) and Flow Matching, CosyVoice demonstrated high prosody naturalness, content consistency, and speaker similarity in speech in-context learning. Recently, significant progress has been made in multi-modal large language models (LLMs), where the response latency and real-time factor of speech synthesis play a crucial role in the interactive experience. Therefore, in this report, we present an improved streaming speech synthesis model, CosyVoice 2, which incorporates comprehensive and systematic optimizations. Specifically, we introduce finite-scalar quantization to improve the codebook utilization of speech tokens. For the text-speech LM, we streamline the model architecture to allow direct use of a pre-trained LLM as the backbone. In addition, we develop a chunk-aware causal flow matching model to support various synthesis scenarios, enabling both streaming and non-streaming synthesis within a single model. By training on a large-scale multilingual dataset, CosyVoice 2 achieves human-parity naturalness, minimal response latency, and virtually lossless synthesis quality in the streaming mode. We invite readers to listen to the demos at https://funaudiollm.github.io/cosyvoice2.",
    "metadata": {
      "arxiv_id": "2412.10117",
      "title": "CosyVoice 2: Scalable Streaming Speech Synthesis with Large Language Models",
      "summary": "In our previous work, we introduced CosyVoice, a multilingual speech synthesis model based on supervised discrete speech tokens. By employing progressive semantic decoding with two popular generative models, language models (LMs) and Flow Matching, CosyVoice demonstrated high prosody naturalness, content consistency, and speaker similarity in speech in-context learning. Recently, significant progress has been made in multi-modal large language models (LLMs), where the response latency and real-time factor of speech synthesis play a crucial role in the interactive experience. Therefore, in this report, we present an improved streaming speech synthesis model, CosyVoice 2, which incorporates comprehensive and systematic optimizations. Specifically, we introduce finite-scalar quantization to improve the codebook utilization of speech tokens. For the text-speech LM, we streamline the model architecture to allow direct use of a pre-trained LLM as the backbone. In addition, we develop a chunk-aware causal flow matching model to support various synthesis scenarios, enabling both streaming and non-streaming synthesis within a single model. By training on a large-scale multilingual dataset, CosyVoice 2 achieves human-parity naturalness, minimal response latency, and virtually lossless synthesis quality in the streaming mode. We invite readers to listen to the demos at https://funaudiollm.github.io/cosyvoice2.",
      "authors": [
        "Zhihao Du",
        "Yuxuan Wang",
        "Qian Chen",
        "Xian Shi",
        "Xiang Lv",
        "Tianyu Zhao",
        "Zhifu Gao",
        "Yexin Yang",
        "Changfeng Gao",
        "Hui Wang",
        "Fan Yu",
        "Huadai Liu",
        "Zhengyan Sheng",
        "Yue Gu",
        "Chong Deng",
        "Wen Wang",
        "Shiliang Zhang",
        "Zhijie Yan",
        "Jingren Zhou"
      ],
      "published": "2024-12-13T12:59:39Z",
      "updated": "2024-12-25T11:54:03Z",
      "categories": [
        "cs.SD",
        "cs.AI",
        "cs.LG",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2412.10117v3",
      "landing_url": "https://arxiv.org/abs/2412.10117v3",
      "doi": "https://doi.org/10.48550/arXiv.2412.10117"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The abstract centers on discrete speech tokens, finite-scalar quantization, and codebooks for speech tokenization, treating discrete token representations as the core mechanism for speech synthesis, which aligns with the inclusion criteria and avoids the exclusion conditions.",
      "evaluation": 5
    },
    "round-A_JuniorNano_reasoning": "The abstract centers on discrete speech tokens, finite-scalar quantization, and codebooks for speech tokenization, treating discrete token representations as the core mechanism for speech synthesis, which aligns with the inclusion criteria and avoids the exclusion conditions.",
    "round-A_JuniorNano_evaluation": 5,
    "round-A_JuniorMini_output": {
      "reasoning": "The study explicitly discusses supervised discrete speech tokens and improvements in codebook utilization for these tokens, which indicates that discrete audio/speech tokenization is a core object of study. It also involves a neural model for synthesis, not traditional codecs, and includes systematic architectural improvements and evaluations relevant to discrete tokenization for speech synthesis tasks. Thus, it meets the inclusion criteria and does not fall under any exclusion criteria.",
      "evaluation": 5
    },
    "round-A_JuniorMini_reasoning": "The study explicitly discusses supervised discrete speech tokens and improvements in codebook utilization for these tokens, which indicates that discrete audio/speech tokenization is a core object of study. It also involves a neural model for synthesis, not traditional codecs, and includes systematic architectural improvements and evaluations relevant to discrete tokenization for speech synthesis tasks. Thus, it meets the inclusion criteria and does not fall under any exclusion criteria.",
    "round-A_JuniorMini_evaluation": 5,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "include (junior:5)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "MVQ:Towards Efficient DNN Compression and Acceleration with Masked Vector Quantization",
    "abstract": "Vector quantization(VQ) is a hardware-friendly DNN compression method that can reduce the storage cost and weight-loading datawidth of hardware accelerators. However, conventional VQ techniques lead to significant accuracy loss because the important weights are not well preserved. To tackle this problem, a novel approach called MVQ is proposed, which aims at better approximating important weights with a limited number of codewords. At the algorithm level, our approach removes the less important weights through N:M pruning and then minimizes the vector clustering error between the remaining weights and codewords by the masked k-means algorithm. Only distances between the unpruned weights and the codewords are computed, which are then used to update the codewords. At the architecture level, our accelerator implements vector quantization on an EWS (Enhanced weight stationary) CNN accelerator and proposes a sparse systolic array design to maximize the benefits brought by masked vector quantization.\\\\ Our algorithm is validated on various models for image classification, object detection, and segmentation tasks. Experimental results demonstrate that MVQ not only outperforms conventional vector quantization methods at comparable compression ratios but also reduces FLOPs. Under ASIC evaluation, our MVQ accelerator boosts energy efficiency by 2.3$\\times$ and reduces the size of the systolic array by 55\\% when compared with the base EWS accelerator. Compared to the previous sparse accelerators, MVQ achieves 1.73$\\times$ higher energy efficiency.",
    "metadata": {
      "arxiv_id": "2412.10261",
      "title": "MVQ:Towards Efficient DNN Compression and Acceleration with Masked Vector Quantization",
      "summary": "Vector quantization(VQ) is a hardware-friendly DNN compression method that can reduce the storage cost and weight-loading datawidth of hardware accelerators. However, conventional VQ techniques lead to significant accuracy loss because the important weights are not well preserved. To tackle this problem, a novel approach called MVQ is proposed, which aims at better approximating important weights with a limited number of codewords. At the algorithm level, our approach removes the less important weights through N:M pruning and then minimizes the vector clustering error between the remaining weights and codewords by the masked k-means algorithm. Only distances between the unpruned weights and the codewords are computed, which are then used to update the codewords. At the architecture level, our accelerator implements vector quantization on an EWS (Enhanced weight stationary) CNN accelerator and proposes a sparse systolic array design to maximize the benefits brought by masked vector quantization.\\\\ Our algorithm is validated on various models for image classification, object detection, and segmentation tasks. Experimental results demonstrate that MVQ not only outperforms conventional vector quantization methods at comparable compression ratios but also reduces FLOPs. Under ASIC evaluation, our MVQ accelerator boosts energy efficiency by 2.3$\\times$ and reduces the size of the systolic array by 55\\% when compared with the base EWS accelerator. Compared to the previous sparse accelerators, MVQ achieves 1.73$\\times$ higher energy efficiency.",
      "authors": [
        "Shuaiting Li",
        "Chengxuan Wang",
        "Juncan Deng",
        "Zeyu Wang",
        "Zewen Ye",
        "Zongsheng Wang",
        "Haibin Shen",
        "Kejie Huang"
      ],
      "published": "2024-12-13T16:30:35Z",
      "updated": "2024-12-16T08:54:43Z",
      "categories": [
        "cs.CV",
        "cs.AR"
      ],
      "pdf_url": "https://arxiv.org/pdf/2412.10261v2",
      "landing_url": "https://arxiv.org/abs/2412.10261v2",
      "doi": "https://doi.org/10.1145/3669940.3707268"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The abstract focuses on discrete vector quantization for DNN weights and hardware acceleration, not discrete audio/speech tokenization required by the inclusion criteria, so it should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The abstract focuses on discrete vector quantization for DNN weights and hardware acceleration, not discrete audio/speech tokenization required by the inclusion criteria, so it should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on vector quantization methods for DNN compression and hardware acceleration, including image classification, object detection, and segmentation tasks, without any mention of discrete audio/speech tokenization or applications to audio or speech tasks, which is the core inclusion criterion.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on vector quantization methods for DNN compression and hardware acceleration, including image classification, object detection, and segmentation tasks, without any mention of discrete audio/speech tokenization or applications to audio or speech tasks, which is the core inclusion criterion.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Empowering LLMs to Understand and Generate Complex Vector Graphics",
    "abstract": "The unprecedented advancements in Large Language Models (LLMs) have profoundly impacted natural language processing but have yet to fully embrace the realm of scalable vector graphics (SVG) generation. While LLMs encode partial knowledge of SVG data from web pages during training, recent findings suggest that semantically ambiguous and tokenized representations within LLMs may result in hallucinations in vector primitive predictions. Additionally, LLM training typically lacks modeling and understanding of the rendering sequence of vector paths, which can lead to occlusion between output vector primitives. In this paper, we present LLM4SVG, an initial yet substantial step toward bridging this gap by enabling LLMs to better understand and generate vector graphics. LLM4SVG facilitates a deeper understanding of SVG components through learnable semantic tokens, which precisely encode these tokens and their corresponding properties to generate semantically aligned SVG outputs. Using a series of learnable semantic tokens, a structured dataset for instruction following is developed to support comprehension and generation across two primary tasks. Our method introduces a modular architecture to existing large language models, integrating semantic tags, vector instruction encoders, fine-tuned commands, and powerful LLMs to tightly combine geometric, appearance, and language information. To overcome the scarcity of SVG-text instruction data, we developed an automated data generation pipeline that collected our SVGX-SFT Dataset, consisting of high-quality human-designed SVGs and 580k SVG instruction following data specifically crafted for LLM training, which facilitated the adoption of the supervised fine-tuning strategy popular in LLM development.",
    "metadata": {
      "arxiv_id": "2412.11102",
      "title": "Empowering LLMs to Understand and Generate Complex Vector Graphics",
      "summary": "The unprecedented advancements in Large Language Models (LLMs) have profoundly impacted natural language processing but have yet to fully embrace the realm of scalable vector graphics (SVG) generation. While LLMs encode partial knowledge of SVG data from web pages during training, recent findings suggest that semantically ambiguous and tokenized representations within LLMs may result in hallucinations in vector primitive predictions. Additionally, LLM training typically lacks modeling and understanding of the rendering sequence of vector paths, which can lead to occlusion between output vector primitives. In this paper, we present LLM4SVG, an initial yet substantial step toward bridging this gap by enabling LLMs to better understand and generate vector graphics. LLM4SVG facilitates a deeper understanding of SVG components through learnable semantic tokens, which precisely encode these tokens and their corresponding properties to generate semantically aligned SVG outputs. Using a series of learnable semantic tokens, a structured dataset for instruction following is developed to support comprehension and generation across two primary tasks. Our method introduces a modular architecture to existing large language models, integrating semantic tags, vector instruction encoders, fine-tuned commands, and powerful LLMs to tightly combine geometric, appearance, and language information. To overcome the scarcity of SVG-text instruction data, we developed an automated data generation pipeline that collected our SVGX-SFT Dataset, consisting of high-quality human-designed SVGs and 580k SVG instruction following data specifically crafted for LLM training, which facilitated the adoption of the supervised fine-tuning strategy popular in LLM development.",
      "authors": [
        "Ximing Xing",
        "Juncheng Hu",
        "Guotao Liang",
        "Jing Zhang",
        "Dong Xu",
        "Qian Yu"
      ],
      "published": "2024-12-15T07:49:31Z",
      "updated": "2025-03-25T15:35:29Z",
      "categories": [
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2412.11102v3",
      "landing_url": "https://arxiv.org/abs/2412.11102v3",
      "doi": "https://doi.org/10.48550/arXiv.2412.11102"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The article centers on enabling LLMs to understand and generate SVG vector graphics using semantic tokens and an SVG data pipeline; there is no discussion of discrete audio/tokenization of audio signals, so it does not meet the discrete audio tokenization criterion.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The article centers on enabling LLMs to understand and generate SVG vector graphics using semantic tokens and an SVG data pipeline; there is no discussion of discrete audio/tokenization of audio signals, so it does not meet the discrete audio tokenization criterion.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on enabling large language models to understand and generate scalable vector graphics (SVG) through learnable semantic tokens and structured datasets; it does not address discrete audio/speech tokenization or audio signal processing in any form, which is the core inclusion criterion.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on enabling large language models to understand and generate scalable vector graphics (SVG) through learnable semantic tokens and structured datasets; it does not address discrete audio/speech tokenization or audio signal processing in any form, which is the core inclusion criterion.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Whisper-GPT: A Hybrid Representation Audio Large Language Model",
    "abstract": "We propose WHISPER-GPT: A generative large language model (LLM) for speech and music that allows us to work with continuous audio representations and discrete tokens simultaneously as part of a single architecture. There has been a huge surge in generative audio, speech, and music models that utilize discrete audio tokens derived from neural compression algorithms, e.g. ENCODEC. However, one of the major drawbacks of this approach is handling the context length. It blows up for high-fidelity generative architecture if one has to account for all the audio contents at various frequencies for the next token prediction. By combining continuous audio representation like the spectrogram and discrete acoustic tokens, we retain the best of both worlds: Have all the information needed from the audio at a specific time instance in a single token, yet allow LLM to predict the future token to allow for sampling and other benefits discrete space provides. We show how our architecture improves the perplexity and negative log-likelihood scores for the next token prediction compared to a token-based LLM for speech and music.",
    "metadata": {
      "arxiv_id": "2412.11449",
      "title": "Whisper-GPT: A Hybrid Representation Audio Large Language Model",
      "summary": "We propose WHISPER-GPT: A generative large language model (LLM) for speech and music that allows us to work with continuous audio representations and discrete tokens simultaneously as part of a single architecture. There has been a huge surge in generative audio, speech, and music models that utilize discrete audio tokens derived from neural compression algorithms, e.g. ENCODEC. However, one of the major drawbacks of this approach is handling the context length. It blows up for high-fidelity generative architecture if one has to account for all the audio contents at various frequencies for the next token prediction. By combining continuous audio representation like the spectrogram and discrete acoustic tokens, we retain the best of both worlds: Have all the information needed from the audio at a specific time instance in a single token, yet allow LLM to predict the future token to allow for sampling and other benefits discrete space provides. We show how our architecture improves the perplexity and negative log-likelihood scores for the next token prediction compared to a token-based LLM for speech and music.",
      "authors": [
        "Prateek Verma"
      ],
      "published": "2024-12-16T05:03:48Z",
      "updated": "2024-12-16T05:03:48Z",
      "categories": [
        "cs.SD",
        "cs.AI",
        "cs.CL",
        "cs.LG",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2412.11449v1",
      "landing_url": "https://arxiv.org/abs/2412.11449v1",
      "doi": "https://doi.org/10.48550/arXiv.2412.11449"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The abstract explicitly centers on discrete acoustic tokens as part of a hybrid representation for audio within an LLM framework, discusses token-based prediction and evaluation against a token-based baseline, and thus directly addresses discrete audio tokenization as a core object of study without falling into the exclusion categories.",
      "evaluation": 5
    },
    "round-A_JuniorNano_reasoning": "The abstract explicitly centers on discrete acoustic tokens as part of a hybrid representation for audio within an LLM framework, discusses token-based prediction and evaluation against a token-based baseline, and thus directly addresses discrete audio tokenization as a core object of study without falling into the exclusion categories.",
    "round-A_JuniorNano_evaluation": 5,
    "round-A_JuniorMini_output": {
      "reasoning": "The study explicitly focuses on combining continuous audio representations with discrete acoustic tokens derived from neural compression algorithms for speech and music generation within a large language model architecture, which aligns well with the inclusion criteria emphasizing discrete audio/speech tokenization for next token prediction, and presents experimental validation of their approach; it is not purely engineering-focused nor limited to continuous features without discrete token design, hence it should be included.",
      "evaluation": 5
    },
    "round-A_JuniorMini_reasoning": "The study explicitly focuses on combining continuous audio representations with discrete acoustic tokens derived from neural compression algorithms for speech and music generation within a large language model architecture, which aligns well with the inclusion criteria emphasizing discrete audio/speech tokenization for next token prediction, and presents experimental validation of their approach; it is not purely engineering-focused nor limited to continuous features without discrete token design, hence it should be included.",
    "round-A_JuniorMini_evaluation": 5,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "include (junior:5)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Understanding Emotional Body Expressions via Large Language Models",
    "abstract": "Emotion recognition based on body movements is vital in human-computer interaction. However, existing emotion recognition methods predominantly focus on enhancing classification accuracy, often neglecting the provision of textual explanations to justify their classifications. In this paper, we propose an Emotion-Action Interpreter powered by Large Language Model (EAI-LLM), which not only recognizes emotions but also generates textual explanations by treating 3D body movement data as unique input tokens within large language models (LLMs). Specifically, we propose a multi-granularity skeleton tokenizer designed for LLMs, which separately extracts spatio-temporal tokens and semantic tokens from the skeleton data. This approach allows LLMs to generate more nuanced classification descriptions while maintaining robust classification performance. Furthermore, we treat the skeleton sequence as a specific language and propose a unified skeleton token module. This module leverages the extensive background knowledge and language processing capabilities of LLMs to address the challenges of joint training on heterogeneous datasets, thereby significantly enhancing recognition accuracy on individual datasets. Experimental results demonstrate that our model achieves recognition accuracy comparable to existing methods. More importantly, with the support of background knowledge from LLMs, our model can generate detailed emotion descriptions based on classification results, even when trained on a limited amount of labeled skeleton data.",
    "metadata": {
      "arxiv_id": "2412.12581",
      "title": "Understanding Emotional Body Expressions via Large Language Models",
      "summary": "Emotion recognition based on body movements is vital in human-computer interaction. However, existing emotion recognition methods predominantly focus on enhancing classification accuracy, often neglecting the provision of textual explanations to justify their classifications. In this paper, we propose an Emotion-Action Interpreter powered by Large Language Model (EAI-LLM), which not only recognizes emotions but also generates textual explanations by treating 3D body movement data as unique input tokens within large language models (LLMs). Specifically, we propose a multi-granularity skeleton tokenizer designed for LLMs, which separately extracts spatio-temporal tokens and semantic tokens from the skeleton data. This approach allows LLMs to generate more nuanced classification descriptions while maintaining robust classification performance. Furthermore, we treat the skeleton sequence as a specific language and propose a unified skeleton token module. This module leverages the extensive background knowledge and language processing capabilities of LLMs to address the challenges of joint training on heterogeneous datasets, thereby significantly enhancing recognition accuracy on individual datasets. Experimental results demonstrate that our model achieves recognition accuracy comparable to existing methods. More importantly, with the support of background knowledge from LLMs, our model can generate detailed emotion descriptions based on classification results, even when trained on a limited amount of labeled skeleton data.",
      "authors": [
        "Haifeng Lu",
        "Jiuyi Chen",
        "Feng Liang",
        "Mingkui Tan",
        "Runhao Zeng",
        "Xiping Hu"
      ],
      "published": "2024-12-17T06:20:39Z",
      "updated": "2024-12-20T11:49:07Z",
      "categories": [
        "cs.HC"
      ],
      "pdf_url": "https://arxiv.org/pdf/2412.12581v2",
      "landing_url": "https://arxiv.org/abs/2412.12581v2",
      "doi": "https://doi.org/10.48550/arXiv.2412.12581"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The abstract focuses on skeleton-based emotion recognition and tokenization of 3D body data for LLMs, not on discrete audio/speech tokenization; it therefore does not meet the inclusion criterion and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The abstract focuses on skeleton-based emotion recognition and tokenization of 3D body data for LLMs, not on discrete audio/speech tokenization; it therefore does not meet the inclusion criterion and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on emotion recognition from 3D body movement data using large language models, with no mention of audio or speech tokenization or discrete audio symbolization; thus, it does not meet the inclusion criteria related to audio/speech tokenization nor does it analyze discrete audio tokenization.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on emotion recognition from 3D body movement data using large language models, with no mention of audio or speech tokenization or discrete audio symbolization; thus, it does not meet the inclusion criteria related to audio/speech tokenization nor does it analyze discrete audio tokenization.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Autoregressive Video Generation without Vector Quantization",
    "abstract": "This paper presents a novel approach that enables autoregressive video generation with high efficiency. We propose to reformulate the video generation problem as a non-quantized autoregressive modeling of temporal frame-by-frame prediction and spatial set-by-set prediction. Unlike raster-scan prediction in prior autoregressive models or joint distribution modeling of fixed-length tokens in diffusion models, our approach maintains the causal property of GPT-style models for flexible in-context capabilities, while leveraging bidirectional modeling within individual frames for efficiency. With the proposed approach, we train a novel video autoregressive model without vector quantization, termed NOVA. Our results demonstrate that NOVA surpasses prior autoregressive video models in data efficiency, inference speed, visual fidelity, and video fluency, even with a much smaller model capacity, i.e., 0.6B parameters. NOVA also outperforms state-of-the-art image diffusion models in text-to-image generation tasks, with a significantly lower training cost. Additionally, NOVA generalizes well across extended video durations and enables diverse zero-shot applications in one unified model. Code and models are publicly available at https://github.com/baaivision/NOVA.",
    "metadata": {
      "arxiv_id": "2412.14169",
      "title": "Autoregressive Video Generation without Vector Quantization",
      "summary": "This paper presents a novel approach that enables autoregressive video generation with high efficiency. We propose to reformulate the video generation problem as a non-quantized autoregressive modeling of temporal frame-by-frame prediction and spatial set-by-set prediction. Unlike raster-scan prediction in prior autoregressive models or joint distribution modeling of fixed-length tokens in diffusion models, our approach maintains the causal property of GPT-style models for flexible in-context capabilities, while leveraging bidirectional modeling within individual frames for efficiency. With the proposed approach, we train a novel video autoregressive model without vector quantization, termed NOVA. Our results demonstrate that NOVA surpasses prior autoregressive video models in data efficiency, inference speed, visual fidelity, and video fluency, even with a much smaller model capacity, i.e., 0.6B parameters. NOVA also outperforms state-of-the-art image diffusion models in text-to-image generation tasks, with a significantly lower training cost. Additionally, NOVA generalizes well across extended video durations and enables diverse zero-shot applications in one unified model. Code and models are publicly available at https://github.com/baaivision/NOVA.",
      "authors": [
        "Haoge Deng",
        "Ting Pan",
        "Haiwen Diao",
        "Zhengxiong Luo",
        "Yufeng Cui",
        "Huchuan Lu",
        "Shiguang Shan",
        "Yonggang Qi",
        "Xinlong Wang"
      ],
      "published": "2024-12-18T18:59:53Z",
      "updated": "2025-03-02T08:09:39Z",
      "categories": [
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2412.14169v2",
      "landing_url": "https://arxiv.org/abs/2412.14169v2",
      "doi": "https://doi.org/10.48550/arXiv.2412.14169"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The abstract concerns autoregressive video generation with no discrete audio tokenization; there is no analysis or use of discrete audio/speech tokens, so it fails the inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The abstract concerns autoregressive video generation with no discrete audio tokenization; there is no analysis or use of discrete audio/speech tokens, so it fails the inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on autoregressive video generation without employing vector quantization or discrete tokenization for audio or speech; it primarily addresses video frame prediction and visual generative modeling, not discrete audio/speech tokenization, and thus does not meet the inclusion criteria related to discrete audio token studies.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on autoregressive video generation without employing vector quantization or discrete tokenization for audio or speech; it primarily addresses video frame prediction and visual generative modeling, not discrete audio/speech tokenization, and thus does not meet the inclusion criteria related to discrete audio token studies.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "RefHCM: A Unified Model for Referring Perceptions in Human-Centric Scenarios",
    "abstract": "Human-centric perceptions play a crucial role in real-world applications. While recent human-centric works have achieved impressive progress, these efforts are often constrained to the visual domain and lack interaction with human instructions, limiting their applicability in broader scenarios such as chatbots and sports analysis. This paper introduces Referring Human Perceptions, where a referring prompt specifies the person of interest in an image. To tackle the new task, we propose RefHCM (Referring Human-Centric Model), a unified framework to integrate a wide range of human-centric referring tasks. Specifically, RefHCM employs sequence mergers to convert raw multimodal data -- including images, text, coordinates, and parsing maps -- into semantic tokens. This standardized representation enables RefHCM to reformulate diverse human-centric referring tasks into a sequence-to-sequence paradigm, solved using a plain encoder-decoder transformer architecture. Benefiting from a unified learning strategy, RefHCM effectively facilitates knowledge transfer across tasks and exhibits unforeseen capabilities in handling complex reasoning. This work represents the first attempt to address referring human perceptions with a general-purpose framework, while simultaneously establishing a corresponding benchmark that sets new standards for the field. Extensive experiments showcase RefHCM's competitive and even superior performance across multiple human-centric referring tasks. The code and data are publicly at https://github.com/JJJYmmm/RefHCM.",
    "metadata": {
      "arxiv_id": "2412.14643",
      "title": "RefHCM: A Unified Model for Referring Perceptions in Human-Centric Scenarios",
      "summary": "Human-centric perceptions play a crucial role in real-world applications. While recent human-centric works have achieved impressive progress, these efforts are often constrained to the visual domain and lack interaction with human instructions, limiting their applicability in broader scenarios such as chatbots and sports analysis. This paper introduces Referring Human Perceptions, where a referring prompt specifies the person of interest in an image. To tackle the new task, we propose RefHCM (Referring Human-Centric Model), a unified framework to integrate a wide range of human-centric referring tasks. Specifically, RefHCM employs sequence mergers to convert raw multimodal data -- including images, text, coordinates, and parsing maps -- into semantic tokens. This standardized representation enables RefHCM to reformulate diverse human-centric referring tasks into a sequence-to-sequence paradigm, solved using a plain encoder-decoder transformer architecture. Benefiting from a unified learning strategy, RefHCM effectively facilitates knowledge transfer across tasks and exhibits unforeseen capabilities in handling complex reasoning. This work represents the first attempt to address referring human perceptions with a general-purpose framework, while simultaneously establishing a corresponding benchmark that sets new standards for the field. Extensive experiments showcase RefHCM's competitive and even superior performance across multiple human-centric referring tasks. The code and data are publicly at https://github.com/JJJYmmm/RefHCM.",
      "authors": [
        "Jie Huang",
        "Ruibing Hou",
        "Jiahe Zhao",
        "Hong Chang",
        "Shiguang Shan"
      ],
      "published": "2024-12-19T08:51:57Z",
      "updated": "2024-12-19T08:51:57Z",
      "categories": [
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2412.14643v1",
      "landing_url": "https://arxiv.org/abs/2412.14643v1",
      "doi": "https://doi.org/10.48550/arXiv.2412.14643"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The work focuses on vision-language multimodal referring tasks and tokenizes data into semantic tokens via a sequence-to-sequence model, but it does not propose or analyze discrete audio/speech tokenization.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The work focuses on vision-language multimodal referring tasks and tokenizes data into semantic tokens via a sequence-to-sequence model, but it does not propose or analyze discrete audio/speech tokenization.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on a unified model for human-centric referring perceptions in visual scenarios without mentioning any discrete audio/speech tokenization or audio signal processing; thus, it does not meet the inclusion criteria related to discrete audio tokenization and falls outside the scope specified.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on a unified model for human-centric referring perceptions in visual scenarios without mentioning any discrete audio/speech tokenization or audio signal processing; thus, it does not meet the inclusion criteria related to discrete audio tokenization and falls outside the scope specified.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Stack Trace Deduplication: Faster, More Accurately, and in More Realistic Scenarios",
    "abstract": "In large-scale software systems, there are often no fully-fledged bug reports with human-written descriptions when an error occurs. In this case, developers rely on stack traces, i.e., series of function calls that led to the error. Since there can be tens and hundreds of thousands of them describing the same issue from different users, automatic deduplication into categories is necessary to allow for processing. Recent works have proposed powerful deep learning-based approaches for this, but they are evaluated and compared in isolation from real-life workflows, and it is not clear whether they will actually work well at scale. To overcome this gap, this work presents three main contributions: a novel model, an industry-based dataset, and a multi-faceted evaluation. Our model consists of two parts - (1) an embedding model with byte-pair encoding and approximate nearest neighbor search to quickly find the most relevant stack traces to the incoming one, and (2) a reranker that re-ranks the most fitting stack traces, taking into account the repeated frames between them. To complement the existing datasets collected from open-source projects, we share with the community SlowOps - a dataset of stack traces from IntelliJ-based products developed by JetBrains, which has an order of magnitude more stack traces per category. Finally, we carry out an evaluation that strives to be realistic: measuring not only the accuracy of categorization, but also the operation time and the ability to create new categories. The evaluation shows that our model strikes a good balance - it outperforms other models on both open-source datasets and SlowOps, while also being faster on time than most. We release all of our code and data, and hope that our work can pave the way to further practice-oriented research in the area.",
    "metadata": {
      "arxiv_id": "2412.14802",
      "title": "Stack Trace Deduplication: Faster, More Accurately, and in More Realistic Scenarios",
      "summary": "In large-scale software systems, there are often no fully-fledged bug reports with human-written descriptions when an error occurs. In this case, developers rely on stack traces, i.e., series of function calls that led to the error. Since there can be tens and hundreds of thousands of them describing the same issue from different users, automatic deduplication into categories is necessary to allow for processing. Recent works have proposed powerful deep learning-based approaches for this, but they are evaluated and compared in isolation from real-life workflows, and it is not clear whether they will actually work well at scale.\n  To overcome this gap, this work presents three main contributions: a novel model, an industry-based dataset, and a multi-faceted evaluation. Our model consists of two parts - (1) an embedding model with byte-pair encoding and approximate nearest neighbor search to quickly find the most relevant stack traces to the incoming one, and (2) a reranker that re-ranks the most fitting stack traces, taking into account the repeated frames between them. To complement the existing datasets collected from open-source projects, we share with the community SlowOps - a dataset of stack traces from IntelliJ-based products developed by JetBrains, which has an order of magnitude more stack traces per category. Finally, we carry out an evaluation that strives to be realistic: measuring not only the accuracy of categorization, but also the operation time and the ability to create new categories. The evaluation shows that our model strikes a good balance - it outperforms other models on both open-source datasets and SlowOps, while also being faster on time than most. We release all of our code and data, and hope that our work can pave the way to further practice-oriented research in the area.",
      "authors": [
        "Egor Shibaev",
        "Denis Sushentsev",
        "Yaroslav Golubev",
        "Aleksandr Khvorov"
      ],
      "published": "2024-12-19T12:48:17Z",
      "updated": "2024-12-19T12:48:17Z",
      "categories": [
        "cs.SE",
        "cs.AI",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2412.14802v1",
      "landing_url": "https://arxiv.org/abs/2412.14802v1",
      "doi": "https://doi.org/10.48550/arXiv.2412.14802"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item is about stack-trace deduplication in software systems and evaluates embedding/reranking for categorizing stack traces; it has no focus on discrete audio tokenization or speech token sequences, so it does not meet the audio/tokenization inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item is about stack-trace deduplication in software systems and evaluates embedding/reranking for categorizing stack traces; it has no focus on discrete audio tokenization or speech token sequences, so it does not meet the audio/tokenization inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on stack trace deduplication in software error reporting and does not involve audio or speech tokenization or any discrete tokenization of audio signals. Therefore, it does not meet the inclusion criteria related to discrete audio/speech tokenization, nor does it align with the exclusion criteria that target audio tokenization studies.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on stack trace deduplication in software error reporting and does not involve audio or speech tokenization or any discrete tokenization of audio signals. Therefore, it does not meet the inclusion criteria related to discrete audio/speech tokenization, nor does it align with the exclusion criteria that target audio tokenization studies.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Preventing Local Pitfalls in Vector Quantization via Optimal Transport",
    "abstract": "Vector-quantized networks (VQNs) have exhibited remarkable performance across various tasks, yet they are prone to training instability, which complicates the training process due to the necessity for techniques such as subtle initialization and model distillation. In this study, we identify the local minima issue as the primary cause of this instability. To address this, we integrate an optimal transport method in place of the nearest neighbor search to achieve a more globally informed assignment. We introduce OptVQ, a novel vector quantization method that employs the Sinkhorn algorithm to optimize the optimal transport problem, thereby enhancing the stability and efficiency of the training process. To mitigate the influence of diverse data distributions on the Sinkhorn algorithm, we implement a straightforward yet effective normalization strategy. Our comprehensive experiments on image reconstruction tasks demonstrate that OptVQ achieves 100% codebook utilization and surpasses current state-of-the-art VQNs in reconstruction quality.",
    "metadata": {
      "arxiv_id": "2412.15195",
      "title": "Preventing Local Pitfalls in Vector Quantization via Optimal Transport",
      "summary": "Vector-quantized networks (VQNs) have exhibited remarkable performance across various tasks, yet they are prone to training instability, which complicates the training process due to the necessity for techniques such as subtle initialization and model distillation. In this study, we identify the local minima issue as the primary cause of this instability. To address this, we integrate an optimal transport method in place of the nearest neighbor search to achieve a more globally informed assignment. We introduce OptVQ, a novel vector quantization method that employs the Sinkhorn algorithm to optimize the optimal transport problem, thereby enhancing the stability and efficiency of the training process. To mitigate the influence of diverse data distributions on the Sinkhorn algorithm, we implement a straightforward yet effective normalization strategy. Our comprehensive experiments on image reconstruction tasks demonstrate that OptVQ achieves 100% codebook utilization and surpasses current state-of-the-art VQNs in reconstruction quality.",
      "authors": [
        "Borui Zhang",
        "Wenzhao Zheng",
        "Jie Zhou",
        "Jiwen Lu"
      ],
      "published": "2024-12-19T18:58:14Z",
      "updated": "2024-12-19T18:58:14Z",
      "categories": [
        "cs.CV",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2412.15195v1",
      "landing_url": "https://arxiv.org/abs/2412.15195v1",
      "doi": "https://doi.org/10.48550/arXiv.2412.15195"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The abstract focuses on vector quantization for image reconstruction and does not propose, analyze, or evaluate discrete audio/speech tokenization as its core object of study, hence it fails the required inclusion criteria for discrete audio tokenization.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The abstract focuses on vector quantization for image reconstruction and does not propose, analyze, or evaluate discrete audio/speech tokenization as its core object of study, hence it fails the required inclusion criteria for discrete audio tokenization.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on vector quantization methods improving training stability and efficiency with applications demonstrated on image reconstruction tasks, without any mention of discrete audio or speech tokenization, analysis, or evaluation; thus it does not meet the inclusion criteria related to audio/speech discrete tokenization, nor does it address discrete tokenization in the audio domain as required.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on vector quantization methods improving training stability and efficiency with applications demonstrated on image reconstruction tasks, without any mention of discrete audio or speech tokenization, analysis, or evaluation; thus it does not meet the inclusion criteria related to audio/speech discrete tokenization, nor does it address discrete tokenization in the audio domain as required.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "SLAM-Omni: Timbre-Controllable Voice Interaction System with Single-Stage Training",
    "abstract": "Recent advancements highlight the potential of end-to-end real-time spoken dialogue systems, showcasing their low latency and high quality. In this paper, we introduce SLAM-Omni, a timbre-controllable, end-to-end voice interaction system with single-stage training. SLAM-Omni achieves zero-shot timbre control by modeling spoken language with semantic tokens and decoupling speaker information to a vocoder. By predicting grouped speech semantic tokens at each step, our method significantly reduces the sequence length of audio tokens, accelerating both training and inference. Additionally, we propose historical text prompting to compress dialogue history, facilitating efficient multi-round interactions. Comprehensive evaluations reveal that SLAM-Omni outperforms prior models of similar scale, requiring only 15 hours of training on 4 GPUs with limited data. Notably, it is the first spoken dialogue system to achieve competitive performance with a single-stage training approach, eliminating the need for pre-training on TTS or ASR tasks. Further experiments validate its multilingual and multi-turn dialogue capabilities on larger datasets.",
    "metadata": {
      "arxiv_id": "2412.15649",
      "title": "SLAM-Omni: Timbre-Controllable Voice Interaction System with Single-Stage Training",
      "summary": "Recent advancements highlight the potential of end-to-end real-time spoken dialogue systems, showcasing their low latency and high quality. In this paper, we introduce SLAM-Omni, a timbre-controllable, end-to-end voice interaction system with single-stage training. SLAM-Omni achieves zero-shot timbre control by modeling spoken language with semantic tokens and decoupling speaker information to a vocoder. By predicting grouped speech semantic tokens at each step, our method significantly reduces the sequence length of audio tokens, accelerating both training and inference. Additionally, we propose historical text prompting to compress dialogue history, facilitating efficient multi-round interactions. Comprehensive evaluations reveal that SLAM-Omni outperforms prior models of similar scale, requiring only 15 hours of training on 4 GPUs with limited data. Notably, it is the first spoken dialogue system to achieve competitive performance with a single-stage training approach, eliminating the need for pre-training on TTS or ASR tasks. Further experiments validate its multilingual and multi-turn dialogue capabilities on larger datasets.",
      "authors": [
        "Wenxi Chen",
        "Ziyang Ma",
        "Ruiqi Yan",
        "Yuzhe Liang",
        "Xiquan Li",
        "Ruiyang Xu",
        "Zhikang Niu",
        "Yanqiao Zhu",
        "Yifan Yang",
        "Zhanxun Liu",
        "Kai Yu",
        "Yuxuan Hu",
        "Jinyu Li",
        "Yan Lu",
        "Shujie Liu",
        "Xie Chen"
      ],
      "published": "2024-12-20T08:05:55Z",
      "updated": "2024-12-20T08:05:55Z",
      "categories": [
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2412.15649v1",
      "landing_url": "https://arxiv.org/abs/2412.15649v1",
      "doi": "https://doi.org/10.48550/arXiv.2412.15649"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The abstract explicitly centers on discrete speech tokens (semantic tokens) and a token-based vocoder workflow for end-to-end speech generation, meeting the discrete-tokenization criterion.",
      "evaluation": 5
    },
    "round-A_JuniorNano_reasoning": "The abstract explicitly centers on discrete speech tokens (semantic tokens) and a token-based vocoder workflow for end-to-end speech generation, meeting the discrete-tokenization criterion.",
    "round-A_JuniorNano_evaluation": 5,
    "round-A_JuniorMini_output": {
      "reasoning": "The study SLAM-Omni discusses modeling spoken language with semantic tokens and decoupling speaker information to a vocoder, reducing sequence length of audio tokens, which indicates the use and analysis of discrete audio tokenization as a core part of the work, fulfilling the inclusion criteria and avoiding exclusion issues; the paper also involves experimental validation and methodological contributions regarding discrete audio tokens in speech tasks.",
      "evaluation": 5
    },
    "round-A_JuniorMini_reasoning": "The study SLAM-Omni discusses modeling spoken language with semantic tokens and decoupling speaker information to a vocoder, reducing sequence length of audio tokens, which indicates the use and analysis of discrete audio tokenization as a core part of the work, fulfilling the inclusion criteria and avoiding exclusion issues; the paper also involves experimental validation and methodological contributions regarding discrete audio tokens in speech tasks.",
    "round-A_JuniorMini_evaluation": 5,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "include (junior:5)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Interleaved Speech-Text Language Models for Simple Streaming Text-to-Speech Synthesis",
    "abstract": "This paper introduces Interleaved Speech-Text Language Model (IST-LM) for zero-shot streaming Text-to-Speech (TTS). Unlike many previous approaches, IST-LM is directly trained on interleaved sequences of text and speech tokens with a fixed ratio, eliminating the need for additional efforts like forced alignment or complex designs. The ratio of text chunk size to speech chunk size is crucial for the performance of IST-LM. To explore this, we conducted a comprehensive series of statistical analyses on the training data and performed correlation analysis with the final performance, uncovering several key factors: 1) the distance between speech tokens and their corresponding text tokens, 2) the number of future text tokens accessible to each speech token, and 3) the frequency of speech tokens precedes their corresponding text tokens. Experimental results demonstrate how to achieve an optimal streaming TTS system with a limited performance gap compared to its non-streaming counterpart. IST-LM is conceptually simple and empirically powerful, enabling streaming TTS with minimal overhead while largely preserving performance, and offering broad potential for integration with real-time text streams from large language models.",
    "metadata": {
      "arxiv_id": "2412.16102",
      "title": "Interleaved Speech-Text Language Models for Simple Streaming Text-to-Speech Synthesis",
      "summary": "This paper introduces Interleaved Speech-Text Language Model (IST-LM) for zero-shot streaming Text-to-Speech (TTS). Unlike many previous approaches, IST-LM is directly trained on interleaved sequences of text and speech tokens with a fixed ratio, eliminating the need for additional efforts like forced alignment or complex designs. The ratio of text chunk size to speech chunk size is crucial for the performance of IST-LM. To explore this, we conducted a comprehensive series of statistical analyses on the training data and performed correlation analysis with the final performance, uncovering several key factors: 1) the distance between speech tokens and their corresponding text tokens, 2) the number of future text tokens accessible to each speech token, and 3) the frequency of speech tokens precedes their corresponding text tokens. Experimental results demonstrate how to achieve an optimal streaming TTS system with a limited performance gap compared to its non-streaming counterpart. IST-LM is conceptually simple and empirically powerful, enabling streaming TTS with minimal overhead while largely preserving performance, and offering broad potential for integration with real-time text streams from large language models.",
      "authors": [
        "Yifan Yang",
        "Shujie Liu",
        "Jinyu Li",
        "Hui Wang",
        "Lingwei Meng",
        "Haiyang Sun",
        "Yuzhe Liang",
        "Ziyang Ma",
        "Yuxuan Hu",
        "Rui Zhao",
        "Jianwei Yu",
        "Yan Lu",
        "Xie Chen"
      ],
      "published": "2024-12-20T17:43:50Z",
      "updated": "2025-08-09T10:01:51Z",
      "categories": [
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2412.16102v3",
      "landing_url": "https://arxiv.org/abs/2412.16102v3",
      "doi": "https://doi.org/10.48550/arXiv.2412.16102"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The paper explicitly studies discrete speech tokens interleaved with text tokens and analyzes token-level relationships for streaming TTS, treating discrete tokenization of speech as a core component, which aligns with the inclusion criteria and avoids excluded categories.",
      "evaluation": 5
    },
    "round-A_JuniorNano_reasoning": "The paper explicitly studies discrete speech tokens interleaved with text tokens and analyzes token-level relationships for streaming TTS, treating discrete tokenization of speech as a core component, which aligns with the inclusion criteria and avoids excluded categories.",
    "round-A_JuniorNano_evaluation": 5,
    "round-A_JuniorMini_output": {
      "reasoning": "The study presents a model (IST-LM) trained on interleaved sequences of text and speech tokens, suggesting discrete speech tokenization is a core object of study, with analysis on token ratios and positioning. It does not rely solely on continuous audio features and appears to have methodological contributions with experimental validation relevant to discrete audio token usage in speech synthesis. Therefore, it meets the inclusion criteria and does not meet exclusion criteria.",
      "evaluation": 5
    },
    "round-A_JuniorMini_reasoning": "The study presents a model (IST-LM) trained on interleaved sequences of text and speech tokens, suggesting discrete speech tokenization is a core object of study, with analysis on token ratios and positioning. It does not rely solely on continuous audio features and appears to have methodological contributions with experimental validation relevant to discrete audio token usage in speech synthesis. Therefore, it meets the inclusion criteria and does not meet exclusion criteria.",
    "round-A_JuniorMini_evaluation": 5,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "include (junior:5)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Mamba-SEUNet: Mamba UNet for Monaural Speech Enhancement",
    "abstract": "In recent speech enhancement (SE) research, transformer and its variants have emerged as the predominant methodologies. However, the quadratic complexity of the self-attention mechanism imposes certain limitations on practical deployment. Mamba, as a novel state-space model (SSM), has gained widespread application in natural language processing and computer vision due to its strong capabilities in modeling long sequences and relatively low computational complexity. In this work, we introduce Mamba-SEUNet, an innovative architecture that integrates Mamba with U-Net for SE tasks. By leveraging bidirectional Mamba to model forward and backward dependencies of speech signals at different resolutions, and incorporating skip connections to capture multi-scale information, our approach achieves state-of-the-art (SOTA) performance. Experimental results on the VCTK+DEMAND dataset indicate that Mamba-SEUNet attains a PESQ score of 3.59, while maintaining low computational complexity. When combined with the Perceptual Contrast Stretching technique, Mamba-SEUNet further improves the PESQ score to 3.73.",
    "metadata": {
      "arxiv_id": "2412.16626",
      "title": "Mamba-SEUNet: Mamba UNet for Monaural Speech Enhancement",
      "summary": "In recent speech enhancement (SE) research, transformer and its variants have emerged as the predominant methodologies. However, the quadratic complexity of the self-attention mechanism imposes certain limitations on practical deployment. Mamba, as a novel state-space model (SSM), has gained widespread application in natural language processing and computer vision due to its strong capabilities in modeling long sequences and relatively low computational complexity. In this work, we introduce Mamba-SEUNet, an innovative architecture that integrates Mamba with U-Net for SE tasks. By leveraging bidirectional Mamba to model forward and backward dependencies of speech signals at different resolutions, and incorporating skip connections to capture multi-scale information, our approach achieves state-of-the-art (SOTA) performance. Experimental results on the VCTK+DEMAND dataset indicate that Mamba-SEUNet attains a PESQ score of 3.59, while maintaining low computational complexity. When combined with the Perceptual Contrast Stretching technique, Mamba-SEUNet further improves the PESQ score to 3.73.",
      "authors": [
        "Junyu Wang",
        "Zizhen Lin",
        "Tianrui Wang",
        "Meng Ge",
        "Longbiao Wang",
        "Jianwu Dang"
      ],
      "published": "2024-12-21T13:43:51Z",
      "updated": "2025-01-02T10:56:07Z",
      "categories": [
        "cs.SD",
        "cs.AI",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2412.16626v2",
      "landing_url": "https://arxiv.org/abs/2412.16626v2",
      "doi": "https://doi.org/10.48550/arXiv.2412.16626"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This work focuses on a neural architecture for monaural speech enhancement using continuous representations (waveforms/spectrograms) and does not discuss discrete audio tokenization or token-based analysis, so it does not meet the inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "This work focuses on a neural architecture for monaural speech enhancement using continuous representations (waveforms/spectrograms) and does not discuss discrete audio tokenization or token-based analysis, so it does not meet the inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study describes a novel architecture for speech enhancement but does not mention discrete audio or speech tokenization as a core object of study or analysis; it focuses on modeling and enhancing continuous speech signals using state-space models without any discrete tokenization or quantization approach.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study describes a novel architecture for speech enhancement but does not mention discrete audio or speech tokenization as a core object of study or analysis; it focuses on modeling and enhancing continuous speech signals using state-space models without any discrete tokenization or quantization approach.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "KALL-E:Autoregressive Speech Synthesis with Next-Distribution Prediction",
    "abstract": "We introduce KALL-E, a novel autoregressive (AR) language model for text-to-speech (TTS) synthesis that operates by predicting the next distribution of continuous speech frames. Unlike existing methods, KALL-E directly models the continuous speech distribution conditioned on text, eliminating the need for any diffusion-based components. Specifically, we utilize a Flow-VAE to extract a continuous latent speech representation from waveforms, instead of relying on discrete speech tokens. A single AR Transformer is then trained to predict these continuous speech distributions from text, optimizing a Kullback-Leibler divergence loss as its objective. Experimental results demonstrate that KALL-E achieves superior speech synthesis quality and can even adapt to a target speaker from just a single sample. Importantly, KALL-E provides a more direct and effective approach for utilizing continuous speech representations in TTS.",
    "metadata": {
      "arxiv_id": "2412.16846",
      "title": "KALL-E:Autoregressive Speech Synthesis with Next-Distribution Prediction",
      "summary": "We introduce KALL-E, a novel autoregressive (AR) language model for text-to-speech (TTS) synthesis that operates by predicting the next distribution of continuous speech frames. Unlike existing methods, KALL-E directly models the continuous speech distribution conditioned on text, eliminating the need for any diffusion-based components. Specifically, we utilize a Flow-VAE to extract a continuous latent speech representation from waveforms, instead of relying on discrete speech tokens. A single AR Transformer is then trained to predict these continuous speech distributions from text, optimizing a Kullback-Leibler divergence loss as its objective. Experimental results demonstrate that KALL-E achieves superior speech synthesis quality and can even adapt to a target speaker from just a single sample. Importantly, KALL-E provides a more direct and effective approach for utilizing continuous speech representations in TTS.",
      "authors": [
        "Kangxiang Xia",
        "Xinfa Zhu",
        "Jixun Yao",
        "Wenjie Tian",
        "Wenhao Li",
        "Lei Xie"
      ],
      "published": "2024-12-22T04:03:24Z",
      "updated": "2025-09-17T16:01:26Z",
      "categories": [
        "eess.AS",
        "cs.CL",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2412.16846v2",
      "landing_url": "https://arxiv.org/abs/2412.16846v2",
      "doi": "https://doi.org/10.48550/arXiv.2412.16846"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The abstract uses a continuous latent speech representation via Flow-VAE and states it does not rely on discrete speech tokens, so it does not propose or evaluate discrete tokenization as a core object; thus it should be excluded under the criteria that require discrete tokenization analysis.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The abstract uses a continuous latent speech representation via Flow-VAE and states it does not rely on discrete speech tokens, so it does not propose or evaluate discrete tokenization as a core object; thus it should be excluded under the criteria that require discrete tokenization analysis.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study introduces KALL-E, which operates with continuous speech distributions and latent continuous speech representations rather than discrete tokens; it explicitly eliminates discrete tokenization by design, focusing on continuous latent spaces without discrete quantization or tokenization. Therefore, it does not meet the inclusion criterion of explicitly proposing or analyzing discrete audio/speech tokenization as a core object.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study introduces KALL-E, which operates with continuous speech distributions and latent continuous speech representations rather than discrete tokens; it explicitly eliminates discrete tokenization by design, focusing on continuous latent spaces without discrete quantization or tokenization. Therefore, it does not meet the inclusion criterion of explicitly proposing or analyzing discrete audio/speech tokenization as a core object.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Why Do Speech Language Models Fail to Generate Semantically Coherent Outputs? A Modality Evolving Perspective",
    "abstract": "Although text-based large language models exhibit human-level writing ability and remarkable intelligence, speech language models (SLMs) still struggle to generate semantically coherent outputs. There are several potential reasons for this performance degradation: (A) speech tokens mainly provide phonetic information rather than semantic information, (B) the length of speech sequences is much longer than that of text sequences, and (C) paralinguistic information, such as prosody, introduces additional complexity and variability. In this paper, we explore the influence of three key factors separately by transiting the modality from text to speech in an evolving manner. Our findings reveal that the impact of the three factors varies. Factor A has a relatively minor impact, factor B influences syntactical and semantic modeling more obviously, and factor C exerts the most significant impact, particularly in the basic lexical modeling. Based on these findings, we provide insights into the unique challenges of training SLMs and highlight pathways to develop more effective end-to-end SLMs.",
    "metadata": {
      "arxiv_id": "2412.17048",
      "title": "Why Do Speech Language Models Fail to Generate Semantically Coherent Outputs? A Modality Evolving Perspective",
      "summary": "Although text-based large language models exhibit human-level writing ability and remarkable intelligence, speech language models (SLMs) still struggle to generate semantically coherent outputs. There are several potential reasons for this performance degradation: (A) speech tokens mainly provide phonetic information rather than semantic information, (B) the length of speech sequences is much longer than that of text sequences, and (C) paralinguistic information, such as prosody, introduces additional complexity and variability. In this paper, we explore the influence of three key factors separately by transiting the modality from text to speech in an evolving manner. Our findings reveal that the impact of the three factors varies. Factor A has a relatively minor impact, factor B influences syntactical and semantic modeling more obviously, and factor C exerts the most significant impact, particularly in the basic lexical modeling. Based on these findings, we provide insights into the unique challenges of training SLMs and highlight pathways to develop more effective end-to-end SLMs.",
      "authors": [
        "Hankun Wang",
        "Haoran Wang",
        "Yiwei Guo",
        "Zhihan Li",
        "Chenpeng Du",
        "Xie Chen",
        "Kai Yu"
      ],
      "published": "2024-12-22T14:59:19Z",
      "updated": "2024-12-22T14:59:19Z",
      "categories": [
        "eess.AS",
        "cs.CL",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2412.17048v1",
      "landing_url": "https://arxiv.org/abs/2412.17048v1",
      "doi": "https://doi.org/10.48550/arXiv.2412.17048"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The abstract discusses semantically coherent outputs in speech language models and modality shifting from text to speech, but it does not analyze or propose discrete audio tokenization into finite-symbol sequences for audio reconstruction/generation, so it does not meet the inclusion criteria.",
      "evaluation": 2
    },
    "round-A_JuniorNano_reasoning": "The abstract discusses semantically coherent outputs in speech language models and modality shifting from text to speech, but it does not analyze or propose discrete audio tokenization into finite-symbol sequences for audio reconstruction/generation, so it does not meet the inclusion criteria.",
    "round-A_JuniorNano_evaluation": 2,
    "round-A_JuniorMini_output": {
      "reasoning": "The study investigates why speech language models fail to generate semantically coherent outputs by analyzing key factors affecting modeling, but it does not explicitly propose, analyze, or evaluate discrete audio/speech tokenization methods where continuous audio is converted into discrete symbols; rather, it focuses on modality challenges without detailing discrete tokenization or quantization approaches required by the inclusion criteria.",
      "evaluation": 2
    },
    "round-A_JuniorMini_reasoning": "The study investigates why speech language models fail to generate semantically coherent outputs by analyzing key factors affecting modeling, but it does not explicitly propose, analyze, or evaluate discrete audio/speech tokenization methods where continuous audio is converted into discrete symbols; rather, it focuses on modality challenges without detailing discrete tokenization or quantization approaches required by the inclusion criteria.",
    "round-A_JuniorMini_evaluation": 2,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:2)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:2)"
  },
  {
    "title": "Enhancing Reconstruction-Based Out-of-Distribution Detection in Brain MRI with Model and Metric Ensembles",
    "abstract": "Out-of-distribution (OOD) detection is crucial for safely deploying automated medical image analysis systems, as abnormal patterns in images could hamper their performance. However, OOD detection in medical imaging remains an open challenge, and we address three gaps: the underexplored potential of a simple OOD detection model, the lack of optimization of deep learning strategies specifically for OOD detection, and the selection of appropriate reconstruction metrics. In this study, we investigated the effectiveness of a reconstruction-based autoencoder for unsupervised detection of synthetic artifacts in brain MRI. We evaluated the general reconstruction capability of the model, analyzed the impact of the selected training epoch and reconstruction metrics, assessed the potential of model and/or metric ensembles, and tested the model on a dataset containing a diverse range of artifacts. Among the metrics assessed, the contrast component of SSIM and LPIPS consistently outperformed others in detecting homogeneous circular anomalies. By combining two well-converged models and using LPIPS and contrast as reconstruction metrics, we achieved a pixel-level area under the Precision-Recall curve of 0.66. Furthermore, with the more realistic OOD dataset, we observed that the detection performance varied between artifact types; local artifacts were more difficult to detect, while global artifacts showed better detection results. These findings underscore the importance of carefully selecting metrics and model configurations, and highlight the need for tailored approaches, as standard deep learning approaches do not always align with the unique needs of OOD detection.",
    "metadata": {
      "arxiv_id": "2412.17586",
      "title": "Enhancing Reconstruction-Based Out-of-Distribution Detection in Brain MRI with Model and Metric Ensembles",
      "summary": "Out-of-distribution (OOD) detection is crucial for safely deploying automated medical image analysis systems, as abnormal patterns in images could hamper their performance. However, OOD detection in medical imaging remains an open challenge, and we address three gaps: the underexplored potential of a simple OOD detection model, the lack of optimization of deep learning strategies specifically for OOD detection, and the selection of appropriate reconstruction metrics. In this study, we investigated the effectiveness of a reconstruction-based autoencoder for unsupervised detection of synthetic artifacts in brain MRI. We evaluated the general reconstruction capability of the model, analyzed the impact of the selected training epoch and reconstruction metrics, assessed the potential of model and/or metric ensembles, and tested the model on a dataset containing a diverse range of artifacts. Among the metrics assessed, the contrast component of SSIM and LPIPS consistently outperformed others in detecting homogeneous circular anomalies. By combining two well-converged models and using LPIPS and contrast as reconstruction metrics, we achieved a pixel-level area under the Precision-Recall curve of 0.66. Furthermore, with the more realistic OOD dataset, we observed that the detection performance varied between artifact types; local artifacts were more difficult to detect, while global artifacts showed better detection results. These findings underscore the importance of carefully selecting metrics and model configurations, and highlight the need for tailored approaches, as standard deep learning approaches do not always align with the unique needs of OOD detection.",
      "authors": [
        "Evi M. C. Huijben",
        "Sina Amirrajab",
        "Josien P. W. Pluim"
      ],
      "published": "2024-12-23T13:58:52Z",
      "updated": "2024-12-23T13:58:52Z",
      "categories": [
        "eess.IV",
        "cs.CV",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2412.17586v1",
      "landing_url": "https://arxiv.org/abs/2412.17586v1",
      "doi": "https://doi.org/10.1016/j.cmpb.2025.109045"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item investigates reconstruction-based OOD detection in brain MRI with model/metric ensembles and does not involve discrete audio tokenization or any speech-related token design, so it does not meet the audio-tokenization inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item investigates reconstruction-based OOD detection in brain MRI with model/metric ensembles and does not involve discrete audio tokenization or any speech-related token design, so it does not meet the audio-tokenization inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on out-of-distribution detection in brain MRI images using reconstruction-based autoencoders and does not address discrete audio/speech tokenization or convert continuous audio signals into discrete symbols, thus failing to meet the inclusion criteria related to audio tokenization.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on out-of-distribution detection in brain MRI images using reconstruction-based autoencoders and does not address discrete audio/speech tokenization or convert continuous audio signals into discrete symbols, thus failing to meet the inclusion criteria related to audio tokenization.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Hierarchical Vector Quantization for Unsupervised Action Segmentation",
    "abstract": "In this work, we address unsupervised temporal action segmentation, which segments a set of long, untrimmed videos into semantically meaningful segments that are consistent across videos. While recent approaches combine representation learning and clustering in a single step for this task, they do not cope with large variations within temporal segments of the same class. To address this limitation, we propose a novel method, termed Hierarchical Vector Quantization (HVQ), that consists of two subsequent vector quantization modules. This results in a hierarchical clustering where the additional subclusters cover the variations within a cluster. We demonstrate that our approach captures the distribution of segment lengths much better than the state of the art. To this end, we introduce a new metric based on the Jensen-Shannon Distance (JSD) for unsupervised temporal action segmentation. We evaluate our approach on three public datasets, namely Breakfast, YouTube Instructional and IKEA ASM. Our approach outperforms the state of the art in terms of F1 score, recall and JSD.",
    "metadata": {
      "arxiv_id": "2412.17640",
      "title": "Hierarchical Vector Quantization for Unsupervised Action Segmentation",
      "summary": "In this work, we address unsupervised temporal action segmentation, which segments a set of long, untrimmed videos into semantically meaningful segments that are consistent across videos. While recent approaches combine representation learning and clustering in a single step for this task, they do not cope with large variations within temporal segments of the same class. To address this limitation, we propose a novel method, termed Hierarchical Vector Quantization (HVQ), that consists of two subsequent vector quantization modules. This results in a hierarchical clustering where the additional subclusters cover the variations within a cluster. We demonstrate that our approach captures the distribution of segment lengths much better than the state of the art. To this end, we introduce a new metric based on the Jensen-Shannon Distance (JSD) for unsupervised temporal action segmentation. We evaluate our approach on three public datasets, namely Breakfast, YouTube Instructional and IKEA ASM. Our approach outperforms the state of the art in terms of F1 score, recall and JSD.",
      "authors": [
        "Federico Spurio",
        "Emad Bahrami",
        "Gianpiero Francesca",
        "Juergen Gall"
      ],
      "published": "2024-12-23T15:18:24Z",
      "updated": "2025-01-24T17:43:56Z",
      "categories": [
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2412.17640v2",
      "landing_url": "https://arxiv.org/abs/2412.17640v2",
      "doi": "https://doi.org/10.48550/arXiv.2412.17640"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The paper focuses on unsupervised video action segmentation using hierarchical vector quantization for visual features, not on discrete audio/speech tokenization as the core object of study, so it does not meet the inclusion criteria and aligns with exclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The paper focuses on unsupervised video action segmentation using hierarchical vector quantization for visual features, not on discrete audio/speech tokenization as the core object of study, so it does not meet the inclusion criteria and aligns with exclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on unsupervised temporal action segmentation for videos and introduces hierarchical vector quantization modules for clustering temporal segments; however, it does not involve discrete audio/speech tokenization or discuss turning continuous audio signals into discrete token sequences for audio reconstruction, generation, or understanding, thus failing to meet the inclusion criteria related to audio tokenization.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on unsupervised temporal action segmentation for videos and introduces hierarchical vector quantization modules for clustering temporal segments; however, it does not involve discrete audio/speech tokenization or discuss turning continuous audio signals into discrete token sequences for audio reconstruction, generation, or understanding, thus failing to meet the inclusion criteria related to audio tokenization.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Causal Speech Enhancement with Predicting Semantics based on Quantized Self-supervised Learning Features",
    "abstract": "Real-time speech enhancement (SE) is essential to online speech communication. Causal SE models use only the previous context while predicting future information, such as phoneme continuation, may help performing causal SE. The phonetic information is often represented by quantizing latent features of self-supervised learning (SSL) models. This work is the first to incorporate SSL features with causality into an SE model. The causal SSL features are encoded and combined with spectrogram features using feature-wise linear modulation to estimate a mask for enhancing the noisy input speech. Simultaneously, we quantize the causal SSL features using vector quantization to represent phonetic characteristics as semantic tokens. The model not only encodes SSL features but also predicts the future semantic tokens in multi-task learning (MTL). The experimental results using VoiceBank + DEMAND dataset show that our proposed method achieves 2.88 in PESQ, especially with semantic prediction MTL, in which we confirm that the semantic prediction played an important role in causal SE.",
    "metadata": {
      "arxiv_id": "2412.19248",
      "title": "Causal Speech Enhancement with Predicting Semantics based on Quantized Self-supervised Learning Features",
      "summary": "Real-time speech enhancement (SE) is essential to online speech communication. Causal SE models use only the previous context while predicting future information, such as phoneme continuation, may help performing causal SE. The phonetic information is often represented by quantizing latent features of self-supervised learning (SSL) models. This work is the first to incorporate SSL features with causality into an SE model. The causal SSL features are encoded and combined with spectrogram features using feature-wise linear modulation to estimate a mask for enhancing the noisy input speech. Simultaneously, we quantize the causal SSL features using vector quantization to represent phonetic characteristics as semantic tokens. The model not only encodes SSL features but also predicts the future semantic tokens in multi-task learning (MTL). The experimental results using VoiceBank + DEMAND dataset show that our proposed method achieves 2.88 in PESQ, especially with semantic prediction MTL, in which we confirm that the semantic prediction played an important role in causal SE.",
      "authors": [
        "Emiru Tsunoo",
        "Yuki Saito",
        "Wataru Nakata",
        "Hiroshi Saruwatari"
      ],
      "published": "2024-12-26T15:08:36Z",
      "updated": "2024-12-26T15:08:36Z",
      "categories": [
        "eess.AS",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2412.19248v1",
      "landing_url": "https://arxiv.org/abs/2412.19248v1",
      "doi": "https://doi.org/10.48550/arXiv.2412.19248"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "Explicit use of discrete audio tokenization via vector quantization of SSL features to represent phonetic semantics and to predict future semantic tokens, aligning with the inclusion criterion; no exclusion criteria are triggered.",
      "evaluation": 5
    },
    "round-A_JuniorNano_reasoning": "Explicit use of discrete audio tokenization via vector quantization of SSL features to represent phonetic semantics and to predict future semantic tokens, aligning with the inclusion criterion; no exclusion criteria are triggered.",
    "round-A_JuniorNano_evaluation": 5,
    "round-A_JuniorMini_output": {
      "reasoning": "The study explicitly incorporates discrete quantization of self-supervised learning features representing phonetic characteristics as semantic tokens, which are central to the causal speech enhancement model. It focuses on the analysis and use of quantized SSL features (discrete tokens) in speech processing, fulfilling the inclusion criteria and avoiding the exclusion criteria related to continuous features only or lack of discrete token analysis.",
      "evaluation": 5
    },
    "round-A_JuniorMini_reasoning": "The study explicitly incorporates discrete quantization of self-supervised learning features representing phonetic characteristics as semantic tokens, which are central to the causal speech enhancement model. It focuses on the analysis and use of quantized SSL features (discrete tokens) in speech processing, fulfilling the inclusion criteria and avoiding the exclusion criteria related to continuous features only or lack of discrete token analysis.",
    "round-A_JuniorMini_evaluation": 5,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "include (junior:5)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "FED: Fast and Efficient Dataset Deduplication Framework with GPU Acceleration",
    "abstract": "Dataset deduplication plays a crucial role in enhancing data quality, ultimately improving the training performance and efficiency of large language models. A commonly used method for data deduplication is the MinHash LSH algorithm. Recently, NVIDIA introduced a GPU-based MinHash LSH deduplication method, but it remains suboptimal, leaving room for further improvement in processing efficiency. This paper proposes a GPU-accelerated deduplication framework, FED, that optimizes MinHash LSH for GPU clusters and leverages computationally efficient, partially reusable non-cryptographic hash functions. FED significantly outperforms the CPU-based deduplication tool in SlimPajama (using 64 logical CPU cores) by up to 107.2 times and the GPU-based tool in NVIDIA NeMo Curator by up to 6.3 times when processing 30 million documents on a node with four GPUs. Notably, our method dramatically accelerates the previously time-consuming MinHash signature generation phase, achieving speed-ups of up to 260 compared to the CPU baseline. Despite these gains in efficiency, FED maintains high deduplication quality, with the duplicate document sets reaching a Jaccard similarity of over 0.96 compared to those identified by the standard MinHash algorithm. In large-scale experiments, the deduplication of 1.2 trillion tokens is completed in just 6 hours in a four-node, 16-GPU environment. The related code is publicly available on GitHub (\\href{https://github.com/mcrl/FED}{https://github.com/mcrl/FED}).",
    "metadata": {
      "arxiv_id": "2501.01046",
      "title": "FED: Fast and Efficient Dataset Deduplication Framework with GPU Acceleration",
      "summary": "Dataset deduplication plays a crucial role in enhancing data quality, ultimately improving the training performance and efficiency of large language models. A commonly used method for data deduplication is the MinHash LSH algorithm. Recently, NVIDIA introduced a GPU-based MinHash LSH deduplication method, but it remains suboptimal, leaving room for further improvement in processing efficiency. This paper proposes a GPU-accelerated deduplication framework, FED, that optimizes MinHash LSH for GPU clusters and leverages computationally efficient, partially reusable non-cryptographic hash functions. FED significantly outperforms the CPU-based deduplication tool in SlimPajama (using 64 logical CPU cores) by up to 107.2 times and the GPU-based tool in NVIDIA NeMo Curator by up to 6.3 times when processing 30 million documents on a node with four GPUs. Notably, our method dramatically accelerates the previously time-consuming MinHash signature generation phase, achieving speed-ups of up to 260 compared to the CPU baseline. Despite these gains in efficiency, FED maintains high deduplication quality, with the duplicate document sets reaching a Jaccard similarity of over 0.96 compared to those identified by the standard MinHash algorithm. In large-scale experiments, the deduplication of 1.2 trillion tokens is completed in just 6 hours in a four-node, 16-GPU environment. The related code is publicly available on GitHub (\\href{https://github.com/mcrl/FED}{https://github.com/mcrl/FED}).",
      "authors": [
        "Youngjun Son",
        "Chaewon Kim",
        "Jaejin Lee"
      ],
      "published": "2025-01-02T04:11:23Z",
      "updated": "2025-03-12T13:36:32Z",
      "categories": [
        "cs.CL"
      ],
      "pdf_url": "https://arxiv.org/pdf/2501.01046v3",
      "landing_url": "https://arxiv.org/abs/2501.01046v3",
      "doi": "https://doi.org/10.48550/arXiv.2501.01046"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item concerns GPU-accelerated dataset deduplication using MinHash LSH, with no discrete audio/speech tokenization analysis or discrete token design discussed, hence it does not meet the inclusion criteria focused on audio tokenization research.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item concerns GPU-accelerated dataset deduplication using MinHash LSH, with no discrete audio/speech tokenization analysis or discrete token design discussed, hence it does not meet the inclusion criteria focused on audio tokenization research.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on dataset deduplication using GPU acceleration and MinHash LSH algorithms to improve processing efficiency for large datasets; however, it does not address discrete audio/speech tokenization or the conversion of continuous audio signals into discrete token sequences, which is central to the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on dataset deduplication using GPU acceleration and MinHash LSH algorithms to improve processing efficiency for large datasets; however, it does not address discrete audio/speech tokenization or the conversion of continuous audio signals into discrete token sequences, which is central to the inclusion criteria.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Exploiting Latent Properties to Optimize Neural Codecs",
    "abstract": "End-to-end image and video codecs are becoming increasingly competitive, compared to traditional compression techniques that have been developed through decades of manual engineering efforts. These trainable codecs have many advantages over traditional techniques, such as their straightforward adaptation to perceptual distortion metrics and high performance in specific fields thanks to their learning ability. However, current state-of-the-art neural codecs do not fully exploit the benefits of vector quantization and the existence of the entropy gradient in decoding devices. In this paper, we propose to leverage these two properties (vector quantization and entropy gradient) to improve the performance of off-the-shelf codecs. Firstly, we demonstrate that using non-uniform scalar quantization cannot improve performance over uniform quantization. We thus suggest using predefined optimal uniform vector quantization to improve performance. Secondly, we show that the entropy gradient, available at the decoder, is correlated with the reconstruction error gradient, which is not available at the decoder. We therefore use the former as a proxy to enhance compression performance. Our experimental results show that these approaches save between 1 to 3% of the rate for the same quality across various pretrained methods. In addition, the entropy gradient based solution improves traditional codec performance significantly as well.",
    "metadata": {
      "arxiv_id": "2501.01231",
      "title": "Exploiting Latent Properties to Optimize Neural Codecs",
      "summary": "End-to-end image and video codecs are becoming increasingly competitive, compared to traditional compression techniques that have been developed through decades of manual engineering efforts. These trainable codecs have many advantages over traditional techniques, such as their straightforward adaptation to perceptual distortion metrics and high performance in specific fields thanks to their learning ability. However, current state-of-the-art neural codecs do not fully exploit the benefits of vector quantization and the existence of the entropy gradient in decoding devices. In this paper, we propose to leverage these two properties (vector quantization and entropy gradient) to improve the performance of off-the-shelf codecs. Firstly, we demonstrate that using non-uniform scalar quantization cannot improve performance over uniform quantization. We thus suggest using predefined optimal uniform vector quantization to improve performance. Secondly, we show that the entropy gradient, available at the decoder, is correlated with the reconstruction error gradient, which is not available at the decoder. We therefore use the former as a proxy to enhance compression performance. Our experimental results show that these approaches save between 1 to 3% of the rate for the same quality across various pretrained methods. In addition, the entropy gradient based solution improves traditional codec performance significantly as well.",
      "authors": [
        "Muhammet Balcilar",
        "Bharath Bhushan Damodaran",
        "Karam Naser",
        "Franck Galpin",
        "Pierre Hellier"
      ],
      "published": "2025-01-02T12:45:31Z",
      "updated": "2025-01-02T12:45:31Z",
      "categories": [
        "cs.CV",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2501.01231v1",
      "landing_url": "https://arxiv.org/abs/2501.01231v1",
      "doi": "https://doi.org/10.1109/TIP.2024.352281"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The abstract discusses neural image/video codecs and vector quantization; it does not propose or analyze discrete audio/speech tokenization or discrete token representations for audio reconstruction, so it fails the inclusion criterion and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The abstract discusses neural image/video codecs and vector quantization; it does not propose or analyze discrete audio/speech tokenization or discrete token representations for audio reconstruction, so it fails the inclusion criterion and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on optimizing neural codecs for image and video compression, discussing vector quantization and entropy gradient benefits, with no indication of dealing with discrete audio/speech tokenization or audio signals; thus, it does not meet the inclusion criteria and falls outside the scope related to audio tokenization.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on optimizing neural codecs for image and video compression, discussing vector quantization and entropy gradient benefits, with no indication of dealing with discrete audio/speech tokenization or audio signals; thus, it does not meet the inclusion criteria and falls outside the scope related to audio tokenization.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Optimizing Audio Compression Through Entropy-Controlled Dithering",
    "abstract": "This paper explores entropy-controlled dithering techniques in audio compression, examining the application of standard and modified TPDFs, combined with noise shaping and entropy-controlled parameters, across various audio contexts, including pitch, loudness, rhythm, and instrumentation variations. Perceptual quality metrics such as VISQOL and STOI were used to evaluate performance. The results demonstrate that TPDF-based dithering consistently outperforms RPDF, particularly under optimal alpha conditions, while highlighting performance variability based on signal characteristics. These findings suggest the situational appropriateness of using various TPDF distributions. This work emphasizes the trade-off between entropy and perceptual fidelity, offering insights into the potential of entropy-controlled dithering as a foundation for enhanced audio compression algorithms. A practical implementation as a Digital Audio Workstation plugin introduces customizable dithering controls, laying the groundwork for future advancements in audio compression algorithms.",
    "metadata": {
      "arxiv_id": "2501.02293",
      "title": "Optimizing Audio Compression Through Entropy-Controlled Dithering",
      "summary": "This paper explores entropy-controlled dithering techniques in audio compression, examining the application of standard and modified TPDFs, combined with noise shaping and entropy-controlled parameters, across various audio contexts, including pitch, loudness, rhythm, and instrumentation variations. Perceptual quality metrics such as VISQOL and STOI were used to evaluate performance. The results demonstrate that TPDF-based dithering consistently outperforms RPDF, particularly under optimal alpha conditions, while highlighting performance variability based on signal characteristics. These findings suggest the situational appropriateness of using various TPDF distributions. This work emphasizes the trade-off between entropy and perceptual fidelity, offering insights into the potential of entropy-controlled dithering as a foundation for enhanced audio compression algorithms. A practical implementation as a Digital Audio Workstation plugin introduces customizable dithering controls, laying the groundwork for future advancements in audio compression algorithms.",
      "authors": [
        "Ellison Murray",
        "Morriel Kasher",
        "Predrag Spasojevic"
      ],
      "published": "2025-01-04T14:03:56Z",
      "updated": "2025-01-09T20:11:27Z",
      "categories": [
        "eess.AS",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2501.02293v2",
      "landing_url": "https://arxiv.org/abs/2501.02293v2",
      "doi": "https://doi.org/10.48550/arXiv.2501.02293"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This item discusses entropy-controlled dithering in audio compression and perceptual metrics on continuous signals, with no explicit analysis or proposal of discrete audio tokenization into a finite vocabulary for reconstruction or language-model-style tasks, so it does not meet the discrete-token criteria and should be excluded.",
      "evaluation": 2
    },
    "round-A_JuniorNano_reasoning": "This item discusses entropy-controlled dithering in audio compression and perceptual metrics on continuous signals, with no explicit analysis or proposal of discrete audio tokenization into a finite vocabulary for reconstruction or language-model-style tasks, so it does not meet the discrete-token criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 2,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper focuses on entropy-controlled dithering in audio compression and evaluates perceptual quality metrics but does not explicitly address discrete audio/speech tokenization or the conversion of continuous audio signals into sequences of discrete tokens for reconstruction, generation, or understanding tasks, which is central to the inclusion criteria. Moreover, the work emphasizes traditional signal processing aspects and does not indicate neural encoding or modern discrete token representations relevant to language-model-style token sequences.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper focuses on entropy-controlled dithering in audio compression and evaluates perceptual quality metrics but does not explicitly address discrete audio/speech tokenization or the conversion of continuous audio signals into sequences of discrete tokens for reconstruction, generation, or understanding tasks, which is central to the inclusion criteria. Moreover, the work emphasizes traditional signal processing aspects and does not indicate neural encoding or modern discrete token representations relevant to language-model-style token sequences.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:2)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:2)"
  },
  {
    "title": "PM-Dedup: Secure Deduplication with Partial Migration from Cloud to Edge Servers",
    "abstract": "Currently, an increasing number of users and enterprises are storing their data in the cloud but do not fully trust cloud providers with their data in plaintext form. To address this concern, they encrypt their data before uploading it to the cloud. However, encryption with different keys means that even identical data will become different ciphertexts, making deduplication less effective. Encrypted deduplication avoids this issue by ensuring that identical data chunks generate the same ciphertext with content-based keys, enabling the cloud to efficiently identify and remove duplicates even in encrypted form. Current encrypted data deduplication work can be classified into two types: target-based and source-based. Target-based encrypted deduplication requires clients to upload all encrypted chunks (the basic unit of deduplication) to the cloud with high network bandwidth overhead. Source-based deduplication involves clients uploading fingerprints (hashes) of encrypted chunks for duplicate checking and only uploading unique encrypted chunks, which reduces network transfer but introduces high latency and potential side-channel attacks, which need to be mitigated by Proof of Ownership (PoW), and high computing overhead of the cloud. So, reducing the latency and the overheads of network and cloud while ensuring security has become a significant challenge for secure data deduplication in cloud storage. In response to this challenge, we present PM-Dedup, a novel secure source-based deduplication approach that relocates a portion of the deduplication checking process and PoW tasks from the cloud to the trusted execution environments (TEEs) in the client-side edge servers. We also propose various designs to enhance the security and efficiency of data deduplication.",
    "metadata": {
      "arxiv_id": "2501.02350",
      "title": "PM-Dedup: Secure Deduplication with Partial Migration from Cloud to Edge Servers",
      "summary": "Currently, an increasing number of users and enterprises are storing their data in the cloud but do not fully trust cloud providers with their data in plaintext form. To address this concern, they encrypt their data before uploading it to the cloud. However, encryption with different keys means that even identical data will become different ciphertexts, making deduplication less effective. Encrypted deduplication avoids this issue by ensuring that identical data chunks generate the same ciphertext with content-based keys, enabling the cloud to efficiently identify and remove duplicates even in encrypted form. Current encrypted data deduplication work can be classified into two types: target-based and source-based. Target-based encrypted deduplication requires clients to upload all encrypted chunks (the basic unit of deduplication) to the cloud with high network bandwidth overhead. Source-based deduplication involves clients uploading fingerprints (hashes) of encrypted chunks for duplicate checking and only uploading unique encrypted chunks, which reduces network transfer but introduces high latency and potential side-channel attacks, which need to be mitigated by Proof of Ownership (PoW), and high computing overhead of the cloud. So, reducing the latency and the overheads of network and cloud while ensuring security has become a significant challenge for secure data deduplication in cloud storage. In response to this challenge, we present PM-Dedup, a novel secure source-based deduplication approach that relocates a portion of the deduplication checking process and PoW tasks from the cloud to the trusted execution environments (TEEs) in the client-side edge servers. We also propose various designs to enhance the security and efficiency of data deduplication.",
      "authors": [
        "Zhaokang Ke",
        "Haoyu Gong",
        "David H. C. Du"
      ],
      "published": "2025-01-04T18:12:23Z",
      "updated": "2025-01-04T18:12:23Z",
      "categories": [
        "cs.CR",
        "cs.NI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2501.02350v1",
      "landing_url": "https://arxiv.org/abs/2501.02350v1",
      "doi": "https://doi.org/10.48550/arXiv.2501.02350"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The article discusses secure data deduplication (PM-Dedup) and edge-based trusted execution environments; it does not address discrete audio/speech tokenization or token-based representations, so it does not meet the inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The article discusses secure data deduplication (PM-Dedup) and edge-based trusted execution environments; it does not address discrete audio/speech tokenization or token-based representations, so it does not meet the inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on secure data deduplication methods involving cloud and edge servers and does not address discrete audio/speech tokenization or analyze discrete tokens for audio reconstruction or understanding; thus, it does not meet any inclusion criteria related to discrete audio tokens.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on secure data deduplication methods involving cloud and edge servers and does not address discrete audio/speech tokenization or analyze discrete tokens for audio reconstruction or understanding; thus, it does not meet any inclusion criteria related to discrete audio tokens.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Phone-purity Guided Discrete Tokens for Dysarthric Speech Recognition",
    "abstract": "Discrete tokens extracted provide efficient and domain adaptable speech features. Their application to disordered speech that exhibits articulation imprecision and large mismatch against normal voice remains unexplored. To improve their phonetic discrimination that is weakened during unsupervised K-means or vector quantization of continuous features, this paper proposes novel phone-purity guided (PPG) discrete tokens for dysarthric speech recognition. Phonetic label supervision is used to regularize maximum likelihood and reconstruction error costs used in standard K-means and VAE-VQ based discrete token extraction. Experiments conducted on the UASpeech corpus suggest that the proposed PPG discrete token features extracted from HuBERT consistently outperform hybrid TDNN and End-to-End (E2E) Conformer systems using non-PPG based K-means or VAE-VQ tokens across varying codebook sizes by statistically significant word error rate (WER) reductions up to 0.99\\% and 1.77\\% absolute (3.21\\% and 4.82\\% relative) respectively on the UASpeech test set of 16 dysarthric speakers. The lowest WER of 23.25\\% was obtained by combining systems using different token features. Consistent improvements on the phone purity metric were also achieved. T-SNE visualization further demonstrates sharper decision boundaries were produced between K-means/VAE-VQ clusters after introducing phone-purity guidance.",
    "metadata": {
      "arxiv_id": "2501.04379",
      "title": "Phone-purity Guided Discrete Tokens for Dysarthric Speech Recognition",
      "summary": "Discrete tokens extracted provide efficient and domain adaptable speech features. Their application to disordered speech that exhibits articulation imprecision and large mismatch against normal voice remains unexplored. To improve their phonetic discrimination that is weakened during unsupervised K-means or vector quantization of continuous features, this paper proposes novel phone-purity guided (PPG) discrete tokens for dysarthric speech recognition. Phonetic label supervision is used to regularize maximum likelihood and reconstruction error costs used in standard K-means and VAE-VQ based discrete token extraction. Experiments conducted on the UASpeech corpus suggest that the proposed PPG discrete token features extracted from HuBERT consistently outperform hybrid TDNN and End-to-End (E2E) Conformer systems using non-PPG based K-means or VAE-VQ tokens across varying codebook sizes by statistically significant word error rate (WER) reductions up to 0.99\\% and 1.77\\% absolute (3.21\\% and 4.82\\% relative) respectively on the UASpeech test set of 16 dysarthric speakers. The lowest WER of 23.25\\% was obtained by combining systems using different token features. Consistent improvements on the phone purity metric were also achieved. T-SNE visualization further demonstrates sharper decision boundaries were produced between K-means/VAE-VQ clusters after introducing phone-purity guidance.",
      "authors": [
        "Huimeng Wang",
        "Xurong Xie",
        "Mengzhe Geng",
        "Shujie Hu",
        "Haoning Xu",
        "Youjun Chen",
        "Zhaoqing Li",
        "Jiajun Deng",
        "Xunying Liu"
      ],
      "published": "2025-01-08T09:45:14Z",
      "updated": "2025-01-08T09:45:14Z",
      "categories": [
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2501.04379v1",
      "landing_url": "https://arxiv.org/abs/2501.04379v1",
      "doi": "https://doi.org/10.48550/arXiv.2501.04379"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The abstract centers on discrete token representations (PPG discrete tokens) for dysarthric speech, with analysis and experiments using K-means/VAE-VQ tokenization and showing WER improvements, thus directly addressing discrete audio/tokenization as the core object.",
      "evaluation": 5
    },
    "round-A_JuniorNano_reasoning": "The abstract centers on discrete token representations (PPG discrete tokens) for dysarthric speech, with analysis and experiments using K-means/VAE-VQ tokenization and showing WER improvements, thus directly addressing discrete audio/tokenization as the core object.",
    "round-A_JuniorNano_evaluation": 5,
    "round-A_JuniorMini_output": {
      "reasoning": "The study explicitly proposes and evaluates discrete tokenization methods (phone-purity guided discrete tokens) for dysarthric speech recognition by converting continuous audio features into discrete symbols and demonstrates improvements in word error rate and phonetic discrimination, meeting all inclusion criteria and avoiding any exclusion criteria.",
      "evaluation": 5
    },
    "round-A_JuniorMini_reasoning": "The study explicitly proposes and evaluates discrete tokenization methods (phone-purity guided discrete tokens) for dysarthric speech recognition by converting continuous audio features into discrete symbols and demonstrates improvements in word error rate and phonetic discrimination, meeting all inclusion criteria and avoiding any exclusion criteria.",
    "round-A_JuniorMini_evaluation": 5,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "include (junior:5)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Popcorn: Accelerating Kernel K-means on GPUs through Sparse Linear Algebra",
    "abstract": "K-means is a popular clustering algorithm with significant applications in numerous scientific and engineering areas. One drawback of K-means is its inability to identify non-linearly separable clusters, which may lead to inaccurate solutions in certain cases. Kernel K-means is a variant of classical K-means that can find non-linearly separable clusters. However, it scales quadratically with respect to the size of the dataset, taking several minutes to cluster even medium-sized datasets on traditional CPU-based machines. In this paper, we present a formulation of Kernel K-means using sparse-dense matrix multiplication (SpMM) and sparse matrix-vector multiplication (SpMV), and we show that our formulation enables the rapid implementation of a fast GPU-based version of Kernel K-means with little programming effort. Our implementation, named Popcorn, is the first open-source GPU-based implementation of Kernel K-means. Popcorn achieves a speedup of up to 123.8x over a CPU implementation of Kernel K-means and a speedup of up to 2.6x over a GPU implementation of Kernel K-means that does not use sparse matrix computations. Our results support the effectiveness of sparse matrices as tools for efficient parallel programming.",
    "metadata": {
      "arxiv_id": "2501.05587",
      "title": "Popcorn: Accelerating Kernel K-means on GPUs through Sparse Linear Algebra",
      "summary": "K-means is a popular clustering algorithm with significant applications in numerous scientific and engineering areas. One drawback of K-means is its inability to identify non-linearly separable clusters, which may lead to inaccurate solutions in certain cases. Kernel K-means is a variant of classical K-means that can find non-linearly separable clusters. However, it scales quadratically with respect to the size of the dataset, taking several minutes to cluster even medium-sized datasets on traditional CPU-based machines. In this paper, we present a formulation of Kernel K-means using sparse-dense matrix multiplication (SpMM) and sparse matrix-vector multiplication (SpMV), and we show that our formulation enables the rapid implementation of a fast GPU-based version of Kernel K-means with little programming effort. Our implementation, named Popcorn, is the first open-source GPU-based implementation of Kernel K-means. Popcorn achieves a speedup of up to 123.8x over a CPU implementation of Kernel K-means and a speedup of up to 2.6x over a GPU implementation of Kernel K-means that does not use sparse matrix computations. Our results support the effectiveness of sparse matrices as tools for efficient parallel programming.",
      "authors": [
        "Julian Bellavita",
        "Thomas Pasquali",
        "Laura Del Rio Martin",
        "Flavio Vella",
        "Giulia Guidi"
      ],
      "published": "2025-01-09T21:43:16Z",
      "updated": "2025-01-09T21:43:16Z",
      "categories": [
        "cs.DC"
      ],
      "pdf_url": "https://arxiv.org/pdf/2501.05587v1",
      "landing_url": "https://arxiv.org/abs/2501.05587v1",
      "doi": "https://doi.org/10.1145/3710848.3710887"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item does not discuss discrete audio/speech tokenization; it's about Kernel K-means acceleration on GPUs, unrelated to speech tokenization.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item does not discuss discrete audio/speech tokenization; it's about Kernel K-means acceleration on GPUs, unrelated to speech tokenization.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on accelerating Kernel K-means clustering on GPUs using sparse linear algebra techniques but does not address discrete audio/speech tokenization or any related analysis of converting continuous audio signals into discrete token sequences as required by the inclusion criteria. Therefore, it does not meet the inclusion criteria and falls outside the scope defined for the study selection.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on accelerating Kernel K-means clustering on GPUs using sparse linear algebra techniques but does not address discrete audio/speech tokenization or any related analysis of converting continuous audio signals into discrete token sequences as required by the inclusion criteria. Therefore, it does not meet the inclusion criteria and falls outside the scope defined for the study selection.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "MARS6: A Small and Robust Hierarchical-Codec Text-to-Speech Model",
    "abstract": "Codec-based text-to-speech (TTS) models have shown impressive quality with zero-shot voice cloning abilities. However, they often struggle with more expressive references or complex text inputs. We present MARS6, a robust encoder-decoder transformer for rapid, expressive TTS. MARS6 is built on recent improvements in spoken language modelling. Utilizing a hierarchical setup for its decoder, new speech tokens are processed at a rate of only 12 Hz, enabling efficient modelling of long-form text while retaining reconstruction quality. We combine several recent training and inference techniques to reduce repetitive generation and improve output stability and quality. This enables the 70M-parameter MARS6 to achieve similar performance to models many times larger. We show this in objective and subjective evaluations, comparing TTS output quality and reference speaker cloning ability. Project page: https://camb-ai.github.io/mars6-turbo/",
    "metadata": {
      "arxiv_id": "2501.05787",
      "title": "MARS6: A Small and Robust Hierarchical-Codec Text-to-Speech Model",
      "summary": "Codec-based text-to-speech (TTS) models have shown impressive quality with zero-shot voice cloning abilities. However, they often struggle with more expressive references or complex text inputs. We present MARS6, a robust encoder-decoder transformer for rapid, expressive TTS. MARS6 is built on recent improvements in spoken language modelling. Utilizing a hierarchical setup for its decoder, new speech tokens are processed at a rate of only 12 Hz, enabling efficient modelling of long-form text while retaining reconstruction quality. We combine several recent training and inference techniques to reduce repetitive generation and improve output stability and quality. This enables the 70M-parameter MARS6 to achieve similar performance to models many times larger. We show this in objective and subjective evaluations, comparing TTS output quality and reference speaker cloning ability. Project page: https://camb-ai.github.io/mars6-turbo/",
      "authors": [
        "Matthew Baas",
        "Pieter Scholtz",
        "Arnav Mehta",
        "Elliott Dyson",
        "Akshat Prakash",
        "Herman Kamper"
      ],
      "published": "2025-01-10T08:41:42Z",
      "updated": "2025-01-10T08:41:42Z",
      "categories": [
        "eess.AS",
        "cs.CL"
      ],
      "pdf_url": "https://arxiv.org/pdf/2501.05787v1",
      "landing_url": "https://arxiv.org/abs/2501.05787v1",
      "doi": "https://doi.org/10.48550/arXiv.2501.05787"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The abstract describes a codec-based TTS model that uses ‘speech tokens’ and a hierarchical decoder, indicating discrete speech tokenization as a central component; this aligns with the inclusion criterion, though the abstract does not provide explicit methodological analysis of the tokenization itself, so it is not absolutely certain from the abstract alone.",
      "evaluation": 4
    },
    "round-A_JuniorNano_reasoning": "The abstract describes a codec-based TTS model that uses ‘speech tokens’ and a hierarchical decoder, indicating discrete speech tokenization as a central component; this aligns with the inclusion criterion, though the abstract does not provide explicit methodological analysis of the tokenization itself, so it is not absolutely certain from the abstract alone.",
    "round-A_JuniorNano_evaluation": 4,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on a codec-based text-to-speech model with a hierarchical decoding approach involving discrete speech tokens, indicating it addresses discrete audio tokenization for speech reconstruction and generation. It compares performance and speaker cloning ability, suggesting experimental validation. Although it does not explicitly detail tokenization design in the abstract, the emphasis on codec-based TTS and speech tokens aligns with the inclusion criteria and is not excluded by the given criteria.",
      "evaluation": 4
    },
    "round-A_JuniorMini_reasoning": "The study focuses on a codec-based text-to-speech model with a hierarchical decoding approach involving discrete speech tokens, indicating it addresses discrete audio tokenization for speech reconstruction and generation. It compares performance and speaker cloning ability, suggesting experimental validation. Although it does not explicitly detail tokenization design in the abstract, the emphasis on codec-based TTS and speech tokens aligns with the inclusion criteria and is not excluded by the given criteria.",
    "round-A_JuniorMini_evaluation": 4,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "include (junior:4)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "MEt3R: Measuring Multi-View Consistency in Generated Images",
    "abstract": "We introduce MEt3R, a metric for multi-view consistency in generated images. Large-scale generative models for multi-view image generation are rapidly advancing the field of 3D inference from sparse observations. However, due to the nature of generative modeling, traditional reconstruction metrics are not suitable to measure the quality of generated outputs and metrics that are independent of the sampling procedure are desperately needed. In this work, we specifically address the aspect of consistency between generated multi-view images, which can be evaluated independently of the specific scene. Our approach uses DUSt3R to obtain dense 3D reconstructions from image pairs in a feed-forward manner, which are used to warp image contents from one view into the other. Then, feature maps of these images are compared to obtain a similarity score that is invariant to view-dependent effects. Using MEt3R, we evaluate the consistency of a large set of previous methods for novel view and video generation, including our open, multi-view latent diffusion model.",
    "metadata": {
      "arxiv_id": "2501.06336",
      "title": "MEt3R: Measuring Multi-View Consistency in Generated Images",
      "summary": "We introduce MEt3R, a metric for multi-view consistency in generated images. Large-scale generative models for multi-view image generation are rapidly advancing the field of 3D inference from sparse observations. However, due to the nature of generative modeling, traditional reconstruction metrics are not suitable to measure the quality of generated outputs and metrics that are independent of the sampling procedure are desperately needed. In this work, we specifically address the aspect of consistency between generated multi-view images, which can be evaluated independently of the specific scene. Our approach uses DUSt3R to obtain dense 3D reconstructions from image pairs in a feed-forward manner, which are used to warp image contents from one view into the other. Then, feature maps of these images are compared to obtain a similarity score that is invariant to view-dependent effects. Using MEt3R, we evaluate the consistency of a large set of previous methods for novel view and video generation, including our open, multi-view latent diffusion model.",
      "authors": [
        "Mohammad Asim",
        "Christopher Wewer",
        "Thomas Wimmer",
        "Bernt Schiele",
        "Jan Eric Lenssen"
      ],
      "published": "2025-01-10T20:43:33Z",
      "updated": "2025-01-10T20:43:33Z",
      "categories": [
        "cs.CV",
        "cs.LG",
        "eess.IV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2501.06336v1",
      "landing_url": "https://arxiv.org/abs/2501.06336v1",
      "doi": "https://doi.org/10.48550/arXiv.2501.06336"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item addresses a vision-domain metric for multi-view consistency in generated images and discusses image-based 3D reconstructions, with no discrete audio/speech tokenization or analysis of discrete audio representations, so it does not meet the audio-tokenization criterion.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item addresses a vision-domain metric for multi-view consistency in generated images and discusses image-based 3D reconstructions, with no discrete audio/speech tokenization or analysis of discrete audio representations, so it does not meet the audio-tokenization criterion.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on measuring multi-view consistency in generated images and does not discuss discrete audio or speech tokenization methods; thus, it does not meet the inclusion criteria related to discrete audio tokenization, nor does it fit any of the exclusion criteria specifically related to audio tokenization studies.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on measuring multi-view consistency in generated images and does not discuss discrete audio or speech tokenization methods; thus, it does not meet the inclusion criteria related to discrete audio tokenization, nor does it fit any of the exclusion criteria specifically related to audio tokenization studies.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Neural Codec Source Tracing: Toward Comprehensive Attribution in Open-Set Condition",
    "abstract": "Current research in audio deepfake detection is gradually transitioning from binary classification to multi-class tasks, referred as audio deepfake source tracing task. However, existing studies on source tracing consider only closed-set scenarios and have not considered the challenges posed by open-set conditions. In this paper, we define the Neural Codec Source Tracing (NCST) task, which is capable of performing open-set neural codec classification and interpretable ALM detection. Specifically, we constructed the ST-Codecfake dataset for the NCST task, which includes bilingual audio samples generated by 11 state-of-the-art neural codec methods and ALM-based out-ofdistribution (OOD) test samples. Furthermore, we establish a comprehensive source tracing benchmark to assess NCST models in open-set conditions. The experimental results reveal that although the NCST models perform well in in-distribution (ID) classification and OOD detection, they lack robustness in classifying unseen real audio. The ST-codecfake dataset and code are available.",
    "metadata": {
      "arxiv_id": "2501.06514",
      "title": "Neural Codec Source Tracing: Toward Comprehensive Attribution in Open-Set Condition",
      "summary": "Current research in audio deepfake detection is gradually transitioning from binary classification to multi-class tasks, referred as audio deepfake source tracing task. However, existing studies on source tracing consider only closed-set scenarios and have not considered the challenges posed by open-set conditions. In this paper, we define the Neural Codec Source Tracing (NCST) task, which is capable of performing open-set neural codec classification and interpretable ALM detection. Specifically, we constructed the ST-Codecfake dataset for the NCST task, which includes bilingual audio samples generated by 11 state-of-the-art neural codec methods and ALM-based out-ofdistribution (OOD) test samples. Furthermore, we establish a comprehensive source tracing benchmark to assess NCST models in open-set conditions. The experimental results reveal that although the NCST models perform well in in-distribution (ID) classification and OOD detection, they lack robustness in classifying unseen real audio. The ST-codecfake dataset and code are available.",
      "authors": [
        "Yuankun Xie",
        "Xiaopeng Wang",
        "Zhiyong Wang",
        "Ruibo Fu",
        "Zhengqi Wen",
        "Songjun Cao",
        "Long Ma",
        "Chenxing Li",
        "Haonnan Cheng",
        "Long Ye"
      ],
      "published": "2025-01-11T11:15:58Z",
      "updated": "2025-01-11T11:15:58Z",
      "categories": [
        "cs.SD",
        "cs.AI",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2501.06514v1",
      "landing_url": "https://arxiv.org/abs/2501.06514v1",
      "doi": "https://doi.org/10.48550/arXiv.2501.06514"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The abstract discusses open-set neural codec classification for audio source tracing but does not explicitly address discrete audio tokenization into a finite-symbol vocabulary for reconstruction, generation, or understanding, so it does not meet the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The abstract discusses open-set neural codec classification for audio source tracing but does not explicitly address discrete audio tokenization into a finite-symbol vocabulary for reconstruction, generation, or understanding, so it does not meet the inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study proposes Neural Codec Source Tracing (NCST) targeting open-set neural codec classification, constructed a dataset involving neural codec methods, and evaluates models for audio deepfake source tracing; however, it does not explicitly discuss discrete audio/speech tokenization or conversion of continuous signals into discrete token sequences as its core focus, suggesting it mainly deals with codec classification and detection rather than discrete tokenization analysis or design.",
      "evaluation": 2
    },
    "round-A_JuniorMini_reasoning": "The study proposes Neural Codec Source Tracing (NCST) targeting open-set neural codec classification, constructed a dataset involving neural codec methods, and evaluates models for audio deepfake source tracing; however, it does not explicitly discuss discrete audio/speech tokenization or conversion of continuous signals into discrete token sequences as its core focus, suggesting it mainly deals with codec classification and detection rather than discrete tokenization analysis or design.",
    "round-A_JuniorMini_evaluation": 2,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:2)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:2)"
  },
  {
    "title": "Humanity's Last Exam",
    "abstract": "Benchmarks are important tools for tracking the rapid advancements in large language model (LLM) capabilities. However, benchmarks are not keeping pace in difficulty: LLMs now achieve over 90\\% accuracy on popular benchmarks like MMLU, limiting informed measurement of state-of-the-art LLM capabilities. In response, we introduce Humanity's Last Exam (HLE), a multi-modal benchmark at the frontier of human knowledge, designed to be the final closed-ended academic benchmark of its kind with broad subject coverage. HLE consists of 2,500 questions across dozens of subjects, including mathematics, humanities, and the natural sciences. HLE is developed globally by subject-matter experts and consists of multiple-choice and short-answer questions suitable for automated grading. Each question has a known solution that is unambiguous and easily verifiable, but cannot be quickly answered via internet retrieval. State-of-the-art LLMs demonstrate low accuracy and calibration on HLE, highlighting a significant gap between current LLM capabilities and the expert human frontier on closed-ended academic questions. To inform research and policymaking upon a clear understanding of model capabilities, we publicly release HLE at https://lastexam.ai.",
    "metadata": {
      "arxiv_id": "2501.14249",
      "title": "Humanity's Last Exam",
      "summary": "Benchmarks are important tools for tracking the rapid advancements in large language model (LLM) capabilities. However, benchmarks are not keeping pace in difficulty: LLMs now achieve over 90\\% accuracy on popular benchmarks like MMLU, limiting informed measurement of state-of-the-art LLM capabilities. In response, we introduce Humanity's Last Exam (HLE), a multi-modal benchmark at the frontier of human knowledge, designed to be the final closed-ended academic benchmark of its kind with broad subject coverage. HLE consists of 2,500 questions across dozens of subjects, including mathematics, humanities, and the natural sciences. HLE is developed globally by subject-matter experts and consists of multiple-choice and short-answer questions suitable for automated grading. Each question has a known solution that is unambiguous and easily verifiable, but cannot be quickly answered via internet retrieval. State-of-the-art LLMs demonstrate low accuracy and calibration on HLE, highlighting a significant gap between current LLM capabilities and the expert human frontier on closed-ended academic questions. To inform research and policymaking upon a clear understanding of model capabilities, we publicly release HLE at https://lastexam.ai.",
      "authors": [
        "Long Phan",
        "Alice Gatti",
        "Ziwen Han",
        "Nathaniel Li",
        "Josephina Hu",
        "Hugh Zhang",
        "Chen Bo Calvin Zhang",
        "Mohamed Shaaban",
        "John Ling",
        "Sean Shi",
        "Michael Choi",
        "Anish Agrawal",
        "Arnav Chopra",
        "Adam Khoja",
        "Ryan Kim",
        "Richard Ren",
        "Jason Hausenloy",
        "Oliver Zhang",
        "Mantas Mazeika",
        "Dmitry Dodonov",
        "Tung Nguyen",
        "Jaeho Lee",
        "Daron Anderson",
        "Mikhail Doroshenko",
        "Alun Cennyth Stokes",
        "Mobeen Mahmood",
        "Oleksandr Pokutnyi",
        "Oleg Iskra",
        "Jessica P. Wang",
        "John-Clark Levin",
        "Mstyslav Kazakov",
        "Fiona Feng",
        "Steven Y. Feng",
        "Haoran Zhao",
        "Michael Yu",
        "Varun Gangal",
        "Chelsea Zou",
        "Zihan Wang",
        "Serguei Popov",
        "Robert Gerbicz",
        "Geoff Galgon",
        "Johannes Schmitt",
        "Will Yeadon",
        "Yongki Lee",
        "Scott Sauers",
        "Alvaro Sanchez",
        "Fabian Giska",
        "Marc Roth",
        "Søren Riis",
        "Saiteja Utpala",
        "Noah Burns",
        "Gashaw M. Goshu",
        "Mohinder Maheshbhai Naiya",
        "Chidozie Agu",
        "Zachary Giboney",
        "Antrell Cheatom",
        "Francesco Fournier-Facio",
        "Sarah-Jane Crowson",
        "Lennart Finke",
        "Zerui Cheng",
        "Jennifer Zampese",
        "Ryan G. Hoerr",
        "Mark Nandor",
        "Hyunwoo Park",
        "Tim Gehrunger",
        "Jiaqi Cai",
        "Ben McCarty",
        "Alexis C Garretson",
        "Edwin Taylor",
        "Damien Sileo",
        "Qiuyu Ren",
        "Usman Qazi",
        "Lianghui Li",
        "Jungbae Nam",
        "John B. Wydallis",
        "Pavel Arkhipov",
        "Jack Wei Lun Shi",
        "Aras Bacho",
        "Chris G. Willcocks",
        "Hangrui Cao",
        "Sumeet Motwani",
        "Emily de Oliveira Santos",
        "Johannes Veith",
        "Edward Vendrow",
        "Doru Cojoc",
        "Kengo Zenitani",
        "Joshua Robinson",
        "Longke Tang",
        "Yuqi Li",
        "Joshua Vendrow",
        "Natanael Wildner Fraga",
        "Vladyslav Kuchkin",
        "Andrey Pupasov Maksimov",
        "Pierre Marion",
        "Denis Efremov",
        "Jayson Lynch",
        "Kaiqu Liang",
        "Aleksandar Mikov",
        "Andrew Gritsevskiy",
        "Julien Guillod",
        "Gözdenur Demir",
        "Dakotah Martinez",
        "Ben Pageler",
        "Kevin Zhou",
        "Saeed Soori",
        "Ori Press",
        "Henry Tang",
        "Paolo Rissone",
        "Sean R. Green",
        "Lina Brüssel",
        "Moon Twayana",
        "Aymeric Dieuleveut",
        "Joseph Marvin Imperial",
        "Ameya Prabhu",
        "Jinzhou Yang",
        "Nick Crispino",
        "Arun Rao",
        "Dimitri Zvonkine",
        "Gabriel Loiseau",
        "Mikhail Kalinin",
        "Marco Lukas",
        "Ciprian Manolescu",
        "Nate Stambaugh",
        "Subrata Mishra",
        "Tad Hogg",
        "Carlo Bosio",
        "Brian P Coppola",
        "Julian Salazar",
        "Jaehyeok Jin",
        "Rafael Sayous",
        "Stefan Ivanov",
        "Philippe Schwaller",
        "Shaipranesh Senthilkuma",
        "Andres M Bran",
        "Andres Algaba",
        "Kelsey Van den Houte",
        "Lynn Van Der Sypt",
        "Brecht Verbeken",
        "David Noever",
        "Alexei Kopylov",
        "Benjamin Myklebust",
        "Bikun Li",
        "Lisa Schut",
        "Evgenii Zheltonozhskii",
        "Qiaochu Yuan",
        "Derek Lim",
        "Richard Stanley",
        "Tong Yang",
        "John Maar",
        "Julian Wykowski",
        "Martí Oller",
        "Anmol Sahu",
        "Cesare Giulio Ardito",
        "Yuzheng Hu",
        "Ariel Ghislain Kemogne Kamdoum",
        "Alvin Jin",
        "Tobias Garcia Vilchis",
        "Yuexuan Zu",
        "Martin Lackner",
        "James Koppel",
        "Gongbo Sun",
        "Daniil S. Antonenko",
        "Steffi Chern",
        "Bingchen Zhao",
        "Pierrot Arsene",
        "Joseph M Cavanagh",
        "Daofeng Li",
        "Jiawei Shen",
        "Donato Crisostomi",
        "Wenjin Zhang",
        "Ali Dehghan",
        "Sergey Ivanov",
        "David Perrella",
        "Nurdin Kaparov",
        "Allen Zang",
        "Ilia Sucholutsky",
        "Arina Kharlamova",
        "Daniil Orel",
        "Vladislav Poritski",
        "Shalev Ben-David",
        "Zachary Berger",
        "Parker Whitfill",
        "Michael Foster",
        "Daniel Munro",
        "Linh Ho",
        "Shankar Sivarajan",
        "Dan Bar Hava",
        "Aleksey Kuchkin",
        "David Holmes",
        "Alexandra Rodriguez-Romero",
        "Frank Sommerhage",
        "Anji Zhang",
        "Richard Moat",
        "Keith Schneider",
        "Zakayo Kazibwe",
        "Don Clarke",
        "Dae Hyun Kim",
        "Felipe Meneguitti Dias",
        "Sara Fish",
        "Veit Elser",
        "Tobias Kreiman",
        "Victor Efren Guadarrama Vilchis",
        "Immo Klose",
        "Ujjwala Anantheswaran",
        "Adam Zweiger",
        "Kaivalya Rawal",
        "Jeffery Li",
        "Jeremy Nguyen",
        "Nicolas Daans",
        "Haline Heidinger",
        "Maksim Radionov",
        "Václav Rozhoň",
        "Vincent Ginis",
        "Christian Stump",
        "Niv Cohen",
        "Rafał Poświata",
        "Josef Tkadlec",
        "Alan Goldfarb",
        "Chenguang Wang",
        "Piotr Padlewski",
        "Stanislaw Barzowski",
        "Kyle Montgomery",
        "Ryan Stendall",
        "Jamie Tucker-Foltz",
        "Jack Stade",
        "T. Ryan Rogers",
        "Tom Goertzen",
        "Declan Grabb",
        "Abhishek Shukla",
        "Alan Givré",
        "John Arnold Ambay",
        "Archan Sen",
        "Muhammad Fayez Aziz",
        "Mark H Inlow",
        "Hao He",
        "Ling Zhang",
        "Younesse Kaddar",
        "Ivar Ängquist",
        "Yanxu Chen",
        "Harrison K Wang",
        "Kalyan Ramakrishnan",
        "Elliott Thornley",
        "Antonio Terpin",
        "Hailey Schoelkopf",
        "Eric Zheng",
        "Avishy Carmi",
        "Ethan D. L. Brown",
        "Kelin Zhu",
        "Max Bartolo",
        "Richard Wheeler",
        "Martin Stehberger",
        "Peter Bradshaw",
        "JP Heimonen",
        "Kaustubh Sridhar",
        "Ido Akov",
        "Jennifer Sandlin",
        "Yury Makarychev",
        "Joanna Tam",
        "Hieu Hoang",
        "David M. Cunningham",
        "Vladimir Goryachev",
        "Demosthenes Patramanis",
        "Michael Krause",
        "Andrew Redenti",
        "David Aldous",
        "Jesyin Lai",
        "Shannon Coleman",
        "Jiangnan Xu",
        "Sangwon Lee",
        "Ilias Magoulas",
        "Sandy Zhao",
        "Ning Tang",
        "Michael K. Cohen",
        "Orr Paradise",
        "Jan Hendrik Kirchner",
        "Maksym Ovchynnikov",
        "Jason O. Matos",
        "Adithya Shenoy",
        "Michael Wang",
        "Yuzhou Nie",
        "Anna Sztyber-Betley",
        "Paolo Faraboschi",
        "Robin Riblet",
        "Jonathan Crozier",
        "Shiv Halasyamani",
        "Shreyas Verma",
        "Prashant Joshi",
        "Eli Meril",
        "Ziqiao Ma",
        "Jérémy Andréoletti",
        "Raghav Singhal",
        "Jacob Platnick",
        "Volodymyr Nevirkovets",
        "Luke Basler",
        "Alexander Ivanov",
        "Seri Khoury",
        "Nils Gustafsson",
        "Marco Piccardo",
        "Hamid Mostaghimi",
        "Qijia Chen",
        "Virendra Singh",
        "Tran Quoc Khánh",
        "Paul Rosu",
        "Hannah Szlyk",
        "Zachary Brown",
        "Himanshu Narayan",
        "Aline Menezes",
        "Jonathan Roberts",
        "William Alley",
        "Kunyang Sun",
        "Arkil Patel",
        "Max Lamparth",
        "Anka Reuel",
        "Linwei Xin",
        "Hanmeng Xu",
        "Jacob Loader",
        "Freddie Martin",
        "Zixuan Wang",
        "Andrea Achilleos",
        "Thomas Preu",
        "Tomek Korbak",
        "Ida Bosio",
        "Fereshteh Kazemi",
        "Ziye Chen",
        "Biró Bálint",
        "Eve J. Y. Lo",
        "Jiaqi Wang",
        "Maria Inês S. Nunes",
        "Jeremiah Milbauer",
        "M Saiful Bari",
        "Zihao Wang",
        "Behzad Ansarinejad",
        "Yewen Sun",
        "Stephane Durand",
        "Hossam Elgnainy",
        "Guillaume Douville",
        "Daniel Tordera",
        "George Balabanian",
        "Hew Wolff",
        "Lynna Kvistad",
        "Hsiaoyun Milliron",
        "Ahmad Sakor",
        "Murat Eron",
        "Andrew Favre D. O.",
        "Shailesh Shah",
        "Xiaoxiang Zhou",
        "Firuz Kamalov",
        "Sherwin Abdoli",
        "Tim Santens",
        "Shaul Barkan",
        "Allison Tee",
        "Robin Zhang",
        "Alessandro Tomasiello",
        "G. Bruno De Luca",
        "Shi-Zhuo Looi",
        "Vinh-Kha Le",
        "Noam Kolt",
        "Jiayi Pan",
        "Emma Rodman",
        "Jacob Drori",
        "Carl J Fossum",
        "Niklas Muennighoff",
        "Milind Jagota",
        "Ronak Pradeep",
        "Honglu Fan",
        "Jonathan Eicher",
        "Michael Chen",
        "Kushal Thaman",
        "William Merrill",
        "Moritz Firsching",
        "Carter Harris",
        "Stefan Ciobâcă",
        "Jason Gross",
        "Rohan Pandey",
        "Ilya Gusev",
        "Adam Jones",
        "Shashank Agnihotri",
        "Pavel Zhelnov",
        "Mohammadreza Mofayezi",
        "Alexander Piperski",
        "David K. Zhang",
        "Kostiantyn Dobarskyi",
        "Roman Leventov",
        "Ignat Soroko",
        "Joshua Duersch",
        "Vage Taamazyan",
        "Andrew Ho",
        "Wenjie Ma",
        "William Held",
        "Ruicheng Xian",
        "Armel Randy Zebaze",
        "Mohanad Mohamed",
        "Julian Noah Leser",
        "Michelle X Yuan",
        "Laila Yacar",
        "Johannes Lengler",
        "Katarzyna Olszewska",
        "Claudio Di Fratta",
        "Edson Oliveira",
        "Joseph W. Jackson",
        "Andy Zou",
        "Muthu Chidambaram",
        "Timothy Manik",
        "Hector Haffenden",
        "Dashiell Stander",
        "Ali Dasouqi",
        "Alexander Shen",
        "Bita Golshani",
        "David Stap",
        "Egor Kretov",
        "Mikalai Uzhou",
        "Alina Borisovna Zhidkovskaya",
        "Nick Winter",
        "Miguel Orbegozo Rodriguez",
        "Robert Lauff",
        "Dustin Wehr",
        "Colin Tang",
        "Zaki Hossain",
        "Shaun Phillips",
        "Fortuna Samuele",
        "Fredrik Ekström",
        "Angela Hammon",
        "Oam Patel",
        "Faraz Farhidi",
        "George Medley",
        "Forough Mohammadzadeh",
        "Madellene Peñaflor",
        "Haile Kassahun",
        "Alena Friedrich",
        "Rayner Hernandez Perez",
        "Daniel Pyda",
        "Taom Sakal",
        "Omkar Dhamane",
        "Ali Khajegili Mirabadi",
        "Eric Hallman",
        "Kenchi Okutsu",
        "Mike Battaglia",
        "Mohammad Maghsoudimehrabani",
        "Alon Amit",
        "Dave Hulbert",
        "Roberto Pereira",
        "Simon Weber",
        "Handoko",
        "Anton Peristyy",
        "Stephen Malina",
        "Mustafa Mehkary",
        "Rami Aly",
        "Frank Reidegeld",
        "Anna-Katharina Dick",
        "Cary Friday",
        "Mukhwinder Singh",
        "Hassan Shapourian",
        "Wanyoung Kim",
        "Mariana Costa",
        "Hubeyb Gurdogan",
        "Harsh Kumar",
        "Chiara Ceconello",
        "Chao Zhuang",
        "Haon Park",
        "Micah Carroll",
        "Andrew R. Tawfeek",
        "Stefan Steinerberger",
        "Daattavya Aggarwal",
        "Michael Kirchhof",
        "Linjie Dai",
        "Evan Kim",
        "Johan Ferret",
        "Jainam Shah",
        "Yuzhou Wang",
        "Minghao Yan",
        "Krzysztof Burdzy",
        "Lixin Zhang",
        "Antonio Franca",
        "Diana T. Pham",
        "Kang Yong Loh",
        "Joshua Robinson",
        "Abram Jackson",
        "Paolo Giordano",
        "Philipp Petersen",
        "Adrian Cosma",
        "Jesus Colino",
        "Colin White",
        "Jacob Votava",
        "Vladimir Vinnikov",
        "Ethan Delaney",
        "Petr Spelda",
        "Vit Stritecky",
        "Syed M. Shahid",
        "Jean-Christophe Mourrat",
        "Lavr Vetoshkin",
        "Koen Sponselee",
        "Renas Bacho",
        "Zheng-Xin Yong",
        "Florencia de la Rosa",
        "Nathan Cho",
        "Xiuyu Li",
        "Guillaume Malod",
        "Orion Weller",
        "Guglielmo Albani",
        "Leon Lang",
        "Julien Laurendeau",
        "Dmitry Kazakov",
        "Fatimah Adesanya",
        "Julien Portier",
        "Lawrence Hollom",
        "Victor Souza",
        "Yuchen Anna Zhou",
        "Julien Degorre",
        "Yiğit Yalın",
        "Gbenga Daniel Obikoya",
        "Rai",
        "Filippo Bigi",
        "M. C. Boscá",
        "Oleg Shumar",
        "Kaniuar Bacho",
        "Gabriel Recchia",
        "Mara Popescu",
        "Nikita Shulga",
        "Ngefor Mildred Tanwie",
        "Thomas C. H. Lux",
        "Ben Rank",
        "Colin Ni",
        "Matthew Brooks",
        "Alesia Yakimchyk",
        "Huanxu",
        "Liu",
        "Stefano Cavalleri",
        "Olle Häggström",
        "Emil Verkama",
        "Joshua Newbould",
        "Hans Gundlach",
        "Leonor Brito-Santana",
        "Brian Amaro",
        "Vivek Vajipey",
        "Rynaa Grover",
        "Ting Wang",
        "Yosi Kratish",
        "Wen-Ding Li",
        "Sivakanth Gopi",
        "Andrea Caciolai",
        "Christian Schroeder de Witt",
        "Pablo Hernández-Cámara",
        "Emanuele Rodolà",
        "Jules Robins",
        "Dominic Williamson",
        "Vincent Cheng",
        "Brad Raynor",
        "Hao Qi",
        "Ben Segev",
        "Jingxuan Fan",
        "Sarah Martinson",
        "Erik Y. Wang",
        "Kaylie Hausknecht",
        "Michael P. Brenner",
        "Mao Mao",
        "Christoph Demian",
        "Peyman Kassani",
        "Xinyu Zhang",
        "David Avagian",
        "Eshawn Jessica Scipio",
        "Alon Ragoler",
        "Justin Tan",
        "Blake Sims",
        "Rebeka Plecnik",
        "Aaron Kirtland",
        "Omer Faruk Bodur",
        "D. P. Shinde",
        "Yan Carlos Leyva Labrador",
        "Zahra Adoul",
        "Mohamed Zekry",
        "Ali Karakoc",
        "Tania C. B. Santos",
        "Samir Shamseldeen",
        "Loukmane Karim",
        "Anna Liakhovitskaia",
        "Nate Resman",
        "Nicholas Farina",
        "Juan Carlos Gonzalez",
        "Gabe Maayan",
        "Earth Anderson",
        "Rodrigo De Oliveira Pena",
        "Elizabeth Kelley",
        "Hodjat Mariji",
        "Rasoul Pouriamanesh",
        "Wentao Wu",
        "Ross Finocchio",
        "Ismail Alarab",
        "Joshua Cole",
        "Danyelle Ferreira",
        "Bryan Johnson",
        "Mohammad Safdari",
        "Liangti Dai",
        "Siriphan Arthornthurasuk",
        "Isaac C. McAlister",
        "Alejandro José Moyano",
        "Alexey Pronin",
        "Jing Fan",
        "Angel Ramirez-Trinidad",
        "Yana Malysheva",
        "Daphiny Pottmaier",
        "Omid Taheri",
        "Stanley Stepanic",
        "Samuel Perry",
        "Luke Askew",
        "Raúl Adrián Huerta Rodríguez",
        "Ali M. R. Minissi",
        "Ricardo Lorena",
        "Krishnamurthy Iyer",
        "Arshad Anil Fasiludeen",
        "Ronald Clark",
        "Josh Ducey",
        "Matheus Piza",
        "Maja Somrak",
        "Eric Vergo",
        "Juehang Qin",
        "Benjámin Borbás",
        "Eric Chu",
        "Jack Lindsey",
        "Antoine Jallon",
        "I. M. J. McInnis",
        "Evan Chen",
        "Avi Semler",
        "Luk Gloor",
        "Tej Shah",
        "Marc Carauleanu",
        "Pascal Lauer",
        "Tran Đuc Huy",
        "Hossein Shahrtash",
        "Emilien Duc",
        "Lukas Lewark",
        "Assaf Brown",
        "Samuel Albanie",
        "Brian Weber",
        "Warren S. Vaz",
        "Pierre Clavier",
        "Yiyang Fan",
        "Gabriel Poesia Reis e Silva",
        "Long",
        "Lian",
        "Marcus Abramovitch",
        "Xi Jiang",
        "Sandra Mendoza",
        "Murat Islam",
        "Juan Gonzalez",
        "Vasilios Mavroudis",
        "Justin Xu",
        "Pawan Kumar",
        "Laxman Prasad Goswami",
        "Daniel Bugas",
        "Nasser Heydari",
        "Ferenc Jeanplong",
        "Thorben Jansen",
        "Antonella Pinto",
        "Archimedes Apronti",
        "Abdallah Galal",
        "Ng Ze-An",
        "Ankit Singh",
        "Tong Jiang",
        "Joan of Arc Xavier",
        "Kanu Priya Agarwal",
        "Mohammed Berkani",
        "Gang Zhang",
        "Zhehang Du",
        "Benedito Alves de Oliveira Junior",
        "Dmitry Malishev",
        "Nicolas Remy",
        "Taylor D. Hartman",
        "Tim Tarver",
        "Stephen Mensah",
        "Gautier Abou Loume",
        "Wiktor Morak",
        "Farzad Habibi",
        "Sarah Hoback",
        "Will Cai",
        "Javier Gimenez",
        "Roselynn Grace Montecillo",
        "Jakub Łucki",
        "Russell Campbell",
        "Asankhaya Sharma",
        "Khalida Meer",
        "Shreen Gul",
        "Daniel Espinosa Gonzalez",
        "Xavier Alapont",
        "Alex Hoover",
        "Gunjan Chhablani",
        "Freddie Vargus",
        "Arunim Agarwal",
        "Yibo Jiang",
        "Deepakkumar Patil",
        "David Outevsky",
        "Kevin Joseph Scaria",
        "Rajat Maheshwari",
        "Abdelkader Dendane",
        "Priti Shukla",
        "Ashley Cartwright",
        "Sergei Bogdanov",
        "Niels Mündler",
        "Sören Möller",
        "Luca Arnaboldi",
        "Kunvar Thaman",
        "Muhammad Rehan Siddiqi",
        "Prajvi Saxena",
        "Himanshu Gupta",
        "Tony Fruhauff",
        "Glen Sherman",
        "Mátyás Vincze",
        "Siranut Usawasutsakorn",
        "Dylan Ler",
        "Anil Radhakrishnan",
        "Innocent Enyekwe",
        "Sk Md Salauddin",
        "Jiang Muzhen",
        "Aleksandr Maksapetyan",
        "Vivien Rossbach",
        "Chris Harjadi",
        "Mohsen Bahaloohoreh",
        "Claire Sparrow",
        "Jasdeep Sidhu",
        "Sam Ali",
        "Song Bian",
        "John Lai",
        "Eric Singer",
        "Justine Leon Uro",
        "Greg Bateman",
        "Mohamed Sayed",
        "Ahmed Menshawy",
        "Darling Duclosel",
        "Dario Bezzi",
        "Yashaswini Jain",
        "Ashley Aaron",
        "Murat Tiryakioglu",
        "Sheeshram Siddh",
        "Keith Krenek",
        "Imad Ali Shah",
        "Jun Jin",
        "Scott Creighton",
        "Denis Peskoff",
        "Zienab EL-Wasif",
        "Ragavendran P",
        "Michael Richmond",
        "Joseph McGowan",
        "Tejal Patwardhan",
        "Hao-Yu Sun",
        "Ting Sun",
        "Nikola Zubić",
        "Samuele Sala",
        "Stephen Ebert",
        "Jean Kaddour",
        "Manuel Schottdorf",
        "Dianzhuo Wang",
        "Gerol Petruzella",
        "Alex Meiburg",
        "Tilen Medved",
        "Ali ElSheikh",
        "S Ashwin Hebbar",
        "Lorenzo Vaquero",
        "Xianjun Yang",
        "Jason Poulos",
        "Vilém Zouhar",
        "Sergey Bogdanik",
        "Mingfang Zhang",
        "Jorge Sanz-Ros",
        "David Anugraha",
        "Yinwei Dai",
        "Anh N. Nhu",
        "Xue Wang",
        "Ali Anil Demircali",
        "Zhibai Jia",
        "Yuyin Zhou",
        "Juncheng Wu",
        "Mike He",
        "Nitin Chandok",
        "Aarush Sinha",
        "Gaoxiang Luo",
        "Long Le",
        "Mickaël Noyé",
        "Michał Perełkiewicz",
        "Ioannis Pantidis",
        "Tianbo Qi",
        "Soham Sachin Purohit",
        "Letitia Parcalabescu",
        "Thai-Hoa Nguyen",
        "Genta Indra Winata",
        "Edoardo M. Ponti",
        "Hanchen Li",
        "Kaustubh Dhole",
        "Jongee Park",
        "Dario Abbondanza",
        "Yuanli Wang",
        "Anupam Nayak",
        "Diogo M. Caetano",
        "Antonio A. W. L. Wong",
        "Maria del Rio-Chanona",
        "Dániel Kondor",
        "Pieter Francois",
        "Ed Chalstrey",
        "Jakob Zsambok",
        "Dan Hoyer",
        "Jenny Reddish",
        "Jakob Hauser",
        "Francisco-Javier Rodrigo-Ginés",
        "Suchandra Datta",
        "Maxwell Shepherd",
        "Thom Kamphuis",
        "Qizheng Zhang",
        "Hyunjun Kim",
        "Ruiji Sun",
        "Jianzhu Yao",
        "Franck Dernoncourt",
        "Satyapriya Krishna",
        "Sina Rismanchian",
        "Bonan Pu",
        "Francesco Pinto",
        "Yingheng Wang",
        "Kumar Shridhar",
        "Kalon J. Overholt",
        "Glib Briia",
        "Hieu Nguyen",
        "David",
        "Soler Bartomeu",
        "Tony CY Pang",
        "Adam Wecker",
        "Yifan Xiong",
        "Fanfei Li",
        "Lukas S. Huber",
        "Joshua Jaeger",
        "Romano De Maddalena",
        "Xing Han Lù",
        "Yuhui Zhang",
        "Claas Beger",
        "Patrick Tser Jern Kon",
        "Sean Li",
        "Vivek Sanker",
        "Ming Yin",
        "Yihao Liang",
        "Xinlu Zhang",
        "Ankit Agrawal",
        "Li S. Yifei",
        "Zechen Zhang",
        "Mu Cai",
        "Yasin Sonmez",
        "Costin Cozianu",
        "Changhao Li",
        "Alex Slen",
        "Shoubin Yu",
        "Hyun Kyu Park",
        "Gabriele Sarti",
        "Marcin Briański",
        "Alessandro Stolfo",
        "Truong An Nguyen",
        "Mike Zhang",
        "Yotam Perlitz",
        "Jose Hernandez-Orallo",
        "Runjia Li",
        "Amin Shabani",
        "Felix Juefei-Xu",
        "Shikhar Dhingra",
        "Orr Zohar",
        "My Chiffon Nguyen",
        "Alexander Pondaven",
        "Abdurrahim Yilmaz",
        "Xuandong Zhao",
        "Chuanyang Jin",
        "Muyan Jiang",
        "Stefan Todoran",
        "Xinyao Han",
        "Jules Kreuer",
        "Brian Rabern",
        "Anna Plassart",
        "Martino Maggetti",
        "Luther Yap",
        "Robert Geirhos",
        "Jonathon Kean",
        "Dingsu Wang",
        "Sina Mollaei",
        "Chenkai Sun",
        "Yifan Yin",
        "Shiqi Wang",
        "Rui Li",
        "Yaowen Chang",
        "Anjiang Wei",
        "Alice Bizeul",
        "Xiaohan Wang",
        "Alexandre Oliveira Arrais",
        "Kushin Mukherjee",
        "Jorge Chamorro-Padial",
        "Jiachen Liu",
        "Xingyu Qu",
        "Junyi Guan",
        "Adam Bouyamourn",
        "Shuyu Wu",
        "Martyna Plomecka",
        "Junda Chen",
        "Mengze Tang",
        "Jiaqi Deng",
        "Shreyas Subramanian",
        "Haocheng Xi",
        "Haoxuan Chen",
        "Weizhi Zhang",
        "Yinuo Ren",
        "Haoqin Tu",
        "Sejong Kim",
        "Yushun Chen",
        "Sara Vera Marjanović",
        "Junwoo Ha",
        "Grzegorz Luczyna",
        "Jeff J. Ma",
        "Zewen Shen",
        "Dawn Song",
        "Cedegao E. Zhang",
        "Zhun Wang",
        "Gaël Gendron",
        "Yunze Xiao",
        "Leo Smucker",
        "Erica Weng",
        "Kwok Hao Lee",
        "Zhe Ye",
        "Stefano Ermon",
        "Ignacio D. Lopez-Miguel",
        "Theo Knights",
        "Anthony Gitter",
        "Namkyu Park",
        "Boyi Wei",
        "Hongzheng Chen",
        "Kunal Pai",
        "Ahmed Elkhanany",
        "Han Lin",
        "Philipp D. Siedler",
        "Jichao Fang",
        "Ritwik Mishra",
        "Károly Zsolnai-Fehér",
        "Xilin Jiang",
        "Shadab Khan",
        "Jun Yuan",
        "Rishab Kumar Jain",
        "Xi Lin",
        "Mike Peterson",
        "Zhe Wang",
        "Aditya Malusare",
        "Maosen Tang",
        "Isha Gupta",
        "Ivan Fosin",
        "Timothy Kang",
        "Barbara Dworakowska",
        "Kazuki Matsumoto",
        "Guangyao Zheng",
        "Gerben Sewuster",
        "Jorge Pretel Villanueva",
        "Ivan Rannev",
        "Igor Chernyavsky",
        "Jiale Chen",
        "Deepayan Banik",
        "Ben Racz",
        "Wenchao Dong",
        "Jianxin Wang",
        "Laila Bashmal",
        "Duarte V. Gonçalves",
        "Wei Hu",
        "Kaushik Bar",
        "Ondrej Bohdal",
        "Atharv Singh Patlan",
        "Shehzaad Dhuliawala",
        "Caroline Geirhos",
        "Julien Wist",
        "Yuval Kansal",
        "Bingsen Chen",
        "Kutay Tire",
        "Atak Talay Yücel",
        "Brandon Christof",
        "Veerupaksh Singla",
        "Zijian Song",
        "Sanxing Chen",
        "Jiaxin Ge",
        "Kaustubh Ponkshe",
        "Isaac Park",
        "Tianneng Shi",
        "Martin Q. Ma",
        "Joshua Mak",
        "Sherwin Lai",
        "Antoine Moulin",
        "Zhuo Cheng",
        "Zhanda Zhu",
        "Ziyi Zhang",
        "Vaidehi Patil",
        "Ketan Jha",
        "Qiutong Men",
        "Jiaxuan Wu",
        "Tianchi Zhang",
        "Bruno Hebling Vieira",
        "Alham Fikri Aji",
        "Jae-Won Chung",
        "Mohammed Mahfoud",
        "Ha Thi Hoang",
        "Marc Sperzel",
        "Wei Hao",
        "Kristof Meding",
        "Sihan Xu",
        "Vassilis Kostakos",
        "Davide Manini",
        "Yueying Liu",
        "Christopher Toukmaji",
        "Jay Paek",
        "Eunmi Yu",
        "Arif Engin Demircali",
        "Zhiyi Sun",
        "Ivan Dewerpe",
        "Hongsen Qin",
        "Roman Pflugfelder",
        "James Bailey",
        "Johnathan Morris",
        "Ville Heilala",
        "Sybille Rosset",
        "Zishun Yu",
        "Peter E. Chen",
        "Woongyeong Yeo",
        "Eeshaan Jain",
        "Ryan Yang",
        "Sreekar Chigurupati",
        "Julia Chernyavsky",
        "Sai Prajwal Reddy",
        "Subhashini Venugopalan",
        "Hunar Batra",
        "Core Francisco Park",
        "Hieu Tran",
        "Guilherme Maximiano",
        "Genghan Zhang",
        "Yizhuo Liang",
        "Hu Shiyu",
        "Rongwu Xu",
        "Rui Pan",
        "Siddharth Suresh",
        "Ziqi Liu",
        "Samaksh Gulati",
        "Songyang Zhang",
        "Peter Turchin",
        "Christopher W. Bartlett",
        "Christopher R. Scotese",
        "Phuong M. Cao",
        "Ben Wu",
        "Jacek Karwowski",
        "Davide Scaramuzza",
        "Aakaash Nattanmai",
        "Gordon McKellips",
        "Anish Cheraku",
        "Asim Suhail",
        "Ethan Luo",
        "Marvin Deng",
        "Jason Luo",
        "Ashley Zhang",
        "Kavin Jindel",
        "Jay Paek",
        "Kasper Halevy",
        "Allen Baranov",
        "Michael Liu",
        "Advaith Avadhanam",
        "David Zhang",
        "Vincent Cheng",
        "Brad Ma",
        "Evan Fu",
        "Liam Do",
        "Joshua Lass",
        "Hubert Yang",
        "Surya Sunkari",
        "Vishruth Bharath",
        "Violet Ai",
        "James Leung",
        "Rishit Agrawal",
        "Alan Zhou",
        "Kevin Chen",
        "Tejas Kalpathi",
        "Ziqi Xu",
        "Gavin Wang",
        "Tyler Xiao",
        "Erik Maung",
        "Sam Lee",
        "Ryan Yang",
        "Roy Yue",
        "Ben Zhao",
        "Julia Yoon",
        "Sunny Sun",
        "Aryan Singh",
        "Ethan Luo",
        "Clark Peng",
        "Tyler Osbey",
        "Taozhi Wang",
        "Daryl Echeazu",
        "Hubert Yang",
        "Timothy Wu",
        "Spandan Patel",
        "Vidhi Kulkarni",
        "Vijaykaarti Sundarapandiyan",
        "Ashley Zhang",
        "Andrew Le",
        "Zafir Nasim",
        "Srikar Yalam",
        "Ritesh Kasamsetty",
        "Soham Samal",
        "Hubert Yang",
        "David Sun",
        "Nihar Shah",
        "Abhijeet Saha",
        "Alex Zhang",
        "Leon Nguyen",
        "Laasya Nagumalli",
        "Kaixin Wang",
        "Alan Zhou",
        "Aidan Wu",
        "Jason Luo",
        "Anwith Telluri",
        "Summer Yue",
        "Alexandr Wang",
        "Dan Hendrycks"
      ],
      "published": "2025-01-24T05:27:46Z",
      "updated": "2025-09-25T19:27:10Z",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "pdf_url": "https://arxiv.org/pdf/2501.14249v9",
      "landing_url": "https://arxiv.org/abs/2501.14249v9",
      "doi": "https://doi.org/10.48550/arXiv.2501.14249"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item describes a benchmark for LLMs with no focus on discrete audio tokenization or discrete speech representations, so it does not meet the inclusion criteria and should be excluded.",
      "evaluation": 2
    },
    "round-A_JuniorNano_reasoning": "The item describes a benchmark for LLMs with no focus on discrete audio tokenization or discrete speech representations, so it does not meet the inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 2,
    "round-A_JuniorMini_output": {
      "reasoning": "The study presents a new multi-modal academic benchmark for large language models focused on various subjects but does not discuss or analyze any discrete audio/speech tokenization methods or discrete audio representation as a core object of study; hence, it does not meet the inclusion criteria and does not avoid the exclusion criteria related to audio tokenization.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study presents a new multi-modal academic benchmark for large language models focused on various subjects but does not discuss or analyze any discrete audio/speech tokenization methods or discrete audio representation as a core object of study; hence, it does not meet the inclusion criteria and does not avoid the exclusion criteria related to audio tokenization.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:2)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:2)"
  },
  {
    "title": "BreezyVoice: Adapting TTS for Taiwanese Mandarin with Enhanced Polyphone Disambiguation -- Challenges and Insights",
    "abstract": "We present BreezyVoice, a Text-to-Speech (TTS) system specifically adapted for Taiwanese Mandarin, highlighting phonetic control abilities to address the unique challenges of polyphone disambiguation in the language. Building upon CosyVoice, we incorporate a $S^{3}$ tokenizer, a large language model (LLM), an optimal-transport conditional flow matching model (OT-CFM), and a grapheme to phoneme prediction model, to generate realistic speech that closely mimics human utterances. Our evaluation demonstrates BreezyVoice's superior performance in both general and code-switching contexts, highlighting its robustness and effectiveness in generating high-fidelity speech. Additionally, we address the challenges of generalizability in modeling long-tail speakers and polyphone disambiguation. Our approach significantly enhances performance and offers valuable insights into the workings of neural codec TTS systems.",
    "metadata": {
      "arxiv_id": "2501.17790",
      "title": "BreezyVoice: Adapting TTS for Taiwanese Mandarin with Enhanced Polyphone Disambiguation -- Challenges and Insights",
      "summary": "We present BreezyVoice, a Text-to-Speech (TTS) system specifically adapted for Taiwanese Mandarin, highlighting phonetic control abilities to address the unique challenges of polyphone disambiguation in the language. Building upon CosyVoice, we incorporate a $S^{3}$ tokenizer, a large language model (LLM), an optimal-transport conditional flow matching model (OT-CFM), and a grapheme to phoneme prediction model, to generate realistic speech that closely mimics human utterances. Our evaluation demonstrates BreezyVoice's superior performance in both general and code-switching contexts, highlighting its robustness and effectiveness in generating high-fidelity speech. Additionally, we address the challenges of generalizability in modeling long-tail speakers and polyphone disambiguation. Our approach significantly enhances performance and offers valuable insights into the workings of neural codec TTS systems.",
      "authors": [
        "Chan-Jan Hsu",
        "Yi-Cheng Lin",
        "Chia-Chun Lin",
        "Wei-Chih Chen",
        "Ho Lam Chung",
        "Chen-An Li",
        "Yi-Chang Chen",
        "Chien-Yu Yu",
        "Ming-Ji Lee",
        "Chien-Cheng Chen",
        "Ru-Heng Huang",
        "Hung-yi Lee",
        "Da-Shan Shiu"
      ],
      "published": "2025-01-29T17:31:26Z",
      "updated": "2025-01-29T17:31:26Z",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2501.17790v1",
      "landing_url": "https://arxiv.org/abs/2501.17790v1",
      "doi": "https://doi.org/10.48550/arXiv.2501.17790"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The abstract explicitly mentions a discrete speech tokenizer (S^3 tokenizer) and a grapheme-to-phoneme model used within a TTS system, positioning discrete audio/tokenization as a core component, and the work includes system evaluation—aligning with the inclusion criterion, while not being solely engineering description or focusing exclusively on non-speech tokenization.",
      "evaluation": 4
    },
    "round-A_JuniorNano_reasoning": "The abstract explicitly mentions a discrete speech tokenizer (S^3 tokenizer) and a grapheme-to-phoneme model used within a TTS system, positioning discrete audio/tokenization as a core component, and the work includes system evaluation—aligning with the inclusion criterion, while not being solely engineering description or focusing exclusively on non-speech tokenization.",
    "round-A_JuniorNano_evaluation": 4,
    "round-A_JuniorMini_output": {
      "reasoning": "The study involves a Text-to-Speech system with advanced polyphone disambiguation and mentions phonetic control and grapheme-to-phoneme models, but it does not explicitly discuss discrete audio/speech tokenization as a core focus nor does it detail the design or analysis of discrete tokenization for audio reconstruction or generation, which is a key inclusion criterion.",
      "evaluation": 2
    },
    "round-A_JuniorMini_reasoning": "The study involves a Text-to-Speech system with advanced polyphone disambiguation and mentions phonetic control and grapheme-to-phoneme models, but it does not explicitly discuss discrete audio/speech tokenization as a core focus nor does it detail the design or analysis of discrete tokenization for audio reconstruction or generation, which is a key inclusion criterion.",
    "round-A_JuniorMini_evaluation": 2,
    "round-B_SeniorLead_output": {
      "reasoning": "Inclusion requires that discrete audio/speech tokenization be explicitly proposed, analyzed, or experimentally evaluated as the core object of study (i.e., converting continuous audio into discrete symbols); the abstract for BreezyVoice mentions incorporating an S^3 tokenizer and refers to neural codec TTS systems, but frames the tokenizer as a component used within a broader TTS engineering effort focused on phonetic control, polyphone disambiguation, and grapheme-to-phoneme modeling rather than presenting a methodological contribution, analysis, or evaluation of discrete tokenization itself. I reviewed the two junior reviewers' comments (one flagged the presence of an S^3 tokenizer and leaned toward inclusion; the other correctly pointed out the lack of explicit tokenization-focused analysis and leaned toward exclusion) and agree with the latter: the abstract does not provide sufficient evidence that discrete audio tokenization is a core research focus. One-sentence explanation: The paper appears to use an existing discrete tokenizer within a TTS system but does not explicitly propose, analyze, or evaluate discrete audio tokenization as the primary contribution, so it likely fails the inclusion criterion.",
      "evaluation": 2
    },
    "round-B_SeniorLead_reasoning": "Inclusion requires that discrete audio/speech tokenization be explicitly proposed, analyzed, or experimentally evaluated as the core object of study (i.e., converting continuous audio into discrete symbols); the abstract for BreezyVoice mentions incorporating an S^3 tokenizer and refers to neural codec TTS systems, but frames the tokenizer as a component used within a broader TTS engineering effort focused on phonetic control, polyphone disambiguation, and grapheme-to-phoneme modeling rather than presenting a methodological contribution, analysis, or evaluation of discrete tokenization itself. I reviewed the two junior reviewers' comments (one flagged the presence of an S^3 tokenizer and leaned toward inclusion; the other correctly pointed out the lack of explicit tokenization-focused analysis and leaned toward exclusion) and agree with the latter: the abstract does not provide sufficient evidence that discrete audio tokenization is a core research focus. One-sentence explanation: The paper appears to use an existing discrete tokenizer within a TTS system but does not explicitly propose, analyze, or evaluate discrete audio tokenization as the primary contribution, so it likely fails the inclusion criterion.",
    "round-B_SeniorLead_evaluation": 2,
    "final_verdict": "exclude (senior:2)",
    "review_skipped": false,
    "discard_reason": "exclude (senior:2)"
  },
  {
    "title": "Self-Supervised Learning Using Nonlinear Dependence",
    "abstract": "Self-supervised learning has gained significant attention in contemporary applications, particularly due to the scarcity of labeled data. While existing SSL methodologies primarily address feature variance and linear correlations, they often neglect the intricate relations between samples and the nonlinear dependencies inherent in complex data--especially prevalent in high-dimensional visual data. In this paper, we introduce Correlation-Dependence Self-Supervised Learning (CDSSL), a novel framework that unifies and extends existing SSL paradigms by integrating both linear correlations and nonlinear dependencies, encapsulating sample-wise and feature-wise interactions. Our approach incorporates the Hilbert-Schmidt Independence Criterion (HSIC) to robustly capture nonlinear dependencies within a Reproducing Kernel Hilbert Space, enriching representation learning. Experimental evaluations on diverse benchmarks demonstrate the efficacy of CDSSL in improving representation quality.",
    "metadata": {
      "arxiv_id": "2501.18875",
      "title": "Self-Supervised Learning Using Nonlinear Dependence",
      "summary": "Self-supervised learning has gained significant attention in contemporary applications, particularly due to the scarcity of labeled data. While existing SSL methodologies primarily address feature variance and linear correlations, they often neglect the intricate relations between samples and the nonlinear dependencies inherent in complex data--especially prevalent in high-dimensional visual data. In this paper, we introduce Correlation-Dependence Self-Supervised Learning (CDSSL), a novel framework that unifies and extends existing SSL paradigms by integrating both linear correlations and nonlinear dependencies, encapsulating sample-wise and feature-wise interactions. Our approach incorporates the Hilbert-Schmidt Independence Criterion (HSIC) to robustly capture nonlinear dependencies within a Reproducing Kernel Hilbert Space, enriching representation learning. Experimental evaluations on diverse benchmarks demonstrate the efficacy of CDSSL in improving representation quality.",
      "authors": [
        "M. Hadi Sepanj",
        "Benyamin Ghojogh",
        "Paul Fieguth"
      ],
      "published": "2025-01-31T04:11:34Z",
      "updated": "2025-11-16T22:59:47Z",
      "categories": [
        "cs.LG",
        "cs.CV",
        "stat.ML"
      ],
      "pdf_url": "https://arxiv.org/pdf/2501.18875v2",
      "landing_url": "https://arxiv.org/abs/2501.18875v2",
      "doi": "https://doi.org/10.1109/ACCESS.2025.3628158"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The abstract concerns a general self-supervised learning framework using HSIC to capture nonlinear dependencies, with no discrete audio/speech tokenization or analysis of discrete tokens, so it does not meet the inclusion criteria for discrete audio tokenization.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The abstract concerns a general self-supervised learning framework using HSIC to capture nonlinear dependencies, with no discrete audio/speech tokenization or analysis of discrete tokens, so it does not meet the inclusion criteria for discrete audio tokenization.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The abstract focuses on a self-supervised learning framework capturing nonlinear dependencies for representation learning, but it does not mention any audio or speech tokenization, discrete tokenization, or conversion of continuous audio signals into discrete symbols—key requirements for inclusion; hence, it does not meet the inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The abstract focuses on a self-supervised learning framework capturing nonlinear dependencies for representation learning, but it does not mention any audio or speech tokenization, discrete tokenization, or conversion of continuous audio signals into discrete symbols—key requirements for inclusion; hence, it does not meet the inclusion criteria and should be excluded.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "GenSE: Generative Speech Enhancement via Language Models using Hierarchical Modeling",
    "abstract": "Semantic information refers to the meaning conveyed through words, phrases, and contextual relationships within a given linguistic structure. Humans can leverage semantic information, such as familiar linguistic patterns and contextual cues, to reconstruct incomplete or masked speech signals in noisy environments. However, existing speech enhancement (SE) approaches often overlook the rich semantic information embedded in speech, which is crucial for improving intelligibility, speaker consistency, and overall quality of enhanced speech signals. To enrich the SE model with semantic information, we employ language models as an efficient semantic learner and propose a comprehensive framework tailored for language model-based speech enhancement, called \\textit{GenSE}. Specifically, we approach SE as a conditional language modeling task rather than a continuous signal regression problem defined in existing works. This is achieved by tokenizing speech signals into semantic tokens using a pre-trained self-supervised model and into acoustic tokens using a custom-designed single-quantizer neural codec model. To improve the stability of language model predictions, we propose a hierarchical modeling method that decouples the generation of clean semantic tokens and clean acoustic tokens into two distinct stages. Moreover, we introduce a token chain prompting mechanism during the acoustic token generation stage to ensure timbre consistency throughout the speech enhancement process. Experimental results on benchmark datasets demonstrate that our proposed approach outperforms state-of-the-art SE systems in terms of speech quality and generalization capability.",
    "metadata": {
      "arxiv_id": "2502.02942",
      "title": "GenSE: Generative Speech Enhancement via Language Models using Hierarchical Modeling",
      "summary": "Semantic information refers to the meaning conveyed through words, phrases, and contextual relationships within a given linguistic structure. Humans can leverage semantic information, such as familiar linguistic patterns and contextual cues, to reconstruct incomplete or masked speech signals in noisy environments. However, existing speech enhancement (SE) approaches often overlook the rich semantic information embedded in speech, which is crucial for improving intelligibility, speaker consistency, and overall quality of enhanced speech signals. To enrich the SE model with semantic information, we employ language models as an efficient semantic learner and propose a comprehensive framework tailored for language model-based speech enhancement, called \\textit{GenSE}. Specifically, we approach SE as a conditional language modeling task rather than a continuous signal regression problem defined in existing works. This is achieved by tokenizing speech signals into semantic tokens using a pre-trained self-supervised model and into acoustic tokens using a custom-designed single-quantizer neural codec model. To improve the stability of language model predictions, we propose a hierarchical modeling method that decouples the generation of clean semantic tokens and clean acoustic tokens into two distinct stages. Moreover, we introduce a token chain prompting mechanism during the acoustic token generation stage to ensure timbre consistency throughout the speech enhancement process. Experimental results on benchmark datasets demonstrate that our proposed approach outperforms state-of-the-art SE systems in terms of speech quality and generalization capability.",
      "authors": [
        "Jixun Yao",
        "Hexin Liu",
        "Chen Chen",
        "Yuchen Hu",
        "EngSiong Chng",
        "Lei Xie"
      ],
      "published": "2025-02-05T07:14:39Z",
      "updated": "2025-02-05T07:14:39Z",
      "categories": [
        "eess.AS",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2502.02942v1",
      "landing_url": "https://arxiv.org/abs/2502.02942v1",
      "doi": "https://doi.org/10.48550/arXiv.2502.02942"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "GenSE tokenizes speech into semantic tokens and acoustic tokens using discrete vocabulary-based representations and a neural codec, with hierarchical generation and token-chain prompting—clearly making discrete audio tokenization central to the method, satisfying inclusion and not falling under exclusions.",
      "evaluation": 5
    },
    "round-A_JuniorNano_reasoning": "GenSE tokenizes speech into semantic tokens and acoustic tokens using discrete vocabulary-based representations and a neural codec, with hierarchical generation and token-chain prompting—clearly making discrete audio tokenization central to the method, satisfying inclusion and not falling under exclusions.",
    "round-A_JuniorNano_evaluation": 5,
    "round-A_JuniorMini_output": {
      "reasoning": "The study explicitly proposes using discrete semantic and acoustic tokens derived from continuous speech signals through self-supervised and neural codec models, framing speech enhancement as a conditional language modeling task over these discrete tokens. It methodologically analyzes discrete token representations for speech enhancement and provides experimental validation on benchmark datasets, aligning with inclusion criteria and avoiding exclusion criteria related to purely continuous features, traditional codecs, or non-speech tokenization.",
      "evaluation": 5
    },
    "round-A_JuniorMini_reasoning": "The study explicitly proposes using discrete semantic and acoustic tokens derived from continuous speech signals through self-supervised and neural codec models, framing speech enhancement as a conditional language modeling task over these discrete tokens. It methodologically analyzes discrete token representations for speech enhancement and provides experimental validation on benchmark datasets, aligning with inclusion criteria and avoiding exclusion criteria related to purely continuous features, traditional codecs, or non-speech tokenization.",
    "round-A_JuniorMini_evaluation": 5,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "include (junior:5)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Metis: A Foundation Speech Generation Model with Masked Generative Pre-training",
    "abstract": "We introduce Metis, a foundation model for unified speech generation. Unlike previous task-specific or multi-task models, Metis follows a pre-training and fine-tuning paradigm. It is pre-trained on large-scale unlabeled speech data using masked generative modeling and then fine-tuned to adapt to diverse speech generation tasks. Specifically, 1) Metis utilizes two discrete speech representations: SSL tokens derived from speech self-supervised learning (SSL) features, and acoustic tokens directly quantized from waveforms. 2) Metis performs masked generative pre-training on SSL tokens, utilizing 300K hours of diverse speech data, without any additional condition. 3) Through fine-tuning with task-specific conditions, Metis achieves efficient adaptation to various speech generation tasks while supporting multimodal input, even when using limited data and trainable parameters. Experiments demonstrate that Metis can serve as a foundation model for unified speech generation: Metis outperforms state-of-the-art task-specific or multi-task systems across five speech generation tasks, including zero-shot text-to-speech, voice conversion, target speaker extraction, speech enhancement, and lip-to-speech, even with fewer than 20M trainable parameters or 300 times less training data. Audio samples are are available at https://metis-demo.github.io/.",
    "metadata": {
      "arxiv_id": "2502.03128",
      "title": "Metis: A Foundation Speech Generation Model with Masked Generative Pre-training",
      "summary": "We introduce Metis, a foundation model for unified speech generation. Unlike previous task-specific or multi-task models, Metis follows a pre-training and fine-tuning paradigm. It is pre-trained on large-scale unlabeled speech data using masked generative modeling and then fine-tuned to adapt to diverse speech generation tasks. Specifically, 1) Metis utilizes two discrete speech representations: SSL tokens derived from speech self-supervised learning (SSL) features, and acoustic tokens directly quantized from waveforms. 2) Metis performs masked generative pre-training on SSL tokens, utilizing 300K hours of diverse speech data, without any additional condition. 3) Through fine-tuning with task-specific conditions, Metis achieves efficient adaptation to various speech generation tasks while supporting multimodal input, even when using limited data and trainable parameters. Experiments demonstrate that Metis can serve as a foundation model for unified speech generation: Metis outperforms state-of-the-art task-specific or multi-task systems across five speech generation tasks, including zero-shot text-to-speech, voice conversion, target speaker extraction, speech enhancement, and lip-to-speech, even with fewer than 20M trainable parameters or 300 times less training data. Audio samples are are available at https://metis-demo.github.io/.",
      "authors": [
        "Yuancheng Wang",
        "Jiachen Zheng",
        "Junan Zhang",
        "Xueyao Zhang",
        "Huan Liao",
        "Zhizheng Wu"
      ],
      "published": "2025-02-05T12:36:21Z",
      "updated": "2025-02-05T12:36:21Z",
      "categories": [
        "cs.SD",
        "cs.AI",
        "cs.LG",
        "eess.AS",
        "eess.SP"
      ],
      "pdf_url": "https://arxiv.org/pdf/2502.03128v1",
      "landing_url": "https://arxiv.org/abs/2502.03128v1",
      "doi": "https://doi.org/10.48550/arXiv.2502.03128"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The work explicitly uses discrete speech representations (SSL tokens and acoustic tokens) as the core tokenization approach for speech generation, aligning with the inclusion criteria and not falling under the exclusion criteria.",
      "evaluation": 5
    },
    "round-A_JuniorNano_reasoning": "The work explicitly uses discrete speech representations (SSL tokens and acoustic tokens) as the core tokenization approach for speech generation, aligning with the inclusion criteria and not falling under the exclusion criteria.",
    "round-A_JuniorNano_evaluation": 5,
    "round-A_JuniorMini_output": {
      "reasoning": "The study, Metis, explicitly involves two discrete speech representations (SSL tokens and acoustic tokens) derived from speech self-supervised learning and quantized waveforms, which indicates a clear focus on discrete audio tokenization for speech generation tasks, meeting inclusion criteria and avoiding exclusion criteria related to continuous features or traditional codecs.",
      "evaluation": 5
    },
    "round-A_JuniorMini_reasoning": "The study, Metis, explicitly involves two discrete speech representations (SSL tokens and acoustic tokens) derived from speech self-supervised learning and quantized waveforms, which indicates a clear focus on discrete audio tokenization for speech generation tasks, meeting inclusion criteria and avoiding exclusion criteria related to continuous features or traditional codecs.",
    "round-A_JuniorMini_evaluation": 5,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "include (junior:5)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "DiTAR: Diffusion Transformer Autoregressive Modeling for Speech Generation",
    "abstract": "Several recent studies have attempted to autoregressively generate continuous speech representations without discrete speech tokens by combining diffusion and autoregressive models, yet they often face challenges with excessive computational loads or suboptimal outcomes. In this work, we propose Diffusion Transformer Autoregressive Modeling (DiTAR), a patch-based autoregressive framework combining a language model with a diffusion transformer. This approach significantly enhances the efficacy of autoregressive models for continuous tokens and reduces computational demands. DiTAR utilizes a divide-and-conquer strategy for patch generation, where the language model processes aggregated patch embeddings and the diffusion transformer subsequently generates the next patch based on the output of the language model. For inference, we propose defining temperature as the time point of introducing noise during the reverse diffusion ODE to balance diversity and determinism. We also show in the extensive scaling analysis that DiTAR has superb scalability. In zero-shot speech generation, DiTAR achieves state-of-the-art performance in robustness, speaker similarity, and naturalness.",
    "metadata": {
      "arxiv_id": "2502.03930",
      "title": "DiTAR: Diffusion Transformer Autoregressive Modeling for Speech Generation",
      "summary": "Several recent studies have attempted to autoregressively generate continuous speech representations without discrete speech tokens by combining diffusion and autoregressive models, yet they often face challenges with excessive computational loads or suboptimal outcomes. In this work, we propose Diffusion Transformer Autoregressive Modeling (DiTAR), a patch-based autoregressive framework combining a language model with a diffusion transformer. This approach significantly enhances the efficacy of autoregressive models for continuous tokens and reduces computational demands. DiTAR utilizes a divide-and-conquer strategy for patch generation, where the language model processes aggregated patch embeddings and the diffusion transformer subsequently generates the next patch based on the output of the language model. For inference, we propose defining temperature as the time point of introducing noise during the reverse diffusion ODE to balance diversity and determinism. We also show in the extensive scaling analysis that DiTAR has superb scalability. In zero-shot speech generation, DiTAR achieves state-of-the-art performance in robustness, speaker similarity, and naturalness.",
      "authors": [
        "Dongya Jia",
        "Zhuo Chen",
        "Jiawei Chen",
        "Chenpeng Du",
        "Jian Wu",
        "Jian Cong",
        "Xiaobin Zhuang",
        "Chumin Li",
        "Zhen Wei",
        "Yuping Wang",
        "Yuxuan Wang"
      ],
      "published": "2025-02-06T10:09:49Z",
      "updated": "2025-12-08T08:11:20Z",
      "categories": [
        "eess.AS",
        "cs.AI",
        "cs.CL",
        "cs.LG",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2502.03930v4",
      "landing_url": "https://arxiv.org/abs/2502.03930v4",
      "doi": "https://doi.org/10.48550/arXiv.2502.03930"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The abstract describes continuous speech representations and a patch-based autoregressive framework without explicit discrete speech tokenization or a finite-token vocabulary, so it does not meet the core inclusion criteria and triggers exclusion.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The abstract describes continuous speech representations and a patch-based autoregressive framework without explicit discrete speech tokenization or a finite-token vocabulary, so it does not meet the core inclusion criteria and triggers exclusion.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study proposes DiTAR, a diffusion transformer autoregressive model for speech generation working with continuous speech representations without discrete speech tokens, and does not explicitly focus on discrete audio/speech tokenization nor analyze or evaluate discrete tokenization methods as required by the inclusion criteria; thus, it uses continuous tokens rather than discrete quantization from a finite vocabulary, aligning with the exclusion criteria that exclude works only using continuous audio features without discrete token design.",
      "evaluation": 2
    },
    "round-A_JuniorMini_reasoning": "The study proposes DiTAR, a diffusion transformer autoregressive model for speech generation working with continuous speech representations without discrete speech tokens, and does not explicitly focus on discrete audio/speech tokenization nor analyze or evaluate discrete tokenization methods as required by the inclusion criteria; thus, it uses continuous tokens rather than discrete quantization from a finite vocabulary, aligning with the exclusion criteria that exclude works only using continuous audio features without discrete token design.",
    "round-A_JuniorMini_evaluation": 2,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:2)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:2)"
  },
  {
    "title": "GenVC: Self-Supervised Zero-Shot Voice Conversion",
    "abstract": "Most current zero-shot voice conversion methods rely on externally supervised components, particularly speaker encoders, for training. To explore alternatives that eliminate this dependency, this paper introduces GenVC, a novel framework that disentangles speaker identity and linguistic content from speech signals in a self-supervised manner. GenVC leverages speech tokenizers and an autoregressive, Transformer-based language model as its backbone for speech generation. This design supports large-scale training while enhancing both source speaker privacy protection and target speaker cloning fidelity. Experimental results demonstrate that GenVC achieves notably higher speaker similarity, with naturalness on par with leading zero-shot approaches. Moreover, due to its autoregressive formulation, GenVC introduces flexibility in temporal alignment, reducing the preservation of source prosody and speaker-specific traits, and making it highly effective for voice anonymization.",
    "metadata": {
      "arxiv_id": "2502.04519",
      "title": "GenVC: Self-Supervised Zero-Shot Voice Conversion",
      "summary": "Most current zero-shot voice conversion methods rely on externally supervised components, particularly speaker encoders, for training. To explore alternatives that eliminate this dependency, this paper introduces GenVC, a novel framework that disentangles speaker identity and linguistic content from speech signals in a self-supervised manner. GenVC leverages speech tokenizers and an autoregressive, Transformer-based language model as its backbone for speech generation. This design supports large-scale training while enhancing both source speaker privacy protection and target speaker cloning fidelity. Experimental results demonstrate that GenVC achieves notably higher speaker similarity, with naturalness on par with leading zero-shot approaches. Moreover, due to its autoregressive formulation, GenVC introduces flexibility in temporal alignment, reducing the preservation of source prosody and speaker-specific traits, and making it highly effective for voice anonymization.",
      "authors": [
        "Zexin Cai",
        "Henry Li Xinyuan",
        "Ashi Garg",
        "Leibny Paola García-Perera",
        "Kevin Duh",
        "Sanjeev Khudanpur",
        "Matthew Wiesner",
        "Nicholas Andrews"
      ],
      "published": "2025-02-06T21:40:09Z",
      "updated": "2025-08-20T17:34:21Z",
      "categories": [
        "eess.AS",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2502.04519v2",
      "landing_url": "https://arxiv.org/abs/2502.04519v2",
      "doi": "https://doi.org/10.48550/arXiv.2502.04519"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The abstract explicitly mentions 'speech tokenizers' and uses discrete tokenization of speech as part of GenVC for self-supervised voice conversion, indicating a core focus on converting continuous audio into discrete tokens for generation; this aligns with inclusion criteria and does not fit the exclusion of relying solely on continuous features.",
      "evaluation": 4
    },
    "round-A_JuniorNano_reasoning": "The abstract explicitly mentions 'speech tokenizers' and uses discrete tokenization of speech as part of GenVC for self-supervised voice conversion, indicating a core focus on converting continuous audio into discrete tokens for generation; this aligns with inclusion criteria and does not fit the exclusion of relying solely on continuous features.",
    "round-A_JuniorNano_evaluation": 4,
    "round-A_JuniorMini_output": {
      "reasoning": "The abstract explicitly mentions the use of speech tokenizers and a Transformer-based model to disentangle speaker and linguistic content in a self-supervised manner, implying discrete audio tokenization is a core component for speech generation and voice conversion tasks; thus, it meets the inclusion criteria and does not fall under any exclusion criteria related to continuous features only or engineering-only reports.",
      "evaluation": 5
    },
    "round-A_JuniorMini_reasoning": "The abstract explicitly mentions the use of speech tokenizers and a Transformer-based model to disentangle speaker and linguistic content in a self-supervised manner, implying discrete audio tokenization is a core component for speech generation and voice conversion tasks; thus, it meets the inclusion criteria and does not fall under any exclusion criteria related to continuous features only or engineering-only reports.",
    "round-A_JuniorMini_evaluation": 5,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "include (junior:4)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Efficient Evaluation of Quantization-Effects in Neural Codecs",
    "abstract": "Neural codecs, comprising an encoder, quantizer, and decoder, enable signal transmission at exceptionally low bitrates. Training these systems requires techniques like the straight-through estimator, soft-to-hard annealing, or statistical quantizer emulation to allow a non-zero gradient across the quantizer. Evaluating the effect of quantization in neural codecs, like the influence of gradient passing techniques on the whole system, is often costly and time-consuming due to training demands and the lack of affordable and reliable metrics. This paper proposes an efficient evaluation framework for neural codecs using simulated data with a defined number of bits and low-complexity neural encoders/decoders to emulate the non-linear behavior in larger networks. Our system is highly efficient in terms of training time and computational and hardware requirements, allowing us to uncover distinct behaviors in neural codecs. We propose a modification to stabilize training with the straight-through estimator based on our findings. We validate our findings against an internal neural audio codec and against the state-of-the-art descript-audio-codec.",
    "metadata": {
      "arxiv_id": "2502.04770",
      "title": "Efficient Evaluation of Quantization-Effects in Neural Codecs",
      "summary": "Neural codecs, comprising an encoder, quantizer, and decoder, enable signal transmission at exceptionally low bitrates. Training these systems requires techniques like the straight-through estimator, soft-to-hard annealing, or statistical quantizer emulation to allow a non-zero gradient across the quantizer. Evaluating the effect of quantization in neural codecs, like the influence of gradient passing techniques on the whole system, is often costly and time-consuming due to training demands and the lack of affordable and reliable metrics. This paper proposes an efficient evaluation framework for neural codecs using simulated data with a defined number of bits and low-complexity neural encoders/decoders to emulate the non-linear behavior in larger networks. Our system is highly efficient in terms of training time and computational and hardware requirements, allowing us to uncover distinct behaviors in neural codecs. We propose a modification to stabilize training with the straight-through estimator based on our findings. We validate our findings against an internal neural audio codec and against the state-of-the-art descript-audio-codec.",
      "authors": [
        "Wolfgang Mack",
        "Ahmed Mustafa",
        "Rafał Łaganowski",
        "Samer Hijazy"
      ],
      "published": "2025-02-07T09:11:19Z",
      "updated": "2025-02-07T09:11:19Z",
      "categories": [
        "eess.AS",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2502.04770v1",
      "landing_url": "https://arxiv.org/abs/2502.04770v1",
      "doi": "https://doi.org/10.48550/arXiv.2502.04770"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The abstract centers on discrete quantization in neural codecs (quantizers producing discrete tokens) and proposes an efficient evaluation framework for quantization effects, including gradient-through techniques, which aligns with analyzing discrete audio tokenization as a core object of study.",
      "evaluation": 5
    },
    "round-A_JuniorNano_reasoning": "The abstract centers on discrete quantization in neural codecs (quantizers producing discrete tokens) and proposes an efficient evaluation framework for quantization effects, including gradient-through techniques, which aligns with analyzing discrete audio tokenization as a core object of study.",
    "round-A_JuniorNano_evaluation": 5,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on neural codecs which involve a quantizer component, suggesting discrete quantization is a core part of the method; however, the abstract does not explicitly mention discrete audio/speech tokenization as defined (conversion to discrete symbols for token-based tasks), nor does it clearly address tokenization for speech tasks specifically; thus, it partially meets inclusion criteria but lacks explicit analysis or evaluation of discrete tokenization, creating uncertainty.",
      "evaluation": 3
    },
    "round-A_JuniorMini_reasoning": "The study focuses on neural codecs which involve a quantizer component, suggesting discrete quantization is a core part of the method; however, the abstract does not explicitly mention discrete audio/speech tokenization as defined (conversion to discrete symbols for token-based tasks), nor does it clearly address tokenization for speech tasks specifically; thus, it partially meets inclusion criteria but lacks explicit analysis or evaluation of discrete tokenization, creating uncertainty.",
    "round-A_JuniorMini_evaluation": 3,
    "round-B_SeniorLead_output": {
      "reasoning": "Junior reviewers: one recommended inclusion (5) noting the focus on quantization in neural codecs, the other was uncertain (3) because the abstract does not explicitly state conversion to discrete token sequences for audio/speech tasks. My assessment: the abstract discusses quantizers and bit-level evaluation in neural codecs but does not explicitly propose or analyze discrete audio/speech tokenization as a sequence-of-symbols tokenization required by the inclusion criteria, so inclusion is ambiguous.",
      "evaluation": 3
    },
    "round-B_SeniorLead_reasoning": "Junior reviewers: one recommended inclusion (5) noting the focus on quantization in neural codecs, the other was uncertain (3) because the abstract does not explicitly state conversion to discrete token sequences for audio/speech tasks. My assessment: the abstract discusses quantizers and bit-level evaluation in neural codecs but does not explicitly propose or analyze discrete audio/speech tokenization as a sequence-of-symbols tokenization required by the inclusion criteria, so inclusion is ambiguous.",
    "round-B_SeniorLead_evaluation": 3,
    "final_verdict": "maybe (senior:3)",
    "review_skipped": false,
    "discard_reason": "review_needs_followup"
  },
  {
    "title": "Koel-TTS: Enhancing LLM based Speech Generation with Preference Alignment and Classifier Free Guidance",
    "abstract": "While autoregressive speech token generation models produce speech with remarkable variety and naturalness, their inherent lack of controllability often results in issues such as hallucinations and undesired vocalizations that do not conform to conditioning inputs. We introduce Koel-TTS, a suite of enhanced encoder-decoder Transformer TTS models that address these challenges by incorporating preference alignment techniques guided by automatic speech recognition and speaker verification models. Additionally, we incorporate classifier-free guidance to further improve synthesis adherence to the transcript and reference speaker audio. Our experiments demonstrate that these optimizations significantly enhance target speaker similarity, intelligibility, and naturalness of synthesized speech. Notably, Koel-TTS directly maps text and context audio to acoustic tokens, and on the aforementioned metrics, outperforms state-of-the-art TTS models, despite being trained on a significantly smaller dataset. Audio samples and demos are available on our website.",
    "metadata": {
      "arxiv_id": "2502.05236",
      "title": "Koel-TTS: Enhancing LLM based Speech Generation with Preference Alignment and Classifier Free Guidance",
      "summary": "While autoregressive speech token generation models produce speech with remarkable variety and naturalness, their inherent lack of controllability often results in issues such as hallucinations and undesired vocalizations that do not conform to conditioning inputs. We introduce Koel-TTS, a suite of enhanced encoder-decoder Transformer TTS models that address these challenges by incorporating preference alignment techniques guided by automatic speech recognition and speaker verification models. Additionally, we incorporate classifier-free guidance to further improve synthesis adherence to the transcript and reference speaker audio. Our experiments demonstrate that these optimizations significantly enhance target speaker similarity, intelligibility, and naturalness of synthesized speech. Notably, Koel-TTS directly maps text and context audio to acoustic tokens, and on the aforementioned metrics, outperforms state-of-the-art TTS models, despite being trained on a significantly smaller dataset. Audio samples and demos are available on our website.",
      "authors": [
        "Shehzeen Hussain",
        "Paarth Neekhara",
        "Xuesong Yang",
        "Edresson Casanova",
        "Subhankar Ghosh",
        "Mikyas T. Desta",
        "Roy Fejgin",
        "Rafael Valle",
        "Jason Li"
      ],
      "published": "2025-02-07T06:47:11Z",
      "updated": "2025-07-22T21:32:13Z",
      "categories": [
        "cs.SD",
        "cs.AI",
        "cs.LG",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2502.05236v2",
      "landing_url": "https://arxiv.org/abs/2502.05236v2",
      "doi": "https://doi.org/10.48550/arXiv.2502.05236"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The work explicitly mentions mapping text and context audio to acoustic tokens, indicating a discrete acoustic tokenization approach as a core representation for speech generation, which aligns with the inclusion criteria and excludes none of the criteria.",
      "evaluation": 5
    },
    "round-A_JuniorNano_reasoning": "The work explicitly mentions mapping text and context audio to acoustic tokens, indicating a discrete acoustic tokenization approach as a core representation for speech generation, which aligns with the inclusion criteria and excludes none of the criteria.",
    "round-A_JuniorNano_evaluation": 5,
    "round-A_JuniorMini_output": {
      "reasoning": "The abstract explicitly mentions that Koel-TTS maps text and context audio directly to discrete acoustic tokens, indicating that discrete tokenization of audio/speech is central to the study, along with evaluation and analysis of these discrete tokens in the context of text-to-speech synthesis; the study appears to go beyond using continuous audio features alone, is clearly focused on speech, and includes experimental validation rather than being an engineering-only report, thus meeting all inclusion and none of the exclusion criteria.",
      "evaluation": 5
    },
    "round-A_JuniorMini_reasoning": "The abstract explicitly mentions that Koel-TTS maps text and context audio directly to discrete acoustic tokens, indicating that discrete tokenization of audio/speech is central to the study, along with evaluation and analysis of these discrete tokens in the context of text-to-speech synthesis; the study appears to go beyond using continuous audio features alone, is clearly focused on speech, and includes experimental validation rather than being an engineering-only report, thus meeting all inclusion and none of the exclusion criteria.",
    "round-A_JuniorMini_evaluation": 5,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "include (junior:5)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "SQ-GAN: Semantic Image Communications Using Masked Vector Quantization",
    "abstract": "This work introduces Semantically Masked Vector Quantized Generative Adversarial Network (SQ-GAN), a novel approach integrating semantically driven image coding and vector quantization to optimize image compression for semantic/task-oriented communications. The method only acts on source coding and is fully compliant with legacy systems. The semantics is extracted from the image computing its semantic segmentation map using off-the-shelf software. A new specifically developed semantic-conditioned adaptive mask module (SAMM) selectively encodes semantically relevant features of the image. The relevance of the different semantic classes is task-specific, and it is incorporated in the training phase by introducing appropriate weights in the loss function. SQ-GAN outperforms state-of-the-art image compression schemes such as JPEG2000, BPG, and deep-learning based methods across multiple metrics, including perceptual quality and semantic segmentation accuracy on the reconstructed image, at extremely low compression rates.",
    "metadata": {
      "arxiv_id": "2502.09520",
      "title": "SQ-GAN: Semantic Image Communications Using Masked Vector Quantization",
      "summary": "This work introduces Semantically Masked Vector Quantized Generative Adversarial Network (SQ-GAN), a novel approach integrating semantically driven image coding and vector quantization to optimize image compression for semantic/task-oriented communications. The method only acts on source coding and is fully compliant with legacy systems. The semantics is extracted from the image computing its semantic segmentation map using off-the-shelf software. A new specifically developed semantic-conditioned adaptive mask module (SAMM) selectively encodes semantically relevant features of the image. The relevance of the different semantic classes is task-specific, and it is incorporated in the training phase by introducing appropriate weights in the loss function. SQ-GAN outperforms state-of-the-art image compression schemes such as JPEG2000, BPG, and deep-learning based methods across multiple metrics, including perceptual quality and semantic segmentation accuracy on the reconstructed image, at extremely low compression rates.",
      "authors": [
        "Francesco Pezone",
        "Sergio Barbarossa",
        "Giuseppe Caire"
      ],
      "published": "2025-02-13T17:35:57Z",
      "updated": "2025-10-10T10:21:13Z",
      "categories": [
        "cs.CV",
        "eess.IV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2502.09520v2",
      "landing_url": "https://arxiv.org/abs/2502.09520v2",
      "doi": "https://doi.org/10.48550/arXiv.2502.09520"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The submission focuses on semantic image communication and vector-quantized image compression, with none of the discrete audio tokenization or speech-token design analyses required by the inclusion criteria; it therefore does not meet the core criterion.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The submission focuses on semantic image communication and vector-quantized image compression, with none of the discrete audio tokenization or speech-token design analyses required by the inclusion criteria; it therefore does not meet the core criterion.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on semantic image communication and compression using generative adversarial networks and vector quantization but does not involve audio or speech tokenization, nor does it analyze or propose discrete tokenization of audio signals, which is the core inclusion criterion.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on semantic image communication and compression using generative adversarial networks and vector quantization but does not involve audio or speech tokenization, nor does it analyze or propose discrete tokenization of audio signals, which is the core inclusion criterion.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "SyncSpeech: Low-Latency and Efficient Dual-Stream Text-to-Speech based on Temporal Masked Transformer",
    "abstract": "This paper presents a dual-stream text-to-speech (TTS) model, SyncSpeech, capable of receiving streaming text input from upstream models while simultaneously generating streaming speech, facilitating seamless interaction with large language models. SyncSpeech has the following advantages: Low latency, as it begins generating streaming speech upon receiving the second text token; High efficiency, as it decodes all speech tokens corresponding to the each arrived text token in one step. To achieve this, we propose a temporal masked transformer as the backbone of SyncSpeech, combined with token-level duration prediction to predict speech tokens and the duration for the next step. Additionally, we design a two-stage training strategy to improve training efficiency and the quality of generated speech. We evaluated the SyncSpeech on both English and Mandarin datasets. Compared to the recent dual-stream TTS models, SyncSpeech significantly reduces the first packet delay of speech tokens and accelerates the real-time factor. Moreover, with the same data scale, SyncSpeech achieves performance comparable to that of traditional autoregressive-based TTS models in terms of both speech quality and robustness. Speech samples are available at https://SyncSpeech.github.io/}{https://SyncSpeech.github.io/.",
    "metadata": {
      "arxiv_id": "2502.11094",
      "title": "SyncSpeech: Low-Latency and Efficient Dual-Stream Text-to-Speech based on Temporal Masked Transformer",
      "summary": "This paper presents a dual-stream text-to-speech (TTS) model, SyncSpeech, capable of receiving streaming text input from upstream models while simultaneously generating streaming speech, facilitating seamless interaction with large language models. SyncSpeech has the following advantages: Low latency, as it begins generating streaming speech upon receiving the second text token; High efficiency, as it decodes all speech tokens corresponding to the each arrived text token in one step. To achieve this, we propose a temporal masked transformer as the backbone of SyncSpeech, combined with token-level duration prediction to predict speech tokens and the duration for the next step. Additionally, we design a two-stage training strategy to improve training efficiency and the quality of generated speech. We evaluated the SyncSpeech on both English and Mandarin datasets. Compared to the recent dual-stream TTS models, SyncSpeech significantly reduces the first packet delay of speech tokens and accelerates the real-time factor. Moreover, with the same data scale, SyncSpeech achieves performance comparable to that of traditional autoregressive-based TTS models in terms of both speech quality and robustness. Speech samples are available at https://SyncSpeech.github.io/}{https://SyncSpeech.github.io/.",
      "authors": [
        "Zhengyan Sheng",
        "Zhihao Du",
        "Shiliang Zhang",
        "Zhijie Yan",
        "Yexin Yang",
        "Zhenhua Ling"
      ],
      "published": "2025-02-16T12:14:17Z",
      "updated": "2025-02-16T12:14:17Z",
      "categories": [
        "cs.SD",
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2502.11094v1",
      "landing_url": "https://arxiv.org/abs/2502.11094v1",
      "doi": "https://doi.org/10.48550/arXiv.2502.11094"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The abstract discusses a dual-stream TTS model using speech tokens, but it does not explicitly analyze or evaluate discrete audio/speech tokenization as a core object (e.g., a discrete vocabulary of speech tokens for reconstruction). It focuses on model architecture and latency rather than a methodological study of discrete tokenization itself, so it does not meet the inclusion criteria and should be excluded.",
      "evaluation": 2
    },
    "round-A_JuniorNano_reasoning": "The abstract discusses a dual-stream TTS model using speech tokens, but it does not explicitly analyze or evaluate discrete audio/speech tokenization as a core object (e.g., a discrete vocabulary of speech tokens for reconstruction). It focuses on model architecture and latency rather than a methodological study of discrete tokenization itself, so it does not meet the inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 2,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper proposes SyncSpeech, a dual-stream text-to-speech model focused on streaming speech generation efficiency and low latency, but the abstract does not mention any discrete audio/speech tokenization methods or analysis where continuous audio is converted into discrete tokens, nor any neural quantization or discrete token representations. Instead, it discusses decoding speech tokens corresponding to text tokens and duration prediction, but without clear evidence of discrete tokenization as a core object of study relevant to the inclusion criteria. Therefore, it does not sufficiently meet the key inclusion criterion of explicit discrete audio tokenization analysis or design.",
      "evaluation": 2
    },
    "round-A_JuniorMini_reasoning": "The paper proposes SyncSpeech, a dual-stream text-to-speech model focused on streaming speech generation efficiency and low latency, but the abstract does not mention any discrete audio/speech tokenization methods or analysis where continuous audio is converted into discrete tokens, nor any neural quantization or discrete token representations. Instead, it discusses decoding speech tokens corresponding to text tokens and duration prediction, but without clear evidence of discrete tokenization as a core object of study relevant to the inclusion criteria. Therefore, it does not sufficiently meet the key inclusion criterion of explicit discrete audio tokenization analysis or design.",
    "round-A_JuniorMini_evaluation": 2,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:2)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:2)"
  },
  {
    "title": "DLFR-VAE: Dynamic Latent Frame Rate VAE for Video Generation",
    "abstract": "In this paper, we propose the Dynamic Latent Frame Rate VAE (DLFR-VAE), a training-free paradigm that can make use of adaptive temporal compression in latent space. While existing video generative models apply fixed compression rates via pretrained VAE, we observe that real-world video content exhibits substantial temporal non-uniformity, with high-motion segments containing more information than static scenes. Based on this insight, DLFR-VAE dynamically adjusts the latent frame rate according to the content complexity. Specifically, DLFR-VAE comprises two core innovations: (1) A Dynamic Latent Frame Rate Scheduler that partitions videos into temporal chunks and adaptively determines optimal frame rates based on information-theoretic content complexity, and (2) A training-free adaptation mechanism that transforms pretrained VAE architectures into a dynamic VAE that can process features with variable frame rates. Our simple but effective DLFR-VAE can function as a plug-and-play module, seamlessly integrating with existing video generation models and accelerating the video generation process.",
    "metadata": {
      "arxiv_id": "2502.11897",
      "title": "DLFR-VAE: Dynamic Latent Frame Rate VAE for Video Generation",
      "summary": "In this paper, we propose the Dynamic Latent Frame Rate VAE (DLFR-VAE), a training-free paradigm that can make use of adaptive temporal compression in latent space. While existing video generative models apply fixed compression rates via pretrained VAE, we observe that real-world video content exhibits substantial temporal non-uniformity, with high-motion segments containing more information than static scenes. Based on this insight, DLFR-VAE dynamically adjusts the latent frame rate according to the content complexity. Specifically, DLFR-VAE comprises two core innovations: (1) A Dynamic Latent Frame Rate Scheduler that partitions videos into temporal chunks and adaptively determines optimal frame rates based on information-theoretic content complexity, and (2) A training-free adaptation mechanism that transforms pretrained VAE architectures into a dynamic VAE that can process features with variable frame rates. Our simple but effective DLFR-VAE can function as a plug-and-play module, seamlessly integrating with existing video generation models and accelerating the video generation process.",
      "authors": [
        "Zhihang Yuan",
        "Siyuan Wang",
        "Rui Xie",
        "Hanling Zhang",
        "Tongcheng Fang",
        "Yuzhang Shang",
        "Shengen Yan",
        "Guohao Dai",
        "Yu Wang"
      ],
      "published": "2025-02-17T15:22:31Z",
      "updated": "2025-04-02T13:25:35Z",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2502.11897v2",
      "landing_url": "https://arxiv.org/abs/2502.11897v2",
      "doi": "https://doi.org/10.48550/arXiv.2502.11897"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This study targets dynamic latent frame rate in video generation using a VAE and does not propose or analyze discrete audio/speech tokenization or discrete audio representations for language tasks.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "This study targets dynamic latent frame rate in video generation using a VAE and does not propose or analyze discrete audio/speech tokenization or discrete audio representations for language tasks.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study proposes a video generation model focusing on adaptive temporal compression in latent space for videos. It does not address audio or speech tokenization, nor does it discuss converting continuous audio signals into discrete tokens, which is a core inclusion criterion. Hence, it does not meet the inclusion criteria and falls outside the relevant domain.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study proposes a video generation model focusing on adaptive temporal compression in latent space for videos. It does not address audio or speech tokenization, nor does it discuss converting continuous audio signals into discrete tokens, which is a core inclusion criterion. Hence, it does not meet the inclusion criteria and falls outside the relevant domain.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "From Principles to Applications: A Comprehensive Survey of Discrete Tokenizers in Generation, Comprehension, Recommendation, and Information Retrieval",
    "abstract": "Discrete tokenizers have emerged as indispensable components in modern machine learning systems, particularly within the context of autoregressive modeling and large language models (LLMs). These tokenizers serve as the critical interface that transforms raw, unstructured data from diverse modalities into discrete tokens, enabling LLMs to operate effectively across a wide range of tasks. Despite their central role in generation, comprehension, and recommendation systems, a comprehensive survey dedicated to discrete tokenizers remains conspicuously absent in the literature. This paper addresses this gap by providing a systematic review of the design principles, applications, and challenges of discrete tokenizers. We begin by dissecting the sub-modules of tokenizers and systematically demonstrate their internal mechanisms to provide a comprehensive understanding of their functionality and design. Building on this foundation, we synthesize state-of-the-art methods, categorizing them into multimodal generation and comprehension tasks, and semantic tokens for personalized recommendations. Furthermore, we critically analyze the limitations of existing tokenizers and outline promising directions for future research. By presenting a unified framework for understanding discrete tokenizers, this survey aims to guide researchers and practitioners in addressing open challenges and advancing the field, ultimately contributing to the development of more robust and versatile AI systems.",
    "metadata": {
      "arxiv_id": "2502.12448",
      "title": "From Principles to Applications: A Comprehensive Survey of Discrete Tokenizers in Generation, Comprehension, Recommendation, and Information Retrieval",
      "summary": "Discrete tokenizers have emerged as indispensable components in modern machine learning systems, particularly within the context of autoregressive modeling and large language models (LLMs). These tokenizers serve as the critical interface that transforms raw, unstructured data from diverse modalities into discrete tokens, enabling LLMs to operate effectively across a wide range of tasks. Despite their central role in generation, comprehension, and recommendation systems, a comprehensive survey dedicated to discrete tokenizers remains conspicuously absent in the literature. This paper addresses this gap by providing a systematic review of the design principles, applications, and challenges of discrete tokenizers. We begin by dissecting the sub-modules of tokenizers and systematically demonstrate their internal mechanisms to provide a comprehensive understanding of their functionality and design. Building on this foundation, we synthesize state-of-the-art methods, categorizing them into multimodal generation and comprehension tasks, and semantic tokens for personalized recommendations. Furthermore, we critically analyze the limitations of existing tokenizers and outline promising directions for future research. By presenting a unified framework for understanding discrete tokenizers, this survey aims to guide researchers and practitioners in addressing open challenges and advancing the field, ultimately contributing to the development of more robust and versatile AI systems.",
      "authors": [
        "Jian Jia",
        "Jingtong Gao",
        "Ben Xue",
        "Junhao Wang",
        "Qingpeng Cai",
        "Quan Chen",
        "Xiangyu Zhao",
        "Peng Jiang",
        "Kun Gai"
      ],
      "published": "2025-02-18T02:29:51Z",
      "updated": "2025-02-18T02:29:51Z",
      "categories": [
        "cs.IR"
      ],
      "pdf_url": "https://arxiv.org/pdf/2502.12448v1",
      "landing_url": "https://arxiv.org/abs/2502.12448v1",
      "doi": "https://doi.org/10.48550/arXiv.2502.12448"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The abstract presents a general survey of discrete tokenizers across modalities and applications but does not treat discrete audio/speech tokenization as the core object, nor analyze discrete audio representations for speech tasks, thus failing the inclusion criterion that requires explicit focus on discrete audio tokenization for speech-related tasks.",
      "evaluation": 2
    },
    "round-A_JuniorNano_reasoning": "The abstract presents a general survey of discrete tokenizers across modalities and applications but does not treat discrete audio/speech tokenization as the core object, nor analyze discrete audio representations for speech tasks, thus failing the inclusion criterion that requires explicit focus on discrete audio tokenization for speech-related tasks.",
    "round-A_JuniorNano_evaluation": 2,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper provides a comprehensive survey of discrete tokenizers across multiple modalities including generation, comprehension, and recommendation, but the abstract does not explicitly mention discrete audio or speech tokenization, nor does it focus solely on audio signals. The emphasis is broad and multimodal, with no clear core object study pertaining to discrete audio/speech tokenization specifically, which is required by the inclusion criteria. Therefore, it does not clearly meet the inclusion criteria nor is it specifically excluded based on available information, leading to uncertainty.",
      "evaluation": 3
    },
    "round-A_JuniorMini_reasoning": "The paper provides a comprehensive survey of discrete tokenizers across multiple modalities including generation, comprehension, and recommendation, but the abstract does not explicitly mention discrete audio or speech tokenization, nor does it focus solely on audio signals. The emphasis is broad and multimodal, with no clear core object study pertaining to discrete audio/speech tokenization specifically, which is required by the inclusion criteria. Therefore, it does not clearly meet the inclusion criteria nor is it specifically excluded based on available information, leading to uncertainty.",
    "round-A_JuniorMini_evaluation": 3,
    "round-B_SeniorLead_output": {
      "reasoning": "Both junior reviewers noted the abstract is a broad, multimodal survey of discrete tokenizers and does not explicitly propose, analyze, or evaluate discrete audio/speech tokenization as the core object of study; the inclusion criteria require an explicit focus on converting continuous audio into discrete symbols for speech reconstruction/generation/understanding, which is not present in the abstract. Therefore, better to exclude. (One-sentence explanation: The paper is a general multimodal tokenizer survey and does not explicitly address discrete audio/speech tokenization, so it fails the inclusion criteria.)",
      "evaluation": 2
    },
    "round-B_SeniorLead_reasoning": "Both junior reviewers noted the abstract is a broad, multimodal survey of discrete tokenizers and does not explicitly propose, analyze, or evaluate discrete audio/speech tokenization as the core object of study; the inclusion criteria require an explicit focus on converting continuous audio into discrete symbols for speech reconstruction/generation/understanding, which is not present in the abstract. Therefore, better to exclude. (One-sentence explanation: The paper is a general multimodal tokenizer survey and does not explicitly address discrete audio/speech tokenization, so it fails the inclusion criteria.)",
    "round-B_SeniorLead_evaluation": 2,
    "final_verdict": "exclude (senior:2)",
    "review_skipped": false,
    "discard_reason": "exclude (senior:2)"
  },
  {
    "title": "Speech Enhancement Using Continuous Embeddings of Neural Audio Codec",
    "abstract": "Recent advancements in Neural Audio Codec (NAC) models have inspired their use in various speech processing tasks, including speech enhancement (SE). In this work, we propose a novel, efficient SE approach by leveraging the pre-quantization output of a pretrained NAC encoder. Unlike prior NAC-based SE methods, which process discrete speech tokens using Language Models (LMs), we perform SE within the continuous embedding space of the pretrained NAC, which is highly compressed along the time dimension for efficient representation. Our lightweight SE model, optimized through an embedding-level loss, delivers results comparable to SE baselines trained on larger datasets, with a significantly lower real-time factor of 0.005. Additionally, our method achieves a low GMAC of 3.94, reducing complexity 18-fold compared to Sepformer in a simulated cloud-based audio transmission environment. This work highlights a new, efficient NAC-based SE solution, particularly suitable for cloud applications where NAC is used to compress audio before transmission. Copyright 20XX IEEE. Personal use of this material is permitted. Permission from IEEE must be obtained for all other uses, in any current or future media, including reprinting/republishing this material for advertising or promotional purposes, creating new collective works, for resale or redistribution to servers or lists, or reuse of any copyrighted component of this work in other works.",
    "metadata": {
      "arxiv_id": "2502.16240",
      "title": "Speech Enhancement Using Continuous Embeddings of Neural Audio Codec",
      "summary": "Recent advancements in Neural Audio Codec (NAC) models have inspired their use in various speech processing tasks, including speech enhancement (SE). In this work, we propose a novel, efficient SE approach by leveraging the pre-quantization output of a pretrained NAC encoder. Unlike prior NAC-based SE methods, which process discrete speech tokens using Language Models (LMs), we perform SE within the continuous embedding space of the pretrained NAC, which is highly compressed along the time dimension for efficient representation. Our lightweight SE model, optimized through an embedding-level loss, delivers results comparable to SE baselines trained on larger datasets, with a significantly lower real-time factor of 0.005. Additionally, our method achieves a low GMAC of 3.94, reducing complexity 18-fold compared to Sepformer in a simulated cloud-based audio transmission environment. This work highlights a new, efficient NAC-based SE solution, particularly suitable for cloud applications where NAC is used to compress audio before transmission.\n  Copyright 20XX IEEE. Personal use of this material is permitted. Permission from IEEE must be obtained for all other uses, in any current or future media, including reprinting/republishing this material for advertising or promotional purposes, creating new collective works, for resale or redistribution to servers or lists, or reuse of any copyrighted component of this work in other works.",
      "authors": [
        "Haoyang Li",
        "Jia Qi Yip",
        "Tianyu Fan",
        "Eng Siong Chng"
      ],
      "published": "2025-02-22T14:25:55Z",
      "updated": "2025-02-22T14:25:55Z",
      "categories": [
        "eess.AS",
        "cs.AI",
        "cs.LG",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2502.16240v1",
      "landing_url": "https://arxiv.org/abs/2502.16240v1",
      "doi": "https://doi.org/10.1109/ICASSP49660.2025.10890379"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item centers on using continuous embeddings from a neural audio codec for speech enhancement and does not involve discrete tokenization or analysis of discrete audio tokens as the core object of study, so it does not meet the inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item centers on using continuous embeddings from a neural audio codec for speech enhancement and does not involve discrete tokenization or analysis of discrete audio tokens as the core object of study, so it does not meet the inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study proposes a speech enhancement approach using continuous embeddings from a neural audio codec (NAC) encoder rather than discrete audio tokenization, and explicitly contrasts its continuous embedding method against prior NAC-based methods working with discrete speech tokens, indicating the core focus is on continuous features rather than discrete token sequences; thus, it does not satisfy the inclusion criterion requiring discrete audio tokenization as a core object of study.",
      "evaluation": 2
    },
    "round-A_JuniorMini_reasoning": "The study proposes a speech enhancement approach using continuous embeddings from a neural audio codec (NAC) encoder rather than discrete audio tokenization, and explicitly contrasts its continuous embedding method against prior NAC-based methods working with discrete speech tokens, indicating the core focus is on continuous features rather than discrete token sequences; thus, it does not satisfy the inclusion criterion requiring discrete audio tokenization as a core object of study.",
    "round-A_JuniorMini_evaluation": 2,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:2)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:2)"
  },
  {
    "title": "Unified Semantic and ID Representation Learning for Deep Recommenders",
    "abstract": "Effective recommendation is crucial for large-scale online platforms. Traditional recommendation systems primarily rely on ID tokens to uniquely identify items, which can effectively capture specific item relationships but suffer from issues such as redundancy and poor performance in cold-start scenarios. Recent approaches have explored using semantic tokens as an alternative, yet they face challenges, including item duplication and inconsistent performance gains, leaving the potential advantages of semantic tokens inadequately examined. To address these limitations, we propose a Unified Semantic and ID Representation Learning framework that leverages the complementary strengths of both token types. In our framework, ID tokens capture unique item attributes, while semantic tokens represent shared, transferable characteristics. Additionally, we analyze the role of cosine similarity and Euclidean distance in embedding search, revealing that cosine similarity is more effective in decoupling accumulated embeddings, while Euclidean distance excels in distinguishing unique items. Our framework integrates cosine similarity in earlier layers and Euclidean distance in the final layer to optimize representation learning. Experiments on three benchmark datasets show that our method significantly outperforms state-of-the-art baselines, with improvements ranging from 6\\% to 17\\% and a reduction in token size by over 80%. These results demonstrate the effectiveness of combining ID and semantic tokenization to enhance the generalization ability of recommender systems.",
    "metadata": {
      "arxiv_id": "2502.16474",
      "title": "Unified Semantic and ID Representation Learning for Deep Recommenders",
      "summary": "Effective recommendation is crucial for large-scale online platforms. Traditional recommendation systems primarily rely on ID tokens to uniquely identify items, which can effectively capture specific item relationships but suffer from issues such as redundancy and poor performance in cold-start scenarios. Recent approaches have explored using semantic tokens as an alternative, yet they face challenges, including item duplication and inconsistent performance gains, leaving the potential advantages of semantic tokens inadequately examined. To address these limitations, we propose a Unified Semantic and ID Representation Learning framework that leverages the complementary strengths of both token types. In our framework, ID tokens capture unique item attributes, while semantic tokens represent shared, transferable characteristics. Additionally, we analyze the role of cosine similarity and Euclidean distance in embedding search, revealing that cosine similarity is more effective in decoupling accumulated embeddings, while Euclidean distance excels in distinguishing unique items. Our framework integrates cosine similarity in earlier layers and Euclidean distance in the final layer to optimize representation learning. Experiments on three benchmark datasets show that our method significantly outperforms state-of-the-art baselines, with improvements ranging from 6\\% to 17\\% and a reduction in token size by over 80%. These results demonstrate the effectiveness of combining ID and semantic tokenization to enhance the generalization ability of recommender systems.",
      "authors": [
        "Guanyu Lin",
        "Zhigang Hua",
        "Tao Feng",
        "Shuang Yang",
        "Bo Long",
        "Jiaxuan You"
      ],
      "published": "2025-02-23T07:17:28Z",
      "updated": "2025-02-23T07:17:28Z",
      "categories": [
        "cs.IR"
      ],
      "pdf_url": "https://arxiv.org/pdf/2502.16474v1",
      "landing_url": "https://arxiv.org/abs/2502.16474v1",
      "doi": "https://doi.org/10.48550/arXiv.2502.16474"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item focuses on discrete token representations (ID tokens and semantic tokens) for recommender systems, not on discrete audio/speech tokenization or speech-specific token quantization for reconstruction/generation/understanding tasks, so it fails the audio-tokenization criterion.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item focuses on discrete token representations (ID tokens and semantic tokens) for recommender systems, not on discrete audio/speech tokenization or speech-specific token quantization for reconstruction/generation/understanding tasks, so it fails the audio-tokenization criterion.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on unified semantic and ID representation learning in recommendation systems without mentioning audio or speech tokenization, discrete audio token quantization, or any analysis related to converting continuous audio signals into discrete tokens; therefore, it does not meet the inclusion criteria related to audio/speech tokenization and falls outside the scope of the given criteria.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on unified semantic and ID representation learning in recommendation systems without mentioning audio or speech tokenization, discrete audio token quantization, or any analysis related to converting continuous audio signals into discrete tokens; therefore, it does not meet the inclusion criteria related to audio/speech tokenization and falls outside the scope of the given criteria.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Balancing Speech Understanding and Generation Using Continual Pre-training for Codec-based Speech LLM",
    "abstract": "Recent advances in speech language models (LLMs) have extended textual LLMs to the speech domain, but balancing speech understanding and generation remains challenging, especially with codec-based representations. We propose a continual pre-training (CPT) framework that adapts a textual LLM to handle codec-discretized speech, mitigating modality mismatch and preserving linguistic reasoning. Our unified model supports both understanding and generation, achieving strong results across ASR, TTS, S2T-Trans, and S2S-Trans. Notably, we present the first end-to-end, single-pass S2S-Trans system using only neural codec tokens, without intermediate transcriptions, translations, or semantic tokens. CPT proves essential for cross-modal alignment and task generalization, making it a powerful tool for building robust, unified speech LLMs.",
    "metadata": {
      "arxiv_id": "2502.16897",
      "title": "Balancing Speech Understanding and Generation Using Continual Pre-training for Codec-based Speech LLM",
      "summary": "Recent advances in speech language models (LLMs) have extended textual LLMs to the speech domain, but balancing speech understanding and generation remains challenging, especially with codec-based representations. We propose a continual pre-training (CPT) framework that adapts a textual LLM to handle codec-discretized speech, mitigating modality mismatch and preserving linguistic reasoning. Our unified model supports both understanding and generation, achieving strong results across ASR, TTS, S2T-Trans, and S2S-Trans. Notably, we present the first end-to-end, single-pass S2S-Trans system using only neural codec tokens, without intermediate transcriptions, translations, or semantic tokens. CPT proves essential for cross-modal alignment and task generalization, making it a powerful tool for building robust, unified speech LLMs.",
      "authors": [
        "Jiatong Shi",
        "Chunlei Zhang",
        "Jinchuan Tian",
        "Junrui Ni",
        "Hao Zhang",
        "Shinji Watanabe",
        "Dong Yu"
      ],
      "published": "2025-02-24T06:50:40Z",
      "updated": "2025-11-27T18:46:39Z",
      "categories": [
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2502.16897v2",
      "landing_url": "https://arxiv.org/abs/2502.16897v2",
      "doi": "https://doi.org/10.48550/arXiv.2502.16897"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The abstract explicitly uses codec-based discrete speech tokens (neural codec tokens) as the central object, proposing continual pre-training to operate on codec-discretized speech for both understanding and generation, which meets the inclusion criteria and is not merely continuous features or non-speech tokenization.",
      "evaluation": 5
    },
    "round-A_JuniorNano_reasoning": "The abstract explicitly uses codec-based discrete speech tokens (neural codec tokens) as the central object, proposing continual pre-training to operate on codec-discretized speech for both understanding and generation, which meets the inclusion criteria and is not merely continuous features or non-speech tokenization.",
    "round-A_JuniorNano_evaluation": 5,
    "round-A_JuniorMini_output": {
      "reasoning": "The study proposes a continual pre-training framework that adapts a textual LLM to handle codec-discretized speech, indicating that discrete audio tokenization is a core object of study. It emphasizes codec-based representations and neural codec tokens, which aligns with modern quantization-based discrete token representations. The work also demonstrates experimental validation across multiple speech tasks, fulfilling inclusion criteria without matching any exclusion criteria.",
      "evaluation": 5
    },
    "round-A_JuniorMini_reasoning": "The study proposes a continual pre-training framework that adapts a textual LLM to handle codec-discretized speech, indicating that discrete audio tokenization is a core object of study. It emphasizes codec-based representations and neural codec tokens, which aligns with modern quantization-based discrete token representations. The work also demonstrates experimental validation across multiple speech tasks, fulfilling inclusion criteria without matching any exclusion criteria.",
    "round-A_JuniorMini_evaluation": 5,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "include (junior:5)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Complexity for billiards in regular N-gons",
    "abstract": "We compute the complexity of the billiard language of the regular Euclidean $N$-gons (and other families of rational lattice polygons), answering a question posed by Cassaigne-Hubert-Troubetzkoy. Our key technical result is a counting result for saddle connections on lattice surfaces, when we count by combinatorial length.",
    "metadata": {
      "arxiv_id": "2502.17627",
      "title": "Complexity for billiards in regular N-gons",
      "summary": "We compute the complexity of the billiard language of the regular Euclidean $N$-gons (and other families of rational lattice polygons), answering a question posed by Cassaigne-Hubert-Troubetzkoy. Our key technical result is a counting result for saddle connections on lattice surfaces, when we count by combinatorial length.",
      "authors": [
        "Jayadev Athreya",
        "Pascal Hubert",
        "Serge Troubetzkoy"
      ],
      "published": "2025-02-24T20:20:59Z",
      "updated": "2025-06-24T13:58:02Z",
      "categories": [
        "math.DS",
        "math.CV",
        "math.GT"
      ],
      "pdf_url": "https://arxiv.org/pdf/2502.17627v2",
      "landing_url": "https://arxiv.org/abs/2502.17627v2",
      "doi": "https://doi.org/10.48550/arXiv.2502.17627"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The abstract is a mathematical study of billiards and lattice surfaces, with no discrete audio/speech tokenization or tokenization methods, so it does not meet the inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The abstract is a mathematical study of billiards and lattice surfaces, with no discrete audio/speech tokenization or tokenization methods, so it does not meet the inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on mathematical complexity related to billiards in regular N-gons, with no mention of audio/speech tokenization, discrete audio representations, or any relevant neural or signal processing methods, thus failing to meet the inclusion criteria and falling outside the scope entirely.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on mathematical complexity related to billiards in regular N-gons, with no mention of audio/speech tokenization, discrete audio representations, or any relevant neural or signal processing methods, thus failing to meet the inclusion criteria and falling outside the scope entirely.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Zero-Shot Semantic Communication with Multimodal Foundation Models",
    "abstract": "Most existing semantic communication (SemCom) systems use deep joint source-channel coding (DeepJSCC) to encode task-specific semantics in a goal-oriented manner. However, their reliance on predefined tasks and datasets significantly limits their flexibility and generalizability in practical deployments. Multi-modal foundation models provide a promising solution by generating universal semantic tokens. Inspired by this, we introduce SemCLIP, a zero-shot SemCom framework leveraging the contrastive language-image pre-training (CLIP) model. By transmitting CLIP-generated image tokens instead of raw images, SemCLIP enables efficient SemCom under low bandwidth and challenging channel conditions, facilitating diverse downstream tasks and zero-shot applications. Specifically, we propose a DeepJSCC scheme for efficient CLIP token encoding. To mitigate potential degradation caused by compression and channel noise, a multi-modal transmission-aware prompt learning mechanism is designed at the receiver, which adapts prompts based on transmission quality, enhancing system robustness and channel adaptability. Simulation results demonstrate that SemCLIP outperforms the baselines, achieving a $41\\%$ improvement in zero-shot performance at low signal-to-noise ratios. Meanwhile, SemCLIP reduces bandwidth usage by more than $50$-fold compared to alternative image transmission methods, demonstrating the potential of foundation models towards a generalized, task-agnostic SemCom solution.",
    "metadata": {
      "arxiv_id": "2502.18200",
      "title": "Zero-Shot Semantic Communication with Multimodal Foundation Models",
      "summary": "Most existing semantic communication (SemCom) systems use deep joint source-channel coding (DeepJSCC) to encode task-specific semantics in a goal-oriented manner. However, their reliance on predefined tasks and datasets significantly limits their flexibility and generalizability in practical deployments. Multi-modal foundation models provide a promising solution by generating universal semantic tokens. Inspired by this, we introduce SemCLIP, a zero-shot SemCom framework leveraging the contrastive language-image pre-training (CLIP) model. By transmitting CLIP-generated image tokens instead of raw images, SemCLIP enables efficient SemCom under low bandwidth and challenging channel conditions, facilitating diverse downstream tasks and zero-shot applications. Specifically, we propose a DeepJSCC scheme for efficient CLIP token encoding. To mitigate potential degradation caused by compression and channel noise, a multi-modal transmission-aware prompt learning mechanism is designed at the receiver, which adapts prompts based on transmission quality, enhancing system robustness and channel adaptability. Simulation results demonstrate that SemCLIP outperforms the baselines, achieving a $41\\%$ improvement in zero-shot performance at low signal-to-noise ratios. Meanwhile, SemCLIP reduces bandwidth usage by more than $50$-fold compared to alternative image transmission methods, demonstrating the potential of foundation models towards a generalized, task-agnostic SemCom solution.",
      "authors": [
        "Jiangjing Hu",
        "Haotian Wu",
        "Wenjing Zhang",
        "Fengyu Wang",
        "Wenjun Xu",
        "Hui Gao",
        "Deniz Gündüz"
      ],
      "published": "2025-02-25T13:41:06Z",
      "updated": "2025-05-29T05:24:20Z",
      "categories": [
        "eess.SP"
      ],
      "pdf_url": "https://arxiv.org/pdf/2502.18200v2",
      "landing_url": "https://arxiv.org/abs/2502.18200v2",
      "doi": "https://doi.org/10.48550/arXiv.2502.18200"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The work discusses zero-shot semantic communication using CLIP image tokens and multimodal prompts, with no focus on discrete audio/speech tokenization or quantization; thus it fails the inclusion criterion requiring explicit study of discrete audio tokens.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The work discusses zero-shot semantic communication using CLIP image tokens and multimodal prompts, with no focus on discrete audio/speech tokenization or quantization; thus it fails the inclusion criterion requiring explicit study of discrete audio tokens.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on zero-shot semantic communication using multi-modal foundation models and primarily deals with image tokens generated by CLIP rather than audio or speech tokens. There is no mention or analysis of discrete audio/speech tokenization, which is a core inclusion criterion; thus, it does not meet the inclusion requirements related to audio tokenization and rather focuses on image token transmission.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on zero-shot semantic communication using multi-modal foundation models and primarily deals with image tokens generated by CLIP rather than audio or speech tokens. There is no mention or analysis of discrete audio/speech tokenization, which is a core inclusion criterion; thus, it does not meet the inclusion requirements related to audio tokenization and rather focuses on image token transmission.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Best Foot Forward: Robust Foot Reconstruction in-the-wild",
    "abstract": "Accurate 3D foot reconstruction is crucial for personalized orthotics, digital healthcare, and virtual fittings. However, existing methods struggle with incomplete scans and anatomical variations, particularly in self-scanning scenarios where user mobility is limited, making it difficult to capture areas like the arch and heel. We present a novel end-to-end pipeline that refines Structure-from-Motion (SfM) reconstruction. It first resolves scan alignment ambiguities using SE(3) canonicalization with a viewpoint prediction module, then completes missing geometry through an attention-based network trained on synthetically augmented point clouds. Our approach achieves state-of-the-art performance on reconstruction metrics while preserving clinically validated anatomical fidelity. By combining synthetic training data with learned geometric priors, we enable robust foot reconstruction under real-world capture conditions, unlocking new opportunities for mobile-based 3D scanning in healthcare and retail.",
    "metadata": {
      "arxiv_id": "2502.20511",
      "title": "Best Foot Forward: Robust Foot Reconstruction in-the-wild",
      "summary": "Accurate 3D foot reconstruction is crucial for personalized orthotics, digital healthcare, and virtual fittings. However, existing methods struggle with incomplete scans and anatomical variations, particularly in self-scanning scenarios where user mobility is limited, making it difficult to capture areas like the arch and heel. We present a novel end-to-end pipeline that refines Structure-from-Motion (SfM) reconstruction. It first resolves scan alignment ambiguities using SE(3) canonicalization with a viewpoint prediction module, then completes missing geometry through an attention-based network trained on synthetically augmented point clouds. Our approach achieves state-of-the-art performance on reconstruction metrics while preserving clinically validated anatomical fidelity. By combining synthetic training data with learned geometric priors, we enable robust foot reconstruction under real-world capture conditions, unlocking new opportunities for mobile-based 3D scanning in healthcare and retail.",
      "authors": [
        "Kyle Fogarty",
        "Jing Yang",
        "Chayan Kumar Patodi",
        "Jack Foster",
        "Aadi Bhanti",
        "Steven Chacko",
        "Cengiz Oztireli",
        "Ujwal Bonde"
      ],
      "published": "2025-02-27T20:40:20Z",
      "updated": "2025-08-18T11:31:41Z",
      "categories": [
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2502.20511v2",
      "landing_url": "https://arxiv.org/abs/2502.20511v2",
      "doi": "https://doi.org/10.48550/arXiv.2502.20511"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The abstract describes 3D foot reconstruction and a pipeline for improving Structure-from-Motion; there is no discussion of discrete audio/speech tokenization, token design, or language-model-style discrete tokens, so it does not meet the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The abstract describes 3D foot reconstruction and a pipeline for improving Structure-from-Motion; there is no discussion of discrete audio/speech tokenization, token design, or language-model-style discrete tokens, so it does not meet the inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses entirely on 3D foot reconstruction from visual data and does not address audio or speech tokenization, discrete audio symbol conversion, or any audio-related methodology as required by the inclusion criteria; thus, it fails to meet the core criteria for inclusion.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses entirely on 3D foot reconstruction from visual data and does not address audio or speech tokenization, discrete audio symbol conversion, or any audio-related methodology as required by the inclusion criteria; thus, it fails to meet the core criteria for inclusion.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "UniWav: Towards Unified Pre-training for Speech Representation Learning and Generation",
    "abstract": "Pre-training and representation learning have been playing an increasingly important role in modern speech processing. Nevertheless, different applications have been relying on different foundation models, since predominant pre-training techniques are either designed for discriminative tasks or generative tasks. In this work, we make the first attempt at building a unified pre-training framework for both types of tasks in speech. We show that with the appropriate design choices for pre-training, one can jointly learn a representation encoder and generative audio decoder that can be applied to both types of tasks. We propose UniWav, an encoder-decoder framework designed to unify pre-training representation learning and generative tasks. On speech recognition, text-to-speech, and speech tokenization, UniWav achieves comparable performance to different existing foundation models, each trained on a specific task. Our findings suggest that a single general-purpose foundation model for speech can be built to replace different foundation models, reducing the overhead and cost of pre-training.",
    "metadata": {
      "arxiv_id": "2503.00733",
      "title": "UniWav: Towards Unified Pre-training for Speech Representation Learning and Generation",
      "summary": "Pre-training and representation learning have been playing an increasingly important role in modern speech processing. Nevertheless, different applications have been relying on different foundation models, since predominant pre-training techniques are either designed for discriminative tasks or generative tasks. In this work, we make the first attempt at building a unified pre-training framework for both types of tasks in speech. We show that with the appropriate design choices for pre-training, one can jointly learn a representation encoder and generative audio decoder that can be applied to both types of tasks. We propose UniWav, an encoder-decoder framework designed to unify pre-training representation learning and generative tasks. On speech recognition, text-to-speech, and speech tokenization, UniWav achieves comparable performance to different existing foundation models, each trained on a specific task. Our findings suggest that a single general-purpose foundation model for speech can be built to replace different foundation models, reducing the overhead and cost of pre-training.",
      "authors": [
        "Alexander H. Liu",
        "Sang-gil Lee",
        "Chao-Han Huck Yang",
        "Yuan Gong",
        "Yu-Chiang Frank Wang",
        "James R. Glass",
        "Rafael Valle",
        "Bryan Catanzaro"
      ],
      "published": "2025-03-02T05:15:40Z",
      "updated": "2025-03-02T05:15:40Z",
      "categories": [
        "eess.AS",
        "cs.CL",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2503.00733v1",
      "landing_url": "https://arxiv.org/abs/2503.00733v1",
      "doi": "https://doi.org/10.48550/arXiv.2503.00733"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "Although the paper mentions 'speech tokenization' as an application, the abstract does not clearly propose or analyze discrete audio tokenization (quantized tokens) as a core object of study; it focuses on a unified encoder–decoder pre-training framework and continuous representations, rather than explicit discrete tokenization design or evaluation for speech tasks.",
      "evaluation": 2
    },
    "round-A_JuniorNano_reasoning": "Although the paper mentions 'speech tokenization' as an application, the abstract does not clearly propose or analyze discrete audio tokenization (quantized tokens) as a core object of study; it focuses on a unified encoder–decoder pre-training framework and continuous representations, rather than explicit discrete tokenization design or evaluation for speech tasks.",
    "round-A_JuniorNano_evaluation": 2,
    "round-A_JuniorMini_output": {
      "reasoning": "The study proposes a unified pre-training framework including speech tokenization, explicitly mentioning its application to speech tokenization as a core part of the model evaluation, indicating involvement with discrete audio/speech tokenization relevant to speech tasks; hence it meets the inclusion criteria and does not fall under any exclusion criterion.",
      "evaluation": 5
    },
    "round-A_JuniorMini_reasoning": "The study proposes a unified pre-training framework including speech tokenization, explicitly mentioning its application to speech tokenization as a core part of the model evaluation, indicating involvement with discrete audio/speech tokenization relevant to speech tasks; hence it meets the inclusion criteria and does not fall under any exclusion criterion.",
    "round-A_JuniorMini_evaluation": 5,
    "round-B_SeniorLead_output": {
      "reasoning": "JuniorNano correctly notes the abstract does not clearly describe discrete quantization/token design, while JuniorMini highlights that the paper lists \"speech tokenization\" among evaluated tasks; however, the abstract only states applicability to \"speech tokenization\" without explicit proposal, analysis, or evaluation details of discrete audio tokens (quantized symbols) nor confirmation of full-text availability in English. Therefore I judge it does not meet the inclusion criterion that discrete audio/speech tokenization be explicitly proposed or analyzed. Explanation: Better to exclude (2) because the abstract lacks clear evidence of discrete token design/analysis required by the inclusion criteria.",
      "evaluation": 2
    },
    "round-B_SeniorLead_reasoning": "JuniorNano correctly notes the abstract does not clearly describe discrete quantization/token design, while JuniorMini highlights that the paper lists \"speech tokenization\" among evaluated tasks; however, the abstract only states applicability to \"speech tokenization\" without explicit proposal, analysis, or evaluation details of discrete audio tokens (quantized symbols) nor confirmation of full-text availability in English. Therefore I judge it does not meet the inclusion criterion that discrete audio/speech tokenization be explicitly proposed or analyzed. Explanation: Better to exclude (2) because the abstract lacks clear evidence of discrete token design/analysis required by the inclusion criteria.",
    "round-B_SeniorLead_evaluation": 2,
    "final_verdict": "exclude (senior:2)",
    "review_skipped": false,
    "discard_reason": "exclude (senior:2)"
  },
  {
    "title": "Category-level Meta-learned NeRF Priors for Efficient Object Mapping",
    "abstract": "In 3D object mapping, category-level priors enable efficient object reconstruction and canonical pose estimation, requiring only a single prior per semantic category (e.g., chair, book, laptop, etc.). DeepSDF has been used predominantly as a category-level shape prior, but it struggles to reconstruct sharp geometry and is computationally expensive. In contrast, NeRFs capture fine details but have yet to be effectively integrated with category-level priors in a real-time multi-object mapping framework. To bridge this gap, we introduce PRENOM, a Prior-based Efficient Neural Object Mapper that integrates category-level priors with object-level NeRFs to enhance reconstruction efficiency and enable canonical object pose estimation. PRENOM gets to know objects on a first-name basis by meta-learning on synthetic reconstruction tasks generated from open-source shape datasets. To account for object category variations, it employs a multi-objective genetic algorithm to optimize the NeRF architecture for each category, balancing reconstruction quality and training time. Additionally, prior-based probabilistic ray sampling directs sampling toward expected object regions, accelerating convergence and improving reconstruction quality under constrained resources. Experimental results highlight the ability of PRENOM to achieve high-quality reconstructions while maintaining computational feasibility. Specifically, comparisons with prior-free NeRF-based approaches on a synthetic dataset show a 21\\% lower Chamfer distance. Furthermore, evaluations against other approaches using shape priors on a noisy real-world dataset indicate a 13\\% improvement averaged across all reconstruction metrics, and comparable pose and size estimation accuracy, while being trained for 5$\\times$ less time. Code available at: https://github.com/snt-arg/PRENOM",
    "metadata": {
      "arxiv_id": "2503.01582",
      "title": "Category-level Meta-learned NeRF Priors for Efficient Object Mapping",
      "summary": "In 3D object mapping, category-level priors enable efficient object reconstruction and canonical pose estimation, requiring only a single prior per semantic category (e.g., chair, book, laptop, etc.). DeepSDF has been used predominantly as a category-level shape prior, but it struggles to reconstruct sharp geometry and is computationally expensive. In contrast, NeRFs capture fine details but have yet to be effectively integrated with category-level priors in a real-time multi-object mapping framework. To bridge this gap, we introduce PRENOM, a Prior-based Efficient Neural Object Mapper that integrates category-level priors with object-level NeRFs to enhance reconstruction efficiency and enable canonical object pose estimation. PRENOM gets to know objects on a first-name basis by meta-learning on synthetic reconstruction tasks generated from open-source shape datasets. To account for object category variations, it employs a multi-objective genetic algorithm to optimize the NeRF architecture for each category, balancing reconstruction quality and training time. Additionally, prior-based probabilistic ray sampling directs sampling toward expected object regions, accelerating convergence and improving reconstruction quality under constrained resources. Experimental results highlight the ability of PRENOM to achieve high-quality reconstructions while maintaining computational feasibility. Specifically, comparisons with prior-free NeRF-based approaches on a synthetic dataset show a 21\\% lower Chamfer distance. Furthermore, evaluations against other approaches using shape priors on a noisy real-world dataset indicate a 13\\% improvement averaged across all reconstruction metrics, and comparable pose and size estimation accuracy, while being trained for 5$\\times$ less time. Code available at: https://github.com/snt-arg/PRENOM",
      "authors": [
        "Saad Ejaz",
        "Hriday Bavle",
        "Laura Ribeiro",
        "Holger Voos",
        "Jose Luis Sanchez-Lopez"
      ],
      "published": "2025-03-03T14:23:37Z",
      "updated": "2025-07-29T14:15:39Z",
      "categories": [
        "cs.CV",
        "cs.RO"
      ],
      "pdf_url": "https://arxiv.org/pdf/2503.01582v3",
      "landing_url": "https://arxiv.org/abs/2503.01582v3",
      "doi": "https://doi.org/10.48550/arXiv.2503.01582"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This paper concerns 3D NeRF priors for object mapping; it does not involve discrete audio tokenization or speech tasks, so it fails the inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "This paper concerns 3D NeRF priors for object mapping; it does not involve discrete audio tokenization or speech tasks, so it fails the inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on 3D object mapping using category-level priors and neural radiance fields (NeRFs) for object reconstruction and pose estimation, with no mention of discrete audio/speech tokenization or any audio-related analysis; thus, it does not meet the inclusion criteria related to discrete audio tokenization and is unrelated to the audio domain.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on 3D object mapping using category-level priors and neural radiance fields (NeRFs) for object reconstruction and pose estimation, with no mention of discrete audio/speech tokenization or any audio-related analysis; thus, it does not meet the inclusion criteria related to discrete audio tokenization and is unrelated to the audio domain.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Spark-TTS: An Efficient LLM-Based Text-to-Speech Model with Single-Stream Decoupled Speech Tokens",
    "abstract": "Recent advancements in large language models (LLMs) have driven significant progress in zero-shot text-to-speech (TTS) synthesis. However, existing foundation models rely on multi-stage processing or complex architectures for predicting multiple codebooks, limiting efficiency and integration flexibility. To overcome these challenges, we introduce Spark-TTS, a novel system powered by BiCodec, a single-stream speech codec that decomposes speech into two complementary token types: low-bitrate semantic tokens for linguistic content and fixed-length global tokens for speaker attributes. This disentangled representation, combined with the Qwen2.5 LLM and a chain-of-thought (CoT) generation approach, enables both coarse-grained control (e.g., gender, speaking style) and fine-grained adjustments (e.g., precise pitch values, speaking rate). To facilitate research in controllable TTS, we introduce VoxBox, a meticulously curated 100,000-hour dataset with comprehensive attribute annotations. Extensive experiments demonstrate that Spark-TTS not only achieves state-of-the-art zero-shot voice cloning but also generates highly customizable voices that surpass the limitations of reference-based synthesis. Source code, pre-trained models, and audio samples are available at https://github.com/SparkAudio/Spark-TTS.",
    "metadata": {
      "arxiv_id": "2503.01710",
      "title": "Spark-TTS: An Efficient LLM-Based Text-to-Speech Model with Single-Stream Decoupled Speech Tokens",
      "summary": "Recent advancements in large language models (LLMs) have driven significant progress in zero-shot text-to-speech (TTS) synthesis. However, existing foundation models rely on multi-stage processing or complex architectures for predicting multiple codebooks, limiting efficiency and integration flexibility. To overcome these challenges, we introduce Spark-TTS, a novel system powered by BiCodec, a single-stream speech codec that decomposes speech into two complementary token types: low-bitrate semantic tokens for linguistic content and fixed-length global tokens for speaker attributes. This disentangled representation, combined with the Qwen2.5 LLM and a chain-of-thought (CoT) generation approach, enables both coarse-grained control (e.g., gender, speaking style) and fine-grained adjustments (e.g., precise pitch values, speaking rate). To facilitate research in controllable TTS, we introduce VoxBox, a meticulously curated 100,000-hour dataset with comprehensive attribute annotations. Extensive experiments demonstrate that Spark-TTS not only achieves state-of-the-art zero-shot voice cloning but also generates highly customizable voices that surpass the limitations of reference-based synthesis. Source code, pre-trained models, and audio samples are available at https://github.com/SparkAudio/Spark-TTS.",
      "authors": [
        "Xinsheng Wang",
        "Mingqi Jiang",
        "Ziyang Ma",
        "Ziyu Zhang",
        "Songxiang Liu",
        "Linqin Li",
        "Zheng Liang",
        "Qixi Zheng",
        "Rui Wang",
        "Xiaoqin Feng",
        "Weizhen Bian",
        "Zhen Ye",
        "Sitong Cheng",
        "Ruibin Yuan",
        "Zhixian Zhao",
        "Xinfa Zhu",
        "Jiahao Pan",
        "Liumeng Xue",
        "Pengcheng Zhu",
        "Yunlin Chen",
        "Zhifei Li",
        "Xie Chen",
        "Lei Xie",
        "Yike Guo",
        "Wei Xue"
      ],
      "published": "2025-03-03T16:23:10Z",
      "updated": "2025-03-03T16:23:10Z",
      "categories": [
        "cs.SD",
        "cs.AI",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2503.01710v1",
      "landing_url": "https://arxiv.org/abs/2503.01710v1",
      "doi": "https://doi.org/10.48550/arXiv.2503.01710"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The article centers on a discrete speech tokenization framework (BiCodec) that converts speech into semantic tokens and global speaker tokens, satisfying the inclusion criteria and not engaging any exclusion criteria.",
      "evaluation": 5
    },
    "round-A_JuniorNano_reasoning": "The article centers on a discrete speech tokenization framework (BiCodec) that converts speech into semantic tokens and global speaker tokens, satisfying the inclusion criteria and not engaging any exclusion criteria.",
    "round-A_JuniorNano_evaluation": 5,
    "round-A_JuniorMini_output": {
      "reasoning": "The study clearly proposes and analyzes a novel neural discrete speech tokenization approach via BiCodec, which produces discrete token types (semantic and global tokens) for speech generation control, meeting the inclusion criteria related to discrete audio tokenization and going beyond traditional codecs; furthermore, it includes methodological contributions and extensive evaluations in the context of text-to-speech synthesis, making it relevant and suitable for inclusion.",
      "evaluation": 5
    },
    "round-A_JuniorMini_reasoning": "The study clearly proposes and analyzes a novel neural discrete speech tokenization approach via BiCodec, which produces discrete token types (semantic and global tokens) for speech generation control, meeting the inclusion criteria related to discrete audio tokenization and going beyond traditional codecs; furthermore, it includes methodological contributions and extensive evaluations in the context of text-to-speech synthesis, making it relevant and suitable for inclusion.",
    "round-A_JuniorMini_evaluation": 5,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "include (junior:5)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Privacy and Accuracy-Aware AI/ML Model Deduplication",
    "abstract": "With the growing adoption of privacy-preserving machine learning algorithms, such as Differentially Private Stochastic Gradient Descent (DP-SGD), training or fine-tuning models on private datasets has become increasingly prevalent. This shift has led to the need for models offering varying privacy guarantees and utility levels to satisfy diverse user requirements. However, managing numerous versions of large models introduces significant operational challenges, including increased inference latency, higher resource consumption, and elevated costs. Model deduplication is a technique widely used by many model serving and database systems to support high-performance and low-cost inference queries and model diagnosis queries. However, none of the existing model deduplication works has considered privacy, leading to unbounded aggregation of privacy costs for certain deduplicated models and inefficiencies when applied to deduplicate DP-trained models. We formalize the problems of deduplicating DP-trained models for the first time and propose a novel privacy- and accuracy-aware deduplication mechanism to address the problems. We developed a greedy strategy to select and assign base models to target models to minimize storage and privacy costs. When deduplicating a target model, we dynamically schedule accuracy validations and apply the Sparse Vector Technique to reduce the privacy costs associated with private validation data. Compared to baselines that do not provide privacy guarantees, our approach improved the compression ratio by up to $35\\times$ for individual models (including large language models and vision transformers). We also observed up to $43\\times$ inference speedup due to the reduction of I/O operations.",
    "metadata": {
      "arxiv_id": "2503.02862",
      "title": "Privacy and Accuracy-Aware AI/ML Model Deduplication",
      "summary": "With the growing adoption of privacy-preserving machine learning algorithms, such as Differentially Private Stochastic Gradient Descent (DP-SGD), training or fine-tuning models on private datasets has become increasingly prevalent. This shift has led to the need for models offering varying privacy guarantees and utility levels to satisfy diverse user requirements. However, managing numerous versions of large models introduces significant operational challenges, including increased inference latency, higher resource consumption, and elevated costs. Model deduplication is a technique widely used by many model serving and database systems to support high-performance and low-cost inference queries and model diagnosis queries. However, none of the existing model deduplication works has considered privacy, leading to unbounded aggregation of privacy costs for certain deduplicated models and inefficiencies when applied to deduplicate DP-trained models. We formalize the problems of deduplicating DP-trained models for the first time and propose a novel privacy- and accuracy-aware deduplication mechanism to address the problems. We developed a greedy strategy to select and assign base models to target models to minimize storage and privacy costs. When deduplicating a target model, we dynamically schedule accuracy validations and apply the Sparse Vector Technique to reduce the privacy costs associated with private validation data. Compared to baselines that do not provide privacy guarantees, our approach improved the compression ratio by up to $35\\times$ for individual models (including large language models and vision transformers). We also observed up to $43\\times$ inference speedup due to the reduction of I/O operations.",
      "authors": [
        "Hong Guan",
        "Lei Yu",
        "Lixi Zhou",
        "Li Xiong",
        "Kanchan Chowdhury",
        "Lulu Xie",
        "Xusheng Xiao",
        "Jia Zou"
      ],
      "published": "2025-03-04T18:40:38Z",
      "updated": "2025-03-04T18:40:38Z",
      "categories": [
        "cs.CR",
        "cs.DB"
      ],
      "pdf_url": "https://arxiv.org/pdf/2503.02862v1",
      "landing_url": "https://arxiv.org/abs/2503.02862v1",
      "doi": "https://doi.org/10.48550/arXiv.2503.02862"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The abstract discusses privacy-aware deduplication of DP-trained ML models and efficiency trade-offs; there is no discussion of discrete audio/speech tokenization or any token design for speech, so it does not meet the audio tokenization inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The abstract discusses privacy-aware deduplication of DP-trained ML models and efficiency trade-offs; there is no discussion of discrete audio/speech tokenization or any token design for speech, so it does not meet the audio tokenization inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on privacy-aware model deduplication for machine learning models trained with differential privacy and does not discuss discrete audio/speech tokenization or any discrete token representation of audio signals, which is the core inclusion criterion.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on privacy-aware model deduplication for machine learning models trained with differential privacy and does not discuss discrete audio/speech tokenization or any discrete token representation of audio signals, which is the core inclusion criterion.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "On the Relation Between Speech Quality and Quantized Latent Representations of Neural Codecs",
    "abstract": "Neural audio signal codecs have attracted significant attention in recent years. In essence, the impressive low bitrate achieved by such encoders is enabled by learning an abstract representation that captures the properties of encoded signals, e.g., speech. In this work, we investigate the relation between the latent representation of the input signal learned by a neural codec and the quality of speech signals. To do so, we introduce Latent-representation-to-Quantization error Ratio (LQR) measures, which quantify the distance from the idealized neural codec's speech signal model for a given speech signal. We compare the proposed metrics to intrusive measures as well as data-driven supervised methods using two subjective speech quality datasets. This analysis shows that the proposed LQR correlates strongly (up to 0.9 Pearson's correlation) with the subjective quality of speech. Despite being a non-intrusive metric, this yields a competitive performance with, or even better than, other pre-trained and intrusive measures. These results show that LQR is a promising basis for more sophisticated speech quality measures.",
    "metadata": {
      "arxiv_id": "2503.03304",
      "title": "On the Relation Between Speech Quality and Quantized Latent Representations of Neural Codecs",
      "summary": "Neural audio signal codecs have attracted significant attention in recent years. In essence, the impressive low bitrate achieved by such encoders is enabled by learning an abstract representation that captures the properties of encoded signals, e.g., speech. In this work, we investigate the relation between the latent representation of the input signal learned by a neural codec and the quality of speech signals. To do so, we introduce Latent-representation-to-Quantization error Ratio (LQR) measures, which quantify the distance from the idealized neural codec's speech signal model for a given speech signal. We compare the proposed metrics to intrusive measures as well as data-driven supervised methods using two subjective speech quality datasets. This analysis shows that the proposed LQR correlates strongly (up to 0.9 Pearson's correlation) with the subjective quality of speech. Despite being a non-intrusive metric, this yields a competitive performance with, or even better than, other pre-trained and intrusive measures. These results show that LQR is a promising basis for more sophisticated speech quality measures.",
      "authors": [
        "Mhd Modar Halimeh",
        "Matteo Torcoli",
        "Philipp Grundhuber",
        "Emanuël A. P. Habets"
      ],
      "published": "2025-03-05T09:37:14Z",
      "updated": "2025-03-05T09:37:14Z",
      "categories": [
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2503.03304v1",
      "landing_url": "https://arxiv.org/abs/2503.03304v1",
      "doi": "https://doi.org/10.48550/arXiv.2503.03304"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The abstract focuses on quantized latent representations in neural codecs and a metric (LQR) for speech quality, but it does not explicitly analyze discrete audio/tokenization as the core object (e.g., token sequences from a finite codebook for speech reconstruction).",
      "evaluation": 2
    },
    "round-A_JuniorNano_reasoning": "The abstract focuses on quantized latent representations in neural codecs and a metric (LQR) for speech quality, but it does not explicitly analyze discrete audio/tokenization as the core object (e.g., token sequences from a finite codebook for speech reconstruction).",
    "round-A_JuniorNano_evaluation": 2,
    "round-A_JuniorMini_output": {
      "reasoning": "The study analyzes the relation between speech quality and quantized latent representations of neural codecs, indicating it deals with discrete representations of speech signals and their quantization, which aligns with the inclusion criteria of analyzing discrete audio tokenization in neural codecs rather than continuous audio features; the study is methodological, using correlation analyses and proposing new metrics, and focuses on speech, not non-speech audio, making it relevant and suitable for inclusion.",
      "evaluation": 5
    },
    "round-A_JuniorMini_reasoning": "The study analyzes the relation between speech quality and quantized latent representations of neural codecs, indicating it deals with discrete representations of speech signals and their quantization, which aligns with the inclusion criteria of analyzing discrete audio tokenization in neural codecs rather than continuous audio features; the study is methodological, using correlation analyses and proposing new metrics, and focuses on speech, not non-speech audio, making it relevant and suitable for inclusion.",
    "round-A_JuniorMini_evaluation": 5,
    "round-B_SeniorLead_output": {
      "reasoning": "I reviewed both junior comments: JuniorNano doubted whether the work explicitly treats discrete tokenization, while JuniorMini noted the paper analyzes quantized latent representations and proposes a metric (LQR) with experiments on speech quality. The abstract explicitly studies quantized latent representations of neural codecs (i.e., discrete quantization/codebook-style latents) and proposes/evaluates a new metric with subjective validation on speech—this meets the inclusion criterion (analysis of discrete audio/speech quantization tokens in neural codecs) and does not trigger any exclusion criteria. Brief explanation: The paper analyzes and evaluates quantized (discrete) latent representations in neural speech codecs, aligning with the required discrete audio tokenization scope, so it should be included.",
      "evaluation": 5
    },
    "round-B_SeniorLead_reasoning": "I reviewed both junior comments: JuniorNano doubted whether the work explicitly treats discrete tokenization, while JuniorMini noted the paper analyzes quantized latent representations and proposes a metric (LQR) with experiments on speech quality. The abstract explicitly studies quantized latent representations of neural codecs (i.e., discrete quantization/codebook-style latents) and proposes/evaluates a new metric with subjective validation on speech—this meets the inclusion criterion (analysis of discrete audio/speech quantization tokens in neural codecs) and does not trigger any exclusion criteria. Brief explanation: The paper analyzes and evaluates quantized (discrete) latent representations in neural speech codecs, aligning with the required discrete audio tokenization scope, so it should be included.",
    "round-B_SeniorLead_evaluation": 5,
    "final_verdict": "include (senior:5)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "The Best of Both Worlds: Integrating Language Models and Diffusion Models for Video Generation",
    "abstract": "Recent advancements in text-to-video (T2V) generation have been driven by two competing paradigms: autoregressive language models and diffusion models. However, each paradigm has intrinsic limitations: language models struggle with visual quality and error accumulation, while diffusion models lack semantic understanding and causal modeling. In this work, we propose LanDiff, a hybrid framework that synergizes the strengths of both paradigms through coarse-to-fine generation. Our architecture introduces three key innovations: (1) a semantic tokenizer that compresses 3D visual features into compact 1D discrete representations through efficient semantic compression, achieving a $\\sim$14,000$\\times$ compression ratio; (2) a language model that generates semantic tokens with high-level semantic relationships; (3) a streaming diffusion model that refines coarse semantics into high-fidelity videos. Experiments show that LanDiff, a 5B model, achieves a score of 85.43 on the VBench T2V benchmark, surpassing the state-of-the-art open-source models Hunyuan Video (13B) and other commercial models such as Sora, Kling, and Hailuo. Furthermore, our model also achieves state-of-the-art performance in long video generation, surpassing other open-source models in this field. Our demo can be viewed at https://landiff.github.io/.",
    "metadata": {
      "arxiv_id": "2503.04606",
      "title": "The Best of Both Worlds: Integrating Language Models and Diffusion Models for Video Generation",
      "summary": "Recent advancements in text-to-video (T2V) generation have been driven by two competing paradigms: autoregressive language models and diffusion models. However, each paradigm has intrinsic limitations: language models struggle with visual quality and error accumulation, while diffusion models lack semantic understanding and causal modeling. In this work, we propose LanDiff, a hybrid framework that synergizes the strengths of both paradigms through coarse-to-fine generation. Our architecture introduces three key innovations: (1) a semantic tokenizer that compresses 3D visual features into compact 1D discrete representations through efficient semantic compression, achieving a $\\sim$14,000$\\times$ compression ratio; (2) a language model that generates semantic tokens with high-level semantic relationships; (3) a streaming diffusion model that refines coarse semantics into high-fidelity videos. Experiments show that LanDiff, a 5B model, achieves a score of 85.43 on the VBench T2V benchmark, surpassing the state-of-the-art open-source models Hunyuan Video (13B) and other commercial models such as Sora, Kling, and Hailuo. Furthermore, our model also achieves state-of-the-art performance in long video generation, surpassing other open-source models in this field. Our demo can be viewed at https://landiff.github.io/.",
      "authors": [
        "Aoxiong Yin",
        "Kai Shen",
        "Yichong Leng",
        "Xu Tan",
        "Xinyu Zhou",
        "Juncheng Li",
        "Siliang Tang"
      ],
      "published": "2025-03-06T16:53:14Z",
      "updated": "2025-04-29T10:34:28Z",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2503.04606v3",
      "landing_url": "https://arxiv.org/abs/2503.04606v3",
      "doi": "https://doi.org/10.48550/arXiv.2503.04606"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The work centers on discrete visual tokens for video generation, not discrete audio/speech tokenization required by the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The work centers on discrete visual tokens for video generation, not discrete audio/speech tokenization required by the inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper centers on video generation by integrating language models and diffusion models, focusing on semantic tokenization of visual features rather than audio or speech tokenization. There is no indication that discrete audio/speech tokenization is a core object of the study, nor does it discuss audio tokenization, reconstruction, or generation.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper centers on video generation by integrating language models and diffusion models, focusing on semantic tokenization of visual features rather than audio or speech tokenization. There is no indication that discrete audio/speech tokenization is a core object of the study, nor does it discuss audio tokenization, reconstruction, or generation.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "VQEL: Enabling Self-Developed Symbolic Language in Agents through Vector Quantization in Emergent Language Games",
    "abstract": "In the field of emergent language, efforts have traditionally focused on developing communication protocols through interactions between agents in referential games. However, the aspect of internal language learning, where language serves not only as a communicative tool with others but also as a means for individual thinking, self-reflection, and problem-solving remains underexplored. Developing a language through self-play, without another agent's involvement, poses a unique challenge. It requires an agent to craft symbolic representations and train them using direct gradient methods. The challenge here is that if an agent attempts to learn symbolic representations through self-play using conventional modeling and techniques such as REINFORCE, the solution will offer no advantage over previous multi-agent approaches. We introduce VQEL, a novel method that incorporates Vector Quantization into the agents' architecture, enabling them to autonomously invent and develop discrete symbolic representations in a self-play referential game. Following the self-play phase, agents can enhance their language through reinforcement learning and interactions with other agents in the mutual-play phase. Our experiments across various datasets demonstrate that VQEL not only outperforms the traditional REINFORCE method but also benefits from improved control and reduced susceptibility to collapse, thanks to the incorporation of vector quantization.",
    "metadata": {
      "arxiv_id": "2503.04940",
      "title": "VQEL: Enabling Self-Developed Symbolic Language in Agents through Vector Quantization in Emergent Language Games",
      "summary": "In the field of emergent language, efforts have traditionally focused on developing communication protocols through interactions between agents in referential games. However, the aspect of internal language learning, where language serves not only as a communicative tool with others but also as a means for individual thinking, self-reflection, and problem-solving remains underexplored. Developing a language through self-play, without another agent's involvement, poses a unique challenge. It requires an agent to craft symbolic representations and train them using direct gradient methods. The challenge here is that if an agent attempts to learn symbolic representations through self-play using conventional modeling and techniques such as REINFORCE, the solution will offer no advantage over previous multi-agent approaches. We introduce VQEL, a novel method that incorporates Vector Quantization into the agents' architecture, enabling them to autonomously invent and develop discrete symbolic representations in a self-play referential game. Following the self-play phase, agents can enhance their language through reinforcement learning and interactions with other agents in the mutual-play phase. Our experiments across various datasets demonstrate that VQEL not only outperforms the traditional REINFORCE method but also benefits from improved control and reduced susceptibility to collapse, thanks to the incorporation of vector quantization.",
      "authors": [
        "Mohammad Mahdi Samiei Paqaleh",
        "Mahdieh Soleymani Baghshah"
      ],
      "published": "2025-03-06T20:15:51Z",
      "updated": "2025-03-06T20:15:51Z",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2503.04940v1",
      "landing_url": "https://arxiv.org/abs/2503.04940v1",
      "doi": "https://doi.org/10.48550/arXiv.2503.04940"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The abstract centers on enabling discrete symbolic representations in agents via vector quantization within emergent language games, focusing on internal language/self-play rather than discrete audio/speech tokenization as a core object for audio reconstruction or speech tasks, so it does not meet the inclusion criteria and aligns with exclusion criteria about non-audio tokenization or non-speech applications.",
      "evaluation": 2
    },
    "round-A_JuniorNano_reasoning": "The abstract centers on enabling discrete symbolic representations in agents via vector quantization within emergent language games, focusing on internal language/self-play rather than discrete audio/speech tokenization as a core object for audio reconstruction or speech tasks, so it does not meet the inclusion criteria and aligns with exclusion criteria about non-audio tokenization or non-speech applications.",
    "round-A_JuniorNano_evaluation": 2,
    "round-A_JuniorMini_output": {
      "reasoning": "The study introduces VQEL, a method involving vector quantization for agents to autonomously invent discrete symbolic representations during emergent language games, but the abstract does not mention audio or speech signals, nor does it discuss discrete audio/speech tokenization or its application to audio reconstruction, generation, or understanding tasks; therefore, it does not meet the inclusion criteria focused on discrete audio tokenization and is unrelated to audio modalities in the stated context.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study introduces VQEL, a method involving vector quantization for agents to autonomously invent discrete symbolic representations during emergent language games, but the abstract does not mention audio or speech signals, nor does it discuss discrete audio/speech tokenization or its application to audio reconstruction, generation, or understanding tasks; therefore, it does not meet the inclusion criteria focused on discrete audio tokenization and is unrelated to audio modalities in the stated context.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:2)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:2)"
  },
  {
    "title": "SemHiTok: A Unified Image Tokenizer via Semantic-Guided Hierarchical Codebook for Multimodal Understanding and Generation",
    "abstract": "In this paper, we introduce SemHiTok, a unified image Tokenizer via Semantic-Guided Hierarchical codebook that provides consistent discrete representations for multimodal understanding and generation. Recently, unified image tokenizers have sparked exploration within research community, which is designed to capture high-level semantic features for understanding and retaining low-level pixel features for generation. Previous works attempt to train a unified image tokenizer by combining loss for semantic distillation and pixel reconstruction. However, due to the differing levels of features prioritized by multimodal understanding and generation, joint training methods face significant challenges in achieving a good trade-off. SemHiTok addresses this challenge through a novel semantic-guided hierarchical codebook, which builds pixel sub-codebooks on a pretrained semantic codebook. This design decouples semantic and pixel both in terms of structure and training strategy, enabling the tokenizer to capture pixel features while retaining its ability to comprehend high-level semantic information. Our experiments demonstrate that SemHiTok achieves SOTA performance in image reconstruction and multimodal understanding under LLaVA-v1.5 setting. Further, we develop a unified MLLM with SemHiTok, which exhibits superior performance across multimodal understanding and generation tasks. For understanding, SemHiTok achieves impressive performance on most benchmarks. For generation, our model achieves SOTA performance on MJHQ30K in unified MLLMs.",
    "metadata": {
      "arxiv_id": "2503.06764",
      "title": "SemHiTok: A Unified Image Tokenizer via Semantic-Guided Hierarchical Codebook for Multimodal Understanding and Generation",
      "summary": "In this paper, we introduce SemHiTok, a unified image Tokenizer via Semantic-Guided Hierarchical codebook that provides consistent discrete representations for multimodal understanding and generation. Recently, unified image tokenizers have sparked exploration within research community, which is designed to capture high-level semantic features for understanding and retaining low-level pixel features for generation. Previous works attempt to train a unified image tokenizer by combining loss for semantic distillation and pixel reconstruction. However, due to the differing levels of features prioritized by multimodal understanding and generation, joint training methods face significant challenges in achieving a good trade-off. SemHiTok addresses this challenge through a novel semantic-guided hierarchical codebook, which builds pixel sub-codebooks on a pretrained semantic codebook. This design decouples semantic and pixel both in terms of structure and training strategy, enabling the tokenizer to capture pixel features while retaining its ability to comprehend high-level semantic information. Our experiments demonstrate that SemHiTok achieves SOTA performance in image reconstruction and multimodal understanding under LLaVA-v1.5 setting. Further, we develop a unified MLLM with SemHiTok, which exhibits superior performance across multimodal understanding and generation tasks. For understanding, SemHiTok achieves impressive performance on most benchmarks. For generation, our model achieves SOTA performance on MJHQ30K in unified MLLMs.",
      "authors": [
        "Zisheng Chen",
        "Chunwei Wang",
        "Xiuwei Chen",
        "Hongbin Xu",
        "Runhui Huang",
        "Jun Zhou",
        "Jianhua Han",
        "Hang Xu",
        "Xiaodan Liang"
      ],
      "published": "2025-03-09T20:42:34Z",
      "updated": "2025-06-04T04:13:07Z",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2503.06764v4",
      "landing_url": "https://arxiv.org/abs/2503.06764v4",
      "doi": "https://doi.org/10.48550/arXiv.2503.06764"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The paper centers on image tokenization and multimodal understanding, not discrete audio/speech tokenization; inclusion criteria require explicit discrete audio tokenization, so it should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The paper centers on image tokenization and multimodal understanding, not discrete audio/speech tokenization; inclusion criteria require explicit discrete audio tokenization, so it should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study SemHiTok deals with unified image tokenization focused on semantic-guided hierarchical codebooks for multimodal understanding and generation, specifically targeting image data rather than audio or speech signals. The inclusion criteria require the study to focus on discrete audio or speech tokenization, which this work does not address, as it centers on image tokenization without mention of audio or speech discrete tokens. Thus, it fails to meet the core inclusion requirements related to audio discrete tokenization.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study SemHiTok deals with unified image tokenization focused on semantic-guided hierarchical codebooks for multimodal understanding and generation, specifically targeting image data rather than audio or speech signals. The inclusion criteria require the study to focus on discrete audio or speech tokenization, which this work does not address, as it centers on image tokenization without mention of audio or speech discrete tokens. Thus, it fails to meet the core inclusion requirements related to audio discrete tokenization.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Task Vector Quantization for Memory-Efficient Model Merging",
    "abstract": "Model merging enables efficient multi-task models by combining task-specific fine-tuned checkpoints. However, storing multiple task-specific checkpoints requires significant memory, limiting scalability and restricting model merging to larger models and diverse tasks. In this paper, we propose quantizing task vectors (i.e., the difference between pre-trained and fine-tuned checkpoints) instead of quantizing fine-tuned checkpoints. We observe that task vectors exhibit a narrow weight range, enabling low precision quantization (e.g., 4 bit) within existing task vector merging frameworks. To further mitigate quantization errors within ultra-low bit precision (e.g., 2 bit), we introduce Residual Task Vector Quantization, which decomposes the task vector into a base vector and offset component. We allocate bits based on quantization sensitivity, ensuring precision while minimizing error within a memory budget. Experiments on image classification and dense prediction show our method maintains or improves model merging performance while using only 8% of the memory required for full-precision checkpoints.",
    "metadata": {
      "arxiv_id": "2503.06921",
      "title": "Task Vector Quantization for Memory-Efficient Model Merging",
      "summary": "Model merging enables efficient multi-task models by combining task-specific fine-tuned checkpoints. However, storing multiple task-specific checkpoints requires significant memory, limiting scalability and restricting model merging to larger models and diverse tasks. In this paper, we propose quantizing task vectors (i.e., the difference between pre-trained and fine-tuned checkpoints) instead of quantizing fine-tuned checkpoints. We observe that task vectors exhibit a narrow weight range, enabling low precision quantization (e.g., 4 bit) within existing task vector merging frameworks. To further mitigate quantization errors within ultra-low bit precision (e.g., 2 bit), we introduce Residual Task Vector Quantization, which decomposes the task vector into a base vector and offset component. We allocate bits based on quantization sensitivity, ensuring precision while minimizing error within a memory budget. Experiments on image classification and dense prediction show our method maintains or improves model merging performance while using only 8% of the memory required for full-precision checkpoints.",
      "authors": [
        "Youngeun Kim",
        "Seunghwan Lee",
        "Aecheon Jung",
        "Bogon Ryu",
        "Sungeun Hong"
      ],
      "published": "2025-03-10T05:00:24Z",
      "updated": "2025-08-07T10:57:05Z",
      "categories": [
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2503.06921v2",
      "landing_url": "https://arxiv.org/abs/2503.06921v2",
      "doi": "https://doi.org/10.48550/arXiv.2503.06921"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This item focuses on quantizing task vectors for memory-efficient model merging in vision tasks, not on discrete audio/speech tokenization, so it does not meet the inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "This item focuses on quantizing task vectors for memory-efficient model merging in vision tasks, not on discrete audio/speech tokenization, so it does not meet the inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on memory-efficient model merging through task vector quantization applied to multi-task models primarily in the context of image classification and dense prediction, with no mention or analysis of discrete audio/speech tokenization or related tasks; thus, it does not meet the inclusion criteria related to audio tokenization, nor does it address any exclusion-specified topics tied to discrete audio tokens.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on memory-efficient model merging through task vector quantization applied to multi-task models primarily in the context of image classification and dense prediction, with no mention or analysis of discrete audio/speech tokenization or related tasks; thus, it does not meet the inclusion criteria related to audio tokenization, nor does it address any exclusion-specified topics tied to discrete audio tokens.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Unleashing the Potential of Large Language Models for Text-to-Image Generation through Autoregressive Representation Alignment",
    "abstract": "We present Autoregressive Representation Alignment (ARRA), a new training framework that unlocks global-coherent text-to-image generation in autoregressive LLMs without architectural modifications. Different from prior works that require complex architectural redesigns, ARRA aligns LLM's hidden states with visual representations from external visual foundational models via a global visual alignment loss and a hybrid token, <HYBNEXT>. This token enforces dual constraints: local next-token prediction and global semantic distillation, enabling LLMs to implicitly learn spatial and contextual coherence while retaining their original autoregressive paradigm. Extensive experiments validate ARRA's plug-and-play versatility. When training T2I LLMs from scratch, ARRA reduces FID by 16.6% (ImageNet), 12.0% (LAION-COCO) for autoregressive LLMs like LlamaGen, without modifying original architecture and inference mechanism. For training from text-generation-only LLMs, ARRA reduces FID by 25.5% (MIMIC-CXR), 8.8% (DeepEyeNet) for advanced LLMs like Chameleon. For domain adaptation, ARRA aligns general-purpose LLMs with specialized models (e.g., BioMedCLIP), achieving an 18.6% FID reduction over direct fine-tuning on medical imaging (MIMIC-CXR). These results demonstrate that training objective redesign, rather than architectural modifications, can resolve cross-modal global coherence challenges. ARRA offers a complementary paradigm for advancing autoregressive models. The code is available at https://github.com/HKU-HealthAI/ARRA.",
    "metadata": {
      "arxiv_id": "2503.07334",
      "title": "Unleashing the Potential of Large Language Models for Text-to-Image Generation through Autoregressive Representation Alignment",
      "summary": "We present Autoregressive Representation Alignment (ARRA), a new training framework that unlocks global-coherent text-to-image generation in autoregressive LLMs without architectural modifications. Different from prior works that require complex architectural redesigns, ARRA aligns LLM's hidden states with visual representations from external visual foundational models via a global visual alignment loss and a hybrid token, <HYBNEXT>. This token enforces dual constraints: local next-token prediction and global semantic distillation, enabling LLMs to implicitly learn spatial and contextual coherence while retaining their original autoregressive paradigm. Extensive experiments validate ARRA's plug-and-play versatility. When training T2I LLMs from scratch, ARRA reduces FID by 16.6% (ImageNet), 12.0% (LAION-COCO) for autoregressive LLMs like LlamaGen, without modifying original architecture and inference mechanism. For training from text-generation-only LLMs, ARRA reduces FID by 25.5% (MIMIC-CXR), 8.8% (DeepEyeNet) for advanced LLMs like Chameleon. For domain adaptation, ARRA aligns general-purpose LLMs with specialized models (e.g., BioMedCLIP), achieving an 18.6% FID reduction over direct fine-tuning on medical imaging (MIMIC-CXR). These results demonstrate that training objective redesign, rather than architectural modifications, can resolve cross-modal global coherence challenges. ARRA offers a complementary paradigm for advancing autoregressive models. The code is available at https://github.com/HKU-HealthAI/ARRA.",
      "authors": [
        "Xing Xie",
        "Jiawei Liu",
        "Ziyue Lin",
        "Huijie Fan",
        "Zhi Han",
        "Yandong Tang",
        "Liangqiong Qu"
      ],
      "published": "2025-03-10T13:49:28Z",
      "updated": "2025-11-14T10:42:03Z",
      "categories": [
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2503.07334v4",
      "landing_url": "https://arxiv.org/abs/2503.07334v4",
      "doi": "https://doi.org/10.48550/arXiv.2503.07334"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item focuses on autoregressive text-to-image generation with LLMs and alignment to visual representations; it does not propose or analyze discrete audio tokenization for speech, so it fails the core audio/tokenization inclusion criterion.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item focuses on autoregressive text-to-image generation with LLMs and alignment to visual representations; it does not propose or analyze discrete audio tokenization for speech, so it fails the core audio/tokenization inclusion criterion.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on text-to-image generation using large language models and autoregressive representation alignment, with no mention or analysis of discrete audio/speech tokenization or related audio signal processing, which is the core requirement of the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on text-to-image generation using large language models and autoregressive representation alignment, with no mention or analysis of discrete audio/speech tokenization or related audio signal processing, which is the core requirement of the inclusion criteria.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "ViM-VQ: Efficient Post-Training Vector Quantization for Visual Mamba",
    "abstract": "Visual Mamba networks (ViMs) extend the selective state space model (Mamba) to various vision tasks and demonstrate significant potential. As a promising compression technique, vector quantization (VQ) decomposes network weights into codebooks and assignments, significantly reducing memory usage and computational latency, thereby enabling the deployment of ViMs on edge devices. Although existing VQ methods have achieved extremely low-bit quantization (e.g., 3-bit, 2-bit, and 1-bit) in convolutional neural networks and Transformer-based networks, directly applying these methods to ViMs results in unsatisfactory accuracy. We identify several key challenges: 1) The weights of Mamba-based blocks in ViMs contain numerous outliers, significantly amplifying quantization errors. 2) When applied to ViMs, the latest VQ methods suffer from excessive memory consumption, lengthy calibration procedures, and suboptimal performance in the search for optimal codewords. In this paper, we propose ViM-VQ, an efficient post-training vector quantization method tailored for ViMs. ViM-VQ consists of two innovative components: 1) a fast convex combination optimization algorithm that efficiently updates both the convex combinations and the convex hulls to search for optimal codewords, and 2) an incremental vector quantization strategy that incrementally confirms optimal codewords to mitigate truncation errors. Experimental results demonstrate that ViM-VQ achieves state-of-the-art performance in low-bit quantization across various visual tasks.",
    "metadata": {
      "arxiv_id": "2503.09509",
      "title": "ViM-VQ: Efficient Post-Training Vector Quantization for Visual Mamba",
      "summary": "Visual Mamba networks (ViMs) extend the selective state space model (Mamba) to various vision tasks and demonstrate significant potential. As a promising compression technique, vector quantization (VQ) decomposes network weights into codebooks and assignments, significantly reducing memory usage and computational latency, thereby enabling the deployment of ViMs on edge devices. Although existing VQ methods have achieved extremely low-bit quantization (e.g., 3-bit, 2-bit, and 1-bit) in convolutional neural networks and Transformer-based networks, directly applying these methods to ViMs results in unsatisfactory accuracy. We identify several key challenges: 1) The weights of Mamba-based blocks in ViMs contain numerous outliers, significantly amplifying quantization errors. 2) When applied to ViMs, the latest VQ methods suffer from excessive memory consumption, lengthy calibration procedures, and suboptimal performance in the search for optimal codewords. In this paper, we propose ViM-VQ, an efficient post-training vector quantization method tailored for ViMs. ViM-VQ consists of two innovative components: 1) a fast convex combination optimization algorithm that efficiently updates both the convex combinations and the convex hulls to search for optimal codewords, and 2) an incremental vector quantization strategy that incrementally confirms optimal codewords to mitigate truncation errors. Experimental results demonstrate that ViM-VQ achieves state-of-the-art performance in low-bit quantization across various visual tasks.",
      "authors": [
        "Juncan Deng",
        "Shuaiting Li",
        "Zeyu Wang",
        "Kedong Xu",
        "Hong Gu",
        "Kejie Huang"
      ],
      "published": "2025-03-12T16:18:45Z",
      "updated": "2025-07-30T16:58:48Z",
      "categories": [
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2503.09509v2",
      "landing_url": "https://arxiv.org/abs/2503.09509v2",
      "doi": "https://doi.org/10.48550/arXiv.2503.09509"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The article addresses efficient post-training vector quantization for Visual Mamba (vision), with no focus on discrete audio/speech tokenization; thus it does not meet the inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The article addresses efficient post-training vector quantization for Visual Mamba (vision), with no focus on discrete audio/speech tokenization; thus it does not meet the inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on vector quantization methods applied to Visual Mamba networks for visual tasks, with no mention or analysis of discrete audio or speech tokenization. It targets vision models and visual tasks rather than audio signal tokenization or discrete audio representations, thus failing to meet the inclusion criteria centered on discrete audio/speech tokenization, and it does not address the exclusion criteria related to audio or speech tokenization.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on vector quantization methods applied to Visual Mamba networks for visual tasks, with no mention or analysis of discrete audio or speech tokenization. It targets vision models and visual tasks rather than audio signal tokenization or discrete audio representations, thus failing to meet the inclusion criteria centered on discrete audio/speech tokenization, and it does not address the exclusion criteria related to audio or speech tokenization.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "A Hierarchical Semantic Distillation Framework for Open-Vocabulary Object Detection",
    "abstract": "Open-vocabulary object detection (OVD) aims to detect objects beyond the training annotations, where detectors are usually aligned to a pre-trained vision-language model, eg, CLIP, to inherit its generalizable recognition ability so that detectors can recognize new or novel objects. However, previous works directly align the feature space with CLIP and fail to learn the semantic knowledge effectively. In this work, we propose a hierarchical semantic distillation framework named HD-OVD to construct a comprehensive distillation process, which exploits generalizable knowledge from the CLIP model in three aspects. In the first hierarchy of HD-OVD, the detector learns fine-grained instance-wise semantics from the CLIP image encoder by modeling relations among single objects in the visual space. Besides, we introduce text space novel-class-aware classification to help the detector assimilate the highly generalizable class-wise semantics from the CLIP text encoder, representing the second hierarchy. Lastly, abundant image-wise semantics containing multi-object and their contexts are also distilled by an image-wise contrastive distillation. Benefiting from the elaborated semantic distillation in triple hierarchies, our HD-OVD inherits generalizable recognition ability from CLIP in instance, class, and image levels. Thus, we boost the novel AP on the OV-COCO dataset to 46.4% with a ResNet50 backbone, which outperforms others by a clear margin. We also conduct extensive ablation studies to analyze how each component works.",
    "metadata": {
      "arxiv_id": "2503.10152",
      "title": "A Hierarchical Semantic Distillation Framework for Open-Vocabulary Object Detection",
      "summary": "Open-vocabulary object detection (OVD) aims to detect objects beyond the training annotations, where detectors are usually aligned to a pre-trained vision-language model, eg, CLIP, to inherit its generalizable recognition ability so that detectors can recognize new or novel objects. However, previous works directly align the feature space with CLIP and fail to learn the semantic knowledge effectively. In this work, we propose a hierarchical semantic distillation framework named HD-OVD to construct a comprehensive distillation process, which exploits generalizable knowledge from the CLIP model in three aspects. In the first hierarchy of HD-OVD, the detector learns fine-grained instance-wise semantics from the CLIP image encoder by modeling relations among single objects in the visual space. Besides, we introduce text space novel-class-aware classification to help the detector assimilate the highly generalizable class-wise semantics from the CLIP text encoder, representing the second hierarchy. Lastly, abundant image-wise semantics containing multi-object and their contexts are also distilled by an image-wise contrastive distillation. Benefiting from the elaborated semantic distillation in triple hierarchies, our HD-OVD inherits generalizable recognition ability from CLIP in instance, class, and image levels. Thus, we boost the novel AP on the OV-COCO dataset to 46.4% with a ResNet50 backbone, which outperforms others by a clear margin. We also conduct extensive ablation studies to analyze how each component works.",
      "authors": [
        "Shenghao Fu",
        "Junkai Yan",
        "Qize Yang",
        "Xihan Wei",
        "Xiaohua Xie",
        "Wei-Shi Zheng"
      ],
      "published": "2025-03-13T08:27:18Z",
      "updated": "2025-03-13T08:27:18Z",
      "categories": [
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2503.10152v1",
      "landing_url": "https://arxiv.org/abs/2503.10152v1",
      "doi": "https://doi.org/10.48550/arXiv.2503.10152"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This is a computer‑vision paper on open‑vocabulary object detection with CLIP and semantic distillation; it involves neither discrete audio tokenization nor speech analysis, so it fails the inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "This is a computer‑vision paper on open‑vocabulary object detection with CLIP and semantic distillation; it involves neither discrete audio tokenization nor speech analysis, so it fails the inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on open-vocabulary object detection and semantic distillation in vision-language models, with no mention of audio or speech tokenization, nor discrete audio token representations; thus, it does not meet the inclusion criteria related to discrete audio/speech tokenization.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on open-vocabulary object detection and semantic distillation in vision-language models, with no mention of audio or speech tokenization, nor discrete audio token representations; thus, it does not meet the inclusion criteria related to discrete audio/speech tokenization.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "MMS-LLaMA: Efficient LLM-based Audio-Visual Speech Recognition with Minimal Multimodal Speech Tokens",
    "abstract": "Audio-Visual Speech Recognition (AVSR) achieves robust speech recognition in noisy environments by combining auditory and visual information. However, recent Large Language Model (LLM) based AVSR systems incur high computational costs due to the high temporal resolution of audio-visual speech processed by LLMs. In this work, we introduce an efficient multimodal speech LLM framework that minimizes token length while preserving essential linguistic content. Our approach employs an early AV-fusion module for streamlined feature integration, an audio-visual speech Q-Former that dynamically allocates tokens based on input duration, and a refined query allocation strategy with a speech rate predictor to adjust token allocation according to speaking speed of each audio sample. Extensive experiments on the LRS3 dataset show that our method achieves state-of-the-art performance with a WER of 0.72% while using only 3.5 tokens per second. Moreover, our approach not only reduces token usage by 86% compared to the previous multimodal speech LLM framework, but also improves computational efficiency by reducing FLOPs by 35.7%.",
    "metadata": {
      "arxiv_id": "2503.11315",
      "title": "MMS-LLaMA: Efficient LLM-based Audio-Visual Speech Recognition with Minimal Multimodal Speech Tokens",
      "summary": "Audio-Visual Speech Recognition (AVSR) achieves robust speech recognition in noisy environments by combining auditory and visual information. However, recent Large Language Model (LLM) based AVSR systems incur high computational costs due to the high temporal resolution of audio-visual speech processed by LLMs. In this work, we introduce an efficient multimodal speech LLM framework that minimizes token length while preserving essential linguistic content. Our approach employs an early AV-fusion module for streamlined feature integration, an audio-visual speech Q-Former that dynamically allocates tokens based on input duration, and a refined query allocation strategy with a speech rate predictor to adjust token allocation according to speaking speed of each audio sample. Extensive experiments on the LRS3 dataset show that our method achieves state-of-the-art performance with a WER of 0.72% while using only 3.5 tokens per second. Moreover, our approach not only reduces token usage by 86% compared to the previous multimodal speech LLM framework, but also improves computational efficiency by reducing FLOPs by 35.7%.",
      "authors": [
        "Jeong Hun Yeo",
        "Hyeongseop Rha",
        "Se Jin Park",
        "Yong Man Ro"
      ],
      "published": "2025-03-14T11:31:30Z",
      "updated": "2025-06-05T05:58:37Z",
      "categories": [
        "cs.CV",
        "cs.MM",
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2503.11315v2",
      "landing_url": "https://arxiv.org/abs/2503.11315v2",
      "doi": "https://doi.org/10.48550/arXiv.2503.11315"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The abstract explicitly discusses minimizing and allocating discrete speech tokens in an LLM-based AVSR system and quantifies token usage (e.g., 3.5 tokens per second), indicating discrete tokenization as a core object of study, which aligns with the inclusion criteria and does not reduce to continuous features or purely engineering content.",
      "evaluation": 5
    },
    "round-A_JuniorNano_reasoning": "The abstract explicitly discusses minimizing and allocating discrete speech tokens in an LLM-based AVSR system and quantifies token usage (e.g., 3.5 tokens per second), indicating discrete tokenization as a core object of study, which aligns with the inclusion criteria and does not reduce to continuous features or purely engineering content.",
    "round-A_JuniorNano_evaluation": 5,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on an efficient AVSR framework that minimizes token length and dynamically allocates tokens for audio-visual speech by using a speech Q-Former and speech rate predictor, indicating a clear engagement with discrete audio token design and allocation relevant to speech recognition tasks, and includes experimental validation; thus it meets the inclusion criteria and none of the exclusion criteria.",
      "evaluation": 5
    },
    "round-A_JuniorMini_reasoning": "The study focuses on an efficient AVSR framework that minimizes token length and dynamically allocates tokens for audio-visual speech by using a speech Q-Former and speech rate predictor, indicating a clear engagement with discrete audio token design and allocation relevant to speech recognition tasks, and includes experimental validation; thus it meets the inclusion criteria and none of the exclusion criteria.",
    "round-A_JuniorMini_evaluation": 5,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "include (junior:5)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Universal Speech Token Learning via Low-Bitrate Neural Codec and Pretrained Representations",
    "abstract": "Current large speech language models are mainly based on semantic tokens from discretization of self-supervised learned representations and acoustic tokens from a neural codec, following a semantic-modeling and acoustic-synthesis paradigm. However, semantic tokens discard paralinguistic attributes of speakers that is important for natural spoken communication, while prompt-based acoustic synthesis from semantic tokens has limits in recovering paralinguistic details and suffers from robustness issues, especially when there are domain gaps between the prompt and the target. This paper unifies two types of tokens and proposes the UniCodec, a universal speech token learning that encapsulates all semantics of speech, including linguistic and paralinguistic information, into a compact and semantically-disentangled unified token. Such a unified token can not only benefit speech language models in understanding with paralinguistic hints but also help speech generation with high-quality output. A low-bitrate neural codec is leveraged to learn such disentangled discrete representations at global and local scales, with knowledge distilled from self-supervised learned features. Extensive evaluations on multilingual datasets demonstrate its effectiveness in generating natural, expressive and long-term consistent output quality with paralinguistic attributes well preserved in several speech processing tasks.",
    "metadata": {
      "arxiv_id": "2503.12115",
      "title": "Universal Speech Token Learning via Low-Bitrate Neural Codec and Pretrained Representations",
      "summary": "Current large speech language models are mainly based on semantic tokens from discretization of self-supervised learned representations and acoustic tokens from a neural codec, following a semantic-modeling and acoustic-synthesis paradigm. However, semantic tokens discard paralinguistic attributes of speakers that is important for natural spoken communication, while prompt-based acoustic synthesis from semantic tokens has limits in recovering paralinguistic details and suffers from robustness issues, especially when there are domain gaps between the prompt and the target. This paper unifies two types of tokens and proposes the UniCodec, a universal speech token learning that encapsulates all semantics of speech, including linguistic and paralinguistic information, into a compact and semantically-disentangled unified token. Such a unified token can not only benefit speech language models in understanding with paralinguistic hints but also help speech generation with high-quality output. A low-bitrate neural codec is leveraged to learn such disentangled discrete representations at global and local scales, with knowledge distilled from self-supervised learned features. Extensive evaluations on multilingual datasets demonstrate its effectiveness in generating natural, expressive and long-term consistent output quality with paralinguistic attributes well preserved in several speech processing tasks.",
      "authors": [
        "Xue Jiang",
        "Xiulian Peng",
        "Yuan Zhang",
        "Yan Lu"
      ],
      "published": "2025-03-15T12:50:43Z",
      "updated": "2025-10-15T06:52:30Z",
      "categories": [
        "cs.SD",
        "cs.AI",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2503.12115v2",
      "landing_url": "https://arxiv.org/abs/2503.12115v2",
      "doi": "https://doi.org/10.1109/JSTSP.2024.3488557"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The paper explicitly proposes and evaluates a unified discrete speech token representation learned via a low-bitrate neural codec, i.e., core discrete tokenization for speech, with experiments on generation/understanding tasks, satisfying the inclusion criteria and not matching exclusion cases.",
      "evaluation": 5
    },
    "round-A_JuniorNano_reasoning": "The paper explicitly proposes and evaluates a unified discrete speech token representation learned via a low-bitrate neural codec, i.e., core discrete tokenization for speech, with experiments on generation/understanding tasks, satisfying the inclusion criteria and not matching exclusion cases.",
    "round-A_JuniorNano_evaluation": 5,
    "round-A_JuniorMini_output": {
      "reasoning": "The study explicitly proposes a novel discrete audio tokenization method called UniCodec, which integrates semantic and paralinguistic information into unified discrete tokens learned via a low-bitrate neural codec, with evaluations on speech tasks demonstrating its effectiveness, thus meeting the inclusion criteria and avoiding the exclusion criteria related to continuous features, traditional codecs, or unrelated tokenizations.",
      "evaluation": 5
    },
    "round-A_JuniorMini_reasoning": "The study explicitly proposes a novel discrete audio tokenization method called UniCodec, which integrates semantic and paralinguistic information into unified discrete tokens learned via a low-bitrate neural codec, with evaluations on speech tasks demonstrating its effectiveness, thus meeting the inclusion criteria and avoiding the exclusion criteria related to continuous features, traditional codecs, or unrelated tokenizations.",
    "round-A_JuniorMini_evaluation": 5,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "include (junior:5)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "RENO: Real-Time Neural Compression for 3D LiDAR Point Clouds",
    "abstract": "Despite the substantial advancements demonstrated by learning-based neural models in the LiDAR Point Cloud Compression (LPCC) task, realizing real-time compression - an indispensable criterion for numerous industrial applications - remains a formidable challenge. This paper proposes RENO, the first real-time neural codec for 3D LiDAR point clouds, achieving superior performance with a lightweight model. RENO skips the octree construction and directly builds upon the multiscale sparse tensor representation. Instead of the multi-stage inferring, RENO devises sparse occupancy codes, which exploit cross-scale correlation and derive voxels' occupancy in a one-shot manner, greatly saving processing time. Experimental results demonstrate that the proposed RENO achieves real-time coding speed, 10 fps at 14-bit depth on a desktop platform (e.g., one RTX 3090 GPU) for both encoding and decoding processes, while providing 12.25% and 48.34% bit-rate savings compared to G-PCCv23 and Draco, respectively, at a similar quality. RENO model size is merely 1MB, making it attractive for practical applications. The source code is available at https://github.com/NJUVISION/RENO.",
    "metadata": {
      "arxiv_id": "2503.12382",
      "title": "RENO: Real-Time Neural Compression for 3D LiDAR Point Clouds",
      "summary": "Despite the substantial advancements demonstrated by learning-based neural models in the LiDAR Point Cloud Compression (LPCC) task, realizing real-time compression - an indispensable criterion for numerous industrial applications - remains a formidable challenge. This paper proposes RENO, the first real-time neural codec for 3D LiDAR point clouds, achieving superior performance with a lightweight model. RENO skips the octree construction and directly builds upon the multiscale sparse tensor representation. Instead of the multi-stage inferring, RENO devises sparse occupancy codes, which exploit cross-scale correlation and derive voxels' occupancy in a one-shot manner, greatly saving processing time. Experimental results demonstrate that the proposed RENO achieves real-time coding speed, 10 fps at 14-bit depth on a desktop platform (e.g., one RTX 3090 GPU) for both encoding and decoding processes, while providing 12.25% and 48.34% bit-rate savings compared to G-PCCv23 and Draco, respectively, at a similar quality. RENO model size is merely 1MB, making it attractive for practical applications. The source code is available at https://github.com/NJUVISION/RENO.",
      "authors": [
        "Kang You",
        "Tong Chen",
        "Dandan Ding",
        "M. Salman Asif",
        "Zhan Ma"
      ],
      "published": "2025-03-16T07:03:12Z",
      "updated": "2025-03-16T07:03:12Z",
      "categories": [
        "cs.CV",
        "eess.IV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2503.12382v1",
      "landing_url": "https://arxiv.org/abs/2503.12382v1",
      "doi": "https://doi.org/10.48550/arXiv.2503.12382"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This item focuses on neural compression of 3D LiDAR point clouds and does not involve discrete audio/speech tokenization or token-based representation, so it fails the required inclusion criteria for discrete audio tokens.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "This item focuses on neural compression of 3D LiDAR point clouds and does not involve discrete audio/speech tokenization or token-based representation, so it fails the required inclusion criteria for discrete audio tokens.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on a neural codec for 3D LiDAR point cloud compression, which is unrelated to audio or speech tokenization, and it does not address discrete audio tokenization or quantization for speech tasks, thus failing to meet the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on a neural codec for 3D LiDAR point cloud compression, which is unrelated to audio or speech tokenization, and it does not address discrete audio tokenization or quantization for speech tasks, thus failing to meet the inclusion criteria.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Shushing! Let's Imagine an Authentic Speech from the Silent Video",
    "abstract": "Vision-guided speech generation aims to produce authentic speech from facial appearance or lip motions without relying on auditory signals, offering significant potential for applications such as dubbing in filmmaking and assisting individuals with aphonia. Despite recent progress, existing methods struggle to achieve unified cross-modal alignment across semantics, timbre, and emotional prosody from visual cues, prompting us to propose Consistent Video-to-Speech (CV2S) as an extended task to enhance cross-modal consistency. To tackle emerging challenges, we introduce ImaginTalk, a novel cross-modal diffusion framework that generates faithful speech using only visual input, operating within a discrete space. Specifically, we propose a discrete lip aligner that predicts discrete speech tokens from lip videos to capture semantic information, while an error detector identifies misaligned tokens, which are subsequently refined through masked language modeling with BERT. To further enhance the expressiveness of the generated speech, we develop a style diffusion transformer equipped with a face-style adapter that adaptively customizes identity and prosody dynamics across both the channel and temporal dimensions while ensuring synchronization with lip-aware semantic features. Extensive experiments demonstrate that ImaginTalk can generate high-fidelity speech with more accurate semantic details and greater expressiveness in timbre and emotion compared to state-of-the-art baselines. Demos are shown at our project page: https://imagintalk.github.io.",
    "metadata": {
      "arxiv_id": "2503.14928",
      "title": "Shushing! Let's Imagine an Authentic Speech from the Silent Video",
      "summary": "Vision-guided speech generation aims to produce authentic speech from facial appearance or lip motions without relying on auditory signals, offering significant potential for applications such as dubbing in filmmaking and assisting individuals with aphonia. Despite recent progress, existing methods struggle to achieve unified cross-modal alignment across semantics, timbre, and emotional prosody from visual cues, prompting us to propose Consistent Video-to-Speech (CV2S) as an extended task to enhance cross-modal consistency. To tackle emerging challenges, we introduce ImaginTalk, a novel cross-modal diffusion framework that generates faithful speech using only visual input, operating within a discrete space. Specifically, we propose a discrete lip aligner that predicts discrete speech tokens from lip videos to capture semantic information, while an error detector identifies misaligned tokens, which are subsequently refined through masked language modeling with BERT. To further enhance the expressiveness of the generated speech, we develop a style diffusion transformer equipped with a face-style adapter that adaptively customizes identity and prosody dynamics across both the channel and temporal dimensions while ensuring synchronization with lip-aware semantic features. Extensive experiments demonstrate that ImaginTalk can generate high-fidelity speech with more accurate semantic details and greater expressiveness in timbre and emotion compared to state-of-the-art baselines. Demos are shown at our project page: https://imagintalk.github.io.",
      "authors": [
        "Jiaxin Ye",
        "Hongming Shan"
      ],
      "published": "2025-03-19T06:28:17Z",
      "updated": "2025-03-19T06:28:17Z",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2503.14928v1",
      "landing_url": "https://arxiv.org/abs/2503.14928v1",
      "doi": "https://doi.org/10.48550/arXiv.2503.14928"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The abstract explicitly centers on discrete speech tokenization with a discrete lip aligner and token sequences for video-to-speech generation, aligning with the inclusion criteria and excluding those that rely only on continuous features.",
      "evaluation": 5
    },
    "round-A_JuniorNano_reasoning": "The abstract explicitly centers on discrete speech tokenization with a discrete lip aligner and token sequences for video-to-speech generation, aligning with the inclusion criteria and excluding those that rely only on continuous features.",
    "round-A_JuniorNano_evaluation": 5,
    "round-A_JuniorMini_output": {
      "reasoning": "The study explicitly proposes and analyzes discrete audio/speech tokenization as a core part of its methodology by introducing a discrete lip aligner and leveraging discrete speech tokens for speech generation from video, fulfilling the inclusion criteria and not conflicting with any exclusion criteria.",
      "evaluation": 5
    },
    "round-A_JuniorMini_reasoning": "The study explicitly proposes and analyzes discrete audio/speech tokenization as a core part of its methodology by introducing a discrete lip aligner and leveraging discrete speech tokens for speech generation from video, fulfilling the inclusion criteria and not conflicting with any exclusion criteria.",
    "round-A_JuniorMini_evaluation": 5,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "include (junior:5)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "AlphaSpace: Enabling Robotic Actions through Semantic Tokenization and Symbolic Reasoning",
    "abstract": "This paper presents AlphaSpace, a novel methodology designed to enhance the spatial reasoning capabilities of language models for robotic manipulation in 3D Cartesian space. AlphaSpace employs a hierarchical semantics-based tokenization strategy that encodes spatial information at both coarse and fine-grained levels. Our approach represents objects with their attributes, positions, and height information through structured tokens, enabling precise spatial reasoning without relying on traditional vision-based embeddings. This approach enables LLMs to accurately manipulate objects by positioning them at specific (x, y, z) coordinates. Experimental results suggest that AlphaSpace demonstrates promising potential for improving manipulation tasks, achieving a total accuracy of 66.67%, compared to 37.5% for GPT-4o and 29.17% for Claude 3.5 Sonnet. These results demonstrate the potential of structured spatial encoding for manipulation tasks and warrant further exploration.",
    "metadata": {
      "arxiv_id": "2503.18769",
      "title": "AlphaSpace: Enabling Robotic Actions through Semantic Tokenization and Symbolic Reasoning",
      "summary": "This paper presents AlphaSpace, a novel methodology designed to enhance the spatial reasoning capabilities of language models for robotic manipulation in 3D Cartesian space. AlphaSpace employs a hierarchical semantics-based tokenization strategy that encodes spatial information at both coarse and fine-grained levels. Our approach represents objects with their attributes, positions, and height information through structured tokens, enabling precise spatial reasoning without relying on traditional vision-based embeddings. This approach enables LLMs to accurately manipulate objects by positioning them at specific (x, y, z) coordinates. Experimental results suggest that AlphaSpace demonstrates promising potential for improving manipulation tasks, achieving a total accuracy of 66.67%, compared to 37.5% for GPT-4o and 29.17% for Claude 3.5 Sonnet. These results demonstrate the potential of structured spatial encoding for manipulation tasks and warrant further exploration.",
      "authors": [
        "Alan Dao",
        "Dinh Bach Vu",
        "Bui Quang Huy"
      ],
      "published": "2025-03-24T15:16:51Z",
      "updated": "2025-03-27T06:39:47Z",
      "categories": [
        "cs.CL",
        "cs.RO"
      ],
      "pdf_url": "https://arxiv.org/pdf/2503.18769v2",
      "landing_url": "https://arxiv.org/abs/2503.18769v2",
      "doi": "https://doi.org/10.48550/arXiv.2503.18769"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item focuses on semantic/token-based spatial encoding for robotic manipulation, not on discrete audio/speech tokenization as the core study, thus it does not meet the audio-tokenization inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item focuses on semantic/token-based spatial encoding for robotic manipulation, not on discrete audio/speech tokenization as the core study, thus it does not meet the audio-tokenization inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on spatial tokenization and symbolic reasoning for robotic manipulation in 3D space, with no mention or analysis of discrete audio/speech tokenization, discrete audio symbol sequences, or any audio-related tokenization tasks. Hence, it does not meet the inclusion criteria requiring discrete audio/speech tokenization as a core object of study.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on spatial tokenization and symbolic reasoning for robotic manipulation in 3D space, with no mention or analysis of discrete audio/speech tokenization, discrete audio symbol sequences, or any audio-related tokenization tasks. Hence, it does not meet the inclusion criteria requiring discrete audio/speech tokenization as a core object of study.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "FireRedTTS-1S: An Upgraded Streamable Foundation Text-to-Speech System",
    "abstract": "In this work, we upgrade FireRedTTS to a new version, FireRedTTS-1S, a high-quality streaming foundation text-to-speech system. FireRedTTS-1S achieves streaming speech generation via two steps: text-to-semantic decoding and semantic-to-acoustic decoding. In text-to-semantic decoding, a semantic-aware speech tokenizer converts the speech signal into semantic tokens, which can be synthesized from the text via a language model in an auto-regressive manner. Meanwhile, the semantic-to-acoustic decoding module simultaneously translates generated semantic tokens into the speech signal in a streaming way. We implement two approaches to achieve this module: 1) a chunk-wise streamable flow-matching approach, and 2) a multi-stream language model-based approach. They both present high-quality and streamable speech generation but differ in real-time factor (RTF) and latency. Specifically, flow-matching decoding can generate speech by chunks, presenting a lower RTF of 0.1 but a higher latency of 300ms. Instead, the multi-stream language model generates speech by frames in an autoregressive manner, presenting a higher RTF of 0.3 but a low latency of 150ms. In experiments on zero-shot voice cloning, the objective results validate FireRedTTS-1S as a high-quality foundation model with comparable intelligibility and speaker similarity over industrial baseline systems. Furthermore, the subjective score of FireRedTTS-1S highlights its impressive synthesis performance, achieving comparable quality to the ground-truth recordings. These results validate FireRedTTS-1S as a high-quality streaming foundation TTS system.",
    "metadata": {
      "arxiv_id": "2503.20499",
      "title": "FireRedTTS-1S: An Upgraded Streamable Foundation Text-to-Speech System",
      "summary": "In this work, we upgrade FireRedTTS to a new version, FireRedTTS-1S, a high-quality streaming foundation text-to-speech system. FireRedTTS-1S achieves streaming speech generation via two steps: text-to-semantic decoding and semantic-to-acoustic decoding. In text-to-semantic decoding, a semantic-aware speech tokenizer converts the speech signal into semantic tokens, which can be synthesized from the text via a language model in an auto-regressive manner. Meanwhile, the semantic-to-acoustic decoding module simultaneously translates generated semantic tokens into the speech signal in a streaming way. We implement two approaches to achieve this module: 1) a chunk-wise streamable flow-matching approach, and 2) a multi-stream language model-based approach. They both present high-quality and streamable speech generation but differ in real-time factor (RTF) and latency. Specifically, flow-matching decoding can generate speech by chunks, presenting a lower RTF of 0.1 but a higher latency of 300ms. Instead, the multi-stream language model generates speech by frames in an autoregressive manner, presenting a higher RTF of 0.3 but a low latency of 150ms. In experiments on zero-shot voice cloning, the objective results validate FireRedTTS-1S as a high-quality foundation model with comparable intelligibility and speaker similarity over industrial baseline systems. Furthermore, the subjective score of FireRedTTS-1S highlights its impressive synthesis performance, achieving comparable quality to the ground-truth recordings. These results validate FireRedTTS-1S as a high-quality streaming foundation TTS system.",
      "authors": [
        "Hao-Han Guo",
        "Yao Hu",
        "Fei-Yu Shen",
        "Xu Tang",
        "Yi-Chen Wu",
        "Feng-Long Xie",
        "Kun Xie"
      ],
      "published": "2025-03-26T12:39:06Z",
      "updated": "2025-05-26T11:34:20Z",
      "categories": [
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2503.20499v3",
      "landing_url": "https://arxiv.org/abs/2503.20499v3",
      "doi": "https://doi.org/10.48550/arXiv.2503.20499"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The abstract describes a semantic tokenizer that converts speech into discrete tokens, making discrete audio representations a core component of the system, which aligns with the inclusion criteria and does not fall under any exclusion criteria.",
      "evaluation": 5
    },
    "round-A_JuniorNano_reasoning": "The abstract describes a semantic tokenizer that converts speech into discrete tokens, making discrete audio representations a core component of the system, which aligns with the inclusion criteria and does not fall under any exclusion criteria.",
    "round-A_JuniorNano_evaluation": 5,
    "round-A_JuniorMini_output": {
      "reasoning": "The study explicitly proposes a semantic-aware speech tokenizer that converts speech signal into discrete semantic tokens, which is a core focus on discrete audio/speech tokenization for text-to-speech generation; it also provides systematic analysis and experimental validation related to these discrete tokens, meeting the inclusion criteria and none of the exclusion criteria.",
      "evaluation": 5
    },
    "round-A_JuniorMini_reasoning": "The study explicitly proposes a semantic-aware speech tokenizer that converts speech signal into discrete semantic tokens, which is a core focus on discrete audio/speech tokenization for text-to-speech generation; it also provides systematic analysis and experimental validation related to these discrete tokens, meeting the inclusion criteria and none of the exclusion criteria.",
    "round-A_JuniorMini_evaluation": 5,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "include (junior:5)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "VoiceCraft-Dub: Automated Video Dubbing with Neural Codec Language Models",
    "abstract": "We present VoiceCraft-Dub, a novel approach for automated video dubbing that synthesizes high-quality speech from text and facial cues. This task has broad applications in filmmaking, multimedia creation, and assisting voice-impaired individuals. Building on the success of Neural Codec Language Models (NCLMs) for speech synthesis, our method extends their capabilities by incorporating video features, ensuring that synthesized speech is time-synchronized and expressively aligned with facial movements while preserving natural prosody. To inject visual cues, we design adapters to align facial features with the NCLM token space and introduce audio-visual fusion layers to merge audio-visual information within the NCLM framework. Additionally, we curate CelebV-Dub, a new dataset of expressive, real-world videos specifically designed for automated video dubbing. Extensive experiments show that our model achieves high-quality, intelligible, and natural speech synthesis with accurate lip synchronization, outperforming existing methods in human perception and performing favorably in objective evaluations. We also adapt VoiceCraft-Dub for the video-to-speech task, demonstrating its versatility for various applications.",
    "metadata": {
      "arxiv_id": "2504.02386",
      "title": "VoiceCraft-Dub: Automated Video Dubbing with Neural Codec Language Models",
      "summary": "We present VoiceCraft-Dub, a novel approach for automated video dubbing that synthesizes high-quality speech from text and facial cues. This task has broad applications in filmmaking, multimedia creation, and assisting voice-impaired individuals. Building on the success of Neural Codec Language Models (NCLMs) for speech synthesis, our method extends their capabilities by incorporating video features, ensuring that synthesized speech is time-synchronized and expressively aligned with facial movements while preserving natural prosody. To inject visual cues, we design adapters to align facial features with the NCLM token space and introduce audio-visual fusion layers to merge audio-visual information within the NCLM framework. Additionally, we curate CelebV-Dub, a new dataset of expressive, real-world videos specifically designed for automated video dubbing. Extensive experiments show that our model achieves high-quality, intelligible, and natural speech synthesis with accurate lip synchronization, outperforming existing methods in human perception and performing favorably in objective evaluations. We also adapt VoiceCraft-Dub for the video-to-speech task, demonstrating its versatility for various applications.",
      "authors": [
        "Kim Sung-Bin",
        "Jeongsoo Choi",
        "Puyuan Peng",
        "Joon Son Chung",
        "Tae-Hyun Oh",
        "David Harwath"
      ],
      "published": "2025-04-03T08:24:47Z",
      "updated": "2025-04-03T08:24:47Z",
      "categories": [
        "cs.CV",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2504.02386v1",
      "landing_url": "https://arxiv.org/abs/2504.02386v1",
      "doi": "https://doi.org/10.48550/arXiv.2504.02386"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "While the work mentions a 'token space' and alignment within an NCLM framework, it does not explicitly analyze or evaluate discrete audio/speech tokenization as a core object of study, making inclusion uncertain.",
      "evaluation": 3
    },
    "round-A_JuniorNano_reasoning": "While the work mentions a 'token space' and alignment within an NCLM framework, it does not explicitly analyze or evaluate discrete audio/speech tokenization as a core object of study, making inclusion uncertain.",
    "round-A_JuniorNano_evaluation": 3,
    "round-A_JuniorMini_output": {
      "reasoning": "The study discusses VoiceCraft-Dub, which builds on Neural Codec Language Models (NCLMs) for speech synthesis by incorporating video features and designing adapters aligning facial features with the NCLM token space. Given that NCLMs inherently rely on discrete audio/speech tokenization to convert continuous audio signals into sequences of discrete tokens for speech synthesis, this work clearly engages with discrete audio tokenization as a core object of study. Additionally, the paper goes beyond engineering reports by providing methodological contributions and experimental validation for discrete audio tokenization in the context of video dubbing. Therefore, it meets the inclusion criteria and does not fall under any exclusion criteria.",
      "evaluation": 5
    },
    "round-A_JuniorMini_reasoning": "The study discusses VoiceCraft-Dub, which builds on Neural Codec Language Models (NCLMs) for speech synthesis by incorporating video features and designing adapters aligning facial features with the NCLM token space. Given that NCLMs inherently rely on discrete audio/speech tokenization to convert continuous audio signals into sequences of discrete tokens for speech synthesis, this work clearly engages with discrete audio tokenization as a core object of study. Additionally, the paper goes beyond engineering reports by providing methodological contributions and experimental validation for discrete audio tokenization in the context of video dubbing. Therefore, it meets the inclusion criteria and does not fall under any exclusion criteria.",
    "round-A_JuniorMini_evaluation": 5,
    "round-B_SeniorLead_output": {
      "reasoning": "Both junior reviewers noted the paper uses Neural Codec Language Models and references a ‘‘token space’’ and alignment with facial features; however, the abstract only describes extending existing NCLMs for video-guided dubbing and does not explicitly propose, analyze, or evaluate discrete audio/speech tokenization itself as a core object of study (no mention of a new quantizer, token design, or systematic analysis of discrete tokens). Given the inclusion criterion requires explicit treatment of discrete audio tokenization and the abstract reads like an application/extension of existing NCLMs rather than a tokenization-focused contribution, I recommend exclusion (leaning toward exclude rather than uncertain).",
      "evaluation": 2
    },
    "round-B_SeniorLead_reasoning": "Both junior reviewers noted the paper uses Neural Codec Language Models and references a ‘‘token space’’ and alignment with facial features; however, the abstract only describes extending existing NCLMs for video-guided dubbing and does not explicitly propose, analyze, or evaluate discrete audio/speech tokenization itself as a core object of study (no mention of a new quantizer, token design, or systematic analysis of discrete tokens). Given the inclusion criterion requires explicit treatment of discrete audio tokenization and the abstract reads like an application/extension of existing NCLMs rather than a tokenization-focused contribution, I recommend exclusion (leaning toward exclude rather than uncertain).",
    "round-B_SeniorLead_evaluation": 2,
    "final_verdict": "exclude (senior:2)",
    "review_skipped": false,
    "discard_reason": "exclude (senior:2)"
  },
  {
    "title": "Variational Self-Supervised Learning",
    "abstract": "We present Variational Self-Supervised Learning (VSSL), a novel framework that combines variational inference with self-supervised learning to enable efficient, decoder-free representation learning. Unlike traditional VAEs that rely on input reconstruction via a decoder, VSSL symmetrically couples two encoders with Gaussian outputs. A momentum-updated teacher network defines a dynamic, data-dependent prior, while the student encoder produces an approximate posterior from augmented views. The reconstruction term in the ELBO is replaced with a cross-view denoising objective, preserving the analytical tractability of Gaussian KL divergence. We further introduce cosine-based formulations of KL and log-likelihood terms to enhance semantic alignment in high-dimensional latent spaces. Experiments on CIFAR-10, CIFAR-100, and ImageNet-100 show that VSSL achieves competitive or superior performance to leading self-supervised methods, including BYOL and MoCo V3. VSSL offers a scalable, probabilistically grounded approach to learning transferable representations without generative reconstruction, bridging the gap between variational modeling and modern self-supervised techniques.",
    "metadata": {
      "arxiv_id": "2504.04318",
      "title": "Variational Self-Supervised Learning",
      "summary": "We present Variational Self-Supervised Learning (VSSL), a novel framework that combines variational inference with self-supervised learning to enable efficient, decoder-free representation learning. Unlike traditional VAEs that rely on input reconstruction via a decoder, VSSL symmetrically couples two encoders with Gaussian outputs. A momentum-updated teacher network defines a dynamic, data-dependent prior, while the student encoder produces an approximate posterior from augmented views. The reconstruction term in the ELBO is replaced with a cross-view denoising objective, preserving the analytical tractability of Gaussian KL divergence. We further introduce cosine-based formulations of KL and log-likelihood terms to enhance semantic alignment in high-dimensional latent spaces. Experiments on CIFAR-10, CIFAR-100, and ImageNet-100 show that VSSL achieves competitive or superior performance to leading self-supervised methods, including BYOL and MoCo V3. VSSL offers a scalable, probabilistically grounded approach to learning transferable representations without generative reconstruction, bridging the gap between variational modeling and modern self-supervised techniques.",
      "authors": [
        "Mehmet Can Yavuz",
        "Berrin Yanikoglu"
      ],
      "published": "2025-04-06T01:28:50Z",
      "updated": "2025-05-01T16:21:49Z",
      "categories": [
        "cs.LG",
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2504.04318v3",
      "landing_url": "https://arxiv.org/abs/2504.04318v3",
      "doi": "https://doi.org/10.48550/arXiv.2504.04318"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item is about Variational Self-Supervised Learning for visual representations and uses Gaussian encoders; there is no discrete audio/speech tokenization or analysis of discrete audio tokens, thus it fails the inclusion criteria and aligns with exclusion criteria for tokenized discrete audio representations.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item is about Variational Self-Supervised Learning for visual representations and uses Gaussian encoders; there is no discrete audio/speech tokenization or analysis of discrete audio tokens, thus it fails the inclusion criteria and aligns with exclusion criteria for tokenized discrete audio representations.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on a novel variational self-supervised learning framework primarily applied to image datasets (CIFAR-10, CIFAR-100, and ImageNet-100) and does not mention any discrete audio/speech tokenization, quantization, or audio-specific representation; thus, it does not meet the inclusion criteria related to discrete audio tokenization and instead aligns with exclusion criteria because it does not involve discrete audio token analysis or neural discrete token representations for audio.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on a novel variational self-supervised learning framework primarily applied to image datasets (CIFAR-10, CIFAR-100, and ImageNet-100) and does not mention any discrete audio/speech tokenization, quantization, or audio-specific representation; thus, it does not meet the inclusion criteria related to discrete audio tokenization and instead aligns with exclusion criteria because it does not involve discrete audio token analysis or neural discrete token representations for audio.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "M$^2$IV: Towards Efficient and Fine-grained Multimodal In-Context Learning via Representation Engineering",
    "abstract": "Multimodal in-context learning (ICL) equips Large Vision-language Models (LVLMs) with the ability to adapt to new tasks via multiple user-provided demonstrations, without requiring any model parameter updates. However, its effectiveness is constrained by the token-intensive nature of multimodal inputs and the complexity of cross-modal few-shot reasoning, which together hinder LVLMs from extracting useful patterns from demonstrations. To address these challenges, we propose \\textbf{M$^2$IV}, a novel representation engineering approach that replaces explicit token-level demonstrations with a set of learnable Multimodal In-context Vectors directly injected into the residual streams of LVLMs. By analyzing the distinct roles of multi-head attention (MHA) and multi-layer perceptrons (MLP) in the ICL process, we design a training strategy that enables M$^2$IV to perform fine-grained semantic distillation and robust cross-modal representation learning. M$^2$IV not only improves performance across diverse tasks and LVLMs but also significantly reduces token overhead, enabling graceful scaling to many-shot scenarios. To further enhance usability, we introduce \\textbf{VLibrary}, a repository that stores trained M$^2$IVs for flexible retrieval and injection. With VLibrary, users can steer pre-trained LVLMs in a customized manner that meets diverse requirements. Extensive experiments demonstrate that M$^2$IV consistently outperforms vanilla ICL and prior representation engineering baselines, achieving an average accuracy gain of 3.74\\% with substantial improvements in overall efficiency.",
    "metadata": {
      "arxiv_id": "2504.04633",
      "title": "M$^2$IV: Towards Efficient and Fine-grained Multimodal In-Context Learning via Representation Engineering",
      "summary": "Multimodal in-context learning (ICL) equips Large Vision-language Models (LVLMs) with the ability to adapt to new tasks via multiple user-provided demonstrations, without requiring any model parameter updates. However, its effectiveness is constrained by the token-intensive nature of multimodal inputs and the complexity of cross-modal few-shot reasoning, which together hinder LVLMs from extracting useful patterns from demonstrations. To address these challenges, we propose \\textbf{M$^2$IV}, a novel representation engineering approach that replaces explicit token-level demonstrations with a set of learnable Multimodal In-context Vectors directly injected into the residual streams of LVLMs. By analyzing the distinct roles of multi-head attention (MHA) and multi-layer perceptrons (MLP) in the ICL process, we design a training strategy that enables M$^2$IV to perform fine-grained semantic distillation and robust cross-modal representation learning. M$^2$IV not only improves performance across diverse tasks and LVLMs but also significantly reduces token overhead, enabling graceful scaling to many-shot scenarios. To further enhance usability, we introduce \\textbf{VLibrary}, a repository that stores trained M$^2$IVs for flexible retrieval and injection. With VLibrary, users can steer pre-trained LVLMs in a customized manner that meets diverse requirements. Extensive experiments demonstrate that M$^2$IV consistently outperforms vanilla ICL and prior representation engineering baselines, achieving an average accuracy gain of 3.74\\% with substantial improvements in overall efficiency.",
      "authors": [
        "Yanshu Li",
        "Yi Cao",
        "Hongyang He",
        "Qisen Cheng",
        "Xiang Fu",
        "Xi Xiao",
        "Tianyang Wang",
        "Ruixiang Tang"
      ],
      "published": "2025-04-06T22:02:21Z",
      "updated": "2025-08-26T10:19:05Z",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2504.04633v3",
      "landing_url": "https://arxiv.org/abs/2504.04633v3",
      "doi": "https://doi.org/10.48550/arXiv.2504.04633"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The article focuses on representation engineering for multimodal in-context learning in vision-language models and introduces learnable multimodal in-context vectors, with no discussion of discrete audio/speech tokenization or speech token quantization; it does not address the core requirement of discrete audio tokenization.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The article focuses on representation engineering for multimodal in-context learning in vision-language models and introduces learnable multimodal in-context vectors, with no discussion of discrete audio/speech tokenization or speech token quantization; it does not address the core requirement of discrete audio tokenization.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on multimodal in-context learning and representation engineering for large vision-language models, specifically addressing token-level demonstrations and semantic distillation, but does not mention discrete audio/speech tokenization or conversion of continuous audio signals into discrete token sequences. Audio or speech tokenization is not a core object of this study; it mainly deals with vision-language models and general token representation without explicit discrete audio token design or analysis. Therefore, it does not fulfill the inclusion criteria and fits the exclusion criteria related to lack of discrete audio tokenization methodology.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on multimodal in-context learning and representation engineering for large vision-language models, specifically addressing token-level demonstrations and semantic distillation, but does not mention discrete audio/speech tokenization or conversion of continuous audio signals into discrete token sequences. Audio or speech tokenization is not a core object of this study; it mainly deals with vision-language models and general token representation without explicit discrete audio token design or analysis. Therefore, it does not fulfill the inclusion criteria and fits the exclusion criteria related to lack of discrete audio tokenization methodology.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "TASTE: Text-Aligned Speech Tokenization and Embedding for Spoken Language Modeling",
    "abstract": "Recent efforts target spoken language models (SLMs) that not only listen but also speak for more natural human-LLM interaction. Joint speech-text modeling is a promising direction to achieve this. However, the effectiveness of recent speech tokens for joint modeling remains underexplored. To address this, we introduce Text-Aligned Speech Tokenization and Embedding (TASTE), a method that directly addresses the modality gap by aligning speech token with the corresponding text transcription during the tokenization stage. We propose a method that can achieve this through a attention-based aggregation mechanism and with speech reconstruction as the training objective. We conduct extensive experiments and show that TASTE can preserve essential paralinguistic information while dramatically reducing the token sequence length. With TASTE, we perform straightforward joint spoken language modeling by using Low-Rank Adaptation on the pre-trained text LLM. Experimental results show that TASTE-based SLMs perform comparable to previous work on SALMON and StoryCloze; while significantly outperform other pre-trained SLMs on speech continuation across subjective and objective evaluations. To our knowledge, TASTE is the first end-to-end approach that utilizes a reconstruction objective to automatically learn a text-aligned speech tokenization and embedding suitable for spoken language modeling. Our demo, code, and model are available at https://mtkresearch.github.io/TASTE-SpokenLM.github.io.",
    "metadata": {
      "arxiv_id": "2504.07053",
      "title": "TASTE: Text-Aligned Speech Tokenization and Embedding for Spoken Language Modeling",
      "summary": "Recent efforts target spoken language models (SLMs) that not only listen but also speak for more natural human-LLM interaction. Joint speech-text modeling is a promising direction to achieve this. However, the effectiveness of recent speech tokens for joint modeling remains underexplored. To address this, we introduce Text-Aligned Speech Tokenization and Embedding (TASTE), a method that directly addresses the modality gap by aligning speech token with the corresponding text transcription during the tokenization stage. We propose a method that can achieve this through a attention-based aggregation mechanism and with speech reconstruction as the training objective. We conduct extensive experiments and show that TASTE can preserve essential paralinguistic information while dramatically reducing the token sequence length. With TASTE, we perform straightforward joint spoken language modeling by using Low-Rank Adaptation on the pre-trained text LLM. Experimental results show that TASTE-based SLMs perform comparable to previous work on SALMON and StoryCloze; while significantly outperform other pre-trained SLMs on speech continuation across subjective and objective evaluations. To our knowledge, TASTE is the first end-to-end approach that utilizes a reconstruction objective to automatically learn a text-aligned speech tokenization and embedding suitable for spoken language modeling. Our demo, code, and model are available at https://mtkresearch.github.io/TASTE-SpokenLM.github.io.",
      "authors": [
        "Liang-Hsuan Tseng",
        "Yi-Chang Chen",
        "Kuan-Yi Lee",
        "Da-Shan Shiu",
        "Hung-yi Lee"
      ],
      "published": "2025-04-09T17:14:33Z",
      "updated": "2025-05-22T14:49:03Z",
      "categories": [
        "cs.CL",
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2504.07053v2",
      "landing_url": "https://arxiv.org/abs/2504.07053v2",
      "doi": "https://doi.org/10.48550/arXiv.2504.07053"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The work explicitly proposes a discrete speech tokenization system that maps continuous speech to a finite vocabulary with a reconstruction objective, addressing tokenization as a core component for spoken language modeling.",
      "evaluation": 5
    },
    "round-A_JuniorNano_reasoning": "The work explicitly proposes a discrete speech tokenization system that maps continuous speech to a finite vocabulary with a reconstruction objective, addressing tokenization as a core component for spoken language modeling.",
    "round-A_JuniorNano_evaluation": 5,
    "round-A_JuniorMini_output": {
      "reasoning": "The study explicitly proposes Text-Aligned Speech Tokenization and Embedding (TASTE), a method that aligns speech tokens with text transcriptions during tokenization, showing a clear focus on discrete audio tokenization for spoken language modeling with experimental validation and code availability. Therefore, it meets all inclusion criteria and none of the exclusion criteria.",
      "evaluation": 5
    },
    "round-A_JuniorMini_reasoning": "The study explicitly proposes Text-Aligned Speech Tokenization and Embedding (TASTE), a method that aligns speech tokens with text transcriptions during tokenization, showing a clear focus on discrete audio tokenization for spoken language modeling with experimental validation and code availability. Therefore, it meets all inclusion criteria and none of the exclusion criteria.",
    "round-A_JuniorMini_evaluation": 5,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "include (junior:5)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Evolutionary Machine Learning meets Self-Supervised Learning: a comprehensive survey",
    "abstract": "The number of studies that combine Evolutionary Machine Learning and self-supervised learning has been growing steadily in recent years. Evolutionary Machine Learning has been shown to help automate the design of machine learning algorithms and to lead to more reliable solutions. Self-supervised learning, on the other hand, has produced good results in learning useful features when labelled data is limited. This suggests that the combination of these two areas can help both in shaping evolutionary processes and in automating the design of deep neural networks, while also reducing the need for labelled data. Still, there are no detailed reviews that explain how Evolutionary Machine Learning and self-supervised learning can be used together. To help with this, we provide an overview of studies that bring these areas together. Based on this growing interest and the range of existing works, we suggest a new sub-area of research, which we call Evolutionary Self-Supervised Learning and introduce a taxonomy for it. Finally, we point out some of the main challenges and suggest directions for future research to help Evolutionary Self-Supervised Learning grow and mature as a field.",
    "metadata": {
      "arxiv_id": "2504.07213",
      "title": "Evolutionary Machine Learning meets Self-Supervised Learning: a comprehensive survey",
      "summary": "The number of studies that combine Evolutionary Machine Learning and self-supervised learning has been growing steadily in recent years. Evolutionary Machine Learning has been shown to help automate the design of machine learning algorithms and to lead to more reliable solutions. Self-supervised learning, on the other hand, has produced good results in learning useful features when labelled data is limited. This suggests that the combination of these two areas can help both in shaping evolutionary processes and in automating the design of deep neural networks, while also reducing the need for labelled data. Still, there are no detailed reviews that explain how Evolutionary Machine Learning and self-supervised learning can be used together. To help with this, we provide an overview of studies that bring these areas together. Based on this growing interest and the range of existing works, we suggest a new sub-area of research, which we call Evolutionary Self-Supervised Learning and introduce a taxonomy for it. Finally, we point out some of the main challenges and suggest directions for future research to help Evolutionary Self-Supervised Learning grow and mature as a field.",
      "authors": [
        "Adriano Vinhas",
        "João Correia",
        "Penousal Machado"
      ],
      "published": "2025-04-09T18:39:41Z",
      "updated": "2025-11-04T10:48:22Z",
      "categories": [
        "cs.NE",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2504.07213v2",
      "landing_url": "https://arxiv.org/abs/2504.07213v2",
      "doi": "https://doi.org/10.48550/arXiv.2504.07213"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The abstract discusses Evolutionary Machine Learning and Self-Supervised Learning but does not address discrete audio/speech tokenization or any discrete token representations for speech tasks, so it fails the inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The abstract discusses Evolutionary Machine Learning and Self-Supervised Learning but does not address discrete audio/speech tokenization or any discrete token representations for speech tasks, so it fails the inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study is a comprehensive survey about combining Evolutionary Machine Learning with self-supervised learning, focusing on algorithm design and feature learning but does not mention discrete audio/speech tokenization or evaluation of discrete audio tokens, which is the core inclusion criterion.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study is a comprehensive survey about combining Evolutionary Machine Learning with self-supervised learning, focusing on algorithm design and feature learning but does not mention discrete audio/speech tokenization or evaluation of discrete audio tokens, which is the core inclusion criterion.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "RadarLLM: Empowering Large Language Models to Understand Human Motion from Millimeter-Wave Point Cloud Sequence",
    "abstract": "Millimeter-wave radar offers a privacy-preserving and environment-robust alternative to vision-based sensing, enabling human motion analysis in challenging conditions such as low light, occlusions, rain, or smoke. However, its sparse point clouds pose significant challenges for semantic understanding. We present RadarLLM, the first framework that leverages large language models (LLMs) for human motion understanding from radar signals. RadarLLM introduces two key innovations: (1) a motion-guided radar tokenizer based on our Aggregate VQ-VAE architecture, integrating deformable body templates and masked trajectory modeling to convert spatial-temporal radar sequences into compact semantic tokens; and (2) a radar-aware language model that establishes cross-modal alignment between radar and text in a shared embedding space. To overcome the scarcity of paired radar-text data, we generate a realistic radar-text dataset from motion-text datasets with a physics-aware synthesis pipeline. Extensive experiments on both synthetic and real-world benchmarks show that RadarLLM achieves state-of-the-art performance, enabling robust and interpretable motion understanding under privacy and visibility constraints, even in adverse environments. This paper has been accepted for presentation at AAAI 2026. This is an extended version with supplementary materials.",
    "metadata": {
      "arxiv_id": "2504.09862",
      "title": "RadarLLM: Empowering Large Language Models to Understand Human Motion from Millimeter-Wave Point Cloud Sequence",
      "summary": "Millimeter-wave radar offers a privacy-preserving and environment-robust alternative to vision-based sensing, enabling human motion analysis in challenging conditions such as low light, occlusions, rain, or smoke. However, its sparse point clouds pose significant challenges for semantic understanding. We present RadarLLM, the first framework that leverages large language models (LLMs) for human motion understanding from radar signals. RadarLLM introduces two key innovations: (1) a motion-guided radar tokenizer based on our Aggregate VQ-VAE architecture, integrating deformable body templates and masked trajectory modeling to convert spatial-temporal radar sequences into compact semantic tokens; and (2) a radar-aware language model that establishes cross-modal alignment between radar and text in a shared embedding space. To overcome the scarcity of paired radar-text data, we generate a realistic radar-text dataset from motion-text datasets with a physics-aware synthesis pipeline. Extensive experiments on both synthetic and real-world benchmarks show that RadarLLM achieves state-of-the-art performance, enabling robust and interpretable motion understanding under privacy and visibility constraints, even in adverse environments. This paper has been accepted for presentation at AAAI 2026. This is an extended version with supplementary materials.",
      "authors": [
        "Zengyuan Lai",
        "Jiarui Yang",
        "Songpengcheng Xia",
        "Lizhou Lin",
        "Lan Sun",
        "Renwen Wang",
        "Jianran Liu",
        "Qi Wu",
        "Ling Pei"
      ],
      "published": "2025-04-14T04:18:25Z",
      "updated": "2025-11-17T04:20:12Z",
      "categories": [
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2504.09862v2",
      "landing_url": "https://arxiv.org/abs/2504.09862v2",
      "doi": "https://doi.org/10.48550/arXiv.2504.09862"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The study tokenizes millimeter-wave radar data into discrete semantic tokens and aligns them with text via an LLM, but the discrete tokenization target is radar signals, not audio/speech. The inclusion criteria require explicit discrete audio/speech tokenization as the core object of study, so this item does not meet the criterion and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The study tokenizes millimeter-wave radar data into discrete semantic tokens and aligns them with text via an LLM, but the discrete tokenization target is radar signals, not audio/speech. The inclusion criteria require explicit discrete audio/speech tokenization as the core object of study, so this item does not meet the criterion and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on converting millimeter-wave radar signals into discrete semantic tokens for understanding human motion, which is not audio or speech signal tokenization. Although it involves a discrete tokenization process, it pertains to radar signals rather than audio signals, and thus it does not meet the inclusion criteria centered specifically on discrete audio/speech tokenization. Furthermore, the exclusion criteria dismiss works where the core tokenization is not related to audio signals. Therefore, it should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on converting millimeter-wave radar signals into discrete semantic tokens for understanding human motion, which is not audio or speech signal tokenization. Although it involves a discrete tokenization process, it pertains to radar signals rather than audio signals, and thus it does not meet the inclusion criteria centered specifically on discrete audio/speech tokenization. Furthermore, the exclusion criteria dismiss works where the core tokenization is not related to audio signals. Therefore, it should be excluded.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Pseudo-Autoregressive Neural Codec Language Models for Efficient Zero-Shot Text-to-Speech Synthesis",
    "abstract": "Recent zero-shot text-to-speech (TTS) systems face a common dilemma: autoregressive (AR) models suffer from slow generation and lack duration controllability, while non-autoregressive (NAR) models lack temporal modeling and typically require complex designs. In this paper, we introduce a novel pseudo-autoregressive (PAR) codec language modeling approach that unifies AR and NAR modeling. Combining explicit temporal modeling from AR with parallel generation from NAR, PAR generates dynamic-length spans at fixed time steps. Building on PAR, we propose PALLE, a two-stage TTS system that leverages PAR for initial generation followed by NAR refinement. In the first stage, PAR progressively generates speech tokens along the time dimension, with each step predicting all positions in parallel but only retaining the left-most span. In the second stage, low-confidence tokens are iteratively refined in parallel, leveraging the global contextual information. Experiments demonstrate that PALLE, trained on LibriTTS, outperforms state-of-the-art systems trained on large-scale data, including F5-TTS, E2-TTS, and MaskGCT, on the LibriSpeech test-clean set in terms of speech quality, speaker similarity, and intelligibility, while achieving up to ten times faster inference speed. Audio samples are available at https://microsoft.com/research/project/vall-e-x/palle.",
    "metadata": {
      "arxiv_id": "2504.10352",
      "title": "Pseudo-Autoregressive Neural Codec Language Models for Efficient Zero-Shot Text-to-Speech Synthesis",
      "summary": "Recent zero-shot text-to-speech (TTS) systems face a common dilemma: autoregressive (AR) models suffer from slow generation and lack duration controllability, while non-autoregressive (NAR) models lack temporal modeling and typically require complex designs. In this paper, we introduce a novel pseudo-autoregressive (PAR) codec language modeling approach that unifies AR and NAR modeling. Combining explicit temporal modeling from AR with parallel generation from NAR, PAR generates dynamic-length spans at fixed time steps. Building on PAR, we propose PALLE, a two-stage TTS system that leverages PAR for initial generation followed by NAR refinement. In the first stage, PAR progressively generates speech tokens along the time dimension, with each step predicting all positions in parallel but only retaining the left-most span. In the second stage, low-confidence tokens are iteratively refined in parallel, leveraging the global contextual information. Experiments demonstrate that PALLE, trained on LibriTTS, outperforms state-of-the-art systems trained on large-scale data, including F5-TTS, E2-TTS, and MaskGCT, on the LibriSpeech test-clean set in terms of speech quality, speaker similarity, and intelligibility, while achieving up to ten times faster inference speed. Audio samples are available at https://microsoft.com/research/project/vall-e-x/palle.",
      "authors": [
        "Yifan Yang",
        "Shujie Liu",
        "Jinyu Li",
        "Yuxuan Hu",
        "Haibin Wu",
        "Hui Wang",
        "Jianwei Yu",
        "Lingwei Meng",
        "Haiyang Sun",
        "Yanqing Liu",
        "Yan Lu",
        "Kai Yu",
        "Xie Chen"
      ],
      "published": "2025-04-14T16:03:21Z",
      "updated": "2025-08-05T15:33:39Z",
      "categories": [
        "eess.AS",
        "cs.CL"
      ],
      "pdf_url": "https://arxiv.org/pdf/2504.10352v3",
      "landing_url": "https://arxiv.org/abs/2504.10352v3",
      "doi": "https://doi.org/10.48550/arXiv.2504.10352"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The abstract centers on a neural codec that uses discrete speech tokens and token-based generation for TTS, i.e., explicit discrete audio tokenization as the core object of study, with methodological evaluation, meeting all inclusion criteria and none of the exclusions.",
      "evaluation": 5
    },
    "round-A_JuniorNano_reasoning": "The abstract centers on a neural codec that uses discrete speech tokens and token-based generation for TTS, i.e., explicit discrete audio tokenization as the core object of study, with methodological evaluation, meeting all inclusion criteria and none of the exclusions.",
    "round-A_JuniorNano_evaluation": 5,
    "round-A_JuniorMini_output": {
      "reasoning": "The study proposes a pseudo-autoregressive neural codec language model for zero-shot text-to-speech synthesis, which inherently involves discrete audio tokens for speech generation and modeling, aligning with the inclusion criteria focusing on discrete audio/speech tokenization rather than continuous features; it also presents methodological contributions and experimental validation exceeding mere engineering reports, making it a relevant study for inclusion.",
      "evaluation": 5
    },
    "round-A_JuniorMini_reasoning": "The study proposes a pseudo-autoregressive neural codec language model for zero-shot text-to-speech synthesis, which inherently involves discrete audio tokens for speech generation and modeling, aligning with the inclusion criteria focusing on discrete audio/speech tokenization rather than continuous features; it also presents methodological contributions and experimental validation exceeding mere engineering reports, making it a relevant study for inclusion.",
    "round-A_JuniorMini_evaluation": 5,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "include (junior:5)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "H3AE: High Compression, High Speed, and High Quality AutoEncoder for Video Diffusion Models",
    "abstract": "Autoencoder (AE) is the key to the success of latent diffusion models for image and video generation, reducing the denoising resolution and improving efficiency. However, the power of AE has long been underexplored in terms of network design, compression ratio, and training strategy. In this work, we systematically examine the architecture design choices and optimize the computation distribution to obtain a series of efficient and high-compression video AEs that can decode in real time even on mobile devices. We also propose an omni-training objective to unify the design of plain Autoencoder and image-conditioned I2V VAE, achieving multifunctionality in a single VAE network but with enhanced quality. In addition, we propose a novel latent consistency loss that provides stable improvements in reconstruction quality. Latent consistency loss outperforms prior auxiliary losses including LPIPS, GAN and DWT in terms of both quality improvements and simplicity. H3AE achieves ultra-high compression ratios and real-time decoding speed on GPU and mobile, and outperforms prior arts in terms of reconstruction metrics by a large margin. We finally validate our AE by training a DiT on its latent space and demonstrate fast, high-quality text-to-video generation capability.",
    "metadata": {
      "arxiv_id": "2504.10567",
      "title": "H3AE: High Compression, High Speed, and High Quality AutoEncoder for Video Diffusion Models",
      "summary": "Autoencoder (AE) is the key to the success of latent diffusion models for image and video generation, reducing the denoising resolution and improving efficiency. However, the power of AE has long been underexplored in terms of network design, compression ratio, and training strategy. In this work, we systematically examine the architecture design choices and optimize the computation distribution to obtain a series of efficient and high-compression video AEs that can decode in real time even on mobile devices. We also propose an omni-training objective to unify the design of plain Autoencoder and image-conditioned I2V VAE, achieving multifunctionality in a single VAE network but with enhanced quality. In addition, we propose a novel latent consistency loss that provides stable improvements in reconstruction quality. Latent consistency loss outperforms prior auxiliary losses including LPIPS, GAN and DWT in terms of both quality improvements and simplicity. H3AE achieves ultra-high compression ratios and real-time decoding speed on GPU and mobile, and outperforms prior arts in terms of reconstruction metrics by a large margin. We finally validate our AE by training a DiT on its latent space and demonstrate fast, high-quality text-to-video generation capability.",
      "authors": [
        "Yushu Wu",
        "Yanyu Li",
        "Ivan Skorokhodov",
        "Anil Kag",
        "Willi Menapace",
        "Sharath Girish",
        "Aliaksandr Siarohin",
        "Yanzhi Wang",
        "Sergey Tulyakov"
      ],
      "published": "2025-04-14T17:59:06Z",
      "updated": "2025-10-01T03:41:01Z",
      "categories": [
        "cs.CV",
        "eess.IV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2504.10567v2",
      "landing_url": "https://arxiv.org/abs/2504.10567v2",
      "doi": "https://doi.org/10.48550/arXiv.2504.10567"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The work centers on video autoencoders and latent diffusion architectures without any discrete audio/speech tokenization or token-based analysis, so it does not meet the inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The work centers on video autoencoders and latent diffusion architectures without any discrete audio/speech tokenization or token-based analysis, so it does not meet the inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on design and optimization of video autoencoders and latent diffusion models for video generation, without any mention or analysis of discrete audio/speech tokenization or quantization; hence, it does not meet the inclusion criteria related to audio tokenization.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on design and optimization of video autoencoders and latent diffusion models for video generation, without any mention or analysis of discrete audio/speech tokenization or quantization; hence, it does not meet the inclusion criteria related to audio tokenization.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "GOAT-TTS: Expressive and Realistic Speech Generation via A Dual-Branch LLM",
    "abstract": "While large language models (LLMs) have revolutionized text-to-speech (TTS) synthesis through discrete tokenization paradigms, current architectures exhibit fundamental tensions between three critical dimensions: 1) irreversible loss of acoustic characteristics caused by quantization of speech prompts; 2) stringent dependence on precisely aligned prompt speech-text pairs that limit real-world deployment; and 3) catastrophic forgetting of the LLM's native text comprehension during optimization for speech token generation. To address these challenges, we propose an LLM-based text-to-speech Generation approach Optimized via a novel dual-branch ArchiTecture (GOAT-TTS). Our framework introduces two key innovations: (1) The modality-alignment branch combines a speech encoder and projector to capture continuous acoustic embeddings, enabling bidirectional correlation between paralinguistic features (language, timbre, emotion) and semantic text representations without transcript dependency; (2) The speech-generation branch employs modular fine-tuning on top-k layers of an LLM for speech token prediction while freezing the bottom-n layers to preserve foundational linguistic knowledge. Moreover, multi-token prediction is introduced to support real-time streaming TTS synthesis. Experimental results demonstrate that our GOAT-TTS achieves performance comparable to state-of-the-art TTS models while validating the efficacy of synthesized dialect speech data.",
    "metadata": {
      "arxiv_id": "2504.12339",
      "title": "GOAT-TTS: Expressive and Realistic Speech Generation via A Dual-Branch LLM",
      "summary": "While large language models (LLMs) have revolutionized text-to-speech (TTS) synthesis through discrete tokenization paradigms, current architectures exhibit fundamental tensions between three critical dimensions: 1) irreversible loss of acoustic characteristics caused by quantization of speech prompts; 2) stringent dependence on precisely aligned prompt speech-text pairs that limit real-world deployment; and 3) catastrophic forgetting of the LLM's native text comprehension during optimization for speech token generation. To address these challenges, we propose an LLM-based text-to-speech Generation approach Optimized via a novel dual-branch ArchiTecture (GOAT-TTS). Our framework introduces two key innovations: (1) The modality-alignment branch combines a speech encoder and projector to capture continuous acoustic embeddings, enabling bidirectional correlation between paralinguistic features (language, timbre, emotion) and semantic text representations without transcript dependency; (2) The speech-generation branch employs modular fine-tuning on top-k layers of an LLM for speech token prediction while freezing the bottom-n layers to preserve foundational linguistic knowledge. Moreover, multi-token prediction is introduced to support real-time streaming TTS synthesis. Experimental results demonstrate that our GOAT-TTS achieves performance comparable to state-of-the-art TTS models while validating the efficacy of synthesized dialect speech data.",
      "authors": [
        "Yaodong Song",
        "Hongjie Chen",
        "Jie Lian",
        "Yuxin Zhang",
        "Guangmin Xia",
        "Zehan Li",
        "Genliang Zhao",
        "Jian Kang",
        "Jie Li",
        "Yongxiang Li",
        "Xuelong Li"
      ],
      "published": "2025-04-15T01:44:56Z",
      "updated": "2025-05-28T14:24:12Z",
      "categories": [
        "cs.CL",
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2504.12339v2",
      "landing_url": "https://arxiv.org/abs/2504.12339v2",
      "doi": "https://doi.org/10.48550/arXiv.2504.12339"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The abstract explicitly discusses discrete speech tokens through speech token generation and discusses quantization issues of speech prompts, indicating a focus on discrete audio tokenization within an LLM-based TTS framework, which aligns with the inclusion criterion, though it is primarily architectural rather than a deep tokenization analysis.",
      "evaluation": 4
    },
    "round-A_JuniorNano_reasoning": "The abstract explicitly discusses discrete speech tokens through speech token generation and discusses quantization issues of speech prompts, indicating a focus on discrete audio tokenization within an LLM-based TTS framework, which aligns with the inclusion criterion, though it is primarily architectural rather than a deep tokenization analysis.",
    "round-A_JuniorNano_evaluation": 4,
    "round-A_JuniorMini_output": {
      "reasoning": "The study explicitly addresses discrete tokenization in TTS by proposing a dual-branch LLM architecture that involves speech token prediction and continuous acoustic embeddings, targeting discrete audio token representations for speech generation tasks rather than solely relying on continuous audio features or traditional codecs; thus, it meets the inclusion criteria and avoids the exclusion criteria.",
      "evaluation": 5
    },
    "round-A_JuniorMini_reasoning": "The study explicitly addresses discrete tokenization in TTS by proposing a dual-branch LLM architecture that involves speech token prediction and continuous acoustic embeddings, targeting discrete audio token representations for speech generation tasks rather than solely relying on continuous audio features or traditional codecs; thus, it meets the inclusion criteria and avoids the exclusion criteria.",
    "round-A_JuniorMini_evaluation": 5,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "include (junior:4)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Hierarchical Vector Quantized Graph Autoencoder with Annealing-Based Code Selection",
    "abstract": "Graph self-supervised learning has gained significant attention recently. However, many existing approaches heavily depend on perturbations, and inappropriate perturbations may corrupt the graph's inherent information. The Vector Quantized Variational Autoencoder (VQ-VAE) is a powerful autoencoder extensively used in fields such as computer vision; however, its application to graph data remains underexplored. In this paper, we provide an empirical analysis of vector quantization in the context of graph autoencoders, demonstrating its significant enhancement of the model's capacity to capture graph topology. Furthermore, we identify two key challenges associated with vector quantization when applying in graph data: codebook underutilization and codebook space sparsity. For the first challenge, we propose an annealing-based encoding strategy that promotes broad code utilization in the early stages of training, gradually shifting focus toward the most effective codes as training progresses. For the second challenge, we introduce a hierarchical two-layer codebook that captures relationships between embeddings through clustering. The second layer codebook links similar codes, encouraging the model to learn closer embeddings for nodes with similar features and structural topology in the graph. Our proposed model outperforms 16 representative baseline methods in self-supervised link prediction and node classification tasks across multiple datasets.",
    "metadata": {
      "arxiv_id": "2504.12715",
      "title": "Hierarchical Vector Quantized Graph Autoencoder with Annealing-Based Code Selection",
      "summary": "Graph self-supervised learning has gained significant attention recently. However, many existing approaches heavily depend on perturbations, and inappropriate perturbations may corrupt the graph's inherent information. The Vector Quantized Variational Autoencoder (VQ-VAE) is a powerful autoencoder extensively used in fields such as computer vision; however, its application to graph data remains underexplored. In this paper, we provide an empirical analysis of vector quantization in the context of graph autoencoders, demonstrating its significant enhancement of the model's capacity to capture graph topology. Furthermore, we identify two key challenges associated with vector quantization when applying in graph data: codebook underutilization and codebook space sparsity. For the first challenge, we propose an annealing-based encoding strategy that promotes broad code utilization in the early stages of training, gradually shifting focus toward the most effective codes as training progresses. For the second challenge, we introduce a hierarchical two-layer codebook that captures relationships between embeddings through clustering. The second layer codebook links similar codes, encouraging the model to learn closer embeddings for nodes with similar features and structural topology in the graph. Our proposed model outperforms 16 representative baseline methods in self-supervised link prediction and node classification tasks across multiple datasets.",
      "authors": [
        "Long Zeng",
        "Jianxiang Yu",
        "Jiapeng Zhu",
        "Qingsong Zhong",
        "Xiang Li"
      ],
      "published": "2025-04-17T07:43:52Z",
      "updated": "2025-04-17T07:43:52Z",
      "categories": [
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2504.12715v1",
      "landing_url": "https://arxiv.org/abs/2504.12715v1",
      "doi": "https://doi.org/10.1145/3696410.3714656"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item targets graph autoencoders with vector quantization and codebook strategies; there is no discrete audio/speech tokenization or analysis of discrete audio tokens, so it does not meet the audio tokenization inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item targets graph autoencoders with vector quantization and codebook strategies; there is no discrete audio/speech tokenization or analysis of discrete audio tokens, so it does not meet the audio tokenization inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on graph self-supervised learning and vector quantization in graph autoencoders, without any mention of discrete audio or speech tokenization or processing continuous audio signals into discrete tokens. The core subject is graph data representation rather than audio or speech signals, which is outside the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on graph self-supervised learning and vector quantization in graph autoencoders, without any mention of discrete audio or speech tokenization or processing continuous audio signals into discrete tokens. The core subject is graph data representation rather than audio or speech signals, which is outside the inclusion criteria.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Lightweight Road Environment Segmentation using Vector Quantization",
    "abstract": "Road environment segmentation plays a significant role in autonomous driving. Numerous works based on Fully Convolutional Networks (FCNs) and Transformer architectures have been proposed to leverage local and global contextual learning for efficient and accurate semantic segmentation. In both architectures, the encoder often relies heavily on extracting continuous representations from the image, which limits the ability to represent meaningful discrete information. To address this limitation, we propose segmentation of the autonomous driving environment using vector quantization. Vector quantization offers three primary advantages for road environment segmentation. (1) Each continuous feature from the encoder is mapped to a discrete vector from the codebook, helping the model discover distinct features more easily than with complex continuous features. (2) Since a discrete feature acts as compressed versions of the encoder's continuous features, they also compress noise or outliers, enhancing the image segmentation task. (3) Vector quantization encourages the latent space to form coarse clusters of continuous features, forcing the model to group similar features, making the learned representations more structured for the decoding process. In this work, we combined vector quantization with the lightweight image segmentation model MobileUNETR and used it as a baseline model for comparison to demonstrate its efficiency. Through experiments, we achieved 77.0 % mIoU on Cityscapes, outperforming the baseline by 2.9 % without increasing the model's initial size or complexity.",
    "metadata": {
      "arxiv_id": "2504.14113",
      "title": "Lightweight Road Environment Segmentation using Vector Quantization",
      "summary": "Road environment segmentation plays a significant role in autonomous driving. Numerous works based on Fully Convolutional Networks (FCNs) and Transformer architectures have been proposed to leverage local and global contextual learning for efficient and accurate semantic segmentation. In both architectures, the encoder often relies heavily on extracting continuous representations from the image, which limits the ability to represent meaningful discrete information. To address this limitation, we propose segmentation of the autonomous driving environment using vector quantization. Vector quantization offers three primary advantages for road environment segmentation. (1) Each continuous feature from the encoder is mapped to a discrete vector from the codebook, helping the model discover distinct features more easily than with complex continuous features. (2) Since a discrete feature acts as compressed versions of the encoder's continuous features, they also compress noise or outliers, enhancing the image segmentation task. (3) Vector quantization encourages the latent space to form coarse clusters of continuous features, forcing the model to group similar features, making the learned representations more structured for the decoding process. In this work, we combined vector quantization with the lightweight image segmentation model MobileUNETR and used it as a baseline model for comparison to demonstrate its efficiency. Through experiments, we achieved 77.0 % mIoU on Cityscapes, outperforming the baseline by 2.9 % without increasing the model's initial size or complexity.",
      "authors": [
        "Jiyong Kwag",
        "Alper Yilmaz",
        "Charles Toth"
      ],
      "published": "2025-04-19T00:13:21Z",
      "updated": "2025-04-19T00:13:21Z",
      "categories": [
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2504.14113v1",
      "landing_url": "https://arxiv.org/abs/2504.14113v1",
      "doi": "https://doi.org/10.48550/arXiv.2504.14113"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This paper discusses discrete vector quantization for image segmentation in an autonomous driving context, which is a CV task, and does not propose or analyze discrete audio/speech tokenization; it does not meet the inclusion criteria requiring discrete audio tokenization analysis.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "This paper discusses discrete vector quantization for image segmentation in an autonomous driving context, which is a CV task, and does not propose or analyze discrete audio/speech tokenization; it does not meet the inclusion criteria requiring discrete audio tokenization analysis.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on road environment segmentation in autonomous driving using vector quantization for image features, with no mention of audio/speech tokenization or discrete audio token representations; thus, it does not meet the inclusion criteria focused on discrete audio/speech tokenization and relates to image, not audio data.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on road environment segmentation in autonomous driving using vector quantization for image features, with no mention of audio/speech tokenization or discrete audio token representations; thus, it does not meet the inclusion criteria focused on discrete audio/speech tokenization and relates to image, not audio data.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "SimulS2S-LLM: Unlocking Simultaneous Inference of Speech LLMs for Speech-to-Speech Translation",
    "abstract": "Simultaneous speech translation (SST) outputs translations in parallel with streaming speech input, balancing translation quality and latency. While large language models (LLMs) have been extended to handle the speech modality, streaming remains challenging as speech is prepended as a prompt for the entire generation process. To unlock LLM streaming capability, this paper proposes SimulS2S-LLM, which trains speech LLMs offline and employs a test-time policy to guide simultaneous inference. SimulS2S-LLM alleviates the mismatch between training and inference by extracting boundary-aware speech prompts that allows it to be better matched with text input data. SimulS2S-LLM achieves simultaneous speech-to-speech translation (Simul-S2ST) by predicting discrete output speech tokens and then synthesising output speech using a pre-trained vocoder. An incremental beam search is designed to expand the search space of speech token prediction without increasing latency. Experiments on the CVSS speech data show that SimulS2S-LLM offers a better translation quality-latency trade-off than existing methods that use the same training data, such as improving ASR-BLEU scores by 3 points at similar latency.",
    "metadata": {
      "arxiv_id": "2504.15509",
      "title": "SimulS2S-LLM: Unlocking Simultaneous Inference of Speech LLMs for Speech-to-Speech Translation",
      "summary": "Simultaneous speech translation (SST) outputs translations in parallel with streaming speech input, balancing translation quality and latency. While large language models (LLMs) have been extended to handle the speech modality, streaming remains challenging as speech is prepended as a prompt for the entire generation process. To unlock LLM streaming capability, this paper proposes SimulS2S-LLM, which trains speech LLMs offline and employs a test-time policy to guide simultaneous inference. SimulS2S-LLM alleviates the mismatch between training and inference by extracting boundary-aware speech prompts that allows it to be better matched with text input data. SimulS2S-LLM achieves simultaneous speech-to-speech translation (Simul-S2ST) by predicting discrete output speech tokens and then synthesising output speech using a pre-trained vocoder. An incremental beam search is designed to expand the search space of speech token prediction without increasing latency. Experiments on the CVSS speech data show that SimulS2S-LLM offers a better translation quality-latency trade-off than existing methods that use the same training data, such as improving ASR-BLEU scores by 3 points at similar latency.",
      "authors": [
        "Keqi Deng",
        "Wenxi Chen",
        "Xie Chen",
        "Philip C. Woodland"
      ],
      "published": "2025-04-22T01:05:32Z",
      "updated": "2025-04-22T01:05:32Z",
      "categories": [
        "cs.CL",
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2504.15509v1",
      "landing_url": "https://arxiv.org/abs/2504.15509v1",
      "doi": "https://doi.org/10.48550/arXiv.2504.15509"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The work explicitly centers on discrete speech tokenization by predicting discrete output speech tokens and using a vocoder to reconstruct audio, meeting the inclusion criteria and not aligning with any exclusion categories.",
      "evaluation": 5
    },
    "round-A_JuniorNano_reasoning": "The work explicitly centers on discrete speech tokenization by predicting discrete output speech tokens and using a vocoder to reconstruct audio, meeting the inclusion criteria and not aligning with any exclusion categories.",
    "round-A_JuniorNano_evaluation": 5,
    "round-A_JuniorMini_output": {
      "reasoning": "The study proposes SimulS2S-LLM which predicts discrete output speech tokens for simultaneous speech-to-speech translation, demonstrating a core analysis and evaluation of discrete audio/speech tokenization, aligning with all inclusion criteria and none of the exclusion criteria.",
      "evaluation": 5
    },
    "round-A_JuniorMini_reasoning": "The study proposes SimulS2S-LLM which predicts discrete output speech tokens for simultaneous speech-to-speech translation, demonstrating a core analysis and evaluation of discrete audio/speech tokenization, aligning with all inclusion criteria and none of the exclusion criteria.",
    "round-A_JuniorMini_evaluation": 5,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "include (junior:5)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "The Empirical Welfare Content of International Price and Income Comparisons",
    "abstract": "Multilateral index numbers are often used to make claims about welfare, such as treating PPPs as cross-country costs of living or real incomes as indicators of living standards. However, such interpretations may not be consistent with the observed data. To study this problem, I derive multilateral bounds on welfare implied by revealed preference and use these to appraise leading comparison methods. My findings support the welfare-interpretability of the contemporary indices I examine, but not of market exchange rates. When using a welfare-consistent multilateral index, the world in 2017 appears larger and more equal vis-à-vis the United States than conventional measures.",
    "metadata": {
      "arxiv_id": "2504.16654",
      "title": "The Empirical Welfare Content of International Price and Income Comparisons",
      "summary": "Multilateral index numbers are often used to make claims about welfare, such as treating PPPs as cross-country costs of living or real incomes as indicators of living standards. However, such interpretations may not be consistent with the observed data. To study this problem, I derive multilateral bounds on welfare implied by revealed preference and use these to appraise leading comparison methods. My findings support the welfare-interpretability of the contemporary indices I examine, but not of market exchange rates. When using a welfare-consistent multilateral index, the world in 2017 appears larger and more equal vis-à-vis the United States than conventional measures.",
      "authors": [
        "Hubert Wu"
      ],
      "published": "2025-04-23T12:22:12Z",
      "updated": "2025-12-09T23:58:18Z",
      "categories": [
        "econ.GN"
      ],
      "pdf_url": "https://arxiv.org/pdf/2504.16654v5",
      "landing_url": "https://arxiv.org/abs/2504.16654v5",
      "doi": "https://doi.org/10.48550/arXiv.2504.16654"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item is an economics article about welfare content in international price and income comparisons and contains no discrete audio/speech tokenization analysis, thus it fails the core inclusion criterion and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item is an economics article about welfare content in international price and income comparisons and contains no discrete audio/speech tokenization analysis, thus it fails the core inclusion criterion and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on multilateral index numbers and welfare evaluation in an economic context, with no mention of audio/speech tokenization, discrete audio symbols, or related signal processing methods; hence, it does not meet any inclusion criteria for audio-related studies.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on multilateral index numbers and welfare evaluation in an economic context, with no mention of audio/speech tokenization, discrete audio symbols, or related signal processing methods; hence, it does not meet any inclusion criteria for audio-related studies.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Enhancing Cochlear Implant Signal Coding with Scaled Dot-Product Attention",
    "abstract": "Cochlear implants (CIs) play a vital role in restoring hearing for individuals with severe to profound sensorineural hearing loss by directly stimulating the auditory nerve with electrical signals. While traditional coding strategies, such as the advanced combination encoder (ACE), have proven effective, they are constrained by their adaptability and precision. This paper investigates the use of deep learning (DL) techniques to generate electrodograms for CIs, presenting our model as an advanced alternative. We compared the performance of our model with the ACE strategy by evaluating the intelligibility of reconstructed audio signals using the short-time objective intelligibility (STOI) metric. The results indicate that our model achieves a STOI score of 0.6031, closely approximating the 0.6126 score of the ACE strategy, and offers potential advantages in flexibility and adaptability. This study underscores the benefits of incorporating artificial intelligent (AI) into CI technology, such as enhanced personalization and efficiency.",
    "metadata": {
      "arxiv_id": "2504.19046",
      "title": "Enhancing Cochlear Implant Signal Coding with Scaled Dot-Product Attention",
      "summary": "Cochlear implants (CIs) play a vital role in restoring hearing for individuals with severe to profound sensorineural hearing loss by directly stimulating the auditory nerve with electrical signals. While traditional coding strategies, such as the advanced combination encoder (ACE), have proven effective, they are constrained by their adaptability and precision. This paper investigates the use of deep learning (DL) techniques to generate electrodograms for CIs, presenting our model as an advanced alternative. We compared the performance of our model with the ACE strategy by evaluating the intelligibility of reconstructed audio signals using the short-time objective intelligibility (STOI) metric. The results indicate that our model achieves a STOI score of 0.6031, closely approximating the 0.6126 score of the ACE strategy, and offers potential advantages in flexibility and adaptability. This study underscores the benefits of incorporating artificial intelligent (AI) into CI technology, such as enhanced personalization and efficiency.",
      "authors": [
        "Billel Essaid",
        "Hamza Kheddar",
        "Noureddine Batel"
      ],
      "published": "2025-04-26T22:49:08Z",
      "updated": "2025-04-26T22:49:08Z",
      "categories": [
        "eess.AS",
        "cs.AI",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2504.19046v1",
      "landing_url": "https://arxiv.org/abs/2504.19046v1",
      "doi": "https://doi.org/10.1109/ICTIS62692.2024.10894163"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "Does not explicitly propose or analyze discrete audio/token-based representations; the abstract focuses on DL-generated electrodograms and continuous evaluation metrics, not discrete tokenization.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "Does not explicitly propose or analyze discrete audio/token-based representations; the abstract focuses on DL-generated electrodograms and continuous evaluation metrics, not discrete tokenization.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on enhancing cochlear implant signal coding using deep learning to generate electrodograms, comparing performance with traditional methods; however, it does not explicitly address discrete audio/speech tokenization or the conversion of continuous audio signals into sequences of discrete symbols, which is the core inclusion criterion. It discusses neural encoding and signal coding but lacks clear analysis or proposal of discrete tokenization for speech audio reconstruction or understanding, making it not fully aligned with the inclusion requirements.",
      "evaluation": 2
    },
    "round-A_JuniorMini_reasoning": "The study focuses on enhancing cochlear implant signal coding using deep learning to generate electrodograms, comparing performance with traditional methods; however, it does not explicitly address discrete audio/speech tokenization or the conversion of continuous audio signals into sequences of discrete symbols, which is the core inclusion criterion. It discusses neural encoding and signal coding but lacks clear analysis or proposal of discrete tokenization for speech audio reconstruction or understanding, making it not fully aligned with the inclusion requirements.",
    "round-A_JuniorMini_evaluation": 2,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:2)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:2)"
  },
  {
    "title": "Accurate and Efficient Multivariate Time Series Forecasting via Offline Clustering",
    "abstract": "Accurate and efficient multivariate time series (MTS) forecasting is essential for applications such as traffic management and weather prediction, which depend on capturing long-range temporal dependencies and interactions between entities. Existing methods, particularly those based on Transformer architectures, compute pairwise dependencies across all time steps, leading to a computational complexity that scales quadratically with the length of the input. To overcome these challenges, we introduce the Forecaster with Offline Clustering Using Segments (FOCUS), a novel approach to MTS forecasting that simplifies long-range dependency modeling through the use of prototypes extracted via offline clustering. These prototypes encapsulate high-level events in the real-world system underlying the data, summarizing the key characteristics of similar time segments. In the online phase, FOCUS dynamically adapts these patterns to the current input and captures dependencies between the input segment and high-level events, enabling both accurate and efficient forecasting. By identifying prototypes during the offline clustering phase, FOCUS reduces the computational complexity of modeling long-range dependencies in the online phase to linear scaling. Extensive experiments across diverse benchmarks demonstrate that FOCUS achieves state-of-the-art accuracy while significantly reducing computational costs.",
    "metadata": {
      "arxiv_id": "2505.05738",
      "title": "Accurate and Efficient Multivariate Time Series Forecasting via Offline Clustering",
      "summary": "Accurate and efficient multivariate time series (MTS) forecasting is essential for applications such as traffic management and weather prediction, which depend on capturing long-range temporal dependencies and interactions between entities. Existing methods, particularly those based on Transformer architectures, compute pairwise dependencies across all time steps, leading to a computational complexity that scales quadratically with the length of the input. To overcome these challenges, we introduce the Forecaster with Offline Clustering Using Segments (FOCUS), a novel approach to MTS forecasting that simplifies long-range dependency modeling through the use of prototypes extracted via offline clustering. These prototypes encapsulate high-level events in the real-world system underlying the data, summarizing the key characteristics of similar time segments. In the online phase, FOCUS dynamically adapts these patterns to the current input and captures dependencies between the input segment and high-level events, enabling both accurate and efficient forecasting. By identifying prototypes during the offline clustering phase, FOCUS reduces the computational complexity of modeling long-range dependencies in the online phase to linear scaling. Extensive experiments across diverse benchmarks demonstrate that FOCUS achieves state-of-the-art accuracy while significantly reducing computational costs.",
      "authors": [
        "Yiming Niu",
        "Jinliang Deng",
        "Lulu Zhang",
        "Zimu Zhou",
        "Yongxin Tong"
      ],
      "published": "2025-05-09T02:34:06Z",
      "updated": "2025-05-25T07:48:29Z",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2505.05738v2",
      "landing_url": "https://arxiv.org/abs/2505.05738v2",
      "doi": "https://doi.org/10.48550/arXiv.2505.05738"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item concerns multivariate time series forecasting using offline clustering/prototypes and does not address discrete audio/speech tokenization or discrete token representations for language tasks, so it fails the required inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item concerns multivariate time series forecasting using offline clustering/prototypes and does not address discrete audio/speech tokenization or discrete token representations for language tasks, so it fails the required inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on multivariate time series forecasting using offline clustering and does not mention any form of discrete audio or speech tokenization, nor does it address converting continuous audio signals into discrete tokens, which is central to the inclusion criteria. Therefore, it does not meet the specified criteria related to discrete audio/speech tokenization.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on multivariate time series forecasting using offline clustering and does not mention any form of discrete audio or speech tokenization, nor does it address converting continuous audio signals into discrete tokens, which is central to the inclusion criteria. Therefore, it does not meet the specified criteria related to discrete audio/speech tokenization.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "ZipLLM: Efficient LLM Storage via Model-Aware Synergistic Data Deduplication and Compression",
    "abstract": "Modern model hubs, such as Hugging Face, store tens of petabytes of LLMs, with fine-tuned variants vastly outnumbering base models and dominating storage consumption. Existing storage reduction techniques -- such as deduplication and compression -- are either LLM-oblivious or not compatible with each other, limiting data reduction effectiveness. Our large-scale characterization study across all publicly available Hugging Face LLM repositories reveals several key insights: (1) fine-tuned models within the same family exhibit highly structured, sparse parameter differences suitable for delta compression; (2) bitwise similarity enables LLM family clustering; and (3) tensor-level deduplication is better aligned with model storage workloads, achieving high data reduction with low metadata overhead. Building on these insights, we design BitX, an effective, fast, lossless delta compression algorithm that compresses XORed difference between fine-tuned and base LLMs. We build ZipLLM, a model storage reduction pipeline that unifies tensor-level deduplication and lossless BitX compression. By synergizing deduplication and compression around LLM family clustering, ZipLLM reduces model storage consumption by 54%, over 20% higher than state-of-the-art deduplication and compression approaches.",
    "metadata": {
      "arxiv_id": "2505.06252",
      "title": "ZipLLM: Efficient LLM Storage via Model-Aware Synergistic Data Deduplication and Compression",
      "summary": "Modern model hubs, such as Hugging Face, store tens of petabytes of LLMs, with fine-tuned variants vastly outnumbering base models and dominating storage consumption. Existing storage reduction techniques -- such as deduplication and compression -- are either LLM-oblivious or not compatible with each other, limiting data reduction effectiveness. Our large-scale characterization study across all publicly available Hugging Face LLM repositories reveals several key insights: (1) fine-tuned models within the same family exhibit highly structured, sparse parameter differences suitable for delta compression; (2) bitwise similarity enables LLM family clustering; and (3) tensor-level deduplication is better aligned with model storage workloads, achieving high data reduction with low metadata overhead. Building on these insights, we design BitX, an effective, fast, lossless delta compression algorithm that compresses XORed difference between fine-tuned and base LLMs. We build ZipLLM, a model storage reduction pipeline that unifies tensor-level deduplication and lossless BitX compression. By synergizing deduplication and compression around LLM family clustering, ZipLLM reduces model storage consumption by 54%, over 20% higher than state-of-the-art deduplication and compression approaches.",
      "authors": [
        "Zirui Wang",
        "Tingfeng Lan",
        "Zhaoyuan Su",
        "Juncheng Yang",
        "Yue Cheng"
      ],
      "published": "2025-04-30T04:16:32Z",
      "updated": "2025-11-08T18:45:50Z",
      "categories": [
        "cs.DB",
        "cs.DC"
      ],
      "pdf_url": "https://arxiv.org/pdf/2505.06252v3",
      "landing_url": "https://arxiv.org/abs/2505.06252v3",
      "doi": "https://doi.org/10.48550/arXiv.2505.06252"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item focuses on model storage, deduplication, and lossless compression for LLMs, with no analysis or use of discrete audio/speech tokenization; it does not meet the inclusion criteria and aligns with exclusion criteria for tokenization-focused audio studies.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item focuses on model storage, deduplication, and lossless compression for LLMs, with no analysis or use of discrete audio/speech tokenization; it does not meet the inclusion criteria and aligns with exclusion criteria for tokenization-focused audio studies.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on storage reduction for large language models (LLMs) via deduplication and compression techniques, with no mention of audio or speech tokenization, discrete audio representations, or any analysis related to converting audio signals into discrete token sequences; therefore, it does not meet the inclusion criteria concerning audio/speech tokenization and is unrelated to the specified topic.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on storage reduction for large language models (LLMs) via deduplication and compression techniques, with no mention of audio or speech tokenization, discrete audio representations, or any analysis related to converting audio signals into discrete token sequences; therefore, it does not meet the inclusion criteria concerning audio/speech tokenization and is unrelated to the specified topic.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Good Things Come in Pairs: Paired Autoencoders for Inverse Problems",
    "abstract": "In this book chapter, we discuss recent advances in data-driven approaches for inverse problems. In particular, we focus on the \\emph{paired autoencoder} framework, which has proven to be a powerful tool for solving inverse problems in scientific computing. The paired autoencoder framework is a novel approach that leverages the strengths of both data-driven and model-based methods by projecting both the data and the quantity of interest into a latent space and mapping these latent spaces to provide surrogate forward and inverse mappings. We illustrate the advantages of this approach through numerical experiments, including seismic imaging and classical inpainting: nonlinear and linear inverse problems, respectively. Although the paired autoencoder framework is likelihood-free, it generates multiple data- and model-based reconstruction metrics that help assess whether examples are in or out of distribution. In addition to direct model estimates from data, the paired autoencoder enables latent-space refinement to fit the observed data accurately. Numerical experiments show that this procedure, combined with the latent-space initial guess, is essential for high-quality estimates, even when data noise exceeds the training regime. We also introduce two novel variants that combine variational and paired autoencoder ideas, maintaining the original benefits while enabling sampling for uncertainty analysis.",
    "metadata": {
      "arxiv_id": "2505.06549",
      "title": "Good Things Come in Pairs: Paired Autoencoders for Inverse Problems",
      "summary": "In this book chapter, we discuss recent advances in data-driven approaches for inverse problems. In particular, we focus on the \\emph{paired autoencoder} framework, which has proven to be a powerful tool for solving inverse problems in scientific computing. The paired autoencoder framework is a novel approach that leverages the strengths of both data-driven and model-based methods by projecting both the data and the quantity of interest into a latent space and mapping these latent spaces to provide surrogate forward and inverse mappings. We illustrate the advantages of this approach through numerical experiments, including seismic imaging and classical inpainting: nonlinear and linear inverse problems, respectively. Although the paired autoencoder framework is likelihood-free, it generates multiple data- and model-based reconstruction metrics that help assess whether examples are in or out of distribution. In addition to direct model estimates from data, the paired autoencoder enables latent-space refinement to fit the observed data accurately. Numerical experiments show that this procedure, combined with the latent-space initial guess, is essential for high-quality estimates, even when data noise exceeds the training regime. We also introduce two novel variants that combine variational and paired autoencoder ideas, maintaining the original benefits while enabling sampling for uncertainty analysis.",
      "authors": [
        "Matthias Chung",
        "Bas Peters",
        "Michael Solomon"
      ],
      "published": "2025-05-10T07:31:09Z",
      "updated": "2025-08-18T23:57:40Z",
      "categories": [
        "cs.LG",
        "stat.ML"
      ],
      "pdf_url": "https://arxiv.org/pdf/2505.06549v2",
      "landing_url": "https://arxiv.org/abs/2505.06549v2",
      "doi": "https://doi.org/10.48550/arXiv.2505.06549"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This item discusses paired autoencoders for inverse problems with applications in seismic imaging and inpainting; it does not address discrete audio/speech tokenization as a core object, so it does not meet the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "This item discusses paired autoencoders for inverse problems with applications in seismic imaging and inpainting; it does not address discrete audio/speech tokenization as a core object, so it does not meet the inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on the paired autoencoder framework for inverse problems in scientific computing, such as seismic imaging and inpainting, without any mention of discrete audio or speech tokenization or audio-related discrete symbol representations, which is the core inclusion criterion.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on the paired autoencoder framework for inverse problems in scientific computing, such as seismic imaging and inpainting, without any mention of discrete audio or speech tokenization or audio-related discrete symbol representations, which is the core inclusion criterion.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "RADE: A Neural Codec for Transmitting Speech over HF Radio Channels",
    "abstract": "Speech compression is commonly used to send voice over radio channels in applications such as mobile telephony and two-way push-to-talk (PTT) radio. In classical systems, the speech codec is combined with forward error correction, modulation and radio hardware. In this paper we describe an autoencoder that replaces many of the traditional signal processing elements with a neural network. The encoder takes a vocoder feature set (short term spectrum, pitch, voicing), and produces discrete time, but continuously valued quadrature amplitude modulation (QAM) symbols. We use orthogonal frequency domain multiplexing (OFDM) to send and receive these symbols over high frequency (HF) radio channels. The decoder converts received QAM symbols to vocoder features suitable for synthesis. The autoencoder has been trained to be robust to additive Gaussian noise and multipath channel impairments while simultaneously maintaining a Peak To Average Power Ratio (PAPR) of less than 1 dB. Over simulated and real world HF radio channels we have achieved output speech intelligibility that clearly surpasses existing analog and digital radio systems over a range of SNRs.",
    "metadata": {
      "arxiv_id": "2505.06671",
      "title": "RADE: A Neural Codec for Transmitting Speech over HF Radio Channels",
      "summary": "Speech compression is commonly used to send voice over radio channels in applications such as mobile telephony and two-way push-to-talk (PTT) radio. In classical systems, the speech codec is combined with forward error correction, modulation and radio hardware. In this paper we describe an autoencoder that replaces many of the traditional signal processing elements with a neural network. The encoder takes a vocoder feature set (short term spectrum, pitch, voicing), and produces discrete time, but continuously valued quadrature amplitude modulation (QAM) symbols. We use orthogonal frequency domain multiplexing (OFDM) to send and receive these symbols over high frequency (HF) radio channels. The decoder converts received QAM symbols to vocoder features suitable for synthesis. The autoencoder has been trained to be robust to additive Gaussian noise and multipath channel impairments while simultaneously maintaining a Peak To Average Power Ratio (PAPR) of less than 1 dB. Over simulated and real world HF radio channels we have achieved output speech intelligibility that clearly surpasses existing analog and digital radio systems over a range of SNRs.",
      "authors": [
        "David Rowe",
        "Jean-Marc Valin"
      ],
      "published": "2025-05-10T15:16:05Z",
      "updated": "2025-07-26T18:38:38Z",
      "categories": [
        "eess.AS",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2505.06671v2",
      "landing_url": "https://arxiv.org/abs/2505.06671v2",
      "doi": "https://doi.org/10.48550/arXiv.2505.06671"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The paper describes a neural speech codec that outputs discrete-time QAM symbols (discrete transmission tokens) and uses vocoder features for synthesis, suggesting a discrete symbol representation; however, the abstract frames the work as a neural codec for speech transmission rather than explicitly studying discrete audio tokenization as the core object of analysis, making its status as meeting the tokenization-focused inclusion criteria uncertain.",
      "evaluation": 3
    },
    "round-A_JuniorNano_reasoning": "The paper describes a neural speech codec that outputs discrete-time QAM symbols (discrete transmission tokens) and uses vocoder features for synthesis, suggesting a discrete symbol representation; however, the abstract frames the work as a neural codec for speech transmission rather than explicitly studying discrete audio tokenization as the core object of analysis, making its status as meeting the tokenization-focused inclusion criteria uncertain.",
    "round-A_JuniorNano_evaluation": 3,
    "round-A_JuniorMini_output": {
      "reasoning": "The study involves a neural codec autoencoder that encodes vocoder features into continuously valued QAM symbols for transmission over HF radio, but it does not explicitly propose or analyze discrete audio/speech tokenization as a core object of study; the QAM symbols are continuous-valued rather than discrete symbols from a finite vocabulary, and there is no indication of discrete quantization or token design relevant to language-model-style token sequences.",
      "evaluation": 2
    },
    "round-A_JuniorMini_reasoning": "The study involves a neural codec autoencoder that encodes vocoder features into continuously valued QAM symbols for transmission over HF radio, but it does not explicitly propose or analyze discrete audio/speech tokenization as a core object of study; the QAM symbols are continuous-valued rather than discrete symbols from a finite vocabulary, and there is no indication of discrete quantization or token design relevant to language-model-style token sequences.",
    "round-A_JuniorMini_evaluation": 2,
    "round-B_SeniorLead_output": {
      "reasoning": "Junior reviewers: round-A_JuniorNano rated 3 and round-A_JuniorMini rated 2. The abstract describes a neural autoencoder that encodes vocoder features into continuously valued QAM symbols for HF radio transmission and decodes them back; it does not propose or analyze any discrete quantization or a finite-vocabulary tokenization of audio/speech (the transmitted symbols are continuous-valued and the work focuses on radio codec performance), so it fails the inclusion criterion and matches the exclusion criteria (uses continuous audio features and is an engineering codec study rather than a discrete-tokenization paper). Brief explanation for scoring: exclude — the paper does not present or evaluate discrete audio/speech tokenization.",
      "evaluation": 1
    },
    "round-B_SeniorLead_reasoning": "Junior reviewers: round-A_JuniorNano rated 3 and round-A_JuniorMini rated 2. The abstract describes a neural autoencoder that encodes vocoder features into continuously valued QAM symbols for HF radio transmission and decodes them back; it does not propose or analyze any discrete quantization or a finite-vocabulary tokenization of audio/speech (the transmitted symbols are continuous-valued and the work focuses on radio codec performance), so it fails the inclusion criterion and matches the exclusion criteria (uses continuous audio features and is an engineering codec study rather than a discrete-tokenization paper). Brief explanation for scoring: exclude — the paper does not present or evaluate discrete audio/speech tokenization.",
    "round-B_SeniorLead_evaluation": 1,
    "final_verdict": "exclude (senior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (senior:1)"
  },
  {
    "title": "VFRTok: Variable Frame Rates Video Tokenizer with Duration-Proportional Information Assumption",
    "abstract": "Modern video generation frameworks based on Latent Diffusion Models suffer from inefficiencies in tokenization due to the Frame-Proportional Information Assumption. Existing tokenizers provide fixed temporal compression rates, causing the computational cost of the diffusion model to scale linearly with the frame rate. The paper proposes the Duration-Proportional Information Assumption: the upper bound on the information capacity of a video is proportional to the duration rather than the number of frames. Based on this insight, the paper introduces VFRTok, a Transformer-based video tokenizer, that enables variable frame rate encoding and decoding through asymmetric frame rate training between the encoder and decoder. Furthermore, the paper proposes Partial Rotary Position Embeddings (RoPE) to decouple position and content modeling, which groups correlated patches into unified tokens. The Partial RoPE effectively improves content-awareness, enhancing the video generation capability. Benefiting from the compact and continuous spatio-temporal representation, VFRTok achieves competitive reconstruction quality and state-of-the-art generation fidelity while using only 1/8 tokens compared to existing tokenizers. The code and weights are released at: https://github.com/KwaiVGI/VFRTok.",
    "metadata": {
      "arxiv_id": "2505.12053",
      "title": "VFRTok: Variable Frame Rates Video Tokenizer with Duration-Proportional Information Assumption",
      "summary": "Modern video generation frameworks based on Latent Diffusion Models suffer from inefficiencies in tokenization due to the Frame-Proportional Information Assumption. Existing tokenizers provide fixed temporal compression rates, causing the computational cost of the diffusion model to scale linearly with the frame rate. The paper proposes the Duration-Proportional Information Assumption: the upper bound on the information capacity of a video is proportional to the duration rather than the number of frames. Based on this insight, the paper introduces VFRTok, a Transformer-based video tokenizer, that enables variable frame rate encoding and decoding through asymmetric frame rate training between the encoder and decoder. Furthermore, the paper proposes Partial Rotary Position Embeddings (RoPE) to decouple position and content modeling, which groups correlated patches into unified tokens. The Partial RoPE effectively improves content-awareness, enhancing the video generation capability. Benefiting from the compact and continuous spatio-temporal representation, VFRTok achieves competitive reconstruction quality and state-of-the-art generation fidelity while using only 1/8 tokens compared to existing tokenizers. The code and weights are released at: https://github.com/KwaiVGI/VFRTok.",
      "authors": [
        "Tianxiong Zhong",
        "Xingye Tian",
        "Boyuan Jiang",
        "Xuebo Wang",
        "Xin Tao",
        "Pengfei Wan",
        "Zhiwei Zhang"
      ],
      "published": "2025-05-17T15:32:54Z",
      "updated": "2025-09-28T09:51:18Z",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2505.12053v2",
      "landing_url": "https://arxiv.org/abs/2505.12053v2",
      "doi": "https://doi.org/10.48550/arXiv.2505.12053"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The paper targets a Transformer-based video tokenizer with variable frame rates, not discrete audio/speech tokenization, so it does not meet the inclusion criteria and fails the audio-tokenization focus.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The paper targets a Transformer-based video tokenizer with variable frame rates, not discrete audio/speech tokenization, so it does not meet the inclusion criteria and fails the audio-tokenization focus.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on video tokenization rather than audio/speech tokenization, and does not mention discrete audio tokenization or related analysis; thus it does not meet the inclusion criteria centered on audio/speech tokenization.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on video tokenization rather than audio/speech tokenization, and does not mention discrete audio tokenization or related analysis; thus it does not meet the inclusion criteria centered on audio/speech tokenization.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Joint Embedding vs Reconstruction: Provable Benefits of Latent Space Prediction for Self Supervised Learning",
    "abstract": "Reconstruction and joint embedding have emerged as two leading paradigms in Self Supervised Learning (SSL). Reconstruction methods focus on recovering the original sample from a different view in input space. On the other hand, joint embedding methods align the representations of different views in latent space. Both approaches offer compelling advantages, yet practitioners lack clear guidelines for choosing between them. In this work, we unveil the core mechanisms that distinguish each paradigm. By leveraging closed form solutions for both approaches, we precisely characterize how the view generation process, e.g. data augmentation, impacts the learned representations. We then demonstrate that, unlike supervised learning, both SSL paradigms require a minimal alignment between augmentations and irrelevant features to achieve asymptotic optimality with increasing sample size. Our findings indicate that in scenarios where these irrelevant features have a large magnitude, joint embedding methods are preferable because they impose a strictly weaker alignment condition compared to reconstruction based methods. These results not only clarify the trade offs between the two paradigms but also substantiate the empirical success of joint embedding approaches on real world challenging datasets.",
    "metadata": {
      "arxiv_id": "2505.12477",
      "title": "Joint Embedding vs Reconstruction: Provable Benefits of Latent Space Prediction for Self Supervised Learning",
      "summary": "Reconstruction and joint embedding have emerged as two leading paradigms in Self Supervised Learning (SSL). Reconstruction methods focus on recovering the original sample from a different view in input space. On the other hand, joint embedding methods align the representations of different views in latent space. Both approaches offer compelling advantages, yet practitioners lack clear guidelines for choosing between them. In this work, we unveil the core mechanisms that distinguish each paradigm. By leveraging closed form solutions for both approaches, we precisely characterize how the view generation process, e.g. data augmentation, impacts the learned representations. We then demonstrate that, unlike supervised learning, both SSL paradigms require a minimal alignment between augmentations and irrelevant features to achieve asymptotic optimality with increasing sample size. Our findings indicate that in scenarios where these irrelevant features have a large magnitude, joint embedding methods are preferable because they impose a strictly weaker alignment condition compared to reconstruction based methods. These results not only clarify the trade offs between the two paradigms but also substantiate the empirical success of joint embedding approaches on real world challenging datasets.",
      "authors": [
        "Hugues Van Assel",
        "Mark Ibrahim",
        "Tommaso Biancalani",
        "Aviv Regev",
        "Randall Balestriero"
      ],
      "published": "2025-05-18T15:54:55Z",
      "updated": "2025-10-14T16:45:33Z",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2505.12477v2",
      "landing_url": "https://arxiv.org/abs/2505.12477v2",
      "doi": "https://doi.org/10.48550/arXiv.2505.12477"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The abstract discusses self-supervised learning paradigms (reconstruction vs. joint embedding) and augmentation alignment with no mention of discrete audio tokenization or quantized token sequences; therefore it does not meet the inclusion criteria centered on discrete audio/speech tokenization and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The abstract discusses self-supervised learning paradigms (reconstruction vs. joint embedding) and augmentation alignment with no mention of discrete audio tokenization or quantized token sequences; therefore it does not meet the inclusion criteria centered on discrete audio/speech tokenization and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on theoretical analysis comparing joint embedding and reconstruction methods for self-supervised learning but does not mention discrete audio or speech tokenization, nor does it address converting continuous audio signals into discrete tokens; thus, it does not meet the inclusion criteria related to discrete audio/speech tokenization.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on theoretical analysis comparing joint embedding and reconstruction methods for self-supervised learning but does not mention discrete audio or speech tokenization, nor does it address converting continuous audio signals into discrete tokens; thus, it does not meet the inclusion criteria related to discrete audio/speech tokenization.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Beyond Semantics: The Unreasonable Effectiveness of Reasonless Intermediate Tokens",
    "abstract": "Recent impressive results from large reasoning models have been interpreted as a triumph of Chain of Thought (CoT), especially of training on CoTs sampled from base LLMs to help find new reasoning patterns. While these traces certainly seem to help model performance, it is not clear how they actually influence it, with some works ascribing semantics to the traces and others cautioning against relying on them as transparent and faithful proxies of the model's internal computational process. To systematically investigate the role of end-user semantics of derivational traces, we set up a controlled study where we train transformer models from scratch on formally verifiable reasoning traces and the solutions they lead to. We notice that, despite significant gains over the solution-only baseline, models trained on entirely correct traces can still produce invalid reasoning traces even when arriving at correct solutions. More interestingly, our experiments also show that models trained on corrupted traces, whose intermediate reasoning steps bear no relation to the problem they accompany, perform similarly to those trained on correct ones, and even generalize better on out-of-distribution tasks. We also study the effect of GRPO-based RL post-training on trace validity, noting that while solution accuracy increase, this is not accompanied by any improvements in trace validity. Finally, we examine whether reasoning-trace length reflects inference-time scaling and find that trace length is largely agnostic to the underlying computational complexity of the problem being solved. These results challenge the assumption that intermediate tokens or ``Chains of Thought'' reflect or induce predictable reasoning behaviors and caution against anthropomorphizing such outputs or over-interpreting them (despite their mostly seemingly forms) as evidence of human-like or algorithmic behaviors in language models.",
    "metadata": {
      "arxiv_id": "2505.13775",
      "title": "Beyond Semantics: The Unreasonable Effectiveness of Reasonless Intermediate Tokens",
      "summary": "Recent impressive results from large reasoning models have been interpreted as a triumph of Chain of Thought (CoT), especially of training on CoTs sampled from base LLMs to help find new reasoning patterns. While these traces certainly seem to help model performance, it is not clear how they actually influence it, with some works ascribing semantics to the traces and others cautioning against relying on them as transparent and faithful proxies of the model's internal computational process. To systematically investigate the role of end-user semantics of derivational traces, we set up a controlled study where we train transformer models from scratch on formally verifiable reasoning traces and the solutions they lead to. We notice that, despite significant gains over the solution-only baseline, models trained on entirely correct traces can still produce invalid reasoning traces even when arriving at correct solutions. More interestingly, our experiments also show that models trained on corrupted traces, whose intermediate reasoning steps bear no relation to the problem they accompany, perform similarly to those trained on correct ones, and even generalize better on out-of-distribution tasks. We also study the effect of GRPO-based RL post-training on trace validity, noting that while solution accuracy increase, this is not accompanied by any improvements in trace validity. Finally, we examine whether reasoning-trace length reflects inference-time scaling and find that trace length is largely agnostic to the underlying computational complexity of the problem being solved. These results challenge the assumption that intermediate tokens or ``Chains of Thought'' reflect or induce predictable reasoning behaviors and caution against anthropomorphizing such outputs or over-interpreting them (despite their mostly seemingly forms) as evidence of human-like or algorithmic behaviors in language models.",
      "authors": [
        "Karthik Valmeekam",
        "Kaya Stechly",
        "Vardhan Palod",
        "Atharva Gundawar",
        "Subbarao Kambhampati"
      ],
      "published": "2025-05-19T23:29:23Z",
      "updated": "2025-11-22T07:49:57Z",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2505.13775v3",
      "landing_url": "https://arxiv.org/abs/2505.13775v3",
      "doi": "https://doi.org/10.48550/arXiv.2505.13775"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This item studies chain-of-thought traces in language models and semantics, not discrete audio tokenization; it does not meet the inclusion criteria for discrete audio tokens, so it should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "This item studies chain-of-thought traces in language models and semantics, not discrete audio tokenization; it does not meet the inclusion criteria for discrete audio tokens, so it should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on reasoning models, intermediate tokens in language models, and semantics in reasoning traces, without any mention or analysis of audio/speech tokenization or discrete tokens derived from continuous audio signals. It does not address audio signal processing, discrete audio tokenization, or neural encoding of audio signals, which are the core inclusion criteria; thus, it does not meet the inclusion requirements and is unrelated to audio/speech tokenization tasks.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on reasoning models, intermediate tokens in language models, and semantics in reasoning traces, without any mention or analysis of audio/speech tokenization or discrete tokens derived from continuous audio signals. It does not address audio signal processing, discrete audio tokenization, or neural encoding of audio signals, which are the core inclusion criteria; thus, it does not meet the inclusion requirements and is unrelated to audio/speech tokenization tasks.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Improving Noise Robustness of LLM-based Zero-shot TTS via Discrete Acoustic Token Denoising",
    "abstract": "Large language model (LLM) based zero-shot text-to-speech (TTS) methods tend to preserve the acoustic environment of the audio prompt, leading to degradation in synthesized speech quality when the audio prompt contains noise. In this paper, we propose a novel neural codec-based speech denoiser and integrate it with the advanced LLM-based TTS model, LauraTTS, to achieve noise-robust zero-shot TTS. The proposed codec denoiser consists of an audio codec, a token denoiser, and an embedding refiner. The token denoiser predicts the first two groups of clean acoustic tokens from the noisy ones, which can serve as the acoustic prompt for LauraTTS to synthesize high-quality personalized speech or be converted to clean speech waveforms through the embedding refiner and codec decoder. Experimental results show that our proposed codec denoiser outperforms state-of-the-art speech enhancement (SE) methods, and the proposed noise-robust LauraTTS surpasses the approach using additional SE models.",
    "metadata": {
      "arxiv_id": "2505.13830",
      "title": "Improving Noise Robustness of LLM-based Zero-shot TTS via Discrete Acoustic Token Denoising",
      "summary": "Large language model (LLM) based zero-shot text-to-speech (TTS) methods tend to preserve the acoustic environment of the audio prompt, leading to degradation in synthesized speech quality when the audio prompt contains noise. In this paper, we propose a novel neural codec-based speech denoiser and integrate it with the advanced LLM-based TTS model, LauraTTS, to achieve noise-robust zero-shot TTS. The proposed codec denoiser consists of an audio codec, a token denoiser, and an embedding refiner. The token denoiser predicts the first two groups of clean acoustic tokens from the noisy ones, which can serve as the acoustic prompt for LauraTTS to synthesize high-quality personalized speech or be converted to clean speech waveforms through the embedding refiner and codec decoder. Experimental results show that our proposed codec denoiser outperforms state-of-the-art speech enhancement (SE) methods, and the proposed noise-robust LauraTTS surpasses the approach using additional SE models.",
      "authors": [
        "Ye-Xin Lu",
        "Hui-Peng Du",
        "Fei Liu",
        "Yang Ai",
        "Zhen-Hua Ling"
      ],
      "published": "2025-05-20T02:18:45Z",
      "updated": "2025-05-22T04:41:35Z",
      "categories": [
        "eess.AS",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2505.13830v2",
      "landing_url": "https://arxiv.org/abs/2505.13830v2",
      "doi": "https://doi.org/10.48550/arXiv.2505.13830"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The paper explicitly uses discrete acoustic tokens (token denoiser predicting clean acoustic tokens) as core components of the method, aligning with the inclusion criterion for discrete tokenization in speech.",
      "evaluation": 5
    },
    "round-A_JuniorNano_reasoning": "The paper explicitly uses discrete acoustic tokens (token denoiser predicting clean acoustic tokens) as core components of the method, aligning with the inclusion criterion for discrete tokenization in speech.",
    "round-A_JuniorNano_evaluation": 5,
    "round-A_JuniorMini_output": {
      "reasoning": "The study explicitly proposes a token denoiser that predicts discrete acoustic tokens from noisy inputs for zero-shot TTS synthesis, focusing on discrete token representations and denoising in a neural codec framework. This aligns well with the inclusion criterion of analyzing discrete audio/speech tokenization for generation tasks, and it is not excluded by any criteria as it involves neural codecs rather than traditional signal-processing codecs, includes methodological contributions and experiments, and is focused on speech. Therefore, it should be included.",
      "evaluation": 5
    },
    "round-A_JuniorMini_reasoning": "The study explicitly proposes a token denoiser that predicts discrete acoustic tokens from noisy inputs for zero-shot TTS synthesis, focusing on discrete token representations and denoising in a neural codec framework. This aligns well with the inclusion criterion of analyzing discrete audio/speech tokenization for generation tasks, and it is not excluded by any criteria as it involves neural codecs rather than traditional signal-processing codecs, includes methodological contributions and experiments, and is focused on speech. Therefore, it should be included.",
    "round-A_JuniorMini_evaluation": 5,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "include (junior:5)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "StPR: Spatiotemporal Preservation and Routing for Exemplar-Free Video Class-Incremental Learning",
    "abstract": "Video Class-Incremental Learning (VCIL) seeks to develop models that continuously learn new action categories over time without forgetting previously acquired knowledge. Unlike traditional Class-Incremental Learning (CIL), VCIL introduces the added complexity of spatiotemporal structures, making it particularly challenging to mitigate catastrophic forgetting while effectively capturing both frame-shared semantics and temporal dynamics. Existing approaches either rely on exemplar rehearsal, raising concerns over memory and privacy, or adapt static image-based methods that neglect temporal modeling. To address these limitations, we propose Spatiotemporal Preservation and Routing (StPR), a unified and exemplar-free VCIL framework that explicitly disentangles and preserves spatiotemporal information. First, we introduce Frame-Shared Semantics Distillation (FSSD), which identifies semantically stable and meaningful channels by jointly considering semantic sensitivity and classification contribution. These important semantic channels are selectively regularized to maintain prior knowledge while allowing for adaptation. Second, we design a Temporal Decomposition-based Mixture-of-Experts (TD-MoE), which dynamically routes task-specific experts based on their temporal dynamics, enabling inference without task ID or stored exemplars. Together, StPR effectively leverages spatial semantics and temporal dynamics, achieving a unified, exemplar-free VCIL framework. Extensive experiments on UCF101, HMDB51, and Kinetics400 show that our method outperforms existing baselines while offering improved interpretability and efficiency in VCIL. Code is available in the supplementary materials.",
    "metadata": {
      "arxiv_id": "2505.13997",
      "title": "StPR: Spatiotemporal Preservation and Routing for Exemplar-Free Video Class-Incremental Learning",
      "summary": "Video Class-Incremental Learning (VCIL) seeks to develop models that continuously learn new action categories over time without forgetting previously acquired knowledge. Unlike traditional Class-Incremental Learning (CIL), VCIL introduces the added complexity of spatiotemporal structures, making it particularly challenging to mitigate catastrophic forgetting while effectively capturing both frame-shared semantics and temporal dynamics. Existing approaches either rely on exemplar rehearsal, raising concerns over memory and privacy, or adapt static image-based methods that neglect temporal modeling. To address these limitations, we propose Spatiotemporal Preservation and Routing (StPR), a unified and exemplar-free VCIL framework that explicitly disentangles and preserves spatiotemporal information. First, we introduce Frame-Shared Semantics Distillation (FSSD), which identifies semantically stable and meaningful channels by jointly considering semantic sensitivity and classification contribution. These important semantic channels are selectively regularized to maintain prior knowledge while allowing for adaptation. Second, we design a Temporal Decomposition-based Mixture-of-Experts (TD-MoE), which dynamically routes task-specific experts based on their temporal dynamics, enabling inference without task ID or stored exemplars. Together, StPR effectively leverages spatial semantics and temporal dynamics, achieving a unified, exemplar-free VCIL framework. Extensive experiments on UCF101, HMDB51, and Kinetics400 show that our method outperforms existing baselines while offering improved interpretability and efficiency in VCIL. Code is available in the supplementary materials.",
      "authors": [
        "Huaijie Wang",
        "De Cheng",
        "Guozhang Li",
        "Zhipeng Xu",
        "Lingfeng He",
        "Jie Li",
        "Nannan Wang",
        "Xinbo Gao"
      ],
      "published": "2025-05-20T06:46:51Z",
      "updated": "2025-09-30T05:06:04Z",
      "categories": [
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2505.13997v2",
      "landing_url": "https://arxiv.org/abs/2505.13997v2",
      "doi": "https://doi.org/10.48550/arXiv.2505.13997"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The study focuses on video class-incremental learning and spatiotemporal modeling, with no discrete audio tokenization or speech-specific discrete symbol design, so it does not meet the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The study focuses on video class-incremental learning and spatiotemporal modeling, with no discrete audio tokenization or speech-specific discrete symbol design, so it does not meet the inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on Video Class-Incremental Learning with an emphasis on spatiotemporal preservation and routing mechanisms, without any mention or analysis of discrete audio/speech tokenization or discrete token representations for audio signals. It does not meet the inclusion criteria relating to discrete audio tokenization nor addresses any relevant exclusion criteria directly tied to audio tokenization studies.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on Video Class-Incremental Learning with an emphasis on spatiotemporal preservation and routing mechanisms, without any mention or analysis of discrete audio/speech tokenization or discrete token representations for audio signals. It does not meet the inclusion criteria relating to discrete audio tokenization nor addresses any relevant exclusion criteria directly tied to audio tokenization studies.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "PAST: Phonetic-Acoustic Speech Tokenizer",
    "abstract": "We present PAST, a novel end-to-end framework that jointly models phonetic information alongside signal reconstruction, eliminating the need for external pretrained models. Unlike previous approaches that rely on pretrained self-supervised models, PAST employs supervised phonetic data, directly integrating domain knowledge into the tokenization process via auxiliary tasks. Additionally, we introduce a streamable, causal variant of PAST, enabling real-time speech applications. Results demonstrate that PAST surpasses existing evaluated baseline tokenizers across common evaluation metrics, including phonetic representation and speech reconstruction. Notably, PAST also achieves superior performance when serving as a speech representation for speech language models, further highlighting its effectiveness as a foundation for spoken language generation. To foster further research, we release the full implementation. For code, model checkpoints, and samples see: https://pages.cs.huji.ac.il/adiyoss-lab/PAST",
    "metadata": {
      "arxiv_id": "2505.14470",
      "title": "PAST: Phonetic-Acoustic Speech Tokenizer",
      "summary": "We present PAST, a novel end-to-end framework that jointly models phonetic information alongside signal reconstruction, eliminating the need for external pretrained models. Unlike previous approaches that rely on pretrained self-supervised models, PAST employs supervised phonetic data, directly integrating domain knowledge into the tokenization process via auxiliary tasks. Additionally, we introduce a streamable, causal variant of PAST, enabling real-time speech applications. Results demonstrate that PAST surpasses existing evaluated baseline tokenizers across common evaluation metrics, including phonetic representation and speech reconstruction. Notably, PAST also achieves superior performance when serving as a speech representation for speech language models, further highlighting its effectiveness as a foundation for spoken language generation. To foster further research, we release the full implementation. For code, model checkpoints, and samples see: https://pages.cs.huji.ac.il/adiyoss-lab/PAST",
      "authors": [
        "Nadav Har-Tuv",
        "Or Tal",
        "Yossi Adi"
      ],
      "published": "2025-05-20T15:05:14Z",
      "updated": "2025-06-04T08:23:18Z",
      "categories": [
        "cs.SD",
        "cs.CL",
        "cs.LG",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2505.14470v2",
      "landing_url": "https://arxiv.org/abs/2505.14470v2",
      "doi": "https://doi.org/10.48550/arXiv.2505.14470"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "PAST explicitly models a discrete phonetic-speech tokenizer that converts continuous audio into a discrete token sequence and evaluates its usefulness for speech reconstruction and downstream language-model tasks, satisfying the inclusion criteria and not matching any exclusion criteria.",
      "evaluation": 5
    },
    "round-A_JuniorNano_reasoning": "PAST explicitly models a discrete phonetic-speech tokenizer that converts continuous audio into a discrete token sequence and evaluates its usefulness for speech reconstruction and downstream language-model tasks, satisfying the inclusion criteria and not matching any exclusion criteria.",
    "round-A_JuniorNano_evaluation": 5,
    "round-A_JuniorMini_output": {
      "reasoning": "The study proposes and evaluates a phonetic-acoustic speech tokenizer that converts continuous audio signals into discrete tokens used for speech reconstruction and language modeling, fulfilling the inclusion criteria; there is no indication of exclusion criteria being met, and the work includes methodological innovation and experimental validation, making it highly relevant for inclusion.",
      "evaluation": 5
    },
    "round-A_JuniorMini_reasoning": "The study proposes and evaluates a phonetic-acoustic speech tokenizer that converts continuous audio signals into discrete tokens used for speech reconstruction and language modeling, fulfilling the inclusion criteria; there is no indication of exclusion criteria being met, and the work includes methodological innovation and experimental validation, making it highly relevant for inclusion.",
    "round-A_JuniorMini_evaluation": 5,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "include (junior:5)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Dynadiff: Single-stage Decoding of Images from Continuously Evolving fMRI",
    "abstract": "Brain-to-image decoding has been recently propelled by the progress in generative AI models and the availability of large ultra-high field functional Magnetic Resonance Imaging (fMRI). However, current approaches depend on complicated multi-stage pipelines and preprocessing steps that typically collapse the temporal dimension of brain recordings, thereby limiting time-resolved brain decoders. Here, we introduce Dynadiff (Dynamic Neural Activity Diffusion for Image Reconstruction), a new single-stage diffusion model designed for reconstructing images from dynamically evolving fMRI recordings. Our approach offers three main contributions. First, Dynadiff simplifies training as compared to existing approaches. Second, our model outperforms state-of-the-art models on time-resolved fMRI signals, especially on high-level semantic image reconstruction metrics, while remaining competitive on preprocessed fMRI data that collapse time. Third, this approach allows a precise characterization of the evolution of image representations in brain activity. Overall, this work lays the foundation for time-resolved brain-to-image decoding.",
    "metadata": {
      "arxiv_id": "2505.14556",
      "title": "Dynadiff: Single-stage Decoding of Images from Continuously Evolving fMRI",
      "summary": "Brain-to-image decoding has been recently propelled by the progress in generative AI models and the availability of large ultra-high field functional Magnetic Resonance Imaging (fMRI). However, current approaches depend on complicated multi-stage pipelines and preprocessing steps that typically collapse the temporal dimension of brain recordings, thereby limiting time-resolved brain decoders. Here, we introduce Dynadiff (Dynamic Neural Activity Diffusion for Image Reconstruction), a new single-stage diffusion model designed for reconstructing images from dynamically evolving fMRI recordings. Our approach offers three main contributions. First, Dynadiff simplifies training as compared to existing approaches. Second, our model outperforms state-of-the-art models on time-resolved fMRI signals, especially on high-level semantic image reconstruction metrics, while remaining competitive on preprocessed fMRI data that collapse time. Third, this approach allows a precise characterization of the evolution of image representations in brain activity. Overall, this work lays the foundation for time-resolved brain-to-image decoding.",
      "authors": [
        "Marlène Careil",
        "Yohann Benchetrit",
        "Jean-Rémi King"
      ],
      "published": "2025-05-20T16:14:37Z",
      "updated": "2025-05-20T16:14:37Z",
      "categories": [
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2505.14556v1",
      "landing_url": "https://arxiv.org/abs/2505.14556v1",
      "doi": "https://doi.org/10.48550/arXiv.2505.14556"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item focuses on brain-to-image decoding from evolving fMRI signals and does not address discrete audio/speech tokenization, so it does not meet the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item focuses on brain-to-image decoding from evolving fMRI signals and does not address discrete audio/speech tokenization, so it does not meet the inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on brain-to-image decoding from fMRI signals and proposes a new diffusion model for image reconstruction. It does not address discrete audio/speech tokenization or any audio-related tokenization concepts; hence, it does not meet the inclusion criteria focused on audio tokenization and is unrelated to the specified domain.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on brain-to-image decoding from fMRI signals and proposes a new diffusion model for image reconstruction. It does not address discrete audio/speech tokenization or any audio-related tokenization concepts; hence, it does not meet the inclusion criteria focused on audio tokenization and is unrelated to the specified domain.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Discrete Audio Representations for Automated Audio Captioning",
    "abstract": "Discrete audio representations, termed audio tokens, are broadly categorized into semantic and acoustic tokens, typically generated through unsupervised tokenization of continuous audio representations. However, their applicability to automated audio captioning (AAC) remains underexplored. This paper systematically investigates the viability of audio token-driven models for AAC through comparative analyses of various tokenization methods. Our findings reveal that audio tokenization leads to performance degradation in AAC models compared to those that directly utilize continuous audio representations. To address this issue, we introduce a supervised audio tokenizer trained with an audio tagging objective. Unlike unsupervised tokenizers, which lack explicit semantic understanding, the proposed tokenizer effectively captures audio event information. Experiments conducted on the Clotho dataset demonstrate that the proposed audio tokens outperform conventional audio tokens in the AAC task.",
    "metadata": {
      "arxiv_id": "2505.14989",
      "title": "Discrete Audio Representations for Automated Audio Captioning",
      "summary": "Discrete audio representations, termed audio tokens, are broadly categorized into semantic and acoustic tokens, typically generated through unsupervised tokenization of continuous audio representations. However, their applicability to automated audio captioning (AAC) remains underexplored. This paper systematically investigates the viability of audio token-driven models for AAC through comparative analyses of various tokenization methods. Our findings reveal that audio tokenization leads to performance degradation in AAC models compared to those that directly utilize continuous audio representations. To address this issue, we introduce a supervised audio tokenizer trained with an audio tagging objective. Unlike unsupervised tokenizers, which lack explicit semantic understanding, the proposed tokenizer effectively captures audio event information. Experiments conducted on the Clotho dataset demonstrate that the proposed audio tokens outperform conventional audio tokens in the AAC task.",
      "authors": [
        "Jingguang Tian",
        "Haoqin Sun",
        "Xinhui Hu",
        "Xinkang Xu"
      ],
      "published": "2025-05-21T00:27:38Z",
      "updated": "2025-05-21T00:27:38Z",
      "categories": [
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2505.14989v1",
      "landing_url": "https://arxiv.org/abs/2505.14989v1",
      "doi": "https://doi.org/10.48550/arXiv.2505.14989"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "It centers on discrete audio tokenization as the core object, proposes and evaluates a supervised tokenizer, and tests token-based AAC performance, with comparisons to continuous representations, meeting all inclusion criteria and avoiding exclusions.",
      "evaluation": 5
    },
    "round-A_JuniorNano_reasoning": "It centers on discrete audio tokenization as the core object, proposes and evaluates a supervised tokenizer, and tests token-based AAC performance, with comparisons to continuous representations, meeting all inclusion criteria and avoiding exclusions.",
    "round-A_JuniorNano_evaluation": 5,
    "round-A_JuniorMini_output": {
      "reasoning": "The study explicitly proposes and analyzes discrete audio tokenization methods for automated audio captioning, focusing on the conversion of continuous audio signals into discrete tokens for understanding tasks, meeting the inclusion criteria; it also involves supervised and unsupervised tokenizers with experimental validation, and does not fall under any exclusion criteria.",
      "evaluation": 5
    },
    "round-A_JuniorMini_reasoning": "The study explicitly proposes and analyzes discrete audio tokenization methods for automated audio captioning, focusing on the conversion of continuous audio signals into discrete tokens for understanding tasks, meeting the inclusion criteria; it also involves supervised and unsupervised tokenizers with experimental validation, and does not fall under any exclusion criteria.",
    "round-A_JuniorMini_evaluation": 5,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "include (junior:5)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Discrete Tokens Exhibit Interlanguage Speech Intelligibility Benefit: an Analytical Study Towards Accent-robust ASR Only with Native Speech Data",
    "abstract": "In this study, we gained insight that contributes to achieving accent-robust ASR using only native speech data. In human perception of non-native speech, the phenomenon known as \"interlanguage speech intelligibility benefit\" (ISIB) is observed, where non-native listeners who share the native language with the speaker understand the speech better compared even to native listeners. Based on the idea that discrete tokens extracted from self-supervised learning (SSL) models represent the human perception of speech, we conducted an analytical study on the robustness of discrete token-based ASR to non-native speech, varying the language used for training the tokenization, which is viewed as a technical implementation of ISIB. The results showed that ISIB actually occurred in the discrete token-based ASR. Since our approach relies only on native speech data to simulate the behavior of human perception, it is expected to be applicable to a wide range of accents for which speech data is scarce.",
    "metadata": {
      "arxiv_id": "2505.16182",
      "title": "Discrete Tokens Exhibit Interlanguage Speech Intelligibility Benefit: an Analytical Study Towards Accent-robust ASR Only with Native Speech Data",
      "summary": "In this study, we gained insight that contributes to achieving accent-robust ASR using only native speech data. In human perception of non-native speech, the phenomenon known as \"interlanguage speech intelligibility benefit\" (ISIB) is observed, where non-native listeners who share the native language with the speaker understand the speech better compared even to native listeners. Based on the idea that discrete tokens extracted from self-supervised learning (SSL) models represent the human perception of speech, we conducted an analytical study on the robustness of discrete token-based ASR to non-native speech, varying the language used for training the tokenization, which is viewed as a technical implementation of ISIB. The results showed that ISIB actually occurred in the discrete token-based ASR. Since our approach relies only on native speech data to simulate the behavior of human perception, it is expected to be applicable to a wide range of accents for which speech data is scarce.",
      "authors": [
        "Kentaro Onda",
        "Keisuke Imoto",
        "Satoru Fukayama",
        "Daisuke Saito",
        "Nobuaki Minematsu"
      ],
      "published": "2025-05-22T03:36:28Z",
      "updated": "2025-05-22T03:36:28Z",
      "categories": [
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2505.16182v1",
      "landing_url": "https://arxiv.org/abs/2505.16182v1",
      "doi": "https://doi.org/10.48550/arXiv.2505.16182"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The article explicitly focuses on discrete token-based ASR, using discrete tokens from SSL to represent and process speech, and analyzes robustness to non-native speech (ISIB) with native data—satisfying inclusion and not falling into exclusion categories.",
      "evaluation": 5
    },
    "round-A_JuniorNano_reasoning": "The article explicitly focuses on discrete token-based ASR, using discrete tokens from SSL to represent and process speech, and analyzes robustness to non-native speech (ISIB) with native data—satisfying inclusion and not falling into exclusion categories.",
    "round-A_JuniorNano_evaluation": 5,
    "round-A_JuniorMini_output": {
      "reasoning": "The study explicitly analyzes discrete token-based automatic speech recognition (ASR), focusing on discrete tokens extracted from self-supervised learning models as a representation of speech perception and their robustness to non-native speech accents. It meets the inclusion criteria by proposing and analyzing discrete audio tokenization for speech understanding tasks and does not fall under any exclusion criteria such as relying solely on continuous features or being an engineering-only report. Therefore, it aligns well with the required criteria for inclusion.",
      "evaluation": 5
    },
    "round-A_JuniorMini_reasoning": "The study explicitly analyzes discrete token-based automatic speech recognition (ASR), focusing on discrete tokens extracted from self-supervised learning models as a representation of speech perception and their robustness to non-native speech accents. It meets the inclusion criteria by proposing and analyzing discrete audio tokenization for speech understanding tasks and does not fall under any exclusion criteria such as relying solely on continuous features or being an engineering-only report. Therefore, it aligns well with the required criteria for inclusion.",
    "round-A_JuniorMini_evaluation": 5,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "include (junior:5)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Performance of Objective Speech Quality Metrics on Languages Beyond Validation Data: A Study of Turkish and Korean",
    "abstract": "Objective speech quality measures are widely used to assess the performance of video conferencing platforms and telecommunication systems. They predict human-rated speech quality and are crucial for assessing the systems quality of experience. Despite the widespread use, the quality measures are developed on a limited set of languages. This can be problematic since the performance on unseen languages is consequently not guaranteed or even studied. Here we raise awareness to this issue by investigating the performance of two objective speech quality measures (PESQ and ViSQOL) on Turkish and Korean. Using English as baseline, we show that Turkish samples have significantly higher ViSQOL scores and that for Turkish male speakers the correlation between PESQ and ViSQOL is highest. These results highlight the need to explore biases across metrics and to develop a labeled speech quality dataset with a variety of languages.",
    "metadata": {
      "arxiv_id": "2505.16616",
      "title": "Performance of Objective Speech Quality Metrics on Languages Beyond Validation Data: A Study of Turkish and Korean",
      "summary": "Objective speech quality measures are widely used to assess the performance of video conferencing platforms and telecommunication systems. They predict human-rated speech quality and are crucial for assessing the systems quality of experience. Despite the widespread use, the quality measures are developed on a limited set of languages. This can be problematic since the performance on unseen languages is consequently not guaranteed or even studied. Here we raise awareness to this issue by investigating the performance of two objective speech quality measures (PESQ and ViSQOL) on Turkish and Korean. Using English as baseline, we show that Turkish samples have significantly higher ViSQOL scores and that for Turkish male speakers the correlation between PESQ and ViSQOL is highest. These results highlight the need to explore biases across metrics and to develop a labeled speech quality dataset with a variety of languages.",
      "authors": [
        "Javier Perez",
        "Dimme de Groot",
        "Jorge Martinez"
      ],
      "published": "2025-05-22T12:50:32Z",
      "updated": "2025-05-22T12:50:32Z",
      "categories": [
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2505.16616v1",
      "landing_url": "https://arxiv.org/abs/2505.16616v1",
      "doi": "https://doi.org/10.48550/arXiv.2505.16616"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The article evaluates objective speech quality metrics across Turkish and Korean but does not involve discrete audio tokenization; there is no analysis or proposal of quantized discrete tokens for speech reconstruction/understanding, so it fails the inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The article evaluates objective speech quality metrics across Turkish and Korean but does not involve discrete audio tokenization; there is no analysis or proposal of quantized discrete tokens for speech reconstruction/understanding, so it fails the inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study evaluates objective speech quality metrics (PESQ and ViSQOL) on different languages but does not propose, analyze, or evaluate any discrete audio/speech tokenization methods, nor does it involve conversion of continuous signals into discrete tokens; hence, it does not meet the inclusion criteria focused on discrete tokenization.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study evaluates objective speech quality metrics (PESQ and ViSQOL) on different languages but does not propose, analyze, or evaluate any discrete audio/speech tokenization methods, nor does it involve conversion of continuous signals into discrete tokens; hence, it does not meet the inclusion criteria focused on discrete tokenization.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "One-Step Diffusion-Based Image Compression with Semantic Distillation",
    "abstract": "While recent diffusion-based generative image codecs have shown impressive performance, their iterative sampling process introduces unpleasing latency. In this work, we revisit the design of a diffusion-based codec and argue that multi-step sampling is not necessary for generative compression. Based on this insight, we propose OneDC, a One-step Diffusion-based generative image Codec -- that integrates a latent compression module with a one-step diffusion generator. Recognizing the critical role of semantic guidance in one-step diffusion, we propose using the hyperprior as a semantic signal, overcoming the limitations of text prompts in representing complex visual content. To further enhance the semantic capability of the hyperprior, we introduce a semantic distillation mechanism that transfers knowledge from a pretrained generative tokenizer to the hyperprior codec. Additionally, we adopt a hybrid pixel- and latent-domain optimization to jointly enhance both reconstruction fidelity and perceptual realism. Extensive experiments demonstrate that OneDC achieves SOTA perceptual quality even with one-step generation, offering over 39% bitrate reduction and 20x faster decoding compared to prior multi-step diffusion-based codecs. Project: https://onedc-codec.github.io/",
    "metadata": {
      "arxiv_id": "2505.16687",
      "title": "One-Step Diffusion-Based Image Compression with Semantic Distillation",
      "summary": "While recent diffusion-based generative image codecs have shown impressive performance, their iterative sampling process introduces unpleasing latency. In this work, we revisit the design of a diffusion-based codec and argue that multi-step sampling is not necessary for generative compression. Based on this insight, we propose OneDC, a One-step Diffusion-based generative image Codec -- that integrates a latent compression module with a one-step diffusion generator. Recognizing the critical role of semantic guidance in one-step diffusion, we propose using the hyperprior as a semantic signal, overcoming the limitations of text prompts in representing complex visual content. To further enhance the semantic capability of the hyperprior, we introduce a semantic distillation mechanism that transfers knowledge from a pretrained generative tokenizer to the hyperprior codec. Additionally, we adopt a hybrid pixel- and latent-domain optimization to jointly enhance both reconstruction fidelity and perceptual realism. Extensive experiments demonstrate that OneDC achieves SOTA perceptual quality even with one-step generation, offering over 39% bitrate reduction and 20x faster decoding compared to prior multi-step diffusion-based codecs. Project: https://onedc-codec.github.io/",
      "authors": [
        "Naifu Xue",
        "Zhaoyang Jia",
        "Jiahao Li",
        "Bin Li",
        "Yuan Zhang",
        "Yan Lu"
      ],
      "published": "2025-05-22T13:54:09Z",
      "updated": "2025-11-26T13:32:20Z",
      "categories": [
        "cs.CV",
        "eess.IV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2505.16687v2",
      "landing_url": "https://arxiv.org/abs/2505.16687v2",
      "doi": "https://doi.org/10.48550/arXiv.2505.16687"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The article focuses on diffusion-based image compression and semantic distillation for images, with no discrete audio/speech tokenization analysis, so it does not meet the inclusion criteria for discrete audio tokenization.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The article focuses on diffusion-based image compression and semantic distillation for images, with no discrete audio/speech tokenization analysis, so it does not meet the inclusion criteria for discrete audio tokenization.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on one-step diffusion-based generative image compression with semantic distillation and does not target audio or speech tokenization. It does not discuss discrete audio tokenization or quantization methods related to audio signals, thus it fails to meet the inclusion criteria related to audio/speech tokenization.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on one-step diffusion-based generative image compression with semantic distillation and does not target audio or speech tokenization. It does not discuss discrete audio tokenization or quantization methods related to audio signals, thus it fails to meet the inclusion criteria related to audio/speech tokenization.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Unlocking Temporal Flexibility: Neural Speech Codec with Variable Frame Rate",
    "abstract": "Most neural speech codecs achieve bitrate adjustment through intra-frame mechanisms, such as codebook dropout, at a Constant Frame Rate (CFR). However, speech segments inherently have time-varying information density (e.g., silent intervals versus voiced regions). This property makes CFR not optimal in terms of bitrate and token sequence length, hindering efficiency in real-time applications. In this work, we propose a Temporally Flexible Coding (TFC) technique, introducing variable frame rate (VFR) into neural speech codecs for the first time. TFC enables seamlessly tunable average frame rates and dynamically allocates frame rates based on temporal entropy. Experimental results show that a codec with TFC achieves optimal reconstruction quality with high flexibility, and maintains competitive performance even at lower frame rates. Our approach is promising for the integration with other efforts to develop low-frame-rate neural speech codecs for more efficient downstream tasks.",
    "metadata": {
      "arxiv_id": "2505.16845",
      "title": "Unlocking Temporal Flexibility: Neural Speech Codec with Variable Frame Rate",
      "summary": "Most neural speech codecs achieve bitrate adjustment through intra-frame mechanisms, such as codebook dropout, at a Constant Frame Rate (CFR). However, speech segments inherently have time-varying information density (e.g., silent intervals versus voiced regions). This property makes CFR not optimal in terms of bitrate and token sequence length, hindering efficiency in real-time applications. In this work, we propose a Temporally Flexible Coding (TFC) technique, introducing variable frame rate (VFR) into neural speech codecs for the first time. TFC enables seamlessly tunable average frame rates and dynamically allocates frame rates based on temporal entropy. Experimental results show that a codec with TFC achieves optimal reconstruction quality with high flexibility, and maintains competitive performance even at lower frame rates. Our approach is promising for the integration with other efforts to develop low-frame-rate neural speech codecs for more efficient downstream tasks.",
      "authors": [
        "Hanglei Zhang",
        "Yiwei Guo",
        "Zhihan Li",
        "Xiang Hao",
        "Xie Chen",
        "Kai Yu"
      ],
      "published": "2025-05-22T16:10:01Z",
      "updated": "2025-05-22T16:10:01Z",
      "categories": [
        "eess.AS",
        "cs.AI",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2505.16845v1",
      "landing_url": "https://arxiv.org/abs/2505.16845v1",
      "doi": "https://doi.org/10.48550/arXiv.2505.16845"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The abstract centers on variable frame rate in neural speech codecs and temporal entropy, but does not explicitly propose or analyze discrete audio/speech tokenization (discrete symbolic representations) as the core object, nor discuss token quantization for speech reconstruction; thus it does not meet the inclusion criteria and is better to exclude.",
      "evaluation": 2
    },
    "round-A_JuniorNano_reasoning": "The abstract centers on variable frame rate in neural speech codecs and temporal entropy, but does not explicitly propose or analyze discrete audio/speech tokenization (discrete symbolic representations) as the core object, nor discuss token quantization for speech reconstruction; thus it does not meet the inclusion criteria and is better to exclude.",
    "round-A_JuniorNano_evaluation": 2,
    "round-A_JuniorMini_output": {
      "reasoning": "The study proposes a neural speech codec with variable frame rate that improves efficiency by dynamically allocating frame rates based on temporal entropy, which suggests a novel neural encoding method potentially involving discrete tokenization of audio frames, although the abstract does not explicitly confirm discrete tokenization usage or analysis as the core object of study; hence, it somewhat aligns with inclusion criteria but lacks full clarity.",
      "evaluation": 3
    },
    "round-A_JuniorMini_reasoning": "The study proposes a neural speech codec with variable frame rate that improves efficiency by dynamically allocating frame rates based on temporal entropy, which suggests a novel neural encoding method potentially involving discrete tokenization of audio frames, although the abstract does not explicitly confirm discrete tokenization usage or analysis as the core object of study; hence, it somewhat aligns with inclusion criteria but lacks full clarity.",
    "round-A_JuniorMini_evaluation": 3,
    "round-B_SeniorLead_output": {
      "reasoning": "Junior reviewers: one recommended exclusion and the other was uncertain. The abstract focuses on variable frame-rate allocation in neural speech codecs and mentions intra-frame mechanisms (e.g., codebook dropout) but does not explicitly propose, analyze, or evaluate discrete audio/speech tokenization (conversion of continuous audio into sequences of discrete symbols) as the core contribution required by the inclusion criteria. Therefore it does not meet the inclusion requirement for explicit discrete-token design/analysis and should be excluded. Score rationale: lacks explicit discrete quantization/tokenization focus.",
      "evaluation": 2
    },
    "round-B_SeniorLead_reasoning": "Junior reviewers: one recommended exclusion and the other was uncertain. The abstract focuses on variable frame-rate allocation in neural speech codecs and mentions intra-frame mechanisms (e.g., codebook dropout) but does not explicitly propose, analyze, or evaluate discrete audio/speech tokenization (conversion of continuous audio into sequences of discrete symbols) as the core contribution required by the inclusion criteria. Therefore it does not meet the inclusion requirement for explicit discrete-token design/analysis and should be excluded. Score rationale: lacks explicit discrete quantization/tokenization focus.",
    "round-B_SeniorLead_evaluation": 2,
    "final_verdict": "exclude (senior:2)",
    "review_skipped": false,
    "discard_reason": "exclude (senior:2)"
  },
  {
    "title": "Impact of Frame Rates on Speech Tokenizer: A Case Study on Mandarin and English",
    "abstract": "The speech tokenizer plays a crucial role in recent speech tasks, generally serving as a bridge between speech signals and language models. While low-frame-rate codecs are widely employed as speech tokenizers, the impact of frame rates on speech tokens remains underexplored. In this study, we investigate how varying frame rates affect speech tokenization by examining Mandarin and English, two typologically distinct languages. We encode speech at different frame rates and evaluate the resulting semantic tokens in the speech recognition task. Our findings reveal that frame rate variations influence speech tokenization differently for each language, highlighting the interplay between frame rates, phonetic density, and language-specific acoustic features. The results provide insights into optimizing frame rate selection for speech tokenizers, with implications for automatic speech recognition, text-to-speech, and other speech-related applications.",
    "metadata": {
      "arxiv_id": "2505.17076",
      "title": "Impact of Frame Rates on Speech Tokenizer: A Case Study on Mandarin and English",
      "summary": "The speech tokenizer plays a crucial role in recent speech tasks, generally serving as a bridge between speech signals and language models. While low-frame-rate codecs are widely employed as speech tokenizers, the impact of frame rates on speech tokens remains underexplored. In this study, we investigate how varying frame rates affect speech tokenization by examining Mandarin and English, two typologically distinct languages. We encode speech at different frame rates and evaluate the resulting semantic tokens in the speech recognition task. Our findings reveal that frame rate variations influence speech tokenization differently for each language, highlighting the interplay between frame rates, phonetic density, and language-specific acoustic features. The results provide insights into optimizing frame rate selection for speech tokenizers, with implications for automatic speech recognition, text-to-speech, and other speech-related applications.",
      "authors": [
        "Haoyang Zhang",
        "Hexin Liu",
        "Xiangyu Zhang",
        "Qiquan Zhang",
        "Yuchen Hu",
        "Junqi Zhao",
        "Fei Tian",
        "Xuerui Yang",
        "Leibny Paola Garcia",
        "Eng Siong Chng"
      ],
      "published": "2025-05-20T06:01:19Z",
      "updated": "2025-06-13T17:21:25Z",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2505.17076v3",
      "landing_url": "https://arxiv.org/abs/2505.17076v3",
      "doi": "https://doi.org/10.48550/arXiv.2505.17076"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The abstract explicitly studies discrete speech tokenization by encoding speech at different frame rates into tokens and evaluating semantic tokens in ASR, aligning with the inclusion criteria and not merely focusing on continuous features or non-speech tokenization.",
      "evaluation": 5
    },
    "round-A_JuniorNano_reasoning": "The abstract explicitly studies discrete speech tokenization by encoding speech at different frame rates into tokens and evaluating semantic tokens in ASR, aligning with the inclusion criteria and not merely focusing on continuous features or non-speech tokenization.",
    "round-A_JuniorNano_evaluation": 5,
    "round-A_JuniorMini_output": {
      "reasoning": "The study explicitly investigates the impact of frame rates on speech tokenization, analyzing how continuous speech signals are converted into discrete semantic tokens for Mandarin and English, which aligns with the inclusion criteria of proposing and evaluating discrete audio tokenization for speech tasks. There is no indication that the study focuses solely on traditional signal-processing codecs or continuous audio features without discrete token analysis. Therefore, it meets all inclusion criteria and none of the exclusion criteria.",
      "evaluation": 5
    },
    "round-A_JuniorMini_reasoning": "The study explicitly investigates the impact of frame rates on speech tokenization, analyzing how continuous speech signals are converted into discrete semantic tokens for Mandarin and English, which aligns with the inclusion criteria of proposing and evaluating discrete audio tokenization for speech tasks. There is no indication that the study focuses solely on traditional signal-processing codecs or continuous audio features without discrete token analysis. Therefore, it meets all inclusion criteria and none of the exclusion criteria.",
    "round-A_JuniorMini_evaluation": 5,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "include (junior:5)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Exploring the Effect of Segmentation and Vocabulary Size on Speech Tokenization for Speech Language Models",
    "abstract": "The purpose of speech tokenization is to transform a speech signal into a sequence of discrete representations, serving as the foundation for speech language models (SLMs). While speech tokenization has many options, their effect on the performance of SLMs remains unclear. This paper investigates two key aspects of speech tokenization: the segmentation width and the cluster size of discrete units. First, we segment speech signals into fixed/variable widths and pooled representations. We then train K-means models in multiple cluster sizes. Through the evaluation on zero-shot spoken language understanding benchmarks, we find the positive effect of moderately coarse segmentation and bigger cluster size. Notably, among the best-performing models, the most efficient one achieves a 50% reduction in training data and a 70% decrease in training runtime. Our analysis highlights the importance of combining multiple tokens to enhance fine-grained spoken language understanding.",
    "metadata": {
      "arxiv_id": "2505.17446",
      "title": "Exploring the Effect of Segmentation and Vocabulary Size on Speech Tokenization for Speech Language Models",
      "summary": "The purpose of speech tokenization is to transform a speech signal into a sequence of discrete representations, serving as the foundation for speech language models (SLMs). While speech tokenization has many options, their effect on the performance of SLMs remains unclear. This paper investigates two key aspects of speech tokenization: the segmentation width and the cluster size of discrete units. First, we segment speech signals into fixed/variable widths and pooled representations. We then train K-means models in multiple cluster sizes. Through the evaluation on zero-shot spoken language understanding benchmarks, we find the positive effect of moderately coarse segmentation and bigger cluster size. Notably, among the best-performing models, the most efficient one achieves a 50% reduction in training data and a 70% decrease in training runtime. Our analysis highlights the importance of combining multiple tokens to enhance fine-grained spoken language understanding.",
      "authors": [
        "Shunsuke Kando",
        "Yusuke Miyao",
        "Shinnosuke Takamichi"
      ],
      "published": "2025-05-23T04:03:27Z",
      "updated": "2025-05-31T13:32:13Z",
      "categories": [
        "cs.CL",
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2505.17446v2",
      "landing_url": "https://arxiv.org/abs/2505.17446v2",
      "doi": "https://doi.org/10.48550/arXiv.2505.17446"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The study explicitly investigates discrete speech tokenization (segmentation and discrete unit clustering) as a core object, using finite-symbol representations for speech-language tasks, which meets the inclusion criteria and avoids the exclusions.",
      "evaluation": 5
    },
    "round-A_JuniorNano_reasoning": "The study explicitly investigates discrete speech tokenization (segmentation and discrete unit clustering) as a core object, using finite-symbol representations for speech-language tasks, which meets the inclusion criteria and avoids the exclusions.",
    "round-A_JuniorNano_evaluation": 5,
    "round-A_JuniorMini_output": {
      "reasoning": "The study explicitly focuses on analyzing different speech tokenization strategies, investigating segmentation widths and discrete cluster sizes for speech tokenization, and their impact on speech language model performance, fully aligning with the inclusion criteria and none of the exclusion criteria appear relevant based on the given abstract.",
      "evaluation": 5
    },
    "round-A_JuniorMini_reasoning": "The study explicitly focuses on analyzing different speech tokenization strategies, investigating segmentation widths and discrete cluster sizes for speech tokenization, and their impact on speech language model performance, fully aligning with the inclusion criteria and none of the exclusion criteria appear relevant based on the given abstract.",
    "round-A_JuniorMini_evaluation": 5,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "include (junior:5)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Reverse-Speech-Finder: A Neural Network Backtracking Architecture for Generating Alzheimer's Disease Speech Samples and Improving Diagnosis Performance",
    "abstract": "This study introduces Reverse-Speech-Finder (RSF), a groundbreaking neural network backtracking architecture designed to enhance Alzheimer's Disease (AD) diagnosis through speech analysis. Leveraging the power of pre-trained large language models, RSF identifies and utilizes the most probable AD-specific speech markers, addressing both the scarcity of real AD speech samples and the challenge of limited interpretability in existing models. RSF's unique approach consists of three core innovations: Firstly, it exploits the observation that speech markers most probable of predicting AD, defined as the most probable speech-markers (MPMs), must have the highest probability of activating those neurons (in the neural network) with the highest probability of predicting AD, defined as the most probable neurons (MPNs). Secondly, it utilizes a speech token representation at the input layer, allowing backtracking from MPNs to identify the most probable speech-tokens (MPTs) of AD. Lastly, it develops an innovative backtracking method to track backwards from the MPNs to the input layer, identifying the MPTs and the corresponding MPMs, and ingeniously uncovering novel speech markers for AD detection. Experimental results demonstrate RSF's superiority over traditional methods such as SHAP and Integrated Gradients, achieving a 3.5% improvement in accuracy and a 3.2% boost in F1-score. By generating speech data that encapsulates novel markers, RSF not only mitigates the limitations of real data scarcity but also significantly enhances the robustness and accuracy of AD diagnostic models. These findings underscore RSF's potential as a transformative tool in speech-based AD detection, offering new insights into AD-related linguistic deficits and paving the way for more effective non-invasive early intervention strategies.",
    "metadata": {
      "arxiv_id": "2505.17477",
      "title": "Reverse-Speech-Finder: A Neural Network Backtracking Architecture for Generating Alzheimer's Disease Speech Samples and Improving Diagnosis Performance",
      "summary": "This study introduces Reverse-Speech-Finder (RSF), a groundbreaking neural network backtracking architecture designed to enhance Alzheimer's Disease (AD) diagnosis through speech analysis. Leveraging the power of pre-trained large language models, RSF identifies and utilizes the most probable AD-specific speech markers, addressing both the scarcity of real AD speech samples and the challenge of limited interpretability in existing models. RSF's unique approach consists of three core innovations: Firstly, it exploits the observation that speech markers most probable of predicting AD, defined as the most probable speech-markers (MPMs), must have the highest probability of activating those neurons (in the neural network) with the highest probability of predicting AD, defined as the most probable neurons (MPNs). Secondly, it utilizes a speech token representation at the input layer, allowing backtracking from MPNs to identify the most probable speech-tokens (MPTs) of AD. Lastly, it develops an innovative backtracking method to track backwards from the MPNs to the input layer, identifying the MPTs and the corresponding MPMs, and ingeniously uncovering novel speech markers for AD detection. Experimental results demonstrate RSF's superiority over traditional methods such as SHAP and Integrated Gradients, achieving a 3.5% improvement in accuracy and a 3.2% boost in F1-score. By generating speech data that encapsulates novel markers, RSF not only mitigates the limitations of real data scarcity but also significantly enhances the robustness and accuracy of AD diagnostic models. These findings underscore RSF's potential as a transformative tool in speech-based AD detection, offering new insights into AD-related linguistic deficits and paving the way for more effective non-invasive early intervention strategies.",
      "authors": [
        "Victor OK Li",
        "Yang Han",
        "Jacqueline CK Lam",
        "Lawrence YL Cheung"
      ],
      "published": "2025-05-23T04:59:27Z",
      "updated": "2025-05-23T04:59:27Z",
      "categories": [
        "cs.LG",
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2505.17477v1",
      "landing_url": "https://arxiv.org/abs/2505.17477v1",
      "doi": "https://doi.org/10.48550/arXiv.2505.17477"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The work explicitly uses a discrete speech-token framework (MPTs/MPMs) and a token-representation at the input layer with backtracking to identify discrete speech tokens, satisfying the inclusion criterion while not representing a study of only continuous features.",
      "evaluation": 5
    },
    "round-A_JuniorNano_reasoning": "The work explicitly uses a discrete speech-token framework (MPTs/MPMs) and a token-representation at the input layer with backtracking to identify discrete speech tokens, satisfying the inclusion criterion while not representing a study of only continuous features.",
    "round-A_JuniorNano_evaluation": 5,
    "round-A_JuniorMini_output": {
      "reasoning": "The abstract describes a neural network method that identifies speech markers for Alzheimer's disease, using speech token representations and backtracking from neurons to input speech tokens. However, the focus is on identifying probable speech markers for AD, not explicitly proposing, analyzing, or evaluating discrete audio/speech tokenization for reconstructing or generating audio from finite vocabularies. The discrete tokens here appear to be speech tokens at a linguistic level rather than the discrete audio tokens required by the criteria. There is no clear mention of quantization-based discrete audio tokenization or modern neural audio codec-like representations. Therefore, it does not fully meet the inclusion criterion of discrete audio/speech tokenization as the core object of study and appears more focused on linguistic speech tokens in the context of AD diagnosis rather than acoustic-level discrete tokenization and generation or reconstruction tasks.",
      "evaluation": 2
    },
    "round-A_JuniorMini_reasoning": "The abstract describes a neural network method that identifies speech markers for Alzheimer's disease, using speech token representations and backtracking from neurons to input speech tokens. However, the focus is on identifying probable speech markers for AD, not explicitly proposing, analyzing, or evaluating discrete audio/speech tokenization for reconstructing or generating audio from finite vocabularies. The discrete tokens here appear to be speech tokens at a linguistic level rather than the discrete audio tokens required by the criteria. There is no clear mention of quantization-based discrete audio tokenization or modern neural audio codec-like representations. Therefore, it does not fully meet the inclusion criterion of discrete audio/speech tokenization as the core object of study and appears more focused on linguistic speech tokens in the context of AD diagnosis rather than acoustic-level discrete tokenization and generation or reconstruction tasks.",
    "round-A_JuniorMini_evaluation": 2,
    "round-B_SeniorLead_output": {
      "reasoning": "Both junior reviewers raise valid points: one interprets the \"speech-token representation\" as meeting the discrete-token criterion, while the other correctly notes the abstract lacks any explicit description of quantization, neural audio codecs, finite-vocabulary discrete audio tokens, or audio reconstruction from token sequences. The abstract focuses on identifying AD-related \"speech markers\" and improving diagnosis, and while it mentions generating speech data and backtracking to \"speech-tokens (MPTs)\", it does not explicitly propose, analyze, or evaluate a discrete audio/speech tokenization scheme (e.g., vector quantization, codec-like discrete vocabularies, or token-based audio reconstruction/generation). Because the inclusion criteria require an explicit, central treatment of discrete audio/speech tokenization converting continuous audio into finite-symbol sequences for reconstruction/generation/understanding, this item does not meet the requirement and should be excluded.",
      "evaluation": 2
    },
    "round-B_SeniorLead_reasoning": "Both junior reviewers raise valid points: one interprets the \"speech-token representation\" as meeting the discrete-token criterion, while the other correctly notes the abstract lacks any explicit description of quantization, neural audio codecs, finite-vocabulary discrete audio tokens, or audio reconstruction from token sequences. The abstract focuses on identifying AD-related \"speech markers\" and improving diagnosis, and while it mentions generating speech data and backtracking to \"speech-tokens (MPTs)\", it does not explicitly propose, analyze, or evaluate a discrete audio/speech tokenization scheme (e.g., vector quantization, codec-like discrete vocabularies, or token-based audio reconstruction/generation). Because the inclusion criteria require an explicit, central treatment of discrete audio/speech tokenization converting continuous audio into finite-symbol sequences for reconstruction/generation/understanding, this item does not meet the requirement and should be excluded.",
    "round-B_SeniorLead_evaluation": 2,
    "final_verdict": "exclude (senior:2)",
    "review_skipped": false,
    "discard_reason": "exclude (senior:2)"
  },
  {
    "title": "Adaptive Semantic Token Communication for Transformer-based Edge Inference",
    "abstract": "This paper presents an adaptive framework for edge inference based on a dynamically configurable transformer-powered deep joint source channel coding (DJSCC) architecture. Motivated by a practical scenario where a resource constrained edge device engages in goal oriented semantic communication, such as selectively transmitting essential features for object detection to an edge server, our approach enables efficient task aware data transmission under varying bandwidth and channel conditions. To achieve this, input data is tokenized into compact high level semantic representations, refined by a transformer, and transmitted over noisy wireless channels. As part of the DJSCC pipeline, we employ a semantic token selection mechanism that adaptively compresses informative features into a user specified number of tokens per sample. These tokens are then further compressed through the JSCC module, enabling a flexible token communication strategy that adjusts both the number of transmitted tokens and their embedding dimensions. We incorporate a resource allocation algorithm based on Lyapunov stochastic optimization to enhance robustness under dynamic network conditions, effectively balancing compression efficiency and task performance. Experimental results demonstrate that our system consistently outperforms existing baselines, highlighting its potential as a strong foundation for AI native semantic communication in edge intelligence applications.",
    "metadata": {
      "arxiv_id": "2505.17604",
      "title": "Adaptive Semantic Token Communication for Transformer-based Edge Inference",
      "summary": "This paper presents an adaptive framework for edge inference based on a dynamically configurable transformer-powered deep joint source channel coding (DJSCC) architecture. Motivated by a practical scenario where a resource constrained edge device engages in goal oriented semantic communication, such as selectively transmitting essential features for object detection to an edge server, our approach enables efficient task aware data transmission under varying bandwidth and channel conditions. To achieve this, input data is tokenized into compact high level semantic representations, refined by a transformer, and transmitted over noisy wireless channels. As part of the DJSCC pipeline, we employ a semantic token selection mechanism that adaptively compresses informative features into a user specified number of tokens per sample. These tokens are then further compressed through the JSCC module, enabling a flexible token communication strategy that adjusts both the number of transmitted tokens and their embedding dimensions. We incorporate a resource allocation algorithm based on Lyapunov stochastic optimization to enhance robustness under dynamic network conditions, effectively balancing compression efficiency and task performance. Experimental results demonstrate that our system consistently outperforms existing baselines, highlighting its potential as a strong foundation for AI native semantic communication in edge intelligence applications.",
      "authors": [
        "Alessio Devoto",
        "Jary Pomponi",
        "Mattia Merluzzi",
        "Paolo Di Lorenzo",
        "Simone Scardapane"
      ],
      "published": "2025-05-23T08:15:05Z",
      "updated": "2025-05-23T08:15:05Z",
      "categories": [
        "cs.LG",
        "cs.ET"
      ],
      "pdf_url": "https://arxiv.org/pdf/2505.17604v1",
      "landing_url": "https://arxiv.org/abs/2505.17604v1",
      "doi": "https://doi.org/10.48550/arXiv.2505.17604"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The abstract discusses semantic token transmission for edge inference but does not present discrete audio/speech tokenization as a core object of study, lacking explicit conversion of continuous audio into discrete tokens for audio reconstruction or speech tasks.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The abstract discusses semantic token transmission for edge inference but does not present discrete audio/speech tokenization as a core object of study, lacking explicit conversion of continuous audio into discrete tokens for audio reconstruction or speech tasks.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper presents an adaptive transformer-based semantic token communication framework for edge inference focusing on efficient task-aware data transmission of visual features (for object detection) over wireless channels, with no mention of discrete audio or speech tokenization or analysis of converting audio signals into discrete tokens, which is the core inclusion criteria; thus, it does not meet the inclusion requirements related to audio/speech tokenization.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper presents an adaptive transformer-based semantic token communication framework for edge inference focusing on efficient task-aware data transmission of visual features (for object detection) over wireless channels, with no mention of discrete audio or speech tokenization or analysis of converting audio signals into discrete tokens, which is the core inclusion criteria; thus, it does not meet the inclusion requirements related to audio/speech tokenization.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "NSNQuant: A Double Normalization Approach for Calibration-Free Low-Bit Vector Quantization of KV Cache",
    "abstract": "Large Language Model (LLM) inference is typically memory-intensive, especially when processing large batch sizes and long sequences, due to the large size of key-value (KV) cache. Vector Quantization (VQ) is recently adopted to alleviate this issue, but we find that the existing approach is susceptible to distribution shift due to its reliance on calibration datasets. To address this limitation, we introduce NSNQuant, a calibration-free Vector Quantization (VQ) technique designed for low-bit compression of the KV cache. By applying a three-step transformation-1) a token-wise normalization (Normalize), 2) a channel-wise centering (Shift), and 3) a second token-wise normalization (Normalize)-with Hadamard transform, NSNQuant effectively aligns the token distribution with the standard normal distribution. This alignment enables robust, calibration-free vector quantization using a single reusable codebook. Extensive experiments show that NSNQuant consistently outperforms prior methods in both 1-bit and 2-bit settings, offering strong generalization and up to 3$\\times$ throughput gain over full-precision baselines.",
    "metadata": {
      "arxiv_id": "2505.18231",
      "title": "NSNQuant: A Double Normalization Approach for Calibration-Free Low-Bit Vector Quantization of KV Cache",
      "summary": "Large Language Model (LLM) inference is typically memory-intensive, especially when processing large batch sizes and long sequences, due to the large size of key-value (KV) cache. Vector Quantization (VQ) is recently adopted to alleviate this issue, but we find that the existing approach is susceptible to distribution shift due to its reliance on calibration datasets. To address this limitation, we introduce NSNQuant, a calibration-free Vector Quantization (VQ) technique designed for low-bit compression of the KV cache. By applying a three-step transformation-1) a token-wise normalization (Normalize), 2) a channel-wise centering (Shift), and 3) a second token-wise normalization (Normalize)-with Hadamard transform, NSNQuant effectively aligns the token distribution with the standard normal distribution. This alignment enables robust, calibration-free vector quantization using a single reusable codebook. Extensive experiments show that NSNQuant consistently outperforms prior methods in both 1-bit and 2-bit settings, offering strong generalization and up to 3$\\times$ throughput gain over full-precision baselines.",
      "authors": [
        "Donghyun Son",
        "Euntae Choi",
        "Sungjoo Yoo"
      ],
      "published": "2025-05-23T12:40:07Z",
      "updated": "2025-12-14T08:17:35Z",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2505.18231v2",
      "landing_url": "https://arxiv.org/abs/2505.18231v2",
      "doi": "https://doi.org/10.48550/arXiv.2505.18231"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This item centers on calibration-free vector quantization for KV caches in LLMs and does not involve discrete audio/speech tokenization or analysis of discrete audio tokens.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "This item centers on calibration-free vector quantization for KV caches in LLMs and does not involve discrete audio/speech tokenization or analysis of discrete audio tokens.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on a vector quantization technique for compressing the key-value cache in large language model inference, which is related to discrete quantization but does not address discrete audio/speech tokenization for audio reconstruction, generation, or understanding tasks as required in the inclusion criteria. It also does not discuss audio or speech data, and instead centers on model inference optimization, making it outside the scope of the specified criteria.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on a vector quantization technique for compressing the key-value cache in large language model inference, which is related to discrete quantization but does not address discrete audio/speech tokenization for audio reconstruction, generation, or understanding tasks as required in the inclusion criteria. It also does not discuss audio or speech data, and instead centers on model inference optimization, making it outside the scope of the specified criteria.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Audio Jailbreak Attacks: Exposing Vulnerabilities in SpeechGPT in a White-Box Framework",
    "abstract": "Recent advances in Multimodal Large Language Models (MLLMs) have significantly enhanced the naturalness and flexibility of human computer interaction by enabling seamless understanding across text, vision, and audio modalities. Among these, voice enabled models such as SpeechGPT have demonstrated considerable improvements in usability, offering expressive, and emotionally responsive interactions that foster deeper connections in real world communication scenarios. However, the use of voice introduces new security risks, as attackers can exploit the unique characteristics of spoken language, such as timing, pronunciation variability, and speech to text translation, to craft inputs that bypass defenses in ways not seen in text-based systems. Despite substantial research on text based jailbreaks, the voice modality remains largely underexplored in terms of both attack strategies and defense mechanisms. In this work, we present an adversarial attack targeting the speech input of aligned MLLMs in a white box scenario. Specifically, we introduce a novel token level attack that leverages access to the model's speech tokenization to generate adversarial token sequences. These sequences are then synthesized into audio prompts, which effectively bypass alignment safeguards and to induce prohibited outputs. Evaluated on SpeechGPT, our approach achieves up to 89 percent attack success rate across multiple restricted tasks, significantly outperforming existing voice based jailbreak methods. Our findings shed light on the vulnerabilities of voice-enabled multimodal systems and to help guide the development of more robust next-generation MLLMs.",
    "metadata": {
      "arxiv_id": "2505.18864",
      "title": "Audio Jailbreak Attacks: Exposing Vulnerabilities in SpeechGPT in a White-Box Framework",
      "summary": "Recent advances in Multimodal Large Language Models (MLLMs) have significantly enhanced the naturalness and flexibility of human computer interaction by enabling seamless understanding across text, vision, and audio modalities. Among these, voice enabled models such as SpeechGPT have demonstrated considerable improvements in usability, offering expressive, and emotionally responsive interactions that foster deeper connections in real world communication scenarios. However, the use of voice introduces new security risks, as attackers can exploit the unique characteristics of spoken language, such as timing, pronunciation variability, and speech to text translation, to craft inputs that bypass defenses in ways not seen in text-based systems. Despite substantial research on text based jailbreaks, the voice modality remains largely underexplored in terms of both attack strategies and defense mechanisms. In this work, we present an adversarial attack targeting the speech input of aligned MLLMs in a white box scenario. Specifically, we introduce a novel token level attack that leverages access to the model's speech tokenization to generate adversarial token sequences. These sequences are then synthesized into audio prompts, which effectively bypass alignment safeguards and to induce prohibited outputs. Evaluated on SpeechGPT, our approach achieves up to 89 percent attack success rate across multiple restricted tasks, significantly outperforming existing voice based jailbreak methods. Our findings shed light on the vulnerabilities of voice-enabled multimodal systems and to help guide the development of more robust next-generation MLLMs.",
      "authors": [
        "Binhao Ma",
        "Hanqing Guo",
        "Zhengping Jay Luo",
        "Rui Duan"
      ],
      "published": "2025-05-24T20:46:36Z",
      "updated": "2025-05-24T20:46:36Z",
      "categories": [
        "cs.CL"
      ],
      "pdf_url": "https://arxiv.org/pdf/2505.18864v1",
      "landing_url": "https://arxiv.org/abs/2505.18864v1",
      "doi": "https://doi.org/10.48550/arXiv.2505.18864"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The article explicitly analyzes discrete speech/tokenization by exploiting the model's speech tokenization to craft adversarial token sequences and synthesize audio, with evaluation on SpeechGPT, thus meeting inclusion criteria and not falling under exclusions.",
      "evaluation": 5
    },
    "round-A_JuniorNano_reasoning": "The article explicitly analyzes discrete speech/tokenization by exploiting the model's speech tokenization to craft adversarial token sequences and synthesize audio, with evaluation on SpeechGPT, thus meeting inclusion criteria and not falling under exclusions.",
    "round-A_JuniorNano_evaluation": 5,
    "round-A_JuniorMini_output": {
      "reasoning": "The study introduces a token-level adversarial attack leveraging access to the model's speech tokenization process in SpeechGPT, effectively involving discrete audio token sequences; thus, it explicitly analyzes discrete audio tokenization as a core object of study and provides experimental validation, meeting all inclusion criteria and none of the exclusion criteria.",
      "evaluation": 5
    },
    "round-A_JuniorMini_reasoning": "The study introduces a token-level adversarial attack leveraging access to the model's speech tokenization process in SpeechGPT, effectively involving discrete audio token sequences; thus, it explicitly analyzes discrete audio tokenization as a core object of study and provides experimental validation, meeting all inclusion criteria and none of the exclusion criteria.",
    "round-A_JuniorMini_evaluation": 5,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "include (junior:5)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Offline Clustering of Linear Bandits: The Power of Clusters under Limited Data",
    "abstract": "Contextual multi-armed bandit is a fundamental learning framework for making a sequence of decisions, e.g., advertising recommendations for a sequence of arriving users. Recent works have shown that clustering these users based on the similarity of their learned preferences can accelerate the learning. However, prior work has primarily focused on the online setting, which requires continually collecting user data, ignoring the offline data widely available in many applications. To tackle these limitations, we study the offline clustering of bandits (Off-ClusBand) problem, which studies how to use the offline dataset to learn cluster properties and improve decision-making. The key challenge in Off-ClusBand arises from data insufficiency for users: unlike the online case where we continually learn from online data, in the offline case, we have a fixed, limited dataset to work from and thus must determine whether we have enough data to confidently cluster users together. To address this challenge, we propose two algorithms: Off-C2LUB, which we show analytically and experimentally outperforms existing methods under limited offline user data, and Off-CLUB, which may incur bias when data is sparse but performs well and nearly matches the lower bound when data is sufficient. We experimentally validate these results on both real and synthetic datasets.",
    "metadata": {
      "arxiv_id": "2505.19043",
      "title": "Offline Clustering of Linear Bandits: The Power of Clusters under Limited Data",
      "summary": "Contextual multi-armed bandit is a fundamental learning framework for making a sequence of decisions, e.g., advertising recommendations for a sequence of arriving users. Recent works have shown that clustering these users based on the similarity of their learned preferences can accelerate the learning. However, prior work has primarily focused on the online setting, which requires continually collecting user data, ignoring the offline data widely available in many applications. To tackle these limitations, we study the offline clustering of bandits (Off-ClusBand) problem, which studies how to use the offline dataset to learn cluster properties and improve decision-making. The key challenge in Off-ClusBand arises from data insufficiency for users: unlike the online case where we continually learn from online data, in the offline case, we have a fixed, limited dataset to work from and thus must determine whether we have enough data to confidently cluster users together. To address this challenge, we propose two algorithms: Off-C2LUB, which we show analytically and experimentally outperforms existing methods under limited offline user data, and Off-CLUB, which may incur bias when data is sparse but performs well and nearly matches the lower bound when data is sufficient. We experimentally validate these results on both real and synthetic datasets.",
      "authors": [
        "Jingyuan Liu",
        "Zeyu Zhang",
        "Xuchuang Wang",
        "Xutong Liu",
        "John C. S. Lui",
        "Mohammad Hajiesmaili",
        "Carlee Joe-Wong"
      ],
      "published": "2025-05-25T08:43:40Z",
      "updated": "2025-10-25T08:29:46Z",
      "categories": [
        "cs.LG",
        "stat.ML"
      ],
      "pdf_url": "https://arxiv.org/pdf/2505.19043v2",
      "landing_url": "https://arxiv.org/abs/2505.19043v2",
      "doi": "https://doi.org/10.48550/arXiv.2505.19043"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item addresses offline clustering in contextual bandits, with no discrete audio/speech tokenization or analysis of discrete tokens, so it does not meet the inclusion criteria focused on tokenization of audio; it should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item addresses offline clustering in contextual bandits, with no discrete audio/speech tokenization or analysis of discrete tokens, so it does not meet the inclusion criteria focused on tokenization of audio; it should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on offline clustering of linear bandits for decision-making applications and does not mention discrete audio/speech tokenization, quantization, or related methods. It is unrelated to audio signal processing or speech tokenization, so it does not meet any inclusion criteria and falls outside the scope of discrete audio token studies.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on offline clustering of linear bandits for decision-making applications and does not mention discrete audio/speech tokenization, quantization, or related methods. It is unrelated to audio signal processing or speech tokenization, so it does not meet any inclusion criteria and falls outside the scope of discrete audio token studies.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "VoiceStar: Robust Zero-Shot Autoregressive TTS with Duration Control and Extrapolation",
    "abstract": "We present VoiceStar, the first zero-shot TTS model that achieves both output duration control and extrapolation. VoiceStar is an autoregressive encoder-decoder neural codec language model, that leverages a novel Progress-Monitoring Rotary Position Embedding (PM-RoPE) and is trained with Continuation-Prompt Mixed (CPM) training. PM-RoPE enables the model to better align text and speech tokens, indicates the target duration for the generated speech, and also allows the model to generate speech waveforms much longer in duration than those seen during. CPM training also helps to mitigate the training/inference mismatch, and significantly improves the quality of the generated speech in terms of speaker similarity and intelligibility. VoiceStar outperforms or is on par with current state-of-the-art models on short-form benchmarks such as Librispeech and Seed-TTS, and significantly outperforms these models on long-form/extrapolation benchmarks (20-50s) in terms of intelligibility and naturalness. Code and models: https://github.com/jasonppy/VoiceStar. Audio samples: https://jasonppy.github.io/VoiceStar_web",
    "metadata": {
      "arxiv_id": "2505.19462",
      "title": "VoiceStar: Robust Zero-Shot Autoregressive TTS with Duration Control and Extrapolation",
      "summary": "We present VoiceStar, the first zero-shot TTS model that achieves both output duration control and extrapolation. VoiceStar is an autoregressive encoder-decoder neural codec language model, that leverages a novel Progress-Monitoring Rotary Position Embedding (PM-RoPE) and is trained with Continuation-Prompt Mixed (CPM) training. PM-RoPE enables the model to better align text and speech tokens, indicates the target duration for the generated speech, and also allows the model to generate speech waveforms much longer in duration than those seen during. CPM training also helps to mitigate the training/inference mismatch, and significantly improves the quality of the generated speech in terms of speaker similarity and intelligibility. VoiceStar outperforms or is on par with current state-of-the-art models on short-form benchmarks such as Librispeech and Seed-TTS, and significantly outperforms these models on long-form/extrapolation benchmarks (20-50s) in terms of intelligibility and naturalness. Code and models: https://github.com/jasonppy/VoiceStar. Audio samples: https://jasonppy.github.io/VoiceStar_web",
      "authors": [
        "Puyuan Peng",
        "Shang-Wen Li",
        "Abdelrahman Mohamed",
        "David Harwath"
      ],
      "published": "2025-05-26T03:35:44Z",
      "updated": "2025-05-31T22:36:04Z",
      "categories": [
        "eess.AS",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2505.19462v2",
      "landing_url": "https://arxiv.org/abs/2505.19462v2",
      "doi": "https://doi.org/10.48550/arXiv.2505.19462"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The abstract discusses a zero-shot TTS model with duration control and extrapolation using neural codecs, but it does not explicitly propose or analyze discrete audio/speech tokenization as the core object of study, so it does not meet the inclusion criteria.",
      "evaluation": 2
    },
    "round-A_JuniorNano_reasoning": "The abstract discusses a zero-shot TTS model with duration control and extrapolation using neural codecs, but it does not explicitly propose or analyze discrete audio/speech tokenization as the core object of study, so it does not meet the inclusion criteria.",
    "round-A_JuniorNano_evaluation": 2,
    "round-A_JuniorMini_output": {
      "reasoning": "The study introduces VoiceStar, a zero-shot TTS model that leverages a neural codec language model and novel position embedding to enable duration control and extrapolation; however, the abstract focuses on speech synthesis performance and duration control without explicitly discussing discrete audio/speech tokenization as a core object of study or specifically addressing discrete token design or analysis. Without clear evidence of discrete tokenization methodology or analysis, it is uncertain whether it meets the inclusion criteria.",
      "evaluation": 3
    },
    "round-A_JuniorMini_reasoning": "The study introduces VoiceStar, a zero-shot TTS model that leverages a neural codec language model and novel position embedding to enable duration control and extrapolation; however, the abstract focuses on speech synthesis performance and duration control without explicitly discussing discrete audio/speech tokenization as a core object of study or specifically addressing discrete token design or analysis. Without clear evidence of discrete tokenization methodology or analysis, it is uncertain whether it meets the inclusion criteria.",
    "round-A_JuniorMini_evaluation": 3,
    "round-B_SeniorLead_output": {
      "reasoning": "Both junior reviewers noted the abstract describes a neural-codec-based zero-shot TTS focusing on duration control and extrapolation but does not explicitly propose, analyze, or evaluate discrete audio/speech tokenization as the core contribution. While a “neural codec language model” implies use of discrete codec tokens, the abstract centers on modeling, position embedding, and training techniques for TTS quality and extrapolation rather than any discrete-token design, quantization analysis, or evaluation of tokenization methods; therefore it does not meet the inclusion criteria. I concur with their leaning to exclude.",
      "evaluation": 2
    },
    "round-B_SeniorLead_reasoning": "Both junior reviewers noted the abstract describes a neural-codec-based zero-shot TTS focusing on duration control and extrapolation but does not explicitly propose, analyze, or evaluate discrete audio/speech tokenization as the core contribution. While a “neural codec language model” implies use of discrete codec tokens, the abstract centers on modeling, position embedding, and training techniques for TTS quality and extrapolation rather than any discrete-token design, quantization analysis, or evaluation of tokenization methods; therefore it does not meet the inclusion criteria. I concur with their leaning to exclude.",
    "round-B_SeniorLead_evaluation": 2,
    "final_verdict": "exclude (senior:2)",
    "review_skipped": false,
    "discard_reason": "exclude (senior:2)"
  },
  {
    "title": "Zero-Shot Streaming Text to Speech Synthesis with Transducer and Auto-Regressive Modeling",
    "abstract": "Zero-shot streaming text-to-speech is an important research topic in human-computer interaction. Existing methods primarily use a lookahead mechanism, relying on future text to achieve natural streaming speech synthesis, which introduces high processing latency. To address this issue, we propose SMLLE, a streaming framework for generating high-quality speech frame-by-frame. SMLLE employs a Transducer to convert text into semantic tokens in real time while simultaneously obtaining duration alignment information. The combined outputs are then fed into a fully autoregressive (AR) streaming model to reconstruct mel-spectrograms. To further stabilize the generation process, we design a Delete < Bos > Mechanism that allows the AR model to access future text introducing as minimal delay as possible. Experimental results suggest that the SMLLE outperforms current streaming TTS methods and achieves comparable performance over sentence-level TTS systems. Samples are available on shy-98.github.io/SMLLE_demo_page/.",
    "metadata": {
      "arxiv_id": "2505.19669",
      "title": "Zero-Shot Streaming Text to Speech Synthesis with Transducer and Auto-Regressive Modeling",
      "summary": "Zero-shot streaming text-to-speech is an important research topic in human-computer interaction. Existing methods primarily use a lookahead mechanism, relying on future text to achieve natural streaming speech synthesis, which introduces high processing latency. To address this issue, we propose SMLLE, a streaming framework for generating high-quality speech frame-by-frame. SMLLE employs a Transducer to convert text into semantic tokens in real time while simultaneously obtaining duration alignment information. The combined outputs are then fed into a fully autoregressive (AR) streaming model to reconstruct mel-spectrograms. To further stabilize the generation process, we design a Delete < Bos > Mechanism that allows the AR model to access future text introducing as minimal delay as possible. Experimental results suggest that the SMLLE outperforms current streaming TTS methods and achieves comparable performance over sentence-level TTS systems. Samples are available on shy-98.github.io/SMLLE_demo_page/.",
      "authors": [
        "Haiyang Sun",
        "Shujie Hu",
        "Shujie Liu",
        "Lingwei Meng",
        "Hui Wang",
        "Bing Han",
        "Yifan Yang",
        "Yanqing Liu",
        "Sheng Zhao",
        "Yan Lu",
        "Yanmin Qian"
      ],
      "published": "2025-05-26T08:25:01Z",
      "updated": "2025-06-02T10:03:25Z",
      "categories": [
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2505.19669v2",
      "landing_url": "https://arxiv.org/abs/2505.19669v2",
      "doi": "https://doi.org/10.48550/arXiv.2505.19669"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The abstract describes a streaming TTS system that uses a transducer to convert text into discrete semantic tokens, which are then used to reconstruct mel-spectrograms, indicating discrete audio tokenization is a core component.",
      "evaluation": 4
    },
    "round-A_JuniorNano_reasoning": "The abstract describes a streaming TTS system that uses a transducer to convert text into discrete semantic tokens, which are then used to reconstruct mel-spectrograms, indicating discrete audio tokenization is a core component.",
    "round-A_JuniorNano_evaluation": 4,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on zero-shot streaming text-to-speech synthesis using a Transducer and auto-regressive modeling to generate mel-spectrograms, but it does not explicitly mention proposing, analyzing, or evaluating discrete audio/speech tokenization where continuous audio is converted into discrete symbols for reconstruction or understanding; the emphasis is on generating mel-spectrograms rather than on discrete token representations, meaning it likely falls under exclusion criteria related to using continuous audio features without discrete quantization analysis.",
      "evaluation": 2
    },
    "round-A_JuniorMini_reasoning": "The study focuses on zero-shot streaming text-to-speech synthesis using a Transducer and auto-regressive modeling to generate mel-spectrograms, but it does not explicitly mention proposing, analyzing, or evaluating discrete audio/speech tokenization where continuous audio is converted into discrete symbols for reconstruction or understanding; the emphasis is on generating mel-spectrograms rather than on discrete token representations, meaning it likely falls under exclusion criteria related to using continuous audio features without discrete quantization analysis.",
    "round-A_JuniorMini_evaluation": 2,
    "round-B_SeniorLead_output": {
      "reasoning": "I reviewed the two junior reviewers' comments: JuniorNano interpreted the term “semantic tokens” as indicating discrete audio tokenization, while JuniorMini correctly noted the paper reconstructs mel-spectrograms and does not describe discrete audio quantization. The abstract describes converting text into semantic tokens and using an AR model to reconstruct mel-spectrograms, with no explicit proposal, analysis, or evaluation of discrete audio/speech tokenization (finite-vocabulary neural quantization or codec-like discrete audio tokens). Therefore the study does not meet the inclusion criterion requiring discrete audio tokenization as a core object of study. Brief explanation: Exclude because the work focuses on streaming TTS producing continuous mel-spectrograms and text-derived “semantic tokens,” not on discrete audio tokenization or quantization.",
      "evaluation": 2
    },
    "round-B_SeniorLead_reasoning": "I reviewed the two junior reviewers' comments: JuniorNano interpreted the term “semantic tokens” as indicating discrete audio tokenization, while JuniorMini correctly noted the paper reconstructs mel-spectrograms and does not describe discrete audio quantization. The abstract describes converting text into semantic tokens and using an AR model to reconstruct mel-spectrograms, with no explicit proposal, analysis, or evaluation of discrete audio/speech tokenization (finite-vocabulary neural quantization or codec-like discrete audio tokens). Therefore the study does not meet the inclusion criterion requiring discrete audio tokenization as a core object of study. Brief explanation: Exclude because the work focuses on streaming TTS producing continuous mel-spectrograms and text-derived “semantic tokens,” not on discrete audio tokenization or quantization.",
    "round-B_SeniorLead_evaluation": 2,
    "final_verdict": "exclude (senior:2)",
    "review_skipped": false,
    "discard_reason": "exclude (senior:2)"
  },
  {
    "title": "Navigating PESQ: Up-to-Date Versions and Open Implementations",
    "abstract": "Perceptual Evaluation of Speech Quality (PESQ) is an objective quality measure that remains widely used despite its withdrawal by the International Telecommunication Union (ITU). PESQ has evolved over two decades, with multiple versions and publicly available implementations emerging during this time. Different versions and their updates can be overwhelming, especially for new PESQ users. This work provides practical guidance on the different versions and implementations of PESQ. We show that differences can be significant, especially between PESQ versions. We stress the importance of specifying the exact version and implementation that is used to compute PESQ, and possibly to detail how multi-channel signals are handled. These practices would facilitate the interpretation of results and allow comparisons of PESQ scores between different studies. We also provide a repository that implements the latest corrections to PESQ, i.e., Corrigendum 2, which is not implemented by any other openly available distribution: https://github.com/audiolabs/PESQ.",
    "metadata": {
      "arxiv_id": "2505.19760",
      "title": "Navigating PESQ: Up-to-Date Versions and Open Implementations",
      "summary": "Perceptual Evaluation of Speech Quality (PESQ) is an objective quality measure that remains widely used despite its withdrawal by the International Telecommunication Union (ITU). PESQ has evolved over two decades, with multiple versions and publicly available implementations emerging during this time. Different versions and their updates can be overwhelming, especially for new PESQ users. This work provides practical guidance on the different versions and implementations of PESQ. We show that differences can be significant, especially between PESQ versions. We stress the importance of specifying the exact version and implementation that is used to compute PESQ, and possibly to detail how multi-channel signals are handled. These practices would facilitate the interpretation of results and allow comparisons of PESQ scores between different studies. We also provide a repository that implements the latest corrections to PESQ, i.e., Corrigendum 2, which is not implemented by any other openly available distribution: https://github.com/audiolabs/PESQ.",
      "authors": [
        "Matteo Torcoli",
        "Mhd Modar Halimeh",
        "Emanuël A. P. Habets"
      ],
      "published": "2025-05-26T09:43:09Z",
      "updated": "2025-08-14T11:53:22Z",
      "categories": [
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2505.19760v2",
      "landing_url": "https://arxiv.org/abs/2505.19760v2",
      "doi": "https://doi.org/10.48550/arXiv.2505.19760"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This item discusses PESQ as an evaluation metric and its versions/implementations; it does not analyze or propose discrete audio tokenization into a finite vocabulary, so it fails the inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "This item discusses PESQ as an evaluation metric and its versions/implementations; it does not analyze or propose discrete audio tokenization into a finite vocabulary, so it fails the inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper discusses versions and implementations of PESQ, an objective speech quality evaluation measure, but does not propose, analyze, or evaluate discrete audio/speech tokenization or discrete quantization; it focuses more on evaluation metrics and tool availability rather than discrete token representations for audio reconstruction or understanding, thus failing the inclusion criteria and falling under exclusion for being an engineering/tool report without discrete token analysis.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper discusses versions and implementations of PESQ, an objective speech quality evaluation measure, but does not propose, analyze, or evaluate discrete audio/speech tokenization or discrete quantization; it focuses more on evaluation metrics and tool availability rather than discrete token representations for audio reconstruction or understanding, thus failing the inclusion criteria and falling under exclusion for being an engineering/tool report without discrete token analysis.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Uni-VERSA: Versatile Speech Assessment with a Unified Network",
    "abstract": "Subjective listening tests remain the golden standard for speech quality assessment, but are costly, variable, and difficult to scale. In contrast, existing objective metrics, such as PESQ, F0 correlation, and DNSMOS, typically capture only specific aspects of speech quality. To address these limitations, we introduce Uni-VERSA, a unified network that simultaneously predicts various objective metrics, encompassing naturalness, intelligibility, speaker characteristics, prosody, and noise, for a comprehensive evaluation of speech signals. We formalize its framework, evaluation protocol, and applications in speech enhancement, synthesis, and quality control. A benchmark based on the URGENT24 challenge, along with a baseline leveraging self-supervised representations, demonstrates that Uni-VERSA provides a viable alternative to single-aspect evaluation methods. Moreover, it aligns closely with human perception, making it a promising approach for future speech quality assessment.",
    "metadata": {
      "arxiv_id": "2505.20741",
      "title": "Uni-VERSA: Versatile Speech Assessment with a Unified Network",
      "summary": "Subjective listening tests remain the golden standard for speech quality assessment, but are costly, variable, and difficult to scale. In contrast, existing objective metrics, such as PESQ, F0 correlation, and DNSMOS, typically capture only specific aspects of speech quality. To address these limitations, we introduce Uni-VERSA, a unified network that simultaneously predicts various objective metrics, encompassing naturalness, intelligibility, speaker characteristics, prosody, and noise, for a comprehensive evaluation of speech signals. We formalize its framework, evaluation protocol, and applications in speech enhancement, synthesis, and quality control. A benchmark based on the URGENT24 challenge, along with a baseline leveraging self-supervised representations, demonstrates that Uni-VERSA provides a viable alternative to single-aspect evaluation methods. Moreover, it aligns closely with human perception, making it a promising approach for future speech quality assessment.",
      "authors": [
        "Jiatong Shi",
        "Hye-Jin Shim",
        "Shinji Watanabe"
      ],
      "published": "2025-05-27T05:31:19Z",
      "updated": "2025-05-27T05:31:19Z",
      "categories": [
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2505.20741v1",
      "landing_url": "https://arxiv.org/abs/2505.20741v1",
      "doi": "https://doi.org/10.48550/arXiv.2505.20741"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The work targets continuous speech quality metrics and prediction; it does not propose or analyze discrete audio tokenization.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The work targets continuous speech quality metrics and prediction; it does not propose or analyze discrete audio tokenization.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study Uni-VERSA focuses on a unified network for speech quality assessment predicting multiple objective metrics related to naturalness, intelligibility, and other speech characteristics; however, the abstract does not mention any discrete audio/speech tokenization method, discrete quantization strategies, or analysis of converting continuous audio into discrete tokens as a core object of study, which is required by the inclusion criteria. It mainly deals with evaluating speech quality rather than tokenizing speech signals. Therefore, it does not meet the key inclusion criterion and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study Uni-VERSA focuses on a unified network for speech quality assessment predicting multiple objective metrics related to naturalness, intelligibility, and other speech characteristics; however, the abstract does not mention any discrete audio/speech tokenization method, discrete quantization strategies, or analysis of converting continuous audio into discrete tokens as a core object of study, which is required by the inclusion criteria. It mainly deals with evaluating speech quality rather than tokenizing speech signals. Therefore, it does not meet the key inclusion criterion and should be excluded.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Vectorized Sequence-Based Chunking for Data Deduplication",
    "abstract": "Data deduplication has gained wide acclaim as a mechanism to improve storage efficiency and conserve network bandwidth. Its most critical phase, data chunking, is responsible for the overall space savings achieved via the deduplication process. However, modern data chunking algorithms are slow and compute-intensive because they scan large amounts of data while simultaneously making data-driven boundary decisions. We present SeqCDC, a novel chunking algorithm that leverages lightweight boundary detection, content-defined skipping, and SSE/AVX acceleration to improve chunking throughput for large chunk sizes. Our evaluation shows that SeqCDC achieves 15x higher throughput than unaccelerated and 1.2x-1.35x higher throughput than vector-accelerated data chunking algorithms while minimally affecting deduplication space savings.",
    "metadata": {
      "arxiv_id": "2505.21194",
      "title": "Vectorized Sequence-Based Chunking for Data Deduplication",
      "summary": "Data deduplication has gained wide acclaim as a mechanism to improve storage efficiency and conserve network bandwidth. Its most critical phase, data chunking, is responsible for the overall space savings achieved via the deduplication process. However, modern data chunking algorithms are slow and compute-intensive because they scan large amounts of data while simultaneously making data-driven boundary decisions.\n  We present SeqCDC, a novel chunking algorithm that leverages lightweight boundary detection, content-defined skipping, and SSE/AVX acceleration to improve chunking throughput for large chunk sizes. Our evaluation shows that SeqCDC achieves 15x higher throughput than unaccelerated and 1.2x-1.35x higher throughput than vector-accelerated data chunking algorithms while minimally affecting deduplication space savings.",
      "authors": [
        "Sreeharsha Udayashankar",
        "Samer Al-Kiswany"
      ],
      "published": "2025-05-27T13:42:33Z",
      "updated": "2025-05-27T13:42:33Z",
      "categories": [
        "cs.DC"
      ],
      "pdf_url": "https://arxiv.org/pdf/2505.21194v1",
      "landing_url": "https://arxiv.org/abs/2505.21194v1",
      "doi": "https://doi.org/10.48550/arXiv.2505.21194"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The paper addresses vectorized chunking for data deduplication, not discrete audio/speech tokenization, so it fails the inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The paper addresses vectorized chunking for data deduplication, not discrete audio/speech tokenization, so it fails the inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on data chunking algorithms for data deduplication in storage systems and does not address discrete audio/speech tokenization or related continuous-to-discrete audio representation tasks; thus, it does not meet the inclusion criteria related to audio tokenization.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on data chunking algorithms for data deduplication in storage systems and does not address discrete audio/speech tokenization or related continuous-to-discrete audio representation tasks; thus, it does not meet the inclusion criteria related to audio tokenization.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Number of Clusters in a Dataset: A Regularized K-means Approach",
    "abstract": "Finding the number of meaningful clusters in an unlabeled dataset is important in many applications. Regularized k-means algorithm is a possible approach frequently used to find the correct number of distinct clusters in datasets. The most common formulation of the regularization function is the additive linear term $λk$, where $k$ is the number of clusters and $λ$ a positive coefficient. Currently, there are no principled guidelines for setting a value for the critical hyperparameter $λ$. In this paper, we derive rigorous bounds for $λ$ assuming clusters are {\\em ideal}. Ideal clusters (defined as $d$-dimensional spheres with identical radii) are close proxies for k-means clusters ($d$-dimensional spherically symmetric distributions with identical standard deviations). Experiments show that the k-means algorithm with additive regularizer often yields multiple solutions. Thus, we also analyze k-means algorithm with multiplicative regularizer. The consensus among k-means solutions with additive and multiplicative regularizations reduces the ambiguity of multiple solutions in certain cases. We also present selected experiments that demonstrate performance of the regularized k-means algorithms as clusters deviate from the ideal assumption.",
    "metadata": {
      "arxiv_id": "2505.22991",
      "title": "Number of Clusters in a Dataset: A Regularized K-means Approach",
      "summary": "Finding the number of meaningful clusters in an unlabeled dataset is important in many applications. Regularized k-means algorithm is a possible approach frequently used to find the correct number of distinct clusters in datasets. The most common formulation of the regularization function is the additive linear term $λk$, where $k$ is the number of clusters and $λ$ a positive coefficient. Currently, there are no principled guidelines for setting a value for the critical hyperparameter $λ$. In this paper, we derive rigorous bounds for $λ$ assuming clusters are {\\em ideal}. Ideal clusters (defined as $d$-dimensional spheres with identical radii) are close proxies for k-means clusters ($d$-dimensional spherically symmetric distributions with identical standard deviations). Experiments show that the k-means algorithm with additive regularizer often yields multiple solutions. Thus, we also analyze k-means algorithm with multiplicative regularizer. The consensus among k-means solutions with additive and multiplicative regularizations reduces the ambiguity of multiple solutions in certain cases. We also present selected experiments that demonstrate performance of the regularized k-means algorithms as clusters deviate from the ideal assumption.",
      "authors": [
        "Behzad Kamgar-Parsi",
        "Behrooz Kamgar-Parsi"
      ],
      "published": "2025-05-29T01:58:44Z",
      "updated": "2025-05-29T01:58:44Z",
      "categories": [
        "cs.LG",
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2505.22991v1",
      "landing_url": "https://arxiv.org/abs/2505.22991v1",
      "doi": "https://doi.org/10.48550/arXiv.2505.22991"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The abstract focuses on regularized k-means for selecting the number of clusters and does not discuss discrete audio/speech tokenization or token sequences, thus it does not meet the inclusion criteria and should be excluded per the specified exclusions.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The abstract focuses on regularized k-means for selecting the number of clusters and does not discuss discrete audio/speech tokenization or token sequences, thus it does not meet the inclusion criteria and should be excluded per the specified exclusions.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on the selection of the number of clusters in a dataset using a regularized k-means algorithm without any mention or analysis of discrete audio or speech tokenization. It does not address converting continuous audio signals into discrete token sequences or evaluate such tokenization for audio reconstruction, generation, or understanding, thus it fails to meet the inclusion criteria and does not violate any exclusion criteria explicitly related to audio tokenization.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on the selection of the number of clusters in a dataset using a regularized k-means algorithm without any mention or analysis of discrete audio or speech tokenization. It does not address converting continuous audio signals into discrete token sequences or evaluate such tokenization for audio reconstruction, generation, or understanding, thus it fails to meet the inclusion criteria and does not violate any exclusion criteria explicitly related to audio tokenization.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "DS-Codec: Dual-Stage Training with Mirror-to-NonMirror Architecture Switching for Speech Codec",
    "abstract": "Neural speech codecs are essential for advancing text-to-speech (TTS) systems. With the recent success of large language models in text generation, developing high-quality speech tokenizers has become increasingly important. This paper introduces DS-Codec, a novel neural speech codec featuring a dual-stage training framework with mirror and non-mirror architectures switching, designed to achieve superior speech reconstruction. We conduct extensive experiments and ablation studies to evaluate the effectiveness of our training strategy and compare the performance of the two architectures. Our results show that the mirrored structure significantly enhances the robustness of the learned codebooks, and the training strategy balances the advantages between mirrored and non-mirrored structures, leading to improved high-fidelity speech reconstruction.",
    "metadata": {
      "arxiv_id": "2505.24314",
      "title": "DS-Codec: Dual-Stage Training with Mirror-to-NonMirror Architecture Switching for Speech Codec",
      "summary": "Neural speech codecs are essential for advancing text-to-speech (TTS) systems. With the recent success of large language models in text generation, developing high-quality speech tokenizers has become increasingly important. This paper introduces DS-Codec, a novel neural speech codec featuring a dual-stage training framework with mirror and non-mirror architectures switching, designed to achieve superior speech reconstruction. We conduct extensive experiments and ablation studies to evaluate the effectiveness of our training strategy and compare the performance of the two architectures. Our results show that the mirrored structure significantly enhances the robustness of the learned codebooks, and the training strategy balances the advantages between mirrored and non-mirrored structures, leading to improved high-fidelity speech reconstruction.",
      "authors": [
        "Peijie Chen",
        "Wenhao Guan",
        "Kaidi Wang",
        "Weijie Wu",
        "Hukai Huang",
        "Qingyang Hong",
        "Lin Li"
      ],
      "published": "2025-05-30T07:53:01Z",
      "updated": "2025-05-30T07:53:01Z",
      "categories": [
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2505.24314v1",
      "landing_url": "https://arxiv.org/abs/2505.24314v1",
      "doi": "https://doi.org/10.48550/arXiv.2505.24314"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The abstract discusses neural speech codecs and codebooks, which might imply discrete tokenization, but it does not explicitly frame discrete tokenization as the core object of study, so inclusion is uncertain.",
      "evaluation": 3
    },
    "round-A_JuniorNano_reasoning": "The abstract discusses neural speech codecs and codebooks, which might imply discrete tokenization, but it does not explicitly frame discrete tokenization as the core object of study, so inclusion is uncertain.",
    "round-A_JuniorNano_evaluation": 3,
    "round-A_JuniorMini_output": {
      "reasoning": "The study proposes DS-Codec, a neural speech codec with a dual-stage training framework involving mirror and non-mirror architectures, focusing on improving speech reconstruction through learned codebooks, which implies discrete representation learning; thus, it explicitly deals with discrete speech tokenization for speech signal reconstruction and meets the inclusion requirements without conflicting with any exclusion criteria.",
      "evaluation": 5
    },
    "round-A_JuniorMini_reasoning": "The study proposes DS-Codec, a neural speech codec with a dual-stage training framework involving mirror and non-mirror architectures, focusing on improving speech reconstruction through learned codebooks, which implies discrete representation learning; thus, it explicitly deals with discrete speech tokenization for speech signal reconstruction and meets the inclusion requirements without conflicting with any exclusion criteria.",
    "round-A_JuniorMini_evaluation": 5,
    "round-B_SeniorLead_output": {
      "reasoning": "I reviewed the abstract and the two junior reviewers' comments (JuniorNano was uncertain; JuniorMini recommended inclusion). The abstract explicitly references a neural speech codec and the robustness of \"learned codebooks\" and positions the training strategy to balance mirrored vs. non-mirrored codebook structures for improved speech reconstruction—this indicates discrete quantization/tokenization is a core object of study rather than mere continuous features. The title and abstract are in English and the work is a neural codec (not a traditional signal-processing codec), so it meets the inclusion criteria and none of the exclusion criteria. Explanation (one sentence): The study explicitly focuses on learned codebooks in a neural speech codec—i.e., discrete tokenization/quantization for speech reconstruction—so it should be included.",
      "evaluation": 5
    },
    "round-B_SeniorLead_reasoning": "I reviewed the abstract and the two junior reviewers' comments (JuniorNano was uncertain; JuniorMini recommended inclusion). The abstract explicitly references a neural speech codec and the robustness of \"learned codebooks\" and positions the training strategy to balance mirrored vs. non-mirrored codebook structures for improved speech reconstruction—this indicates discrete quantization/tokenization is a core object of study rather than mere continuous features. The title and abstract are in English and the work is a neural codec (not a traditional signal-processing codec), so it meets the inclusion criteria and none of the exclusion criteria. Explanation (one sentence): The study explicitly focuses on learned codebooks in a neural speech codec—i.e., discrete tokenization/quantization for speech reconstruction—so it should be included.",
    "round-B_SeniorLead_evaluation": 5,
    "final_verdict": "include (senior:5)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Speech Token Prediction via Compressed-to-fine Language Modeling for Speech Generation",
    "abstract": "Neural audio codecs, used as speech tokenizers, have demonstrated remarkable potential in the field of speech generation. However, to ensure high-fidelity audio reconstruction, neural audio codecs typically encode audio into long sequences of speech tokens, posing a significant challenge for downstream language models in long-context modeling. We observe that speech token sequences exhibit short-range dependency: due to the monotonic alignment between text and speech in text-to-speech (TTS) tasks, the prediction of the current token primarily relies on its local context, while long-range tokens contribute less to the current token prediction and often contain redundant information. Inspired by this observation, we propose a \\textbf{compressed-to-fine language modeling} approach to address the challenge of long sequence speech tokens within neural codec language models: (1) \\textbf{Fine-grained Initial and Short-range Information}: Our approach retains the prompt and local tokens during prediction to ensure text alignment and the integrity of paralinguistic information; (2) \\textbf{Compressed Long-range Context}: Our approach compresses long-range token spans into compact representations to reduce redundant information while preserving essential semantics. Extensive experiments on various neural audio codecs and downstream language models validate the effectiveness and generalizability of the proposed approach, highlighting the importance of token compression in improving speech generation within neural codec language models. The demo of audio samples will be available at https://anonymous.4open.science/r/SpeechTokenPredictionViaCompressedToFinedLM.",
    "metadata": {
      "arxiv_id": "2505.24496",
      "title": "Speech Token Prediction via Compressed-to-fine Language Modeling for Speech Generation",
      "summary": "Neural audio codecs, used as speech tokenizers, have demonstrated remarkable potential in the field of speech generation. However, to ensure high-fidelity audio reconstruction, neural audio codecs typically encode audio into long sequences of speech tokens, posing a significant challenge for downstream language models in long-context modeling. We observe that speech token sequences exhibit short-range dependency: due to the monotonic alignment between text and speech in text-to-speech (TTS) tasks, the prediction of the current token primarily relies on its local context, while long-range tokens contribute less to the current token prediction and often contain redundant information. Inspired by this observation, we propose a \\textbf{compressed-to-fine language modeling} approach to address the challenge of long sequence speech tokens within neural codec language models: (1) \\textbf{Fine-grained Initial and Short-range Information}: Our approach retains the prompt and local tokens during prediction to ensure text alignment and the integrity of paralinguistic information; (2) \\textbf{Compressed Long-range Context}: Our approach compresses long-range token spans into compact representations to reduce redundant information while preserving essential semantics. Extensive experiments on various neural audio codecs and downstream language models validate the effectiveness and generalizability of the proposed approach, highlighting the importance of token compression in improving speech generation within neural codec language models. The demo of audio samples will be available at https://anonymous.4open.science/r/SpeechTokenPredictionViaCompressedToFinedLM.",
      "authors": [
        "Wenrui Liu",
        "Qian Chen",
        "Wen Wang",
        "Yafeng Chen",
        "Jin Xu",
        "Zhifang Guo",
        "Guanrou Yang",
        "Weiqin Li",
        "Xiaoda Yang",
        "Tao Jin",
        "Minghui Fang",
        "Jialong Zuo",
        "Bai Jionghao",
        "Zemin Liu"
      ],
      "published": "2025-05-30T11:47:29Z",
      "updated": "2025-05-30T11:47:29Z",
      "categories": [
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2505.24496v1",
      "landing_url": "https://arxiv.org/abs/2505.24496v1",
      "doi": "https://doi.org/10.48550/arXiv.2505.24496"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "It centers on discrete speech tokenization via neural audio codecs and proposes a token-based modeling approach with experiments, meeting inclusion criteria and avoiding the listed exclusions.",
      "evaluation": 5
    },
    "round-A_JuniorNano_reasoning": "It centers on discrete speech tokenization via neural audio codecs and proposes a token-based modeling approach with experiments, meeting inclusion criteria and avoiding the listed exclusions.",
    "round-A_JuniorNano_evaluation": 5,
    "round-A_JuniorMini_output": {
      "reasoning": "The study explicitly focuses on neural audio codecs that convert continuous audio into discrete speech token sequences, addressing challenges in discrete speech token prediction and compression for speech generation, meeting all inclusion criteria and none of the exclusion criteria.",
      "evaluation": 5
    },
    "round-A_JuniorMini_reasoning": "The study explicitly focuses on neural audio codecs that convert continuous audio into discrete speech token sequences, addressing challenges in discrete speech token prediction and compression for speech generation, meeting all inclusion criteria and none of the exclusion criteria.",
    "round-A_JuniorMini_evaluation": 5,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "include (junior:5)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "ARECHO: Autoregressive Evaluation via Chain-Based Hypothesis Optimization for Speech Multi-Metric Estimation",
    "abstract": "Speech signal analysis poses significant challenges, particularly in tasks such as speech quality evaluation and profiling, where the goal is to predict multiple perceptual and objective metrics. For instance, metrics like PESQ (Perceptual Evaluation of Speech Quality), STOI (Short-Time Objective Intelligibility), and MOS (Mean Opinion Score) each capture different aspects of speech quality. However, these metrics often have different scales, assumptions, and dependencies, making joint estimation non-trivial. To address these issues, we introduce ARECHO (Autoregressive Evaluation via Chain-based Hypothesis Optimization), a chain-based, versatile evaluation system for speech assessment grounded in autoregressive dependency modeling. ARECHO is distinguished by three key innovations: (1) a comprehensive speech information tokenization pipeline; (2) a dynamic classifier chain that explicitly captures inter-metric dependencies; and (3) a two-step confidence-oriented decoding algorithm that enhances inference reliability. Experiments demonstrate that ARECHO significantly outperforms the baseline framework across diverse evaluation scenarios, including enhanced speech analysis, speech generation evaluation, and, noisy speech evaluation. Furthermore, its dynamic dependency modeling improves interpretability by capturing inter-metric relationships. Across tasks, ARECHO offers reference-free evaluation using its dynamic classifier chain to support subset queries (single or multiple metrics) and reduces error propagation via confidence-oriented decoding.",
    "metadata": {
      "arxiv_id": "2505.24518",
      "title": "ARECHO: Autoregressive Evaluation via Chain-Based Hypothesis Optimization for Speech Multi-Metric Estimation",
      "summary": "Speech signal analysis poses significant challenges, particularly in tasks such as speech quality evaluation and profiling, where the goal is to predict multiple perceptual and objective metrics. For instance, metrics like PESQ (Perceptual Evaluation of Speech Quality), STOI (Short-Time Objective Intelligibility), and MOS (Mean Opinion Score) each capture different aspects of speech quality. However, these metrics often have different scales, assumptions, and dependencies, making joint estimation non-trivial. To address these issues, we introduce ARECHO (Autoregressive Evaluation via Chain-based Hypothesis Optimization), a chain-based, versatile evaluation system for speech assessment grounded in autoregressive dependency modeling. ARECHO is distinguished by three key innovations: (1) a comprehensive speech information tokenization pipeline; (2) a dynamic classifier chain that explicitly captures inter-metric dependencies; and (3) a two-step confidence-oriented decoding algorithm that enhances inference reliability. Experiments demonstrate that ARECHO significantly outperforms the baseline framework across diverse evaluation scenarios, including enhanced speech analysis, speech generation evaluation, and, noisy speech evaluation. Furthermore, its dynamic dependency modeling improves interpretability by capturing inter-metric relationships. Across tasks, ARECHO offers reference-free evaluation using its dynamic classifier chain to support subset queries (single or multiple metrics) and reduces error propagation via confidence-oriented decoding.",
      "authors": [
        "Jiatong Shi",
        "Yifan Cheng",
        "Bo-Hao Su",
        "Hye-jin Shim",
        "Jinchuan Tian",
        "Samuele Cornell",
        "Yiwen Zhao",
        "Siddhant Arora",
        "Shinji Watanabe"
      ],
      "published": "2025-05-30T12:30:04Z",
      "updated": "2025-10-30T15:08:00Z",
      "categories": [
        "cs.SD",
        "cs.MM",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2505.24518v2",
      "landing_url": "https://arxiv.org/abs/2505.24518v2",
      "doi": "https://doi.org/10.48550/arXiv.2505.24518"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The abstract explicitly mentions a 'speech information tokenization pipeline' as an innovation, indicating discrete tokenization of speech is a core object of study; thus it meets inclusion and does not align with the exclusion criteria.",
      "evaluation": 5
    },
    "round-A_JuniorNano_reasoning": "The abstract explicitly mentions a 'speech information tokenization pipeline' as an innovation, indicating discrete tokenization of speech is a core object of study; thus it meets inclusion and does not align with the exclusion criteria.",
    "round-A_JuniorNano_evaluation": 5,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on a speech evaluation system based on autoregressive dependency modeling and classifier chains for estimating speech quality metrics, but it does not explicitly propose or analyze discrete audio/speech tokenization as a core contribution; instead, it addresses multi-metric evaluation without describing a discrete tokenization approach converting continuous audio into discrete symbols for reconstruction/generation tasks.",
      "evaluation": 2
    },
    "round-A_JuniorMini_reasoning": "The study focuses on a speech evaluation system based on autoregressive dependency modeling and classifier chains for estimating speech quality metrics, but it does not explicitly propose or analyze discrete audio/speech tokenization as a core contribution; instead, it addresses multi-metric evaluation without describing a discrete tokenization approach converting continuous audio into discrete symbols for reconstruction/generation tasks.",
    "round-A_JuniorMini_evaluation": 2,
    "round-B_SeniorLead_output": {
      "reasoning": "Both junior reviewers flagged disagreement: JuniorNano interpreted the phrase \"speech information tokenization pipeline\" as indicating discrete tokenization and recommended inclusion, while JuniorMini judged the paper focuses on autoregressive multi-metric estimation without explicit discrete audio token design and recommended exclusion. After reviewing the abstract, although it mentions a \"tokenization pipeline\" as one innovation, the abstract centers on predicting perceptual/objective metrics (PESQ, STOI, MOS) via a classifier-chain autoregressive model and does not explicitly describe converting continuous audio into discrete symbols from a finite vocabulary, nor does it discuss using those tokens for audio reconstruction/generation or analyze quantization/token design—requirements in the inclusion criteria. The scope appears to be metric estimation (reference-free evaluation) rather than proposing or evaluating discrete audio token representations; therefore it does not meet the inclusion requirements and meets the exclusion criteria for studies that only use continuous features or vaguely mention tokenization without methodological detail. Conclusion (one-sentence explanation): The abstract is ambiguous and lacks explicit, substantive treatment of discrete audio tokenization as a core contribution, so this study should be excluded.",
      "evaluation": 2
    },
    "round-B_SeniorLead_reasoning": "Both junior reviewers flagged disagreement: JuniorNano interpreted the phrase \"speech information tokenization pipeline\" as indicating discrete tokenization and recommended inclusion, while JuniorMini judged the paper focuses on autoregressive multi-metric estimation without explicit discrete audio token design and recommended exclusion. After reviewing the abstract, although it mentions a \"tokenization pipeline\" as one innovation, the abstract centers on predicting perceptual/objective metrics (PESQ, STOI, MOS) via a classifier-chain autoregressive model and does not explicitly describe converting continuous audio into discrete symbols from a finite vocabulary, nor does it discuss using those tokens for audio reconstruction/generation or analyze quantization/token design—requirements in the inclusion criteria. The scope appears to be metric estimation (reference-free evaluation) rather than proposing or evaluating discrete audio token representations; therefore it does not meet the inclusion requirements and meets the exclusion criteria for studies that only use continuous features or vaguely mention tokenization without methodological detail. Conclusion (one-sentence explanation): The abstract is ambiguous and lacks explicit, substantive treatment of discrete audio tokenization as a core contribution, so this study should be excluded.",
    "round-B_SeniorLead_evaluation": 2,
    "final_verdict": "exclude (senior:2)",
    "review_skipped": false,
    "discard_reason": "exclude (senior:2)"
  },
  {
    "title": "FUSE: Universal Speech Enhancement using Multi-Stage Fusion of Sparse Compression and Token Generation Models for the URGENT 2025 Challenge",
    "abstract": "We propose a multi-stage framework for universal speech enhancement, designed for the Interspeech 2025 URGENT Challenge. Our system first employs a Sparse Compression Network to robustly separate sources and extract an initial clean speech estimate from noisy inputs. This is followed by an efficient generative model that refines speech quality by leveraging self-supervised features and optimizing a masked language modeling objective on acoustic tokens derived from a neural audio codec. In the final stage, a fusion network integrates the outputs of the first two stages with the original noisy signal, achieving a balanced improvement in both signal fidelity and perceptual quality. Additionally, a shift trick that aggregates multiple time-shifted predictions, along with output blending, further boosts performance. Experimental results on challenging multilingual datasets with variable sampling rates and diverse distortion types validate the effectiveness of our approach.",
    "metadata": {
      "arxiv_id": "2506.00809",
      "title": "FUSE: Universal Speech Enhancement using Multi-Stage Fusion of Sparse Compression and Token Generation Models for the URGENT 2025 Challenge",
      "summary": "We propose a multi-stage framework for universal speech enhancement, designed for the Interspeech 2025 URGENT Challenge. Our system first employs a Sparse Compression Network to robustly separate sources and extract an initial clean speech estimate from noisy inputs. This is followed by an efficient generative model that refines speech quality by leveraging self-supervised features and optimizing a masked language modeling objective on acoustic tokens derived from a neural audio codec. In the final stage, a fusion network integrates the outputs of the first two stages with the original noisy signal, achieving a balanced improvement in both signal fidelity and perceptual quality. Additionally, a shift trick that aggregates multiple time-shifted predictions, along with output blending, further boosts performance. Experimental results on challenging multilingual datasets with variable sampling rates and diverse distortion types validate the effectiveness of our approach.",
      "authors": [
        "Nabarun Goswami",
        "Tatsuya Harada"
      ],
      "published": "2025-06-01T03:23:27Z",
      "updated": "2025-06-01T03:23:27Z",
      "categories": [
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2506.00809v1",
      "landing_url": "https://arxiv.org/abs/2506.00809v1",
      "doi": "https://doi.org/10.48550/arXiv.2506.00809"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The abstract describes a speech enhancement approach that relies on discrete acoustic tokens generated by a neural audio codec and a token-generation model with a masked language modeling objective, which aligns with the inclusion criterion for discrete audio tokenization and does not trigger any exclusion criteria.",
      "evaluation": 5
    },
    "round-A_JuniorNano_reasoning": "The abstract describes a speech enhancement approach that relies on discrete acoustic tokens generated by a neural audio codec and a token-generation model with a masked language modeling objective, which aligns with the inclusion criterion for discrete audio tokenization and does not trigger any exclusion criteria.",
    "round-A_JuniorNano_evaluation": 5,
    "round-A_JuniorMini_output": {
      "reasoning": "The study explicitly involves a generative model optimizing a masked language modeling objective on acoustic tokens derived from a neural audio codec, indicating the use and analysis of discrete audio tokenization in speech enhancement, meeting the inclusion criteria and avoiding all exclusion criteria.",
      "evaluation": 5
    },
    "round-A_JuniorMini_reasoning": "The study explicitly involves a generative model optimizing a masked language modeling objective on acoustic tokens derived from a neural audio codec, indicating the use and analysis of discrete audio tokenization in speech enhancement, meeting the inclusion criteria and avoiding all exclusion criteria.",
    "round-A_JuniorMini_evaluation": 5,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "include (junior:5)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "HASRD: Hierarchical Acoustic and Semantic Representation Disentanglement",
    "abstract": "Effective speech representations for spoken language models must balance semantic relevance with acoustic fidelity for high-quality reconstruction. However, existing approaches struggle to achieve both simultaneously. To address this, we introduce Hierarchical Acoustic and Semantic Representation Disentanglement (HASRD, pronounced `hazard'), a framework that factorizes self-supervised learning representations into discrete semantic and acoustic tokens. HASRD assigns the semantic representation to the first codebook, while encoding acoustic residuals in subsequent codebooks. This preserves ASR performance while achieving high-quality reconstruction. Additionally, we enhance HASRD's encoder efficiency, improving ASR performance without compromising reconstruction quality. Compared to SpeechTokenizer, HASRD achieves a 44% relative WER improvement, superior reconstruction quality, and 2x lower bitrate, demonstrating its effectiveness in disentangling acoustic and semantic information.",
    "metadata": {
      "arxiv_id": "2506.00843",
      "title": "HASRD: Hierarchical Acoustic and Semantic Representation Disentanglement",
      "summary": "Effective speech representations for spoken language models must balance semantic relevance with acoustic fidelity for high-quality reconstruction. However, existing approaches struggle to achieve both simultaneously. To address this, we introduce Hierarchical Acoustic and Semantic Representation Disentanglement (HASRD, pronounced `hazard'), a framework that factorizes self-supervised learning representations into discrete semantic and acoustic tokens. HASRD assigns the semantic representation to the first codebook, while encoding acoustic residuals in subsequent codebooks. This preserves ASR performance while achieving high-quality reconstruction. Additionally, we enhance HASRD's encoder efficiency, improving ASR performance without compromising reconstruction quality. Compared to SpeechTokenizer, HASRD achieves a 44% relative WER improvement, superior reconstruction quality, and 2x lower bitrate, demonstrating its effectiveness in disentangling acoustic and semantic information.",
      "authors": [
        "Amir Hussein",
        "Sameer Khurana",
        "Gordon Wichern",
        "Francois G. Germain",
        "Jonathan Le Roux"
      ],
      "published": "2025-06-01T05:38:39Z",
      "updated": "2025-06-01T05:38:39Z",
      "categories": [
        "eess.AS",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2506.00843v1",
      "landing_url": "https://arxiv.org/abs/2506.00843v1",
      "doi": "https://doi.org/10.48550/arXiv.2506.00843"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The abstract explicitly introduces discrete semantic and acoustic tokens via hierarchical codebooks for speech representations, evaluating tokenization as the core object of study, which matches the inclusion criteria and does not fit any exclusion.",
      "evaluation": 5
    },
    "round-A_JuniorNano_reasoning": "The abstract explicitly introduces discrete semantic and acoustic tokens via hierarchical codebooks for speech representations, evaluating tokenization as the core object of study, which matches the inclusion criteria and does not fit any exclusion.",
    "round-A_JuniorNano_evaluation": 5,
    "round-A_JuniorMini_output": {
      "reasoning": "The study explicitly proposes a method (HASRD) focused on discrete speech tokenization by factorizing speech into semantic and acoustic discrete tokens, aiming to improve speech representation for reconstruction and ASR, matching the inclusion criteria and avoiding the exclusion criteria.",
      "evaluation": 5
    },
    "round-A_JuniorMini_reasoning": "The study explicitly proposes a method (HASRD) focused on discrete speech tokenization by factorizing speech into semantic and acoustic discrete tokens, aiming to improve speech representation for reconstruction and ASR, matching the inclusion criteria and avoiding the exclusion criteria.",
    "round-A_JuniorMini_evaluation": 5,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "include (junior:5)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Benchmarking Neural Speech Codec Intelligibility with SITool",
    "abstract": "Speech intelligibility assessment is essential for evaluating neural speech codecs, yet most evaluation efforts focus on overall quality rather than intelligibility. Only a few publicly available tools exist for conducting standardized intelligibility tests, like the Diagnostic Rhyme Test (DRT) and Modified Rhyme Test (MRT). We introduce the Speech Intelligibility Toolkit for Subjective Evaluation (SITool), a Flask-based web application for conducting DRT and MRT in laboratory and crowdsourcing settings. We use SITool to benchmark 13 neural and traditional speech codecs, analyzing phoneme-level degradations and comparing subjective DRT results with objective intelligibility metrics. Our findings show that, while neural speech codecs can outperform traditional ones in subjective intelligibility, only STOI and ESTOI - not WER - significantly correlate with subjective results, although they struggle to capture gender and wordlist-specific variations observed in subjective evaluations.",
    "metadata": {
      "arxiv_id": "2506.01731",
      "title": "Benchmarking Neural Speech Codec Intelligibility with SITool",
      "summary": "Speech intelligibility assessment is essential for evaluating neural speech codecs, yet most evaluation efforts focus on overall quality rather than intelligibility. Only a few publicly available tools exist for conducting standardized intelligibility tests, like the Diagnostic Rhyme Test (DRT) and Modified Rhyme Test (MRT). We introduce the Speech Intelligibility Toolkit for Subjective Evaluation (SITool), a Flask-based web application for conducting DRT and MRT in laboratory and crowdsourcing settings. We use SITool to benchmark 13 neural and traditional speech codecs, analyzing phoneme-level degradations and comparing subjective DRT results with objective intelligibility metrics. Our findings show that, while neural speech codecs can outperform traditional ones in subjective intelligibility, only STOI and ESTOI - not WER - significantly correlate with subjective results, although they struggle to capture gender and wordlist-specific variations observed in subjective evaluations.",
      "authors": [
        "Anna Leschanowsky",
        "Kishor Kayyar Lakshminarayana",
        "Anjana Rajasekhar",
        "Lyonel Behringer",
        "Ibrahim Kilinc",
        "Guillaume Fuchs",
        "Emanuël A. P. Habets"
      ],
      "published": "2025-06-02T14:42:50Z",
      "updated": "2025-06-02T14:42:50Z",
      "categories": [
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2506.01731v1",
      "landing_url": "https://arxiv.org/abs/2506.01731v1",
      "doi": "https://doi.org/10.48550/arXiv.2506.01731"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The abstract describes evaluating neural speech codecs and intelligibility tests using SITool; it does not address discrete audio tokenization or token-based representations, so it does not meet the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The abstract describes evaluating neural speech codecs and intelligibility tests using SITool; it does not address discrete audio tokenization or token-based representations, so it does not meet the inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study introduces SITool, a tool for subjective intelligibility evaluation of neural speech codecs, and benchmarks codecs including neural and traditional types, but it does not propose, analyze, or evaluate discrete audio/speech tokenization methods themselves; rather, it focuses on speech intelligibility assessment and codec performance without discussing discrete token representations or quantization-based token design as a core object of study, which is required by the inclusion criteria. Therefore, it does not meet the inclusion criteria related to discrete tokenization analysis.",
      "evaluation": 2
    },
    "round-A_JuniorMini_reasoning": "The study introduces SITool, a tool for subjective intelligibility evaluation of neural speech codecs, and benchmarks codecs including neural and traditional types, but it does not propose, analyze, or evaluate discrete audio/speech tokenization methods themselves; rather, it focuses on speech intelligibility assessment and codec performance without discussing discrete token representations or quantization-based token design as a core object of study, which is required by the inclusion criteria. Therefore, it does not meet the inclusion criteria related to discrete tokenization analysis.",
    "round-A_JuniorMini_evaluation": 2,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:2)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:2)"
  },
  {
    "title": "Towards Efficient Speech-Text Jointly Decoding within One Speech Language Model",
    "abstract": "Speech language models (Speech LMs) enable end-to-end speech-text modelling within a single model, offering a promising direction for spoken dialogue systems. The choice of speech-text jointly decoding paradigm plays a critical role in performance, efficiency, and alignment quality. In this work, we systematically compare representative joint speech-text decoding strategies-including the interleaved, and parallel generation paradigms-under a controlled experimental setup using the same base language model, speech tokenizer and training data. Our results show that the interleaved approach achieves the best alignment. However it suffers from slow inference due to long token sequence length. To address this, we propose a novel early-stop interleaved (ESI) pattern that not only significantly accelerates decoding but also yields slightly better performance. Additionally, we curate high-quality question answering (QA) datasets to further improve speech QA performance.",
    "metadata": {
      "arxiv_id": "2506.04518",
      "title": "Towards Efficient Speech-Text Jointly Decoding within One Speech Language Model",
      "summary": "Speech language models (Speech LMs) enable end-to-end speech-text modelling within a single model, offering a promising direction for spoken dialogue systems. The choice of speech-text jointly decoding paradigm plays a critical role in performance, efficiency, and alignment quality. In this work, we systematically compare representative joint speech-text decoding strategies-including the interleaved, and parallel generation paradigms-under a controlled experimental setup using the same base language model, speech tokenizer and training data. Our results show that the interleaved approach achieves the best alignment. However it suffers from slow inference due to long token sequence length. To address this, we propose a novel early-stop interleaved (ESI) pattern that not only significantly accelerates decoding but also yields slightly better performance. Additionally, we curate high-quality question answering (QA) datasets to further improve speech QA performance.",
      "authors": [
        "Haibin Wu",
        "Yuxuan Hu",
        "Ruchao Fan",
        "Xiaofei Wang",
        "Kenichi Kumatani",
        "Bo Ren",
        "Jianwei Yu",
        "Heng Lu",
        "Lijuan Wang",
        "Yao Qian",
        "Jinyu Li"
      ],
      "published": "2025-06-04T23:53:49Z",
      "updated": "2025-06-13T03:55:18Z",
      "categories": [
        "eess.AS",
        "cs.CL"
      ],
      "pdf_url": "https://arxiv.org/pdf/2506.04518v2",
      "landing_url": "https://arxiv.org/abs/2506.04518v2",
      "doi": "https://doi.org/10.48550/arXiv.2506.04518"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The paper focuses on joint decoding strategies and decoding efficiency in a single Speech-LM; it does not analyze or propose discrete audio tokenization/quantization as a core object (tokens are only mentioned), so it should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The paper focuses on joint decoding strategies and decoding efficiency in a single Speech-LM; it does not analyze or propose discrete audio tokenization/quantization as a core object (tokens are only mentioned), so it should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on comparing joint speech-text decoding methods and proposes an early-stop interleaved pattern to accelerate decoding and improve performance; however, it does not explicitly mention proposing, analyzing, or evaluating discrete audio/speech tokenization as a core element where continuous audio signals are converted into discrete tokens, nor does it discuss discrete quantization/token design central to audio reconstruction or generation tasks, which does not fully meet the inclusion criteria.",
      "evaluation": 2
    },
    "round-A_JuniorMini_reasoning": "The study focuses on comparing joint speech-text decoding methods and proposes an early-stop interleaved pattern to accelerate decoding and improve performance; however, it does not explicitly mention proposing, analyzing, or evaluating discrete audio/speech tokenization as a core element where continuous audio signals are converted into discrete tokens, nor does it discuss discrete quantization/token design central to audio reconstruction or generation tasks, which does not fully meet the inclusion criteria.",
    "round-A_JuniorMini_evaluation": 2,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:2)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:2)"
  },
  {
    "title": "Kernel $k$-Medoids as General Vector Quantization",
    "abstract": "Vector Quantization (VQ) is a widely used technique in machine learning and data compression, valued for its simplicity and interpretability. Among hard VQ methods, $k$-medoids clustering and Kernel Density Estimation (KDE) approaches represent two prominent yet seemingly unrelated paradigms -- one distance-based, the other rooted in probability density matching. In this paper, we investigate their connection through the lens of Quadratic Unconstrained Binary Optimization (QUBO). We compare a heuristic QUBO formulation for $k$-medoids, which balances centrality and diversity, with a principled QUBO derived from minimizing Maximum Mean Discrepancy in KDE-based VQ. Surprisingly, we show that the KDE-QUBO is a special case of the $k$-medoids-QUBO under mild assumptions on the kernel's feature map. This reveals a deeper structural relationship between these two approaches and provides new insight into the geometric interpretation of the weighting parameters used in QUBO formulations for VQ.",
    "metadata": {
      "arxiv_id": "2506.04786",
      "title": "Kernel $k$-Medoids as General Vector Quantization",
      "summary": "Vector Quantization (VQ) is a widely used technique in machine learning and data compression, valued for its simplicity and interpretability. Among hard VQ methods, $k$-medoids clustering and Kernel Density Estimation (KDE) approaches represent two prominent yet seemingly unrelated paradigms -- one distance-based, the other rooted in probability density matching. In this paper, we investigate their connection through the lens of Quadratic Unconstrained Binary Optimization (QUBO). We compare a heuristic QUBO formulation for $k$-medoids, which balances centrality and diversity, with a principled QUBO derived from minimizing Maximum Mean Discrepancy in KDE-based VQ. Surprisingly, we show that the KDE-QUBO is a special case of the $k$-medoids-QUBO under mild assumptions on the kernel's feature map. This reveals a deeper structural relationship between these two approaches and provides new insight into the geometric interpretation of the weighting parameters used in QUBO formulations for VQ.",
      "authors": [
        "Thore Gerlach",
        "Sascha Mücke",
        "Christian Bauckhage"
      ],
      "published": "2025-06-05T09:14:25Z",
      "updated": "2025-09-04T20:11:34Z",
      "categories": [
        "cs.LG",
        "quant-ph"
      ],
      "pdf_url": "https://arxiv.org/pdf/2506.04786v2",
      "landing_url": "https://arxiv.org/abs/2506.04786v2",
      "doi": "https://doi.org/10.48550/arXiv.2506.04786"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The abstract addresses general vector quantization methods (k-medoids and KDE) and their QUBO formulation, with no explicit discrete audio/speech tokenization or evaluation for speech tasks.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The abstract addresses general vector quantization methods (k-medoids and KDE) and their QUBO formulation, with no explicit discrete audio/speech tokenization or evaluation for speech tasks.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study discusses kernel k-medoids as a general vector quantization method focusing on clustering and density estimation in a theoretical context without any explicit mention or focus on discrete audio/speech tokenization, nor addressing conversion of continuous audio signals to discrete symbols for speech/audio tasks, thus failing to meet the inclusion criteria and aligning with exclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study discusses kernel k-medoids as a general vector quantization method focusing on clustering and density estimation in a theoretical context without any explicit mention or focus on discrete audio/speech tokenization, nor addressing conversion of continuous audio signals to discrete symbols for speech/audio tasks, thus failing to meet the inclusion criteria and aligning with exclusion criteria.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "PCDVQ: Enhancing Vector Quantization for Large Language Models via Polar Coordinate Decoupling",
    "abstract": "Large Language Models (LLMs) face significant challenges in edge deployment due to their massive parameter scale. Vector Quantization (VQ), a clustering-based quantization method, serves as a prevalent solution to this issue for its extremely low-bit (even at 2-bit) and considerable accuracy. Since a vector is a quantity in mathematics and physics that has both direction and magnitude, existing VQ works typically quantize them in a coupled manner. However, we find that direction exhibits significantly greater sensitivity to quantization compared to the magnitude. For instance, when separately clustering the directions and magnitudes of weight vectors in LLaMA-2-7B, the accuracy drop of zero-shot tasks are 46.5\\% and 2.3\\%, respectively. This gap even increases with the reduction of clustering centers. Further, Euclidean distance, a common metric to access vector similarities in current VQ works, places greater emphasis on reducing the magnitude error. This property is contrary to the above finding, unavoidably leading to larger quantization errors. To these ends, this paper proposes Polar Coordinate Decoupled Vector Quantization (PCDVQ), an effective and efficient VQ framework consisting of two key modules: 1) Polar Coordinate Decoupling (PCD), which transforms vectors into their polar coordinate representations and perform independent quantization of the direction and magnitude parameters.2) Distribution Aligned Codebook Construction (DACC), which optimizes the direction and magnitude codebooks in accordance with the source distribution. Experimental results show that PCDVQ outperforms baseline methods at 2-bit level by at least 1.5\\% zero-shot accuracy, establishing a novel paradigm for accurate and highly compressed LLMs.",
    "metadata": {
      "arxiv_id": "2506.05432",
      "title": "PCDVQ: Enhancing Vector Quantization for Large Language Models via Polar Coordinate Decoupling",
      "summary": "Large Language Models (LLMs) face significant challenges in edge deployment due to their massive parameter scale. Vector Quantization (VQ), a clustering-based quantization method, serves as a prevalent solution to this issue for its extremely low-bit (even at 2-bit) and considerable accuracy. Since a vector is a quantity in mathematics and physics that has both direction and magnitude, existing VQ works typically quantize them in a coupled manner. However, we find that direction exhibits significantly greater sensitivity to quantization compared to the magnitude. For instance, when separately clustering the directions and magnitudes of weight vectors in LLaMA-2-7B, the accuracy drop of zero-shot tasks are 46.5\\% and 2.3\\%, respectively. This gap even increases with the reduction of clustering centers. Further, Euclidean distance, a common metric to access vector similarities in current VQ works, places greater emphasis on reducing the magnitude error. This property is contrary to the above finding, unavoidably leading to larger quantization errors. To these ends, this paper proposes Polar Coordinate Decoupled Vector Quantization (PCDVQ), an effective and efficient VQ framework consisting of two key modules: 1) Polar Coordinate Decoupling (PCD), which transforms vectors into their polar coordinate representations and perform independent quantization of the direction and magnitude parameters.2) Distribution Aligned Codebook Construction (DACC), which optimizes the direction and magnitude codebooks in accordance with the source distribution. Experimental results show that PCDVQ outperforms baseline methods at 2-bit level by at least 1.5\\% zero-shot accuracy, establishing a novel paradigm for accurate and highly compressed LLMs.",
      "authors": [
        "Yuxuan Yue",
        "Zukang Xu",
        "Zhihang Yuan",
        "Dawei Yang",
        "Jianlong Wu",
        "Liqiang Nie"
      ],
      "published": "2025-06-05T08:58:58Z",
      "updated": "2025-06-26T06:17:49Z",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2506.05432v2",
      "landing_url": "https://arxiv.org/abs/2506.05432v2",
      "doi": "https://doi.org/10.48550/arXiv.2506.05432"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The paper is about polarization-decoupled vector quantization for large language models and does not analyze discrete audio/speech tokenization or discrete token representations for audio.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The paper is about polarization-decoupled vector quantization for large language models and does not analyze discrete audio/speech tokenization or discrete token representations for audio.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on enhancing vector quantization methods for large language models, specifically on the quantization of model weights represented as vectors; it does not deal with discrete audio/speech tokenization or convert continuous audio signals into discrete token sequences, nor does it analyze discrete audio representations. Therefore, it does not meet the inclusion criteria regarding discrete audio tokenization and is unrelated to audio/speech tokenization tasks.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on enhancing vector quantization methods for large language models, specifically on the quantization of model weights represented as vectors; it does not deal with discrete audio/speech tokenization or convert continuous audio signals into discrete token sequences, nor does it analyze discrete audio representations. Therefore, it does not meet the inclusion criteria regarding discrete audio tokenization and is unrelated to audio/speech tokenization tasks.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Modified K-means Algorithm with Local Optimality Guarantees",
    "abstract": "The K-means algorithm is one of the most widely studied clustering algorithms in machine learning. While extensive research has focused on its ability to achieve a globally optimal solution, there still lacks a rigorous analysis of its local optimality guarantees. In this paper, we first present conditions under which the K-means algorithm converges to a locally optimal solution. Based on this, we propose simple modifications to the K-means algorithm which ensure local optimality in both the continuous and discrete sense, with the same computational complexity as the original K-means algorithm. As the dissimilarity measure, we consider a general Bregman divergence, which is an extension of the squared Euclidean distance often used in the K-means algorithm. Numerical experiments confirm that the K-means algorithm does not always find a locally optimal solution in practice, while our proposed methods provide improved locally optimal solutions with reduced clustering loss. Our code is available at https://github.com/lmingyi/LO-K-means.",
    "metadata": {
      "arxiv_id": "2506.06990",
      "title": "Modified K-means Algorithm with Local Optimality Guarantees",
      "summary": "The K-means algorithm is one of the most widely studied clustering algorithms in machine learning. While extensive research has focused on its ability to achieve a globally optimal solution, there still lacks a rigorous analysis of its local optimality guarantees. In this paper, we first present conditions under which the K-means algorithm converges to a locally optimal solution. Based on this, we propose simple modifications to the K-means algorithm which ensure local optimality in both the continuous and discrete sense, with the same computational complexity as the original K-means algorithm. As the dissimilarity measure, we consider a general Bregman divergence, which is an extension of the squared Euclidean distance often used in the K-means algorithm. Numerical experiments confirm that the K-means algorithm does not always find a locally optimal solution in practice, while our proposed methods provide improved locally optimal solutions with reduced clustering loss. Our code is available at https://github.com/lmingyi/LO-K-means.",
      "authors": [
        "Mingyi Li",
        "Michael R. Metel",
        "Akiko Takeda"
      ],
      "published": "2025-06-08T04:37:28Z",
      "updated": "2025-06-11T06:52:53Z",
      "categories": [
        "cs.LG",
        "math.OC",
        "stat.ML"
      ],
      "pdf_url": "https://arxiv.org/pdf/2506.06990v2",
      "landing_url": "https://arxiv.org/abs/2506.06990v2",
      "doi": "https://doi.org/10.48550/arXiv.2506.06990"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The article discusses a modified K-means algorithm with local optimality guarantees and discusses general clustering with Bregman divergences; it does not address discrete audio/speech tokenization or discrete token representations for speech tasks, so it does not meet the inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The article discusses a modified K-means algorithm with local optimality guarantees and discusses general clustering with Bregman divergences; it does not address discrete audio/speech tokenization or discrete token representations for speech tasks, so it does not meet the inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on algorithmic improvements to the K-means clustering algorithm and does not propose, analyze, or evaluate any discrete audio/speech tokenization methods; it lacks any discussion about conversion of continuous audio signals into discrete tokens or related discrete token representations for audio tasks, which is essential for inclusion.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on algorithmic improvements to the K-means clustering algorithm and does not propose, analyze, or evaluate any discrete audio/speech tokenization methods; it lacks any discussion about conversion of continuous audio signals into discrete tokens or related discrete token representations for audio tasks, which is essential for inclusion.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Audio synthesizer inversion in symmetric parameter spaces with approximately equivariant flow matching",
    "abstract": "Many audio synthesizers can produce the same signal given different parameter configurations, meaning the inversion from sound to parameters is an inherently ill-posed problem. We show that this is largely due to intrinsic symmetries of the synthesizer, and focus in particular on permutation invariance. First, we demonstrate on a synthetic task that regressing point estimates under permutation symmetry degrades performance, even when using a permutation-invariant loss function or symmetry-breaking heuristics. Then, viewing equivalent solutions as modes of a probability distribution, we show that a conditional generative model substantially improves performance. Further, acknowledging the invariance of the implicit parameter distribution, we find that performance is further improved by using a permutation equivariant continuous normalizing flow. To accommodate intricate symmetries in real synthesizers, we also propose a relaxed equivariance strategy that adaptively discovers relevant symmetries from data. Applying our method to Surge XT, a full-featured open source synthesizer used in real world audio production, we find our method outperforms regression and generative baselines across audio reconstruction metrics.",
    "metadata": {
      "arxiv_id": "2506.07199",
      "title": "Audio synthesizer inversion in symmetric parameter spaces with approximately equivariant flow matching",
      "summary": "Many audio synthesizers can produce the same signal given different parameter configurations, meaning the inversion from sound to parameters is an inherently ill-posed problem. We show that this is largely due to intrinsic symmetries of the synthesizer, and focus in particular on permutation invariance. First, we demonstrate on a synthetic task that regressing point estimates under permutation symmetry degrades performance, even when using a permutation-invariant loss function or symmetry-breaking heuristics. Then, viewing equivalent solutions as modes of a probability distribution, we show that a conditional generative model substantially improves performance. Further, acknowledging the invariance of the implicit parameter distribution, we find that performance is further improved by using a permutation equivariant continuous normalizing flow. To accommodate intricate symmetries in real synthesizers, we also propose a relaxed equivariance strategy that adaptively discovers relevant symmetries from data. Applying our method to Surge XT, a full-featured open source synthesizer used in real world audio production, we find our method outperforms regression and generative baselines across audio reconstruction metrics.",
      "authors": [
        "Ben Hayes",
        "Charalampos Saitis",
        "György Fazekas"
      ],
      "published": "2025-06-08T15:47:44Z",
      "updated": "2025-06-08T15:47:44Z",
      "categories": [
        "cs.SD",
        "cs.LG",
        "eess.AS",
        "eess.SP"
      ],
      "pdf_url": "https://arxiv.org/pdf/2506.07199v1",
      "landing_url": "https://arxiv.org/abs/2506.07199v1",
      "doi": "https://doi.org/10.48550/arXiv.2506.07199"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The paper deals with continuous parameter estimation and generative inversion for audio synthesizers, focusing on permutation symmetry and flows; it does not address discrete audio/speech tokenization or tokenization-based representations, so it should be excluded under the tokenization criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The paper deals with continuous parameter estimation and generative inversion for audio synthesizers, focusing on permutation symmetry and flows; it does not address discrete audio/speech tokenization or tokenization-based representations, so it should be excluded under the tokenization criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on inverting audio synthesizer parameters using generative models and equivariant flows to handle symmetries, without addressing discrete audio/speech tokenization or converting continuous audio signals into sequences of discrete symbols; thus, it does not meet the inclusion criteria centered on discrete tokenization nor does it contradict any exclusion criteria specifically.",
      "evaluation": 2
    },
    "round-A_JuniorMini_reasoning": "The study focuses on inverting audio synthesizer parameters using generative models and equivariant flows to handle symmetries, without addressing discrete audio/speech tokenization or converting continuous audio signals into sequences of discrete symbols; thus, it does not meet the inclusion criteria centered on discrete tokenization nor does it contradict any exclusion criteria specifically.",
    "round-A_JuniorMini_evaluation": 2,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:2)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:2)"
  },
  {
    "title": "VIVAT: Virtuous Improving VAE Training through Artifact Mitigation",
    "abstract": "Variational Autoencoders (VAEs) remain a cornerstone of generative computer vision, yet their training is often plagued by artifacts that degrade reconstruction and generation quality. This paper introduces VIVAT, a systematic approach to mitigating common artifacts in KL-VAE training without requiring radical architectural changes. We present a detailed taxonomy of five prevalent artifacts - color shift, grid patterns, blur, corner and droplet artifacts - and analyze their root causes. Through straightforward modifications, including adjustments to loss weights, padding strategies, and the integration of Spatially Conditional Normalization, we demonstrate significant improvements in VAE performance. Our method achieves state-of-the-art results in image reconstruction metrics (PSNR and SSIM) across multiple benchmarks and enhances text-to-image generation quality, as evidenced by superior CLIP scores. By preserving the simplicity of the KL-VAE framework while addressing its practical challenges, VIVAT offers actionable insights for researchers and practitioners aiming to optimize VAE training.",
    "metadata": {
      "arxiv_id": "2506.07863",
      "title": "VIVAT: Virtuous Improving VAE Training through Artifact Mitigation",
      "summary": "Variational Autoencoders (VAEs) remain a cornerstone of generative computer vision, yet their training is often plagued by artifacts that degrade reconstruction and generation quality. This paper introduces VIVAT, a systematic approach to mitigating common artifacts in KL-VAE training without requiring radical architectural changes. We present a detailed taxonomy of five prevalent artifacts - color shift, grid patterns, blur, corner and droplet artifacts - and analyze their root causes. Through straightforward modifications, including adjustments to loss weights, padding strategies, and the integration of Spatially Conditional Normalization, we demonstrate significant improvements in VAE performance. Our method achieves state-of-the-art results in image reconstruction metrics (PSNR and SSIM) across multiple benchmarks and enhances text-to-image generation quality, as evidenced by superior CLIP scores. By preserving the simplicity of the KL-VAE framework while addressing its practical challenges, VIVAT offers actionable insights for researchers and practitioners aiming to optimize VAE training.",
      "authors": [
        "Lev Novitskiy",
        "Viacheslav Vasilev",
        "Maria Kovaleva",
        "Vladimir Arkhipkin",
        "Denis Dimitrov"
      ],
      "published": "2025-06-09T15:27:03Z",
      "updated": "2025-12-01T18:51:34Z",
      "categories": [
        "cs.CV",
        "cs.LG",
        "cs.MM"
      ],
      "pdf_url": "https://arxiv.org/pdf/2506.07863v2",
      "landing_url": "https://arxiv.org/abs/2506.07863v2",
      "doi": "https://doi.org/10.48550/arXiv.2506.07863"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This item targets image VAEs and artifact mitigation, with no discrete audio tokenization or speech token analysis, so it does not meet the inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "This item targets image VAEs and artifact mitigation, with no discrete audio tokenization or speech token analysis, so it does not meet the inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on mitigating artifacts in Variational Autoencoder training for generative computer vision tasks and does not mention any discrete audio/speech tokenization or related analysis required by the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on mitigating artifacts in Variational Autoencoder training for generative computer vision tasks and does not mention any discrete audio/speech tokenization or related analysis required by the inclusion criteria.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "DrVoice: Parallel Speech-Text Voice Conversation Model via Dual-Resolution Speech Representations",
    "abstract": "Recent studies on end-to-end (E2E) speech generation with large language models (LLMs) have attracted significant community attention, with multiple works extending text-based LLMs to generate discrete speech tokens. Existing E2E approaches primarily fall into two categories: (1) Methods that generate discrete speech tokens independently without incorporating them into the LLM's autoregressive process, resulting in text generation being unaware of concurrent speech synthesis. (2) Models that generate interleaved or parallel speech-text tokens through joint autoregressive modeling, enabling mutual modality awareness during generation. This paper presents DrVoice, a parallel speech-text voice conversation model based on joint autoregressive modeling, featuring dual-resolution speech representations. Notably, while current methods utilize mainly 12.5Hz input audio representation, our proposed dual-resolution mechanism reduces the input frequency for the LLM to 5Hz, significantly reducing computational cost and alleviating the frequency discrepancy between speech and text tokens and in turn better exploiting LLMs' capabilities. Experimental results demonstrate that DrVoice-7B establishes new state-of-the-art (SOTA) on prominent speech benchmarks including OpenAudioBench, VoiceBench, UltraEval-Audio and Big Bench Audio, making it a leading open-source speech foundation model in ~7B models.",
    "metadata": {
      "arxiv_id": "2506.09349",
      "title": "DrVoice: Parallel Speech-Text Voice Conversation Model via Dual-Resolution Speech Representations",
      "summary": "Recent studies on end-to-end (E2E) speech generation with large language models (LLMs) have attracted significant community attention, with multiple works extending text-based LLMs to generate discrete speech tokens. Existing E2E approaches primarily fall into two categories: (1) Methods that generate discrete speech tokens independently without incorporating them into the LLM's autoregressive process, resulting in text generation being unaware of concurrent speech synthesis. (2) Models that generate interleaved or parallel speech-text tokens through joint autoregressive modeling, enabling mutual modality awareness during generation. This paper presents DrVoice, a parallel speech-text voice conversation model based on joint autoregressive modeling, featuring dual-resolution speech representations. Notably, while current methods utilize mainly 12.5Hz input audio representation, our proposed dual-resolution mechanism reduces the input frequency for the LLM to 5Hz, significantly reducing computational cost and alleviating the frequency discrepancy between speech and text tokens and in turn better exploiting LLMs' capabilities. Experimental results demonstrate that DrVoice-7B establishes new state-of-the-art (SOTA) on prominent speech benchmarks including OpenAudioBench, VoiceBench, UltraEval-Audio and Big Bench Audio, making it a leading open-source speech foundation model in ~7B models.",
      "authors": [
        "Chao-Hong Tan",
        "Qian Chen",
        "Wen Wang",
        "Chong Deng",
        "Qinglin Zhang",
        "Luyao Cheng",
        "Hai Yu",
        "Xin Zhang",
        "Xiang Lv",
        "Tianyu Zhao",
        "Chong Zhang",
        "Yukun Ma",
        "Yafeng Chen",
        "Hui Wang",
        "Jiaqing Liu",
        "Xiangang Li",
        "Jieping Ye"
      ],
      "published": "2025-06-11T02:57:22Z",
      "updated": "2025-12-23T08:50:59Z",
      "categories": [
        "cs.CL"
      ],
      "pdf_url": "https://arxiv.org/pdf/2506.09349v4",
      "landing_url": "https://arxiv.org/abs/2506.09349v4",
      "doi": "https://doi.org/10.48550/arXiv.2506.09349"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The abstract describes a parallel speech-text model with dual-resolution representations but does not explicitly propose, analyze, or evaluate discrete audio/speech tokenization as a core object of study.",
      "evaluation": 2
    },
    "round-A_JuniorNano_reasoning": "The abstract describes a parallel speech-text model with dual-resolution representations but does not explicitly propose, analyze, or evaluate discrete audio/speech tokenization as a core object of study.",
    "round-A_JuniorNano_evaluation": 2,
    "round-A_JuniorMini_output": {
      "reasoning": "The study presents DrVoice, a speech-text voice conversation model that uses dual-resolution speech representations and joint autoregressive modeling to reduce computational cost and improve generation; however, the abstract does not explicitly mention proposing, analyzing, or evaluating discrete audio/speech tokenization as the core object of study, nor does it clearly detail discrete tokenization methods, focusing more on model architecture and benchmark results; thus, it is unclear whether the study meets the inclusion criteria about discrete tokenization analysis specifically.",
      "evaluation": 3
    },
    "round-A_JuniorMini_reasoning": "The study presents DrVoice, a speech-text voice conversation model that uses dual-resolution speech representations and joint autoregressive modeling to reduce computational cost and improve generation; however, the abstract does not explicitly mention proposing, analyzing, or evaluating discrete audio/speech tokenization as the core object of study, nor does it clearly detail discrete tokenization methods, focusing more on model architecture and benchmark results; thus, it is unclear whether the study meets the inclusion criteria about discrete tokenization analysis specifically.",
    "round-A_JuniorMini_evaluation": 3,
    "round-B_SeniorLead_output": {
      "reasoning": "Both junior reviewers observed that while the abstract situates the work in the space of LLMs generating discrete speech tokens and describes a joint autoregressive model with dual-resolution speech representations, it does not explicitly propose, analyze, or evaluate a discrete audio/speech tokenization method as the core contribution; the focus is on model architecture, input frequency reduction, and benchmark performance, and the abstract gives no clear evidence of methodological or experimental treatment of discrete-token design/quantization (nor confirmation that the full paper explicitly does so). Given the inclusion criterion requires explicit tokenization proposal/analysis and the exclusion concerns unclear or absent discrete quantization, this item is better excluded.",
      "evaluation": 2
    },
    "round-B_SeniorLead_reasoning": "Both junior reviewers observed that while the abstract situates the work in the space of LLMs generating discrete speech tokens and describes a joint autoregressive model with dual-resolution speech representations, it does not explicitly propose, analyze, or evaluate a discrete audio/speech tokenization method as the core contribution; the focus is on model architecture, input frequency reduction, and benchmark performance, and the abstract gives no clear evidence of methodological or experimental treatment of discrete-token design/quantization (nor confirmation that the full paper explicitly does so). Given the inclusion criterion requires explicit tokenization proposal/analysis and the exclusion concerns unclear or absent discrete quantization, this item is better excluded.",
    "round-B_SeniorLead_evaluation": 2,
    "final_verdict": "exclude (senior:2)",
    "review_skipped": false,
    "discard_reason": "exclude (senior:2)"
  },
  {
    "title": "A Study on Speech Assessment with Visual Cues",
    "abstract": "Non-intrusive assessment of speech quality and intelligibility is essential when clean reference signals are unavailable. In this work, we propose a multimodal framework that integrates audio features and visual cues to predict PESQ and STOI scores. It employs a dual-branch architecture, where spectral features are extracted using STFT, and visual embeddings are obtained via a visual encoder. These features are then fused and processed by a CNN-BLSTM with attention, followed by multi-task learning to simultaneously predict PESQ and STOI. Evaluations on the LRS3-TED dataset, augmented with noise from the DEMAND corpus, show that our model outperforms the audio-only baseline. Under seen noise conditions, it improves LCC by 9.61% (0.8397->0.9205) for PESQ and 11.47% (0.7403->0.8253) for STOI. These results highlight the effectiveness of incorporating visual cues in enhancing the accuracy of non-intrusive speech assessment.",
    "metadata": {
      "arxiv_id": "2506.09549",
      "title": "A Study on Speech Assessment with Visual Cues",
      "summary": "Non-intrusive assessment of speech quality and intelligibility is essential when clean reference signals are unavailable. In this work, we propose a multimodal framework that integrates audio features and visual cues to predict PESQ and STOI scores. It employs a dual-branch architecture, where spectral features are extracted using STFT, and visual embeddings are obtained via a visual encoder. These features are then fused and processed by a CNN-BLSTM with attention, followed by multi-task learning to simultaneously predict PESQ and STOI. Evaluations on the LRS3-TED dataset, augmented with noise from the DEMAND corpus, show that our model outperforms the audio-only baseline. Under seen noise conditions, it improves LCC by 9.61% (0.8397->0.9205) for PESQ and 11.47% (0.7403->0.8253) for STOI. These results highlight the effectiveness of incorporating visual cues in enhancing the accuracy of non-intrusive speech assessment.",
      "authors": [
        "Shafique Ahmed",
        "Ryandhimas E. Zezario",
        "Nasir Saleem",
        "Amir Hussain",
        "Hsin-Min Wang",
        "Yu Tsao"
      ],
      "published": "2025-06-11T09:32:12Z",
      "updated": "2025-06-11T09:32:12Z",
      "categories": [
        "eess.AS",
        "cs.SD",
        "eess.SP"
      ],
      "pdf_url": "https://arxiv.org/pdf/2506.09549v1",
      "landing_url": "https://arxiv.org/abs/2506.09549v1",
      "doi": "https://doi.org/10.48550/arXiv.2506.09549"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The abstract describes a multimodal, continuous-audio-based approach to predict PESQ/STOI without any discrete audio tokenization or token design, so it does not meet the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The abstract describes a multimodal, continuous-audio-based approach to predict PESQ/STOI without any discrete audio tokenization or token design, so it does not meet the inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on non-intrusive speech quality assessment using continuous audio features (STFT spectral features) and visual embeddings, without mentioning any discrete audio or speech tokenization or quantization methods; thus, it does not meet the core inclusion criterion regarding discrete tokenization.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on non-intrusive speech quality assessment using continuous audio features (STFT spectral features) and visual embeddings, without mentioning any discrete audio or speech tokenization or quantization methods; thus, it does not meet the core inclusion criterion regarding discrete tokenization.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Factorized RVQ-GAN For Disentangled Speech Tokenization",
    "abstract": "We propose Hierarchical Audio Codec (HAC), a unified neural speech codec that factorizes its bottleneck into three linguistic levels-acoustic, phonetic, and lexical-within a single model. HAC leverages two knowledge distillation objectives: one from a pre-trained speech encoder (HuBERT) for phoneme-level structure, and another from a text-based encoder (LaBSE) for lexical cues. Experiments on English and multilingual data show that HAC's factorized bottleneck yields disentangled token sets: one aligns with phonemes, while another captures word-level semantics. Quantitative evaluations confirm that HAC tokens preserve naturalness and provide interpretable linguistic information, outperforming single-level baselines in both disentanglement and reconstruction quality. These findings underscore HAC's potential as a unified discrete speech representation, bridging acoustic detail and lexical meaning for downstream speech generation and understanding tasks.",
    "metadata": {
      "arxiv_id": "2506.15456",
      "title": "Factorized RVQ-GAN For Disentangled Speech Tokenization",
      "summary": "We propose Hierarchical Audio Codec (HAC), a unified neural speech codec that factorizes its bottleneck into three linguistic levels-acoustic, phonetic, and lexical-within a single model. HAC leverages two knowledge distillation objectives: one from a pre-trained speech encoder (HuBERT) for phoneme-level structure, and another from a text-based encoder (LaBSE) for lexical cues. Experiments on English and multilingual data show that HAC's factorized bottleneck yields disentangled token sets: one aligns with phonemes, while another captures word-level semantics. Quantitative evaluations confirm that HAC tokens preserve naturalness and provide interpretable linguistic information, outperforming single-level baselines in both disentanglement and reconstruction quality. These findings underscore HAC's potential as a unified discrete speech representation, bridging acoustic detail and lexical meaning for downstream speech generation and understanding tasks.",
      "authors": [
        "Sameer Khurana",
        "Dominik Klement",
        "Antoine Laurent",
        "Dominik Bobos",
        "Juraj Novosad",
        "Peter Gazdik",
        "Ellen Zhang",
        "Zili Huang",
        "Amir Hussein",
        "Ricard Marxer",
        "Yoshiki Masuyama",
        "Ryo Aihara",
        "Chiori Hori",
        "Francois G. Germain",
        "Gordon Wichern",
        "Jonathan Le Roux"
      ],
      "published": "2025-06-18T13:36:34Z",
      "updated": "2025-06-18T13:36:34Z",
      "categories": [
        "eess.AS",
        "cs.CL",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2506.15456v1",
      "landing_url": "https://arxiv.org/abs/2506.15456v1",
      "doi": "https://doi.org/10.48550/arXiv.2506.15456"
    },
    "round-A_JuniorNano_output": null,
    "round-A_JuniorNano_reasoning": null,
    "round-A_JuniorNano_evaluation": null,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "discard (published_on_or_after_cutoff:2025-06-18)",
    "review_skipped": true,
    "discard_reason": "published_on_or_after_cutoff:2025-06-18"
  },
  {
    "title": "LM-SPT: LM-Aligned Semantic Distillation for Speech Tokenization",
    "abstract": "With the rapid progress of speech language models (SLMs), discrete speech tokens have emerged as a core interface between speech and text, enabling unified modeling across modalities. Recent speech tokenization approaches aim to isolate semantic information from low-level acoustics to better align with language models. In particular, previous methods use SSL teachers such as HuBERT to extract semantic representations, which are then distilled into a semantic quantizer to suppress acoustic redundancy as well as capture content-related latent structures. However, they still produce speech token sequences significantly longer than their textual counterparts, creating challenges for efficient speech-language modeling. Reducing the frame rate is a natural solution, but standard techniques, such as rigid average pooling across frames, can distort or dilute the semantic structure required for effective LM alignment. To address this, we propose LM-SPT, a speech tokenization method that introduces a novel semantic distillation. Instead of directly matching teacher and student features via pooling, we reconstruct speech solely from semantic tokens and minimize the discrepancy between the encoded representations of the original and reconstructed waveforms, obtained from a frozen automatic speech recognition (ASR) encoder. This indirect yet data-driven supervision enables the tokenizer to learn discrete units that are more semantically aligned with language models. LM-SPT further incorporates architectural improvements to the encoder and decoder for speech tokenization, and supports multiple frame rates, including 25Hz, 12.5Hz, and 6.25Hz. Experimental results show that LM-SPT achieves superior reconstruction fidelity compared to baselines, and that SLMs trained with LM-SPT tokens achieve competitive performances on speech-to-text and consistently outperform baselines on text-to-speech tasks.",
    "metadata": {
      "arxiv_id": "2506.16738",
      "title": "LM-SPT: LM-Aligned Semantic Distillation for Speech Tokenization",
      "summary": "With the rapid progress of speech language models (SLMs), discrete speech tokens have emerged as a core interface between speech and text, enabling unified modeling across modalities. Recent speech tokenization approaches aim to isolate semantic information from low-level acoustics to better align with language models. In particular, previous methods use SSL teachers such as HuBERT to extract semantic representations, which are then distilled into a semantic quantizer to suppress acoustic redundancy as well as capture content-related latent structures. However, they still produce speech token sequences significantly longer than their textual counterparts, creating challenges for efficient speech-language modeling. Reducing the frame rate is a natural solution, but standard techniques, such as rigid average pooling across frames, can distort or dilute the semantic structure required for effective LM alignment. To address this, we propose LM-SPT, a speech tokenization method that introduces a novel semantic distillation. Instead of directly matching teacher and student features via pooling, we reconstruct speech solely from semantic tokens and minimize the discrepancy between the encoded representations of the original and reconstructed waveforms, obtained from a frozen automatic speech recognition (ASR) encoder. This indirect yet data-driven supervision enables the tokenizer to learn discrete units that are more semantically aligned with language models. LM-SPT further incorporates architectural improvements to the encoder and decoder for speech tokenization, and supports multiple frame rates, including 25Hz, 12.5Hz, and 6.25Hz. Experimental results show that LM-SPT achieves superior reconstruction fidelity compared to baselines, and that SLMs trained with LM-SPT tokens achieve competitive performances on speech-to-text and consistently outperform baselines on text-to-speech tasks.",
      "authors": [
        "Daejin Jo",
        "Jeeyoung Yun",
        "Byungseok Roh",
        "Sungwoong Kim"
      ],
      "published": "2025-06-20T04:15:14Z",
      "updated": "2025-06-20T04:15:14Z",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2506.16738v1",
      "landing_url": "https://arxiv.org/abs/2506.16738v1",
      "doi": "https://doi.org/10.48550/arXiv.2506.16738"
    },
    "round-A_JuniorNano_output": null,
    "round-A_JuniorNano_reasoning": null,
    "round-A_JuniorNano_evaluation": null,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "discard (published_on_or_after_cutoff:2025-06-20)",
    "review_skipped": true,
    "discard_reason": "published_on_or_after_cutoff:2025-06-20"
  },
  {
    "title": "Quantifying Fairness in LLMs Beyond Tokens: A Semantic and Statistical Perspective",
    "abstract": "Large Language Models (LLMs) often generate responses with inherent biases, undermining their reliability in real-world applications. Existing evaluation methods often overlook biases in long-form responses and the intrinsic variability of LLM outputs. To address these challenges, we propose FiSCo (Fine-grained Semantic Comparison), a novel statistical framework to evaluate group-level fairness in LLMs by detecting subtle semantic differences in long-form responses across demographic groups. Unlike prior work focusing on sentiment or token-level comparisons, FiSCo goes beyond surface-level analysis by operating at the claim level, leveraging entailment checks to assess the consistency of meaning across responses. We decompose model outputs into semantically distinct claims and apply statistical hypothesis testing to compare inter- and intra-group similarities, enabling robust detection of subtle biases. We formalize a new group counterfactual fairness definition and validate FiSCo on both synthetic and human-annotated datasets spanning gender, race, and age. Experiments show that FiSCo more reliably identifies nuanced biases while reducing the impact of stochastic LLM variability, outperforming various evaluation metrics.",
    "metadata": {
      "arxiv_id": "2506.19028",
      "title": "Quantifying Fairness in LLMs Beyond Tokens: A Semantic and Statistical Perspective",
      "summary": "Large Language Models (LLMs) often generate responses with inherent biases, undermining their reliability in real-world applications. Existing evaluation methods often overlook biases in long-form responses and the intrinsic variability of LLM outputs. To address these challenges, we propose FiSCo (Fine-grained Semantic Comparison), a novel statistical framework to evaluate group-level fairness in LLMs by detecting subtle semantic differences in long-form responses across demographic groups. Unlike prior work focusing on sentiment or token-level comparisons, FiSCo goes beyond surface-level analysis by operating at the claim level, leveraging entailment checks to assess the consistency of meaning across responses. We decompose model outputs into semantically distinct claims and apply statistical hypothesis testing to compare inter- and intra-group similarities, enabling robust detection of subtle biases. We formalize a new group counterfactual fairness definition and validate FiSCo on both synthetic and human-annotated datasets spanning gender, race, and age. Experiments show that FiSCo more reliably identifies nuanced biases while reducing the impact of stochastic LLM variability, outperforming various evaluation metrics.",
      "authors": [
        "Weijie Xu",
        "Yiwen Wang",
        "Chi Xue",
        "Xiangkun Hu",
        "Xi Fang",
        "Guimin Dong",
        "Chandan K. Reddy"
      ],
      "published": "2025-06-23T18:31:22Z",
      "updated": "2025-10-10T17:14:10Z",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.CY"
      ],
      "pdf_url": "https://arxiv.org/pdf/2506.19028v5",
      "landing_url": "https://arxiv.org/abs/2506.19028v5",
      "doi": "https://doi.org/10.48550/arXiv.2506.19028"
    },
    "round-A_JuniorNano_output": null,
    "round-A_JuniorNano_reasoning": null,
    "round-A_JuniorNano_evaluation": null,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "discard (published_on_or_after_cutoff:2025-06-23)",
    "review_skipped": true,
    "discard_reason": "published_on_or_after_cutoff:2025-06-23"
  },
  {
    "title": "Autoregressive Speech Enhancement via Acoustic Tokens",
    "abstract": "In speech processing pipelines, improving the quality and intelligibility of real-world recordings is crucial. While supervised regression is the primary method for speech enhancement, audio tokenization is emerging as a promising alternative for a smooth integration with other modalities. However, research on speech enhancement using discrete representations is still limited. Previous work has mainly focused on semantic tokens, which tend to discard key acoustic details such as speaker identity. Additionally, these studies typically employ non-autoregressive models, assuming conditional independence of outputs and overlooking the potential improvements offered by autoregressive modeling. To address these gaps we: 1) conduct a comprehensive study of the performance of acoustic tokens for speech enhancement, including the effect of bitrate and noise strength; 2) introduce a novel transducer-based autoregressive architecture specifically designed for this task. Experiments on VoiceBank and Libri1Mix datasets show that acoustic tokens outperform semantic tokens in terms of preserving speaker identity, and that our autoregressive approach can further improve performance. Nevertheless, we observe that discrete representations still fall short compared to continuous ones, highlighting the need for further research in this area.",
    "metadata": {
      "arxiv_id": "2507.12825",
      "title": "Autoregressive Speech Enhancement via Acoustic Tokens",
      "summary": "In speech processing pipelines, improving the quality and intelligibility of real-world recordings is crucial. While supervised regression is the primary method for speech enhancement, audio tokenization is emerging as a promising alternative for a smooth integration with other modalities. However, research on speech enhancement using discrete representations is still limited. Previous work has mainly focused on semantic tokens, which tend to discard key acoustic details such as speaker identity. Additionally, these studies typically employ non-autoregressive models, assuming conditional independence of outputs and overlooking the potential improvements offered by autoregressive modeling. To address these gaps we: 1) conduct a comprehensive study of the performance of acoustic tokens for speech enhancement, including the effect of bitrate and noise strength; 2) introduce a novel transducer-based autoregressive architecture specifically designed for this task. Experiments on VoiceBank and Libri1Mix datasets show that acoustic tokens outperform semantic tokens in terms of preserving speaker identity, and that our autoregressive approach can further improve performance. Nevertheless, we observe that discrete representations still fall short compared to continuous ones, highlighting the need for further research in this area.",
      "authors": [
        "Luca Della Libera",
        "Cem Subakan",
        "Mirco Ravanelli"
      ],
      "published": "2025-07-17T06:32:22Z",
      "updated": "2025-07-17T06:32:22Z",
      "categories": [
        "cs.SD",
        "cs.LG",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2507.12825v1",
      "landing_url": "https://arxiv.org/abs/2507.12825v1",
      "doi": "https://doi.org/10.48550/arXiv.2507.12825"
    },
    "round-A_JuniorNano_output": null,
    "round-A_JuniorNano_reasoning": null,
    "round-A_JuniorNano_evaluation": null,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "discard (published_on_or_after_cutoff:2025-07-17)",
    "review_skipped": true,
    "discard_reason": "published_on_or_after_cutoff:2025-07-17"
  },
  {
    "title": "CompressKV: Semantic Retrieval Heads Know What Tokens are Not Important Before Generation",
    "abstract": "Recent advances in large language models (LLMs) have significantly boosted long-context processing. However, the increasing key-value (KV) cache size poses critical challenges to memory and execution efficiency. Most KV cache compression methods rely on heuristic token eviction using all attention heads in Grouped Query Attention (GQA)-based LLMs. This method ignores the different functionalities of attention heads, leading to the eviction of critical tokens and thus degrades the performance of LLMs. To address the issue above, instead of using all the attention heads in GQA-based LLMs to determine important tokens as in the previous work, we first identify the attention heads in each layer that are not only capable of retrieving the initial and final tokens of a prompt, but also capable of retrieving important tokens within the text and attending to their surrounding semantic context. Afterwards, we exploit such heads to determine the important tokens and retain their corresponding KV cache pairs. Furthermore, we analyze the cache eviction error of each layer individually and introduce a layer-adaptive KV cache allocation strategy. Experimental results demonstrate the proposed CompressKV consistently outperforms state-of-the-art approaches under various memory budgets on LongBench and Needle-in-a-Haystack benchmarks. Our code is publicly available at: https://github.com/TUDa-HWAI/CompressKV.git.",
    "metadata": {
      "arxiv_id": "2508.02401",
      "title": "CompressKV: Semantic Retrieval Heads Know What Tokens are Not Important Before Generation",
      "summary": "Recent advances in large language models (LLMs) have significantly boosted long-context processing. However, the increasing key-value (KV) cache size poses critical challenges to memory and execution efficiency. Most KV cache compression methods rely on heuristic token eviction using all attention heads in Grouped Query Attention (GQA)-based LLMs. This method ignores the different functionalities of attention heads, leading to the eviction of critical tokens and thus degrades the performance of LLMs.\n  To address the issue above, instead of using all the attention heads in GQA-based LLMs to determine important tokens as in the previous work, we first identify the attention heads in each layer that are not only capable of retrieving the initial and final tokens of a prompt, but also capable of retrieving important tokens within the text and attending to their surrounding semantic context. Afterwards, we exploit such heads to determine the important tokens and retain their corresponding KV cache pairs. Furthermore, we analyze the cache eviction error of each layer individually and introduce a layer-adaptive KV cache allocation strategy. Experimental results demonstrate the proposed CompressKV consistently outperforms state-of-the-art approaches under various memory budgets on LongBench and Needle-in-a-Haystack benchmarks. Our code is publicly available at: https://github.com/TUDa-HWAI/CompressKV.git.",
      "authors": [
        "Xiaolin Lin",
        "Jingcun Wang",
        "Olga Kondrateva",
        "Yiyu Shi",
        "Bing Li",
        "Grace Li Zhang"
      ],
      "published": "2025-08-04T13:26:16Z",
      "updated": "2025-08-04T13:26:16Z",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2508.02401v1",
      "landing_url": "https://arxiv.org/abs/2508.02401v1",
      "doi": "https://doi.org/10.48550/arXiv.2508.02401"
    },
    "round-A_JuniorNano_output": null,
    "round-A_JuniorNano_reasoning": null,
    "round-A_JuniorNano_evaluation": null,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "discard (published_on_or_after_cutoff:2025-08-04)",
    "review_skipped": true,
    "discard_reason": "published_on_or_after_cutoff:2025-08-04"
  },
  {
    "title": "Trokens: Semantic-Aware Relational Trajectory Tokens for Few-Shot Action Recognition",
    "abstract": "Video understanding requires effective modeling of both motion and appearance information, particularly for few-shot action recognition. While recent advances in point tracking have been shown to improve few-shot action recognition, two fundamental challenges persist: selecting informative points to track and effectively modeling their motion patterns. We present Trokens, a novel approach that transforms trajectory points into semantic-aware relational tokens for action recognition. First, we introduce a semantic-aware sampling strategy to adaptively distribute tracking points based on object scale and semantic relevance. Second, we develop a motion modeling framework that captures both intra-trajectory dynamics through the Histogram of Oriented Displacements (HoD) and inter-trajectory relationships to model complex action patterns. Our approach effectively combines these trajectory tokens with semantic features to enhance appearance features with motion information, achieving state-of-the-art performance across six diverse few-shot action recognition benchmarks: Something-Something-V2 (both full and small splits), Kinetics, UCF101, HMDB51, and FineGym. For project page see https://trokens-iccv25.github.io",
    "metadata": {
      "arxiv_id": "2508.03695",
      "title": "Trokens: Semantic-Aware Relational Trajectory Tokens for Few-Shot Action Recognition",
      "summary": "Video understanding requires effective modeling of both motion and appearance information, particularly for few-shot action recognition. While recent advances in point tracking have been shown to improve few-shot action recognition, two fundamental challenges persist: selecting informative points to track and effectively modeling their motion patterns. We present Trokens, a novel approach that transforms trajectory points into semantic-aware relational tokens for action recognition. First, we introduce a semantic-aware sampling strategy to adaptively distribute tracking points based on object scale and semantic relevance. Second, we develop a motion modeling framework that captures both intra-trajectory dynamics through the Histogram of Oriented Displacements (HoD) and inter-trajectory relationships to model complex action patterns. Our approach effectively combines these trajectory tokens with semantic features to enhance appearance features with motion information, achieving state-of-the-art performance across six diverse few-shot action recognition benchmarks: Something-Something-V2 (both full and small splits), Kinetics, UCF101, HMDB51, and FineGym. For project page see https://trokens-iccv25.github.io",
      "authors": [
        "Pulkit Kumar",
        "Shuaiyi Huang",
        "Matthew Walmer",
        "Sai Saketh Rambhatla",
        "Abhinav Shrivastava"
      ],
      "published": "2025-08-05T17:59:58Z",
      "updated": "2025-08-05T17:59:58Z",
      "categories": [
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2508.03695v1",
      "landing_url": "https://arxiv.org/abs/2508.03695v1",
      "doi": "https://doi.org/10.48550/arXiv.2508.03695"
    },
    "round-A_JuniorNano_output": null,
    "round-A_JuniorNano_reasoning": null,
    "round-A_JuniorNano_evaluation": null,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "discard (published_on_or_after_cutoff:2025-08-05)",
    "review_skipped": true,
    "discard_reason": "published_on_or_after_cutoff:2025-08-05"
  },
  {
    "title": "Speech Discrete Tokens or Continuous Features? A Comparative Analysis for Spoken Language Understanding in SpeechLLMs",
    "abstract": "With the rise of Speech Large Language Models (SpeechLLMs), two dominant approaches have emerged for speech processing: discrete tokens and continuous features. Each approach has demonstrated strong capabilities in audio-related processing tasks. However, the performance gap between these two paradigms has not been thoroughly explored. To address this gap, we present a fair comparison of self-supervised learning (SSL)-based discrete and continuous features under the same experimental settings. We evaluate their performance across six spoken language understanding-related tasks using both small and large-scale LLMs (Qwen1.5-0.5B and Llama3.1-8B). We further conduct in-depth analyses, including efficient comparison, SSL layer analysis, LLM layer analysis, and robustness comparison. Our findings reveal that continuous features generally outperform discrete tokens in various tasks. Each speech processing method exhibits distinct characteristics and patterns in how it learns and processes speech information. We hope our results will provide valuable insights to advance spoken language understanding in SpeechLLMs.",
    "metadata": {
      "arxiv_id": "2508.17863",
      "title": "Speech Discrete Tokens or Continuous Features? A Comparative Analysis for Spoken Language Understanding in SpeechLLMs",
      "summary": "With the rise of Speech Large Language Models (SpeechLLMs), two dominant approaches have emerged for speech processing: discrete tokens and continuous features. Each approach has demonstrated strong capabilities in audio-related processing tasks. However, the performance gap between these two paradigms has not been thoroughly explored. To address this gap, we present a fair comparison of self-supervised learning (SSL)-based discrete and continuous features under the same experimental settings. We evaluate their performance across six spoken language understanding-related tasks using both small and large-scale LLMs (Qwen1.5-0.5B and Llama3.1-8B). We further conduct in-depth analyses, including efficient comparison, SSL layer analysis, LLM layer analysis, and robustness comparison. Our findings reveal that continuous features generally outperform discrete tokens in various tasks. Each speech processing method exhibits distinct characteristics and patterns in how it learns and processes speech information. We hope our results will provide valuable insights to advance spoken language understanding in SpeechLLMs.",
      "authors": [
        "Dingdong Wang",
        "Junan Li",
        "Mingyu Cui",
        "Dongchao Yang",
        "Xueyuan Chen",
        "Helen Meng"
      ],
      "published": "2025-08-25T10:16:07Z",
      "updated": "2025-08-25T10:16:07Z",
      "categories": [
        "cs.CL",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2508.17863v1",
      "landing_url": "https://arxiv.org/abs/2508.17863v1",
      "doi": "https://doi.org/10.48550/arXiv.2508.17863"
    },
    "round-A_JuniorNano_output": null,
    "round-A_JuniorNano_reasoning": null,
    "round-A_JuniorNano_evaluation": null,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "discard (published_on_or_after_cutoff:2025-08-25)",
    "review_skipped": true,
    "discard_reason": "published_on_or_after_cutoff:2025-08-25"
  },
  {
    "title": "Say More with Less: Variable-Frame-Rate Speech Tokenization via Adaptive Clustering and Implicit Duration Coding",
    "abstract": "Existing speech tokenizers typically assign a fixed number of tokens per second, regardless of the varying information density or temporal fluctuations in the speech signal. This uniform token allocation mismatches the intrinsic structure of speech, where information is distributed unevenly over time. To address this, we propose VARSTok, a VAriable-frame-Rate Speech Tokenizer that adapts token allocation based on local feature similarity. VARSTok introduces two key innovations: (1) a temporal-aware density peak clustering algorithm that adaptively segments speech into variable-length units, and (2) a novel implicit duration coding scheme that embeds both content and temporal span into a single token index, eliminating the need for auxiliary duration predictors. Extensive experiments show that VARSTok significantly outperforms strong fixed-rate baselines. Notably, it achieves superior reconstruction naturalness while using up to 23% fewer tokens than a 40 Hz fixed-frame-rate baseline. VARSTok further yields lower word error rates and improved naturalness in zero-shot text-to-speech synthesis. To the best of our knowledge, this is the first work to demonstrate that a fully dynamic, variable-frame-rate acoustic speech tokenizer can be seamlessly integrated into downstream speech language models.",
    "metadata": {
      "arxiv_id": "2509.04685",
      "title": "Say More with Less: Variable-Frame-Rate Speech Tokenization via Adaptive Clustering and Implicit Duration Coding",
      "summary": "Existing speech tokenizers typically assign a fixed number of tokens per second, regardless of the varying information density or temporal fluctuations in the speech signal. This uniform token allocation mismatches the intrinsic structure of speech, where information is distributed unevenly over time. To address this, we propose VARSTok, a VAriable-frame-Rate Speech Tokenizer that adapts token allocation based on local feature similarity. VARSTok introduces two key innovations: (1) a temporal-aware density peak clustering algorithm that adaptively segments speech into variable-length units, and (2) a novel implicit duration coding scheme that embeds both content and temporal span into a single token index, eliminating the need for auxiliary duration predictors. Extensive experiments show that VARSTok significantly outperforms strong fixed-rate baselines. Notably, it achieves superior reconstruction naturalness while using up to 23% fewer tokens than a 40 Hz fixed-frame-rate baseline. VARSTok further yields lower word error rates and improved naturalness in zero-shot text-to-speech synthesis. To the best of our knowledge, this is the first work to demonstrate that a fully dynamic, variable-frame-rate acoustic speech tokenizer can be seamlessly integrated into downstream speech language models.",
      "authors": [
        "Rui-Chen Zheng",
        "Wenrui Liu",
        "Hui-Peng Du",
        "Qinglin Zhang",
        "Chong Deng",
        "Qian Chen",
        "Wen Wang",
        "Yang Ai",
        "Zhen-Hua Ling"
      ],
      "published": "2025-09-04T22:15:17Z",
      "updated": "2025-11-13T18:04:16Z",
      "categories": [
        "eess.AS",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2509.04685v3",
      "landing_url": "https://arxiv.org/abs/2509.04685v3",
      "doi": "https://doi.org/10.48550/arXiv.2509.04685"
    },
    "round-A_JuniorNano_output": null,
    "round-A_JuniorNano_reasoning": null,
    "round-A_JuniorNano_evaluation": null,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "discard (published_on_or_after_cutoff:2025-09-04)",
    "review_skipped": true,
    "discard_reason": "published_on_or_after_cutoff:2025-09-04"
  },
  {
    "title": "DiFlow-TTS: Discrete Flow Matching with Factorized Speech Tokens for Low-Latency Zero-Shot Text-To-Speech",
    "abstract": "Zero-shot Text-to-Speech (TTS) aims to synthesize high-quality speech that mimics the voice of an unseen speaker using only a short reference sample, requiring not only speaker adaptation but also accurate modeling of prosodic attributes. Recent approaches based on language models, diffusion, and flow matching have shown promising results in zero-shot TTS, but still suffer from slow inference and repetition artifacts. Discrete codec representations have been widely adopted for speech synthesis, and recent works have begun to explore diffusion models in purely discrete settings, suggesting the potential of discrete generative modeling for speech synthesis. However, existing flow-matching methods typically embed these discrete tokens into a continuous space and apply continuous flow matching, which may not fully leverage the advantages of discrete representations. To address these challenges, we introduce DiFlow-TTS, which, to the best of our knowledge, is the first model to explore purely Discrete Flow Matching for speech synthesis. DiFlow-TTS explicitly models factorized speech attributes within a compact and unified architecture. It leverages in-context learning by conditioning on textual content, along with prosodic and acoustic attributes extracted from a reference speech, enabling effective attribute cloning in a zero-shot setting. In addition, the model employs a factorized flow prediction mechanism with distinct heads for prosody and acoustic details, allowing it to learn aspect-specific distributions. Experimental results demonstrate that DiFlow-TTS achieves promising performance in several key metrics, including naturalness, prosody, preservation of speaker style, and energy control. It also maintains a compact model size and achieves low-latency inference, generating speech up to 25.8 times faster than the latest existing baselines.",
    "metadata": {
      "arxiv_id": "2509.09631",
      "title": "DiFlow-TTS: Discrete Flow Matching with Factorized Speech Tokens for Low-Latency Zero-Shot Text-To-Speech",
      "summary": "Zero-shot Text-to-Speech (TTS) aims to synthesize high-quality speech that mimics the voice of an unseen speaker using only a short reference sample, requiring not only speaker adaptation but also accurate modeling of prosodic attributes. Recent approaches based on language models, diffusion, and flow matching have shown promising results in zero-shot TTS, but still suffer from slow inference and repetition artifacts. Discrete codec representations have been widely adopted for speech synthesis, and recent works have begun to explore diffusion models in purely discrete settings, suggesting the potential of discrete generative modeling for speech synthesis. However, existing flow-matching methods typically embed these discrete tokens into a continuous space and apply continuous flow matching, which may not fully leverage the advantages of discrete representations. To address these challenges, we introduce DiFlow-TTS, which, to the best of our knowledge, is the first model to explore purely Discrete Flow Matching for speech synthesis. DiFlow-TTS explicitly models factorized speech attributes within a compact and unified architecture. It leverages in-context learning by conditioning on textual content, along with prosodic and acoustic attributes extracted from a reference speech, enabling effective attribute cloning in a zero-shot setting. In addition, the model employs a factorized flow prediction mechanism with distinct heads for prosody and acoustic details, allowing it to learn aspect-specific distributions. Experimental results demonstrate that DiFlow-TTS achieves promising performance in several key metrics, including naturalness, prosody, preservation of speaker style, and energy control. It also maintains a compact model size and achieves low-latency inference, generating speech up to 25.8 times faster than the latest existing baselines.",
      "authors": [
        "Ngoc-Son Nguyen",
        "Hieu-Nghia Huynh-Nguyen",
        "Thanh V. T. Tran",
        "Truong-Son Hy",
        "Van Nguyen"
      ],
      "published": "2025-09-11T17:16:52Z",
      "updated": "2025-09-12T01:59:18Z",
      "categories": [
        "cs.SD",
        "cs.CL",
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2509.09631v2",
      "landing_url": "https://arxiv.org/abs/2509.09631v2",
      "doi": "https://doi.org/10.48550/arXiv.2509.09631"
    },
    "round-A_JuniorNano_output": null,
    "round-A_JuniorNano_reasoning": null,
    "round-A_JuniorNano_evaluation": null,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "discard (published_on_or_after_cutoff:2025-09-11)",
    "review_skipped": true,
    "discard_reason": "published_on_or_after_cutoff:2025-09-11"
  },
  {
    "title": "Llama-Mimi: Speech Language Models with Interleaved Semantic and Acoustic Tokens",
    "abstract": "We propose Llama-Mimi, a speech language model that uses a unified tokenizer and a single Transformer decoder to jointly model sequences of interleaved semantic and acoustic tokens. Comprehensive evaluation shows that Llama-Mimi achieves state-of-the-art performance in acoustic consistency and possesses the ability to preserve speaker identity. Our analysis further demonstrates that increasing the number of quantizers improves acoustic fidelity but degrades linguistic performance, highlighting the inherent challenge of maintaining long-term coherence. We additionally introduce an LLM-as-a-Judge-based evaluation to assess the spoken content quality of generated outputs. Our models, code, and speech samples are publicly available.",
    "metadata": {
      "arxiv_id": "2509.14882",
      "title": "Llama-Mimi: Speech Language Models with Interleaved Semantic and Acoustic Tokens",
      "summary": "We propose Llama-Mimi, a speech language model that uses a unified tokenizer and a single Transformer decoder to jointly model sequences of interleaved semantic and acoustic tokens. Comprehensive evaluation shows that Llama-Mimi achieves state-of-the-art performance in acoustic consistency and possesses the ability to preserve speaker identity. Our analysis further demonstrates that increasing the number of quantizers improves acoustic fidelity but degrades linguistic performance, highlighting the inherent challenge of maintaining long-term coherence. We additionally introduce an LLM-as-a-Judge-based evaluation to assess the spoken content quality of generated outputs. Our models, code, and speech samples are publicly available.",
      "authors": [
        "Issa Sugiura",
        "Shuhei Kurita",
        "Yusuke Oda",
        "Ryuichiro Higashinaka"
      ],
      "published": "2025-09-18T12:00:07Z",
      "updated": "2025-09-18T12:00:07Z",
      "categories": [
        "cs.CL"
      ],
      "pdf_url": "https://arxiv.org/pdf/2509.14882v1",
      "landing_url": "https://arxiv.org/abs/2509.14882v1",
      "doi": "https://doi.org/10.48550/arXiv.2509.14882"
    },
    "round-A_JuniorNano_output": null,
    "round-A_JuniorNano_reasoning": null,
    "round-A_JuniorNano_evaluation": null,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "discard (published_on_or_after_cutoff:2025-09-18)",
    "review_skipped": true,
    "discard_reason": "published_on_or_after_cutoff:2025-09-18"
  }
]