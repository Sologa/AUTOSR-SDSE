[
  {
    "title": "Retrieval Augmented End-to-End Spoken Dialog Models",
    "abstract": "We recently developed a joint speech and language model (SLM [1]) which fuses a pretrained foundational speech model and a large language model (LLM), while preserving the in-context learning capability intrinsic to the pretrained LLM. In this paper, we apply SLM to dialog applications where the dialog states are inferred directly from the audio signal.Task-oriented dialogs often contain domain-specific entities, i.e., restaurants, hotels, train stations, and city names, which are difficult to recognize, however, critical for the downstream applications. Inspired by the RAG (retrieval-augmented generation) models, we propose a retrieval augmented SLM (ReSLM) that overcomes this weakness. We first train a retriever to retrieve text entities given audio inputs. The retrieved entities are then added as text inputs to the underlying LLM to bias model predictions. We evaluated ReSLM on speech MultiWoz task (DSTC-11 Challenge), and found that the retrieval augmentation boosts model performance, achieving joint goal accuracy (38.6% vs 32.7%), slot error rate (20.6% vs 24.8%) and ASR word error rate (5.5% vs 6.7%). While demonstrated on dialog state tracking, our approach is broadly applicable to speech tasks requiring custom contextual information or domain-specific entities.",
    "metadata": {
      "title": "Retrieval Augmented End-to-End Spoken Dialog Models",
      "summary": "We recently developed a joint speech and language model (SLM [1]) which fuses a pretrained foundational speech model and a large language model (LLM), while preserving the in-context learning capability intrinsic to the pretrained LLM. In this paper, we apply SLM to dialog applications where the dialog states are inferred directly from the audio signal.Task-oriented dialogs often contain domain-specific entities, i.e., restaurants, hotels, train stations, and city names, which are difficult to recognize, however, critical for the downstream applications. Inspired by the RAG (retrieval-augmented generation) models, we propose a retrieval augmented SLM (ReSLM) that overcomes this weakness. We first train a retriever to retrieve text entities given audio inputs. The retrieved entities are then added as text inputs to the underlying LLM to bias model predictions. We evaluated ReSLM on speech MultiWoz task (DSTC-11 Challenge), and found that the retrieval augmentation boosts model performance, achieving joint goal accuracy (38.6% vs 32.7%), slot error rate (20.6% vs 24.8%) and ASR word error rate (5.5% vs 6.7%). While demonstrated on dialog state tracking, our approach is broadly applicable to speech tasks requiring custom contextual information or domain-specific entities.",
      "abstract": "We recently developed a joint speech and language model (SLM [1]) which fuses a pretrained foundational speech model and a large language model (LLM), while preserving the in-context learning capability intrinsic to the pretrained LLM. In this paper, we apply SLM to dialog applications where the dialog states are inferred directly from the audio signal.Task-oriented dialogs often contain domain-specific entities, i.e., restaurants, hotels, train stations, and city names, which are difficult to recognize, however, critical for the downstream applications. Inspired by the RAG (retrieval-augmented generation) models, we propose a retrieval augmented SLM (ReSLM) that overcomes this weakness. We first train a retriever to retrieve text entities given audio inputs. The retrieved entities are then added as text inputs to the underlying LLM to bias model predictions. We evaluated ReSLM on speech MultiWoz task (DSTC-11 Challenge), and found that the retrieval augmentation boosts model performance, achieving joint goal accuracy (38.6% vs 32.7%), slot error rate (20.6% vs 24.8%) and ASR word error rate (5.5% vs 6.7%). While demonstrated on dialog state tracking, our approach is broadly applicable to speech tasks requiring custom contextual information or domain-specific entities.",
      "doi": "https://doi.org/10.1109/icassp48485.2024.10447448",
      "openalex_id": "https://openalex.org/W4392902876",
      "arxiv_id": "",
      "publication_date": "2024-03-18",
      "published": "2024-03-18",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This item centers on a Spoken Language Model (SLM) architecture for dialog with audio input, uses a retrieval-augmented end-to-end approach (ReSLM), and evaluates on a multi-turn dialog task, aligning with the inclusion criteria while not aligning with the exclusion criteria.",
      "evaluation": 5
    },
    "round-A_JuniorNano_reasoning": "This item centers on a Spoken Language Model (SLM) architecture for dialog with audio input, uses a retrieval-augmented end-to-end approach (ReSLM), and evaluates on a multi-turn dialog task, aligning with the inclusion criteria while not aligning with the exclusion criteria.",
    "round-A_JuniorNano_evaluation": 5,
    "round-A_JuniorMini_output": {
      "reasoning": "The study presents a retrieval-augmented speech and language model (SLM) that integrates speech inputs directly for dialog applications, showing model architecture and evaluation beyond traditional ASR tasks; however, it does not clearly address full-duplex spoken interaction, cognitive parallelism, low-latency interaction, or synchronization aspects, which are key elements of the inclusion criteria, and the evaluation is focused mainly on DST and ASR metrics without discussing real-time interaction capabilities. Therefore, it partially meets inclusion criteria but lacks sufficient coverage of full-duplex and complex interactive behaviors.",
      "evaluation": 3
    },
    "round-A_JuniorMini_reasoning": "The study presents a retrieval-augmented speech and language model (SLM) that integrates speech inputs directly for dialog applications, showing model architecture and evaluation beyond traditional ASR tasks; however, it does not clearly address full-duplex spoken interaction, cognitive parallelism, low-latency interaction, or synchronization aspects, which are key elements of the inclusion criteria, and the evaluation is focused mainly on DST and ASR metrics without discussing real-time interaction capabilities. Therefore, it partially meets inclusion criteria but lacks sufficient coverage of full-duplex and complex interactive behaviors.",
    "round-A_JuniorMini_evaluation": 3,
    "round-B_SeniorLead_output": {
      "reasoning": "The paper explicitly develops and names a speech-and-language model (SLM) and proposes retrieval augmentation for end-to-end spoken dialog, and I considered the two junior reviews (one supportive, one cautious). However, it focuses on turn-based dialog state tracking (DST) and ASR/DST metrics without addressing full-duplex interaction, synchronization/behavior arbitration (overlap, interruptions, cognitive parallelism) or low-latency real-time evaluation required by the topic; this triggers the exclusion criterion for works that only treat traditional half-duplex/dialogue-state tasks. Therefore it should not be included.",
      "evaluation": 2
    },
    "round-B_SeniorLead_reasoning": "The paper explicitly develops and names a speech-and-language model (SLM) and proposes retrieval augmentation for end-to-end spoken dialog, and I considered the two junior reviews (one supportive, one cautious). However, it focuses on turn-based dialog state tracking (DST) and ASR/DST metrics without addressing full-duplex interaction, synchronization/behavior arbitration (overlap, interruptions, cognitive parallelism) or low-latency real-time evaluation required by the topic; this triggers the exclusion criterion for works that only treat traditional half-duplex/dialogue-state tasks. Therefore it should not be included.",
    "round-B_SeniorLead_evaluation": 2,
    "final_verdict": "exclude (senior:2)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Advancements in Speech Recognition: A Systematic Review of Deep Learning Transformer Models, Trends, Innovations, and Future Directions",
    "abstract": "The transformer is a Deep Learning (DL) model that revolutionized language processing with its self-attention mechanism, enabling parallel processing and improving model efficiency, which dramatically reshaped the landscape of speech recognition technology, based on the ability to efficiently manage the dynamic and context-rich nature of speech. The proposed systematic review in this article critically examines the impact of transformer models on speech recognition, covering the published research over the past seven years (from January 1, 2017, to May 15, 2024). The goals of this article are to synthesize the current knowledge, pinpoint emerging trends, and identify research gaps that could be beneficial for future investigations. From an initial pool of 2,838 publications sourced from leading digital libraries, a rigorous two-step screening process applied to distill high-quality studies relevant to the review criteria. We concentrated our analysis on seven pivotal areas as following: the environmental conditions (neutral versus noisy) addressed by the studies, methods of feature extraction employed, characteristics of the transformer models used, datasets utilized, variations in model efficiency, the influence of noise on model generalizability, and trends in self-supervised learning, ending up with 37 articles to review in this paper. Our findings underscore the transformative potential of transformers in enhancing the accuracy and robustness of speech recognition systems, especially in challenging acoustic environments. In addition, this review highlights areas where more research is needed to make speech recognition even better by using transformer technology.",
    "metadata": {
      "title": "Advancements in Speech Recognition: A Systematic Review of Deep Learning Transformer Models, Trends, Innovations, and Future Directions",
      "summary": "The transformer is a Deep Learning (DL) model that revolutionized language processing with its self-attention mechanism, enabling parallel processing and improving model efficiency, which dramatically reshaped the landscape of speech recognition technology, based on the ability to efficiently manage the dynamic and context-rich nature of speech. The proposed systematic review in this article critically examines the impact of transformer models on speech recognition, covering the published research over the past seven years (from January 1, 2017, to May 15, 2024). The goals of this article are to synthesize the current knowledge, pinpoint emerging trends, and identify research gaps that could be beneficial for future investigations. From an initial pool of 2,838 publications sourced from leading digital libraries, a rigorous two-step screening process applied to distill high-quality studies relevant to the review criteria. We concentrated our analysis on seven pivotal areas as following: the environmental conditions (neutral versus noisy) addressed by the studies, methods of feature extraction employed, characteristics of the transformer models used, datasets utilized, variations in model efficiency, the influence of noise on model generalizability, and trends in self-supervised learning, ending up with 37 articles to review in this paper. Our findings underscore the transformative potential of transformers in enhancing the accuracy and robustness of speech recognition systems, especially in challenging acoustic environments. In addition, this review highlights areas where more research is needed to make speech recognition even better by using transformer technology.",
      "abstract": "The transformer is a Deep Learning (DL) model that revolutionized language processing with its self-attention mechanism, enabling parallel processing and improving model efficiency, which dramatically reshaped the landscape of speech recognition technology, based on the ability to efficiently manage the dynamic and context-rich nature of speech. The proposed systematic review in this article critically examines the impact of transformer models on speech recognition, covering the published research over the past seven years (from January 1, 2017, to May 15, 2024). The goals of this article are to synthesize the current knowledge, pinpoint emerging trends, and identify research gaps that could be beneficial for future investigations. From an initial pool of 2,838 publications sourced from leading digital libraries, a rigorous two-step screening process applied to distill high-quality studies relevant to the review criteria. We concentrated our analysis on seven pivotal areas as following: the environmental conditions (neutral versus noisy) addressed by the studies, methods of feature extraction employed, characteristics of the transformer models used, datasets utilized, variations in model efficiency, the influence of noise on model generalizability, and trends in self-supervised learning, ending up with 37 articles to review in this paper. Our findings underscore the transformative potential of transformers in enhancing the accuracy and robustness of speech recognition systems, especially in challenging acoustic environments. In addition, this review highlights areas where more research is needed to make speech recognition even better by using transformer technology.",
      "doi": "https://doi.org/10.1109/access.2025.3550855",
      "openalex_id": "https://openalex.org/W4408358330",
      "arxiv_id": "",
      "publication_date": "2025-01-01",
      "published": "2025-01-01",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item centers on transformer-based speech recognition and is a general literature review without explicitly focusing on Spoken Language Models or Full-Duplex Spoken Language Models, nor does it discuss end-to-end speech-language modeling or synchronization/interaction aspects; it also lacks explicit keywords indicating SLMs or speech-language modeling, thus failing the inclusion criteria and aligning with exclusion criteria.",
      "evaluation": 2
    },
    "round-A_JuniorNano_reasoning": "The item centers on transformer-based speech recognition and is a general literature review without explicitly focusing on Spoken Language Models or Full-Duplex Spoken Language Models, nor does it discuss end-to-end speech-language modeling or synchronization/interaction aspects; it also lacks explicit keywords indicating SLMs or speech-language modeling, thus failing the inclusion criteria and aligning with exclusion criteria.",
    "round-A_JuniorNano_evaluation": 2,
    "round-A_JuniorMini_output": {
      "reasoning": "The article primarily reviews transformer models in speech recognition, focusing on model efficiency, noise robustness, datasets, and trends in self-supervised learning, without emphasizing spoken language models (SLMs) or full-duplex spoken language models (FD-SLMs), nor discussing synchronous interactive dialogue behaviors, cognitive parallelism, or low-latency real-time interaction as required in the inclusion criteria. It also broadly covers speech recognition trends without clear mention of spoken language modeling or multi-dimensional evaluation aspects beyond ASR performance. Therefore, it does not meet all the inclusion criteria and conflicts with several exclusion points.",
      "evaluation": 2
    },
    "round-A_JuniorMini_reasoning": "The article primarily reviews transformer models in speech recognition, focusing on model efficiency, noise robustness, datasets, and trends in self-supervised learning, without emphasizing spoken language models (SLMs) or full-duplex spoken language models (FD-SLMs), nor discussing synchronous interactive dialogue behaviors, cognitive parallelism, or low-latency real-time interaction as required in the inclusion criteria. It also broadly covers speech recognition trends without clear mention of spoken language modeling or multi-dimensional evaluation aspects beyond ASR performance. Therefore, it does not meet all the inclusion criteria and conflicts with several exclusion points.",
    "round-A_JuniorMini_evaluation": 2,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:2)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "AUDIO-TO-TEXT TRANSLATION FOR THE HARD OF HEARING: A WHISPER MODEL-BASED STUDY",
    "abstract": "This study investigates the effectiveness of the Whisper model for audio-to-text transcription, specifically targeting the enhancement of accessibility for individuals with hearing impairments. The research focuses on the processing of audio recordings obtained from WhatsApp messenger, which often contain significant background noise that complicates speech recognition. To address this issue, advanced audio processing techniques were employed, including the use of the Librosa library and the Noisereduce package for noise reduction. The spectral gating methods applied in this study effectively diminished wind noise and other ambient sounds, allowing for clearer recognition of spoken content. To ensure the quality of the processed audio, we assessed its clarity using a SimpleRNN model. The training results demonstrated a progressive reduction in loss values across epochs, confirming the successful enhancement of audio quality. Once the audio files were adequately cleaned, we utilized the Whisper model, a sophisticated machine learning tool for speech recognition developed by OpenAI, to transcribe the audio into text. The transcription process yielded accurate Kazakh language output, despite the initial challenges posed by background noise. These findings underscore the critical role of high-quality audio input in achieving reliable transcription results and highlight the potential of machine learning technologies in improving communication access for hearing-impaired individuals. This study concludes with recommendations for future research, including the exploration of additional noise reduction techniques and the application of the Whisper model across various languages and dialects. Such advancements could significantly contribute to creating more inclusive digital environments and enhancing the overall user experience for individuals with hearing impairments.",
    "metadata": {
      "title": "AUDIO-TO-TEXT TRANSLATION FOR THE HARD OF HEARING: A WHISPER MODEL-BASED STUDY",
      "summary": "This study investigates the effectiveness of the Whisper model for audio-to-text transcription, specifically targeting the enhancement of accessibility for individuals with hearing impairments. The research focuses on the processing of audio recordings obtained from WhatsApp messenger, which often contain significant background noise that complicates speech recognition. To address this issue, advanced audio processing techniques were employed, including the use of the Librosa library and the Noisereduce package for noise reduction. The spectral gating methods applied in this study effectively diminished wind noise and other ambient sounds, allowing for clearer recognition of spoken content. To ensure the quality of the processed audio, we assessed its clarity using a SimpleRNN model. The training results demonstrated a progressive reduction in loss values across epochs, confirming the successful enhancement of audio quality. Once the audio files were adequately cleaned, we utilized the Whisper model, a sophisticated machine learning tool for speech recognition developed by OpenAI, to transcribe the audio into text. The transcription process yielded accurate Kazakh language output, despite the initial challenges posed by background noise. These findings underscore the critical role of high-quality audio input in achieving reliable transcription results and highlight the potential of machine learning technologies in improving communication access for hearing-impaired individuals. This study concludes with recommendations for future research, including the exploration of additional noise reduction techniques and the application of the Whisper model across various languages and dialects. Such advancements could significantly contribute to creating more inclusive digital environments and enhancing the overall user experience for individuals with hearing impairments.",
      "abstract": "This study investigates the effectiveness of the Whisper model for audio-to-text transcription, specifically targeting the enhancement of accessibility for individuals with hearing impairments. The research focuses on the processing of audio recordings obtained from WhatsApp messenger, which often contain significant background noise that complicates speech recognition. To address this issue, advanced audio processing techniques were employed, including the use of the Librosa library and the Noisereduce package for noise reduction. The spectral gating methods applied in this study effectively diminished wind noise and other ambient sounds, allowing for clearer recognition of spoken content. To ensure the quality of the processed audio, we assessed its clarity using a SimpleRNN model. The training results demonstrated a progressive reduction in loss values across epochs, confirming the successful enhancement of audio quality. Once the audio files were adequately cleaned, we utilized the Whisper model, a sophisticated machine learning tool for speech recognition developed by OpenAI, to transcribe the audio into text. The transcription process yielded accurate Kazakh language output, despite the initial challenges posed by background noise. These findings underscore the critical role of high-quality audio input in achieving reliable transcription results and highlight the potential of machine learning technologies in improving communication access for hearing-impaired individuals. This study concludes with recommendations for future research, including the exploration of additional noise reduction techniques and the application of the Whisper model across various languages and dialects. Such advancements could significantly contribute to creating more inclusive digital environments and enhancing the overall user experience for individuals with hearing impairments.",
      "doi": "https://doi.org/10.37943/22snok5872",
      "openalex_id": "https://openalex.org/W4412899262",
      "arxiv_id": "",
      "publication_date": "2025-06-30",
      "published": "2025-06-30",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "本項目是以 Whisper 作為語音識別工具並進行音頻降噪與轉錄的應用研究，屬於 ASR 與音頻前處理的範疇，未以 Spoken Language Models / Speech Language Models 的架構設計、端到端或混合式語音-文字模型、或同步互動、低延遲等語音語言建模的核心議題為主，因此不符合必須以語音語言模型為核心的 inclusion 條件與排除條件。雖在過去3年內公開，但不符合評選所需的研究方向。",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "本項目是以 Whisper 作為語音識別工具並進行音頻降噪與轉錄的應用研究，屬於 ASR 與音頻前處理的範疇，未以 Spoken Language Models / Speech Language Models 的架構設計、端到端或混合式語音-文字模型、或同步互動、低延遲等語音語言建模的核心議題為主，因此不符合必須以語音語言模型為核心的 inclusion 條件與排除條件。雖在過去3年內公開，但不符合評選所需的研究方向。",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on using the Whisper model for audio-to-text transcription to improve accessibility for the hard of hearing. However, it primarily addresses automatic speech recognition (ASR) challenges with noisy audio and does not focus on Spoken Language Models or full-duplex interactive models as required. There is no discussion of speech-level language modeling, full-duplex interaction, or joint speech-text modeling architectures. The work emphasizes noise reduction and transcription accuracy rather than synchronous dialogue or cognitive parallelism aspects, and it lacks evaluation related to interactive behaviors or low-latency spoken language modeling. Therefore, it does not meet the core inclusion criteria and aligns more with exclusion criteria focused on ASR-only systems without interactive or modeling innovations.",
      "evaluation": 2
    },
    "round-A_JuniorMini_reasoning": "The study focuses on using the Whisper model for audio-to-text transcription to improve accessibility for the hard of hearing. However, it primarily addresses automatic speech recognition (ASR) challenges with noisy audio and does not focus on Spoken Language Models or full-duplex interactive models as required. There is no discussion of speech-level language modeling, full-duplex interaction, or joint speech-text modeling architectures. The work emphasizes noise reduction and transcription accuracy rather than synchronous dialogue or cognitive parallelism aspects, and it lacks evaluation related to interactive behaviors or low-latency spoken language modeling. Therefore, it does not meet the core inclusion criteria and aligns more with exclusion criteria focused on ASR-only systems without interactive or modeling innovations.",
    "round-A_JuniorMini_evaluation": 2,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:2)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "MRRL: Modifying the Reference via Reinforcement Learning for Non-Autoregressive Joint Multiple Intent Detection and Slot Filling",
    "abstract": "With the rise of non-autoregressive approach, some non-autoregressive models for joint multiple intent detection and slot filling have obtained the promising inference speed. However, most existing SLU models (1) suffer from the multi-modality problem that leads to reference intents and slots may not be suitable for training; (2) lack of alignment between the correct predictions of the two tasks, which extremely limits the overall accuracy. Therefore, in this paper, we propose Modifying the Reference via Reinforcement Learning (MRRL), a novel method for multiple intent detection and slot filling, which introduces a modifier and employs reinforcement learning. Specifically, we try to provide the better training target for the non-autoregressive SLU model via modifying the reference based on the output of the non-autoregressive SLU model, and propose a suitability reward to ensure that the output of the modifier module could fit well with the output of the non-autoregressive SLU model and does not deviate too far from the reference. In addition, we also propose a compromise reward to realize a flexible trade-off between the two subtasks. Experiments on two multi-intent datasets and non-autoregressive baselines demonstrate that our MRRL could consistently improve the performance of baselines. More encouragingly, our best variant achieves new state-of-the-art results, outperforming the previous best approach by 3.6 overall accuracy on MixATIS dataset.",
    "metadata": {
      "title": "MRRL: Modifying the Reference via Reinforcement Learning for Non-Autoregressive Joint Multiple Intent Detection and Slot Filling",
      "summary": "With the rise of non-autoregressive approach, some non-autoregressive models for joint multiple intent detection and slot filling have obtained the promising inference speed. However, most existing SLU models (1) suffer from the multi-modality problem that leads to reference intents and slots may not be suitable for training; (2) lack of alignment between the correct predictions of the two tasks, which extremely limits the overall accuracy. Therefore, in this paper, we propose Modifying the Reference via Reinforcement Learning (MRRL), a novel method for multiple intent detection and slot filling, which introduces a modifier and employs reinforcement learning. Specifically, we try to provide the better training target for the non-autoregressive SLU model via modifying the reference based on the output of the non-autoregressive SLU model, and propose a suitability reward to ensure that the output of the modifier module could fit well with the output of the non-autoregressive SLU model and does not deviate too far from the reference. In addition, we also propose a compromise reward to realize a flexible trade-off between the two subtasks. Experiments on two multi-intent datasets and non-autoregressive baselines demonstrate that our MRRL could consistently improve the performance of baselines. More encouragingly, our best variant achieves new state-of-the-art results, outperforming the previous best approach by 3.6 overall accuracy on MixATIS dataset.",
      "abstract": "With the rise of non-autoregressive approach, some non-autoregressive models for joint multiple intent detection and slot filling have obtained the promising inference speed. However, most existing SLU models (1) suffer from the multi-modality problem that leads to reference intents and slots may not be suitable for training; (2) lack of alignment between the correct predictions of the two tasks, which extremely limits the overall accuracy. Therefore, in this paper, we propose Modifying the Reference via Reinforcement Learning (MRRL), a novel method for multiple intent detection and slot filling, which introduces a modifier and employs reinforcement learning. Specifically, we try to provide the better training target for the non-autoregressive SLU model via modifying the reference based on the output of the non-autoregressive SLU model, and propose a suitability reward to ensure that the output of the modifier module could fit well with the output of the non-autoregressive SLU model and does not deviate too far from the reference. In addition, we also propose a compromise reward to realize a flexible trade-off between the two subtasks. Experiments on two multi-intent datasets and non-autoregressive baselines demonstrate that our MRRL could consistently improve the performance of baselines. More encouragingly, our best variant achieves new state-of-the-art results, outperforming the previous best approach by 3.6 overall accuracy on MixATIS dataset.",
      "doi": "https://doi.org/10.18653/v1/2023.findings-emnlp.704",
      "openalex_id": "https://openalex.org/W4389519009",
      "arxiv_id": "",
      "publication_date": "2023-01-01",
      "published": "2023-01-01",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The paper targets non-autoregressive SLU for joint intent detection and slot filling with RL; however, it does not explicitly focus on Spoken Language Models or speech-centric modeling, lacking emphasis on audio/speech data or evaluation beyond text-based SLU, so it likely does not meet the core inclusion criteria and should be excluded.",
      "evaluation": 2
    },
    "round-A_JuniorNano_reasoning": "The paper targets non-autoregressive SLU for joint intent detection and slot filling with RL; however, it does not explicitly focus on Spoken Language Models or speech-centric modeling, lacking emphasis on audio/speech data or evaluation beyond text-based SLU, so it likely does not meet the core inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 2,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper focuses on non-autoregressive joint multiple intent detection and slot filling in spoken language understanding (SLU), revolving around textual intent and slot outputs rather than modeling or reasoning over speech or acoustic signals as the core. It does not explicitly address spoken language models (SLMs) or full-duplex spoken language models (FD-SLMs), nor does it focus on speech-level language modeling, cognitive parallelism, low-latency interaction, or engineered/learned synchronization of speech; speech is only an indirect input to the SLU system and is not part of the model's core design or evaluation concerning speech modeling. Hence, it fails to meet the inclusion criteria needing speech-based language modeling or spoken language models as the research core.",
      "evaluation": 2
    },
    "round-A_JuniorMini_reasoning": "The paper focuses on non-autoregressive joint multiple intent detection and slot filling in spoken language understanding (SLU), revolving around textual intent and slot outputs rather than modeling or reasoning over speech or acoustic signals as the core. It does not explicitly address spoken language models (SLMs) or full-duplex spoken language models (FD-SLMs), nor does it focus on speech-level language modeling, cognitive parallelism, low-latency interaction, or engineered/learned synchronization of speech; speech is only an indirect input to the SLU system and is not part of the model's core design or evaluation concerning speech modeling. Hence, it fails to meet the inclusion criteria needing speech-based language modeling or spoken language models as the research core.",
    "round-A_JuniorMini_evaluation": 2,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:2)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Optical Character Recognition Systems for Accurate Interpretation of Handwritten Telugu Scripts",
    "abstract": "Visual data dominates the digital landscape these days. However, there remains a significant gap in the digitization of textual content in regional languages such as Telugu. This research addresses this gap and utilizes the capabilities of the programming language and set of specialized libraries pytesseract for optical character recognition (OCR), PIL for image processing and Levenshtein for accuracy evaluation to appropriately extract text from images in the Telugu script. There are several important steps in this project. Initially, this work sets up a dependable system for uploading pictures. Secondly, carefully extracting text from these submitted photos using py-tesseract. Using the Levenshtein distance measure, and then compared with the ground truth to determine correctness. This metric measures the accuracy of the text output through looking at the slight variations between the word error rate (WER) and character error rate (CER). This is capable of finding misspelled words and unknown characters providing a precise evaluation of the OCR's effectiveness. This study also examines the current state of OCR in Telugu text processing and indicates the benefits of current technologies for locating and converting Telugu text discovered in image documents. It also demonstrates improvements in OCR technology.",
    "metadata": {
      "title": "Optical Character Recognition Systems for Accurate Interpretation of Handwritten Telugu Scripts",
      "summary": "Visual data dominates the digital landscape these days. However, there remains a significant gap in the digitization of textual content in regional languages such as Telugu. This research addresses this gap and utilizes the capabilities of the programming language and set of specialized libraries pytesseract for optical character recognition (OCR), PIL for image processing and Levenshtein for accuracy evaluation to appropriately extract text from images in the Telugu script. There are several important steps in this project. Initially, this work sets up a dependable system for uploading pictures. Secondly, carefully extracting text from these submitted photos using py-tesseract. Using the Levenshtein distance measure, and then compared with the ground truth to determine correctness. This metric measures the accuracy of the text output through looking at the slight variations between the word error rate (WER) and character error rate (CER). This is capable of finding misspelled words and unknown characters providing a precise evaluation of the OCR's effectiveness. This study also examines the current state of OCR in Telugu text processing and indicates the benefits of current technologies for locating and converting Telugu text discovered in image documents. It also demonstrates improvements in OCR technology.",
      "abstract": "Visual data dominates the digital landscape these days. However, there remains a significant gap in the digitization of textual content in regional languages such as Telugu. This research addresses this gap and utilizes the capabilities of the programming language and set of specialized libraries pytesseract for optical character recognition (OCR), PIL for image processing and Levenshtein for accuracy evaluation to appropriately extract text from images in the Telugu script. There are several important steps in this project. Initially, this work sets up a dependable system for uploading pictures. Secondly, carefully extracting text from these submitted photos using py-tesseract. Using the Levenshtein distance measure, and then compared with the ground truth to determine correctness. This metric measures the accuracy of the text output through looking at the slight variations between the word error rate (WER) and character error rate (CER). This is capable of finding misspelled words and unknown characters providing a precise evaluation of the OCR's effectiveness. This study also examines the current state of OCR in Telugu text processing and indicates the benefits of current technologies for locating and converting Telugu text discovered in image documents. It also demonstrates improvements in OCR technology.",
      "doi": "https://doi.org/10.1109/ic2pct60090.2024.10486323",
      "openalex_id": "https://openalex.org/W4394583388",
      "arxiv_id": "",
      "publication_date": "2024-02-09",
      "published": "2024-02-09",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item describes an OCR system for handwritten Telugu text, focusing on text extraction and evaluation metrics, with no discussion of Spoken Language Models or speech-based architectures, thus failing all inclusion criteria and not fitting the exclusion criteria either as it is not a speech-only technology paper; overall it should be excluded on relevance.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item describes an OCR system for handwritten Telugu text, focusing on text extraction and evaluation metrics, with no discussion of Spoken Language Models or speech-based architectures, thus failing all inclusion criteria and not fitting the exclusion criteria either as it is not a speech-only technology paper; overall it should be excluded on relevance.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on Optical Character Recognition (OCR) technology for Telugu script text extraction from images, which is primarily a text processing task rather than spoken language modeling or speech language modeling. The abstract does not mention spoken language models, speech language models, or architecture/training/evaluation related to speech as a core modeling object, nor does it cover full-duplex spoken language interactions, cognitive parallelism, or real-time interactive features as required by the inclusion criteria. Therefore, the study does not meet the inclusion criteria centered on spoken language models or full-duplex speech interaction, and it falls outside the defined scope.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on Optical Character Recognition (OCR) technology for Telugu script text extraction from images, which is primarily a text processing task rather than spoken language modeling or speech language modeling. The abstract does not mention spoken language models, speech language models, or architecture/training/evaluation related to speech as a core modeling object, nor does it cover full-duplex spoken language interactions, cognitive parallelism, or real-time interactive features as required by the inclusion criteria. Therefore, the study does not meet the inclusion criteria centered on spoken language models or full-duplex speech interaction, and it falls outside the defined scope.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "BAHBench: A Unified Benchmark for Evaluating Bio-Acoustic Health With Acoustic Foundation Models",
    "abstract": "Acoustic foundation models, through self-supervised learning on large amounts of unlabeled speech data, can acquire rich acoustic representations. In recent years, these models have demonstrated substantial potential in audio-based health-related tasks, remarkably enhancing the efficiency and quality of healthcare services and contributing to the advancement of smart healthcare. However, there is currently a lack of systematic research and exploration on the performance of acoustic foundation models in health-related tasks. Furthermore, inconsistencies in evaluation methods and experimental setups hinder fair comparisons between different methods, severely impeding progress in this field. To address these challenges, we establish a unified Benchmark for evaluating Bio-Acoustic health via acoustic foundation models, namely BAHBench. BAHBench encompasses 6 distinct health-related tasks and evaluates 12 acoustic foundation models within a unified evaluation framework and parameter settings, enabling fair comparisons across different models. Our objective is to explore the effectiveness of current acoustic foundation models in health-related tasks. Thus, we discuss the impact of model size and data diversity on performance, and investigate feature selection and efficient fine-tuning strategy. Experimental results show that different health-related tasks benefit from features from different layers of the foundation model, while LoRA fine-tuning further enhances the model's performance on downstream tasks. Our goal is to provide clear and comprehensive guidance for future researchers. The code related to this study will be available to the research community to promote transparency and reproducibility.",
    "metadata": {
      "title": "BAHBench: A Unified Benchmark for Evaluating Bio-Acoustic Health With Acoustic Foundation Models",
      "summary": "Acoustic foundation models, through self-supervised learning on large amounts of unlabeled speech data, can acquire rich acoustic representations. In recent years, these models have demonstrated substantial potential in audio-based health-related tasks, remarkably enhancing the efficiency and quality of healthcare services and contributing to the advancement of smart healthcare. However, there is currently a lack of systematic research and exploration on the performance of acoustic foundation models in health-related tasks. Furthermore, inconsistencies in evaluation methods and experimental setups hinder fair comparisons between different methods, severely impeding progress in this field. To address these challenges, we establish a unified Benchmark for evaluating Bio-Acoustic health via acoustic foundation models, namely BAHBench. BAHBench encompasses 6 distinct health-related tasks and evaluates 12 acoustic foundation models within a unified evaluation framework and parameter settings, enabling fair comparisons across different models. Our objective is to explore the effectiveness of current acoustic foundation models in health-related tasks. Thus, we discuss the impact of model size and data diversity on performance, and investigate feature selection and efficient fine-tuning strategy. Experimental results show that different health-related tasks benefit from features from different layers of the foundation model, while LoRA fine-tuning further enhances the model's performance on downstream tasks. Our goal is to provide clear and comprehensive guidance for future researchers. The code related to this study will be available to the research community to promote transparency and reproducibility.",
      "abstract": "Acoustic foundation models, through self-supervised learning on large amounts of unlabeled speech data, can acquire rich acoustic representations. In recent years, these models have demonstrated substantial potential in audio-based health-related tasks, remarkably enhancing the efficiency and quality of healthcare services and contributing to the advancement of smart healthcare. However, there is currently a lack of systematic research and exploration on the performance of acoustic foundation models in health-related tasks. Furthermore, inconsistencies in evaluation methods and experimental setups hinder fair comparisons between different methods, severely impeding progress in this field. To address these challenges, we establish a unified Benchmark for evaluating Bio-Acoustic health via acoustic foundation models, namely BAHBench. BAHBench encompasses 6 distinct health-related tasks and evaluates 12 acoustic foundation models within a unified evaluation framework and parameter settings, enabling fair comparisons across different models. Our objective is to explore the effectiveness of current acoustic foundation models in health-related tasks. Thus, we discuss the impact of model size and data diversity on performance, and investigate feature selection and efficient fine-tuning strategy. Experimental results show that different health-related tasks benefit from features from different layers of the foundation model, while LoRA fine-tuning further enhances the model's performance on downstream tasks. Our goal is to provide clear and comprehensive guidance for future researchers. The code related to this study will be available to the research community to promote transparency and reproducibility.",
      "doi": "https://doi.org/10.1109/jbhi.2025.3543968",
      "openalex_id": "https://openalex.org/W4407782501",
      "arxiv_id": "",
      "publication_date": "2025-02-20",
      "published": "2025-02-20",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "Does not focus on Spoken Language Models (SLMs) or equivalent speech-language modeling concepts; centers on bio-acoustic health benchmarks for general acoustic foundation models rather than speech/voice-centric modeling, training, or evaluation typical of SLMs, and thus fails the inclusion criteria while not satisfying exclusion criteria specifics.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "Does not focus on Spoken Language Models (SLMs) or equivalent speech-language modeling concepts; centers on bio-acoustic health benchmarks for general acoustic foundation models rather than speech/voice-centric modeling, training, or evaluation typical of SLMs, and thus fails the inclusion criteria while not satisfying exclusion criteria specifics.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on evaluating acoustic foundation models for health-related tasks using bio-acoustic signals, not specifically on spoken language models or full-duplex spoken language models as defined in the inclusion criteria. It does not discuss speech language modeling, training, interaction protocols like full-duplex communication, or behavioral arbitration, but rather concentrates on health-related acoustic representations and benchmarking models for these tasks. Therefore, it does not meet the core thematic requirements and lacks discussion on key concepts such as synchronous spoken interaction and language modeling of speech sequences, making it outside the scope for inclusion.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on evaluating acoustic foundation models for health-related tasks using bio-acoustic signals, not specifically on spoken language models or full-duplex spoken language models as defined in the inclusion criteria. It does not discuss speech language modeling, training, interaction protocols like full-duplex communication, or behavioral arbitration, but rather concentrates on health-related acoustic representations and benchmarking models for these tasks. Therefore, it does not meet the core thematic requirements and lacks discussion on key concepts such as synchronous spoken interaction and language modeling of speech sequences, making it outside the scope for inclusion.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "ImageBind One Embedding Space to Bind Them All",
    "abstract": "We present ImageBind, an approach to learn a joint embedding across six different modalities - images, text, audio, depth, thermal, and IMU data. We show that all combinations of paired data are not necessary to train such a joint embedding, and only image-paired data is sufficient to bind the modalities together. ImageBind can leverage recent large scale vision-language models, and extends their zero-shot capabilities to new modalities just by using their natural pairing with images. It enables novel emergent applications 'out-of-the-box' including cross-modal retrieval, composing modalities with arithmetic, cross-modal detection and generation. The emergent capabilities improve with the strength of the image encoder and we set a new state-of-the-art on emergent zero-shot recognition tasks across modalities, outperforming specialist supervised models. Finally, we show strong few-shot recognition results outperforming prior work, and that ImageBind serves as a new way to evaluate vision models for visual and non-visual tasks.",
    "metadata": {
      "title": "ImageBind One Embedding Space to Bind Them All",
      "summary": "We present ImageBind, an approach to learn a joint embedding across six different modalities - images, text, audio, depth, thermal, and IMU data. We show that all combinations of paired data are not necessary to train such a joint embedding, and only image-paired data is sufficient to bind the modalities together. ImageBind can leverage recent large scale vision-language models, and extends their zero-shot capabilities to new modalities just by using their natural pairing with images. It enables novel emergent applications 'out-of-the-box' including cross-modal retrieval, composing modalities with arithmetic, cross-modal detection and generation. The emergent capabilities improve with the strength of the image encoder and we set a new state-of-the-art on emergent zero-shot recognition tasks across modalities, outperforming specialist supervised models. Finally, we show strong few-shot recognition results outperforming prior work, and that ImageBind serves as a new way to evaluate vision models for visual and non-visual tasks.",
      "abstract": "We present ImageBind, an approach to learn a joint embedding across six different modalities - images, text, audio, depth, thermal, and IMU data. We show that all combinations of paired data are not necessary to train such a joint embedding, and only image-paired data is sufficient to bind the modalities together. ImageBind can leverage recent large scale vision-language models, and extends their zero-shot capabilities to new modalities just by using their natural pairing with images. It enables novel emergent applications 'out-of-the-box' including cross-modal retrieval, composing modalities with arithmetic, cross-modal detection and generation. The emergent capabilities improve with the strength of the image encoder and we set a new state-of-the-art on emergent zero-shot recognition tasks across modalities, outperforming specialist supervised models. Finally, we show strong few-shot recognition results outperforming prior work, and that ImageBind serves as a new way to evaluate vision models for visual and non-visual tasks.",
      "doi": "https://doi.org/10.1109/cvpr52729.2023.01457",
      "openalex_id": "https://openalex.org/W4386071707",
      "arxiv_id": "",
      "publication_date": "2023-06-01",
      "published": "2023-06-01",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The paper centers on multimodal embeddings across images, text, and audio, and does not treat spoken language models or speech-language modeling as its core focus; it lacks emphasis on turn-taking, synchronization, or speech-specific evaluation.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The paper centers on multimodal embeddings across images, text, and audio, and does not treat spoken language models or speech-language modeling as its core focus; it lacks emphasis on turn-taking, synchronization, or speech-specific evaluation.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper presents ImageBind, a multi-modal embedding model emphasizing images and other modalities like text, audio, and sensors, but does not focus on spoken language models or speech as the core modeling object, nor does it address full-duplex spoken interaction capabilities, synchronization, or speech language modeling as per the inclusion criteria; thus, it does not meet the thematic and conceptual relevancy requirements.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper presents ImageBind, a multi-modal embedding model emphasizing images and other modalities like text, audio, and sensors, but does not focus on spoken language models or speech as the core modeling object, nor does it address full-duplex spoken interaction capabilities, synchronization, or speech language modeling as per the inclusion criteria; thus, it does not meet the thematic and conceptual relevancy requirements.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "PnG BERT: Augmented BERT on Phonemes and Graphemes for Neural TTS",
    "abstract": "This paper introduces PnG BERT, a new encoder model for neural TTS.This model is augmented from the original BERT model, by taking both phoneme and grapheme representations of text as input, as well as the word-level alignment between them.It can be pre-trained on a large text corpus in a selfsupervised manner, and fine-tuned in a TTS task.Experimental results show that a neural TTS model using a pre-trained PnG BERT as its encoder yields more natural prosody and more accurate pronunciation than a baseline model using only phoneme input with no pre-training.Subjective side-by-side preference evaluations show that raters have no statistically significant preference between the speech synthesized using a PnG BERT and ground truth recordings from professional speakers.",
    "metadata": {
      "title": "PnG BERT: Augmented BERT on Phonemes and Graphemes for Neural TTS",
      "summary": "This paper introduces PnG BERT, a new encoder model for neural TTS.This model is augmented from the original BERT model, by taking both phoneme and grapheme representations of text as input, as well as the word-level alignment between them.It can be pre-trained on a large text corpus in a selfsupervised manner, and fine-tuned in a TTS task.Experimental results show that a neural TTS model using a pre-trained PnG BERT as its encoder yields more natural prosody and more accurate pronunciation than a baseline model using only phoneme input with no pre-training.Subjective side-by-side preference evaluations show that raters have no statistically significant preference between the speech synthesized using a PnG BERT and ground truth recordings from professional speakers.",
      "abstract": "This paper introduces PnG BERT, a new encoder model for neural TTS.This model is augmented from the original BERT model, by taking both phoneme and grapheme representations of text as input, as well as the word-level alignment between them.It can be pre-trained on a large text corpus in a selfsupervised manner, and fine-tuned in a TTS task.Experimental results show that a neural TTS model using a pre-trained PnG BERT as its encoder yields more natural prosody and more accurate pronunciation than a baseline model using only phoneme input with no pre-training.Subjective side-by-side preference evaluations show that raters have no statistically significant preference between the speech synthesized using a PnG BERT and ground truth recordings from professional speakers.",
      "doi": "https://doi.org/10.21437/interspeech.2021-1757",
      "openalex_id": "https://openalex.org/W3197324626",
      "arxiv_id": "",
      "publication_date": "2021-08-27",
      "published": "2021-08-27",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "PnG BERT centers on a neural TTS encoder using phoneme and grapheme inputs; it treats speech generation as a downstream task rather than making speech the core modeling object or addressing interactive, full-duplex speech-language modeling, so it does not meet the 'SLM/FD-SLM core' criterion and should be excluded.",
      "evaluation": 2
    },
    "round-A_JuniorNano_reasoning": "PnG BERT centers on a neural TTS encoder using phoneme and grapheme inputs; it treats speech generation as a downstream task rather than making speech the core modeling object or addressing interactive, full-duplex speech-language modeling, so it does not meet the 'SLM/FD-SLM core' criterion and should be excluded.",
    "round-A_JuniorNano_evaluation": 2,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on an augmented BERT model for neural text-to-speech synthesis using phoneme and grapheme inputs. However, it centers on enhancing TTS quality rather than explicitly modeling spoken language with speech as the core modeling object or addressing full-duplex interactive speech communication, synchronization, or behavioral arbitration in interaction contexts. The paper does not mention spoken language models, speech language models, or related concepts in the abstract, nor does it discuss modeling or evaluating real-time interactive dialogue scenarios demanded by the inclusion criteria. Therefore, it does not satisfy the inclusion criteria and falls under exclusion for focusing on single-task TTS without spoken language modeling or synchronization aspects.",
      "evaluation": 2
    },
    "round-A_JuniorMini_reasoning": "The study focuses on an augmented BERT model for neural text-to-speech synthesis using phoneme and grapheme inputs. However, it centers on enhancing TTS quality rather than explicitly modeling spoken language with speech as the core modeling object or addressing full-duplex interactive speech communication, synchronization, or behavioral arbitration in interaction contexts. The paper does not mention spoken language models, speech language models, or related concepts in the abstract, nor does it discuss modeling or evaluating real-time interactive dialogue scenarios demanded by the inclusion criteria. Therefore, it does not satisfy the inclusion criteria and falls under exclusion for focusing on single-task TTS without spoken language modeling or synchronization aspects.",
    "round-A_JuniorMini_evaluation": 2,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:2)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "FLEURS: FEW-Shot Learning Evaluation of Universal Representations of Speech",
    "abstract": "We introduce FLEURS, the Few-shot Learning Evaluation of Universal Representations of Speech benchmark. FLEURS is an n-way parallel speech dataset in 102 languages built on top of the machine translation FLoRes-101 benchmark, with approximately 12 hours of speech supervision per language. FLEURS can be used for a variety of speech tasks, including Automatic Speech Recognition (ASR), Speech Language Identification (Speech LangID), Speech-Text Retrieval. In this paper, we provide baselines for the tasks based on multilingual pre-trained models like speech-only w2v-BERT [1] and speech-text multimodal mSLAM [2]. The goal of FLEURS is to enable speech technology in more languages and catalyze research in low-resource speech understanding. <sup xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">1</sup> .",
    "metadata": {
      "title": "FLEURS: FEW-Shot Learning Evaluation of Universal Representations of Speech",
      "summary": "We introduce FLEURS, the Few-shot Learning Evaluation of Universal Representations of Speech benchmark. FLEURS is an n-way parallel speech dataset in 102 languages built on top of the machine translation FLoRes-101 benchmark, with approximately 12 hours of speech supervision per language. FLEURS can be used for a variety of speech tasks, including Automatic Speech Recognition (ASR), Speech Language Identification (Speech LangID), Speech-Text Retrieval. In this paper, we provide baselines for the tasks based on multilingual pre-trained models like speech-only w2v-BERT [1] and speech-text multimodal mSLAM [2]. The goal of FLEURS is to enable speech technology in more languages and catalyze research in low-resource speech understanding. <sup xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">1</sup> .",
      "abstract": "We introduce FLEURS, the Few-shot Learning Evaluation of Universal Representations of Speech benchmark. FLEURS is an n-way parallel speech dataset in 102 languages built on top of the machine translation FLoRes-101 benchmark, with approximately 12 hours of speech supervision per language. FLEURS can be used for a variety of speech tasks, including Automatic Speech Recognition (ASR), Speech Language Identification (Speech LangID), Speech-Text Retrieval. In this paper, we provide baselines for the tasks based on multilingual pre-trained models like speech-only w2v-BERT [1] and speech-text multimodal mSLAM [2]. The goal of FLEURS is to enable speech technology in more languages and catalyze research in low-resource speech understanding. <sup xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">1</sup> .",
      "doi": "https://doi.org/10.1109/slt54892.2023.10023141",
      "openalex_id": "https://openalex.org/W4319862635",
      "arxiv_id": "",
      "publication_date": "2023-01-09",
      "published": "2023-01-09",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "雖然是與語音表徵與多語言任務相關的研究，但摘要未明確出現「spoken language model(s) / speech language model(s)」或等價描述，核心聚焦於語音表徵的基準與多任務評估，未明確以語音為核心的語音語言模型訓練/推理框架，因此不符合必須出現 SLM 規範。",
      "evaluation": 2
    },
    "round-A_JuniorNano_reasoning": "雖然是與語音表徵與多語言任務相關的研究，但摘要未明確出現「spoken language model(s) / speech language model(s)」或等價描述，核心聚焦於語音表徵的基準與多任務評估，未明確以語音為核心的語音語言模型訓練/推理框架，因此不符合必須出現 SLM 規範。",
    "round-A_JuniorNano_evaluation": 2,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper introduces FLEURS, a benchmark for universal speech representations with multilingual pre-trained models focusing on tasks like ASR and language identification; however, it does not explicitly address spoken language models (SLMs) or full-duplex interaction, nor does it discuss model architectures or evaluations oriented toward simultaneous turn-taking, cognitive parallelism, or real-time interaction as required by the inclusion criteria.",
      "evaluation": 2
    },
    "round-A_JuniorMini_reasoning": "The paper introduces FLEURS, a benchmark for universal speech representations with multilingual pre-trained models focusing on tasks like ASR and language identification; however, it does not explicitly address spoken language models (SLMs) or full-duplex interaction, nor does it discuss model architectures or evaluations oriented toward simultaneous turn-taking, cognitive parallelism, or real-time interaction as required by the inclusion criteria.",
    "round-A_JuniorMini_evaluation": 2,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:2)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "From WER and RIL to MER and WIL: improved evaluation measures for connected speech recognition",
    "abstract": "The word error rate (WER), commonly used in ASR assessment, measures the cost of restoring the output word sequence to the original input sequence. However, for most CSR applications apart from dictation machines a more meaningful performance measure would be given by the proportion of information communicated. In this article we introduce two new absolute CSR performance measures: MER (match error rate) and WIL (word information lost). MER is the proportion of I/O word matches which are errors. WIL is a simple approximation to the proportion of word information lost which overcomes the problems associated with the RIL (relative information lost) measure that was proposed half a century ago. Issues relating to ideal performance measurement are discussed and the commonly used Viterbi input/output alignment procedure, with zero weight for hits and equal weight for substitutions, deletions and insertions, is shown to be optimal.",
    "metadata": {
      "title": "From WER and RIL to MER and WIL: improved evaluation measures for connected speech recognition",
      "summary": "The word error rate (WER), commonly used in ASR assessment, measures the cost of restoring the output word sequence to the original input sequence. However, for most CSR applications apart from dictation machines a more meaningful performance measure would be given by the proportion of information communicated. In this article we introduce two new absolute CSR performance measures: MER (match error rate) and WIL (word information lost). MER is the proportion of I/O word matches which are errors. WIL is a simple approximation to the proportion of word information lost which overcomes the problems associated with the RIL (relative information lost) measure that was proposed half a century ago. Issues relating to ideal performance measurement are discussed and the commonly used Viterbi input/output alignment procedure, with zero weight for hits and equal weight for substitutions, deletions and insertions, is shown to be optimal.",
      "abstract": "The word error rate (WER), commonly used in ASR assessment, measures the cost of restoring the output word sequence to the original input sequence. However, for most CSR applications apart from dictation machines a more meaningful performance measure would be given by the proportion of information communicated. In this article we introduce two new absolute CSR performance measures: MER (match error rate) and WIL (word information lost). MER is the proportion of I/O word matches which are errors. WIL is a simple approximation to the proportion of word information lost which overcomes the problems associated with the RIL (relative information lost) measure that was proposed half a century ago. Issues relating to ideal performance measurement are discussed and the commonly used Viterbi input/output alignment procedure, with zero weight for hits and equal weight for substitutions, deletions and insertions, is shown to be optimal.",
      "doi": "https://doi.org/10.21437/interspeech.2004-668",
      "openalex_id": "https://openalex.org/W38194800",
      "arxiv_id": "",
      "publication_date": "2004-10-04",
      "published": "2004-10-04",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "此篇論文僅討論聽寫評估指標（WER/MER/WIL），非以 Spoken Language Models/FD-SLM 作為核心的語音語言模型研究，也未在近三年內聚焦或評估語音模型架構與同步互動，因此不符合納入條件。",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "此篇論文僅討論聽寫評估指標（WER/MER/WIL），非以 Spoken Language Models/FD-SLM 作為核心的語音語言模型研究，也未在近三年內聚焦或評估語音模型架構與同步互動，因此不符合納入條件。",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on improved evaluation metrics for connected speech recognition (CSR) specifically by proposing MER and WIL measures as better alternatives to the traditional WER used in ASR assessment. However, the work centers on evaluation of CSR systems and does not explicitly involve Spoken Language Models (SLMs) or full-duplex spoken interaction capabilities, nor does it discuss model architecture, training, or end-to-end speech language modeling; it mainly addresses performance metrics for ASR. It also lacks discussion on synchronization, cognitive parallelism, real-time interaction, or speech-language modeling frameworks as specified in the inclusion criteria. Thus, it does not meet the required inclusion thresholds and fits some of the exclusion criteria focusing solely on ASR evaluation metrics without broader spoken language modeling or interactive dialogue system perspectives.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on improved evaluation metrics for connected speech recognition (CSR) specifically by proposing MER and WIL measures as better alternatives to the traditional WER used in ASR assessment. However, the work centers on evaluation of CSR systems and does not explicitly involve Spoken Language Models (SLMs) or full-duplex spoken interaction capabilities, nor does it discuss model architecture, training, or end-to-end speech language modeling; it mainly addresses performance metrics for ASR. It also lacks discussion on synchronization, cognitive parallelism, real-time interaction, or speech-language modeling frameworks as specified in the inclusion criteria. Thus, it does not meet the required inclusion thresholds and fits some of the exclusion criteria focusing solely on ASR evaluation metrics without broader spoken language modeling or interactive dialogue system perspectives.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "NAM+: Towards Scalable End-to-End Contextual Biasing for Adaptive ASR",
    "abstract": "Attention-based biasing techniques for end-to-end ASR systems are able to achieve large accuracy gains without requiring the inference algorithm adjustments and parameter tuning common to fusion approaches. However, it is challenging to simultaneously scale up attention-based biasing to realistic numbers of biased phrases; maintain in-domain WER gains, while minimizing out-of-domain losses; and run in real time. We present NAM+, an attention-based biasing approach which achieves a 16X inference speedup per acoustic frame over prior work when run with 3,000 biasing entities, as measured on a typical mobile CPU. NAM+ achieves these run-time gains through a combination of Two-Pass Hierarchical Attention and Dilated Context Update. Compared to the adapted baseline, NAM+ further decreases the in-domain WER by up to 12.6% relative, while incurring an out-of-domain WER regression of 20% relative. Compared to the non-adapted baseline, the out-of-domain WER regression is 7.1 % relative.",
    "metadata": {
      "title": "NAM+: Towards Scalable End-to-End Contextual Biasing for Adaptive ASR",
      "summary": "Attention-based biasing techniques for end-to-end ASR systems are able to achieve large accuracy gains without requiring the inference algorithm adjustments and parameter tuning common to fusion approaches. However, it is challenging to simultaneously scale up attention-based biasing to realistic numbers of biased phrases; maintain in-domain WER gains, while minimizing out-of-domain losses; and run in real time. We present NAM+, an attention-based biasing approach which achieves a 16X inference speedup per acoustic frame over prior work when run with 3,000 biasing entities, as measured on a typical mobile CPU. NAM+ achieves these run-time gains through a combination of Two-Pass Hierarchical Attention and Dilated Context Update. Compared to the adapted baseline, NAM+ further decreases the in-domain WER by up to 12.6% relative, while incurring an out-of-domain WER regression of 20% relative. Compared to the non-adapted baseline, the out-of-domain WER regression is 7.1 % relative.",
      "abstract": "Attention-based biasing techniques for end-to-end ASR systems are able to achieve large accuracy gains without requiring the inference algorithm adjustments and parameter tuning common to fusion approaches. However, it is challenging to simultaneously scale up attention-based biasing to realistic numbers of biased phrases; maintain in-domain WER gains, while minimizing out-of-domain losses; and run in real time. We present NAM+, an attention-based biasing approach which achieves a 16X inference speedup per acoustic frame over prior work when run with 3,000 biasing entities, as measured on a typical mobile CPU. NAM+ achieves these run-time gains through a combination of Two-Pass Hierarchical Attention and Dilated Context Update. Compared to the adapted baseline, NAM+ further decreases the in-domain WER by up to 12.6% relative, while incurring an out-of-domain WER regression of 20% relative. Compared to the non-adapted baseline, the out-of-domain WER regression is 7.1 % relative.",
      "doi": "https://doi.org/10.1109/slt54892.2023.10023323",
      "openalex_id": "https://openalex.org/W4319862232",
      "arxiv_id": "",
      "publication_date": "2023-01-09",
      "published": "2023-01-09",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The work centers on end-to-end speech modeling by proposing attention-based contextual biasing for ASR, with architectural innovations and evaluation of runtime and WER, aligning with the Spoken Language Models / speech-centric modeling criteria, and it falls within the past 3 years; it focuses on ASR architecture rather than dialogue interaction, but that does not disqualify it from being included.",
      "evaluation": 4
    },
    "round-A_JuniorNano_reasoning": "The work centers on end-to-end speech modeling by proposing attention-based contextual biasing for ASR, with architectural innovations and evaluation of runtime and WER, aligning with the Spoken Language Models / speech-centric modeling criteria, and it falls within the past 3 years; it focuses on ASR architecture rather than dialogue interaction, but that does not disqualify it from being included.",
    "round-A_JuniorNano_evaluation": 4,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on an attention-based biasing approach for end-to-end ASR systems, emphasizing improvements in inference speed and WER metrics. However, the research primarily concentrates on ASR without involving Spoken Language Models or full-duplex interaction capabilities, cognitive parallelism, or synchronization mechanisms. It evaluates performance using WER, without addressing spoken language modeling as a core research focus or multi-dimensional interactive metrics. Therefore, it does not meet the inclusion criteria centered on spoken language models and interactive, synchronized speech modeling.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on an attention-based biasing approach for end-to-end ASR systems, emphasizing improvements in inference speed and WER metrics. However, the research primarily concentrates on ASR without involving Spoken Language Models or full-duplex interaction capabilities, cognitive parallelism, or synchronization mechanisms. It evaluates performance using WER, without addressing spoken language modeling as a core research focus or multi-dimensional interactive metrics. Therefore, it does not meet the inclusion criteria centered on spoken language models and interactive, synchronized speech modeling.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": {
      "reasoning": "I reviewed the two junior reviewers (one rated 4, the other 1) and re-examined the title and abstract: this paper proposes attention-based contextual biasing for end-to-end ASR (NAM+), reporting runtime and WER improvements, but it does not mention spoken language models, speech-level language modeling, full-duplex interaction, synchronization, overlap handling, or any interactive/FD-SLM evaluation—only ASR biasing and WER/runtime metrics. This places it squarely within the exclusion criteria (focus solely on ASR techniques and WER evaluation, not on SLM/FD-SLM or interactive speech modeling). Therefore it should be excluded.",
      "evaluation": 1
    },
    "round-B_SeniorLead_reasoning": "I reviewed the two junior reviewers (one rated 4, the other 1) and re-examined the title and abstract: this paper proposes attention-based contextual biasing for end-to-end ASR (NAM+), reporting runtime and WER improvements, but it does not mention spoken language models, speech-level language modeling, full-duplex interaction, synchronization, overlap handling, or any interactive/FD-SLM evaluation—only ASR biasing and WER/runtime metrics. This places it squarely within the exclusion criteria (focus solely on ASR techniques and WER evaluation, not on SLM/FD-SLM or interactive speech modeling). Therefore it should be excluded.",
    "round-B_SeniorLead_evaluation": 1,
    "final_verdict": "exclude (senior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Overcoming Catastrophic Forgetting in Zero-Shot Cross-Lingual Generation",
    "abstract": "In this paper, we explore the challenging problem of performing a generative task in a target language when labeled data is only available in English, using summarization as a case study. We assume a strict setting with no access to parallel data or machine translation and find that common transfer learning approaches struggle in this setting, as a generative multilingual model fine-tuned purely on English catastrophically forgets how to generate non-English. Given the recent rise of parameter-efficient adaptation techniques, we conduct the first investigation into how one such method, prompt tuning (Lester et al., 2021), can overcome catastrophic forgetting to enable zero-shot cross-lingual generation. Our experiments show that parameter-efficient prompt tuning provides gains over standard fine-tuning when transferring between less-related languages, e.g., from English to Thai. However, a significant gap still remains between these methods and fully-supervised baselines. To improve cross-lingual transfer further, we explore several approaches, including: (1) mixing in unlabeled multilingual data, and (2) explicitly factoring prompts into recombinable language and task components. Our approaches can provide further quality gains, suggesting that robust zero-shot cross-lingual generation is within reach.",
    "metadata": {
      "title": "Overcoming Catastrophic Forgetting in Zero-Shot Cross-Lingual Generation",
      "summary": "In this paper, we explore the challenging problem of performing a generative task in a target language when labeled data is only available in English, using summarization as a case study. We assume a strict setting with no access to parallel data or machine translation and find that common transfer learning approaches struggle in this setting, as a generative multilingual model fine-tuned purely on English catastrophically forgets how to generate non-English. Given the recent rise of parameter-efficient adaptation techniques, we conduct the first investigation into how one such method, prompt tuning (Lester et al., 2021), can overcome catastrophic forgetting to enable zero-shot cross-lingual generation. Our experiments show that parameter-efficient prompt tuning provides gains over standard fine-tuning when transferring between less-related languages, e.g., from English to Thai. However, a significant gap still remains between these methods and fully-supervised baselines. To improve cross-lingual transfer further, we explore several approaches, including: (1) mixing in unlabeled multilingual data, and (2) explicitly factoring prompts into recombinable language and task components. Our approaches can provide further quality gains, suggesting that robust zero-shot cross-lingual generation is within reach.",
      "abstract": "In this paper, we explore the challenging problem of performing a generative task in a target language when labeled data is only available in English, using summarization as a case study. We assume a strict setting with no access to parallel data or machine translation and find that common transfer learning approaches struggle in this setting, as a generative multilingual model fine-tuned purely on English catastrophically forgets how to generate non-English. Given the recent rise of parameter-efficient adaptation techniques, we conduct the first investigation into how one such method, prompt tuning (Lester et al., 2021), can overcome catastrophic forgetting to enable zero-shot cross-lingual generation. Our experiments show that parameter-efficient prompt tuning provides gains over standard fine-tuning when transferring between less-related languages, e.g., from English to Thai. However, a significant gap still remains between these methods and fully-supervised baselines. To improve cross-lingual transfer further, we explore several approaches, including: (1) mixing in unlabeled multilingual data, and (2) explicitly factoring prompts into recombinable language and task components. Our approaches can provide further quality gains, suggesting that robust zero-shot cross-lingual generation is within reach.",
      "doi": "https://doi.org/10.18653/v1/2022.emnlp-main.630",
      "openalex_id": "https://openalex.org/W4385573964",
      "arxiv_id": "",
      "publication_date": "2022-01-01",
      "published": "2022-01-01",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "此研究聚焦跨語言文本生成，未以語音/語音模型為核心，也不符合以 Spoken Language Models 或語音語言建模為核心的納入條件，因此應排除。",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "此研究聚焦跨語言文本生成，未以語音/語音模型為核心，也不符合以 Spoken Language Models 或語音語言建模為核心的納入條件，因此應排除。",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on zero-shot cross-lingual generation using summarization tasks, with no explicit mention of Spoken Language Models, speech modeling, or full-duplex spoken interaction capabilities that are central to the inclusion criteria. The research is centered on text generation and transfer learning methods without focusing on speech sequence modeling, end-to-end speech architectures, synchronous dialogue behavior, or detailed speech-based evaluation. Therefore, it does not meet the inclusion criteria related to spoken language modeling as the core subject and modeling target.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on zero-shot cross-lingual generation using summarization tasks, with no explicit mention of Spoken Language Models, speech modeling, or full-duplex spoken interaction capabilities that are central to the inclusion criteria. The research is centered on text generation and transfer learning methods without focusing on speech sequence modeling, end-to-end speech architectures, synchronous dialogue behavior, or detailed speech-based evaluation. Therefore, it does not meet the inclusion criteria related to spoken language modeling as the core subject and modeling target.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Non-Attentive Tacotron: Robust and Controllable Neural TTS Synthesis Including Unsupervised Duration Modeling",
    "abstract": "This paper presents Non-Attentive Tacotron based on the Tacotron 2 text-to-speech model, replacing the attention mechanism with an explicit duration predictor. This improves robustness significantly as measured by unaligned duration ratio and word deletion rate, two metrics introduced in this paper for large-scale robustness evaluation using a pre-trained speech recognition model. With the use of Gaussian upsampling, Non-Attentive Tacotron achieves a 5-scale mean opinion score for naturalness of 4.41, slightly outperforming Tacotron 2. The duration predictor enables both utterance-wide and per-phoneme control of duration at inference time. When accurate target durations are scarce or unavailable in the training data, we propose a method using a fine-grained variational auto-encoder to train the duration predictor in a semi-supervised or unsupervised manner, with results almost as good as supervised training.",
    "metadata": {
      "title": "Non-Attentive Tacotron: Robust and Controllable Neural TTS Synthesis Including Unsupervised Duration Modeling",
      "summary": "This paper presents Non-Attentive Tacotron based on the Tacotron 2 text-to-speech model, replacing the attention mechanism with an explicit duration predictor. This improves robustness significantly as measured by unaligned duration ratio and word deletion rate, two metrics introduced in this paper for large-scale robustness evaluation using a pre-trained speech recognition model. With the use of Gaussian upsampling, Non-Attentive Tacotron achieves a 5-scale mean opinion score for naturalness of 4.41, slightly outperforming Tacotron 2. The duration predictor enables both utterance-wide and per-phoneme control of duration at inference time. When accurate target durations are scarce or unavailable in the training data, we propose a method using a fine-grained variational auto-encoder to train the duration predictor in a semi-supervised or unsupervised manner, with results almost as good as supervised training.",
      "abstract": "This paper presents Non-Attentive Tacotron based on the Tacotron 2 text-to-speech model, replacing the attention mechanism with an explicit duration predictor. This improves robustness significantly as measured by unaligned duration ratio and word deletion rate, two metrics introduced in this paper for large-scale robustness evaluation using a pre-trained speech recognition model. With the use of Gaussian upsampling, Non-Attentive Tacotron achieves a 5-scale mean opinion score for naturalness of 4.41, slightly outperforming Tacotron 2. The duration predictor enables both utterance-wide and per-phoneme control of duration at inference time. When accurate target durations are scarce or unavailable in the training data, we propose a method using a fine-grained variational auto-encoder to train the duration predictor in a semi-supervised or unsupervised manner, with results almost as good as supervised training.",
      "doi": "https://doi.org/10.48550/arxiv.2010.04301",
      "openalex_id": "https://openalex.org/W3091928890",
      "arxiv_id": "",
      "publication_date": "2020-10-08",
      "published": "2020-10-08",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "本研究聚焦于文本到语音合成（TTS），并非以 Spoken Language Models/Full-Duplex Spoken Language Models 为核心的语音语言建模研究，且缺乏对话交互或同步行为的评估，因此不符合核心纳入标准。",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "本研究聚焦于文本到语音合成（TTS），并非以 Spoken Language Models/Full-Duplex Spoken Language Models 为核心的语音语言建模研究，且缺乏对话交互或同步行为的评估，因此不符合核心纳入标准。",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper focuses on a text-to-speech synthesis model (Tacotron 2 variant) with improvements in duration modeling and robustness, emphasizing acoustic modeling and synthesis quality. However, it does not explicitly mention spoken language models, speech language modeling, or full-duplex interaction capabilities such as turn-taking, cognitive parallelism, or real-time interruption/backchanneling. The study is centered on TTS technology and robustness metrics rather than on synchronization or spoken language modeling architecture evaluations required by the inclusion criteria. Hence, it aligns more with single-task TTS system improvements rather than the broader spoken language model framework with interactive dialogue behavior and evaluation as required.",
      "evaluation": 2
    },
    "round-A_JuniorMini_reasoning": "The paper focuses on a text-to-speech synthesis model (Tacotron 2 variant) with improvements in duration modeling and robustness, emphasizing acoustic modeling and synthesis quality. However, it does not explicitly mention spoken language models, speech language modeling, or full-duplex interaction capabilities such as turn-taking, cognitive parallelism, or real-time interruption/backchanneling. The study is centered on TTS technology and robustness metrics rather than on synchronization or spoken language modeling architecture evaluations required by the inclusion criteria. Hence, it aligns more with single-task TTS system improvements rather than the broader spoken language model framework with interactive dialogue behavior and evaluation as required.",
    "round-A_JuniorMini_evaluation": 2,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:2)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Pengi: An Audio Language Model for Audio Tasks",
    "abstract": "In the domain of audio processing, Transfer Learning has facilitated the rise of Self-Supervised Learning and Zero-Shot Learning techniques. These approaches have led to the development of versatile models capable of tackling a wide array of tasks, while delivering state-of-the-art performance. However, current models inherently lack the capacity to produce the requisite language for open-ended tasks, such as Audio Captioning or Audio Question &amp; Answering. We introduce Pengi, a novel Audio Language Model that leverages Transfer Learning by framing all audio tasks as text-generation tasks. It takes as input, an audio recording, and text, and generates free-form text as output. The input audio is represented as a sequence of continuous embeddings by an audio encoder. A text encoder does the same for the corresponding text input. Both sequences are combined as a prefix to prompt a pre-trained frozen language model. The unified architecture of Pengi enables open-ended tasks and close-ended tasks without any additional fine-tuning or task-specific extensions. When evaluated on 22 downstream tasks, our approach yields state-of-the-art performance in several of them. Our results show that connecting language models with audio models is a major step towards general-purpose audio understanding",
    "metadata": {
      "title": "Pengi: An Audio Language Model for Audio Tasks",
      "summary": "In the domain of audio processing, Transfer Learning has facilitated the rise of Self-Supervised Learning and Zero-Shot Learning techniques. These approaches have led to the development of versatile models capable of tackling a wide array of tasks, while delivering state-of-the-art performance. However, current models inherently lack the capacity to produce the requisite language for open-ended tasks, such as Audio Captioning or Audio Question &amp; Answering. We introduce Pengi, a novel Audio Language Model that leverages Transfer Learning by framing all audio tasks as text-generation tasks. It takes as input, an audio recording, and text, and generates free-form text as output. The input audio is represented as a sequence of continuous embeddings by an audio encoder. A text encoder does the same for the corresponding text input. Both sequences are combined as a prefix to prompt a pre-trained frozen language model. The unified architecture of Pengi enables open-ended tasks and close-ended tasks without any additional fine-tuning or task-specific extensions. When evaluated on 22 downstream tasks, our approach yields state-of-the-art performance in several of them. Our results show that connecting language models with audio models is a major step towards general-purpose audio understanding",
      "abstract": "In the domain of audio processing, Transfer Learning has facilitated the rise of Self-Supervised Learning and Zero-Shot Learning techniques. These approaches have led to the development of versatile models capable of tackling a wide array of tasks, while delivering state-of-the-art performance. However, current models inherently lack the capacity to produce the requisite language for open-ended tasks, such as Audio Captioning or Audio Question &amp; Answering. We introduce Pengi, a novel Audio Language Model that leverages Transfer Learning by framing all audio tasks as text-generation tasks. It takes as input, an audio recording, and text, and generates free-form text as output. The input audio is represented as a sequence of continuous embeddings by an audio encoder. A text encoder does the same for the corresponding text input. Both sequences are combined as a prefix to prompt a pre-trained frozen language model. The unified architecture of Pengi enables open-ended tasks and close-ended tasks without any additional fine-tuning or task-specific extensions. When evaluated on 22 downstream tasks, our approach yields state-of-the-art performance in several of them. Our results show that connecting language models with audio models is a major step towards general-purpose audio understanding",
      "doi": "https://doi.org/10.48550/arxiv.2305.11834",
      "openalex_id": "https://openalex.org/W4377372369",
      "arxiv_id": "",
      "publication_date": "2023-05-19",
      "published": "2023-05-19",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This work explicitly develops an Audio Language Model that treats audio tasks as text-generation using audio/text encoders and a frozen LLM, enabling multi-task spoken-language modeling and evaluation across many tasks, within the last 3 years, meeting the inclusion criteria for Spoken Language Models.",
      "evaluation": 5
    },
    "round-A_JuniorNano_reasoning": "This work explicitly develops an Audio Language Model that treats audio tasks as text-generation using audio/text encoders and a frozen LLM, enabling multi-task spoken-language modeling and evaluation across many tasks, within the last 3 years, meeting the inclusion criteria for Spoken Language Models.",
    "round-A_JuniorNano_evaluation": 5,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper introduces Pengi, an Audio Language Model focusing on framing audio tasks as text-generation tasks, with an architecture combining audio embeddings and text to prompt a language model. However, it does not explicitly mention Spoken Language Models (SLMs), Full-Duplex SLMs, or focus on core modeling and reasoning of speech sequences nor discuss aspects like full-duplex interaction, cognitive parallelism, synchronization, or low-latency interaction. It is also more about audio-language tasks in general than about speech language modeling. Hence, it does not meet the core inclusion criteria centered on SLMs and related interaction characteristics.",
      "evaluation": 2
    },
    "round-A_JuniorMini_reasoning": "The paper introduces Pengi, an Audio Language Model focusing on framing audio tasks as text-generation tasks, with an architecture combining audio embeddings and text to prompt a language model. However, it does not explicitly mention Spoken Language Models (SLMs), Full-Duplex SLMs, or focus on core modeling and reasoning of speech sequences nor discuss aspects like full-duplex interaction, cognitive parallelism, synchronization, or low-latency interaction. It is also more about audio-language tasks in general than about speech language modeling. Hence, it does not meet the core inclusion criteria centered on SLMs and related interaction characteristics.",
    "round-A_JuniorMini_evaluation": 2,
    "round-B_SeniorLead_output": {
      "reasoning": "Reviewed junior feedback (one reviewer mistakenly treated this as SLM-related, the other correctly excluded it). The paper describes an \"audio language model\" for general audio tasks (captioning, QA) that encodes audio into continuous embeddings to prompt a frozen LLM, but it does not use the terms \"spoken language model\"/\"speech language model\" nor focus on speech-level language modeling, full‑duplex interaction, turn-taking, overlap/interrupt handling, low-latency behavior, or related evaluations required by the inclusion criteria — therefore it fails the SLM/FD‑SLM-centered scope and should be excluded.",
      "evaluation": 1
    },
    "round-B_SeniorLead_reasoning": "Reviewed junior feedback (one reviewer mistakenly treated this as SLM-related, the other correctly excluded it). The paper describes an \"audio language model\" for general audio tasks (captioning, QA) that encodes audio into continuous embeddings to prompt a frozen LLM, but it does not use the terms \"spoken language model\"/\"speech language model\" nor focus on speech-level language modeling, full‑duplex interaction, turn-taking, overlap/interrupt handling, low-latency behavior, or related evaluations required by the inclusion criteria — therefore it fails the SLM/FD‑SLM-centered scope and should be excluded.",
    "round-B_SeniorLead_evaluation": 1,
    "final_verdict": "exclude (senior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "SpeechStew: Simply Mix All Available Speech Recognition Data to Train One Large Neural Network",
    "abstract": "We present SpeechStew, a speech recognition model that is trained on a combination of various publicly available speech recognition datasets: AMI, Broadcast News, Common Voice, LibriSpeech, Switchboard/Fisher, Tedlium, and Wall Street Journal. SpeechStew simply mixes all of these datasets together, without any special re-weighting or re-balancing of the datasets. SpeechStew achieves SoTA or near SoTA results across a variety of tasks, without the use of an external language model. Our results include 9.0\\% WER on AMI-IHM, 4.7\\% WER on Switchboard, 8.3\\% WER on CallHome, and 1.3\\% on WSJ, which significantly outperforms prior work with strong external language models. We also demonstrate that SpeechStew learns powerful transfer learning representations. We fine-tune SpeechStew on a noisy low resource speech dataset, CHiME-6. We achieve 38.9\\% WER without a language model, which compares to 38.6\\% WER to a strong HMM baseline with a language model.",
    "metadata": {
      "title": "SpeechStew: Simply Mix All Available Speech Recognition Data to Train One Large Neural Network",
      "summary": "We present SpeechStew, a speech recognition model that is trained on a combination of various publicly available speech recognition datasets: AMI, Broadcast News, Common Voice, LibriSpeech, Switchboard/Fisher, Tedlium, and Wall Street Journal. SpeechStew simply mixes all of these datasets together, without any special re-weighting or re-balancing of the datasets. SpeechStew achieves SoTA or near SoTA results across a variety of tasks, without the use of an external language model. Our results include 9.0\\% WER on AMI-IHM, 4.7\\% WER on Switchboard, 8.3\\% WER on CallHome, and 1.3\\% on WSJ, which significantly outperforms prior work with strong external language models. We also demonstrate that SpeechStew learns powerful transfer learning representations. We fine-tune SpeechStew on a noisy low resource speech dataset, CHiME-6. We achieve 38.9\\% WER without a language model, which compares to 38.6\\% WER to a strong HMM baseline with a language model.",
      "abstract": "We present SpeechStew, a speech recognition model that is trained on a combination of various publicly available speech recognition datasets: AMI, Broadcast News, Common Voice, LibriSpeech, Switchboard/Fisher, Tedlium, and Wall Street Journal. SpeechStew simply mixes all of these datasets together, without any special re-weighting or re-balancing of the datasets. SpeechStew achieves SoTA or near SoTA results across a variety of tasks, without the use of an external language model. Our results include 9.0\\% WER on AMI-IHM, 4.7\\% WER on Switchboard, 8.3\\% WER on CallHome, and 1.3\\% on WSJ, which significantly outperforms prior work with strong external language models. We also demonstrate that SpeechStew learns powerful transfer learning representations. We fine-tune SpeechStew on a noisy low resource speech dataset, CHiME-6. We achieve 38.9\\% WER without a language model, which compares to 38.6\\% WER to a strong HMM baseline with a language model.",
      "doi": "https://doi.org/10.48550/arxiv.2104.02133",
      "openalex_id": "https://openalex.org/W3139918052",
      "arxiv_id": "",
      "publication_date": "2021-04-05",
      "published": "2021-04-05",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "Although the study trains a large ASR model on diverse speech datasets and reports WER, it does not place Spoken Language Models or equivalent speech-centric modeling at its core (no dialogue interaction, synchronization, or speech-language modeling analysis), thus it should be excluded under the exclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "Although the study trains a large ASR model on diverse speech datasets and reports WER, it does not place Spoken Language Models or equivalent speech-centric modeling at its core (no dialogue interaction, synchronization, or speech-language modeling analysis), thus it should be excluded under the exclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses primarily on training a speech recognition model by mixing various datasets to improve ASR performance, emphasizing word error rates (WER) without discussion of spoken language modeling, full-duplex interactive capabilities, or architectural and evaluation aspects related to SLMs or dialogue behavior arbitration. It does not explicitly address spoken language models, nor does it explore synchronization, cognitive parallelism, low-latency interaction, or multi-dimensional evaluation beyond WER, thus falling outside the inclusion criteria and meeting several exclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses primarily on training a speech recognition model by mixing various datasets to improve ASR performance, emphasizing word error rates (WER) without discussion of spoken language modeling, full-duplex interactive capabilities, or architectural and evaluation aspects related to SLMs or dialogue behavior arbitration. It does not explicitly address spoken language models, nor does it explore synchronization, cognitive parallelism, low-latency interaction, or multi-dimensional evaluation beyond WER, thus falling outside the inclusion criteria and meeting several exclusion criteria.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "mT5: A Massively Multilingual Pre-trained Text-to-Text Transformer",
    "abstract": "Linting Xue, Noah Constant, Adam Roberts, Mihir Kale, Rami Al-Rfou, Aditya Siddhant, Aditya Barua, Colin Raffel. Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. 2021.",
    "metadata": {
      "title": "mT5: A Massively Multilingual Pre-trained Text-to-Text Transformer",
      "summary": "Linting Xue, Noah Constant, Adam Roberts, Mihir Kale, Rami Al-Rfou, Aditya Siddhant, Aditya Barua, Colin Raffel. Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. 2021.",
      "abstract": "Linting Xue, Noah Constant, Adam Roberts, Mihir Kale, Rami Al-Rfou, Aditya Siddhant, Aditya Barua, Colin Raffel. Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. 2021.",
      "doi": "https://doi.org/10.18653/v1/2021.naacl-main.41",
      "openalex_id": "https://openalex.org/W3169483174",
      "arxiv_id": "",
      "publication_date": "2021-01-01",
      "published": "2021-01-01",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This item analyzes mT5, a multilingual text-to-text model with no focus on Spoken Language Models or speech-based interaction, and it is from 2021 (older than 3 years), so it should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "This item analyzes mT5, a multilingual text-to-text model with no focus on Spoken Language Models or speech-based interaction, and it is from 2021 (older than 3 years), so it should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The title and abstract describe mT5, a massively multilingual text-to-text transformer that focuses on text processing rather than spoken language or speech modeling. There is no mention of spoken language models, speech as a core modeling object, full-duplex interaction, or relevant evaluation metrics like synchronization, overlap management, or real-time interaction. Therefore, it does not meet the inclusion criteria for focusing on spoken language models or speech-centric modeling, nor does it avoid the exclusion criteria of being limited to text processing without speech modeling innovation.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The title and abstract describe mT5, a massively multilingual text-to-text transformer that focuses on text processing rather than spoken language or speech modeling. There is no mention of spoken language models, speech as a core modeling object, full-duplex interaction, or relevant evaluation metrics like synchronization, overlap management, or real-time interaction. Therefore, it does not meet the inclusion criteria for focusing on spoken language models or speech-centric modeling, nor does it avoid the exclusion criteria of being limited to text processing without speech modeling innovation.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "mSLAM: Massively multilingual joint pre-training for speech and text",
    "abstract": "We present mSLAM, a multilingual Speech and LAnguage Model that learns cross-lingual cross-modal representations of speech and text by pre-training jointly on large amounts of unlabeled speech and text in multiple languages. mSLAM combines w2v-BERT pre-training on speech with SpanBERT pre-training on character-level text, along with Connectionist Temporal Classification (CTC) losses on paired speech and transcript data, to learn a single model capable of learning from and representing both speech and text signals in a shared representation space. We evaluate mSLAM on several downstream speech understanding tasks and find that joint pre-training with text improves quality on speech translation, speech intent classification and speech language-ID while being competitive on multilingual ASR, when compared against speech-only pre-training. Our speech translation model demonstrates zero-shot text translation without seeing any text translation data, providing evidence for cross-modal alignment of representations. mSLAM also benefits from multi-modal fine-tuning, further improving the quality of speech translation by directly leveraging text translation data during the fine-tuning process. Our empirical analysis highlights several opportunities and challenges arising from large-scale multimodal pre-training, suggesting directions for future research.",
    "metadata": {
      "title": "mSLAM: Massively multilingual joint pre-training for speech and text",
      "summary": "We present mSLAM, a multilingual Speech and LAnguage Model that learns cross-lingual cross-modal representations of speech and text by pre-training jointly on large amounts of unlabeled speech and text in multiple languages. mSLAM combines w2v-BERT pre-training on speech with SpanBERT pre-training on character-level text, along with Connectionist Temporal Classification (CTC) losses on paired speech and transcript data, to learn a single model capable of learning from and representing both speech and text signals in a shared representation space. We evaluate mSLAM on several downstream speech understanding tasks and find that joint pre-training with text improves quality on speech translation, speech intent classification and speech language-ID while being competitive on multilingual ASR, when compared against speech-only pre-training. Our speech translation model demonstrates zero-shot text translation without seeing any text translation data, providing evidence for cross-modal alignment of representations. mSLAM also benefits from multi-modal fine-tuning, further improving the quality of speech translation by directly leveraging text translation data during the fine-tuning process. Our empirical analysis highlights several opportunities and challenges arising from large-scale multimodal pre-training, suggesting directions for future research.",
      "abstract": "We present mSLAM, a multilingual Speech and LAnguage Model that learns cross-lingual cross-modal representations of speech and text by pre-training jointly on large amounts of unlabeled speech and text in multiple languages. mSLAM combines w2v-BERT pre-training on speech with SpanBERT pre-training on character-level text, along with Connectionist Temporal Classification (CTC) losses on paired speech and transcript data, to learn a single model capable of learning from and representing both speech and text signals in a shared representation space. We evaluate mSLAM on several downstream speech understanding tasks and find that joint pre-training with text improves quality on speech translation, speech intent classification and speech language-ID while being competitive on multilingual ASR, when compared against speech-only pre-training. Our speech translation model demonstrates zero-shot text translation without seeing any text translation data, providing evidence for cross-modal alignment of representations. mSLAM also benefits from multi-modal fine-tuning, further improving the quality of speech translation by directly leveraging text translation data during the fine-tuning process. Our empirical analysis highlights several opportunities and challenges arising from large-scale multimodal pre-training, suggesting directions for future research.",
      "doi": "https://doi.org/10.48550/arxiv.2202.01374",
      "openalex_id": "https://openalex.org/W4221155340",
      "arxiv_id": "",
      "publication_date": "2022-02-03",
      "published": "2022-02-03",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The paper centers on a Spoken Language Model / Speech–Language Model with cross-modal speech and text pre-training and evaluation, not limited to ASR, and is within the last 3 years, thus meeting the inclusion criteria.",
      "evaluation": 5
    },
    "round-A_JuniorNano_reasoning": "The paper centers on a Spoken Language Model / Speech–Language Model with cross-modal speech and text pre-training and evaluation, not limited to ASR, and is within the last 3 years, thus meeting the inclusion criteria.",
    "round-A_JuniorNano_evaluation": 5,
    "round-A_JuniorMini_output": {
      "reasoning": "The study presents mSLAM, a multilingual speech and language model jointly pre-trained on speech and text, focusing on cross-modal learning and evaluation on multiple speech understanding tasks. While it involves spoken language modeling and cross-modal representation learning with joint training of speech and text, the abstract does not explicitly mention architecture or evaluation related to full-duplex spoken language models, synchronous interaction (e.g., turn-taking, cognitive parallelism, interruptions, backchanneling), or low-latency real-time dialogue behavior arbitration, which are required by the inclusion criteria. The evaluation metrics focus on speech translation, intent classification, language identification, and ASR performance rather than real-time interaction or multi-dimensional behavioral arbitration. Hence, the work partially aligns with the scope but lacks confirming details on synchronous interaction capabilities or full-duplex dialogue modeling, placing it in an ambiguous inclusion position.",
      "evaluation": 3
    },
    "round-A_JuniorMini_reasoning": "The study presents mSLAM, a multilingual speech and language model jointly pre-trained on speech and text, focusing on cross-modal learning and evaluation on multiple speech understanding tasks. While it involves spoken language modeling and cross-modal representation learning with joint training of speech and text, the abstract does not explicitly mention architecture or evaluation related to full-duplex spoken language models, synchronous interaction (e.g., turn-taking, cognitive parallelism, interruptions, backchanneling), or low-latency real-time dialogue behavior arbitration, which are required by the inclusion criteria. The evaluation metrics focus on speech translation, intent classification, language identification, and ASR performance rather than real-time interaction or multi-dimensional behavioral arbitration. Hence, the work partially aligns with the scope but lacks confirming details on synchronous interaction capabilities or full-duplex dialogue modeling, placing it in an ambiguous inclusion position.",
    "round-A_JuniorMini_evaluation": 3,
    "round-B_SeniorLead_output": {
      "reasoning": "Both junior reviewers noted mSLAM is a multilingual Speech-and-Language Model that jointly pre-trains on speech and text (JuniorNano recommended inclusion), but also flagged that the abstract lacks any mention of synchronous/full‑duplex interaction, real‑time low‑latency behavior, overlap/interrupt handling or behavior arbitration (JuniorMini judged it ambiguous). While the paper does meet the literal SLM naming requirement and studies speech-as-core representation learning, it does not address the FD‑SLM focus of synchronous, full‑duplex interaction or provide any evaluation/benchmarks for real‑time interaction, overlap management, or arbitration; therefore it violates the exclusion criteria that rule out works limited to standard speech/text pretraining and downstream ASR/translation/classification without synchronous interaction analysis. Score explanation: better to exclude because it lacks the required full‑duplex/synchronous interaction scope and evaluations.",
      "evaluation": 2
    },
    "round-B_SeniorLead_reasoning": "Both junior reviewers noted mSLAM is a multilingual Speech-and-Language Model that jointly pre-trains on speech and text (JuniorNano recommended inclusion), but also flagged that the abstract lacks any mention of synchronous/full‑duplex interaction, real‑time low‑latency behavior, overlap/interrupt handling or behavior arbitration (JuniorMini judged it ambiguous). While the paper does meet the literal SLM naming requirement and studies speech-as-core representation learning, it does not address the FD‑SLM focus of synchronous, full‑duplex interaction or provide any evaluation/benchmarks for real‑time interaction, overlap management, or arbitration; therefore it violates the exclusion criteria that rule out works limited to standard speech/text pretraining and downstream ASR/translation/classification without synchronous interaction analysis. Score explanation: better to exclude because it lacks the required full‑duplex/synchronous interaction scope and evaluations.",
    "round-B_SeniorLead_evaluation": 2,
    "final_verdict": "exclude (senior:2)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Listen, Think, and Understand",
    "abstract": "The ability of artificial intelligence (AI) systems to perceive and comprehend audio signals is crucial for many applications. Although significant progress has been made in this area since the development of AudioSet, most existing models are designed to map audio inputs to pre-defined, discrete sound label sets. In contrast, humans possess the ability to not only classify sounds into general categories, but also to listen to the finer details of the sounds, explain the reason for the predictions, think about what the sound infers, and understand the scene and what action needs to be taken, if any. Such capabilities beyond perception are not yet present in existing audio models. On the other hand, modern large language models (LLMs) exhibit emerging reasoning ability but they lack audio perception capabilities. Therefore, we ask the question: can we build a model that has both audio perception and a reasoning ability? In this paper, we propose a new audio foundation model, called LTU (Listen, Think, and Understand). To train LTU, we created a new OpenAQA-5M dataset consisting of 1.9 million closed-ended and 3.7 million open-ended, diverse (audio, question, answer) tuples, and have used an autoregressive training framework with a perception-to-understanding curriculum. LTU demonstrates strong performance and generalization ability on conventional audio tasks such as classification and captioning. More importantly, it exhibits emerging audio reasoning and comprehension abilities that are absent in existing audio models. To the best of our knowledge, LTU is one of the first multimodal large language models that focus on general audio (rather than just speech) understanding.",
    "metadata": {
      "title": "Listen, Think, and Understand",
      "summary": "The ability of artificial intelligence (AI) systems to perceive and comprehend audio signals is crucial for many applications. Although significant progress has been made in this area since the development of AudioSet, most existing models are designed to map audio inputs to pre-defined, discrete sound label sets. In contrast, humans possess the ability to not only classify sounds into general categories, but also to listen to the finer details of the sounds, explain the reason for the predictions, think about what the sound infers, and understand the scene and what action needs to be taken, if any. Such capabilities beyond perception are not yet present in existing audio models. On the other hand, modern large language models (LLMs) exhibit emerging reasoning ability but they lack audio perception capabilities. Therefore, we ask the question: can we build a model that has both audio perception and a reasoning ability? In this paper, we propose a new audio foundation model, called LTU (Listen, Think, and Understand). To train LTU, we created a new OpenAQA-5M dataset consisting of 1.9 million closed-ended and 3.7 million open-ended, diverse (audio, question, answer) tuples, and have used an autoregressive training framework with a perception-to-understanding curriculum. LTU demonstrates strong performance and generalization ability on conventional audio tasks such as classification and captioning. More importantly, it exhibits emerging audio reasoning and comprehension abilities that are absent in existing audio models. To the best of our knowledge, LTU is one of the first multimodal large language models that focus on general audio (rather than just speech) understanding.",
      "abstract": "The ability of artificial intelligence (AI) systems to perceive and comprehend audio signals is crucial for many applications. Although significant progress has been made in this area since the development of AudioSet, most existing models are designed to map audio inputs to pre-defined, discrete sound label sets. In contrast, humans possess the ability to not only classify sounds into general categories, but also to listen to the finer details of the sounds, explain the reason for the predictions, think about what the sound infers, and understand the scene and what action needs to be taken, if any. Such capabilities beyond perception are not yet present in existing audio models. On the other hand, modern large language models (LLMs) exhibit emerging reasoning ability but they lack audio perception capabilities. Therefore, we ask the question: can we build a model that has both audio perception and a reasoning ability? In this paper, we propose a new audio foundation model, called LTU (Listen, Think, and Understand). To train LTU, we created a new OpenAQA-5M dataset consisting of 1.9 million closed-ended and 3.7 million open-ended, diverse (audio, question, answer) tuples, and have used an autoregressive training framework with a perception-to-understanding curriculum. LTU demonstrates strong performance and generalization ability on conventional audio tasks such as classification and captioning. More importantly, it exhibits emerging audio reasoning and comprehension abilities that are absent in existing audio models. To the best of our knowledge, LTU is one of the first multimodal large language models that focus on general audio (rather than just speech) understanding.",
      "doi": "https://doi.org/10.48550/arxiv.2305.10790",
      "openalex_id": "https://openalex.org/W4377130946",
      "arxiv_id": "",
      "publication_date": "2023-05-18",
      "published": "2023-05-18",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The LTU paper describes a general audio foundation model with perception and reasoning, focusing on broad audio understanding rather than core Spoken Language Models or Speech Language Models; it does not present audio-centric speech-language modelling (SLMs/FD-SLMs) with explicit turn-taking, synchronization, or speech-based reasoning, so it does not meet the inclusion criteria despite being within the 3-year window.",
      "evaluation": 2
    },
    "round-A_JuniorNano_reasoning": "The LTU paper describes a general audio foundation model with perception and reasoning, focusing on broad audio understanding rather than core Spoken Language Models or Speech Language Models; it does not present audio-centric speech-language modelling (SLMs/FD-SLMs) with explicit turn-taking, synchronization, or speech-based reasoning, so it does not meet the inclusion criteria despite being within the 3-year window.",
    "round-A_JuniorNano_evaluation": 2,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper presents an audio foundation model focusing on general audio understanding and reasoning, but it does not explicitly focus on spoken language modeling or full-duplex spoken language interaction based on the inclusion criteria, nor does it discuss speech sequence modeling or cognitive parallelism and real-time interaction aspects; thus, it does not fully meet the core requirements related to spoken language models or synchronous interactive behaviors.",
      "evaluation": 2
    },
    "round-A_JuniorMini_reasoning": "The paper presents an audio foundation model focusing on general audio understanding and reasoning, but it does not explicitly focus on spoken language modeling or full-duplex spoken language interaction based on the inclusion criteria, nor does it discuss speech sequence modeling or cognitive parallelism and real-time interaction aspects; thus, it does not fully meet the core requirements related to spoken language models or synchronous interactive behaviors.",
    "round-A_JuniorMini_evaluation": 2,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:2)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "AudioPaLM: A Large Language Model That Can Speak and Listen",
    "abstract": "We introduce AudioPaLM, a large language model for speech understanding and generation. AudioPaLM fuses text-based and speech-based language models, PaLM-2 [Anil et al., 2023] and AudioLM [Borsos et al., 2022], into a unified multimodal architecture that can process and generate text and speech with applications including speech recognition and speech-to-speech translation. AudioPaLM inherits the capability to preserve paralinguistic information such as speaker identity and intonation from AudioLM and the linguistic knowledge present only in text large language models such as PaLM-2. We demonstrate that initializing AudioPaLM with the weights of a text-only large language model improves speech processing, successfully leveraging the larger quantity of text training data used in pretraining to assist with the speech tasks. The resulting model significantly outperforms existing systems for speech translation tasks and has the ability to perform zero-shot speech-to-text translation for many languages for which input/target language combinations were not seen in training. AudioPaLM also demonstrates features of audio language models, such as transferring a voice across languages based on a short spoken prompt. We release examples of our method at https://google-research.github.io/seanet/audiopalm/examples",
    "metadata": {
      "title": "AudioPaLM: A Large Language Model That Can Speak and Listen",
      "summary": "We introduce AudioPaLM, a large language model for speech understanding and generation. AudioPaLM fuses text-based and speech-based language models, PaLM-2 [Anil et al., 2023] and AudioLM [Borsos et al., 2022], into a unified multimodal architecture that can process and generate text and speech with applications including speech recognition and speech-to-speech translation. AudioPaLM inherits the capability to preserve paralinguistic information such as speaker identity and intonation from AudioLM and the linguistic knowledge present only in text large language models such as PaLM-2. We demonstrate that initializing AudioPaLM with the weights of a text-only large language model improves speech processing, successfully leveraging the larger quantity of text training data used in pretraining to assist with the speech tasks. The resulting model significantly outperforms existing systems for speech translation tasks and has the ability to perform zero-shot speech-to-text translation for many languages for which input/target language combinations were not seen in training. AudioPaLM also demonstrates features of audio language models, such as transferring a voice across languages based on a short spoken prompt. We release examples of our method at https://google-research.github.io/seanet/audiopalm/examples",
      "abstract": "We introduce AudioPaLM, a large language model for speech understanding and generation. AudioPaLM fuses text-based and speech-based language models, PaLM-2 [Anil et al., 2023] and AudioLM [Borsos et al., 2022], into a unified multimodal architecture that can process and generate text and speech with applications including speech recognition and speech-to-speech translation. AudioPaLM inherits the capability to preserve paralinguistic information such as speaker identity and intonation from AudioLM and the linguistic knowledge present only in text large language models such as PaLM-2. We demonstrate that initializing AudioPaLM with the weights of a text-only large language model improves speech processing, successfully leveraging the larger quantity of text training data used in pretraining to assist with the speech tasks. The resulting model significantly outperforms existing systems for speech translation tasks and has the ability to perform zero-shot speech-to-text translation for many languages for which input/target language combinations were not seen in training. AudioPaLM also demonstrates features of audio language models, such as transferring a voice across languages based on a short spoken prompt. We release examples of our method at https://google-research.github.io/seanet/audiopalm/examples",
      "doi": "https://doi.org/10.48550/arxiv.2306.12925",
      "openalex_id": "https://openalex.org/W4381827575",
      "arxiv_id": "",
      "publication_date": "2023-06-22",
      "published": "2023-06-22",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item describes AudioPaLM, a multimodal LLM that can speak and listen and handles speech understanding/generation, which aligns with SLM/FD-SLM topics. However, the inclusion criteria require the title/abstract/keywords to explicitly include terms like 'spoken language model(s)' or 'speech language model(s)' or an equivalent description such as 'language modeling of speech'; the abstract uses 'speech understanding and generation' and 'multimodal' but does not explicitly state those terms, so it fails that explicit keyword criterion and should be excluded under the given rules (despite relevance).",
      "evaluation": 2
    },
    "round-A_JuniorNano_reasoning": "The item describes AudioPaLM, a multimodal LLM that can speak and listen and handles speech understanding/generation, which aligns with SLM/FD-SLM topics. However, the inclusion criteria require the title/abstract/keywords to explicitly include terms like 'spoken language model(s)' or 'speech language model(s)' or an equivalent description such as 'language modeling of speech'; the abstract uses 'speech understanding and generation' and 'multimodal' but does not explicitly state those terms, so it fails that explicit keyword criterion and should be excluded under the given rules (despite relevance).",
    "round-A_JuniorNano_evaluation": 2,
    "round-A_JuniorMini_output": {
      "reasoning": "The study presents AudioPaLM, a multimodal large language model integrating speech and text for speech understanding and generation with significant enhancements in speech translation and zero-shot capabilities, aligning with the inclusion criteria focusing on spoken language models and multimodal speech-text architectures within the last three years. However, the abstract does not explicitly mention full-duplex synchronous interaction capabilities, cognitive parallelism, low-latency interaction, or behavioral arbitration assessments, which are key considerations in the inclusion criteria. Although the model handles speech as a primary modality, enabling speech-to-speech translation and preserving paralinguistic features, it primarily emphasizes speech processing and translation tasks rather than explicit evaluations or features of synchronous, full-duplex conversational interaction or multitasking arbitration. Therefore, it fits partially but lacks evidence of full compliance with all inclusion criteria especially regarding full-duplex interaction and behavioral arbitration features, which prevents a top inclusion score but indicates it is still a relevant and advanced spoken language model study worthy of consideration.",
      "evaluation": 4
    },
    "round-A_JuniorMini_reasoning": "The study presents AudioPaLM, a multimodal large language model integrating speech and text for speech understanding and generation with significant enhancements in speech translation and zero-shot capabilities, aligning with the inclusion criteria focusing on spoken language models and multimodal speech-text architectures within the last three years. However, the abstract does not explicitly mention full-duplex synchronous interaction capabilities, cognitive parallelism, low-latency interaction, or behavioral arbitration assessments, which are key considerations in the inclusion criteria. Although the model handles speech as a primary modality, enabling speech-to-speech translation and preserving paralinguistic features, it primarily emphasizes speech processing and translation tasks rather than explicit evaluations or features of synchronous, full-duplex conversational interaction or multitasking arbitration. Therefore, it fits partially but lacks evidence of full compliance with all inclusion criteria especially regarding full-duplex interaction and behavioral arbitration features, which prevents a top inclusion score but indicates it is still a relevant and advanced spoken language model study worthy of consideration.",
    "round-A_JuniorMini_evaluation": 4,
    "round-B_SeniorLead_output": {
      "reasoning": "Integrated assessment: Junior reviewer A excluded due to a strict literal keyword interpretation, while Junior reviewer B judged it relevant but noted the paper does not emphasize full‑duplex interaction specifics; reconciling both, the abstract/title explicitly describe AudioPaLM as “a large language model for speech” and details a speech-centric multimodal architecture, initialization/training choices, and evaluations across speech tasks (ASR, speech-to-speech translation, zero‑shot transfer, paralinguistic preservation), so it treats speech as the primary modeling object and goes beyond single-task ASR/TTS—thus meeting the inclusion criteria and not violating the exclusions. One-sentence justification and final score: Include (5) — the item explicitly frames and evaluates a spoken-language model (speech-centered LLM/multimodal speech–LLM) with architectural and evaluation contributions, satisfying the required criteria.",
      "evaluation": 5
    },
    "round-B_SeniorLead_reasoning": "Integrated assessment: Junior reviewer A excluded due to a strict literal keyword interpretation, while Junior reviewer B judged it relevant but noted the paper does not emphasize full‑duplex interaction specifics; reconciling both, the abstract/title explicitly describe AudioPaLM as “a large language model for speech” and details a speech-centric multimodal architecture, initialization/training choices, and evaluations across speech tasks (ASR, speech-to-speech translation, zero‑shot transfer, paralinguistic preservation), so it treats speech as the primary modeling object and goes beyond single-task ASR/TTS—thus meeting the inclusion criteria and not violating the exclusions. One-sentence justification and final score: Include (5) — the item explicitly frames and evaluates a spoken-language model (speech-centered LLM/multimodal speech–LLM) with architectural and evaluation contributions, satisfying the required criteria.",
    "round-B_SeniorLead_evaluation": 5,
    "final_verdict": "include (senior:5)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Prefix-Tuning: Optimizing Continuous Prompts for Generation",
    "abstract": "Xiang Lisa Li, Percy Liang. Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers). 2021.",
    "metadata": {
      "title": "Prefix-Tuning: Optimizing Continuous Prompts for Generation",
      "summary": "Xiang Lisa Li, Percy Liang. Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers). 2021.",
      "abstract": "Xiang Lisa Li, Percy Liang. Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers). 2021.",
      "doi": "https://doi.org/10.18653/v1/2021.acl-long.353",
      "openalex_id": "https://openalex.org/W3174770825",
      "arxiv_id": "",
      "publication_date": "2021-01-01",
      "published": "2021-01-01",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "Reasoning: This item is a 2021 text-only prompt-tuning paper focused on generation with non-spoken language models, not addressing Spoken Language Models or speech-based interactions, and it is outside the 3-year inclusion window (2022–2025).",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "Reasoning: This item is a 2021 text-only prompt-tuning paper focused on generation with non-spoken language models, not addressing Spoken Language Models or speech-based interactions, and it is outside the 3-year inclusion window (2022–2025).",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper titled 'Prefix-Tuning: Optimizing Continuous Prompts for Generation' appears to focus on prompt optimization for generative models and lacks clear mention of spoken language models, speech as the core modeling object, or full-duplex spoken interaction aspects required by the inclusion criteria. Additionally, it does not mention speech-specific modeling or evaluation metrics related to real-time interaction or speech overlap, suggesting it does not meet the core requirements and seems more aligned with text-based language modeling techniques. The publication year (2021) is within 3 years, but the topical focus does not match the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper titled 'Prefix-Tuning: Optimizing Continuous Prompts for Generation' appears to focus on prompt optimization for generative models and lacks clear mention of spoken language models, speech as the core modeling object, or full-duplex spoken interaction aspects required by the inclusion criteria. Additionally, it does not mention speech-specific modeling or evaluation metrics related to real-time interaction or speech overlap, suggesting it does not meet the core requirements and seems more aligned with text-based language modeling techniques. The publication year (2021) is within 3 years, but the topical focus does not match the inclusion criteria.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "P-Tuning: Prompt Tuning Can Be Comparable to Fine-tuning Across Scales and Tasks",
    "abstract": "Prompt tuning, which only tunes continuous prompts with a frozen language model, substantially reduces per-task storage and memory usage at training. However, in the context of NLU, prior work reveals that prompt tuning does not perform well for normal-sized pretrained models. We also find that existing methods of prompt tuning cannot handle hard sequence labeling tasks, indicating a lack of universality. We present a novel empirical finding that properly optimized prompt tuning can be universally effective across a wide range of model scales and NLU tasks. It matches the performance of finetuning while having only 0.1%-3% tuned parameters. Our method P-Tuning v2 is an implementation of Deep Prompt Tuning (CITATION) optimized and adapted for NLU. Given the universality and simplicity of P-Tuning v2, we believe it can serve as an alternative to finetuning and a strong baseline for future research.",
    "metadata": {
      "title": "P-Tuning: Prompt Tuning Can Be Comparable to Fine-tuning Across Scales and Tasks",
      "summary": "Prompt tuning, which only tunes continuous prompts with a frozen language model, substantially reduces per-task storage and memory usage at training. However, in the context of NLU, prior work reveals that prompt tuning does not perform well for normal-sized pretrained models. We also find that existing methods of prompt tuning cannot handle hard sequence labeling tasks, indicating a lack of universality. We present a novel empirical finding that properly optimized prompt tuning can be universally effective across a wide range of model scales and NLU tasks. It matches the performance of finetuning while having only 0.1%-3% tuned parameters. Our method P-Tuning v2 is an implementation of Deep Prompt Tuning (CITATION) optimized and adapted for NLU. Given the universality and simplicity of P-Tuning v2, we believe it can serve as an alternative to finetuning and a strong baseline for future research.",
      "abstract": "Prompt tuning, which only tunes continuous prompts with a frozen language model, substantially reduces per-task storage and memory usage at training. However, in the context of NLU, prior work reveals that prompt tuning does not perform well for normal-sized pretrained models. We also find that existing methods of prompt tuning cannot handle hard sequence labeling tasks, indicating a lack of universality. We present a novel empirical finding that properly optimized prompt tuning can be universally effective across a wide range of model scales and NLU tasks. It matches the performance of finetuning while having only 0.1%-3% tuned parameters. Our method P-Tuning v2 is an implementation of Deep Prompt Tuning (CITATION) optimized and adapted for NLU. Given the universality and simplicity of P-Tuning v2, we believe it can serve as an alternative to finetuning and a strong baseline for future research.",
      "doi": "https://doi.org/10.18653/v1/2022.acl-short.8",
      "openalex_id": "https://openalex.org/W4285247752",
      "arxiv_id": "",
      "publication_date": "2022-01-01",
      "published": "2022-01-01",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item centers on NLP prompt tuning for text tasks and does not address Spoken Language Models or speech-based modeling, thus it does not meet the core inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item centers on NLP prompt tuning for text tasks and does not address Spoken Language Models or speech-based modeling, thus it does not meet the core inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on prompt tuning methods for natural language understanding tasks and does not address spoken language models or speech as the primary modeling target; it also lacks any mention of synchronous full-duplex spoken interaction or related core concepts specified in the inclusion criteria, and thus does not meet the thematic requirements.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on prompt tuning methods for natural language understanding tasks and does not address spoken language models or speech as the primary modeling target; it also lacks any mention of synchronous full-duplex spoken interaction or related core concepts specified in the inclusion criteria, and thus does not meet the thematic requirements.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Exploring Efficient-Tuning Methods in Self-Supervised Speech Models",
    "abstract": "In this study, we aim to explore efficient tuning methods for speech self-supervised learning. Recent studies show that self-supervised learning (SSL) can learn powerful representations for different speech tasks. However, fine-tuning pre-trained models for each downstream task is parameter-inefficient since SSL models are notoriously large with millions of parameters. Adapters are lightweight modules commonly used in NLP to solve this problem. In downstream tasks, the parameters of SSL models are frozen, and only the adapters are trained. Given the lack of studies generally exploring the effectiveness of adapters for self-supervised speech tasks, we intend to fill this gap by adding various adapter modules in pre-trained speech SSL models. We show that the performance parity can be achieved with over 90% parameter reduction, and discussed the pros and cons of efficient tuning techniques. This is the first comprehensive investigation of various adapter types across speech tasks.",
    "metadata": {
      "title": "Exploring Efficient-Tuning Methods in Self-Supervised Speech Models",
      "summary": "In this study, we aim to explore efficient tuning methods for speech self-supervised learning. Recent studies show that self-supervised learning (SSL) can learn powerful representations for different speech tasks. However, fine-tuning pre-trained models for each downstream task is parameter-inefficient since SSL models are notoriously large with millions of parameters. Adapters are lightweight modules commonly used in NLP to solve this problem. In downstream tasks, the parameters of SSL models are frozen, and only the adapters are trained. Given the lack of studies generally exploring the effectiveness of adapters for self-supervised speech tasks, we intend to fill this gap by adding various adapter modules in pre-trained speech SSL models. We show that the performance parity can be achieved with over 90% parameter reduction, and discussed the pros and cons of efficient tuning techniques. This is the first comprehensive investigation of various adapter types across speech tasks.",
      "abstract": "In this study, we aim to explore efficient tuning methods for speech self-supervised learning. Recent studies show that self-supervised learning (SSL) can learn powerful representations for different speech tasks. However, fine-tuning pre-trained models for each downstream task is parameter-inefficient since SSL models are notoriously large with millions of parameters. Adapters are lightweight modules commonly used in NLP to solve this problem. In downstream tasks, the parameters of SSL models are frozen, and only the adapters are trained. Given the lack of studies generally exploring the effectiveness of adapters for self-supervised speech tasks, we intend to fill this gap by adding various adapter modules in pre-trained speech SSL models. We show that the performance parity can be achieved with over 90% parameter reduction, and discussed the pros and cons of efficient tuning techniques. This is the first comprehensive investigation of various adapter types across speech tasks.",
      "doi": "https://doi.org/10.1109/slt54892.2023.10023274",
      "openalex_id": "https://openalex.org/W4319862642",
      "arxiv_id": "",
      "publication_date": "2023-01-09",
      "published": "2023-01-09",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The paper centers on efficient tuning of self-supervised speech models using adapters, addressing speech-model architecture and training/evaluation across tasks, which aligns with Spoken Language Models/FD-SLM concepts and uses a timeframe within the last 3 years; it is not limited to a single speech task like ASR or TTS.",
      "evaluation": 5
    },
    "round-A_JuniorNano_reasoning": "The paper centers on efficient tuning of self-supervised speech models using adapters, addressing speech-model architecture and training/evaluation across tasks, which aligns with Spoken Language Models/FD-SLM concepts and uses a timeframe within the last 3 years; it is not limited to a single speech task like ASR or TTS.",
    "round-A_JuniorNano_evaluation": 5,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on efficient tuning methods for self-supervised speech models, discussing adapters and parameter efficiency. However, it does not explicitly address Spoken Language Models or Full-Duplex Spoken Language Models, nor does it cover synchronous interaction capabilities such as cognitive parallelism, turn-taking, or low-latency interactive speech. The research centers primarily on SSL model adaptation rather than language modeling of speech with joint speech-text architectures or dialogue behavior arbitration. Thus, it does not fully match the inclusion criteria nor explicitly violate exclusion criteria but lacks critical elements like synchronization, real-time interaction, and spoken language modeling focus.",
      "evaluation": 2
    },
    "round-A_JuniorMini_reasoning": "The study focuses on efficient tuning methods for self-supervised speech models, discussing adapters and parameter efficiency. However, it does not explicitly address Spoken Language Models or Full-Duplex Spoken Language Models, nor does it cover synchronous interaction capabilities such as cognitive parallelism, turn-taking, or low-latency interactive speech. The research centers primarily on SSL model adaptation rather than language modeling of speech with joint speech-text architectures or dialogue behavior arbitration. Thus, it does not fully match the inclusion criteria nor explicitly violate exclusion criteria but lacks critical elements like synchronization, real-time interaction, and spoken language modeling focus.",
    "round-A_JuniorMini_evaluation": 2,
    "round-B_SeniorLead_output": {
      "reasoning": "Junior reviewers' inputs: JuniorNano rated 5 noting the paper studies efficient tuning of self-supervised speech models; JuniorMini rated 2 noting it does not address Spoken Language Models, full-duplex interaction, or synchronization. My assessment: the paper investigates adapters and parameter-efficient fine-tuning for pre-trained speech SSL models across downstream tasks, but it does not claim or evaluate \"spoken language models\" or \"language modeling of speech\", nor does it address full‑duplex/synchronization, turn‑taking, overlap handling, low‑latency interaction, or related evaluation benchmarks required by the inclusion criteria. Therefore it fails the requirement that the work explicitly center on SLMs/FD‑SLMs or language modeling of speech and meets exclusion conditions (focus on model adaptation/ASR-style downstream tasks rather than spoken language modeling or synchronous interaction).",
      "evaluation": 1
    },
    "round-B_SeniorLead_reasoning": "Junior reviewers' inputs: JuniorNano rated 5 noting the paper studies efficient tuning of self-supervised speech models; JuniorMini rated 2 noting it does not address Spoken Language Models, full-duplex interaction, or synchronization. My assessment: the paper investigates adapters and parameter-efficient fine-tuning for pre-trained speech SSL models across downstream tasks, but it does not claim or evaluate \"spoken language models\" or \"language modeling of speech\", nor does it address full‑duplex/synchronization, turn‑taking, overlap handling, low‑latency interaction, or related evaluation benchmarks required by the inclusion criteria. Therefore it fails the requirement that the work explicitly center on SLMs/FD‑SLMs or language modeling of speech and meets exclusion conditions (focus on model adaptation/ASR-style downstream tasks rather than spoken language modeling or synchronous interaction).",
    "round-B_SeniorLead_evaluation": 1,
    "final_verdict": "exclude (senior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "AdapterBias: Parameter-efficient Token-dependent Representation Shift for Adapters in NLP Tasks",
    "abstract": "Transformer-based pre-trained models with millions of parameters require large storage. Recent approaches tackle this shortcoming by training adapters, but these approaches still require a relatively large number of parameters. In this study, AdapterBias, a surprisingly simple yet effective adapter architecture, is proposed. AdapterBias adds a token-dependent shift to the hidden output of transformer layers to adapt to downstream tasks with only a vector and a linear layer. Extensive experiments are conducted to demonstrate the effectiveness of AdapterBias. The experiments show that our proposed method can dramatically reduce the trainable parameters compared to the previous works with a minimal decrease in task performances compared with fine-tuned pre-trained models. We further find that AdapterBias automatically learns to assign more significant representation shifts to the tokens related to the task in consideration.",
    "metadata": {
      "title": "AdapterBias: Parameter-efficient Token-dependent Representation Shift for Adapters in NLP Tasks",
      "summary": "Transformer-based pre-trained models with millions of parameters require large storage. Recent approaches tackle this shortcoming by training adapters, but these approaches still require a relatively large number of parameters. In this study, AdapterBias, a surprisingly simple yet effective adapter architecture, is proposed. AdapterBias adds a token-dependent shift to the hidden output of transformer layers to adapt to downstream tasks with only a vector and a linear layer. Extensive experiments are conducted to demonstrate the effectiveness of AdapterBias. The experiments show that our proposed method can dramatically reduce the trainable parameters compared to the previous works with a minimal decrease in task performances compared with fine-tuned pre-trained models. We further find that AdapterBias automatically learns to assign more significant representation shifts to the tokens related to the task in consideration.",
      "abstract": "Transformer-based pre-trained models with millions of parameters require large storage. Recent approaches tackle this shortcoming by training adapters, but these approaches still require a relatively large number of parameters. In this study, AdapterBias, a surprisingly simple yet effective adapter architecture, is proposed. AdapterBias adds a token-dependent shift to the hidden output of transformer layers to adapt to downstream tasks with only a vector and a linear layer. Extensive experiments are conducted to demonstrate the effectiveness of AdapterBias. The experiments show that our proposed method can dramatically reduce the trainable parameters compared to the previous works with a minimal decrease in task performances compared with fine-tuned pre-trained models. We further find that AdapterBias automatically learns to assign more significant representation shifts to the tokens related to the task in consideration.",
      "doi": "https://doi.org/10.18653/v1/2022.findings-naacl.199",
      "openalex_id": "https://openalex.org/W4225410153",
      "arxiv_id": "",
      "publication_date": "2022-01-01",
      "published": "2022-01-01",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This paper discusses adapters for NLP tasks and does not center Spoken Language Models or speech-centric modeling, thus it fails the inclusion criteria and meets exclusion.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "This paper discusses adapters for NLP tasks and does not center Spoken Language Models or speech-centric modeling, thus it fails the inclusion criteria and meets exclusion.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on adapter optimization in transformer-based NLP models, with no mention of spoken language models, speech modeling, or full-duplex interaction related to speech; it treats text token representations rather than acoustic or speech token modeling, and thus does not meet the inclusion criteria that require speech as core modeling object or mention of spoken language models.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on adapter optimization in transformer-based NLP models, with no mention of spoken language models, speech modeling, or full-duplex interaction related to speech; it treats text token representations rather than acoustic or speech token modeling, and thus does not meet the inclusion criteria that require speech as core modeling object or mention of spoken language models.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "On the Effectiveness of Adapter-based Tuning for Pretrained Language Model Adaptation",
    "abstract": "Ruidan He, Linlin Liu, Hai Ye, Qingyu Tan, Bosheng Ding, Liying Cheng, Jiawei Low, Lidong Bing, Luo Si. Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers). 2021.",
    "metadata": {
      "title": "On the Effectiveness of Adapter-based Tuning for Pretrained Language Model Adaptation",
      "summary": "Ruidan He, Linlin Liu, Hai Ye, Qingyu Tan, Bosheng Ding, Liying Cheng, Jiawei Low, Lidong Bing, Luo Si. Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers). 2021.",
      "abstract": "Ruidan He, Linlin Liu, Hai Ye, Qingyu Tan, Bosheng Ding, Liying Cheng, Jiawei Low, Lidong Bing, Luo Si. Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers). 2021.",
      "doi": "https://doi.org/10.18653/v1/2021.acl-long.172",
      "openalex_id": "https://openalex.org/W3176693010",
      "arxiv_id": "",
      "publication_date": "2021-01-01",
      "published": "2021-01-01",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "此項目專注於預訓練語言模型的適配器（adapter）微調，與語音/口語語言模型（SLMs、FD-SLMs）之核心議題（如語音為主的建模與即時互動、語音重疊處理、跨語音對話等）無關，亦未出現在標題或摘要中之語音模型焦點，因此不符合包含條件且符合排除條件，應排除。",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "此項目專注於預訓練語言模型的適配器（adapter）微調，與語音/口語語言模型（SLMs、FD-SLMs）之核心議題（如語音為主的建模與即時互動、語音重疊處理、跨語音對話等）無關，亦未出現在標題或摘要中之語音模型焦點，因此不符合包含條件且符合排除條件，應排除。",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The title and abstract focus on adapter-based tuning for pretrained language model adaptation, without any explicit mention of Spoken Language Models, speech language modeling, or aspects related to full-duplex spoken interaction or speech as a core modeling object. Also, the study is from 2021, outside the 3-year inclusion window, and appears to mainly concern text-based pretrained LM adaptation rather than speech or synchronous spoken interaction modeling. Therefore, it does not meet the inclusion criteria and falls under exclusion due to lack of speech modeling focus and timeline.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The title and abstract focus on adapter-based tuning for pretrained language model adaptation, without any explicit mention of Spoken Language Models, speech language modeling, or aspects related to full-duplex spoken interaction or speech as a core modeling object. Also, the study is from 2021, outside the 3-year inclusion window, and appears to mainly concern text-based pretrained LM adaptation rather than speech or synchronous spoken interaction modeling. Therefore, it does not meet the inclusion criteria and falls under exclusion due to lack of speech modeling focus and timeline.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "On the Utility of Self-Supervised Models for Prosody-Related Tasks",
    "abstract": "Self-Supervised Learning (SSL) from speech data has produced models that have achieved remarkable performance in many tasks, and that are known to implicitly represent many aspects of information latently present in speech signals. However, relatively little is known about the suitability of such models for prosody-related tasks or the extent to which they encode prosodic information. We present a new evaluation framework, \"SUPERB-prosody,\" consisting of three prosody-related downstream tasks and two pseudo tasks. We find that 13 of the 15 SSL models outperformed the baseline on all the prosody-related tasks. We also show good performance on two pseudo tasks: prosody reconstruction and future prosody prediction. We further analyze the layerwise contributions of the SSL models. Overall we conclude that SSL speech models are highly effective for prosody-related tasks. We release our code <sup xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">1</sup> <sup xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">1</sup> https://github.com/JSALT-2022-SSL/superb-prosody for the community to support further investigation of SSL models' utility for prosody.",
    "metadata": {
      "title": "On the Utility of Self-Supervised Models for Prosody-Related Tasks",
      "summary": "Self-Supervised Learning (SSL) from speech data has produced models that have achieved remarkable performance in many tasks, and that are known to implicitly represent many aspects of information latently present in speech signals. However, relatively little is known about the suitability of such models for prosody-related tasks or the extent to which they encode prosodic information. We present a new evaluation framework, \"SUPERB-prosody,\" consisting of three prosody-related downstream tasks and two pseudo tasks. We find that 13 of the 15 SSL models outperformed the baseline on all the prosody-related tasks. We also show good performance on two pseudo tasks: prosody reconstruction and future prosody prediction. We further analyze the layerwise contributions of the SSL models. Overall we conclude that SSL speech models are highly effective for prosody-related tasks. We release our code <sup xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">1</sup> <sup xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">1</sup> https://github.com/JSALT-2022-SSL/superb-prosody for the community to support further investigation of SSL models' utility for prosody.",
      "abstract": "Self-Supervised Learning (SSL) from speech data has produced models that have achieved remarkable performance in many tasks, and that are known to implicitly represent many aspects of information latently present in speech signals. However, relatively little is known about the suitability of such models for prosody-related tasks or the extent to which they encode prosodic information. We present a new evaluation framework, \"SUPERB-prosody,\" consisting of three prosody-related downstream tasks and two pseudo tasks. We find that 13 of the 15 SSL models outperformed the baseline on all the prosody-related tasks. We also show good performance on two pseudo tasks: prosody reconstruction and future prosody prediction. We further analyze the layerwise contributions of the SSL models. Overall we conclude that SSL speech models are highly effective for prosody-related tasks. We release our code <sup xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">1</sup> <sup xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">1</sup> https://github.com/JSALT-2022-SSL/superb-prosody for the community to support further investigation of SSL models' utility for prosody.",
      "doi": "https://doi.org/10.1109/slt54892.2023.10023234",
      "openalex_id": "https://openalex.org/W4319862652",
      "arxiv_id": "",
      "publication_date": "2023-01-09",
      "published": "2023-01-09",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The paper centers on SSL speech models applied to prosody tasks and discusses evaluation frameworks and tasks, uses terminology like 'SSL speech models', appears to be within the last 3 years, and is not restricted to ASR/TTS or non-academic sources, aligning with inclusion criteria and not with exclusions.",
      "evaluation": 5
    },
    "round-A_JuniorNano_reasoning": "The paper centers on SSL speech models applied to prosody tasks and discusses evaluation frameworks and tasks, uses terminology like 'SSL speech models', appears to be within the last 3 years, and is not restricted to ASR/TTS or non-academic sources, aligning with inclusion criteria and not with exclusions.",
    "round-A_JuniorNano_evaluation": 5,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on self-supervised speech models evaluated on prosody-related tasks, which involves speech representations but does not explicitly address spoken language modeling, full-duplex interaction, or synchronization aspects required by the inclusion criteria; it lacks explicit discussion on model architectures for spoken language modeling or interaction behaviors such as real-time dialogue arbitration, turn-taking, or cognitive parallelism.",
      "evaluation": 2
    },
    "round-A_JuniorMini_reasoning": "The study focuses on self-supervised speech models evaluated on prosody-related tasks, which involves speech representations but does not explicitly address spoken language modeling, full-duplex interaction, or synchronization aspects required by the inclusion criteria; it lacks explicit discussion on model architectures for spoken language modeling or interaction behaviors such as real-time dialogue arbitration, turn-taking, or cognitive parallelism.",
    "round-A_JuniorMini_evaluation": 2,
    "round-B_SeniorLead_output": {
      "reasoning": "The abstract examines SSL speech models for prosody evaluation (SUPERB-prosody) but does not frame the work as a Spoken Language Model or speech-language modeling research focused on end-to-end or full‑duplex interaction, nor does it mention spoken language models or language modeling of speech in the title/abstract; it instead evaluates downstream prosody tasks and representation quality, which falls outside the inclusion criteria and matches exclusion points (not addressing FD‑SLM, synchronization, turn-taking, or speech-level language modeling). Junior reviewers disagreed (one mistakenly judged it in-scope, the other correctly flagged it out-of-scope); I agree with the latter. 1-sentence explanation: It should be excluded because it does not treat spoken language models or full‑duplex spoken-language modeling as the core research focus and thus fails the required inclusion criteria.",
      "evaluation": 1
    },
    "round-B_SeniorLead_reasoning": "The abstract examines SSL speech models for prosody evaluation (SUPERB-prosody) but does not frame the work as a Spoken Language Model or speech-language modeling research focused on end-to-end or full‑duplex interaction, nor does it mention spoken language models or language modeling of speech in the title/abstract; it instead evaluates downstream prosody tasks and representation quality, which falls outside the inclusion criteria and matches exclusion points (not addressing FD‑SLM, synchronization, turn-taking, or speech-level language modeling). Junior reviewers disagreed (one mistakenly judged it in-scope, the other correctly flagged it out-of-scope); I agree with the latter. 1-sentence explanation: It should be excluded because it does not treat spoken language models or full‑duplex spoken-language modeling as the core research focus and thus fails the required inclusion criteria.",
    "round-B_SeniorLead_evaluation": 1,
    "final_verdict": "exclude (senior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Whisper-AT: Noise-Robust Automatic Speech Recognizers are Also Strong General Audio Event Taggers",
    "abstract": "In this paper, we focus on Whisper [1], a recent automatic speech recognition model trained with a massive 680k hour labeled speech corpus recorded in diverse conditions.We first show an interesting finding that while Whisper is very robust against real-world background sounds (e.g., music), its audio representation is actually not noise-invariant, but is instead highly correlated to non-speech sounds, indicating that Whisper recognizes speech conditioned on the noise type.With this finding, we build a unified audio tagging and speech recognition model Whisper-AT by freezing the backbone of Whisper, and training a lightweight audio tagging model on top of it.With <1% extra computational cost, Whisper-AT can recognize audio events, in addition to spoken text, in a single forward pass.",
    "metadata": {
      "title": "Whisper-AT: Noise-Robust Automatic Speech Recognizers are Also Strong General Audio Event Taggers",
      "summary": "In this paper, we focus on Whisper [1], a recent automatic speech recognition model trained with a massive 680k hour labeled speech corpus recorded in diverse conditions.We first show an interesting finding that while Whisper is very robust against real-world background sounds (e.g., music), its audio representation is actually not noise-invariant, but is instead highly correlated to non-speech sounds, indicating that Whisper recognizes speech conditioned on the noise type.With this finding, we build a unified audio tagging and speech recognition model Whisper-AT by freezing the backbone of Whisper, and training a lightweight audio tagging model on top of it.With <1% extra computational cost, Whisper-AT can recognize audio events, in addition to spoken text, in a single forward pass.",
      "abstract": "In this paper, we focus on Whisper [1], a recent automatic speech recognition model trained with a massive 680k hour labeled speech corpus recorded in diverse conditions.We first show an interesting finding that while Whisper is very robust against real-world background sounds (e.g., music), its audio representation is actually not noise-invariant, but is instead highly correlated to non-speech sounds, indicating that Whisper recognizes speech conditioned on the noise type.With this finding, we build a unified audio tagging and speech recognition model Whisper-AT by freezing the backbone of Whisper, and training a lightweight audio tagging model on top of it.With <1% extra computational cost, Whisper-AT can recognize audio events, in addition to spoken text, in a single forward pass.",
      "doi": "https://doi.org/10.21437/interspeech.2023-2193",
      "openalex_id": "https://openalex.org/W4385807453",
      "arxiv_id": "",
      "publication_date": "2023-08-14",
      "published": "2023-08-14",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The paper focuses on ASR and audio tagging via Whisper, without making Spoken Language Models or full-duplex, speech-language modeling the core focus, thus not meeting the core inclusion criteria.",
      "evaluation": 2
    },
    "round-A_JuniorNano_reasoning": "The paper focuses on ASR and audio tagging via Whisper, without making Spoken Language Models or full-duplex, speech-language modeling the core focus, thus not meeting the core inclusion criteria.",
    "round-A_JuniorNano_evaluation": 2,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper focuses on an automatic speech recognition (ASR) model's robustness and capability for audio event tagging, but it does not address spoken language models (SLMs) or full-duplex spoken language models (FD-SLMs) explicitly, nor does it discuss model architectures, training, or evaluation related to synchronous interaction, cognitive parallelism, or dynamic turn-taking. The main focus is on improving ASR noise robustness and expanding audio tagging, which falls outside the core inclusion criteria emphasizing spoken language modeling and synchronous, interactive speech systems. Therefore, it does not meet the inclusion criteria and aligns with exclusion criteria related to focusing solely on ASR without interactive or spoken language modeling aspects.",
      "evaluation": 2
    },
    "round-A_JuniorMini_reasoning": "The paper focuses on an automatic speech recognition (ASR) model's robustness and capability for audio event tagging, but it does not address spoken language models (SLMs) or full-duplex spoken language models (FD-SLMs) explicitly, nor does it discuss model architectures, training, or evaluation related to synchronous interaction, cognitive parallelism, or dynamic turn-taking. The main focus is on improving ASR noise robustness and expanding audio tagging, which falls outside the core inclusion criteria emphasizing spoken language modeling and synchronous, interactive speech systems. Therefore, it does not meet the inclusion criteria and aligns with exclusion criteria related to focusing solely on ASR without interactive or spoken language modeling aspects.",
    "round-A_JuniorMini_evaluation": 2,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:2)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "SpeechGen: Unlocking the Generative Power of Speech Language Models with Prompts",
    "abstract": "Large language models (LLMs) have gained considerable attention for Artificial Intelligence Generated Content (AIGC), particularly with the emergence of ChatGPT. However, the direct adaptation of continuous speech to LLMs that process discrete tokens remains an unsolved challenge, hindering the application of LLMs for speech generation. The advanced speech LMs are in the corner, as that speech signals encapsulate a wealth of information, including speaker and emotion, beyond textual data alone. Prompt tuning has demonstrated notable gains in parameter efficiency and competitive performance on some speech classification tasks. However, the extent to which prompts can effectively elicit generation tasks from speech LMs remains an open question. In this paper, we present pioneering research that explores the application of prompt tuning to stimulate speech LMs for various generation tasks, within a unified framework called SpeechGen, with around 10M trainable parameters. The proposed unified framework holds great promise for efficiency and effectiveness, particularly with the imminent arrival of advanced speech LMs, which will significantly enhance the capabilities of the framework. The code and demos of SpeechGen will be available on the project website: \\url{https://ga642381.github.io/SpeechPrompt/speechgen}",
    "metadata": {
      "title": "SpeechGen: Unlocking the Generative Power of Speech Language Models with Prompts",
      "summary": "Large language models (LLMs) have gained considerable attention for Artificial Intelligence Generated Content (AIGC), particularly with the emergence of ChatGPT. However, the direct adaptation of continuous speech to LLMs that process discrete tokens remains an unsolved challenge, hindering the application of LLMs for speech generation. The advanced speech LMs are in the corner, as that speech signals encapsulate a wealth of information, including speaker and emotion, beyond textual data alone. Prompt tuning has demonstrated notable gains in parameter efficiency and competitive performance on some speech classification tasks. However, the extent to which prompts can effectively elicit generation tasks from speech LMs remains an open question. In this paper, we present pioneering research that explores the application of prompt tuning to stimulate speech LMs for various generation tasks, within a unified framework called SpeechGen, with around 10M trainable parameters. The proposed unified framework holds great promise for efficiency and effectiveness, particularly with the imminent arrival of advanced speech LMs, which will significantly enhance the capabilities of the framework. The code and demos of SpeechGen will be available on the project website: \\url{https://ga642381.github.io/SpeechPrompt/speechgen}",
      "abstract": "Large language models (LLMs) have gained considerable attention for Artificial Intelligence Generated Content (AIGC), particularly with the emergence of ChatGPT. However, the direct adaptation of continuous speech to LLMs that process discrete tokens remains an unsolved challenge, hindering the application of LLMs for speech generation. The advanced speech LMs are in the corner, as that speech signals encapsulate a wealth of information, including speaker and emotion, beyond textual data alone. Prompt tuning has demonstrated notable gains in parameter efficiency and competitive performance on some speech classification tasks. However, the extent to which prompts can effectively elicit generation tasks from speech LMs remains an open question. In this paper, we present pioneering research that explores the application of prompt tuning to stimulate speech LMs for various generation tasks, within a unified framework called SpeechGen, with around 10M trainable parameters. The proposed unified framework holds great promise for efficiency and effectiveness, particularly with the imminent arrival of advanced speech LMs, which will significantly enhance the capabilities of the framework. The code and demos of SpeechGen will be available on the project website: \\url{https://ga642381.github.io/SpeechPrompt/speechgen}",
      "doi": "https://doi.org/10.48550/arxiv.2306.02207",
      "openalex_id": "https://openalex.org/W4379539302",
      "arxiv_id": "",
      "publication_date": "2023-06-03",
      "published": "2023-06-03",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item centers on Spoken Language Models/Speech-LM concepts, discusses a SpeechGen framework for prompting-based generation tasks from speech inputs (i.e., architecture and training for speech-based generation), is within a 3-year window (arXiv:2504...), and is academic; it does not clearly fall under exclusion criteria like focusing only on ASR/TTS, half/duplex limitations, or treating speech merely as input to text NLP. Thus it meets inclusion criteria, though it isn’t explicit about full-duplex interactive evaluation, which prevents a top score.",
      "evaluation": 4
    },
    "round-A_JuniorNano_reasoning": "The item centers on Spoken Language Models/Speech-LM concepts, discusses a SpeechGen framework for prompting-based generation tasks from speech inputs (i.e., architecture and training for speech-based generation), is within a 3-year window (arXiv:2504...), and is academic; it does not clearly fall under exclusion criteria like focusing only on ASR/TTS, half/duplex limitations, or treating speech merely as input to text NLP. Thus it meets inclusion criteria, though it isn’t explicit about full-duplex interactive evaluation, which prevents a top score.",
    "round-A_JuniorNano_evaluation": 4,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on unlocking generative power of speech language models with prompts, highlighting a unified framework (SpeechGen) that operates with speech LMs and integrates speech generation tasks. It discusses speech language models as core modeling objects beyond text and addresses training and architectural considerations. However, the abstract does not clearly indicate addressing full-duplex interaction capabilities, cognitive parallelism, synchronous turn-taking, or multi-dimensional evaluation criteria like behavior arbitration or low-latency interaction, which are essential for inclusion. Also, the abstract lacks explicit mention of real-time interaction evaluation or full-duplex synchronization, suggesting it may not fully meet all inclusion criteria, though it shows relevant advances in speech language modeling for generation tasks. Hence, inclusion is uncertain but leaning towards ignoring full-duplex interactive aspects required by the criteria.",
      "evaluation": 3
    },
    "round-A_JuniorMini_reasoning": "The study focuses on unlocking generative power of speech language models with prompts, highlighting a unified framework (SpeechGen) that operates with speech LMs and integrates speech generation tasks. It discusses speech language models as core modeling objects beyond text and addresses training and architectural considerations. However, the abstract does not clearly indicate addressing full-duplex interaction capabilities, cognitive parallelism, synchronous turn-taking, or multi-dimensional evaluation criteria like behavior arbitration or low-latency interaction, which are essential for inclusion. Also, the abstract lacks explicit mention of real-time interaction evaluation or full-duplex synchronization, suggesting it may not fully meet all inclusion criteria, though it shows relevant advances in speech language modeling for generation tasks. Hence, inclusion is uncertain but leaning towards ignoring full-duplex interactive aspects required by the criteria.",
    "round-A_JuniorMini_evaluation": 3,
    "round-B_SeniorLead_output": {
      "reasoning": "Both junior reviewers correctly noted this paper is centered on speech language models (it explicitly uses terms like “speech LMs”/“speech language models”), proposes a unified prompt-tuning framework (SpeechGen) with architecture/training details (≈10M trainable parameters), and is within the allowed time window — satisfying the primary inclusion requirement that speech be the core modeling object rather than merely a pre-processing input to text NLP. They also both flagged that the abstract does not address full-duplex interaction, synchronous turn-taking, overlap handling, low-latency evaluation, or multi-dimensional interaction benchmarks required for FD-SLM work; however, lack of FD-specific content is not an explicit exclusion unless the paper is solely about half-duplex or single-direction ASR/TTS or merely pipelines speech→text without speech-modeling contributions. The abstract indicates model/training contributions for generative speech tasks (not just ASR/TTS or text-only processing), so it does not meet the exclusion criteria. Therefore I recommend inclusion, but not the highest score because it omits explicit full‑duplex/synchronous interaction evaluation.",
      "evaluation": 4
    },
    "round-B_SeniorLead_reasoning": "Both junior reviewers correctly noted this paper is centered on speech language models (it explicitly uses terms like “speech LMs”/“speech language models”), proposes a unified prompt-tuning framework (SpeechGen) with architecture/training details (≈10M trainable parameters), and is within the allowed time window — satisfying the primary inclusion requirement that speech be the core modeling object rather than merely a pre-processing input to text NLP. They also both flagged that the abstract does not address full-duplex interaction, synchronous turn-taking, overlap handling, low-latency evaluation, or multi-dimensional interaction benchmarks required for FD-SLM work; however, lack of FD-specific content is not an explicit exclusion unless the paper is solely about half-duplex or single-direction ASR/TTS or merely pipelines speech→text without speech-modeling contributions. The abstract indicates model/training contributions for generative speech tasks (not just ASR/TTS or text-only processing), so it does not meet the exclusion criteria. Therefore I recommend inclusion, but not the highest score because it omits explicit full‑duplex/synchronous interaction evaluation.",
    "round-B_SeniorLead_evaluation": 4,
    "final_verdict": "include (senior:4)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "SpeechPrompt v2: Prompt Tuning for Speech Classification Tasks",
    "abstract": "Prompt tuning is a technology that tunes a small set of parameters to steer a pre-trained language model (LM) to directly generate the output for downstream tasks. Recently, prompt tuning has demonstrated its storage and computation efficiency in both natural language processing (NLP) and speech processing fields. These advantages have also revealed prompt tuning as a candidate approach to serving pre-trained LM for multiple tasks in a unified manner. For speech processing, SpeechPrompt shows its high parameter efficiency and competitive performance on a few speech classification tasks. However, whether SpeechPrompt is capable of serving a large number of tasks is unanswered. In this work, we propose SpeechPrompt v2, a prompt tuning framework capable of performing a wide variety of speech classification tasks, covering multiple languages and prosody-related tasks. The experiment result shows that SpeechPrompt v2 achieves performance on par with prior works with less than 0.15M trainable parameters in a unified framework.",
    "metadata": {
      "title": "SpeechPrompt v2: Prompt Tuning for Speech Classification Tasks",
      "summary": "Prompt tuning is a technology that tunes a small set of parameters to steer a pre-trained language model (LM) to directly generate the output for downstream tasks. Recently, prompt tuning has demonstrated its storage and computation efficiency in both natural language processing (NLP) and speech processing fields. These advantages have also revealed prompt tuning as a candidate approach to serving pre-trained LM for multiple tasks in a unified manner. For speech processing, SpeechPrompt shows its high parameter efficiency and competitive performance on a few speech classification tasks. However, whether SpeechPrompt is capable of serving a large number of tasks is unanswered. In this work, we propose SpeechPrompt v2, a prompt tuning framework capable of performing a wide variety of speech classification tasks, covering multiple languages and prosody-related tasks. The experiment result shows that SpeechPrompt v2 achieves performance on par with prior works with less than 0.15M trainable parameters in a unified framework.",
      "abstract": "Prompt tuning is a technology that tunes a small set of parameters to steer a pre-trained language model (LM) to directly generate the output for downstream tasks. Recently, prompt tuning has demonstrated its storage and computation efficiency in both natural language processing (NLP) and speech processing fields. These advantages have also revealed prompt tuning as a candidate approach to serving pre-trained LM for multiple tasks in a unified manner. For speech processing, SpeechPrompt shows its high parameter efficiency and competitive performance on a few speech classification tasks. However, whether SpeechPrompt is capable of serving a large number of tasks is unanswered. In this work, we propose SpeechPrompt v2, a prompt tuning framework capable of performing a wide variety of speech classification tasks, covering multiple languages and prosody-related tasks. The experiment result shows that SpeechPrompt v2 achieves performance on par with prior works with less than 0.15M trainable parameters in a unified framework.",
      "doi": "https://doi.org/10.48550/arxiv.2303.00733",
      "openalex_id": "https://openalex.org/W4322825254",
      "arxiv_id": "",
      "publication_date": "2023-03-01",
      "published": "2023-03-01",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The work uses a pre-trained language model for speech classification via prompt tuning but does not position speech as the primary modeling object nor discuss Spoken Language Models/FD-SLMs or interactive, synchronous dialogue; thus it does not meet the core inclusion criteria.",
      "evaluation": 2
    },
    "round-A_JuniorNano_reasoning": "The work uses a pre-trained language model for speech classification via prompt tuning but does not position speech as the primary modeling object nor discuss Spoken Language Models/FD-SLMs or interactive, synchronous dialogue; thus it does not meet the core inclusion criteria.",
    "round-A_JuniorNano_evaluation": 2,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on prompt tuning for speech classification tasks using a pre-trained language model, but it does not clearly present research on Spoken Language Models or Full-Duplex Spoken Language Models as defined by the inclusion criteria, nor does it address synchronous interaction, cognitive parallelism, or low-latency real-time interaction evaluation; it also centers on speech classification, which may not cover speech language modeling or dialogue behavior arbitration comprehensively. Therefore, while related to speech processing, it likely falls short of the core requirements regarding speech language modeling and synchronous full-duplex interaction.",
      "evaluation": 2
    },
    "round-A_JuniorMini_reasoning": "The study focuses on prompt tuning for speech classification tasks using a pre-trained language model, but it does not clearly present research on Spoken Language Models or Full-Duplex Spoken Language Models as defined by the inclusion criteria, nor does it address synchronous interaction, cognitive parallelism, or low-latency real-time interaction evaluation; it also centers on speech classification, which may not cover speech language modeling or dialogue behavior arbitration comprehensively. Therefore, while related to speech processing, it likely falls short of the core requirements regarding speech language modeling and synchronous full-duplex interaction.",
    "round-A_JuniorMini_evaluation": 2,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:2)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "ESB: A Benchmark For Multi-Domain End-to-End Speech Recognition",
    "abstract": "Speech recognition applications cover a range of different audio and text distributions, with different speaking styles, background noise, transcription punctuation and character casing. However, many speech recognition systems require dataset-specific tuning (audio filtering, punctuation removal and normalisation of casing), therefore assuming a-priori knowledge of both the audio and text distributions. This tuning requirement can lead to systems failing to generalise to other datasets and domains. To promote the development of multi-domain speech systems, we introduce the End-to-end Speech Benchmark (ESB) for evaluating the performance of a single automatic speech recognition (ASR) system across a broad set of speech datasets. Benchmarked systems must use the same data pre- and post-processing algorithm across datasets - assuming the audio and text data distributions are a-priori unknown. We compare a series of state-of-the-art (SoTA) end-to-end (E2E) systems on this benchmark, demonstrating how a single speech system can be applied and evaluated on a wide range of data distributions. We find E2E systems to be effective across datasets: in a fair comparison, E2E systems achieve within 2.6% of SoTA systems tuned to a specific dataset. Our analysis reveals that transcription artefacts, such as punctuation and casing, pose difficulties for ASR systems and should be included in evaluation. We believe E2E benchmarking over a range of datasets promotes the research of multi-domain speech recognition systems. ESB is available at https://huggingface.co/esb.",
    "metadata": {
      "title": "ESB: A Benchmark For Multi-Domain End-to-End Speech Recognition",
      "summary": "Speech recognition applications cover a range of different audio and text distributions, with different speaking styles, background noise, transcription punctuation and character casing. However, many speech recognition systems require dataset-specific tuning (audio filtering, punctuation removal and normalisation of casing), therefore assuming a-priori knowledge of both the audio and text distributions. This tuning requirement can lead to systems failing to generalise to other datasets and domains. To promote the development of multi-domain speech systems, we introduce the End-to-end Speech Benchmark (ESB) for evaluating the performance of a single automatic speech recognition (ASR) system across a broad set of speech datasets. Benchmarked systems must use the same data pre- and post-processing algorithm across datasets - assuming the audio and text data distributions are a-priori unknown. We compare a series of state-of-the-art (SoTA) end-to-end (E2E) systems on this benchmark, demonstrating how a single speech system can be applied and evaluated on a wide range of data distributions. We find E2E systems to be effective across datasets: in a fair comparison, E2E systems achieve within 2.6% of SoTA systems tuned to a specific dataset. Our analysis reveals that transcription artefacts, such as punctuation and casing, pose difficulties for ASR systems and should be included in evaluation. We believe E2E benchmarking over a range of datasets promotes the research of multi-domain speech recognition systems. ESB is available at https://huggingface.co/esb.",
      "abstract": "Speech recognition applications cover a range of different audio and text distributions, with different speaking styles, background noise, transcription punctuation and character casing. However, many speech recognition systems require dataset-specific tuning (audio filtering, punctuation removal and normalisation of casing), therefore assuming a-priori knowledge of both the audio and text distributions. This tuning requirement can lead to systems failing to generalise to other datasets and domains. To promote the development of multi-domain speech systems, we introduce the End-to-end Speech Benchmark (ESB) for evaluating the performance of a single automatic speech recognition (ASR) system across a broad set of speech datasets. Benchmarked systems must use the same data pre- and post-processing algorithm across datasets - assuming the audio and text data distributions are a-priori unknown. We compare a series of state-of-the-art (SoTA) end-to-end (E2E) systems on this benchmark, demonstrating how a single speech system can be applied and evaluated on a wide range of data distributions. We find E2E systems to be effective across datasets: in a fair comparison, E2E systems achieve within 2.6% of SoTA systems tuned to a specific dataset. Our analysis reveals that transcription artefacts, such as punctuation and casing, pose difficulties for ASR systems and should be included in evaluation. We believe E2E benchmarking over a range of datasets promotes the research of multi-domain speech recognition systems. ESB is available at https://huggingface.co/esb.",
      "doi": "https://doi.org/10.48550/arxiv.2210.13352",
      "openalex_id": "https://openalex.org/W4307322847",
      "arxiv_id": "",
      "publication_date": "2022-10-24",
      "published": "2022-10-24",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This item is an ASR benchmark paper focusing on end-to-end speech recognition across multiple datasets; it does not center on Spoken Language Models or full-duplex conversational speech modeling, nor does it address speech–language modeling paradigms, synchronous interaction, or related evaluation beyond recognition, so it does not meet the inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "This item is an ASR benchmark paper focusing on end-to-end speech recognition across multiple datasets; it does not center on Spoken Language Models or full-duplex conversational speech modeling, nor does it address speech–language modeling paradigms, synchronous interaction, or related evaluation beyond recognition, so it does not meet the inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study introduces a benchmark for multi-domain end-to-end speech recognition systems and focuses on evaluation of ASR systems' performance across diverse datasets without dataset-specific tuning. However, it mainly centers on automatic speech recognition (ASR) performance and does not explicitly discuss spoken language models (SLMs), language modeling of speech, or full-duplex dialogue features such as cognitive parallelism or synchronous interaction. It also does not address multiple modalities or interactional aspects beyond ASR evaluation. Therefore, it does not meet the inclusion criterion that the research must explicitly focus on spoken language models or equivalent concepts as the core modeling and reasoning object. It also falls under several exclusion criteria by focusing on ASR alone and lacks discussion on full-duplex or synchronous interaction capabilities.",
      "evaluation": 2
    },
    "round-A_JuniorMini_reasoning": "The study introduces a benchmark for multi-domain end-to-end speech recognition systems and focuses on evaluation of ASR systems' performance across diverse datasets without dataset-specific tuning. However, it mainly centers on automatic speech recognition (ASR) performance and does not explicitly discuss spoken language models (SLMs), language modeling of speech, or full-duplex dialogue features such as cognitive parallelism or synchronous interaction. It also does not address multiple modalities or interactional aspects beyond ASR evaluation. Therefore, it does not meet the inclusion criterion that the research must explicitly focus on spoken language models or equivalent concepts as the core modeling and reasoning object. It also falls under several exclusion criteria by focusing on ASR alone and lacks discussion on full-duplex or synchronous interaction capabilities.",
    "round-A_JuniorMini_evaluation": 2,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:2)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "CSTR VCTK Corpus: English Multi-speaker Corpus for CSTR Voice Cloning Toolkit (version 0.92)",
    "abstract": "This CSTR VCTK Corpus includes speech data uttered by 110 English speakers with various accents. Each speaker reads out about 400 sentences, which were selected from a newspaper, the rainbow passage and an elicitation paragraph used for the speech accent archive. The newspaper texts were taken from Herald Glasgow, with permission from Herald &amp; Times Group. Each speaker has a different set of the newspaper texts selected based a greedy algorithm that increases the contextual and phonetic coverage. The details of the text selection algorithms are described in the following paper: C. Veaux, J. Yamagishi and S. King, \"The voice bank corpus: Design, collection and data analysis of a large regional accent speech database,\" https://doi.org/10.1109/ICSDA.2013.6709856 The rainbow passage and elicitation paragraph are the same for all speakers. The rainbow passage can be found at International Dialects of English Archive: (http://web.ku.edu/~idea/readings/rainbow.htm). The elicitation paragraph is identical to the one used for the speech accent archive (http://accent.gmu.edu). The details of the the speech accent archive can be found at http://www.ualberta.ca/~aacl2009/PDFs/WeinbergerKunath2009AACL.pdf All speech data was recorded using an identical recording setup: an omni-directional microphone (DPA 4035) and a small diaphragm condenser microphone with very wide bandwidth (Sennheiser MKH 800), 96kHz sampling frequency at 24 bits and in a hemi-anechoic chamber of the University of Edinburgh. (However, two speakers, p280 and p315 had technical issues of the audio recordings using MKH 800). All recordings were converted into 16 bits, were downsampled to 48 kHz, and were manually end-pointed. This corpus was originally aimed for HMM-based text-to-speech synthesis systems, especially for speaker-adaptive HMM-based speech synthesis that uses average voice models trained on multiple speakers and speaker adaptation technologies. This corpus is also suitable for DNN-based multi-speaker text-to-speech synthesis systems and neural waveform modeling. The dataset was was referenced in the Google DeepMind work on WaveNet: https://arxiv.org/pdf/1609.03499.pdf . Please note while text files containing transcripts of the speech are provided for 109 of the 110 recordings, in the '/txt' folder, the 'p315' text was lost due to a hard disk error.",
    "metadata": {
      "title": "CSTR VCTK Corpus: English Multi-speaker Corpus for CSTR Voice Cloning Toolkit (version 0.92)",
      "summary": "This CSTR VCTK Corpus includes speech data uttered by 110 English speakers with various accents. Each speaker reads out about 400 sentences, which were selected from a newspaper, the rainbow passage and an elicitation paragraph used for the speech accent archive. The newspaper texts were taken from Herald Glasgow, with permission from Herald &amp; Times Group. Each speaker has a different set of the newspaper texts selected based a greedy algorithm that increases the contextual and phonetic coverage. The details of the text selection algorithms are described in the following paper: C. Veaux, J. Yamagishi and S. King, \"The voice bank corpus: Design, collection and data analysis of a large regional accent speech database,\" https://doi.org/10.1109/ICSDA.2013.6709856 The rainbow passage and elicitation paragraph are the same for all speakers. The rainbow passage can be found at International Dialects of English Archive: (http://web.ku.edu/~idea/readings/rainbow.htm). The elicitation paragraph is identical to the one used for the speech accent archive (http://accent.gmu.edu). The details of the the speech accent archive can be found at http://www.ualberta.ca/~aacl2009/PDFs/WeinbergerKunath2009AACL.pdf All speech data was recorded using an identical recording setup: an omni-directional microphone (DPA 4035) and a small diaphragm condenser microphone with very wide bandwidth (Sennheiser MKH 800), 96kHz sampling frequency at 24 bits and in a hemi-anechoic chamber of the University of Edinburgh. (However, two speakers, p280 and p315 had technical issues of the audio recordings using MKH 800). All recordings were converted into 16 bits, were downsampled to 48 kHz, and were manually end-pointed. This corpus was originally aimed for HMM-based text-to-speech synthesis systems, especially for speaker-adaptive HMM-based speech synthesis that uses average voice models trained on multiple speakers and speaker adaptation technologies. This corpus is also suitable for DNN-based multi-speaker text-to-speech synthesis systems and neural waveform modeling. The dataset was was referenced in the Google DeepMind work on WaveNet: https://arxiv.org/pdf/1609.03499.pdf . Please note while text files containing transcripts of the speech are provided for 109 of the 110 recordings, in the '/txt' folder, the 'p315' text was lost due to a hard disk error.",
      "abstract": "This CSTR VCTK Corpus includes speech data uttered by 110 English speakers with various accents. Each speaker reads out about 400 sentences, which were selected from a newspaper, the rainbow passage and an elicitation paragraph used for the speech accent archive. The newspaper texts were taken from Herald Glasgow, with permission from Herald &amp; Times Group. Each speaker has a different set of the newspaper texts selected based a greedy algorithm that increases the contextual and phonetic coverage. The details of the text selection algorithms are described in the following paper: C. Veaux, J. Yamagishi and S. King, \"The voice bank corpus: Design, collection and data analysis of a large regional accent speech database,\" https://doi.org/10.1109/ICSDA.2013.6709856 The rainbow passage and elicitation paragraph are the same for all speakers. The rainbow passage can be found at International Dialects of English Archive: (http://web.ku.edu/~idea/readings/rainbow.htm). The elicitation paragraph is identical to the one used for the speech accent archive (http://accent.gmu.edu). The details of the the speech accent archive can be found at http://www.ualberta.ca/~aacl2009/PDFs/WeinbergerKunath2009AACL.pdf All speech data was recorded using an identical recording setup: an omni-directional microphone (DPA 4035) and a small diaphragm condenser microphone with very wide bandwidth (Sennheiser MKH 800), 96kHz sampling frequency at 24 bits and in a hemi-anechoic chamber of the University of Edinburgh. (However, two speakers, p280 and p315 had technical issues of the audio recordings using MKH 800). All recordings were converted into 16 bits, were downsampled to 48 kHz, and were manually end-pointed. This corpus was originally aimed for HMM-based text-to-speech synthesis systems, especially for speaker-adaptive HMM-based speech synthesis that uses average voice models trained on multiple speakers and speaker adaptation technologies. This corpus is also suitable for DNN-based multi-speaker text-to-speech synthesis systems and neural waveform modeling. The dataset was was referenced in the Google DeepMind work on WaveNet: https://arxiv.org/pdf/1609.03499.pdf . Please note while text files containing transcripts of the speech are provided for 109 of the 110 recordings, in the '/txt' folder, the 'p315' text was lost due to a hard disk error.",
      "doi": "https://doi.org/10.7488/ds/2645",
      "openalex_id": "https://openalex.org/W2998572311",
      "arxiv_id": "",
      "publication_date": "2019-01-01",
      "published": "2019-01-01",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item describes a speech corpus for TTS (CSTR VCTK) and its use for voice cloning, with no emphasis on Spoken Language Models or full-duplex interactive speech; it is not within the past 3 years and does not meet the inclusion criteria, so it should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item describes a speech corpus for TTS (CSTR VCTK) and its use for voice cloning, with no emphasis on Spoken Language Models or full-duplex interactive speech; it is not within the past 3 years and does not meet the inclusion criteria, so it should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The article describes a multi-speaker speech corpus designed primarily for HMM- and DNN-based TTS synthesis systems, focusing on speech data collection rather than modeling spoken language models or full-duplex spoken language interaction; it lacks discussion on spoken language models, model architectures, training, evaluation for real-time interaction, or any of the inclusion criteria such as full-duplex interaction or synchronization, and only concerns TTS data without downstream interactive or cognitive parallelism aspects.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The article describes a multi-speaker speech corpus designed primarily for HMM- and DNN-based TTS synthesis systems, focusing on speech data collection rather than modeling spoken language models or full-duplex spoken language interaction; it lacks discussion on spoken language models, model architectures, training, evaluation for real-time interaction, or any of the inclusion criteria such as full-duplex interaction or synchronization, and only concerns TTS data without downstream interactive or cognitive parallelism aspects.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  }
]