openalex_id,doi,title,abstract,referenced_works,publication_date
https://openalex.org/W4392902876,https://doi.org/10.1109/icassp48485.2024.10447448,Retrieval Augmented End-to-End Spoken Dialog Models,"We recently developed a joint speech and language model (SLM [1]) which fuses a pretrained foundational speech model and a large language model (LLM), while preserving the in-context learning capability intrinsic to the pretrained LLM. In this paper, we apply SLM to dialog applications where the dialog states are inferred directly from the audio signal.Task-oriented dialogs often contain domain-specific entities, i.e., restaurants, hotels, train stations, and city names, which are difficult to recognize, however, critical for the downstream applications. Inspired by the RAG (retrieval-augmented generation) models, we propose a retrieval augmented SLM (ReSLM) that overcomes this weakness. We first train a retriever to retrieve text entities given audio inputs. The retrieved entities are then added as text inputs to the underlying LLM to bias model predictions. We evaluated ReSLM on speech MultiWoz task (DSTC-11 Challenge), and found that the retrieval augmentation boosts model performance, achieving joint goal accuracy (38.6% vs 32.7%), slot error rate (20.6% vs 24.8%) and ASR word error rate (5.5% vs 6.7%). While demonstrated on dialog state tracking, our approach is broadly applicable to speech tasks requiring custom contextual information or domain-specific entities.","['https://openalex.org/W4391021623', 'https://openalex.org/W6777615688', 'https://openalex.org/W6779857854', 'https://openalex.org/W3099700870', 'https://openalex.org/W6810722945', 'https://openalex.org/W4385822641', 'https://openalex.org/W6764961350', 'https://openalex.org/W4385573970', 'https://openalex.org/W6803092890', 'https://openalex.org/W4226120743', 'https://openalex.org/W4225272718', 'https://openalex.org/W4225308107', 'https://openalex.org/W6799838802', 'https://openalex.org/W6810334672', 'https://openalex.org/W4381786045', 'https://openalex.org/W4372270126', 'https://openalex.org/W6783813245', 'https://openalex.org/W6810730852', 'https://openalex.org/W6845404407', 'https://openalex.org/W2945475330', 'https://openalex.org/W6770169672', 'https://openalex.org/W2997771882', 'https://openalex.org/W3206547074', 'https://openalex.org/W4223510772', 'https://openalex.org/W6850218400', 'https://openalex.org/W6769627184', 'https://openalex.org/W2127141656', 'https://openalex.org/W4372267461', 'https://openalex.org/W6777399232', 'https://openalex.org/W6810673746', 'https://openalex.org/W3097777922', 'https://openalex.org/W4225323055', 'https://openalex.org/W4288027128', 'https://openalex.org/W3207222250', 'https://openalex.org/W4226082499', 'https://openalex.org/W4301243929', 'https://openalex.org/W4225553189', 'https://openalex.org/W2954492830', 'https://openalex.org/W4323066695', 'https://openalex.org/W3190965961']",2024-03-18
https://openalex.org/W4408358330,https://doi.org/10.1109/access.2025.3550855,"Advancements in Speech Recognition: A Systematic Review of Deep Learning Transformer Models, Trends, Innovations, and Future Directions","The transformer is a Deep Learning (DL) model that revolutionized language processing with its self-attention mechanism, enabling parallel processing and improving model efficiency, which dramatically reshaped the landscape of speech recognition technology, based on the ability to efficiently manage the dynamic and context-rich nature of speech. The proposed systematic review in this article critically examines the impact of transformer models on speech recognition, covering the published research over the past seven years (from January 1, 2017, to May 15, 2024). The goals of this article are to synthesize the current knowledge, pinpoint emerging trends, and identify research gaps that could be beneficial for future investigations. From an initial pool of 2,838 publications sourced from leading digital libraries, a rigorous two-step screening process applied to distill high-quality studies relevant to the review criteria. We concentrated our analysis on seven pivotal areas as following: the environmental conditions (neutral versus noisy) addressed by the studies, methods of feature extraction employed, characteristics of the transformer models used, datasets utilized, variations in model efficiency, the influence of noise on model generalizability, and trends in self-supervised learning, ending up with 37 articles to review in this paper. Our findings underscore the transformative potential of transformers in enhancing the accuracy and robustness of speech recognition systems, especially in challenging acoustic environments. In addition, this review highlights areas where more research is needed to make speech recognition even better by using transformer technology.","['https://openalex.org/W6866711220', 'https://openalex.org/W6866150908', 'https://openalex.org/W6857228344', 'https://openalex.org/W6857421307', 'https://openalex.org/W4385822429', 'https://openalex.org/W4385822766', 'https://openalex.org/W6853611000', 'https://openalex.org/W4385822815', 'https://openalex.org/W4327500526', 'https://openalex.org/W3206114047', 'https://openalex.org/W4225808286', 'https://openalex.org/W4224925623', 'https://openalex.org/W4210463634', 'https://openalex.org/W6770514103', 'https://openalex.org/W3207595101', 'https://openalex.org/W4223945715', 'https://openalex.org/W4226420874', 'https://openalex.org/W3015810689', 'https://openalex.org/W4391021623', 'https://openalex.org/W4226380987', 'https://openalex.org/W3015747801', 'https://openalex.org/W3162313915', 'https://openalex.org/W2981857663', 'https://openalex.org/W4393863141', 'https://openalex.org/W4221165942', 'https://openalex.org/W3016010032', 'https://openalex.org/W4391021542', 'https://openalex.org/W4392904027', 'https://openalex.org/W4385245566', 'https://openalex.org/W4400771889', 'https://openalex.org/W4283818106', 'https://openalex.org/W4379805013', 'https://openalex.org/W4205608408', 'https://openalex.org/W4412474694', 'https://openalex.org/W2781626870', 'https://openalex.org/W3157175643', 'https://openalex.org/W4399426355', 'https://openalex.org/W4317881096']",2025-01-01
https://openalex.org/W4412899262,https://doi.org/10.37943/22snok5872,AUDIO-TO-TEXT TRANSLATION FOR THE HARD OF HEARING: A WHISPER MODEL-BASED STUDY,"This study investigates the effectiveness of the Whisper model for audio-to-text transcription, specifically targeting the enhancement of accessibility for individuals with hearing impairments. The research focuses on the processing of audio recordings obtained from WhatsApp messenger, which often contain significant background noise that complicates speech recognition. To address this issue, advanced audio processing techniques were employed, including the use of the Librosa library and the Noisereduce package for noise reduction. The spectral gating methods applied in this study effectively diminished wind noise and other ambient sounds, allowing for clearer recognition of spoken content. To ensure the quality of the processed audio, we assessed its clarity using a SimpleRNN model. The training results demonstrated a progressive reduction in loss values across epochs, confirming the successful enhancement of audio quality. Once the audio files were adequately cleaned, we utilized the Whisper model, a sophisticated machine learning tool for speech recognition developed by OpenAI, to transcribe the audio into text. The transcription process yielded accurate Kazakh language output, despite the initial challenges posed by background noise. These findings underscore the critical role of high-quality audio input in achieving reliable transcription results and highlight the potential of machine learning technologies in improving communication access for hearing-impaired individuals. This study concludes with recommendations for future research, including the exploration of additional noise reduction techniques and the application of the Whisper model across various languages and dialects. Such advancements could significantly contribute to creating more inclusive digital environments and enhancing the overall user experience for individuals with hearing impairments.","['https://openalex.org/W2029430921', 'https://openalex.org/W4391384041', 'https://openalex.org/W4387790057', 'https://openalex.org/W4361003384', 'https://openalex.org/W4389669656', 'https://openalex.org/W4387303718', 'https://openalex.org/W4297259271', 'https://openalex.org/W4319601831', 'https://openalex.org/W4403496717', 'https://openalex.org/W3092424727', 'https://openalex.org/W4385764125', 'https://openalex.org/W4375869257', 'https://openalex.org/W3162037819', 'https://openalex.org/W4309951156', 'https://openalex.org/W4391021666', 'https://openalex.org/W3206573929', 'https://openalex.org/W4391021623', 'https://openalex.org/W4385805046', 'https://openalex.org/W3119991380', 'https://openalex.org/W4320884090', 'https://openalex.org/W4394978572', 'https://openalex.org/W4386575267', 'https://openalex.org/W4402981815']",2025-06-30
https://openalex.org/W4389524500,https://doi.org/10.18653/v1/2023.findings-emnlp.1055,SpeechGPT: Empowering Large Language Models with Intrinsic Cross-Modal Conversational Abilities,"Multi-modal large language models are regarded as a crucial step towards Artificial General Intelligence (AGI) and have garnered significant interest with the emergence of ChatGPT. However, current speech-language models typically adopt the cascade paradigm, preventing inter-modal knowledge transfer. In this paper, we propose SpeechGPT, a large language model with intrinsic cross-modal conversational abilities, capable of perceiving and generating multi-modal content. With discrete speech representations, we construct SpeechInstruct, the first large-scale cross-modal speech instruction dataset. Additionally, we employ a three-stage training strategy that includes modality-adaptation pre-training, cross-modal instruction fine-tuning, and chain-of-modality instruction fine-tuning. The experimental results demonstrate that SpeechGPT has an impressive capacity to follow cross-modal human instructions and highlight the potential of handling multiple modalities with one model. Code and models are available in https://github.com/0nutation/SpeechGPT. Demos are shown in https://0nutation.github.io/SpeechGPT.github.io/.","['https://openalex.org/W4385807416', 'https://openalex.org/W4394671563', 'https://openalex.org/W3166396011', 'https://openalex.org/W4367061106', 'https://openalex.org/W4381786045', 'https://openalex.org/W4385570101', 'https://openalex.org/W4385572615', 'https://openalex.org/W4323572061', 'https://openalex.org/W4361866031', 'https://openalex.org/W3140429000', 'https://openalex.org/W4322718246', 'https://openalex.org/W3169320628', 'https://openalex.org/W4310924890', 'https://openalex.org/W4287120025', 'https://openalex.org/W2995181338', 'https://openalex.org/W4307680525', 'https://openalex.org/W4313679638', 'https://openalex.org/W3036601975', 'https://openalex.org/W4224308101', 'https://openalex.org/W3094502228', 'https://openalex.org/W1494198834', 'https://openalex.org/W3168867926', 'https://openalex.org/W4366330503', 'https://openalex.org/W4375958083', 'https://openalex.org/W4361229539', 'https://openalex.org/W3030437843', 'https://openalex.org/W4386384714', 'https://openalex.org/W4377297670', 'https://openalex.org/W4323651091', 'https://openalex.org/W4322718191']",2023-01-01
https://openalex.org/W4389519009,https://doi.org/10.18653/v1/2023.findings-emnlp.704,MRRL: Modifying the Reference via Reinforcement Learning for Non-Autoregressive Joint Multiple Intent Detection and Slot Filling,"With the rise of non-autoregressive approach, some non-autoregressive models for joint multiple intent detection and slot filling have obtained the promising inference speed. However, most existing SLU models (1) suffer from the multi-modality problem that leads to reference intents and slots may not be suitable for training; (2) lack of alignment between the correct predictions of the two tasks, which extremely limits the overall accuracy. Therefore, in this paper, we propose Modifying the Reference via Reinforcement Learning (MRRL), a novel method for multiple intent detection and slot filling, which introduces a modifier and employs reinforcement learning. Specifically, we try to provide the better training target for the non-autoregressive SLU model via modifying the reference based on the output of the non-autoregressive SLU model, and propose a suitability reward to ensure that the output of the modifier module could fit well with the output of the non-autoregressive SLU model and does not deviate too far from the reference. In addition, we also propose a compromise reward to realize a flexible trade-off between the two subtasks. Experiments on two multi-intent datasets and non-autoregressive baselines demonstrate that our MRRL could consistently improve the performance of baselines. More encouragingly, our best variant achieves new state-of-the-art results, outperforming the previous best approach by 3.6 overall accuracy on MixATIS dataset.","['https://openalex.org/W2581637843', 'https://openalex.org/W3161463965', 'https://openalex.org/W4389520437', 'https://openalex.org/W3103899394', 'https://openalex.org/W3034216012', 'https://openalex.org/W3161631007', 'https://openalex.org/W648947103', 'https://openalex.org/W4312426068', 'https://openalex.org/W3209239055', 'https://openalex.org/W4384807869', 'https://openalex.org/W4297733535', 'https://openalex.org/W4382202516', 'https://openalex.org/W4382202608', 'https://openalex.org/W3098800734', 'https://openalex.org/W4386384714', 'https://openalex.org/W4387848696', 'https://openalex.org/W3173794693', 'https://openalex.org/W3099757670', 'https://openalex.org/W2963974889', 'https://openalex.org/W2804945011', 'https://openalex.org/W2963034998', 'https://openalex.org/W4385572615', 'https://openalex.org/W3212491726', 'https://openalex.org/W3170264707', 'https://openalex.org/W2963066655', 'https://openalex.org/W4385571399', 'https://openalex.org/W4378718233', 'https://openalex.org/W3100532071', 'https://openalex.org/W2474111273', 'https://openalex.org/W4224212267', 'https://openalex.org/W4385822432', 'https://openalex.org/W4389519101', 'https://openalex.org/W4238557920', 'https://openalex.org/W4288090877', 'https://openalex.org/W3113593351', 'https://openalex.org/W3162342118', 'https://openalex.org/W4385573070', 'https://openalex.org/W2077302143', 'https://openalex.org/W4298093396', 'https://openalex.org/W4385567042', 'https://openalex.org/W2898637464', 'https://openalex.org/W2888442053', 'https://openalex.org/W4385570101', 'https://openalex.org/W4385567178', 'https://openalex.org/W1522301498', 'https://openalex.org/W2888921503', 'https://openalex.org/W2970450228', 'https://openalex.org/W2971167298', 'https://openalex.org/W4297683418', 'https://openalex.org/W4375868795', 'https://openalex.org/W4385823351', 'https://openalex.org/W2963167310', 'https://openalex.org/W2964071174', 'https://openalex.org/W4382119366', 'https://openalex.org/W4285217170', 'https://openalex.org/W2766371743', 'https://openalex.org/W2803392141', 'https://openalex.org/W4389520050', 'https://openalex.org/W2094472029', 'https://openalex.org/W4287887858', 'https://openalex.org/W3093528669', 'https://openalex.org/W2119717200', 'https://openalex.org/W4224933765', 'https://openalex.org/W2473329891', 'https://openalex.org/W4288346477', 'https://openalex.org/W2946085385', 'https://openalex.org/W1975244201']",2023-01-01
https://openalex.org/W4394583388,https://doi.org/10.1109/ic2pct60090.2024.10486323,Optical Character Recognition Systems for Accurate Interpretation of Handwritten Telugu Scripts,"Visual data dominates the digital landscape these days. However, there remains a significant gap in the digitization of textual content in regional languages such as Telugu. This research addresses this gap and utilizes the capabilities of the programming language and set of specialized libraries pytesseract for optical character recognition (OCR), PIL for image processing and Levenshtein for accuracy evaluation to appropriately extract text from images in the Telugu script. There are several important steps in this project. Initially, this work sets up a dependable system for uploading pictures. Secondly, carefully extracting text from these submitted photos using py-tesseract. Using the Levenshtein distance measure, and then compared with the ground truth to determine correctness. This metric measures the accuracy of the text output through looking at the slight variations between the word error rate (WER) and character error rate (CER). This is capable of finding misspelled words and unknown characters providing a precise evaluation of the OCR's effectiveness. This study also examines the current state of OCR in Telugu text processing and indicates the benefits of current technologies for locating and converting Telugu text discovered in image documents. It also demonstrates improvements in OCR technology.","['https://openalex.org/W2117182510', 'https://openalex.org/W4293493277', 'https://openalex.org/W6774023782', 'https://openalex.org/W3082735848', 'https://openalex.org/W4223415467', 'https://openalex.org/W4205445027', 'https://openalex.org/W6795700344', 'https://openalex.org/W3210847758', 'https://openalex.org/W4307194360', 'https://openalex.org/W3214042726', 'https://openalex.org/W2332561139', 'https://openalex.org/W2140485652', 'https://openalex.org/W2971777065', 'https://openalex.org/W6690217923', 'https://openalex.org/W1993011068', 'https://openalex.org/W6855885476', 'https://openalex.org/W3183597859', 'https://openalex.org/W4386384714', 'https://openalex.org/W4285058206', 'https://openalex.org/W4302344755']",2024-02-09
https://openalex.org/W4401246677,https://doi.org/10.1109/taslp.2024.3436618,SpeechPrompt: Prompting Speech Language Models for Speech Processing Tasks,"Prompting has become a practical method for utilizing pre-trained language\nmodels (LMs). This approach offers several advantages. It allows an LM to adapt\nto new tasks with minimal training and parameter updates, thus achieving\nefficiency in both storage and computation. Additionally, prompting modifies\nonly the LM's inputs and harnesses the generative capabilities of language\nmodels to address various downstream tasks in a unified manner. This\nsignificantly reduces the need for human labor in designing task-specific\nmodels. These advantages become even more evident as the number of tasks served\nby the LM scales up. Motivated by the strengths of prompting, we are the first\nto explore the potential of prompting speech LMs in the domain of speech\nprocessing. Recently, there has been a growing interest in converting speech\ninto discrete units for language modeling. Our pioneer research demonstrates\nthat these quantized speech units are highly versatile within our unified\nprompting framework. Not only can they serve as class labels, but they also\ncontain rich phonetic information that can be re-synthesized back into speech\nsignals for speech generation tasks. Specifically, we reformulate speech\nprocessing tasks into speech-to-unit generation tasks. As a result, we can\nseamlessly integrate tasks such as speech classification, sequence generation,\nand speech generation within a single, unified prompting framework. The\nexperiment results show that the prompting method can achieve competitive\nperformance compared to the strong fine-tuning method based on self-supervised\nlearning models with a similar number of trainable parameters. The prompting\nmethod also shows promising results in the few-shot setting. Moreover, with the\nadvanced speech LMs coming into the stage, the proposed prompting framework\nattains great potential.\n","['https://openalex.org/W4281492411', 'https://openalex.org/W3197580070', 'https://openalex.org/W3189296823', 'https://openalex.org/W4285250921', 'https://openalex.org/W3185341429', 'https://openalex.org/W3098267758', 'https://openalex.org/W6769627184', 'https://openalex.org/W3172642864', 'https://openalex.org/W3153427360', 'https://openalex.org/W3188542058', 'https://openalex.org/W4322766882', 'https://openalex.org/W4386187806', 'https://openalex.org/W4205991051', 'https://openalex.org/W4312651322', 'https://openalex.org/W4226162428', 'https://openalex.org/W6810313920', 'https://openalex.org/W6790356757', 'https://openalex.org/W3198217962', 'https://openalex.org/W4296070387', 'https://openalex.org/W3140429000', 'https://openalex.org/W4385822683', 'https://openalex.org/W2951082691', 'https://openalex.org/W3034999214', 'https://openalex.org/W4295308567', 'https://openalex.org/W4292825791', 'https://openalex.org/W4287887366', 'https://openalex.org/W3209059054', 'https://openalex.org/W6810673746', 'https://openalex.org/W3016181583', 'https://openalex.org/W6780218876', 'https://openalex.org/W3198608154', 'https://openalex.org/W3016011332', 'https://openalex.org/W3015265920', 'https://openalex.org/W3041561163', 'https://openalex.org/W4392909068', 'https://openalex.org/W4392909760', 'https://openalex.org/W2964243274', 'https://openalex.org/W6783867762', 'https://openalex.org/W4381786045', 'https://openalex.org/W6852781825', 'https://openalex.org/W6853530687', 'https://openalex.org/W4297841687', 'https://openalex.org/W4385822890', 'https://openalex.org/W6847363464', 'https://openalex.org/W6752946794', 'https://openalex.org/W4393157525', 'https://openalex.org/W6796456916', 'https://openalex.org/W4385823335', 'https://openalex.org/W4385245566', 'https://openalex.org/W4385567149', 'https://openalex.org/W4285286749', 'https://openalex.org/W6750665317', 'https://openalex.org/W3096251052', 'https://openalex.org/W3108231750', 'https://openalex.org/W3134187040', 'https://openalex.org/W6788231366', 'https://openalex.org/W2972584841', 'https://openalex.org/W6838356808', 'https://openalex.org/W6777335856', 'https://openalex.org/W3160747466', 'https://openalex.org/W6746278845', 'https://openalex.org/W1494198834', 'https://openalex.org/W3161223924', 'https://openalex.org/W3196509775', 'https://openalex.org/W4372260337', 'https://openalex.org/W6917585676', 'https://openalex.org/W2995181338', 'https://openalex.org/W2972495969', 'https://openalex.org/W3180374548', 'https://openalex.org/W4297841405', 'https://openalex.org/W4391021530', 'https://openalex.org/W4372270126', 'https://openalex.org/W4392902623', 'https://openalex.org/W4391021627', 'https://openalex.org/W6857054612', 'https://openalex.org/W4394671563', 'https://openalex.org/W4288089799', 'https://openalex.org/W2797583228']",2024-01-01
https://openalex.org/W4407782501,https://doi.org/10.1109/jbhi.2025.3543968,BAHBench: A Unified Benchmark for Evaluating Bio-Acoustic Health With Acoustic Foundation Models,"Acoustic foundation models, through self-supervised learning on large amounts of unlabeled speech data, can acquire rich acoustic representations. In recent years, these models have demonstrated substantial potential in audio-based health-related tasks, remarkably enhancing the efficiency and quality of healthcare services and contributing to the advancement of smart healthcare. However, there is currently a lack of systematic research and exploration on the performance of acoustic foundation models in health-related tasks. Furthermore, inconsistencies in evaluation methods and experimental setups hinder fair comparisons between different methods, severely impeding progress in this field. To address these challenges, we establish a unified Benchmark for evaluating Bio-Acoustic health via acoustic foundation models, namely BAHBench. BAHBench encompasses 6 distinct health-related tasks and evaluates 12 acoustic foundation models within a unified evaluation framework and parameter settings, enabling fair comparisons across different models. Our objective is to explore the effectiveness of current acoustic foundation models in health-related tasks. Thus, we discuss the impact of model size and data diversity on performance, and investigate feature selection and efficient fine-tuning strategy. Experimental results show that different health-related tasks benefit from features from different layers of the foundation model, while LoRA fine-tuning further enhances the model's performance on downstream tasks. Our goal is to provide clear and comprehensive guidance for future researchers. The code related to this study will be available to the research community to promote transparency and reproducibility.","['https://openalex.org/W4400579821', 'https://openalex.org/W4399489506', 'https://openalex.org/W4360994649', 'https://openalex.org/W3211062931', 'https://openalex.org/W4399154371', 'https://openalex.org/W4392902672', 'https://openalex.org/W4400229026', 'https://openalex.org/W4386248431', 'https://openalex.org/W4365135358', 'https://openalex.org/W3023585707', 'https://openalex.org/W4205860088', 'https://openalex.org/W3011285251', 'https://openalex.org/W3174249533', 'https://openalex.org/W4205687123', 'https://openalex.org/W4297841747', 'https://openalex.org/W4321608155', 'https://openalex.org/W4392903056', 'https://openalex.org/W3107962736', 'https://openalex.org/W4225319493', 'https://openalex.org/W2989852193', 'https://openalex.org/W4372260310', 'https://openalex.org/W6847363464', 'https://openalex.org/W2973049979', 'https://openalex.org/W4388766108', 'https://openalex.org/W3209984917', 'https://openalex.org/W4319878709', 'https://openalex.org/W6870254097', 'https://openalex.org/W4391827429', 'https://openalex.org/W4372260155', 'https://openalex.org/W3163826605', 'https://openalex.org/W4224315877', 'https://openalex.org/W3117926103', 'https://openalex.org/W4386996915', 'https://openalex.org/W3197580070', 'https://openalex.org/W4392902623', 'https://openalex.org/W4404708901', 'https://openalex.org/W4391865211', 'https://openalex.org/W6796581206', 'https://openalex.org/W4394862910', 'https://openalex.org/W6860853044', 'https://openalex.org/W4385245566', 'https://openalex.org/W2896457183', 'https://openalex.org/W6780218876', 'https://openalex.org/W3209059054', 'https://openalex.org/W4392904633', 'https://openalex.org/W3015591594', 'https://openalex.org/W2593116425', 'https://openalex.org/W4391604988', 'https://openalex.org/W4288437714', 'https://openalex.org/W3088067841', 'https://openalex.org/W6801976681', 'https://openalex.org/W1494198834', 'https://openalex.org/W2995181338', 'https://openalex.org/W3119308075', 'https://openalex.org/W3198694222', 'https://openalex.org/W6810007534', 'https://openalex.org/W2399733683', 'https://openalex.org/W6791353385', 'https://openalex.org/W4386223259', 'https://openalex.org/W2559143428', 'https://openalex.org/W2964182121', 'https://openalex.org/W4223646484', 'https://openalex.org/W2018363392', 'https://openalex.org/W3161639065', 'https://openalex.org/W2972301465', 'https://openalex.org/W2052666245', 'https://openalex.org/W3135028703', 'https://openalex.org/W2963652649', 'https://openalex.org/W4391547780', 'https://openalex.org/W3192599558']",2025-02-20
https://openalex.org/W4405547956,https://doi.org/10.32388/758n37,Benchmarking Open-ended Audio Dialogue Understanding for Large Audio-Language Models,"Large Audio-Language Models (LALMs) have unclocked audio dialogue capabilities, where audio dialogues are a direct exchange of spoken language between LALMs and humans. Recent advances, such as GPT-4o, have enabled LALMs in back-and-forth audio dialogues with humans. This progression not only underscores the potential of LALMs but also broadens their applicability across a wide range of practical scenarios supported by audio dialogues. However, given these advancements, a comprehensive benchmark to evaluate the performance of LALMs in the open-ended audio dialogue understanding remains absent currently. To address this gap, we propose an Audio Dialogue Understanding Benchmark (ADU-Bench), which consists of 4 benchmark datasets. They assess the open-ended audio dialogue ability for LALMs in 3 general scenarios, 12 skills, 9 multilingual languages, and 4 categories of ambiguity handling. Notably, we _firstly propose the evaluation of ambiguity handling in audio dialogues_ that expresses different intentions beyond the same literal meaning of sentences, e.g., “Really!?” with different intonations. In summary, ADU-Bench includes over 20,000 open-ended audio dialogues for the assessment of LALMs. Through extensive experiments conducted on 13 LALMs, our analysis reveals that there is still considerable room for improvement in the audio dialogue understanding abilities of existing LALMs. In particular, they struggle with mathematical symbols and formulas, understanding human behavior such as roleplay, comprehending multiple languages, and handling audio dialogue ambiguities from different phonetic elements, such as intonations, pause positions, and homophones.","['https://openalex.org/W4389524500', 'https://openalex.org/W4386552581', 'https://openalex.org/W4388718054', 'https://openalex.org/W4387891768', 'https://openalex.org/W4391021666', 'https://openalex.org/W4391591726', 'https://openalex.org/W4402683976', 'https://openalex.org/W4403556012', 'https://openalex.org/W4402386435', 'https://openalex.org/W3196509775', 'https://openalex.org/W4226444650', 'https://openalex.org/W2030931454', 'https://openalex.org/W2803193013', 'https://openalex.org/W4402684170', 'https://openalex.org/W6852874933', 'https://openalex.org/W4402671584', 'https://openalex.org/W6852447913', 'https://openalex.org/W6856794988', 'https://openalex.org/W4392902623', 'https://openalex.org/W2912924812', 'https://openalex.org/W4385568240', 'https://openalex.org/W4385571440', 'https://openalex.org/W2995929068', 'https://openalex.org/W6803096969', 'https://openalex.org/W4367000491', 'https://openalex.org/W6800166007', 'https://openalex.org/W3083410900', 'https://openalex.org/W2889787757', 'https://openalex.org/W3159959439', 'https://openalex.org/W4384918448', 'https://openalex.org/W1985975449', 'https://openalex.org/W4387321091', 'https://openalex.org/W3034302278', 'https://openalex.org/W3217804443', 'https://openalex.org/W4400375155', 'https://openalex.org/W4397028323', 'https://openalex.org/W4386076050', 'https://openalex.org/W4388726687', 'https://openalex.org/W4391157527', 'https://openalex.org/W4395687052', 'https://openalex.org/W4403571375', 'https://openalex.org/W4399554777']",2024-12-18
https://openalex.org/W4381786045,https://doi.org/10.1109/taslp.2023.3288409,AudioLM: A Language Modeling Approach to Audio Generation,"We introduce AudioLM, a framework for high-quality audio generation with long-term consistency. AudioLM maps the input audio to a sequence of discrete tokens and casts audio generation as a language modeling task in this representation space. We show how existing audio tokenizers provide different trade-offs between reconstruction quality and long-term structure, and we propose a hybrid tokenization scheme to achieve both objectives. Namely, we leverage the discretized activations of a masked language model pre-trained on audio to capture long-term structure and the discrete codes produced by a neural audio codec to achieve high-quality synthesis. By training on large corpora of raw audio waveforms, AudioLM learns to generate natural and coherent continuations given short prompts. When trained on speech, and without any transcript or annotation, AudioLM generates syntactically and semantically plausible speech continuations while also maintaining speaker identity and prosody for unseen speakers. Furthermore, we demonstrate how our approach extends beyond speech by generating coherent piano music continuations, despite being trained without any symbolic representation of music.","['https://openalex.org/W3197259906', 'https://openalex.org/W6840487619', 'https://openalex.org/W6839643428', 'https://openalex.org/W3160799772', 'https://openalex.org/W6739901393', 'https://openalex.org/W6809593508', 'https://openalex.org/W6790356757', 'https://openalex.org/W4292825791', 'https://openalex.org/W2995181338', 'https://openalex.org/W6734901337', 'https://openalex.org/W6802517614', 'https://openalex.org/W3148101939', 'https://openalex.org/W6810081322', 'https://openalex.org/W1494198834', 'https://openalex.org/W4226033575', 'https://openalex.org/W3215615641', 'https://openalex.org/W6755207826', 'https://openalex.org/W2395899413', 'https://openalex.org/W3037038648', 'https://openalex.org/W4287887366', 'https://openalex.org/W3198217962', 'https://openalex.org/W4296068815', 'https://openalex.org/W3097777922', 'https://openalex.org/W3180355996', 'https://openalex.org/W6810311916', 'https://openalex.org/W6776218486', 'https://openalex.org/W4312633146', 'https://openalex.org/W1728888090', 'https://openalex.org/W6778883912', 'https://openalex.org/W6782760101', 'https://openalex.org/W6783867762', 'https://openalex.org/W6767111847', 'https://openalex.org/W6783182287', 'https://openalex.org/W3095095816', 'https://openalex.org/W6783944145', 'https://openalex.org/W4226380987', 'https://openalex.org/W6771324808', 'https://openalex.org/W6798182279', 'https://openalex.org/W3144810982', 'https://openalex.org/W3162391496', 'https://openalex.org/W3209059054', 'https://openalex.org/W6769196770', 'https://openalex.org/W6748409065', 'https://openalex.org/W3131922516', 'https://openalex.org/W6769627184', 'https://openalex.org/W6735849998', 'https://openalex.org/W6762931180', 'https://openalex.org/W2972354707', 'https://openalex.org/W2963182577', 'https://openalex.org/W6843673214', 'https://openalex.org/W6805710207', 'https://openalex.org/W6800767084', 'https://openalex.org/W2752796333', 'https://openalex.org/W6768435317', 'https://openalex.org/W6844194202', 'https://openalex.org/W3186609711', 'https://openalex.org/W6780218876', 'https://openalex.org/W6755182157', 'https://openalex.org/W3198815374', 'https://openalex.org/W4292779060', 'https://openalex.org/W4283388932', 'https://openalex.org/W2604231067', 'https://openalex.org/W4221161768', 'https://openalex.org/W2896457183', 'https://openalex.org/W2996286887', 'https://openalex.org/W2519091744', 'https://openalex.org/W4224308101', 'https://openalex.org/W2995359496', 'https://openalex.org/W4288348042', 'https://openalex.org/W3123097577', 'https://openalex.org/W2963799213', 'https://openalex.org/W4287802874', 'https://openalex.org/W2950547518', 'https://openalex.org/W3129651364', 'https://openalex.org/W4385245566', 'https://openalex.org/W3092028330', 'https://openalex.org/W3177813494', 'https://openalex.org/W2979476256', 'https://openalex.org/W3206395542', 'https://openalex.org/W2593779438', 'https://openalex.org/W4226275767', 'https://openalex.org/W2970006822', 'https://openalex.org/W4309793872', 'https://openalex.org/W3036601975', 'https://openalex.org/W4225680573', 'https://openalex.org/W4394671563', 'https://openalex.org/W4288089799', 'https://openalex.org/W3198123200', 'https://openalex.org/W4294619240', 'https://openalex.org/W4298580827', 'https://openalex.org/W2971074500', 'https://openalex.org/W4297808394']",2023-01-01
https://openalex.org/W4386071707,https://doi.org/10.1109/cvpr52729.2023.01457,ImageBind One Embedding Space to Bind Them All,"We present ImageBind, an approach to learn a joint embedding across six different modalities - images, text, audio, depth, thermal, and IMU data. We show that all combinations of paired data are not necessary to train such a joint embedding, and only image-paired data is sufficient to bind the modalities together. ImageBind can leverage recent large scale vision-language models, and extends their zero-shot capabilities to new modalities just by using their natural pairing with images. It enables novel emergent applications 'out-of-the-box' including cross-modal retrieval, composing modalities with arithmetic, cross-modal detection and generation. The emergent capabilities improve with the strength of the image encoder and we set a new state-of-the-art on emergent zero-shot recognition tasks across modalities, outperforming specialist supervised models. Finally, we show strong few-shot recognition results outperforming prior work, and that ImageBind serves as a new way to evaluate vision models for visual and non-visual tasks.","['https://openalex.org/W6747225742', 'https://openalex.org/W6845452229', 'https://openalex.org/W6784333009', 'https://openalex.org/W6784184991', 'https://openalex.org/W6838789689', 'https://openalex.org/W6791353385', 'https://openalex.org/W6797148833', 'https://openalex.org/W2052666245', 'https://openalex.org/W2842511635', 'https://openalex.org/W3198452188', 'https://openalex.org/W3035682985', 'https://openalex.org/W6775244068', 'https://openalex.org/W6774314701', 'https://openalex.org/W6750591037', 'https://openalex.org/W6678470764', 'https://openalex.org/W2033875152', 'https://openalex.org/W6839264832', 'https://openalex.org/W2593116425', 'https://openalex.org/W6797613833', 'https://openalex.org/W4313190371', 'https://openalex.org/W3035635319', 'https://openalex.org/W6751037545', 'https://openalex.org/W2984008963', 'https://openalex.org/W6845288475', 'https://openalex.org/W6809576537', 'https://openalex.org/W6803602085', 'https://openalex.org/W6679792166', 'https://openalex.org/W6843151239', 'https://openalex.org/W3175300676', 'https://openalex.org/W3015371781', 'https://openalex.org/W2963524571', 'https://openalex.org/W4226442948', 'https://openalex.org/W6810640834', 'https://openalex.org/W2619697695', 'https://openalex.org/W3159481202', 'https://openalex.org/W3204588463', 'https://openalex.org/W3213454282', 'https://openalex.org/W6803872405', 'https://openalex.org/W6811433417', 'https://openalex.org/W2998508940', 'https://openalex.org/W4312818263', 'https://openalex.org/W6811072154', 'https://openalex.org/W3163937874', 'https://openalex.org/W6839165094', 'https://openalex.org/W6955071965', 'https://openalex.org/W4312658081', 'https://openalex.org/W6631516269', 'https://openalex.org/W3213472242', 'https://openalex.org/W6790019176', 'https://openalex.org/W2798991696', 'https://openalex.org/W2100031962', 'https://openalex.org/W6843263998', 'https://openalex.org/W2550821151', 'https://openalex.org/W2425121537', 'https://openalex.org/W6780294235', 'https://openalex.org/W6810334672', 'https://openalex.org/W6745388339', 'https://openalex.org/W3205475937', 'https://openalex.org/W6739901393', 'https://openalex.org/W6788135285', 'https://openalex.org/W6838673894', 'https://openalex.org/W3216270236', 'https://openalex.org/W2067912884', 'https://openalex.org/W6763442200', 'https://openalex.org/W6802517928', 'https://openalex.org/W1923184257', 'https://openalex.org/W1565402342', 'https://openalex.org/W6810265253', 'https://openalex.org/W4312372834', 'https://openalex.org/W125693051', 'https://openalex.org/W6802987763', 'https://openalex.org/W3205786327', 'https://openalex.org/W2149557440', 'https://openalex.org/W3196974791', 'https://openalex.org/W4312777209', 'https://openalex.org/W2138621090', 'https://openalex.org/W6809885388', 'https://openalex.org/W2117539524', 'https://openalex.org/W6782298681', 'https://openalex.org/W2134670479', 'https://openalex.org/W3206072662', 'https://openalex.org/W4298393544', 'https://openalex.org/W1527575280', 'https://openalex.org/W4301914798', 'https://openalex.org/W4221167396', 'https://openalex.org/W3176799298', 'https://openalex.org/W4312685069', 'https://openalex.org/W4312558481', 'https://openalex.org/W3094502228', 'https://openalex.org/W2796992393', 'https://openalex.org/W4386071576', 'https://openalex.org/W3108655343', 'https://openalex.org/W3005680577', 'https://openalex.org/W4385245566', 'https://openalex.org/W4281702809', 'https://openalex.org/W3215626407', 'https://openalex.org/W4225323055', 'https://openalex.org/W4285483774', 'https://openalex.org/W4295990387', 'https://openalex.org/W2123024445', 'https://openalex.org/W3170874841', 'https://openalex.org/W2774267535', 'https://openalex.org/W3166396011', 'https://openalex.org/W4295124836', 'https://openalex.org/W4309435465', 'https://openalex.org/W3176445421', 'https://openalex.org/W3174906557', 'https://openalex.org/W4224035735', 'https://openalex.org/W4280490805', 'https://openalex.org/W3209532394', 'https://openalex.org/W4226452378', 'https://openalex.org/W4229042118', 'https://openalex.org/W3122640483', 'https://openalex.org/W3037309139', 'https://openalex.org/W3126337491', 'https://openalex.org/W2619947201', 'https://openalex.org/W2963703197', 'https://openalex.org/W4312424618', 'https://openalex.org/W3081167590', 'https://openalex.org/W4297808394', 'https://openalex.org/W4312741694', 'https://openalex.org/W3010094231']",2023-06-01
https://openalex.org/W3197324626,https://doi.org/10.21437/interspeech.2021-1757,PnG BERT: Augmented BERT on Phonemes and Graphemes for Neural TTS,"This paper introduces PnG BERT, a new encoder model for neural TTS.This model is augmented from the original BERT model, by taking both phoneme and grapheme representations of text as input, as well as the word-level alignment between them.It can be pre-trained on a large text corpus in a selfsupervised manner, and fine-tuned in a TTS task.Experimental results show that a neural TTS model using a pre-trained PnG BERT as its encoder yields more natural prosody and more accurate pronunciation than a baseline model using only phoneme input with no pre-training.Subjective side-by-side preference evaluations show that raters have no statistically significant preference between the speech synthesized using a PnG BERT and ground truth recordings from professional speakers.","['https://openalex.org/W3163339651', 'https://openalex.org/W2952744660', 'https://openalex.org/W3081488690', 'https://openalex.org/W2591927543', 'https://openalex.org/W2808706139', 'https://openalex.org/W3161296985', 'https://openalex.org/W2914120296', 'https://openalex.org/W2901997113', 'https://openalex.org/W2619368999', 'https://openalex.org/W2103085228', 'https://openalex.org/W2519091744', 'https://openalex.org/W3161782335', 'https://openalex.org/W2963250244', 'https://openalex.org/W4299287062', 'https://openalex.org/W2946200149', 'https://openalex.org/W4385245566', 'https://openalex.org/W2121879602', 'https://openalex.org/W2896457183', 'https://openalex.org/W3091928890', 'https://openalex.org/W2963827314', 'https://openalex.org/W3033411150', 'https://openalex.org/W4287547221', 'https://openalex.org/W2964243274', 'https://openalex.org/W2962784628', 'https://openalex.org/W2794490148', 'https://openalex.org/W2866343820', 'https://openalex.org/W2963609956', 'https://openalex.org/W4298580827', 'https://openalex.org/W2907916773', 'https://openalex.org/W2903739847']",2021-08-27
https://openalex.org/W4319862635,https://doi.org/10.1109/slt54892.2023.10023141,FLEURS: FEW-Shot Learning Evaluation of Universal Representations of Speech,"We introduce FLEURS, the Few-shot Learning Evaluation of Universal Representations of Speech benchmark. FLEURS is an n-way parallel speech dataset in 102 languages built on top of the machine translation FLoRes-101 benchmark, with approximately 12 hours of speech supervision per language. FLEURS can be used for a variety of speech tasks, including Automatic Speech Recognition (ASR), Speech Language Identification (Speech LangID), Speech-Text Retrieval. In this paper, we provide baselines for the tasks based on multilingual pre-trained models like speech-only w2v-BERT [1] and speech-text multimodal mSLAM [2]. The goal of FLEURS is to enable speech technology in more languages and catalyze research in low-resource speech understanding. <sup xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">1</sup> .","['https://openalex.org/W4226033575', 'https://openalex.org/W6810259195', 'https://openalex.org/W3097777922', 'https://openalex.org/W6780218876', 'https://openalex.org/W3198429080', 'https://openalex.org/W3204696009', 'https://openalex.org/W4210463634', 'https://openalex.org/W3096215352', 'https://openalex.org/W1494198834', 'https://openalex.org/W3160525311', 'https://openalex.org/W3213029956', 'https://openalex.org/W3095410713', 'https://openalex.org/W3119308075', 'https://openalex.org/W3196509775', 'https://openalex.org/W6771467084', 'https://openalex.org/W2743686029', 'https://openalex.org/W3015698636', 'https://openalex.org/W3092085609', 'https://openalex.org/W3197771105', 'https://openalex.org/W6810701745', 'https://openalex.org/W3139878283', 'https://openalex.org/W2937197076', 'https://openalex.org/W3169369929', 'https://openalex.org/W27049869', 'https://openalex.org/W6784577980', 'https://openalex.org/W2064675550', 'https://openalex.org/W2127141656', 'https://openalex.org/W3015877095', 'https://openalex.org/W2914699162', 'https://openalex.org/W2105981609', 'https://openalex.org/W1984076147', 'https://openalex.org/W3213148312', 'https://openalex.org/W6803675045', 'https://openalex.org/W2965538726', 'https://openalex.org/W3039695075', 'https://openalex.org/W2220457451', 'https://openalex.org/W3169483174', 'https://openalex.org/W4226444650', 'https://openalex.org/W4221155340', 'https://openalex.org/W3213018012']",2023-01-09
https://openalex.org/W38194800,https://doi.org/10.21437/interspeech.2004-668,From WER and RIL to MER and WIL: improved evaluation measures for connected speech recognition,"The word error rate (WER), commonly used in ASR assessment, measures the cost of restoring the output word sequence to the original input sequence. However, for most CSR applications apart from dictation machines a more meaningful performance measure would be given by the proportion of information communicated. In this article we introduce two new absolute CSR performance measures: MER (match error rate) and WIL (word information lost). MER is the proportion of I/O word matches which are errors. WIL is a simple approximation to the proportion of word information lost which overcomes the problems associated with the RIL (relative information lost) measure that was proposed half a century ago. Issues relating to ideal performance measurement are discussed and the commonly used Viterbi input/output alignment procedure, with zero weight for hits and equal weight for substitutions, deletions and insertions, is shown to be optimal.","['https://openalex.org/W1667165204', 'https://openalex.org/W4285719527', 'https://openalex.org/W1548235210', 'https://openalex.org/W1970533835', 'https://openalex.org/W2087617622', 'https://openalex.org/W2157477135']",2004-10-04
https://openalex.org/W4319862232,https://doi.org/10.1109/slt54892.2023.10023323,NAM+: Towards Scalable End-to-End Contextual Biasing for Adaptive ASR,"Attention-based biasing techniques for end-to-end ASR systems are able to achieve large accuracy gains without requiring the inference algorithm adjustments and parameter tuning common to fusion approaches. However, it is challenging to simultaneously scale up attention-based biasing to realistic numbers of biased phrases; maintain in-domain WER gains, while minimizing out-of-domain losses; and run in real time. We present NAM+, an attention-based biasing approach which achieves a 16X inference speedup per acoustic frame over prior work when run with 3,000 biasing entities, as measured on a typical mobile CPU. NAM+ achieves these run-time gains through a combination of Two-Pass Hierarchical Attention and Dilated Context Update. Compared to the adapted baseline, NAM+ further decreases the in-domain WER by up to 12.6% relative, while incurring an out-of-domain WER regression of 20% relative. Compared to the non-adapted baseline, the out-of-domain WER regression is 7.1 % relative.","['https://openalex.org/W2403440562', 'https://openalex.org/W2293829681', 'https://openalex.org/W2972625221', 'https://openalex.org/W2889012072', 'https://openalex.org/W2886319145', 'https://openalex.org/W6679434410', 'https://openalex.org/W6638749077', 'https://openalex.org/W3097794466', 'https://openalex.org/W4225985539', 'https://openalex.org/W3016010032', 'https://openalex.org/W3202725408', 'https://openalex.org/W3097777922', 'https://openalex.org/W4225289150', 'https://openalex.org/W4226292626', 'https://openalex.org/W2971840980', 'https://openalex.org/W3201225328', 'https://openalex.org/W4225307083', 'https://openalex.org/W3197976839', 'https://openalex.org/W4385245566', 'https://openalex.org/W6631190155', 'https://openalex.org/W6760633627', 'https://openalex.org/W2953212265', 'https://openalex.org/W2963122170', 'https://openalex.org/W3163203022', 'https://openalex.org/W6843673214', 'https://openalex.org/W2617258110', 'https://openalex.org/W2928941594', 'https://openalex.org/W2133564696']",2023-01-09
https://openalex.org/W2912924812,https://doi.org/10.1162/tacl_a_00276,Natural Questions: A Benchmark for Question Answering Research,"We present the Natural Questions corpus, a question answering data set. Questions consist of real anonymized, aggregated queries issued to the Google search engine. An annotator is presented with a question along with a Wikipedia page from the top 5 search results, and annotates a long answer (typically a paragraph) and a short answer (one or more entities) if present on the page, or marks null if no long/short answer is present. The public release consists of 307,373 training examples with single annotations; 7,830 examples with 5-way annotations for development data; and a further 7,842 examples with 5-way annotated sequestered as test data. We present experiments validating quality of the data. We also describe analysis of 25-way annotations on 302 examples, giving insights into human variability on the annotation task. We introduce robust metrics for the purposes of evaluating question answering systems; demonstrate high human upper bounds on these metrics; and establish baseline results using competitive methods drawn from related literature.","['https://openalex.org/W1840435438', 'https://openalex.org/W2962985038', 'https://openalex.org/W2888302696', 'https://openalex.org/W2962718483', 'https://openalex.org/W2963957489', 'https://openalex.org/W2963339397', 'https://openalex.org/W2963963993', 'https://openalex.org/W2890894339', 'https://openalex.org/W2963681467', 'https://openalex.org/W2963015836', 'https://openalex.org/W2413794162', 'https://openalex.org/W2963323070', 'https://openalex.org/W2963748441', 'https://openalex.org/W2251818205', 'https://openalex.org/W2889787757', 'https://openalex.org/W2964267515', 'https://openalex.org/W2558203065', 'https://openalex.org/W2951534261', 'https://openalex.org/W2888296173', 'https://openalex.org/W2101105183', 'https://openalex.org/W2607303097', 'https://openalex.org/W2962809918', 'https://openalex.org/W2125436846', 'https://openalex.org/W2913059114', 'https://openalex.org/W1593271688', 'https://openalex.org/W2068737686', 'https://openalex.org/W4238634189', 'https://openalex.org/W1564947197', 'https://openalex.org/W1516184288', 'https://openalex.org/W2963846996', 'https://openalex.org/W2963969878', 'https://openalex.org/W2949615363', 'https://openalex.org/W2911430044', 'https://openalex.org/W2950501607']",2019-08-02
https://openalex.org/W4385573964,https://doi.org/10.18653/v1/2022.emnlp-main.630,Overcoming Catastrophic Forgetting in Zero-Shot Cross-Lingual Generation,"In this paper, we explore the challenging problem of performing a generative task in a target language when labeled data is only available in English, using summarization as a case study. We assume a strict setting with no access to parallel data or machine translation and find that common transfer learning approaches struggle in this setting, as a generative multilingual model fine-tuned purely on English catastrophically forgets how to generate non-English. Given the recent rise of parameter-efficient adaptation techniques, we conduct the first investigation into how one such method, prompt tuning (Lester et al., 2021), can overcome catastrophic forgetting to enable zero-shot cross-lingual generation. Our experiments show that parameter-efficient prompt tuning provides gains over standard fine-tuning when transferring between less-related languages, e.g., from English to Thai. However, a significant gap still remains between these methods and fully-supervised baselines. To improve cross-lingual transfer further, we explore several approaches, including: (1) mixing in unlabeled multilingual data, and (2) explicitly factoring prompts into recombinable language and task components. Our approaches can provide further quality gains, suggesting that robust zero-shot cross-lingual generation is within reach.","['https://openalex.org/W3093517588', 'https://openalex.org/W2963026768', 'https://openalex.org/W4287827771', 'https://openalex.org/W3034469191', 'https://openalex.org/W4287775202', 'https://openalex.org/W3171961701', 'https://openalex.org/W3205949070', 'https://openalex.org/W4288089799', 'https://openalex.org/W3201363102', 'https://openalex.org/W3214173179', 'https://openalex.org/W3205068155', 'https://openalex.org/W2963250244', 'https://openalex.org/W2998653236', 'https://openalex.org/W3035390927', 'https://openalex.org/W4286987939', 'https://openalex.org/W3035579820', 'https://openalex.org/W3205616434', 'https://openalex.org/W2560647685', 'https://openalex.org/W3186655327', 'https://openalex.org/W4322614701', 'https://openalex.org/W3035497479', 'https://openalex.org/W4280534475', 'https://openalex.org/W3169483174', 'https://openalex.org/W2896457183', 'https://openalex.org/W4206529673', 'https://openalex.org/W4320086632', 'https://openalex.org/W3035252911', 'https://openalex.org/W4287122891', 'https://openalex.org/W2914120296', 'https://openalex.org/W3137010024', 'https://openalex.org/W3175759677', 'https://openalex.org/W2116522068', 'https://openalex.org/W3205717164', 'https://openalex.org/W4297801719', 'https://openalex.org/W3175575876', 'https://openalex.org/W3176828726', 'https://openalex.org/W2154652894', 'https://openalex.org/W2131296021', 'https://openalex.org/W2891555348', 'https://openalex.org/W2963211188', 'https://openalex.org/W2606974598', 'https://openalex.org/W3098068947', 'https://openalex.org/W4224442590', 'https://openalex.org/W3174770825', 'https://openalex.org/W2742113707', 'https://openalex.org/W4205991051', 'https://openalex.org/W3168867926', 'https://openalex.org/W2964303773', 'https://openalex.org/W4300963525', 'https://openalex.org/W2970752815', 'https://openalex.org/W4221151920', 'https://openalex.org/W3106445907', 'https://openalex.org/W3101498587']",2022-01-01
https://openalex.org/W3168867926,https://doi.org/10.48550/arxiv.2106.09685,LoRA: Low-Rank Adaptation of Large Language Models,"An important paradigm of natural language processing consists of large-scale pre-training on general domain data and adaptation to particular tasks or domains. As we pre-train larger models, full fine-tuning, which retrains all model parameters, becomes less feasible. Using GPT-3 175B as an example -- deploying independent instances of fine-tuned models, each with 175B parameters, is prohibitively expensive. We propose Low-Rank Adaptation, or LoRA, which freezes the pre-trained model weights and injects trainable rank decomposition matrices into each layer of the Transformer architecture, greatly reducing the number of trainable parameters for downstream tasks. Compared to GPT-3 175B fine-tuned with Adam, LoRA can reduce the number of trainable parameters by 10,000 times and the GPU memory requirement by 3 times. LoRA performs on-par or better than fine-tuning in model quality on RoBERTa, DeBERTa, GPT-2, and GPT-3, despite having fewer trainable parameters, a higher training throughput, and, unlike adapters, no additional inference latency. We also provide an empirical investigation into rank-deficiency in language model adaptation, which sheds light on the efficacy of LoRA. We release a package that facilitates the integration of LoRA with PyTorch models and provide our implementations and model checkpoints for RoBERTa, DeBERTa, and GPT-2 at https://github.com/microsoft/LoRA.","['https://openalex.org/W3037180580', 'https://openalex.org/W2964035320', 'https://openalex.org/W2949960976', 'https://openalex.org/W2963403868', 'https://openalex.org/W2090958120', 'https://openalex.org/W2786660442', 'https://openalex.org/W2899748887', 'https://openalex.org/W3039127676', 'https://openalex.org/W3030163527', 'https://openalex.org/W2908510526', 'https://openalex.org/W1967077133', 'https://openalex.org/W2251939518', 'https://openalex.org/W3033187248', 'https://openalex.org/W2971169274', 'https://openalex.org/W2964121744', 'https://openalex.org/W2952899695', 'https://openalex.org/W2616957565', 'https://openalex.org/W3152956381', 'https://openalex.org/W2950967261', 'https://openalex.org/W3214715529', 'https://openalex.org/W3166140588', 'https://openalex.org/W3174784402', 'https://openalex.org/W2058641082', 'https://openalex.org/W2949819354', 'https://openalex.org/W2751448157', 'https://openalex.org/W2798836595', 'https://openalex.org/W3028525609', 'https://openalex.org/W2732004306', 'https://openalex.org/W3176828726', 'https://openalex.org/W3119438769', 'https://openalex.org/W2126017757', 'https://openalex.org/W2888867175', 'https://openalex.org/W2117130368', 'https://openalex.org/W2964031251', 'https://openalex.org/W3098824823', 'https://openalex.org/W3104033643', 'https://openalex.org/W3000127803', 'https://openalex.org/W2963477238', 'https://openalex.org/W3118216348', 'https://openalex.org/W2965373594', 'https://openalex.org/W131533222', 'https://openalex.org/W2103972604', 'https://openalex.org/W3181384422', 'https://openalex.org/W2990704537', 'https://openalex.org/W2973727699', 'https://openalex.org/W2394882406', 'https://openalex.org/W3158697128', 'https://openalex.org/W2963341956', 'https://openalex.org/W2963846996', 'https://openalex.org/W2886067286', 'https://openalex.org/W3040573126', 'https://openalex.org/W2913946806', 'https://openalex.org/W3139080614', 'https://openalex.org/W2806120502', 'https://openalex.org/W2899063268', 'https://openalex.org/W2595741664', 'https://openalex.org/W2963310665']",2021-06-17
https://openalex.org/W3091928890,https://doi.org/10.48550/arxiv.2010.04301,Non-Attentive Tacotron: Robust and Controllable Neural TTS Synthesis Including Unsupervised Duration Modeling,"This paper presents Non-Attentive Tacotron based on the Tacotron 2 text-to-speech model, replacing the attention mechanism with an explicit duration predictor. This improves robustness significantly as measured by unaligned duration ratio and word deletion rate, two metrics introduced in this paper for large-scale robustness evaluation using a pre-trained speech recognition model. With the use of Gaussian upsampling, Non-Attentive Tacotron achieves a 5-scale mean opinion score for naturalness of 4.41, slightly outperforming Tacotron 2. The duration predictor enables both utterance-wide and per-phoneme control of duration at inference time. When accurate target durations are scarce or unavailable in the training data, we propose a method using a fine-grained variational auto-encoder to train the duration predictor in a semi-supervised or unsupervised manner, with results almost as good as supervised training.","['https://openalex.org/W2963403868', 'https://openalex.org/W1902237438', 'https://openalex.org/W2129142580', 'https://openalex.org/W3123097577', 'https://openalex.org/W2970006822', 'https://openalex.org/W2963590452', 'https://openalex.org/W3015922793', 'https://openalex.org/W3024747869', 'https://openalex.org/W2972951102', 'https://openalex.org/W2327501763', 'https://openalex.org/W2963927338', 'https://openalex.org/W2964243274', 'https://openalex.org/W2963432880', 'https://openalex.org/W1810943226', 'https://openalex.org/W2972359262', 'https://openalex.org/W2963609956', 'https://openalex.org/W2995181338', 'https://openalex.org/W2402737981', 'https://openalex.org/W1494198834', 'https://openalex.org/W2971753973', 'https://openalex.org/W2945544731', 'https://openalex.org/W2970730223', 'https://openalex.org/W3048487650', 'https://openalex.org/W2963300588', 'https://openalex.org/W2952269766', 'https://openalex.org/W3033913438', 'https://openalex.org/W2886769154', 'https://openalex.org/W3016021263', 'https://openalex.org/W3025013833', 'https://openalex.org/W2949382160', 'https://openalex.org/W2767206889', 'https://openalex.org/W2901997113', 'https://openalex.org/W2016589492', 'https://openalex.org/W3025793647', 'https://openalex.org/W2972495969', 'https://openalex.org/W2964307104', 'https://openalex.org/W2972970915', 'https://openalex.org/W1959608418', 'https://openalex.org/W3016136182', 'https://openalex.org/W2964281804', 'https://openalex.org/W2964308564', 'https://openalex.org/W3026041220', 'https://openalex.org/W2972702018', 'https://openalex.org/W3130016944', 'https://openalex.org/W2969521066', 'https://openalex.org/W854541894', 'https://openalex.org/W2964138190', 'https://openalex.org/W2963691546', 'https://openalex.org/W2903739847', 'https://openalex.org/W2963712897', 'https://openalex.org/W3093733783', 'https://openalex.org/W2102003408']",2020-10-08
https://openalex.org/W4377372369,https://doi.org/10.48550/arxiv.2305.11834,Pengi: An Audio Language Model for Audio Tasks,"In the domain of audio processing, Transfer Learning has facilitated the rise of Self-Supervised Learning and Zero-Shot Learning techniques. These approaches have led to the development of versatile models capable of tackling a wide array of tasks, while delivering state-of-the-art performance. However, current models inherently lack the capacity to produce the requisite language for open-ended tasks, such as Audio Captioning or Audio Question &amp; Answering. We introduce Pengi, a novel Audio Language Model that leverages Transfer Learning by framing all audio tasks as text-generation tasks. It takes as input, an audio recording, and text, and generates free-form text as output. The input audio is represented as a sequence of continuous embeddings by an audio encoder. A text encoder does the same for the corresponding text input. Both sequences are combined as a prefix to prompt a pre-trained frozen language model. The unified architecture of Pengi enables open-ended tasks and close-ended tasks without any additional fine-tuning or task-specific extensions. When evaluated on 22 downstream tasks, our approach yields state-of-the-art performance in several of them. Our results show that connecting language models with audio models is a major step towards general-purpose audio understanding",[],2023-05-19
https://openalex.org/W3139918052,https://doi.org/10.48550/arxiv.2104.02133,SpeechStew: Simply Mix All Available Speech Recognition Data to Train One Large Neural Network,"We present SpeechStew, a speech recognition model that is trained on a combination of various publicly available speech recognition datasets: AMI, Broadcast News, Common Voice, LibriSpeech, Switchboard/Fisher, Tedlium, and Wall Street Journal. SpeechStew simply mixes all of these datasets together, without any special re-weighting or re-balancing of the datasets. SpeechStew achieves SoTA or near SoTA results across a variety of tasks, without the use of an external language model. Our results include 9.0\% WER on AMI-IHM, 4.7\% WER on Switchboard, 8.3\% WER on CallHome, and 1.3\% on WSJ, which significantly outperforms prior work with strong external language models. We also demonstrate that SpeechStew learns powerful transfer learning representations. We fine-tune SpeechStew on a noisy low resource speech dataset, CHiME-6. We achieve 38.9\% WER without a language model, which compares to 38.6\% WER to a strong HMM baseline with a language model.","['https://openalex.org/W2155893237', 'https://openalex.org/W2168231600', 'https://openalex.org/W1494198834', 'https://openalex.org/W2962911098', 'https://openalex.org/W3098903812', 'https://openalex.org/W2155541015', 'https://openalex.org/W2102113734', 'https://openalex.org/W2963742216', 'https://openalex.org/W3030437843', 'https://openalex.org/W2936774411', 'https://openalex.org/W2995181338', 'https://openalex.org/W3131736947', 'https://openalex.org/W3036601975', 'https://openalex.org/W2996603747', 'https://openalex.org/W2327501763', 'https://openalex.org/W3026041220', 'https://openalex.org/W2125336414', 'https://openalex.org/W3015726069', 'https://openalex.org/W3100859887', 'https://openalex.org/W1828163288', 'https://openalex.org/W2963122170', 'https://openalex.org/W2108677974', 'https://openalex.org/W2250357346', 'https://openalex.org/W2900212944', 'https://openalex.org/W3094841848', 'https://openalex.org/W2123798005', 'https://openalex.org/W3099782249', 'https://openalex.org/W3093579165', 'https://openalex.org/W2973049979', 'https://openalex.org/W3016400019', 'https://openalex.org/W3035445001', 'https://openalex.org/W3097217077', 'https://openalex.org/W2804935296', 'https://openalex.org/W3097777922', 'https://openalex.org/W3093502935', 'https://openalex.org/W3001279689', 'https://openalex.org/W2964002616', 'https://openalex.org/W2530876040', 'https://openalex.org/W3101648800', 'https://openalex.org/W2991213871', 'https://openalex.org/W3149390355', 'https://openalex.org/W2163605009', 'https://openalex.org/W2962824709', 'https://openalex.org/W2889282842', 'https://openalex.org/W2939690918', 'https://openalex.org/W2963403868', 'https://openalex.org/W3147962056', 'https://openalex.org/W3018441253', 'https://openalex.org/W2963341956', 'https://openalex.org/W3131505732', 'https://openalex.org/W3015995734', 'https://openalex.org/W2194775991', 'https://openalex.org/W1978660892', 'https://openalex.org/W1524333225']",2021-04-05
https://openalex.org/W4288089799,https://doi.org/10.48550/arxiv.1910.10683,Exploring the Limits of Transfer Learning with a Unified Text-to-Text\n Transformer,"Transfer learning, where a model is first pre-trained on a data-rich task\nbefore being fine-tuned on a downstream task, has emerged as a powerful\ntechnique in natural language processing (NLP). The effectiveness of transfer\nlearning has given rise to a diversity of approaches, methodology, and\npractice. In this paper, we explore the landscape of transfer learning\ntechniques for NLP by introducing a unified framework that converts all\ntext-based language problems into a text-to-text format. Our systematic study\ncompares pre-training objectives, architectures, unlabeled data sets, transfer\napproaches, and other factors on dozens of language understanding tasks. By\ncombining the insights from our exploration with scale and our new ``Colossal\nClean Crawled Corpus'', we achieve state-of-the-art results on many benchmarks\ncovering summarization, question answering, text classification, and more. To\nfacilitate future work on transfer learning for NLP, we release our data set,\npre-trained models, and code.\n",[],2019-10-23
https://openalex.org/W3169483174,https://doi.org/10.18653/v1/2021.naacl-main.41,mT5: A Massively Multilingual Pre-trained Text-to-Text Transformer,"Linting Xue, Noah Constant, Adam Roberts, Mihir Kale, Rami Al-Rfou, Aditya Siddhant, Aditya Barua, Colin Raffel. Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. 2021.","['https://openalex.org/W3105220303', 'https://openalex.org/W3039017601', 'https://openalex.org/W3126822054', 'https://openalex.org/W3098637735', 'https://openalex.org/W2963748441', 'https://openalex.org/W3032816972', 'https://openalex.org/W2965373594', 'https://openalex.org/W3034999214', 'https://openalex.org/W3023133114', 'https://openalex.org/W2970752815', 'https://openalex.org/W3040245432', 'https://openalex.org/W2989539713', 'https://openalex.org/W3013840636', 'https://openalex.org/W2963026768', 'https://openalex.org/W3085479580', 'https://openalex.org/W4287727940', 'https://openalex.org/W4389016403', 'https://openalex.org/W3006439205', 'https://openalex.org/W2996580882', 'https://openalex.org/W3082274269', 'https://openalex.org/W3045462440', 'https://openalex.org/W2999168658', 'https://openalex.org/W4287687023', 'https://openalex.org/W3102659883', 'https://openalex.org/W3035390927', 'https://openalex.org/W4293350112', 'https://openalex.org/W3105721709', 'https://openalex.org/W2742113707', 'https://openalex.org/W2963341956', 'https://openalex.org/W4287633642', 'https://openalex.org/W3107826490', 'https://openalex.org/W3035497479', 'https://openalex.org/W2891555348', 'https://openalex.org/W2914120296', 'https://openalex.org/W2963979492', 'https://openalex.org/W2963250244', 'https://openalex.org/W3103187652', 'https://openalex.org/W3032532958', 'https://openalex.org/W3037854022', 'https://openalex.org/W2958953787', 'https://openalex.org/W3034238904', 'https://openalex.org/W2809324505', 'https://openalex.org/W3023690688', 'https://openalex.org/W3116343068', 'https://openalex.org/W2990188683', 'https://openalex.org/W2986154550', 'https://openalex.org/W3042711927', 'https://openalex.org/W4287760320', 'https://openalex.org/W2963403868', 'https://openalex.org/W3099655892', 'https://openalex.org/W3081210419', 'https://openalex.org/W3034469191', 'https://openalex.org/W4301581299', 'https://openalex.org/W3100107515', 'https://openalex.org/W2953958347', 'https://openalex.org/W3097879195', 'https://openalex.org/W4385245566', 'https://openalex.org/W2940024477', 'https://openalex.org/W3156789018']",2021-01-01
https://openalex.org/W4221155340,https://doi.org/10.48550/arxiv.2202.01374,mSLAM: Massively multilingual joint pre-training for speech and text,"We present mSLAM, a multilingual Speech and LAnguage Model that learns cross-lingual cross-modal representations of speech and text by pre-training jointly on large amounts of unlabeled speech and text in multiple languages. mSLAM combines w2v-BERT pre-training on speech with SpanBERT pre-training on character-level text, along with Connectionist Temporal Classification (CTC) losses on paired speech and transcript data, to learn a single model capable of learning from and representing both speech and text signals in a shared representation space. We evaluate mSLAM on several downstream speech understanding tasks and find that joint pre-training with text improves quality on speech translation, speech intent classification and speech language-ID while being competitive on multilingual ASR, when compared against speech-only pre-training. Our speech translation model demonstrates zero-shot text translation without seeing any text translation data, providing evidence for cross-modal alignment of representations. mSLAM also benefits from multi-modal fine-tuning, further improving the quality of speech translation by directly leveraging text translation data during the fine-tuning process. Our empirical analysis highlights several opportunities and challenges arising from large-scale multimodal pre-training, suggesting directions for future research.",[],2022-02-03
https://openalex.org/W4377130946,https://doi.org/10.48550/arxiv.2305.10790,"Listen, Think, and Understand","The ability of artificial intelligence (AI) systems to perceive and comprehend audio signals is crucial for many applications. Although significant progress has been made in this area since the development of AudioSet, most existing models are designed to map audio inputs to pre-defined, discrete sound label sets. In contrast, humans possess the ability to not only classify sounds into general categories, but also to listen to the finer details of the sounds, explain the reason for the predictions, think about what the sound infers, and understand the scene and what action needs to be taken, if any. Such capabilities beyond perception are not yet present in existing audio models. On the other hand, modern large language models (LLMs) exhibit emerging reasoning ability but they lack audio perception capabilities. Therefore, we ask the question: can we build a model that has both audio perception and a reasoning ability? In this paper, we propose a new audio foundation model, called LTU (Listen, Think, and Understand). To train LTU, we created a new OpenAQA-5M dataset consisting of 1.9 million closed-ended and 3.7 million open-ended, diverse (audio, question, answer) tuples, and have used an autoregressive training framework with a perception-to-understanding curriculum. LTU demonstrates strong performance and generalization ability on conventional audio tasks such as classification and captioning. More importantly, it exhibits emerging audio reasoning and comprehension abilities that are absent in existing audio models. To the best of our knowledge, LTU is one of the first multimodal large language models that focus on general audio (rather than just speech) understanding.",[],2023-05-18
https://openalex.org/W4381827575,https://doi.org/10.48550/arxiv.2306.12925,AudioPaLM: A Large Language Model That Can Speak and Listen,"We introduce AudioPaLM, a large language model for speech understanding and generation. AudioPaLM fuses text-based and speech-based language models, PaLM-2 [Anil et al., 2023] and AudioLM [Borsos et al., 2022], into a unified multimodal architecture that can process and generate text and speech with applications including speech recognition and speech-to-speech translation. AudioPaLM inherits the capability to preserve paralinguistic information such as speaker identity and intonation from AudioLM and the linguistic knowledge present only in text large language models such as PaLM-2. We demonstrate that initializing AudioPaLM with the weights of a text-only large language model improves speech processing, successfully leveraging the larger quantity of text training data used in pretraining to assist with the speech tasks. The resulting model significantly outperforms existing systems for speech translation tasks and has the ability to perform zero-shot speech-to-text translation for many languages for which input/target language combinations were not seen in training. AudioPaLM also demonstrates features of audio language models, such as transferring a voice across languages based on a short spoken prompt. We release examples of our method at https://google-research.github.io/seanet/audiopalm/examples",[],2023-06-22
https://openalex.org/W2842511635,,Representation Learning with Contrastive Predictive Coding,"While supervised learning has enabled great progress in many applications, unsupervised learning has not seen such widespread adoption, and remains an important and challenging endeavor for artificial intelligence. In this work, we propose a universal unsupervised learning approach to extract useful representations from high-dimensional data, which we call Contrastive Predictive Coding. The key insight of our model is to learn such representations by predicting the future in latent space by using powerful autoregressive models. We use a probabilistic contrastive loss which induces the latent space to capture information that is maximally useful to predict future samples. It also makes the model tractable by using negative sampling. While most prior work has focused on evaluating representations for a particular modality, we demonstrate that our approach is able to learn useful representations achieving strong performance on four distinct domains: speech, images, text and reinforcement learning in 3D environments.","['https://openalex.org/W2412320034', 'https://openalex.org/W2187089797', 'https://openalex.org/W2160660844', 'https://openalex.org/W2152808281', 'https://openalex.org/W2964127152', 'https://openalex.org/W2163605009', 'https://openalex.org/W2302255633', 'https://openalex.org/W2014902591', 'https://openalex.org/W343636949', 'https://openalex.org/W2157331557', 'https://openalex.org/W2950726992', 'https://openalex.org/W1494198834', 'https://openalex.org/W2106053110', 'https://openalex.org/W2160815625', 'https://openalex.org/W2606347107', 'https://openalex.org/W1522301498', 'https://openalex.org/W2321533354', 'https://openalex.org/W2963403868', 'https://openalex.org/W2127958135', 'https://openalex.org/W3099206234', 'https://openalex.org/W219040644', 'https://openalex.org/W2964043796', 'https://openalex.org/W2326925005', 'https://openalex.org/W1895577753', 'https://openalex.org/W2259472270', 'https://openalex.org/W2786036274', 'https://openalex.org/W2952186591', 'https://openalex.org/W1836465849', 'https://openalex.org/W2949382160', 'https://openalex.org/W2962824366', 'https://openalex.org/W3037932933', 'https://openalex.org/W2114524997', 'https://openalex.org/W1681397005', 'https://openalex.org/W2130942839', 'https://openalex.org/W2070246124', 'https://openalex.org/W2037034710', 'https://openalex.org/W2950577311', 'https://openalex.org/W2112129677', 'https://openalex.org/W2119885245', 'https://openalex.org/W2950797609', 'https://openalex.org/W2963762683', 'https://openalex.org/W2146444479', 'https://openalex.org/W2152790380', 'https://openalex.org/W2131744502', 'https://openalex.org/W3189092450', 'https://openalex.org/W1566289585', 'https://openalex.org/W1524333225', 'https://openalex.org/W2949536664', 'https://openalex.org/W2157364932', 'https://openalex.org/W2117539524']",2018-07-10
https://openalex.org/W4281492411,https://doi.org/10.1109/jstsp.2022.3207050,Self-Supervised Speech Representation Learning: A Review,"Although supervised deep learning has revolutionized speech and audio\nprocessing, it has necessitated the building of specialist models for\nindividual tasks and application scenarios. It is likewise difficult to apply\nthis to dialects and languages for which only limited labeled data is\navailable. Self-supervised representation learning methods promise a single\nuniversal model that would benefit a wide variety of tasks and domains. Such\nmethods have shown success in natural language processing and computer vision\ndomains, achieving new levels of performance while reducing the number of\nlabels required for many downstream scenarios. Speech representation learning\nis experiencing similar progress in three main categories: generative,\ncontrastive, and predictive methods. Other approaches rely on multi-modal data\nfor pre-training, mixing text or visual data streams with speech. Although\nself-supervised speech representation is still a nascent research area, it is\nclosely related to acoustic word embedding and learning with zero lexical\nresources, both of which have seen active research for many years. This review\npresents approaches for self-supervised speech representation learning and\ntheir connection to other research areas. Since many current methods focus\nsolely on automatic speech recognition as a downstream task, we review recent\nefforts on benchmarking learned representations to extend the application\nbeyond speech recognition.\n","['https://openalex.org/W2919115771', 'https://openalex.org/W2160815625', 'https://openalex.org/W811578723', 'https://openalex.org/W1555037511', 'https://openalex.org/W1975113979', 'https://openalex.org/W2110073835', 'https://openalex.org/W2124537004', 'https://openalex.org/W6683825394', 'https://openalex.org/W2163922914', 'https://openalex.org/W1901616594', 'https://openalex.org/W4244017338', 'https://openalex.org/W4239390603', 'https://openalex.org/W6675401909', 'https://openalex.org/W1902027874', 'https://openalex.org/W2100495367', 'https://openalex.org/W6800751262', 'https://openalex.org/W3207924272', 'https://openalex.org/W3035725276', 'https://openalex.org/W6773996589', 'https://openalex.org/W3185341429', 'https://openalex.org/W6784023748', 'https://openalex.org/W3011574394', 'https://openalex.org/W3023371261', 'https://openalex.org/W6811170316', 'https://openalex.org/W6772230580', 'https://openalex.org/W1703050006', 'https://openalex.org/W2048648518', 'https://openalex.org/W2100969003', 'https://openalex.org/W1979447841', 'https://openalex.org/W1877570817', 'https://openalex.org/W6680522077', 'https://openalex.org/W1487784522', 'https://openalex.org/W2155230809', 'https://openalex.org/W2150769028', 'https://openalex.org/W2408021097', 'https://openalex.org/W6681096077', 'https://openalex.org/W6680106237', 'https://openalex.org/W2145889472', 'https://openalex.org/W2113606819', 'https://openalex.org/W2067474491', 'https://openalex.org/W6736430770', 'https://openalex.org/W2116064496', 'https://openalex.org/W2923014074', 'https://openalex.org/W3197580070', 'https://openalex.org/W2326925005', 'https://openalex.org/W2883725317', 'https://openalex.org/W343636949', 'https://openalex.org/W6640963894', 'https://openalex.org/W6639732818', 'https://openalex.org/W3217536461', 'https://openalex.org/W2962739339', 'https://openalex.org/W6767997687', 'https://openalex.org/W2896457183', 'https://openalex.org/W6766673545', 'https://openalex.org/W6844194202', 'https://openalex.org/W3035524453', 'https://openalex.org/W6774670964', 'https://openalex.org/W6779997284', 'https://openalex.org/W2962907457', 'https://openalex.org/W3026041220', 'https://openalex.org/W3096338464', 'https://openalex.org/W6811088048', 'https://openalex.org/W2913340405', 'https://openalex.org/W2291975472', 'https://openalex.org/W3112702554', 'https://openalex.org/W4226380987', 'https://openalex.org/W2973157397', 'https://openalex.org/W6617744952', 'https://openalex.org/W6712395597', 'https://openalex.org/W2962850167', 'https://openalex.org/W6745117592', 'https://openalex.org/W6757193177', 'https://openalex.org/W2752796333', 'https://openalex.org/W3140429000', 'https://openalex.org/W3198217962', 'https://openalex.org/W6790356757', 'https://openalex.org/W6690026940', 'https://openalex.org/W2097012520', 'https://openalex.org/W1945356021', 'https://openalex.org/W3100270690', 'https://openalex.org/W6729448088', 'https://openalex.org/W2972867623', 'https://openalex.org/W2035424729', 'https://openalex.org/W2020607164', 'https://openalex.org/W2396043527', 'https://openalex.org/W1545920196', 'https://openalex.org/W1796128977', 'https://openalex.org/W2932675979', 'https://openalex.org/W2972943112', 'https://openalex.org/W3016011332', 'https://openalex.org/W2020883660', 'https://openalex.org/W2146444479', 'https://openalex.org/W3035202887', 'https://openalex.org/W3097286738', 'https://openalex.org/W3015265920', 'https://openalex.org/W2982223350', 'https://openalex.org/W6769238691', 'https://openalex.org/W6778265221', 'https://openalex.org/W3003875258', 'https://openalex.org/W3160345865', 'https://openalex.org/W3196919915', 'https://openalex.org/W3041561163', 'https://openalex.org/W3198858531', 'https://openalex.org/W3096485810', 'https://openalex.org/W3196798358', 'https://openalex.org/W6674330103', 'https://openalex.org/W3015213852', 'https://openalex.org/W2963425185', 'https://openalex.org/W6840487619', 'https://openalex.org/W3015949486', 'https://openalex.org/W2982039329', 'https://openalex.org/W2963571336', 'https://openalex.org/W3148040514', 'https://openalex.org/W2963317665', 'https://openalex.org/W2973049979', 'https://openalex.org/W3016181583', 'https://openalex.org/W3102342027', 'https://openalex.org/W6769196770', 'https://openalex.org/W6780218876', 'https://openalex.org/W3197411683', 'https://openalex.org/W4226033575', 'https://openalex.org/W3198608154', 'https://openalex.org/W3015356564', 'https://openalex.org/W3209059054', 'https://openalex.org/W3209984917', 'https://openalex.org/W6810007534', 'https://openalex.org/W6810673746', 'https://openalex.org/W6677884823', 'https://openalex.org/W6682948231', 'https://openalex.org/W2124509324', 'https://openalex.org/W6762931180', 'https://openalex.org/W3159481202', 'https://openalex.org/W1536680647', 'https://openalex.org/W6766978945', 'https://openalex.org/W6600971220', 'https://openalex.org/W2096391593', 'https://openalex.org/W1974783905', 'https://openalex.org/W2091863306', 'https://openalex.org/W4237938692', 'https://openalex.org/W142945732', 'https://openalex.org/W2594690981', 'https://openalex.org/W6810168380', 'https://openalex.org/W2016538560', 'https://openalex.org/W4396724964', 'https://openalex.org/W2296654356', 'https://openalex.org/W6686207219', 'https://openalex.org/W900447646', 'https://openalex.org/W6606244218', 'https://openalex.org/W4237723258', 'https://openalex.org/W6631216910', 'https://openalex.org/W6639103823', 'https://openalex.org/W6728094556', 'https://openalex.org/W6693697572', 'https://openalex.org/W1484147505', 'https://openalex.org/W2130055251', 'https://openalex.org/W2049252044', 'https://openalex.org/W6678885645', 'https://openalex.org/W1531883353', 'https://openalex.org/W6633682082', 'https://openalex.org/W2136189984', 'https://openalex.org/W4297841641', 'https://openalex.org/W3157861865', 'https://openalex.org/W6864391120', 'https://openalex.org/W6729977899', 'https://openalex.org/W2962862718', 'https://openalex.org/W2971709506', 'https://openalex.org/W3197828817', 'https://openalex.org/W3200287550', 'https://openalex.org/W2988907666', 'https://openalex.org/W3196698946', 'https://openalex.org/W3205715971', 'https://openalex.org/W6809593508', 'https://openalex.org/W6770596778', 'https://openalex.org/W2586148577', 'https://openalex.org/W2964115348', 'https://openalex.org/W2963902314', 'https://openalex.org/W4224875474', 'https://openalex.org/W3095293218', 'https://openalex.org/W2963330681', 'https://openalex.org/W2920166246', 'https://openalex.org/W2895651543', 'https://openalex.org/W2973135958', 'https://openalex.org/W6803092890', 'https://openalex.org/W1577418252', 'https://openalex.org/W1496120315', 'https://openalex.org/W2962980711', 'https://openalex.org/W2964169922', 'https://openalex.org/W6720204814', 'https://openalex.org/W2296681920', 'https://openalex.org/W2059652594', 'https://openalex.org/W2889313720', 'https://openalex.org/W2963720603', 'https://openalex.org/W6786885278', 'https://openalex.org/W2407151108', 'https://openalex.org/W2190506272', 'https://openalex.org/W3037530970', 'https://openalex.org/W3201254286', 'https://openalex.org/W3150635893', 'https://openalex.org/W2995181338', 'https://openalex.org/W2593116425', 'https://openalex.org/W4289665794', 'https://openalex.org/W6603931906', 'https://openalex.org/W1494198834', 'https://openalex.org/W2024490156', 'https://openalex.org/W6771467084', 'https://openalex.org/W3095410713', 'https://openalex.org/W3119308075', 'https://openalex.org/W6696449567', 'https://openalex.org/W3213029956', 'https://openalex.org/W3206252155', 'https://openalex.org/W3198694222', 'https://openalex.org/W2799473636', 'https://openalex.org/W6691509046', 'https://openalex.org/W2166637769', 'https://openalex.org/W3139878283', 'https://openalex.org/W1526236009', 'https://openalex.org/W2963242190', 'https://openalex.org/W2963127222', 'https://openalex.org/W2884797218', 'https://openalex.org/W6712941328', 'https://openalex.org/W2883409523', 'https://openalex.org/W6936113694', 'https://openalex.org/W2726515241', 'https://openalex.org/W2972584841', 'https://openalex.org/W1567520911', 'https://openalex.org/W6748215858', 'https://openalex.org/W3196509775', 'https://openalex.org/W2094544353', 'https://openalex.org/W6727418883', 'https://openalex.org/W6712757354', 'https://openalex.org/W6731521493', 'https://openalex.org/W6688816777', 'https://openalex.org/W2883595988', 'https://openalex.org/W6750665317', 'https://openalex.org/W2775794021', 'https://openalex.org/W6736723571', 'https://openalex.org/W2972894903', 'https://openalex.org/W3033038061', 'https://openalex.org/W942963634', 'https://openalex.org/W3197223534', 'https://openalex.org/W6784614252', 'https://openalex.org/W6753575415', 'https://openalex.org/W3160554450', 'https://openalex.org/W6803378298', 'https://openalex.org/W3189296823', 'https://openalex.org/W3093096176', 'https://openalex.org/W6809947431', 'https://openalex.org/W3006926732', 'https://openalex.org/W4225713393', 'https://openalex.org/W3096216486', 'https://openalex.org/W3095361818', 'https://openalex.org/W2251253014', 'https://openalex.org/W6795952400', 'https://openalex.org/W2970820321', 'https://openalex.org/W6801828775', 'https://openalex.org/W3198815374', 'https://openalex.org/W3024182269', 'https://openalex.org/W4224934179', 'https://openalex.org/W3096017728', 'https://openalex.org/W3162133897', 'https://openalex.org/W6784614126', 'https://openalex.org/W2786608204', 'https://openalex.org/W3207558756', 'https://openalex.org/W3198771897', 'https://openalex.org/W3198429080', 'https://openalex.org/W2962799225', 'https://openalex.org/W2802557066', 'https://openalex.org/W6735913928', 'https://openalex.org/W6751433836', 'https://openalex.org/W6744957266', 'https://openalex.org/W2899134946', 'https://openalex.org/W6750365303', 'https://openalex.org/W6757699909', 'https://openalex.org/W2962799131', 'https://openalex.org/W6738077056', 'https://openalex.org/W6760519848', 'https://openalex.org/W3214697273', 'https://openalex.org/W6678282225', 'https://openalex.org/W4319862670', 'https://openalex.org/W6745740328', 'https://openalex.org/W6745388339', 'https://openalex.org/W3204917342', 'https://openalex.org/W2962699523', 'https://openalex.org/W2963739817', 'https://openalex.org/W3015280134', 'https://openalex.org/W2963796886', 'https://openalex.org/W2963581463', 'https://openalex.org/W2972889948', 'https://openalex.org/W2119717200', 'https://openalex.org/W2046932483', 'https://openalex.org/W6640059789', 'https://openalex.org/W2577366047', 'https://openalex.org/W2964243274', 'https://openalex.org/W6756385397', 'https://openalex.org/W2940200615', 'https://openalex.org/W3008480565', 'https://openalex.org/W3024464021', 'https://openalex.org/W3016008406', 'https://openalex.org/W2883586237', 'https://openalex.org/W2964012862', 'https://openalex.org/W3097632072', 'https://openalex.org/W2963216553', 'https://openalex.org/W2025482506', 'https://openalex.org/W2963620343', 'https://openalex.org/W2940544976', 'https://openalex.org/W6786696081', 'https://openalex.org/W2347098582', 'https://openalex.org/W2787447541', 'https://openalex.org/W2972374322', 'https://openalex.org/W3197381195', 'https://openalex.org/W6803066952', 'https://openalex.org/W4307680525', 'https://openalex.org/W4221146627', 'https://openalex.org/W6805530253', 'https://openalex.org/W6777028661', 'https://openalex.org/W4287854499', 'https://openalex.org/W6759579507', 'https://openalex.org/W3176828726', 'https://openalex.org/W6787411158', 'https://openalex.org/W4225274946', 'https://openalex.org/W4226162428', 'https://openalex.org/W6796551075', 'https://openalex.org/W3203140070', 'https://openalex.org/W6685943813', 'https://openalex.org/W3137147200', 'https://openalex.org/W3085139254', 'https://openalex.org/W3198039885', 'https://openalex.org/W6839738141', 'https://openalex.org/W6803547063', 'https://openalex.org/W4297841871', 'https://openalex.org/W3214576767', 'https://openalex.org/W4296068785', 'https://openalex.org/W3209376089', 'https://openalex.org/W4221140371', 'https://openalex.org/W4292825791', 'https://openalex.org/W3101648800', 'https://openalex.org/W4200635400', 'https://openalex.org/W3088409176', 'https://openalex.org/W4286918540', 'https://openalex.org/W2102409316', 'https://openalex.org/W3211224152', 'https://openalex.org/W4288348042', 'https://openalex.org/W2981991061', 'https://openalex.org/W2998249245', 'https://openalex.org/W3009561768', 'https://openalex.org/W2242818861', 'https://openalex.org/W2997574889', 'https://openalex.org/W1515020792', 'https://openalex.org/W2965373594', 'https://openalex.org/W2530846021', 'https://openalex.org/W4297808394', 'https://openalex.org/W1915251500', 'https://openalex.org/W4295116917', 'https://openalex.org/W2964303773', 'https://openalex.org/W3026842484', 'https://openalex.org/W2125290066', 'https://openalex.org/W2219249508', 'https://openalex.org/W2095705004', 'https://openalex.org/W22517275', 'https://openalex.org/W4221145109', 'https://openalex.org/W4287591426', 'https://openalex.org/W2898727538', 'https://openalex.org/W2797583228', 'https://openalex.org/W3195577433', 'https://openalex.org/W3099142230', 'https://openalex.org/W3107298252', 'https://openalex.org/W3213873715', 'https://openalex.org/W2973026522', 'https://openalex.org/W4289750118', 'https://openalex.org/W4394671563', 'https://openalex.org/W3207222250', 'https://openalex.org/W3024605872', 'https://openalex.org/W3161411634', 'https://openalex.org/W2973727699', 'https://openalex.org/W4320013820', 'https://openalex.org/W1508165687', 'https://openalex.org/W2138204974']",2022-09-15
https://openalex.org/W4226162428,https://doi.org/10.21437/interspeech.2022-10610,An Exploration of Prompt Tuning on Generative Spoken Language Model for Speech Processing Tasks,"Speech representations learned from Self-supervised learning (SSL) models can benefit various speech processing tasks. However, utilizing SSL representations usually requires fine-tuning the pre-trained models or designing task-specific downstream models and loss functions, causing much memory usage and human labor. Recently, prompting in Natural Language Processing (NLP) has been found to be an efficient technique to leverage pre-trained language models (LMs). Specifically, prompt tuning optimizes a limited number of task-specific parameters with a fixed pre-trained model; as a result, only a small set of parameters is needed to be stored for each task. Prompt tuning improves computation and memory efficiency by leveraging the pre-trained LM's prediction ability. Nevertheless, such a paradigm is little studied in the speech community. We report in this paper the first exploration of the prompt tuning paradigm for speech processing tasks based on Generative Spoken Language Model (GSLM). Experiment results show that the prompt tuning technique achieves competitive performance in speech classification tasks with fewer trainable parameters than fine-tuning specialized downstream models. We further study the technique in challenging sequence generation tasks. Prompt tuning also demonstrates its potential, while the limitation and possible research directions are discussed in this paper. The source code is available on https://github.com/ga642381/SpeechPrompt.",[],2022-09-16
https://openalex.org/W3174770825,https://doi.org/10.18653/v1/2021.acl-long.353,Prefix-Tuning: Optimizing Continuous Prompts for Generation,"Xiang Lisa Li, Percy Liang. Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers). 2021.","['https://openalex.org/W2970419734', 'https://openalex.org/W3107826490', 'https://openalex.org/W4285719527', 'https://openalex.org/W3020268419', 'https://openalex.org/W3166846774', 'https://openalex.org/W2988937804', 'https://openalex.org/W4292779060', 'https://openalex.org/W3098267758', 'https://openalex.org/W3034999214', 'https://openalex.org/W3098824823', 'https://openalex.org/W3170806096', 'https://openalex.org/W4205991051', 'https://openalex.org/W3023285645', 'https://openalex.org/W3044438666', 'https://openalex.org/W3002104146', 'https://openalex.org/W3039127676', 'https://openalex.org/W3153427360', 'https://openalex.org/W2963341956', 'https://openalex.org/W3153675281', 'https://openalex.org/W2133512280', 'https://openalex.org/W3006381853', 'https://openalex.org/W2929900303', 'https://openalex.org/W2936695845', 'https://openalex.org/W3023622314', 'https://openalex.org/W2283463896', 'https://openalex.org/W2973049837', 'https://openalex.org/W2994928925', 'https://openalex.org/W2997195635', 'https://openalex.org/W2963211188', 'https://openalex.org/W4288480287', 'https://openalex.org/W2971147102', 'https://openalex.org/W2149327368', 'https://openalex.org/W2970785793', 'https://openalex.org/W4297795751', 'https://openalex.org/W2101105183', 'https://openalex.org/W3035252911', 'https://openalex.org/W2888482885', 'https://openalex.org/W3085190015', 'https://openalex.org/W3139080614', 'https://openalex.org/W3118216348', 'https://openalex.org/W2979826702', 'https://openalex.org/W3153805297', 'https://openalex.org/W2965373594', 'https://openalex.org/W3082274269', 'https://openalex.org/W2908510526', 'https://openalex.org/W1763968285', 'https://openalex.org/W2996403597', 'https://openalex.org/W3152956381', 'https://openalex.org/W2053637704', 'https://openalex.org/W1956340063', 'https://openalex.org/W3115894062', 'https://openalex.org/W3103616906', 'https://openalex.org/W3035050380', 'https://openalex.org/W2956352737', 'https://openalex.org/W3023528699', 'https://openalex.org/W2154652894', 'https://openalex.org/W3174784402', 'https://openalex.org/W3177323791', 'https://openalex.org/W2964303773', 'https://openalex.org/W2786660442']",2021-01-01
https://openalex.org/W4285247752,https://doi.org/10.18653/v1/2022.acl-short.8,P-Tuning: Prompt Tuning Can Be Comparable to Fine-tuning Across Scales and Tasks,"Prompt tuning, which only tunes continuous prompts with a frozen language model, substantially reduces per-task storage and memory usage at training. However, in the context of NLU, prior work reveals that prompt tuning does not perform well for normal-sized pretrained models. We also find that existing methods of prompt tuning cannot handle hard sequence labeling tasks, indicating a lack of universality. We present a novel empirical finding that properly optimized prompt tuning can be universally effective across a wide range of model scales and NLU tasks. It matches the performance of finetuning while having only 0.1%-3% tuned parameters. Our method P-Tuning v2 is an implementation of Deep Prompt Tuning (CITATION) optimized and adapted for NLU. Given the universality and simplicity of P-Tuning v2, we believe it can serve as an alternative to finetuning and a strong baseline for future research.","['https://openalex.org/W3166986030', 'https://openalex.org/W4287026929', 'https://openalex.org/W3172642864', 'https://openalex.org/W2896457183', 'https://openalex.org/W2952087486', 'https://openalex.org/W3173777717', 'https://openalex.org/W3166846774', 'https://openalex.org/W4205991051', 'https://openalex.org/W2970597249', 'https://openalex.org/W3159084885', 'https://openalex.org/W4288089799', 'https://openalex.org/W4229506649', 'https://openalex.org/W2923014074', 'https://openalex.org/W4286961470', 'https://openalex.org/W3174770825', 'https://openalex.org/W3137214022', 'https://openalex.org/W2963748441', 'https://openalex.org/W3200814992', 'https://openalex.org/W2943552823', 'https://openalex.org/W3154560120', 'https://openalex.org/W4297795751', 'https://openalex.org/W3098267758', 'https://openalex.org/W2155069789', 'https://openalex.org/W3122890974', 'https://openalex.org/W4286981949', 'https://openalex.org/W3173783648', 'https://openalex.org/W4292779060', 'https://openalex.org/W2915816387', 'https://openalex.org/W2965373594']",2022-01-01
https://openalex.org/W3098267758,https://doi.org/10.18653/v1/2020.emnlp-main.346,AutoPrompt: Eliciting Knowledge from Language Models with Automatically Generated Prompts,"The remarkable success of pretrained language models has motivated the study of what kinds of knowledge these models learn during pretraining. Reformulating tasks as fill-in-the-blanks problems (e.g., cloze tests) is a natural approach for gauging such knowledge, however, its usage is limited by the manual effort and guesswork required to write suitable prompts. To address this, we develop AutoPrompt, an automated method to create prompts for a diverse set of tasks, based on a gradient-guided search. Using AutoPrompt, we show that masked language models (MLMs) have an inherent capability to perform sentiment analysis and natural language inference without additional parameters or finetuning, sometimes achieving performance on par with recent state-of-the-art supervised models. We also show that our prompts elicit more accurate factual knowledge from MLMs than the manually created prompts on the LAMA benchmark, and that MLMs can be used as relation extractors more effectively than supervised relation extraction models. These results demonstrate that automatically generated prompts are a viable parameter-free alternative to existing probing methods, and as pretrained LMs become more sophisticated and capable, potentially a replacement for finetuning.","['https://openalex.org/W2785611959', 'https://openalex.org/W3104235057', 'https://openalex.org/W2964303116', 'https://openalex.org/W4287867774', 'https://openalex.org/W2982756474', 'https://openalex.org/W3005700362', 'https://openalex.org/W2923014074', 'https://openalex.org/W2970862333', 'https://openalex.org/W2980282514', 'https://openalex.org/W2805206884', 'https://openalex.org/W2998557616', 'https://openalex.org/W3104499181', 'https://openalex.org/W2130158090', 'https://openalex.org/W3044438666', 'https://openalex.org/W2963430447', 'https://openalex.org/W2963310665', 'https://openalex.org/W2934842096', 'https://openalex.org/W2250790822', 'https://openalex.org/W1840435438', 'https://openalex.org/W2251939518', 'https://openalex.org/W4292779060', 'https://openalex.org/W2982906145', 'https://openalex.org/W2951025380', 'https://openalex.org/W2970476646', 'https://openalex.org/W3153427360', 'https://openalex.org/W2799124508', 'https://openalex.org/W2970726176', 'https://openalex.org/W2896457183', 'https://openalex.org/W2962739339', 'https://openalex.org/W2963341956', 'https://openalex.org/W2756566873']",2020-01-01
https://openalex.org/W4319862642,https://doi.org/10.1109/slt54892.2023.10023274,Exploring Efficient-Tuning Methods in Self-Supervised Speech Models,"In this study, we aim to explore efficient tuning methods for speech self-supervised learning. Recent studies show that self-supervised learning (SSL) can learn powerful representations for different speech tasks. However, fine-tuning pre-trained models for each downstream task is parameter-inefficient since SSL models are notoriously large with millions of parameters. Adapters are lightweight modules commonly used in NLP to solve this problem. In downstream tasks, the parameters of SSL models are frozen, and only the adapters are trained. Given the lack of studies generally exploring the effectiveness of adapters for self-supervised speech tasks, we intend to fill this gap by adding various adapter modules in pre-trained speech SSL models. We show that the performance parity can be achieved with over 90% parameter reduction, and discussed the pros and cons of efficient tuning techniques. This is the first comprehensive investigation of various adapter types across speech tasks.","['https://openalex.org/W4281492411', 'https://openalex.org/W3209059054', 'https://openalex.org/W3197580070', 'https://openalex.org/W6759579507', 'https://openalex.org/W3174770825', 'https://openalex.org/W3200396895', 'https://openalex.org/W3101498587', 'https://openalex.org/W4225274946', 'https://openalex.org/W6780218876', 'https://openalex.org/W6786669483', 'https://openalex.org/W6778883912', 'https://openalex.org/W6796581206', 'https://openalex.org/W4225410153', 'https://openalex.org/W3176693010', 'https://openalex.org/W4283073456', 'https://openalex.org/W4224930323', 'https://openalex.org/W4226162428', 'https://openalex.org/W6790356757', 'https://openalex.org/W3176828726', 'https://openalex.org/W6802744804', 'https://openalex.org/W2127141656', 'https://openalex.org/W1494198834', 'https://openalex.org/W4226380987', 'https://openalex.org/W4394671563', 'https://openalex.org/W3036601975', 'https://openalex.org/W4292779060', 'https://openalex.org/W3112034174', 'https://openalex.org/W3168867926']",2023-01-09
https://openalex.org/W4225410153,https://doi.org/10.18653/v1/2022.findings-naacl.199,AdapterBias: Parameter-efficient Token-dependent Representation Shift for Adapters in NLP Tasks,"Transformer-based pre-trained models with millions of parameters require large storage. Recent approaches tackle this shortcoming by training adapters, but these approaches still require a relatively large number of parameters. In this study, AdapterBias, a surprisingly simple yet effective adapter architecture, is proposed. AdapterBias adds a token-dependent shift to the hidden output of transformer layers to adapt to downstream tasks with only a vector and a linear layer. Extensive experiments are conducted to demonstrate the effectiveness of AdapterBias. The experiments show that our proposed method can dramatically reduce the trainable parameters compared to the previous works with a minimal decrease in task performances compared with fine-tuned pre-trained models. We further find that AdapterBias automatically learns to assign more significant representation shifts to the tokens related to the task in consideration.","['https://openalex.org/W6631349028', 'https://openalex.org/W4322588812', 'https://openalex.org/W2251939518', 'https://openalex.org/W2908510526', 'https://openalex.org/W3168867926', 'https://openalex.org/W4288410857', 'https://openalex.org/W3174702398', 'https://openalex.org/W2948947170', 'https://openalex.org/W2963828549', 'https://openalex.org/W2963854351', 'https://openalex.org/W2970120757', 'https://openalex.org/W4253067820', 'https://openalex.org/W2742079690', 'https://openalex.org/W3176828726', 'https://openalex.org/W4287122891', 'https://openalex.org/W4385245566', 'https://openalex.org/W3153675281', 'https://openalex.org/W2799124508', 'https://openalex.org/W2547875792', 'https://openalex.org/W2923014074', 'https://openalex.org/W2548228487', 'https://openalex.org/W4313908941', 'https://openalex.org/W2965373594', 'https://openalex.org/W3099793224', 'https://openalex.org/W4206178588', 'https://openalex.org/W2896457183', 'https://openalex.org/W2964303773', 'https://openalex.org/W2963846996', 'https://openalex.org/W2946417913', 'https://openalex.org/W2970925270', 'https://openalex.org/W2980282514', 'https://openalex.org/W2978670439', 'https://openalex.org/W2963748441']",2022-01-01
https://openalex.org/W3176693010,https://doi.org/10.18653/v1/2021.acl-long.172,On the Effectiveness of Adapter-based Tuning for Pretrained Language Model Adaptation,"Ruidan He, Linlin Liu, Hai Ye, Qingyu Tan, Bosheng Ding, Liying Cheng, Jiawei Low, Lidong Bing, Luo Si. Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers). 2021.","['https://openalex.org/W2963403868', 'https://openalex.org/W1682403713', 'https://openalex.org/W3007685714', 'https://openalex.org/W4322614701', 'https://openalex.org/W2971033911', 'https://openalex.org/W2948337921', 'https://openalex.org/W2973047874', 'https://openalex.org/W4287692509', 'https://openalex.org/W2964067969', 'https://openalex.org/W2898700502', 'https://openalex.org/W2964352358', 'https://openalex.org/W2777662428', 'https://openalex.org/W3034469191', 'https://openalex.org/W2963026768', 'https://openalex.org/W2975185270', 'https://openalex.org/W2965373594', 'https://openalex.org/W3127622310', 'https://openalex.org/W4206178588', 'https://openalex.org/W2946296745', 'https://openalex.org/W4287867774', 'https://openalex.org/W2963310665', 'https://openalex.org/W2970925270', 'https://openalex.org/W3006647218', 'https://openalex.org/W2060277733', 'https://openalex.org/W4292779060', 'https://openalex.org/W3034199299', 'https://openalex.org/W3003289092', 'https://openalex.org/W2964121744', 'https://openalex.org/W3034238904', 'https://openalex.org/W2896457183', 'https://openalex.org/W2980708516', 'https://openalex.org/W3175604467', 'https://openalex.org/W2980282514', 'https://openalex.org/W2912811302', 'https://openalex.org/W2911300548', 'https://openalex.org/W4385245566', 'https://openalex.org/W2963341956', 'https://openalex.org/W2995998574', 'https://openalex.org/W3104215796', 'https://openalex.org/W2915774325', 'https://openalex.org/W3023528699', 'https://openalex.org/W3099793224', 'https://openalex.org/W3035204084', 'https://openalex.org/W1522301498', 'https://openalex.org/W3035579820', 'https://openalex.org/W2915977242', 'https://openalex.org/W3153675281', 'https://openalex.org/W2923014074', 'https://openalex.org/W2891555348', 'https://openalex.org/W3126074026', 'https://openalex.org/W3120490999', 'https://openalex.org/W3005700362', 'https://openalex.org/W3093345276', 'https://openalex.org/W3023911605', 'https://openalex.org/W2742113707', 'https://openalex.org/W2964303773', 'https://openalex.org/W2962933129', 'https://openalex.org/W3005441132', 'https://openalex.org/W3100311862', 'https://openalex.org/W3094491777', 'https://openalex.org/W3101498587', 'https://openalex.org/W3035390927', 'https://openalex.org/W2009284521', 'https://openalex.org/W4322588812', 'https://openalex.org/W3103368673']",2021-01-01
https://openalex.org/W4393178509,https://doi.org/10.1609/aaai.v38i21.30570,"AudioGPT: Understanding and Generating Speech, Music, Sound, and Talking Head","Large language models (LLMs) have exhibited remarkable capabilities across a variety of domains and tasks, challenging our understanding of learning and cognition. Despite the recent success, current LLMs are not capable of processing complex audio information or conducting spoken conversations (like Siri or Alexa). In this work, we propose a multi-modal AI system named AudioGPT, which complements LLMs (i.e., ChatGPT) with 1) foundation models to process complex audio information and solve numerous understanding and generation tasks; and 2) the input/output interface (ASR, TTS) to support spoken dialogue. With an increasing demand to evaluate multi-modal LLMs of human intention understanding and cooperation with foundation models, we outline the principles and processes and test AudioGPT in terms of consistency, capability, and robustness. Experimental results demonstrate the capabilities of AudioGPT in solving 16 AI tasks with speech, music, sound, and talking head understanding and generation in multi-round dialogues, which empower humans to create rich and diverse audio content with unprecedented ease. Code can be found in https://github.com/AIGC-Audio/AudioGPT","['https://openalex.org/W3158405307', 'https://openalex.org/W3169320628', 'https://openalex.org/W3205398360', 'https://openalex.org/W3158762648', 'https://openalex.org/W2952218014', 'https://openalex.org/W6792340124', 'https://openalex.org/W3208601549', 'https://openalex.org/W4297841486', 'https://openalex.org/W3207340675', 'https://openalex.org/W3206857696', 'https://openalex.org/W4226419874', 'https://openalex.org/W4287184558', 'https://openalex.org/W4318718996', 'https://openalex.org/W4224871700', 'https://openalex.org/W4323717348', 'https://openalex.org/W4372271367', 'https://openalex.org/W4285345683', 'https://openalex.org/W4318906029', 'https://openalex.org/W4292779060', 'https://openalex.org/W3033411150', 'https://openalex.org/W4303519914', 'https://openalex.org/W4311000453', 'https://openalex.org/W4280542470', 'https://openalex.org/W4361866031', 'https://openalex.org/W3172862365', 'https://openalex.org/W4226278401', 'https://openalex.org/W4367359628', 'https://openalex.org/W3138953166', 'https://openalex.org/W4298017177', 'https://openalex.org/W4288089799', 'https://openalex.org/W2896457183']",2024-03-24
https://openalex.org/W3189296823,https://doi.org/10.21437/interspeech.2021-556,LeBenchmark: A Reproducible Framework for Assessing Self-Supervised\n Representation Learning from Speech,"Self-Supervised Learning (SSL) using huge unlabeled data has been\nsuccessfully explored for image and natural language processing. Recent works\nalso investigated SSL from speech. They were notably successful to improve\nperformance on downstream tasks such as automatic speech recognition (ASR).\nWhile these works suggest it is possible to reduce dependence on labeled data\nfor building efficient speech systems, their evaluation was mostly made on ASR\nand using multiple and heterogeneous experimental settings (most of them for\nEnglish). This questions the objective comparison of SSL approaches and the\nevaluation of their impact on building speech systems. In this paper, we\npropose LeBenchmark: a reproducible framework for assessing SSL from speech. It\nnot only includes ASR (high and low resource) tasks but also spoken language\nunderstanding, speech translation and emotion recognition. We also focus on\nspeech technologies in a language different than English: French. SSL models of\ndifferent sizes are trained from carefully sourced and documented datasets.\nExperiments show that SSL is beneficial for most but not all tasks which\nconfirms the need for exhaustive and reliable benchmarks to evaluate its real\nimpact. LeBenchmark is shared with the scientific community for reproducible\nresearch in SSL from speech.\n","['https://openalex.org/W3095410713', 'https://openalex.org/W2988736778', 'https://openalex.org/W3198429080', 'https://openalex.org/W4385245566', 'https://openalex.org/W72302491', 'https://openalex.org/W2888867175', 'https://openalex.org/W2133564696', 'https://openalex.org/W2313339984', 'https://openalex.org/W2810556878', 'https://openalex.org/W3049256661', 'https://openalex.org/W3016011332', 'https://openalex.org/W2514741789', 'https://openalex.org/W2962784628', 'https://openalex.org/W2962739339', 'https://openalex.org/W3016181583', 'https://openalex.org/W3015867372', 'https://openalex.org/W2576530755', 'https://openalex.org/W157724941', 'https://openalex.org/W3005680577', 'https://openalex.org/W3030437843', 'https://openalex.org/W3102342027', 'https://openalex.org/W2933138175', 'https://openalex.org/W2896457183', 'https://openalex.org/W2327501763', 'https://openalex.org/W3099944122', 'https://openalex.org/W2972943112', 'https://openalex.org/W2948012107', 'https://openalex.org/W3054645415', 'https://openalex.org/W3092424727', 'https://openalex.org/W3015935472', 'https://openalex.org/W2973049979', 'https://openalex.org/W3021934733', 'https://openalex.org/W2045528981', 'https://openalex.org/W1524333225', 'https://openalex.org/W3035202887', 'https://openalex.org/W2249819665', 'https://openalex.org/W3197771105', 'https://openalex.org/W2972327934', 'https://openalex.org/W2329093554', 'https://openalex.org/W3036601975', 'https://openalex.org/W2402146185', 'https://openalex.org/W3119308075', 'https://openalex.org/W2399733683']",2021-04-23
https://openalex.org/W4319862652,https://doi.org/10.1109/slt54892.2023.10023234,On the Utility of Self-Supervised Models for Prosody-Related Tasks,"Self-Supervised Learning (SSL) from speech data has produced models that have achieved remarkable performance in many tasks, and that are known to implicitly represent many aspects of information latently present in speech signals. However, relatively little is known about the suitability of such models for prosody-related tasks or the extent to which they encode prosodic information. We present a new evaluation framework, ""SUPERB-prosody,"" consisting of three prosody-related downstream tasks and two pseudo tasks. We find that 13 of the 15 SSL models outperformed the baseline on all the prosody-related tasks. We also show good performance on two pseudo tasks: prosody reconstruction and future prosody prediction. We further analyze the layerwise contributions of the SSL models. Overall we conclude that SSL speech models are highly effective for prosody-related tasks. We release our code <sup xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">1</sup> <sup xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">1</sup> https://github.com/JSALT-2022-SSL/superb-prosody for the community to support further investigation of SSL models' utility for prosody.","['https://openalex.org/W3197580070', 'https://openalex.org/W4285250921', 'https://openalex.org/W4281492411', 'https://openalex.org/W3182074706', 'https://openalex.org/W2921495256', 'https://openalex.org/W6881984725', 'https://openalex.org/W6795949861', 'https://openalex.org/W4296068425', 'https://openalex.org/W3006926732', 'https://openalex.org/W4226103796', 'https://openalex.org/W4226380987', 'https://openalex.org/W6788328058', 'https://openalex.org/W3162133897', 'https://openalex.org/W2972943112', 'https://openalex.org/W3097286738', 'https://openalex.org/W3198858531', 'https://openalex.org/W2982223350', 'https://openalex.org/W3041561163', 'https://openalex.org/W3005511757', 'https://openalex.org/W2973049979', 'https://openalex.org/W6769196770', 'https://openalex.org/W3203140070', 'https://openalex.org/W6780218876', 'https://openalex.org/W3209059054', 'https://openalex.org/W3209984917', 'https://openalex.org/W2883409523', 'https://openalex.org/W2963349408', 'https://openalex.org/W2075398069', 'https://openalex.org/W6761768988', 'https://openalex.org/W4210296532', 'https://openalex.org/W2546919788', 'https://openalex.org/W6766673545', 'https://openalex.org/W6776048684', 'https://openalex.org/W3198533616', 'https://openalex.org/W3095410713', 'https://openalex.org/W4296070431', 'https://openalex.org/W3121914243', 'https://openalex.org/W3015468748', 'https://openalex.org/W3036601975', 'https://openalex.org/W2224323142', 'https://openalex.org/W2972359262', 'https://openalex.org/W2979476256', 'https://openalex.org/W1583837637', 'https://openalex.org/W3016181583', 'https://openalex.org/W2965373594']",2023-01-09
https://openalex.org/W1494198834,https://doi.org/10.1109/icassp.2015.7178964,Librispeech: An ASR corpus based on public domain audio books,"This paper introduces a new corpus of read English speech, suitable for training and evaluating speech recognition systems. The LibriSpeech corpus is derived from audiobooks that are part of the LibriVox project, and contains 1000 hours of speech sampled at 16 kHz. We have made the corpus freely available for download, along with separately prepared language-model training data and pre-built language models. We show that acoustic models trained on LibriSpeech give lower error rate on the Wall Street Journal (WSJ) test sets than models trained on WSJ itself. We are also releasing Kaldi scripts that make it easy to build these systems.","['https://openalex.org/W2113641473', 'https://openalex.org/W2090755665', 'https://openalex.org/W2148154194', 'https://openalex.org/W2106554350', 'https://openalex.org/W2087064593', 'https://openalex.org/W1647671624', 'https://openalex.org/W1517939602', 'https://openalex.org/W2037740282', 'https://openalex.org/W1599512239', 'https://openalex.org/W6631362777', 'https://openalex.org/W6603477829', 'https://openalex.org/W2024490156', 'https://openalex.org/W2164107060', 'https://openalex.org/W6712802073', 'https://openalex.org/W6677973343', 'https://openalex.org/W6636811518', 'https://openalex.org/W6738902873', 'https://openalex.org/W2097927681', 'https://openalex.org/W2026369565', 'https://openalex.org/W2330075180', 'https://openalex.org/W2950186769', 'https://openalex.org/W2125234026', 'https://openalex.org/W1524333225', 'https://openalex.org/W1934041838', 'https://openalex.org/W1631260214', 'https://openalex.org/W2397159106', 'https://openalex.org/W2620757702', 'https://openalex.org/W85707815', 'https://openalex.org/W2916535084']",2015-04-01
https://openalex.org/W4385807453,https://doi.org/10.21437/interspeech.2023-2193,Whisper-AT: Noise-Robust Automatic Speech Recognizers are Also Strong General Audio Event Taggers,"In this paper, we focus on Whisper [1], a recent automatic speech recognition model trained with a massive 680k hour labeled speech corpus recorded in diverse conditions.We first show an interesting finding that while Whisper is very robust against real-world background sounds (e.g., music), its audio representation is actually not noise-invariant, but is instead highly correlated to non-speech sounds, indicating that Whisper recognizes speech conditioned on the noise type.With this finding, we build a unified audio tagging and speech recognition model Whisper-AT by freezing the backbone of Whisper, and training a lightweight audio tagging model on top of it.With <1% extra computational cost, Whisper-AT can recognize audio events, in addition to spoken text, in a single forward pass.","['https://openalex.org/W2584667682', 'https://openalex.org/W4297841853', 'https://openalex.org/W2995181338', 'https://openalex.org/W4221150524', 'https://openalex.org/W3206996142', 'https://openalex.org/W2062164080', 'https://openalex.org/W1522301498', 'https://openalex.org/W4385245566', 'https://openalex.org/W4398958419', 'https://openalex.org/W3095738461', 'https://openalex.org/W4311000453', 'https://openalex.org/W3174906557', 'https://openalex.org/W4285483774', 'https://openalex.org/W3209059054', 'https://openalex.org/W4224932123', 'https://openalex.org/W3036601975', 'https://openalex.org/W4221140371', 'https://openalex.org/W3198771897', 'https://openalex.org/W3205743929', 'https://openalex.org/W2593116425', 'https://openalex.org/W3196974791', 'https://openalex.org/W1494198834', 'https://openalex.org/W3094550259', 'https://openalex.org/W2129674987', 'https://openalex.org/W2962909949', 'https://openalex.org/W2885724687']",2023-08-14
https://openalex.org/W4384918448,https://doi.org/10.48550/arxiv.2307.09288,Llama 2: Open Foundation and Fine-Tuned Chat Models,"In this work, we develop and release Llama 2, a collection of pretrained and fine-tuned large language models (LLMs) ranging in scale from 7 billion to 70 billion parameters. Our fine-tuned LLMs, called Llama 2-Chat, are optimized for dialogue use cases. Our models outperform open-source chat models on most benchmarks we tested, and based on our human evaluations for helpfulness and safety, may be a suitable substitute for closed-source models. We provide a detailed description of our approach to fine-tuning and safety improvements of Llama 2-Chat in order to enable the community to build on our work and contribute to the responsible development of LLMs.",[],2023-07-18
https://openalex.org/W4379539302,https://doi.org/10.48550/arxiv.2306.02207,SpeechGen: Unlocking the Generative Power of Speech Language Models with Prompts,"Large language models (LLMs) have gained considerable attention for Artificial Intelligence Generated Content (AIGC), particularly with the emergence of ChatGPT. However, the direct adaptation of continuous speech to LLMs that process discrete tokens remains an unsolved challenge, hindering the application of LLMs for speech generation. The advanced speech LMs are in the corner, as that speech signals encapsulate a wealth of information, including speaker and emotion, beyond textual data alone. Prompt tuning has demonstrated notable gains in parameter efficiency and competitive performance on some speech classification tasks. However, the extent to which prompts can effectively elicit generation tasks from speech LMs remains an open question. In this paper, we present pioneering research that explores the application of prompt tuning to stimulate speech LMs for various generation tasks, within a unified framework called SpeechGen, with around 10M trainable parameters. The proposed unified framework holds great promise for efficiency and effectiveness, particularly with the imminent arrival of advanced speech LMs, which will significantly enhance the capabilities of the framework. The code and demos of SpeechGen will be available on the project website: \url{https://ga642381.github.io/SpeechPrompt/speechgen}",[],2023-06-03
https://openalex.org/W4322718191,https://doi.org/10.48550/arxiv.2302.13971,LLaMA: Open and Efficient Foundation Language Models,"We introduce LLaMA, a collection of foundation language models ranging from 7B to 65B parameters. We train our models on trillions of tokens, and show that it is possible to train state-of-the-art models using publicly available datasets exclusively, without resorting to proprietary and inaccessible datasets. In particular, LLaMA-13B outperforms GPT-3 (175B) on most benchmarks, and LLaMA-65B is competitive with the best models, Chinchilla-70B and PaLM-540B. We release all our models to the research community.",[],2023-02-27
https://openalex.org/W4361229539,https://doi.org/10.48550/arxiv.2303.16199,LLaMA-Adapter: Efficient Fine-tuning of Language Models with Zero-init Attention,"We present LLaMA-Adapter, a lightweight adaption method to efficiently fine-tune LLaMA into an instruction-following model. Using 52K self-instruct demonstrations, LLaMA-Adapter only introduces 1.2M learnable parameters upon the frozen LLaMA 7B model, and costs less than one hour for fine-tuning on 8 A100 GPUs. Specifically, we adopt a set of learnable adaption prompts, and prepend them to the word tokens at higher transformer layers. Then, a zero-initialized attention mechanism with zero gating is proposed, which adaptively injects the new instructional cues into LLaMA, while effectively preserves its pre-trained knowledge. With our efficient training, LLaMA-Adapter can generate high-quality responses, comparable to Alpaca with fully fine-tuned 7B parameters. Besides language commands, our approach can be simply extended to multi-modal instructions for learning image-conditioned LLaMA model, which achieves superior reasoning performance on ScienceQA and COCO Caption benchmarks. Furthermore, we also evaluate the zero-initialized attention mechanism for fine-tuning other pre-trained models (ViT, RoBERTa) on traditional vision and language tasks, demonstrating the superior generalization capacity of our approach. Code is released at https://github.com/OpenGVLab/LLaMA-Adapter.",[],2023-03-28
https://openalex.org/W4322825254,https://doi.org/10.48550/arxiv.2303.00733,SpeechPrompt v2: Prompt Tuning for Speech Classification Tasks,"Prompt tuning is a technology that tunes a small set of parameters to steer a pre-trained language model (LM) to directly generate the output for downstream tasks. Recently, prompt tuning has demonstrated its storage and computation efficiency in both natural language processing (NLP) and speech processing fields. These advantages have also revealed prompt tuning as a candidate approach to serving pre-trained LM for multiple tasks in a unified manner. For speech processing, SpeechPrompt shows its high parameter efficiency and competitive performance on a few speech classification tasks. However, whether SpeechPrompt is capable of serving a large number of tasks is unanswered. In this work, we propose SpeechPrompt v2, a prompt tuning framework capable of performing a wide variety of speech classification tasks, covering multiple languages and prosody-related tasks. The experiment result shows that SpeechPrompt v2 achieves performance on par with prior works with less than 0.15M trainable parameters in a unified framework.",[],2023-03-01
https://openalex.org/W3036601975,https://doi.org/10.48550/arxiv.2006.11477,wav2vec 2.0: A Framework for Self-Supervised Learning of Speech Representations,"We show for the first time that learning powerful representations from speech audio alone followed by fine-tuning on transcribed speech can outperform the best semi-supervised methods while being conceptually simpler. wav2vec 2.0 masks the speech input in the latent space and solves a contrastive task defined over a quantization of the latent representations which are jointly learned. Experiments using all labeled data of Librispeech achieve 1.8/3.3 WER on the clean/other test sets. When lowering the amount of labeled data to one hour, wav2vec 2.0 outperforms the previous state of the art on the 100 hour subset while using 100 times less labeled data. Using just ten minutes of labeled data and pre-training on 53k hours of unlabeled data still achieves 4.8/8.2 WER. This demonstrates the feasibility of speech recognition with limited amounts of labeled data.","['https://openalex.org/W273093436', 'https://openalex.org/W3005680577', 'https://openalex.org/W3107298252', 'https://openalex.org/W3016181583', 'https://openalex.org/W2908336025', 'https://openalex.org/W2936295285', 'https://openalex.org/W2973049979', 'https://openalex.org/W2953190524', 'https://openalex.org/W2941814890', 'https://openalex.org/W2962901777', 'https://openalex.org/W2971155163', 'https://openalex.org/W2936774411', 'https://openalex.org/W3025165719', 'https://openalex.org/W2127141656', 'https://openalex.org/W2996383576', 'https://openalex.org/W2995181338', 'https://openalex.org/W2996159613', 'https://openalex.org/W3004728855', 'https://openalex.org/W2124509324', 'https://openalex.org/W2995680346', 'https://openalex.org/W10548402', 'https://openalex.org/W3026041220', 'https://openalex.org/W2896457183', 'https://openalex.org/W3103005696', 'https://openalex.org/W2121879602', 'https://openalex.org/W2794209590', 'https://openalex.org/W2991213871', 'https://openalex.org/W2944828972', 'https://openalex.org/W2949892913', 'https://openalex.org/W2994536315', 'https://openalex.org/W3021469861', 'https://openalex.org/W2152790380', 'https://openalex.org/W2964121744', 'https://openalex.org/W2547875792', 'https://openalex.org/W2962942158', 'https://openalex.org/W3127686677', 'https://openalex.org/W2963799213', 'https://openalex.org/W3003875258', 'https://openalex.org/W2963807318', 'https://openalex.org/W3027083471', 'https://openalex.org/W2899663614', 'https://openalex.org/W2296701362', 'https://openalex.org/W2965373594', 'https://openalex.org/W2963403868', 'https://openalex.org/W3002741552', 'https://openalex.org/W2981991061', 'https://openalex.org/W2962739339', 'https://openalex.org/W2952509486', 'https://openalex.org/W2988736778', 'https://openalex.org/W3035524453', 'https://openalex.org/W3037932933', 'https://openalex.org/W2963631907', 'https://openalex.org/W2842511635', 'https://openalex.org/W1494198834', 'https://openalex.org/W3015419784', 'https://openalex.org/W2972943112', 'https://openalex.org/W2972374322']",2020-06-20
https://openalex.org/W4307322847,https://doi.org/10.48550/arxiv.2210.13352,ESB: A Benchmark For Multi-Domain End-to-End Speech Recognition,"Speech recognition applications cover a range of different audio and text distributions, with different speaking styles, background noise, transcription punctuation and character casing. However, many speech recognition systems require dataset-specific tuning (audio filtering, punctuation removal and normalisation of casing), therefore assuming a-priori knowledge of both the audio and text distributions. This tuning requirement can lead to systems failing to generalise to other datasets and domains. To promote the development of multi-domain speech systems, we introduce the End-to-end Speech Benchmark (ESB) for evaluating the performance of a single automatic speech recognition (ASR) system across a broad set of speech datasets. Benchmarked systems must use the same data pre- and post-processing algorithm across datasets - assuming the audio and text data distributions are a-priori unknown. We compare a series of state-of-the-art (SoTA) end-to-end (E2E) systems on this benchmark, demonstrating how a single speech system can be applied and evaluated on a wide range of data distributions. We find E2E systems to be effective across datasets: in a fair comparison, E2E systems achieve within 2.6% of SoTA systems tuned to a specific dataset. Our analysis reveals that transcription artefacts, such as punctuation and casing, pose difficulties for ASR systems and should be included in evaluation. We believe E2E benchmarking over a range of datasets promotes the research of multi-domain speech recognition systems. ESB is available at https://huggingface.co/esb.",[],2022-10-24
https://openalex.org/W2998572311,https://doi.org/10.7488/ds/2645,CSTR VCTK Corpus: English Multi-speaker Corpus for CSTR Voice Cloning Toolkit (version 0.92),"This CSTR VCTK Corpus includes speech data uttered by 110 English speakers with various accents. Each speaker reads out about 400 sentences, which were selected from a newspaper, the rainbow passage and an elicitation paragraph used for the speech accent archive. The newspaper texts were taken from Herald Glasgow, with permission from Herald &amp; Times Group. Each speaker has a different set of the newspaper texts selected based a greedy algorithm that increases the contextual and phonetic coverage. The details of the text selection algorithms are described in the following paper: C. Veaux, J. Yamagishi and S. King, ""The voice bank corpus: Design, collection and data analysis of a large regional accent speech database,"" https://doi.org/10.1109/ICSDA.2013.6709856 The rainbow passage and elicitation paragraph are the same for all speakers. The rainbow passage can be found at International Dialects of English Archive: (http://web.ku.edu/~idea/readings/rainbow.htm). The elicitation paragraph is identical to the one used for the speech accent archive (http://accent.gmu.edu). The details of the the speech accent archive can be found at http://www.ualberta.ca/~aacl2009/PDFs/WeinbergerKunath2009AACL.pdf All speech data was recorded using an identical recording setup: an omni-directional microphone (DPA 4035) and a small diaphragm condenser microphone with very wide bandwidth (Sennheiser MKH 800), 96kHz sampling frequency at 24 bits and in a hemi-anechoic chamber of the University of Edinburgh. (However, two speakers, p280 and p315 had technical issues of the audio recordings using MKH 800). All recordings were converted into 16 bits, were downsampled to 48 kHz, and were manually end-pointed. This corpus was originally aimed for HMM-based text-to-speech synthesis systems, especially for speaker-adaptive HMM-based speech synthesis that uses average voice models trained on multiple speakers and speaker adaptation technologies. This corpus is also suitable for DNN-based multi-speaker text-to-speech synthesis systems and neural waveform modeling. The dataset was was referenced in the Google DeepMind work on WaveNet: https://arxiv.org/pdf/1609.03499.pdf . Please note while text files containing transcripts of the speech are provided for 109 of the 110 recordings, in the '/txt' folder, the 'p315' text was lost due to a hard disk error.",[],2019-01-01
