openalex_id,doi,title,abstract,referenced_works,publication_date
https://openalex.org/W4392902876,https://doi.org/10.1109/icassp48485.2024.10447448,Retrieval Augmented End-to-End Spoken Dialog Models,"We recently developed a joint speech and language model (SLM [1]) which fuses a pretrained foundational speech model and a large language model (LLM), while preserving the in-context learning capability intrinsic to the pretrained LLM. In this paper, we apply SLM to dialog applications where the dialog states are inferred directly from the audio signal.Task-oriented dialogs often contain domain-specific entities, i.e., restaurants, hotels, train stations, and city names, which are difficult to recognize, however, critical for the downstream applications. Inspired by the RAG (retrieval-augmented generation) models, we propose a retrieval augmented SLM (ReSLM) that overcomes this weakness. We first train a retriever to retrieve text entities given audio inputs. The retrieved entities are then added as text inputs to the underlying LLM to bias model predictions. We evaluated ReSLM on speech MultiWoz task (DSTC-11 Challenge), and found that the retrieval augmentation boosts model performance, achieving joint goal accuracy (38.6% vs 32.7%), slot error rate (20.6% vs 24.8%) and ASR word error rate (5.5% vs 6.7%). While demonstrated on dialog state tracking, our approach is broadly applicable to speech tasks requiring custom contextual information or domain-specific entities.","['https://openalex.org/W4391021623', 'https://openalex.org/W6777615688', 'https://openalex.org/W6779857854', 'https://openalex.org/W3099700870', 'https://openalex.org/W6810722945', 'https://openalex.org/W4385822641', 'https://openalex.org/W6764961350', 'https://openalex.org/W4385573970', 'https://openalex.org/W6803092890', 'https://openalex.org/W4226120743', 'https://openalex.org/W4225272718', 'https://openalex.org/W4225308107', 'https://openalex.org/W6799838802', 'https://openalex.org/W6810334672', 'https://openalex.org/W4381786045', 'https://openalex.org/W4372270126', 'https://openalex.org/W6783813245', 'https://openalex.org/W6810730852', 'https://openalex.org/W6845404407', 'https://openalex.org/W2945475330', 'https://openalex.org/W6770169672', 'https://openalex.org/W2997771882', 'https://openalex.org/W3206547074', 'https://openalex.org/W4223510772', 'https://openalex.org/W6850218400', 'https://openalex.org/W6769627184', 'https://openalex.org/W2127141656', 'https://openalex.org/W4372267461', 'https://openalex.org/W6777399232', 'https://openalex.org/W6810673746', 'https://openalex.org/W3097777922', 'https://openalex.org/W4225323055', 'https://openalex.org/W4288027128', 'https://openalex.org/W3207222250', 'https://openalex.org/W4226082499', 'https://openalex.org/W4301243929', 'https://openalex.org/W4225553189', 'https://openalex.org/W2954492830', 'https://openalex.org/W4323066695', 'https://openalex.org/W3190965961']",2024-03-18
https://openalex.org/W4408358330,https://doi.org/10.1109/access.2025.3550855,"Advancements in Speech Recognition: A Systematic Review of Deep Learning Transformer Models, Trends, Innovations, and Future Directions","The transformer is a Deep Learning (DL) model that revolutionized language processing with its self-attention mechanism, enabling parallel processing and improving model efficiency, which dramatically reshaped the landscape of speech recognition technology, based on the ability to efficiently manage the dynamic and context-rich nature of speech. The proposed systematic review in this article critically examines the impact of transformer models on speech recognition, covering the published research over the past seven years (from January 1, 2017, to May 15, 2024). The goals of this article are to synthesize the current knowledge, pinpoint emerging trends, and identify research gaps that could be beneficial for future investigations. From an initial pool of 2,838 publications sourced from leading digital libraries, a rigorous two-step screening process applied to distill high-quality studies relevant to the review criteria. We concentrated our analysis on seven pivotal areas as following: the environmental conditions (neutral versus noisy) addressed by the studies, methods of feature extraction employed, characteristics of the transformer models used, datasets utilized, variations in model efficiency, the influence of noise on model generalizability, and trends in self-supervised learning, ending up with 37 articles to review in this paper. Our findings underscore the transformative potential of transformers in enhancing the accuracy and robustness of speech recognition systems, especially in challenging acoustic environments. In addition, this review highlights areas where more research is needed to make speech recognition even better by using transformer technology.","['https://openalex.org/W6866711220', 'https://openalex.org/W6866150908', 'https://openalex.org/W6857228344', 'https://openalex.org/W6857421307', 'https://openalex.org/W4385822429', 'https://openalex.org/W4385822766', 'https://openalex.org/W6853611000', 'https://openalex.org/W4385822815', 'https://openalex.org/W4327500526', 'https://openalex.org/W3206114047', 'https://openalex.org/W4225808286', 'https://openalex.org/W4224925623', 'https://openalex.org/W4210463634', 'https://openalex.org/W6770514103', 'https://openalex.org/W3207595101', 'https://openalex.org/W4223945715', 'https://openalex.org/W4226420874', 'https://openalex.org/W3015810689', 'https://openalex.org/W4391021623', 'https://openalex.org/W4226380987', 'https://openalex.org/W3015747801', 'https://openalex.org/W3162313915', 'https://openalex.org/W2981857663', 'https://openalex.org/W4393863141', 'https://openalex.org/W4221165942', 'https://openalex.org/W3016010032', 'https://openalex.org/W4391021542', 'https://openalex.org/W4392904027', 'https://openalex.org/W4385245566', 'https://openalex.org/W4400771889', 'https://openalex.org/W4283818106', 'https://openalex.org/W4379805013', 'https://openalex.org/W4205608408', 'https://openalex.org/W4412474694', 'https://openalex.org/W2781626870', 'https://openalex.org/W3157175643', 'https://openalex.org/W4399426355', 'https://openalex.org/W4317881096']",2025-01-01
https://openalex.org/W4412899262,https://doi.org/10.37943/22snok5872,AUDIO-TO-TEXT TRANSLATION FOR THE HARD OF HEARING: A WHISPER MODEL-BASED STUDY,"This study investigates the effectiveness of the Whisper model for audio-to-text transcription, specifically targeting the enhancement of accessibility for individuals with hearing impairments. The research focuses on the processing of audio recordings obtained from WhatsApp messenger, which often contain significant background noise that complicates speech recognition. To address this issue, advanced audio processing techniques were employed, including the use of the Librosa library and the Noisereduce package for noise reduction. The spectral gating methods applied in this study effectively diminished wind noise and other ambient sounds, allowing for clearer recognition of spoken content. To ensure the quality of the processed audio, we assessed its clarity using a SimpleRNN model. The training results demonstrated a progressive reduction in loss values across epochs, confirming the successful enhancement of audio quality. Once the audio files were adequately cleaned, we utilized the Whisper model, a sophisticated machine learning tool for speech recognition developed by OpenAI, to transcribe the audio into text. The transcription process yielded accurate Kazakh language output, despite the initial challenges posed by background noise. These findings underscore the critical role of high-quality audio input in achieving reliable transcription results and highlight the potential of machine learning technologies in improving communication access for hearing-impaired individuals. This study concludes with recommendations for future research, including the exploration of additional noise reduction techniques and the application of the Whisper model across various languages and dialects. Such advancements could significantly contribute to creating more inclusive digital environments and enhancing the overall user experience for individuals with hearing impairments.","['https://openalex.org/W2029430921', 'https://openalex.org/W4391384041', 'https://openalex.org/W4387790057', 'https://openalex.org/W4361003384', 'https://openalex.org/W4389669656', 'https://openalex.org/W4387303718', 'https://openalex.org/W4297259271', 'https://openalex.org/W4319601831', 'https://openalex.org/W4403496717', 'https://openalex.org/W3092424727', 'https://openalex.org/W4385764125', 'https://openalex.org/W4375869257', 'https://openalex.org/W3162037819', 'https://openalex.org/W4309951156', 'https://openalex.org/W4391021666', 'https://openalex.org/W3206573929', 'https://openalex.org/W4391021623', 'https://openalex.org/W4385805046', 'https://openalex.org/W3119991380', 'https://openalex.org/W4320884090', 'https://openalex.org/W4394978572', 'https://openalex.org/W4386575267', 'https://openalex.org/W4402981815']",2025-06-30
https://openalex.org/W4389519009,https://doi.org/10.18653/v1/2023.findings-emnlp.704,MRRL: Modifying the Reference via Reinforcement Learning for Non-Autoregressive Joint Multiple Intent Detection and Slot Filling,"With the rise of non-autoregressive approach, some non-autoregressive models for joint multiple intent detection and slot filling have obtained the promising inference speed. However, most existing SLU models (1) suffer from the multi-modality problem that leads to reference intents and slots may not be suitable for training; (2) lack of alignment between the correct predictions of the two tasks, which extremely limits the overall accuracy. Therefore, in this paper, we propose Modifying the Reference via Reinforcement Learning (MRRL), a novel method for multiple intent detection and slot filling, which introduces a modifier and employs reinforcement learning. Specifically, we try to provide the better training target for the non-autoregressive SLU model via modifying the reference based on the output of the non-autoregressive SLU model, and propose a suitability reward to ensure that the output of the modifier module could fit well with the output of the non-autoregressive SLU model and does not deviate too far from the reference. In addition, we also propose a compromise reward to realize a flexible trade-off between the two subtasks. Experiments on two multi-intent datasets and non-autoregressive baselines demonstrate that our MRRL could consistently improve the performance of baselines. More encouragingly, our best variant achieves new state-of-the-art results, outperforming the previous best approach by 3.6 overall accuracy on MixATIS dataset.","['https://openalex.org/W2581637843', 'https://openalex.org/W3161463965', 'https://openalex.org/W4389520437', 'https://openalex.org/W3103899394', 'https://openalex.org/W3034216012', 'https://openalex.org/W3161631007', 'https://openalex.org/W648947103', 'https://openalex.org/W4312426068', 'https://openalex.org/W3209239055', 'https://openalex.org/W4384807869', 'https://openalex.org/W4297733535', 'https://openalex.org/W4382202516', 'https://openalex.org/W4382202608', 'https://openalex.org/W3098800734', 'https://openalex.org/W4386384714', 'https://openalex.org/W4387848696', 'https://openalex.org/W3173794693', 'https://openalex.org/W3099757670', 'https://openalex.org/W2963974889', 'https://openalex.org/W2804945011', 'https://openalex.org/W2963034998', 'https://openalex.org/W4385572615', 'https://openalex.org/W3212491726', 'https://openalex.org/W3170264707', 'https://openalex.org/W2963066655', 'https://openalex.org/W4385571399', 'https://openalex.org/W4378718233', 'https://openalex.org/W3100532071', 'https://openalex.org/W2474111273', 'https://openalex.org/W4224212267', 'https://openalex.org/W4385822432', 'https://openalex.org/W4389519101', 'https://openalex.org/W4238557920', 'https://openalex.org/W4288090877', 'https://openalex.org/W3113593351', 'https://openalex.org/W3162342118', 'https://openalex.org/W4385573070', 'https://openalex.org/W2077302143', 'https://openalex.org/W4298093396', 'https://openalex.org/W4385567042', 'https://openalex.org/W2898637464', 'https://openalex.org/W2888442053', 'https://openalex.org/W4385570101', 'https://openalex.org/W4385567178', 'https://openalex.org/W1522301498', 'https://openalex.org/W2888921503', 'https://openalex.org/W2970450228', 'https://openalex.org/W2971167298', 'https://openalex.org/W4297683418', 'https://openalex.org/W4375868795', 'https://openalex.org/W4385823351', 'https://openalex.org/W2963167310', 'https://openalex.org/W2964071174', 'https://openalex.org/W4382119366', 'https://openalex.org/W4285217170', 'https://openalex.org/W2766371743', 'https://openalex.org/W2803392141', 'https://openalex.org/W4389520050', 'https://openalex.org/W2094472029', 'https://openalex.org/W4287887858', 'https://openalex.org/W3093528669', 'https://openalex.org/W2119717200', 'https://openalex.org/W4224933765', 'https://openalex.org/W2473329891', 'https://openalex.org/W4288346477', 'https://openalex.org/W2946085385', 'https://openalex.org/W1975244201']",2023-01-01
https://openalex.org/W4394583388,https://doi.org/10.1109/ic2pct60090.2024.10486323,Optical Character Recognition Systems for Accurate Interpretation of Handwritten Telugu Scripts,"Visual data dominates the digital landscape these days. However, there remains a significant gap in the digitization of textual content in regional languages such as Telugu. This research addresses this gap and utilizes the capabilities of the programming language and set of specialized libraries pytesseract for optical character recognition (OCR), PIL for image processing and Levenshtein for accuracy evaluation to appropriately extract text from images in the Telugu script. There are several important steps in this project. Initially, this work sets up a dependable system for uploading pictures. Secondly, carefully extracting text from these submitted photos using py-tesseract. Using the Levenshtein distance measure, and then compared with the ground truth to determine correctness. This metric measures the accuracy of the text output through looking at the slight variations between the word error rate (WER) and character error rate (CER). This is capable of finding misspelled words and unknown characters providing a precise evaluation of the OCR's effectiveness. This study also examines the current state of OCR in Telugu text processing and indicates the benefits of current technologies for locating and converting Telugu text discovered in image documents. It also demonstrates improvements in OCR technology.","['https://openalex.org/W2117182510', 'https://openalex.org/W4293493277', 'https://openalex.org/W6774023782', 'https://openalex.org/W3082735848', 'https://openalex.org/W4223415467', 'https://openalex.org/W4205445027', 'https://openalex.org/W6795700344', 'https://openalex.org/W3210847758', 'https://openalex.org/W4307194360', 'https://openalex.org/W3214042726', 'https://openalex.org/W2332561139', 'https://openalex.org/W2140485652', 'https://openalex.org/W2971777065', 'https://openalex.org/W6690217923', 'https://openalex.org/W1993011068', 'https://openalex.org/W6855885476', 'https://openalex.org/W3183597859', 'https://openalex.org/W4386384714', 'https://openalex.org/W4285058206', 'https://openalex.org/W4302344755']",2024-02-09
https://openalex.org/W4407782501,https://doi.org/10.1109/jbhi.2025.3543968,BAHBench: A Unified Benchmark for Evaluating Bio-Acoustic Health With Acoustic Foundation Models,"Acoustic foundation models, through self-supervised learning on large amounts of unlabeled speech data, can acquire rich acoustic representations. In recent years, these models have demonstrated substantial potential in audio-based health-related tasks, remarkably enhancing the efficiency and quality of healthcare services and contributing to the advancement of smart healthcare. However, there is currently a lack of systematic research and exploration on the performance of acoustic foundation models in health-related tasks. Furthermore, inconsistencies in evaluation methods and experimental setups hinder fair comparisons between different methods, severely impeding progress in this field. To address these challenges, we establish a unified Benchmark for evaluating Bio-Acoustic health via acoustic foundation models, namely BAHBench. BAHBench encompasses 6 distinct health-related tasks and evaluates 12 acoustic foundation models within a unified evaluation framework and parameter settings, enabling fair comparisons across different models. Our objective is to explore the effectiveness of current acoustic foundation models in health-related tasks. Thus, we discuss the impact of model size and data diversity on performance, and investigate feature selection and efficient fine-tuning strategy. Experimental results show that different health-related tasks benefit from features from different layers of the foundation model, while LoRA fine-tuning further enhances the model's performance on downstream tasks. Our goal is to provide clear and comprehensive guidance for future researchers. The code related to this study will be available to the research community to promote transparency and reproducibility.","['https://openalex.org/W4400579821', 'https://openalex.org/W4399489506', 'https://openalex.org/W4360994649', 'https://openalex.org/W3211062931', 'https://openalex.org/W4399154371', 'https://openalex.org/W4392902672', 'https://openalex.org/W4400229026', 'https://openalex.org/W4386248431', 'https://openalex.org/W4365135358', 'https://openalex.org/W3023585707', 'https://openalex.org/W4205860088', 'https://openalex.org/W3011285251', 'https://openalex.org/W3174249533', 'https://openalex.org/W4205687123', 'https://openalex.org/W4297841747', 'https://openalex.org/W4321608155', 'https://openalex.org/W4392903056', 'https://openalex.org/W3107962736', 'https://openalex.org/W4225319493', 'https://openalex.org/W2989852193', 'https://openalex.org/W4372260310', 'https://openalex.org/W6847363464', 'https://openalex.org/W2973049979', 'https://openalex.org/W4388766108', 'https://openalex.org/W3209984917', 'https://openalex.org/W4319878709', 'https://openalex.org/W6870254097', 'https://openalex.org/W4391827429', 'https://openalex.org/W4372260155', 'https://openalex.org/W3163826605', 'https://openalex.org/W4224315877', 'https://openalex.org/W3117926103', 'https://openalex.org/W4386996915', 'https://openalex.org/W3197580070', 'https://openalex.org/W4392902623', 'https://openalex.org/W4404708901', 'https://openalex.org/W4391865211', 'https://openalex.org/W6796581206', 'https://openalex.org/W4394862910', 'https://openalex.org/W6860853044', 'https://openalex.org/W4385245566', 'https://openalex.org/W2896457183', 'https://openalex.org/W6780218876', 'https://openalex.org/W3209059054', 'https://openalex.org/W4392904633', 'https://openalex.org/W3015591594', 'https://openalex.org/W2593116425', 'https://openalex.org/W4391604988', 'https://openalex.org/W4288437714', 'https://openalex.org/W3088067841', 'https://openalex.org/W6801976681', 'https://openalex.org/W1494198834', 'https://openalex.org/W2995181338', 'https://openalex.org/W3119308075', 'https://openalex.org/W3198694222', 'https://openalex.org/W6810007534', 'https://openalex.org/W2399733683', 'https://openalex.org/W6791353385', 'https://openalex.org/W4386223259', 'https://openalex.org/W2559143428', 'https://openalex.org/W2964182121', 'https://openalex.org/W4223646484', 'https://openalex.org/W2018363392', 'https://openalex.org/W3161639065', 'https://openalex.org/W2972301465', 'https://openalex.org/W2052666245', 'https://openalex.org/W3135028703', 'https://openalex.org/W2963652649', 'https://openalex.org/W4391547780', 'https://openalex.org/W3192599558']",2025-02-20
https://openalex.org/W4386071707,https://doi.org/10.1109/cvpr52729.2023.01457,ImageBind One Embedding Space to Bind Them All,"We present ImageBind, an approach to learn a joint embedding across six different modalities - images, text, audio, depth, thermal, and IMU data. We show that all combinations of paired data are not necessary to train such a joint embedding, and only image-paired data is sufficient to bind the modalities together. ImageBind can leverage recent large scale vision-language models, and extends their zero-shot capabilities to new modalities just by using their natural pairing with images. It enables novel emergent applications 'out-of-the-box' including cross-modal retrieval, composing modalities with arithmetic, cross-modal detection and generation. The emergent capabilities improve with the strength of the image encoder and we set a new state-of-the-art on emergent zero-shot recognition tasks across modalities, outperforming specialist supervised models. Finally, we show strong few-shot recognition results outperforming prior work, and that ImageBind serves as a new way to evaluate vision models for visual and non-visual tasks.","['https://openalex.org/W6747225742', 'https://openalex.org/W6845452229', 'https://openalex.org/W6784333009', 'https://openalex.org/W6784184991', 'https://openalex.org/W6838789689', 'https://openalex.org/W6791353385', 'https://openalex.org/W6797148833', 'https://openalex.org/W2052666245', 'https://openalex.org/W2842511635', 'https://openalex.org/W3198452188', 'https://openalex.org/W3035682985', 'https://openalex.org/W6775244068', 'https://openalex.org/W6774314701', 'https://openalex.org/W6750591037', 'https://openalex.org/W6678470764', 'https://openalex.org/W2033875152', 'https://openalex.org/W6839264832', 'https://openalex.org/W2593116425', 'https://openalex.org/W6797613833', 'https://openalex.org/W4313190371', 'https://openalex.org/W3035635319', 'https://openalex.org/W6751037545', 'https://openalex.org/W2984008963', 'https://openalex.org/W6845288475', 'https://openalex.org/W6809576537', 'https://openalex.org/W6803602085', 'https://openalex.org/W6679792166', 'https://openalex.org/W6843151239', 'https://openalex.org/W3175300676', 'https://openalex.org/W3015371781', 'https://openalex.org/W2963524571', 'https://openalex.org/W4226442948', 'https://openalex.org/W6810640834', 'https://openalex.org/W2619697695', 'https://openalex.org/W3159481202', 'https://openalex.org/W3204588463', 'https://openalex.org/W3213454282', 'https://openalex.org/W6803872405', 'https://openalex.org/W6811433417', 'https://openalex.org/W2998508940', 'https://openalex.org/W4312818263', 'https://openalex.org/W6811072154', 'https://openalex.org/W3163937874', 'https://openalex.org/W6839165094', 'https://openalex.org/W6955071965', 'https://openalex.org/W4312658081', 'https://openalex.org/W6631516269', 'https://openalex.org/W3213472242', 'https://openalex.org/W6790019176', 'https://openalex.org/W2798991696', 'https://openalex.org/W2100031962', 'https://openalex.org/W6843263998', 'https://openalex.org/W2550821151', 'https://openalex.org/W2425121537', 'https://openalex.org/W6780294235', 'https://openalex.org/W6810334672', 'https://openalex.org/W6745388339', 'https://openalex.org/W3205475937', 'https://openalex.org/W6739901393', 'https://openalex.org/W6788135285', 'https://openalex.org/W6838673894', 'https://openalex.org/W3216270236', 'https://openalex.org/W2067912884', 'https://openalex.org/W6763442200', 'https://openalex.org/W6802517928', 'https://openalex.org/W1923184257', 'https://openalex.org/W1565402342', 'https://openalex.org/W6810265253', 'https://openalex.org/W4312372834', 'https://openalex.org/W125693051', 'https://openalex.org/W6802987763', 'https://openalex.org/W3205786327', 'https://openalex.org/W2149557440', 'https://openalex.org/W3196974791', 'https://openalex.org/W4312777209', 'https://openalex.org/W2138621090', 'https://openalex.org/W6809885388', 'https://openalex.org/W2117539524', 'https://openalex.org/W6782298681', 'https://openalex.org/W2134670479', 'https://openalex.org/W3206072662', 'https://openalex.org/W4298393544', 'https://openalex.org/W1527575280', 'https://openalex.org/W4301914798', 'https://openalex.org/W4221167396', 'https://openalex.org/W3176799298', 'https://openalex.org/W4312685069', 'https://openalex.org/W4312558481', 'https://openalex.org/W3094502228', 'https://openalex.org/W2796992393', 'https://openalex.org/W4386071576', 'https://openalex.org/W3108655343', 'https://openalex.org/W3005680577', 'https://openalex.org/W4385245566', 'https://openalex.org/W4281702809', 'https://openalex.org/W3215626407', 'https://openalex.org/W4225323055', 'https://openalex.org/W4285483774', 'https://openalex.org/W4295990387', 'https://openalex.org/W2123024445', 'https://openalex.org/W3170874841', 'https://openalex.org/W2774267535', 'https://openalex.org/W3166396011', 'https://openalex.org/W4295124836', 'https://openalex.org/W4309435465', 'https://openalex.org/W3176445421', 'https://openalex.org/W3174906557', 'https://openalex.org/W4224035735', 'https://openalex.org/W4280490805', 'https://openalex.org/W3209532394', 'https://openalex.org/W4226452378', 'https://openalex.org/W4229042118', 'https://openalex.org/W3122640483', 'https://openalex.org/W3037309139', 'https://openalex.org/W3126337491', 'https://openalex.org/W2619947201', 'https://openalex.org/W2963703197', 'https://openalex.org/W4312424618', 'https://openalex.org/W3081167590', 'https://openalex.org/W4297808394', 'https://openalex.org/W4312741694', 'https://openalex.org/W3010094231']",2023-06-01
https://openalex.org/W3197324626,https://doi.org/10.21437/interspeech.2021-1757,PnG BERT: Augmented BERT on Phonemes and Graphemes for Neural TTS,"This paper introduces PnG BERT, a new encoder model for neural TTS.This model is augmented from the original BERT model, by taking both phoneme and grapheme representations of text as input, as well as the word-level alignment between them.It can be pre-trained on a large text corpus in a selfsupervised manner, and fine-tuned in a TTS task.Experimental results show that a neural TTS model using a pre-trained PnG BERT as its encoder yields more natural prosody and more accurate pronunciation than a baseline model using only phoneme input with no pre-training.Subjective side-by-side preference evaluations show that raters have no statistically significant preference between the speech synthesized using a PnG BERT and ground truth recordings from professional speakers.","['https://openalex.org/W3163339651', 'https://openalex.org/W2952744660', 'https://openalex.org/W3081488690', 'https://openalex.org/W2591927543', 'https://openalex.org/W2808706139', 'https://openalex.org/W3161296985', 'https://openalex.org/W2914120296', 'https://openalex.org/W2901997113', 'https://openalex.org/W2619368999', 'https://openalex.org/W2103085228', 'https://openalex.org/W2519091744', 'https://openalex.org/W3161782335', 'https://openalex.org/W2963250244', 'https://openalex.org/W4299287062', 'https://openalex.org/W2946200149', 'https://openalex.org/W4385245566', 'https://openalex.org/W2121879602', 'https://openalex.org/W2896457183', 'https://openalex.org/W3091928890', 'https://openalex.org/W2963827314', 'https://openalex.org/W3033411150', 'https://openalex.org/W4287547221', 'https://openalex.org/W2964243274', 'https://openalex.org/W2962784628', 'https://openalex.org/W2794490148', 'https://openalex.org/W2866343820', 'https://openalex.org/W2963609956', 'https://openalex.org/W4298580827', 'https://openalex.org/W2907916773', 'https://openalex.org/W2903739847']",2021-08-27
https://openalex.org/W4319862635,https://doi.org/10.1109/slt54892.2023.10023141,FLEURS: FEW-Shot Learning Evaluation of Universal Representations of Speech,"We introduce FLEURS, the Few-shot Learning Evaluation of Universal Representations of Speech benchmark. FLEURS is an n-way parallel speech dataset in 102 languages built on top of the machine translation FLoRes-101 benchmark, with approximately 12 hours of speech supervision per language. FLEURS can be used for a variety of speech tasks, including Automatic Speech Recognition (ASR), Speech Language Identification (Speech LangID), Speech-Text Retrieval. In this paper, we provide baselines for the tasks based on multilingual pre-trained models like speech-only w2v-BERT [1] and speech-text multimodal mSLAM [2]. The goal of FLEURS is to enable speech technology in more languages and catalyze research in low-resource speech understanding. <sup xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">1</sup> .","['https://openalex.org/W4226033575', 'https://openalex.org/W6810259195', 'https://openalex.org/W3097777922', 'https://openalex.org/W6780218876', 'https://openalex.org/W3198429080', 'https://openalex.org/W3204696009', 'https://openalex.org/W4210463634', 'https://openalex.org/W3096215352', 'https://openalex.org/W1494198834', 'https://openalex.org/W3160525311', 'https://openalex.org/W3213029956', 'https://openalex.org/W3095410713', 'https://openalex.org/W3119308075', 'https://openalex.org/W3196509775', 'https://openalex.org/W6771467084', 'https://openalex.org/W2743686029', 'https://openalex.org/W3015698636', 'https://openalex.org/W3092085609', 'https://openalex.org/W3197771105', 'https://openalex.org/W6810701745', 'https://openalex.org/W3139878283', 'https://openalex.org/W2937197076', 'https://openalex.org/W3169369929', 'https://openalex.org/W27049869', 'https://openalex.org/W6784577980', 'https://openalex.org/W2064675550', 'https://openalex.org/W2127141656', 'https://openalex.org/W3015877095', 'https://openalex.org/W2914699162', 'https://openalex.org/W2105981609', 'https://openalex.org/W1984076147', 'https://openalex.org/W3213148312', 'https://openalex.org/W6803675045', 'https://openalex.org/W2965538726', 'https://openalex.org/W3039695075', 'https://openalex.org/W2220457451', 'https://openalex.org/W3169483174', 'https://openalex.org/W4226444650', 'https://openalex.org/W4221155340', 'https://openalex.org/W3213018012']",2023-01-09
https://openalex.org/W38194800,https://doi.org/10.21437/interspeech.2004-668,From WER and RIL to MER and WIL: improved evaluation measures for connected speech recognition,"The word error rate (WER), commonly used in ASR assessment, measures the cost of restoring the output word sequence to the original input sequence. However, for most CSR applications apart from dictation machines a more meaningful performance measure would be given by the proportion of information communicated. In this article we introduce two new absolute CSR performance measures: MER (match error rate) and WIL (word information lost). MER is the proportion of I/O word matches which are errors. WIL is a simple approximation to the proportion of word information lost which overcomes the problems associated with the RIL (relative information lost) measure that was proposed half a century ago. Issues relating to ideal performance measurement are discussed and the commonly used Viterbi input/output alignment procedure, with zero weight for hits and equal weight for substitutions, deletions and insertions, is shown to be optimal.","['https://openalex.org/W1667165204', 'https://openalex.org/W4285719527', 'https://openalex.org/W1548235210', 'https://openalex.org/W1970533835', 'https://openalex.org/W2087617622', 'https://openalex.org/W2157477135']",2004-10-04
https://openalex.org/W4319862232,https://doi.org/10.1109/slt54892.2023.10023323,NAM+: Towards Scalable End-to-End Contextual Biasing for Adaptive ASR,"Attention-based biasing techniques for end-to-end ASR systems are able to achieve large accuracy gains without requiring the inference algorithm adjustments and parameter tuning common to fusion approaches. However, it is challenging to simultaneously scale up attention-based biasing to realistic numbers of biased phrases; maintain in-domain WER gains, while minimizing out-of-domain losses; and run in real time. We present NAM+, an attention-based biasing approach which achieves a 16X inference speedup per acoustic frame over prior work when run with 3,000 biasing entities, as measured on a typical mobile CPU. NAM+ achieves these run-time gains through a combination of Two-Pass Hierarchical Attention and Dilated Context Update. Compared to the adapted baseline, NAM+ further decreases the in-domain WER by up to 12.6% relative, while incurring an out-of-domain WER regression of 20% relative. Compared to the non-adapted baseline, the out-of-domain WER regression is 7.1 % relative.","['https://openalex.org/W2403440562', 'https://openalex.org/W2293829681', 'https://openalex.org/W2972625221', 'https://openalex.org/W2889012072', 'https://openalex.org/W2886319145', 'https://openalex.org/W6679434410', 'https://openalex.org/W6638749077', 'https://openalex.org/W3097794466', 'https://openalex.org/W4225985539', 'https://openalex.org/W3016010032', 'https://openalex.org/W3202725408', 'https://openalex.org/W3097777922', 'https://openalex.org/W4225289150', 'https://openalex.org/W4226292626', 'https://openalex.org/W2971840980', 'https://openalex.org/W3201225328', 'https://openalex.org/W4225307083', 'https://openalex.org/W3197976839', 'https://openalex.org/W4385245566', 'https://openalex.org/W6631190155', 'https://openalex.org/W6760633627', 'https://openalex.org/W2953212265', 'https://openalex.org/W2963122170', 'https://openalex.org/W3163203022', 'https://openalex.org/W6843673214', 'https://openalex.org/W2617258110', 'https://openalex.org/W2928941594', 'https://openalex.org/W2133564696']",2023-01-09
https://openalex.org/W4385573964,https://doi.org/10.18653/v1/2022.emnlp-main.630,Overcoming Catastrophic Forgetting in Zero-Shot Cross-Lingual Generation,"In this paper, we explore the challenging problem of performing a generative task in a target language when labeled data is only available in English, using summarization as a case study. We assume a strict setting with no access to parallel data or machine translation and find that common transfer learning approaches struggle in this setting, as a generative multilingual model fine-tuned purely on English catastrophically forgets how to generate non-English. Given the recent rise of parameter-efficient adaptation techniques, we conduct the first investigation into how one such method, prompt tuning (Lester et al., 2021), can overcome catastrophic forgetting to enable zero-shot cross-lingual generation. Our experiments show that parameter-efficient prompt tuning provides gains over standard fine-tuning when transferring between less-related languages, e.g., from English to Thai. However, a significant gap still remains between these methods and fully-supervised baselines. To improve cross-lingual transfer further, we explore several approaches, including: (1) mixing in unlabeled multilingual data, and (2) explicitly factoring prompts into recombinable language and task components. Our approaches can provide further quality gains, suggesting that robust zero-shot cross-lingual generation is within reach.","['https://openalex.org/W3093517588', 'https://openalex.org/W2963026768', 'https://openalex.org/W4287827771', 'https://openalex.org/W3034469191', 'https://openalex.org/W4287775202', 'https://openalex.org/W3171961701', 'https://openalex.org/W3205949070', 'https://openalex.org/W4288089799', 'https://openalex.org/W3201363102', 'https://openalex.org/W3214173179', 'https://openalex.org/W3205068155', 'https://openalex.org/W2963250244', 'https://openalex.org/W2998653236', 'https://openalex.org/W3035390927', 'https://openalex.org/W4286987939', 'https://openalex.org/W3035579820', 'https://openalex.org/W3205616434', 'https://openalex.org/W2560647685', 'https://openalex.org/W3186655327', 'https://openalex.org/W4322614701', 'https://openalex.org/W3035497479', 'https://openalex.org/W4280534475', 'https://openalex.org/W3169483174', 'https://openalex.org/W2896457183', 'https://openalex.org/W4206529673', 'https://openalex.org/W4320086632', 'https://openalex.org/W3035252911', 'https://openalex.org/W4287122891', 'https://openalex.org/W2914120296', 'https://openalex.org/W3137010024', 'https://openalex.org/W3175759677', 'https://openalex.org/W2116522068', 'https://openalex.org/W3205717164', 'https://openalex.org/W4297801719', 'https://openalex.org/W3175575876', 'https://openalex.org/W3176828726', 'https://openalex.org/W2154652894', 'https://openalex.org/W2131296021', 'https://openalex.org/W2891555348', 'https://openalex.org/W2963211188', 'https://openalex.org/W2606974598', 'https://openalex.org/W3098068947', 'https://openalex.org/W4224442590', 'https://openalex.org/W3174770825', 'https://openalex.org/W2742113707', 'https://openalex.org/W4205991051', 'https://openalex.org/W3168867926', 'https://openalex.org/W2964303773', 'https://openalex.org/W4300963525', 'https://openalex.org/W2970752815', 'https://openalex.org/W4221151920', 'https://openalex.org/W3106445907', 'https://openalex.org/W3101498587']",2022-01-01
https://openalex.org/W3091928890,https://doi.org/10.48550/arxiv.2010.04301,Non-Attentive Tacotron: Robust and Controllable Neural TTS Synthesis Including Unsupervised Duration Modeling,"This paper presents Non-Attentive Tacotron based on the Tacotron 2 text-to-speech model, replacing the attention mechanism with an explicit duration predictor. This improves robustness significantly as measured by unaligned duration ratio and word deletion rate, two metrics introduced in this paper for large-scale robustness evaluation using a pre-trained speech recognition model. With the use of Gaussian upsampling, Non-Attentive Tacotron achieves a 5-scale mean opinion score for naturalness of 4.41, slightly outperforming Tacotron 2. The duration predictor enables both utterance-wide and per-phoneme control of duration at inference time. When accurate target durations are scarce or unavailable in the training data, we propose a method using a fine-grained variational auto-encoder to train the duration predictor in a semi-supervised or unsupervised manner, with results almost as good as supervised training.","['https://openalex.org/W2963403868', 'https://openalex.org/W1902237438', 'https://openalex.org/W2129142580', 'https://openalex.org/W3123097577', 'https://openalex.org/W2970006822', 'https://openalex.org/W2963590452', 'https://openalex.org/W3015922793', 'https://openalex.org/W3024747869', 'https://openalex.org/W2972951102', 'https://openalex.org/W2327501763', 'https://openalex.org/W2963927338', 'https://openalex.org/W2964243274', 'https://openalex.org/W2963432880', 'https://openalex.org/W1810943226', 'https://openalex.org/W2972359262', 'https://openalex.org/W2963609956', 'https://openalex.org/W2995181338', 'https://openalex.org/W2402737981', 'https://openalex.org/W1494198834', 'https://openalex.org/W2971753973', 'https://openalex.org/W2945544731', 'https://openalex.org/W2970730223', 'https://openalex.org/W3048487650', 'https://openalex.org/W2963300588', 'https://openalex.org/W2952269766', 'https://openalex.org/W3033913438', 'https://openalex.org/W2886769154', 'https://openalex.org/W3016021263', 'https://openalex.org/W3025013833', 'https://openalex.org/W2949382160', 'https://openalex.org/W2767206889', 'https://openalex.org/W2901997113', 'https://openalex.org/W2016589492', 'https://openalex.org/W3025793647', 'https://openalex.org/W2972495969', 'https://openalex.org/W2964307104', 'https://openalex.org/W2972970915', 'https://openalex.org/W1959608418', 'https://openalex.org/W3016136182', 'https://openalex.org/W2964281804', 'https://openalex.org/W2964308564', 'https://openalex.org/W3026041220', 'https://openalex.org/W2972702018', 'https://openalex.org/W3130016944', 'https://openalex.org/W2969521066', 'https://openalex.org/W854541894', 'https://openalex.org/W2964138190', 'https://openalex.org/W2963691546', 'https://openalex.org/W2903739847', 'https://openalex.org/W2963712897', 'https://openalex.org/W3093733783', 'https://openalex.org/W2102003408']",2020-10-08
https://openalex.org/W4377372369,https://doi.org/10.48550/arxiv.2305.11834,Pengi: An Audio Language Model for Audio Tasks,"In the domain of audio processing, Transfer Learning has facilitated the rise of Self-Supervised Learning and Zero-Shot Learning techniques. These approaches have led to the development of versatile models capable of tackling a wide array of tasks, while delivering state-of-the-art performance. However, current models inherently lack the capacity to produce the requisite language for open-ended tasks, such as Audio Captioning or Audio Question &amp; Answering. We introduce Pengi, a novel Audio Language Model that leverages Transfer Learning by framing all audio tasks as text-generation tasks. It takes as input, an audio recording, and text, and generates free-form text as output. The input audio is represented as a sequence of continuous embeddings by an audio encoder. A text encoder does the same for the corresponding text input. Both sequences are combined as a prefix to prompt a pre-trained frozen language model. The unified architecture of Pengi enables open-ended tasks and close-ended tasks without any additional fine-tuning or task-specific extensions. When evaluated on 22 downstream tasks, our approach yields state-of-the-art performance in several of them. Our results show that connecting language models with audio models is a major step towards general-purpose audio understanding",[],2023-05-19
https://openalex.org/W3139918052,https://doi.org/10.48550/arxiv.2104.02133,SpeechStew: Simply Mix All Available Speech Recognition Data to Train One Large Neural Network,"We present SpeechStew, a speech recognition model that is trained on a combination of various publicly available speech recognition datasets: AMI, Broadcast News, Common Voice, LibriSpeech, Switchboard/Fisher, Tedlium, and Wall Street Journal. SpeechStew simply mixes all of these datasets together, without any special re-weighting or re-balancing of the datasets. SpeechStew achieves SoTA or near SoTA results across a variety of tasks, without the use of an external language model. Our results include 9.0\% WER on AMI-IHM, 4.7\% WER on Switchboard, 8.3\% WER on CallHome, and 1.3\% on WSJ, which significantly outperforms prior work with strong external language models. We also demonstrate that SpeechStew learns powerful transfer learning representations. We fine-tune SpeechStew on a noisy low resource speech dataset, CHiME-6. We achieve 38.9\% WER without a language model, which compares to 38.6\% WER to a strong HMM baseline with a language model.","['https://openalex.org/W2155893237', 'https://openalex.org/W2168231600', 'https://openalex.org/W1494198834', 'https://openalex.org/W2962911098', 'https://openalex.org/W3098903812', 'https://openalex.org/W2155541015', 'https://openalex.org/W2102113734', 'https://openalex.org/W2963742216', 'https://openalex.org/W3030437843', 'https://openalex.org/W2936774411', 'https://openalex.org/W2995181338', 'https://openalex.org/W3131736947', 'https://openalex.org/W3036601975', 'https://openalex.org/W2996603747', 'https://openalex.org/W2327501763', 'https://openalex.org/W3026041220', 'https://openalex.org/W2125336414', 'https://openalex.org/W3015726069', 'https://openalex.org/W3100859887', 'https://openalex.org/W1828163288', 'https://openalex.org/W2963122170', 'https://openalex.org/W2108677974', 'https://openalex.org/W2250357346', 'https://openalex.org/W2900212944', 'https://openalex.org/W3094841848', 'https://openalex.org/W2123798005', 'https://openalex.org/W3099782249', 'https://openalex.org/W3093579165', 'https://openalex.org/W2973049979', 'https://openalex.org/W3016400019', 'https://openalex.org/W3035445001', 'https://openalex.org/W3097217077', 'https://openalex.org/W2804935296', 'https://openalex.org/W3097777922', 'https://openalex.org/W3093502935', 'https://openalex.org/W3001279689', 'https://openalex.org/W2964002616', 'https://openalex.org/W2530876040', 'https://openalex.org/W3101648800', 'https://openalex.org/W2991213871', 'https://openalex.org/W3149390355', 'https://openalex.org/W2163605009', 'https://openalex.org/W2962824709', 'https://openalex.org/W2889282842', 'https://openalex.org/W2939690918', 'https://openalex.org/W2963403868', 'https://openalex.org/W3147962056', 'https://openalex.org/W3018441253', 'https://openalex.org/W2963341956', 'https://openalex.org/W3131505732', 'https://openalex.org/W3015995734', 'https://openalex.org/W2194775991', 'https://openalex.org/W1978660892', 'https://openalex.org/W1524333225']",2021-04-05
https://openalex.org/W3169483174,https://doi.org/10.18653/v1/2021.naacl-main.41,mT5: A Massively Multilingual Pre-trained Text-to-Text Transformer,"Linting Xue, Noah Constant, Adam Roberts, Mihir Kale, Rami Al-Rfou, Aditya Siddhant, Aditya Barua, Colin Raffel. Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. 2021.","['https://openalex.org/W3105220303', 'https://openalex.org/W3039017601', 'https://openalex.org/W3126822054', 'https://openalex.org/W3098637735', 'https://openalex.org/W2963748441', 'https://openalex.org/W3032816972', 'https://openalex.org/W2965373594', 'https://openalex.org/W3034999214', 'https://openalex.org/W3023133114', 'https://openalex.org/W2970752815', 'https://openalex.org/W3040245432', 'https://openalex.org/W2989539713', 'https://openalex.org/W3013840636', 'https://openalex.org/W2963026768', 'https://openalex.org/W3085479580', 'https://openalex.org/W4287727940', 'https://openalex.org/W4389016403', 'https://openalex.org/W3006439205', 'https://openalex.org/W2996580882', 'https://openalex.org/W3082274269', 'https://openalex.org/W3045462440', 'https://openalex.org/W2999168658', 'https://openalex.org/W4287687023', 'https://openalex.org/W3102659883', 'https://openalex.org/W3035390927', 'https://openalex.org/W4293350112', 'https://openalex.org/W3105721709', 'https://openalex.org/W2742113707', 'https://openalex.org/W2963341956', 'https://openalex.org/W4287633642', 'https://openalex.org/W3107826490', 'https://openalex.org/W3035497479', 'https://openalex.org/W2891555348', 'https://openalex.org/W2914120296', 'https://openalex.org/W2963979492', 'https://openalex.org/W2963250244', 'https://openalex.org/W3103187652', 'https://openalex.org/W3032532958', 'https://openalex.org/W3037854022', 'https://openalex.org/W2958953787', 'https://openalex.org/W3034238904', 'https://openalex.org/W2809324505', 'https://openalex.org/W3023690688', 'https://openalex.org/W3116343068', 'https://openalex.org/W2990188683', 'https://openalex.org/W2986154550', 'https://openalex.org/W3042711927', 'https://openalex.org/W4287760320', 'https://openalex.org/W2963403868', 'https://openalex.org/W3099655892', 'https://openalex.org/W3081210419', 'https://openalex.org/W3034469191', 'https://openalex.org/W4301581299', 'https://openalex.org/W3100107515', 'https://openalex.org/W2953958347', 'https://openalex.org/W3097879195', 'https://openalex.org/W4385245566', 'https://openalex.org/W2940024477', 'https://openalex.org/W3156789018']",2021-01-01
https://openalex.org/W4221155340,https://doi.org/10.48550/arxiv.2202.01374,mSLAM: Massively multilingual joint pre-training for speech and text,"We present mSLAM, a multilingual Speech and LAnguage Model that learns cross-lingual cross-modal representations of speech and text by pre-training jointly on large amounts of unlabeled speech and text in multiple languages. mSLAM combines w2v-BERT pre-training on speech with SpanBERT pre-training on character-level text, along with Connectionist Temporal Classification (CTC) losses on paired speech and transcript data, to learn a single model capable of learning from and representing both speech and text signals in a shared representation space. We evaluate mSLAM on several downstream speech understanding tasks and find that joint pre-training with text improves quality on speech translation, speech intent classification and speech language-ID while being competitive on multilingual ASR, when compared against speech-only pre-training. Our speech translation model demonstrates zero-shot text translation without seeing any text translation data, providing evidence for cross-modal alignment of representations. mSLAM also benefits from multi-modal fine-tuning, further improving the quality of speech translation by directly leveraging text translation data during the fine-tuning process. Our empirical analysis highlights several opportunities and challenges arising from large-scale multimodal pre-training, suggesting directions for future research.",[],2022-02-03
https://openalex.org/W4377130946,https://doi.org/10.48550/arxiv.2305.10790,"Listen, Think, and Understand","The ability of artificial intelligence (AI) systems to perceive and comprehend audio signals is crucial for many applications. Although significant progress has been made in this area since the development of AudioSet, most existing models are designed to map audio inputs to pre-defined, discrete sound label sets. In contrast, humans possess the ability to not only classify sounds into general categories, but also to listen to the finer details of the sounds, explain the reason for the predictions, think about what the sound infers, and understand the scene and what action needs to be taken, if any. Such capabilities beyond perception are not yet present in existing audio models. On the other hand, modern large language models (LLMs) exhibit emerging reasoning ability but they lack audio perception capabilities. Therefore, we ask the question: can we build a model that has both audio perception and a reasoning ability? In this paper, we propose a new audio foundation model, called LTU (Listen, Think, and Understand). To train LTU, we created a new OpenAQA-5M dataset consisting of 1.9 million closed-ended and 3.7 million open-ended, diverse (audio, question, answer) tuples, and have used an autoregressive training framework with a perception-to-understanding curriculum. LTU demonstrates strong performance and generalization ability on conventional audio tasks such as classification and captioning. More importantly, it exhibits emerging audio reasoning and comprehension abilities that are absent in existing audio models. To the best of our knowledge, LTU is one of the first multimodal large language models that focus on general audio (rather than just speech) understanding.",[],2023-05-18
https://openalex.org/W4381827575,https://doi.org/10.48550/arxiv.2306.12925,AudioPaLM: A Large Language Model That Can Speak and Listen,"We introduce AudioPaLM, a large language model for speech understanding and generation. AudioPaLM fuses text-based and speech-based language models, PaLM-2 [Anil et al., 2023] and AudioLM [Borsos et al., 2022], into a unified multimodal architecture that can process and generate text and speech with applications including speech recognition and speech-to-speech translation. AudioPaLM inherits the capability to preserve paralinguistic information such as speaker identity and intonation from AudioLM and the linguistic knowledge present only in text large language models such as PaLM-2. We demonstrate that initializing AudioPaLM with the weights of a text-only large language model improves speech processing, successfully leveraging the larger quantity of text training data used in pretraining to assist with the speech tasks. The resulting model significantly outperforms existing systems for speech translation tasks and has the ability to perform zero-shot speech-to-text translation for many languages for which input/target language combinations were not seen in training. AudioPaLM also demonstrates features of audio language models, such as transferring a voice across languages based on a short spoken prompt. We release examples of our method at https://google-research.github.io/seanet/audiopalm/examples",[],2023-06-22
https://openalex.org/W3174770825,https://doi.org/10.18653/v1/2021.acl-long.353,Prefix-Tuning: Optimizing Continuous Prompts for Generation,"Xiang Lisa Li, Percy Liang. Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers). 2021.","['https://openalex.org/W2970419734', 'https://openalex.org/W3107826490', 'https://openalex.org/W4285719527', 'https://openalex.org/W3020268419', 'https://openalex.org/W3166846774', 'https://openalex.org/W2988937804', 'https://openalex.org/W4292779060', 'https://openalex.org/W3098267758', 'https://openalex.org/W3034999214', 'https://openalex.org/W3098824823', 'https://openalex.org/W3170806096', 'https://openalex.org/W4205991051', 'https://openalex.org/W3023285645', 'https://openalex.org/W3044438666', 'https://openalex.org/W3002104146', 'https://openalex.org/W3039127676', 'https://openalex.org/W3153427360', 'https://openalex.org/W2963341956', 'https://openalex.org/W3153675281', 'https://openalex.org/W2133512280', 'https://openalex.org/W3006381853', 'https://openalex.org/W2929900303', 'https://openalex.org/W2936695845', 'https://openalex.org/W3023622314', 'https://openalex.org/W2283463896', 'https://openalex.org/W2973049837', 'https://openalex.org/W2994928925', 'https://openalex.org/W2997195635', 'https://openalex.org/W2963211188', 'https://openalex.org/W4288480287', 'https://openalex.org/W2971147102', 'https://openalex.org/W2149327368', 'https://openalex.org/W2970785793', 'https://openalex.org/W4297795751', 'https://openalex.org/W2101105183', 'https://openalex.org/W3035252911', 'https://openalex.org/W2888482885', 'https://openalex.org/W3085190015', 'https://openalex.org/W3139080614', 'https://openalex.org/W3118216348', 'https://openalex.org/W2979826702', 'https://openalex.org/W3153805297', 'https://openalex.org/W2965373594', 'https://openalex.org/W3082274269', 'https://openalex.org/W2908510526', 'https://openalex.org/W1763968285', 'https://openalex.org/W2996403597', 'https://openalex.org/W3152956381', 'https://openalex.org/W2053637704', 'https://openalex.org/W1956340063', 'https://openalex.org/W3115894062', 'https://openalex.org/W3103616906', 'https://openalex.org/W3035050380', 'https://openalex.org/W2956352737', 'https://openalex.org/W3023528699', 'https://openalex.org/W2154652894', 'https://openalex.org/W3174784402', 'https://openalex.org/W3177323791', 'https://openalex.org/W2964303773', 'https://openalex.org/W2786660442']",2021-01-01
https://openalex.org/W4285247752,https://doi.org/10.18653/v1/2022.acl-short.8,P-Tuning: Prompt Tuning Can Be Comparable to Fine-tuning Across Scales and Tasks,"Prompt tuning, which only tunes continuous prompts with a frozen language model, substantially reduces per-task storage and memory usage at training. However, in the context of NLU, prior work reveals that prompt tuning does not perform well for normal-sized pretrained models. We also find that existing methods of prompt tuning cannot handle hard sequence labeling tasks, indicating a lack of universality. We present a novel empirical finding that properly optimized prompt tuning can be universally effective across a wide range of model scales and NLU tasks. It matches the performance of finetuning while having only 0.1%-3% tuned parameters. Our method P-Tuning v2 is an implementation of Deep Prompt Tuning (CITATION) optimized and adapted for NLU. Given the universality and simplicity of P-Tuning v2, we believe it can serve as an alternative to finetuning and a strong baseline for future research.","['https://openalex.org/W3166986030', 'https://openalex.org/W4287026929', 'https://openalex.org/W3172642864', 'https://openalex.org/W2896457183', 'https://openalex.org/W2952087486', 'https://openalex.org/W3173777717', 'https://openalex.org/W3166846774', 'https://openalex.org/W4205991051', 'https://openalex.org/W2970597249', 'https://openalex.org/W3159084885', 'https://openalex.org/W4288089799', 'https://openalex.org/W4229506649', 'https://openalex.org/W2923014074', 'https://openalex.org/W4286961470', 'https://openalex.org/W3174770825', 'https://openalex.org/W3137214022', 'https://openalex.org/W2963748441', 'https://openalex.org/W3200814992', 'https://openalex.org/W2943552823', 'https://openalex.org/W3154560120', 'https://openalex.org/W4297795751', 'https://openalex.org/W3098267758', 'https://openalex.org/W2155069789', 'https://openalex.org/W3122890974', 'https://openalex.org/W4286981949', 'https://openalex.org/W3173783648', 'https://openalex.org/W4292779060', 'https://openalex.org/W2915816387', 'https://openalex.org/W2965373594']",2022-01-01
https://openalex.org/W4319862642,https://doi.org/10.1109/slt54892.2023.10023274,Exploring Efficient-Tuning Methods in Self-Supervised Speech Models,"In this study, we aim to explore efficient tuning methods for speech self-supervised learning. Recent studies show that self-supervised learning (SSL) can learn powerful representations for different speech tasks. However, fine-tuning pre-trained models for each downstream task is parameter-inefficient since SSL models are notoriously large with millions of parameters. Adapters are lightweight modules commonly used in NLP to solve this problem. In downstream tasks, the parameters of SSL models are frozen, and only the adapters are trained. Given the lack of studies generally exploring the effectiveness of adapters for self-supervised speech tasks, we intend to fill this gap by adding various adapter modules in pre-trained speech SSL models. We show that the performance parity can be achieved with over 90% parameter reduction, and discussed the pros and cons of efficient tuning techniques. This is the first comprehensive investigation of various adapter types across speech tasks.","['https://openalex.org/W4281492411', 'https://openalex.org/W3209059054', 'https://openalex.org/W3197580070', 'https://openalex.org/W6759579507', 'https://openalex.org/W3174770825', 'https://openalex.org/W3200396895', 'https://openalex.org/W3101498587', 'https://openalex.org/W4225274946', 'https://openalex.org/W6780218876', 'https://openalex.org/W6786669483', 'https://openalex.org/W6778883912', 'https://openalex.org/W6796581206', 'https://openalex.org/W4225410153', 'https://openalex.org/W3176693010', 'https://openalex.org/W4283073456', 'https://openalex.org/W4224930323', 'https://openalex.org/W4226162428', 'https://openalex.org/W6790356757', 'https://openalex.org/W3176828726', 'https://openalex.org/W6802744804', 'https://openalex.org/W2127141656', 'https://openalex.org/W1494198834', 'https://openalex.org/W4226380987', 'https://openalex.org/W4394671563', 'https://openalex.org/W3036601975', 'https://openalex.org/W4292779060', 'https://openalex.org/W3112034174', 'https://openalex.org/W3168867926']",2023-01-09
https://openalex.org/W4225410153,https://doi.org/10.18653/v1/2022.findings-naacl.199,AdapterBias: Parameter-efficient Token-dependent Representation Shift for Adapters in NLP Tasks,"Transformer-based pre-trained models with millions of parameters require large storage. Recent approaches tackle this shortcoming by training adapters, but these approaches still require a relatively large number of parameters. In this study, AdapterBias, a surprisingly simple yet effective adapter architecture, is proposed. AdapterBias adds a token-dependent shift to the hidden output of transformer layers to adapt to downstream tasks with only a vector and a linear layer. Extensive experiments are conducted to demonstrate the effectiveness of AdapterBias. The experiments show that our proposed method can dramatically reduce the trainable parameters compared to the previous works with a minimal decrease in task performances compared with fine-tuned pre-trained models. We further find that AdapterBias automatically learns to assign more significant representation shifts to the tokens related to the task in consideration.","['https://openalex.org/W6631349028', 'https://openalex.org/W4322588812', 'https://openalex.org/W2251939518', 'https://openalex.org/W2908510526', 'https://openalex.org/W3168867926', 'https://openalex.org/W4288410857', 'https://openalex.org/W3174702398', 'https://openalex.org/W2948947170', 'https://openalex.org/W2963828549', 'https://openalex.org/W2963854351', 'https://openalex.org/W2970120757', 'https://openalex.org/W4253067820', 'https://openalex.org/W2742079690', 'https://openalex.org/W3176828726', 'https://openalex.org/W4287122891', 'https://openalex.org/W4385245566', 'https://openalex.org/W3153675281', 'https://openalex.org/W2799124508', 'https://openalex.org/W2547875792', 'https://openalex.org/W2923014074', 'https://openalex.org/W2548228487', 'https://openalex.org/W4313908941', 'https://openalex.org/W2965373594', 'https://openalex.org/W3099793224', 'https://openalex.org/W4206178588', 'https://openalex.org/W2896457183', 'https://openalex.org/W2964303773', 'https://openalex.org/W2963846996', 'https://openalex.org/W2946417913', 'https://openalex.org/W2970925270', 'https://openalex.org/W2980282514', 'https://openalex.org/W2978670439', 'https://openalex.org/W2963748441']",2022-01-01
https://openalex.org/W3176693010,https://doi.org/10.18653/v1/2021.acl-long.172,On the Effectiveness of Adapter-based Tuning for Pretrained Language Model Adaptation,"Ruidan He, Linlin Liu, Hai Ye, Qingyu Tan, Bosheng Ding, Liying Cheng, Jiawei Low, Lidong Bing, Luo Si. Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers). 2021.","['https://openalex.org/W2963403868', 'https://openalex.org/W1682403713', 'https://openalex.org/W3007685714', 'https://openalex.org/W4322614701', 'https://openalex.org/W2971033911', 'https://openalex.org/W2948337921', 'https://openalex.org/W2973047874', 'https://openalex.org/W4287692509', 'https://openalex.org/W2964067969', 'https://openalex.org/W2898700502', 'https://openalex.org/W2964352358', 'https://openalex.org/W2777662428', 'https://openalex.org/W3034469191', 'https://openalex.org/W2963026768', 'https://openalex.org/W2975185270', 'https://openalex.org/W2965373594', 'https://openalex.org/W3127622310', 'https://openalex.org/W4206178588', 'https://openalex.org/W2946296745', 'https://openalex.org/W4287867774', 'https://openalex.org/W2963310665', 'https://openalex.org/W2970925270', 'https://openalex.org/W3006647218', 'https://openalex.org/W2060277733', 'https://openalex.org/W4292779060', 'https://openalex.org/W3034199299', 'https://openalex.org/W3003289092', 'https://openalex.org/W2964121744', 'https://openalex.org/W3034238904', 'https://openalex.org/W2896457183', 'https://openalex.org/W2980708516', 'https://openalex.org/W3175604467', 'https://openalex.org/W2980282514', 'https://openalex.org/W2912811302', 'https://openalex.org/W2911300548', 'https://openalex.org/W4385245566', 'https://openalex.org/W2963341956', 'https://openalex.org/W2995998574', 'https://openalex.org/W3104215796', 'https://openalex.org/W2915774325', 'https://openalex.org/W3023528699', 'https://openalex.org/W3099793224', 'https://openalex.org/W3035204084', 'https://openalex.org/W1522301498', 'https://openalex.org/W3035579820', 'https://openalex.org/W2915977242', 'https://openalex.org/W3153675281', 'https://openalex.org/W2923014074', 'https://openalex.org/W2891555348', 'https://openalex.org/W3126074026', 'https://openalex.org/W3120490999', 'https://openalex.org/W3005700362', 'https://openalex.org/W3093345276', 'https://openalex.org/W3023911605', 'https://openalex.org/W2742113707', 'https://openalex.org/W2964303773', 'https://openalex.org/W2962933129', 'https://openalex.org/W3005441132', 'https://openalex.org/W3100311862', 'https://openalex.org/W3094491777', 'https://openalex.org/W3101498587', 'https://openalex.org/W3035390927', 'https://openalex.org/W2009284521', 'https://openalex.org/W4322588812', 'https://openalex.org/W3103368673']",2021-01-01
https://openalex.org/W4319862652,https://doi.org/10.1109/slt54892.2023.10023234,On the Utility of Self-Supervised Models for Prosody-Related Tasks,"Self-Supervised Learning (SSL) from speech data has produced models that have achieved remarkable performance in many tasks, and that are known to implicitly represent many aspects of information latently present in speech signals. However, relatively little is known about the suitability of such models for prosody-related tasks or the extent to which they encode prosodic information. We present a new evaluation framework, ""SUPERB-prosody,"" consisting of three prosody-related downstream tasks and two pseudo tasks. We find that 13 of the 15 SSL models outperformed the baseline on all the prosody-related tasks. We also show good performance on two pseudo tasks: prosody reconstruction and future prosody prediction. We further analyze the layerwise contributions of the SSL models. Overall we conclude that SSL speech models are highly effective for prosody-related tasks. We release our code <sup xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">1</sup> <sup xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">1</sup> https://github.com/JSALT-2022-SSL/superb-prosody for the community to support further investigation of SSL models' utility for prosody.","['https://openalex.org/W3197580070', 'https://openalex.org/W4285250921', 'https://openalex.org/W4281492411', 'https://openalex.org/W3182074706', 'https://openalex.org/W2921495256', 'https://openalex.org/W6881984725', 'https://openalex.org/W6795949861', 'https://openalex.org/W4296068425', 'https://openalex.org/W3006926732', 'https://openalex.org/W4226103796', 'https://openalex.org/W4226380987', 'https://openalex.org/W6788328058', 'https://openalex.org/W3162133897', 'https://openalex.org/W2972943112', 'https://openalex.org/W3097286738', 'https://openalex.org/W3198858531', 'https://openalex.org/W2982223350', 'https://openalex.org/W3041561163', 'https://openalex.org/W3005511757', 'https://openalex.org/W2973049979', 'https://openalex.org/W6769196770', 'https://openalex.org/W3203140070', 'https://openalex.org/W6780218876', 'https://openalex.org/W3209059054', 'https://openalex.org/W3209984917', 'https://openalex.org/W2883409523', 'https://openalex.org/W2963349408', 'https://openalex.org/W2075398069', 'https://openalex.org/W6761768988', 'https://openalex.org/W4210296532', 'https://openalex.org/W2546919788', 'https://openalex.org/W6766673545', 'https://openalex.org/W6776048684', 'https://openalex.org/W3198533616', 'https://openalex.org/W3095410713', 'https://openalex.org/W4296070431', 'https://openalex.org/W3121914243', 'https://openalex.org/W3015468748', 'https://openalex.org/W3036601975', 'https://openalex.org/W2224323142', 'https://openalex.org/W2972359262', 'https://openalex.org/W2979476256', 'https://openalex.org/W1583837637', 'https://openalex.org/W3016181583', 'https://openalex.org/W2965373594']",2023-01-09
https://openalex.org/W4385807453,https://doi.org/10.21437/interspeech.2023-2193,Whisper-AT: Noise-Robust Automatic Speech Recognizers are Also Strong General Audio Event Taggers,"In this paper, we focus on Whisper [1], a recent automatic speech recognition model trained with a massive 680k hour labeled speech corpus recorded in diverse conditions.We first show an interesting finding that while Whisper is very robust against real-world background sounds (e.g., music), its audio representation is actually not noise-invariant, but is instead highly correlated to non-speech sounds, indicating that Whisper recognizes speech conditioned on the noise type.With this finding, we build a unified audio tagging and speech recognition model Whisper-AT by freezing the backbone of Whisper, and training a lightweight audio tagging model on top of it.With <1% extra computational cost, Whisper-AT can recognize audio events, in addition to spoken text, in a single forward pass.","['https://openalex.org/W2584667682', 'https://openalex.org/W4297841853', 'https://openalex.org/W2995181338', 'https://openalex.org/W4221150524', 'https://openalex.org/W3206996142', 'https://openalex.org/W2062164080', 'https://openalex.org/W1522301498', 'https://openalex.org/W4385245566', 'https://openalex.org/W4398958419', 'https://openalex.org/W3095738461', 'https://openalex.org/W4311000453', 'https://openalex.org/W3174906557', 'https://openalex.org/W4285483774', 'https://openalex.org/W3209059054', 'https://openalex.org/W4224932123', 'https://openalex.org/W3036601975', 'https://openalex.org/W4221140371', 'https://openalex.org/W3198771897', 'https://openalex.org/W3205743929', 'https://openalex.org/W2593116425', 'https://openalex.org/W3196974791', 'https://openalex.org/W1494198834', 'https://openalex.org/W3094550259', 'https://openalex.org/W2129674987', 'https://openalex.org/W2962909949', 'https://openalex.org/W2885724687']",2023-08-14
https://openalex.org/W4379539302,https://doi.org/10.48550/arxiv.2306.02207,SpeechGen: Unlocking the Generative Power of Speech Language Models with Prompts,"Large language models (LLMs) have gained considerable attention for Artificial Intelligence Generated Content (AIGC), particularly with the emergence of ChatGPT. However, the direct adaptation of continuous speech to LLMs that process discrete tokens remains an unsolved challenge, hindering the application of LLMs for speech generation. The advanced speech LMs are in the corner, as that speech signals encapsulate a wealth of information, including speaker and emotion, beyond textual data alone. Prompt tuning has demonstrated notable gains in parameter efficiency and competitive performance on some speech classification tasks. However, the extent to which prompts can effectively elicit generation tasks from speech LMs remains an open question. In this paper, we present pioneering research that explores the application of prompt tuning to stimulate speech LMs for various generation tasks, within a unified framework called SpeechGen, with around 10M trainable parameters. The proposed unified framework holds great promise for efficiency and effectiveness, particularly with the imminent arrival of advanced speech LMs, which will significantly enhance the capabilities of the framework. The code and demos of SpeechGen will be available on the project website: \url{https://ga642381.github.io/SpeechPrompt/speechgen}",[],2023-06-03
https://openalex.org/W4322825254,https://doi.org/10.48550/arxiv.2303.00733,SpeechPrompt v2: Prompt Tuning for Speech Classification Tasks,"Prompt tuning is a technology that tunes a small set of parameters to steer a pre-trained language model (LM) to directly generate the output for downstream tasks. Recently, prompt tuning has demonstrated its storage and computation efficiency in both natural language processing (NLP) and speech processing fields. These advantages have also revealed prompt tuning as a candidate approach to serving pre-trained LM for multiple tasks in a unified manner. For speech processing, SpeechPrompt shows its high parameter efficiency and competitive performance on a few speech classification tasks. However, whether SpeechPrompt is capable of serving a large number of tasks is unanswered. In this work, we propose SpeechPrompt v2, a prompt tuning framework capable of performing a wide variety of speech classification tasks, covering multiple languages and prosody-related tasks. The experiment result shows that SpeechPrompt v2 achieves performance on par with prior works with less than 0.15M trainable parameters in a unified framework.",[],2023-03-01
https://openalex.org/W4307322847,https://doi.org/10.48550/arxiv.2210.13352,ESB: A Benchmark For Multi-Domain End-to-End Speech Recognition,"Speech recognition applications cover a range of different audio and text distributions, with different speaking styles, background noise, transcription punctuation and character casing. However, many speech recognition systems require dataset-specific tuning (audio filtering, punctuation removal and normalisation of casing), therefore assuming a-priori knowledge of both the audio and text distributions. This tuning requirement can lead to systems failing to generalise to other datasets and domains. To promote the development of multi-domain speech systems, we introduce the End-to-end Speech Benchmark (ESB) for evaluating the performance of a single automatic speech recognition (ASR) system across a broad set of speech datasets. Benchmarked systems must use the same data pre- and post-processing algorithm across datasets - assuming the audio and text data distributions are a-priori unknown. We compare a series of state-of-the-art (SoTA) end-to-end (E2E) systems on this benchmark, demonstrating how a single speech system can be applied and evaluated on a wide range of data distributions. We find E2E systems to be effective across datasets: in a fair comparison, E2E systems achieve within 2.6% of SoTA systems tuned to a specific dataset. Our analysis reveals that transcription artefacts, such as punctuation and casing, pose difficulties for ASR systems and should be included in evaluation. We believe E2E benchmarking over a range of datasets promotes the research of multi-domain speech recognition systems. ESB is available at https://huggingface.co/esb.",[],2022-10-24
https://openalex.org/W2998572311,https://doi.org/10.7488/ds/2645,CSTR VCTK Corpus: English Multi-speaker Corpus for CSTR Voice Cloning Toolkit (version 0.92),"This CSTR VCTK Corpus includes speech data uttered by 110 English speakers with various accents. Each speaker reads out about 400 sentences, which were selected from a newspaper, the rainbow passage and an elicitation paragraph used for the speech accent archive. The newspaper texts were taken from Herald Glasgow, with permission from Herald &amp; Times Group. Each speaker has a different set of the newspaper texts selected based a greedy algorithm that increases the contextual and phonetic coverage. The details of the text selection algorithms are described in the following paper: C. Veaux, J. Yamagishi and S. King, ""The voice bank corpus: Design, collection and data analysis of a large regional accent speech database,"" https://doi.org/10.1109/ICSDA.2013.6709856 The rainbow passage and elicitation paragraph are the same for all speakers. The rainbow passage can be found at International Dialects of English Archive: (http://web.ku.edu/~idea/readings/rainbow.htm). The elicitation paragraph is identical to the one used for the speech accent archive (http://accent.gmu.edu). The details of the the speech accent archive can be found at http://www.ualberta.ca/~aacl2009/PDFs/WeinbergerKunath2009AACL.pdf All speech data was recorded using an identical recording setup: an omni-directional microphone (DPA 4035) and a small diaphragm condenser microphone with very wide bandwidth (Sennheiser MKH 800), 96kHz sampling frequency at 24 bits and in a hemi-anechoic chamber of the University of Edinburgh. (However, two speakers, p280 and p315 had technical issues of the audio recordings using MKH 800). All recordings were converted into 16 bits, were downsampled to 48 kHz, and were manually end-pointed. This corpus was originally aimed for HMM-based text-to-speech synthesis systems, especially for speaker-adaptive HMM-based speech synthesis that uses average voice models trained on multiple speakers and speaker adaptation technologies. This corpus is also suitable for DNN-based multi-speaker text-to-speech synthesis systems and neural waveform modeling. The dataset was was referenced in the Google DeepMind work on WaveNet: https://arxiv.org/pdf/1609.03499.pdf . Please note while text files containing transcripts of the speech are provided for 109 of the 110 recordings, in the '/txt' folder, the 'p315' text was lost due to a hard disk error.",[],2019-01-01
