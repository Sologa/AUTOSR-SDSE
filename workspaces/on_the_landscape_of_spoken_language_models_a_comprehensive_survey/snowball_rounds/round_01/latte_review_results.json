[
  {
    "title": "Speech-Text Pre-training for Spoken Dialog Understanding with Explicit Cross-Modal Alignment",
    "abstract": "Tianshu Yu, Haoyu Gao, Ting-En Lin, Min Yang, Yuchuan Wu, Wentao Ma, Chao Wang, Fei Huang, Yongbin Li. Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). 2023.",
    "metadata": {
      "title": "Speech-Text Pre-training for Spoken Dialog Understanding with Explicit Cross-Modal Alignment",
      "summary": "Tianshu Yu, Haoyu Gao, Ting-En Lin, Min Yang, Yuchuan Wu, Wentao Ma, Chao Wang, Fei Huang, Yongbin Li. Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). 2023.",
      "abstract": "Tianshu Yu, Haoyu Gao, Ting-En Lin, Min Yang, Yuchuan Wu, Wentao Ma, Chao Wang, Fei Huang, Yongbin Li. Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). 2023.",
      "doi": "https://doi.org/10.18653/v1/2023.acl-long.438",
      "openalex_id": "https://openalex.org/W4385571111",
      "arxiv_id": "",
      "publication_date": "2023-01-01",
      "published": "2023-01-01",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This item centers on spoken dialog understanding with speech-text pretraining and explicit cross-modal alignment, treats speech as the modeling target and discusses architecture, training, and evaluation, within the last 3 years, thus meeting the inclusion criteria and not the exclusion criteria.",
      "evaluation": 4
    },
    "round-A_JuniorNano_reasoning": "This item centers on spoken dialog understanding with speech-text pretraining and explicit cross-modal alignment, treats speech as the modeling target and discusses architecture, training, and evaluation, within the last 3 years, thus meeting the inclusion criteria and not the exclusion criteria.",
    "round-A_JuniorNano_evaluation": 4,
    "round-A_JuniorMini_output": {
      "reasoning": "The title and abstract indicate a focus on Speech-Text pre-training for spoken dialog understanding with explicit cross-modal alignment, suggesting involvement with speech-language model architectures; however, the abstract lacks detail about whether the study addresses full-duplex spoken language models, explicit synchronization or cognitive parallelism, and does not mention specific evaluation metrics related to low-latency interaction or behavior arbitration, making it unclear if all inclusion criteria particularly about full-duplex interaction and detailed evaluation aspects are met.",
      "evaluation": 3
    },
    "round-A_JuniorMini_reasoning": "The title and abstract indicate a focus on Speech-Text pre-training for spoken dialog understanding with explicit cross-modal alignment, suggesting involvement with speech-language model architectures; however, the abstract lacks detail about whether the study addresses full-duplex spoken language models, explicit synchronization or cognitive parallelism, and does not mention specific evaluation metrics related to low-latency interaction or behavior arbitration, making it unclear if all inclusion criteria particularly about full-duplex interaction and detailed evaluation aspects are met.",
    "round-A_JuniorMini_evaluation": 3,
    "round-B_SeniorLead_output": {
      "reasoning": "I reviewed the two junior reviewers' notes and the bibliographic entry: the paper focuses on speech–text pre-training for spoken dialog understanding and presents cross-modal alignment (ACL 2023), which is within the 3‑year window; however the title/available info does not use the required “Spoken Language Model / speech language model / language modeling of speech” phrasing, and there is no evidence in the provided abstract/citation of treating speech as the primary modeling/inference object (rather than as an input to improve dialog understanding), nor of addressing full‑duplex interaction, overlap/interrupt handling, low‑latency synchronization, or an evaluation framework for such behaviors. Junior reviewers conflicted (one leaned include, one uncertain), but given the lack of explicit SLM framing and lack of any sign of full‑duplex or behavior‑arbitration evaluation, this fails the inclusion requirements and matches exclusion criteria (speech used mainly as input for dialog understanding).",
      "evaluation": 2
    },
    "round-B_SeniorLead_reasoning": "I reviewed the two junior reviewers' notes and the bibliographic entry: the paper focuses on speech–text pre-training for spoken dialog understanding and presents cross-modal alignment (ACL 2023), which is within the 3‑year window; however the title/available info does not use the required “Spoken Language Model / speech language model / language modeling of speech” phrasing, and there is no evidence in the provided abstract/citation of treating speech as the primary modeling/inference object (rather than as an input to improve dialog understanding), nor of addressing full‑duplex interaction, overlap/interrupt handling, low‑latency synchronization, or an evaluation framework for such behaviors. Junior reviewers conflicted (one leaned include, one uncertain), but given the lack of explicit SLM framing and lack of any sign of full‑duplex or behavior‑arbitration evaluation, this fails the inclusion requirements and matches exclusion criteria (speech used mainly as input for dialog understanding).",
    "round-B_SeniorLead_evaluation": 2,
    "final_verdict": "exclude (senior:2)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "UniSA: Unified Generative Framework for Sentiment Analysis",
    "abstract": "Sentiment analysis is a crucial task that aims to understand people's emotional states and predict emotional categories based on multimodal information. It consists of several subtasks, such as emotion recognition in conversation (ERC), aspect-based sentiment analysis (ABSA), and multimodal sentiment analysis (MSA). However, unifying all subtasks in sentiment analysis presents numerous challenges, including modality alignment, unified input/output forms, and dataset bias. To address these challenges, we propose a Task-Specific Prompt method to jointly model subtasks and introduce a multimodal generative framework called UniSA. Additionally, we organize the benchmark datasets of main subtasks into a new Sentiment Analysis Evaluation benchmark, SAEval. We design novel pre-training tasks and training methods to enable the model to learn generic sentiment knowledge among subtasks to improve the model's multimodal sentiment perception ability. Our experimental results show that UniSA performs comparably to the state-of-the-art on all subtasks and generalizes well to various subtasks in sentiment analysis.",
    "metadata": {
      "title": "UniSA: Unified Generative Framework for Sentiment Analysis",
      "summary": "Sentiment analysis is a crucial task that aims to understand people's emotional states and predict emotional categories based on multimodal information. It consists of several subtasks, such as emotion recognition in conversation (ERC), aspect-based sentiment analysis (ABSA), and multimodal sentiment analysis (MSA). However, unifying all subtasks in sentiment analysis presents numerous challenges, including modality alignment, unified input/output forms, and dataset bias. To address these challenges, we propose a Task-Specific Prompt method to jointly model subtasks and introduce a multimodal generative framework called UniSA. Additionally, we organize the benchmark datasets of main subtasks into a new Sentiment Analysis Evaluation benchmark, SAEval. We design novel pre-training tasks and training methods to enable the model to learn generic sentiment knowledge among subtasks to improve the model's multimodal sentiment perception ability. Our experimental results show that UniSA performs comparably to the state-of-the-art on all subtasks and generalizes well to various subtasks in sentiment analysis.",
      "abstract": "Sentiment analysis is a crucial task that aims to understand people's emotional states and predict emotional categories based on multimodal information. It consists of several subtasks, such as emotion recognition in conversation (ERC), aspect-based sentiment analysis (ABSA), and multimodal sentiment analysis (MSA). However, unifying all subtasks in sentiment analysis presents numerous challenges, including modality alignment, unified input/output forms, and dataset bias. To address these challenges, we propose a Task-Specific Prompt method to jointly model subtasks and introduce a multimodal generative framework called UniSA. Additionally, we organize the benchmark datasets of main subtasks into a new Sentiment Analysis Evaluation benchmark, SAEval. We design novel pre-training tasks and training methods to enable the model to learn generic sentiment knowledge among subtasks to improve the model's multimodal sentiment perception ability. Our experimental results show that UniSA performs comparably to the state-of-the-art on all subtasks and generalizes well to various subtasks in sentiment analysis.",
      "doi": "https://doi.org/10.1145/3581783.3612336",
      "openalex_id": "https://openalex.org/W4387969507",
      "arxiv_id": "",
      "publication_date": "2023-10-26",
      "published": "2023-10-26",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item centers on multimodal sentiment analysis and does not treat Spoken Language Models or speech-centric modeling; there is no focus on audio as the core modeling/inference object or on end-to-end speech–text architectures, so it does not meet the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item centers on multimodal sentiment analysis and does not treat Spoken Language Models or speech-centric modeling; there is no focus on audio as the core modeling/inference object or on end-to-end speech–text architectures, so it does not meet the inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on sentiment analysis using a multimodal generative framework, but it does not center on spoken language models or speech language modeling as a core element; it lacks discussion on modeling or reasoning primarily on speech signals, full-duplex interaction, or synchronization aspects required by the inclusion criteria. It primarily discusses sentiment tasks rather than speech sequence or acoustic modeling, and there's no mention of speech-based language modeling or full-duplex interactions.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on sentiment analysis using a multimodal generative framework, but it does not center on spoken language models or speech language modeling as a core element; it lacks discussion on modeling or reasoning primarily on speech signals, full-duplex interaction, or synchronization aspects required by the inclusion criteria. It primarily discusses sentiment tasks rather than speech sequence or acoustic modeling, and there's no mention of speech-based language modeling or full-duplex interactions.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Unsupervised Dialogue Topic Segmentation with Topic-aware Contrastive Learning",
    "abstract": "Dialogue Topic Segmentation (DTS) plays an essential role in a variety of dialogue modeling tasks. Previous DTS methods either focus on semantic similarity or dialogue coherence to assess topic similarity for unsupervised dialogue segmentation. However, the topic similarity cannot be fully identified via semantic similarity or dialogue coherence. In addition, the unlabeled dialogue data, which contains useful clues of utterance relationships, remains underexploited. In this paper, we propose a novel unsupervised DTS framework, which learns topic-aware utterance representations from unlabeled dialogue data through neighboring utterance matching and pseudo-segmentation. Extensive experiments on two benchmark datasets (i.e., DialSeg711 and Doc2Dial) demonstrate that our method significantly outperforms the strong baseline methods. For reproducibility, we provide our code and data at: https://github.com/AlibabaResearch/DAMO-ConvAI/tree/main/dial-start.",
    "metadata": {
      "title": "Unsupervised Dialogue Topic Segmentation with Topic-aware Contrastive Learning",
      "summary": "Dialogue Topic Segmentation (DTS) plays an essential role in a variety of dialogue modeling tasks. Previous DTS methods either focus on semantic similarity or dialogue coherence to assess topic similarity for unsupervised dialogue segmentation. However, the topic similarity cannot be fully identified via semantic similarity or dialogue coherence. In addition, the unlabeled dialogue data, which contains useful clues of utterance relationships, remains underexploited. In this paper, we propose a novel unsupervised DTS framework, which learns topic-aware utterance representations from unlabeled dialogue data through neighboring utterance matching and pseudo-segmentation. Extensive experiments on two benchmark datasets (i.e., DialSeg711 and Doc2Dial) demonstrate that our method significantly outperforms the strong baseline methods. For reproducibility, we provide our code and data at: https://github.com/AlibabaResearch/DAMO-ConvAI/tree/main/dial-start.",
      "abstract": "Dialogue Topic Segmentation (DTS) plays an essential role in a variety of dialogue modeling tasks. Previous DTS methods either focus on semantic similarity or dialogue coherence to assess topic similarity for unsupervised dialogue segmentation. However, the topic similarity cannot be fully identified via semantic similarity or dialogue coherence. In addition, the unlabeled dialogue data, which contains useful clues of utterance relationships, remains underexploited. In this paper, we propose a novel unsupervised DTS framework, which learns topic-aware utterance representations from unlabeled dialogue data through neighboring utterance matching and pseudo-segmentation. Extensive experiments on two benchmark datasets (i.e., DialSeg711 and Doc2Dial) demonstrate that our method significantly outperforms the strong baseline methods. For reproducibility, we provide our code and data at: https://github.com/AlibabaResearch/DAMO-ConvAI/tree/main/dial-start.",
      "doi": "https://doi.org/10.1145/3539618.3592081",
      "openalex_id": "https://openalex.org/W4384890868",
      "arxiv_id": "",
      "publication_date": "2023-07-18",
      "published": "2023-07-18",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The title/abstract describe unsupervised dialogue topic segmentation using textual utterances without any focus on spoken language models or speech signals; there is no speech modeling, training, or evaluation evident, so it does not meet the Spoken Language Models (SLMs/FD-SLMs) inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The title/abstract describe unsupervised dialogue topic segmentation using textual utterances without any focus on spoken language models or speech signals; there is no speech modeling, training, or evaluation evident, so it does not meet the Spoken Language Models (SLMs/FD-SLMs) inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on unsupervised dialogue topic segmentation using text-based dialogue data, without explicit mention of spoken language models, speech signal modeling, or full-duplex spoken interaction aspects such as low-latency synchronization or speech overlap management. It does not meet the inclusion criteria centered on spoken language modeling or speech-based architectures, and it does not address any exclusion aspects but mainly is unrelated to the core topic of spoken language models and their real-time interaction capabilities.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on unsupervised dialogue topic segmentation using text-based dialogue data, without explicit mention of spoken language models, speech signal modeling, or full-duplex spoken interaction aspects such as low-latency synchronization or speech overlap management. It does not meet the inclusion criteria centered on spoken language modeling or speech-based architectures, and it does not address any exclusion aspects but mainly is unrelated to the core topic of spoken language models and their real-time interaction capabilities.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "A Humanoid Robot Dialogue System Architecture Targeting Patient Interview Tasks",
    "abstract": "Humanoid robots are promising approach to automating patient interviews routinely conducted by medical staff. Their human-like appearance enables them to use the full gamut of verbal and behavioral cues that are critical to a successful interview. On the other hand, anthropomorphism can induce expectations of human-level performance by the robot. Not meeting such expectations degrades the quality of interaction. Specifically, humans expect rich real-time interactions during speech exchange, such as backchanneling and barge-ins. The nature of the patient interview task differs from most other scenarios where task oriented dialogue systems have been used, as there is increased potential of engagement breakdown during interaction. We describe a dialogue system architecture that improves the performance of humanoid robots on the patient interview task. Our architecture adds a nested inner real-time control loop to improve the timeliness of the robot's responses based on the notion of \"stance\", an elaboration of the concept of a \"turn\", common in most existing dialogue systems. It also expands the dialogue state to monitor not only task progress, but also human engagement. Experiments using a humanoid robot running our proposed architecture reveal improved performance on interview tasks in terms of the perceived timeliness of responses and users' impressions of the system.",
    "metadata": {
      "title": "A Humanoid Robot Dialogue System Architecture Targeting Patient Interview Tasks",
      "summary": "Humanoid robots are promising approach to automating patient interviews routinely conducted by medical staff. Their human-like appearance enables them to use the full gamut of verbal and behavioral cues that are critical to a successful interview. On the other hand, anthropomorphism can induce expectations of human-level performance by the robot. Not meeting such expectations degrades the quality of interaction. Specifically, humans expect rich real-time interactions during speech exchange, such as backchanneling and barge-ins. The nature of the patient interview task differs from most other scenarios where task oriented dialogue systems have been used, as there is increased potential of engagement breakdown during interaction. We describe a dialogue system architecture that improves the performance of humanoid robots on the patient interview task. Our architecture adds a nested inner real-time control loop to improve the timeliness of the robot's responses based on the notion of \"stance\", an elaboration of the concept of a \"turn\", common in most existing dialogue systems. It also expands the dialogue state to monitor not only task progress, but also human engagement. Experiments using a humanoid robot running our proposed architecture reveal improved performance on interview tasks in terms of the perceived timeliness of responses and users' impressions of the system.",
      "abstract": "Humanoid robots are promising approach to automating patient interviews routinely conducted by medical staff. Their human-like appearance enables them to use the full gamut of verbal and behavioral cues that are critical to a successful interview. On the other hand, anthropomorphism can induce expectations of human-level performance by the robot. Not meeting such expectations degrades the quality of interaction. Specifically, humans expect rich real-time interactions during speech exchange, such as backchanneling and barge-ins. The nature of the patient interview task differs from most other scenarios where task oriented dialogue systems have been used, as there is increased potential of engagement breakdown during interaction. We describe a dialogue system architecture that improves the performance of humanoid robots on the patient interview task. Our architecture adds a nested inner real-time control loop to improve the timeliness of the robot's responses based on the notion of \"stance\", an elaboration of the concept of a \"turn\", common in most existing dialogue systems. It also expands the dialogue state to monitor not only task progress, but also human engagement. Experiments using a humanoid robot running our proposed architecture reveal improved performance on interview tasks in terms of the perceived timeliness of responses and users' impressions of the system.",
      "doi": "https://doi.org/10.1109/ro-man60168.2024.10731285",
      "openalex_id": "https://openalex.org/W4403918885",
      "arxiv_id": "",
      "publication_date": "2024-08-26",
      "published": "2024-08-26",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item centers on humanoid-robot dialogue architecture with real-time control and engagement management, but it does not explicitly focus on Spoken Language Models / Speech Language Models as the core modeling object, nor discuss full-duplex speech interaction or speech-language modeling frameworks required by the inclusion criteria.",
      "evaluation": 2
    },
    "round-A_JuniorNano_reasoning": "The item centers on humanoid-robot dialogue architecture with real-time control and engagement management, but it does not explicitly focus on Spoken Language Models / Speech Language Models as the core modeling object, nor discuss full-duplex speech interaction or speech-language modeling frameworks required by the inclusion criteria.",
    "round-A_JuniorNano_evaluation": 2,
    "round-A_JuniorMini_output": {
      "reasoning": "The abstract discusses a humanoid robot dialogue system that focuses on real-time interaction improvements such as backchanneling and barge-ins, which relate to synchronized dialogue interaction. However, it does not explicitly mention spoken language models (SLMs), speech language modeling, or language modeling of speech as core components, nor does it discuss model architecture, training, or evaluation in terms of SLMs or FD-SLMs specifically. Although it involves dialogue and patient interview tasks with timing improvements and engagement monitoring, it lacks clear references to speech-based language modeling or evaluation metrics involving latency below 200ms or cognitive parallelism as required by the inclusion criteria. Therefore, it does not fully meet the inclusion criteria nor completely fall under the exclusion criteria, leading to uncertainty.",
      "evaluation": 3
    },
    "round-A_JuniorMini_reasoning": "The abstract discusses a humanoid robot dialogue system that focuses on real-time interaction improvements such as backchanneling and barge-ins, which relate to synchronized dialogue interaction. However, it does not explicitly mention spoken language models (SLMs), speech language modeling, or language modeling of speech as core components, nor does it discuss model architecture, training, or evaluation in terms of SLMs or FD-SLMs specifically. Although it involves dialogue and patient interview tasks with timing improvements and engagement monitoring, it lacks clear references to speech-based language modeling or evaluation metrics involving latency below 200ms or cognitive parallelism as required by the inclusion criteria. Therefore, it does not fully meet the inclusion criteria nor completely fall under the exclusion criteria, leading to uncertainty.",
    "round-A_JuniorMini_evaluation": 3,
    "round-B_SeniorLead_output": {
      "reasoning": "Both junior reviewers correctly identified that the manuscript focuses on a humanoid-robot dialogue control architecture (real-time control loop, stance/turn elaboration, engagement monitoring) and improvements in perceived timeliness and user impressions; however, the abstract contains no explicit reference to Spoken Language Models / Speech Language Models or equivalent speech-level language modeling (no mention of language modeling of speech, speech-token or continuous acoustic modeling, Speech–LLM architectures, training/regime details, or benchmarked low-latency/full-duplex SLM evaluation). While it addresses real-time interaction behaviors (backchanneling, barge-ins), it treats them as system-control/dialogue engineering improvements rather than proposing or evaluating SLM/FD‑SLM methods, so it fails the inclusion requirement that SLMs (or equivalent) be the core modeling object and be explicitly named; it therefore meets exclusion conditions (not centered on speech-level modeling or SLM contributions). I concur with the junior reviewers' leaning to exclude. Score explanation (1-sentence): The paper should be excluded because it addresses dialogue-control and robot interaction engineering without explicitly presenting or evaluating Spoken Language Models or speech-level language modeling as required by the inclusion criteria.",
      "evaluation": 2
    },
    "round-B_SeniorLead_reasoning": "Both junior reviewers correctly identified that the manuscript focuses on a humanoid-robot dialogue control architecture (real-time control loop, stance/turn elaboration, engagement monitoring) and improvements in perceived timeliness and user impressions; however, the abstract contains no explicit reference to Spoken Language Models / Speech Language Models or equivalent speech-level language modeling (no mention of language modeling of speech, speech-token or continuous acoustic modeling, Speech–LLM architectures, training/regime details, or benchmarked low-latency/full-duplex SLM evaluation). While it addresses real-time interaction behaviors (backchanneling, barge-ins), it treats them as system-control/dialogue engineering improvements rather than proposing or evaluating SLM/FD‑SLM methods, so it fails the inclusion requirement that SLMs (or equivalent) be the core modeling object and be explicitly named; it therefore meets exclusion conditions (not centered on speech-level modeling or SLM contributions). I concur with the junior reviewers' leaning to exclude. Score explanation (1-sentence): The paper should be excluded because it addresses dialogue-control and robot interaction engineering without explicitly presenting or evaluating Spoken Language Models or speech-level language modeling as required by the inclusion criteria.",
    "round-B_SeniorLead_evaluation": 2,
    "final_verdict": "exclude (senior:2)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Interruption Audio &amp; Transcript: Derived from Group Affect and Performance Dataset",
    "abstract": "Despite the widespread development and use of chatbots, there is a lack of audio-based interruption datasets. This study provides a dataset of 200 manually annotated interruptions from a broader set of 355 data points of overlapping utterances. The dataset is derived from the Group Affect and Performance dataset managed by the University of the Fraser Valley, Canada. It includes both audio files and transcripts, allowing for multi-modal analysis. Given the extensive literature and the varied definitions of interruptions, it was necessary to establish precise definitions. The study aims to provide a comprehensive dataset for researchers to build and improve interruption prediction models. The findings demonstrate that classification models can generalize well to identify interruptions based on this dataset’s audio. This opens up research avenues with respect to interruption-related topics, ranging from multi-modal interruption classification using text and audio modalities to the analysis of group dynamics.",
    "metadata": {
      "title": "Interruption Audio &amp; Transcript: Derived from Group Affect and Performance Dataset",
      "summary": "Despite the widespread development and use of chatbots, there is a lack of audio-based interruption datasets. This study provides a dataset of 200 manually annotated interruptions from a broader set of 355 data points of overlapping utterances. The dataset is derived from the Group Affect and Performance dataset managed by the University of the Fraser Valley, Canada. It includes both audio files and transcripts, allowing for multi-modal analysis. Given the extensive literature and the varied definitions of interruptions, it was necessary to establish precise definitions. The study aims to provide a comprehensive dataset for researchers to build and improve interruption prediction models. The findings demonstrate that classification models can generalize well to identify interruptions based on this dataset’s audio. This opens up research avenues with respect to interruption-related topics, ranging from multi-modal interruption classification using text and audio modalities to the analysis of group dynamics.",
      "abstract": "Despite the widespread development and use of chatbots, there is a lack of audio-based interruption datasets. This study provides a dataset of 200 manually annotated interruptions from a broader set of 355 data points of overlapping utterances. The dataset is derived from the Group Affect and Performance dataset managed by the University of the Fraser Valley, Canada. It includes both audio files and transcripts, allowing for multi-modal analysis. Given the extensive literature and the varied definitions of interruptions, it was necessary to establish precise definitions. The study aims to provide a comprehensive dataset for researchers to build and improve interruption prediction models. The findings demonstrate that classification models can generalize well to identify interruptions based on this dataset’s audio. This opens up research avenues with respect to interruption-related topics, ranging from multi-modal interruption classification using text and audio modalities to the analysis of group dynamics.",
      "doi": "https://doi.org/10.3390/data9090104",
      "openalex_id": "https://openalex.org/W4402134537",
      "arxiv_id": "",
      "publication_date": "2024-08-31",
      "published": "2024-08-31",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item does not center on Spoken Language Models or equivalent speech-centric modeling; it presents a dataset and interruption classification work without SLM/FD-SLM architecture, training, or evaluation discussion, failing the core inclusion criterion despite being recent.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item does not center on Spoken Language Models or equivalent speech-centric modeling; it presents a dataset and interruption classification work without SLM/FD-SLM architecture, training, or evaluation discussion, failing the core inclusion criterion despite being recent.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study provides a dataset focused on audio-based interruptions and their classification, which is related to interruption handling and audio data; however, it does not explicitly address spoken language models (SLMs) or full-duplex spoken language model architectures, nor does it mention model training, inference, or evaluation from a speech language modeling perspective, and lacks discussion of core model architectures or learned synchronization; therefore, it does not fully meet the inclusion criteria centered on speech language modeling research and the development of spoken language models or equivalents.",
      "evaluation": 2
    },
    "round-A_JuniorMini_reasoning": "The study provides a dataset focused on audio-based interruptions and their classification, which is related to interruption handling and audio data; however, it does not explicitly address spoken language models (SLMs) or full-duplex spoken language model architectures, nor does it mention model training, inference, or evaluation from a speech language modeling perspective, and lacks discussion of core model architectures or learned synchronization; therefore, it does not fully meet the inclusion criteria centered on speech language modeling research and the development of spoken language models or equivalents.",
    "round-A_JuniorMini_evaluation": 2,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:2)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Prosody-TTS: Improving Prosody with Masked Autoencoder and Conditional Diffusion Model For Expressive Text-to-Speech",
    "abstract": "Expressive text-to-speech aims to generate high-quality samples with rich and diverse prosody, which is hampered by dual challenges: 1) prosodic attributes in highly dynamic voices are difficult to capture and model without intonation; and 2) highly multimodal prosodic representations cannot be well learned by simple regression (e.g., MSE) objectives, which causes blurry and over-smoothing predictions. This paper proposes Prosody-TTS, a two-stage pipeline that enhances prosody modeling and sampling by introducing several components: 1) a self-supervised masked autoencoder to model the prosodic representation without relying on text transcriptions or local prosody attributes, which ensures to cover diverse speaking voices with superior generalization; and 2) a diffusion model to sample diverse prosodic patterns within the latent space, which prevents TTS models from generating samples with dull prosodic performance. Experimental results show that Prosody-TTS achieves new state-of-the-art in text-to-speech with natural and expressive synthesis. Both subjective and objective evaluation demonstrate that it exhibits superior audio quality and prosody naturalness with rich and diverse prosodic attributes. Audio samples are available at https://improved_prosody.github.io",
    "metadata": {
      "title": "Prosody-TTS: Improving Prosody with Masked Autoencoder and Conditional Diffusion Model For Expressive Text-to-Speech",
      "summary": "Expressive text-to-speech aims to generate high-quality samples with rich and diverse prosody, which is hampered by dual challenges: 1) prosodic attributes in highly dynamic voices are difficult to capture and model without intonation; and 2) highly multimodal prosodic representations cannot be well learned by simple regression (e.g., MSE) objectives, which causes blurry and over-smoothing predictions. This paper proposes Prosody-TTS, a two-stage pipeline that enhances prosody modeling and sampling by introducing several components: 1) a self-supervised masked autoencoder to model the prosodic representation without relying on text transcriptions or local prosody attributes, which ensures to cover diverse speaking voices with superior generalization; and 2) a diffusion model to sample diverse prosodic patterns within the latent space, which prevents TTS models from generating samples with dull prosodic performance. Experimental results show that Prosody-TTS achieves new state-of-the-art in text-to-speech with natural and expressive synthesis. Both subjective and objective evaluation demonstrate that it exhibits superior audio quality and prosody naturalness with rich and diverse prosodic attributes. Audio samples are available at https://improved_prosody.github.io",
      "abstract": "Expressive text-to-speech aims to generate high-quality samples with rich and diverse prosody, which is hampered by dual challenges: 1) prosodic attributes in highly dynamic voices are difficult to capture and model without intonation; and 2) highly multimodal prosodic representations cannot be well learned by simple regression (e.g., MSE) objectives, which causes blurry and over-smoothing predictions. This paper proposes Prosody-TTS, a two-stage pipeline that enhances prosody modeling and sampling by introducing several components: 1) a self-supervised masked autoencoder to model the prosodic representation without relying on text transcriptions or local prosody attributes, which ensures to cover diverse speaking voices with superior generalization; and 2) a diffusion model to sample diverse prosodic patterns within the latent space, which prevents TTS models from generating samples with dull prosodic performance. Experimental results show that Prosody-TTS achieves new state-of-the-art in text-to-speech with natural and expressive synthesis. Both subjective and objective evaluation demonstrate that it exhibits superior audio quality and prosody naturalness with rich and diverse prosodic attributes. Audio samples are available at https://improved_prosody.github.io",
      "doi": "https://doi.org/10.18653/v1/2023.findings-acl.508",
      "openalex_id": "https://openalex.org/W4385570943",
      "arxiv_id": "",
      "publication_date": "2023-01-01",
      "published": "2023-01-01",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The paper focuses on expressive TTS prosody using a masked autoencoder and diffusion model, addressing speech synthesis rather than treating speech as the core Spoken Language Model (SLM/FD-SLM) with interactive, turn-taking or low-latency dialogue capabilities; it does not meet the inclusion criteria and is more aligned with single-task TTS, so it should be excluded.",
      "evaluation": 2
    },
    "round-A_JuniorNano_reasoning": "The paper focuses on expressive TTS prosody using a masked autoencoder and diffusion model, addressing speech synthesis rather than treating speech as the core Spoken Language Model (SLM/FD-SLM) with interactive, turn-taking or low-latency dialogue capabilities; it does not meet the inclusion criteria and is more aligned with single-task TTS, so it should be excluded.",
    "round-A_JuniorNano_evaluation": 2,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on improving expressive text-to-speech (TTS) with better prosody modeling via a masked autoencoder and diffusion model but does not address spoken language modeling itself, full-duplex interactive dialogue, or cognitive synchronization, which are core to the inclusion criteria; it also primarily targets a single downstream task (TTS) without discussing spoken language models or interactive dialogue behaviors and synchronization, thus aligning with multiple exclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on improving expressive text-to-speech (TTS) with better prosody modeling via a masked autoencoder and diffusion model but does not address spoken language modeling itself, full-duplex interactive dialogue, or cognitive synchronization, which are core to the inclusion criteria; it also primarily targets a single downstream task (TTS) without discussing spoken language models or interactive dialogue behaviors and synchronization, thus aligning with multiple exclusion criteria.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:2)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Speaking Style Conversion in the Waveform Domain Using Discrete Self-Supervised Units",
    "abstract": "We introduce DISSC, a novel, lightweight method that converts the rhythm, pitch contour and timbre of a recording to a target speaker in a textless manner. Unlike DISSC, most voice conversion (VC) methods focus primarily on timbre, and ignore people’s unique speaking style (prosody). The proposed approach uses a pretrained, self-supervised model for encoding speech to discrete units, which makes it simple, effective, and fast to train. All conversion modules are only trained on reconstruction like tasks, thus suitable for any-to-many VC with no paired data. We introduce a suite of quantitative and qualitative evaluation metrics for this setup, and empirically demonstrate that DISSC significantly outperforms the evaluated baselines. Code and samples are available at https://pages.cs.huji.ac.il/adiyoss-lab/dissc/.",
    "metadata": {
      "title": "Speaking Style Conversion in the Waveform Domain Using Discrete Self-Supervised Units",
      "summary": "We introduce DISSC, a novel, lightweight method that converts the rhythm, pitch contour and timbre of a recording to a target speaker in a textless manner. Unlike DISSC, most voice conversion (VC) methods focus primarily on timbre, and ignore people’s unique speaking style (prosody). The proposed approach uses a pretrained, self-supervised model for encoding speech to discrete units, which makes it simple, effective, and fast to train. All conversion modules are only trained on reconstruction like tasks, thus suitable for any-to-many VC with no paired data. We introduce a suite of quantitative and qualitative evaluation metrics for this setup, and empirically demonstrate that DISSC significantly outperforms the evaluated baselines. Code and samples are available at https://pages.cs.huji.ac.il/adiyoss-lab/dissc/.",
      "abstract": "We introduce DISSC, a novel, lightweight method that converts the rhythm, pitch contour and timbre of a recording to a target speaker in a textless manner. Unlike DISSC, most voice conversion (VC) methods focus primarily on timbre, and ignore people’s unique speaking style (prosody). The proposed approach uses a pretrained, self-supervised model for encoding speech to discrete units, which makes it simple, effective, and fast to train. All conversion modules are only trained on reconstruction like tasks, thus suitable for any-to-many VC with no paired data. We introduce a suite of quantitative and qualitative evaluation metrics for this setup, and empirically demonstrate that DISSC significantly outperforms the evaluated baselines. Code and samples are available at https://pages.cs.huji.ac.il/adiyoss-lab/dissc/.",
      "doi": "https://doi.org/10.18653/v1/2023.findings-emnlp.541",
      "openalex_id": "https://openalex.org/W4389524060",
      "arxiv_id": "",
      "publication_date": "2023-01-01",
      "published": "2023-01-01",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The paper focuses on waveform-domain voice conversion using discrete self-supervised units and prosody transfer, not on Spoken Language Models / Speech Language Modeling as the core object or on interactive speech modeling tasks.",
      "evaluation": 2
    },
    "round-A_JuniorNano_reasoning": "The paper focuses on waveform-domain voice conversion using discrete self-supervised units and prosody transfer, not on Spoken Language Models / Speech Language Modeling as the core object or on interactive speech modeling tasks.",
    "round-A_JuniorNano_evaluation": 2,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on speaking style conversion using discrete self-supervised units to model speech characteristics like rhythm, pitch, and timbre, but it does not clearly address Spoken Language Models or Full-Duplex Spoken Language Models, nor does it discuss end-to-end or hybrid speech-LLM architectures, synchronization, or multi-dimensional interactive evaluation as required by the inclusion criteria; thus, it largely concerns speech signal transformation without advancing spoken language modeling for interactive or synchronous dialogue.",
      "evaluation": 2
    },
    "round-A_JuniorMini_reasoning": "The study focuses on speaking style conversion using discrete self-supervised units to model speech characteristics like rhythm, pitch, and timbre, but it does not clearly address Spoken Language Models or Full-Duplex Spoken Language Models, nor does it discuss end-to-end or hybrid speech-LLM architectures, synchronization, or multi-dimensional interactive evaluation as required by the inclusion criteria; thus, it largely concerns speech signal transformation without advancing spoken language modeling for interactive or synchronous dialogue.",
    "round-A_JuniorMini_evaluation": 2,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:2)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "The Clinicopathological Characteristics of Pure and Mixed Invasive Micropapillary Breast Carcinomas: A Single Center Experience",
    "abstract": "Compared to the mixed IMPC, pure IMPC appears to have a more aggressive behavior with lower locoregional recurrence-free survival and more locoregional recurrences. This may be due to the low progesterone receptor positivity rate.",
    "metadata": {
      "title": "The Clinicopathological Characteristics of Pure and Mixed Invasive Micropapillary Breast Carcinomas: A Single Center Experience",
      "summary": "Compared to the mixed IMPC, pure IMPC appears to have a more aggressive behavior with lower locoregional recurrence-free survival and more locoregional recurrences. This may be due to the low progesterone receptor positivity rate.",
      "abstract": "Compared to the mixed IMPC, pure IMPC appears to have a more aggressive behavior with lower locoregional recurrence-free survival and more locoregional recurrences. This may be due to the low progesterone receptor positivity rate.",
      "doi": "https://doi.org/10.4274/balkanmedj.galenos.2022.2022-4-7",
      "openalex_id": "https://openalex.org/W4286716540",
      "arxiv_id": "",
      "publication_date": "2022-07-22",
      "published": "2022-07-22",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This article is about clinicopathological characteristics of breast carcinomas and does not address Spoken Language Models or any speech-language modeling concepts, so it does not meet the inclusion criteria (and is excluded).",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "This article is about clinicopathological characteristics of breast carcinomas and does not address Spoken Language Models or any speech-language modeling concepts, so it does not meet the inclusion criteria (and is excluded).",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on clinicopathological characteristics of invasive micropapillary breast carcinomas, which is unrelated to Spoken Language Models, speech modeling, or the specified research domain focused on speech-based language modeling, architectures, or evaluations.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on clinicopathological characteristics of invasive micropapillary breast carcinomas, which is unrelated to Spoken Language Models, speech modeling, or the specified research domain focused on speech-based language modeling, architectures, or evaluations.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "SpeechPrompt: Prompting Speech Language Models for Speech Processing Tasks",
    "abstract": "Prompting has become a practical method for utilizing pre-trained language\\nmodels (LMs). This approach offers several advantages. It allows an LM to adapt\\nto new tasks with minimal training and parameter updates, thus achieving\\nefficiency in both storage and computation. Additionally, prompting modifies\\nonly the LM's inputs and harnesses the generative capabilities of language\\nmodels to address various downstream tasks in a unified manner. This\\nsignificantly reduces the need for human labor in designing task-specific\\nmodels. These advantages become even more evident as the number of tasks served\\nby the LM scales up. Motivated by the strengths of prompting, we are the first\\nto explore the potential of prompting speech LMs in the domain of speech\\nprocessing. Recently, there has been a growing interest in converting speech\\ninto discrete units for language modeling. Our pioneer research demonstrates\\nthat these quantized speech units are highly versatile within our unified\\nprompting framework. Not only can they serve as class labels, but they also\\ncontain rich phonetic information that can be re-synthesized back into speech\\nsignals for speech generation tasks. Specifically, we reformulate speech\\nprocessing tasks into speech-to-unit generation tasks. As a result, we can\\nseamlessly integrate tasks such as speech classification, sequence generation,\\nand speech generation within a single, unified prompting framework. The\\nexperiment results show that the prompting method can achieve competitive\\nperformance compared to the strong fine-tuning method based on self-supervised\\nlearning models with a similar number of trainable parameters. The prompting\\nmethod also shows promising results in the few-shot setting. Moreover, with the\\nadvanced speech LMs coming into the stage, the proposed prompting framework\\nattains great potential.\\n",
    "metadata": {
      "title": "SpeechPrompt: Prompting Speech Language Models for Speech Processing Tasks",
      "summary": "Prompting has become a practical method for utilizing pre-trained language\\nmodels (LMs). This approach offers several advantages. It allows an LM to adapt\\nto new tasks with minimal training and parameter updates, thus achieving\\nefficiency in both storage and computation. Additionally, prompting modifies\\nonly the LM's inputs and harnesses the generative capabilities of language\\nmodels to address various downstream tasks in a unified manner. This\\nsignificantly reduces the need for human labor in designing task-specific\\nmodels. These advantages become even more evident as the number of tasks served\\nby the LM scales up. Motivated by the strengths of prompting, we are the first\\nto explore the potential of prompting speech LMs in the domain of speech\\nprocessing. Recently, there has been a growing interest in converting speech\\ninto discrete units for language modeling. Our pioneer research demonstrates\\nthat these quantized speech units are highly versatile within our unified\\nprompting framework. Not only can they serve as class labels, but they also\\ncontain rich phonetic information that can be re-synthesized back into speech\\nsignals for speech generation tasks. Specifically, we reformulate speech\\nprocessing tasks into speech-to-unit generation tasks. As a result, we can\\nseamlessly integrate tasks such as speech classification, sequence generation,\\nand speech generation within a single, unified prompting framework. The\\nexperiment results show that the prompting method can achieve competitive\\nperformance compared to the strong fine-tuning method based on self-supervised\\nlearning models with a similar number of trainable parameters. The prompting\\nmethod also shows promising results in the few-shot setting. Moreover, with the\\nadvanced speech LMs coming into the stage, the proposed prompting framework\\nattains great potential.\\n",
      "abstract": "Prompting has become a practical method for utilizing pre-trained language\\nmodels (LMs). This approach offers several advantages. It allows an LM to adapt\\nto new tasks with minimal training and parameter updates, thus achieving\\nefficiency in both storage and computation. Additionally, prompting modifies\\nonly the LM's inputs and harnesses the generative capabilities of language\\nmodels to address various downstream tasks in a unified manner. This\\nsignificantly reduces the need for human labor in designing task-specific\\nmodels. These advantages become even more evident as the number of tasks served\\nby the LM scales up. Motivated by the strengths of prompting, we are the first\\nto explore the potential of prompting speech LMs in the domain of speech\\nprocessing. Recently, there has been a growing interest in converting speech\\ninto discrete units for language modeling. Our pioneer research demonstrates\\nthat these quantized speech units are highly versatile within our unified\\nprompting framework. Not only can they serve as class labels, but they also\\ncontain rich phonetic information that can be re-synthesized back into speech\\nsignals for speech generation tasks. Specifically, we reformulate speech\\nprocessing tasks into speech-to-unit generation tasks. As a result, we can\\nseamlessly integrate tasks such as speech classification, sequence generation,\\nand speech generation within a single, unified prompting framework. The\\nexperiment results show that the prompting method can achieve competitive\\nperformance compared to the strong fine-tuning method based on self-supervised\\nlearning models with a similar number of trainable parameters. The prompting\\nmethod also shows promising results in the few-shot setting. Moreover, with the\\nadvanced speech LMs coming into the stage, the proposed prompting framework\\nattains great potential.\\n",
      "doi": "https://doi.org/10.1109/taslp.2024.3436618",
      "openalex_id": "https://openalex.org/W4401246677",
      "arxiv_id": "",
      "publication_date": "2024-01-01",
      "published": "2024-01-01",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The title/abstract explicitly target Spoken/Speech Language Models, framing tasks as speech-to-unit generation within a unified prompting framework, with multi-task evaluation and no indications of half-duplex-only or traditional ASR/TTS limitations, thus meeting all inclusion criteria and not triggering exclusions.",
      "evaluation": 5
    },
    "round-A_JuniorNano_reasoning": "The title/abstract explicitly target Spoken/Speech Language Models, framing tasks as speech-to-unit generation within a unified prompting framework, with multi-task evaluation and no indications of half-duplex-only or traditional ASR/TTS limitations, thus meeting all inclusion criteria and not triggering exclusions.",
    "round-A_JuniorNano_evaluation": 5,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on prompting speech language models for various speech processing tasks, including speech classification and generation, within a unified prompting framework. However, the abstract does not explicitly mention spoken language models or full-duplex spoken language models, nor does it discuss synchronous interaction, cognitive parallelism, or low-latency communication, which are critical inclusion criteria. The approach is about speech-to-unit generation, leveraging quantized speech units, but it lacks clear emphasis on full-duplex interactive capabilities or detailed multi-dimensional evaluation tied to spoken language modeling. Therefore, it partially meets the topic of speech language models but misses key aspects related to full-duplex communication and synchronized interaction, and no mention of evaluation on interaction metrics is provided. Thus, it is uncertain whether it fully complies with all inclusion criteria and none of the exclusion criteria.",
      "evaluation": 3
    },
    "round-A_JuniorMini_reasoning": "The study focuses on prompting speech language models for various speech processing tasks, including speech classification and generation, within a unified prompting framework. However, the abstract does not explicitly mention spoken language models or full-duplex spoken language models, nor does it discuss synchronous interaction, cognitive parallelism, or low-latency communication, which are critical inclusion criteria. The approach is about speech-to-unit generation, leveraging quantized speech units, but it lacks clear emphasis on full-duplex interactive capabilities or detailed multi-dimensional evaluation tied to spoken language modeling. Therefore, it partially meets the topic of speech language models but misses key aspects related to full-duplex communication and synchronized interaction, and no mention of evaluation on interaction metrics is provided. Thus, it is uncertain whether it fully complies with all inclusion criteria and none of the exclusion criteria.",
    "round-A_JuniorMini_evaluation": 3,
    "round-B_SeniorLead_output": {
      "reasoning": "綜合判定：本文標題與摘要明確以「Speech Language Models / speech LMs」為核心（標題即含 Speech Language Models），並將語音轉為離散單位進行語音層級的 language modeling，將多項語音任務（classification、sequence generation、speech generation）統一為 speech-to-unit 生成任務，符合「以語音為主要建模與推理對象且在標題/摘要出現等價描述」之納入要件；雖然摘要未討論 full‑duplex 即時互動或語音重疊/低延遲等細節（JuniorMini 因此持保留），但納入標準主要要求以 SLM/語音語言建模為核心且涵蓋模型/方法面向，本文符合該核心條件並未落入僅為 ASR/TTS 或僅作為文字前處理等排除情形（JuniorNano 亦認為可納入）。因此決定納入。簡短說明：標題與摘要明確以 speech language models 及語音層級的離散 token LM 為研究核心，符合納入條件。",
      "evaluation": 5
    },
    "round-B_SeniorLead_reasoning": "綜合判定：本文標題與摘要明確以「Speech Language Models / speech LMs」為核心（標題即含 Speech Language Models），並將語音轉為離散單位進行語音層級的 language modeling，將多項語音任務（classification、sequence generation、speech generation）統一為 speech-to-unit 生成任務，符合「以語音為主要建模與推理對象且在標題/摘要出現等價描述」之納入要件；雖然摘要未討論 full‑duplex 即時互動或語音重疊/低延遲等細節（JuniorMini 因此持保留），但納入標準主要要求以 SLM/語音語言建模為核心且涵蓋模型/方法面向，本文符合該核心條件並未落入僅為 ASR/TTS 或僅作為文字前處理等排除情形（JuniorNano 亦認為可納入）。因此決定納入。簡短說明：標題與摘要明確以 speech language models 及語音層級的離散 token LM 為研究核心，符合納入條件。",
    "round-B_SeniorLead_evaluation": 5,
    "final_verdict": "include (senior:5)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "End-to-End Speech Recognition with Deep Fusion: Leveraging External Language Models for Low-Resource Scenarios",
    "abstract": "With the rapid development of Automatic Speech Recognition (ASR) technology, end-to-end speech recognition systems have gained significant attention due to their ability to directly convert raw speech signals into text. However, such systems heavily rely on large amounts of labeled speech data, which severely limits model training performance and generalization, especially in low-resource language environments. To address this issue, this paper proposes an end-to-end speech recognition approach based on deep fusion, which tightly integrates an external language model (LM) with the end-to-end model during the training phase, effectively compensating for the lack of linguistic prior knowledge. Unlike traditional shallow fusion methods, deep fusion enables the model and the external LM to share representations and jointly optimize during training, thereby enhancing recognition performance under low-resource conditions. Experiments conducted on the Common Voice dataset show that, in a 10 h extremely low-resource scenario, the deep fusion method reduces the character error rate (CER) from 51.1% to 17.65%. In a 100 h scenario, it achieves a relative reduction of approximately 2.8%. Furthermore, ablation studies on model layers demonstrate that even with a reduced number of encoder and decoder layers to decrease model complexity, deep fusion continues to effectively leverage external linguistic priors, significantly improving performance in low-resource speech recognition tasks.",
    "metadata": {
      "title": "End-to-End Speech Recognition with Deep Fusion: Leveraging External Language Models for Low-Resource Scenarios",
      "summary": "With the rapid development of Automatic Speech Recognition (ASR) technology, end-to-end speech recognition systems have gained significant attention due to their ability to directly convert raw speech signals into text. However, such systems heavily rely on large amounts of labeled speech data, which severely limits model training performance and generalization, especially in low-resource language environments. To address this issue, this paper proposes an end-to-end speech recognition approach based on deep fusion, which tightly integrates an external language model (LM) with the end-to-end model during the training phase, effectively compensating for the lack of linguistic prior knowledge. Unlike traditional shallow fusion methods, deep fusion enables the model and the external LM to share representations and jointly optimize during training, thereby enhancing recognition performance under low-resource conditions. Experiments conducted on the Common Voice dataset show that, in a 10 h extremely low-resource scenario, the deep fusion method reduces the character error rate (CER) from 51.1% to 17.65%. In a 100 h scenario, it achieves a relative reduction of approximately 2.8%. Furthermore, ablation studies on model layers demonstrate that even with a reduced number of encoder and decoder layers to decrease model complexity, deep fusion continues to effectively leverage external linguistic priors, significantly improving performance in low-resource speech recognition tasks.",
      "abstract": "With the rapid development of Automatic Speech Recognition (ASR) technology, end-to-end speech recognition systems have gained significant attention due to their ability to directly convert raw speech signals into text. However, such systems heavily rely on large amounts of labeled speech data, which severely limits model training performance and generalization, especially in low-resource language environments. To address this issue, this paper proposes an end-to-end speech recognition approach based on deep fusion, which tightly integrates an external language model (LM) with the end-to-end model during the training phase, effectively compensating for the lack of linguistic prior knowledge. Unlike traditional shallow fusion methods, deep fusion enables the model and the external LM to share representations and jointly optimize during training, thereby enhancing recognition performance under low-resource conditions. Experiments conducted on the Common Voice dataset show that, in a 10 h extremely low-resource scenario, the deep fusion method reduces the character error rate (CER) from 51.1% to 17.65%. In a 100 h scenario, it achieves a relative reduction of approximately 2.8%. Furthermore, ablation studies on model layers demonstrate that even with a reduced number of encoder and decoder layers to decrease model complexity, deep fusion continues to effectively leverage external linguistic priors, significantly improving performance in low-resource speech recognition tasks.",
      "doi": "https://doi.org/10.3390/electronics14040802",
      "openalex_id": "https://openalex.org/W4407748379",
      "arxiv_id": "",
      "publication_date": "2025-02-19",
      "published": "2025-02-19",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The paper deals with end-to-end ASR with deep fusion of an external language model, which is relevant to ASR but does not center on Spoken Language Models or Speech Language Models as the core object, nor does it discuss synchronous interactive speech or full-duplex dialogue; it is essentially a traditional ASR setup with LM integration and thus fails the explicit inclusion criterion and aligns with exclusion components (no interactive speech modeling).",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The paper deals with end-to-end ASR with deep fusion of an external language model, which is relevant to ASR but does not center on Spoken Language Models or Speech Language Models as the core object, nor does it discuss synchronous interactive speech or full-duplex dialogue; it is essentially a traditional ASR setup with LM integration and thus fails the explicit inclusion criterion and aligns with exclusion components (no interactive speech modeling).",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper focuses on an end-to-end speech recognition system integrating an external language model to improve performance in low-resource scenarios. However, it primarily addresses ASR tasks without explicitly involving spoken language models (SLMs) or full-duplex spoken interaction capabilities, and does not discuss synchronization, cognitive parallelism, real-time interaction, or multi-dimensional evaluation as specified in the inclusion criteria. Thus, it does not meet the required criteria for inclusion focused on spoken language modeling and full-duplex interaction capabilities.",
      "evaluation": 2
    },
    "round-A_JuniorMini_reasoning": "The paper focuses on an end-to-end speech recognition system integrating an external language model to improve performance in low-resource scenarios. However, it primarily addresses ASR tasks without explicitly involving spoken language models (SLMs) or full-duplex spoken interaction capabilities, and does not discuss synchronization, cognitive parallelism, real-time interaction, or multi-dimensional evaluation as specified in the inclusion criteria. Thus, it does not meet the required criteria for inclusion focused on spoken language modeling and full-duplex interaction capabilities.",
    "round-A_JuniorMini_evaluation": 2,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:2)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "XLS-R fine-tuning on noisy word boundaries for unsupervised speech segmentation into words",
    "abstract": "Due to the absence of explicit word boundaries in the speech stream, the task of segmenting spoken sentences into word units without text supervision is particularly challenging. In this work, we leverage the most recent self-supervised speech models that have proved to quickly adapt to new tasks through fine-tuning, even in low resource conditions. Taking inspiration from semi-supervised learning, we fine-tune an XLS-R model to predict word boundaries themselves produced by top-tier speech segmentation systems: DPDP, VG-HuBERT and DP-Parse. Once XLS-R is fine-tuned, it is used to infer new word boundary labels that are used in turn for another fine-tuning step. Our method consistently improves the performance of each system and set a new state-of-the-art that is, on average 130% higher than the previous one as measured by the F1 score on correctly discovered word tokens on five corpora featuring different languages. Finally, our system can segment speech from languages unseen during fine-tuning in a zero-shot fashion.",
    "metadata": {
      "title": "XLS-R fine-tuning on noisy word boundaries for unsupervised speech segmentation into words",
      "summary": "Due to the absence of explicit word boundaries in the speech stream, the task of segmenting spoken sentences into word units without text supervision is particularly challenging. In this work, we leverage the most recent self-supervised speech models that have proved to quickly adapt to new tasks through fine-tuning, even in low resource conditions. Taking inspiration from semi-supervised learning, we fine-tune an XLS-R model to predict word boundaries themselves produced by top-tier speech segmentation systems: DPDP, VG-HuBERT and DP-Parse. Once XLS-R is fine-tuned, it is used to infer new word boundary labels that are used in turn for another fine-tuning step. Our method consistently improves the performance of each system and set a new state-of-the-art that is, on average 130% higher than the previous one as measured by the F1 score on correctly discovered word tokens on five corpora featuring different languages. Finally, our system can segment speech from languages unseen during fine-tuning in a zero-shot fashion.",
      "abstract": "Due to the absence of explicit word boundaries in the speech stream, the task of segmenting spoken sentences into word units without text supervision is particularly challenging. In this work, we leverage the most recent self-supervised speech models that have proved to quickly adapt to new tasks through fine-tuning, even in low resource conditions. Taking inspiration from semi-supervised learning, we fine-tune an XLS-R model to predict word boundaries themselves produced by top-tier speech segmentation systems: DPDP, VG-HuBERT and DP-Parse. Once XLS-R is fine-tuned, it is used to infer new word boundary labels that are used in turn for another fine-tuning step. Our method consistently improves the performance of each system and set a new state-of-the-art that is, on average 130% higher than the previous one as measured by the F1 score on correctly discovered word tokens on five corpora featuring different languages. Finally, our system can segment speech from languages unseen during fine-tuning in a zero-shot fashion.",
      "doi": "https://doi.org/10.18653/v1/2023.findings-emnlp.810",
      "openalex_id": "https://openalex.org/W4389524126",
      "arxiv_id": "",
      "publication_date": "2023-01-01",
      "published": "2023-01-01",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This item centers on self-supervised speech modeling (XLS-R) for predicting word boundaries in speech and evaluates segmentation performance across languages, addressing speech as the core modeling object with architecture/training and evaluation details, fitting the Spoken Language Model/SLM scope within the past 3 years; although it doesn't use the exact phrase 'spoken language model', it aligns with 'language modeling of speech' concepts.",
      "evaluation": 4
    },
    "round-A_JuniorNano_reasoning": "This item centers on self-supervised speech modeling (XLS-R) for predicting word boundaries in speech and evaluates segmentation performance across languages, addressing speech as the core modeling object with architecture/training and evaluation details, fitting the Spoken Language Model/SLM scope within the past 3 years; although it doesn't use the exact phrase 'spoken language model', it aligns with 'language modeling of speech' concepts.",
    "round-A_JuniorNano_evaluation": 4,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on fine-tuning a self-supervised speech model (XLS-R) for unsupervised speech segmentation, emphasizing acoustic modeling of spoken language without explicit text supervision, which aligns with Spoken Language Models (SLMs) as it treats speech as the core object of modeling. However, it does not explicitly discuss aspects such as full-duplex interaction, cognitive parallelism, or synchronous dialogue behavior arbitration, nor does it mention speech–LLM or hybrid speech-text architectures or interactive synchronization mechanisms. The evaluation concentrates on segmentation performance (F1 scores) rather than interactive or behavioral metrics. Given these mixed alignments and the lack of explicit mention of spoken language modeling in the abstract/title, the study partially meets inclusion criteria but lacks full alignment with key concepts such as full-duplex interaction or learned synchronization, resulting in indecision for inclusion.",
      "evaluation": 3
    },
    "round-A_JuniorMini_reasoning": "The study focuses on fine-tuning a self-supervised speech model (XLS-R) for unsupervised speech segmentation, emphasizing acoustic modeling of spoken language without explicit text supervision, which aligns with Spoken Language Models (SLMs) as it treats speech as the core object of modeling. However, it does not explicitly discuss aspects such as full-duplex interaction, cognitive parallelism, or synchronous dialogue behavior arbitration, nor does it mention speech–LLM or hybrid speech-text architectures or interactive synchronization mechanisms. The evaluation concentrates on segmentation performance (F1 scores) rather than interactive or behavioral metrics. Given these mixed alignments and the lack of explicit mention of spoken language modeling in the abstract/title, the study partially meets inclusion criteria but lacks full alignment with key concepts such as full-duplex interaction or learned synchronization, resulting in indecision for inclusion.",
    "round-A_JuniorMini_evaluation": 3,
    "round-B_SeniorLead_output": {
      "reasoning": "The paper focuses on fine-tuning a self-supervised speech model (XLS-R) to perform unsupervised speech segmentation into words, treating speech representations as the primary modeling object and describing model fine-tuning and evaluation across multiple languages (including zero-shot), so it aligns with the Spoken Language Model (SLM) scope (speech-centric modeling, architecture/training and evaluation). One junior reviewer rated 4 (in favor of inclusion) noting alignment with language modeling of speech, while the other rated 3 due to lack of the exact phrase \"spoken language model\" and absence of interactive/full‑duplex aspects; I judge that the abstract's use of \"self-supervised speech models\" and explicit modeling/training/evaluation details are an acceptable equivalent to \"speech language models\" and that the work demonstrates generality beyond a single downstream task. It does not address full‑duplex interaction or synchronization behaviors, but inclusion criteria require that the study be centered on SLMs or equivalent (not necessarily on full‑duplex), which this paper meets. One-sentence explanation: Meets the SLM-focused inclusion criteria (speech-centric modeling, training and evaluation with demonstrated cross-lingual generality) despite lacking explicit full‑duplex/interactive claims, so it is better to include.",
      "evaluation": 4
    },
    "round-B_SeniorLead_reasoning": "The paper focuses on fine-tuning a self-supervised speech model (XLS-R) to perform unsupervised speech segmentation into words, treating speech representations as the primary modeling object and describing model fine-tuning and evaluation across multiple languages (including zero-shot), so it aligns with the Spoken Language Model (SLM) scope (speech-centric modeling, architecture/training and evaluation). One junior reviewer rated 4 (in favor of inclusion) noting alignment with language modeling of speech, while the other rated 3 due to lack of the exact phrase \"spoken language model\" and absence of interactive/full‑duplex aspects; I judge that the abstract's use of \"self-supervised speech models\" and explicit modeling/training/evaluation details are an acceptable equivalent to \"speech language models\" and that the work demonstrates generality beyond a single downstream task. It does not address full‑duplex interaction or synchronization behaviors, but inclusion criteria require that the study be centered on SLMs or equivalent (not necessarily on full‑duplex), which this paper meets. One-sentence explanation: Meets the SLM-focused inclusion criteria (speech-centric modeling, training and evaluation with demonstrated cross-lingual generality) despite lacking explicit full‑duplex/interactive claims, so it is better to include.",
    "round-B_SeniorLead_evaluation": 4,
    "final_verdict": "include (senior:4)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Editorial Editorial of Special Issue on Self-Supervised Learning for Speech and Audio Processing",
    "abstract": "The papers in this special section focus on self-supervised learning for speech and audio processing. A current trend in the machine learning community is the adoption of self-supervised approaches to pretrain deep networks. Self-supervised learning utilizes proxy-supervised learning tasks (or pretext tasks)—for example, distinguishing parts of the input signal from distractors or reconstructing masked input segments conditioned on unmasked segments—to obtain training data from unlabeled corpora. These approaches make it possible to use the tremendous amount of unlabeled data available on the web to train large neural models. Recent self-supervised approaches for speech and audio processing are also gaining attention.",
    "metadata": {
      "title": "Editorial Editorial of Special Issue on Self-Supervised Learning for Speech and Audio Processing",
      "summary": "The papers in this special section focus on self-supervised learning for speech and audio processing. A current trend in the machine learning community is the adoption of self-supervised approaches to pretrain deep networks. Self-supervised learning utilizes proxy-supervised learning tasks (or pretext tasks)—for example, distinguishing parts of the input signal from distractors or reconstructing masked input segments conditioned on unmasked segments—to obtain training data from unlabeled corpora. These approaches make it possible to use the tremendous amount of unlabeled data available on the web to train large neural models. Recent self-supervised approaches for speech and audio processing are also gaining attention.",
      "abstract": "The papers in this special section focus on self-supervised learning for speech and audio processing. A current trend in the machine learning community is the adoption of self-supervised approaches to pretrain deep networks. Self-supervised learning utilizes proxy-supervised learning tasks (or pretext tasks)—for example, distinguishing parts of the input signal from distractors or reconstructing masked input segments conditioned on unmasked segments—to obtain training data from unlabeled corpora. These approaches make it possible to use the tremendous amount of unlabeled data available on the web to train large neural models. Recent self-supervised approaches for speech and audio processing are also gaining attention.",
      "doi": "https://doi.org/10.1109/jstsp.2022.3205434",
      "openalex_id": "https://openalex.org/W4308480316",
      "arxiv_id": "",
      "publication_date": "2022-10-01",
      "published": "2022-10-01",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "Does not explicitly focus on Spoken Language Models (SLMs) or Full-Duplex SLMs, nor discuss architecture/training/evaluation of speech-language modeling; editorial overview of SSL for speech/audio lacks the SLM-centric scope required by inclusion criteria, so it should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "Does not explicitly focus on Spoken Language Models (SLMs) or Full-Duplex SLMs, nor discuss architecture/training/evaluation of speech-language modeling; editorial overview of SSL for speech/audio lacks the SLM-centric scope required by inclusion criteria, so it should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The title and abstract describe an editorial overview of self-supervised learning for speech and audio processing broadly without specific focus on Spoken Language Models, full-duplex interactions, or detailed modeling and evaluation of speech as a core modeling object as required. It discusses self-supervised learning generally and does not meet the specific inclusion criteria for spoken language modeling architectures, synchronous interactive behaviors, or multi-dimensional evaluations; nor does it address the exclusion criteria explicitly. Therefore, it does not fulfill the required thematic focus and specificity.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The title and abstract describe an editorial overview of self-supervised learning for speech and audio processing broadly without specific focus on Spoken Language Models, full-duplex interactions, or detailed modeling and evaluation of speech as a core modeling object as required. It discusses self-supervised learning generally and does not meet the specific inclusion criteria for spoken language modeling architectures, synchronous interactive behaviors, or multi-dimensional evaluations; nor does it address the exclusion criteria explicitly. Therefore, it does not fulfill the required thematic focus and specificity.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Simultaneous or Sequential Training? How Speech Representations Cooperate in a Multi-Task Self-Supervised Learning System*",
    "abstract": "Speech representation learning with self-supervised algorithms has resulted in notable performance boosts in many downstream tasks. Recent work combined self-supervised learning (SSL) and visually grounded speech (VGS) processing mechanisms for representation learning. The joint training with SSL and VGS mechanisms provides the opportunity to utilize both unlabeled speech and speech-related visual information based on data availability. This has shown to enhance the quality of learned representations, especially at encoding semantic- and lexical-level knowledge. In this work, we further study the joint optimization of wav2vec 2.0-based SSL and transformer-based VGS as a multitask learning system. We explore a set of training scenarios to understand how speech representations are shared or transferred between the two tasks, and what is the optimal training strategy for cross-modal semantic retrieval and phoneme discrimination performance. As a result, we find that sequential training with wav2vec 2.0 first and VGS next provides higher performance on audio-visual retrieval compared to simultaneous optimization of both learning mechanisms. However, the parallel SSL-VGS training reduces the effects of catastrophic forgetting when switching between optimization criteria. Moreover, the results suggest that phonemic representations learned through the VGS mechanism may generalize better across datasets compared to those learned with SSL.",
    "metadata": {
      "title": "Simultaneous or Sequential Training? How Speech Representations Cooperate in a Multi-Task Self-Supervised Learning System*",
      "summary": "Speech representation learning with self-supervised algorithms has resulted in notable performance boosts in many downstream tasks. Recent work combined self-supervised learning (SSL) and visually grounded speech (VGS) processing mechanisms for representation learning. The joint training with SSL and VGS mechanisms provides the opportunity to utilize both unlabeled speech and speech-related visual information based on data availability. This has shown to enhance the quality of learned representations, especially at encoding semantic- and lexical-level knowledge. In this work, we further study the joint optimization of wav2vec 2.0-based SSL and transformer-based VGS as a multitask learning system. We explore a set of training scenarios to understand how speech representations are shared or transferred between the two tasks, and what is the optimal training strategy for cross-modal semantic retrieval and phoneme discrimination performance. As a result, we find that sequential training with wav2vec 2.0 first and VGS next provides higher performance on audio-visual retrieval compared to simultaneous optimization of both learning mechanisms. However, the parallel SSL-VGS training reduces the effects of catastrophic forgetting when switching between optimization criteria. Moreover, the results suggest that phonemic representations learned through the VGS mechanism may generalize better across datasets compared to those learned with SSL.",
      "abstract": "Speech representation learning with self-supervised algorithms has resulted in notable performance boosts in many downstream tasks. Recent work combined self-supervised learning (SSL) and visually grounded speech (VGS) processing mechanisms for representation learning. The joint training with SSL and VGS mechanisms provides the opportunity to utilize both unlabeled speech and speech-related visual information based on data availability. This has shown to enhance the quality of learned representations, especially at encoding semantic- and lexical-level knowledge. In this work, we further study the joint optimization of wav2vec 2.0-based SSL and transformer-based VGS as a multitask learning system. We explore a set of training scenarios to understand how speech representations are shared or transferred between the two tasks, and what is the optimal training strategy for cross-modal semantic retrieval and phoneme discrimination performance. As a result, we find that sequential training with wav2vec 2.0 first and VGS next provides higher performance on audio-visual retrieval compared to simultaneous optimization of both learning mechanisms. However, the parallel SSL-VGS training reduces the effects of catastrophic forgetting when switching between optimization criteria. Moreover, the results suggest that phonemic representations learned through the VGS mechanism may generalize better across datasets compared to those learned with SSL.",
      "doi": "https://doi.org/10.23919/eusipco58844.2023.10290051",
      "openalex_id": "https://openalex.org/W4388118108",
      "arxiv_id": "",
      "publication_date": "2023-09-04",
      "published": "2023-09-04",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "Although the item is within the 3-year window and investigates multi-task training of speech representations, it does not center on Spoken Language Models or equivalent core concepts (full-duplex SLMs/SLM-oriented architectures), focusing instead on SSL and visually grounded speech representations, so it likely does not meet the primary inclusion criterion.",
      "evaluation": 2
    },
    "round-A_JuniorNano_reasoning": "Although the item is within the 3-year window and investigates multi-task training of speech representations, it does not center on Spoken Language Models or equivalent core concepts (full-duplex SLMs/SLM-oriented architectures), focusing instead on SSL and visually grounded speech representations, so it likely does not meet the primary inclusion criterion.",
    "round-A_JuniorNano_evaluation": 2,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on speech representation learning using self-supervised learning and visually grounded speech mechanisms, which relates to speech representation and modeling; however, it does not explicitly mention spoken language models, full-duplex interaction, or synchronization mechanisms central to the inclusion criteria, nor does it address turn-taking or interactive dialogue behaviors. The evaluation metrics focus on retrieval and phoneme discrimination rather than real-time interaction, overlap, or behavioral arbitration, and the abstract lacks references to speech language modeling architectures or full-duplex spoken language models. Therefore, it does not fully meet the inclusion criteria and partially aligns with exclusion criteria.",
      "evaluation": 2
    },
    "round-A_JuniorMini_reasoning": "The study focuses on speech representation learning using self-supervised learning and visually grounded speech mechanisms, which relates to speech representation and modeling; however, it does not explicitly mention spoken language models, full-duplex interaction, or synchronization mechanisms central to the inclusion criteria, nor does it address turn-taking or interactive dialogue behaviors. The evaluation metrics focus on retrieval and phoneme discrimination rather than real-time interaction, overlap, or behavioral arbitration, and the abstract lacks references to speech language modeling architectures or full-duplex spoken language models. Therefore, it does not fully meet the inclusion criteria and partially aligns with exclusion criteria.",
    "round-A_JuniorMini_evaluation": 2,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:2)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Simultaneous or Sequential Training? How Speech Representations Cooperate in a Multi-Task Self-Supervised Learning System",
    "abstract": "Speech representation learning with self-supervised algorithms has resulted in notable performance boosts in many downstream tasks. Recent work combined self-supervised learning (SSL) and visually grounded speech (VGS) processing mechanisms for representation learning. The joint training with SSL and VGS mechanisms provides the opportunity to utilize both unlabeled speech and speech-related visual information based on data availability. This has shown to enhance the quality of learned representations, especially at encoding semantic- and lexical-level knowledge. In this work, we further study the joint optimization of wav2vec 2.0-based SSL and transformer-based VGS as a multi-task learning system. We explore a set of training scenarios to understand how speech representations are shared or transferred between the two tasks, and what is the optimal training strategy for cross-modal semantic retrieval and phoneme discrimination performance. As a result, we find that sequential training with wav2vec 2.0 first and VGS next provides higher performance on audio-visual retrieval compared to simultaneous optimization of both learning mechanisms. However, the parallel SSL-VGS training reduces the effects of catastrophic forgetting when switching between optimization criteria. Moreover, the results suggest that phonemic representations learned through the VGS mechanism may generalize better across datasets compared to those learned with SSL.",
    "metadata": {
      "title": "Simultaneous or Sequential Training? How Speech Representations Cooperate in a Multi-Task Self-Supervised Learning System",
      "summary": "Speech representation learning with self-supervised algorithms has resulted in notable performance boosts in many downstream tasks. Recent work combined self-supervised learning (SSL) and visually grounded speech (VGS) processing mechanisms for representation learning. The joint training with SSL and VGS mechanisms provides the opportunity to utilize both unlabeled speech and speech-related visual information based on data availability. This has shown to enhance the quality of learned representations, especially at encoding semantic- and lexical-level knowledge. In this work, we further study the joint optimization of wav2vec 2.0-based SSL and transformer-based VGS as a multi-task learning system. We explore a set of training scenarios to understand how speech representations are shared or transferred between the two tasks, and what is the optimal training strategy for cross-modal semantic retrieval and phoneme discrimination performance. As a result, we find that sequential training with wav2vec 2.0 first and VGS next provides higher performance on audio-visual retrieval compared to simultaneous optimization of both learning mechanisms. However, the parallel SSL-VGS training reduces the effects of catastrophic forgetting when switching between optimization criteria. Moreover, the results suggest that phonemic representations learned through the VGS mechanism may generalize better across datasets compared to those learned with SSL.",
      "abstract": "Speech representation learning with self-supervised algorithms has resulted in notable performance boosts in many downstream tasks. Recent work combined self-supervised learning (SSL) and visually grounded speech (VGS) processing mechanisms for representation learning. The joint training with SSL and VGS mechanisms provides the opportunity to utilize both unlabeled speech and speech-related visual information based on data availability. This has shown to enhance the quality of learned representations, especially at encoding semantic- and lexical-level knowledge. In this work, we further study the joint optimization of wav2vec 2.0-based SSL and transformer-based VGS as a multi-task learning system. We explore a set of training scenarios to understand how speech representations are shared or transferred between the two tasks, and what is the optimal training strategy for cross-modal semantic retrieval and phoneme discrimination performance. As a result, we find that sequential training with wav2vec 2.0 first and VGS next provides higher performance on audio-visual retrieval compared to simultaneous optimization of both learning mechanisms. However, the parallel SSL-VGS training reduces the effects of catastrophic forgetting when switching between optimization criteria. Moreover, the results suggest that phonemic representations learned through the VGS mechanism may generalize better across datasets compared to those learned with SSL.",
      "doi": "https://doi.org/10.48550/arxiv.2306.02972",
      "openalex_id": "https://openalex.org/W4379540254",
      "arxiv_id": "",
      "publication_date": "2023-06-05",
      "published": "2023-06-05",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "雖涉及語音表示學習與跨模態視覺語音結合，但未在標題或摘要中明確以 Spoken/Speech Language Models 為核心或直接討論語音語言建模，因此不符合納入標準的關鍵字與主題要求。",
      "evaluation": 2
    },
    "round-A_JuniorNano_reasoning": "雖涉及語音表示學習與跨模態視覺語音結合，但未在標題或摘要中明確以 Spoken/Speech Language Models 為核心或直接討論語音語言建模，因此不符合納入標準的關鍵字與主題要求。",
    "round-A_JuniorNano_evaluation": 2,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on speech representation learning through self-supervised learning combined with visually grounded speech processing, which involves modeling speech representations rather than merely using speech as input to text processing. However, it does not explicitly address Spoken Language Models (SLMs) or Full-Duplex Spoken Language Models (FD-SLMs), nor does it discuss end-to-end or hybrid architectures specifically facilitating synchronous full-duplex interactions, turn-taking, interruptions, or related behavioral arbitration. It centers on improving representations for downstream tasks like audio-visual retrieval and phoneme discrimination, without clear emphasis on low-latency real-time dialogue interaction or synchronization mechanisms required by the inclusion criteria. Therefore, while relevant to speech modeling, the paper does not fully meet the inclusion criteria focused on SLMs/FD-SLMs and synchronization in speech interaction contexts.",
      "evaluation": 2
    },
    "round-A_JuniorMini_reasoning": "The study focuses on speech representation learning through self-supervised learning combined with visually grounded speech processing, which involves modeling speech representations rather than merely using speech as input to text processing. However, it does not explicitly address Spoken Language Models (SLMs) or Full-Duplex Spoken Language Models (FD-SLMs), nor does it discuss end-to-end or hybrid architectures specifically facilitating synchronous full-duplex interactions, turn-taking, interruptions, or related behavioral arbitration. It centers on improving representations for downstream tasks like audio-visual retrieval and phoneme discrimination, without clear emphasis on low-latency real-time dialogue interaction or synchronization mechanisms required by the inclusion criteria. Therefore, while relevant to speech modeling, the paper does not fully meet the inclusion criteria focused on SLMs/FD-SLMs and synchronization in speech interaction contexts.",
    "round-A_JuniorMini_evaluation": 2,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:2)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Word segmentation from transcriptions of child-directed speech using lexical and sub-lexical cues",
    "abstract": "Abstract We compare two frameworks for the segmentation of words in child-directed speech, PHOCUS and MULTICUE. PHOCUS is driven by lexical recognition, whereas MULTICUE combines sub-lexical properties to make boundary decisions, representing differing views of speech processing. We replicate these frameworks, perform novel benchmarking and confirm that both achieve competitive results. We develop a new framework for segmentation, the DYnamic Programming MULTIple-cue framework (DYMULTI), which combines the strengths of PHOCUS and MULTICUE by considering both sub-lexical and lexical cues when making boundary decisions. DYMULTI achieves state-of-the-art results and outperforms PHOCUS and MULTICUE on 15 of 26 languages in a cross-lingual experiment. As a model built on psycholinguistic principles, this validates DYMULTI as a robust model for speech segmentation and a contribution to the understanding of language acquisition.",
    "metadata": {
      "title": "Word segmentation from transcriptions of child-directed speech using lexical and sub-lexical cues",
      "summary": "Abstract We compare two frameworks for the segmentation of words in child-directed speech, PHOCUS and MULTICUE. PHOCUS is driven by lexical recognition, whereas MULTICUE combines sub-lexical properties to make boundary decisions, representing differing views of speech processing. We replicate these frameworks, perform novel benchmarking and confirm that both achieve competitive results. We develop a new framework for segmentation, the DYnamic Programming MULTIple-cue framework (DYMULTI), which combines the strengths of PHOCUS and MULTICUE by considering both sub-lexical and lexical cues when making boundary decisions. DYMULTI achieves state-of-the-art results and outperforms PHOCUS and MULTICUE on 15 of 26 languages in a cross-lingual experiment. As a model built on psycholinguistic principles, this validates DYMULTI as a robust model for speech segmentation and a contribution to the understanding of language acquisition.",
      "abstract": "Abstract We compare two frameworks for the segmentation of words in child-directed speech, PHOCUS and MULTICUE. PHOCUS is driven by lexical recognition, whereas MULTICUE combines sub-lexical properties to make boundary decisions, representing differing views of speech processing. We replicate these frameworks, perform novel benchmarking and confirm that both achieve competitive results. We develop a new framework for segmentation, the DYnamic Programming MULTIple-cue framework (DYMULTI), which combines the strengths of PHOCUS and MULTICUE by considering both sub-lexical and lexical cues when making boundary decisions. DYMULTI achieves state-of-the-art results and outperforms PHOCUS and MULTICUE on 15 of 26 languages in a cross-lingual experiment. As a model built on psycholinguistic principles, this validates DYMULTI as a robust model for speech segmentation and a contribution to the understanding of language acquisition.",
      "doi": "https://doi.org/10.1017/s0305000923000491",
      "openalex_id": "https://openalex.org/W4386623759",
      "arxiv_id": "",
      "publication_date": "2023-09-12",
      "published": "2023-09-12",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The article investigates word segmentation frameworks for child-directed speech and psycholinguistic evaluation, but does not center on Spoken Language Models or Speech-Language Modeling as the core object, nor discuss real-time interaction/synchronization, thus it fails the inclusion criteria requiring SLM/FD-SLM focus and related evaluation frameworks.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The article investigates word segmentation frameworks for child-directed speech and psycholinguistic evaluation, but does not center on Spoken Language Models or Speech-Language Modeling as the core object, nor discuss real-time interaction/synchronization, thus it fails the inclusion criteria requiring SLM/FD-SLM focus and related evaluation frameworks.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The presented study focuses on word segmentation in child-directed speech using psycholinguistic models, but it neither explicitly addresses spoken language models or full-duplex spoken language modeling nor discusses model architectures or evaluation in the context of end-to-end or mixed speech-LLM frameworks, turn-taking, or synchronization, which are central to the inclusion criteria; it rather centers on linguistic segmentation tasks without mention of speech language models or interaction evaluation metrics.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The presented study focuses on word segmentation in child-directed speech using psycholinguistic models, but it neither explicitly addresses spoken language models or full-duplex spoken language modeling nor discusses model architectures or evaluation in the context of end-to-end or mixed speech-LLM frameworks, turn-taking, or synchronization, which are central to the inclusion criteria; it rather centers on linguistic segmentation tasks without mention of speech language models or interaction evaluation metrics.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Simulating Early Phonetic and Word Learning Without Linguistic Categories",
    "abstract": "ABSTRACT Before they even talk, infants become sensitive to the speech sounds of their native language and recognize the auditory form of an increasing number of words. Traditionally, these early perceptual changes are attributed to an emerging knowledge of linguistic categories such as phonemes or words. However, there is growing skepticism surrounding this interpretation due to limited evidence of category knowledge in infants. Previous modeling work has shown that a distributional learning algorithm could reproduce perceptual changes in infants' early phonetic learning without acquiring phonetic categories. Taking this inquiry further, we propose that linguistic categories may not be needed for early word learning. We introduce STELA, a predictive coding algorithm designed to extract statistical patterns from continuous raw speech data. Our findings demonstrate that STELA can reproduce some developmental patterns of phonetic and word form learning without relying on linguistic categories such as phonemes or words nor requiring explicit word segmentation. Through an analysis of the learned representations, we show evidence that linguistic categories may emerge as an end product of learning rather than being prerequisites during early language acquisition.",
    "metadata": {
      "title": "Simulating Early Phonetic and Word Learning Without Linguistic Categories",
      "summary": "ABSTRACT Before they even talk, infants become sensitive to the speech sounds of their native language and recognize the auditory form of an increasing number of words. Traditionally, these early perceptual changes are attributed to an emerging knowledge of linguistic categories such as phonemes or words. However, there is growing skepticism surrounding this interpretation due to limited evidence of category knowledge in infants. Previous modeling work has shown that a distributional learning algorithm could reproduce perceptual changes in infants' early phonetic learning without acquiring phonetic categories. Taking this inquiry further, we propose that linguistic categories may not be needed for early word learning. We introduce STELA, a predictive coding algorithm designed to extract statistical patterns from continuous raw speech data. Our findings demonstrate that STELA can reproduce some developmental patterns of phonetic and word form learning without relying on linguistic categories such as phonemes or words nor requiring explicit word segmentation. Through an analysis of the learned representations, we show evidence that linguistic categories may emerge as an end product of learning rather than being prerequisites during early language acquisition.",
      "abstract": "ABSTRACT Before they even talk, infants become sensitive to the speech sounds of their native language and recognize the auditory form of an increasing number of words. Traditionally, these early perceptual changes are attributed to an emerging knowledge of linguistic categories such as phonemes or words. However, there is growing skepticism surrounding this interpretation due to limited evidence of category knowledge in infants. Previous modeling work has shown that a distributional learning algorithm could reproduce perceptual changes in infants' early phonetic learning without acquiring phonetic categories. Taking this inquiry further, we propose that linguistic categories may not be needed for early word learning. We introduce STELA, a predictive coding algorithm designed to extract statistical patterns from continuous raw speech data. Our findings demonstrate that STELA can reproduce some developmental patterns of phonetic and word form learning without relying on linguistic categories such as phonemes or words nor requiring explicit word segmentation. Through an analysis of the learned representations, we show evidence that linguistic categories may emerge as an end product of learning rather than being prerequisites during early language acquisition.",
      "doi": "https://doi.org/10.1111/desc.13606",
      "openalex_id": "https://openalex.org/W4406132022",
      "arxiv_id": "",
      "publication_date": "2025-01-06",
      "published": "2025-01-06",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The abstract describes a cognitive modeling study of early phonetic/word learning from raw speech without linguistic categories, but it does not present a Spoken Language Model (SLM) or Speech Language Model (Speech-LM) architecture, training, or evaluation, nor does it focus on real-time interactive dialogue or FD-SLM-like synchronization, so it does not clearly meet the inclusion criteria and should be excluded.",
      "evaluation": 2
    },
    "round-A_JuniorNano_reasoning": "The abstract describes a cognitive modeling study of early phonetic/word learning from raw speech without linguistic categories, but it does not present a Spoken Language Model (SLM) or Speech Language Model (Speech-LM) architecture, training, or evaluation, nor does it focus on real-time interactive dialogue or FD-SLM-like synchronization, so it does not clearly meet the inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 2,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on modeling early phonetic and word learning in infants using a predictive coding algorithm that analyzes continuous raw speech data, but it does not discuss or mention spoken language models (SLMs), full-duplex interaction capabilities, synchronization, or relevant evaluation metrics aligned with the inclusion criteria. Moreover, the paper is centered on developmental cognitive modeling rather than engineering or evaluation of spoken language model architectures or real-time dialogue systems, thus failing to meet the required topic scope and terminology demands.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on modeling early phonetic and word learning in infants using a predictive coding algorithm that analyzes continuous raw speech data, but it does not discuss or mention spoken language models (SLMs), full-duplex interaction capabilities, synchronization, or relevant evaluation metrics aligned with the inclusion criteria. Moreover, the paper is centered on developmental cognitive modeling rather than engineering or evaluation of spoken language model architectures or real-time dialogue systems, thus failing to meet the required topic scope and terminology demands.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:2)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Phoneme-Aware Hierarchical Augmentation and Semantic-Aware SpecAugment for Low-Resource Cantonese Speech Recognition",
    "abstract": "Cantonese Automatic Speech Recognition (ASR) is hindered by tonal complexity, acoustic diversity, and a lack of labelled data. This study proposes a phoneme-aware hierarchical augmentation framework that enhances performance without additional annotation. A Phoneme Substitution Matrix (PSM), built from Montreal Forced Aligner alignments and Tacotron-2 synthesis, injects adversarial phoneme variants into both transcripts and their aligned audio segments, enlarging pronunciation diversity. Concurrently, a semantic-aware SpecAugment scheme exploits wav2vec 2.0 attention heat maps and keyword boundaries to adaptively mask informative time–frequency regions; a reinforcement-learning controller tunes the masking schedule online, forcing the model to rely on a wider context. On the Common Voice Cantonese 50 h subset, the combined strategy reduces the character error rate (CER) from 26.17% to 16.88% with wav2vec 2.0 and from 38.83% to 23.55% with Zipformer. At 100 h, the CER further drops to 4.27% and 2.32%, yielding relative gains of 32–44%. Ablation studies confirm that phoneme-level and masking components provide complementary benefits. The framework offers a practical, model-independent path toward accurate ASR for Cantonese and other low-resource tonal languages. This paper presents an intelligent sensing-oriented modeling framework for speech signals, which is suitable for deployment on edge or embedded systems to process input from audio sensors (e.g., microphones) and shows promising potential for voice-interactive terminal applications.",
    "metadata": {
      "title": "Phoneme-Aware Hierarchical Augmentation and Semantic-Aware SpecAugment for Low-Resource Cantonese Speech Recognition",
      "summary": "Cantonese Automatic Speech Recognition (ASR) is hindered by tonal complexity, acoustic diversity, and a lack of labelled data. This study proposes a phoneme-aware hierarchical augmentation framework that enhances performance without additional annotation. A Phoneme Substitution Matrix (PSM), built from Montreal Forced Aligner alignments and Tacotron-2 synthesis, injects adversarial phoneme variants into both transcripts and their aligned audio segments, enlarging pronunciation diversity. Concurrently, a semantic-aware SpecAugment scheme exploits wav2vec 2.0 attention heat maps and keyword boundaries to adaptively mask informative time–frequency regions; a reinforcement-learning controller tunes the masking schedule online, forcing the model to rely on a wider context. On the Common Voice Cantonese 50 h subset, the combined strategy reduces the character error rate (CER) from 26.17% to 16.88% with wav2vec 2.0 and from 38.83% to 23.55% with Zipformer. At 100 h, the CER further drops to 4.27% and 2.32%, yielding relative gains of 32–44%. Ablation studies confirm that phoneme-level and masking components provide complementary benefits. The framework offers a practical, model-independent path toward accurate ASR for Cantonese and other low-resource tonal languages. This paper presents an intelligent sensing-oriented modeling framework for speech signals, which is suitable for deployment on edge or embedded systems to process input from audio sensors (e.g., microphones) and shows promising potential for voice-interactive terminal applications.",
      "abstract": "Cantonese Automatic Speech Recognition (ASR) is hindered by tonal complexity, acoustic diversity, and a lack of labelled data. This study proposes a phoneme-aware hierarchical augmentation framework that enhances performance without additional annotation. A Phoneme Substitution Matrix (PSM), built from Montreal Forced Aligner alignments and Tacotron-2 synthesis, injects adversarial phoneme variants into both transcripts and their aligned audio segments, enlarging pronunciation diversity. Concurrently, a semantic-aware SpecAugment scheme exploits wav2vec 2.0 attention heat maps and keyword boundaries to adaptively mask informative time–frequency regions; a reinforcement-learning controller tunes the masking schedule online, forcing the model to rely on a wider context. On the Common Voice Cantonese 50 h subset, the combined strategy reduces the character error rate (CER) from 26.17% to 16.88% with wav2vec 2.0 and from 38.83% to 23.55% with Zipformer. At 100 h, the CER further drops to 4.27% and 2.32%, yielding relative gains of 32–44%. Ablation studies confirm that phoneme-level and masking components provide complementary benefits. The framework offers a practical, model-independent path toward accurate ASR for Cantonese and other low-resource tonal languages. This paper presents an intelligent sensing-oriented modeling framework for speech signals, which is suitable for deployment on edge or embedded systems to process input from audio sensors (e.g., microphones) and shows promising potential for voice-interactive terminal applications.",
      "doi": "https://doi.org/10.3390/s25144288",
      "openalex_id": "https://openalex.org/W4412195314",
      "arxiv_id": "",
      "publication_date": "2025-07-09",
      "published": "2025-07-09",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This paper centers on phoneme-aware augmentation and SpecAugment for Cantonese ASR; it treats speech primarily as input to ASR and does not present Spoken Language Model (SLM) development, end-to-end speech–language modeling, dialogue interaction, or multi-turn synchronization that the criteria require.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "This paper centers on phoneme-aware augmentation and SpecAugment for Cantonese ASR; it treats speech primarily as input to ASR and does not present Spoken Language Model (SLM) development, end-to-end speech–language modeling, dialogue interaction, or multi-turn synchronization that the criteria require.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on low-resource Cantonese ASR enhancement using phoneme-aware augmentation and SpecAugment methods, with improvements measured by character error rate. However, it mainly addresses ASR performance and acoustic augmentation rather than spoken language models or full-duplex spoken interaction models. There is no mention of spoken language models, speech language models, or related concepts in the title or abstract. Additionally, it lacks discussion of full-duplex interaction, cognitive parallelism, synchronization, or related evaluation criteria beyond ASR accuracy.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on low-resource Cantonese ASR enhancement using phoneme-aware augmentation and SpecAugment methods, with improvements measured by character error rate. However, it mainly addresses ASR performance and acoustic augmentation rather than spoken language models or full-duplex spoken interaction models. There is no mention of spoken language models, speech language models, or related concepts in the title or abstract. Additionally, it lacks discussion of full-duplex interaction, cognitive parallelism, synchronization, or related evaluation criteria beyond ASR accuracy.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "LoRA-INT8 Whisper: A Low-Cost Cantonese Speech Recognition Framework for Edge Devices",
    "abstract": "To address the triple bottlenecks of data scarcity, oversized models, and slow inference that hinder Cantonese automatic speech recognition (ASR) in low-resource and edge-deployment settings, this study proposes a cost-effective Cantonese ASR system based on LoRA fine-tuning and INT8 quantization. First, Whisper-tiny is parameter-efficiently fine-tuned on the Common Voice zh-HK training set using LoRA with rank = 8. Only 1.6% of the original weights are updated, reducing the character error rate (CER) from 49.5% to 11.1%, a performance close to full fine-tuning (10.3%), while cutting the training memory footprint and computational cost by approximately one order of magnitude. Next, the fine-tuned model is compressed into a 60 MB INT8 checkpoint via dynamic quantization in ONNX Runtime. On a MacBook Pro M1 Max CPU, the quantized model achieves an RTF = 0.20 (offline inference 5 × real-time) and 43% lower latency than the FP16 baseline; on an NVIDIA A10 GPU, it reaches RTF = 0.06, meeting the requirements of high-concurrency cloud services. Ablation studies confirm that the LoRA-INT8 configuration offers the best trade-off among accuracy, speed, and model size. Limitations include the absence of spontaneous-speech noise data, extreme-hardware validation, and adaptive LoRA structure optimization. Future work will incorporate large-scale self-supervised pre-training, tone-aware loss functions, AdaLoRA architecture search, and INT4/NPU quantization, and will establish an mJ/char energy–accuracy curve. The ultimate goal is to achieve CER ≤ 8%, RTF &lt; 0.1, and mJ/char &lt; 1 for low-power real-time Cantonese ASR in practical IoT scenarios.",
    "metadata": {
      "title": "LoRA-INT8 Whisper: A Low-Cost Cantonese Speech Recognition Framework for Edge Devices",
      "summary": "To address the triple bottlenecks of data scarcity, oversized models, and slow inference that hinder Cantonese automatic speech recognition (ASR) in low-resource and edge-deployment settings, this study proposes a cost-effective Cantonese ASR system based on LoRA fine-tuning and INT8 quantization. First, Whisper-tiny is parameter-efficiently fine-tuned on the Common Voice zh-HK training set using LoRA with rank = 8. Only 1.6% of the original weights are updated, reducing the character error rate (CER) from 49.5% to 11.1%, a performance close to full fine-tuning (10.3%), while cutting the training memory footprint and computational cost by approximately one order of magnitude. Next, the fine-tuned model is compressed into a 60 MB INT8 checkpoint via dynamic quantization in ONNX Runtime. On a MacBook Pro M1 Max CPU, the quantized model achieves an RTF = 0.20 (offline inference 5 × real-time) and 43% lower latency than the FP16 baseline; on an NVIDIA A10 GPU, it reaches RTF = 0.06, meeting the requirements of high-concurrency cloud services. Ablation studies confirm that the LoRA-INT8 configuration offers the best trade-off among accuracy, speed, and model size. Limitations include the absence of spontaneous-speech noise data, extreme-hardware validation, and adaptive LoRA structure optimization. Future work will incorporate large-scale self-supervised pre-training, tone-aware loss functions, AdaLoRA architecture search, and INT4/NPU quantization, and will establish an mJ/char energy–accuracy curve. The ultimate goal is to achieve CER ≤ 8%, RTF &lt; 0.1, and mJ/char &lt; 1 for low-power real-time Cantonese ASR in practical IoT scenarios.",
      "abstract": "To address the triple bottlenecks of data scarcity, oversized models, and slow inference that hinder Cantonese automatic speech recognition (ASR) in low-resource and edge-deployment settings, this study proposes a cost-effective Cantonese ASR system based on LoRA fine-tuning and INT8 quantization. First, Whisper-tiny is parameter-efficiently fine-tuned on the Common Voice zh-HK training set using LoRA with rank = 8. Only 1.6% of the original weights are updated, reducing the character error rate (CER) from 49.5% to 11.1%, a performance close to full fine-tuning (10.3%), while cutting the training memory footprint and computational cost by approximately one order of magnitude. Next, the fine-tuned model is compressed into a 60 MB INT8 checkpoint via dynamic quantization in ONNX Runtime. On a MacBook Pro M1 Max CPU, the quantized model achieves an RTF = 0.20 (offline inference 5 × real-time) and 43% lower latency than the FP16 baseline; on an NVIDIA A10 GPU, it reaches RTF = 0.06, meeting the requirements of high-concurrency cloud services. Ablation studies confirm that the LoRA-INT8 configuration offers the best trade-off among accuracy, speed, and model size. Limitations include the absence of spontaneous-speech noise data, extreme-hardware validation, and adaptive LoRA structure optimization. Future work will incorporate large-scale self-supervised pre-training, tone-aware loss functions, AdaLoRA architecture search, and INT4/NPU quantization, and will establish an mJ/char energy–accuracy curve. The ultimate goal is to achieve CER ≤ 8%, RTF &lt; 0.1, and mJ/char &lt; 1 for low-power real-time Cantonese ASR in practical IoT scenarios.",
      "doi": "https://doi.org/10.3390/s25175404",
      "openalex_id": "https://openalex.org/W4413930132",
      "arxiv_id": "",
      "publication_date": "2025-09-01",
      "published": "2025-09-01",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "此項目聚焦於語音識別（ASR）框架的模型微調與量化，未涉及語音語言模型（SLM/FD-SLM）核心架構、跨語音交互或同步行為等議題，符合排除條件中的“僅聚焦於單一語音技術/未涵蓋對話互動與語音語言建模觀點”，因此應予以排除。",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "此項目聚焦於語音識別（ASR）框架的模型微調與量化，未涉及語音語言模型（SLM/FD-SLM）核心架構、跨語音交互或同步行為等議題，符合排除條件中的“僅聚焦於單一語音技術/未涵蓋對話互動與語音語言建模觀點”，因此應予以排除。",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on an ASR (Automatic Speech Recognition) system optimized for Cantonese on edge devices using LoRA fine-tuning and quantization techniques; however, it does not emphasize spoken language models, full-duplex interactions, synchronization in conversation, or cognitive parallelism—key inclusion criteria; it also treats speech as input for recognition rather than modeling speech as a primary object or addressing interactive speech behavior or dialogue arbitration, and its evaluation centers chiefly on recognition error rates and inference speed rather than real-time interactions or speech overlap management. Therefore, it fails to meet the necessary inclusion criteria and falls into exclusion criteria concerning single-task ASR focus without modeling spoken language or interactive behavior.",
      "evaluation": 2
    },
    "round-A_JuniorMini_reasoning": "The study focuses on an ASR (Automatic Speech Recognition) system optimized for Cantonese on edge devices using LoRA fine-tuning and quantization techniques; however, it does not emphasize spoken language models, full-duplex interactions, synchronization in conversation, or cognitive parallelism—key inclusion criteria; it also treats speech as input for recognition rather than modeling speech as a primary object or addressing interactive speech behavior or dialogue arbitration, and its evaluation centers chiefly on recognition error rates and inference speed rather than real-time interactions or speech overlap management. Therefore, it fails to meet the necessary inclusion criteria and falls into exclusion criteria concerning single-task ASR focus without modeling spoken language or interactive behavior.",
    "round-A_JuniorMini_evaluation": 2,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:2)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "A pipeline for stochastic and controlled generation of realistic language input for simulating infant language acquisition",
    "abstract": "Abstract Computational models of early language development involve implementing theories of learning as functional learning algorithms, exposing these models to realistic language input, and comparing learning outcomes to those in infants. While recent research has made major strides in developing more powerful learning models and evaluation protocols grounded in infant data, models are still predominantly trained with non-naturalistic input data, such as crowd-sourced read speech or text transcripts. This is due to the lack of suitable child-directed speech (CDS) corpora in terms of scale and quality. In parallel, the question of how properties and individual variability in language input affect learning outcomes is an active area of empirical research, underlining the need for realistic yet controllable data for modeling such phenomena. This paper presents a solution to the training data problem through stochastic generation of naturalistic CDS data using statistical models, thereby enabling controlled computational simulations with naturalistic input. We provide a proof-of-concept demonstration of the approach by showing how naturalistic CDS transcripts can be generated with a language model conditioned on recipient information (here, infant age), and how text-to-speech systems can be used to convert the transcripts to high-quality speech with a controllable speaking style. We also conduct modeling experiments with generated speech corpora by varying different aspects of the data, showing how this maps into different learning outcomes, thereby demonstrating the feasibility of the approach for controlled language learning simulations. Finally, we discuss the limitations of using synthetic data in general, and of the present proof-of-concept pipeline in particular.",
    "metadata": {
      "title": "A pipeline for stochastic and controlled generation of realistic language input for simulating infant language acquisition",
      "summary": "Abstract Computational models of early language development involve implementing theories of learning as functional learning algorithms, exposing these models to realistic language input, and comparing learning outcomes to those in infants. While recent research has made major strides in developing more powerful learning models and evaluation protocols grounded in infant data, models are still predominantly trained with non-naturalistic input data, such as crowd-sourced read speech or text transcripts. This is due to the lack of suitable child-directed speech (CDS) corpora in terms of scale and quality. In parallel, the question of how properties and individual variability in language input affect learning outcomes is an active area of empirical research, underlining the need for realistic yet controllable data for modeling such phenomena. This paper presents a solution to the training data problem through stochastic generation of naturalistic CDS data using statistical models, thereby enabling controlled computational simulations with naturalistic input. We provide a proof-of-concept demonstration of the approach by showing how naturalistic CDS transcripts can be generated with a language model conditioned on recipient information (here, infant age), and how text-to-speech systems can be used to convert the transcripts to high-quality speech with a controllable speaking style. We also conduct modeling experiments with generated speech corpora by varying different aspects of the data, showing how this maps into different learning outcomes, thereby demonstrating the feasibility of the approach for controlled language learning simulations. Finally, we discuss the limitations of using synthetic data in general, and of the present proof-of-concept pipeline in particular.",
      "abstract": "Abstract Computational models of early language development involve implementing theories of learning as functional learning algorithms, exposing these models to realistic language input, and comparing learning outcomes to those in infants. While recent research has made major strides in developing more powerful learning models and evaluation protocols grounded in infant data, models are still predominantly trained with non-naturalistic input data, such as crowd-sourced read speech or text transcripts. This is due to the lack of suitable child-directed speech (CDS) corpora in terms of scale and quality. In parallel, the question of how properties and individual variability in language input affect learning outcomes is an active area of empirical research, underlining the need for realistic yet controllable data for modeling such phenomena. This paper presents a solution to the training data problem through stochastic generation of naturalistic CDS data using statistical models, thereby enabling controlled computational simulations with naturalistic input. We provide a proof-of-concept demonstration of the approach by showing how naturalistic CDS transcripts can be generated with a language model conditioned on recipient information (here, infant age), and how text-to-speech systems can be used to convert the transcripts to high-quality speech with a controllable speaking style. We also conduct modeling experiments with generated speech corpora by varying different aspects of the data, showing how this maps into different learning outcomes, thereby demonstrating the feasibility of the approach for controlled language learning simulations. Finally, we discuss the limitations of using synthetic data in general, and of the present proof-of-concept pipeline in particular.",
      "doi": "https://doi.org/10.3758/s13428-025-02772-6",
      "openalex_id": "https://openalex.org/W4413974959",
      "arxiv_id": "",
      "publication_date": "2025-09-04",
      "published": "2025-09-04",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item centers on using a language model to generate naturalistic child-directed speech and convert it to speech via TTS for simulations of infant language learning, aligning with Spoken Language/ Speech Language Modeling concepts and including architecture/data-generation aspects; it is within the last 3 years and discusses evaluation via learning outcomes, not restricted to single-task ASR/TTS or non-interactive metrics.",
      "evaluation": 4
    },
    "round-A_JuniorNano_reasoning": "The item centers on using a language model to generate naturalistic child-directed speech and convert it to speech via TTS for simulations of infant language learning, aligning with Spoken Language/ Speech Language Modeling concepts and including architecture/data-generation aspects; it is within the last 3 years and discusses evaluation via learning outcomes, not restricted to single-task ASR/TTS or non-interactive metrics.",
    "round-A_JuniorNano_evaluation": 4,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on generating realistic child-directed speech data using language models and text-to-speech systems for infant language acquisition simulations; however, it does not clearly address Spoken Language Models (SLMs) or Full-Duplex Spoken Language Models (FD-SLMs) nor does it discuss modeling architectures or evaluations related to synchronous speech interactions, low-latency, or interactive behaviors as required by the inclusion criteria. It mainly treats speech as a medium for data generation rather than as a core object of speech-level modeling with advanced interactive capabilities.",
      "evaluation": 2
    },
    "round-A_JuniorMini_reasoning": "The study focuses on generating realistic child-directed speech data using language models and text-to-speech systems for infant language acquisition simulations; however, it does not clearly address Spoken Language Models (SLMs) or Full-Duplex Spoken Language Models (FD-SLMs) nor does it discuss modeling architectures or evaluations related to synchronous speech interactions, low-latency, or interactive behaviors as required by the inclusion criteria. It mainly treats speech as a medium for data generation rather than as a core object of speech-level modeling with advanced interactive capabilities.",
    "round-A_JuniorMini_evaluation": 2,
    "round-B_SeniorLead_output": {
      "reasoning": "Junior reviewers disagreed (one gave 4, one 2); after review, this paper focuses on stochastic generation of child-directed speech via a text-conditioned language model and converting transcripts to speech with TTS to create synthetic corpora for infant language-learning simulations—it treats speech mainly as a medium for data generation and does not present spoken-language-model architectures, speech-level language modeling, end-to-end Speech–LLM designs, or any full-duplex/synchronous interaction evaluation required by the inclusion criteria; therefore it fails the core requirement that spoken language models or equivalent speech-language modeling be central and explicitly discussed in title/abstract.",
      "evaluation": 2
    },
    "round-B_SeniorLead_reasoning": "Junior reviewers disagreed (one gave 4, one 2); after review, this paper focuses on stochastic generation of child-directed speech via a text-conditioned language model and converting transcripts to speech with TTS to create synthetic corpora for infant language-learning simulations—it treats speech mainly as a medium for data generation and does not present spoken-language-model architectures, speech-level language modeling, end-to-end Speech–LLM designs, or any full-duplex/synchronous interaction evaluation required by the inclusion criteria; therefore it fails the core requirement that spoken language models or equivalent speech-language modeling be central and explicitly discussed in title/abstract.",
    "round-B_SeniorLead_evaluation": 2,
    "final_verdict": "exclude (senior:2)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Phoneme-Aware Augmentation for Robust Cantonese ASR Under Low-Resource Conditions",
    "abstract": "Cantonese automatic speech recognition (ASR) faces persistent challenges due to its nine lexical tones, extensive phonological variation, and the scarcity of professionally transcribed corpora. To address these issues, we propose a lightweight and data-efficient framework that leverages weak phonetic supervision (WPS) in conjunction with two pho-neme-aware augmentation strategies. (1) Dynamic Boundary-Aligned Phoneme Dropout progressively removes entire IPA segments according to a curriculum schedule, simulating real-world phenomena such as elision, lenition, and tonal drift while ensuring training stability. (2) Phoneme-Aware SpecAugment confines all time- and frequency-masking operations within phoneme boundaries and prioritizes high-attention regions, thereby preserving intra-phonemic contours and formant integrity. Built on the Whistle encoder—which integrates a Conformer backbone, Connectionist Temporal Classification–Conditional Random Field (CTC-CRF) alignment, and a multi-lingual phonetic space—the approach requires only a grapheme-to-phoneme lexicon and Montreal Forced Aligner outputs, without any additional manual labeling. Experiments on the Cantonese subset of Common Voice demonstrate consistent gains: Dynamic Dropout alone reduces phoneme error rate (PER) from 17.8% to 16.7% with 50 h of speech and 16.4% to 15.1% with 100 h, while the combination of the two augmentations further lowers PER to 15.9%/14.4%. These results confirm that structure-aware phoneme-level perturbations provide an effective and low-cost solution for building robust Cantonese ASR systems under low-resource conditions.",
    "metadata": {
      "title": "Phoneme-Aware Augmentation for Robust Cantonese ASR Under Low-Resource Conditions",
      "summary": "Cantonese automatic speech recognition (ASR) faces persistent challenges due to its nine lexical tones, extensive phonological variation, and the scarcity of professionally transcribed corpora. To address these issues, we propose a lightweight and data-efficient framework that leverages weak phonetic supervision (WPS) in conjunction with two pho-neme-aware augmentation strategies. (1) Dynamic Boundary-Aligned Phoneme Dropout progressively removes entire IPA segments according to a curriculum schedule, simulating real-world phenomena such as elision, lenition, and tonal drift while ensuring training stability. (2) Phoneme-Aware SpecAugment confines all time- and frequency-masking operations within phoneme boundaries and prioritizes high-attention regions, thereby preserving intra-phonemic contours and formant integrity. Built on the Whistle encoder—which integrates a Conformer backbone, Connectionist Temporal Classification–Conditional Random Field (CTC-CRF) alignment, and a multi-lingual phonetic space—the approach requires only a grapheme-to-phoneme lexicon and Montreal Forced Aligner outputs, without any additional manual labeling. Experiments on the Cantonese subset of Common Voice demonstrate consistent gains: Dynamic Dropout alone reduces phoneme error rate (PER) from 17.8% to 16.7% with 50 h of speech and 16.4% to 15.1% with 100 h, while the combination of the two augmentations further lowers PER to 15.9%/14.4%. These results confirm that structure-aware phoneme-level perturbations provide an effective and low-cost solution for building robust Cantonese ASR systems under low-resource conditions.",
      "abstract": "Cantonese automatic speech recognition (ASR) faces persistent challenges due to its nine lexical tones, extensive phonological variation, and the scarcity of professionally transcribed corpora. To address these issues, we propose a lightweight and data-efficient framework that leverages weak phonetic supervision (WPS) in conjunction with two pho-neme-aware augmentation strategies. (1) Dynamic Boundary-Aligned Phoneme Dropout progressively removes entire IPA segments according to a curriculum schedule, simulating real-world phenomena such as elision, lenition, and tonal drift while ensuring training stability. (2) Phoneme-Aware SpecAugment confines all time- and frequency-masking operations within phoneme boundaries and prioritizes high-attention regions, thereby preserving intra-phonemic contours and formant integrity. Built on the Whistle encoder—which integrates a Conformer backbone, Connectionist Temporal Classification–Conditional Random Field (CTC-CRF) alignment, and a multi-lingual phonetic space—the approach requires only a grapheme-to-phoneme lexicon and Montreal Forced Aligner outputs, without any additional manual labeling. Experiments on the Cantonese subset of Common Voice demonstrate consistent gains: Dynamic Dropout alone reduces phoneme error rate (PER) from 17.8% to 16.7% with 50 h of speech and 16.4% to 15.1% with 100 h, while the combination of the two augmentations further lowers PER to 15.9%/14.4%. These results confirm that structure-aware phoneme-level perturbations provide an effective and low-cost solution for building robust Cantonese ASR systems under low-resource conditions.",
      "doi": "https://doi.org/10.3390/sym17091478",
      "openalex_id": "https://openalex.org/W4414069553",
      "arxiv_id": "",
      "publication_date": "2025-09-08",
      "published": "2025-09-08",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item centers on Cantonese ASR with phoneme-aware data augmentation and does not treat speech as the core spoken language model or address full-duplex interaction; it focuses on ASR accuracy (PER/WER) rather than SLM architecture, end-to-end SLM frameworks, or conversational synchronization, thus it fails the inclusion criteria and meets an exclusion criterion.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item centers on Cantonese ASR with phoneme-aware data augmentation and does not treat speech as the core spoken language model or address full-duplex interaction; it focuses on ASR accuracy (PER/WER) rather than SLM architecture, end-to-end SLM frameworks, or conversational synchronization, thus it fails the inclusion criteria and meets an exclusion criterion.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper focuses on Cantonese automatic speech recognition (ASR) under low-resource conditions using phoneme-aware augmentation techniques, which centers on improving ASR performance without addressing spoken language models (SLMs), full-duplex spoken language models (FD-SLMs), or multi-modal speech-language interaction features such as synchronization, turn-taking, or real-time dialogue management; the study does not discuss spoken language model architectures, training, or evaluation beyond ASR error rates, and lacks any mention of the key concepts related to spoken language modeling or full-duplex interaction required for inclusion.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper focuses on Cantonese automatic speech recognition (ASR) under low-resource conditions using phoneme-aware augmentation techniques, which centers on improving ASR performance without addressing spoken language models (SLMs), full-duplex spoken language models (FD-SLMs), or multi-modal speech-language interaction features such as synchronization, turn-taking, or real-time dialogue management; the study does not discuss spoken language model architectures, training, or evaluation beyond ASR error rates, and lacks any mention of the key concepts related to spoken language modeling or full-duplex interaction required for inclusion.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Realistic and broad-scope learning simulations: first results and challenges",
    "abstract": "Abstract There is a current ‘theory crisis’ in language acquisition research, resulting from fragmentation both at the level of the approaches and the linguistic level studied. We identify a need for integrative approaches that go beyond these limitations, and propose to analyse the strengths and weaknesses of current theoretical approaches of language acquisition. In particular, we advocate that language learning simulations, if they integrate realistic input and multiple levels of language, have the potential to contribute significantly to our understanding of language acquisition. We then review recent results obtained through such language learning simulations. Finally, we propose some guidelines for the community to build better simulations.",
    "metadata": {
      "title": "Realistic and broad-scope learning simulations: first results and challenges",
      "summary": "Abstract There is a current ‘theory crisis’ in language acquisition research, resulting from fragmentation both at the level of the approaches and the linguistic level studied. We identify a need for integrative approaches that go beyond these limitations, and propose to analyse the strengths and weaknesses of current theoretical approaches of language acquisition. In particular, we advocate that language learning simulations, if they integrate realistic input and multiple levels of language, have the potential to contribute significantly to our understanding of language acquisition. We then review recent results obtained through such language learning simulations. Finally, we propose some guidelines for the community to build better simulations.",
      "abstract": "Abstract There is a current ‘theory crisis’ in language acquisition research, resulting from fragmentation both at the level of the approaches and the linguistic level studied. We identify a need for integrative approaches that go beyond these limitations, and propose to analyse the strengths and weaknesses of current theoretical approaches of language acquisition. In particular, we advocate that language learning simulations, if they integrate realistic input and multiple levels of language, have the potential to contribute significantly to our understanding of language acquisition. We then review recent results obtained through such language learning simulations. Finally, we propose some guidelines for the community to build better simulations.",
      "doi": "https://doi.org/10.1017/s0305000923000272",
      "openalex_id": "https://openalex.org/W4378619943",
      "arxiv_id": "",
      "publication_date": "2023-05-29",
      "published": "2023-05-29",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The abstract concerns language acquisition simulations and theoretical integration, not focused on Spoken Language Models or real-time speech interaction; it does not meet the core inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The abstract concerns language acquisition simulations and theoretical integration, not focused on Spoken Language Models or real-time speech interaction; it does not meet the core inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper discusses language learning simulations and the theory crisis in language acquisition research but does not explicitly focus on spoken language models or speech as the core modeling target; it also does not mention full-duplex interaction, synchronization, or relevant evaluation criteria aligned with the inclusion criteria. Therefore, it lacks critical elements such as spoken language modeling architecture, training, or evaluation and does not appear to meet the inclusion requirements.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper discusses language learning simulations and the theory crisis in language acquisition research but does not explicitly focus on spoken language models or speech as the core modeling target; it also does not mention full-duplex interaction, synchronization, or relevant evaluation criteria aligned with the inclusion criteria. Therefore, it lacks critical elements such as spoken language modeling architecture, training, or evaluation and does not appear to meet the inclusion requirements.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Self-Supervised Models of Speech Infer Universal Articulatory Kinematics",
    "abstract": "Self-Supervised Learning (SSL) based models of speech have shown remarkable performance on a range of downstream tasks. These state-of-the-art models have remained blackboxes, but many recent studies have begun \"probing\" models like HuBERT, to correlate their internal representations to different aspects of speech. In this paper, we show \"inference of articulatory kinematics\" as fundamental property of SSL models, i.e., the ability of these models to transform acoustics into the causal articulatory dynamics underlying the speech signal. We also show that this abstraction is largely overlapping across the language of the data used to train the model, with preference to the language with similar phonological system. Furthermore, we show that with simple affine transformations, Acoustic-to-Articulatory inversion (AAI) is transferrable across speakers, even across genders, languages, and dialects, showing the generalizability of this property. Together, these results shed new light on the internals of SSL models that are critical to their superior performance, and open up new avenues into language-agnostic universal models for speech engineering, that are interpretable and grounded in speech science.",
    "metadata": {
      "title": "Self-Supervised Models of Speech Infer Universal Articulatory Kinematics",
      "summary": "Self-Supervised Learning (SSL) based models of speech have shown remarkable performance on a range of downstream tasks. These state-of-the-art models have remained blackboxes, but many recent studies have begun \"probing\" models like HuBERT, to correlate their internal representations to different aspects of speech. In this paper, we show \"inference of articulatory kinematics\" as fundamental property of SSL models, i.e., the ability of these models to transform acoustics into the causal articulatory dynamics underlying the speech signal. We also show that this abstraction is largely overlapping across the language of the data used to train the model, with preference to the language with similar phonological system. Furthermore, we show that with simple affine transformations, Acoustic-to-Articulatory inversion (AAI) is transferrable across speakers, even across genders, languages, and dialects, showing the generalizability of this property. Together, these results shed new light on the internals of SSL models that are critical to their superior performance, and open up new avenues into language-agnostic universal models for speech engineering, that are interpretable and grounded in speech science.",
      "abstract": "Self-Supervised Learning (SSL) based models of speech have shown remarkable performance on a range of downstream tasks. These state-of-the-art models have remained blackboxes, but many recent studies have begun \"probing\" models like HuBERT, to correlate their internal representations to different aspects of speech. In this paper, we show \"inference of articulatory kinematics\" as fundamental property of SSL models, i.e., the ability of these models to transform acoustics into the causal articulatory dynamics underlying the speech signal. We also show that this abstraction is largely overlapping across the language of the data used to train the model, with preference to the language with similar phonological system. Furthermore, we show that with simple affine transformations, Acoustic-to-Articulatory inversion (AAI) is transferrable across speakers, even across genders, languages, and dialects, showing the generalizability of this property. Together, these results shed new light on the internals of SSL models that are critical to their superior performance, and open up new avenues into language-agnostic universal models for speech engineering, that are interpretable and grounded in speech science.",
      "doi": "https://doi.org/10.1109/icassp48485.2024.10447345",
      "openalex_id": "https://openalex.org/W4392902939",
      "arxiv_id": "",
      "publication_date": "2024-03-18",
      "published": "2024-03-18",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "題目聚焦於以自监督語音模型研究語音表徵及跨語言普遍性，將語音視為主要研究對象、探討模型架構與評估，符合「Spoken Language Models」核心與近年論文範圍，且非僅限於 ASR/TTS或單一任務，因此應納入。",
      "evaluation": 4
    },
    "round-A_JuniorNano_reasoning": "題目聚焦於以自监督語音模型研究語音表徵及跨語言普遍性，將語音視為主要研究對象、探討模型架構與評估，符合「Spoken Language Models」核心與近年論文範圍，且非僅限於 ASR/TTS或單一任務，因此應納入。",
    "round-A_JuniorNano_evaluation": 4,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on self-supervised speech models and their internal representations related to articulatory dynamics, but it does not explicitly address spoken language models (SLMs) or full-duplex spoken language models, nor does it discuss end-to-end or hybrid architectures for training, inference, or evaluation with respect to interaction behavior such as turn-taking, overlap, or low-latency synchronization relevant to the inclusion criteria. Additionally, the terms \"spoken language model(s)\" or equivalent phrases are not present in the title or abstract. Therefore, it fails to fully meet the core inclusion requirements.",
      "evaluation": 2
    },
    "round-A_JuniorMini_reasoning": "The study focuses on self-supervised speech models and their internal representations related to articulatory dynamics, but it does not explicitly address spoken language models (SLMs) or full-duplex spoken language models, nor does it discuss end-to-end or hybrid architectures for training, inference, or evaluation with respect to interaction behavior such as turn-taking, overlap, or low-latency synchronization relevant to the inclusion criteria. Additionally, the terms \"spoken language model(s)\" or equivalent phrases are not present in the title or abstract. Therefore, it fails to fully meet the core inclusion requirements.",
    "round-A_JuniorMini_evaluation": 2,
    "round-B_SeniorLead_output": {
      "reasoning": "Junior reviewers disagreed (one recommended inclusion, one exclusion). The abstract studies self-supervised speech representations and articulatory inversion but does not explicitly address Spoken Language Models, speech-level language modeling, or full-duplex/interactive SLM behaviors required by the inclusion criteria. Score explanation: 2 — better to exclude because it does not meet the core requirement of explicitly focusing on SLMs or speech language modeling/interaction.",
      "evaluation": 2
    },
    "round-B_SeniorLead_reasoning": "Junior reviewers disagreed (one recommended inclusion, one exclusion). The abstract studies self-supervised speech representations and articulatory inversion but does not explicitly address Spoken Language Models, speech-level language modeling, or full-duplex/interactive SLM behaviors required by the inclusion criteria. Score explanation: 2 — better to exclude because it does not meet the core requirement of explicitly focusing on SLMs or speech language modeling/interaction.",
    "round-B_SeniorLead_evaluation": 2,
    "final_verdict": "exclude (senior:2)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Towards Matching Phones and Speech Representations",
    "abstract": "Learning phone types from phone instances has been a long-standing problem, while still being open. In this work, we revisit this problem in the context of self-supervised learning, and pose it as the problem of matching cluster centroids to phone embeddings. We study two key properties that enable matching, namely, whether cluster centroids of self-supervised representations reduce the variability of phone instances and respect the relationship among phones. We then use the matching result to produce pseudo-labels and introduce a new loss function for improving self-supervised representations. Our experiments show that the matching result captures the relationship among phones. Training the new loss function jointly with the regular self-supervised losses, such as APC and CPC, significantly improves the downstream phone classification.",
    "metadata": {
      "title": "Towards Matching Phones and Speech Representations",
      "summary": "Learning phone types from phone instances has been a long-standing problem, while still being open. In this work, we revisit this problem in the context of self-supervised learning, and pose it as the problem of matching cluster centroids to phone embeddings. We study two key properties that enable matching, namely, whether cluster centroids of self-supervised representations reduce the variability of phone instances and respect the relationship among phones. We then use the matching result to produce pseudo-labels and introduce a new loss function for improving self-supervised representations. Our experiments show that the matching result captures the relationship among phones. Training the new loss function jointly with the regular self-supervised losses, such as APC and CPC, significantly improves the downstream phone classification.",
      "abstract": "Learning phone types from phone instances has been a long-standing problem, while still being open. In this work, we revisit this problem in the context of self-supervised learning, and pose it as the problem of matching cluster centroids to phone embeddings. We study two key properties that enable matching, namely, whether cluster centroids of self-supervised representations reduce the variability of phone instances and respect the relationship among phones. We then use the matching result to produce pseudo-labels and introduce a new loss function for improving self-supervised representations. Our experiments show that the matching result captures the relationship among phones. Training the new loss function jointly with the regular self-supervised losses, such as APC and CPC, significantly improves the downstream phone classification.",
      "doi": "https://doi.org/10.1109/asru57964.2023.10389757",
      "openalex_id": "https://openalex.org/W4391021372",
      "arxiv_id": "",
      "publication_date": "2023-12-16",
      "published": "2023-12-16",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The paper focuses on self-supervised learning of phone-level representations and pseudo-labeling for phoneme embeddings, not on Spoken Language Models (SLMs) or full-duplex interactive speech systems. It lacks explicit references to SLM/FD-SLM concepts in the title/abstract/keywords and does not address turn-taking, real-time interaction, or speech-language modeling as a core objective, thus failing to meet the inclusion criteria requiring core emphasis on SLMs and evaluated in interactive/speech-language contexts.",
      "evaluation": 2
    },
    "round-A_JuniorNano_reasoning": "The paper focuses on self-supervised learning of phone-level representations and pseudo-labeling for phoneme embeddings, not on Spoken Language Models (SLMs) or full-duplex interactive speech systems. It lacks explicit references to SLM/FD-SLM concepts in the title/abstract/keywords and does not address turn-taking, real-time interaction, or speech-language modeling as a core objective, thus failing to meet the inclusion criteria requiring core emphasis on SLMs and evaluated in interactive/speech-language contexts.",
    "round-A_JuniorNano_evaluation": 2,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper focuses on learning phone types using self-supervised learning and clustering of speech representations, which relates to acoustic modeling and phone classification; however, it does not explicitly discuss Spoken Language Models (SLMs) or Full-Duplex Spoken Language Models (FD-SLMs), nor does it address end-to-end or hybrid speech-LLM architectures, real-time interactive capabilities, synchronization strategies, or multi-dimensional evaluation aligned with the inclusion criteria. The scope pertains more to phone-level representation learning rather than broader spoken language modeling or synchronized interactive speech systems as required.",
      "evaluation": 2
    },
    "round-A_JuniorMini_reasoning": "The paper focuses on learning phone types using self-supervised learning and clustering of speech representations, which relates to acoustic modeling and phone classification; however, it does not explicitly discuss Spoken Language Models (SLMs) or Full-Duplex Spoken Language Models (FD-SLMs), nor does it address end-to-end or hybrid speech-LLM architectures, real-time interactive capabilities, synchronization strategies, or multi-dimensional evaluation aligned with the inclusion criteria. The scope pertains more to phone-level representation learning rather than broader spoken language modeling or synchronized interactive speech systems as required.",
    "round-A_JuniorMini_evaluation": 2,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:2)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Artificial neural networks to analyze and simulate language acquisition in children",
    "abstract": "Lightweight child-worn recorders that collect audio across an entire day allow for a big-data approach to the study of language development. By collecting the child's production and linguistic environment, these recordings offer us a uniquely naturalistic view of everyday language uses. However, such recordings quickly accumulate thousands of hours of audio and require the use of automatic speech processing algorithms. Besides providing ecologically-valid measures of what children hear and say, these recordings can fuel computational models of early language acquisition with what infants truly hear. This opens up new opportunities for running realistic language learning simulations.A first aspect of my doctoral work is dedicated to developing automatic speech processing algorithms for child-centered long-form recordings. In this manuscript, I first show that current state-of-the-art automatic speech recognition systems fail to capture the complexity of naturalistic speech as found in long-forms. I then present our attempt to propose a free, open-source, and more accurate alternative to the LENA proprietary software, which is currently the standard tool for obtaining automatic analyses of long-forms. Using supervised learning methods, my collaborators and I built a suite of speech processing tools to detect voice activity, identify voice signal sources (child vocalizations, female or male speech), count the number of linguistic units (phonemes, syllables, or words), and estimate the quantity of background noise and reverberation. A second aspect of my doctoral work is dedicated to computational models of early language acquisition. I present a first modeling study showing that self-supervised learning algorithms trained on audiobooks can learn phonetic and lexical aspects of their training language. I then show that the same algorithm trained on ecological long-forms needs inductive biases to learn phonetic aspects of its training language reliably and reflect on whether similar inductive biases may guide language learning in infants. Interestingly, there is no evidence for lexical learning on long-forms, contrary to what has been shown in the literature on more curated data. This series of studies illustrates the importance of considering ecologically-valid input data when modeling language acquisition.",
    "metadata": {
      "title": "Artificial neural networks to analyze and simulate language acquisition in children",
      "summary": "Lightweight child-worn recorders that collect audio across an entire day allow for a big-data approach to the study of language development. By collecting the child's production and linguistic environment, these recordings offer us a uniquely naturalistic view of everyday language uses. However, such recordings quickly accumulate thousands of hours of audio and require the use of automatic speech processing algorithms. Besides providing ecologically-valid measures of what children hear and say, these recordings can fuel computational models of early language acquisition with what infants truly hear. This opens up new opportunities for running realistic language learning simulations.A first aspect of my doctoral work is dedicated to developing automatic speech processing algorithms for child-centered long-form recordings. In this manuscript, I first show that current state-of-the-art automatic speech recognition systems fail to capture the complexity of naturalistic speech as found in long-forms. I then present our attempt to propose a free, open-source, and more accurate alternative to the LENA proprietary software, which is currently the standard tool for obtaining automatic analyses of long-forms. Using supervised learning methods, my collaborators and I built a suite of speech processing tools to detect voice activity, identify voice signal sources (child vocalizations, female or male speech), count the number of linguistic units (phonemes, syllables, or words), and estimate the quantity of background noise and reverberation. A second aspect of my doctoral work is dedicated to computational models of early language acquisition. I present a first modeling study showing that self-supervised learning algorithms trained on audiobooks can learn phonetic and lexical aspects of their training language. I then show that the same algorithm trained on ecological long-forms needs inductive biases to learn phonetic aspects of its training language reliably and reflect on whether similar inductive biases may guide language learning in infants. Interestingly, there is no evidence for lexical learning on long-forms, contrary to what has been shown in the literature on more curated data. This series of studies illustrates the importance of considering ecologically-valid input data when modeling language acquisition.",
      "abstract": "Lightweight child-worn recorders that collect audio across an entire day allow for a big-data approach to the study of language development. By collecting the child's production and linguistic environment, these recordings offer us a uniquely naturalistic view of everyday language uses. However, such recordings quickly accumulate thousands of hours of audio and require the use of automatic speech processing algorithms. Besides providing ecologically-valid measures of what children hear and say, these recordings can fuel computational models of early language acquisition with what infants truly hear. This opens up new opportunities for running realistic language learning simulations.A first aspect of my doctoral work is dedicated to developing automatic speech processing algorithms for child-centered long-form recordings. In this manuscript, I first show that current state-of-the-art automatic speech recognition systems fail to capture the complexity of naturalistic speech as found in long-forms. I then present our attempt to propose a free, open-source, and more accurate alternative to the LENA proprietary software, which is currently the standard tool for obtaining automatic analyses of long-forms. Using supervised learning methods, my collaborators and I built a suite of speech processing tools to detect voice activity, identify voice signal sources (child vocalizations, female or male speech), count the number of linguistic units (phonemes, syllables, or words), and estimate the quantity of background noise and reverberation. A second aspect of my doctoral work is dedicated to computational models of early language acquisition. I present a first modeling study showing that self-supervised learning algorithms trained on audiobooks can learn phonetic and lexical aspects of their training language. I then show that the same algorithm trained on ecological long-forms needs inductive biases to learn phonetic aspects of its training language reliably and reflect on whether similar inductive biases may guide language learning in infants. Interestingly, there is no evidence for lexical learning on long-forms, contrary to what has been shown in the literature on more curated data. This series of studies illustrates the importance of considering ecologically-valid input data when modeling language acquisition.",
      "doi": "https://doi.org/10.31234/osf.io/5p8ge",
      "openalex_id": "https://openalex.org/W4387877468",
      "arxiv_id": "",
      "publication_date": "2023-10-22",
      "published": "2023-10-22",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item does not center on Spoken Language Models or equivalent speech-language modeling objects, and the abstract discusses ASR tools and general computational models of language acquisition rather than SLM/FD-SLM architectures, training, or evaluation as required by the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item does not center on Spoken Language Models or equivalent speech-language modeling objects, and the abstract discusses ASR tools and general computational models of language acquisition rather than SLM/FD-SLM architectures, training, or evaluation as required by the inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on analyzing naturalistic child language acquisition using automatic speech processing algorithms and computational simulations but does not explicitly address Spoken Language Models or Full-Duplex Spoken Language Models, nor does it discuss model architectures, training, or evaluation related to speech language modeling as defined. It primarily centers on speech recognition and computational modeling without engaging with real-time interactive speech-language modeling, synchronization, or multi-dimensional evaluation criteria outlined.",
      "evaluation": 2
    },
    "round-A_JuniorMini_reasoning": "The study focuses on analyzing naturalistic child language acquisition using automatic speech processing algorithms and computational simulations but does not explicitly address Spoken Language Models or Full-Duplex Spoken Language Models, nor does it discuss model architectures, training, or evaluation related to speech language modeling as defined. It primarily centers on speech recognition and computational modeling without engaging with real-time interactive speech-language modeling, synchronization, or multi-dimensional evaluation criteria outlined.",
    "round-A_JuniorMini_evaluation": 2,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:2)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "SpeechGPT: Empowering Large Language Models with Intrinsic Cross-Modal Conversational Abilities",
    "abstract": "Multi-modal large language models are regarded as a crucial step towards Artificial General Intelligence (AGI) and have garnered significant interest with the emergence of ChatGPT. However, current speech-language models typically adopt the cascade paradigm, preventing inter-modal knowledge transfer. In this paper, we propose SpeechGPT, a large language model with intrinsic cross-modal conversational abilities, capable of perceiving and generating multi-modal content. With discrete speech representations, we construct SpeechInstruct, the first large-scale cross-modal speech instruction dataset. Additionally, we employ a three-stage training strategy that includes modality-adaptation pre-training, cross-modal instruction fine-tuning, and chain-of-modality instruction fine-tuning. The experimental results demonstrate that SpeechGPT has an impressive capacity to follow cross-modal human instructions and highlight the potential of handling multiple modalities with one model. Code and models are available in https://github.com/0nutation/SpeechGPT. Demos are shown in https://0nutation.github.io/SpeechGPT.github.io/.",
    "metadata": {
      "title": "SpeechGPT: Empowering Large Language Models with Intrinsic Cross-Modal Conversational Abilities",
      "summary": "Multi-modal large language models are regarded as a crucial step towards Artificial General Intelligence (AGI) and have garnered significant interest with the emergence of ChatGPT. However, current speech-language models typically adopt the cascade paradigm, preventing inter-modal knowledge transfer. In this paper, we propose SpeechGPT, a large language model with intrinsic cross-modal conversational abilities, capable of perceiving and generating multi-modal content. With discrete speech representations, we construct SpeechInstruct, the first large-scale cross-modal speech instruction dataset. Additionally, we employ a three-stage training strategy that includes modality-adaptation pre-training, cross-modal instruction fine-tuning, and chain-of-modality instruction fine-tuning. The experimental results demonstrate that SpeechGPT has an impressive capacity to follow cross-modal human instructions and highlight the potential of handling multiple modalities with one model. Code and models are available in https://github.com/0nutation/SpeechGPT. Demos are shown in https://0nutation.github.io/SpeechGPT.github.io/.",
      "abstract": "Multi-modal large language models are regarded as a crucial step towards Artificial General Intelligence (AGI) and have garnered significant interest with the emergence of ChatGPT. However, current speech-language models typically adopt the cascade paradigm, preventing inter-modal knowledge transfer. In this paper, we propose SpeechGPT, a large language model with intrinsic cross-modal conversational abilities, capable of perceiving and generating multi-modal content. With discrete speech representations, we construct SpeechInstruct, the first large-scale cross-modal speech instruction dataset. Additionally, we employ a three-stage training strategy that includes modality-adaptation pre-training, cross-modal instruction fine-tuning, and chain-of-modality instruction fine-tuning. The experimental results demonstrate that SpeechGPT has an impressive capacity to follow cross-modal human instructions and highlight the potential of handling multiple modalities with one model. Code and models are available in https://github.com/0nutation/SpeechGPT. Demos are shown in https://0nutation.github.io/SpeechGPT.github.io/.",
      "doi": "https://doi.org/10.18653/v1/2023.findings-emnlp.1055",
      "openalex_id": "https://openalex.org/W4389524500",
      "arxiv_id": "",
      "publication_date": "2023-01-01",
      "published": "2023-01-01",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "本論文提出 SpeechGPT，作為具備內在跨模態對話能力的語言模型，能感知與生成多模態內容，且構建了跨模態語音指令資料集，採用模態適配預訓練與跨模態/連鎖式指令微調等訓練路徑，明顯以語音為建模與推理的核心之一，符合「Spoken Language Models / Speech Language Models」概念與端到端或混合式語音–文字架構的訓練與評估要求；且在過去3年內公開，且摘要/標題中出現語音相關概念。雖未在摘要中明確使用「speech language model」字樣，但核心聚焦語音與多模態整合，非僅限單一語音技術或文字先行的處理，因此值得納入。",
      "evaluation": 4
    },
    "round-A_JuniorNano_reasoning": "本論文提出 SpeechGPT，作為具備內在跨模態對話能力的語言模型，能感知與生成多模態內容，且構建了跨模態語音指令資料集，採用模態適配預訓練與跨模態/連鎖式指令微調等訓練路徑，明顯以語音為建模與推理的核心之一，符合「Spoken Language Models / Speech Language Models」概念與端到端或混合式語音–文字架構的訓練與評估要求；且在過去3年內公開，且摘要/標題中出現語音相關概念。雖未在摘要中明確使用「speech language model」字樣，但核心聚焦語音與多模態整合，非僅限單一語音技術或文字先行的處理，因此值得納入。",
    "round-A_JuniorNano_evaluation": 4,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on SpeechGPT, a large language model with intrinsic cross-modal conversational abilities including speech, indicating a focus on speech language modeling. It involves constructing a speech instruction dataset and stages of training related to speech and multi-modal content generation. However, the abstract does not explicitly mention modeling turn-taking, full-duplex spoken language interaction, cognitive parallelism, overlap handling, or low-latency interactions, nor does it detail explicit evaluation on such behaviors or synchronization aspects required by the inclusion criteria. The research appears recent and involves speech as a core modeling aspect rather than only as input. Nonetheless, the lack of explicit reference to full-duplex or synchronous dialogue interaction or detailed behavioral arbitration evaluation makes it unclear if it fully meets the detailed topic scope.",
      "evaluation": 3
    },
    "round-A_JuniorMini_reasoning": "The study focuses on SpeechGPT, a large language model with intrinsic cross-modal conversational abilities including speech, indicating a focus on speech language modeling. It involves constructing a speech instruction dataset and stages of training related to speech and multi-modal content generation. However, the abstract does not explicitly mention modeling turn-taking, full-duplex spoken language interaction, cognitive parallelism, overlap handling, or low-latency interactions, nor does it detail explicit evaluation on such behaviors or synchronization aspects required by the inclusion criteria. The research appears recent and involves speech as a core modeling aspect rather than only as input. Nonetheless, the lack of explicit reference to full-duplex or synchronous dialogue interaction or detailed behavioral arbitration evaluation makes it unclear if it fully meets the detailed topic scope.",
    "round-A_JuniorMini_evaluation": 3,
    "round-B_SeniorLead_output": {
      "reasoning": "Junior reviewers: one recommended inclusion (score 4) noting speech-centered multi-modal modeling and a new speech instruction dataset; the other was uncertain (score 3) because the abstract lacks explicit full‑duplex/synchronous interaction evaluation. I rate this a 4 — the paper explicitly centers speech-language modeling (mentions “speech-language models”, uses discrete speech representations, introduces SpeechInstruct, and proposes end-to-end cross-modal training), satisfying the inclusion criteria and not meeting any exclusion criteria, but it does not demonstrate explicit full‑duplex / overlap / low-latency interaction evaluations so I recommend inclusion with moderate confidence.",
      "evaluation": 4
    },
    "round-B_SeniorLead_reasoning": "Junior reviewers: one recommended inclusion (score 4) noting speech-centered multi-modal modeling and a new speech instruction dataset; the other was uncertain (score 3) because the abstract lacks explicit full‑duplex/synchronous interaction evaluation. I rate this a 4 — the paper explicitly centers speech-language modeling (mentions “speech-language models”, uses discrete speech representations, introduces SpeechInstruct, and proposes end-to-end cross-modal training), satisfying the inclusion criteria and not meeting any exclusion criteria, but it does not demonstrate explicit full‑duplex / overlap / low-latency interaction evaluations so I recommend inclusion with moderate confidence.",
    "round-B_SeniorLead_evaluation": 4,
    "final_verdict": "include (senior:4)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "On Decoder-Only Architecture For Speech-to-Text and Large Language Model Integration",
    "abstract": "Large language models (LLMs) have achieved remarkable success in the field of natural language processing, enabling better human-computer interaction using natural language. However, the seamless integration of speech signals into LLMs has not been explored well. The \"decoder-only\" architecture has also not been well studied for speech processing tasks. In this research, we introduce Speech-LLaMA, a novel approach that effectively incorporates acoustic information into text-based large language models. Our method leverages Connectionist Temporal Classification and a simple audio encoder to map the compressed acoustic features to the continuous semantic space of the LLM. In addition, we further probe the decoder-only architecture for speech-to-text tasks by training a smaller scale randomly initialized speech-LLaMA model from speech-text paired data alone. We conduct experiments on multilingual speech-to-text translation tasks and demonstrate a significant improvement over strong baselines, highlighting the potential advantages of decoder-only models for speech-to-text conversion.",
    "metadata": {
      "title": "On Decoder-Only Architecture For Speech-to-Text and Large Language Model Integration",
      "summary": "Large language models (LLMs) have achieved remarkable success in the field of natural language processing, enabling better human-computer interaction using natural language. However, the seamless integration of speech signals into LLMs has not been explored well. The \"decoder-only\" architecture has also not been well studied for speech processing tasks. In this research, we introduce Speech-LLaMA, a novel approach that effectively incorporates acoustic information into text-based large language models. Our method leverages Connectionist Temporal Classification and a simple audio encoder to map the compressed acoustic features to the continuous semantic space of the LLM. In addition, we further probe the decoder-only architecture for speech-to-text tasks by training a smaller scale randomly initialized speech-LLaMA model from speech-text paired data alone. We conduct experiments on multilingual speech-to-text translation tasks and demonstrate a significant improvement over strong baselines, highlighting the potential advantages of decoder-only models for speech-to-text conversion.",
      "abstract": "Large language models (LLMs) have achieved remarkable success in the field of natural language processing, enabling better human-computer interaction using natural language. However, the seamless integration of speech signals into LLMs has not been explored well. The \"decoder-only\" architecture has also not been well studied for speech processing tasks. In this research, we introduce Speech-LLaMA, a novel approach that effectively incorporates acoustic information into text-based large language models. Our method leverages Connectionist Temporal Classification and a simple audio encoder to map the compressed acoustic features to the continuous semantic space of the LLM. In addition, we further probe the decoder-only architecture for speech-to-text tasks by training a smaller scale randomly initialized speech-LLaMA model from speech-text paired data alone. We conduct experiments on multilingual speech-to-text translation tasks and demonstrate a significant improvement over strong baselines, highlighting the potential advantages of decoder-only models for speech-to-text conversion.",
      "doi": "https://doi.org/10.1109/asru57964.2023.10389705",
      "openalex_id": "https://openalex.org/W4391021666",
      "arxiv_id": "",
      "publication_date": "2023-12-16",
      "published": "2023-12-16",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "雖然摘要描述了把聲學資訊整合入大語言模型以實現語音到文字的任務，但並未在標題或摘要中明確出現與主題相關的關鍵詞「Spoken Language Models / Speech Language Models / language modeling of speech」，因此不符合該語言模型主題的明確包含標準。",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "雖然摘要描述了把聲學資訊整合入大語言模型以實現語音到文字的任務，但並未在標題或摘要中明確出現與主題相關的關鍵詞「Spoken Language Models / Speech Language Models / language modeling of speech」，因此不符合該語言模型主題的明確包含標準。",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study introduces Speech-LLaMA, a decoder-only architecture that incorporates acoustic features into large language models for speech-to-text tasks, focusing on model architecture and training with paired speech-text data; however, the abstract does not address full-duplex spoken language interaction, cognitive parallelism, real-time interruption handling, or low-latency synchronous interaction, which are key inclusion aspects, nor does it mention evaluation beyond improved speech-to-text baselines. It also treats speech primarily as input to LLMs rather than emphasizing speech as the core modeling and reasoning object, lacking explicit mention of 'spoken language models' or equivalent in title or abstract, and does not describe multi-dimensional evaluation metrics relevant to dialogue or interaction dynamics.",
      "evaluation": 2
    },
    "round-A_JuniorMini_reasoning": "The study introduces Speech-LLaMA, a decoder-only architecture that incorporates acoustic features into large language models for speech-to-text tasks, focusing on model architecture and training with paired speech-text data; however, the abstract does not address full-duplex spoken language interaction, cognitive parallelism, real-time interruption handling, or low-latency synchronous interaction, which are key inclusion aspects, nor does it mention evaluation beyond improved speech-to-text baselines. It also treats speech primarily as input to LLMs rather than emphasizing speech as the core modeling and reasoning object, lacking explicit mention of 'spoken language models' or equivalent in title or abstract, and does not describe multi-dimensional evaluation metrics relevant to dialogue or interaction dynamics.",
    "round-A_JuniorMini_evaluation": 2,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:2)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "ZMM-TTS: Zero-Shot Multilingual and Multispeaker Speech Synthesis Conditioned on Self-Supervised Discrete Speech Representations",
    "abstract": "Neural text-to-speech (TTS) has achieved human-like synthetic speech for single-speaker, single-language synthesis. Multilingual TTS systems are limited to resource-rich languages due to the lack of large paired text and studio-quality audio data. TTS systems are typically built using a single speaker’s voice, but there is growing interest in developing systems that can synthesize voices for new speakers using only a few seconds of their speech. This paper presents ZMM-TTS, a multilingual and multispeaker framework utilizing quantized latent speech representations from a large-scale, pre-trained, self-supervised model. Our paper combines text-based and speech-based self-supervised learning models for multilingual speech synthesis. Our proposed model has zero-shot generalization ability not only for unseen speakers but also for unseen languages. We have conducted comprehensive subjective and objective evaluations through a series of experiments. Our model has proven effective in terms of speech naturalness and similarity for both seen and unseen speakers in six high-resource languages. We also tested the efficiency of our method on two hypothetically low-resource languages. The results are promising, indicating that our proposed approach can synthesize audio that is intelligible and has a high degree of similarity to the target speaker’s voice, even without any training data for the new, unseen language.",
    "metadata": {
      "title": "ZMM-TTS: Zero-Shot Multilingual and Multispeaker Speech Synthesis Conditioned on Self-Supervised Discrete Speech Representations",
      "summary": "Neural text-to-speech (TTS) has achieved human-like synthetic speech for single-speaker, single-language synthesis. Multilingual TTS systems are limited to resource-rich languages due to the lack of large paired text and studio-quality audio data. TTS systems are typically built using a single speaker’s voice, but there is growing interest in developing systems that can synthesize voices for new speakers using only a few seconds of their speech. This paper presents ZMM-TTS, a multilingual and multispeaker framework utilizing quantized latent speech representations from a large-scale, pre-trained, self-supervised model. Our paper combines text-based and speech-based self-supervised learning models for multilingual speech synthesis. Our proposed model has zero-shot generalization ability not only for unseen speakers but also for unseen languages. We have conducted comprehensive subjective and objective evaluations through a series of experiments. Our model has proven effective in terms of speech naturalness and similarity for both seen and unseen speakers in six high-resource languages. We also tested the efficiency of our method on two hypothetically low-resource languages. The results are promising, indicating that our proposed approach can synthesize audio that is intelligible and has a high degree of similarity to the target speaker’s voice, even without any training data for the new, unseen language.",
      "abstract": "Neural text-to-speech (TTS) has achieved human-like synthetic speech for single-speaker, single-language synthesis. Multilingual TTS systems are limited to resource-rich languages due to the lack of large paired text and studio-quality audio data. TTS systems are typically built using a single speaker’s voice, but there is growing interest in developing systems that can synthesize voices for new speakers using only a few seconds of their speech. This paper presents ZMM-TTS, a multilingual and multispeaker framework utilizing quantized latent speech representations from a large-scale, pre-trained, self-supervised model. Our paper combines text-based and speech-based self-supervised learning models for multilingual speech synthesis. Our proposed model has zero-shot generalization ability not only for unseen speakers but also for unseen languages. We have conducted comprehensive subjective and objective evaluations through a series of experiments. Our model has proven effective in terms of speech naturalness and similarity for both seen and unseen speakers in six high-resource languages. We also tested the efficiency of our method on two hypothetically low-resource languages. The results are promising, indicating that our proposed approach can synthesize audio that is intelligible and has a high degree of similarity to the target speaker’s voice, even without any training data for the new, unseen language.",
      "doi": "https://doi.org/10.1109/taslp.2024.3451951",
      "openalex_id": "https://openalex.org/W4402301063",
      "arxiv_id": "",
      "publication_date": "2024-01-01",
      "published": "2024-01-01",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The abstract targets zero-shot multilingual and multispeaker TTS using self-supervised speech representations, which is a single-task speech synthesis paper and does not explicitly frame or analyze Spoken Language Models or Speech Language Models, nor discuss interactive dialogue, synchronization, or speech-language modeling concepts required by the inclusion criteria. It also focuses on TTS performance across languages rather than broader SLM/FD-SLM paradigms or conversational cognitive interactions.",
      "evaluation": 2
    },
    "round-A_JuniorNano_reasoning": "The abstract targets zero-shot multilingual and multispeaker TTS using self-supervised speech representations, which is a single-task speech synthesis paper and does not explicitly frame or analyze Spoken Language Models or Speech Language Models, nor discuss interactive dialogue, synchronization, or speech-language modeling concepts required by the inclusion criteria. It also focuses on TTS performance across languages rather than broader SLM/FD-SLM paradigms or conversational cognitive interactions.",
    "round-A_JuniorNano_evaluation": 2,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses primarily on zero-shot multilingual and multispeaker TTS using self-supervised discrete speech representations, which is mainly about speech synthesis technology rather than spoken language modeling or full-duplex spoken language models. It does not explicitly discuss spoken language model architectures, training, or evaluation related to real-time synchronous interaction, cognitive parallelism, low latency, or multi-party behavioral arbitration relevant to the inclusion criteria. The evaluation centers on speech naturalness and similarity rather than interaction or behavior arbitration metrics. Thus, it does not meet the core thematic or methodological inclusion criteria and falls under exclusion for focusing on single-task TTS without dialogue or interaction modeling.",
      "evaluation": 2
    },
    "round-A_JuniorMini_reasoning": "The study focuses primarily on zero-shot multilingual and multispeaker TTS using self-supervised discrete speech representations, which is mainly about speech synthesis technology rather than spoken language modeling or full-duplex spoken language models. It does not explicitly discuss spoken language model architectures, training, or evaluation related to real-time synchronous interaction, cognitive parallelism, low latency, or multi-party behavioral arbitration relevant to the inclusion criteria. The evaluation centers on speech naturalness and similarity rather than interaction or behavior arbitration metrics. Thus, it does not meet the core thematic or methodological inclusion criteria and falls under exclusion for focusing on single-task TTS without dialogue or interaction modeling.",
    "round-A_JuniorMini_evaluation": 2,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:2)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Joint speech and text machine translation for up to 100 languages",
    "abstract": "Creating the Babel Fish, a tool that helps individuals translate speech between any two languages, requires advanced technological innovation and linguistic expertise. Although conventional speech-to-speech translation systems composed of multiple subsystems performing translation in a cascaded fashion exist<sup>1-3</sup>, scalable and high-performing unified systems<sup>4,5</sup> remain underexplored. To address this gap, here we introduce SEAMLESSM4T-Massively Multilingual and Multimodal Machine Translation-a single model that supports speech-to-speech translation (101 to 36 languages), speech-to-text translation (from 101 to 96 languages), text-to-speech translation (from 96 to 36 languages), text-to-text translation (96 languages) and automatic speech recognition (96 languages). Built using a new multimodal corpus of automatically aligned speech translations and other publicly available data, SEAMLESSM4T is one of the first multilingual systems that can translate from and into English for both speech and text. Moreover, it outperforms the existing state-of-the-art cascaded systems, achieving up to 8% and 23% higher BLEU (Bilingual Evaluation Understudy) scores in speech-to-text and speech-to-speech tasks, respectively. Beyond quality, when tested for robustness, our system is, on average, approximately 50% more resilient against background noise and speaker variations in speech-to-text tasks than the previous state-of-the-art systems. We evaluated SEAMLESSM4T on added toxicity and gender bias to assess translation safety. For the former, we included two strategies for added toxicity mitigation working at either training or inference time. Finally, all contributions in this work are publicly available for non-commercial use to propel further research on inclusive speech translation technologies.",
    "metadata": {
      "title": "Joint speech and text machine translation for up to 100 languages",
      "summary": "Creating the Babel Fish, a tool that helps individuals translate speech between any two languages, requires advanced technological innovation and linguistic expertise. Although conventional speech-to-speech translation systems composed of multiple subsystems performing translation in a cascaded fashion exist<sup>1-3</sup>, scalable and high-performing unified systems<sup>4,5</sup> remain underexplored. To address this gap, here we introduce SEAMLESSM4T-Massively Multilingual and Multimodal Machine Translation-a single model that supports speech-to-speech translation (101 to 36 languages), speech-to-text translation (from 101 to 96 languages), text-to-speech translation (from 96 to 36 languages), text-to-text translation (96 languages) and automatic speech recognition (96 languages). Built using a new multimodal corpus of automatically aligned speech translations and other publicly available data, SEAMLESSM4T is one of the first multilingual systems that can translate from and into English for both speech and text. Moreover, it outperforms the existing state-of-the-art cascaded systems, achieving up to 8% and 23% higher BLEU (Bilingual Evaluation Understudy) scores in speech-to-text and speech-to-speech tasks, respectively. Beyond quality, when tested for robustness, our system is, on average, approximately 50% more resilient against background noise and speaker variations in speech-to-text tasks than the previous state-of-the-art systems. We evaluated SEAMLESSM4T on added toxicity and gender bias to assess translation safety. For the former, we included two strategies for added toxicity mitigation working at either training or inference time. Finally, all contributions in this work are publicly available for non-commercial use to propel further research on inclusive speech translation technologies.",
      "abstract": "Creating the Babel Fish, a tool that helps individuals translate speech between any two languages, requires advanced technological innovation and linguistic expertise. Although conventional speech-to-speech translation systems composed of multiple subsystems performing translation in a cascaded fashion exist<sup>1-3</sup>, scalable and high-performing unified systems<sup>4,5</sup> remain underexplored. To address this gap, here we introduce SEAMLESSM4T-Massively Multilingual and Multimodal Machine Translation-a single model that supports speech-to-speech translation (101 to 36 languages), speech-to-text translation (from 101 to 96 languages), text-to-speech translation (from 96 to 36 languages), text-to-text translation (96 languages) and automatic speech recognition (96 languages). Built using a new multimodal corpus of automatically aligned speech translations and other publicly available data, SEAMLESSM4T is one of the first multilingual systems that can translate from and into English for both speech and text. Moreover, it outperforms the existing state-of-the-art cascaded systems, achieving up to 8% and 23% higher BLEU (Bilingual Evaluation Understudy) scores in speech-to-text and speech-to-speech tasks, respectively. Beyond quality, when tested for robustness, our system is, on average, approximately 50% more resilient against background noise and speaker variations in speech-to-text tasks than the previous state-of-the-art systems. We evaluated SEAMLESSM4T on added toxicity and gender bias to assess translation safety. For the former, we included two strategies for added toxicity mitigation working at either training or inference time. Finally, all contributions in this work are publicly available for non-commercial use to propel further research on inclusive speech translation technologies.",
      "doi": "https://doi.org/10.1038/s41586-024-08359-z",
      "openalex_id": "https://openalex.org/W4406385636",
      "arxiv_id": "",
      "publication_date": "2025-01-15",
      "published": "2025-01-15",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The paper presents a unified, multilingual system SEAMLESSM4T that handles speech-to-speech, speech-to-text, text-to-speech, and text-to-text translations across many languages, treating speech as a core input/output in an end-to-end/multimodal framework with architecture and evaluation details; it is within the last 3 years and evaluates translation quality and robustness, but it does not explicitly use the term ‘spoken language model’ or an equivalent label in the abstract, though the core focus aligns with the spoken-language modeling paradigm.",
      "evaluation": 4
    },
    "round-A_JuniorNano_reasoning": "The paper presents a unified, multilingual system SEAMLESSM4T that handles speech-to-speech, speech-to-text, text-to-speech, and text-to-text translations across many languages, treating speech as a core input/output in an end-to-end/multimodal framework with architecture and evaluation details; it is within the last 3 years and evaluates translation quality and robustness, but it does not explicitly use the term ‘spoken language model’ or an equivalent label in the abstract, though the core focus aligns with the spoken-language modeling paradigm.",
    "round-A_JuniorNano_evaluation": 4,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on a multilingual speech-to-speech and speech-to-text translation system rather than on Spoken Language Models (SLMs) or Full-Duplex Spoken Language Models (FD-SLMs) as core to modeling and reasoning; it does not address synchronous interaction behaviors, cognitive parallelism, or low-latency turn-taking, nor does it explicitly discuss language modeling of speech or architecture/evaluation in the context required by inclusion criteria. Hence, it primarily deals with speech translation, not spoken language modeling or interactive speech systems.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on a multilingual speech-to-speech and speech-to-text translation system rather than on Spoken Language Models (SLMs) or Full-Duplex Spoken Language Models (FD-SLMs) as core to modeling and reasoning; it does not address synchronous interaction behaviors, cognitive parallelism, or low-latency turn-taking, nor does it explicitly discuss language modeling of speech or architecture/evaluation in the context required by inclusion criteria. Hence, it primarily deals with speech translation, not spoken language modeling or interactive speech systems.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": {
      "reasoning": "Junior reviewers conflicted: JuniorNano saw alignment with spoken-language modeling broadly, but JuniorMini correctly noted the paper focuses on multilingual speech↔text translation and standard speech tasks. The abstract describes a unified multimodal translation system (SEAMLESSM4T) and reports BLEU, robustness to noise, toxicity/gender evaluations, and public release, but it does not use “spoken language model(s)” or equivalent nor discuss speech-level language modeling, full‑duplex interaction, low‑latency synchronous behavior, or arbitration of overlapping speech — all required by the inclusion criteria. Therefore it fails the requirement that the study explicitly center on Spoken Language Models/FD‑SLMs and related evaluation/architectural claims. One-sentence explanation: Exclude because the work is a multilingual speech translation system (ASR/TTS/MT) rather than research on spoken-language modeling or full‑duplex interactive speech models.",
      "evaluation": 1
    },
    "round-B_SeniorLead_reasoning": "Junior reviewers conflicted: JuniorNano saw alignment with spoken-language modeling broadly, but JuniorMini correctly noted the paper focuses on multilingual speech↔text translation and standard speech tasks. The abstract describes a unified multimodal translation system (SEAMLESSM4T) and reports BLEU, robustness to noise, toxicity/gender evaluations, and public release, but it does not use “spoken language model(s)” or equivalent nor discuss speech-level language modeling, full‑duplex interaction, low‑latency synchronous behavior, or arbitration of overlapping speech — all required by the inclusion criteria. Therefore it fails the requirement that the study explicitly center on Spoken Language Models/FD‑SLMs and related evaluation/architectural claims. One-sentence explanation: Exclude because the work is a multilingual speech translation system (ASR/TTS/MT) rather than research on spoken-language modeling or full‑duplex interactive speech models.",
    "round-B_SeniorLead_evaluation": 1,
    "final_verdict": "exclude (senior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Minimally-Supervised Speech Synthesis with Conditional Diffusion Model and Language Model: A Comparative Study of Semantic Coding",
    "abstract": "Recently, there has been a growing interest in text-to-speech (TTS) methods that can be trained with minimal supervision by combining two types of discrete speech representations and using two sequence-to-sequence tasks to decouple TTS. However, existing methods suffer from three problems: the high-frequency waveform distortion of discrete speech representations, the prosodic averaging problem caused by the duration prediction model in non-autoregressive frameworks, and difficulty in prediction due to the information redundancy and dimension explosion of existing semantic coding methods. To address these problems, three progressive methods are proposed. First, we propose Diff-LM-Speech, an autoregressive structure consisting of a language model and diffusion models, which models the semantic embedding into the mel-spectrogram based on a diffusion model to achieve higher audio quality. We also introduce a prompt encoder structure based on a variational autoencoder and a prosody bottleneck to improve prompt representation ability. Second, we propose Tetra-Diff-Speech, a non-autoregressive structure consisting of four diffusion model-based modules that design a duration diffusion model to achieve diverse prosodic expressions. Finally, we propose Tri-Diff-Speech, a non-autoregressive structure consisting of three diffusion model-based modules that verify the non-necessity of existing semantic coding models and achieve the best results. Experimental results show that our proposed methods outperform baseline methods. We provide a website with audio samples. <sup xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">1</sup>",
    "metadata": {
      "title": "Minimally-Supervised Speech Synthesis with Conditional Diffusion Model and Language Model: A Comparative Study of Semantic Coding",
      "summary": "Recently, there has been a growing interest in text-to-speech (TTS) methods that can be trained with minimal supervision by combining two types of discrete speech representations and using two sequence-to-sequence tasks to decouple TTS. However, existing methods suffer from three problems: the high-frequency waveform distortion of discrete speech representations, the prosodic averaging problem caused by the duration prediction model in non-autoregressive frameworks, and difficulty in prediction due to the information redundancy and dimension explosion of existing semantic coding methods. To address these problems, three progressive methods are proposed. First, we propose Diff-LM-Speech, an autoregressive structure consisting of a language model and diffusion models, which models the semantic embedding into the mel-spectrogram based on a diffusion model to achieve higher audio quality. We also introduce a prompt encoder structure based on a variational autoencoder and a prosody bottleneck to improve prompt representation ability. Second, we propose Tetra-Diff-Speech, a non-autoregressive structure consisting of four diffusion model-based modules that design a duration diffusion model to achieve diverse prosodic expressions. Finally, we propose Tri-Diff-Speech, a non-autoregressive structure consisting of three diffusion model-based modules that verify the non-necessity of existing semantic coding models and achieve the best results. Experimental results show that our proposed methods outperform baseline methods. We provide a website with audio samples. <sup xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">1</sup>",
      "abstract": "Recently, there has been a growing interest in text-to-speech (TTS) methods that can be trained with minimal supervision by combining two types of discrete speech representations and using two sequence-to-sequence tasks to decouple TTS. However, existing methods suffer from three problems: the high-frequency waveform distortion of discrete speech representations, the prosodic averaging problem caused by the duration prediction model in non-autoregressive frameworks, and difficulty in prediction due to the information redundancy and dimension explosion of existing semantic coding methods. To address these problems, three progressive methods are proposed. First, we propose Diff-LM-Speech, an autoregressive structure consisting of a language model and diffusion models, which models the semantic embedding into the mel-spectrogram based on a diffusion model to achieve higher audio quality. We also introduce a prompt encoder structure based on a variational autoencoder and a prosody bottleneck to improve prompt representation ability. Second, we propose Tetra-Diff-Speech, a non-autoregressive structure consisting of four diffusion model-based modules that design a duration diffusion model to achieve diverse prosodic expressions. Finally, we propose Tri-Diff-Speech, a non-autoregressive structure consisting of three diffusion model-based modules that verify the non-necessity of existing semantic coding models and achieve the best results. Experimental results show that our proposed methods outperform baseline methods. We provide a website with audio samples. <sup xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">1</sup>",
      "doi": "https://doi.org/10.1109/icassp48485.2024.10446203",
      "openalex_id": "https://openalex.org/W4392903524",
      "arxiv_id": "",
      "publication_date": "2024-03-18",
      "published": "2024-03-18",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The paper centers on speech modeling by integrating a language model with diffusion-based speech synthesis (Diff-LM-Speech, diffusion modules) for TTS, includes architectural/ training/evaluation details, and is within the last 3 years, aligning with Spoken Language Model / Speech-LM concepts; it may not emphasize interactive synchronization or dialogue arbitration, but meets the core SLM-oriented criteria and provides evaluative results beyond simple captions.",
      "evaluation": 4
    },
    "round-A_JuniorNano_reasoning": "The paper centers on speech modeling by integrating a language model with diffusion-based speech synthesis (Diff-LM-Speech, diffusion modules) for TTS, includes architectural/ training/evaluation details, and is within the last 3 years, aligning with Spoken Language Model / Speech-LM concepts; it may not emphasize interactive synchronization or dialogue arbitration, but meets the core SLM-oriented criteria and provides evaluative results beyond simple captions.",
    "round-A_JuniorNano_evaluation": 4,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper focuses on text-to-speech (TTS) techniques using minimal supervision and sequence-to-sequence modeling with diffusion and language models to improve speech synthesis quality. However, it centers on TTS technology rather than spoken language models involving speech sequence modeling, dialogue interaction, full-duplex capabilities, or behavioral arbitration as required by the inclusion criteria. The evaluation also focuses on audio quality improvement, without addressing synchronous interaction, real-time behavior arbitration, or multi-dimensional assessment. Therefore, it does not meet the inclusion requirements focused on Spoken Language Models or Full-Duplex Spoken Language Models and should be excluded.",
      "evaluation": 2
    },
    "round-A_JuniorMini_reasoning": "The paper focuses on text-to-speech (TTS) techniques using minimal supervision and sequence-to-sequence modeling with diffusion and language models to improve speech synthesis quality. However, it centers on TTS technology rather than spoken language models involving speech sequence modeling, dialogue interaction, full-duplex capabilities, or behavioral arbitration as required by the inclusion criteria. The evaluation also focuses on audio quality improvement, without addressing synchronous interaction, real-time behavior arbitration, or multi-dimensional assessment. Therefore, it does not meet the inclusion requirements focused on Spoken Language Models or Full-Duplex Spoken Language Models and should be excluded.",
    "round-A_JuniorMini_evaluation": 2,
    "round-B_SeniorLead_output": {
      "reasoning": "I reviewed the two junior reviewers' inputs (one leaned include citing language-model components; the other excluded as TTS-only). The paper is clearly focused on minimal-supervision TTS—diffusion models, semantic coding, duration/prosody modeling and mel-spectrogram generation—and does not treat spoken language models or language modeling of speech as the core research axis, nor does it address turn-taking/full‑duplex interaction, synchronization, or behavioral arbitration; therefore it meets the exclusion criteria (single-task TTS) and fails the inclusion requirements. Score explanation: This is a TTS-methods paper without the SLM/FD‑SLM/dialogue or real‑time interaction contributions required, so it should be excluded.",
      "evaluation": 2
    },
    "round-B_SeniorLead_reasoning": "I reviewed the two junior reviewers' inputs (one leaned include citing language-model components; the other excluded as TTS-only). The paper is clearly focused on minimal-supervision TTS—diffusion models, semantic coding, duration/prosody modeling and mel-spectrogram generation—and does not treat spoken language models or language modeling of speech as the core research axis, nor does it address turn-taking/full‑duplex interaction, synchronization, or behavioral arbitration; therefore it meets the exclusion criteria (single-task TTS) and fails the inclusion requirements. Score explanation: This is a TTS-methods paper without the SLM/FD‑SLM/dialogue or real‑time interaction contributions required, so it should be excluded.",
    "round-B_SeniorLead_evaluation": 2,
    "final_verdict": "exclude (senior:2)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "High-Fidelity Speech Synthesis with Minimal Supervision: All Using Diffusion Models",
    "abstract": "Text-to-speech (TTS) methods have shown promising results in voice cloning, but they require a large number of labeled text-speech pairs. Minimally-supervised speech synthesis decouples TTS by combining two types of discrete speech representations(semantic & acoustic) and using two sequence-to-sequence tasks to enable training with minimal supervision. However, existing methods suffer from information redundancy and dimension explosion in semantic representation, and high-frequency waveform distortion in discrete acoustic representation. Autoregressive frameworks exhibit typical instability and uncontrollability issues. And non-autoregressive frameworks suffer from prosodic averaging caused by duration prediction models. To address these issues, we propose a minimally-supervised high-fidelity speech synthesis method, where all modules are constructed based on the diffusion models. The non-autoregressive framework enhances controllability, and the duration diffusion model enables diversified prosodic expression. Contrastive Token-Acoustic Pretraining (CTAP) is used as an intermediate semantic representation to solve the problems of information redundancy and dimension explosion in existing semantic coding methods. Mel-spectrogram is used as the acoustic representation. Both semantic and acoustic representations are predicted by continuous variable regression tasks to solve the problem of high-frequency fine-grained waveform distortion. Experimental results show that our proposed method outperforms the baseline method. We provide audio samples on our website. <sup xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">1</sup>",
    "metadata": {
      "title": "High-Fidelity Speech Synthesis with Minimal Supervision: All Using Diffusion Models",
      "summary": "Text-to-speech (TTS) methods have shown promising results in voice cloning, but they require a large number of labeled text-speech pairs. Minimally-supervised speech synthesis decouples TTS by combining two types of discrete speech representations(semantic & acoustic) and using two sequence-to-sequence tasks to enable training with minimal supervision. However, existing methods suffer from information redundancy and dimension explosion in semantic representation, and high-frequency waveform distortion in discrete acoustic representation. Autoregressive frameworks exhibit typical instability and uncontrollability issues. And non-autoregressive frameworks suffer from prosodic averaging caused by duration prediction models. To address these issues, we propose a minimally-supervised high-fidelity speech synthesis method, where all modules are constructed based on the diffusion models. The non-autoregressive framework enhances controllability, and the duration diffusion model enables diversified prosodic expression. Contrastive Token-Acoustic Pretraining (CTAP) is used as an intermediate semantic representation to solve the problems of information redundancy and dimension explosion in existing semantic coding methods. Mel-spectrogram is used as the acoustic representation. Both semantic and acoustic representations are predicted by continuous variable regression tasks to solve the problem of high-frequency fine-grained waveform distortion. Experimental results show that our proposed method outperforms the baseline method. We provide audio samples on our website. <sup xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">1</sup>",
      "abstract": "Text-to-speech (TTS) methods have shown promising results in voice cloning, but they require a large number of labeled text-speech pairs. Minimally-supervised speech synthesis decouples TTS by combining two types of discrete speech representations(semantic & acoustic) and using two sequence-to-sequence tasks to enable training with minimal supervision. However, existing methods suffer from information redundancy and dimension explosion in semantic representation, and high-frequency waveform distortion in discrete acoustic representation. Autoregressive frameworks exhibit typical instability and uncontrollability issues. And non-autoregressive frameworks suffer from prosodic averaging caused by duration prediction models. To address these issues, we propose a minimally-supervised high-fidelity speech synthesis method, where all modules are constructed based on the diffusion models. The non-autoregressive framework enhances controllability, and the duration diffusion model enables diversified prosodic expression. Contrastive Token-Acoustic Pretraining (CTAP) is used as an intermediate semantic representation to solve the problems of information redundancy and dimension explosion in existing semantic coding methods. Mel-spectrogram is used as the acoustic representation. Both semantic and acoustic representations are predicted by continuous variable regression tasks to solve the problem of high-frequency fine-grained waveform distortion. Experimental results show that our proposed method outperforms the baseline method. We provide audio samples on our website. <sup xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">1</sup>",
      "doi": "https://doi.org/10.1109/icassp48485.2024.10448495",
      "openalex_id": "https://openalex.org/W4392931282",
      "arxiv_id": "",
      "publication_date": "2024-03-18",
      "published": "2024-03-18",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The work focuses on high-fidelity speech synthesis via diffusion models for minimally supervised TTS, treating speech as input to a standalone synthesis system rather than as a core component of Spoken Language Models or interactive speech-language modeling, and it does not address full-duplex interaction, synchronization, or speech-language modeling constructs required by the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The work focuses on high-fidelity speech synthesis via diffusion models for minimally supervised TTS, treating speech as input to a standalone synthesis system rather than as a core component of Spoken Language Models or interactive speech-language modeling, and it does not address full-duplex interaction, synchronization, or speech-language modeling constructs required by the inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper focuses on minimally-supervised high-fidelity speech synthesis using diffusion models, mainly addressing TTS challenges without discussing spoken language models, full-duplex spoken interactions, cognitive parallelism, or real-time interactive behaviors; it does not mention spoken language modeling or speech-language models explicitly, nor does it address interactive or synchronous dialogue behavior arbitration, and its evaluation centers on synthesis quality rather than the multi-dimensional interactive metrics required by the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper focuses on minimally-supervised high-fidelity speech synthesis using diffusion models, mainly addressing TTS challenges without discussing spoken language models, full-duplex spoken interactions, cognitive parallelism, or real-time interactive behaviors; it does not mention spoken language modeling or speech-language models explicitly, nor does it address interactive or synchronous dialogue behavior arbitration, and its evaluation centers on synthesis quality rather than the multi-dimensional interactive metrics required by the inclusion criteria.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Data Redaction from Conditional Generative Models",
    "abstract": "Deep generative models are known to produce undesirable samples such as harmful content. Traditional mitigation methods include re-training from scratch, filtering, or editing; however, these are either computationally expensive or can be circumvented by third parties. In this paper, we take a different approach and study how to post-edit an already-trained conditional generative model so that it redacts certain conditionals that will, with high probability, lead to undesirable content. This is done by distilling the conditioning network in the models, giving a solution that is effective, efficient, controllable, and universal for a class of deep generative models. We conduct experiments on redacting prompts in text-to-image models and redacting voices in text-to-speech models. Our method is computationally light, leads to better redaction quality and robustness than baseline methods while still retaining high generation quality.",
    "metadata": {
      "title": "Data Redaction from Conditional Generative Models",
      "summary": "Deep generative models are known to produce undesirable samples such as harmful content. Traditional mitigation methods include re-training from scratch, filtering, or editing; however, these are either computationally expensive or can be circumvented by third parties. In this paper, we take a different approach and study how to post-edit an already-trained conditional generative model so that it redacts certain conditionals that will, with high probability, lead to undesirable content. This is done by distilling the conditioning network in the models, giving a solution that is effective, efficient, controllable, and universal for a class of deep generative models. We conduct experiments on redacting prompts in text-to-image models and redacting voices in text-to-speech models. Our method is computationally light, leads to better redaction quality and robustness than baseline methods while still retaining high generation quality.",
      "abstract": "Deep generative models are known to produce undesirable samples such as harmful content. Traditional mitigation methods include re-training from scratch, filtering, or editing; however, these are either computationally expensive or can be circumvented by third parties. In this paper, we take a different approach and study how to post-edit an already-trained conditional generative model so that it redacts certain conditionals that will, with high probability, lead to undesirable content. This is done by distilling the conditioning network in the models, giving a solution that is effective, efficient, controllable, and universal for a class of deep generative models. We conduct experiments on redacting prompts in text-to-image models and redacting voices in text-to-speech models. Our method is computationally light, leads to better redaction quality and robustness than baseline methods while still retaining high generation quality.",
      "doi": "https://doi.org/10.1109/satml59370.2024.00035",
      "openalex_id": "https://openalex.org/W4396815655",
      "arxiv_id": "",
      "publication_date": "2024-04-09",
      "published": "2024-04-09",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The article focuses on post-editing/redacting prompts in conditional generative models (text-to-image and text-to-speech) and does not center on Spoken Language Models or speech-language modeling with interactive evaluation, so it fails the inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The article focuses on post-editing/redacting prompts in conditional generative models (text-to-image and text-to-speech) and does not center on Spoken Language Models or speech-language modeling with interactive evaluation, so it fails the inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper focuses on redacting undesirable content from conditional generative models, including text-to-image and text-to-speech models, but does not explicitly discuss Spoken Language Models or full-duplex spoken language model frameworks; it centers on post-editing conditioned generation rather than core speech modeling or interactive speech-to-speech communication, and there is no mention of key concepts like cognitive parallelism, low-latency full-duplex interaction, or multi-dimensional evaluation as required by the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper focuses on redacting undesirable content from conditional generative models, including text-to-image and text-to-speech models, but does not explicitly discuss Spoken Language Models or full-duplex spoken language model frameworks; it centers on post-editing conditioned generation rather than core speech modeling or interactive speech-to-speech communication, and there is no mention of key concepts like cognitive parallelism, low-latency full-duplex interaction, or multi-dimensional evaluation as required by the inclusion criteria.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Development of an intelligent virtual assistant for digitalization of Moroccan agriculture",
    "abstract": "This paper presents the design, development, and implementation of an innovative text-to-text chatbot system aimed at digitalizing the agriculture sector in Morocco, with a focus on supporting Darija-speaking farmers. The project also encompasses the curation of a comprehensive database to facilitate future fine-tuning of Speech-to-Text (STT) and Text-to-Speech (TTS) models in Darija. The project’s primary objective is the development of chatbot capable of responding to farmers’ text queries in Darija, providing them with instant access to critical agricultural information and support. Concurrently, we have curated an extensive database of Darija agricultural terminology, phrases, and dialogues, laying the groundwork for future voice-enabled interactions. Throughout the project, we have conducted thorough research into Darija linguistics and agricultural practices, followed by an in-depth development phase of the chatbot system. This included natural language processing, intent recognition, and response generation tailored to the nuances of Darija. The database curation involved extensive collaboration with agricultural experts to ensure authenticity and relevance. Looking forward, this project serves as a crucial stepping stone towards a fully voice-enabled agricultural support system in Darija. The curated database will be instrumental in fine-tuning STT and TTS models, potentially revolutionizing how Moroccan farmers access and interact with digital agricultural resources.",
    "metadata": {
      "title": "Development of an intelligent virtual assistant for digitalization of Moroccan agriculture",
      "summary": "This paper presents the design, development, and implementation of an innovative text-to-text chatbot system aimed at digitalizing the agriculture sector in Morocco, with a focus on supporting Darija-speaking farmers. The project also encompasses the curation of a comprehensive database to facilitate future fine-tuning of Speech-to-Text (STT) and Text-to-Speech (TTS) models in Darija. The project’s primary objective is the development of chatbot capable of responding to farmers’ text queries in Darija, providing them with instant access to critical agricultural information and support. Concurrently, we have curated an extensive database of Darija agricultural terminology, phrases, and dialogues, laying the groundwork for future voice-enabled interactions. Throughout the project, we have conducted thorough research into Darija linguistics and agricultural practices, followed by an in-depth development phase of the chatbot system. This included natural language processing, intent recognition, and response generation tailored to the nuances of Darija. The database curation involved extensive collaboration with agricultural experts to ensure authenticity and relevance. Looking forward, this project serves as a crucial stepping stone towards a fully voice-enabled agricultural support system in Darija. The curated database will be instrumental in fine-tuning STT and TTS models, potentially revolutionizing how Moroccan farmers access and interact with digital agricultural resources.",
      "abstract": "This paper presents the design, development, and implementation of an innovative text-to-text chatbot system aimed at digitalizing the agriculture sector in Morocco, with a focus on supporting Darija-speaking farmers. The project also encompasses the curation of a comprehensive database to facilitate future fine-tuning of Speech-to-Text (STT) and Text-to-Speech (TTS) models in Darija. The project’s primary objective is the development of chatbot capable of responding to farmers’ text queries in Darija, providing them with instant access to critical agricultural information and support. Concurrently, we have curated an extensive database of Darija agricultural terminology, phrases, and dialogues, laying the groundwork for future voice-enabled interactions. Throughout the project, we have conducted thorough research into Darija linguistics and agricultural practices, followed by an in-depth development phase of the chatbot system. This included natural language processing, intent recognition, and response generation tailored to the nuances of Darija. The database curation involved extensive collaboration with agricultural experts to ensure authenticity and relevance. Looking forward, this project serves as a crucial stepping stone towards a fully voice-enabled agricultural support system in Darija. The curated database will be instrumental in fine-tuning STT and TTS models, potentially revolutionizing how Moroccan farmers access and interact with digital agricultural resources.",
      "doi": "https://doi.org/10.1051/itmconf/20246901003",
      "openalex_id": "https://openalex.org/W4405379347",
      "arxiv_id": "",
      "publication_date": "2024-01-01",
      "published": "2024-01-01",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item centers on a text-based chatbot with a plan for Darija STT/TTS data, but it does not present a core Spoken Language Model / FD-SLM architecture, training, or evaluation, thus not meeting the main inclusion criteria for speech-language modeling and full-duplex dialogue evaluation.",
      "evaluation": 2
    },
    "round-A_JuniorNano_reasoning": "The item centers on a text-based chatbot with a plan for Darija STT/TTS data, but it does not present a core Spoken Language Model / FD-SLM architecture, training, or evaluation, thus not meeting the main inclusion criteria for speech-language modeling and full-duplex dialogue evaluation.",
    "round-A_JuniorNano_evaluation": 2,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on developing a text-to-text chatbot system for agricultural support in Darija, emphasizing text-based interactions rather than speech-based modeling or full-duplex spoken language models. Although it mentions future plans for voice-enabled interactions (STT, TTS fine-tuning), the current research does not primarily center on spoken language models or full-duplex speech interaction, lacking discussion on speech modeling as the core or real-time interaction capabilities detailed in the inclusion criteria. Therefore, it does not fully meet the inclusion requirements related to speech-based language modeling and real-time spoken interaction.",
      "evaluation": 2
    },
    "round-A_JuniorMini_reasoning": "The study focuses on developing a text-to-text chatbot system for agricultural support in Darija, emphasizing text-based interactions rather than speech-based modeling or full-duplex spoken language models. Although it mentions future plans for voice-enabled interactions (STT, TTS fine-tuning), the current research does not primarily center on spoken language models or full-duplex speech interaction, lacking discussion on speech modeling as the core or real-time interaction capabilities detailed in the inclusion criteria. Therefore, it does not fully meet the inclusion requirements related to speech-based language modeling and real-time spoken interaction.",
    "round-A_JuniorMini_evaluation": 2,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:2)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "The Kyoto Speech-to-Speech Translation System for IWSLT 2023",
    "abstract": "This paper describes the Kyoto speech-to-speech translation system for IWSLT 2023. Our system is a combination of speech-to-text translation and text-to-speech synthesis. For the speech-to-text translation model, we used the dual-decoderTransformer model. For text-to-speech synthesis model, we took a cascade approach of an acoustic model and a vocoder.",
    "metadata": {
      "title": "The Kyoto Speech-to-Speech Translation System for IWSLT 2023",
      "summary": "This paper describes the Kyoto speech-to-speech translation system for IWSLT 2023. Our system is a combination of speech-to-text translation and text-to-speech synthesis. For the speech-to-text translation model, we used the dual-decoderTransformer model. For text-to-speech synthesis model, we took a cascade approach of an acoustic model and a vocoder.",
      "abstract": "This paper describes the Kyoto speech-to-speech translation system for IWSLT 2023. Our system is a combination of speech-to-text translation and text-to-speech synthesis. For the speech-to-text translation model, we used the dual-decoderTransformer model. For text-to-speech synthesis model, we took a cascade approach of an acoustic model and a vocoder.",
      "doi": "https://doi.org/10.18653/v1/2023.iwslt-1.33",
      "openalex_id": "https://openalex.org/W4385571610",
      "arxiv_id": "",
      "publication_date": "2023-01-01",
      "published": "2023-01-01",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item presents a speech-to-speech translation pipeline (STT+TTS) and does not address Spoken Language Models or full-duplex interactive speech modeling, nor evaluates interactive/speech-language modeling criteria, thus fails all inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item presents a speech-to-speech translation pipeline (STT+TTS) and does not address Spoken Language Models or full-duplex interactive speech modeling, nor evaluates interactive/speech-language modeling criteria, thus fails all inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper focuses on a speech-to-speech translation system, combining speech-to-text translation and text-to-speech synthesis, but it does not explicitly discuss spoken language models (SLMs) or full-duplex spoken language models, nor does it address synchronous interaction capabilities such as turn-taking or cognitive parallelism. It mainly deals with individual speech technologies (ASR and TTS) without elaborating on speech language modeling architectures, training, evaluation related to full-duplex interaction, or synchronization, and no mention of the specified keywords in title or abstract appears. Therefore, it does not meet the inclusion criteria and fits the exclusion criteria related to sole focus on single speech technologies and absence of synchronous interaction features or spoken language modeling perspective.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper focuses on a speech-to-speech translation system, combining speech-to-text translation and text-to-speech synthesis, but it does not explicitly discuss spoken language models (SLMs) or full-duplex spoken language models, nor does it address synchronous interaction capabilities such as turn-taking or cognitive parallelism. It mainly deals with individual speech technologies (ASR and TTS) without elaborating on speech language modeling architectures, training, evaluation related to full-duplex interaction, or synchronization, and no mention of the specified keywords in title or abstract appears. Therefore, it does not meet the inclusion criteria and fits the exclusion criteria related to sole focus on single speech technologies and absence of synchronous interaction features or spoken language modeling perspective.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Emotion transfer in audio using mel-cepstral representation and CycleGANs",
    "abstract": "Abstract The field of audio synthesis is currently confronted with two major challenges: to more effectively eliminate non-emotional influences in emotional feature extraction work, and to improve the emotional expression when reference audio is scarce. Therefore, an innovative audio deep feature decoupling and emotion adaptive fusion model, which combines Mel Frequency Cepstral Coefficients (MFCCs) with Cycle-consistent Generative Adversarial Networks (CycleGANs), is proposed in this paper. We designed a Deep Feature Decoupled Encoder Group (DFDEG), which is based on Gated Linear Units (GLU), Self-Attention, and Average Pooling. Meanwhile, we designed a feature fusion method called Emotion Adaptive Instance Normalization (Emo-AdaIN), which is based on AdaIN. By integrating the DFDEG, Emo-AdaIN, and CycleGANs, an unsupervised bidirectional multi-emotion transfer method within the MFCCs is successfully achieved. This method performs well in terms of emotion decoupling and transfer on unseen datasets: for different speakers, the transfer result’s Lowest Emotional Similarity (LES) is 94.56%, and Average Confidence Level (ACL) is 0.51. This demonstrates the generalization performance across different speakers and the robustness across different emotion granularity.",
    "metadata": {
      "title": "Emotion transfer in audio using mel-cepstral representation and CycleGANs",
      "summary": "Abstract The field of audio synthesis is currently confronted with two major challenges: to more effectively eliminate non-emotional influences in emotional feature extraction work, and to improve the emotional expression when reference audio is scarce. Therefore, an innovative audio deep feature decoupling and emotion adaptive fusion model, which combines Mel Frequency Cepstral Coefficients (MFCCs) with Cycle-consistent Generative Adversarial Networks (CycleGANs), is proposed in this paper. We designed a Deep Feature Decoupled Encoder Group (DFDEG), which is based on Gated Linear Units (GLU), Self-Attention, and Average Pooling. Meanwhile, we designed a feature fusion method called Emotion Adaptive Instance Normalization (Emo-AdaIN), which is based on AdaIN. By integrating the DFDEG, Emo-AdaIN, and CycleGANs, an unsupervised bidirectional multi-emotion transfer method within the MFCCs is successfully achieved. This method performs well in terms of emotion decoupling and transfer on unseen datasets: for different speakers, the transfer result’s Lowest Emotional Similarity (LES) is 94.56%, and Average Confidence Level (ACL) is 0.51. This demonstrates the generalization performance across different speakers and the robustness across different emotion granularity.",
      "abstract": "Abstract The field of audio synthesis is currently confronted with two major challenges: to more effectively eliminate non-emotional influences in emotional feature extraction work, and to improve the emotional expression when reference audio is scarce. Therefore, an innovative audio deep feature decoupling and emotion adaptive fusion model, which combines Mel Frequency Cepstral Coefficients (MFCCs) with Cycle-consistent Generative Adversarial Networks (CycleGANs), is proposed in this paper. We designed a Deep Feature Decoupled Encoder Group (DFDEG), which is based on Gated Linear Units (GLU), Self-Attention, and Average Pooling. Meanwhile, we designed a feature fusion method called Emotion Adaptive Instance Normalization (Emo-AdaIN), which is based on AdaIN. By integrating the DFDEG, Emo-AdaIN, and CycleGANs, an unsupervised bidirectional multi-emotion transfer method within the MFCCs is successfully achieved. This method performs well in terms of emotion decoupling and transfer on unseen datasets: for different speakers, the transfer result’s Lowest Emotional Similarity (LES) is 94.56%, and Average Confidence Level (ACL) is 0.51. This demonstrates the generalization performance across different speakers and the robustness across different emotion granularity.",
      "doi": "https://doi.org/10.1007/s44443-025-00082-7",
      "openalex_id": "https://openalex.org/W4411569810",
      "arxiv_id": "",
      "publication_date": "2025-06-23",
      "published": "2025-06-23",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "本研究未以 Spoken Language Models 或等價概念為核心，而是聚焦於情感轉換的音訊特徵轉換，與語音語言建模/全雙工互動關鍵要素不符，因此應排除。",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "本研究未以 Spoken Language Models 或等價概念為核心，而是聚焦於情感轉換的音訊特徵轉換，與語音語言建模/全雙工互動關鍵要素不符，因此應排除。",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on audio synthesis and emotion transfer in audio using Mel Frequency Cepstral Coefficients and CycleGANs, without explicit mention of Spoken Language Models or language modeling of speech as the core. It does not discuss models related to full-duplex spoken language interaction, language modeling architectures, training, inference, or evaluation for synchronous interactive communication, nor does it mention evaluation on tasks related to turn-taking, overlap, or real-time feedback. Hence, it does not satisfy the main inclusion criteria and relates more to traditional speech synthesis/emotion transfer rather than SLMs or FD-SLMs.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on audio synthesis and emotion transfer in audio using Mel Frequency Cepstral Coefficients and CycleGANs, without explicit mention of Spoken Language Models or language modeling of speech as the core. It does not discuss models related to full-duplex spoken language interaction, language modeling architectures, training, inference, or evaluation for synchronous interactive communication, nor does it mention evaluation on tasks related to turn-taking, overlap, or real-time feedback. Hence, it does not satisfy the main inclusion criteria and relates more to traditional speech synthesis/emotion transfer rather than SLMs or FD-SLMs.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "FunCodec: A Fundamental, Reproducible and Integrable Open-Source Toolkit for Neural Speech Codec",
    "abstract": "This paper presents FunCodec, a fundamental neural speech codec toolkit, which is an extension of the open-source speech processing toolkit FunASR. FunCodec provides reproducible training recipes and inference scripts for the latest neural speech codec models, such as SoundStream and Encodec. Thanks to the unified design with FunASR, FunCodec can be easily integrated into downstream tasks, such as speech recognition. Along with FunCodec, pretrained models are also provided, which can be used for academic or generalized purposes. Based on the toolkit, we further propose the frequency-domain codec models, FreqCodec, which can achieve comparable speech quality with much lower computation and parameter complexity. Experimental results show that, under the same compression ratio, FunCodec can achieve better reconstruction quality compared with other toolkits and released models. We also demonstrate that the pre-trained models are suitable for downstream tasks, including automatic speech recognition and personalized text-to-speech synthesis. This toolkit is publicly available at https://github.com/alibaba-damo-academy/FunCodec.",
    "metadata": {
      "title": "FunCodec: A Fundamental, Reproducible and Integrable Open-Source Toolkit for Neural Speech Codec",
      "summary": "This paper presents FunCodec, a fundamental neural speech codec toolkit, which is an extension of the open-source speech processing toolkit FunASR. FunCodec provides reproducible training recipes and inference scripts for the latest neural speech codec models, such as SoundStream and Encodec. Thanks to the unified design with FunASR, FunCodec can be easily integrated into downstream tasks, such as speech recognition. Along with FunCodec, pretrained models are also provided, which can be used for academic or generalized purposes. Based on the toolkit, we further propose the frequency-domain codec models, FreqCodec, which can achieve comparable speech quality with much lower computation and parameter complexity. Experimental results show that, under the same compression ratio, FunCodec can achieve better reconstruction quality compared with other toolkits and released models. We also demonstrate that the pre-trained models are suitable for downstream tasks, including automatic speech recognition and personalized text-to-speech synthesis. This toolkit is publicly available at https://github.com/alibaba-damo-academy/FunCodec.",
      "abstract": "This paper presents FunCodec, a fundamental neural speech codec toolkit, which is an extension of the open-source speech processing toolkit FunASR. FunCodec provides reproducible training recipes and inference scripts for the latest neural speech codec models, such as SoundStream and Encodec. Thanks to the unified design with FunASR, FunCodec can be easily integrated into downstream tasks, such as speech recognition. Along with FunCodec, pretrained models are also provided, which can be used for academic or generalized purposes. Based on the toolkit, we further propose the frequency-domain codec models, FreqCodec, which can achieve comparable speech quality with much lower computation and parameter complexity. Experimental results show that, under the same compression ratio, FunCodec can achieve better reconstruction quality compared with other toolkits and released models. We also demonstrate that the pre-trained models are suitable for downstream tasks, including automatic speech recognition and personalized text-to-speech synthesis. This toolkit is publicly available at https://github.com/alibaba-damo-academy/FunCodec.",
      "doi": "https://doi.org/10.1109/icassp48485.2024.10447523",
      "openalex_id": "https://openalex.org/W4392903389",
      "arxiv_id": "",
      "publication_date": "2024-03-18",
      "published": "2024-03-18",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The work centers on a neural speech codec toolkit rather than Spoken Language Models or speech-language modeling, lacking core focus on SLM/FD-SLM and relevant evaluation beyond codec performance, thus it does not meet inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The work centers on a neural speech codec toolkit rather than Spoken Language Models or speech-language modeling, lacking core focus on SLM/FD-SLM and relevant evaluation beyond codec performance, thus it does not meet inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper presents FunCodec, an open-source neural speech codec toolkit primarily focusing on speech codec models, speech reconstruction quality, and integration into downstream tasks like ASR and TTS. However, it does not explicitly address Spoken Language Models (SLMs), full-duplex spoken language modeling, synchronous interaction, or behavioral arbitration aspects as required by the inclusion criteria. The focus lies on speech signal processing and codec modelling rather than speech language modeling or end-to-end interaction, and it does not mention speech language modeling architectures or synchronous interaction paradigms. Therefore, it does not meet the inclusion criteria and falls under exclusion for focusing on speech technologies (ASR, TTS) without discussing synchronous interaction, full-duplex modeling, or speech language modeling per se.",
      "evaluation": 2
    },
    "round-A_JuniorMini_reasoning": "The paper presents FunCodec, an open-source neural speech codec toolkit primarily focusing on speech codec models, speech reconstruction quality, and integration into downstream tasks like ASR and TTS. However, it does not explicitly address Spoken Language Models (SLMs), full-duplex spoken language modeling, synchronous interaction, or behavioral arbitration aspects as required by the inclusion criteria. The focus lies on speech signal processing and codec modelling rather than speech language modeling or end-to-end interaction, and it does not mention speech language modeling architectures or synchronous interaction paradigms. Therefore, it does not meet the inclusion criteria and falls under exclusion for focusing on speech technologies (ASR, TTS) without discussing synchronous interaction, full-duplex modeling, or speech language modeling per se.",
    "round-A_JuniorMini_evaluation": 2,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:2)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Evaluation of large language models (LLMs) on the mastery of knowledge and skills in the heating, ventilation and air conditioning (HVAC) industry",
    "abstract": "Large language models (LLMs) have shown human-level capabilities in solving various complex tasks. However, it is still unknown whether state-of-the-art LLMs master sufficient knowledge related to heating, ventilation and air conditioning (HVAC) systems. It will be inspiring if LLMs can think and learn like professionals in the HVAC industry. Hence, this study investigates the performance of LLMs on mastering the knowledge and skills related to the HVAC industry by letting them take the ASHRAE Certified HVAC Designer examination, an authoritative examination in the HVAC industry. Three key knowledge capabilities are explored: recall, analysis and application. Twelve representative LLMs are tested such as GPT-3.5, GPT-4 and LLaMA. According to the results, GPT-4 passes the ASHRAE Certified HVAC Designer examination with scores from 74 to 78, which is higher than about half of human examinees. Besides, GPT-3.5 passes the examination twice out of five times. It demonstrates that some LLMs such as GPT-4 and GPT-3.5 have great potential to assist or replace humans in designing and operating HVAC systems. However, they still make some mistakes sometimes due to the lack of knowledge, poor reasoning capabilities and unsatisfactory equation calculation abilities. Accordingly, four future research directions are proposed to reveal how to utilize and improve LLMs in the HVAC industry: teaching LLMs to use design tools or software in the HVAC industry, enabling LLMs to read and analyze the operational data from HVAC systems, developing tailored corpuses for the HVAC industry, and assessing the performance of LLMs in real-world HVAC design and operation scenarios.",
    "metadata": {
      "title": "Evaluation of large language models (LLMs) on the mastery of knowledge and skills in the heating, ventilation and air conditioning (HVAC) industry",
      "summary": "Large language models (LLMs) have shown human-level capabilities in solving various complex tasks. However, it is still unknown whether state-of-the-art LLMs master sufficient knowledge related to heating, ventilation and air conditioning (HVAC) systems. It will be inspiring if LLMs can think and learn like professionals in the HVAC industry. Hence, this study investigates the performance of LLMs on mastering the knowledge and skills related to the HVAC industry by letting them take the ASHRAE Certified HVAC Designer examination, an authoritative examination in the HVAC industry. Three key knowledge capabilities are explored: recall, analysis and application. Twelve representative LLMs are tested such as GPT-3.5, GPT-4 and LLaMA. According to the results, GPT-4 passes the ASHRAE Certified HVAC Designer examination with scores from 74 to 78, which is higher than about half of human examinees. Besides, GPT-3.5 passes the examination twice out of five times. It demonstrates that some LLMs such as GPT-4 and GPT-3.5 have great potential to assist or replace humans in designing and operating HVAC systems. However, they still make some mistakes sometimes due to the lack of knowledge, poor reasoning capabilities and unsatisfactory equation calculation abilities. Accordingly, four future research directions are proposed to reveal how to utilize and improve LLMs in the HVAC industry: teaching LLMs to use design tools or software in the HVAC industry, enabling LLMs to read and analyze the operational data from HVAC systems, developing tailored corpuses for the HVAC industry, and assessing the performance of LLMs in real-world HVAC design and operation scenarios.",
      "abstract": "Large language models (LLMs) have shown human-level capabilities in solving various complex tasks. However, it is still unknown whether state-of-the-art LLMs master sufficient knowledge related to heating, ventilation and air conditioning (HVAC) systems. It will be inspiring if LLMs can think and learn like professionals in the HVAC industry. Hence, this study investigates the performance of LLMs on mastering the knowledge and skills related to the HVAC industry by letting them take the ASHRAE Certified HVAC Designer examination, an authoritative examination in the HVAC industry. Three key knowledge capabilities are explored: recall, analysis and application. Twelve representative LLMs are tested such as GPT-3.5, GPT-4 and LLaMA. According to the results, GPT-4 passes the ASHRAE Certified HVAC Designer examination with scores from 74 to 78, which is higher than about half of human examinees. Besides, GPT-3.5 passes the examination twice out of five times. It demonstrates that some LLMs such as GPT-4 and GPT-3.5 have great potential to assist or replace humans in designing and operating HVAC systems. However, they still make some mistakes sometimes due to the lack of knowledge, poor reasoning capabilities and unsatisfactory equation calculation abilities. Accordingly, four future research directions are proposed to reveal how to utilize and improve LLMs in the HVAC industry: teaching LLMs to use design tools or software in the HVAC industry, enabling LLMs to read and analyze the operational data from HVAC systems, developing tailored corpuses for the HVAC industry, and assessing the performance of LLMs in real-world HVAC design and operation scenarios.",
      "doi": "https://doi.org/10.1016/j.enbenv.2024.03.010",
      "openalex_id": "https://openalex.org/W4393236964",
      "arxiv_id": "",
      "publication_date": "2024-03-27",
      "published": "2024-03-27",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "Not aligned with Spoken Language Models (SLMs/FD-SLMs) core focus; no speech-centric architecture, training, or evaluation; only domain knowledge assessment of LLMs on HVAC exam.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "Not aligned with Spoken Language Models (SLMs/FD-SLMs) core focus; no speech-centric architecture, training, or evaluation; only domain knowledge assessment of LLMs on HVAC exam.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on evaluating large language models on mastery of HVAC knowledge and skills, which is primarily text-based and domain-specific application of LLMs, without focus on spoken language modeling, speech as the primary object, or interactive full-duplex spoken language capabilities; it lacks discussion on speech language modeling, synchronization, or speech-based interactive assessment as required by the inclusion criteria and does not mention speech modeling, real-time interaction, or speech signal processing.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on evaluating large language models on mastery of HVAC knowledge and skills, which is primarily text-based and domain-specific application of LLMs, without focus on spoken language modeling, speech as the primary object, or interactive full-duplex spoken language capabilities; it lacks discussion on speech language modeling, synchronization, or speech-based interactive assessment as required by the inclusion criteria and does not mention speech modeling, real-time interaction, or speech signal processing.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "VoxtLM: Unified Decoder-Only Models for Consolidating Speech Recognition, Synthesis and Speech, Text Continuation Tasks",
    "abstract": "We propose a decoder-only language model, VoxtLM, that can perform four tasks: speech recognition, speech synthesis, text generation, and speech continuation. VoxtLM integrates text vocabulary with discrete speech tokens from self-supervised speech features and uses special tokens to enable multitask learning. Compared to a single-task model, VoxtLM exhibits a significant improvement in speech synthesis, with improvements in both speech intelligibility from 28.9 to 5.6 and objective quality from 2.68 to 3.90. VoxtLM also improves speech generation and speech recognition performance over the single-task counterpart. Further, VoxtLM is trained with publicly available data and training recipes and model checkpoints are open-sourced to make fully reproducible work.",
    "metadata": {
      "title": "VoxtLM: Unified Decoder-Only Models for Consolidating Speech Recognition, Synthesis and Speech, Text Continuation Tasks",
      "summary": "We propose a decoder-only language model, VoxtLM, that can perform four tasks: speech recognition, speech synthesis, text generation, and speech continuation. VoxtLM integrates text vocabulary with discrete speech tokens from self-supervised speech features and uses special tokens to enable multitask learning. Compared to a single-task model, VoxtLM exhibits a significant improvement in speech synthesis, with improvements in both speech intelligibility from 28.9 to 5.6 and objective quality from 2.68 to 3.90. VoxtLM also improves speech generation and speech recognition performance over the single-task counterpart. Further, VoxtLM is trained with publicly available data and training recipes and model checkpoints are open-sourced to make fully reproducible work.",
      "abstract": "We propose a decoder-only language model, VoxtLM, that can perform four tasks: speech recognition, speech synthesis, text generation, and speech continuation. VoxtLM integrates text vocabulary with discrete speech tokens from self-supervised speech features and uses special tokens to enable multitask learning. Compared to a single-task model, VoxtLM exhibits a significant improvement in speech synthesis, with improvements in both speech intelligibility from 28.9 to 5.6 and objective quality from 2.68 to 3.90. VoxtLM also improves speech generation and speech recognition performance over the single-task counterpart. Further, VoxtLM is trained with publicly available data and training recipes and model checkpoints are open-sourced to make fully reproducible work.",
      "doi": "https://doi.org/10.1109/icassp48485.2024.10447112",
      "openalex_id": "https://openalex.org/W4392904805",
      "arxiv_id": "",
      "publication_date": "2024-03-18",
      "published": "2024-03-18",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This VoxtLM paper presents a unified decoder-only model for speech recognition, synthesis, and speech–text tasks (i.e., a full-duplex Spoken Language Model), discusses architecture, training, and evaluation, is within the last 3 years, and centers spoken-language modeling rather than treating speech merely as input to NLP, so it meets inclusion criteria.",
      "evaluation": 5
    },
    "round-A_JuniorNano_reasoning": "This VoxtLM paper presents a unified decoder-only model for speech recognition, synthesis, and speech–text tasks (i.e., a full-duplex Spoken Language Model), discusses architecture, training, and evaluation, is within the last 3 years, and centers spoken-language modeling rather than treating speech merely as input to NLP, so it meets inclusion criteria.",
    "round-A_JuniorNano_evaluation": 5,
    "round-A_JuniorMini_output": {
      "reasoning": "The study proposes VoxtLM, a decoder-only language model integrating speech recognition, synthesis, text generation, and speech continuation with multitask learning, focusing on unified speech and text token modeling. However, the abstract does not explicitly mention aspects of full-duplex interaction, cognitive parallelism, or synchronization required by the inclusion criteria, nor does it discuss real-time interactive behaviors or arbitration relevant to FD-SLMs. The evaluation metrics focus on speech intelligibility and quality improvements, without assessment of real-time interaction or turn-taking. Therefore, the study addresses speech language models but lacks critical elements related to full-duplex spoken language modeling emphasized in the inclusion criteria, suggesting it does not fully meet the requirements for inclusion.",
      "evaluation": 2
    },
    "round-A_JuniorMini_reasoning": "The study proposes VoxtLM, a decoder-only language model integrating speech recognition, synthesis, text generation, and speech continuation with multitask learning, focusing on unified speech and text token modeling. However, the abstract does not explicitly mention aspects of full-duplex interaction, cognitive parallelism, or synchronization required by the inclusion criteria, nor does it discuss real-time interactive behaviors or arbitration relevant to FD-SLMs. The evaluation metrics focus on speech intelligibility and quality improvements, without assessment of real-time interaction or turn-taking. Therefore, the study addresses speech language models but lacks critical elements related to full-duplex spoken language modeling emphasized in the inclusion criteria, suggesting it does not fully meet the requirements for inclusion.",
    "round-A_JuniorMini_evaluation": 2,
    "round-B_SeniorLead_output": {
      "reasoning": "Junior reviewers disagreed: one flagged VoxtLM as a valid spoken-language model (multitask integration of discrete speech tokens and text), while the other correctly noted the abstract contains no discussion or evaluation of full‑duplex interaction, synchronization, overlap handling, low‑latency or arbitration. Although VoxtLM clearly treats speech tokens as core and is a multitask SLM, the abstract only reports standard ASR/TTS/generation metrics (intelligibility, quality, recognition/generation performance) and lacks any explicit treatment or benchmarks for full‑duplex/synchronous interactive behaviors required by the inclusion/exclusion rules—therefore it should be excluded. Score explanation: Exclude (2) because the work, as described in the abstract, does not address full‑duplex synchronization or real‑time interactive evaluation required by the topic.",
      "evaluation": 2
    },
    "round-B_SeniorLead_reasoning": "Junior reviewers disagreed: one flagged VoxtLM as a valid spoken-language model (multitask integration of discrete speech tokens and text), while the other correctly noted the abstract contains no discussion or evaluation of full‑duplex interaction, synchronization, overlap handling, low‑latency or arbitration. Although VoxtLM clearly treats speech tokens as core and is a multitask SLM, the abstract only reports standard ASR/TTS/generation metrics (intelligibility, quality, recognition/generation performance) and lacks any explicit treatment or benchmarks for full‑duplex/synchronous interactive behaviors required by the inclusion/exclusion rules—therefore it should be excluded. Score explanation: Exclude (2) because the work, as described in the abstract, does not address full‑duplex synchronization or real‑time interactive evaluation required by the topic.",
    "round-B_SeniorLead_evaluation": 2,
    "final_verdict": "exclude (senior:2)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Loss Masking Is Not Needed In Decoder-Only Transformer For Discrete-Token-Based ASR",
    "abstract": "Recently, unified speech-text models, such as SpeechGPT, VioLA, and AudioPaLM, have achieved remarkable performance on various speech tasks. These models discretize speech signals into tokens (speech discretization) and use a shared vocabulary for both text and speech tokens. Then they train a single decoder-only Transformer on a mixture of speech tasks. However, these models rely on the Loss Masking strategy for the ASR task, which ignores the dependency among speech tokens. In this paper, we propose to model speech tokens in an autoregressive way, similar to text. We find that applying the conventional cross-entropy loss on input speech tokens does not consistently improve the ASR performance over the Loss Masking approach. To address this issue, we propose a novel approach denoted Smoothed Label Distillation (SLD), which applies a KL divergence loss with smoothed labels on speech tokens. Our experiments show that SLD effectively models speech tokens and outperforms Loss Masking for decoder-only Transformers in ASR tasks with different speech discretization methods <sup xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">1</sup> .",
    "metadata": {
      "title": "Loss Masking Is Not Needed In Decoder-Only Transformer For Discrete-Token-Based ASR",
      "summary": "Recently, unified speech-text models, such as SpeechGPT, VioLA, and AudioPaLM, have achieved remarkable performance on various speech tasks. These models discretize speech signals into tokens (speech discretization) and use a shared vocabulary for both text and speech tokens. Then they train a single decoder-only Transformer on a mixture of speech tasks. However, these models rely on the Loss Masking strategy for the ASR task, which ignores the dependency among speech tokens. In this paper, we propose to model speech tokens in an autoregressive way, similar to text. We find that applying the conventional cross-entropy loss on input speech tokens does not consistently improve the ASR performance over the Loss Masking approach. To address this issue, we propose a novel approach denoted Smoothed Label Distillation (SLD), which applies a KL divergence loss with smoothed labels on speech tokens. Our experiments show that SLD effectively models speech tokens and outperforms Loss Masking for decoder-only Transformers in ASR tasks with different speech discretization methods <sup xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">1</sup> .",
      "abstract": "Recently, unified speech-text models, such as SpeechGPT, VioLA, and AudioPaLM, have achieved remarkable performance on various speech tasks. These models discretize speech signals into tokens (speech discretization) and use a shared vocabulary for both text and speech tokens. Then they train a single decoder-only Transformer on a mixture of speech tasks. However, these models rely on the Loss Masking strategy for the ASR task, which ignores the dependency among speech tokens. In this paper, we propose to model speech tokens in an autoregressive way, similar to text. We find that applying the conventional cross-entropy loss on input speech tokens does not consistently improve the ASR performance over the Loss Masking approach. To address this issue, we propose a novel approach denoted Smoothed Label Distillation (SLD), which applies a KL divergence loss with smoothed labels on speech tokens. Our experiments show that SLD effectively models speech tokens and outperforms Loss Masking for decoder-only Transformers in ASR tasks with different speech discretization methods <sup xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">1</sup> .",
      "doi": "https://doi.org/10.1109/icassp48485.2024.10447296",
      "openalex_id": "https://openalex.org/W4392902656",
      "arxiv_id": "",
      "publication_date": "2024-03-18",
      "published": "2024-03-18",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The paper focuses on ASR-specific modeling of discrete speech tokens with a decoder-only Transformer, not on Spoken Language Models or full-duplex speech-language modeling with interactive capabilities, hence it does not meet the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The paper focuses on ASR-specific modeling of discrete speech tokens with a decoder-only Transformer, not on Spoken Language Models or full-duplex speech-language modeling with interactive capabilities, hence it does not meet the inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on improving modeling of speech tokens within a decoder-only Transformer for ASR, emphasizing speech token modeling and training methods, but it does not explicitly address Spoken Language Models (SLMs) or Full-Duplex Spoken Language Models (FD-SLMs), nor does it discuss synchronous interaction capabilities, cognitive parallelism, or multi-dimensional evaluations related to turn-taking or full-duplex interactions; it centers on ASR performance with discrete speech tokens rather than broader speech-language modeling or interactive speech behavior, thus failing the inclusion criteria requiring explicit mention of spoken language models or equivalent and focus on synchronous interaction; it also falls under exclusion for focusing solely on ASR as a task without broader speech-language modeling context or interaction.",
      "evaluation": 2
    },
    "round-A_JuniorMini_reasoning": "The study focuses on improving modeling of speech tokens within a decoder-only Transformer for ASR, emphasizing speech token modeling and training methods, but it does not explicitly address Spoken Language Models (SLMs) or Full-Duplex Spoken Language Models (FD-SLMs), nor does it discuss synchronous interaction capabilities, cognitive parallelism, or multi-dimensional evaluations related to turn-taking or full-duplex interactions; it centers on ASR performance with discrete speech tokens rather than broader speech-language modeling or interactive speech behavior, thus failing the inclusion criteria requiring explicit mention of spoken language models or equivalent and focus on synchronous interaction; it also falls under exclusion for focusing solely on ASR as a task without broader speech-language modeling context or interaction.",
    "round-A_JuniorMini_evaluation": 2,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:2)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "A Study on the Adverse Impact of Synthetic Speech on Speech Recognition",
    "abstract": "The high-quality synthetic speech by TTS has been widely used in the field of human-computer interaction, bringing users better experience. However, synthetic speech is prone to be mixed with real human speech as part of the noise and recorded by the microphone, which leads to performance decrease for speech recognition. To address this issue, we propose different methods to study the adverse impact of synthetic speech on speech recognition, thereby enhancing its robustness. On the one hand, we adopt the concept of fake audio detection and incorporate an additional module into speech recognition model to differentiate between real and synthetic speech. On the other hand, we propose various methods of incorporating prompt labels from a language semantics perspective to achieve differentiation. These prompt labels provide contextual cues that help speech recognition model to better understand the difference between the two types of speech. The experimental results demonstrate the acoustic modeling of ASR is capable of distinguishing between real and synthetic speech effectively. Putting the prompt labels at the beginning achieves the best performance in a clean synthetic data scenario, while emptying the transcripts of synthetic speech obtains the best performance in a noisy synthetic data scenario.",
    "metadata": {
      "title": "A Study on the Adverse Impact of Synthetic Speech on Speech Recognition",
      "summary": "The high-quality synthetic speech by TTS has been widely used in the field of human-computer interaction, bringing users better experience. However, synthetic speech is prone to be mixed with real human speech as part of the noise and recorded by the microphone, which leads to performance decrease for speech recognition. To address this issue, we propose different methods to study the adverse impact of synthetic speech on speech recognition, thereby enhancing its robustness. On the one hand, we adopt the concept of fake audio detection and incorporate an additional module into speech recognition model to differentiate between real and synthetic speech. On the other hand, we propose various methods of incorporating prompt labels from a language semantics perspective to achieve differentiation. These prompt labels provide contextual cues that help speech recognition model to better understand the difference between the two types of speech. The experimental results demonstrate the acoustic modeling of ASR is capable of distinguishing between real and synthetic speech effectively. Putting the prompt labels at the beginning achieves the best performance in a clean synthetic data scenario, while emptying the transcripts of synthetic speech obtains the best performance in a noisy synthetic data scenario.",
      "abstract": "The high-quality synthetic speech by TTS has been widely used in the field of human-computer interaction, bringing users better experience. However, synthetic speech is prone to be mixed with real human speech as part of the noise and recorded by the microphone, which leads to performance decrease for speech recognition. To address this issue, we propose different methods to study the adverse impact of synthetic speech on speech recognition, thereby enhancing its robustness. On the one hand, we adopt the concept of fake audio detection and incorporate an additional module into speech recognition model to differentiate between real and synthetic speech. On the other hand, we propose various methods of incorporating prompt labels from a language semantics perspective to achieve differentiation. These prompt labels provide contextual cues that help speech recognition model to better understand the difference between the two types of speech. The experimental results demonstrate the acoustic modeling of ASR is capable of distinguishing between real and synthetic speech effectively. Putting the prompt labels at the beginning achieves the best performance in a clean synthetic data scenario, while emptying the transcripts of synthetic speech obtains the best performance in a noisy synthetic data scenario.",
      "doi": "https://doi.org/10.1109/icassp48485.2024.10446991",
      "openalex_id": "https://openalex.org/W4392931320",
      "arxiv_id": "",
      "publication_date": "2024-03-18",
      "published": "2024-03-18",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The article targets ASR robustness to synthetic speech and detection methods, with no emphasis on Spoken Language Models or full-duplex interactive speech-language modeling, thus it does not meet the inclusion criteria and aligns with exclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The article targets ASR robustness to synthetic speech and detection methods, with no emphasis on Spoken Language Models or full-duplex interactive speech-language modeling, thus it does not meet the inclusion criteria and aligns with exclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on the impact of synthetic speech on speech recognition, proposing methods to differentiate real and synthetic speech, but it centers on improving ASR robustness rather than on spoken language models, full-duplex interactions, or language modeling of speech as the core modeling target; it does not discuss SLM architectures, training, or full-duplex interaction capabilities, nor does it cover evaluation aspects like real-time interaction, cognitive parallelism, or synchronization; thus, it fails to meet the inclusion criteria and fits several exclusion points, especially being focused on ASR as a single task with no relation to spoken language modeling.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on the impact of synthetic speech on speech recognition, proposing methods to differentiate real and synthetic speech, but it centers on improving ASR robustness rather than on spoken language models, full-duplex interactions, or language modeling of speech as the core modeling target; it does not discuss SLM architectures, training, or full-duplex interaction capabilities, nor does it cover evaluation aspects like real-time interaction, cognitive parallelism, or synchronization; thus, it fails to meet the inclusion criteria and fits several exclusion points, especially being focused on ASR as a single task with no relation to spoken language modeling.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Large Language Model in Materials Science: Roles, Challenges, and Strategic Outlook",
    "abstract": "Large language models (LLMs) are creating a new paradigm for materials science by transforming textual insights into experimental findings. Leveraging their strengths in natural language understanding, multimodal alignment, and few‐shot reasoning, LLMs already show potential in property prediction, synthesis planning, and uncertainty quantification. This perspective highlights four key roles, Oracle, Surrogate, Quant, and Arbiter, to systematize recent advancements of LLMs in knowledge extraction, property inference, risk assessment, and decision‐making. Experience suggests that true value arises from integrating these capabilities into a verifiable, traceable loop rather than merely scaling model size. However, LLMs still face challenges due to data heterogeneity, limited interpretability, hallucination control, and misalignment with scientific tasks. To address these issues, we propose three forward‐looking directions: developing domain‐adapted foundation models infused with materials science context, establishing a standardized cross‐modal data infrastructure, and incorporating expert feedback alongside robotic automated experimentation into a fully traceable research loop. Through enhanced human–AI collaboration and methodological innovation, LLMs can transform from general‐purpose language tools into scientifically aware partners, advancing materials discovery toward a more efficient, interpretable, and sustainable future.",
    "metadata": {
      "title": "Large Language Model in Materials Science: Roles, Challenges, and Strategic Outlook",
      "summary": "Large language models (LLMs) are creating a new paradigm for materials science by transforming textual insights into experimental findings. Leveraging their strengths in natural language understanding, multimodal alignment, and few‐shot reasoning, LLMs already show potential in property prediction, synthesis planning, and uncertainty quantification. This perspective highlights four key roles, Oracle, Surrogate, Quant, and Arbiter, to systematize recent advancements of LLMs in knowledge extraction, property inference, risk assessment, and decision‐making. Experience suggests that true value arises from integrating these capabilities into a verifiable, traceable loop rather than merely scaling model size. However, LLMs still face challenges due to data heterogeneity, limited interpretability, hallucination control, and misalignment with scientific tasks. To address these issues, we propose three forward‐looking directions: developing domain‐adapted foundation models infused with materials science context, establishing a standardized cross‐modal data infrastructure, and incorporating expert feedback alongside robotic automated experimentation into a fully traceable research loop. Through enhanced human–AI collaboration and methodological innovation, LLMs can transform from general‐purpose language tools into scientifically aware partners, advancing materials discovery toward a more efficient, interpretable, and sustainable future.",
      "abstract": "Large language models (LLMs) are creating a new paradigm for materials science by transforming textual insights into experimental findings. Leveraging their strengths in natural language understanding, multimodal alignment, and few‐shot reasoning, LLMs already show potential in property prediction, synthesis planning, and uncertainty quantification. This perspective highlights four key roles, Oracle, Surrogate, Quant, and Arbiter, to systematize recent advancements of LLMs in knowledge extraction, property inference, risk assessment, and decision‐making. Experience suggests that true value arises from integrating these capabilities into a verifiable, traceable loop rather than merely scaling model size. However, LLMs still face challenges due to data heterogeneity, limited interpretability, hallucination control, and misalignment with scientific tasks. To address these issues, we propose three forward‐looking directions: developing domain‐adapted foundation models infused with materials science context, establishing a standardized cross‐modal data infrastructure, and incorporating expert feedback alongside robotic automated experimentation into a fully traceable research loop. Through enhanced human–AI collaboration and methodological innovation, LLMs can transform from general‐purpose language tools into scientifically aware partners, advancing materials discovery toward a more efficient, interpretable, and sustainable future.",
      "doi": "https://doi.org/10.1002/aidi.202500085",
      "openalex_id": "https://openalex.org/W4412570610",
      "arxiv_id": "",
      "publication_date": "2025-07-22",
      "published": "2025-07-22",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "此項與材料科學領域的LLM應用相關，但未涉及語音/語音模型、全雙工互動、或語音建模相關評估與架構，因而不符合包含條件，應排除。",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "此項與材料科學領域的LLM應用相關，但未涉及語音/語音模型、全雙工互動、或語音建模相關評估與架構，因而不符合包含條件，應排除。",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper focuses on large language models applied to materials science using textual and multimodal data, with no mention of spoken language models, speech or acoustic signal modeling, full-duplex interaction, or synchronization aspects related to speech; the core research is not centered on speech language modeling or related evaluations specified in the inclusion criteria, thus it does not meet the topic requirements nor discuss relevant architectures or evaluations for spoken language models or full-duplex spoken interactions.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper focuses on large language models applied to materials science using textual and multimodal data, with no mention of spoken language models, speech or acoustic signal modeling, full-duplex interaction, or synchronization aspects related to speech; the core research is not centered on speech language modeling or related evaluations specified in the inclusion criteria, thus it does not meet the topic requirements nor discuss relevant architectures or evaluations for spoken language models or full-duplex spoken interactions.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "What Do Self-Supervised Speech Models Know About Words?",
    "abstract": "Abstract Many self-supervised speech models (S3Ms) have been introduced over the last few years, improving performance and data efficiency on various speech tasks. However, these empirical successes alone do not give a complete picture of what is learned during pre-training. Recent work has begun analyzing how S3Ms encode certain properties, such as phonetic and speaker information, but we still lack a proper understanding of knowledge encoded at the word level and beyond. In this work, we use lightweight analysis methods to study segment-level linguistic properties—word identity, boundaries, pronunciation, syntactic features, and semantic features—encoded in S3Ms. We present a comparative study of layer-wise representations from ten S3Ms and find that (i) the frame-level representations within each word segment are not all equally informative, and (ii) the pre-training objective and model size heavily influence the accessibility and distribution of linguistic information across layers. We also find that on several tasks—word discrimination, word segmentation, and semantic sentence similarity—S3Ms trained with visual grounding outperform their speech-only counterparts. Finally, our task-based analyses demonstrate improved performance on word segmentation and acoustic word discrimination while using simpler methods than prior work.1",
    "metadata": {
      "title": "What Do Self-Supervised Speech Models Know About Words?",
      "summary": "Abstract Many self-supervised speech models (S3Ms) have been introduced over the last few years, improving performance and data efficiency on various speech tasks. However, these empirical successes alone do not give a complete picture of what is learned during pre-training. Recent work has begun analyzing how S3Ms encode certain properties, such as phonetic and speaker information, but we still lack a proper understanding of knowledge encoded at the word level and beyond. In this work, we use lightweight analysis methods to study segment-level linguistic properties—word identity, boundaries, pronunciation, syntactic features, and semantic features—encoded in S3Ms. We present a comparative study of layer-wise representations from ten S3Ms and find that (i) the frame-level representations within each word segment are not all equally informative, and (ii) the pre-training objective and model size heavily influence the accessibility and distribution of linguistic information across layers. We also find that on several tasks—word discrimination, word segmentation, and semantic sentence similarity—S3Ms trained with visual grounding outperform their speech-only counterparts. Finally, our task-based analyses demonstrate improved performance on word segmentation and acoustic word discrimination while using simpler methods than prior work.1",
      "abstract": "Abstract Many self-supervised speech models (S3Ms) have been introduced over the last few years, improving performance and data efficiency on various speech tasks. However, these empirical successes alone do not give a complete picture of what is learned during pre-training. Recent work has begun analyzing how S3Ms encode certain properties, such as phonetic and speaker information, but we still lack a proper understanding of knowledge encoded at the word level and beyond. In this work, we use lightweight analysis methods to study segment-level linguistic properties—word identity, boundaries, pronunciation, syntactic features, and semantic features—encoded in S3Ms. We present a comparative study of layer-wise representations from ten S3Ms and find that (i) the frame-level representations within each word segment are not all equally informative, and (ii) the pre-training objective and model size heavily influence the accessibility and distribution of linguistic information across layers. We also find that on several tasks—word discrimination, word segmentation, and semantic sentence similarity—S3Ms trained with visual grounding outperform their speech-only counterparts. Finally, our task-based analyses demonstrate improved performance on word segmentation and acoustic word discrimination while using simpler methods than prior work.1",
      "doi": "https://doi.org/10.1162/tacl_a_00656",
      "openalex_id": "https://openalex.org/W4394773771",
      "arxiv_id": "",
      "publication_date": "2024-01-01",
      "published": "2024-01-01",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The abstract analyzes word-level linguistic information encoded in self-supervised speech models (S3Ms), directly addressing Spoken Language Models and their representations, within the last 3 years, without focusing solely on ASR/TTS or interactive dialogue, thus meeting inclusion and not triggering exclusion criteria.",
      "evaluation": 5
    },
    "round-A_JuniorNano_reasoning": "The abstract analyzes word-level linguistic information encoded in self-supervised speech models (S3Ms), directly addressing Spoken Language Models and their representations, within the last 3 years, without focusing solely on ASR/TTS or interactive dialogue, thus meeting inclusion and not triggering exclusion criteria.",
    "round-A_JuniorNano_evaluation": 5,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on analyzing self-supervised speech models and their encoding of various linguistic properties at the word level, which relates to speech modeling and representation. However, it does not explicitly discuss spoken language models or full-duplex spoken language models as a core concept, nor does it mention synchronization, real-time interaction, or cognitive parallelism relevant to full-duplex communication. The work appears empirical and linguistic-analysis focused rather than centered on architectural or interactional aspects outlined in the inclusion criteria. Thus, it only partially aligns with the specified criteria.",
      "evaluation": 2
    },
    "round-A_JuniorMini_reasoning": "The study focuses on analyzing self-supervised speech models and their encoding of various linguistic properties at the word level, which relates to speech modeling and representation. However, it does not explicitly discuss spoken language models or full-duplex spoken language models as a core concept, nor does it mention synchronization, real-time interaction, or cognitive parallelism relevant to full-duplex communication. The work appears empirical and linguistic-analysis focused rather than centered on architectural or interactional aspects outlined in the inclusion criteria. Thus, it only partially aligns with the specified criteria.",
    "round-A_JuniorMini_evaluation": 2,
    "round-B_SeniorLead_output": {
      "reasoning": "The paper analyzes self-supervised speech models (S3Ms) and what they encode at the word level, but the abstract does not use the required terminology (spoken language model(s) / speech language model(s) or explicit “language modeling of speech”), nor does it address spoken-language-model architectures, speech-level language modeling, or any full‑duplex / real‑time interaction, synchronization or evaluation concerns mandated by the inclusion criteria. One junior reviewer favored inclusion but likely conflated S3Ms with SLMs; the other correctly noted the work is an empirical representation analysis rather than an SLM/FD‑SLM study, so it fails the core topical requirement and should be excluded (better to exclude).",
      "evaluation": 2
    },
    "round-B_SeniorLead_reasoning": "The paper analyzes self-supervised speech models (S3Ms) and what they encode at the word level, but the abstract does not use the required terminology (spoken language model(s) / speech language model(s) or explicit “language modeling of speech”), nor does it address spoken-language-model architectures, speech-level language modeling, or any full‑duplex / real‑time interaction, synchronization or evaluation concerns mandated by the inclusion criteria. One junior reviewer favored inclusion but likely conflated S3Ms with SLMs; the other correctly noted the work is an empirical representation analysis rather than an SLM/FD‑SLM study, so it fails the core topical requirement and should be excluded (better to exclude).",
    "round-B_SeniorLead_evaluation": 2,
    "final_verdict": "exclude (senior:2)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Perception of Phonological Assimilation by Neural Speech Recognition Models",
    "abstract": "Abstract Human listeners effortlessly compensate for phonological changes during speech perception, often unconsciously inferring the intended sounds. For example, listeners infer the underlying /n/ when hearing an utterance such as “clea[m] pan”, where [m] arises from place assimilation to the following labial [p]. This article explores how the neural speech recognition model Wav2Vec2 perceives assimilated sounds, and identifies the linguistic knowledge that is implemented by the model to compensate for assimilation during Automatic Speech Recognition (ASR). Using psycholinguistic stimuli, we systematically analyze how various linguistic context cues influence compensation patterns in the model’s output. Complementing these behavioral experiments, our probing experiments indicate that the model shifts its interpretation of assimilated sounds from their acoustic form to their underlying form in its final layers. Finally, our causal intervention experiments suggest that the model relies on minimal phonological context cues to accomplish this shift. These findings represent a step towards better understanding the similarities and differences in phonological processing between neural ASR models and humans.",
    "metadata": {
      "title": "Perception of Phonological Assimilation by Neural Speech Recognition Models",
      "summary": "Abstract Human listeners effortlessly compensate for phonological changes during speech perception, often unconsciously inferring the intended sounds. For example, listeners infer the underlying /n/ when hearing an utterance such as “clea[m] pan”, where [m] arises from place assimilation to the following labial [p]. This article explores how the neural speech recognition model Wav2Vec2 perceives assimilated sounds, and identifies the linguistic knowledge that is implemented by the model to compensate for assimilation during Automatic Speech Recognition (ASR). Using psycholinguistic stimuli, we systematically analyze how various linguistic context cues influence compensation patterns in the model’s output. Complementing these behavioral experiments, our probing experiments indicate that the model shifts its interpretation of assimilated sounds from their acoustic form to their underlying form in its final layers. Finally, our causal intervention experiments suggest that the model relies on minimal phonological context cues to accomplish this shift. These findings represent a step towards better understanding the similarities and differences in phonological processing between neural ASR models and humans.",
      "abstract": "Abstract Human listeners effortlessly compensate for phonological changes during speech perception, often unconsciously inferring the intended sounds. For example, listeners infer the underlying /n/ when hearing an utterance such as “clea[m] pan”, where [m] arises from place assimilation to the following labial [p]. This article explores how the neural speech recognition model Wav2Vec2 perceives assimilated sounds, and identifies the linguistic knowledge that is implemented by the model to compensate for assimilation during Automatic Speech Recognition (ASR). Using psycholinguistic stimuli, we systematically analyze how various linguistic context cues influence compensation patterns in the model’s output. Complementing these behavioral experiments, our probing experiments indicate that the model shifts its interpretation of assimilated sounds from their acoustic form to their underlying form in its final layers. Finally, our causal intervention experiments suggest that the model relies on minimal phonological context cues to accomplish this shift. These findings represent a step towards better understanding the similarities and differences in phonological processing between neural ASR models and humans.",
      "doi": "https://doi.org/10.1162/coli_a_00526",
      "openalex_id": "https://openalex.org/W4401117812",
      "arxiv_id": "",
      "publication_date": "2024-01-01",
      "published": "2024-01-01",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item analyzes a neural speech recognition model’s processing of phonological assimilation, treats speech as the core modeling object, and probes model representations/interventions (not only ASR accuracy), aligns with Spoken Language Model concepts, is from a 3-year window, and discusses model architecture/representations rather than being solely a downstream NLP task.",
      "evaluation": 4
    },
    "round-A_JuniorNano_reasoning": "The item analyzes a neural speech recognition model’s processing of phonological assimilation, treats speech as the core modeling object, and probes model representations/interventions (not only ASR accuracy), aligns with Spoken Language Model concepts, is from a 3-year window, and discusses model architecture/representations rather than being solely a downstream NLP task.",
    "round-A_JuniorNano_evaluation": 4,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on how the neural speech recognition model Wav2Vec2 handles phonological assimilation, with attention to linguistic context cues and model interpretation of speech sounds, but it primarily concentrates on ASR behavior and phonological processing rather than on spoken language models or full-duplex spoken language interaction capabilities. The abstract does not mention full-duplex interaction, speech language modeling architectures, synchronization mechanisms, or real-time interactive behaviors, and it lacks discussion of evaluation beyond speech recognition models. Thus, it fails to meet the core inclusion criteria centered on spoken language modeling and full-duplex interaction, while overlapping with exclusion criteria focused on ASR without synchronous dialogue or language modeling perspectives.",
      "evaluation": 2
    },
    "round-A_JuniorMini_reasoning": "The study focuses on how the neural speech recognition model Wav2Vec2 handles phonological assimilation, with attention to linguistic context cues and model interpretation of speech sounds, but it primarily concentrates on ASR behavior and phonological processing rather than on spoken language models or full-duplex spoken language interaction capabilities. The abstract does not mention full-duplex interaction, speech language modeling architectures, synchronization mechanisms, or real-time interactive behaviors, and it lacks discussion of evaluation beyond speech recognition models. Thus, it fails to meet the core inclusion criteria centered on spoken language modeling and full-duplex interaction, while overlapping with exclusion criteria focused on ASR without synchronous dialogue or language modeling perspectives.",
    "round-A_JuniorMini_evaluation": 2,
    "round-B_SeniorLead_output": {
      "reasoning": "Both junior reviewers flagged the paper’s focus on a speech model (Wav2Vec2) and probing of phonological processing, but disagreed on inclusion: JuniorNano favored inclusion (4) citing speech-model representation analysis, while JuniorMini recommended exclusion (2) because the work centers on ASR/phonological processing rather than spoken language modeling or interactive/full‑duplex capabilities. Per the inclusion criteria, the study must explicitly center on Spoken Language Models / speech-level language modeling (or equivalent phrasing) and address architectures, training, evaluation, or full‑duplex interaction/synchronization — requirements not met here. The abstract describes a psycholinguistic and probing study of ASR model representations and compensation for phonological assimilation, with no mention of SLM/FD‑SLM, speech language modeling, real‑time interaction, turn‑taking, or synchronization benchmarks. Therefore it should be excluded.",
      "evaluation": 2
    },
    "round-B_SeniorLead_reasoning": "Both junior reviewers flagged the paper’s focus on a speech model (Wav2Vec2) and probing of phonological processing, but disagreed on inclusion: JuniorNano favored inclusion (4) citing speech-model representation analysis, while JuniorMini recommended exclusion (2) because the work centers on ASR/phonological processing rather than spoken language modeling or interactive/full‑duplex capabilities. Per the inclusion criteria, the study must explicitly center on Spoken Language Models / speech-level language modeling (or equivalent phrasing) and address architectures, training, evaluation, or full‑duplex interaction/synchronization — requirements not met here. The abstract describes a psycholinguistic and probing study of ASR model representations and compensation for phonological assimilation, with no mention of SLM/FD‑SLM, speech language modeling, real‑time interaction, turn‑taking, or synchronization benchmarks. Therefore it should be excluded.",
    "round-B_SeniorLead_evaluation": 2,
    "final_verdict": "exclude (senior:2)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Homophone Disambiguation Reveals Patterns of Context Mixing in Speech Transformers",
    "abstract": "Transformers have become a key architecture in speech processing, but our understanding of how they build up representations of acoustic and linguistic structure is limited. In this study, we address this gap by investigating how measures of ‘context-mixing’ developed for text models can be adapted and applied to models of spoken language. We identify a linguistic phenomenon that is ideal for such a case study: homophony in French (e.g. livre vs livres), where a speech recognition model has to attend to syntactic cues such as determiners and pronouns in order to disambiguate spoken words with identical pronunciations and transcribe them while respecting grammatical agreement. We perform a series of controlled experiments and probing analyses on Transformer-based speech models. Our findings reveal that representations in encoder-only models effectively incorporate these cues to identify the correct transcription, whereas encoders in encoder-decoder models mainly relegate the task of capturing contextual dependencies to decoder modules.",
    "metadata": {
      "title": "Homophone Disambiguation Reveals Patterns of Context Mixing in Speech Transformers",
      "summary": "Transformers have become a key architecture in speech processing, but our understanding of how they build up representations of acoustic and linguistic structure is limited. In this study, we address this gap by investigating how measures of ‘context-mixing’ developed for text models can be adapted and applied to models of spoken language. We identify a linguistic phenomenon that is ideal for such a case study: homophony in French (e.g. livre vs livres), where a speech recognition model has to attend to syntactic cues such as determiners and pronouns in order to disambiguate spoken words with identical pronunciations and transcribe them while respecting grammatical agreement. We perform a series of controlled experiments and probing analyses on Transformer-based speech models. Our findings reveal that representations in encoder-only models effectively incorporate these cues to identify the correct transcription, whereas encoders in encoder-decoder models mainly relegate the task of capturing contextual dependencies to decoder modules.",
      "abstract": "Transformers have become a key architecture in speech processing, but our understanding of how they build up representations of acoustic and linguistic structure is limited. In this study, we address this gap by investigating how measures of ‘context-mixing’ developed for text models can be adapted and applied to models of spoken language. We identify a linguistic phenomenon that is ideal for such a case study: homophony in French (e.g. livre vs livres), where a speech recognition model has to attend to syntactic cues such as determiners and pronouns in order to disambiguate spoken words with identical pronunciations and transcribe them while respecting grammatical agreement. We perform a series of controlled experiments and probing analyses on Transformer-based speech models. Our findings reveal that representations in encoder-only models effectively incorporate these cues to identify the correct transcription, whereas encoders in encoder-decoder models mainly relegate the task of capturing contextual dependencies to decoder modules.",
      "doi": "https://doi.org/10.18653/v1/2023.emnlp-main.513",
      "openalex_id": "https://openalex.org/W4389520784",
      "arxiv_id": "",
      "publication_date": "2023-01-01",
      "published": "2023-01-01",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item centers on spoken language models and speech modeling (discussing architecture and representations of speech transformers for disambiguation), explicitly uses ‘spoken language’ in the abstract, and falls within the 3-year window with analyses beyond pure ASR metrics, meeting all inclusion criteria and avoiding exclusion criteria.",
      "evaluation": 5
    },
    "round-A_JuniorNano_reasoning": "The item centers on spoken language models and speech modeling (discussing architecture and representations of speech transformers for disambiguation), explicitly uses ‘spoken language’ in the abstract, and falls within the 3-year window with analyses beyond pure ASR metrics, meeting all inclusion criteria and avoiding exclusion criteria.",
    "round-A_JuniorNano_evaluation": 5,
    "round-A_JuniorMini_output": {
      "reasoning": "The study investigates Transformer-based speech models specifically on homophone disambiguation in spoken language, focusing on acoustic and linguistic structure representations within speech sequences, which aligns with Spoken Language Models as core objects with modeling and reasoning on speech. However, it does not mention full-duplex interaction, cognitive parallelism, or synchronous interaction topics central to the inclusion criteria, nor does it explicitly address learned or engineered synchronization or real-time interaction capabilities. The focus is more on analyzing context mixing in speech recognition rather than dialogue management or multi-turn interactive behaviors. Therefore, while it partially meets some inclusion criteria, it misses critical aspects about interaction and behavioral arbitration required for full inclusion and does not fall under exclusion criteria strictly but lacks full alignment with all inclusion points.",
      "evaluation": 3
    },
    "round-A_JuniorMini_reasoning": "The study investigates Transformer-based speech models specifically on homophone disambiguation in spoken language, focusing on acoustic and linguistic structure representations within speech sequences, which aligns with Spoken Language Models as core objects with modeling and reasoning on speech. However, it does not mention full-duplex interaction, cognitive parallelism, or synchronous interaction topics central to the inclusion criteria, nor does it explicitly address learned or engineered synchronization or real-time interaction capabilities. The focus is more on analyzing context mixing in speech recognition rather than dialogue management or multi-turn interactive behaviors. Therefore, while it partially meets some inclusion criteria, it misses critical aspects about interaction and behavioral arbitration required for full inclusion and does not fall under exclusion criteria strictly but lacks full alignment with all inclusion points.",
    "round-A_JuniorMini_evaluation": 3,
    "round-B_SeniorLead_output": {
      "reasoning": "Integrating the two junior reviews: Junior A rated 5 noting the paper explicitly studies spoken-language/speech transformer models and representation-level analyses beyond pure ASR metrics; Junior B rated 3 pointing out the work does not address full‑duplex interaction or real‑time synchronization. The abstract explicitly uses \"models of spoken language\"/\"speech models,\" analyzes Transformer-based speech models' internal representations (context‑mixing and probing) rather than reporting only WER, and compares encoder-only vs encoder–decoder behavior — so it treats speech as the core modeling object and discusses model architecture and analysis. It does not cover full‑duplex interaction, overlap management, or low‑latency dialogue arbitration, but those are special topics within the broader SLM theme rather than mandatory exclusion criteria here. It therefore meets the main inclusion requirement (Spoken Language Models as the core focus with architectural/analysis content) and does not violate the exclusion criteria (not just ASR metrics or purely text NLP).",
      "evaluation": 5
    },
    "round-B_SeniorLead_reasoning": "Integrating the two junior reviews: Junior A rated 5 noting the paper explicitly studies spoken-language/speech transformer models and representation-level analyses beyond pure ASR metrics; Junior B rated 3 pointing out the work does not address full‑duplex interaction or real‑time synchronization. The abstract explicitly uses \"models of spoken language\"/\"speech models,\" analyzes Transformer-based speech models' internal representations (context‑mixing and probing) rather than reporting only WER, and compares encoder-only vs encoder–decoder behavior — so it treats speech as the core modeling object and discusses model architecture and analysis. It does not cover full‑duplex interaction, overlap management, or low‑latency dialogue arbitration, but those are special topics within the broader SLM theme rather than mandatory exclusion criteria here. It therefore meets the main inclusion requirement (Spoken Language Models as the core focus with architectural/analysis content) and does not violate the exclusion criteria (not just ASR metrics or purely text NLP).",
    "round-B_SeniorLead_evaluation": 5,
    "final_verdict": "include (senior:5)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Human-like Linguistic Biases in Neural Speech Models: Phonetic Categorization and Phonotactic Constraints in Wav2Vec2.0",
    "abstract": "What do deep neural speech models know about phonology? Existing work has\\nexamined the encoding of individual linguistic units such as phonemes in these\\nmodels. Here we investigate interactions between units. Inspired by classic\\nexperiments on human speech perception, we study how Wav2Vec2 resolves\\nphonotactic constraints. We synthesize sounds on an acoustic continuum between\\n/l/ and /r/ and embed them in controlled contexts where only /l/, only /r/, or\\nneither occur in English. Like humans, Wav2Vec2 models show a bias towards the\\nphonotactically admissable category in processing such ambiguous sounds. Using\\nsimple measures to analyze model internals on the level of individual stimuli,\\nwe find that this bias emerges in early layers of the model's Transformer\\nmodule. This effect is amplified by ASR finetuning but also present in fully\\nself-supervised models. Our approach demonstrates how controlled stimulus\\ndesigns can help localize specific linguistic knowledge in neural speech\\nmodels.\\n",
    "metadata": {
      "title": "Human-like Linguistic Biases in Neural Speech Models: Phonetic Categorization and Phonotactic Constraints in Wav2Vec2.0",
      "summary": "What do deep neural speech models know about phonology? Existing work has\\nexamined the encoding of individual linguistic units such as phonemes in these\\nmodels. Here we investigate interactions between units. Inspired by classic\\nexperiments on human speech perception, we study how Wav2Vec2 resolves\\nphonotactic constraints. We synthesize sounds on an acoustic continuum between\\n/l/ and /r/ and embed them in controlled contexts where only /l/, only /r/, or\\nneither occur in English. Like humans, Wav2Vec2 models show a bias towards the\\nphonotactically admissable category in processing such ambiguous sounds. Using\\nsimple measures to analyze model internals on the level of individual stimuli,\\nwe find that this bias emerges in early layers of the model's Transformer\\nmodule. This effect is amplified by ASR finetuning but also present in fully\\nself-supervised models. Our approach demonstrates how controlled stimulus\\ndesigns can help localize specific linguistic knowledge in neural speech\\nmodels.\\n",
      "abstract": "What do deep neural speech models know about phonology? Existing work has\\nexamined the encoding of individual linguistic units such as phonemes in these\\nmodels. Here we investigate interactions between units. Inspired by classic\\nexperiments on human speech perception, we study how Wav2Vec2 resolves\\nphonotactic constraints. We synthesize sounds on an acoustic continuum between\\n/l/ and /r/ and embed them in controlled contexts where only /l/, only /r/, or\\nneither occur in English. Like humans, Wav2Vec2 models show a bias towards the\\nphonotactically admissable category in processing such ambiguous sounds. Using\\nsimple measures to analyze model internals on the level of individual stimuli,\\nwe find that this bias emerges in early layers of the model's Transformer\\nmodule. This effect is amplified by ASR finetuning but also present in fully\\nself-supervised models. Our approach demonstrates how controlled stimulus\\ndesigns can help localize specific linguistic knowledge in neural speech\\nmodels.\\n",
      "doi": "https://doi.org/10.21437/interspeech.2024-2490",
      "openalex_id": "https://openalex.org/W4400376370",
      "arxiv_id": "",
      "publication_date": "2024-09-01",
      "published": "2024-09-01",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This item analyzes a spoken-language model (Wav2Vec2) and its internal phonological knowledge, discusses model architecture/training/evaluation aspects, is within the last 3 years, and does not reduce to single-task ASR or non-academic content, thus meeting all inclusion criteria and none of the exclusions.",
      "evaluation": 5
    },
    "round-A_JuniorNano_reasoning": "This item analyzes a spoken-language model (Wav2Vec2) and its internal phonological knowledge, discusses model architecture/training/evaluation aspects, is within the last 3 years, and does not reduce to single-task ASR or non-academic content, thus meeting all inclusion criteria and none of the exclusions.",
    "round-A_JuniorNano_evaluation": 5,
    "round-A_JuniorMini_output": {
      "reasoning": "The study investigates Wav2Vec2, a neural speech model, focusing on phonetic categorization and phonotactic constraints, emphasizing internal linguistic knowledge relevant to speech representation. However, it does not explicitly discuss spoken language models or full-duplex spoken language models, nor does it address full-duplex interaction capabilities, synchronous interaction, cognitive parallelism, or low latency real-time interaction; it does analyze speech-based models but does not meet the specific inclusion criteria centered on spoken language modeling as defined. Therefore, it does not fully meet all inclusion criteria and does not particularly meet any exclusion criteria explicitly, but it misses key thematic elements required for inclusion.",
      "evaluation": 2
    },
    "round-A_JuniorMini_reasoning": "The study investigates Wav2Vec2, a neural speech model, focusing on phonetic categorization and phonotactic constraints, emphasizing internal linguistic knowledge relevant to speech representation. However, it does not explicitly discuss spoken language models or full-duplex spoken language models, nor does it address full-duplex interaction capabilities, synchronous interaction, cognitive parallelism, or low latency real-time interaction; it does analyze speech-based models but does not meet the specific inclusion criteria centered on spoken language modeling as defined. Therefore, it does not fully meet all inclusion criteria and does not particularly meet any exclusion criteria explicitly, but it misses key thematic elements required for inclusion.",
    "round-A_JuniorMini_evaluation": 2,
    "round-B_SeniorLead_output": {
      "reasoning": "JuniorNano favored inclusion because the paper analyzes internal phonological knowledge of a speech model (Wav2Vec2.0); JuniorMini opposed inclusion because the paper does not address the specific SLM/FD‑SLM framing (full‑duplex interaction, synchronization, latency, or spoken-language modeling objectives) required by the topic. While the title/abstract clearly study neural speech representations and phonology in Wav2Vec2 (so it is relevant to speech representation research), the work does not treat Spoken Language Models or Full‑Duplex SLM capabilities, nor does it discuss interaction, low‑latency or synchronization evaluation — therefore it fails to meet the inclusion requirements and meets exclusion conditions. Brief explanation (1 sentence): I rate this a 2 (better to exclude) because, despite analyzing speech models, the manuscript does not address SLM/FD‑SLM objectives (full‑duplex interaction, real‑time synchronization, or spoken‑language modeling) required for inclusion.",
      "evaluation": 2
    },
    "round-B_SeniorLead_reasoning": "JuniorNano favored inclusion because the paper analyzes internal phonological knowledge of a speech model (Wav2Vec2.0); JuniorMini opposed inclusion because the paper does not address the specific SLM/FD‑SLM framing (full‑duplex interaction, synchronization, latency, or spoken-language modeling objectives) required by the topic. While the title/abstract clearly study neural speech representations and phonology in Wav2Vec2 (so it is relevant to speech representation research), the work does not treat Spoken Language Models or Full‑Duplex SLM capabilities, nor does it discuss interaction, low‑latency or synchronization evaluation — therefore it fails to meet the inclusion requirements and meets exclusion conditions. Brief explanation (1 sentence): I rate this a 2 (better to exclude) because, despite analyzing speech models, the manuscript does not address SLM/FD‑SLM objectives (full‑duplex interaction, real‑time synchronization, or spoken‑language modeling) required for inclusion.",
    "round-B_SeniorLead_evaluation": 2,
    "final_verdict": "exclude (senior:2)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "The Roles of Neural Networks in Language Acquisition",
    "abstract": "ABSTRACT How can modern neural networks like language models be useful to the field of language acquisition, and more broadly cognitive science, if they are not a priori designed to be cognitive models? As developments towards natural language understanding and generation have improved leaps and bounds, with models like GPT‐4, the question of how they can inform our understanding of human language acquisition has re‐emerged. As such, it is critical to examine how in practice linking hypotheses between models and human learners can be safely established. To address these questions, we propose a model taxonomy, including four modelling approaches, each having differing goals, from exploratory hypothesis generation to hypothesis differentiation and testing. We show how the goals of these approaches align with the overarching goals of science and linguistics by connecting our taxonomy to the realist versus instrumentalist approaches in philosophy of science. We survey recent work having adopted each of our modelling approaches and address the importance of computational modelling in language acquisition studies.",
    "metadata": {
      "title": "The Roles of Neural Networks in Language Acquisition",
      "summary": "ABSTRACT How can modern neural networks like language models be useful to the field of language acquisition, and more broadly cognitive science, if they are not a priori designed to be cognitive models? As developments towards natural language understanding and generation have improved leaps and bounds, with models like GPT‐4, the question of how they can inform our understanding of human language acquisition has re‐emerged. As such, it is critical to examine how in practice linking hypotheses between models and human learners can be safely established. To address these questions, we propose a model taxonomy, including four modelling approaches, each having differing goals, from exploratory hypothesis generation to hypothesis differentiation and testing. We show how the goals of these approaches align with the overarching goals of science and linguistics by connecting our taxonomy to the realist versus instrumentalist approaches in philosophy of science. We survey recent work having adopted each of our modelling approaches and address the importance of computational modelling in language acquisition studies.",
      "abstract": "ABSTRACT How can modern neural networks like language models be useful to the field of language acquisition, and more broadly cognitive science, if they are not a priori designed to be cognitive models? As developments towards natural language understanding and generation have improved leaps and bounds, with models like GPT‐4, the question of how they can inform our understanding of human language acquisition has re‐emerged. As such, it is critical to examine how in practice linking hypotheses between models and human learners can be safely established. To address these questions, we propose a model taxonomy, including four modelling approaches, each having differing goals, from exploratory hypothesis generation to hypothesis differentiation and testing. We show how the goals of these approaches align with the overarching goals of science and linguistics by connecting our taxonomy to the realist versus instrumentalist approaches in philosophy of science. We survey recent work having adopted each of our modelling approaches and address the importance of computational modelling in language acquisition studies.",
      "doi": "https://doi.org/10.1111/lnc3.70001",
      "openalex_id": "https://openalex.org/W4403779633",
      "arxiv_id": "",
      "publication_date": "2024-10-24",
      "published": "2024-10-24",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item centers on neural networks for language acquisition in general, not on Spoken Language Models or speech-centric modeling with end-to-end/Speech-LLM frameworks, lacks emphasis on speech signals, synchronization, or speech-specific evaluation benchmarks, thus failing the specified inclusion criteria and not aligning with exclusion scope.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item centers on neural networks for language acquisition in general, not on Spoken Language Models or speech-centric modeling with end-to-end/Speech-LLM frameworks, lacks emphasis on speech signals, synchronization, or speech-specific evaluation benchmarks, thus failing the specified inclusion criteria and not aligning with exclusion scope.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The title and abstract primarily discuss the roles of neural networks in language acquisition and cognitive science without mentioning spoken language models (SLMs), speech language models, or full-duplex spoken language models, nor do they focus on modeling speech signals or real-time interactive speech modeling aspects such as low latency, speech overlap handling, or cognitive parallelism. The research described does not appear to meet the inclusion criteria of emphasizing spoken language modeling at the acoustic or speech sequence level and does not provide specific references to architecture, training, or evaluation of such models within a recent timeframe. Therefore, it does not satisfy the inclusion requirements nor appear to meet the exclusion criteria relating to non-spoken language modeling focus, indicating it should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The title and abstract primarily discuss the roles of neural networks in language acquisition and cognitive science without mentioning spoken language models (SLMs), speech language models, or full-duplex spoken language models, nor do they focus on modeling speech signals or real-time interactive speech modeling aspects such as low latency, speech overlap handling, or cognitive parallelism. The research described does not appear to meet the inclusion criteria of emphasizing spoken language modeling at the acoustic or speech sequence level and does not provide specific references to architecture, training, or evaluation of such models within a recent timeframe. Therefore, it does not satisfy the inclusion requirements nor appear to meet the exclusion criteria relating to non-spoken language modeling focus, indicating it should be excluded.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Analysis of Self-Supervised Speech Models on Children’s Speech and Infant Vocalizations",
    "abstract": "To understand why self-supervised learning (SSL) models have empirically achieved strong performances on several speech-processing downstream tasks, numerous studies have focused on analyzing the encoded information of the SSL layer representations in adult speech. Limited work has investigated how pre-training and fine-tuning affect SSL models encoding children's speech and vocalizations. In this study, we aim to bridge this gap by probing SSL models on two relevant downstream tasks: (1) phoneme recognition (PR) on the speech of adults, older children (8-10 years old), and younger children (1-4 years old), and (2) vocalization classification (VC) distinguishing cry, fuss, and babble for infants under 14 months old. For younger children's PR, the superiority of fine-tuned SSL models is largely due to their ability to learn features that represent older children's speech and then adapt those features to the speech of younger children. For infant VC, SSL models pre-trained on large-scale home recordings learn to leverage phonetic representations at middle layers, and thereby enhance the performance of this task.",
    "metadata": {
      "title": "Analysis of Self-Supervised Speech Models on Children’s Speech and Infant Vocalizations",
      "summary": "To understand why self-supervised learning (SSL) models have empirically achieved strong performances on several speech-processing downstream tasks, numerous studies have focused on analyzing the encoded information of the SSL layer representations in adult speech. Limited work has investigated how pre-training and fine-tuning affect SSL models encoding children's speech and vocalizations. In this study, we aim to bridge this gap by probing SSL models on two relevant downstream tasks: (1) phoneme recognition (PR) on the speech of adults, older children (8-10 years old), and younger children (1-4 years old), and (2) vocalization classification (VC) distinguishing cry, fuss, and babble for infants under 14 months old. For younger children's PR, the superiority of fine-tuned SSL models is largely due to their ability to learn features that represent older children's speech and then adapt those features to the speech of younger children. For infant VC, SSL models pre-trained on large-scale home recordings learn to leverage phonetic representations at middle layers, and thereby enhance the performance of this task.",
      "abstract": "To understand why self-supervised learning (SSL) models have empirically achieved strong performances on several speech-processing downstream tasks, numerous studies have focused on analyzing the encoded information of the SSL layer representations in adult speech. Limited work has investigated how pre-training and fine-tuning affect SSL models encoding children's speech and vocalizations. In this study, we aim to bridge this gap by probing SSL models on two relevant downstream tasks: (1) phoneme recognition (PR) on the speech of adults, older children (8-10 years old), and younger children (1-4 years old), and (2) vocalization classification (VC) distinguishing cry, fuss, and babble for infants under 14 months old. For younger children's PR, the superiority of fine-tuned SSL models is largely due to their ability to learn features that represent older children's speech and then adapt those features to the speech of younger children. For infant VC, SSL models pre-trained on large-scale home recordings learn to leverage phonetic representations at middle layers, and thereby enhance the performance of this task.",
      "doi": "https://doi.org/10.1109/icasspw62465.2024.10626416",
      "openalex_id": "https://openalex.org/W4401609323",
      "arxiv_id": "",
      "publication_date": "2024-04-14",
      "published": "2024-04-14",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "本項目聚焦自我監督語音模型在兒童語音與嬰兒聲音的表徵分析，涉及語音模型架構、訓練與評估，且非僅限單一ASR/TTS任務；符合 Spoken Language Models / Speech Language Models 的核心概念，且在過去3年內發布，因而符合納入條件。",
      "evaluation": 5
    },
    "round-A_JuniorNano_reasoning": "本項目聚焦自我監督語音模型在兒童語音與嬰兒聲音的表徵分析，涉及語音模型架構、訓練與評估，且非僅限單一ASR/TTS任務；符合 Spoken Language Models / Speech Language Models 的核心概念，且在過去3年內發布，因而符合納入條件。",
    "round-A_JuniorNano_evaluation": 5,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on analyzing self-supervised speech models on children’s speech and infant vocalizations, which involves phoneme recognition and vocalization classification, but it lacks explicit discussion on Spoken Language Models or Full-Duplex Spoken Language Models, nor does it address aspects like turn-taking, cognitive parallelism, or synchronization in speech interaction; it also does not mention any speech language model terminology or describe evaluation concerning real-time interactive behaviors, thus falling outside the defined inclusion criteria and partially matching exclusion criteria related to a focus on single downstream tasks and absence of interactive modeling aspects.",
      "evaluation": 2
    },
    "round-A_JuniorMini_reasoning": "The study focuses on analyzing self-supervised speech models on children’s speech and infant vocalizations, which involves phoneme recognition and vocalization classification, but it lacks explicit discussion on Spoken Language Models or Full-Duplex Spoken Language Models, nor does it address aspects like turn-taking, cognitive parallelism, or synchronization in speech interaction; it also does not mention any speech language model terminology or describe evaluation concerning real-time interactive behaviors, thus falling outside the defined inclusion criteria and partially matching exclusion criteria related to a focus on single downstream tasks and absence of interactive modeling aspects.",
    "round-A_JuniorMini_evaluation": 2,
    "round-B_SeniorLead_output": {
      "reasoning": "JuniorNano judged inclusion but appears to conflate general SSL speech modeling with the required Spoken Language Model (SLM) scope; JuniorMini correctly noted the abstract focuses on representation analysis for phoneme recognition and infant vocalization classification without discussing spoken language modeling, full-duplex interaction, synchronization, or other SLM/FD-SLM concepts. The abstract lacks the required terminology (e.g., “spoken language model(s)” or equivalent), does not address interactive/full‑duplex behaviors or synchronization, and centers on downstream tasks (PR and VC), which falls under the exclusion criteria (single-task ASR-like focus). Explanation: better to exclude because the study does not meet the core SLM/FD-SLM inclusion requirements and instead investigates SSL representations for downstream speech tasks.",
      "evaluation": 2
    },
    "round-B_SeniorLead_reasoning": "JuniorNano judged inclusion but appears to conflate general SSL speech modeling with the required Spoken Language Model (SLM) scope; JuniorMini correctly noted the abstract focuses on representation analysis for phoneme recognition and infant vocalization classification without discussing spoken language modeling, full-duplex interaction, synchronization, or other SLM/FD-SLM concepts. The abstract lacks the required terminology (e.g., “spoken language model(s)” or equivalent), does not address interactive/full‑duplex behaviors or synchronization, and centers on downstream tasks (PR and VC), which falls under the exclusion criteria (single-task ASR-like focus). Explanation: better to exclude because the study does not meet the core SLM/FD-SLM inclusion requirements and instead investigates SSL representations for downstream speech tasks.",
    "round-B_SeniorLead_evaluation": 2,
    "final_verdict": "exclude (senior:2)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Modeling the initial state of early phonetic learning in infants",
    "abstract": "What are the necessary conditions to acquire language? Do infants rely on simple statistical mechanisms, or do they come pre-wired with innate capabilities allowing them to learn their native language(s)? Previous modeling studies have shown that unsupervised learning algorithms could reproduce some aspects of infant phonetic learning. Despite these successes, algorithms still fail to reproduce the learning trajectories observed in infants. Here, we advocate that this failure is partly due to a wrong initial state. Contrary to infants, unsupervised learning algorithms start with little to no prior knowledge of speech sounds. In this work, we propose a modeling approach to investigate the relative contribution of innate factors and language experience in infant speech perception. Our approach allows us to investigate theories hypothesizing a more significant role of innate factors, offering new modeling opportunities for studying infant language acquisition.",
    "metadata": {
      "title": "Modeling the initial state of early phonetic learning in infants",
      "summary": "What are the necessary conditions to acquire language? Do infants rely on simple statistical mechanisms, or do they come pre-wired with innate capabilities allowing them to learn their native language(s)? Previous modeling studies have shown that unsupervised learning algorithms could reproduce some aspects of infant phonetic learning. Despite these successes, algorithms still fail to reproduce the learning trajectories observed in infants. Here, we advocate that this failure is partly due to a wrong initial state. Contrary to infants, unsupervised learning algorithms start with little to no prior knowledge of speech sounds. In this work, we propose a modeling approach to investigate the relative contribution of innate factors and language experience in infant speech perception. Our approach allows us to investigate theories hypothesizing a more significant role of innate factors, offering new modeling opportunities for studying infant language acquisition.",
      "abstract": "What are the necessary conditions to acquire language? Do infants rely on simple statistical mechanisms, or do they come pre-wired with innate capabilities allowing them to learn their native language(s)? Previous modeling studies have shown that unsupervised learning algorithms could reproduce some aspects of infant phonetic learning. Despite these successes, algorithms still fail to reproduce the learning trajectories observed in infants. Here, we advocate that this failure is partly due to a wrong initial state. Contrary to infants, unsupervised learning algorithms start with little to no prior knowledge of speech sounds. In this work, we propose a modeling approach to investigate the relative contribution of innate factors and language experience in infant speech perception. Our approach allows us to investigate theories hypothesizing a more significant role of innate factors, offering new modeling opportunities for studying infant language acquisition.",
      "doi": "https://doi.org/10.31234/osf.io/gc5kp",
      "openalex_id": "https://openalex.org/W4395036961",
      "arxiv_id": "",
      "publication_date": "2024-04-23",
      "published": "2024-04-23",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item focuses on infant phonetic learning and innate vs. statistical mechanisms, not on Spoken Language Models or FD-SLMs as core modeling objects with architecture/training/evaluation. Although within a recent timeframe, it does not align with the SLM/FD-SLM-centric criteria and lacks evaluation of multi-turn interactive speech systems or synchronization/interaction aspects. Therefore it should be excluded.",
      "evaluation": 2
    },
    "round-A_JuniorNano_reasoning": "The item focuses on infant phonetic learning and innate vs. statistical mechanisms, not on Spoken Language Models or FD-SLMs as core modeling objects with architecture/training/evaluation. Although within a recent timeframe, it does not align with the SLM/FD-SLM-centric criteria and lacks evaluation of multi-turn interactive speech systems or synchronization/interaction aspects. Therefore it should be excluded.",
    "round-A_JuniorNano_evaluation": 2,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on modeling early phonetic learning in infants using unsupervised learning algorithms to investigate innate factors and language experience in speech perception. However, it does not address Spoken Language Models (SLMs) or Full-Duplex Spoken Language Models (FD-SLMs), nor does it emphasize speech language modeling architectures, training, evaluation, synchronous interaction, or low-latency full-duplex communication as required by the inclusion criteria. It also lacks mention of speech–LLM or related hybrid architectures and does not align with the specified topic scope of spoken language modeling beyond phonetic perceptual modeling.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on modeling early phonetic learning in infants using unsupervised learning algorithms to investigate innate factors and language experience in speech perception. However, it does not address Spoken Language Models (SLMs) or Full-Duplex Spoken Language Models (FD-SLMs), nor does it emphasize speech language modeling architectures, training, evaluation, synchronous interaction, or low-latency full-duplex communication as required by the inclusion criteria. It also lacks mention of speech–LLM or related hybrid architectures and does not align with the specified topic scope of spoken language modeling beyond phonetic perceptual modeling.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:2)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "The difficulty and importance of estimating the lower and upper bounds of infant speech exposure",
    "abstract": "Estimates of infants’ language exposure are necessary for computational studies that attempt to model and learn from infant language experiences. However, there are no well-established input estimates usable for this purpose. This paper explores empirical data on infant language exposure across various cultural settings to derive plausible limits on the speech exposure infants might receive during their first years of life. First, we argue that several assumptions lack unanimous agreement and demonstrate that existing data are problematic in multiple ways. Integrating these uncertainties and published information, we find estimates that range from 1 to 3,300 hours per year. We end by discussing how such a large possible range may impact evaluation of the plausibility and benchmarking of computational models.",
    "metadata": {
      "title": "The difficulty and importance of estimating the lower and upper bounds of infant speech exposure",
      "summary": "Estimates of infants’ language exposure are necessary for computational studies that attempt to model and learn from infant language experiences. However, there are no well-established input estimates usable for this purpose. This paper explores empirical data on infant language exposure across various cultural settings to derive plausible limits on the speech exposure infants might receive during their first years of life. First, we argue that several assumptions lack unanimous agreement and demonstrate that existing data are problematic in multiple ways. Integrating these uncertainties and published information, we find estimates that range from 1 to 3,300 hours per year. We end by discussing how such a large possible range may impact evaluation of the plausibility and benchmarking of computational models.",
      "abstract": "Estimates of infants’ language exposure are necessary for computational studies that attempt to model and learn from infant language experiences. However, there are no well-established input estimates usable for this purpose. This paper explores empirical data on infant language exposure across various cultural settings to derive plausible limits on the speech exposure infants might receive during their first years of life. First, we argue that several assumptions lack unanimous agreement and demonstrate that existing data are problematic in multiple ways. Integrating these uncertainties and published information, we find estimates that range from 1 to 3,300 hours per year. We end by discussing how such a large possible range may impact evaluation of the plausibility and benchmarking of computational models.",
      "doi": "https://doi.org/10.31234/osf.io/29muj",
      "openalex_id": "https://openalex.org/W4399742250",
      "arxiv_id": "",
      "publication_date": "2024-06-17",
      "published": "2024-06-17",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The abstract concerns estimating infant speech exposure for benchmarking computational models but does not focus on Spoken Language Models or speech-language modeling architectures/training/evaluation, thus failing the inclusion criteria and aligning with exclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The abstract concerns estimating infant speech exposure for benchmarking computational models but does not focus on Spoken Language Models or speech-language modeling architectures/training/evaluation, thus failing the inclusion criteria and aligning with exclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on estimating infants' speech exposure, which is an empirical data analysis on language input quantity and not directly related to spoken language modeling architectures, training, inference, or evaluation, nor does it discuss spoken language models, full-duplex interaction capabilities, synchronization, or relevant evaluations within the specified research scope. Therefore, it does not meet the inclusion criteria centered on spoken language model research with speech as the core modeling target, nor does it violate any exclusion criteria by focusing solely on linguistic exposure estimation without involving spoken language modeling technologies.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on estimating infants' speech exposure, which is an empirical data analysis on language input quantity and not directly related to spoken language modeling architectures, training, inference, or evaluation, nor does it discuss spoken language models, full-duplex interaction capabilities, synchronization, or relevant evaluations within the specified research scope. Therefore, it does not meet the inclusion criteria centered on spoken language model research with speech as the core modeling target, nor does it violate any exclusion criteria by focusing solely on linguistic exposure estimation without involving spoken language modeling technologies.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "SALM: Speech-Augmented Language Model with in-Context Learning for Speech Recognition and Translation",
    "abstract": "We present a novel Speech Augmented Language Model (SALM) with multitask and in-context learning capabilities. SALM comprises a frozen text LLM, a audio encoder, a modality adapter module, and LoRA layers to accommodate speech input and associated task instructions. The unified SALM not only achieves performance on par with task-specific Conformer baselines for Automatic Speech Recognition (ASR) and Speech Translation (AST), but also exhibits zero-shot in-context learning capabilities, demonstrated through keyword-boosting task for ASR and AST. Moreover, speech supervised in-context training is proposed to bridge the gap between LLM training and downstream speech tasks, which further boosts the in-context learning ability of speech-to-text models. Proposed model is open-sourced via NeMo toolkit <sup xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">1</sup> .",
    "metadata": {
      "title": "SALM: Speech-Augmented Language Model with in-Context Learning for Speech Recognition and Translation",
      "summary": "We present a novel Speech Augmented Language Model (SALM) with multitask and in-context learning capabilities. SALM comprises a frozen text LLM, a audio encoder, a modality adapter module, and LoRA layers to accommodate speech input and associated task instructions. The unified SALM not only achieves performance on par with task-specific Conformer baselines for Automatic Speech Recognition (ASR) and Speech Translation (AST), but also exhibits zero-shot in-context learning capabilities, demonstrated through keyword-boosting task for ASR and AST. Moreover, speech supervised in-context training is proposed to bridge the gap between LLM training and downstream speech tasks, which further boosts the in-context learning ability of speech-to-text models. Proposed model is open-sourced via NeMo toolkit <sup xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">1</sup> .",
      "abstract": "We present a novel Speech Augmented Language Model (SALM) with multitask and in-context learning capabilities. SALM comprises a frozen text LLM, a audio encoder, a modality adapter module, and LoRA layers to accommodate speech input and associated task instructions. The unified SALM not only achieves performance on par with task-specific Conformer baselines for Automatic Speech Recognition (ASR) and Speech Translation (AST), but also exhibits zero-shot in-context learning capabilities, demonstrated through keyword-boosting task for ASR and AST. Moreover, speech supervised in-context training is proposed to bridge the gap between LLM training and downstream speech tasks, which further boosts the in-context learning ability of speech-to-text models. Proposed model is open-sourced via NeMo toolkit <sup xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">1</sup> .",
      "doi": "https://doi.org/10.1109/icassp48485.2024.10447553",
      "openalex_id": "https://openalex.org/W4392903872",
      "arxiv_id": "",
      "publication_date": "2024-03-18",
      "published": "2024-03-18",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "SALM clearly targets a Spoken Language Model by integrating an audio encoder with a frozen LLM and modular adapters to perform ASR and speech translation, discusses architecture, training (speech-supervised in-context training), and evaluation, and uses the term 'Speech-Augmented Language Model' in the title/abstract, meeting the inclusion criteria and within the past 3 years.",
      "evaluation": 5
    },
    "round-A_JuniorNano_reasoning": "SALM clearly targets a Spoken Language Model by integrating an audio encoder with a frozen LLM and modular adapters to perform ASR and speech translation, discusses architecture, training (speech-supervised in-context training), and evaluation, and uses the term 'Speech-Augmented Language Model' in the title/abstract, meeting the inclusion criteria and within the past 3 years.",
    "round-A_JuniorNano_evaluation": 5,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper presents SALM, a Speech-Augmented Language Model incorporating speech input and multitask learning, including speech recognition and translation, with a unified model that also supports in-context learning for speech tasks. However, the abstract focuses mainly on performance in ASR and AST tasks and does not explicitly mention full-duplex spoken language models, synchronization, turn-taking, or interactive dialogue behavior arbitration. The study discusses a novel speech-language model architecture, but does not clearly address the core inclusion criteria such as explicit spoken language modeling, full-duplex interaction capabilities, or multidimensional evaluation beyond traditional speech recognition metrics. Thus, while it involves speech-LLM integration, it appears more aligned with advancing ASR and speech translation rather than the comprehensive spoken language model research focus defined here.",
      "evaluation": 3
    },
    "round-A_JuniorMini_reasoning": "The paper presents SALM, a Speech-Augmented Language Model incorporating speech input and multitask learning, including speech recognition and translation, with a unified model that also supports in-context learning for speech tasks. However, the abstract focuses mainly on performance in ASR and AST tasks and does not explicitly mention full-duplex spoken language models, synchronization, turn-taking, or interactive dialogue behavior arbitration. The study discusses a novel speech-language model architecture, but does not clearly address the core inclusion criteria such as explicit spoken language modeling, full-duplex interaction capabilities, or multidimensional evaluation beyond traditional speech recognition metrics. Thus, while it involves speech-LLM integration, it appears more aligned with advancing ASR and speech translation rather than the comprehensive spoken language model research focus defined here.",
    "round-A_JuniorMini_evaluation": 3,
    "round-B_SeniorLead_output": {
      "reasoning": "The paper proposes a Speech-Augmented Language Model (SALM) that integrates an audio encoder and modality adapters to enable ASR and speech translation with in-context learning, and one junior reviewer rated it highly for using a speech-LLM integration; however, the abstract focuses on ASR/AST performance and speech-supervised in-context training without framing speech as the primary language-modeling object (e.g., discrete/continuous speech token LM), nor does it address full‑duplex interaction, synchronization, overlap handling, low-latency real‑time arbitration, or multidimensional evaluations required by this topic—matching the concerns of the second junior reviewer—so it meets the superficial “speech-augmented” wording but fails the substantive inclusion requirements and triggers exclusion criteria (primarily single-task ASR/AST focus and treating speech mainly as LLM input).",
      "evaluation": 2
    },
    "round-B_SeniorLead_reasoning": "The paper proposes a Speech-Augmented Language Model (SALM) that integrates an audio encoder and modality adapters to enable ASR and speech translation with in-context learning, and one junior reviewer rated it highly for using a speech-LLM integration; however, the abstract focuses on ASR/AST performance and speech-supervised in-context training without framing speech as the primary language-modeling object (e.g., discrete/continuous speech token LM), nor does it address full‑duplex interaction, synchronization, overlap handling, low-latency real‑time arbitration, or multidimensional evaluations required by this topic—matching the concerns of the second junior reviewer—so it meets the superficial “speech-augmented” wording but fails the substantive inclusion requirements and triggers exclusion criteria (primarily single-task ASR/AST focus and treating speech mainly as LLM input).",
    "round-B_SeniorLead_evaluation": 2,
    "final_verdict": "exclude (senior:2)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Optimizing feature fusion for improved zero-shot adaptation in text-to-speech synthesis",
    "abstract": "Abstract In the era of advanced text-to-speech (TTS) systems capable of generating high-fidelity, human-like speech by referring a reference speech, voice cloning (VC), or zero-shot TTS (ZS-TTS), stands out as an important subtask. A primary challenge in VC is maintaining speech quality and speaker similarity with limited reference data for a specific speaker. However, existing VC systems often rely on naive combinations of embedded speaker vectors for speaker control, which compromises the capture of speaking style, voice print, and semantic accuracy. To overcome this, we introduce the Two-branch Speaker Control Module (TSCM), a novel and highly adaptable voice cloning module designed to precisely processing speaker or style control for a target speaker. Our method uses an advanced fusion of local-level features from a Gated Convolutional Network (GCN) and utterance-level features from a gated recurrent unit (GRU) to enhance speaker control. We demonstrate the effectiveness of TSCM by integrating it into advanced TTS systems like FastSpeech 2 and VITS architectures, significantly optimizing their performance. Experimental results show that TSCM enables accurate voice cloning for a target speaker with minimal data through both zero-shot or few-shot fine-tuning of pretrained TTS models. Furthermore, our TSCM-based VITS (TSCM-VITS) showcases superior performance in zero-shot scenarios compared to existing state-of-the-art VC systems, even with basic dataset configurations. Our method’s superiority is validated through comprehensive subjective and objective evaluations. A demonstration of our system is available at https://great-research.github.io/tsct-tts-demo/ , providing practical insights into its application and effectiveness.",
    "metadata": {
      "title": "Optimizing feature fusion for improved zero-shot adaptation in text-to-speech synthesis",
      "summary": "Abstract In the era of advanced text-to-speech (TTS) systems capable of generating high-fidelity, human-like speech by referring a reference speech, voice cloning (VC), or zero-shot TTS (ZS-TTS), stands out as an important subtask. A primary challenge in VC is maintaining speech quality and speaker similarity with limited reference data for a specific speaker. However, existing VC systems often rely on naive combinations of embedded speaker vectors for speaker control, which compromises the capture of speaking style, voice print, and semantic accuracy. To overcome this, we introduce the Two-branch Speaker Control Module (TSCM), a novel and highly adaptable voice cloning module designed to precisely processing speaker or style control for a target speaker. Our method uses an advanced fusion of local-level features from a Gated Convolutional Network (GCN) and utterance-level features from a gated recurrent unit (GRU) to enhance speaker control. We demonstrate the effectiveness of TSCM by integrating it into advanced TTS systems like FastSpeech 2 and VITS architectures, significantly optimizing their performance. Experimental results show that TSCM enables accurate voice cloning for a target speaker with minimal data through both zero-shot or few-shot fine-tuning of pretrained TTS models. Furthermore, our TSCM-based VITS (TSCM-VITS) showcases superior performance in zero-shot scenarios compared to existing state-of-the-art VC systems, even with basic dataset configurations. Our method’s superiority is validated through comprehensive subjective and objective evaluations. A demonstration of our system is available at https://great-research.github.io/tsct-tts-demo/ , providing practical insights into its application and effectiveness.",
      "abstract": "Abstract In the era of advanced text-to-speech (TTS) systems capable of generating high-fidelity, human-like speech by referring a reference speech, voice cloning (VC), or zero-shot TTS (ZS-TTS), stands out as an important subtask. A primary challenge in VC is maintaining speech quality and speaker similarity with limited reference data for a specific speaker. However, existing VC systems often rely on naive combinations of embedded speaker vectors for speaker control, which compromises the capture of speaking style, voice print, and semantic accuracy. To overcome this, we introduce the Two-branch Speaker Control Module (TSCM), a novel and highly adaptable voice cloning module designed to precisely processing speaker or style control for a target speaker. Our method uses an advanced fusion of local-level features from a Gated Convolutional Network (GCN) and utterance-level features from a gated recurrent unit (GRU) to enhance speaker control. We demonstrate the effectiveness of TSCM by integrating it into advanced TTS systems like FastSpeech 2 and VITS architectures, significantly optimizing their performance. Experimental results show that TSCM enables accurate voice cloning for a target speaker with minimal data through both zero-shot or few-shot fine-tuning of pretrained TTS models. Furthermore, our TSCM-based VITS (TSCM-VITS) showcases superior performance in zero-shot scenarios compared to existing state-of-the-art VC systems, even with basic dataset configurations. Our method’s superiority is validated through comprehensive subjective and objective evaluations. A demonstration of our system is available at https://great-research.github.io/tsct-tts-demo/ , providing practical insights into its application and effectiveness.",
      "doi": "https://doi.org/10.1186/s13636-024-00351-9",
      "openalex_id": "https://openalex.org/W4399096462",
      "arxiv_id": "",
      "publication_date": "2024-05-28",
      "published": "2024-05-28",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item focuses on improving zero-shot voice cloning and feature fusion within text-to-speech synthesis (TTS). It treats speech primarily as a signal for synthesis rather than as a core object of Spoken Language Modeling, and there is no emphasis on interactive dialogue, turn-taking, or full-duplex speech modeling. The evaluation metrics described are not aligned with interactive speech dynamics, and the work does not discuss speech-language modeling architectures beyond TTS. Therefore it does not meet the inclusion criteria and aligns with the exclusion criteria for being a single-task TTS/voice synthesis study without conversational/speech-language modeling scope.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item focuses on improving zero-shot voice cloning and feature fusion within text-to-speech synthesis (TTS). It treats speech primarily as a signal for synthesis rather than as a core object of Spoken Language Modeling, and there is no emphasis on interactive dialogue, turn-taking, or full-duplex speech modeling. The evaluation metrics described are not aligned with interactive speech dynamics, and the work does not discuss speech-language modeling architectures beyond TTS. Therefore it does not meet the inclusion criteria and aligns with the exclusion criteria for being a single-task TTS/voice synthesis study without conversational/speech-language modeling scope.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on improving voice cloning in TTS systems using advanced feature fusion methods, focusing on speaker control and zero-shot adaptation, but it does not explicitly address Spoken Language Models or full-duplex spoken interaction concepts, nor does it discuss modeling speech sequences as core in an SLM or FD-SLM framework; it mainly targets TTS enhancement without involving interactive or synchronization aspects or speech language modeling as defined in the inclusion criteria.",
      "evaluation": 2
    },
    "round-A_JuniorMini_reasoning": "The study focuses on improving voice cloning in TTS systems using advanced feature fusion methods, focusing on speaker control and zero-shot adaptation, but it does not explicitly address Spoken Language Models or full-duplex spoken interaction concepts, nor does it discuss modeling speech sequences as core in an SLM or FD-SLM framework; it mainly targets TTS enhancement without involving interactive or synchronization aspects or speech language modeling as defined in the inclusion criteria.",
    "round-A_JuniorMini_evaluation": 2,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:2)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Benchmarking Open-ended Audio Dialogue Understanding for Large Audio-Language Models",
    "abstract": "Large Audio-Language Models (LALMs) have unclocked audio dialogue capabilities, where audio dialogues are a direct exchange of spoken language between LALMs and humans. Recent advances, such as GPT-4o, have enabled LALMs in back-and-forth audio dialogues with humans. This progression not only underscores the potential of LALMs but also broadens their applicability across a wide range of practical scenarios supported by audio dialogues. However, given these advancements, a comprehensive benchmark to evaluate the performance of LALMs in the open-ended audio dialogue understanding remains absent currently. To address this gap, we propose an Audio Dialogue Understanding Benchmark (ADU-Bench), which consists of 4 benchmark datasets. They assess the open-ended audio dialogue ability for LALMs in 3 general scenarios, 12 skills, 9 multilingual languages, and 4 categories of ambiguity handling. Notably, we _firstly propose the evaluation of ambiguity handling in audio dialogues_ that expresses different intentions beyond the same literal meaning of sentences, e.g., “Really!?” with different intonations. In summary, ADU-Bench includes over 20,000 open-ended audio dialogues for the assessment of LALMs. Through extensive experiments conducted on 13 LALMs, our analysis reveals that there is still considerable room for improvement in the audio dialogue understanding abilities of existing LALMs. In particular, they struggle with mathematical symbols and formulas, understanding human behavior such as roleplay, comprehending multiple languages, and handling audio dialogue ambiguities from different phonetic elements, such as intonations, pause positions, and homophones.",
    "metadata": {
      "title": "Benchmarking Open-ended Audio Dialogue Understanding for Large Audio-Language Models",
      "summary": "Large Audio-Language Models (LALMs) have unclocked audio dialogue capabilities, where audio dialogues are a direct exchange of spoken language between LALMs and humans. Recent advances, such as GPT-4o, have enabled LALMs in back-and-forth audio dialogues with humans. This progression not only underscores the potential of LALMs but also broadens their applicability across a wide range of practical scenarios supported by audio dialogues. However, given these advancements, a comprehensive benchmark to evaluate the performance of LALMs in the open-ended audio dialogue understanding remains absent currently. To address this gap, we propose an Audio Dialogue Understanding Benchmark (ADU-Bench), which consists of 4 benchmark datasets. They assess the open-ended audio dialogue ability for LALMs in 3 general scenarios, 12 skills, 9 multilingual languages, and 4 categories of ambiguity handling. Notably, we _firstly propose the evaluation of ambiguity handling in audio dialogues_ that expresses different intentions beyond the same literal meaning of sentences, e.g., “Really!?” with different intonations. In summary, ADU-Bench includes over 20,000 open-ended audio dialogues for the assessment of LALMs. Through extensive experiments conducted on 13 LALMs, our analysis reveals that there is still considerable room for improvement in the audio dialogue understanding abilities of existing LALMs. In particular, they struggle with mathematical symbols and formulas, understanding human behavior such as roleplay, comprehending multiple languages, and handling audio dialogue ambiguities from different phonetic elements, such as intonations, pause positions, and homophones.",
      "abstract": "Large Audio-Language Models (LALMs) have unclocked audio dialogue capabilities, where audio dialogues are a direct exchange of spoken language between LALMs and humans. Recent advances, such as GPT-4o, have enabled LALMs in back-and-forth audio dialogues with humans. This progression not only underscores the potential of LALMs but also broadens their applicability across a wide range of practical scenarios supported by audio dialogues. However, given these advancements, a comprehensive benchmark to evaluate the performance of LALMs in the open-ended audio dialogue understanding remains absent currently. To address this gap, we propose an Audio Dialogue Understanding Benchmark (ADU-Bench), which consists of 4 benchmark datasets. They assess the open-ended audio dialogue ability for LALMs in 3 general scenarios, 12 skills, 9 multilingual languages, and 4 categories of ambiguity handling. Notably, we _firstly propose the evaluation of ambiguity handling in audio dialogues_ that expresses different intentions beyond the same literal meaning of sentences, e.g., “Really!?” with different intonations. In summary, ADU-Bench includes over 20,000 open-ended audio dialogues for the assessment of LALMs. Through extensive experiments conducted on 13 LALMs, our analysis reveals that there is still considerable room for improvement in the audio dialogue understanding abilities of existing LALMs. In particular, they struggle with mathematical symbols and formulas, understanding human behavior such as roleplay, comprehending multiple languages, and handling audio dialogue ambiguities from different phonetic elements, such as intonations, pause positions, and homophones.",
      "doi": "https://doi.org/10.32388/758n37",
      "openalex_id": "https://openalex.org/W4405547956",
      "arxiv_id": "",
      "publication_date": "2024-12-18",
      "published": "2024-12-18",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item centers on open-ended audio dialogue understanding for Large Audio-Language Models, treating speech/audio as the core modality and providing benchmarks/evaluation across multilingual and interactive scenarios (full-duplex dialogue, ambiguity handling), within the last 3 years; while the abstract doesn't use the exact term 'spoken language model', 'audio-language model' implies the same concept and it aligns with the inclusion criteria.",
      "evaluation": 5
    },
    "round-A_JuniorNano_reasoning": "The item centers on open-ended audio dialogue understanding for Large Audio-Language Models, treating speech/audio as the core modality and providing benchmarks/evaluation across multilingual and interactive scenarios (full-duplex dialogue, ambiguity handling), within the last 3 years; while the abstract doesn't use the exact term 'spoken language model', 'audio-language model' implies the same concept and it aligns with the inclusion criteria.",
    "round-A_JuniorNano_evaluation": 5,
    "round-A_JuniorMini_output": {
      "reasoning": "The title and abstract discuss benchmarking open-ended audio dialogue understanding for Large Audio-Language Models (LALMs), focusing on dialogue capabilities, ambiguity handling, and multilingual scenarios. However, the study does not explicitly mention 'Spoken Language Models' or 'speech language models' as core concepts, nor does it detail model architectures, training methodologies, or explicit aspects of full-duplex or synchronous dialogue interaction such as cognitive parallelism, speech overlap, or low latency interaction as emphasized in the inclusion criteria. The focus appears to be more on high-level benchmarking of audio dialogue understanding rather than detailed end-to-end spoken language modeling or synchronization. Therefore, it only partially meets the inclusion criteria and does not address key exclusion criteria about focusing solely on ASR, TTS, or half-duplex interaction. Thus, it is uncertain whether it should be included based strictly on the criteria provided.",
      "evaluation": 3
    },
    "round-A_JuniorMini_reasoning": "The title and abstract discuss benchmarking open-ended audio dialogue understanding for Large Audio-Language Models (LALMs), focusing on dialogue capabilities, ambiguity handling, and multilingual scenarios. However, the study does not explicitly mention 'Spoken Language Models' or 'speech language models' as core concepts, nor does it detail model architectures, training methodologies, or explicit aspects of full-duplex or synchronous dialogue interaction such as cognitive parallelism, speech overlap, or low latency interaction as emphasized in the inclusion criteria. The focus appears to be more on high-level benchmarking of audio dialogue understanding rather than detailed end-to-end spoken language modeling or synchronization. Therefore, it only partially meets the inclusion criteria and does not address key exclusion criteria about focusing solely on ASR, TTS, or half-duplex interaction. Thus, it is uncertain whether it should be included based strictly on the criteria provided.",
    "round-A_JuniorMini_evaluation": 3,
    "round-B_SeniorLead_output": {
      "reasoning": "Both junior reviewers noted relevant points (one favored inclusion, the other was uncertain); after integrating their comments I conclude this work should be included because the title/abstract explicitly centers on Large Audio-Language Models (an equivalent to Spoken/Speech Language Models), proposes a multi-dataset benchmark and quantitative evaluation (not just ASR/TTS), and specifically addresses speech-level phenomena (intonation, pauses, homophones) rather than treating speech only as ASR input — therefore it meets the inclusion criteria and none of the exclusion criteria.",
      "evaluation": 5
    },
    "round-B_SeniorLead_reasoning": "Both junior reviewers noted relevant points (one favored inclusion, the other was uncertain); after integrating their comments I conclude this work should be included because the title/abstract explicitly centers on Large Audio-Language Models (an equivalent to Spoken/Speech Language Models), proposes a multi-dataset benchmark and quantitative evaluation (not just ASR/TTS), and specifically addresses speech-level phenomena (intonation, pauses, homophones) rather than treating speech only as ASR input — therefore it meets the inclusion criteria and none of the exclusion criteria.",
    "round-B_SeniorLead_evaluation": 5,
    "final_verdict": "include (senior:5)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Using ontologies to contextualize queries to large language models",
    "abstract": "The use of large language models has become a common phenomenon in question-answering and dialog systems. For this, the model must be pre-trained on prepared text data, enabling it to generate highly probable correct responses in a dialog with the user. However, answer quality decreases when the questions pertain to objects, processes, or phenomena that are less described in the texts used to train the model. For this purpose, data that is new to the model is transferred to it along with the user query in the form of context, which is usually generated using a vector database of text fragments. The article proposes to use an ontology of a subject area as a source of contextual data instead of a vector database. Ontologies are supplied with a lexical representation of their formalized terminology system to identify an ontological fragment relevant to the user query and convert it into a natural language text of the formed context. This allows to reduce the response text volume while improving its semantic alignment with the user query. The article discusses the minimum structural requirements for the lexical representation of an ontology, including natural language names, their forms for concepts and relations, as well as their lexical meanings. The application of the proposed approach is shown through an example of obtaining an answer to a question on scientific articles using a large language model. The advantages and disadvantages of the proposed approach are discussed.",
    "metadata": {
      "title": "Using ontologies to contextualize queries to large language models",
      "summary": "The use of large language models has become a common phenomenon in question-answering and dialog systems. For this, the model must be pre-trained on prepared text data, enabling it to generate highly probable correct responses in a dialog with the user. However, answer quality decreases when the questions pertain to objects, processes, or phenomena that are less described in the texts used to train the model. For this purpose, data that is new to the model is transferred to it along with the user query in the form of context, which is usually generated using a vector database of text fragments. The article proposes to use an ontology of a subject area as a source of contextual data instead of a vector database. Ontologies are supplied with a lexical representation of their formalized terminology system to identify an ontological fragment relevant to the user query and convert it into a natural language text of the formed context. This allows to reduce the response text volume while improving its semantic alignment with the user query. The article discusses the minimum structural requirements for the lexical representation of an ontology, including natural language names, their forms for concepts and relations, as well as their lexical meanings. The application of the proposed approach is shown through an example of obtaining an answer to a question on scientific articles using a large language model. The advantages and disadvantages of the proposed approach are discussed.",
      "abstract": "The use of large language models has become a common phenomenon in question-answering and dialog systems. For this, the model must be pre-trained on prepared text data, enabling it to generate highly probable correct responses in a dialog with the user. However, answer quality decreases when the questions pertain to objects, processes, or phenomena that are less described in the texts used to train the model. For this purpose, data that is new to the model is transferred to it along with the user query in the form of context, which is usually generated using a vector database of text fragments. The article proposes to use an ontology of a subject area as a source of contextual data instead of a vector database. Ontologies are supplied with a lexical representation of their formalized terminology system to identify an ontological fragment relevant to the user query and convert it into a natural language text of the formed context. This allows to reduce the response text volume while improving its semantic alignment with the user query. The article discusses the minimum structural requirements for the lexical representation of an ontology, including natural language names, their forms for concepts and relations, as well as their lexical meanings. The application of the proposed approach is shown through an example of obtaining an answer to a question on scientific articles using a large language model. The advantages and disadvantages of the proposed approach are discussed.",
      "doi": "https://doi.org/10.18287/2223-9537-2025-15-2-239-248",
      "openalex_id": "https://openalex.org/W4409571543",
      "arxiv_id": "",
      "publication_date": "2025-04-18",
      "published": "2025-04-18",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This item focuses on ontology-based contextual data for large language models and does not address Spoken Language Models or speech-language modeling, nor does it discuss audio-based architecture, training, or evaluation relevant to the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "This item focuses on ontology-based contextual data for large language models and does not address Spoken Language Models or speech-language modeling, nor does it discuss audio-based architecture, training, or evaluation relevant to the inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on using ontologies to contextualize queries to large language models but does not address spoken language models, speech as a core modeling object, or related synchronous interactive behaviors detailed in the inclusion criteria; it is centered on textual language models and ontology usage rather than speech-based modeling or full-duplex spoken interactions.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on using ontologies to contextualize queries to large language models but does not address spoken language models, speech as a core modeling object, or related synchronous interactive behaviors detailed in the inclusion criteria; it is centered on textual language models and ontology usage rather than speech-based modeling or full-duplex spoken interactions.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "HiProbe-VAD: Video Anomaly Detection via Hidden States Probing in Tuning-Free Multimodal LLMs",
    "abstract": "Video Anomaly Detection (VAD) aims to identify and locate deviations from normal patterns in video sequences. Traditional methods often struggle with substantial computational demands and a reliance on extensive labeled datasets, thereby restricting their practical applicability. To address these constraints, we propose HiProbe-VAD, a novel framework that leverages pre-trained Multimodal Large Language Models (MLLMs) for VAD without requiring fine-tuning. In this paper, we discover that the intermediate hidden states of MLLMs contain information-rich representations, exhibiting higher sensitivity and linear separability for anomalies compared to the output layer. To capitalize on this, we propose a Dynamic Layer Saliency Probing (DLSP) mechanism that intelligently identifies and extracts the most informative hidden states from the optimal intermediate layer during the MLLMs reasoning. Then a lightweight anomaly scorer and temporal localization module efficiently detects anomalies using these extracted hidden states and finally generate explanations. Experiments on the UCF-Crime and XD-Violence datasets demonstrate that HiProbe-VAD outperforms existing training-free and most traditional approaches. Furthermore, our framework exhibits remarkable cross-model generalization capabilities in different MLLMs without any tuning, unlocking the potential of pre-trained MLLMs for video anomaly detection and paving the way for more practical and scalable solutions.",
    "metadata": {
      "title": "HiProbe-VAD: Video Anomaly Detection via Hidden States Probing in Tuning-Free Multimodal LLMs",
      "summary": "Video Anomaly Detection (VAD) aims to identify and locate deviations from normal patterns in video sequences. Traditional methods often struggle with substantial computational demands and a reliance on extensive labeled datasets, thereby restricting their practical applicability. To address these constraints, we propose HiProbe-VAD, a novel framework that leverages pre-trained Multimodal Large Language Models (MLLMs) for VAD without requiring fine-tuning. In this paper, we discover that the intermediate hidden states of MLLMs contain information-rich representations, exhibiting higher sensitivity and linear separability for anomalies compared to the output layer. To capitalize on this, we propose a Dynamic Layer Saliency Probing (DLSP) mechanism that intelligently identifies and extracts the most informative hidden states from the optimal intermediate layer during the MLLMs reasoning. Then a lightweight anomaly scorer and temporal localization module efficiently detects anomalies using these extracted hidden states and finally generate explanations. Experiments on the UCF-Crime and XD-Violence datasets demonstrate that HiProbe-VAD outperforms existing training-free and most traditional approaches. Furthermore, our framework exhibits remarkable cross-model generalization capabilities in different MLLMs without any tuning, unlocking the potential of pre-trained MLLMs for video anomaly detection and paving the way for more practical and scalable solutions.",
      "abstract": "Video Anomaly Detection (VAD) aims to identify and locate deviations from normal patterns in video sequences. Traditional methods often struggle with substantial computational demands and a reliance on extensive labeled datasets, thereby restricting their practical applicability. To address these constraints, we propose HiProbe-VAD, a novel framework that leverages pre-trained Multimodal Large Language Models (MLLMs) for VAD without requiring fine-tuning. In this paper, we discover that the intermediate hidden states of MLLMs contain information-rich representations, exhibiting higher sensitivity and linear separability for anomalies compared to the output layer. To capitalize on this, we propose a Dynamic Layer Saliency Probing (DLSP) mechanism that intelligently identifies and extracts the most informative hidden states from the optimal intermediate layer during the MLLMs reasoning. Then a lightweight anomaly scorer and temporal localization module efficiently detects anomalies using these extracted hidden states and finally generate explanations. Experiments on the UCF-Crime and XD-Violence datasets demonstrate that HiProbe-VAD outperforms existing training-free and most traditional approaches. Furthermore, our framework exhibits remarkable cross-model generalization capabilities in different MLLMs without any tuning, unlocking the potential of pre-trained MLLMs for video anomaly detection and paving the way for more practical and scalable solutions.",
      "doi": "https://doi.org/10.1145/3746027.3755575",
      "openalex_id": "https://openalex.org/W4414880188",
      "arxiv_id": "",
      "publication_date": "2025-10-25",
      "published": "2025-10-25",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "該論文以視訊異常檢測為主，核心是多模態語言模型的推理/特徵探勘，非以 Spoken Language Models 或語音語言模型為核心，因此不符合包含條件。",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "該論文以視訊異常檢測為主，核心是多模態語言模型的推理/特徵探勘，非以 Spoken Language Models 或語音語言模型為核心，因此不符合包含條件。",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on video anomaly detection using multimodal large language models without fine-tuning, with no mention of spoken language models, speech language models, or modeling of speech sequences as core components; it does not address full-duplex spoken interaction, synchronous turn-taking, speech overlap, or other criteria related to spoken language model research. Therefore, it does not meet the inclusion criteria and is not related to the research topic of spoken or full-duplex spoken language models.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on video anomaly detection using multimodal large language models without fine-tuning, with no mention of spoken language models, speech language models, or modeling of speech sequences as core components; it does not address full-duplex spoken interaction, synchronous turn-taking, speech overlap, or other criteria related to spoken language model research. Therefore, it does not meet the inclusion criteria and is not related to the research topic of spoken or full-duplex spoken language models.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Cajal’s legacy in the digital era: from neuroscience foundations to deep learning",
    "abstract": "Santiago Ramón y Cajal’s pioneering work laid the foundations for modern neuroscience and continues to impact the development of artificial intelligence, particularly deep learning. His neuron theory, the principle of dynamic polarization, and his insights into brain plasticity and network organization have significantly influenced both our understanding of the nervous system and the design of artificial neural networks. This article reviews Cajal’s key contributions, explores their role in the evolution of AI, and emphasizes the enduring links between neuroscience and machine learning in the digital era.",
    "metadata": {
      "title": "Cajal’s legacy in the digital era: from neuroscience foundations to deep learning",
      "summary": "Santiago Ramón y Cajal’s pioneering work laid the foundations for modern neuroscience and continues to impact the development of artificial intelligence, particularly deep learning. His neuron theory, the principle of dynamic polarization, and his insights into brain plasticity and network organization have significantly influenced both our understanding of the nervous system and the design of artificial neural networks. This article reviews Cajal’s key contributions, explores their role in the evolution of AI, and emphasizes the enduring links between neuroscience and machine learning in the digital era.",
      "abstract": "Santiago Ramón y Cajal’s pioneering work laid the foundations for modern neuroscience and continues to impact the development of artificial intelligence, particularly deep learning. His neuron theory, the principle of dynamic polarization, and his insights into brain plasticity and network organization have significantly influenced both our understanding of the nervous system and the design of artificial neural networks. This article reviews Cajal’s key contributions, explores their role in the evolution of AI, and emphasizes the enduring links between neuroscience and machine learning in the digital era.",
      "doi": "https://doi.org/10.3389/fnana.2025.1672016",
      "openalex_id": "https://openalex.org/W4415215100",
      "arxiv_id": "",
      "publication_date": "2025-10-15",
      "published": "2025-10-15",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The article is a historical/overview piece on Cajal’s neuroscience foundations and their influence on deep learning, not centered on Spoken Language Models or speech-language modeling, nor on end-to-end or speech–text architectures with explicit speech-centric evaluation within the last 3 years.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The article is a historical/overview piece on Cajal’s neuroscience foundations and their influence on deep learning, not centered on Spoken Language Models or speech-language modeling, nor on end-to-end or speech–text architectures with explicit speech-centric evaluation within the last 3 years.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The article focuses on the legacy of neuroscience foundations applied to deep learning in AI, emphasizing historical and conceptual links rather than specifically addressing Spoken Language Models or speech-based modeling architectures, training, or evaluation as required by the inclusion criteria. It does not mention any spoken language modeling, speech-LLM architectures, or behaviors such as full-duplex interaction, real-time latency, or speech overlap management, and therefore does not meet the defined inclusion criteria or address the core research focus.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The article focuses on the legacy of neuroscience foundations applied to deep learning in AI, emphasizing historical and conceptual links rather than specifically addressing Spoken Language Models or speech-based modeling architectures, training, or evaluation as required by the inclusion criteria. It does not mention any spoken language modeling, speech-LLM architectures, or behaviors such as full-duplex interaction, real-time latency, or speech overlap management, and therefore does not meet the defined inclusion criteria or address the core research focus.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "The Cambridge Handbook of Child Language",
    "abstract": "The Cambridge Handbook of Child Language brings together the world's foremost researchers to provide a one-stop resource for the study of language acquisition and development. Grouped into five thematic sections, the handbook is organized by topic, making it easier for students and researchers to use when looking up specific in-depth information. It covers a wider range of subjects than any other handbook on the market, with chapters covering both theories and methods in child language research and tracing the development of language from prelinguistic infancy to teenager. Drawing on both established and more recent research, the Handbook surveys the crosslinguistic study of language acquisition; prelinguistic development; bilingualism; sign languages; specific language impairment, language and autism, Down syndrome and Williams syndrome. This book will be an essential reference for students and researchers working in linguistics, psychology, cognitive science, speech pathology, education and anthropology.",
    "metadata": {
      "title": "The Cambridge Handbook of Child Language",
      "summary": "The Cambridge Handbook of Child Language brings together the world's foremost researchers to provide a one-stop resource for the study of language acquisition and development. Grouped into five thematic sections, the handbook is organized by topic, making it easier for students and researchers to use when looking up specific in-depth information. It covers a wider range of subjects than any other handbook on the market, with chapters covering both theories and methods in child language research and tracing the development of language from prelinguistic infancy to teenager. Drawing on both established and more recent research, the Handbook surveys the crosslinguistic study of language acquisition; prelinguistic development; bilingualism; sign languages; specific language impairment, language and autism, Down syndrome and Williams syndrome. This book will be an essential reference for students and researchers working in linguistics, psychology, cognitive science, speech pathology, education and anthropology.",
      "abstract": "The Cambridge Handbook of Child Language brings together the world's foremost researchers to provide a one-stop resource for the study of language acquisition and development. Grouped into five thematic sections, the handbook is organized by topic, making it easier for students and researchers to use when looking up specific in-depth information. It covers a wider range of subjects than any other handbook on the market, with chapters covering both theories and methods in child language research and tracing the development of language from prelinguistic infancy to teenager. Drawing on both established and more recent research, the Handbook surveys the crosslinguistic study of language acquisition; prelinguistic development; bilingualism; sign languages; specific language impairment, language and autism, Down syndrome and Williams syndrome. This book will be an essential reference for students and researchers working in linguistics, psychology, cognitive science, speech pathology, education and anthropology.",
      "doi": "https://doi.org/10.1017/cbo9780511576164",
      "openalex_id": "https://openalex.org/W1529056402",
      "arxiv_id": "",
      "publication_date": "2009-01-01",
      "published": "2009-01-01",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This is a general handbook about child language and not about Spoken Language Models or any speech-language modeling architecture/evaluation; it is not within the last 3 years and does not discuss SLMs, thus fails inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "This is a general handbook about child language and not about Spoken Language Models or any speech-language modeling architecture/evaluation; it is not within the last 3 years and does not discuss SLMs, thus fails inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The title and abstract describe a comprehensive handbook on child language acquisition and development with no mention of spoken language models, speech language modeling architectures, real-time interactive speech processing, or any relevant modeling, training, or evaluation related to full-duplex spoken language models or their associated criteria. Moreover, it appears to be a broad educational resource rather than a research study reporting novel methods or evaluations in the specified domain. Therefore, it does not meet any inclusion criteria and aligns with exclusion criteria regarding lack of core research focus on SLMs or associated evaluation.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The title and abstract describe a comprehensive handbook on child language acquisition and development with no mention of spoken language models, speech language modeling architectures, real-time interactive speech processing, or any relevant modeling, training, or evaluation related to full-duplex spoken language models or their associated criteria. Moreover, it appears to be a broad educational resource rather than a research study reporting novel methods or evaluations in the specified domain. Therefore, it does not meet any inclusion criteria and aligns with exclusion criteria regarding lack of core research focus on SLMs or associated evaluation.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "The 'neural' phonetic typewriter",
    "abstract": "The factors that make speech recognition difficult are examined, and the potential of neural computers for this purpose is discussed. A speaker-adaptive system that transcribes dictation using an unlimited vocabulary is presented that is based on a neural network processor for the recognition of phonetic units of speech. The acoustic preprocessing, vector quantization, neural network model, and shortcut learning algorithm used are described. The utilization of phonotopic maps and of postprocessing in symbolic forms are discussed. Hardware implementations and performance of the neural networks are considered.< <ETX xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">&gt;</ETX>",
    "metadata": {
      "title": "The 'neural' phonetic typewriter",
      "summary": "The factors that make speech recognition difficult are examined, and the potential of neural computers for this purpose is discussed. A speaker-adaptive system that transcribes dictation using an unlimited vocabulary is presented that is based on a neural network processor for the recognition of phonetic units of speech. The acoustic preprocessing, vector quantization, neural network model, and shortcut learning algorithm used are described. The utilization of phonotopic maps and of postprocessing in symbolic forms are discussed. Hardware implementations and performance of the neural networks are considered.< <ETX xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">&gt;</ETX>",
      "abstract": "The factors that make speech recognition difficult are examined, and the potential of neural computers for this purpose is discussed. A speaker-adaptive system that transcribes dictation using an unlimited vocabulary is presented that is based on a neural network processor for the recognition of phonetic units of speech. The acoustic preprocessing, vector quantization, neural network model, and shortcut learning algorithm used are described. The utilization of phonotopic maps and of postprocessing in symbolic forms are discussed. Hardware implementations and performance of the neural networks are considered.< <ETX xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">&gt;</ETX>",
      "doi": "https://doi.org/10.1109/2.28",
      "openalex_id": "https://openalex.org/W2098500169",
      "arxiv_id": "",
      "publication_date": "1988-03-01",
      "published": "1988-03-01",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This abstract focuses on ASR with neural networks and hardware for phonetic recognition rather than on Spoken Language Models or speech-language modeling with interactive, end-to-end evaluation, and it does not appear to address full-duplex dialogue capabilities or recent SLM frameworks, so it should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "This abstract focuses on ASR with neural networks and hardware for phonetic recognition rather than on Spoken Language Models or speech-language modeling with interactive, end-to-end evaluation, and it does not appear to address full-duplex dialogue capabilities or recent SLM frameworks, so it should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study discusses a neural network-based speaker-adaptive speech recognition system focusing on phonetic unit recognition, acoustic preprocessing, and neural network models, without addressing spoken language models or full-duplex spoken language model frameworks. It lacks discussion on modeling speech sequences for end-to-end inference, synchronous interactive capabilities, or comprehensive evaluation metrics related to conversational dynamics or low latency. Therefore, it does not meet the inclusion criteria centered on spoken language modeling and full-duplex interaction capabilities, nor does it cover relevant evaluation aspects or architecture specifics as required.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study discusses a neural network-based speaker-adaptive speech recognition system focusing on phonetic unit recognition, acoustic preprocessing, and neural network models, without addressing spoken language models or full-duplex spoken language model frameworks. It lacks discussion on modeling speech sequences for end-to-end inference, synchronous interactive capabilities, or comprehensive evaluation metrics related to conversational dynamics or low latency. Therefore, it does not meet the inclusion criteria centered on spoken language modeling and full-duplex interaction capabilities, nor does it cover relevant evaluation aspects or architecture specifics as required.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Unsupervised learning of acoustic sub-word units",
    "abstract": "Accurate unsupervised learning of phonemes of a language directly from speech is demonstrated via an algorithm for joint unsupervised learning of the topology and parameters of a hidden Markov model (HMM); states and short state-sequences through this HMM correspond to the learnt sub-word units. The algorithm, originally proposed for unsupervised learning of allophonic variations within a given phoneme set, has been adapted to learn without any knowledge of the phonemes. An evaluation methodology is also proposed, whereby the state-sequence that aligns to a test utterance is transduced in an automatic manner to a phoneme-sequence and compared to its manual transcription. Over 85% phoneme recognition accuracy is demonstrated for speaker-dependent learning from fluent, large-vocabulary speech.",
    "metadata": {
      "title": "Unsupervised learning of acoustic sub-word units",
      "summary": "Accurate unsupervised learning of phonemes of a language directly from speech is demonstrated via an algorithm for joint unsupervised learning of the topology and parameters of a hidden Markov model (HMM); states and short state-sequences through this HMM correspond to the learnt sub-word units. The algorithm, originally proposed for unsupervised learning of allophonic variations within a given phoneme set, has been adapted to learn without any knowledge of the phonemes. An evaluation methodology is also proposed, whereby the state-sequence that aligns to a test utterance is transduced in an automatic manner to a phoneme-sequence and compared to its manual transcription. Over 85% phoneme recognition accuracy is demonstrated for speaker-dependent learning from fluent, large-vocabulary speech.",
      "abstract": "Accurate unsupervised learning of phonemes of a language directly from speech is demonstrated via an algorithm for joint unsupervised learning of the topology and parameters of a hidden Markov model (HMM); states and short state-sequences through this HMM correspond to the learnt sub-word units. The algorithm, originally proposed for unsupervised learning of allophonic variations within a given phoneme set, has been adapted to learn without any knowledge of the phonemes. An evaluation methodology is also proposed, whereby the state-sequence that aligns to a test utterance is transduced in an automatic manner to a phoneme-sequence and compared to its manual transcription. Over 85% phoneme recognition accuracy is demonstrated for speaker-dependent learning from fluent, large-vocabulary speech.",
      "doi": "https://doi.org/10.3115/1557690.1557736",
      "openalex_id": "https://openalex.org/W2117041980",
      "arxiv_id": "",
      "publication_date": "2008-01-01",
      "published": "2008-01-01",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item focuses on unsupervised acoustic unit discovery and phoneme-level HMM modeling rather than framing speech as a core component of a Spoken Language Model or end-to-end speech–language modeling; it targets acoustic/phoneme learning rather than interactive dialogue, synchronization, or multi-turn speech processing, and it may not satisfy the recent 3-year window requirement, thus it does not meet the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item focuses on unsupervised acoustic unit discovery and phoneme-level HMM modeling rather than framing speech as a core component of a Spoken Language Model or end-to-end speech–language modeling; it targets acoustic/phoneme learning rather than interactive dialogue, synchronization, or multi-turn speech processing, and it may not satisfy the recent 3-year window requirement, thus it does not meet the inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper discusses unsupervised learning of phonemes from speech using hidden Markov models, focusing on acoustic sub-word units without discussing spoken language models, speech LM architectures, full-duplex interaction, or synchronization as required. It primarily addresses a speech recognition component (phoneme recognition) without integrating speech language modeling or interaction aspects, and does not mention end-to-end or multi-modal training or evaluation relevant to Spoken Language Models or FD-SLMs. Therefore, it does not meet the inclusion criteria and aligns more with traditional ASR approach without the interactive or cognitive parallelism focus.",
      "evaluation": 2
    },
    "round-A_JuniorMini_reasoning": "The paper discusses unsupervised learning of phonemes from speech using hidden Markov models, focusing on acoustic sub-word units without discussing spoken language models, speech LM architectures, full-duplex interaction, or synchronization as required. It primarily addresses a speech recognition component (phoneme recognition) without integrating speech language modeling or interaction aspects, and does not mention end-to-end or multi-modal training or evaluation relevant to Spoken Language Models or FD-SLMs. Therefore, it does not meet the inclusion criteria and aligns more with traditional ASR approach without the interactive or cognitive parallelism focus.",
    "round-A_JuniorMini_evaluation": 2,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:2)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Unsupervised acoustic sub-word unit detection for query-by-example spoken term detection",
    "abstract": "In this paper we present a method for automatically generating acoustic sub-word units that can substitute conventional phone models in a query-by-example spoken term detection system. We generate the sub-word units with a modified version of our speaker diarization system. Given a speech recording, the original diarization system generates a set of speaker models in an unsupervised manner without the need for training or development data. Modifying the diarization system to process the speech of a single speaker and decreasing the minimum segment duration constraint allows us to detect speaker-dependent sub-word units. For the task of query-by-example spoken term detection, we show that the pro posed system performs well on both broadcast and non-broadcast recordings, unlike a conventional phone-based system trained solely on broadcast data. A mean average precision of 0.28 and 0.38 was obtained for experiments on broadcast news and on a set of war veteran interviews, respectively.",
    "metadata": {
      "title": "Unsupervised acoustic sub-word unit detection for query-by-example spoken term detection",
      "summary": "In this paper we present a method for automatically generating acoustic sub-word units that can substitute conventional phone models in a query-by-example spoken term detection system. We generate the sub-word units with a modified version of our speaker diarization system. Given a speech recording, the original diarization system generates a set of speaker models in an unsupervised manner without the need for training or development data. Modifying the diarization system to process the speech of a single speaker and decreasing the minimum segment duration constraint allows us to detect speaker-dependent sub-word units. For the task of query-by-example spoken term detection, we show that the pro posed system performs well on both broadcast and non-broadcast recordings, unlike a conventional phone-based system trained solely on broadcast data. A mean average precision of 0.28 and 0.38 was obtained for experiments on broadcast news and on a set of war veteran interviews, respectively.",
      "abstract": "In this paper we present a method for automatically generating acoustic sub-word units that can substitute conventional phone models in a query-by-example spoken term detection system. We generate the sub-word units with a modified version of our speaker diarization system. Given a speech recording, the original diarization system generates a set of speaker models in an unsupervised manner without the need for training or development data. Modifying the diarization system to process the speech of a single speaker and decreasing the minimum segment duration constraint allows us to detect speaker-dependent sub-word units. For the task of query-by-example spoken term detection, we show that the pro posed system performs well on both broadcast and non-broadcast recordings, unlike a conventional phone-based system trained solely on broadcast data. A mean average precision of 0.28 and 0.38 was obtained for experiments on broadcast news and on a set of war veteran interviews, respectively.",
      "doi": "https://doi.org/10.1109/icassp.2011.5947338",
      "openalex_id": "https://openalex.org/W2170659185",
      "arxiv_id": "",
      "publication_date": "2011-05-01",
      "published": "2011-05-01",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "Although it uses speech data and sub-word units for a SED task, it does not place Spoken Language Models/FD-SLMs as the core modeling target nor discuss SLM-level architecture or evaluation within a recent 3-year window.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "Although it uses speech data and sub-word units for a SED task, it does not place Spoken Language Models/FD-SLMs as the core modeling target nor discuss SLM-level architecture or evaluation within a recent 3-year window.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper focuses on unsupervised acoustic sub-word unit detection to improve query-by-example spoken term detection, which is primarily about signal processing and speech retrieval, but it does not explicitly discuss Spoken Language Models or speech language modeling architectures, training, or evaluation; nor does it address full-duplex interaction, cognitive parallelism, synchronization, or multimodal modeling features as required by the inclusion criteria. Hence, it does not meet the inclusion criteria focusing on SLMs and FD-SLMs, nor does it address the exclusion criteria specifically, but overall it falls outside the core thematic scope.",
      "evaluation": 2
    },
    "round-A_JuniorMini_reasoning": "The paper focuses on unsupervised acoustic sub-word unit detection to improve query-by-example spoken term detection, which is primarily about signal processing and speech retrieval, but it does not explicitly discuss Spoken Language Models or speech language modeling architectures, training, or evaluation; nor does it address full-duplex interaction, cognitive parallelism, synchronization, or multimodal modeling features as required by the inclusion criteria. Hence, it does not meet the inclusion criteria focusing on SLMs and FD-SLMs, nor does it address the exclusion criteria specifically, but overall it falls outside the core thematic scope.",
    "round-A_JuniorMini_evaluation": 2,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:2)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "A summary of the 2012 JHU CLSP workshop on zero resource speech technologies and models of early language acquisition",
    "abstract": "We summarize the accomplishments of a multi-disciplinary workshop exploring the computational and scientific issues surrounding zero resource (unsupervised) speech technologies and related models of early language acquisition. Centered around the tasks of phonetic and lexical discovery, we consider unified evaluation metrics, present two new approaches for improving speaker independence in the absence of supervision, and evaluate the application of Bayesian word segmentation algorithms to automatic subword unit tokenizations. Finally, we present two strategies for integrating zero resource techniques into supervised settings, demonstrating the potential of unsupervised methods to improve mainstream technologies.",
    "metadata": {
      "title": "A summary of the 2012 JHU CLSP workshop on zero resource speech technologies and models of early language acquisition",
      "summary": "We summarize the accomplishments of a multi-disciplinary workshop exploring the computational and scientific issues surrounding zero resource (unsupervised) speech technologies and related models of early language acquisition. Centered around the tasks of phonetic and lexical discovery, we consider unified evaluation metrics, present two new approaches for improving speaker independence in the absence of supervision, and evaluate the application of Bayesian word segmentation algorithms to automatic subword unit tokenizations. Finally, we present two strategies for integrating zero resource techniques into supervised settings, demonstrating the potential of unsupervised methods to improve mainstream technologies.",
      "abstract": "We summarize the accomplishments of a multi-disciplinary workshop exploring the computational and scientific issues surrounding zero resource (unsupervised) speech technologies and related models of early language acquisition. Centered around the tasks of phonetic and lexical discovery, we consider unified evaluation metrics, present two new approaches for improving speaker independence in the absence of supervision, and evaluate the application of Bayesian word segmentation algorithms to automatic subword unit tokenizations. Finally, we present two strategies for integrating zero resource techniques into supervised settings, demonstrating the potential of unsupervised methods to improve mainstream technologies.",
      "doi": "https://doi.org/10.1109/icassp.2013.6639245",
      "openalex_id": "https://openalex.org/W2025482506",
      "arxiv_id": "",
      "publication_date": "2013-05-01",
      "published": "2013-05-01",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "此項為2012年工作坊摘要，距今已久且未符合過去3年的時效性要求，且摘要未清楚以 Spoken Language Models 為核心，因此不符合包含條件，應排除。",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "此項為2012年工作坊摘要，距今已久且未符合過去3年的時效性要求，且摘要未清楚以 Spoken Language Models 為核心，因此不符合包含條件，應排除。",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper is a summary of a 2012 workshop focusing on zero resource speech technologies and early language acquisition models, emphasizing unsupervised techniques for phonetic and lexical discovery rather than on Spoken Language Models (SLMs) or Full-Duplex Spoken Language Models (FD-SLMs) as core modeling objects. It does not discuss contemporary model architectures, training, or evaluation of spoken language models within the past three years, nor does it mention speech language models, speech-LLM hybrid architectures, or aspects like full-duplex interaction, cognitive parallelism, or low-latency interaction specific to the inclusion criteria. Therefore, it fails to meet the inclusion criteria on topicality and recency, and likely falls under exclusions due to outdated, non-core focus on speech language modeling. Hence, the study should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper is a summary of a 2012 workshop focusing on zero resource speech technologies and early language acquisition models, emphasizing unsupervised techniques for phonetic and lexical discovery rather than on Spoken Language Models (SLMs) or Full-Duplex Spoken Language Models (FD-SLMs) as core modeling objects. It does not discuss contemporary model architectures, training, or evaluation of spoken language models within the past three years, nor does it mention speech language models, speech-LLM hybrid architectures, or aspects like full-duplex interaction, cognitive parallelism, or low-latency interaction specific to the inclusion criteria. Therefore, it fails to meet the inclusion criteria on topicality and recency, and likely falls under exclusions due to outdated, non-core focus on speech language modeling. Hence, the study should be excluded.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "An auto-encoder based approach to unsupervised learning of subword units",
    "abstract": "In this paper we propose an autoencoder-based method for the unsupervised identification of subword units. We experiment with different types and architectures of autoencoders to asses what autoencoder properties are most important for this task. We first show that the encoded representation of speech produced by standard autencoders is more effective than Gaussian posteriorgrams in a spoken query classification task. Finally we evaluate the subword inventories produced by the proposed method both in terms of classification accuracy in a word classification task (with lexicon size up to 263 words) and in terms of consistency between subword transcription of different word examples of a same word type. The evaluation is carried out on Italian and American English datasets. © 2014 IEEE.",
    "metadata": {
      "title": "An auto-encoder based approach to unsupervised learning of subword units",
      "summary": "In this paper we propose an autoencoder-based method for the unsupervised identification of subword units. We experiment with different types and architectures of autoencoders to asses what autoencoder properties are most important for this task. We first show that the encoded representation of speech produced by standard autencoders is more effective than Gaussian posteriorgrams in a spoken query classification task. Finally we evaluate the subword inventories produced by the proposed method both in terms of classification accuracy in a word classification task (with lexicon size up to 263 words) and in terms of consistency between subword transcription of different word examples of a same word type. The evaluation is carried out on Italian and American English datasets. © 2014 IEEE.",
      "abstract": "In this paper we propose an autoencoder-based method for the unsupervised identification of subword units. We experiment with different types and architectures of autoencoders to asses what autoencoder properties are most important for this task. We first show that the encoded representation of speech produced by standard autencoders is more effective than Gaussian posteriorgrams in a spoken query classification task. Finally we evaluate the subword inventories produced by the proposed method both in terms of classification accuracy in a word classification task (with lexicon size up to 263 words) and in terms of consistency between subword transcription of different word examples of a same word type. The evaluation is carried out on Italian and American English datasets. © 2014 IEEE.",
      "doi": "https://doi.org/10.1109/icassp.2014.6855085",
      "openalex_id": "https://openalex.org/W2020607164",
      "arxiv_id": "",
      "publication_date": "2014-05-01",
      "published": "2014-05-01",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item is from 2014 and thus outside the past-3-years inclusion window, and it does not clearly present itself as a current Spoken Language Model (SLM) core study with end-to-end or interactive evaluation, so it should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item is from 2014 and thus outside the past-3-years inclusion window, and it does not clearly present itself as a current Spoken Language Model (SLM) core study with end-to-end or interactive evaluation, so it should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper focuses on an autoencoder-based unsupervised approach to identifying subword units from speech data, but it does not explicitly discuss spoken language models or language modeling of speech as a core concept, nor does it mention full-duplex capability, synchronization, or interactive dialogue behaviors. The evaluation relies on classification accuracy without addressing real-time interaction, cognitive parallelism, low-latency, or multi-dimensional behavioral arbitration. Also, the publication date is from 2014, which is outside the 3-year inclusion window. Therefore, the study does not satisfy the inclusion criteria and is more aligned with traditional speech unit identification rather than spoken language modeling or full-duplex spoken language interaction.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper focuses on an autoencoder-based unsupervised approach to identifying subword units from speech data, but it does not explicitly discuss spoken language models or language modeling of speech as a core concept, nor does it mention full-duplex capability, synchronization, or interactive dialogue behaviors. The evaluation relies on classification accuracy without addressing real-time interaction, cognitive parallelism, low-latency, or multi-dimensional behavioral arbitration. Also, the publication date is from 2014, which is outside the 3-year inclusion window. Therefore, the study does not satisfy the inclusion criteria and is more aligned with traditional speech unit identification rather than spoken language modeling or full-duplex spoken language interaction.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "The Zero Resource Speech Challenge 2015: Proposed Approaches and Results",
    "abstract": "This paper reports on the results of the Zero Resource Speech Challenge 2015, the first unified benchmark for zero resource speech technology, which aims at the unsupervised discovery of subword and word units from raw speech. This paper discusses the motivation for the challenge, its data sets, tasks and baseline systems. We outline the ideas behind the systems that were submitted for the two challenge tracks: unsupervised subword unit modeling and spoken term discovery, and summarize their results. The results obtained by participating teams show great promise; many systems beat the provided baselines and some even perform better than comparable supervised systems.",
    "metadata": {
      "title": "The Zero Resource Speech Challenge 2015: Proposed Approaches and Results",
      "summary": "This paper reports on the results of the Zero Resource Speech Challenge 2015, the first unified benchmark for zero resource speech technology, which aims at the unsupervised discovery of subword and word units from raw speech. This paper discusses the motivation for the challenge, its data sets, tasks and baseline systems. We outline the ideas behind the systems that were submitted for the two challenge tracks: unsupervised subword unit modeling and spoken term discovery, and summarize their results. The results obtained by participating teams show great promise; many systems beat the provided baselines and some even perform better than comparable supervised systems.",
      "abstract": "This paper reports on the results of the Zero Resource Speech Challenge 2015, the first unified benchmark for zero resource speech technology, which aims at the unsupervised discovery of subword and word units from raw speech. This paper discusses the motivation for the challenge, its data sets, tasks and baseline systems. We outline the ideas behind the systems that were submitted for the two challenge tracks: unsupervised subword unit modeling and spoken term discovery, and summarize their results. The results obtained by participating teams show great promise; many systems beat the provided baselines and some even perform better than comparable supervised systems.",
      "doi": "https://doi.org/10.1016/j.procs.2016.04.031",
      "openalex_id": "https://openalex.org/W2346964103",
      "arxiv_id": "",
      "publication_date": "2016-01-01",
      "published": "2016-01-01",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item is a 2015 Zero Resource Speech Challenge paper focusing on unsupervised speech discovery rather than Spoken Language Models or speech-language modeling, and it is outside the 3-year WINDOW, so it does not meet the core SLM criterion nor the recency requirement.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item is a 2015 Zero Resource Speech Challenge paper focusing on unsupervised speech discovery rather than Spoken Language Models or speech-language modeling, and it is outside the 3-year WINDOW, so it does not meet the core SLM criterion nor the recency requirement.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper focuses on zero resource speech technology aiming at unsupervised discovery of subword and word units directly from raw speech, which aligns with speech-level language modeling; however, it does not explicitly discuss spoken language models, full-duplex interaction capabilities, or speech–LLM hybrid architectures, nor does it mention real-time interaction features or synchronization mechanisms required by the inclusion criteria. Additionally, there is no indication that the paper addresses multidimensional evaluations like cognitive parallelism, overlap management, or behavioral arbitration. Therefore, it partially matches the theme but lacks critical aspects of spoken language models and full-duplex interaction focus.",
      "evaluation": 2
    },
    "round-A_JuniorMini_reasoning": "The paper focuses on zero resource speech technology aiming at unsupervised discovery of subword and word units directly from raw speech, which aligns with speech-level language modeling; however, it does not explicitly discuss spoken language models, full-duplex interaction capabilities, or speech–LLM hybrid architectures, nor does it mention real-time interaction features or synchronization mechanisms required by the inclusion criteria. Additionally, there is no indication that the paper addresses multidimensional evaluations like cognitive parallelism, overlap management, or behavioral arbitration. Therefore, it partially matches the theme but lacks critical aspects of spoken language models and full-duplex interaction focus.",
    "round-A_JuniorMini_evaluation": 2,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:2)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "The Zero Resource Speech Challenge 2019: TTS Without T",
    "abstract": "We present the Zero Resource Speech Challenge 2019, which proposes to build a\\nspeech synthesizer without any text or phonetic labels: hence, TTS without T\\n(text-to-speech without text). We provide raw audio for a target voice in an\\nunknown language (the Voice dataset), but no alignment, text or labels.\\nParticipants must discover subword units in an unsupervised way (using the Unit\\nDiscovery dataset) and align them to the voice recordings in a way that works\\nbest for the purpose of synthesizing novel utterances from novel speakers,\\nsimilar to the target speaker's voice. We describe the metrics used for\\nevaluation, a baseline system consisting of unsupervised subword unit discovery\\nplus a standard TTS system, and a topline TTS using gold phoneme\\ntranscriptions. We present an overview of the 19 submitted systems from 10\\nteams and discuss the main results.\\n",
    "metadata": {
      "title": "The Zero Resource Speech Challenge 2019: TTS Without T",
      "summary": "We present the Zero Resource Speech Challenge 2019, which proposes to build a\\nspeech synthesizer without any text or phonetic labels: hence, TTS without T\\n(text-to-speech without text). We provide raw audio for a target voice in an\\nunknown language (the Voice dataset), but no alignment, text or labels.\\nParticipants must discover subword units in an unsupervised way (using the Unit\\nDiscovery dataset) and align them to the voice recordings in a way that works\\nbest for the purpose of synthesizing novel utterances from novel speakers,\\nsimilar to the target speaker's voice. We describe the metrics used for\\nevaluation, a baseline system consisting of unsupervised subword unit discovery\\nplus a standard TTS system, and a topline TTS using gold phoneme\\ntranscriptions. We present an overview of the 19 submitted systems from 10\\nteams and discuss the main results.\\n",
      "abstract": "We present the Zero Resource Speech Challenge 2019, which proposes to build a\\nspeech synthesizer without any text or phonetic labels: hence, TTS without T\\n(text-to-speech without text). We provide raw audio for a target voice in an\\nunknown language (the Voice dataset), but no alignment, text or labels.\\nParticipants must discover subword units in an unsupervised way (using the Unit\\nDiscovery dataset) and align them to the voice recordings in a way that works\\nbest for the purpose of synthesizing novel utterances from novel speakers,\\nsimilar to the target speaker's voice. We describe the metrics used for\\nevaluation, a baseline system consisting of unsupervised subword unit discovery\\nplus a standard TTS system, and a topline TTS using gold phoneme\\ntranscriptions. We present an overview of the 19 submitted systems from 10\\nteams and discuss the main results.\\n",
      "doi": "https://doi.org/10.21437/interspeech.2019-2904",
      "openalex_id": "https://openalex.org/W2940544976",
      "arxiv_id": "",
      "publication_date": "2019-09-13",
      "published": "2019-09-13",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "此為2019年的 Zero Resource Speech Challenge，時間落在過去3年之外，且摘要未以 Spoken Language Models / FD-SLMs 為核心的語音語言建模，故不符合條件，應排除。",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "此為2019年的 Zero Resource Speech Challenge，時間落在過去3年之外，且摘要未以 Spoken Language Models / FD-SLMs 為核心的語音語言建模，故不符合條件，應排除。",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on zero-resource TTS, building a speech synthesizer without text or phonetic labels, which is primarily about speech synthesis (TTS) technology and unsupervised subword unit discovery. There is no mention of spoken language models (SLMs), full-duplex interaction, end-to-end architecture for simultaneous speech understanding and generation, or interactive dialog behavior arbitration. The focus is on TTS from audio data without labels, not on speech language modeling or synchronous interaction with low latency or cognitive parallelism. The evaluation metrics and scope do not include multi-dimensional dynamic assessments related to full-duplex spoken language models or interactive behavior management. Therefore, the study does not meet the inclusion criteria centered on spoken language modeling and interactive full-duplex systems, and fits exclusion criteria as a work focused on single speech technology (TTS) without broader language modeling or interaction.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on zero-resource TTS, building a speech synthesizer without text or phonetic labels, which is primarily about speech synthesis (TTS) technology and unsupervised subword unit discovery. There is no mention of spoken language models (SLMs), full-duplex interaction, end-to-end architecture for simultaneous speech understanding and generation, or interactive dialog behavior arbitration. The focus is on TTS from audio data without labels, not on speech language modeling or synchronous interaction with low latency or cognitive parallelism. The evaluation metrics and scope do not include multi-dimensional dynamic assessments related to full-duplex spoken language models or interactive behavior management. Therefore, the study does not meet the inclusion criteria centered on spoken language modeling and interactive full-duplex systems, and fits exclusion criteria as a work focused on single speech technology (TTS) without broader language modeling or interaction.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "The Zero Resource Speech Challenge 2020: Discovering Discrete Subword and Word Units",
    "abstract": "We present the Zero Resource Speech Challenge 2020, which aims at learning speech representations from raw audio signals without any labels. It combines the data sets and metrics from two previous benchmarks (2017 and 2019) and features two tasks which tap into two levels of speech representation. The first task is to discover low bit-rate subword representations that optimize the quality of speech synthesis; the second one is to discover word-like units from unsegmented raw speech. We present the results of the twenty submitted models and discuss the implications of the main findings for unsupervised speech learning.",
    "metadata": {
      "title": "The Zero Resource Speech Challenge 2020: Discovering Discrete Subword and Word Units",
      "summary": "We present the Zero Resource Speech Challenge 2020, which aims at learning speech representations from raw audio signals without any labels. It combines the data sets and metrics from two previous benchmarks (2017 and 2019) and features two tasks which tap into two levels of speech representation. The first task is to discover low bit-rate subword representations that optimize the quality of speech synthesis; the second one is to discover word-like units from unsegmented raw speech. We present the results of the twenty submitted models and discuss the implications of the main findings for unsupervised speech learning.",
      "abstract": "We present the Zero Resource Speech Challenge 2020, which aims at learning speech representations from raw audio signals without any labels. It combines the data sets and metrics from two previous benchmarks (2017 and 2019) and features two tasks which tap into two levels of speech representation. The first task is to discover low bit-rate subword representations that optimize the quality of speech synthesis; the second one is to discover word-like units from unsegmented raw speech. We present the results of the twenty submitted models and discuss the implications of the main findings for unsupervised speech learning.",
      "doi": "https://doi.org/10.21437/interspeech.2020-2743",
      "openalex_id": "https://openalex.org/W3093096176",
      "arxiv_id": "",
      "publication_date": "2020-10-25",
      "published": "2020-10-25",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "此項目為 2020 年的 Zero Resource Speech Challenge，未在標題或摘要中出現 spoken language model / speech language model 的核心描述，且超出近三年的時程限制，因此不符合 inclusion 條件，應排除。",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "此項目為 2020 年的 Zero Resource Speech Challenge，未在標題或摘要中出現 spoken language model / speech language model 的核心描述，且超出近三年的時程限制，因此不符合 inclusion 條件，應排除。",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on unsupervised speech representations from raw audio signals without labels, presenting models for discovering subword and word units, which relates to speech-level representation learning but does not explicitly discuss spoken language models, full-duplex interaction, synchronization, or real-time interaction aspects emphasized in the inclusion criteria. It fits within the past 3 years and involves speech modeling but mainly targets low bit-rate subword and word unit discovery for synthesis quality and word-like unit discovery, not explicitly spoken language modeling or interactive dialogue behavior arbitration. Hence, it partially meets the criteria but lacks clear connection to core topics such as Spoken Language Models, FD-SLMs, or synchronization.",
      "evaluation": 3
    },
    "round-A_JuniorMini_reasoning": "The study focuses on unsupervised speech representations from raw audio signals without labels, presenting models for discovering subword and word units, which relates to speech-level representation learning but does not explicitly discuss spoken language models, full-duplex interaction, synchronization, or real-time interaction aspects emphasized in the inclusion criteria. It fits within the past 3 years and involves speech modeling but mainly targets low bit-rate subword and word unit discovery for synthesis quality and word-like unit discovery, not explicitly spoken language modeling or interactive dialogue behavior arbitration. Hence, it partially meets the criteria but lacks clear connection to core topics such as Spoken Language Models, FD-SLMs, or synchronization.",
    "round-A_JuniorMini_evaluation": 3,
    "round-B_SeniorLead_output": {
      "reasoning": "綜合兩位初審意見：JuniorNano 建議排除（理由：為 2020 年的挑戰賽，不在近三年內），JuniorMini 對其與語音表示學習的相關性持中立看法但也指出摘要未明確為 Spoken Language Models/FD-SLM 等主題。依據納入準則，研究必須在標題或摘要明確以 “Spoken Language Models” 或等價描述為核心，且為過去三年內的工作；本項目為 2020 年 Zero Resource Speech Challenge，重點在無監督語音表示、子字/詞單位發現與語音合成評估，摘要未提及 spoken language model / speech language model、full-duplex 互動、同步或行為仲裁等要求，且超出時限，因此不符合納入條件並同時觸及多項排除條件。 1-sentence explanation: 不符合時間範圍且未以 Spoken Language Models / FD-SLM 等語音語言建模議題為核心，因此應排除。",
      "evaluation": 1
    },
    "round-B_SeniorLead_reasoning": "綜合兩位初審意見：JuniorNano 建議排除（理由：為 2020 年的挑戰賽，不在近三年內），JuniorMini 對其與語音表示學習的相關性持中立看法但也指出摘要未明確為 Spoken Language Models/FD-SLM 等主題。依據納入準則，研究必須在標題或摘要明確以 “Spoken Language Models” 或等價描述為核心，且為過去三年內的工作；本項目為 2020 年 Zero Resource Speech Challenge，重點在無監督語音表示、子字/詞單位發現與語音合成評估，摘要未提及 spoken language model / speech language model、full-duplex 互動、同步或行為仲裁等要求，且超出時限，因此不符合納入條件並同時觸及多項排除條件。 1-sentence explanation: 不符合時間範圍且未以 Spoken Language Models / FD-SLM 等語音語言建模議題為核心，因此應排除。",
    "round-B_SeniorLead_evaluation": 1,
    "final_verdict": "exclude (senior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Parallel inference of dirichlet process Gaussian mixture models for unsupervised acoustic modeling: a feasibility study",
    "abstract": "We adopt a Dirichlet process Gaussian mixture model (DPGMM) for unsupervised acoustic modeling and represent speech frames with Gaussian posteriorgrams. The model performs unsupervised clustering on untranscribed data, and each Gaussian component can be considered as a cluster of sounds from various speakers. The model infers its model complexity (i.e. the number of Gaussian components) from the data. For computation efficiency, we use a parallel sampler for the model inference. Our experiments are conducted on the corpus provided by the zero resource speech challenge. Experimental results show that the unsupervised DPGMM posteriorgrams obviously outperformMFCC, and perform comparably to the posteriorgrams derived from language-mismatched phoneme recognizers in terms of the error rate of ABX discrimination test. The error rates can be further reduced by the fusion of these two kinds of posteriorgrams.",
    "metadata": {
      "title": "Parallel inference of dirichlet process Gaussian mixture models for unsupervised acoustic modeling: a feasibility study",
      "summary": "We adopt a Dirichlet process Gaussian mixture model (DPGMM) for unsupervised acoustic modeling and represent speech frames with Gaussian posteriorgrams. The model performs unsupervised clustering on untranscribed data, and each Gaussian component can be considered as a cluster of sounds from various speakers. The model infers its model complexity (i.e. the number of Gaussian components) from the data. For computation efficiency, we use a parallel sampler for the model inference. Our experiments are conducted on the corpus provided by the zero resource speech challenge. Experimental results show that the unsupervised DPGMM posteriorgrams obviously outperformMFCC, and perform comparably to the posteriorgrams derived from language-mismatched phoneme recognizers in terms of the error rate of ABX discrimination test. The error rates can be further reduced by the fusion of these two kinds of posteriorgrams.",
      "abstract": "We adopt a Dirichlet process Gaussian mixture model (DPGMM) for unsupervised acoustic modeling and represent speech frames with Gaussian posteriorgrams. The model performs unsupervised clustering on untranscribed data, and each Gaussian component can be considered as a cluster of sounds from various speakers. The model infers its model complexity (i.e. the number of Gaussian components) from the data. For computation efficiency, we use a parallel sampler for the model inference. Our experiments are conducted on the corpus provided by the zero resource speech challenge. Experimental results show that the unsupervised DPGMM posteriorgrams obviously outperformMFCC, and perform comparably to the posteriorgrams derived from language-mismatched phoneme recognizers in terms of the error rate of ABX discrimination test. The error rates can be further reduced by the fusion of these two kinds of posteriorgrams.",
      "doi": "https://doi.org/10.21437/interspeech.2015-642",
      "openalex_id": "https://openalex.org/W2399576818",
      "arxiv_id": "",
      "publication_date": "2015-09-06",
      "published": "2015-09-06",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The paper centers on unsupervised acoustic modeling using DPGMM and posteriorgrams, with no explicit focus on Spoken Language Models or speech-language modeling in the title/abstract, thus failing the SLM-oriented inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The paper centers on unsupervised acoustic modeling using DPGMM and posteriorgrams, with no explicit focus on Spoken Language Models or speech-language modeling in the title/abstract, thus failing the SLM-oriented inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on unsupervised acoustic modeling using a Dirichlet process Gaussian mixture model (DPGMM) to cluster speech frames, which pertains to acoustic representation but does not explicitly mention spoken language models, speech language models, or language modeling of speech as core concepts. The abstract lacks discussion on full-duplex spoken language models, synchronization, turn-taking, or real-time interaction, and does not highlight model architectures oriented towards spoken language modeling or interaction, nor does it reference evaluation frameworks related to those topics. Hence, while it addresses speech acoustic modeling, it does not meet all inclusion criteria centered on spoken language models and interactive speech modeling.",
      "evaluation": 2
    },
    "round-A_JuniorMini_reasoning": "The study focuses on unsupervised acoustic modeling using a Dirichlet process Gaussian mixture model (DPGMM) to cluster speech frames, which pertains to acoustic representation but does not explicitly mention spoken language models, speech language models, or language modeling of speech as core concepts. The abstract lacks discussion on full-duplex spoken language models, synchronization, turn-taking, or real-time interaction, and does not highlight model architectures oriented towards spoken language modeling or interaction, nor does it reference evaluation frameworks related to those topics. Hence, while it addresses speech acoustic modeling, it does not meet all inclusion criteria centered on spoken language models and interactive speech modeling.",
    "round-A_JuniorMini_evaluation": 2,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:2)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Partitioning of Posteriorgrams Using Siamese Models for Unsupervised Acoustic Modelling",
    "abstract": "Unsupervised methods tend to discover highly speaker-specific representations of speech.We propose a method for improving the quality of posteriorgrams generated from an unsupervised model through partitioning of the latent classes.We do this by training a sparse siamese model to find a linear transformation of the input posteriorgrams to lower-dimensional posteriorgrams.The siamese model makes use of same-category and differentcategory speech fragment pairs obtained by unsupervised term discovery.After training, the model is converted into an exact partitioning of the posteriorgrams.We evaluate the model on the minimal-pair ABX task in the context of the Zero Resource Speech Challenge.We are able to demonstrate that our method significantly reduces the dimensionality of standard Gaussian mixture model posteriorgrams, while still making them more robust to speaker variations.This suggests that the model may be viable as a general post-processing step to improve probabilistic acoustic features obtained by unsupervised learning.",
    "metadata": {
      "title": "Partitioning of Posteriorgrams Using Siamese Models for Unsupervised Acoustic Modelling",
      "summary": "Unsupervised methods tend to discover highly speaker-specific representations of speech.We propose a method for improving the quality of posteriorgrams generated from an unsupervised model through partitioning of the latent classes.We do this by training a sparse siamese model to find a linear transformation of the input posteriorgrams to lower-dimensional posteriorgrams.The siamese model makes use of same-category and differentcategory speech fragment pairs obtained by unsupervised term discovery.After training, the model is converted into an exact partitioning of the posteriorgrams.We evaluate the model on the minimal-pair ABX task in the context of the Zero Resource Speech Challenge.We are able to demonstrate that our method significantly reduces the dimensionality of standard Gaussian mixture model posteriorgrams, while still making them more robust to speaker variations.This suggests that the model may be viable as a general post-processing step to improve probabilistic acoustic features obtained by unsupervised learning.",
      "abstract": "Unsupervised methods tend to discover highly speaker-specific representations of speech.We propose a method for improving the quality of posteriorgrams generated from an unsupervised model through partitioning of the latent classes.We do this by training a sparse siamese model to find a linear transformation of the input posteriorgrams to lower-dimensional posteriorgrams.The siamese model makes use of same-category and differentcategory speech fragment pairs obtained by unsupervised term discovery.After training, the model is converted into an exact partitioning of the posteriorgrams.We evaluate the model on the minimal-pair ABX task in the context of the Zero Resource Speech Challenge.We are able to demonstrate that our method significantly reduces the dimensionality of standard Gaussian mixture model posteriorgrams, while still making them more robust to speaker variations.This suggests that the model may be viable as a general post-processing step to improve probabilistic acoustic features obtained by unsupervised learning.",
      "doi": "https://doi.org/10.21437/glu.2017-6",
      "openalex_id": "https://openalex.org/W2745710152",
      "arxiv_id": "",
      "publication_date": "2017-08-25",
      "published": "2017-08-25",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item centers on unsupervised acoustic feature processing (posteriorgrams via siamese models) and does not clearly present Spoken Language Models or speech-language modeling as the core object or provide dialogue/interactive evaluation.",
      "evaluation": 2
    },
    "round-A_JuniorNano_reasoning": "The item centers on unsupervised acoustic feature processing (posteriorgrams via siamese models) and does not clearly present Spoken Language Models or speech-language modeling as the core object or provide dialogue/interactive evaluation.",
    "round-A_JuniorNano_evaluation": 2,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on unsupervised acoustic modeling by transforming posteriorgrams using a siamese model, related to speech representation; however, the abstract does not mention 'spoken language models' or equivalent terms, nor addresses full-duplex interaction, speech–LLM architectures, cognitive parallelism, or low-latency real-time interaction, which are key inclusion criteria. The work is more about acoustic feature improvement than end-to-end spoken language modeling or interactive dialogue behavior modeling, thus it does not fully meet the inclusion requirements and partially overlaps with exclusion criteria focusing only on acoustic features without linking to spoken language modeling or interaction.",
      "evaluation": 2
    },
    "round-A_JuniorMini_reasoning": "The study focuses on unsupervised acoustic modeling by transforming posteriorgrams using a siamese model, related to speech representation; however, the abstract does not mention 'spoken language models' or equivalent terms, nor addresses full-duplex interaction, speech–LLM architectures, cognitive parallelism, or low-latency real-time interaction, which are key inclusion criteria. The work is more about acoustic feature improvement than end-to-end spoken language modeling or interactive dialogue behavior modeling, thus it does not fully meet the inclusion requirements and partially overlaps with exclusion criteria focusing only on acoustic features without linking to spoken language modeling or interaction.",
    "round-A_JuniorMini_evaluation": 2,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:2)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Unsupervised Linear Discriminant Analysis for Supporting DPGMM Clustering in the Zero Resource Scenario",
    "abstract": "In this work we make use of unsupervised linear discriminant analysis (LDA) to support acoustic unit discovery in a zero resource scenario. The idea is to automatically find a mapping of feature vectors into a subspace that is more suitable for Dirichlet process Gaussian mixture model (DPGMM) based clustering, without the need of supervision. Supervised acoustic modeling typically makes use of feature transformations such as LDA to minimize intra-class discriminability, to maximize inter-class discriminability and to extract relevant informations from high-dimensional features spanning larger contexts. The need of class labels makes it difficult to use this technique in a zero resource setting where the classes and even their amount are unknown. To overcome this issue we use a first iteration of DPGMM clustering on standard features to generate labels for the data, that serve as basis for learning a proper transformation. A second clustering operates on the transformed features. The application of unsupervised LDA demonstrably leads to better clustering results given the unsupervised data. We show that the improved input features consistently outperform our baseline input features.",
    "metadata": {
      "title": "Unsupervised Linear Discriminant Analysis for Supporting DPGMM Clustering in the Zero Resource Scenario",
      "summary": "In this work we make use of unsupervised linear discriminant analysis (LDA) to support acoustic unit discovery in a zero resource scenario. The idea is to automatically find a mapping of feature vectors into a subspace that is more suitable for Dirichlet process Gaussian mixture model (DPGMM) based clustering, without the need of supervision. Supervised acoustic modeling typically makes use of feature transformations such as LDA to minimize intra-class discriminability, to maximize inter-class discriminability and to extract relevant informations from high-dimensional features spanning larger contexts. The need of class labels makes it difficult to use this technique in a zero resource setting where the classes and even their amount are unknown. To overcome this issue we use a first iteration of DPGMM clustering on standard features to generate labels for the data, that serve as basis for learning a proper transformation. A second clustering operates on the transformed features. The application of unsupervised LDA demonstrably leads to better clustering results given the unsupervised data. We show that the improved input features consistently outperform our baseline input features.",
      "abstract": "In this work we make use of unsupervised linear discriminant analysis (LDA) to support acoustic unit discovery in a zero resource scenario. The idea is to automatically find a mapping of feature vectors into a subspace that is more suitable for Dirichlet process Gaussian mixture model (DPGMM) based clustering, without the need of supervision. Supervised acoustic modeling typically makes use of feature transformations such as LDA to minimize intra-class discriminability, to maximize inter-class discriminability and to extract relevant informations from high-dimensional features spanning larger contexts. The need of class labels makes it difficult to use this technique in a zero resource setting where the classes and even their amount are unknown. To overcome this issue we use a first iteration of DPGMM clustering on standard features to generate labels for the data, that serve as basis for learning a proper transformation. A second clustering operates on the transformed features. The application of unsupervised LDA demonstrably leads to better clustering results given the unsupervised data. We show that the improved input features consistently outperform our baseline input features.",
      "doi": "https://doi.org/10.1016/j.procs.2016.04.032",
      "openalex_id": "https://openalex.org/W2345811097",
      "arxiv_id": "",
      "publication_date": "2016-01-01",
      "published": "2016-01-01",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item centers on unsupervised feature transformation for acoustic unit discovery and clustering, not on Spoken Language Models or Speech Language Modeling as core, and it does not address interactive/dialogue aspects or multi-modal/speech-language modeling evaluations; thus it does not meet the inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item centers on unsupervised feature transformation for acoustic unit discovery and clustering, not on Spoken Language Models or Speech Language Modeling as core, and it does not address interactive/dialogue aspects or multi-modal/speech-language modeling evaluations; thus it does not meet the inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on unsupervised linear discriminant analysis to support clustering for acoustic unit discovery in a zero resource scenario, primarily addressing feature transformation for acoustic data without mentioning spoken language models, speech language modeling, full-duplex interaction, or any related modeling or evaluation aspects specified in the inclusion criteria. It lacks references to spoken language models or full-duplex spoken language models as core concepts, and there is no discussion on synchronization, real-time interaction, or turn-taking management.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on unsupervised linear discriminant analysis to support clustering for acoustic unit discovery in a zero resource scenario, primarily addressing feature transformation for acoustic data without mentioning spoken language models, speech language modeling, full-duplex interaction, or any related modeling or evaluation aspects specified in the inclusion criteria. It lacks references to spoken language models or full-duplex spoken language models as core concepts, and there is no discussion on synchronization, real-time interaction, or turn-taking management.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "A comparison of neural network methods for unsupervised representation learning on the zero resource speech challenge",
    "abstract": "The success of supervised deep neural networks (DNNs) in speech recognition cannot be transferred to zero-resource languages where the requisite transcriptions are unavailable. We investigate unsupervised neural network based methods for learning frame-level representations. Good frame representations eliminate differences in accent, gender, channel characteristics, and other factors to model subword units for withinand acrossspeaker phonetic discrimination. We enhance the correspondence autoencoder (cAE) and show that it can transform Mel Frequency Cepstral Coefficients (MFCCs) into more effective frame representations given a set of matched word pairs from an unsupervised term discovery (UTD) system. The cAE combines the feature extraction power of autoencoders with the weak supervision signal from UTD pairs to better approximate the extrinsic task’s objective during training. We use the Zero Resource Speech Challenge’s minimal triphone pair ABX discrimination task to evaluate our methods. Optimizing a cAE architecture on English and applying it to a zero-resource language, Xitsonga, we obtain a relative error rate reduction of 35% compared to the original MFCCs. We also show that Xitsonga frame representations extracted from the bottleneck layer of a supervised DNN trained on English can be further enhanced by the cAE, yielding a relative error rate reduction of 39%.",
    "metadata": {
      "title": "A comparison of neural network methods for unsupervised representation learning on the zero resource speech challenge",
      "summary": "The success of supervised deep neural networks (DNNs) in speech recognition cannot be transferred to zero-resource languages where the requisite transcriptions are unavailable. We investigate unsupervised neural network based methods for learning frame-level representations. Good frame representations eliminate differences in accent, gender, channel characteristics, and other factors to model subword units for withinand acrossspeaker phonetic discrimination. We enhance the correspondence autoencoder (cAE) and show that it can transform Mel Frequency Cepstral Coefficients (MFCCs) into more effective frame representations given a set of matched word pairs from an unsupervised term discovery (UTD) system. The cAE combines the feature extraction power of autoencoders with the weak supervision signal from UTD pairs to better approximate the extrinsic task’s objective during training. We use the Zero Resource Speech Challenge’s minimal triphone pair ABX discrimination task to evaluate our methods. Optimizing a cAE architecture on English and applying it to a zero-resource language, Xitsonga, we obtain a relative error rate reduction of 35% compared to the original MFCCs. We also show that Xitsonga frame representations extracted from the bottleneck layer of a supervised DNN trained on English can be further enhanced by the cAE, yielding a relative error rate reduction of 39%.",
      "abstract": "The success of supervised deep neural networks (DNNs) in speech recognition cannot be transferred to zero-resource languages where the requisite transcriptions are unavailable. We investigate unsupervised neural network based methods for learning frame-level representations. Good frame representations eliminate differences in accent, gender, channel characteristics, and other factors to model subword units for withinand acrossspeaker phonetic discrimination. We enhance the correspondence autoencoder (cAE) and show that it can transform Mel Frequency Cepstral Coefficients (MFCCs) into more effective frame representations given a set of matched word pairs from an unsupervised term discovery (UTD) system. The cAE combines the feature extraction power of autoencoders with the weak supervision signal from UTD pairs to better approximate the extrinsic task’s objective during training. We use the Zero Resource Speech Challenge’s minimal triphone pair ABX discrimination task to evaluate our methods. Optimizing a cAE architecture on English and applying it to a zero-resource language, Xitsonga, we obtain a relative error rate reduction of 35% compared to the original MFCCs. We also show that Xitsonga frame representations extracted from the bottleneck layer of a supervised DNN trained on English can be further enhanced by the cAE, yielding a relative error rate reduction of 39%.",
      "doi": "https://doi.org/10.21437/interspeech.2015-644",
      "openalex_id": "https://openalex.org/W1796128977",
      "arxiv_id": "",
      "publication_date": "2015-09-06",
      "published": "2015-09-06",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "此項目聚焦於無監督的幀級表徵學習與零資源語音挑戰的評估（ABX），主要是在語音信號層面的表示學習與特徵轉換，未以「Spoken Language Models（SLMs）/全雙工互動語音模型」為核心，也未討論端到端/混合式架構的訓練推理、同步互動、語音協調與即時回饋等題材，且評估指標非僅限語音辨識表現。因此不符合題意中關於以語音為核心的語音語言建模與全雙工互動評估框架的包含條件，屬於不符合的範疇。",
      "evaluation": 2
    },
    "round-A_JuniorNano_reasoning": "此項目聚焦於無監督的幀級表徵學習與零資源語音挑戰的評估（ABX），主要是在語音信號層面的表示學習與特徵轉換，未以「Spoken Language Models（SLMs）/全雙工互動語音模型」為核心，也未討論端到端/混合式架構的訓練推理、同步互動、語音協調與即時回饋等題材，且評估指標非僅限語音辨識表現。因此不符合題意中關於以語音為核心的語音語言建模與全雙工互動評估框架的包含條件，屬於不符合的範疇。",
    "round-A_JuniorNano_evaluation": 2,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on unsupervised neural network methods for frame-level acoustic representation learning in zero-resource speech recognition, which pertains to speech signal processing but does not explicitly cover Spoken Language Models, full-duplex interaction, or models treating speech as core for end-to-end modeling and interaction. The title and abstract do not mention spoken language models (SLMs) or equivalent language modeling concepts, nor do they discuss interactive aspects or low-latency synchronization, which are crucial inclusion criteria. The evaluation metric is phonetic discrimination error reduction rather than dialogue management or interaction metrics. Therefore, it does not meet the core thematic or architectural criteria required by the inclusion rules and aligns more with traditional representation learning for ASR tasks than with speech language modeling or interaction frameworks.",
      "evaluation": 2
    },
    "round-A_JuniorMini_reasoning": "The study focuses on unsupervised neural network methods for frame-level acoustic representation learning in zero-resource speech recognition, which pertains to speech signal processing but does not explicitly cover Spoken Language Models, full-duplex interaction, or models treating speech as core for end-to-end modeling and interaction. The title and abstract do not mention spoken language models (SLMs) or equivalent language modeling concepts, nor do they discuss interactive aspects or low-latency synchronization, which are crucial inclusion criteria. The evaluation metric is phonetic discrimination error reduction rather than dialogue management or interaction metrics. Therefore, it does not meet the core thematic or architectural criteria required by the inclusion rules and aligns more with traditional representation learning for ASR tasks than with speech language modeling or interaction frameworks.",
    "round-A_JuniorMini_evaluation": 2,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:2)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "A hybrid dynamic time warping-deep neural network architecture for unsupervised acoustic modeling",
    "abstract": "We report on an architecture for the unsupervised discovery of talker-invariant subword embeddings. It is made out of two components: a dynamic-time warping based spoken term discovery (STD) system and a Siamese deep neural network (DNN). The STD system clusters word-sized repeated fragments in the acoustic streams while the DNN is trained to minimize the distance between time aligned frames of tokens of the same cluster, and maximize the distance between tokens of different clusters. We use additional side information regarding the average duration of phonemic units, as well as talker identity tags. For evaluation we use the datasets and metrics of the Zero Resource Speech Challenge. The model shows improvement over the baseline in subword unit modeling.",
    "metadata": {
      "title": "A hybrid dynamic time warping-deep neural network architecture for unsupervised acoustic modeling",
      "summary": "We report on an architecture for the unsupervised discovery of talker-invariant subword embeddings. It is made out of two components: a dynamic-time warping based spoken term discovery (STD) system and a Siamese deep neural network (DNN). The STD system clusters word-sized repeated fragments in the acoustic streams while the DNN is trained to minimize the distance between time aligned frames of tokens of the same cluster, and maximize the distance between tokens of different clusters. We use additional side information regarding the average duration of phonemic units, as well as talker identity tags. For evaluation we use the datasets and metrics of the Zero Resource Speech Challenge. The model shows improvement over the baseline in subword unit modeling.",
      "abstract": "We report on an architecture for the unsupervised discovery of talker-invariant subword embeddings. It is made out of two components: a dynamic-time warping based spoken term discovery (STD) system and a Siamese deep neural network (DNN). The STD system clusters word-sized repeated fragments in the acoustic streams while the DNN is trained to minimize the distance between time aligned frames of tokens of the same cluster, and maximize the distance between tokens of different clusters. We use additional side information regarding the average duration of phonemic units, as well as talker identity tags. For evaluation we use the datasets and metrics of the Zero Resource Speech Challenge. The model shows improvement over the baseline in subword unit modeling.",
      "doi": "https://doi.org/10.21437/interspeech.2015-640",
      "openalex_id": "https://openalex.org/W2404799143",
      "arxiv_id": "",
      "publication_date": "2015-09-06",
      "published": "2015-09-06",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The article focuses on unsupervised acoustic modeling and subword discovery rather than on Spoken Language Models or full-duplex interactive speech modeling, and it does not clearly center on turn-taking, interactive speech, or explicit speech-language modeling concepts as required.",
      "evaluation": 2
    },
    "round-A_JuniorNano_reasoning": "The article focuses on unsupervised acoustic modeling and subword discovery rather than on Spoken Language Models or full-duplex interactive speech modeling, and it does not clearly center on turn-taking, interactive speech, or explicit speech-language modeling concepts as required.",
    "round-A_JuniorNano_evaluation": 2,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on unsupervised acoustic modeling using a hybrid dynamic time warping and deep neural network architecture to discover talker-invariant subword embeddings, with an emphasis on spoken term discovery and subword unit modeling. However, the abstract does not explicitly mention spoken language models (SLMs) or full-duplex spoken language models (FD-SLMs), nor does it reference dialogue synchronization, cognitive parallelism, or real-time interaction aspects like low latency or speech overlap management. The evaluation also centers on zero resource speech challenge datasets and subword modeling improvements, without explicit discussion of speech language modeling architectures or turn-taking interaction metrics. Thus, while it relates to speech acoustic modeling, it does not clearly satisfy the core inclusion criteria for spoken language modeling or full-duplex interaction and thus does not strongly meet the inclusion criteria nor the exclusion criteria for irrelevant tasks such as ASR or TTS alone. Overall, it is borderline and uncertain if it fully fits the study scope.",
      "evaluation": 3
    },
    "round-A_JuniorMini_reasoning": "The study focuses on unsupervised acoustic modeling using a hybrid dynamic time warping and deep neural network architecture to discover talker-invariant subword embeddings, with an emphasis on spoken term discovery and subword unit modeling. However, the abstract does not explicitly mention spoken language models (SLMs) or full-duplex spoken language models (FD-SLMs), nor does it reference dialogue synchronization, cognitive parallelism, or real-time interaction aspects like low latency or speech overlap management. The evaluation also centers on zero resource speech challenge datasets and subword modeling improvements, without explicit discussion of speech language modeling architectures or turn-taking interaction metrics. Thus, while it relates to speech acoustic modeling, it does not clearly satisfy the core inclusion criteria for spoken language modeling or full-duplex interaction and thus does not strongly meet the inclusion criteria nor the exclusion criteria for irrelevant tasks such as ASR or TTS alone. Overall, it is borderline and uncertain if it fully fits the study scope.",
    "round-A_JuniorMini_evaluation": 3,
    "round-B_SeniorLead_output": {
      "reasoning": "The paper describes unsupervised acoustic/subword representation learning (DTW-based spoken term discovery + Siamese DNN) evaluated on Zero Resource metrics, but the title/abstract do not mention Spoken Language Models, speech-level language modeling, or any full‑duplex / synchronization / interaction aspects required by the inclusion criteria—both junior reviewers likewise flagged it as outside scope (one clear exclude, one borderline); therefore it should be excluded.",
      "evaluation": 2
    },
    "round-B_SeniorLead_reasoning": "The paper describes unsupervised acoustic/subword representation learning (DTW-based spoken term discovery + Siamese DNN) evaluated on Zero Resource metrics, but the title/abstract do not mention Spoken Language Models, speech-level language modeling, or any full‑duplex / synchronization / interaction aspects required by the inclusion criteria—both junior reviewers likewise flagged it as outside scope (one clear exclude, one borderline); therefore it should be excluded.",
    "round-B_SeniorLead_evaluation": 2,
    "final_verdict": "exclude (senior:2)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "A deep scattering spectrum — Deep Siamese network pipeline for unsupervised acoustic modeling",
    "abstract": "Recent work has explored deep architectures for learning acoustic features in an unsupervised or weakly-supervised way for phone recognition. Here we investigate the role of the input features, and in particular we test whether standard mel-scaled filterbanks could be replaced by inherently richer representations, such as derived from an analytic scattering spectrum. We use a Siamese network using lexical side information similar to a well-performing architecture used in the Zero Resource Speech Challenge (2015), and show a substantial improvement when the filterbanks are replaced by scattering features, even though these features yield similar performance when tested without training. This shows that unsupervised and weakly-supervised architectures can benefit from richer features than the traditional ones.",
    "metadata": {
      "title": "A deep scattering spectrum — Deep Siamese network pipeline for unsupervised acoustic modeling",
      "summary": "Recent work has explored deep architectures for learning acoustic features in an unsupervised or weakly-supervised way for phone recognition. Here we investigate the role of the input features, and in particular we test whether standard mel-scaled filterbanks could be replaced by inherently richer representations, such as derived from an analytic scattering spectrum. We use a Siamese network using lexical side information similar to a well-performing architecture used in the Zero Resource Speech Challenge (2015), and show a substantial improvement when the filterbanks are replaced by scattering features, even though these features yield similar performance when tested without training. This shows that unsupervised and weakly-supervised architectures can benefit from richer features than the traditional ones.",
      "abstract": "Recent work has explored deep architectures for learning acoustic features in an unsupervised or weakly-supervised way for phone recognition. Here we investigate the role of the input features, and in particular we test whether standard mel-scaled filterbanks could be replaced by inherently richer representations, such as derived from an analytic scattering spectrum. We use a Siamese network using lexical side information similar to a well-performing architecture used in the Zero Resource Speech Challenge (2015), and show a substantial improvement when the filterbanks are replaced by scattering features, even though these features yield similar performance when tested without training. This shows that unsupervised and weakly-supervised architectures can benefit from richer features than the traditional ones.",
      "doi": "https://doi.org/10.1109/icassp.2016.7472622",
      "openalex_id": "https://openalex.org/W2400549570",
      "arxiv_id": "",
      "publication_date": "2016-03-01",
      "published": "2016-03-01",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The paper centers on acoustic feature representations and unsupervised phonetic modeling, not on Spoken Language Models (SLMs) or dialogue-centric, real-time interactions, and it does not appear to be within the last 3 years.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The paper centers on acoustic feature representations and unsupervised phonetic modeling, not on Spoken Language Models (SLMs) or dialogue-centric, real-time interactions, and it does not appear to be within the last 3 years.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper focuses on using a deep Siamese network for unsupervised acoustic modeling, testing richer input features for phone recognition. However, it does not explicitly address Spoken Language Models or Full-Duplex Spoken Language Models, nor does it discuss turn-taking, synchronous interaction, real-time interaction, or evaluation metrics associated with dialogue or interactive speech behaviors. The work centers on acoustic feature learning rather than language modeling or synchronization in speech interaction.",
      "evaluation": 2
    },
    "round-A_JuniorMini_reasoning": "The paper focuses on using a deep Siamese network for unsupervised acoustic modeling, testing richer input features for phone recognition. However, it does not explicitly address Spoken Language Models or Full-Duplex Spoken Language Models, nor does it discuss turn-taking, synchronous interaction, real-time interaction, or evaluation metrics associated with dialogue or interactive speech behaviors. The work centers on acoustic feature learning rather than language modeling or synchronization in speech interaction.",
    "round-A_JuniorMini_evaluation": 2,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:2)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Deep learning methods for unsupervised acoustic modeling — Leap submission to ZeroSpeech challenge 2017",
    "abstract": "In this paper, we present our system submission to the ZeroSpeech 2017 Challenge. The track1 of this challenge is intended to develop language independent speech representations that provide the least pairwise ABX distance computed for within speaker and across speaker pairs of spoken words. We investigate two approaches based on deep learning methods for unsupervised modeling. In the first approach, a deep neural network (DNN) is trained on the posteriors of mixture component indices obtained from training a Gaussian mixture model (GMM)-UBM. In the second approach, we develop a similar hidden Markov model (HMM) based DNN model to learn the unsupervised acoustic units provided by HMM state alignments. In addition, we also develop a deep autoencoder which learns language independent embeddings of speech to train the HMM-DNN model. Both the approaches do not use any labeled training data or require any supervision. We perform several experiments using the ZeroSpeech 2017 corpus with the minimal pair ABX error measure. In these experiments, we find that the two proposed approaches significantly improve over the baseline system using MFCC features (average relative improvements of 30–40%). Furthermore, the system combination of the two proposed approaches improves the performance over the best individual system.",
    "metadata": {
      "title": "Deep learning methods for unsupervised acoustic modeling — Leap submission to ZeroSpeech challenge 2017",
      "summary": "In this paper, we present our system submission to the ZeroSpeech 2017 Challenge. The track1 of this challenge is intended to develop language independent speech representations that provide the least pairwise ABX distance computed for within speaker and across speaker pairs of spoken words. We investigate two approaches based on deep learning methods for unsupervised modeling. In the first approach, a deep neural network (DNN) is trained on the posteriors of mixture component indices obtained from training a Gaussian mixture model (GMM)-UBM. In the second approach, we develop a similar hidden Markov model (HMM) based DNN model to learn the unsupervised acoustic units provided by HMM state alignments. In addition, we also develop a deep autoencoder which learns language independent embeddings of speech to train the HMM-DNN model. Both the approaches do not use any labeled training data or require any supervision. We perform several experiments using the ZeroSpeech 2017 corpus with the minimal pair ABX error measure. In these experiments, we find that the two proposed approaches significantly improve over the baseline system using MFCC features (average relative improvements of 30–40%). Furthermore, the system combination of the two proposed approaches improves the performance over the best individual system.",
      "abstract": "In this paper, we present our system submission to the ZeroSpeech 2017 Challenge. The track1 of this challenge is intended to develop language independent speech representations that provide the least pairwise ABX distance computed for within speaker and across speaker pairs of spoken words. We investigate two approaches based on deep learning methods for unsupervised modeling. In the first approach, a deep neural network (DNN) is trained on the posteriors of mixture component indices obtained from training a Gaussian mixture model (GMM)-UBM. In the second approach, we develop a similar hidden Markov model (HMM) based DNN model to learn the unsupervised acoustic units provided by HMM state alignments. In addition, we also develop a deep autoencoder which learns language independent embeddings of speech to train the HMM-DNN model. Both the approaches do not use any labeled training data or require any supervision. We perform several experiments using the ZeroSpeech 2017 corpus with the minimal pair ABX error measure. In these experiments, we find that the two proposed approaches significantly improve over the baseline system using MFCC features (average relative improvements of 30–40%). Furthermore, the system combination of the two proposed approaches improves the performance over the best individual system.",
      "doi": "https://doi.org/10.1109/asru.2017.8269013",
      "openalex_id": "https://openalex.org/W2787223168",
      "arxiv_id": "",
      "publication_date": "2017-12-01",
      "published": "2017-12-01",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This item focuses on unsupervised acoustic modeling for speech representations and is not framed as Spoken Language Models (SLMs) nor within the last 3 years; thus it does not meet the core inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "This item focuses on unsupervised acoustic modeling for speech representations and is not framed as Spoken Language Models (SLMs) nor within the last 3 years; thus it does not meet the core inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on unsupervised acoustic modeling using deep learning, but it does not explicitly refer to 'Spoken Language Models' or any equivalent term, nor does it address full-duplex interaction capabilities, cognitive parallelism, or synchronization aspects mentioned in the inclusion criteria. It primarily emphasizes acoustic feature representation and ABX error measures without discussing end-to-end spoken language modeling or dialogue-level interaction, which are core themes in the inclusion criteria. The study also lacks mention of tasks related to real-time interactive behaviors, overlapping speech, or multi-dimensional evaluation beyond acoustic metrics.",
      "evaluation": 2
    },
    "round-A_JuniorMini_reasoning": "The study focuses on unsupervised acoustic modeling using deep learning, but it does not explicitly refer to 'Spoken Language Models' or any equivalent term, nor does it address full-duplex interaction capabilities, cognitive parallelism, or synchronization aspects mentioned in the inclusion criteria. It primarily emphasizes acoustic feature representation and ABX error measures without discussing end-to-end spoken language modeling or dialogue-level interaction, which are core themes in the inclusion criteria. The study also lacks mention of tasks related to real-time interactive behaviors, overlapping speech, or multi-dimensional evaluation beyond acoustic metrics.",
    "round-A_JuniorMini_evaluation": 2,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:2)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Multilingual bottle-neck feature learning from untranscribed speech",
    "abstract": "We propose to learn a low-dimensional feature representation for multiple languages without access to their manual transcription. The multilingual features are extracted from a shared bottleneck layer of a multi-task learning deep neural network which is trained using un-supervised phoneme-like labels. The unsupervised phoneme-like labels are obtained from language-dependent Dirichlet process Gaussian mixture models (DPGMMs). Vocal tract length normalization (VTLN) is applied to mel-frequency cepstral coefficients to reduce talker variation when DPGMMs are trained. The proposed features are evaluated using the ABX phoneme discriminability test in the Zero Resource Speech Challenge 2017. In the experiments, we show that the proposed features perform well across different languages, and they consistently outperform our previously proposed DPGMM posteriorgrams which topped the performance in the same challenge in 2015.",
    "metadata": {
      "title": "Multilingual bottle-neck feature learning from untranscribed speech",
      "summary": "We propose to learn a low-dimensional feature representation for multiple languages without access to their manual transcription. The multilingual features are extracted from a shared bottleneck layer of a multi-task learning deep neural network which is trained using un-supervised phoneme-like labels. The unsupervised phoneme-like labels are obtained from language-dependent Dirichlet process Gaussian mixture models (DPGMMs). Vocal tract length normalization (VTLN) is applied to mel-frequency cepstral coefficients to reduce talker variation when DPGMMs are trained. The proposed features are evaluated using the ABX phoneme discriminability test in the Zero Resource Speech Challenge 2017. In the experiments, we show that the proposed features perform well across different languages, and they consistently outperform our previously proposed DPGMM posteriorgrams which topped the performance in the same challenge in 2015.",
      "abstract": "We propose to learn a low-dimensional feature representation for multiple languages without access to their manual transcription. The multilingual features are extracted from a shared bottleneck layer of a multi-task learning deep neural network which is trained using un-supervised phoneme-like labels. The unsupervised phoneme-like labels are obtained from language-dependent Dirichlet process Gaussian mixture models (DPGMMs). Vocal tract length normalization (VTLN) is applied to mel-frequency cepstral coefficients to reduce talker variation when DPGMMs are trained. The proposed features are evaluated using the ABX phoneme discriminability test in the Zero Resource Speech Challenge 2017. In the experiments, we show that the proposed features perform well across different languages, and they consistently outperform our previously proposed DPGMM posteriorgrams which topped the performance in the same challenge in 2015.",
      "doi": "https://doi.org/10.1109/asru.2017.8269009",
      "openalex_id": "https://openalex.org/W2786902352",
      "arxiv_id": "",
      "publication_date": "2017-12-01",
      "published": "2017-12-01",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item focuses on multilingual speech feature learning (unsupervised phoneme-like labels) and not on Spoken Language Models or full-duplex interactive speech systems, and it predates the required 3-year window (2017). Therefore it does not meet the core inclusion criteria and fails the recency/exclusion filters.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item focuses on multilingual speech feature learning (unsupervised phoneme-like labels) and not on Spoken Language Models or full-duplex interactive speech systems, and it predates the required 3-year window (2017). Therefore it does not meet the core inclusion criteria and fails the recency/exclusion filters.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on learning speech feature representations from untranscribed speech using unsupervised methods, without explicitly involving spoken language models or full-duplex spoken language modeling architectures, nor does it discuss synchronization, real-time interaction, or behavioral arbitration; it mainly targets acoustic feature learning evaluated by phoneme discriminability, falling outside the inclusion criteria emphasizing SLM/FD-SLM with modeling, training, and evaluation of speech language models in interactive settings.",
      "evaluation": 2
    },
    "round-A_JuniorMini_reasoning": "The study focuses on learning speech feature representations from untranscribed speech using unsupervised methods, without explicitly involving spoken language models or full-duplex spoken language modeling architectures, nor does it discuss synchronization, real-time interaction, or behavioral arbitration; it mainly targets acoustic feature learning evaluated by phoneme discriminability, falling outside the inclusion criteria emphasizing SLM/FD-SLM with modeling, training, and evaluation of speech language models in interactive settings.",
    "round-A_JuniorMini_evaluation": 2,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:2)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Feature optimized DPGMM clustering for unsupervised subword modeling: A contribution to zerospeech 2017",
    "abstract": "This paper describes our unsupervised subword modeling pipeline for the zero resource speech challenge (ZeroSpeech) 2017. Our approach is built around the Dirichlet process Gaussian mixture model (DPGMM) that we use to cluster speech feature vectors into a dynamically sized set of classes. By considering each class an acoustic unit, speech can be represented as sequence of class posteriorgrams. We enhance this method by automatically optimizing the DPGMM sampler's input features in a multi-stage clustering framework, where we unsupervisedly learn transformations using LDA, MLLT and (basis) fMLLR to reduce variance in the features. We show that this optimization considerably boosts the subword modeling quality, according to the performance on the ABX phone discriminability task. For the first time, we apply inferred subword models to previously unseen data from a new set of speakers. We demonstrate our method's good generalization and the effectiveness of its blind speaker adaptation in extensive experiments on a multitude of datasets. Our pipeline has very little need for hyper-parameter adjustment and is entirely unsupervised, i.e., it only takes raw audio recordings as input, without requiring any pre-defined segmentation, explicit speaker IDs or other meta data.",
    "metadata": {
      "title": "Feature optimized DPGMM clustering for unsupervised subword modeling: A contribution to zerospeech 2017",
      "summary": "This paper describes our unsupervised subword modeling pipeline for the zero resource speech challenge (ZeroSpeech) 2017. Our approach is built around the Dirichlet process Gaussian mixture model (DPGMM) that we use to cluster speech feature vectors into a dynamically sized set of classes. By considering each class an acoustic unit, speech can be represented as sequence of class posteriorgrams. We enhance this method by automatically optimizing the DPGMM sampler's input features in a multi-stage clustering framework, where we unsupervisedly learn transformations using LDA, MLLT and (basis) fMLLR to reduce variance in the features. We show that this optimization considerably boosts the subword modeling quality, according to the performance on the ABX phone discriminability task. For the first time, we apply inferred subword models to previously unseen data from a new set of speakers. We demonstrate our method's good generalization and the effectiveness of its blind speaker adaptation in extensive experiments on a multitude of datasets. Our pipeline has very little need for hyper-parameter adjustment and is entirely unsupervised, i.e., it only takes raw audio recordings as input, without requiring any pre-defined segmentation, explicit speaker IDs or other meta data.",
      "abstract": "This paper describes our unsupervised subword modeling pipeline for the zero resource speech challenge (ZeroSpeech) 2017. Our approach is built around the Dirichlet process Gaussian mixture model (DPGMM) that we use to cluster speech feature vectors into a dynamically sized set of classes. By considering each class an acoustic unit, speech can be represented as sequence of class posteriorgrams. We enhance this method by automatically optimizing the DPGMM sampler's input features in a multi-stage clustering framework, where we unsupervisedly learn transformations using LDA, MLLT and (basis) fMLLR to reduce variance in the features. We show that this optimization considerably boosts the subword modeling quality, according to the performance on the ABX phone discriminability task. For the first time, we apply inferred subword models to previously unseen data from a new set of speakers. We demonstrate our method's good generalization and the effectiveness of its blind speaker adaptation in extensive experiments on a multitude of datasets. Our pipeline has very little need for hyper-parameter adjustment and is entirely unsupervised, i.e., it only takes raw audio recordings as input, without requiring any pre-defined segmentation, explicit speaker IDs or other meta data.",
      "doi": "https://doi.org/10.1109/asru.2017.8269011",
      "openalex_id": "https://openalex.org/W2787447541",
      "arxiv_id": "",
      "publication_date": "2017-12-01",
      "published": "2017-12-01",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The paper is an unsupervised subword modeling study from 2017 (not within the last 3 years) and does not explicitly discuss Spoken Language Models or Speech Language Models in its title/abstract, nor address full-duplex interactive speech or dialogue-based evaluation criteria, thus it should be excluded under the given criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The paper is an unsupervised subword modeling study from 2017 (not within the last 3 years) and does not explicitly discuss Spoken Language Models or Speech Language Models in its title/abstract, nor address full-duplex interactive speech or dialogue-based evaluation criteria, thus it should be excluded under the given criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper focuses on unsupervised subword modeling through clustering of speech features, which relates to speech acoustic representation. However, it does not explicitly discuss Spoken Language Models or language modeling of speech as a core part, nor does it address full-duplex interaction, synchronization, or behavior arbitration required by the inclusion criteria. The evaluation is based on phone discriminability rather than interactive or low-latency speech interaction metrics. Therefore, it does not meet the inclusion criteria centered on spoken language models or full-duplex interaction capabilities.",
      "evaluation": 2
    },
    "round-A_JuniorMini_reasoning": "The paper focuses on unsupervised subword modeling through clustering of speech features, which relates to speech acoustic representation. However, it does not explicitly discuss Spoken Language Models or language modeling of speech as a core part, nor does it address full-duplex interaction, synchronization, or behavior arbitration required by the inclusion criteria. The evaluation is based on phone discriminability rather than interactive or low-latency speech interaction metrics. Therefore, it does not meet the inclusion criteria centered on spoken language models or full-duplex interaction capabilities.",
    "round-A_JuniorMini_evaluation": 2,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:2)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Composite embedding systems for ZeroSpeech2017 Track1",
    "abstract": "This paper investigates novel composite embedding systems for language-independent high-performance feature extraction using triphone-based DNN-HMM and character-based end-to-end speech recognition systems. The DNN-HMM is trained with phoneme transcripts based on a large-scale Japanese ASR recipe included in the Kaldi toolkit from the Corpus of Spontaneous Japanese (CSJ) with some modifications. The end-to-end ASR system is based on a hybrid architecture consisting of an attention-based encoder-decoder and connectionist temporal classification. This model is trained with multi-language speech data using character transcripts in a pure end-to-end fashion without requiring phonemic representation. Posterior features, PCA-transformed features, and bottleneck features are extracted from the two systems; then, various combinations of features are explored. Additionally, a bypassed autoencoder (bypassed AE) is proposed to normalize speaker characteristics in an unsupervised manner. An evaluation using the ABX test showed that the DNN-HMM-based CSJ bottleneck features resulted in a good performance regardless of the input language. The pre-activation vectors extracted from the multilingual end-to-end system with PCA provided a somewhat better performance than did the CSJ bottleneck features. The bypassed AE yielded an improved performance over a baseline AE. The lowest error rates were obtained by composite features that concatenated the end-to-end features with the CSJ bottleneck features.",
    "metadata": {
      "title": "Composite embedding systems for ZeroSpeech2017 Track1",
      "summary": "This paper investigates novel composite embedding systems for language-independent high-performance feature extraction using triphone-based DNN-HMM and character-based end-to-end speech recognition systems. The DNN-HMM is trained with phoneme transcripts based on a large-scale Japanese ASR recipe included in the Kaldi toolkit from the Corpus of Spontaneous Japanese (CSJ) with some modifications. The end-to-end ASR system is based on a hybrid architecture consisting of an attention-based encoder-decoder and connectionist temporal classification. This model is trained with multi-language speech data using character transcripts in a pure end-to-end fashion without requiring phonemic representation. Posterior features, PCA-transformed features, and bottleneck features are extracted from the two systems; then, various combinations of features are explored. Additionally, a bypassed autoencoder (bypassed AE) is proposed to normalize speaker characteristics in an unsupervised manner. An evaluation using the ABX test showed that the DNN-HMM-based CSJ bottleneck features resulted in a good performance regardless of the input language. The pre-activation vectors extracted from the multilingual end-to-end system with PCA provided a somewhat better performance than did the CSJ bottleneck features. The bypassed AE yielded an improved performance over a baseline AE. The lowest error rates were obtained by composite features that concatenated the end-to-end features with the CSJ bottleneck features.",
      "abstract": "This paper investigates novel composite embedding systems for language-independent high-performance feature extraction using triphone-based DNN-HMM and character-based end-to-end speech recognition systems. The DNN-HMM is trained with phoneme transcripts based on a large-scale Japanese ASR recipe included in the Kaldi toolkit from the Corpus of Spontaneous Japanese (CSJ) with some modifications. The end-to-end ASR system is based on a hybrid architecture consisting of an attention-based encoder-decoder and connectionist temporal classification. This model is trained with multi-language speech data using character transcripts in a pure end-to-end fashion without requiring phonemic representation. Posterior features, PCA-transformed features, and bottleneck features are extracted from the two systems; then, various combinations of features are explored. Additionally, a bypassed autoencoder (bypassed AE) is proposed to normalize speaker characteristics in an unsupervised manner. An evaluation using the ABX test showed that the DNN-HMM-based CSJ bottleneck features resulted in a good performance regardless of the input language. The pre-activation vectors extracted from the multilingual end-to-end system with PCA provided a somewhat better performance than did the CSJ bottleneck features. The bypassed AE yielded an improved performance over a baseline AE. The lowest error rates were obtained by composite features that concatenated the end-to-end features with the CSJ bottleneck features.",
      "doi": "https://doi.org/10.1109/asru.2017.8269012",
      "openalex_id": "https://openalex.org/W2787426069",
      "arxiv_id": "",
      "publication_date": "2017-12-01",
      "published": "2017-12-01",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This item is from 2017 and focuses on feature extraction/ASR architectures rather than a Spoken Language Model (SLM) or speech-language modeling framework; it does not meet the last-3-years window and lacks explicit emphasis on SLM/FD-SLM concepts, thus should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "This item is from 2017 and focuses on feature extraction/ASR architectures rather than a Spoken Language Model (SLM) or speech-language modeling framework; it does not meet the last-3-years window and lacks explicit emphasis on SLM/FD-SLM concepts, thus should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on developing composite embeddings for zero-resource speech feature extraction and recognition systems, primarily related to ASR architectures and feature extraction methods. It does not explicitly mention Spoken Language Models, full-duplex interaction capabilities, real-time interaction, or synchronization aspects outlined in the inclusion criteria. The evaluation metric is an ABX test reflecting feature discriminability, not interactions or behavioral arbitration. Therefore, it does not meet the specific research focus on spoken language models or full-duplex spoken language modeling, nor does it discuss real-time interaction or synchronization.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on developing composite embeddings for zero-resource speech feature extraction and recognition systems, primarily related to ASR architectures and feature extraction methods. It does not explicitly mention Spoken Language Models, full-duplex interaction capabilities, real-time interaction, or synchronization aspects outlined in the inclusion criteria. The evaluation metric is an ABX test reflecting feature discriminability, not interactions or behavioral arbitration. Therefore, it does not meet the specific research focus on spoken language models or full-duplex spoken language modeling, nor does it discuss real-time interaction or synchronization.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Extracting bottleneck features and word-like pairs from untranscribed speech for feature representation",
    "abstract": "We propose a framework to learn a frame-level speech representation in a scenario where no manual transcription is available. Our framework is based on pairwise learning using bottleneck features (BNFs). Initial frame-level features are extracted from a bottleneck-shaped multilingual deep neural network (DNN) which is trained with unsupervised phoneme-like labels. Word-like pairs are discovered in the untranscribed speech using the initial features, and frame alignment is performed on each word-like speech pair. The matching frame pairs are used as input-output to train another DNN with the mean square error (MSE) loss function. The final frame-level features are extracted from an internal hidden layer of MSE-based DNN. Our pairwise learned feature representation is evaluated on the ZeroSpeech 2017 challenge. The experiments show that pairwise learning improves phoneme discrimination in 10s and 120s test conditions. We find that it is important to use BNFs as initial features when pairwise learning is performed. With more word pairs obtained from the Switchboard corpus and its manual transcription, the phoneme discrimination of three languages in the evaluation data can further be improved despite data mismatch.",
    "metadata": {
      "title": "Extracting bottleneck features and word-like pairs from untranscribed speech for feature representation",
      "summary": "We propose a framework to learn a frame-level speech representation in a scenario where no manual transcription is available. Our framework is based on pairwise learning using bottleneck features (BNFs). Initial frame-level features are extracted from a bottleneck-shaped multilingual deep neural network (DNN) which is trained with unsupervised phoneme-like labels. Word-like pairs are discovered in the untranscribed speech using the initial features, and frame alignment is performed on each word-like speech pair. The matching frame pairs are used as input-output to train another DNN with the mean square error (MSE) loss function. The final frame-level features are extracted from an internal hidden layer of MSE-based DNN. Our pairwise learned feature representation is evaluated on the ZeroSpeech 2017 challenge. The experiments show that pairwise learning improves phoneme discrimination in 10s and 120s test conditions. We find that it is important to use BNFs as initial features when pairwise learning is performed. With more word pairs obtained from the Switchboard corpus and its manual transcription, the phoneme discrimination of three languages in the evaluation data can further be improved despite data mismatch.",
      "abstract": "We propose a framework to learn a frame-level speech representation in a scenario where no manual transcription is available. Our framework is based on pairwise learning using bottleneck features (BNFs). Initial frame-level features are extracted from a bottleneck-shaped multilingual deep neural network (DNN) which is trained with unsupervised phoneme-like labels. Word-like pairs are discovered in the untranscribed speech using the initial features, and frame alignment is performed on each word-like speech pair. The matching frame pairs are used as input-output to train another DNN with the mean square error (MSE) loss function. The final frame-level features are extracted from an internal hidden layer of MSE-based DNN. Our pairwise learned feature representation is evaluated on the ZeroSpeech 2017 challenge. The experiments show that pairwise learning improves phoneme discrimination in 10s and 120s test conditions. We find that it is important to use BNFs as initial features when pairwise learning is performed. With more word pairs obtained from the Switchboard corpus and its manual transcription, the phoneme discrimination of three languages in the evaluation data can further be improved despite data mismatch.",
      "doi": "https://doi.org/10.1109/asru.2017.8269010",
      "openalex_id": "https://openalex.org/W2785415724",
      "arxiv_id": "",
      "publication_date": "2017-12-01",
      "published": "2017-12-01",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "文章聚焦於從未轉錄語音中學習框架以產生幀級特徵，屬於語音表示/SLM的相關研究；但未清楚落在近三年內公開，且多處提及 ZeroSpeech 2017 等較早期基準，未符合最近三年納入範圍，因此建議排除。",
      "evaluation": 2
    },
    "round-A_JuniorNano_reasoning": "文章聚焦於從未轉錄語音中學習框架以產生幀級特徵，屬於語音表示/SLM的相關研究；但未清楚落在近三年內公開，且多處提及 ZeroSpeech 2017 等較早期基準，未符合最近三年納入範圍，因此建議排除。",
    "round-A_JuniorNano_evaluation": 2,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on learning frame-level speech representations from untranscribed speech using bottleneck features and neural networks, evaluated on phoneme discrimination tasks, without explicit mention of spoken language models, full-duplex interaction, real-time turn-taking, or synchronization mechanisms; the work centers more on speech feature extraction rather than spoken language modeling or interactive dialog capabilities, and doesn't mention evaluation involving low-latency or behavioral arbitration aspects, which are central to inclusion criteria. Therefore, it does not meet the criteria of explicitly addressing spoken language models or full-duplex spoken language interaction architectures.",
      "evaluation": 2
    },
    "round-A_JuniorMini_reasoning": "The study focuses on learning frame-level speech representations from untranscribed speech using bottleneck features and neural networks, evaluated on phoneme discrimination tasks, without explicit mention of spoken language models, full-duplex interaction, real-time turn-taking, or synchronization mechanisms; the work centers more on speech feature extraction rather than spoken language modeling or interactive dialog capabilities, and doesn't mention evaluation involving low-latency or behavioral arbitration aspects, which are central to inclusion criteria. Therefore, it does not meet the criteria of explicitly addressing spoken language models or full-duplex spoken language interaction architectures.",
    "round-A_JuniorMini_evaluation": 2,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:2)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Data Augmenting Contrastive Learning of Speech Representations in the\\n Time Domain",
    "abstract": "Contrastive Predictive Coding (CPC), based on predicting future segments of\\nspeech based on past segments is emerging as a powerful algorithm for\\nrepresentation learning of speech signal. However, it still under-performs\\nother methods on unsupervised evaluation benchmarks. Here, we introduce\\nWavAugment, a time-domain data augmentation library and find that applying\\naugmentation in the past is generally more efficient and yields better\\nperformances than other methods. We find that a combination of pitch\\nmodification, additive noise and reverberation substantially increase the\\nperformance of CPC (relative improvement of 18-22%), beating the reference\\nLibri-light results with 600 times less data. Using an out-of-domain dataset,\\ntime-domain data augmentation can push CPC to be on par with the state of the\\nart on the Zero Speech Benchmark 2017. We also show that time-domain data\\naugmentation consistently improves downstream limited-supervision phoneme\\nclassification tasks by a factor of 12-15% relative.\\n",
    "metadata": {
      "title": "Data Augmenting Contrastive Learning of Speech Representations in the\\n Time Domain",
      "summary": "Contrastive Predictive Coding (CPC), based on predicting future segments of\\nspeech based on past segments is emerging as a powerful algorithm for\\nrepresentation learning of speech signal. However, it still under-performs\\nother methods on unsupervised evaluation benchmarks. Here, we introduce\\nWavAugment, a time-domain data augmentation library and find that applying\\naugmentation in the past is generally more efficient and yields better\\nperformances than other methods. We find that a combination of pitch\\nmodification, additive noise and reverberation substantially increase the\\nperformance of CPC (relative improvement of 18-22%), beating the reference\\nLibri-light results with 600 times less data. Using an out-of-domain dataset,\\ntime-domain data augmentation can push CPC to be on par with the state of the\\nart on the Zero Speech Benchmark 2017. We also show that time-domain data\\naugmentation consistently improves downstream limited-supervision phoneme\\nclassification tasks by a factor of 12-15% relative.\\n",
      "abstract": "Contrastive Predictive Coding (CPC), based on predicting future segments of\\nspeech based on past segments is emerging as a powerful algorithm for\\nrepresentation learning of speech signal. However, it still under-performs\\nother methods on unsupervised evaluation benchmarks. Here, we introduce\\nWavAugment, a time-domain data augmentation library and find that applying\\naugmentation in the past is generally more efficient and yields better\\nperformances than other methods. We find that a combination of pitch\\nmodification, additive noise and reverberation substantially increase the\\nperformance of CPC (relative improvement of 18-22%), beating the reference\\nLibri-light results with 600 times less data. Using an out-of-domain dataset,\\ntime-domain data augmentation can push CPC to be on par with the state of the\\nart on the Zero Speech Benchmark 2017. We also show that time-domain data\\naugmentation consistently improves downstream limited-supervision phoneme\\nclassification tasks by a factor of 12-15% relative.\\n",
      "doi": "https://doi.org/10.48550/arxiv.2007.00991",
      "openalex_id": "https://openalex.org/W3144810982",
      "arxiv_id": "",
      "publication_date": "2020-07-02",
      "published": "2020-07-02",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "Although it proposes data augmentation for speech representation learning (CPC), the title/abstract do not mention Spoken Language Models or Speech Language Models, nor discuss full-duplex interaction or core speech-language modeling aspects; thus it does not meet the explicit inclusion criterion that requires SLM/SLM-equivalent emphasis in title/abstract/keywords, and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "Although it proposes data augmentation for speech representation learning (CPC), the title/abstract do not mention Spoken Language Models or Speech Language Models, nor discuss full-duplex interaction or core speech-language modeling aspects; thus it does not meet the explicit inclusion criterion that requires SLM/SLM-equivalent emphasis in title/abstract/keywords, and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on improving representation learning of speech signals using contrastive predictive coding and data augmentation techniques; however, it lacks explicit mention of Spoken Language Models or equivalent concepts, does not address full-duplex interactions or dialogue behavior arbitration, and primarily evaluates unsupervised speech representation performance and phoneme classification without discussing synchronous interaction capabilities or specific spoken language modeling architectures.",
      "evaluation": 2
    },
    "round-A_JuniorMini_reasoning": "The study focuses on improving representation learning of speech signals using contrastive predictive coding and data augmentation techniques; however, it lacks explicit mention of Spoken Language Models or equivalent concepts, does not address full-duplex interactions or dialogue behavior arbitration, and primarily evaluates unsupervised speech representation performance and phoneme classification without discussing synchronous interaction capabilities or specific spoken language modeling architectures.",
    "round-A_JuniorMini_evaluation": 2,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:2)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "An evaluation of graph clustering methods for unsupervised term discovery",
    "abstract": "Unsupervised term discovery (UTD) is the task of automatically identifying the repeated words and phrases in a collection of speech audio without relying on any language-specific resources. While the solution space for the task is far from fully explored, the dominant approach to date decomposes the discovery problem into two steps, where (i) segmental dynamic time warping is used to search the speech audio for repeated acoustic patterns, and (ii) these individual repetitions are partitioned into word/phrase categories using graph clustering. In this paper, we perform an unprecedented evaluation of a wide range of advanced graph clustering methods for the UTD task. We conduct our study in the evaluation framework of the Zero Resource Speech Challenge. We find that, for a range of features and languages, modularity-based clustering improves UTD performance most consistently, often by a wide margin. When paired with out-of-language deep neural net bottleneck features, we find performance near that of a high-resource UTD system.",
    "metadata": {
      "title": "An evaluation of graph clustering methods for unsupervised term discovery",
      "summary": "Unsupervised term discovery (UTD) is the task of automatically identifying the repeated words and phrases in a collection of speech audio without relying on any language-specific resources. While the solution space for the task is far from fully explored, the dominant approach to date decomposes the discovery problem into two steps, where (i) segmental dynamic time warping is used to search the speech audio for repeated acoustic patterns, and (ii) these individual repetitions are partitioned into word/phrase categories using graph clustering. In this paper, we perform an unprecedented evaluation of a wide range of advanced graph clustering methods for the UTD task. We conduct our study in the evaluation framework of the Zero Resource Speech Challenge. We find that, for a range of features and languages, modularity-based clustering improves UTD performance most consistently, often by a wide margin. When paired with out-of-language deep neural net bottleneck features, we find performance near that of a high-resource UTD system.",
      "abstract": "Unsupervised term discovery (UTD) is the task of automatically identifying the repeated words and phrases in a collection of speech audio without relying on any language-specific resources. While the solution space for the task is far from fully explored, the dominant approach to date decomposes the discovery problem into two steps, where (i) segmental dynamic time warping is used to search the speech audio for repeated acoustic patterns, and (ii) these individual repetitions are partitioned into word/phrase categories using graph clustering. In this paper, we perform an unprecedented evaluation of a wide range of advanced graph clustering methods for the UTD task. We conduct our study in the evaluation framework of the Zero Resource Speech Challenge. We find that, for a range of features and languages, modularity-based clustering improves UTD performance most consistently, often by a wide margin. When paired with out-of-language deep neural net bottleneck features, we find performance near that of a high-resource UTD system.",
      "doi": "https://doi.org/10.21437/interspeech.2015-646",
      "openalex_id": "https://openalex.org/W2407614114",
      "arxiv_id": "",
      "publication_date": "2015-09-06",
      "published": "2015-09-06",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "Although the paper targets speech processing, it focuses on unsupervised term discovery and graph clustering rather than Spoken Language Models (SLMs) or FD-SLMs as the core modeling paradigm, and does not address speech-language modeling architecture/training/evaluation, so it does not meet inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "Although the paper targets speech processing, it focuses on unsupervised term discovery and graph clustering rather than Spoken Language Models (SLMs) or FD-SLMs as the core modeling paradigm, and does not address speech-language modeling architecture/training/evaluation, so it does not meet inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on unsupervised term discovery in speech audio using graph clustering methods and features acoustic pattern discovery, but it does not explicitly mention spoken language models or speech language models as core modeling objects, nor does it discuss full-duplex interactions, cognitive parallelism, or synchronization aspects required by the inclusion criteria. It also appears to be more about acoustic pattern clustering than modeling speech sequences or speech language modeling architectures explicitly. Therefore, it does not satisfy all inclusion criteria and meets some exclusion criteria due to missing core topics like spoken language modeling and synchronous interactive behavior.",
      "evaluation": 2
    },
    "round-A_JuniorMini_reasoning": "The study focuses on unsupervised term discovery in speech audio using graph clustering methods and features acoustic pattern discovery, but it does not explicitly mention spoken language models or speech language models as core modeling objects, nor does it discuss full-duplex interactions, cognitive parallelism, or synchronization aspects required by the inclusion criteria. It also appears to be more about acoustic pattern clustering than modeling speech sequences or speech language modeling architectures explicitly. Therefore, it does not satisfy all inclusion criteria and meets some exclusion criteria due to missing core topics like spoken language modeling and synchronous interactive behavior.",
    "round-A_JuniorMini_evaluation": 2,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:2)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Word Segmentation on Discovered Phone Units With Dynamic Programming and Self-Supervised Scoring",
    "abstract": "Recent work on unsupervised speech segmentation has used self-supervised models with phone and word segmentation modules that are trained jointly. This paper instead revisits an older approach to word segmentation: bottom-up phone-like unit discovery is performed first, and symbolic word segmentation is then performed on top of the discovered units (without influencing the lower level). To do this, I propose a new unit discovery model, a new symbolic word segmentation model, and then chain the two models to segment speech. Both models use dynamic programming to minimize segment costs from a self-supervised network with an additional duration penalty that encourages longer units. Concretely, for acoustic unit discovery, duration-penalized dynamic programming (DPDP) is used with a contrastive predictive coding model as the scoring network. For word segmentation, DPDP is applied with an autoencoding recurrent neural as the scoring network. The two models are chained in order to segment speech. This approach gives comparable word segmentation results to state-of-the-art joint self-supervised segmentation models on an English benchmark. On French, Mandarin, German and Wolof data, it outperforms previous systems on the ZeroSpeech benchmarks. Analysis shows that the chained DPDP system segments shorter filler words well, but longer words might require some external top-down signal.",
    "metadata": {
      "title": "Word Segmentation on Discovered Phone Units With Dynamic Programming and Self-Supervised Scoring",
      "summary": "Recent work on unsupervised speech segmentation has used self-supervised models with phone and word segmentation modules that are trained jointly. This paper instead revisits an older approach to word segmentation: bottom-up phone-like unit discovery is performed first, and symbolic word segmentation is then performed on top of the discovered units (without influencing the lower level). To do this, I propose a new unit discovery model, a new symbolic word segmentation model, and then chain the two models to segment speech. Both models use dynamic programming to minimize segment costs from a self-supervised network with an additional duration penalty that encourages longer units. Concretely, for acoustic unit discovery, duration-penalized dynamic programming (DPDP) is used with a contrastive predictive coding model as the scoring network. For word segmentation, DPDP is applied with an autoencoding recurrent neural as the scoring network. The two models are chained in order to segment speech. This approach gives comparable word segmentation results to state-of-the-art joint self-supervised segmentation models on an English benchmark. On French, Mandarin, German and Wolof data, it outperforms previous systems on the ZeroSpeech benchmarks. Analysis shows that the chained DPDP system segments shorter filler words well, but longer words might require some external top-down signal.",
      "abstract": "Recent work on unsupervised speech segmentation has used self-supervised models with phone and word segmentation modules that are trained jointly. This paper instead revisits an older approach to word segmentation: bottom-up phone-like unit discovery is performed first, and symbolic word segmentation is then performed on top of the discovered units (without influencing the lower level). To do this, I propose a new unit discovery model, a new symbolic word segmentation model, and then chain the two models to segment speech. Both models use dynamic programming to minimize segment costs from a self-supervised network with an additional duration penalty that encourages longer units. Concretely, for acoustic unit discovery, duration-penalized dynamic programming (DPDP) is used with a contrastive predictive coding model as the scoring network. For word segmentation, DPDP is applied with an autoencoding recurrent neural as the scoring network. The two models are chained in order to segment speech. This approach gives comparable word segmentation results to state-of-the-art joint self-supervised segmentation models on an English benchmark. On French, Mandarin, German and Wolof data, it outperforms previous systems on the ZeroSpeech benchmarks. Analysis shows that the chained DPDP system segments shorter filler words well, but longer words might require some external top-down signal.",
      "doi": "https://doi.org/10.1109/taslp.2022.3229264",
      "openalex_id": "https://openalex.org/W4313182775",
      "arxiv_id": "",
      "publication_date": "2022-12-14",
      "published": "2022-12-14",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The paper centers on unsupervised bottom-up phoneme-like unit discovery and DP-based word segmentation for speech, without addressing Spoken Language Models/ Speech Language Models, real-time interaction, or dialogue arbitration; thus it does not meet inclusion and aligns with the exclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The paper centers on unsupervised bottom-up phoneme-like unit discovery and DP-based word segmentation for speech, without addressing Spoken Language Models/ Speech Language Models, real-time interaction, or dialogue arbitration; thus it does not meet inclusion and aligns with the exclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on unsupervised speech segmentation by discovering phone-like units and segmenting words from speech, which relates to spoken language modeling at the acoustic unit level. However, it lacks explicit mention of spoken language models, full-duplex interaction, synchronization, or real-time interactive behavior modeling that are central to the inclusion criteria. The approach is more on unit discovery and segmentation rather than modeling dialogue interaction or full-duplex spoken language modeling. Also, there is no indication that the paper addresses low-latency interaction, multi-dimensional evaluation frameworks, or synchronization phenomena as required. Therefore, while it relates to speech modeling components, it does not meet the full criteria for inclusion.",
      "evaluation": 2
    },
    "round-A_JuniorMini_reasoning": "The study focuses on unsupervised speech segmentation by discovering phone-like units and segmenting words from speech, which relates to spoken language modeling at the acoustic unit level. However, it lacks explicit mention of spoken language models, full-duplex interaction, synchronization, or real-time interactive behavior modeling that are central to the inclusion criteria. The approach is more on unit discovery and segmentation rather than modeling dialogue interaction or full-duplex spoken language modeling. Also, there is no indication that the paper addresses low-latency interaction, multi-dimensional evaluation frameworks, or synchronization phenomena as required. Therefore, while it relates to speech modeling components, it does not meet the full criteria for inclusion.",
    "round-A_JuniorMini_evaluation": 2,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:2)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "DP-Parse: Finding Word Boundaries from Raw Speech with an Instance Lexicon",
    "abstract": "Abstract Finding word boundaries in continuous speech is challenging as there is little or no equivalent of a ‘space’ delimiter between words. Popular Bayesian non-parametric models for text segmentation (Goldwater et al., 2006, 2009) use a Dirichlet process to jointly segment sentences and build a lexicon of word types. We introduce DP-Parse, which uses similar principles but only relies on an instance lexicon of word tokens, avoiding the clustering errors that arise with a lexicon of word types. On the Zero Resource Speech Benchmark 2017, our model sets a new speech segmentation state-of-the-art in 5 languages. The algorithm monotonically improves with better input representations, achieving yet higher scores when fed with weakly supervised inputs. Despite lacking a type lexicon, DP-Parse can be pipelined to a language model and learn semantic and syntactic representations as assessed by a new spoken word embedding benchmark. 1",
    "metadata": {
      "title": "DP-Parse: Finding Word Boundaries from Raw Speech with an Instance Lexicon",
      "summary": "Abstract Finding word boundaries in continuous speech is challenging as there is little or no equivalent of a ‘space’ delimiter between words. Popular Bayesian non-parametric models for text segmentation (Goldwater et al., 2006, 2009) use a Dirichlet process to jointly segment sentences and build a lexicon of word types. We introduce DP-Parse, which uses similar principles but only relies on an instance lexicon of word tokens, avoiding the clustering errors that arise with a lexicon of word types. On the Zero Resource Speech Benchmark 2017, our model sets a new speech segmentation state-of-the-art in 5 languages. The algorithm monotonically improves with better input representations, achieving yet higher scores when fed with weakly supervised inputs. Despite lacking a type lexicon, DP-Parse can be pipelined to a language model and learn semantic and syntactic representations as assessed by a new spoken word embedding benchmark. 1",
      "abstract": "Abstract Finding word boundaries in continuous speech is challenging as there is little or no equivalent of a ‘space’ delimiter between words. Popular Bayesian non-parametric models for text segmentation (Goldwater et al., 2006, 2009) use a Dirichlet process to jointly segment sentences and build a lexicon of word types. We introduce DP-Parse, which uses similar principles but only relies on an instance lexicon of word tokens, avoiding the clustering errors that arise with a lexicon of word types. On the Zero Resource Speech Benchmark 2017, our model sets a new speech segmentation state-of-the-art in 5 languages. The algorithm monotonically improves with better input representations, achieving yet higher scores when fed with weakly supervised inputs. Despite lacking a type lexicon, DP-Parse can be pipelined to a language model and learn semantic and syntactic representations as assessed by a new spoken word embedding benchmark. 1",
      "doi": "https://doi.org/10.1162/tacl_a_00505",
      "openalex_id": "https://openalex.org/W4296710617",
      "arxiv_id": "",
      "publication_date": "2022-01-01",
      "published": "2022-01-01",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "Although the paper addresses processing of raw speech and language representations (segmentation and embedding), the abstract does not explicitly frame the work around Spoken Language Models or speech-language modeling as the core objective, nor does it discuss end-to-end SLM/FD-SLM architectures or interactive dialogue aspects. It focuses on word boundary detection and segmentation from raw speech, which is more fundamental speech processing rather than the targeted SLM/dialect of speech-language modeling required by the inclusion criteria. Therefore it does not clearly meet the inclusion criteria and should be excluded.",
      "evaluation": 2
    },
    "round-A_JuniorNano_reasoning": "Although the paper addresses processing of raw speech and language representations (segmentation and embedding), the abstract does not explicitly frame the work around Spoken Language Models or speech-language modeling as the core objective, nor does it discuss end-to-end SLM/FD-SLM architectures or interactive dialogue aspects. It focuses on word boundary detection and segmentation from raw speech, which is more fundamental speech processing rather than the targeted SLM/dialect of speech-language modeling required by the inclusion criteria. Therefore it does not clearly meet the inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 2,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on segmenting raw speech to find word boundaries using a Bayesian model that builds an instance lexicon of word tokens, primarily emphasizing speech segmentation rather than holistic spoken language modeling (SLM) or full-duplex spoken interaction capabilities. Although it deals with speech input and modeling, it does not explicitly address spoken language models as conceptualized in the inclusion criteria, nor does it discuss end-to-end or mixed architectures, full-duplex interaction, or relevant evaluations like cognitive parallelism or real-time interaction benchmarks. Therefore, it is not fully aligned with the required thematic scope and evaluation demands.",
      "evaluation": 2
    },
    "round-A_JuniorMini_reasoning": "The study focuses on segmenting raw speech to find word boundaries using a Bayesian model that builds an instance lexicon of word tokens, primarily emphasizing speech segmentation rather than holistic spoken language modeling (SLM) or full-duplex spoken interaction capabilities. Although it deals with speech input and modeling, it does not explicitly address spoken language models as conceptualized in the inclusion criteria, nor does it discuss end-to-end or mixed architectures, full-duplex interaction, or relevant evaluations like cognitive parallelism or real-time interaction benchmarks. Therefore, it is not fully aligned with the required thematic scope and evaluation demands.",
    "round-A_JuniorMini_evaluation": 2,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:2)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Comparison of Non-Parametric Bayesian Mixture Models for Syllable Clustering and Zero-Resource Speech Processing",
    "abstract": "Zero-resource speech processing (ZS) systems aim to learn structural representations of speech without access to labeled data. A starting point for these systems is the extraction of syllable tokens utilizing the rhythmic structure of a speech signal. Several recent ZS systems have therefore focused on clustering such syllable tokens into linguistically meaningful units. These systems have so far used heuristically set number of clusters, which can, however, be highly dataset dependent and cannot be optimized in actual unsupervised settings. This paper focuses on improving the flexibility of ZS systems using Bayesian non-parametric (BNP) mixture models that are capable of simultaneously learning the cluster models as well as their number based on the properties of the dataset. We also compare different model design choices, namely priors over the weights and the cluster component models, as the impact of these choices is rarely reported in the previous studies. Experiments are conducted using conversational speech from several languages. The models are first evaluated in a separate syllable clustering task and then as a part of a full ZS system in order to examine the potential of BNP methods and illuminate the relative importance of different model design choices.",
    "metadata": {
      "title": "Comparison of Non-Parametric Bayesian Mixture Models for Syllable Clustering and Zero-Resource Speech Processing",
      "summary": "Zero-resource speech processing (ZS) systems aim to learn structural representations of speech without access to labeled data. A starting point for these systems is the extraction of syllable tokens utilizing the rhythmic structure of a speech signal. Several recent ZS systems have therefore focused on clustering such syllable tokens into linguistically meaningful units. These systems have so far used heuristically set number of clusters, which can, however, be highly dataset dependent and cannot be optimized in actual unsupervised settings. This paper focuses on improving the flexibility of ZS systems using Bayesian non-parametric (BNP) mixture models that are capable of simultaneously learning the cluster models as well as their number based on the properties of the dataset. We also compare different model design choices, namely priors over the weights and the cluster component models, as the impact of these choices is rarely reported in the previous studies. Experiments are conducted using conversational speech from several languages. The models are first evaluated in a separate syllable clustering task and then as a part of a full ZS system in order to examine the potential of BNP methods and illuminate the relative importance of different model design choices.",
      "abstract": "Zero-resource speech processing (ZS) systems aim to learn structural representations of speech without access to labeled data. A starting point for these systems is the extraction of syllable tokens utilizing the rhythmic structure of a speech signal. Several recent ZS systems have therefore focused on clustering such syllable tokens into linguistically meaningful units. These systems have so far used heuristically set number of clusters, which can, however, be highly dataset dependent and cannot be optimized in actual unsupervised settings. This paper focuses on improving the flexibility of ZS systems using Bayesian non-parametric (BNP) mixture models that are capable of simultaneously learning the cluster models as well as their number based on the properties of the dataset. We also compare different model design choices, namely priors over the weights and the cluster component models, as the impact of these choices is rarely reported in the previous studies. Experiments are conducted using conversational speech from several languages. The models are first evaluated in a separate syllable clustering task and then as a part of a full ZS system in order to examine the potential of BNP methods and illuminate the relative importance of different model design choices.",
      "doi": "https://doi.org/10.21437/interspeech.2017-339",
      "openalex_id": "https://openalex.org/W2748009955",
      "arxiv_id": "",
      "publication_date": "2017-08-16",
      "published": "2017-08-16",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The paper focuses on zero-resource speech processing and Bayesian non-parametric clustering of syllables, but does not explicitly address Spoken Language Models or speech-language modeling concepts in the title/abstract, and lacks keywords or framing around SLM/FD-SLM, so it does not meet the specified inclusion criteria despite being a speech-representational work.",
      "evaluation": 2
    },
    "round-A_JuniorNano_reasoning": "The paper focuses on zero-resource speech processing and Bayesian non-parametric clustering of syllables, but does not explicitly address Spoken Language Models or speech-language modeling concepts in the title/abstract, and lacks keywords or framing around SLM/FD-SLM, so it does not meet the specified inclusion criteria despite being a speech-representational work.",
    "round-A_JuniorNano_evaluation": 2,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on zero-resource speech processing systems that cluster syllable tokens for speech representation learning without labeled data, using Bayesian non-parametric mixture models. However, it does not explicitly mention spoken language models (SLMs), full-duplex spoken language models, nor does it discuss synchronous interaction capabilities, cognitive parallelism, or real-time interaction aspects central to the inclusion criteria. The core emphasis is on syllable clustering and unsupervised speech representation extraction, which is more about acoustic modeling than spoken language modeling or interactive multi-turn dialogue systems. Therefore, it does not meet the key inclusion criteria regarding spoken language modeling and full-duplex interaction features.",
      "evaluation": 2
    },
    "round-A_JuniorMini_reasoning": "The study focuses on zero-resource speech processing systems that cluster syllable tokens for speech representation learning without labeled data, using Bayesian non-parametric mixture models. However, it does not explicitly mention spoken language models (SLMs), full-duplex spoken language models, nor does it discuss synchronous interaction capabilities, cognitive parallelism, or real-time interaction aspects central to the inclusion criteria. The core emphasis is on syllable clustering and unsupervised speech representation extraction, which is more about acoustic modeling than spoken language modeling or interactive multi-turn dialogue systems. Therefore, it does not meet the key inclusion criteria regarding spoken language modeling and full-duplex interaction features.",
    "round-A_JuniorMini_evaluation": 2,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:2)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Variational Inference for Acoustic Unit Discovery",
    "abstract": "Recently, several nonparametric Bayesian models have been proposed to automatically discover acoustic units in unlabeled data. Most of them are trained using various versions of the Gibbs Sampling (GS) method. In this work, we consider Variational Bayes (VB) as alternative inference process. Even though VB yields an approximate solution of the posterior distribution it can be easily parallelized which makes it more suitable for large database. Results show that, notwithstanding VB inference is an order of magnitude faster, it outperforms GS in terms of accuracy.",
    "metadata": {
      "title": "Variational Inference for Acoustic Unit Discovery",
      "summary": "Recently, several nonparametric Bayesian models have been proposed to automatically discover acoustic units in unlabeled data. Most of them are trained using various versions of the Gibbs Sampling (GS) method. In this work, we consider Variational Bayes (VB) as alternative inference process. Even though VB yields an approximate solution of the posterior distribution it can be easily parallelized which makes it more suitable for large database. Results show that, notwithstanding VB inference is an order of magnitude faster, it outperforms GS in terms of accuracy.",
      "abstract": "Recently, several nonparametric Bayesian models have been proposed to automatically discover acoustic units in unlabeled data. Most of them are trained using various versions of the Gibbs Sampling (GS) method. In this work, we consider Variational Bayes (VB) as alternative inference process. Even though VB yields an approximate solution of the posterior distribution it can be easily parallelized which makes it more suitable for large database. Results show that, notwithstanding VB inference is an order of magnitude faster, it outperforms GS in terms of accuracy.",
      "doi": "https://doi.org/10.1016/j.procs.2016.04.033",
      "openalex_id": "https://openalex.org/W2347098582",
      "arxiv_id": "",
      "publication_date": "2016-01-01",
      "published": "2016-01-01",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "Reasoning: Although the work models acoustic units in speech data with VB inference, the abstract does not mention Spoken Language Models or language modeling of speech, nor does it frame the study around SLM/FD-SLM architectures or speech-language modeling objectives, so it likely does not meet the explicit inclusion criterion requiring SLM-related focus, and should be excluded.",
      "evaluation": 2
    },
    "round-A_JuniorNano_reasoning": "Reasoning: Although the work models acoustic units in speech data with VB inference, the abstract does not mention Spoken Language Models or language modeling of speech, nor does it frame the study around SLM/FD-SLM architectures or speech-language modeling objectives, so it likely does not meet the explicit inclusion criterion requiring SLM-related focus, and should be excluded.",
    "round-A_JuniorNano_evaluation": 2,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on using Variational Bayes for acoustic unit discovery from spoken data, which involves modeling acoustic units but does not explicitly address Spoken Language Models or full-duplex spoken interaction as defined in the inclusion criteria, nor does it mention synchronization, real-time interaction, or spoken language modeling architectures; thus, it lacks core elements such as spoken language modeling or interactive behavior arbitration and seems more aligned with acoustic modeling prior to language modeling stages.",
      "evaluation": 2
    },
    "round-A_JuniorMini_reasoning": "The study focuses on using Variational Bayes for acoustic unit discovery from spoken data, which involves modeling acoustic units but does not explicitly address Spoken Language Models or full-duplex spoken interaction as defined in the inclusion criteria, nor does it mention synchronization, real-time interaction, or spoken language modeling architectures; thus, it lacks core elements such as spoken language modeling or interactive behavior arbitration and seems more aligned with acoustic modeling prior to language modeling stages.",
    "round-A_JuniorMini_evaluation": 2,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:2)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "VQVAE Unsupervised Unit Discovery and Multi-Scale Code2Spec Inverter for Zerospeech Challenge 2019",
    "abstract": "We describe our submitted system for the ZeroSpeech Challenge 2019.The current challenge theme addresses the difficulty of constructing a speech synthesizer without any text or phonetic labels and requires a system that can (1) discover subword units in an unsupervised way, and (2) synthesize the speech with a target speaker's voice.Moreover, the system should also balance the discrimination score ABX, the bit-rate compression rate, and the naturalness and the intelligibility of the constructed voice.To tackle these problems and achieve the best tradeoff, we utilize a vector quantized variational autoencoder (VQ-VAE) and a multi-scale codebook-tospectrogram (Code2Spec) inverter trained by mean square error and adversarial loss.The VQ-VAE extracts the speech to a latent space, forces itself to map it into the nearest codebook and produces compressed representation.Next, the inverter generates a magnitude spectrogram to the target voice, given the codebook vectors from VQ-VAE.In our experiments, we also investigated several other clustering algorithms, including K-Means and GMM, and compared them with the VQ-VAE result on ABX scores and bit rates.Our proposed approach significantly improved the intelligibility (in CER), the MOS, and discrimination ABX scores compared to the official ZeroSpeech 2019 baseline or even the topline.",
    "metadata": {
      "title": "VQVAE Unsupervised Unit Discovery and Multi-Scale Code2Spec Inverter for Zerospeech Challenge 2019",
      "summary": "We describe our submitted system for the ZeroSpeech Challenge 2019.The current challenge theme addresses the difficulty of constructing a speech synthesizer without any text or phonetic labels and requires a system that can (1) discover subword units in an unsupervised way, and (2) synthesize the speech with a target speaker's voice.Moreover, the system should also balance the discrimination score ABX, the bit-rate compression rate, and the naturalness and the intelligibility of the constructed voice.To tackle these problems and achieve the best tradeoff, we utilize a vector quantized variational autoencoder (VQ-VAE) and a multi-scale codebook-tospectrogram (Code2Spec) inverter trained by mean square error and adversarial loss.The VQ-VAE extracts the speech to a latent space, forces itself to map it into the nearest codebook and produces compressed representation.Next, the inverter generates a magnitude spectrogram to the target voice, given the codebook vectors from VQ-VAE.In our experiments, we also investigated several other clustering algorithms, including K-Means and GMM, and compared them with the VQ-VAE result on ABX scores and bit rates.Our proposed approach significantly improved the intelligibility (in CER), the MOS, and discrimination ABX scores compared to the official ZeroSpeech 2019 baseline or even the topline.",
      "abstract": "We describe our submitted system for the ZeroSpeech Challenge 2019.The current challenge theme addresses the difficulty of constructing a speech synthesizer without any text or phonetic labels and requires a system that can (1) discover subword units in an unsupervised way, and (2) synthesize the speech with a target speaker's voice.Moreover, the system should also balance the discrimination score ABX, the bit-rate compression rate, and the naturalness and the intelligibility of the constructed voice.To tackle these problems and achieve the best tradeoff, we utilize a vector quantized variational autoencoder (VQ-VAE) and a multi-scale codebook-tospectrogram (Code2Spec) inverter trained by mean square error and adversarial loss.The VQ-VAE extracts the speech to a latent space, forces itself to map it into the nearest codebook and produces compressed representation.Next, the inverter generates a magnitude spectrogram to the target voice, given the codebook vectors from VQ-VAE.In our experiments, we also investigated several other clustering algorithms, including K-Means and GMM, and compared them with the VQ-VAE result on ABX scores and bit rates.Our proposed approach significantly improved the intelligibility (in CER), the MOS, and discrimination ABX scores compared to the official ZeroSpeech 2019 baseline or even the topline.",
      "doi": "https://doi.org/10.21437/interspeech.2019-3232",
      "openalex_id": "https://openalex.org/W2972374322",
      "arxiv_id": "",
      "publication_date": "2019-09-13",
      "published": "2019-09-13",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item centers on unsupervised unit discovery and Code2Spec for ZeroSpeech 2019, which is not framed as a Spoken Language Model and is outside the last 3 years, thus failing both the core SLM focus and the time window criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item centers on unsupervised unit discovery and Code2Spec for ZeroSpeech 2019, which is not framed as a Spoken Language Model and is outside the last 3 years, thus failing both the core SLM focus and the time window criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "This study focuses on unsupervised discovery of subword units and speech synthesis using VQ-VAE, which pertains to speech representation and synthesis, yet does not explicitly address Spoken Language Models or their architectures, nor does it discuss turn-taking, full-duplex interaction, or multi-dimensional behavioral arbitration as required; also, the evaluation metrics focus on discrimination scores, compression, and intelligibility rather than synchronization or interactive dialogue behaviors. Therefore, it fails to meet the core inclusion criteria focusing on Spoken Language Models and full-duplex conversational modeling.",
      "evaluation": 2
    },
    "round-A_JuniorMini_reasoning": "This study focuses on unsupervised discovery of subword units and speech synthesis using VQ-VAE, which pertains to speech representation and synthesis, yet does not explicitly address Spoken Language Models or their architectures, nor does it discuss turn-taking, full-duplex interaction, or multi-dimensional behavioral arbitration as required; also, the evaluation metrics focus on discrimination scores, compression, and intelligibility rather than synchronization or interactive dialogue behaviors. Therefore, it fails to meet the core inclusion criteria focusing on Spoken Language Models and full-duplex conversational modeling.",
    "round-A_JuniorMini_evaluation": 2,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:2)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Zero Resource Speech Synthesis Using Transcripts Derived from Perceptual Acoustic Units",
    "abstract": "Zerospeech synthesis is the task of building vocabulary independent speech synthesis systems, where transcriptions are not available for training data. It is, therefore, necessary to convert training data into a sequence of fundamental acoustic units that can be used for synthesis during the test. This paper attempts to discover, and model perceptual acoustic units consisting of steady-state, and transient regions in speech. The transients roughly correspond to CV, VC units, while the steady-state corresponds to sonorants and fricatives. The speech signal is first preprocessed by segmenting the same into CVC-like units using a short-term energy-like contour. These CVC segments are clustered using a connected components-based graph clustering technique. The clustered CVC segments are initialized such that the onset (CV) and decays (VC) correspond to transients, and the rhyme corresponds to steady-states. Following this initialization, the units are allowed to re-organise on the continuous speech into a final set of AUs in an HMM-GMM framework. AU sequences thus obtained are used to train synthesis models. The performance of the proposed approach is evaluated on the Zerospeech 2019 challenge database. Subjective and objective scores show that reasonably good quality synthesis with low bit rate encoding can be achieved using the proposed AUs.",
    "metadata": {
      "title": "Zero Resource Speech Synthesis Using Transcripts Derived from Perceptual Acoustic Units",
      "summary": "Zerospeech synthesis is the task of building vocabulary independent speech synthesis systems, where transcriptions are not available for training data. It is, therefore, necessary to convert training data into a sequence of fundamental acoustic units that can be used for synthesis during the test. This paper attempts to discover, and model perceptual acoustic units consisting of steady-state, and transient regions in speech. The transients roughly correspond to CV, VC units, while the steady-state corresponds to sonorants and fricatives. The speech signal is first preprocessed by segmenting the same into CVC-like units using a short-term energy-like contour. These CVC segments are clustered using a connected components-based graph clustering technique. The clustered CVC segments are initialized such that the onset (CV) and decays (VC) correspond to transients, and the rhyme corresponds to steady-states. Following this initialization, the units are allowed to re-organise on the continuous speech into a final set of AUs in an HMM-GMM framework. AU sequences thus obtained are used to train synthesis models. The performance of the proposed approach is evaluated on the Zerospeech 2019 challenge database. Subjective and objective scores show that reasonably good quality synthesis with low bit rate encoding can be achieved using the proposed AUs.",
      "abstract": "Zerospeech synthesis is the task of building vocabulary independent speech synthesis systems, where transcriptions are not available for training data. It is, therefore, necessary to convert training data into a sequence of fundamental acoustic units that can be used for synthesis during the test. This paper attempts to discover, and model perceptual acoustic units consisting of steady-state, and transient regions in speech. The transients roughly correspond to CV, VC units, while the steady-state corresponds to sonorants and fricatives. The speech signal is first preprocessed by segmenting the same into CVC-like units using a short-term energy-like contour. These CVC segments are clustered using a connected components-based graph clustering technique. The clustered CVC segments are initialized such that the onset (CV) and decays (VC) correspond to transients, and the rhyme corresponds to steady-states. Following this initialization, the units are allowed to re-organise on the continuous speech into a final set of AUs in an HMM-GMM framework. AU sequences thus obtained are used to train synthesis models. The performance of the proposed approach is evaluated on the Zerospeech 2019 challenge database. Subjective and objective scores show that reasonably good quality synthesis with low bit rate encoding can be achieved using the proposed AUs.",
      "doi": "https://doi.org/10.21437/interspeech.2019-2336",
      "openalex_id": "https://openalex.org/W2972964185",
      "arxiv_id": "",
      "publication_date": "2019-09-13",
      "published": "2019-09-13",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This item centers on zero-resource speech synthesis using perceptual acoustic units and does not present a Spoken Language Model or speech-language modeling framework, and it predates the last 3-year window, so it should be excluded.",
      "evaluation": 2
    },
    "round-A_JuniorNano_reasoning": "This item centers on zero-resource speech synthesis using perceptual acoustic units and does not present a Spoken Language Model or speech-language modeling framework, and it predates the last 3-year window, so it should be excluded.",
    "round-A_JuniorNano_evaluation": 2,
    "round-A_JuniorMini_output": {
      "reasoning": "The work focuses on zero resource speech synthesis by modeling acoustic units for speech synthesis without transcripts, and while it addresses acoustic representation and synthesis, it does not explicitly discuss spoken language models or their architecture, training, or assessment as defined in the inclusion criteria. It also does not mention full-duplex interaction, cognitive parallelism, or synchronization aspects, and evaluation is based on subjective and objective synthesis quality rather than interaction or behavior arbitration metrics, thus failing to meet the required thematic and evaluative criteria.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The work focuses on zero resource speech synthesis by modeling acoustic units for speech synthesis without transcripts, and while it addresses acoustic representation and synthesis, it does not explicitly discuss spoken language models or their architecture, training, or assessment as defined in the inclusion criteria. It also does not mention full-duplex interaction, cognitive parallelism, or synchronization aspects, and evaluation is based on subjective and objective synthesis quality rather than interaction or behavior arbitration metrics, thus failing to meet the required thematic and evaluative criteria.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:2)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Combining Adversarial Training and Disentangled Speech Representation for Robust Zero-Resource Subword Modeling",
    "abstract": "This study addresses the problem of unsupervised subword unit discovery from untranscribed speech. It forms the basis of the ultimate goal of ZeroSpeech 2019, building text-to-speech systems without text labels. In this work, unit discovery is formulated as a pipeline of phonetically discriminative feature learning and unit inference. One major difficulty in robust unsupervised feature learning is dealing with speaker variation. Here the robustness towards speaker variation is achieved by applying adversarial training and FHVAE based disentangled speech representation learning. A comparison of the two approaches as well as their combination is studied in a DNN-bottleneck feature (DNN-BNF) architecture. Experiments are conducted on ZeroSpeech 2019 and 2017. Experimental results on ZeroSpeech 2017 show that both approaches are effective while the latter is more prominent, and that their combination brings further marginal improvement in across-speaker condition. Results on ZeroSpeech 2019 show that in the ABX discriminability task, our approaches significantly outperform the official baseline, and are competitive to or even outperform the official topline. The proposed unit sequence smoothing algorithm improves synthesis quality, at a cost of slight decrease in ABX discriminability.",
    "metadata": {
      "title": "Combining Adversarial Training and Disentangled Speech Representation for Robust Zero-Resource Subword Modeling",
      "summary": "This study addresses the problem of unsupervised subword unit discovery from untranscribed speech. It forms the basis of the ultimate goal of ZeroSpeech 2019, building text-to-speech systems without text labels. In this work, unit discovery is formulated as a pipeline of phonetically discriminative feature learning and unit inference. One major difficulty in robust unsupervised feature learning is dealing with speaker variation. Here the robustness towards speaker variation is achieved by applying adversarial training and FHVAE based disentangled speech representation learning. A comparison of the two approaches as well as their combination is studied in a DNN-bottleneck feature (DNN-BNF) architecture. Experiments are conducted on ZeroSpeech 2019 and 2017. Experimental results on ZeroSpeech 2017 show that both approaches are effective while the latter is more prominent, and that their combination brings further marginal improvement in across-speaker condition. Results on ZeroSpeech 2019 show that in the ABX discriminability task, our approaches significantly outperform the official baseline, and are competitive to or even outperform the official topline. The proposed unit sequence smoothing algorithm improves synthesis quality, at a cost of slight decrease in ABX discriminability.",
      "abstract": "This study addresses the problem of unsupervised subword unit discovery from untranscribed speech. It forms the basis of the ultimate goal of ZeroSpeech 2019, building text-to-speech systems without text labels. In this work, unit discovery is formulated as a pipeline of phonetically discriminative feature learning and unit inference. One major difficulty in robust unsupervised feature learning is dealing with speaker variation. Here the robustness towards speaker variation is achieved by applying adversarial training and FHVAE based disentangled speech representation learning. A comparison of the two approaches as well as their combination is studied in a DNN-bottleneck feature (DNN-BNF) architecture. Experiments are conducted on ZeroSpeech 2019 and 2017. Experimental results on ZeroSpeech 2017 show that both approaches are effective while the latter is more prominent, and that their combination brings further marginal improvement in across-speaker condition. Results on ZeroSpeech 2019 show that in the ABX discriminability task, our approaches significantly outperform the official baseline, and are competitive to or even outperform the official topline. The proposed unit sequence smoothing algorithm improves synthesis quality, at a cost of slight decrease in ABX discriminability.",
      "doi": "https://doi.org/10.21437/interspeech.2019-1337",
      "openalex_id": "https://openalex.org/W2950414763",
      "arxiv_id": "",
      "publication_date": "2019-09-13",
      "published": "2019-09-13",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item centers on unsupervised subword discovery from speech, i.e., speech representation learning and modeling of speech units, which aligns with Spoken Language Models as it treats speech as the core modeling object and discusses model architecture, training, and evaluation (ABX, synthesis quality). It is not limited to a single ASR/TTS task and uses speech-based evaluation beyond WER. The timeframe is unclear from the abstract alone, but the work appears to be within the recent literature window (post-2019 era benchmarks like ZeroSpeech 2019). Exclusion criteria do not apply.",
      "evaluation": 4
    },
    "round-A_JuniorNano_reasoning": "The item centers on unsupervised subword discovery from speech, i.e., speech representation learning and modeling of speech units, which aligns with Spoken Language Models as it treats speech as the core modeling object and discusses model architecture, training, and evaluation (ABX, synthesis quality). It is not limited to a single ASR/TTS task and uses speech-based evaluation beyond WER. The timeframe is unclear from the abstract alone, but the work appears to be within the recent literature window (post-2019 era benchmarks like ZeroSpeech 2019). Exclusion criteria do not apply.",
    "round-A_JuniorNano_evaluation": 4,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on unsupervised subword unit discovery from untranscribed speech, emphasizing speech representation and adversarial training. However, it does not explicitly address Spoken Language Models or Full-Duplex Spoken Language Models nor discuss model architectures in terms of turn-taking, cognitive parallelism, or full-duplex interaction. The evaluation focuses on ABX discriminability and synthesis quality rather than benchmarks related to synchronous interaction, overlapping speech, or dialog arbitration. Therefore, it does not meet the inclusion criteria that require explicit focus on spoken language modeling or synchronous interactive capabilities, nor does it violate exclusion criteria based on half-duplex or single-speech technology focus, but it lacks the comprehensive spoken language model context required.",
      "evaluation": 2
    },
    "round-A_JuniorMini_reasoning": "The study focuses on unsupervised subword unit discovery from untranscribed speech, emphasizing speech representation and adversarial training. However, it does not explicitly address Spoken Language Models or Full-Duplex Spoken Language Models nor discuss model architectures in terms of turn-taking, cognitive parallelism, or full-duplex interaction. The evaluation focuses on ABX discriminability and synthesis quality rather than benchmarks related to synchronous interaction, overlapping speech, or dialog arbitration. Therefore, it does not meet the inclusion criteria that require explicit focus on spoken language modeling or synchronous interactive capabilities, nor does it violate exclusion criteria based on half-duplex or single-speech technology focus, but it lacks the comprehensive spoken language model context required.",
    "round-A_JuniorMini_evaluation": 2,
    "round-B_SeniorLead_output": {
      "reasoning": "The paper addresses unsupervised subword/unit discovery and speaker-robust representation learning for ZeroSpeech (DNN-BNF, adversarial training, FHVAE) and evaluates with ABX discriminability and synthesis quality, but it does not explicitly present or frame work as a Spoken Language Model (SLM) or Full-Duplex SLM, nor does it study synchronous/real-time interaction, overlap handling, or related benchmarks required by the inclusion criteria. Junior reviewers were split (one leaned include noting relevance to speech-level modeling; the other correctly noted the lack of explicit SLM/FD-SLM framing and interaction-focused evaluation); after review I agree with the latter. Brief explanation: Exclude — the study focuses on unsupervised representation/unit discovery for ASR/TTS-style pipelines (ZeroSpeech) without explicit spoken-language-modeling or full-duplex interaction claims, and uses ABX/synthesis metrics rather than interaction/synchronization evaluations.",
      "evaluation": 2
    },
    "round-B_SeniorLead_reasoning": "The paper addresses unsupervised subword/unit discovery and speaker-robust representation learning for ZeroSpeech (DNN-BNF, adversarial training, FHVAE) and evaluates with ABX discriminability and synthesis quality, but it does not explicitly present or frame work as a Spoken Language Model (SLM) or Full-Duplex SLM, nor does it study synchronous/real-time interaction, overlap handling, or related benchmarks required by the inclusion criteria. Junior reviewers were split (one leaned include noting relevance to speech-level modeling; the other correctly noted the lack of explicit SLM/FD-SLM framing and interaction-focused evaluation); after review I agree with the latter. Brief explanation: Exclude — the study focuses on unsupervised representation/unit discovery for ASR/TTS-style pipelines (ZeroSpeech) without explicit spoken-language-modeling or full-duplex interaction claims, and uses ABX/synthesis metrics rather than interaction/synchronization evaluations.",
    "round-B_SeniorLead_evaluation": 2,
    "final_verdict": "exclude (senior:2)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Unsupervised End-to-End Learning of Discrete Linguistic Units for Voice Conversion",
    "abstract": "We present an unsupervised end-to-end training scheme where we discover\\ndiscrete subword units from speech without using any labels. The discrete\\nsubword units are learned under an ASR-TTS autoencoder reconstruction setting,\\nwhere an ASR-Encoder is trained to discover a set of common linguistic units\\ngiven a variety of speakers, and a TTS-Decoder trained to project the\\ndiscovered units back to the designated speech. We propose a discrete encoding\\nmethod, Multilabel-Binary Vectors (MBV), to make the ASR-TTS autoencoder\\ndifferentiable. We found that the proposed encoding method offers automatic\\nextraction of speech content from speaker style, and is sufficient to cover\\nfull linguistic content in a given language. Therefore, the TTS-Decoder can\\nsynthesize speech with the same content as the input of ASR-Encoder but with\\ndifferent speaker characteristics, which achieves voice conversion (VC). We\\nfurther improve the quality of VC using adversarial training, where we train a\\nTTS-Patcher that augments the output of TTS-Decoder. Objective and subjective\\nevaluations show that the proposed approach offers strong VC results as it\\neliminates speaker identity while preserving content within speech. In the\\nZeroSpeech 2019 Challenge, we achieved outstanding performance in terms of low\\nbitrate.\\n",
    "metadata": {
      "title": "Unsupervised End-to-End Learning of Discrete Linguistic Units for Voice Conversion",
      "summary": "We present an unsupervised end-to-end training scheme where we discover\\ndiscrete subword units from speech without using any labels. The discrete\\nsubword units are learned under an ASR-TTS autoencoder reconstruction setting,\\nwhere an ASR-Encoder is trained to discover a set of common linguistic units\\ngiven a variety of speakers, and a TTS-Decoder trained to project the\\ndiscovered units back to the designated speech. We propose a discrete encoding\\nmethod, Multilabel-Binary Vectors (MBV), to make the ASR-TTS autoencoder\\ndifferentiable. We found that the proposed encoding method offers automatic\\nextraction of speech content from speaker style, and is sufficient to cover\\nfull linguistic content in a given language. Therefore, the TTS-Decoder can\\nsynthesize speech with the same content as the input of ASR-Encoder but with\\ndifferent speaker characteristics, which achieves voice conversion (VC). We\\nfurther improve the quality of VC using adversarial training, where we train a\\nTTS-Patcher that augments the output of TTS-Decoder. Objective and subjective\\nevaluations show that the proposed approach offers strong VC results as it\\neliminates speaker identity while preserving content within speech. In the\\nZeroSpeech 2019 Challenge, we achieved outstanding performance in terms of low\\nbitrate.\\n",
      "abstract": "We present an unsupervised end-to-end training scheme where we discover\\ndiscrete subword units from speech without using any labels. The discrete\\nsubword units are learned under an ASR-TTS autoencoder reconstruction setting,\\nwhere an ASR-Encoder is trained to discover a set of common linguistic units\\ngiven a variety of speakers, and a TTS-Decoder trained to project the\\ndiscovered units back to the designated speech. We propose a discrete encoding\\nmethod, Multilabel-Binary Vectors (MBV), to make the ASR-TTS autoencoder\\ndifferentiable. We found that the proposed encoding method offers automatic\\nextraction of speech content from speaker style, and is sufficient to cover\\nfull linguistic content in a given language. Therefore, the TTS-Decoder can\\nsynthesize speech with the same content as the input of ASR-Encoder but with\\ndifferent speaker characteristics, which achieves voice conversion (VC). We\\nfurther improve the quality of VC using adversarial training, where we train a\\nTTS-Patcher that augments the output of TTS-Decoder. Objective and subjective\\nevaluations show that the proposed approach offers strong VC results as it\\neliminates speaker identity while preserving content within speech. In the\\nZeroSpeech 2019 Challenge, we achieved outstanding performance in terms of low\\nbitrate.\\n",
      "doi": "https://doi.org/10.21437/interspeech.2019-2048",
      "openalex_id": "https://openalex.org/W2947445680",
      "arxiv_id": "",
      "publication_date": "2019-09-13",
      "published": "2019-09-13",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The work focuses on unsupervised discovery of discrete linguistic units for voice conversion, but it does not explicitly address Spoken Language Models or full-duplex, interactive speech systems, nor clearly frame speech as the primary modeling object with conversational synchronization; thus it does not meet the stated inclusion criteria.",
      "evaluation": 2
    },
    "round-A_JuniorNano_reasoning": "The work focuses on unsupervised discovery of discrete linguistic units for voice conversion, but it does not explicitly address Spoken Language Models or full-duplex, interactive speech systems, nor clearly frame speech as the primary modeling object with conversational synchronization; thus it does not meet the stated inclusion criteria.",
    "round-A_JuniorNano_evaluation": 2,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on unsupervised learning of discrete linguistic units from speech and voice conversion, centering on ASR-TTS autoencoder architecture, but it does not explicitly address Spoken Language Models or Full-Duplex Spoken Language Models, nor does it discuss turn-taking, real-time interaction, cognitive parallelism, or synchronization in speech interactions. The evaluation metrics relate to voice conversion quality rather than multi-dimensional assessments of speech language modeling or interactive dialogue behaviors, thus it does not fully meet the inclusion criteria nor avoid the exclusion criteria focused on speech language modeling and interactive dialogue behaviors.",
      "evaluation": 2
    },
    "round-A_JuniorMini_reasoning": "The study focuses on unsupervised learning of discrete linguistic units from speech and voice conversion, centering on ASR-TTS autoencoder architecture, but it does not explicitly address Spoken Language Models or Full-Duplex Spoken Language Models, nor does it discuss turn-taking, real-time interaction, cognitive parallelism, or synchronization in speech interactions. The evaluation metrics relate to voice conversion quality rather than multi-dimensional assessments of speech language modeling or interactive dialogue behaviors, thus it does not fully meet the inclusion criteria nor avoid the exclusion criteria focused on speech language modeling and interactive dialogue behaviors.",
    "round-A_JuniorMini_evaluation": 2,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:2)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Virtual Phone Discovery for Speech Synthesis Without Text",
    "abstract": "The objective of this work is to re-synthesize speech directly from the speech signals without using any text in a different speaker's voice. The speech signals are transformed into a sequence of acoustic subword units or virtual phones which are discovered automatically from the given speech signals in an unsupervised manner. The speech signal is initially segmented into acoustically homogeneous segments through kernel-Gram segmentation using MFCC and autoencoder bottleneck features. These segments are then clustered using different clustering techniques. The cluster labels thus obtained are considered as virtual phone units which are used to transcribe the speech signals. The virtual phones for the utterances to be resynthesized are encoded as one-hot vector sequences. Deep neural network based duration model and acoustic model are trained for synthesis using these sequences. A vocoder is used to synthesize speech in target speaker's voice from the features estimated by the acoustic model. The performance evaluation is done on ZeroSpeech 2019 challenge on English and Indonesian language. The bitrate and speaker similarity were found to be better than the challenge baseline with slightly lower intelligibility due to the compact encoding.",
    "metadata": {
      "title": "Virtual Phone Discovery for Speech Synthesis Without Text",
      "summary": "The objective of this work is to re-synthesize speech directly from the speech signals without using any text in a different speaker's voice. The speech signals are transformed into a sequence of acoustic subword units or virtual phones which are discovered automatically from the given speech signals in an unsupervised manner. The speech signal is initially segmented into acoustically homogeneous segments through kernel-Gram segmentation using MFCC and autoencoder bottleneck features. These segments are then clustered using different clustering techniques. The cluster labels thus obtained are considered as virtual phone units which are used to transcribe the speech signals. The virtual phones for the utterances to be resynthesized are encoded as one-hot vector sequences. Deep neural network based duration model and acoustic model are trained for synthesis using these sequences. A vocoder is used to synthesize speech in target speaker's voice from the features estimated by the acoustic model. The performance evaluation is done on ZeroSpeech 2019 challenge on English and Indonesian language. The bitrate and speaker similarity were found to be better than the challenge baseline with slightly lower intelligibility due to the compact encoding.",
      "abstract": "The objective of this work is to re-synthesize speech directly from the speech signals without using any text in a different speaker's voice. The speech signals are transformed into a sequence of acoustic subword units or virtual phones which are discovered automatically from the given speech signals in an unsupervised manner. The speech signal is initially segmented into acoustically homogeneous segments through kernel-Gram segmentation using MFCC and autoencoder bottleneck features. These segments are then clustered using different clustering techniques. The cluster labels thus obtained are considered as virtual phone units which are used to transcribe the speech signals. The virtual phones for the utterances to be resynthesized are encoded as one-hot vector sequences. Deep neural network based duration model and acoustic model are trained for synthesis using these sequences. A vocoder is used to synthesize speech in target speaker's voice from the features estimated by the acoustic model. The performance evaluation is done on ZeroSpeech 2019 challenge on English and Indonesian language. The bitrate and speaker similarity were found to be better than the challenge baseline with slightly lower intelligibility due to the compact encoding.",
      "doi": "https://doi.org/10.1109/globalsip45357.2019.8969412",
      "openalex_id": "https://openalex.org/W3003750857",
      "arxiv_id": "",
      "publication_date": "2019-11-01",
      "published": "2019-11-01",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item centers on unsupervised discovery of virtual phones for speech synthesis from speech signals (TTS without text) and does not clearly present Spoken/Speech Language Models as the core focus, plus it is from 2019 and thus outside the required 3-year window.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item centers on unsupervised discovery of virtual phones for speech synthesis from speech signals (TTS without text) and does not clearly present Spoken/Speech Language Models as the core focus, plus it is from 2019 and thus outside the required 3-year window.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The abstract describes a speech synthesis approach that discovers acoustic subword units (virtual phones) from speech signals without text, focusing on unsupervised clustering and synthesis. However, it does not mention spoken language models (SLMs) or full-duplex interaction capabilities, nor does it discuss model architectures involving language modeling of speech sequences or real-time interactive behaviors. The evaluation is limited to bitrate, speaker similarity, and intelligibility, with no indication of handling synchronous interaction, cognitive parallelism, or speech overlap management. Therefore, it does not meet the inclusion criteria centered on spoken language modeling and is better excluded.",
      "evaluation": 2
    },
    "round-A_JuniorMini_reasoning": "The abstract describes a speech synthesis approach that discovers acoustic subword units (virtual phones) from speech signals without text, focusing on unsupervised clustering and synthesis. However, it does not mention spoken language models (SLMs) or full-duplex interaction capabilities, nor does it discuss model architectures involving language modeling of speech sequences or real-time interactive behaviors. The evaluation is limited to bitrate, speaker similarity, and intelligibility, with no indication of handling synchronous interaction, cognitive parallelism, or speech overlap management. Therefore, it does not meet the inclusion criteria centered on spoken language modeling and is better excluded.",
    "round-A_JuniorMini_evaluation": 2,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:2)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "A Hierarchical Subspace Model for Language-Attuned Acoustic Unit\\n Discovery",
    "abstract": "In this work, we propose a hierarchical subspace model for acoustic unit\\ndiscovery. In this approach, we frame the task as one of learning embeddings on\\na low-dimensional phonetic subspace, and simultaneously specify the subspace\\nitself as an embedding on a hyper-subspace. We train the hyper-subspace on a\\nset of transcribed languages and transfer it to the target language. In the\\ntarget language, we infer both the language and unit embeddings in an\\nunsupervised manner, and in so doing, we simultaneously learn a subspace of\\nunits specific to that language and the units that dwell on it. We conduct our\\nexperiments on TIMIT and two low-resource languages: Mboshi and Yoruba. Results\\nshow that our model outperforms major acoustic unit discovery techniques, both\\nin terms of clustering quality and segmentation accuracy.\\n",
    "metadata": {
      "title": "A Hierarchical Subspace Model for Language-Attuned Acoustic Unit\\n Discovery",
      "summary": "In this work, we propose a hierarchical subspace model for acoustic unit\\ndiscovery. In this approach, we frame the task as one of learning embeddings on\\na low-dimensional phonetic subspace, and simultaneously specify the subspace\\nitself as an embedding on a hyper-subspace. We train the hyper-subspace on a\\nset of transcribed languages and transfer it to the target language. In the\\ntarget language, we infer both the language and unit embeddings in an\\nunsupervised manner, and in so doing, we simultaneously learn a subspace of\\nunits specific to that language and the units that dwell on it. We conduct our\\nexperiments on TIMIT and two low-resource languages: Mboshi and Yoruba. Results\\nshow that our model outperforms major acoustic unit discovery techniques, both\\nin terms of clustering quality and segmentation accuracy.\\n",
      "abstract": "In this work, we propose a hierarchical subspace model for acoustic unit\\ndiscovery. In this approach, we frame the task as one of learning embeddings on\\na low-dimensional phonetic subspace, and simultaneously specify the subspace\\nitself as an embedding on a hyper-subspace. We train the hyper-subspace on a\\nset of transcribed languages and transfer it to the target language. In the\\ntarget language, we infer both the language and unit embeddings in an\\nunsupervised manner, and in so doing, we simultaneously learn a subspace of\\nunits specific to that language and the units that dwell on it. We conduct our\\nexperiments on TIMIT and two low-resource languages: Mboshi and Yoruba. Results\\nshow that our model outperforms major acoustic unit discovery techniques, both\\nin terms of clustering quality and segmentation accuracy.\\n",
      "doi": "https://doi.org/10.48550/arxiv.2011.03115",
      "openalex_id": "https://openalex.org/W3161215977",
      "arxiv_id": "",
      "publication_date": "2020-11-04",
      "published": "2020-11-04",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The paper probes acoustic unit discovery with language-aware subspaces rather than focusing on Spoken Language Models or speech-language modeling with full-duplex interaction and real-time dialogue metrics, thus it does not meet the inclusion criteria and aligns with exclusion.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The paper probes acoustic unit discovery with language-aware subspaces rather than focusing on Spoken Language Models or speech-language modeling with full-duplex interaction and real-time dialogue metrics, thus it does not meet the inclusion criteria and aligns with exclusion.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper proposes a hierarchical subspace model for acoustic unit discovery focusing on unsupervised learning of speech units, but it does not explicitly mention spoken language models, speech language modeling, full-duplex interaction, or synchronization mechanisms as required by the inclusion criteria, nor does it address the evaluation of real-time interactive speech behaviors or multi-dimensional assessments beyond clustering and segmentation accuracy, leading to uncertainty about its fit in the specified thematic scope.",
      "evaluation": 3
    },
    "round-A_JuniorMini_reasoning": "The paper proposes a hierarchical subspace model for acoustic unit discovery focusing on unsupervised learning of speech units, but it does not explicitly mention spoken language models, speech language modeling, full-duplex interaction, or synchronization mechanisms as required by the inclusion criteria, nor does it address the evaluation of real-time interactive speech behaviors or multi-dimensional assessments beyond clustering and segmentation accuracy, leading to uncertainty about its fit in the specified thematic scope.",
    "round-A_JuniorMini_evaluation": 3,
    "round-B_SeniorLead_output": {
      "reasoning": "I reviewed the title/abstract and the two junior reviewers' comments (one recommending exclusion, one uncertain). The paper proposes a hierarchical subspace model for acoustic unit discovery (unsupervised phonetic/unit embeddings, evaluated on TIMIT, Mboshi, Yoruba with clustering and segmentation metrics). It does not mention or center Spoken Language Models, speech-level language modeling, Speech–LLM or full‑duplex interaction, nor does it address synchronization, real‑time interaction, or evaluation of dialogue/overlap/behavior arbitration as required by the inclusion criteria. Because the inclusion rules require explicit mention of spoken language models or equivalent speech language modeling in the title/abstract and the work focuses on acoustic unit discovery (an ASR-level/representation task), it fails to meet the inclusion criteria and falls under the exclusion scope. Therefore I agree with the exclusion judgment.",
      "evaluation": 1
    },
    "round-B_SeniorLead_reasoning": "I reviewed the title/abstract and the two junior reviewers' comments (one recommending exclusion, one uncertain). The paper proposes a hierarchical subspace model for acoustic unit discovery (unsupervised phonetic/unit embeddings, evaluated on TIMIT, Mboshi, Yoruba with clustering and segmentation metrics). It does not mention or center Spoken Language Models, speech-level language modeling, Speech–LLM or full‑duplex interaction, nor does it address synchronization, real‑time interaction, or evaluation of dialogue/overlap/behavior arbitration as required by the inclusion criteria. Because the inclusion rules require explicit mention of spoken language models or equivalent speech language modeling in the title/abstract and the work focuses on acoustic unit discovery (an ASR-level/representation task), it fails to meet the inclusion criteria and falls under the exclusion scope. Therefore I agree with the exclusion judgment.",
    "round-B_SeniorLead_evaluation": 1,
    "final_verdict": "exclude (senior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Exploration of End-to-End Synthesisers for Zero Resource Speech Challenge 2020",
    "abstract": "A Spoken dialogue system for an unseen language is referred to as Zero resource speech. It is especially beneficial for developing applications for languages that have low digital resources. Zero resource speech synthesis is the task of building text-to-speech (TTS) models in the absence of transcriptions. In this work, speech is modelled as a sequence of transient and steady-state acoustic units, and a unique set of acoustic units is discovered by iterative training. Using the acoustic unit sequence, TTS models are trained. The main goal of this work is to improve the synthesis quality of zero resource TTS system. Four different systems are proposed. All the systems consist of three stages: unit discovery, followed by unit sequence to spectrogram mapping, and finally spectrogram to speech inversion. Modifications are proposed to the spectrogram mapping stage. These modifications include training the mapping on voice data, using x-vectors to improve the mapping, two-stage learning, and gender-specific modelling. Evaluation of the proposed systems in the Zerospeech 2020 challenge shows that quite good quality synthesis can be achieved.",
    "metadata": {
      "title": "Exploration of End-to-End Synthesisers for Zero Resource Speech Challenge 2020",
      "summary": "A Spoken dialogue system for an unseen language is referred to as Zero resource speech. It is especially beneficial for developing applications for languages that have low digital resources. Zero resource speech synthesis is the task of building text-to-speech (TTS) models in the absence of transcriptions. In this work, speech is modelled as a sequence of transient and steady-state acoustic units, and a unique set of acoustic units is discovered by iterative training. Using the acoustic unit sequence, TTS models are trained. The main goal of this work is to improve the synthesis quality of zero resource TTS system. Four different systems are proposed. All the systems consist of three stages: unit discovery, followed by unit sequence to spectrogram mapping, and finally spectrogram to speech inversion. Modifications are proposed to the spectrogram mapping stage. These modifications include training the mapping on voice data, using x-vectors to improve the mapping, two-stage learning, and gender-specific modelling. Evaluation of the proposed systems in the Zerospeech 2020 challenge shows that quite good quality synthesis can be achieved.",
      "abstract": "A Spoken dialogue system for an unseen language is referred to as Zero resource speech. It is especially beneficial for developing applications for languages that have low digital resources. Zero resource speech synthesis is the task of building text-to-speech (TTS) models in the absence of transcriptions. In this work, speech is modelled as a sequence of transient and steady-state acoustic units, and a unique set of acoustic units is discovered by iterative training. Using the acoustic unit sequence, TTS models are trained. The main goal of this work is to improve the synthesis quality of zero resource TTS system. Four different systems are proposed. All the systems consist of three stages: unit discovery, followed by unit sequence to spectrogram mapping, and finally spectrogram to speech inversion. Modifications are proposed to the spectrogram mapping stage. These modifications include training the mapping on voice data, using x-vectors to improve the mapping, two-stage learning, and gender-specific modelling. Evaluation of the proposed systems in the Zerospeech 2020 challenge shows that quite good quality synthesis can be achieved.",
      "doi": "https://doi.org/10.21437/interspeech.2020-2731",
      "openalex_id": "https://openalex.org/W3096262326",
      "arxiv_id": "",
      "publication_date": "2020-10-25",
      "published": "2020-10-25",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item describes zero-resource TTS for unseen languages, which centers on speech synthesis rather than Spoken Language Models or speech–language modeling with end-to-end or interactive dialogue components; it lacks interactive evaluation and is older than 3 years, so it does not meet the inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item describes zero-resource TTS for unseen languages, which centers on speech synthesis rather than Spoken Language Models or speech–language modeling with end-to-end or interactive dialogue components; it lacks interactive evaluation and is older than 3 years, so it does not meet the inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on zero resource text-to-speech synthesis with a pipeline involving unit discovery and spectrogram mapping, primarily addressing acoustic unit modeling and TTS quality improvement. However, it does not explicitly discuss spoken language models (SLMs) or full-duplex interaction capabilities, nor does it mention architectures, training, or evaluation of dialogue models involving speech-based synchronization or interaction behaviors as required by the inclusion criteria. The evaluation metrics are speech synthesis quality measures, not synchronous interaction or behavioral arbitration. Therefore, despite being on speech synthesis, it lacks the core focus on spoken language modeling or full-duplex spoken interaction.",
      "evaluation": 2
    },
    "round-A_JuniorMini_reasoning": "The study focuses on zero resource text-to-speech synthesis with a pipeline involving unit discovery and spectrogram mapping, primarily addressing acoustic unit modeling and TTS quality improvement. However, it does not explicitly discuss spoken language models (SLMs) or full-duplex interaction capabilities, nor does it mention architectures, training, or evaluation of dialogue models involving speech-based synchronization or interaction behaviors as required by the inclusion criteria. The evaluation metrics are speech synthesis quality measures, not synchronous interaction or behavioral arbitration. Therefore, despite being on speech synthesis, it lacks the core focus on spoken language modeling or full-duplex spoken interaction.",
    "round-A_JuniorMini_evaluation": 2,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:2)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Unsupervised Acoustic Unit Representation Learning for Voice Conversion Using WaveNet Auto-Encoders",
    "abstract": "This is a repository copy of Unsupervised acoustic unit representation learning for voice conversion using WaveNet auto-encoders.",
    "metadata": {
      "title": "Unsupervised Acoustic Unit Representation Learning for Voice Conversion Using WaveNet Auto-Encoders",
      "summary": "This is a repository copy of Unsupervised acoustic unit representation learning for voice conversion using WaveNet auto-encoders.",
      "abstract": "This is a repository copy of Unsupervised acoustic unit representation learning for voice conversion using WaveNet auto-encoders.",
      "doi": "https://doi.org/10.21437/interspeech.2020-1785",
      "openalex_id": "https://openalex.org/W3097692357",
      "arxiv_id": "",
      "publication_date": "2020-10-25",
      "published": "2020-10-25",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This item focuses on unsupervised acoustic unit representation learning for voice conversion using WaveNet auto-encoders, which centers on speech signal representation rather than Spoken Language Models (SLMs) or full speech–language modeling with turn-taking/synchronization; the abstract lacks clear emphasis on SLMs, dialogue interactivity, or multi-turn evaluation, so it does not meet the inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "This item focuses on unsupervised acoustic unit representation learning for voice conversion using WaveNet auto-encoders, which centers on speech signal representation rather than Spoken Language Models (SLMs) or full speech–language modeling with turn-taking/synchronization; the abstract lacks clear emphasis on SLMs, dialogue interactivity, or multi-turn evaluation, so it does not meet the inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The title and abstract indicate the work focuses on unsupervised acoustic unit representation learning for voice conversion using WaveNet auto-encoders, which relates to acoustic/speech feature learning but does not explicitly mention spoken language modeling, full-duplex spoken language models, synchronization, or interactive dialogue behavior arbitration, nor does it clarify the inclusion of language modeling or evaluation on interaction-related benchmarks; the abstract is too brief to confirm compliance with inclusion criteria or absence of exclusion criteria.",
      "evaluation": 2
    },
    "round-A_JuniorMini_reasoning": "The title and abstract indicate the work focuses on unsupervised acoustic unit representation learning for voice conversion using WaveNet auto-encoders, which relates to acoustic/speech feature learning but does not explicitly mention spoken language modeling, full-duplex spoken language models, synchronization, or interactive dialogue behavior arbitration, nor does it clarify the inclusion of language modeling or evaluation on interaction-related benchmarks; the abstract is too brief to confirm compliance with inclusion criteria or absence of exclusion criteria.",
    "round-A_JuniorMini_evaluation": 2,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:2)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Transformer VQ-VAE for Unsupervised Unit Discovery and Speech Synthesis: ZeroSpeech 2020 Challenge",
    "abstract": "In this paper, we report our submitted system for the ZeroSpeech 2020 challenge on Track 2019.The main theme in this challenge is to build a speech synthesizer without any textual information or phonetic labels.In order to tackle those challenges, we build a system that must address two major components such as 1) given speech audio, extract subword units in an unsupervised way and 2) resynthesize the audio from novel speakers.The system also needs to balance the codebook performance between the ABX error rate and the bitrate compression rate.Our main contribution here is we proposed Transformer-based VQ-VAE for unsupervised unit discovery and Transformerbased inverter for the speech synthesis given the extracted codebook.Additionally, we also explored several regularization methods to improve performance even further.",
    "metadata": {
      "title": "Transformer VQ-VAE for Unsupervised Unit Discovery and Speech Synthesis: ZeroSpeech 2020 Challenge",
      "summary": "In this paper, we report our submitted system for the ZeroSpeech 2020 challenge on Track 2019.The main theme in this challenge is to build a speech synthesizer without any textual information or phonetic labels.In order to tackle those challenges, we build a system that must address two major components such as 1) given speech audio, extract subword units in an unsupervised way and 2) resynthesize the audio from novel speakers.The system also needs to balance the codebook performance between the ABX error rate and the bitrate compression rate.Our main contribution here is we proposed Transformer-based VQ-VAE for unsupervised unit discovery and Transformerbased inverter for the speech synthesis given the extracted codebook.Additionally, we also explored several regularization methods to improve performance even further.",
      "abstract": "In this paper, we report our submitted system for the ZeroSpeech 2020 challenge on Track 2019.The main theme in this challenge is to build a speech synthesizer without any textual information or phonetic labels.In order to tackle those challenges, we build a system that must address two major components such as 1) given speech audio, extract subword units in an unsupervised way and 2) resynthesize the audio from novel speakers.The system also needs to balance the codebook performance between the ABX error rate and the bitrate compression rate.Our main contribution here is we proposed Transformer-based VQ-VAE for unsupervised unit discovery and Transformerbased inverter for the speech synthesis given the extracted codebook.Additionally, we also explored several regularization methods to improve performance even further.",
      "doi": "https://doi.org/10.21437/interspeech.2020-3033",
      "openalex_id": "https://openalex.org/W3096216486",
      "arxiv_id": "",
      "publication_date": "2020-10-25",
      "published": "2020-10-25",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "雖然論文屬於語音模型/語音合成領域，但未在標題、摘要或關鍵詞中出現與「spoken language model(s)」或等價描述的字樣；且發表於 2020 年，遠超過過去 3 年的納入時程，因此不符合全部納入條件且同時符合排除條件。",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "雖然論文屬於語音模型/語音合成領域，但未在標題、摘要或關鍵詞中出現與「spoken language model(s)」或等價描述的字樣；且發表於 2020 年，遠超過過去 3 年的納入時程，因此不符合全部納入條件且同時符合排除條件。",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper focuses on unsupervised unit discovery and speech synthesis using Transformer-based VQ-VAE, emphasizing modeling speech audio without textual information, which aligns partially with spoken language modeling by treating speech as the primary input. However, it does not explicitly mention spoken language models, speech language models, or related full-duplex interactive dialogue capabilities, nor does it discuss synchronization, cognitive parallelism, or real-time interaction metrics. The evaluation focuses on ABX error rate and bitrate compression, rather than interaction or behavior arbitration. Therefore, while relevant to speech modeling, it does not fulfill the full inclusion criteria nor violate all exclusion criteria but lacks explicit focus on core topics required for inclusion.",
      "evaluation": 3
    },
    "round-A_JuniorMini_reasoning": "The paper focuses on unsupervised unit discovery and speech synthesis using Transformer-based VQ-VAE, emphasizing modeling speech audio without textual information, which aligns partially with spoken language modeling by treating speech as the primary input. However, it does not explicitly mention spoken language models, speech language models, or related full-duplex interactive dialogue capabilities, nor does it discuss synchronization, cognitive parallelism, or real-time interaction metrics. The evaluation focuses on ABX error rate and bitrate compression, rather than interaction or behavior arbitration. Therefore, while relevant to speech modeling, it does not fulfill the full inclusion criteria nor violate all exclusion criteria but lacks explicit focus on core topics required for inclusion.",
    "round-A_JuniorMini_evaluation": 3,
    "round-B_SeniorLead_output": {
      "reasoning": "Both junior reviewers noted concerns: JuniorNano correctly points out the paper is from 2020 (outside the 3-year window) and lacks explicit \"spoken language model(s)\" terminology; JuniorMini observed the work is relevant to speech modeling but does not explicitly frame itself as an SLM and focuses on unit discovery and TTS-style resynthesis with ABX/bitrate metrics. The paper therefore fails the explicit inclusion requirements (not within the past 3 years and no spoken-language-model/equivalent term in title/abstract) and matches multiple exclusion criteria (focused on unsupervised unit discovery and speech synthesis/TTS evaluation rather than SLMs, and does not address full‑duplex/synchronization/interaction aspects). Score justification sentence: Exclude (1) — the study is outside the time window and does not meet the required SLM framing or interaction/synchronization scope, instead focusing on unit discovery and speech resynthesis.",
      "evaluation": 1
    },
    "round-B_SeniorLead_reasoning": "Both junior reviewers noted concerns: JuniorNano correctly points out the paper is from 2020 (outside the 3-year window) and lacks explicit \"spoken language model(s)\" terminology; JuniorMini observed the work is relevant to speech modeling but does not explicitly frame itself as an SLM and focuses on unit discovery and TTS-style resynthesis with ABX/bitrate metrics. The paper therefore fails the explicit inclusion requirements (not within the past 3 years and no spoken-language-model/equivalent term in title/abstract) and matches multiple exclusion criteria (focused on unsupervised unit discovery and speech synthesis/TTS evaluation rather than SLMs, and does not address full‑duplex/synchronization/interaction aspects). Score justification sentence: Exclude (1) — the study is outside the time window and does not meet the required SLM framing or interaction/synchronization scope, instead focusing on unit discovery and speech resynthesis.",
    "round-B_SeniorLead_evaluation": 1,
    "final_verdict": "exclude (senior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Information Retrieval for ZeroSpeech 2021: The Submission by University of Wroclaw",
    "abstract": "We present a number of low-resource approaches to the tasks of the Zero Resource Speech Challenge 2021. We build on the unsupervised representations of speech proposed by the organizers as a baseline, derived from CPC and clustered with the k-means algorithm. We demonstrate that simple methods of refining those representations can narrow the gap, or even improve upon the solutions which use a high computational budget. The results lead to the conclusion that the CPC-derived representations are still too noisy for training language models, but stable enough for simpler forms of pattern matching and retrieval.",
    "metadata": {
      "title": "Information Retrieval for ZeroSpeech 2021: The Submission by University of Wroclaw",
      "summary": "We present a number of low-resource approaches to the tasks of the Zero Resource Speech Challenge 2021. We build on the unsupervised representations of speech proposed by the organizers as a baseline, derived from CPC and clustered with the k-means algorithm. We demonstrate that simple methods of refining those representations can narrow the gap, or even improve upon the solutions which use a high computational budget. The results lead to the conclusion that the CPC-derived representations are still too noisy for training language models, but stable enough for simpler forms of pattern matching and retrieval.",
      "abstract": "We present a number of low-resource approaches to the tasks of the Zero Resource Speech Challenge 2021. We build on the unsupervised representations of speech proposed by the organizers as a baseline, derived from CPC and clustered with the k-means algorithm. We demonstrate that simple methods of refining those representations can narrow the gap, or even improve upon the solutions which use a high computational budget. The results lead to the conclusion that the CPC-derived representations are still too noisy for training language models, but stable enough for simpler forms of pattern matching and retrieval.",
      "doi": "https://doi.org/10.21437/interspeech.2021-1465",
      "openalex_id": "https://openalex.org/W3197349023",
      "arxiv_id": "",
      "publication_date": "2021-06-22",
      "published": "2021-06-22",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The submission is from 2021 and does not clearly focus on Spoken Language Models or a speech-language modeling framework, and it falls outside the required 3-year window for inclusion, thus not meeting the criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The submission is from 2021 and does not clearly focus on Spoken Language Models or a speech-language modeling framework, and it falls outside the required 3-year window for inclusion, thus not meeting the criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on low-resource speech representation methods for Zero Resource Speech Challenge, relating to speech processing but does not explicitly address Spoken Language Models or full-duplex capabilities, nor does it discuss model architectures or interaction paradigms central to the inclusion criteria. The abstract lacks mention of spoken language modeling or synchronization for interactive dialogue, failing to meet the core topic requirements.",
      "evaluation": 2
    },
    "round-A_JuniorMini_reasoning": "The study focuses on low-resource speech representation methods for Zero Resource Speech Challenge, relating to speech processing but does not explicitly address Spoken Language Models or full-duplex capabilities, nor does it discuss model architectures or interaction paradigms central to the inclusion criteria. The abstract lacks mention of spoken language modeling or synchronization for interactive dialogue, failing to meet the core topic requirements.",
    "round-A_JuniorMini_evaluation": 2,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:2)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Unsupervised Speech Segmentation and Variable Rate Representation Learning Using Segmental Contrastive Predictive Coding",
    "abstract": "Typically, unsupervised segmentation of speech into the phone- and word-like units are treated as separate tasks and are often done via different methods which do not fully leverage the inter-dependence of the two tasks. Here, we unify them and propose a technique that can jointly perform both, showing that these two tasks indeed benefit from each other. Recent attempts employ self-supervised learning, such as contrastive predictive coding (CPC), where the next frame is predicted given past context. However, CPC only looks at the audio signal's frame-level structure. We overcome this limitation with a segmental contrastive predictive coding (SCPC) framework to model the signal structure at a higher level, e.g., phone level. A convolutional neural network learns frame-level representation from the raw waveform via noise-contrastive estimation (NCE). A differentiable boundary detector finds variable-length segments, which are then used to optimize a segment encoder via NCE to learn segment representations. The differentiable boundary detector allows us to train frame-level and segment-level encoders jointly. Experiments show that our single model outperforms existing phone and word segmentation methods on TIMIT and Buckeye datasets. We analyze the impact of the threshold on boundary detector performance, and our results suggest that automatically learning the boundary threshold can be as effective as manually tuning that threshold. We discover that phone class impacts the boundary detection performance, and the boundaries between successive vowels or semivowels are the most difficult. Finally, we use SCPC to extract speech features at the segment level rather than at the uniformly spaced frame level (e.g., 10 ms) and produce variable rate representations that change according to the contents of the utterance. We can lower the feature extraction rate from the typical 100 Hz to as low as 14.5 Hz on average while still outperforming the hand-crafted features such as MFCC on the linear phone classification task.",
    "metadata": {
      "title": "Unsupervised Speech Segmentation and Variable Rate Representation Learning Using Segmental Contrastive Predictive Coding",
      "summary": "Typically, unsupervised segmentation of speech into the phone- and word-like units are treated as separate tasks and are often done via different methods which do not fully leverage the inter-dependence of the two tasks. Here, we unify them and propose a technique that can jointly perform both, showing that these two tasks indeed benefit from each other. Recent attempts employ self-supervised learning, such as contrastive predictive coding (CPC), where the next frame is predicted given past context. However, CPC only looks at the audio signal's frame-level structure. We overcome this limitation with a segmental contrastive predictive coding (SCPC) framework to model the signal structure at a higher level, e.g., phone level. A convolutional neural network learns frame-level representation from the raw waveform via noise-contrastive estimation (NCE). A differentiable boundary detector finds variable-length segments, which are then used to optimize a segment encoder via NCE to learn segment representations. The differentiable boundary detector allows us to train frame-level and segment-level encoders jointly. Experiments show that our single model outperforms existing phone and word segmentation methods on TIMIT and Buckeye datasets. We analyze the impact of the threshold on boundary detector performance, and our results suggest that automatically learning the boundary threshold can be as effective as manually tuning that threshold. We discover that phone class impacts the boundary detection performance, and the boundaries between successive vowels or semivowels are the most difficult. Finally, we use SCPC to extract speech features at the segment level rather than at the uniformly spaced frame level (e.g., 10 ms) and produce variable rate representations that change according to the contents of the utterance. We can lower the feature extraction rate from the typical 100 Hz to as low as 14.5 Hz on average while still outperforming the hand-crafted features such as MFCC on the linear phone classification task.",
      "abstract": "Typically, unsupervised segmentation of speech into the phone- and word-like units are treated as separate tasks and are often done via different methods which do not fully leverage the inter-dependence of the two tasks. Here, we unify them and propose a technique that can jointly perform both, showing that these two tasks indeed benefit from each other. Recent attempts employ self-supervised learning, such as contrastive predictive coding (CPC), where the next frame is predicted given past context. However, CPC only looks at the audio signal's frame-level structure. We overcome this limitation with a segmental contrastive predictive coding (SCPC) framework to model the signal structure at a higher level, e.g., phone level. A convolutional neural network learns frame-level representation from the raw waveform via noise-contrastive estimation (NCE). A differentiable boundary detector finds variable-length segments, which are then used to optimize a segment encoder via NCE to learn segment representations. The differentiable boundary detector allows us to train frame-level and segment-level encoders jointly. Experiments show that our single model outperforms existing phone and word segmentation methods on TIMIT and Buckeye datasets. We analyze the impact of the threshold on boundary detector performance, and our results suggest that automatically learning the boundary threshold can be as effective as manually tuning that threshold. We discover that phone class impacts the boundary detection performance, and the boundaries between successive vowels or semivowels are the most difficult. Finally, we use SCPC to extract speech features at the segment level rather than at the uniformly spaced frame level (e.g., 10 ms) and produce variable rate representations that change according to the contents of the utterance. We can lower the feature extraction rate from the typical 100 Hz to as low as 14.5 Hz on average while still outperforming the hand-crafted features such as MFCC on the linear phone classification task.",
      "doi": "https://doi.org/10.1109/taslp.2022.3180684",
      "openalex_id": "https://openalex.org/W3204915839",
      "arxiv_id": "",
      "publication_date": "2022-01-01",
      "published": "2022-01-01",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This paper focuses on unsupervised segmentation and segment-level representation learning for speech but does not center on Spoken Language Models or speech-language modeling as the core objective, nor address interactive/low-latency dialogue aspects; therefore it does not meet the inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "This paper focuses on unsupervised segmentation and segment-level representation learning for speech but does not center on Spoken Language Models or speech-language modeling as the core objective, nor address interactive/low-latency dialogue aspects; therefore it does not meet the inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on unsupervised segmentation and representation learning of speech signals, mainly targeting phone- and word-level units using a self-supervised method (segmental contrastive predictive coding). However, it does not explicitly address spoken language models or full-duplex spoken language models, nor does it mention modeling or reasoning over speech sequences in terms of turn-taking, synchronization, or interactive dialogue behaviors. The evaluation also centers on segmentation and feature extraction tasks rather than real-time interaction or behavioral arbitration aspects. Hence, it does not meet the inclusion criteria centered on spoken language models or full-duplex interaction capabilities.",
      "evaluation": 2
    },
    "round-A_JuniorMini_reasoning": "The study focuses on unsupervised segmentation and representation learning of speech signals, mainly targeting phone- and word-level units using a self-supervised method (segmental contrastive predictive coding). However, it does not explicitly address spoken language models or full-duplex spoken language models, nor does it mention modeling or reasoning over speech sequences in terms of turn-taking, synchronization, or interactive dialogue behaviors. The evaluation also centers on segmentation and feature extraction tasks rather than real-time interaction or behavioral arbitration aspects. Hence, it does not meet the inclusion criteria centered on spoken language models or full-duplex interaction capabilities.",
    "round-A_JuniorMini_evaluation": 2,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:2)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Contrastive Prediction Strategies for Unsupervised Segmentation and Categorization of Phonemes and Words",
    "abstract": "We investigate the performance on phoneme categorization and phoneme and word segmentation of several self-supervised learning (SSL) methods based on Contrastive Predictive Coding (CPC). Our experiments show that with the existing algorithms there is a trade off between categorization and segmentation performance. We investigate the source of this conflict and conclude that the use of context building networks, albeit necessary for superior performance on categorization tasks, harms segmentation performance by causing a temporal shift on the learned representations. Aiming to bridge this gap, we take inspiration from the leading approach on segmentation, which simultaneously models the speech signal at the frame and phoneme level, and incorporate multi-level modelling into Aligned CPC (ACPC), a variation of CPC which exhibits the best performance on categorization tasks. Our multi-level ACPC (mACPC) improves in all categorization metrics and achieves state-of-the-art performance in word segmentation.",
    "metadata": {
      "title": "Contrastive Prediction Strategies for Unsupervised Segmentation and Categorization of Phonemes and Words",
      "summary": "We investigate the performance on phoneme categorization and phoneme and word segmentation of several self-supervised learning (SSL) methods based on Contrastive Predictive Coding (CPC). Our experiments show that with the existing algorithms there is a trade off between categorization and segmentation performance. We investigate the source of this conflict and conclude that the use of context building networks, albeit necessary for superior performance on categorization tasks, harms segmentation performance by causing a temporal shift on the learned representations. Aiming to bridge this gap, we take inspiration from the leading approach on segmentation, which simultaneously models the speech signal at the frame and phoneme level, and incorporate multi-level modelling into Aligned CPC (ACPC), a variation of CPC which exhibits the best performance on categorization tasks. Our multi-level ACPC (mACPC) improves in all categorization metrics and achieves state-of-the-art performance in word segmentation.",
      "abstract": "We investigate the performance on phoneme categorization and phoneme and word segmentation of several self-supervised learning (SSL) methods based on Contrastive Predictive Coding (CPC). Our experiments show that with the existing algorithms there is a trade off between categorization and segmentation performance. We investigate the source of this conflict and conclude that the use of context building networks, albeit necessary for superior performance on categorization tasks, harms segmentation performance by causing a temporal shift on the learned representations. Aiming to bridge this gap, we take inspiration from the leading approach on segmentation, which simultaneously models the speech signal at the frame and phoneme level, and incorporate multi-level modelling into Aligned CPC (ACPC), a variation of CPC which exhibits the best performance on categorization tasks. Our multi-level ACPC (mACPC) improves in all categorization metrics and achieves state-of-the-art performance in word segmentation.",
      "doi": "https://doi.org/10.1109/icassp43922.2022.9746102",
      "openalex_id": "https://openalex.org/W3209993061",
      "arxiv_id": "",
      "publication_date": "2022-04-27",
      "published": "2022-04-27",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item centers on modeling spoken language representations at the phoneme/word level using self-supervised speech models, treating speech as the core object, discusses architecture/training/evaluation, and falls within the last 3 years without focusing solely on ASR/TTS or single-task constraints.",
      "evaluation": 5
    },
    "round-A_JuniorNano_reasoning": "The item centers on modeling spoken language representations at the phoneme/word level using self-supervised speech models, treating speech as the core object, discusses architecture/training/evaluation, and falls within the last 3 years without focusing solely on ASR/TTS or single-task constraints.",
    "round-A_JuniorNano_evaluation": 5,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on contrastive predictive coding for phoneme and word segmentation and categorization, which relates primarily to speech signal processing rather than explicit spoken language modeling or full-duplex spoken language models, and it does not mention spoken language models, speech language models, or language modeling of speech in the title or abstract; there is no clear indication of addressing synchronous interaction, cognitive parallelism, or low-latency full-duplex dialogue capabilities, nor does it evaluate interactive or behavioral arbitration metrics. Therefore, it does not clearly satisfy the inclusion criteria nor address the exclusion criteria directly related to synchronized interaction or spoken language models.",
      "evaluation": 2
    },
    "round-A_JuniorMini_reasoning": "The study focuses on contrastive predictive coding for phoneme and word segmentation and categorization, which relates primarily to speech signal processing rather than explicit spoken language modeling or full-duplex spoken language models, and it does not mention spoken language models, speech language models, or language modeling of speech in the title or abstract; there is no clear indication of addressing synchronous interaction, cognitive parallelism, or low-latency full-duplex dialogue capabilities, nor does it evaluate interactive or behavioral arbitration metrics. Therefore, it does not clearly satisfy the inclusion criteria nor address the exclusion criteria directly related to synchronized interaction or spoken language models.",
    "round-A_JuniorMini_evaluation": 2,
    "round-B_SeniorLead_output": {
      "reasoning": "Junior reviewers disagreed (one recommended inclusion (5), the other recommended exclusion (2)); after review, I exclude this item because the abstract describes low-level self-supervised CPC methods for phoneme/word segmentation and categorization and does not explicitly present or discuss \"spoken language models\" or language modeling of speech, nor any full-duplex interaction, synchronization, or Speech–LLM hybrid architectures required by the inclusion criteria, so it fails the mandatory inclusion conditions.",
      "evaluation": 1
    },
    "round-B_SeniorLead_reasoning": "Junior reviewers disagreed (one recommended inclusion (5), the other recommended exclusion (2)); after review, I exclude this item because the abstract describes low-level self-supervised CPC methods for phoneme/word segmentation and categorization and does not explicitly present or discuss \"spoken language models\" or language modeling of speech, nor any full-duplex interaction, synchronization, or Speech–LLM hybrid architectures required by the inclusion criteria, so it fails the mandatory inclusion conditions.",
    "round-B_SeniorLead_evaluation": 1,
    "final_verdict": "exclude (senior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Evaluating speech features with the minimal-pair ABX task: analysis of the classical MFC/PLP pipeline",
    "abstract": "We present a new framework for the evaluation of speech representations in zero-resource settings, that extends and complements previous work by Carlin, Jansen and Hermansky [1].In particular, we replace their Same/Different discrimination task by several Minimal-Pair ABX (MP-ABX) tasks.We explain the analytical advantages of this new framework and apply it to decompose the standard signal processing pipelines for computing PLP and MFC coefficients.This method enables us to confirm and quantify a variety of well-known and not-so-well-known results in a single framework.",
    "metadata": {
      "title": "Evaluating speech features with the minimal-pair ABX task: analysis of the classical MFC/PLP pipeline",
      "summary": "We present a new framework for the evaluation of speech representations in zero-resource settings, that extends and complements previous work by Carlin, Jansen and Hermansky [1].In particular, we replace their Same/Different discrimination task by several Minimal-Pair ABX (MP-ABX) tasks.We explain the analytical advantages of this new framework and apply it to decompose the standard signal processing pipelines for computing PLP and MFC coefficients.This method enables us to confirm and quantify a variety of well-known and not-so-well-known results in a single framework.",
      "abstract": "We present a new framework for the evaluation of speech representations in zero-resource settings, that extends and complements previous work by Carlin, Jansen and Hermansky [1].In particular, we replace their Same/Different discrimination task by several Minimal-Pair ABX (MP-ABX) tasks.We explain the analytical advantages of this new framework and apply it to decompose the standard signal processing pipelines for computing PLP and MFC coefficients.This method enables us to confirm and quantify a variety of well-known and not-so-well-known results in a single framework.",
      "doi": "https://doi.org/10.21437/interspeech.2013-441",
      "openalex_id": "https://openalex.org/W2395899413",
      "arxiv_id": "",
      "publication_date": "2013-08-25",
      "published": "2013-08-25",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This article focuses on evaluating traditional speech features (PLP/MFC) using an MP-ABX framework and does not treat spoken language models as the core object of modelling, nor discuss end-to-end speech–language architectures; thus it does not meet inclusion and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "This article focuses on evaluating traditional speech features (PLP/MFC) using an MP-ABX framework and does not treat spoken language models as the core object of modelling, nor discuss end-to-end speech–language architectures; thus it does not meet inclusion and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on evaluating speech features using the minimal-pair ABX task within classical acoustic feature pipelines (MFC/PLP), but does not discuss Spoken Language Models, speech language modeling architectures, training, or evaluation in the context of full-duplex interaction or advanced synchronization; it mainly targets signal processing aspects rather than language modeling or end-to-end speech interaction systems, thus not meeting the inclusion criteria.",
      "evaluation": 2
    },
    "round-A_JuniorMini_reasoning": "The study focuses on evaluating speech features using the minimal-pair ABX task within classical acoustic feature pipelines (MFC/PLP), but does not discuss Spoken Language Models, speech language modeling architectures, training, or evaluation in the context of full-duplex interaction or advanced synchronization; it mainly targets signal processing aspects rather than language modeling or end-to-end speech interaction systems, thus not meeting the inclusion criteria.",
    "round-A_JuniorMini_evaluation": 2,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:2)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Evaluating speech features with the minimal-pair ABX task (II): resistance to noise",
    "abstract": "The Minimal-Pair ABX (MP-ABX) paradigm has been proposed as a method for evaluating speech features for zeroresource/unsupervised speech technologies. We apply it in a phoneme discrimination task on the Articulation Index corpus to evaluate the resistance to noise of various speech features. In Experiment 1, we evaluate the robustness to additive noise at different signal-to-noise ratios, using car and babble noise from the Aurora-4 database and white noise. In Experiment 2, we examine the robustness to different kinds of convolutional noise. In both experiments we consider two classes of techniques to induce noise resistance: smoothing of the time-frequency representation and short-term adaptation in the time-domain. We consider smoothing along the spectral axis (as in PLP) and along the time axis (as in FDLP). For short-term adaptation in the time-domain, we compare the use of a static compressive non-linearity followed by RASTA filtering to an adaptive compression scheme.",
    "metadata": {
      "title": "Evaluating speech features with the minimal-pair ABX task (II): resistance to noise",
      "summary": "The Minimal-Pair ABX (MP-ABX) paradigm has been proposed as a method for evaluating speech features for zeroresource/unsupervised speech technologies. We apply it in a phoneme discrimination task on the Articulation Index corpus to evaluate the resistance to noise of various speech features. In Experiment 1, we evaluate the robustness to additive noise at different signal-to-noise ratios, using car and babble noise from the Aurora-4 database and white noise. In Experiment 2, we examine the robustness to different kinds of convolutional noise. In both experiments we consider two classes of techniques to induce noise resistance: smoothing of the time-frequency representation and short-term adaptation in the time-domain. We consider smoothing along the spectral axis (as in PLP) and along the time axis (as in FDLP). For short-term adaptation in the time-domain, we compare the use of a static compressive non-linearity followed by RASTA filtering to an adaptive compression scheme.",
      "abstract": "The Minimal-Pair ABX (MP-ABX) paradigm has been proposed as a method for evaluating speech features for zeroresource/unsupervised speech technologies. We apply it in a phoneme discrimination task on the Articulation Index corpus to evaluate the resistance to noise of various speech features. In Experiment 1, we evaluate the robustness to additive noise at different signal-to-noise ratios, using car and babble noise from the Aurora-4 database and white noise. In Experiment 2, we examine the robustness to different kinds of convolutional noise. In both experiments we consider two classes of techniques to induce noise resistance: smoothing of the time-frequency representation and short-term adaptation in the time-domain. We consider smoothing along the spectral axis (as in PLP) and along the time axis (as in FDLP). For short-term adaptation in the time-domain, we compare the use of a static compressive non-linearity followed by RASTA filtering to an adaptive compression scheme.",
      "doi": "https://doi.org/10.21437/interspeech.2014-228",
      "openalex_id": "https://openalex.org/W2406349064",
      "arxiv_id": "",
      "publication_date": "2014-09-14",
      "published": "2014-09-14",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The article analyzes speech features and noise-robustness using MP-ABX; it does not target Spoken Language Models or speech-language modeling, nor full-duplex interaction or end-to-end SLM frameworks as required by inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The article analyzes speech features and noise-robustness using MP-ABX; it does not target Spoken Language Models or speech-language modeling, nor full-duplex interaction or end-to-end SLM frameworks as required by inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on evaluating speech features' resistance to noise using the Minimal-Pair ABX paradigm and does not explicitly discuss Spoken Language Models (SLMs) or Full-Duplex Spoken Language Models (FD-SLMs). It lacks discussion on model architecture, training, or evaluation specifically related to speech language modeling or synchronous interaction features, and instead focuses on acoustic feature robustness in noise, which falls outside the specified inclusion criteria and aligns with exclusion criteria such as focusing on single speech technologies and not involving speech language modeling or interactive behavior arbitration.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on evaluating speech features' resistance to noise using the Minimal-Pair ABX paradigm and does not explicitly discuss Spoken Language Models (SLMs) or Full-Duplex Spoken Language Models (FD-SLMs). It lacks discussion on model architecture, training, or evaluation specifically related to speech language modeling or synchronous interaction features, and instead focuses on acoustic feature robustness in noise, which falls outside the specified inclusion criteria and aligns with exclusion criteria such as focusing on single speech technologies and not involving speech language modeling or interactive behavior arbitration.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "A role for the developing lexicon in phonetic category acquisition.",
    "abstract": "Infants segment words from fluent speech during the same period when they are learning phonetic categories, yet accounts of phonetic category acquisition typically ignore information about the words in which sounds appear. We use a Bayesian model to illustrate how feedback from segmented words might constrain phonetic category learning by providing information about which sounds occur together in words. Simulations demonstrate that word-level information can successfully disambiguate overlapping English vowel categories. Learning patterns in the model are shown to parallel human behavior from artificial language learning tasks. These findings point to a central role for the developing lexicon in phonetic category acquisition and provide a framework for incorporating top-down constraints into models of category learning.",
    "metadata": {
      "title": "A role for the developing lexicon in phonetic category acquisition.",
      "summary": "Infants segment words from fluent speech during the same period when they are learning phonetic categories, yet accounts of phonetic category acquisition typically ignore information about the words in which sounds appear. We use a Bayesian model to illustrate how feedback from segmented words might constrain phonetic category learning by providing information about which sounds occur together in words. Simulations demonstrate that word-level information can successfully disambiguate overlapping English vowel categories. Learning patterns in the model are shown to parallel human behavior from artificial language learning tasks. These findings point to a central role for the developing lexicon in phonetic category acquisition and provide a framework for incorporating top-down constraints into models of category learning.",
      "abstract": "Infants segment words from fluent speech during the same period when they are learning phonetic categories, yet accounts of phonetic category acquisition typically ignore information about the words in which sounds appear. We use a Bayesian model to illustrate how feedback from segmented words might constrain phonetic category learning by providing information about which sounds occur together in words. Simulations demonstrate that word-level information can successfully disambiguate overlapping English vowel categories. Learning patterns in the model are shown to parallel human behavior from artificial language learning tasks. These findings point to a central role for the developing lexicon in phonetic category acquisition and provide a framework for incorporating top-down constraints into models of category learning.",
      "doi": "https://doi.org/10.1037/a0034245",
      "openalex_id": "https://openalex.org/W1993755070",
      "arxiv_id": "",
      "publication_date": "2013-01-01",
      "published": "2013-01-01",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item does not focus on Spoken Language Models or speech-language modeling as the core target; it concerns a cognitive model of phonetic category learning with word-level information and does not address SLM architectures, training, or evaluation.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item does not focus on Spoken Language Models or speech-language modeling as the core target; it concerns a cognitive model of phonetic category learning with word-level information and does not address SLM architectures, training, or evaluation.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study explores phonetic category acquisition through Bayesian modeling of lexical feedback in infants, but does not address Spoken Language Models, speech language modeling architectures, training, evaluation, or real-time interactive speech systems as specified in the inclusion criteria. It lacks discussion of full-duplex spoken language modeling, synchronization, cognitive parallelism, or related evaluation metrics relevant to the theme.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study explores phonetic category acquisition through Bayesian modeling of lexical feedback in infants, but does not address Spoken Language Models, speech language modeling architectures, training, evaluation, or real-time interactive speech systems as specified in the inclusion criteria. It lacks discussion of full-duplex spoken language modeling, synchronization, cognitive parallelism, or related evaluation metrics relevant to the theme.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "A Single‐Stage Approach to Learning Phonological Categories: Insights From Inuktitut",
    "abstract": "Abstract To acquire one’s native phonological system, language‐specific phonological categories and relationships must be extracted from the input. The acquisition of the categories and relationships has each in its own right been the focus of intense research. However, it is remarkable that research on the acquisition of categories and the relations between them has proceeded, for the most part, independently of one another. We argue that this has led to the implicit view that phonological acquisition is a “two‐stage” process: Phonetic categories are first acquired and then subsequently mapped onto abstract phoneme categories. We present simulations that suggest two problems with this view: First, the learner might mistake the phoneme‐level categories for phonetic‐level categories and thus be unable to learn the relationships between phonetic‐level categories; on the other hand, the learner might construct inaccurate phonetic‐level representations that prevent it from finding regular relations among them. We suggest an alternative conception of the phonological acquisition problem that sidesteps this apparent inevitability and acquires phonemic categories in a single stage. Using acoustic data from Inuktitut, we show that this model reliably converges on a set of phoneme‐level categories and phonetic‐level relations among subcategories, without making use of a lexicon.",
    "metadata": {
      "title": "A Single‐Stage Approach to Learning Phonological Categories: Insights From Inuktitut",
      "summary": "Abstract To acquire one’s native phonological system, language‐specific phonological categories and relationships must be extracted from the input. The acquisition of the categories and relationships has each in its own right been the focus of intense research. However, it is remarkable that research on the acquisition of categories and the relations between them has proceeded, for the most part, independently of one another. We argue that this has led to the implicit view that phonological acquisition is a “two‐stage” process: Phonetic categories are first acquired and then subsequently mapped onto abstract phoneme categories. We present simulations that suggest two problems with this view: First, the learner might mistake the phoneme‐level categories for phonetic‐level categories and thus be unable to learn the relationships between phonetic‐level categories; on the other hand, the learner might construct inaccurate phonetic‐level representations that prevent it from finding regular relations among them. We suggest an alternative conception of the phonological acquisition problem that sidesteps this apparent inevitability and acquires phonemic categories in a single stage. Using acoustic data from Inuktitut, we show that this model reliably converges on a set of phoneme‐level categories and phonetic‐level relations among subcategories, without making use of a lexicon.",
      "abstract": "Abstract To acquire one’s native phonological system, language‐specific phonological categories and relationships must be extracted from the input. The acquisition of the categories and relationships has each in its own right been the focus of intense research. However, it is remarkable that research on the acquisition of categories and the relations between them has proceeded, for the most part, independently of one another. We argue that this has led to the implicit view that phonological acquisition is a “two‐stage” process: Phonetic categories are first acquired and then subsequently mapped onto abstract phoneme categories. We present simulations that suggest two problems with this view: First, the learner might mistake the phoneme‐level categories for phonetic‐level categories and thus be unable to learn the relationships between phonetic‐level categories; on the other hand, the learner might construct inaccurate phonetic‐level representations that prevent it from finding regular relations among them. We suggest an alternative conception of the phonological acquisition problem that sidesteps this apparent inevitability and acquires phonemic categories in a single stage. Using acoustic data from Inuktitut, we show that this model reliably converges on a set of phoneme‐level categories and phonetic‐level relations among subcategories, without making use of a lexicon.",
      "doi": "https://doi.org/10.1111/cogs.12008",
      "openalex_id": "https://openalex.org/W2110627398",
      "arxiv_id": "",
      "publication_date": "2012-11-08",
      "published": "2012-11-08",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This item investigates phonological category learning in Inuktitut and language acquisition, with no focus on Spoken Language Models or speech-language modeling architectures, and it does not meet the last 3-year SLM core criteria, so it should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "This item investigates phonological category learning in Inuktitut and language acquisition, with no focus on Spoken Language Models or speech-language modeling architectures, and it does not meet the last 3-year SLM core criteria, so it should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on phonological category acquisition from acoustic data but does not address Spoken Language Models, full-duplex interaction, or related architectures and evaluations within the past 3 years; it primarily discusses linguistic phonological acquisition rather than speech language modeling or synchronization in conversation systems, thus not meeting inclusion criteria nor addressing exclusion criteria regarding interactive or model-oriented research.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on phonological category acquisition from acoustic data but does not address Spoken Language Models, full-duplex interaction, or related architectures and evaluations within the past 3 years; it primarily discusses linguistic phonological acquisition rather than speech language modeling or synchronization in conversation systems, thus not meeting inclusion criteria nor addressing exclusion criteria regarding interactive or model-oriented research.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Predicting non-native speech perception using the Perceptual Assimilation Model and state-of-the-art acoustic models",
    "abstract": "Our native language influences the way we perceive speech sounds, affecting our ability to discriminate non-native sounds. We compare two ideas about the influence of the native language on speech perception: the Perceptual Assimilation Model, which appeals to a mental classification of sounds into native phoneme categories, versus the idea that rich, fine-grained phonetic representations tuned to the statistics of the native language, are sufficient. We operationalize this idea using representations from two state-of-the-art speech models, a Dirichlet process Gaussian mixture model and the more recent wav2vec 2.0 model. We present a new, open dataset of French- and English-speaking participants' speech perception behaviour for 61 vowel sounds from six languages. We show that phoneme assimilation is a better predictor than fine-grained phonetic modelling, both for the discrimination behaviour as a whole, and for predicting differences in discriminability associated with differences in native language background. We also show that wav2vec 2.0, while not good at capturing the effects of native language on speech perception, is complementary to information about native phoneme assimilation, and provides a good model of low-level phonetic representations, supporting the idea that both categorical and fine-grained perception are used during speech perception.",
    "metadata": {
      "title": "Predicting non-native speech perception using the Perceptual Assimilation Model and state-of-the-art acoustic models",
      "summary": "Our native language influences the way we perceive speech sounds, affecting our ability to discriminate non-native sounds. We compare two ideas about the influence of the native language on speech perception: the Perceptual Assimilation Model, which appeals to a mental classification of sounds into native phoneme categories, versus the idea that rich, fine-grained phonetic representations tuned to the statistics of the native language, are sufficient. We operationalize this idea using representations from two state-of-the-art speech models, a Dirichlet process Gaussian mixture model and the more recent wav2vec 2.0 model. We present a new, open dataset of French- and English-speaking participants' speech perception behaviour for 61 vowel sounds from six languages. We show that phoneme assimilation is a better predictor than fine-grained phonetic modelling, both for the discrimination behaviour as a whole, and for predicting differences in discriminability associated with differences in native language background. We also show that wav2vec 2.0, while not good at capturing the effects of native language on speech perception, is complementary to information about native phoneme assimilation, and provides a good model of low-level phonetic representations, supporting the idea that both categorical and fine-grained perception are used during speech perception.",
      "abstract": "Our native language influences the way we perceive speech sounds, affecting our ability to discriminate non-native sounds. We compare two ideas about the influence of the native language on speech perception: the Perceptual Assimilation Model, which appeals to a mental classification of sounds into native phoneme categories, versus the idea that rich, fine-grained phonetic representations tuned to the statistics of the native language, are sufficient. We operationalize this idea using representations from two state-of-the-art speech models, a Dirichlet process Gaussian mixture model and the more recent wav2vec 2.0 model. We present a new, open dataset of French- and English-speaking participants' speech perception behaviour for 61 vowel sounds from six languages. We show that phoneme assimilation is a better predictor than fine-grained phonetic modelling, both for the discrimination behaviour as a whole, and for predicting differences in discriminability associated with differences in native language background. We also show that wav2vec 2.0, while not good at capturing the effects of native language on speech perception, is complementary to information about native phoneme assimilation, and provides a good model of low-level phonetic representations, supporting the idea that both categorical and fine-grained perception are used during speech perception.",
      "doi": "https://doi.org/10.18653/v1/2021.conll-1.51",
      "openalex_id": "https://openalex.org/W3212943633",
      "arxiv_id": "",
      "publication_date": "2021-01-01",
      "published": "2021-01-01",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "Although the abstract discusses speech perception with modern acoustic models (e.g., wav2vec 2.0), it does not explicitly frame the work around Spoken Language Models or Speech Language Models in the title/abstract/keywords, so it fails the explicit inclusion criterion.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "Although the abstract discusses speech perception with modern acoustic models (e.g., wav2vec 2.0), it does not explicitly frame the work around Spoken Language Models or Speech Language Models in the title/abstract/keywords, so it fails the explicit inclusion criterion.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The article focuses on perceptual models for non-native speech sound discrimination, comparing phoneme assimilation versus fine-grained phonetic modeling, and utilizes acoustic models; however, it does not address spoken language models or full-duplex spoken language models, nor does it discuss model architectures or evaluations related to spoken language modeling, synchronous multi-turn interactions or low-latency feedback relevant to the inclusion criteria. The study is more perceptual and cognitive-science oriented rather than about spoken language modeling or dialogue system capabilities as required.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The article focuses on perceptual models for non-native speech sound discrimination, comparing phoneme assimilation versus fine-grained phonetic modeling, and utilizes acoustic models; however, it does not address spoken language models or full-duplex spoken language models, nor does it discuss model architectures or evaluations related to spoken language modeling, synchronous multi-turn interactions or low-latency feedback relevant to the inclusion criteria. The study is more perceptual and cognitive-science oriented rather than about spoken language modeling or dialogue system capabilities as required.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "The Psychometrics of Automatic Speech Recognition",
    "abstract": "Deep neural networks have had considerable success in neuroscience as models of the visual system, and recent work has suggested this may also extend to the auditory system. We tested the behaviour of a range of state of the art deep learning-based automatic speech recognition systems on a wide collection of manipulated sounds used in standard human psychometric experiments. While some systems showed qualitative agreement with humans in certain tests, in others all tested systems diverged markedly from humans. In particular, all systems used spectral invariance, temporal fine structure and speech periodicity differently from humans. We conclude that despite some promising results, none of the tested automatic speech recognition systems can yet act as a strong proxy for human speech recognition. However, we note that the more recent systems with better performance also tend to better match human results, suggesting that continued cross-fertilisation of ideas between human and automatic speech recognition may be fruitful. Our open source toolbox allows researchers to assess future automatic speech recognition systems or add additional psychoacoustic measures.",
    "metadata": {
      "title": "The Psychometrics of Automatic Speech Recognition",
      "summary": "Deep neural networks have had considerable success in neuroscience as models of the visual system, and recent work has suggested this may also extend to the auditory system. We tested the behaviour of a range of state of the art deep learning-based automatic speech recognition systems on a wide collection of manipulated sounds used in standard human psychometric experiments. While some systems showed qualitative agreement with humans in certain tests, in others all tested systems diverged markedly from humans. In particular, all systems used spectral invariance, temporal fine structure and speech periodicity differently from humans. We conclude that despite some promising results, none of the tested automatic speech recognition systems can yet act as a strong proxy for human speech recognition. However, we note that the more recent systems with better performance also tend to better match human results, suggesting that continued cross-fertilisation of ideas between human and automatic speech recognition may be fruitful. Our open source toolbox allows researchers to assess future automatic speech recognition systems or add additional psychoacoustic measures.",
      "abstract": "Deep neural networks have had considerable success in neuroscience as models of the visual system, and recent work has suggested this may also extend to the auditory system. We tested the behaviour of a range of state of the art deep learning-based automatic speech recognition systems on a wide collection of manipulated sounds used in standard human psychometric experiments. While some systems showed qualitative agreement with humans in certain tests, in others all tested systems diverged markedly from humans. In particular, all systems used spectral invariance, temporal fine structure and speech periodicity differently from humans. We conclude that despite some promising results, none of the tested automatic speech recognition systems can yet act as a strong proxy for human speech recognition. However, we note that the more recent systems with better performance also tend to better match human results, suggesting that continued cross-fertilisation of ideas between human and automatic speech recognition may be fruitful. Our open source toolbox allows researchers to assess future automatic speech recognition systems or add additional psychoacoustic measures.",
      "doi": "https://doi.org/10.1101/2021.04.19.440438",
      "openalex_id": "https://openalex.org/W3156899336",
      "arxiv_id": "",
      "publication_date": "2021-04-20",
      "published": "2021-04-20",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item analyzes ASR systems and psychometric comparisons to humans, but it does not center on Spoken Language Models or Full-Duplex Spoken Language Models, nor discuss speech-language modeling architectures/training/evaluation; it treats ASR as a standalone speech technology and focuses on single-task recognition rather than interactive speech-language modeling, thus not meeting the inclusion criteria and aligning with exclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item analyzes ASR systems and psychometric comparisons to humans, but it does not center on Spoken Language Models or Full-Duplex Spoken Language Models, nor discuss speech-language modeling architectures/training/evaluation; it treats ASR as a standalone speech technology and focuses on single-task recognition rather than interactive speech-language modeling, thus not meeting the inclusion criteria and aligning with exclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on evaluating automatic speech recognition (ASR) systems' psychometrics but does not mention spoken language models (SLMs) or full-duplex spoken language models (FD-SLMs), nor does it discuss model architecture or training of language models based on speech sequences; it centers on ASR technology performance rather than synchronous interaction or language modeling, and does not address the required inclusion criteria, thus it falls into exclusion categories such as focusing merely on ASR and lacking the core emphasis on spoken language models or synchronous interaction.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on evaluating automatic speech recognition (ASR) systems' psychometrics but does not mention spoken language models (SLMs) or full-duplex spoken language models (FD-SLMs), nor does it discuss model architecture or training of language models based on speech sequences; it centers on ASR technology performance rather than synchronous interaction or language modeling, and does not address the required inclusion criteria, thus it falls into exclusion categories such as focusing merely on ASR and lacking the core emphasis on spoken language models or synchronous interaction.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Unsupervised Pattern Discovery in Speech",
    "abstract": "We present a novel approach to speech processing based on the principle of pattern discovery. Our work represents a departure from traditional models of speech recognition, where the end goal is to classify speech into categories defined by a prespecified inventory of lexical units (i.e., phones or words). Instead, we attempt to discover such an inventory in an unsupervised manner by exploiting the structure of repeating patterns within the speech signal. We show how pattern discovery can be used to automatically acquire lexical entities directly from an untranscribed audio stream. Our approach to unsupervised word acquisition utilizes a segmental variant of a widely used dynamic programming technique, which allows us to find matching acoustic patterns between spoken utterances. By aggregating information about these matching patterns across audio streams, we demonstrate how to group similar acoustic sequences together to form clusters corresponding to lexical entities such as words and short multiword phrases. On a corpus of academic lecture material, we demonstrate that clusters found using this technique exhibit high purity and that many of the corresponding lexical identities are relevant to the underlying audio stream.",
    "metadata": {
      "title": "Unsupervised Pattern Discovery in Speech",
      "summary": "We present a novel approach to speech processing based on the principle of pattern discovery. Our work represents a departure from traditional models of speech recognition, where the end goal is to classify speech into categories defined by a prespecified inventory of lexical units (i.e., phones or words). Instead, we attempt to discover such an inventory in an unsupervised manner by exploiting the structure of repeating patterns within the speech signal. We show how pattern discovery can be used to automatically acquire lexical entities directly from an untranscribed audio stream. Our approach to unsupervised word acquisition utilizes a segmental variant of a widely used dynamic programming technique, which allows us to find matching acoustic patterns between spoken utterances. By aggregating information about these matching patterns across audio streams, we demonstrate how to group similar acoustic sequences together to form clusters corresponding to lexical entities such as words and short multiword phrases. On a corpus of academic lecture material, we demonstrate that clusters found using this technique exhibit high purity and that many of the corresponding lexical identities are relevant to the underlying audio stream.",
      "abstract": "We present a novel approach to speech processing based on the principle of pattern discovery. Our work represents a departure from traditional models of speech recognition, where the end goal is to classify speech into categories defined by a prespecified inventory of lexical units (i.e., phones or words). Instead, we attempt to discover such an inventory in an unsupervised manner by exploiting the structure of repeating patterns within the speech signal. We show how pattern discovery can be used to automatically acquire lexical entities directly from an untranscribed audio stream. Our approach to unsupervised word acquisition utilizes a segmental variant of a widely used dynamic programming technique, which allows us to find matching acoustic patterns between spoken utterances. By aggregating information about these matching patterns across audio streams, we demonstrate how to group similar acoustic sequences together to form clusters corresponding to lexical entities such as words and short multiword phrases. On a corpus of academic lecture material, we demonstrate that clusters found using this technique exhibit high purity and that many of the corresponding lexical identities are relevant to the underlying audio stream.",
      "doi": "https://doi.org/10.1109/tasl.2007.909282",
      "openalex_id": "https://openalex.org/W2114347655",
      "arxiv_id": "",
      "publication_date": "2007-12-20",
      "published": "2007-12-20",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "此論文主題為在語音中無監督地發現模式以自動從未轉錄語音流中獲取詞彙實體，未以 Spoken Language Models / 語音語言模型為核心，亦未在摘要中呈現語音模型的結構、訓練或評估等核心內容，因此不符合核心的 SLM/FD‑SLM 定義及近期研究範圍。",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "此論文主題為在語音中無監督地發現模式以自動從未轉錄語音流中獲取詞彙實體，未以 Spoken Language Models / 語音語言模型為核心，亦未在摘要中呈現語音模型的結構、訓練或評估等核心內容，因此不符合核心的 SLM/FD‑SLM 定義及近期研究範圍。",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study proposes an unsupervised pattern discovery approach for speech to acquire lexical entities without prespecified inventories, focusing on acoustic pattern clustering rather than spoken language modeling or full-duplex interaction capabilities; it neither mentions spoken language models, language modeling of speech, nor addresses synchronous interaction, cognitive parallelism, or low-latency feedback, hence it does not meet the inclusion criteria related to Spoken Language Models or advanced interaction modeling.",
      "evaluation": 2
    },
    "round-A_JuniorMini_reasoning": "The study proposes an unsupervised pattern discovery approach for speech to acquire lexical entities without prespecified inventories, focusing on acoustic pattern clustering rather than spoken language modeling or full-duplex interaction capabilities; it neither mentions spoken language models, language modeling of speech, nor addresses synchronous interaction, cognitive parallelism, or low-latency feedback, hence it does not meet the inclusion criteria related to Spoken Language Models or advanced interaction modeling.",
    "round-A_JuniorMini_evaluation": 2,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:2)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Weak top-down constraints for unsupervised acoustic model training",
    "abstract": "Typical supervised acoustic model training relies on strong top-down constraints provided by dynamic programming alignment of the input observations to phonetic sequences derived from orthographic word transcripts and pronunciation dictionaries. This paper investigates a much weaker form of top-down supervision for use in place of transcripts and dictionaries in the zero resource setting. Our proposed constraints, which can be produced using recent spoken term discovery systems, come in the form of pairs of isolated word examples that share the same unknown type. For each pair, we perform a dynamic programming alignment of the acoustic observations of the two constituent examples, generating an inventory of cross-speaker frame pairs that each provide evidence that the same subword unit model should account for them. We find these weak top-down constraints are capable of improving model speaker independence by up to 57% relative over bottom-up training alone.",
    "metadata": {
      "title": "Weak top-down constraints for unsupervised acoustic model training",
      "summary": "Typical supervised acoustic model training relies on strong top-down constraints provided by dynamic programming alignment of the input observations to phonetic sequences derived from orthographic word transcripts and pronunciation dictionaries. This paper investigates a much weaker form of top-down supervision for use in place of transcripts and dictionaries in the zero resource setting. Our proposed constraints, which can be produced using recent spoken term discovery systems, come in the form of pairs of isolated word examples that share the same unknown type. For each pair, we perform a dynamic programming alignment of the acoustic observations of the two constituent examples, generating an inventory of cross-speaker frame pairs that each provide evidence that the same subword unit model should account for them. We find these weak top-down constraints are capable of improving model speaker independence by up to 57% relative over bottom-up training alone.",
      "abstract": "Typical supervised acoustic model training relies on strong top-down constraints provided by dynamic programming alignment of the input observations to phonetic sequences derived from orthographic word transcripts and pronunciation dictionaries. This paper investigates a much weaker form of top-down supervision for use in place of transcripts and dictionaries in the zero resource setting. Our proposed constraints, which can be produced using recent spoken term discovery systems, come in the form of pairs of isolated word examples that share the same unknown type. For each pair, we perform a dynamic programming alignment of the acoustic observations of the two constituent examples, generating an inventory of cross-speaker frame pairs that each provide evidence that the same subword unit model should account for them. We find these weak top-down constraints are capable of improving model speaker independence by up to 57% relative over bottom-up training alone.",
      "doi": "https://doi.org/10.1109/icassp.2013.6639241",
      "openalex_id": "https://openalex.org/W1967924372",
      "arxiv_id": "",
      "publication_date": "2013-05-01",
      "published": "2013-05-01",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "這篇題為 'Weak top-down constraints for unsupervised acoustic model training' 的研究聚焦無監督的聲學模型訓練與弱的自上而下約束，未在標題、摘要或關鍵詞中出現 'Spoken Language Model' / 'Speech Language Model' 或等價描述，因此不符合必須包含 SLM/語音語言建模相關字眼的入選條件；儘管與語音建模的某些層面相關，但缺乏直接以語音作為核心建模與推理對象的明確說明，且可能不符合近三年的必須範圍中的語言模型焦點，因此應排除。",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "這篇題為 'Weak top-down constraints for unsupervised acoustic model training' 的研究聚焦無監督的聲學模型訓練與弱的自上而下約束，未在標題、摘要或關鍵詞中出現 'Spoken Language Model' / 'Speech Language Model' 或等價描述，因此不符合必須包含 SLM/語音語言建模相關字眼的入選條件；儘管與語音建模的某些層面相關，但缺乏直接以語音作為核心建模與推理對象的明確說明，且可能不符合近三年的必須範圍中的語言模型焦點，因此應排除。",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper discusses unsupervised acoustic model training focusing on aligning acoustic observations of speech, which relates to acoustic modeling rather than comprehensive spoken language modeling or full-duplex speech interaction; there is no indication of addressing spoken language models, language modeling of speech, full-duplex interaction, or synchronous dialog behavior arbitration, nor does it meet the inclusion criteria of mentioning SLMs or equivalent explicit keywords in title/abstract.",
      "evaluation": 2
    },
    "round-A_JuniorMini_reasoning": "The paper discusses unsupervised acoustic model training focusing on aligning acoustic observations of speech, which relates to acoustic modeling rather than comprehensive spoken language modeling or full-duplex speech interaction; there is no indication of addressing spoken language models, language modeling of speech, full-duplex interaction, or synchronous dialog behavior arbitration, nor does it meet the inclusion criteria of mentioning SLMs or equivalent explicit keywords in title/abstract.",
    "round-A_JuniorMini_evaluation": 2,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:2)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Evaluating the Reliability of Acoustic Speech Embeddings",
    "abstract": "Speech embeddings are fixed-size acoustic representations of variable-length\\nspeech sequences. They are increasingly used for a variety of tasks ranging\\nfrom information retrieval to unsupervised term discovery and speech\\nsegmentation. However, there is currently no clear methodology to compare or\\noptimise the quality of these embeddings in a task-neutral way. Here, we\\nsystematically compare two popular metrics, ABX discrimination and Mean Average\\nPrecision (MAP), on 5 languages across 17 embedding methods, ranging from\\nsupervised to fully unsupervised, and using different loss functions\\n(autoencoders, correspondence autoencoders, siamese). Then we use the ABX and\\nMAP to predict performances on a new downstream task: the unsupervised\\nestimation of the frequencies of speech segments in a given corpus. We find\\nthat overall, ABX and MAP correlate with one another and with frequency\\nestimation. However, substantial discrepancies appear in the fine-grained\\ndistinctions across languages and/or embedding methods. This makes it\\nunrealistic at present to propose a task-independent silver bullet method for\\ncomputing the intrinsic quality of speech embeddings. There is a need for more\\ndetailed analysis of the metrics currently used to evaluate such embeddings.\\n",
    "metadata": {
      "title": "Evaluating the Reliability of Acoustic Speech Embeddings",
      "summary": "Speech embeddings are fixed-size acoustic representations of variable-length\\nspeech sequences. They are increasingly used for a variety of tasks ranging\\nfrom information retrieval to unsupervised term discovery and speech\\nsegmentation. However, there is currently no clear methodology to compare or\\noptimise the quality of these embeddings in a task-neutral way. Here, we\\nsystematically compare two popular metrics, ABX discrimination and Mean Average\\nPrecision (MAP), on 5 languages across 17 embedding methods, ranging from\\nsupervised to fully unsupervised, and using different loss functions\\n(autoencoders, correspondence autoencoders, siamese). Then we use the ABX and\\nMAP to predict performances on a new downstream task: the unsupervised\\nestimation of the frequencies of speech segments in a given corpus. We find\\nthat overall, ABX and MAP correlate with one another and with frequency\\nestimation. However, substantial discrepancies appear in the fine-grained\\ndistinctions across languages and/or embedding methods. This makes it\\nunrealistic at present to propose a task-independent silver bullet method for\\ncomputing the intrinsic quality of speech embeddings. There is a need for more\\ndetailed analysis of the metrics currently used to evaluate such embeddings.\\n",
      "abstract": "Speech embeddings are fixed-size acoustic representations of variable-length\\nspeech sequences. They are increasingly used for a variety of tasks ranging\\nfrom information retrieval to unsupervised term discovery and speech\\nsegmentation. However, there is currently no clear methodology to compare or\\noptimise the quality of these embeddings in a task-neutral way. Here, we\\nsystematically compare two popular metrics, ABX discrimination and Mean Average\\nPrecision (MAP), on 5 languages across 17 embedding methods, ranging from\\nsupervised to fully unsupervised, and using different loss functions\\n(autoencoders, correspondence autoencoders, siamese). Then we use the ABX and\\nMAP to predict performances on a new downstream task: the unsupervised\\nestimation of the frequencies of speech segments in a given corpus. We find\\nthat overall, ABX and MAP correlate with one another and with frequency\\nestimation. However, substantial discrepancies appear in the fine-grained\\ndistinctions across languages and/or embedding methods. This makes it\\nunrealistic at present to propose a task-independent silver bullet method for\\ncomputing the intrinsic quality of speech embeddings. There is a need for more\\ndetailed analysis of the metrics currently used to evaluate such embeddings.\\n",
      "doi": "https://doi.org/10.21437/interspeech.2020-2362",
      "openalex_id": "https://openalex.org/W3044967013",
      "arxiv_id": "",
      "publication_date": "2020-10-25",
      "published": "2020-10-25",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item analyzes intrinsic evaluation of acoustic speech embeddings rather than focusing on Spoken Language Models or speech-language modeling as the core object, with no emphasis on full-duplex interaction, synchronization, or SLM-based architectures, so it does not meet the inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item analyzes intrinsic evaluation of acoustic speech embeddings rather than focusing on Spoken Language Models or speech-language modeling as the core object, with no emphasis on full-duplex interaction, synchronization, or SLM-based architectures, so it does not meet the inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper focuses on evaluating acoustic speech embeddings, which are fixed-size representations of speech sequences, without mention of spoken language models (SLMs) or full-duplex spoken language models (FD-SLMs) as core topics, nor does it discuss model architectures, training, or evaluation in the context of synchronous interactive spoken language modeling or low-latency full-duplex interaction. There is no reference to spoken language modeling or related key concepts in the title or abstract, and the evaluation metrics concern intrinsic embedding quality rather than interactive or synchronization aspects required by inclusion criteria. Therefore, it does not meet the inclusion criteria and overlaps partially with exclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper focuses on evaluating acoustic speech embeddings, which are fixed-size representations of speech sequences, without mention of spoken language models (SLMs) or full-duplex spoken language models (FD-SLMs) as core topics, nor does it discuss model architectures, training, or evaluation in the context of synchronous interactive spoken language modeling or low-latency full-duplex interaction. There is no reference to spoken language modeling or related key concepts in the title or abstract, and the evaluation metrics concern intrinsic embedding quality rather than interactive or synchronization aspects required by inclusion criteria. Therefore, it does not meet the inclusion criteria and overlaps partially with exclusion criteria.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Adaptor Grammars: A Framework for Specifying Compositional Nonparametric Bayesian Models",
    "abstract": "This paper introduces adaptor grammars, a class of probabilistic models of language that generalize probabilistic context-free grammars (PCFGs).Adaptor grammars augment the probabilistic rules of PCFGs with \"adaptors\" that can induce dependencies among successive uses.With a particular choice of adaptor, based on the Pitman-Yor process, nonparametric Bayesian models of language using Dirichlet processes and hierarchical Dirichlet processes can be written as simple grammars.We present a general-purpose inference algorithm for adaptor grammars, making it easy to define and use such models, and illustrate how several existing nonparametric Bayesian models can be expressed within this framework.",
    "metadata": {
      "title": "Adaptor Grammars: A Framework for Specifying Compositional Nonparametric Bayesian Models",
      "summary": "This paper introduces adaptor grammars, a class of probabilistic models of language that generalize probabilistic context-free grammars (PCFGs).Adaptor grammars augment the probabilistic rules of PCFGs with \"adaptors\" that can induce dependencies among successive uses.With a particular choice of adaptor, based on the Pitman-Yor process, nonparametric Bayesian models of language using Dirichlet processes and hierarchical Dirichlet processes can be written as simple grammars.We present a general-purpose inference algorithm for adaptor grammars, making it easy to define and use such models, and illustrate how several existing nonparametric Bayesian models can be expressed within this framework.",
      "abstract": "This paper introduces adaptor grammars, a class of probabilistic models of language that generalize probabilistic context-free grammars (PCFGs).Adaptor grammars augment the probabilistic rules of PCFGs with \"adaptors\" that can induce dependencies among successive uses.With a particular choice of adaptor, based on the Pitman-Yor process, nonparametric Bayesian models of language using Dirichlet processes and hierarchical Dirichlet processes can be written as simple grammars.We present a general-purpose inference algorithm for adaptor grammars, making it easy to define and use such models, and illustrate how several existing nonparametric Bayesian models can be expressed within this framework.",
      "doi": "https://doi.org/10.7551/mitpress/7503.003.0085",
      "openalex_id": "https://openalex.org/W2117126688",
      "arxiv_id": "",
      "publication_date": "2007-09-07",
      "published": "2007-09-07",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This article presents adaptor grammars for text-based, nonparametric Bayesian language models and does not address Spoken Language Models, speech signals, or full-duplex interactive speech systems.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "This article presents adaptor grammars for text-based, nonparametric Bayesian language models and does not address Spoken Language Models, speech signals, or full-duplex interactive speech systems.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper discusses adaptor grammars as probabilistic models for language, generalizing PCFGs and involving nonparametric Bayesian models; however, it does not specifically focus on spoken language models or speech as a primary modeling object, nor does it address the inclusion criteria related to full-duplex spoken language interaction, cognitive parallelism, or synchronization. The focus is more on abstract language modeling rather than speech-centric or interactive spoken language modeling frameworks, and the time frame of the publication is not specified to be within the past 3 years.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper discusses adaptor grammars as probabilistic models for language, generalizing PCFGs and involving nonparametric Bayesian models; however, it does not specifically focus on spoken language models or speech as a primary modeling object, nor does it address the inclusion criteria related to full-duplex spoken language interaction, cognitive parallelism, or synchronization. The focus is more on abstract language modeling rather than speech-centric or interactive spoken language modeling frameworks, and the time frame of the publication is not specified to be within the past 3 years.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Linguistic unit discovery from multimodal inputs in unwritten languages: Summary of the \"Speaking Rosetta\" JSALT 2017 Workshop",
    "abstract": "We summarize the accomplishments of a multi-disciplinary workshop exploring the computational and scientific issues surrounding the discovery of linguistic units (subwords and words) in a language without orthography. We study the replacement of orthographic transcriptions by images and/or translated text in a well-resourced language to help unsupervised discovery from raw speech.",
    "metadata": {
      "title": "Linguistic unit discovery from multimodal inputs in unwritten languages: Summary of the \"Speaking Rosetta\" JSALT 2017 Workshop",
      "summary": "We summarize the accomplishments of a multi-disciplinary workshop exploring the computational and scientific issues surrounding the discovery of linguistic units (subwords and words) in a language without orthography. We study the replacement of orthographic transcriptions by images and/or translated text in a well-resourced language to help unsupervised discovery from raw speech.",
      "abstract": "We summarize the accomplishments of a multi-disciplinary workshop exploring the computational and scientific issues surrounding the discovery of linguistic units (subwords and words) in a language without orthography. We study the replacement of orthographic transcriptions by images and/or translated text in a well-resourced language to help unsupervised discovery from raw speech.",
      "doi": "",
      "openalex_id": "https://openalex.org/W2964115348",
      "arxiv_id": "",
      "publication_date": "2018-04-15",
      "published": "2018-04-15",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This item is a 2017 workshop summary focusing on multimodal linguistic-unit discovery in unwritten languages; it does not clearly center Spoken Language Models or speech-centric modeling, and it is outside the 3-year recency window, so it fails both core inclusion and recency criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "This item is a 2017 workshop summary focusing on multimodal linguistic-unit discovery in unwritten languages; it does not clearly center Spoken Language Models or speech-centric modeling, and it is outside the 3-year recency window, so it fails both core inclusion and recency criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study summarizes a workshop on discovering linguistic units in unwritten languages using multimodal inputs without relying on orthographic transcriptions, focusing on raw speech and related computational issues. However, it does not explicitly discuss Spoken Language Models or Full-Duplex Spoken Language Models, nor does it mention model architectures, training, or evaluation related to speech language modeling. The abstract lacks references to synchronous interaction, behavior arbitration, or real-time interaction aspects required by the inclusion criteria. Additionally, there is no mention of the presence of spoken language modeling or equivalent descriptions in title or abstract, and the study appears more descriptive of linguistic unit discovery than focused on SLM-based modeling frameworks. Therefore, it does not fully meet the inclusion criteria and overlaps partially with exclusion criteria about missing explicit SLM focus and interactive modeling.",
      "evaluation": 2
    },
    "round-A_JuniorMini_reasoning": "The study summarizes a workshop on discovering linguistic units in unwritten languages using multimodal inputs without relying on orthographic transcriptions, focusing on raw speech and related computational issues. However, it does not explicitly discuss Spoken Language Models or Full-Duplex Spoken Language Models, nor does it mention model architectures, training, or evaluation related to speech language modeling. The abstract lacks references to synchronous interaction, behavior arbitration, or real-time interaction aspects required by the inclusion criteria. Additionally, there is no mention of the presence of spoken language modeling or equivalent descriptions in title or abstract, and the study appears more descriptive of linguistic unit discovery than focused on SLM-based modeling frameworks. Therefore, it does not fully meet the inclusion criteria and overlaps partially with exclusion criteria about missing explicit SLM focus and interactive modeling.",
    "round-A_JuniorMini_evaluation": 2,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:2)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Merlin: An Open Source Neural Network Speech Synthesis System",
    "abstract": "We introduce the Merlin speech synthesis toolkit for neural network-based speech synthesis.The system takes linguistic features as input, and employs neural networks to predict acoustic features, which are then passed to a vocoder to produce the speech waveform.Various neural network architectures are implemented, including a standard feedforward neural network, mixture density neural network, recurrent neural network (RNN), long short-term memory (LSTM) recurrent neural network, amongst others.The toolkit is Open Source, written in Python, and is extensible.This paper briefly describes the system, and provides some benchmarking results on a freelyavailable corpus.",
    "metadata": {
      "title": "Merlin: An Open Source Neural Network Speech Synthesis System",
      "summary": "We introduce the Merlin speech synthesis toolkit for neural network-based speech synthesis.The system takes linguistic features as input, and employs neural networks to predict acoustic features, which are then passed to a vocoder to produce the speech waveform.Various neural network architectures are implemented, including a standard feedforward neural network, mixture density neural network, recurrent neural network (RNN), long short-term memory (LSTM) recurrent neural network, amongst others.The toolkit is Open Source, written in Python, and is extensible.This paper briefly describes the system, and provides some benchmarking results on a freelyavailable corpus.",
      "abstract": "We introduce the Merlin speech synthesis toolkit for neural network-based speech synthesis.The system takes linguistic features as input, and employs neural networks to predict acoustic features, which are then passed to a vocoder to produce the speech waveform.Various neural network architectures are implemented, including a standard feedforward neural network, mixture density neural network, recurrent neural network (RNN), long short-term memory (LSTM) recurrent neural network, amongst others.The toolkit is Open Source, written in Python, and is extensible.This paper briefly describes the system, and provides some benchmarking results on a freelyavailable corpus.",
      "doi": "https://doi.org/10.21437/ssw.2016-33",
      "openalex_id": "https://openalex.org/W2598638573",
      "arxiv_id": "",
      "publication_date": "2016-09-13",
      "published": "2016-09-13",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The Merlin paper centers on a neural network-based speech synthesis (TTS) toolkit and benchmarking, not on Spoken Language Models or full-duplex interactive speech with conversational evaluation, hence it does not meet the inclusion criteria for SLM/FD-SLM focus or required interactions.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The Merlin paper centers on a neural network-based speech synthesis (TTS) toolkit and benchmarking, not on Spoken Language Models or full-duplex interactive speech with conversational evaluation, hence it does not meet the inclusion criteria for SLM/FD-SLM focus or required interactions.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper introduces an open-source neural network speech synthesis system focusing on TTS (text-to-speech) technologies using various neural network architectures, without addressing spoken language modeling, full-duplex interaction, or speech language model architectures explicitly. It lacks discussion on synchronous interaction, cognitive parallelism, or evaluations related to turn-taking, interruptions, or backchanneling, and focuses on speech synthesis rather than speech language modeling or interactive dialogue systems. It does not mention spoken language models or equivalent concepts in the title or abstract and mainly concerns traditional one-way speech synthesis, therefore it fails to meet the inclusion criteria and falls under multiple exclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper introduces an open-source neural network speech synthesis system focusing on TTS (text-to-speech) technologies using various neural network architectures, without addressing spoken language modeling, full-duplex interaction, or speech language model architectures explicitly. It lacks discussion on synchronous interaction, cognitive parallelism, or evaluations related to turn-taking, interruptions, or backchanneling, and focuses on speech synthesis rather than speech language modeling or interactive dialogue systems. It does not mention spoken language models or equivalent concepts in the title or abstract and mainly concerns traditional one-way speech synthesis, therefore it fails to meet the inclusion criteria and falls under multiple exclusion criteria.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Libri-Light: A Benchmark for ASR with Limited or No Supervision",
    "abstract": "We introduce a new collection of spoken English audio suitable for training\\nspeech recognition systems under limited or no supervision. It is derived from\\nopen-source audio books from the LibriVox project. It contains over 60K hours\\nof audio, which is, to our knowledge, the largest freely-available corpus of\\nspeech. The audio has been segmented using voice activity detection and is\\ntagged with SNR, speaker ID and genre descriptions. Additionally, we provide\\nbaseline systems and evaluation metrics working under three settings: (1) the\\nzero resource/unsupervised setting (ABX), (2) the semi-supervised setting (PER,\\nCER) and (3) the distant supervision setting (WER). Settings (2) and (3) use\\nlimited textual resources (10 minutes to 10 hours) aligned with the speech.\\nSetting (3) uses large amounts of unaligned text. They are evaluated on the\\nstandard LibriSpeech dev and test sets for comparison with the supervised\\nstate-of-the-art.\\n",
    "metadata": {
      "title": "Libri-Light: A Benchmark for ASR with Limited or No Supervision",
      "summary": "We introduce a new collection of spoken English audio suitable for training\\nspeech recognition systems under limited or no supervision. It is derived from\\nopen-source audio books from the LibriVox project. It contains over 60K hours\\nof audio, which is, to our knowledge, the largest freely-available corpus of\\nspeech. The audio has been segmented using voice activity detection and is\\ntagged with SNR, speaker ID and genre descriptions. Additionally, we provide\\nbaseline systems and evaluation metrics working under three settings: (1) the\\nzero resource/unsupervised setting (ABX), (2) the semi-supervised setting (PER,\\nCER) and (3) the distant supervision setting (WER). Settings (2) and (3) use\\nlimited textual resources (10 minutes to 10 hours) aligned with the speech.\\nSetting (3) uses large amounts of unaligned text. They are evaluated on the\\nstandard LibriSpeech dev and test sets for comparison with the supervised\\nstate-of-the-art.\\n",
      "abstract": "We introduce a new collection of spoken English audio suitable for training\\nspeech recognition systems under limited or no supervision. It is derived from\\nopen-source audio books from the LibriVox project. It contains over 60K hours\\nof audio, which is, to our knowledge, the largest freely-available corpus of\\nspeech. The audio has been segmented using voice activity detection and is\\ntagged with SNR, speaker ID and genre descriptions. Additionally, we provide\\nbaseline systems and evaluation metrics working under three settings: (1) the\\nzero resource/unsupervised setting (ABX), (2) the semi-supervised setting (PER,\\nCER) and (3) the distant supervision setting (WER). Settings (2) and (3) use\\nlimited textual resources (10 minutes to 10 hours) aligned with the speech.\\nSetting (3) uses large amounts of unaligned text. They are evaluated on the\\nstandard LibriSpeech dev and test sets for comparison with the supervised\\nstate-of-the-art.\\n",
      "doi": "https://doi.org/10.1109/icassp40776.2020.9052942",
      "openalex_id": "https://openalex.org/W2995181338",
      "arxiv_id": "",
      "publication_date": "2020-04-09",
      "published": "2020-04-09",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "此項目聚焦於 ASR 數據集與基準，核心是語音辨識任務，而非以語音為核心的語言模型（SLMs / FD-SLMs），且未明示端到端語音語言建模或同步互動，因此不符合主題與評估框架；另就時間範圍而言，未必屬於過去3年內公開的研究。",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "此項目聚焦於 ASR 數據集與基準，核心是語音辨識任務，而非以語音為核心的語言模型（SLMs / FD-SLMs），且未明示端到端語音語言建模或同步互動，因此不符合主題與評估框架；另就時間範圍而言，未必屬於過去3年內公開的研究。",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study presents a large-scale spoken English audio dataset designed for training speech recognition systems under limited or no supervision, focusing primarily on Automatic Speech Recognition (ASR) benchmarks and evaluation metrics such as WER, CER, and PER. However, it does not discuss Spoken Language Models or Full-Duplex Spoken Language Models as core research topics, nor does it address modeling or reasoning directly on speech sequences, synchronization, or interactive dialogue behaviors such as cognitive parallelism or speech overlap management. Furthermore, the evaluation solely emphasizes traditional speech recognition accuracy metrics without coverage of real-time interactive or behavioral arbitration assessments. Thus, the study aligns with traditional ASR technology rather than the inclusion criteria focused on advanced spoken language modeling or interactive full-duplex conversational models, leading to exclusion based on the provided criteria.",
      "evaluation": 2
    },
    "round-A_JuniorMini_reasoning": "The study presents a large-scale spoken English audio dataset designed for training speech recognition systems under limited or no supervision, focusing primarily on Automatic Speech Recognition (ASR) benchmarks and evaluation metrics such as WER, CER, and PER. However, it does not discuss Spoken Language Models or Full-Duplex Spoken Language Models as core research topics, nor does it address modeling or reasoning directly on speech sequences, synchronization, or interactive dialogue behaviors such as cognitive parallelism or speech overlap management. Furthermore, the evaluation solely emphasizes traditional speech recognition accuracy metrics without coverage of real-time interactive or behavioral arbitration assessments. Thus, the study aligns with traditional ASR technology rather than the inclusion criteria focused on advanced spoken language modeling or interactive full-duplex conversational models, leading to exclusion based on the provided criteria.",
    "round-A_JuniorMini_evaluation": 2,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:2)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "BLiMP: The Benchmark of Linguistic Minimal Pairs for English (Electronic Resources)",
    "abstract": "We introduce The Benchmark of Linguistic Minimal Pairs (BLiMP),1 a challenge set for evaluating the linguistic knowledge of language models (LMs) on major grammatical phenomena in English. BLiMP consists of 67 individual datasets, each containing 1,000 minimal pairs—that is, pairs of minimally different sentences that contrast in grammatical acceptability and isolate specific phenomenon in syntax, morphology, or semantics. We generate the data according to linguist-crafted grammar templates, and human aggregate agreement with the labels is 96.4%. We evaluate n-gram, LSTM, and Transformer (GPT-2 and Transformer-XL) LMs by observing whether they assign a higher probability to the acceptable sentence in each minimal pair. We find that state-of-the-art models identify morphological contrasts related to agreement reliably, but they struggle with some subtle semantic and syntactic phenomena, such as negative polarity items and extraction islands.",
    "metadata": {
      "title": "BLiMP: The Benchmark of Linguistic Minimal Pairs for English (Electronic Resources)",
      "summary": "We introduce The Benchmark of Linguistic Minimal Pairs (BLiMP),1 a challenge set for evaluating the linguistic knowledge of language models (LMs) on major grammatical phenomena in English. BLiMP consists of 67 individual datasets, each containing 1,000 minimal pairs—that is, pairs of minimally different sentences that contrast in grammatical acceptability and isolate specific phenomenon in syntax, morphology, or semantics. We generate the data according to linguist-crafted grammar templates, and human aggregate agreement with the labels is 96.4%. We evaluate n-gram, LSTM, and Transformer (GPT-2 and Transformer-XL) LMs by observing whether they assign a higher probability to the acceptable sentence in each minimal pair. We find that state-of-the-art models identify morphological contrasts related to agreement reliably, but they struggle with some subtle semantic and syntactic phenomena, such as negative polarity items and extraction islands.",
      "abstract": "We introduce The Benchmark of Linguistic Minimal Pairs (BLiMP),1 a challenge set for evaluating the linguistic knowledge of language models (LMs) on major grammatical phenomena in English. BLiMP consists of 67 individual datasets, each containing 1,000 minimal pairs—that is, pairs of minimally different sentences that contrast in grammatical acceptability and isolate specific phenomenon in syntax, morphology, or semantics. We generate the data according to linguist-crafted grammar templates, and human aggregate agreement with the labels is 96.4%. We evaluate n-gram, LSTM, and Transformer (GPT-2 and Transformer-XL) LMs by observing whether they assign a higher probability to the acceptable sentence in each minimal pair. We find that state-of-the-art models identify morphological contrasts related to agreement reliably, but they struggle with some subtle semantic and syntactic phenomena, such as negative polarity items and extraction islands.",
      "doi": "https://doi.org/10.1162/tacl_a_00321",
      "openalex_id": "https://openalex.org/W2996728628",
      "arxiv_id": "",
      "publication_date": "2020-07-29",
      "published": "2020-07-29",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "BLiMP is a text-only benchmark for English grammar and does not center Spoken Language Models or speech-language modeling, thus it fails the inclusion criteria and aligns with the exclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "BLiMP is a text-only benchmark for English grammar and does not center Spoken Language Models or speech-language modeling, thus it fails the inclusion criteria and aligns with the exclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on evaluating language models on English grammatical phenomena using text-based minimal pairs, with no mention of speech as the core modeling or inference object, nor does it address spoken language models, speech-centric modeling, or full-duplex spoken interaction aspects required by the inclusion criteria. It also does not meet any criteria related to speech language modeling or the unique features of spoken interaction specified in the inclusion criteria, warranting exclusion based on the detailed criteria.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on evaluating language models on English grammatical phenomena using text-based minimal pairs, with no mention of speech as the core modeling or inference object, nor does it address spoken language models, speech-centric modeling, or full-duplex spoken interaction aspects required by the inclusion criteria. It also does not meet any criteria related to speech language modeling or the unique features of spoken interaction specified in the inclusion criteria, warranting exclusion based on the detailed criteria.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Speech2Vec: A Sequence-to-Sequence Framework for Learning Word Embeddings from Speech",
    "abstract": "In this paper, we propose a novel deep neural network architecture, Speech2Vec, for learning fixed-length vector representations of audio segments excised from a speech corpus, where the vectors contain semantic information pertaining to the underlying spoken words, and are close to other vectors in the embedding space if their corresponding underlying spoken words are semantically similar.The proposed model can be viewed as a speech version of Word2Vec [1].Its design is based on a RNN Encoder-Decoder framework, and borrows the methodology of skipgrams or continuous bag-of-words for training.Learning word embeddings directly from speech enables Speech2Vec to make use of the semantic information carried by speech that does not exist in plain text.The learned word embeddings are evaluated and analyzed on 13 widely used word similarity benchmarks, and outperform word embeddings learned by Word2Vec from the transcriptions.",
    "metadata": {
      "title": "Speech2Vec: A Sequence-to-Sequence Framework for Learning Word Embeddings from Speech",
      "summary": "In this paper, we propose a novel deep neural network architecture, Speech2Vec, for learning fixed-length vector representations of audio segments excised from a speech corpus, where the vectors contain semantic information pertaining to the underlying spoken words, and are close to other vectors in the embedding space if their corresponding underlying spoken words are semantically similar.The proposed model can be viewed as a speech version of Word2Vec [1].Its design is based on a RNN Encoder-Decoder framework, and borrows the methodology of skipgrams or continuous bag-of-words for training.Learning word embeddings directly from speech enables Speech2Vec to make use of the semantic information carried by speech that does not exist in plain text.The learned word embeddings are evaluated and analyzed on 13 widely used word similarity benchmarks, and outperform word embeddings learned by Word2Vec from the transcriptions.",
      "abstract": "In this paper, we propose a novel deep neural network architecture, Speech2Vec, for learning fixed-length vector representations of audio segments excised from a speech corpus, where the vectors contain semantic information pertaining to the underlying spoken words, and are close to other vectors in the embedding space if their corresponding underlying spoken words are semantically similar.The proposed model can be viewed as a speech version of Word2Vec [1].Its design is based on a RNN Encoder-Decoder framework, and borrows the methodology of skipgrams or continuous bag-of-words for training.Learning word embeddings directly from speech enables Speech2Vec to make use of the semantic information carried by speech that does not exist in plain text.The learned word embeddings are evaluated and analyzed on 13 widely used word similarity benchmarks, and outperform word embeddings learned by Word2Vec from the transcriptions.",
      "doi": "https://doi.org/10.21437/interspeech.2018-2341",
      "openalex_id": "https://openalex.org/W2963425185",
      "arxiv_id": "",
      "publication_date": "2018-08-28",
      "published": "2018-08-28",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "符合核心為以語音作為主要建模與推理對象，提出了語音嵌入學習的模型架構（Speech2Vec）並在訓練與評估方面有報告；文章在過去三年內公開（2025 年）且與 spoken language model 概念相關，因此應納入。",
      "evaluation": 5
    },
    "round-A_JuniorNano_reasoning": "符合核心為以語音作為主要建模與推理對象，提出了語音嵌入學習的模型架構（Speech2Vec）並在訓練與評估方面有報告；文章在過去三年內公開（2025 年）且與 spoken language model 概念相關，因此應納入。",
    "round-A_JuniorNano_evaluation": 5,
    "round-A_JuniorMini_output": {
      "reasoning": "The study proposes Speech2Vec, a model for learning word embeddings directly from speech audio segments, focusing on semantic representation rather than text; however, it does not explicitly mention spoken language models as a core concept, full-duplex interaction, or synchronization aspects, nor does it discuss evaluation metrics related to interactive dialogue or real-time behavior arbitration. The focus is more on embedding learning from speech rather than spoken language modeling or interactive speech systems as defined in the inclusion criteria. Thus, it partially aligns with the inclusion criteria but lacks explicit mentions of key concepts such as spoken language modeling, full-duplex interaction, or synchronization, making it unlikely to be included strictly under the defined scope.",
      "evaluation": 2
    },
    "round-A_JuniorMini_reasoning": "The study proposes Speech2Vec, a model for learning word embeddings directly from speech audio segments, focusing on semantic representation rather than text; however, it does not explicitly mention spoken language models as a core concept, full-duplex interaction, or synchronization aspects, nor does it discuss evaluation metrics related to interactive dialogue or real-time behavior arbitration. The focus is more on embedding learning from speech rather than spoken language modeling or interactive speech systems as defined in the inclusion criteria. Thus, it partially aligns with the inclusion criteria but lacks explicit mentions of key concepts such as spoken language modeling, full-duplex interaction, or synchronization, making it unlikely to be included strictly under the defined scope.",
    "round-A_JuniorMini_evaluation": 2,
    "round-B_SeniorLead_output": {
      "reasoning": "I reviewed both junior reviewers' comments: JuniorNano recommended inclusion, but their rationale incorrectly emphasized recency and relation to spoken language models despite the abstract not using the term; JuniorMini correctly noted the paper focuses on learning word embeddings from speech (Speech2Vec) rather than on Spoken Language Models, full‑duplex interaction, synchronization, or language modeling of speech. The abstract does describe modeling speech-derived embeddings, but it does not explicitly claim or evaluate ‘spoken language models’ or language modeling of speech, nor does it address full‑duplex interaction, synchronization, or the required evaluation dimensions in the inclusion criteria. Therefore it fails the explicit inclusion requirement and meets several exclusion conditions (focus on an embedding/representation task rather than SLM/FD‑SLM or interactive behavior).",
      "evaluation": 2
    },
    "round-B_SeniorLead_reasoning": "I reviewed both junior reviewers' comments: JuniorNano recommended inclusion, but their rationale incorrectly emphasized recency and relation to spoken language models despite the abstract not using the term; JuniorMini correctly noted the paper focuses on learning word embeddings from speech (Speech2Vec) rather than on Spoken Language Models, full‑duplex interaction, synchronization, or language modeling of speech. The abstract does describe modeling speech-derived embeddings, but it does not explicitly claim or evaluate ‘spoken language models’ or language modeling of speech, nor does it address full‑duplex interaction, synchronization, or the required evaluation dimensions in the inclusion criteria. Therefore it fails the explicit inclusion requirement and meets several exclusion conditions (focus on an embedding/representation task rather than SLM/FD‑SLM or interactive behavior).",
    "round-B_SeniorLead_evaluation": 2,
    "final_verdict": "exclude (senior:2)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "A study on similarity and relatedness using distributional and WordNet-based approaches",
    "abstract": "This paper presents and compares WordNet-based and distributional similarity approaches. The strengths and weaknesses of each approach regarding similarity and relatedness tasks are discussed, and a combination is presented. Each of our methods independently provide the best results in their class on the RG and WordSim353 datasets, and a supervised combination of them yields the best published results on all datasets. Finally, we pioneer cross-lingual similarity, showing that our methods are easily adapted for a cross-lingual task with minor losses.",
    "metadata": {
      "title": "A study on similarity and relatedness using distributional and WordNet-based approaches",
      "summary": "This paper presents and compares WordNet-based and distributional similarity approaches. The strengths and weaknesses of each approach regarding similarity and relatedness tasks are discussed, and a combination is presented. Each of our methods independently provide the best results in their class on the RG and WordSim353 datasets, and a supervised combination of them yields the best published results on all datasets. Finally, we pioneer cross-lingual similarity, showing that our methods are easily adapted for a cross-lingual task with minor losses.",
      "abstract": "This paper presents and compares WordNet-based and distributional similarity approaches. The strengths and weaknesses of each approach regarding similarity and relatedness tasks are discussed, and a combination is presented. Each of our methods independently provide the best results in their class on the RG and WordSim353 datasets, and a supervised combination of them yields the best published results on all datasets. Finally, we pioneer cross-lingual similarity, showing that our methods are easily adapted for a cross-lingual task with minor losses.",
      "doi": "https://doi.org/10.3115/1620754.1620758",
      "openalex_id": "https://openalex.org/W2170682101",
      "arxiv_id": "",
      "publication_date": "2009-01-01",
      "published": "2009-01-01",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item concerns text-based semantic similarity methods (WordNet and distributional approaches) and cross-linguality, with no focus on Spoken Language Models or speech as the core modeling target, thus not meeting the SLM/FD-SLM core requirement and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item concerns text-based semantic similarity methods (WordNet and distributional approaches) and cross-linguality, with no focus on Spoken Language Models or speech as the core modeling target, thus not meeting the SLM/FD-SLM core requirement and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on similarity and relatedness using WordNet-based and distributional approaches, which are centered on textual semantic similarity and cross-lingual tasks. It does not mention any spoken language models, speech-level modeling, full-duplex interaction, or speech-based modeling and reasoning. The research is primarily about word similarity measures and not about spoken language models or real-time speech interaction, thus it fails to meet the core inclusion criteria focusing on speech-level language modeling and interactive spoken dialogue systems.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on similarity and relatedness using WordNet-based and distributional approaches, which are centered on textual semantic similarity and cross-lingual tasks. It does not mention any spoken language models, speech-level modeling, full-duplex interaction, or speech-based modeling and reasoning. The research is primarily about word similarity measures and not about spoken language models or real-time speech interaction, thus it fails to meet the core inclusion criteria focusing on speech-level language modeling and interactive spoken dialogue systems.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Contextual correlates of semantic similarity",
    "abstract": "Abstract The relationship between semantic and contextual similarity is investigated for pairs of nouns that vary from high to low semantic similarity. Semantic similarity is estimated by subjective ratings; contextual similarity is estimated by the method of sorting sentential contexts. The results show an inverse linear relationship between similarity of meaning and the discriminability of contexts. This relation, is obtained for two separate corpora of sentence contexts. It is concluded that, on average, for words in the same language drawn from the same syntactic and semantic categories, the more often two words can be substituted into the same contexts the more similar in meaning they are judged to be.",
    "metadata": {
      "title": "Contextual correlates of semantic similarity",
      "summary": "Abstract The relationship between semantic and contextual similarity is investigated for pairs of nouns that vary from high to low semantic similarity. Semantic similarity is estimated by subjective ratings; contextual similarity is estimated by the method of sorting sentential contexts. The results show an inverse linear relationship between similarity of meaning and the discriminability of contexts. This relation, is obtained for two separate corpora of sentence contexts. It is concluded that, on average, for words in the same language drawn from the same syntactic and semantic categories, the more often two words can be substituted into the same contexts the more similar in meaning they are judged to be.",
      "abstract": "Abstract The relationship between semantic and contextual similarity is investigated for pairs of nouns that vary from high to low semantic similarity. Semantic similarity is estimated by subjective ratings; contextual similarity is estimated by the method of sorting sentential contexts. The results show an inverse linear relationship between similarity of meaning and the discriminability of contexts. This relation, is obtained for two separate corpora of sentence contexts. It is concluded that, on average, for words in the same language drawn from the same syntactic and semantic categories, the more often two words can be substituted into the same contexts the more similar in meaning they are judged to be.",
      "doi": "https://doi.org/10.1080/01690969108406936",
      "openalex_id": "https://openalex.org/W2103318667",
      "arxiv_id": "",
      "publication_date": "1991-01-01",
      "published": "1991-01-01",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "此论文聚焦词汇的语义与情境相似性之间的关系，属于纯文本语义/认知语言学研究，未以 Spoken Language Models 或等价的语音建模为核心，也不涉及端到端的语音-文字系统、同步互动或实时评估，因此不符合包含标准，应排除。",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "此论文聚焦词汇的语义与情境相似性之间的关系，属于纯文本语义/认知语言学研究，未以 Spoken Language Models 或等价的语音建模为核心，也不涉及端到端的语音-文字系统、同步互动或实时评估，因此不符合包含标准，应排除。",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on the relationship between semantic and contextual similarity of nouns, with no mention of spoken language models, speech as the primary modeling object, or any of the inclusion criteria related to full-duplex spoken language models or synchronous interaction. It does not discuss model architectures, training, evaluation of spoken language models, or speech-language modeling, and does not fall within the scope of the inclusion criteria provided.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on the relationship between semantic and contextual similarity of nouns, with no mention of spoken language models, speech as the primary modeling object, or any of the inclusion criteria related to full-duplex spoken language models or synchronous interaction. It does not discuss model architectures, training, evaluation of spoken language models, or speech-language modeling, and does not fall within the scope of the inclusion criteria provided.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Contextual correlates of synonymy",
    "abstract": "article Free AccessContextual correlates of synonymy Authors: Herbert Rubenstein Decision Sciences Lab, Bedford, MA Decision Sciences Lab, Bedford, MAView Profile , John B. Goodenough Decision Sciences Lab, Bedford, MA Decision Sciences Lab, Bedford, MAView Profile Authors Info & Claims Communications of the ACMVolume 8Issue 10Oct. 1965 pp 627–633https://doi.org/10.1145/365628.365657Published:01 October 1965Publication History 695citation3,459DownloadsMetricsTotal Citations695Total Downloads3,459Last 12 Months573Last 6 weeks59 Get Citation AlertsNew Citation Alert added!This alert has been successfully added and will be sent to:You will be notified whenever a record that you have chosen has been cited.To manage your alert preferences, click on the button below.Manage my AlertsNew Citation Alert!Please log in to your account Save to BinderSave to BinderCreate a New BinderNameCancelCreateExport CitationPublisher SiteeReaderPDF",
    "metadata": {
      "title": "Contextual correlates of synonymy",
      "summary": "article Free AccessContextual correlates of synonymy Authors: Herbert Rubenstein Decision Sciences Lab, Bedford, MA Decision Sciences Lab, Bedford, MAView Profile , John B. Goodenough Decision Sciences Lab, Bedford, MA Decision Sciences Lab, Bedford, MAView Profile Authors Info & Claims Communications of the ACMVolume 8Issue 10Oct. 1965 pp 627–633https://doi.org/10.1145/365628.365657Published:01 October 1965Publication History 695citation3,459DownloadsMetricsTotal Citations695Total Downloads3,459Last 12 Months573Last 6 weeks59 Get Citation AlertsNew Citation Alert added!This alert has been successfully added and will be sent to:You will be notified whenever a record that you have chosen has been cited.To manage your alert preferences, click on the button below.Manage my AlertsNew Citation Alert!Please log in to your account Save to BinderSave to BinderCreate a New BinderNameCancelCreateExport CitationPublisher SiteeReaderPDF",
      "abstract": "article Free AccessContextual correlates of synonymy Authors: Herbert Rubenstein Decision Sciences Lab, Bedford, MA Decision Sciences Lab, Bedford, MAView Profile , John B. Goodenough Decision Sciences Lab, Bedford, MA Decision Sciences Lab, Bedford, MAView Profile Authors Info & Claims Communications of the ACMVolume 8Issue 10Oct. 1965 pp 627–633https://doi.org/10.1145/365628.365657Published:01 October 1965Publication History 695citation3,459DownloadsMetricsTotal Citations695Total Downloads3,459Last 12 Months573Last 6 weeks59 Get Citation AlertsNew Citation Alert added!This alert has been successfully added and will be sent to:You will be notified whenever a record that you have chosen has been cited.To manage your alert preferences, click on the button below.Manage my AlertsNew Citation Alert!Please log in to your account Save to BinderSave to BinderCreate a New BinderNameCancelCreateExport CitationPublisher SiteeReaderPDF",
      "doi": "https://doi.org/10.1145/365628.365657",
      "openalex_id": "https://openalex.org/W2080100102",
      "arxiv_id": "",
      "publication_date": "1965-10-01",
      "published": "1965-10-01",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This item does not focus on Spoken Language Models or speech-language modeling and is outside the 3-year window, so it should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "This item does not focus on Spoken Language Models or speech-language modeling and is outside the 3-year window, so it should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The article titled 'Contextual correlates of synonymy' is published in 1965, well outside the inclusion time frame of the past 3 years, and the title and abstract do not mention Spoken Language Models or any related concepts focusing on speech-based modeling, training, or evaluation; moreover, the content appears unrelated to full-duplex spoken language models or their interactive components, thus it does not meet the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The article titled 'Contextual correlates of synonymy' is published in 1965, well outside the inclusion time frame of the past 3 years, and the title and abstract do not mention Spoken Language Models or any related concepts focusing on speech-based modeling, training, or evaluation; moreover, the content appears unrelated to full-duplex spoken language models or their interactive components, thus it does not meet the inclusion criteria.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "SimLex-999: Evaluating Semantic Models With (Genuine) Similarity Estimation",
    "abstract": "We present SimLex-999, a gold standard resource for evaluating distributional semantic models that improves on existing resources in several important ways. First, in contrast to gold standards such as WordSim-353 and MEN, it explicitly quantifies similarity rather than association or relatedness so that pairs of entities that are associated but not actually similar (Freud, psychology) have a low rating. We show that, via this focus on similarity, SimLex-999 incentivizes the development of models with a different, and arguably wider, range of applications than those which reflect conceptual association. Second, SimLex-999 contains a range of concrete and abstract adjective, noun, and verb pairs, together with an independent rating of concreteness and (free) association strength for each pair. This diversity enables fine-grained analyses of the performance of models on concepts of different types, and consequently greater insight into how architectures can be improved. Further, unlike existing gold standard evaluations, for which automatic approaches have reached or surpassed the inter-annotator agreement ceiling, state-of-the-art models perform well below this ceiling on SimLex-999. There is therefore plenty of scope for SimLex-999 to quantify future improvements to distributional semantic models, guiding the development of the next generation of representation-learning architectures.",
    "metadata": {
      "title": "SimLex-999: Evaluating Semantic Models With (Genuine) Similarity Estimation",
      "summary": "We present SimLex-999, a gold standard resource for evaluating distributional semantic models that improves on existing resources in several important ways. First, in contrast to gold standards such as WordSim-353 and MEN, it explicitly quantifies similarity rather than association or relatedness so that pairs of entities that are associated but not actually similar (Freud, psychology) have a low rating. We show that, via this focus on similarity, SimLex-999 incentivizes the development of models with a different, and arguably wider, range of applications than those which reflect conceptual association. Second, SimLex-999 contains a range of concrete and abstract adjective, noun, and verb pairs, together with an independent rating of concreteness and (free) association strength for each pair. This diversity enables fine-grained analyses of the performance of models on concepts of different types, and consequently greater insight into how architectures can be improved. Further, unlike existing gold standard evaluations, for which automatic approaches have reached or surpassed the inter-annotator agreement ceiling, state-of-the-art models perform well below this ceiling on SimLex-999. There is therefore plenty of scope for SimLex-999 to quantify future improvements to distributional semantic models, guiding the development of the next generation of representation-learning architectures.",
      "abstract": "We present SimLex-999, a gold standard resource for evaluating distributional semantic models that improves on existing resources in several important ways. First, in contrast to gold standards such as WordSim-353 and MEN, it explicitly quantifies similarity rather than association or relatedness so that pairs of entities that are associated but not actually similar (Freud, psychology) have a low rating. We show that, via this focus on similarity, SimLex-999 incentivizes the development of models with a different, and arguably wider, range of applications than those which reflect conceptual association. Second, SimLex-999 contains a range of concrete and abstract adjective, noun, and verb pairs, together with an independent rating of concreteness and (free) association strength for each pair. This diversity enables fine-grained analyses of the performance of models on concepts of different types, and consequently greater insight into how architectures can be improved. Further, unlike existing gold standard evaluations, for which automatic approaches have reached or surpassed the inter-annotator agreement ceiling, state-of-the-art models perform well below this ceiling on SimLex-999. There is therefore plenty of scope for SimLex-999 to quantify future improvements to distributional semantic models, guiding the development of the next generation of representation-learning architectures.",
      "doi": "https://doi.org/10.1162/coli_a_00237",
      "openalex_id": "https://openalex.org/W1854884267",
      "arxiv_id": "",
      "publication_date": "2015-12-01",
      "published": "2015-12-01",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The abstract describes SimLex-999 as a semantic similarity resource for text-only models; it does not mention Spoken Language Models, speech-language modeling, or speech-based evaluation, so it does not meet the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The abstract describes SimLex-999 as a semantic similarity resource for text-only models; it does not mention Spoken Language Models, speech-language modeling, or speech-based evaluation, so it does not meet the inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The title and abstract describe SimLex-999, a dataset for evaluating semantic similarity in distributional semantic models focused on text-based representation learning, without mention of spoken language models, speech as a core modeling object, or aspects of full-duplex spoken interaction, synchronization, or cognitive parallelism. The study does not meet the inclusion criteria requiring the focus on speech-level language modeling or FD-SLMs, nor does it address multimodal speech-LLM or synchronous interaction aspects. Therefore, it neither aligns with the key concepts nor the required evaluation dimensions for the targeted research area.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The title and abstract describe SimLex-999, a dataset for evaluating semantic similarity in distributional semantic models focused on text-based representation learning, without mention of spoken language models, speech as a core modeling object, or aspects of full-duplex spoken interaction, synchronization, or cognitive parallelism. The study does not meet the inclusion criteria requiring the focus on speech-level language modeling or FD-SLMs, nor does it address multimodal speech-LLM or synchronous interaction aspects. Therefore, it neither aligns with the key concepts nor the required evaluation dimensions for the targeted research area.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "An Unsupervised Model for Instance Level Subcategorization Acquisition",
    "abstract": "Most existing systems for subcategorization frame (SCF) acquisition rely on supervised parsing and infer SCF distributions at type, rather than instance level.These systems suffer from poor portability across domains and their benefit for NLP tasks that involve sentence-level processing is limited.We propose a new unsupervised, Markov Random Field-based model for SCF acquisition which is designed to address these problems.The system relies on supervised POS tagging rather than parsing, and is capable of learning SCFs at instance level.We perform evaluation against gold standard data which shows that our system outperforms several supervised and type-level SCF baselines.We also conduct task-based evaluation in the context of verb similarity prediction, demonstrating that a vector space model based on our SCFs substantially outperforms a lexical model and a model based on a supervised parser 1 .",
    "metadata": {
      "title": "An Unsupervised Model for Instance Level Subcategorization Acquisition",
      "summary": "Most existing systems for subcategorization frame (SCF) acquisition rely on supervised parsing and infer SCF distributions at type, rather than instance level.These systems suffer from poor portability across domains and their benefit for NLP tasks that involve sentence-level processing is limited.We propose a new unsupervised, Markov Random Field-based model for SCF acquisition which is designed to address these problems.The system relies on supervised POS tagging rather than parsing, and is capable of learning SCFs at instance level.We perform evaluation against gold standard data which shows that our system outperforms several supervised and type-level SCF baselines.We also conduct task-based evaluation in the context of verb similarity prediction, demonstrating that a vector space model based on our SCFs substantially outperforms a lexical model and a model based on a supervised parser 1 .",
      "abstract": "Most existing systems for subcategorization frame (SCF) acquisition rely on supervised parsing and infer SCF distributions at type, rather than instance level.These systems suffer from poor portability across domains and their benefit for NLP tasks that involve sentence-level processing is limited.We propose a new unsupervised, Markov Random Field-based model for SCF acquisition which is designed to address these problems.The system relies on supervised POS tagging rather than parsing, and is capable of learning SCFs at instance level.We perform evaluation against gold standard data which shows that our system outperforms several supervised and type-level SCF baselines.We also conduct task-based evaluation in the context of verb similarity prediction, demonstrating that a vector space model based on our SCFs substantially outperforms a lexical model and a model based on a supervised parser 1 .",
      "doi": "https://doi.org/10.3115/v1/d14-1034",
      "openalex_id": "https://openalex.org/W2176085882",
      "arxiv_id": "",
      "publication_date": "2014-01-01",
      "published": "2014-01-01",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The article centers on unsupervised subcategorization frame acquisition in text-based NLP, without focus on Spoken Language Models, speech modeling, turn-taking, or interactive speech tasks; it does not meet the spoken-language modeling criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The article centers on unsupervised subcategorization frame acquisition in text-based NLP, without focus on Spoken Language Models, speech modeling, turn-taking, or interactive speech tasks; it does not meet the spoken-language modeling criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on unsupervised acquisition of subcategorization frames at the instance level for NLP tasks, relying on supervised POS tagging, without any mention of spoken language modeling, acoustic signal processing, full-duplex spoken language models, or speech-LLM architectures. It deals primarily with text-level NLP parsing and verb similarity prediction tasks, and does not involve spoken language models or related synchrony, real-time interactive features, or speech-based modeling as required by the inclusion criteria. Therefore, it does not meet the inclusion criteria nor contradict the exclusion criteria explicitly but is clearly outside the topic scope.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on unsupervised acquisition of subcategorization frames at the instance level for NLP tasks, relying on supervised POS tagging, without any mention of spoken language modeling, acoustic signal processing, full-duplex spoken language models, or speech-LLM architectures. It deals primarily with text-level NLP parsing and verb similarity prediction tasks, and does not involve spoken language models or related synchrony, real-time interactive features, or speech-based modeling as required by the inclusion criteria. Therefore, it does not meet the inclusion criteria nor contradict the exclusion criteria explicitly but is clearly outside the topic scope.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "A word at a time",
    "abstract": "Computing the degree of semantic relatedness of words is a key functionality of many language applications such as search, clustering, and disambiguation. Previous approaches to computing semantic relatedness mostly used static language resources, while essentially ignoring their temporal aspects. We believe that a considerable amount of relatedness information can also be found in studying patterns of word usage over time. Consider, for instance, a newspaper archive spanning many years. Two words such as \"war\" and \"peace\" might rarely co-occur in the same articles, yet their patterns of use over time might be similar. In this paper, we propose a new semantic relatedness model, Temporal Semantic Analysis (TSA), which captures this temporal information. The previous state of the art method, Explicit Semantic Analysis (ESA), represented word semantics as a vector of concepts. TSA uses a more refined representation, where each concept is no longer scalar, but is instead represented as time series over a corpus of temporally-ordered documents. To the best of our knowledge, this is the first attempt to incorporate temporal evidence into models of semantic relatedness. Empirical evaluation shows that TSA provides consistent improvements over the state of the art ESA results on multiple benchmarks.",
    "metadata": {
      "title": "A word at a time",
      "summary": "Computing the degree of semantic relatedness of words is a key functionality of many language applications such as search, clustering, and disambiguation. Previous approaches to computing semantic relatedness mostly used static language resources, while essentially ignoring their temporal aspects. We believe that a considerable amount of relatedness information can also be found in studying patterns of word usage over time. Consider, for instance, a newspaper archive spanning many years. Two words such as \"war\" and \"peace\" might rarely co-occur in the same articles, yet their patterns of use over time might be similar. In this paper, we propose a new semantic relatedness model, Temporal Semantic Analysis (TSA), which captures this temporal information. The previous state of the art method, Explicit Semantic Analysis (ESA), represented word semantics as a vector of concepts. TSA uses a more refined representation, where each concept is no longer scalar, but is instead represented as time series over a corpus of temporally-ordered documents. To the best of our knowledge, this is the first attempt to incorporate temporal evidence into models of semantic relatedness. Empirical evaluation shows that TSA provides consistent improvements over the state of the art ESA results on multiple benchmarks.",
      "abstract": "Computing the degree of semantic relatedness of words is a key functionality of many language applications such as search, clustering, and disambiguation. Previous approaches to computing semantic relatedness mostly used static language resources, while essentially ignoring their temporal aspects. We believe that a considerable amount of relatedness information can also be found in studying patterns of word usage over time. Consider, for instance, a newspaper archive spanning many years. Two words such as \"war\" and \"peace\" might rarely co-occur in the same articles, yet their patterns of use over time might be similar. In this paper, we propose a new semantic relatedness model, Temporal Semantic Analysis (TSA), which captures this temporal information. The previous state of the art method, Explicit Semantic Analysis (ESA), represented word semantics as a vector of concepts. TSA uses a more refined representation, where each concept is no longer scalar, but is instead represented as time series over a corpus of temporally-ordered documents. To the best of our knowledge, this is the first attempt to incorporate temporal evidence into models of semantic relatedness. Empirical evaluation shows that TSA provides consistent improvements over the state of the art ESA results on multiple benchmarks.",
      "doi": "https://doi.org/10.1145/1963405.1963455",
      "openalex_id": "https://openalex.org/W2026487812",
      "arxiv_id": "",
      "publication_date": "2011-03-28",
      "published": "2011-03-28",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This item focuses on text-based semantic relatedness using temporal analysis (Temporal Semantic Analysis) and does not involve Spoken Language Models, speech-language modeling, or full-duplex interactive speech systems required by the inclusion criteria; thus it fails all core criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "This item focuses on text-based semantic relatedness using temporal analysis (Temporal Semantic Analysis) and does not involve Spoken Language Models, speech-language modeling, or full-duplex interactive speech systems required by the inclusion criteria; thus it fails all core criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on semantic relatedness of words using temporal patterns in static text corpora and does not address spoken language models (SLMs) or full-duplex spoken language interaction, nor does it involve acoustic or speech signal modeling. It neither discusses speech-based model architectures, training, nor evaluation in the context of spoken language modeling, and thus fails to meet the core inclusion criteria centered on spoken language modeling and synchronous speech interaction. The study is purely text-based NLP research and does not address synchronization, real-time speech interaction, or cognitive parallelism in speech; therefore, it should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on semantic relatedness of words using temporal patterns in static text corpora and does not address spoken language models (SLMs) or full-duplex spoken language interaction, nor does it involve acoustic or speech signal modeling. It neither discusses speech-based model architectures, training, nor evaluation in the context of spoken language modeling, and thus fails to meet the core inclusion criteria centered on spoken language modeling and synchronous speech interaction. The study is purely text-based NLP research and does not address synchronization, real-time speech interaction, or cognitive parallelism in speech; therefore, it should be excluded.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Large-scale learning of word relatedness with constraints",
    "abstract": "Prior work on computing semantic relatedness of words focused on representing their meaning in isolation, effectively disregarding inter-word affinities. We propose a large-scale data mining approach to learning word-word relatedness, where known pairs of related words impose constraints on the learning process. We learn for each word a low-dimensional representation, which strives to maximize the likelihood of a word given the contexts in which it appears. Our method, called CLEAR, is shown to significantly outperform previously published approaches. The proposed method is based on first principles, and is generic enough to exploit diverse types of text corpora, while having the flexibility to impose constraints on the derived word similarities. We also make publicly available a new labeled dataset for evaluating word relatedness algorithms, which we believe to be the largest such dataset to date.",
    "metadata": {
      "title": "Large-scale learning of word relatedness with constraints",
      "summary": "Prior work on computing semantic relatedness of words focused on representing their meaning in isolation, effectively disregarding inter-word affinities. We propose a large-scale data mining approach to learning word-word relatedness, where known pairs of related words impose constraints on the learning process. We learn for each word a low-dimensional representation, which strives to maximize the likelihood of a word given the contexts in which it appears. Our method, called CLEAR, is shown to significantly outperform previously published approaches. The proposed method is based on first principles, and is generic enough to exploit diverse types of text corpora, while having the flexibility to impose constraints on the derived word similarities. We also make publicly available a new labeled dataset for evaluating word relatedness algorithms, which we believe to be the largest such dataset to date.",
      "abstract": "Prior work on computing semantic relatedness of words focused on representing their meaning in isolation, effectively disregarding inter-word affinities. We propose a large-scale data mining approach to learning word-word relatedness, where known pairs of related words impose constraints on the learning process. We learn for each word a low-dimensional representation, which strives to maximize the likelihood of a word given the contexts in which it appears. Our method, called CLEAR, is shown to significantly outperform previously published approaches. The proposed method is based on first principles, and is generic enough to exploit diverse types of text corpora, while having the flexibility to impose constraints on the derived word similarities. We also make publicly available a new labeled dataset for evaluating word relatedness algorithms, which we believe to be the largest such dataset to date.",
      "doi": "https://doi.org/10.1145/2339530.2339751",
      "openalex_id": "https://openalex.org/W2142625445",
      "arxiv_id": "",
      "publication_date": "2012-08-12",
      "published": "2012-08-12",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The article focuses on learning word-word relatedness from text corpora and word embeddings, with no emphasis on Spoken Language Models or speech-based interaction, thus not meeting the inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The article focuses on learning word-word relatedness from text corpora and word embeddings, with no emphasis on Spoken Language Models or speech-based interaction, thus not meeting the inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on learning word relatedness using low-dimensional word representations from text corpora, without any mention of spoken language models, speech as a primary modeling target, or full-duplex spoken interaction capabilities; thus, it does not satisfy the inclusion criteria centered on spoken language modeling and full-duplex speech interaction.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on learning word relatedness using low-dimensional word representations from text corpora, without any mention of spoken language models, speech as a primary modeling target, or full-duplex spoken interaction capabilities; thus, it does not satisfy the inclusion criteria centered on spoken language modeling and full-duplex speech interaction.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Librispeech: An ASR corpus based on public domain audio books",
    "abstract": "This paper introduces a new corpus of read English speech, suitable for training and evaluating speech recognition systems. The LibriSpeech corpus is derived from audiobooks that are part of the LibriVox project, and contains 1000 hours of speech sampled at 16 kHz. We have made the corpus freely available for download, along with separately prepared language-model training data and pre-built language models. We show that acoustic models trained on LibriSpeech give lower error rate on the Wall Street Journal (WSJ) test sets than models trained on WSJ itself. We are also releasing Kaldi scripts that make it easy to build these systems.",
    "metadata": {
      "title": "Librispeech: An ASR corpus based on public domain audio books",
      "summary": "This paper introduces a new corpus of read English speech, suitable for training and evaluating speech recognition systems. The LibriSpeech corpus is derived from audiobooks that are part of the LibriVox project, and contains 1000 hours of speech sampled at 16 kHz. We have made the corpus freely available for download, along with separately prepared language-model training data and pre-built language models. We show that acoustic models trained on LibriSpeech give lower error rate on the Wall Street Journal (WSJ) test sets than models trained on WSJ itself. We are also releasing Kaldi scripts that make it easy to build these systems.",
      "abstract": "This paper introduces a new corpus of read English speech, suitable for training and evaluating speech recognition systems. The LibriSpeech corpus is derived from audiobooks that are part of the LibriVox project, and contains 1000 hours of speech sampled at 16 kHz. We have made the corpus freely available for download, along with separately prepared language-model training data and pre-built language models. We show that acoustic models trained on LibriSpeech give lower error rate on the Wall Street Journal (WSJ) test sets than models trained on WSJ itself. We are also releasing Kaldi scripts that make it easy to build these systems.",
      "doi": "https://doi.org/10.1109/icassp.2015.7178964",
      "openalex_id": "https://openalex.org/W1494198834",
      "arxiv_id": "",
      "publication_date": "2015-04-01",
      "published": "2015-04-01",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "此論文為 LibriSpeech 資料集與語音識別資源之說明，未以 Spoken Language Models / Speech Language Models 作為核心研究對象，亦非近三年新研究，因此不符合納入條件。",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "此論文為 LibriSpeech 資料集與語音識別資源之說明，未以 Spoken Language Models / Speech Language Models 作為核心研究對象，亦非近三年新研究，因此不符合納入條件。",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper introduces a speech corpus (LibriSpeech) primarily for training and evaluating ASR systems, focusing on acoustic and language models for speech recognition, without discussing spoken language models, full-duplex interaction, synchronous behavior arbitration, or multimodal speech-language modeling architectures; it does not meet the inclusion criteria of explicitly addressing spoken language modeling or full-duplex spoken interaction, nor does it cover the required evaluation metrics related to interactive speech behaviors.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper introduces a speech corpus (LibriSpeech) primarily for training and evaluating ASR systems, focusing on acoustic and language models for speech recognition, without discussing spoken language models, full-duplex interaction, synchronous behavior arbitration, or multimodal speech-language modeling architectures; it does not meet the inclusion criteria of explicitly addressing spoken language modeling or full-duplex spoken interaction, nor does it cover the required evaluation metrics related to interactive speech behaviors.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Look, Listen and Learn",
    "abstract": "We consider the question: what can be learnt by looking at and listening to a large number of unlabelled videos? There is a valuable, but so far untapped, source of information contained in the video itself - the correspondence between the visual and the audio streams, and we introduce a novel \"Audio-Visual Correspondence\" learning task that makes use of this. Training visual and audio networks from scratch, without any additional supervision other than the raw unconstrained videos themselves, is shown to successfully solve this task, and, more interestingly, result in good visual and audio representations. These features set the new state-of-the-art on two sound classification benchmarks, and perform on par with the state-of-the-art selfsupervised approaches on ImageNet classification. We also demonstrate that the network is able to localize objects in both modalities, as well as perform fine-grained recognition tasks.",
    "metadata": {
      "title": "Look, Listen and Learn",
      "summary": "We consider the question: what can be learnt by looking at and listening to a large number of unlabelled videos? There is a valuable, but so far untapped, source of information contained in the video itself - the correspondence between the visual and the audio streams, and we introduce a novel \"Audio-Visual Correspondence\" learning task that makes use of this. Training visual and audio networks from scratch, without any additional supervision other than the raw unconstrained videos themselves, is shown to successfully solve this task, and, more interestingly, result in good visual and audio representations. These features set the new state-of-the-art on two sound classification benchmarks, and perform on par with the state-of-the-art selfsupervised approaches on ImageNet classification. We also demonstrate that the network is able to localize objects in both modalities, as well as perform fine-grained recognition tasks.",
      "abstract": "We consider the question: what can be learnt by looking at and listening to a large number of unlabelled videos? There is a valuable, but so far untapped, source of information contained in the video itself - the correspondence between the visual and the audio streams, and we introduce a novel \"Audio-Visual Correspondence\" learning task that makes use of this. Training visual and audio networks from scratch, without any additional supervision other than the raw unconstrained videos themselves, is shown to successfully solve this task, and, more interestingly, result in good visual and audio representations. These features set the new state-of-the-art on two sound classification benchmarks, and perform on par with the state-of-the-art selfsupervised approaches on ImageNet classification. We also demonstrate that the network is able to localize objects in both modalities, as well as perform fine-grained recognition tasks.",
      "doi": "https://doi.org/10.1109/iccv.2017.73",
      "openalex_id": "https://openalex.org/W2619697695",
      "arxiv_id": "",
      "publication_date": "2017-10-01",
      "published": "2017-10-01",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "這篇論文並非以 Spoken Language Models / Speech Language Models 為核心，也非聚焦於語音語言建模、端到端的語音互動或多模態語音對話評估，且年份超出近3年的範圍，因此不符合包含條件。",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "這篇論文並非以 Spoken Language Models / Speech Language Models 為核心，也非聚焦於語音語言建模、端到端的語音互動或多模態語音對話評估，且年份超出近3年的範圍，因此不符合包含條件。",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study addresses learning from audio-visual data without explicit focus on spoken language models or full-duplex spoken language interaction as required; it does not discuss spoken language modeling architecture, synchronization, or interaction aspects central to the inclusion criteria, and the language modeling focus is on audio-visual correspondence rather than speech language model training or evaluation, thus it does not meet the core inclusion requirements.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study addresses learning from audio-visual data without explicit focus on spoken language models or full-duplex spoken language interaction as required; it does not discuss spoken language modeling architecture, synchronization, or interaction aspects central to the inclusion criteria, and the language modeling focus is on audio-visual correspondence rather than speech language model training or evaluation, thus it does not meet the core inclusion requirements.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Representations of language in a model of visually grounded speech signal",
    "abstract": "We present a visually grounded model of speech perception which projects spoken utterances and images to a joint semantic space. We use a multi-layer recurrent highway network to model the temporal nature of spoken speech, and show that it learns to extract both form and meaning-based linguistic knowledge from the input signal. We carry out an in-depth analysis of the representations used by different components of the trained model and show that encoding of semantic aspects tends to become richer as we go up the hierarchy of layers, whereas encoding of form-related aspects of the language input tends to initially increase and then plateau or decrease.",
    "metadata": {
      "title": "Representations of language in a model of visually grounded speech signal",
      "summary": "We present a visually grounded model of speech perception which projects spoken utterances and images to a joint semantic space. We use a multi-layer recurrent highway network to model the temporal nature of spoken speech, and show that it learns to extract both form and meaning-based linguistic knowledge from the input signal. We carry out an in-depth analysis of the representations used by different components of the trained model and show that encoding of semantic aspects tends to become richer as we go up the hierarchy of layers, whereas encoding of form-related aspects of the language input tends to initially increase and then plateau or decrease.",
      "abstract": "We present a visually grounded model of speech perception which projects spoken utterances and images to a joint semantic space. We use a multi-layer recurrent highway network to model the temporal nature of spoken speech, and show that it learns to extract both form and meaning-based linguistic knowledge from the input signal. We carry out an in-depth analysis of the representations used by different components of the trained model and show that encoding of semantic aspects tends to become richer as we go up the hierarchy of layers, whereas encoding of form-related aspects of the language input tends to initially increase and then plateau or decrease.",
      "doi": "https://doi.org/10.18653/v1/p17-1057",
      "openalex_id": "https://openalex.org/W2586148577",
      "arxiv_id": "",
      "publication_date": "2017-01-01",
      "published": "2017-01-01",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The abstract describes a visually grounded model of speech perception and analyzes spoken representations, but it does not explicitly frame the work as a Spoken Language Model / Speech Language Model nor include the required keywords (e.g., 'spoken language model' or 'speech language model') in the title/abstract, which is the stated gating criterion, despite addressing speech representations.",
      "evaluation": 2
    },
    "round-A_JuniorNano_reasoning": "The abstract describes a visually grounded model of speech perception and analyzes spoken representations, but it does not explicitly frame the work as a Spoken Language Model / Speech Language Model nor include the required keywords (e.g., 'spoken language model' or 'speech language model') in the title/abstract, which is the stated gating criterion, despite addressing speech representations.",
    "round-A_JuniorNano_evaluation": 2,
    "round-A_JuniorMini_output": {
      "reasoning": "The study presents a visually grounded model of speech perception focusing on learning linguistic knowledge from speech input, which aligns partially with spoken language modeling; however, the abstract lacks explicit mention of full-duplex interaction capabilities, synchronization, or evaluation related to real-time interactive behaviors that are central to the inclusion criteria, and it does not clearly frame itself within the context of Spoken Language Models or Speech Language Models terminology as required.",
      "evaluation": 2
    },
    "round-A_JuniorMini_reasoning": "The study presents a visually grounded model of speech perception focusing on learning linguistic knowledge from speech input, which aligns partially with spoken language modeling; however, the abstract lacks explicit mention of full-duplex interaction capabilities, synchronization, or evaluation related to real-time interactive behaviors that are central to the inclusion criteria, and it does not clearly frame itself within the context of Spoken Language Models or Speech Language Models terminology as required.",
    "round-A_JuniorMini_evaluation": 2,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:2)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "In Spoken Word Recognition, the Future Predicts the Past",
    "abstract": "Speech is an inherently noisy and ambiguous signal. To fluently derive meaning, a listener must integrate contextual information to guide interpretations of the sensory input. Although many studies have demonstrated the influence of prior context on speech perception, the neural mechanisms supporting the integration of subsequent context remain unknown. Using MEG to record from human auditory cortex, we analyzed responses to spoken words with a varyingly ambiguous onset phoneme, the identity of which is later disambiguated at the lexical uniqueness point. Fifty participants (both male and female) were recruited across two MEG experiments. Our findings suggest that primary auditory cortex is sensitive to phonological ambiguity very early during processing at just 50 ms after onset. Subphonemic detail is preserved in auditory cortex over long timescales and re-evoked at subsequent phoneme positions. Commitments to phonological categories occur in parallel, resolving on the shorter timescale of ∼450 ms. These findings provide evidence that future input determines the perception of earlier speech sounds by maintaining sensory features until they can be integrated with top-down lexical information.",
    "metadata": {
      "title": "In Spoken Word Recognition, the Future Predicts the Past",
      "summary": "Speech is an inherently noisy and ambiguous signal. To fluently derive meaning, a listener must integrate contextual information to guide interpretations of the sensory input. Although many studies have demonstrated the influence of prior context on speech perception, the neural mechanisms supporting the integration of subsequent context remain unknown. Using MEG to record from human auditory cortex, we analyzed responses to spoken words with a varyingly ambiguous onset phoneme, the identity of which is later disambiguated at the lexical uniqueness point. Fifty participants (both male and female) were recruited across two MEG experiments. Our findings suggest that primary auditory cortex is sensitive to phonological ambiguity very early during processing at just 50 ms after onset. Subphonemic detail is preserved in auditory cortex over long timescales and re-evoked at subsequent phoneme positions. Commitments to phonological categories occur in parallel, resolving on the shorter timescale of ∼450 ms. These findings provide evidence that future input determines the perception of earlier speech sounds by maintaining sensory features until they can be integrated with top-down lexical information.",
      "abstract": "Speech is an inherently noisy and ambiguous signal. To fluently derive meaning, a listener must integrate contextual information to guide interpretations of the sensory input. Although many studies have demonstrated the influence of prior context on speech perception, the neural mechanisms supporting the integration of subsequent context remain unknown. Using MEG to record from human auditory cortex, we analyzed responses to spoken words with a varyingly ambiguous onset phoneme, the identity of which is later disambiguated at the lexical uniqueness point. Fifty participants (both male and female) were recruited across two MEG experiments. Our findings suggest that primary auditory cortex is sensitive to phonological ambiguity very early during processing at just 50 ms after onset. Subphonemic detail is preserved in auditory cortex over long timescales and re-evoked at subsequent phoneme positions. Commitments to phonological categories occur in parallel, resolving on the shorter timescale of ∼450 ms. These findings provide evidence that future input determines the perception of earlier speech sounds by maintaining sensory features until they can be integrated with top-down lexical information.",
      "doi": "https://doi.org/10.1523/jneurosci.0065-18.2018",
      "openalex_id": "https://openalex.org/W2949387496",
      "arxiv_id": "",
      "publication_date": "2018-07-16",
      "published": "2018-07-16",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This MEG study investigates neural processing of spoken words and does not discuss Spoken Language Models or Full-Duplex Spoken Language Models, thus it does not meet the inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "This MEG study investigates neural processing of spoken words and does not discuss Spoken Language Models or Full-Duplex Spoken Language Models, thus it does not meet the inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study investigates neural mechanisms of spoken word recognition using MEG, focusing on auditory cortex responses to phonological ambiguity and top-down lexical integration, without addressing Spoken Language Models, speech language model architectures, training, or evaluation. It also does not mention full-duplex interaction, synchronization, or joint speech modeling from acoustic signal to modeling and reasoning in a computational framework. The main focus is on neurocognitive processing rather than computational spoken language modeling or interactive speech systems, so it does not meet inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study investigates neural mechanisms of spoken word recognition using MEG, focusing on auditory cortex responses to phonological ambiguity and top-down lexical integration, without addressing Spoken Language Models, speech language model architectures, training, or evaluation. It also does not mention full-duplex interaction, synchronization, or joint speech modeling from acoustic signal to modeling and reasoning in a computational framework. The main focus is on neurocognitive processing rather than computational spoken language modeling or interactive speech systems, so it does not meet inclusion criteria.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Probing Lexical Ambiguity: Word Vectors Encode Number and Relatedness of Senses",
    "abstract": "Abstract Lexical ambiguity—the phenomenon of a single word having multiple, distinguishable senses—is pervasive in language. Both the degree of ambiguity of a word (roughly, its number of senses) and the relatedness of those senses have been found to have widespread effects on language acquisition and processing. Recently, distributional approaches to semantics, in which a word's meaning is determined by its contexts, have led to successful research quantifying the degree of ambiguity, but these measures have not distinguished between the ambiguity of words with multiple related senses versus multiple unrelated meanings. In this work, we present the first assessment of whether distributional meaning representations can capture the ambiguity structure of a word, including both the number and relatedness of senses. On a very large sample of English words, we find that some, but not all, distributional semantic representations that we test exhibit detectable differences between sets of monosemes (unambiguous words; N = 964), polysemes (with multiple related senses; N = 4,096), and homonyms (with multiple unrelated senses; N = 355). Our findings begin to answer open questions from earlier work regarding whether distributional semantic representations of words, which successfully capture various semantic relationships, also reflect fine‐grained aspects of meaning structure that influence human behavior. Our findings emphasize the importance of measuring whether proposed lexical representations capture such distinctions: In addition to standard benchmarks that test the similarity structure of distributional semantic models, we need to also consider whether they have cognitively plausible ambiguity structure.",
    "metadata": {
      "title": "Probing Lexical Ambiguity: Word Vectors Encode Number and Relatedness of Senses",
      "summary": "Abstract Lexical ambiguity—the phenomenon of a single word having multiple, distinguishable senses—is pervasive in language. Both the degree of ambiguity of a word (roughly, its number of senses) and the relatedness of those senses have been found to have widespread effects on language acquisition and processing. Recently, distributional approaches to semantics, in which a word's meaning is determined by its contexts, have led to successful research quantifying the degree of ambiguity, but these measures have not distinguished between the ambiguity of words with multiple related senses versus multiple unrelated meanings. In this work, we present the first assessment of whether distributional meaning representations can capture the ambiguity structure of a word, including both the number and relatedness of senses. On a very large sample of English words, we find that some, but not all, distributional semantic representations that we test exhibit detectable differences between sets of monosemes (unambiguous words; N = 964), polysemes (with multiple related senses; N = 4,096), and homonyms (with multiple unrelated senses; N = 355). Our findings begin to answer open questions from earlier work regarding whether distributional semantic representations of words, which successfully capture various semantic relationships, also reflect fine‐grained aspects of meaning structure that influence human behavior. Our findings emphasize the importance of measuring whether proposed lexical representations capture such distinctions: In addition to standard benchmarks that test the similarity structure of distributional semantic models, we need to also consider whether they have cognitively plausible ambiguity structure.",
      "abstract": "Abstract Lexical ambiguity—the phenomenon of a single word having multiple, distinguishable senses—is pervasive in language. Both the degree of ambiguity of a word (roughly, its number of senses) and the relatedness of those senses have been found to have widespread effects on language acquisition and processing. Recently, distributional approaches to semantics, in which a word's meaning is determined by its contexts, have led to successful research quantifying the degree of ambiguity, but these measures have not distinguished between the ambiguity of words with multiple related senses versus multiple unrelated meanings. In this work, we present the first assessment of whether distributional meaning representations can capture the ambiguity structure of a word, including both the number and relatedness of senses. On a very large sample of English words, we find that some, but not all, distributional semantic representations that we test exhibit detectable differences between sets of monosemes (unambiguous words; N = 964), polysemes (with multiple related senses; N = 4,096), and homonyms (with multiple unrelated senses; N = 355). Our findings begin to answer open questions from earlier work regarding whether distributional semantic representations of words, which successfully capture various semantic relationships, also reflect fine‐grained aspects of meaning structure that influence human behavior. Our findings emphasize the importance of measuring whether proposed lexical representations capture such distinctions: In addition to standard benchmarks that test the similarity structure of distributional semantic models, we need to also consider whether they have cognitively plausible ambiguity structure.",
      "doi": "https://doi.org/10.1111/cogs.12943",
      "openalex_id": "https://openalex.org/W3160251434",
      "arxiv_id": "",
      "publication_date": "2021-05-01",
      "published": "2021-05-01",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This item centers on text-based distributional semantics and lexical ambiguity; it does not address Spoken Language Models or speech-based interaction (FD-SLMs), thus it fails to meet the inclusion criteria and does not align with the exclusion criteria focused on speech/interaction modeling.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "This item centers on text-based distributional semantics and lexical ambiguity; it does not address Spoken Language Models or speech-based interaction (FD-SLMs), thus it fails to meet the inclusion criteria and does not align with the exclusion criteria focused on speech/interaction modeling.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on lexical ambiguity and distributional semantic representations related to word senses, with no mention of spoken language models, speech-level language modeling, full-duplex spoken interaction, or any related topics from the inclusion criteria, nor does it discuss model architectures, training, or evaluation related to speech signals; therefore, it clearly falls outside the topic scope defined.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on lexical ambiguity and distributional semantic representations related to word senses, with no mention of spoken language models, speech-level language modeling, full-duplex spoken interaction, or any related topics from the inclusion criteria, nor does it discuss model architectures, training, or evaluation related to speech signals; therefore, it clearly falls outside the topic scope defined.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "WavLM: Large-Scale Self-Supervised Pre-Training for Full Stack Speech Processing",
    "abstract": "Self-supervised learning (SSL) achieves great success in speech recognition,\\nwhile limited exploration has been attempted for other speech processing tasks.\\nAs speech signal contains multi-faceted information including speaker identity,\\nparalinguistics, spoken content, etc., learning universal representations for\\nall speech tasks is challenging. To tackle the problem, we propose a new\\npre-trained model, WavLM, to solve full-stack downstream speech tasks. WavLM\\njointly learns masked speech prediction and denoising in pre-training. By this\\nmeans, WavLM does not only keep the speech content modeling capability by the\\nmasked speech prediction, but also improves the potential to non-ASR tasks by\\nthe speech denoising. In addition, WavLM employs gated relative position bias\\nfor the Transformer structure to better capture the sequence ordering of input\\nspeech. We also scale up the training dataset from 60k hours to 94k hours.\\nWavLM Large achieves state-of-the-art performance on the SUPERB benchmark, and\\nbrings significant improvements for various speech processing tasks on their\\nrepresentative benchmarks. The code and pre-trained models are available at\\nhttps://aka.ms/wavlm.\\n",
    "metadata": {
      "title": "WavLM: Large-Scale Self-Supervised Pre-Training for Full Stack Speech Processing",
      "summary": "Self-supervised learning (SSL) achieves great success in speech recognition,\\nwhile limited exploration has been attempted for other speech processing tasks.\\nAs speech signal contains multi-faceted information including speaker identity,\\nparalinguistics, spoken content, etc., learning universal representations for\\nall speech tasks is challenging. To tackle the problem, we propose a new\\npre-trained model, WavLM, to solve full-stack downstream speech tasks. WavLM\\njointly learns masked speech prediction and denoising in pre-training. By this\\nmeans, WavLM does not only keep the speech content modeling capability by the\\nmasked speech prediction, but also improves the potential to non-ASR tasks by\\nthe speech denoising. In addition, WavLM employs gated relative position bias\\nfor the Transformer structure to better capture the sequence ordering of input\\nspeech. We also scale up the training dataset from 60k hours to 94k hours.\\nWavLM Large achieves state-of-the-art performance on the SUPERB benchmark, and\\nbrings significant improvements for various speech processing tasks on their\\nrepresentative benchmarks. The code and pre-trained models are available at\\nhttps://aka.ms/wavlm.\\n",
      "abstract": "Self-supervised learning (SSL) achieves great success in speech recognition,\\nwhile limited exploration has been attempted for other speech processing tasks.\\nAs speech signal contains multi-faceted information including speaker identity,\\nparalinguistics, spoken content, etc., learning universal representations for\\nall speech tasks is challenging. To tackle the problem, we propose a new\\npre-trained model, WavLM, to solve full-stack downstream speech tasks. WavLM\\njointly learns masked speech prediction and denoising in pre-training. By this\\nmeans, WavLM does not only keep the speech content modeling capability by the\\nmasked speech prediction, but also improves the potential to non-ASR tasks by\\nthe speech denoising. In addition, WavLM employs gated relative position bias\\nfor the Transformer structure to better capture the sequence ordering of input\\nspeech. We also scale up the training dataset from 60k hours to 94k hours.\\nWavLM Large achieves state-of-the-art performance on the SUPERB benchmark, and\\nbrings significant improvements for various speech processing tasks on their\\nrepresentative benchmarks. The code and pre-trained models are available at\\nhttps://aka.ms/wavlm.\\n",
      "doi": "https://doi.org/10.1109/jstsp.2022.3188113",
      "openalex_id": "https://openalex.org/W3209984917",
      "arxiv_id": "",
      "publication_date": "2022-07-04",
      "published": "2022-07-04",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The paper focuses on a spoken-language-model style pretraining approach (WavLM) for full-stack speech processing, addressing multi-task speech representations and non-ASR tasks, with architecture/training/evaluation across benchmarks (SUPERB), and is from within the last 3 years (2025). It treats speech as the core modeling target, not merely an intermediate ASR/text NLP pipeline, and goes beyond single-task ASR/TTS exclusions.",
      "evaluation": 5
    },
    "round-A_JuniorNano_reasoning": "The paper focuses on a spoken-language-model style pretraining approach (WavLM) for full-stack speech processing, addressing multi-task speech representations and non-ASR tasks, with architecture/training/evaluation across benchmarks (SUPERB), and is from within the last 3 years (2025). It treats speech as the core modeling target, not merely an intermediate ASR/text NLP pipeline, and goes beyond single-task ASR/TTS exclusions.",
    "round-A_JuniorNano_evaluation": 5,
    "round-A_JuniorMini_output": {
      "reasoning": "The presented study focuses on a large-scale self-supervised pre-training model for full stack speech processing, which mainly addresses masked speech prediction and speech denoising to improve speech content modeling and speaker-related tasks, primarily targeting speech recognition benchmarks. Although it deals with speech sequence modeling, it does not explicitly state \"spoken language model\" or equivalent terms, nor does it mention full-duplex interaction, cognitive parallelism, or synchronization mechanisms required by the inclusion criteria. The evaluation metrics focus on speech processing tasks but not on interactive or real-time behavior arbitration aspects. Therefore, it fails to meet the stringent inclusion criteria centered on spoken language modeling and full-duplex interactive capabilities.",
      "evaluation": 2
    },
    "round-A_JuniorMini_reasoning": "The presented study focuses on a large-scale self-supervised pre-training model for full stack speech processing, which mainly addresses masked speech prediction and speech denoising to improve speech content modeling and speaker-related tasks, primarily targeting speech recognition benchmarks. Although it deals with speech sequence modeling, it does not explicitly state \"spoken language model\" or equivalent terms, nor does it mention full-duplex interaction, cognitive parallelism, or synchronization mechanisms required by the inclusion criteria. The evaluation metrics focus on speech processing tasks but not on interactive or real-time behavior arbitration aspects. Therefore, it fails to meet the stringent inclusion criteria centered on spoken language modeling and full-duplex interactive capabilities.",
    "round-A_JuniorMini_evaluation": 2,
    "round-B_SeniorLead_output": {
      "reasoning": "Junior reviewers disagreed (one scored 5, one 2). After review: WavLM is a large-scale self-supervised speech representation model (masked prediction + denoising) evaluated on SUPERB and various speech tasks, but the title/abstract do not use “spoken language model(s)” or equivalent language-modeling-of-speech phrasing, nor do they address full‑duplex interaction, cognitive parallelism, overlap/interrupt handling, low-latency realtime arbitration, or related evaluation metrics — therefore it fails the inclusion criteria and meets exclusion conditions (focused on general SSL for ASR/other tasks rather than SLM/FD‑SLM research). One-sentence explanation: Exclude because the work is a general speech-pretraining/representation paper without explicit spoken-language-model framing or full‑duplex/interactive modeling and evaluation required by the topic.",
      "evaluation": 1
    },
    "round-B_SeniorLead_reasoning": "Junior reviewers disagreed (one scored 5, one 2). After review: WavLM is a large-scale self-supervised speech representation model (masked prediction + denoising) evaluated on SUPERB and various speech tasks, but the title/abstract do not use “spoken language model(s)” or equivalent language-modeling-of-speech phrasing, nor do they address full‑duplex interaction, cognitive parallelism, overlap/interrupt handling, low-latency realtime arbitration, or related evaluation metrics — therefore it fails the inclusion criteria and meets exclusion conditions (focused on general SSL for ASR/other tasks rather than SLM/FD‑SLM research). One-sentence explanation: Exclude because the work is a general speech-pretraining/representation paper without explicit spoken-language-model framing or full‑duplex/interactive modeling and evaluation required by the topic.",
    "round-B_SeniorLead_evaluation": 1,
    "final_verdict": "exclude (senior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Probing phoneme, language and speaker information in unsupervised speech representations",
    "abstract": "Unsupervised models of representations based on Contrastive Predictive Coding\\n(CPC)[1] are primarily used in spoken language modelling in that they encode\\nphonetic information. In this study, we ask what other types of information are\\npresent in CPC speech representations. We focus on three categories: phone\\nclass, gender and language, and compare monolingual and bilingual models. Using\\nqualitative and quantitative tools, we find that both gender and phone class\\ninformation are present in both types of models. Language information, however,\\nis very salient in the bilingual model only, suggesting CPC models learn to\\ndiscriminate languages when trained on multiple languages. Some language\\ninformation can also be retrieved from monolingual models, but it is more\\ndiffused across all features. These patterns hold when analyses are carried on\\nthe discrete units from a downstream clustering model. However, although there\\nis no effect of the number of target clusters on phone class and language\\ninformation, more gender information is encoded with more clusters. Finally, we\\nfind that there is some cost to being exposed to two languages on a downstream\\nphoneme discrimination task.\\n",
    "metadata": {
      "title": "Probing phoneme, language and speaker information in unsupervised speech representations",
      "summary": "Unsupervised models of representations based on Contrastive Predictive Coding\\n(CPC)[1] are primarily used in spoken language modelling in that they encode\\nphonetic information. In this study, we ask what other types of information are\\npresent in CPC speech representations. We focus on three categories: phone\\nclass, gender and language, and compare monolingual and bilingual models. Using\\nqualitative and quantitative tools, we find that both gender and phone class\\ninformation are present in both types of models. Language information, however,\\nis very salient in the bilingual model only, suggesting CPC models learn to\\ndiscriminate languages when trained on multiple languages. Some language\\ninformation can also be retrieved from monolingual models, but it is more\\ndiffused across all features. These patterns hold when analyses are carried on\\nthe discrete units from a downstream clustering model. However, although there\\nis no effect of the number of target clusters on phone class and language\\ninformation, more gender information is encoded with more clusters. Finally, we\\nfind that there is some cost to being exposed to two languages on a downstream\\nphoneme discrimination task.\\n",
      "abstract": "Unsupervised models of representations based on Contrastive Predictive Coding\\n(CPC)[1] are primarily used in spoken language modelling in that they encode\\nphonetic information. In this study, we ask what other types of information are\\npresent in CPC speech representations. We focus on three categories: phone\\nclass, gender and language, and compare monolingual and bilingual models. Using\\nqualitative and quantitative tools, we find that both gender and phone class\\ninformation are present in both types of models. Language information, however,\\nis very salient in the bilingual model only, suggesting CPC models learn to\\ndiscriminate languages when trained on multiple languages. Some language\\ninformation can also be retrieved from monolingual models, but it is more\\ndiffused across all features. These patterns hold when analyses are carried on\\nthe discrete units from a downstream clustering model. However, although there\\nis no effect of the number of target clusters on phone class and language\\ninformation, more gender information is encoded with more clusters. Finally, we\\nfind that there is some cost to being exposed to two languages on a downstream\\nphoneme discrimination task.\\n",
      "doi": "https://doi.org/10.21437/interspeech.2022-373",
      "openalex_id": "https://openalex.org/W4225726571",
      "arxiv_id": "",
      "publication_date": "2022-09-16",
      "published": "2022-09-16",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item analyzes information encoded in unsupervised speech representations (phoneme, language, speaker/gender) within CPC, but does not address Spoken Language Models with end-to-end or full-duplex interactive capabilities, turn-taking, synchronization, or real-time interactive evaluation frameworks, thus failing the core inclusion criteria and meeting one or more exclusion aspects.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item analyzes information encoded in unsupervised speech representations (phoneme, language, speaker/gender) within CPC, but does not address Spoken Language Models with end-to-end or full-duplex interactive capabilities, turn-taking, synchronization, or real-time interactive evaluation frameworks, thus failing the core inclusion criteria and meeting one or more exclusion aspects.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on analyzing unsupervised speech representations derived from Contrastive Predictive Coding, exploring phoneme, language, and speaker information in these speech models. However, it centers on phonetic and representation information extraction rather than the broader aspects of spoken language models or full-duplex spoken language models, especially lacking discussion on interactive capabilities, synchronization, low-latency, or dialogue behavior arbitration. Furthermore, the paper does not explicitly mention spoken language modeling or speech language models as per the inclusion criteria, nor does it address full-duplex or synchronous interaction aspects required for inclusion. Therefore, it doesn’t meet the all-inclusive criteria for spoken language models focusing on real-time interaction and modeling as described, nor does it violate obvious exclusion criteria but is outside the core scope required.",
      "evaluation": 2
    },
    "round-A_JuniorMini_reasoning": "The study focuses on analyzing unsupervised speech representations derived from Contrastive Predictive Coding, exploring phoneme, language, and speaker information in these speech models. However, it centers on phonetic and representation information extraction rather than the broader aspects of spoken language models or full-duplex spoken language models, especially lacking discussion on interactive capabilities, synchronization, low-latency, or dialogue behavior arbitration. Furthermore, the paper does not explicitly mention spoken language modeling or speech language models as per the inclusion criteria, nor does it address full-duplex or synchronous interaction aspects required for inclusion. Therefore, it doesn’t meet the all-inclusive criteria for spoken language models focusing on real-time interaction and modeling as described, nor does it violate obvious exclusion criteria but is outside the core scope required.",
    "round-A_JuniorMini_evaluation": 2,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:2)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Perceptimatic: A Human Speech Perception Benchmark for Unsupervised Subword Modelling",
    "abstract": "In this paper, we present a data set and methods to compare speech processing models and human behaviour on a phone discrimination task. We provide Perceptimatic, an open data set which consists of French and English speech stimuli, as well as the results of 91 English- and 93 French-speaking listeners. The stimuli test a wide range of French and English contrasts, and are extracted directly from corpora of natural running read speech, used for the 2017 Zero Resource Speech Challenge. We provide a method to compare humans' perceptual space with models' representational space, and we apply it to models previously submitted to the Challenge. We show that, unlike unsupervised models and supervised multilingual models, a standard supervised monolingual HMM-GMM phone recognition system, while good at discriminating phones, yields a representational space very different from that of human native listeners.",
    "metadata": {
      "title": "Perceptimatic: A Human Speech Perception Benchmark for Unsupervised Subword Modelling",
      "summary": "In this paper, we present a data set and methods to compare speech processing models and human behaviour on a phone discrimination task. We provide Perceptimatic, an open data set which consists of French and English speech stimuli, as well as the results of 91 English- and 93 French-speaking listeners. The stimuli test a wide range of French and English contrasts, and are extracted directly from corpora of natural running read speech, used for the 2017 Zero Resource Speech Challenge. We provide a method to compare humans' perceptual space with models' representational space, and we apply it to models previously submitted to the Challenge. We show that, unlike unsupervised models and supervised multilingual models, a standard supervised monolingual HMM-GMM phone recognition system, while good at discriminating phones, yields a representational space very different from that of human native listeners.",
      "abstract": "In this paper, we present a data set and methods to compare speech processing models and human behaviour on a phone discrimination task. We provide Perceptimatic, an open data set which consists of French and English speech stimuli, as well as the results of 91 English- and 93 French-speaking listeners. The stimuli test a wide range of French and English contrasts, and are extracted directly from corpora of natural running read speech, used for the 2017 Zero Resource Speech Challenge. We provide a method to compare humans' perceptual space with models' representational space, and we apply it to models previously submitted to the Challenge. We show that, unlike unsupervised models and supervised multilingual models, a standard supervised monolingual HMM-GMM phone recognition system, while good at discriminating phones, yields a representational space very different from that of human native listeners.",
      "doi": "https://doi.org/10.21437/interspeech.2020-1671",
      "openalex_id": "https://openalex.org/W3093121832",
      "arxiv_id": "",
      "publication_date": "2020-10-25",
      "published": "2020-10-25",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "Although the paper benchmarks human speech perception against models and discusses subword representations, it does not clearly center on Spoken Language Models / Speech Language Models as the core modeling object, nor on interactive full-duplex speech processing; it is primarily a perception benchmark rather than a model architecture/training/evaluation for SLMs.",
      "evaluation": 2
    },
    "round-A_JuniorNano_reasoning": "Although the paper benchmarks human speech perception against models and discusses subword representations, it does not clearly center on Spoken Language Models / Speech Language Models as the core modeling object, nor on interactive full-duplex speech processing; it is primarily a perception benchmark rather than a model architecture/training/evaluation for SLMs.",
    "round-A_JuniorNano_evaluation": 2,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper presents a dataset and analysis comparing human speech perception to models in a phone discrimination task without explicit focus on spoken language models, speech language model architecture, training, or evaluation aspects related to full-duplex spoken language modeling or real-time interactive behaviors as required; it rather centers on phone discrimination and representational similarity, not on SLMs or FD-SLMs or their interaction capabilities.",
      "evaluation": 2
    },
    "round-A_JuniorMini_reasoning": "The paper presents a dataset and analysis comparing human speech perception to models in a phone discrimination task without explicit focus on spoken language models, speech language model architecture, training, or evaluation aspects related to full-duplex spoken language modeling or real-time interactive behaviors as required; it rather centers on phone discrimination and representational similarity, not on SLMs or FD-SLMs or their interaction capabilities.",
    "round-A_JuniorMini_evaluation": 2,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:2)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Towards Unsupervised Learning of Speech Features in the Wild",
    "abstract": "Recent work on unsupervised contrastive learning of speech representation has shown promising results, but so far has mostly been applied to clean, curated speech datasets. Can it also be used with unprepared audio data \"in the wild\"? Here, we explore three potential problems in this setting: (i) presence of non-speech data, (ii) noisy or low quality speech data, and (iii) imbalance in speaker distribution. We show that on the Libri-light train set, which is itself a relatively clean speech-only dataset, these problems combined can already have a performance cost of up to 30% relative for the ABX score. We show that the first two problems can be alleviated by data filtering, with voice activity detection selecting speech segments, while perplexity of a model trained with clean data helping to discard entire files. We show that the third problem can be alleviated by learning a speaker embedding in the predictive branch of the model. We show that these techniques build more robust speech features that can be transferred to an ASR task in the low resource setting.",
    "metadata": {
      "title": "Towards Unsupervised Learning of Speech Features in the Wild",
      "summary": "Recent work on unsupervised contrastive learning of speech representation has shown promising results, but so far has mostly been applied to clean, curated speech datasets. Can it also be used with unprepared audio data \"in the wild\"? Here, we explore three potential problems in this setting: (i) presence of non-speech data, (ii) noisy or low quality speech data, and (iii) imbalance in speaker distribution. We show that on the Libri-light train set, which is itself a relatively clean speech-only dataset, these problems combined can already have a performance cost of up to 30% relative for the ABX score. We show that the first two problems can be alleviated by data filtering, with voice activity detection selecting speech segments, while perplexity of a model trained with clean data helping to discard entire files. We show that the third problem can be alleviated by learning a speaker embedding in the predictive branch of the model. We show that these techniques build more robust speech features that can be transferred to an ASR task in the low resource setting.",
      "abstract": "Recent work on unsupervised contrastive learning of speech representation has shown promising results, but so far has mostly been applied to clean, curated speech datasets. Can it also be used with unprepared audio data \"in the wild\"? Here, we explore three potential problems in this setting: (i) presence of non-speech data, (ii) noisy or low quality speech data, and (iii) imbalance in speaker distribution. We show that on the Libri-light train set, which is itself a relatively clean speech-only dataset, these problems combined can already have a performance cost of up to 30% relative for the ABX score. We show that the first two problems can be alleviated by data filtering, with voice activity detection selecting speech segments, while perplexity of a model trained with clean data helping to discard entire files. We show that the third problem can be alleviated by learning a speaker embedding in the predictive branch of the model. We show that these techniques build more robust speech features that can be transferred to an ASR task in the low resource setting.",
      "doi": "https://doi.org/10.1109/slt48900.2021.9383461",
      "openalex_id": "https://openalex.org/W3148101939",
      "arxiv_id": "",
      "publication_date": "2021-01-19",
      "published": "2021-01-19",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "Although it deals with speech representations for ASR, it is not centered on Spoken Language Models or full-duplex speech–language modeling, and it lacks explicit SLM/SLM-related keywords in the title/abstract, focusing instead on unsupervised feature learning for speech rather than speech-language modeling architectures.",
      "evaluation": 2
    },
    "round-A_JuniorNano_reasoning": "Although it deals with speech representations for ASR, it is not centered on Spoken Language Models or full-duplex speech–language modeling, and it lacks explicit SLM/SLM-related keywords in the title/abstract, focusing instead on unsupervised feature learning for speech rather than speech-language modeling architectures.",
    "round-A_JuniorNano_evaluation": 2,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on unsupervised learning of speech features primarily targeting speech representation for downstream tasks like ASR. However, it does not explicitly address Spoken Language Models (SLMs) or Full-Duplex Spoken Language Models (FD-SLMs), nor does it discuss language modeling of speech or synchronization aspects relevant to full-duplex interaction, cognitive parallelism, or behavioral arbitration. The evaluation focuses on ABX score and ASR performance, missing considerations of real-time interaction or multi-dimensional evaluation as required. It also primarily treats speech as input features rather than modeling or reasoning directly on speech sequences as the core object. Thus, it does not meet the inclusion criteria and aligns with the exclusion criteria focusing on single speech technology rather than dialogue or interaction modeling.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on unsupervised learning of speech features primarily targeting speech representation for downstream tasks like ASR. However, it does not explicitly address Spoken Language Models (SLMs) or Full-Duplex Spoken Language Models (FD-SLMs), nor does it discuss language modeling of speech or synchronization aspects relevant to full-duplex interaction, cognitive parallelism, or behavioral arbitration. The evaluation focuses on ABX score and ASR performance, missing considerations of real-time interaction or multi-dimensional evaluation as required. It also primarily treats speech as input features rather than modeling or reasoning directly on speech sequences as the core object. Thus, it does not meet the inclusion criteria and aligns with the exclusion criteria focusing on single speech technology rather than dialogue or interaction modeling.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:2)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Speech-to-Speech Translation Between Untranscribed Unknown Languages",
    "abstract": "In this paper, we explore a method for training speech-to-speech translation tasks without any transcription or linguistic supervision. Our proposed method consists of two steps: First, we train and generate discrete representation with unsupervised term discovery with a discrete quantized autoencoder. Second, we train a sequence-to-sequence model that directly maps the source language speech to the target languages discrete representation. Our proposed method can directly generate target speech without any auxiliary or pre-training steps with a source or target transcription. To the best of our knowledge, this is the first work that performed pure speech-to-speech translation between untranscribed unknown languages.",
    "metadata": {
      "title": "Speech-to-Speech Translation Between Untranscribed Unknown Languages",
      "summary": "In this paper, we explore a method for training speech-to-speech translation tasks without any transcription or linguistic supervision. Our proposed method consists of two steps: First, we train and generate discrete representation with unsupervised term discovery with a discrete quantized autoencoder. Second, we train a sequence-to-sequence model that directly maps the source language speech to the target languages discrete representation. Our proposed method can directly generate target speech without any auxiliary or pre-training steps with a source or target transcription. To the best of our knowledge, this is the first work that performed pure speech-to-speech translation between untranscribed unknown languages.",
      "abstract": "In this paper, we explore a method for training speech-to-speech translation tasks without any transcription or linguistic supervision. Our proposed method consists of two steps: First, we train and generate discrete representation with unsupervised term discovery with a discrete quantized autoencoder. Second, we train a sequence-to-sequence model that directly maps the source language speech to the target languages discrete representation. Our proposed method can directly generate target speech without any auxiliary or pre-training steps with a source or target transcription. To the best of our knowledge, this is the first work that performed pure speech-to-speech translation between untranscribed unknown languages.",
      "doi": "https://doi.org/10.1109/asru46091.2019.9003853",
      "openalex_id": "https://openalex.org/W3007068036",
      "arxiv_id": "",
      "publication_date": "2019-12-01",
      "published": "2019-12-01",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The work targets end-to-end speech-to-speech translation without transcription, which aligns with speech-based modeling; however, it does not explicitly mention 'Spoken Language Models' or an equivalent term in the title/abstract, failing the stated inclusion criterion that the paper must explicitly cite SLM/SLM-equivalents in the title, abstract, or keywords.",
      "evaluation": 2
    },
    "round-A_JuniorNano_reasoning": "The work targets end-to-end speech-to-speech translation without transcription, which aligns with speech-based modeling; however, it does not explicitly mention 'Spoken Language Models' or an equivalent term in the title/abstract, failing the stated inclusion criterion that the paper must explicitly cite SLM/SLM-equivalents in the title, abstract, or keywords.",
    "round-A_JuniorNano_evaluation": 2,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on speech-to-speech translation between untranscribed unknown languages using discrete speech representations without linguistic supervision, but it does not explicitly mention or discuss 'Spoken Language Models' or equivalent concepts, full-duplex interaction, cognitive parallelism, or synchronization mechanisms, nor does it provide evaluation related to real-time interactive dialogue behaviors or multiparty interaction features as required by inclusion criteria. The main emphasis is on speech translation, not on spoken language modeling or full-duplex spoken language systems.",
      "evaluation": 2
    },
    "round-A_JuniorMini_reasoning": "The study focuses on speech-to-speech translation between untranscribed unknown languages using discrete speech representations without linguistic supervision, but it does not explicitly mention or discuss 'Spoken Language Models' or equivalent concepts, full-duplex interaction, cognitive parallelism, or synchronization mechanisms, nor does it provide evaluation related to real-time interactive dialogue behaviors or multiparty interaction features as required by inclusion criteria. The main emphasis is on speech translation, not on spoken language modeling or full-duplex spoken language systems.",
    "round-A_JuniorMini_evaluation": 2,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:2)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Direct Speech-to-Speech Translation with a Sequence-to-Sequence Model",
    "abstract": "We present an attention-based sequence-to-sequence neural network which can directly translate speech from one language into speech in another language, without relying on an intermediate text representation.The network is trained end-to-end, learning to map speech spectrograms into target spectrograms in another language, corresponding to the translated content (in a different canonical voice).We further demonstrate the ability to synthesize translated speech using the voice of the source speaker.We conduct experiments on two Spanish-to-English speech translation datasets, and find that the proposed model slightly underperforms a baseline cascade of a direct speech-to-text translation model and a text-to-speech synthesis model, demonstrating the feasibility of the approach on this very challenging task.",
    "metadata": {
      "title": "Direct Speech-to-Speech Translation with a Sequence-to-Sequence Model",
      "summary": "We present an attention-based sequence-to-sequence neural network which can directly translate speech from one language into speech in another language, without relying on an intermediate text representation.The network is trained end-to-end, learning to map speech spectrograms into target spectrograms in another language, corresponding to the translated content (in a different canonical voice).We further demonstrate the ability to synthesize translated speech using the voice of the source speaker.We conduct experiments on two Spanish-to-English speech translation datasets, and find that the proposed model slightly underperforms a baseline cascade of a direct speech-to-text translation model and a text-to-speech synthesis model, demonstrating the feasibility of the approach on this very challenging task.",
      "abstract": "We present an attention-based sequence-to-sequence neural network which can directly translate speech from one language into speech in another language, without relying on an intermediate text representation.The network is trained end-to-end, learning to map speech spectrograms into target spectrograms in another language, corresponding to the translated content (in a different canonical voice).We further demonstrate the ability to synthesize translated speech using the voice of the source speaker.We conduct experiments on two Spanish-to-English speech translation datasets, and find that the proposed model slightly underperforms a baseline cascade of a direct speech-to-text translation model and a text-to-speech synthesis model, demonstrating the feasibility of the approach on this very challenging task.",
      "doi": "https://doi.org/10.21437/interspeech.2019-1951",
      "openalex_id": "https://openalex.org/W2972495969",
      "arxiv_id": "",
      "publication_date": "2019-09-13",
      "published": "2019-09-13",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "Although the work presents an end-to-end direct speech-to-speech translation model, it does not explicitly frame itself around Spoken Language Models or an equivalent concept in the title/abstract/keywords, failing the specified inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "Although the work presents an end-to-end direct speech-to-speech translation model, it does not explicitly frame itself around Spoken Language Models or an equivalent concept in the title/abstract/keywords, failing the specified inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study presents an attention-based sequence-to-sequence model that directly translates speech from one language to another without intermediate text, focusing on speech spectrograms as core modeling units. However, while it involves end-to-end speech-to-speech translation (which aligns with modeling speech sequences directly), the paper does not mention spoken language models (SLMs), full-duplex spoken language models (FD-SLMs), or related concepts explicitly in the title or abstract. It also lacks discussion of synchronous interaction capabilities such as turn-taking, cognitive parallelism, speech overlap management, or low-latency interaction. The evaluation metrics appear limited to translation performance compared to a baseline, without any assessment of real-time interaction or dialogue behavior arbitration. The research is relevant to speech model architectures but does not fulfil the requirement of explicitly addressing spoken language modeling or full-duplex interactive capabilities as specified in the inclusion criteria, nor does it involve behavior arbitration or multi-dimensional evaluation as per the exclusion criteria. Therefore, it slightly falls short of the full inclusion criteria and partially overlaps with exclusion criteria (lack of synchronous interaction and relevant evaluation).",
      "evaluation": 2
    },
    "round-A_JuniorMini_reasoning": "The study presents an attention-based sequence-to-sequence model that directly translates speech from one language to another without intermediate text, focusing on speech spectrograms as core modeling units. However, while it involves end-to-end speech-to-speech translation (which aligns with modeling speech sequences directly), the paper does not mention spoken language models (SLMs), full-duplex spoken language models (FD-SLMs), or related concepts explicitly in the title or abstract. It also lacks discussion of synchronous interaction capabilities such as turn-taking, cognitive parallelism, speech overlap management, or low-latency interaction. The evaluation metrics appear limited to translation performance compared to a baseline, without any assessment of real-time interaction or dialogue behavior arbitration. The research is relevant to speech model architectures but does not fulfil the requirement of explicitly addressing spoken language modeling or full-duplex interactive capabilities as specified in the inclusion criteria, nor does it involve behavior arbitration or multi-dimensional evaluation as per the exclusion criteria. Therefore, it slightly falls short of the full inclusion criteria and partially overlaps with exclusion criteria (lack of synchronous interaction and relevant evaluation).",
    "round-A_JuniorMini_evaluation": 2,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:2)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Direct Speech-to-Speech Translation With Discrete Units",
    "abstract": "Ann Lee, Peng-Jen Chen, Changhan Wang, Jiatao Gu, Sravya Popuri, Xutai Ma, Adam Polyak, Yossi Adi, Qing He, Yun Tang, Juan Pino, Wei-Ning Hsu. Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). 2022.",
    "metadata": {
      "title": "Direct Speech-to-Speech Translation With Discrete Units",
      "summary": "Ann Lee, Peng-Jen Chen, Changhan Wang, Jiatao Gu, Sravya Popuri, Xutai Ma, Adam Polyak, Yossi Adi, Qing He, Yun Tang, Juan Pino, Wei-Ning Hsu. Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). 2022.",
      "abstract": "Ann Lee, Peng-Jen Chen, Changhan Wang, Jiatao Gu, Sravya Popuri, Xutai Ma, Adam Polyak, Yossi Adi, Qing He, Yun Tang, Juan Pino, Wei-Ning Hsu. Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). 2022.",
      "doi": "https://doi.org/10.18653/v1/2022.acl-long.235",
      "openalex_id": "https://openalex.org/W3180374548",
      "arxiv_id": "",
      "publication_date": "2022-01-01",
      "published": "2022-01-01",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The paper presents an end-to-end speech-to-speech translation approach using discrete units, treating speech as the core modeling target (not just text processing), within the required time window, and it includes architecture/methods and evaluation; while it may not explicitly address full-duplex interactive dialogue or conversational arbitration, it aligns with the speech-centric modeling criteria more than token/text-only baselines.",
      "evaluation": 4
    },
    "round-A_JuniorNano_reasoning": "The paper presents an end-to-end speech-to-speech translation approach using discrete units, treating speech as the core modeling target (not just text processing), within the required time window, and it includes architecture/methods and evaluation; while it may not explicitly address full-duplex interactive dialogue or conversational arbitration, it aligns with the speech-centric modeling criteria more than token/text-only baselines.",
    "round-A_JuniorNano_evaluation": 4,
    "round-A_JuniorMini_output": {
      "reasoning": "The title 'Direct Speech-to-Speech Translation With Discrete Units' suggests a study focusing on speech-to-speech translation using discrete units, which aligns with speech modeling. However, the abstract only provides citation details without sufficient information on whether the study centers on Spoken Language Models, full-duplex interaction, model architecture, or evaluation beyond standard speech tasks like ASR or TTS. There is no explicit mention of spoken language models or related key concepts in the title or abstract, nor evidence of addressing full-duplex systems or interaction behaviors. The lack of detailed abstract prevents confident inclusion per the criteria.",
      "evaluation": 3
    },
    "round-A_JuniorMini_reasoning": "The title 'Direct Speech-to-Speech Translation With Discrete Units' suggests a study focusing on speech-to-speech translation using discrete units, which aligns with speech modeling. However, the abstract only provides citation details without sufficient information on whether the study centers on Spoken Language Models, full-duplex interaction, model architecture, or evaluation beyond standard speech tasks like ASR or TTS. There is no explicit mention of spoken language models or related key concepts in the title or abstract, nor evidence of addressing full-duplex systems or interaction behaviors. The lack of detailed abstract prevents confident inclusion per the criteria.",
    "round-A_JuniorMini_evaluation": 3,
    "round-B_SeniorLead_output": {
      "reasoning": "Both junior reviewers noted this is a speech-focused paper (speech-to-speech translation using discrete units) but the citation-only abstract provides no evidence the work frames itself as a Spoken Language Model (SLM) or addresses full‑duplex interaction/behavior arbitration; moreover the paper is from ACL 2022 (outside the past-3-years window) and the title/abstract do not contain the required SLM/spoken‑language modeling keywords—so it fails the explicit-content and recency inclusion criteria. Therefore I concur with the cautious assessments of the juniors and recommend exclusion.",
      "evaluation": 1
    },
    "round-B_SeniorLead_reasoning": "Both junior reviewers noted this is a speech-focused paper (speech-to-speech translation using discrete units) but the citation-only abstract provides no evidence the work frames itself as a Spoken Language Model (SLM) or addresses full‑duplex interaction/behavior arbitration; moreover the paper is from ACL 2022 (outside the past-3-years window) and the title/abstract do not contain the required SLM/spoken‑language modeling keywords—so it fails the explicit-content and recency inclusion criteria. Therefore I concur with the cautious assessments of the juniors and recommend exclusion.",
    "round-B_SeniorLead_evaluation": 1,
    "final_verdict": "exclude (senior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Textless Speech-to-Speech Translation on Real Data",
    "abstract": "Ann Lee, Hongyu Gong, Paul-Ambroise Duquenne, Holger Schwenk, Peng-Jen Chen, Changhan Wang, Sravya Popuri, Yossi Adi, Juan Pino, Jiatao Gu, Wei-Ning Hsu. Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. 2022.",
    "metadata": {
      "title": "Textless Speech-to-Speech Translation on Real Data",
      "summary": "Ann Lee, Hongyu Gong, Paul-Ambroise Duquenne, Holger Schwenk, Peng-Jen Chen, Changhan Wang, Sravya Popuri, Yossi Adi, Juan Pino, Jiatao Gu, Wei-Ning Hsu. Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. 2022.",
      "abstract": "Ann Lee, Hongyu Gong, Paul-Ambroise Duquenne, Holger Schwenk, Peng-Jen Chen, Changhan Wang, Sravya Popuri, Yossi Adi, Juan Pino, Jiatao Gu, Wei-Ning Hsu. Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. 2022.",
      "doi": "https://doi.org/10.18653/v1/2022.naacl-main.63",
      "openalex_id": "https://openalex.org/W4287854499",
      "arxiv_id": "",
      "publication_date": "2022-01-01",
      "published": "2022-01-01",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The inclusion criteria require explicit focus on Spoken Language Models or an equivalent concept with speech as the core modeling target, plus discussion of architecture/training/evaluation and mention of SLMs in the title/abstract/keywords. The item provided is a 2022 textless speech-to-speech translation paper; based on the brief title/abstract available, there is no clear indication of SLM-centric modeling or explicit reference to spoken language models in the abstract, so it likely does not meet the core inclusion criteria despite being within the time window.",
      "evaluation": 2
    },
    "round-A_JuniorNano_reasoning": "The inclusion criteria require explicit focus on Spoken Language Models or an equivalent concept with speech as the core modeling target, plus discussion of architecture/training/evaluation and mention of SLMs in the title/abstract/keywords. The item provided is a 2022 textless speech-to-speech translation paper; based on the brief title/abstract available, there is no clear indication of SLM-centric modeling or explicit reference to spoken language models in the abstract, so it likely does not meet the core inclusion criteria despite being within the time window.",
    "round-A_JuniorNano_evaluation": 2,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper titled 'Textless Speech-to-Speech Translation on Real Data' appears to focus on speech-to-speech translation involving speech signals, but the abstract provides no explicit mention of spoken language models or related terms such as SLM or FD-SLM, nor does it describe model architecture, synchronization, or evaluation of interactive features like cognitive parallelism or low-latency full-duplex capabilities important in the inclusion criteria. There's also no indication that it engages beyond traditional speech recognition or synthesis tasks or discusses multi-dimensional evaluation beyond typical ASR/TTS metrics, which aligns with several exclusion criteria. Furthermore, although the publication is recent, without explicit mentioning of spoken language models or their evaluation, it doesn't clearly meet the inclusion criteria.",
      "evaluation": 2
    },
    "round-A_JuniorMini_reasoning": "The paper titled 'Textless Speech-to-Speech Translation on Real Data' appears to focus on speech-to-speech translation involving speech signals, but the abstract provides no explicit mention of spoken language models or related terms such as SLM or FD-SLM, nor does it describe model architecture, synchronization, or evaluation of interactive features like cognitive parallelism or low-latency full-duplex capabilities important in the inclusion criteria. There's also no indication that it engages beyond traditional speech recognition or synthesis tasks or discusses multi-dimensional evaluation beyond typical ASR/TTS metrics, which aligns with several exclusion criteria. Furthermore, although the publication is recent, without explicit mentioning of spoken language models or their evaluation, it doesn't clearly meet the inclusion criteria.",
    "round-A_JuniorMini_evaluation": 2,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:2)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "The Zero Resource Speech Benchmark 2021: Metrics and baselines for unsupervised spoken language modeling",
    "abstract": "We introduce a new unsupervised task, spoken language modeling: the learning of linguistic representations from raw audio signals without any labels, along with the Zero Resource Speech Benchmark 2021: a suite of 4 black-box, zero-shot metrics probing for the quality of the learned models at 4 linguistic levels: phonetics, lexicon, syntax and semantics. We present the results and analyses of a composite baseline made of the concatenation of three unsupervised systems: self-supervised contrastive representation learning (CPC), clustering (k-means) and language modeling (LSTM or BERT). The language models learn on the basis of the pseudo-text derived from clustering the learned representations. This simple pipeline shows better than chance performance on all four metrics, demonstrating the feasibility of spoken language modeling from raw speech. It also yields worse performance compared to text-based 'topline' systems trained on the same data, delineating the space to be explored by more sophisticated end-to-end models.",
    "metadata": {
      "title": "The Zero Resource Speech Benchmark 2021: Metrics and baselines for unsupervised spoken language modeling",
      "summary": "We introduce a new unsupervised task, spoken language modeling: the learning of linguistic representations from raw audio signals without any labels, along with the Zero Resource Speech Benchmark 2021: a suite of 4 black-box, zero-shot metrics probing for the quality of the learned models at 4 linguistic levels: phonetics, lexicon, syntax and semantics. We present the results and analyses of a composite baseline made of the concatenation of three unsupervised systems: self-supervised contrastive representation learning (CPC), clustering (k-means) and language modeling (LSTM or BERT). The language models learn on the basis of the pseudo-text derived from clustering the learned representations. This simple pipeline shows better than chance performance on all four metrics, demonstrating the feasibility of spoken language modeling from raw speech. It also yields worse performance compared to text-based 'topline' systems trained on the same data, delineating the space to be explored by more sophisticated end-to-end models.",
      "abstract": "We introduce a new unsupervised task, spoken language modeling: the learning of linguistic representations from raw audio signals without any labels, along with the Zero Resource Speech Benchmark 2021: a suite of 4 black-box, zero-shot metrics probing for the quality of the learned models at 4 linguistic levels: phonetics, lexicon, syntax and semantics. We present the results and analyses of a composite baseline made of the concatenation of three unsupervised systems: self-supervised contrastive representation learning (CPC), clustering (k-means) and language modeling (LSTM or BERT). The language models learn on the basis of the pseudo-text derived from clustering the learned representations. This simple pipeline shows better than chance performance on all four metrics, demonstrating the feasibility of spoken language modeling from raw speech. It also yields worse performance compared to text-based 'topline' systems trained on the same data, delineating the space to be explored by more sophisticated end-to-end models.",
      "doi": "https://doi.org/10.48550/arxiv.2011.11588",
      "openalex_id": "https://openalex.org/W4287591426",
      "arxiv_id": "",
      "publication_date": "2020-11-23",
      "published": "2020-11-23",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "Although the work centers on unsupervised spoken language modeling (SLMs) and proposes metrics for evaluation, the key publication year is 2021, which falls outside the required past-3-years window.",
      "evaluation": 2
    },
    "round-A_JuniorNano_reasoning": "Although the work centers on unsupervised spoken language modeling (SLMs) and proposes metrics for evaluation, the key publication year is 2021, which falls outside the required past-3-years window.",
    "round-A_JuniorNano_evaluation": 2,
    "round-A_JuniorMini_output": {
      "reasoning": "The study centers on unsupervised spoken language modeling from raw audio signals, explicitly addressing language modeling of speech and introducing benchmarks for evaluation across multiple linguistic levels, aligning well with the inclusion criteria focusing on spoken language models as the core modeling object. However, the abstract does not mention synchronous or full-duplex interaction capabilities, cognitive parallelism, or low-latency interactive features, which are important aspects in the inclusion criteria. The work focuses on proposing models and evaluation metrics for speech language modeling rather than addressing behavior arbitration or full-duplex interaction, and thus it misses some critical elements required for inclusion but does focus deeply on speech language modeling itself, so the study partially meets but not fully meets inclusion requirements.",
      "evaluation": 3
    },
    "round-A_JuniorMini_reasoning": "The study centers on unsupervised spoken language modeling from raw audio signals, explicitly addressing language modeling of speech and introducing benchmarks for evaluation across multiple linguistic levels, aligning well with the inclusion criteria focusing on spoken language models as the core modeling object. However, the abstract does not mention synchronous or full-duplex interaction capabilities, cognitive parallelism, or low-latency interactive features, which are important aspects in the inclusion criteria. The work focuses on proposing models and evaluation metrics for speech language modeling rather than addressing behavior arbitration or full-duplex interaction, and thus it misses some critical elements required for inclusion but does focus deeply on speech language modeling itself, so the study partially meets but not fully meets inclusion requirements.",
    "round-A_JuniorMini_evaluation": 3,
    "round-B_SeniorLead_output": {
      "reasoning": "The paper clearly focuses on spoken language modeling (the abstract explicitly uses “spoken language modeling” and proposes benchmarks and baselines), so it meets the topical inclusion criteria; however, it was published in 2021 and therefore falls outside the required past-3-years window, and JuniorReviewer A flagged this date exclusion while JuniorReviewer B noted it otherwise aligns topically but lacks full‑duplex/real‑time interaction aspects. One-sentence explanation: Exclude — topically relevant but disqualified by being published in 2021 (outside the required 3-year window).",
      "evaluation": 2
    },
    "round-B_SeniorLead_reasoning": "The paper clearly focuses on spoken language modeling (the abstract explicitly uses “spoken language modeling” and proposes benchmarks and baselines), so it meets the topical inclusion criteria; however, it was published in 2021 and therefore falls outside the required past-3-years window, and JuniorReviewer A flagged this date exclusion while JuniorReviewer B noted it otherwise aligns topically but lacks full‑duplex/real‑time interaction aspects. One-sentence explanation: Exclude — topically relevant but disqualified by being published in 2021 (outside the required 3-year window).",
    "round-B_SeniorLead_evaluation": 2,
    "final_verdict": "exclude (senior:2)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Masked Autoencoders that Listen",
    "abstract": "This paper studies a simple extension of image-based Masked Autoencoders (MAE) to self-supervised representation learning from audio spectrograms. Following the Transformer encoder-decoder design in MAE, our Audio-MAE first encodes audio spectrogram patches with a high masking ratio, feeding only the non-masked tokens through encoder layers. The decoder then re-orders and decodes the encoded context padded with mask tokens, in order to reconstruct the input spectrogram. We find it beneficial to incorporate local window attention in the decoder, as audio spectrograms are highly correlated in local time and frequency bands. We then fine-tune the encoder with a lower masking ratio on target datasets. Empirically, Audio-MAE sets new state-of-the-art performance on six audio and speech classification tasks, outperforming other recent models that use external supervised pre-training. The code and models will be at https://github.com/facebookresearch/AudioMAE.",
    "metadata": {
      "title": "Masked Autoencoders that Listen",
      "summary": "This paper studies a simple extension of image-based Masked Autoencoders (MAE) to self-supervised representation learning from audio spectrograms. Following the Transformer encoder-decoder design in MAE, our Audio-MAE first encodes audio spectrogram patches with a high masking ratio, feeding only the non-masked tokens through encoder layers. The decoder then re-orders and decodes the encoded context padded with mask tokens, in order to reconstruct the input spectrogram. We find it beneficial to incorporate local window attention in the decoder, as audio spectrograms are highly correlated in local time and frequency bands. We then fine-tune the encoder with a lower masking ratio on target datasets. Empirically, Audio-MAE sets new state-of-the-art performance on six audio and speech classification tasks, outperforming other recent models that use external supervised pre-training. The code and models will be at https://github.com/facebookresearch/AudioMAE.",
      "abstract": "This paper studies a simple extension of image-based Masked Autoencoders (MAE) to self-supervised representation learning from audio spectrograms. Following the Transformer encoder-decoder design in MAE, our Audio-MAE first encodes audio spectrogram patches with a high masking ratio, feeding only the non-masked tokens through encoder layers. The decoder then re-orders and decodes the encoded context padded with mask tokens, in order to reconstruct the input spectrogram. We find it beneficial to incorporate local window attention in the decoder, as audio spectrograms are highly correlated in local time and frequency bands. We then fine-tune the encoder with a lower masking ratio on target datasets. Empirically, Audio-MAE sets new state-of-the-art performance on six audio and speech classification tasks, outperforming other recent models that use external supervised pre-training. The code and models will be at https://github.com/facebookresearch/AudioMAE.",
      "doi": "https://doi.org/10.48550/arxiv.2207.06405",
      "openalex_id": "https://openalex.org/W4285483774",
      "arxiv_id": "",
      "publication_date": "2022-07-13",
      "published": "2022-07-13",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "雖然為近年音頻表徵自我監督學習的研究，但摘要未以 Spoken Language Models 或等價概念為核心，亦未涉及語音-語言建模的端到端/混合架構、同時互動等議題，因此不符合納入條件且應排除。",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "雖然為近年音頻表徵自我監督學習的研究，但摘要未以 Spoken Language Models 或等價概念為核心，亦未涉及語音-語言建模的端到端/混合架構、同時互動等議題，因此不符合納入條件且應排除。",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper focuses on self-supervised learning of audio spectrogram representations using masked autoencoders, which is primarily about acoustic feature encoding and classification tasks, without explicit mention or focus on Spoken Language Models, speech language modeling architecture, synchronous dialog interaction, or full-duplex spoken language modeling as required by the inclusion criteria. It does not address full-duplex interaction, cognitive parallelism, low-latency interactive behavior, or behavioral arbitration, nor does it discuss evaluation metrics related to real-time dialog or synchronization. It is more about general audio representation and classification rather than modeling or reasoning directly with spoken language sequences or full-duplex spoken language models, and no direct mention of the required keywords such as \"spoken language model\" or \"speech language model\" is present in the title or abstract. Hence, it does not meet the main inclusion criteria and fits exclusion criteria on focusing solely on acoustic representation without interaction or spoken language modeling aspects.",
      "evaluation": 2
    },
    "round-A_JuniorMini_reasoning": "The paper focuses on self-supervised learning of audio spectrogram representations using masked autoencoders, which is primarily about acoustic feature encoding and classification tasks, without explicit mention or focus on Spoken Language Models, speech language modeling architecture, synchronous dialog interaction, or full-duplex spoken language modeling as required by the inclusion criteria. It does not address full-duplex interaction, cognitive parallelism, low-latency interactive behavior, or behavioral arbitration, nor does it discuss evaluation metrics related to real-time dialog or synchronization. It is more about general audio representation and classification rather than modeling or reasoning directly with spoken language sequences or full-duplex spoken language models, and no direct mention of the required keywords such as \"spoken language model\" or \"speech language model\" is present in the title or abstract. Hence, it does not meet the main inclusion criteria and fits exclusion criteria on focusing solely on acoustic representation without interaction or spoken language modeling aspects.",
    "round-A_JuniorMini_evaluation": 2,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:2)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Introduction to the Psychology of Hearing",
    "abstract": "The author's stated general approach is to relate the psychological and perceptual aspects of sound to the underlying physiological mechanisms of hearing in a way that the material can be used as a text to accompany an advanced undergraduate- or graduate-level course in auditory perception. The attempt is to provide an account of current trends in auditory research on a level not too technical for the novice. Psychoacoustic studies on humans and physiological studies on animals serve as the primary bases for subject matter presentation, and many practical applications are offered. Among the chapters are the following: the nature of sound and the structure of the auditory system; loudness, adaptation, and fatigue; frequency analysis, masking, and critical bands; pitch perception and auditory pattern perception; space perception; and speech perception. Within these chapter headings special attention is given to a number of topics, including signal detection theory, monaural and binaural hearing,",
    "metadata": {
      "title": "Introduction to the Psychology of Hearing",
      "summary": "The author's stated general approach is to relate the psychological and perceptual aspects of sound to the underlying physiological mechanisms of hearing in a way that the material can be used as a text to accompany an advanced undergraduate- or graduate-level course in auditory perception. The attempt is to provide an account of current trends in auditory research on a level not too technical for the novice. Psychoacoustic studies on humans and physiological studies on animals serve as the primary bases for subject matter presentation, and many practical applications are offered. Among the chapters are the following: the nature of sound and the structure of the auditory system; loudness, adaptation, and fatigue; frequency analysis, masking, and critical bands; pitch perception and auditory pattern perception; space perception; and speech perception. Within these chapter headings special attention is given to a number of topics, including signal detection theory, monaural and binaural hearing,",
      "abstract": "The author's stated general approach is to relate the psychological and perceptual aspects of sound to the underlying physiological mechanisms of hearing in a way that the material can be used as a text to accompany an advanced undergraduate- or graduate-level course in auditory perception. The attempt is to provide an account of current trends in auditory research on a level not too technical for the novice. Psychoacoustic studies on humans and physiological studies on animals serve as the primary bases for subject matter presentation, and many practical applications are offered. Among the chapters are the following: the nature of sound and the structure of the auditory system; loudness, adaptation, and fatigue; frequency analysis, masking, and critical bands; pitch perception and auditory pattern perception; space perception; and speech perception. Within these chapter headings special attention is given to a number of topics, including signal detection theory, monaural and binaural hearing,",
      "doi": "https://doi.org/10.1001/archotol.1977.00780290081022",
      "openalex_id": "https://openalex.org/W1973746598",
      "arxiv_id": "",
      "publication_date": "1977-12-01",
      "published": "1977-12-01",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item is about the psychology and psychoacoustics of hearing, not about Spoken Language Models or speech-language modeling architectures, training, or evaluation, thus it does not meet the specified inclusion criteria and does not trigger any exclusion issues beyond irrelevance.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item is about the psychology and psychoacoustics of hearing, not about Spoken Language Models or speech-language modeling architectures, training, or evaluation, thus it does not meet the specified inclusion criteria and does not trigger any exclusion issues beyond irrelevance.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The title and abstract focus on the psychology of hearing, auditory perception, and physiological mechanisms rather than on Spoken Language Models (SLMs) or full-duplex spoken language modeling architectures, training, or evaluation as required by the inclusion criteria; there is no mention of speech language modeling, full-duplex interaction, or related evaluation metrics, therefore it does not meet the inclusion criteria and fits exclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The title and abstract focus on the psychology of hearing, auditory perception, and physiological mechanisms rather than on Spoken Language Models (SLMs) or full-duplex spoken language modeling architectures, training, or evaluation as required by the inclusion criteria; there is no mention of speech language modeling, full-duplex interaction, or related evaluation metrics, therefore it does not meet the inclusion criteria and fits exclusion criteria.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Translatotron 2: Robust direct speech-to-speech translation.",
    "abstract": "We present Translatotron 2, a neural direct speech-to-speech translation model that can be trained end-to-end. Translatotron 2 consists of a speech encoder, a phoneme decoder, a mel-spectrogram synthesizer, and an attention module that connects all the previous three components. Experimental results suggest that Translatotron 2 outperforms the original Translatotron by a large margin in terms of translation quality and predicted speech naturalness, and drastically improves the robustness of the predicted speech by mitigating over-generation, such as babbling or long pause. We also propose a new method for retaining the source speaker's voice in the translated speech. The trained model is restricted to retain the source speaker's voice, and unlike the original Translatotron, it is not able to generate speech in a different speaker's voice, making the model more robust for production deployment, by mitigating potential misuse for creating spoofing audio artifacts. When the new method is used together with a simple concatenation-based data augmentation, the trained Translatotron 2 model is able to retain each speaker's voice for input with speaker turns.",
    "metadata": {
      "title": "Translatotron 2: Robust direct speech-to-speech translation.",
      "summary": "We present Translatotron 2, a neural direct speech-to-speech translation model that can be trained end-to-end. Translatotron 2 consists of a speech encoder, a phoneme decoder, a mel-spectrogram synthesizer, and an attention module that connects all the previous three components. Experimental results suggest that Translatotron 2 outperforms the original Translatotron by a large margin in terms of translation quality and predicted speech naturalness, and drastically improves the robustness of the predicted speech by mitigating over-generation, such as babbling or long pause. We also propose a new method for retaining the source speaker's voice in the translated speech. The trained model is restricted to retain the source speaker's voice, and unlike the original Translatotron, it is not able to generate speech in a different speaker's voice, making the model more robust for production deployment, by mitigating potential misuse for creating spoofing audio artifacts. When the new method is used together with a simple concatenation-based data augmentation, the trained Translatotron 2 model is able to retain each speaker's voice for input with speaker turns.",
      "abstract": "We present Translatotron 2, a neural direct speech-to-speech translation model that can be trained end-to-end. Translatotron 2 consists of a speech encoder, a phoneme decoder, a mel-spectrogram synthesizer, and an attention module that connects all the previous three components. Experimental results suggest that Translatotron 2 outperforms the original Translatotron by a large margin in terms of translation quality and predicted speech naturalness, and drastically improves the robustness of the predicted speech by mitigating over-generation, such as babbling or long pause. We also propose a new method for retaining the source speaker's voice in the translated speech. The trained model is restricted to retain the source speaker's voice, and unlike the original Translatotron, it is not able to generate speech in a different speaker's voice, making the model more robust for production deployment, by mitigating potential misuse for creating spoofing audio artifacts. When the new method is used together with a simple concatenation-based data augmentation, the trained Translatotron 2 model is able to retain each speaker's voice for input with speaker turns.",
      "doi": "",
      "openalex_id": "https://openalex.org/W3186843219",
      "arxiv_id": "",
      "publication_date": "2021-07-19",
      "published": "2021-07-19",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This paper presents an end-to-end direct speech-to-speech translation model (Translatotron 2), treating speech as the core modeling unit and aligning with Spoken Language Models/FD-SLM concepts, within the past 3 years, and it covers model architecture and cross-language speech output rather than being limited to ASR/TTS.",
      "evaluation": 4
    },
    "round-A_JuniorNano_reasoning": "This paper presents an end-to-end direct speech-to-speech translation model (Translatotron 2), treating speech as the core modeling unit and aligning with Spoken Language Models/FD-SLM concepts, within the past 3 years, and it covers model architecture and cross-language speech output rather than being limited to ASR/TTS.",
    "round-A_JuniorNano_evaluation": 4,
    "round-A_JuniorMini_output": {
      "reasoning": "The study presents a speech-to-speech translation model focusing on translation quality, speech naturalness, speaker voice retention, and robustness against spoofing. However, it does not explicitly discuss Spoken Language Models (SLMs) or Full-Duplex Spoken Language Models (FD-SLMs), nor does it address synchronous interaction capabilities like turn-taking, overlap management, low-latency interaction, or behavioral arbitration. The focus is on speech synthesis and translation rather than spoken language modeling or real-time duplex interaction, which are key inclusion criteria. Therefore, it does not fully meet the inclusion requirements nor violates direct exclusion criteria but lacks sufficient alignment with the research focus specified.",
      "evaluation": 2
    },
    "round-A_JuniorMini_reasoning": "The study presents a speech-to-speech translation model focusing on translation quality, speech naturalness, speaker voice retention, and robustness against spoofing. However, it does not explicitly discuss Spoken Language Models (SLMs) or Full-Duplex Spoken Language Models (FD-SLMs), nor does it address synchronous interaction capabilities like turn-taking, overlap management, low-latency interaction, or behavioral arbitration. The focus is on speech synthesis and translation rather than spoken language modeling or real-time duplex interaction, which are key inclusion criteria. Therefore, it does not fully meet the inclusion requirements nor violates direct exclusion criteria but lacks sufficient alignment with the research focus specified.",
    "round-A_JuniorMini_evaluation": 2,
    "round-B_SeniorLead_output": {
      "reasoning": "Both junior reviews considered: JuniorNano noted it’s an end-to-end speech-to-speech model and saw some alignment with speech-centric modeling, while JuniorMini correctly pointed out the abstract contains no mention of Spoken Language Models (SLMs) or Full-Duplex SLMs nor any discussion of synchronous/real-time interaction, turn-taking, overlap management, or related evaluations. The paper focuses on direct speech-to-speech translation, synthesis quality, speaker retention, and robustness (ASR/TTS/S2S task improvements) rather than on speech-level language modeling or full-duplex interactive behaviors required by the inclusion criteria. 1-sentence explanation: Exclude (score=1) because the abstract does not meet the explicit inclusion requirement of centering on Spoken Language Models / speech language modeling or on FD-SLM synchronous interaction and instead addresses S2S translation/TTS concerns.",
      "evaluation": 1
    },
    "round-B_SeniorLead_reasoning": "Both junior reviews considered: JuniorNano noted it’s an end-to-end speech-to-speech model and saw some alignment with speech-centric modeling, while JuniorMini correctly pointed out the abstract contains no mention of Spoken Language Models (SLMs) or Full-Duplex SLMs nor any discussion of synchronous/real-time interaction, turn-taking, overlap management, or related evaluations. The paper focuses on direct speech-to-speech translation, synthesis quality, speaker retention, and robustness (ASR/TTS/S2S task improvements) rather than on speech-level language modeling or full-duplex interactive behaviors required by the inclusion criteria. 1-sentence explanation: Exclude (score=1) because the abstract does not meet the explicit inclusion requirement of centering on Spoken Language Models / speech language modeling or on FD-SLM synchronous interaction and instead addresses S2S translation/TTS concerns.",
    "round-B_SeniorLead_evaluation": 1,
    "final_verdict": "exclude (senior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Representation Learning with Contrastive Predictive Coding",
    "abstract": "While supervised learning has enabled great progress in many applications, unsupervised learning has not seen such widespread adoption, and remains an important and challenging endeavor for artificial intelligence. In this work, we propose a universal unsupervised learning approach to extract useful representations from high-dimensional data, which we call Contrastive Predictive Coding. The key insight of our model is to learn such representations by predicting the future in latent space by using powerful autoregressive models. We use a probabilistic contrastive loss which induces the latent space to capture information that is maximally useful to predict future samples. It also makes the model tractable by using negative sampling. While most prior work has focused on evaluating representations for a particular modality, we demonstrate that our approach is able to learn useful representations achieving strong performance on four distinct domains: speech, images, text and reinforcement learning in 3D environments.",
    "metadata": {
      "title": "Representation Learning with Contrastive Predictive Coding",
      "summary": "While supervised learning has enabled great progress in many applications, unsupervised learning has not seen such widespread adoption, and remains an important and challenging endeavor for artificial intelligence. In this work, we propose a universal unsupervised learning approach to extract useful representations from high-dimensional data, which we call Contrastive Predictive Coding. The key insight of our model is to learn such representations by predicting the future in latent space by using powerful autoregressive models. We use a probabilistic contrastive loss which induces the latent space to capture information that is maximally useful to predict future samples. It also makes the model tractable by using negative sampling. While most prior work has focused on evaluating representations for a particular modality, we demonstrate that our approach is able to learn useful representations achieving strong performance on four distinct domains: speech, images, text and reinforcement learning in 3D environments.",
      "abstract": "While supervised learning has enabled great progress in many applications, unsupervised learning has not seen such widespread adoption, and remains an important and challenging endeavor for artificial intelligence. In this work, we propose a universal unsupervised learning approach to extract useful representations from high-dimensional data, which we call Contrastive Predictive Coding. The key insight of our model is to learn such representations by predicting the future in latent space by using powerful autoregressive models. We use a probabilistic contrastive loss which induces the latent space to capture information that is maximally useful to predict future samples. It also makes the model tractable by using negative sampling. While most prior work has focused on evaluating representations for a particular modality, we demonstrate that our approach is able to learn useful representations achieving strong performance on four distinct domains: speech, images, text and reinforcement learning in 3D environments.",
      "doi": "https://doi.org/10.48550/arxiv.1807.03748",
      "openalex_id": "https://openalex.org/W4297808394",
      "arxiv_id": "",
      "publication_date": "2018-07-10",
      "published": "2018-07-10",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "不是以 Spoken Language Models/語音語言模型為核心，且僅將語音作為多模態之一的通用表示學習方法，且該工作出版時間早於近三年，未聚焦於語音互動與實時對話等評估。",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "不是以 Spoken Language Models/語音語言模型為核心，且僅將語音作為多模態之一的通用表示學習方法，且該工作出版時間早於近三年，未聚焦於語音互動與實時對話等評估。",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study proposes a universal unsupervised learning approach for contrastive predictive coding applied to multiple domains including speech, but it does not specifically focus on Spoken Language Models or full-duplex speech interaction modeling; it lacks direct reference to speech language modeling as the core modeling object or discussion of full-duplex interaction, synchronization, or real-time speech interaction features required by the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study proposes a universal unsupervised learning approach for contrastive predictive coding applied to multiple domains including speech, but it does not specifically focus on Spoken Language Models or full-duplex speech interaction modeling; it lacks direct reference to speech language modeling as the core modeling object or discussion of full-duplex interaction, synchronization, or real-time speech interaction features required by the inclusion criteria.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Verb similarity on the taxonomy of WordNet",
    "abstract": "In this paper, we introduce two kinds of word similarity algorithms, SHE and RHE, to investigate the capability of WordNet in measuring verb similarity. In the absence of a standard verb set we have proposed two new verb similarity",
    "metadata": {
      "title": "Verb similarity on the taxonomy of WordNet",
      "summary": "In this paper, we introduce two kinds of word similarity algorithms, SHE and RHE, to investigate the capability of WordNet in measuring verb similarity. In the absence of a standard verb set we have proposed two new verb similarity",
      "abstract": "In this paper, we introduce two kinds of word similarity algorithms, SHE and RHE, to investigate the capability of WordNet in measuring verb similarity. In the absence of a standard verb set we have proposed two new verb similarity",
      "doi": "",
      "openalex_id": "https://openalex.org/W2132631284",
      "arxiv_id": "",
      "publication_date": "2006-01-01",
      "published": "2006-01-01",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This paper focuses on lexical verb similarity with WordNet and does not center Spoken Language Models or speech-based modeling, so it fails the inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "This paper focuses on lexical verb similarity with WordNet and does not center Spoken Language Models or speech-based modeling, so it fails the inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper focuses on word similarity algorithms related to WordNet verbs, which is a lexical semantic resource, without mentioning spoken language models, speech language models, or any speech/audio-centric modeling, architectures, training, or evaluation. It does not address full-duplex spoken language models, synchronization, or interactive aspects of speech communication as required by the inclusion criteria. Thus, it does not meet the inclusion criteria and falls into exclusion since it focuses on lexical semantics rather than speech modeling or spoken language interaction.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper focuses on word similarity algorithms related to WordNet verbs, which is a lexical semantic resource, without mentioning spoken language models, speech language models, or any speech/audio-centric modeling, architectures, training, or evaluation. It does not address full-duplex spoken language models, synchronization, or interactive aspects of speech communication as required by the inclusion criteria. Thus, it does not meet the inclusion criteria and falls into exclusion since it focuses on lexical semantics rather than speech modeling or spoken language interaction.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Self-Supervised Representation Learning for Speech Using Visual Grounding and Masked Language Modeling",
    "abstract": "In this paper, we describe our submissions to the ZeroSpeech 2021 Challenge and SUPERB benchmark. Our submissions are based on the recently proposed FaST-VGS model, which is a Transformer-based model that learns to associate raw speech waveforms with semantically related images, all without the use of any transcriptions of the speech. Additionally, we introduce a novel extension of this model, FaST-VGS+, which is learned in a multi-task fashion with a masked language modeling objective in addition to the visual grounding objective. On ZeroSpeech 2021, we show that our models perform competitively on the ABX task, outperform all other concurrent submissions on the Syntactic and Semantic tasks, and nearly match the best system on the Lexical task. On the SUPERB benchmark, we show that our models also achieve strong performance, in some cases even outperforming the popular wav2vec2.0 model.",
    "metadata": {
      "title": "Self-Supervised Representation Learning for Speech Using Visual Grounding and Masked Language Modeling",
      "summary": "In this paper, we describe our submissions to the ZeroSpeech 2021 Challenge and SUPERB benchmark. Our submissions are based on the recently proposed FaST-VGS model, which is a Transformer-based model that learns to associate raw speech waveforms with semantically related images, all without the use of any transcriptions of the speech. Additionally, we introduce a novel extension of this model, FaST-VGS+, which is learned in a multi-task fashion with a masked language modeling objective in addition to the visual grounding objective. On ZeroSpeech 2021, we show that our models perform competitively on the ABX task, outperform all other concurrent submissions on the Syntactic and Semantic tasks, and nearly match the best system on the Lexical task. On the SUPERB benchmark, we show that our models also achieve strong performance, in some cases even outperforming the popular wav2vec2.0 model.",
      "abstract": "In this paper, we describe our submissions to the ZeroSpeech 2021 Challenge and SUPERB benchmark. Our submissions are based on the recently proposed FaST-VGS model, which is a Transformer-based model that learns to associate raw speech waveforms with semantically related images, all without the use of any transcriptions of the speech. Additionally, we introduce a novel extension of this model, FaST-VGS+, which is learned in a multi-task fashion with a masked language modeling objective in addition to the visual grounding objective. On ZeroSpeech 2021, we show that our models perform competitively on the ABX task, outperform all other concurrent submissions on the Syntactic and Semantic tasks, and nearly match the best system on the Lexical task. On the SUPERB benchmark, we show that our models also achieve strong performance, in some cases even outperforming the popular wav2vec2.0 model.",
      "doi": "https://doi.org/10.48550/arxiv.2202.03543",
      "openalex_id": "https://openalex.org/W4221161768",
      "arxiv_id": "",
      "publication_date": "2022-02-07",
      "published": "2022-02-07",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This work centers on self-supervised speech representation learning (speech as the modeling target) with visual grounding and masked language modeling, discusses architecture, training, and evaluation (ZeroSpeech/SUPERB) and is within the last 3 years, thus meeting all inclusion criteria and none of the exclusions.",
      "evaluation": 5
    },
    "round-A_JuniorNano_reasoning": "This work centers on self-supervised speech representation learning (speech as the modeling target) with visual grounding and masked language modeling, discusses architecture, training, and evaluation (ZeroSpeech/SUPERB) and is within the last 3 years, thus meeting all inclusion criteria and none of the exclusions.",
    "round-A_JuniorNano_evaluation": 5,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on self-supervised representation learning of speech using visual grounding and masked language modeling, with a Transformer-based model associating raw speech waveforms and semantic images, without transcriptions. However, the abstract does not explicitly mention 'spoken language models' or equivalent terminology, nor does it emphasize synchronous turn-taking, full-duplex interaction, or behavioral arbitration which are central to the inclusion criteria. The benchmarks used (ZeroSpeech and SUPERB) primarily evaluate speech representation learning and recognition accuracy, without mention of low-latency interaction, interruption handling, or backchanneling. Thus, while it advances speech representation learning, it lacks direct evidence of addressing full-duplex spoken language modeling or the specified multi-dimensional behavioral and temporal evaluations required by the inclusion criteria.",
      "evaluation": 2
    },
    "round-A_JuniorMini_reasoning": "The study focuses on self-supervised representation learning of speech using visual grounding and masked language modeling, with a Transformer-based model associating raw speech waveforms and semantic images, without transcriptions. However, the abstract does not explicitly mention 'spoken language models' or equivalent terminology, nor does it emphasize synchronous turn-taking, full-duplex interaction, or behavioral arbitration which are central to the inclusion criteria. The benchmarks used (ZeroSpeech and SUPERB) primarily evaluate speech representation learning and recognition accuracy, without mention of low-latency interaction, interruption handling, or backchanneling. Thus, while it advances speech representation learning, it lacks direct evidence of addressing full-duplex spoken language modeling or the specified multi-dimensional behavioral and temporal evaluations required by the inclusion criteria.",
    "round-A_JuniorMini_evaluation": 2,
    "round-B_SeniorLead_output": {
      "reasoning": "I reviewed the two junior reviewers' notes (JuniorNano leaned toward inclusion because this is self-supervised speech representation work; JuniorMini argued correctly that the abstract lacks SLM/FD‑SLM and interaction focus). The paper describes self-supervised speech representations and visual grounding with a masked language modeling objective and reports ZeroSpeech/SUPERB results, but the title/abstract do not use the required terminology (spoken language model(s) / language modeling of speech) nor discuss full‑duplex interaction, synchronization, low‑latency, overlap/interrupt handling, or related multi‑dimensional behavioral evaluations required by the inclusion criteria. Therefore it fails the explicit inclusion requirement and matches exclusion criteria (focus on representation/benchmarks rather than SLMs or real‑time interactive modeling). Score explanation (1 sentence): I score this a 2 because, although relevant to speech representation research, it does not explicitly present Spoken Language Models or full‑duplex / synchronous interaction contributions required for inclusion.",
      "evaluation": 2
    },
    "round-B_SeniorLead_reasoning": "I reviewed the two junior reviewers' notes (JuniorNano leaned toward inclusion because this is self-supervised speech representation work; JuniorMini argued correctly that the abstract lacks SLM/FD‑SLM and interaction focus). The paper describes self-supervised speech representations and visual grounding with a masked language modeling objective and reports ZeroSpeech/SUPERB results, but the title/abstract do not use the required terminology (spoken language model(s) / language modeling of speech) nor discuss full‑duplex interaction, synchronization, low‑latency, overlap/interrupt handling, or related multi‑dimensional behavioral evaluations required by the inclusion criteria. Therefore it fails the explicit inclusion requirement and matches exclusion criteria (focus on representation/benchmarks rather than SLMs or real‑time interactive modeling). Score explanation (1 sentence): I score this a 2 because, although relevant to speech representation research, it does not explicitly present Spoken Language Models or full‑duplex / synchronous interaction contributions required for inclusion.",
    "round-B_SeniorLead_evaluation": 2,
    "final_verdict": "exclude (senior:2)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "A Brief Overview of Unsupervised Neural Speech Representation Learning",
    "abstract": "Unsupervised representation learning for speech processing has matured greatly in the last few years. Work in computer vision and natural language processing has paved the way, but speech data offers unique challenges. As a result, methods from other domains rarely translate directly. We review the development of unsupervised representation learning for speech over the last decade. We identify two primary model categories: self-supervised methods and probabilistic latent variable models. We describe the models and develop a comprehensive taxonomy. Finally, we discuss and compare models from the two categories.",
    "metadata": {
      "title": "A Brief Overview of Unsupervised Neural Speech Representation Learning",
      "summary": "Unsupervised representation learning for speech processing has matured greatly in the last few years. Work in computer vision and natural language processing has paved the way, but speech data offers unique challenges. As a result, methods from other domains rarely translate directly. We review the development of unsupervised representation learning for speech over the last decade. We identify two primary model categories: self-supervised methods and probabilistic latent variable models. We describe the models and develop a comprehensive taxonomy. Finally, we discuss and compare models from the two categories.",
      "abstract": "Unsupervised representation learning for speech processing has matured greatly in the last few years. Work in computer vision and natural language processing has paved the way, but speech data offers unique challenges. As a result, methods from other domains rarely translate directly. We review the development of unsupervised representation learning for speech over the last decade. We identify two primary model categories: self-supervised methods and probabilistic latent variable models. We describe the models and develop a comprehensive taxonomy. Finally, we discuss and compare models from the two categories.",
      "doi": "https://doi.org/10.48550/arxiv.2203.01829",
      "openalex_id": "https://openalex.org/W4226199158",
      "arxiv_id": "",
      "publication_date": "2022-03-01",
      "published": "2022-03-01",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The abstract reviews unsupervised speech representation learning in general and does not present Spoken Language Models / Speech Language Models as the core subject, nor does it address interactive full-duplex speech dialogue or synchronization. It’s unclear if the work was published within the last 3 years and it does not emphasize speech-language modeling concepts, architectures, or evaluation frameworks specific to SLMs.",
      "evaluation": 2
    },
    "round-A_JuniorNano_reasoning": "The abstract reviews unsupervised speech representation learning in general and does not present Spoken Language Models / Speech Language Models as the core subject, nor does it address interactive full-duplex speech dialogue or synchronization. It’s unclear if the work was published within the last 3 years and it does not emphasize speech-language modeling concepts, architectures, or evaluation frameworks specific to SLMs.",
    "round-A_JuniorNano_evaluation": 2,
    "round-A_JuniorMini_output": {
      "reasoning": "The study reviews unsupervised neural speech representation learning, focusing on model development and taxonomy rather than specifically on Spoken Language Models or Full-Duplex Spoken Language Models as core objects for modeling speech sequences and synchronous interaction; it lacks mention of full-duplex interaction, cognitive parallelism, synchronization, or relevant evaluation metrics and does not explicitly discuss spoken language modeling or speech-LLM architectures within the last 3 years; thus, it does not meet all inclusion criteria and partially aligns with exclusion criteria regarding focus on foundational representation rather than spoken language modeling or full-duplex interactive systems.",
      "evaluation": 2
    },
    "round-A_JuniorMini_reasoning": "The study reviews unsupervised neural speech representation learning, focusing on model development and taxonomy rather than specifically on Spoken Language Models or Full-Duplex Spoken Language Models as core objects for modeling speech sequences and synchronous interaction; it lacks mention of full-duplex interaction, cognitive parallelism, synchronization, or relevant evaluation metrics and does not explicitly discuss spoken language modeling or speech-LLM architectures within the last 3 years; thus, it does not meet all inclusion criteria and partially aligns with exclusion criteria regarding focus on foundational representation rather than spoken language modeling or full-duplex interactive systems.",
    "round-A_JuniorMini_evaluation": 2,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:2)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "textless-lib: a Library for Textless Spoken Language Processing",
    "abstract": "Eugene Kharitonov, Jade Copet, Kushal Lakhotia, Tu Anh Nguyen, Paden Tomasello, Ann Lee, Ali Elkahky, Wei-Ning Hsu, Abdelrahman Mohamed, Emmanuel Dupoux, Yossi Adi. Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies: System Demonstrations. 2022.",
    "metadata": {
      "title": "textless-lib: a Library for Textless Spoken Language Processing",
      "summary": "Eugene Kharitonov, Jade Copet, Kushal Lakhotia, Tu Anh Nguyen, Paden Tomasello, Ann Lee, Ali Elkahky, Wei-Ning Hsu, Abdelrahman Mohamed, Emmanuel Dupoux, Yossi Adi. Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies: System Demonstrations. 2022.",
      "abstract": "Eugene Kharitonov, Jade Copet, Kushal Lakhotia, Tu Anh Nguyen, Paden Tomasello, Ann Lee, Ali Elkahky, Wei-Ning Hsu, Abdelrahman Mohamed, Emmanuel Dupoux, Yossi Adi. Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies: System Demonstrations. 2022.",
      "doi": "https://doi.org/10.18653/v1/2022.naacl-demo.1",
      "openalex_id": "https://openalex.org/W4287887366",
      "arxiv_id": "",
      "publication_date": "2022-01-01",
      "published": "2022-01-01",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The title suggests textless Spoken Language Processing, which could align with SLM/FD-SLM themes, but the abstract is unavailable and it's unclear whether the work covers end-to-end or interactive speech modeling, synchronization, evaluation frameworks, or even if it truly centers on speech as the modeling target. The year (2022) may also fall outside a strict 3-year window depending on interpretation. Therefore inclusion cannot be confidently determined from the available information.",
      "evaluation": 3
    },
    "round-A_JuniorNano_reasoning": "The title suggests textless Spoken Language Processing, which could align with SLM/FD-SLM themes, but the abstract is unavailable and it's unclear whether the work covers end-to-end or interactive speech modeling, synchronization, evaluation frameworks, or even if it truly centers on speech as the modeling target. The year (2022) may also fall outside a strict 3-year window depending on interpretation. Therefore inclusion cannot be confidently determined from the available information.",
    "round-A_JuniorNano_evaluation": 3,
    "round-A_JuniorMini_output": {
      "reasoning": "The title 'textless-lib: a Library for Textless Spoken Language Processing' suggests a focus on spoken language processing possibly without reliance on text, which fits part of the inclusion criteria about modeling speech signals rather than text. However, the abstract provided does not contain detailed information about whether the study explicitly addresses Spoken Language Models (SLMs), full-duplex interaction, cognitive parallelism, engineered or learned synchronization, or evaluation on relevant tasks beyond traditional benchmarks such as WER or audio quality. Furthermore, the paper is from 2022, which is within the 3-year range. The absence of explicit mention of spoken language models or equivalent terms in the abstract or title makes it unclear if it strictly meets all inclusion criteria, particularly regarding model architecture, training and evaluation. Therefore, the evidence is insufficient to decisively include or exclude the paper based on given criteria, hence a neutral rating is appropriate.",
      "evaluation": 3
    },
    "round-A_JuniorMini_reasoning": "The title 'textless-lib: a Library for Textless Spoken Language Processing' suggests a focus on spoken language processing possibly without reliance on text, which fits part of the inclusion criteria about modeling speech signals rather than text. However, the abstract provided does not contain detailed information about whether the study explicitly addresses Spoken Language Models (SLMs), full-duplex interaction, cognitive parallelism, engineered or learned synchronization, or evaluation on relevant tasks beyond traditional benchmarks such as WER or audio quality. Furthermore, the paper is from 2022, which is within the 3-year range. The absence of explicit mention of spoken language models or equivalent terms in the abstract or title makes it unclear if it strictly meets all inclusion criteria, particularly regarding model architecture, training and evaluation. Therefore, the evidence is insufficient to decisively include or exclude the paper based on given criteria, hence a neutral rating is appropriate.",
    "round-A_JuniorMini_evaluation": 3,
    "round-B_SeniorLead_output": {
      "reasoning": "I reviewed the two junior reviews (both rated 3) and the item: the title ‘‘textless-lib: a Library for Textless Spoken Language Processing’’ suggests relevance (speech-centric processing without text), but the abstract is unavailable, the title does not explicitly state “Spoken Language Model(s)” or equivalent, and the paper is from 2022 which may fall outside the required 3-year window—so there is insufficient evidence that it meets the inclusion requirements (explicit SLM focus, architecture/training/evaluation details, and in-window date). Brief explanation: Insufficient information (no abstract), missing explicit SLM wording, and borderline/out-of-window publication year mean I cannot confidently include it.",
      "evaluation": 3
    },
    "round-B_SeniorLead_reasoning": "I reviewed the two junior reviews (both rated 3) and the item: the title ‘‘textless-lib: a Library for Textless Spoken Language Processing’’ suggests relevance (speech-centric processing without text), but the abstract is unavailable, the title does not explicitly state “Spoken Language Model(s)” or equivalent, and the paper is from 2022 which may fall outside the required 3-year window—so there is insufficient evidence that it meets the inclusion requirements (explicit SLM focus, architecture/training/evaluation details, and in-window date). Brief explanation: Insufficient information (no abstract), missing explicit SLM wording, and borderline/out-of-window publication year mean I cannot confidently include it.",
    "round-B_SeniorLead_evaluation": 3,
    "final_verdict": "maybe (senior:3)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Natural TTS Synthesis by Conditioning Wavenet on MEL Spectrogram Predictions",
    "abstract": "This paper describes Tacotron 2, a neural network architecture for speech synthesis directly from text. The system is composed of a recurrent sequence-to-sequence feature prediction network that maps character embeddings to mel-scale spectrograms, followed by a modified WaveNet model acting as a vocoder to synthesize time-domain waveforms from those spectrograms. Our model achieves a mean opinion score (MOS) of 4.53 comparable to a MOS of 4.58 for professionally recorded speech. To validate our design choices, we present ablation studies of key components of our system and evaluate the impact of using mel spectrograms as the conditioning input to WaveNet instead of linguistic, duration, and F0 features. We further show that using this compact acoustic intermediate representation allows for a significant reduction in the size of the WaveNet architecture.",
    "metadata": {
      "title": "Natural TTS Synthesis by Conditioning Wavenet on MEL Spectrogram Predictions",
      "summary": "This paper describes Tacotron 2, a neural network architecture for speech synthesis directly from text. The system is composed of a recurrent sequence-to-sequence feature prediction network that maps character embeddings to mel-scale spectrograms, followed by a modified WaveNet model acting as a vocoder to synthesize time-domain waveforms from those spectrograms. Our model achieves a mean opinion score (MOS) of 4.53 comparable to a MOS of 4.58 for professionally recorded speech. To validate our design choices, we present ablation studies of key components of our system and evaluate the impact of using mel spectrograms as the conditioning input to WaveNet instead of linguistic, duration, and F0 features. We further show that using this compact acoustic intermediate representation allows for a significant reduction in the size of the WaveNet architecture.",
      "abstract": "This paper describes Tacotron 2, a neural network architecture for speech synthesis directly from text. The system is composed of a recurrent sequence-to-sequence feature prediction network that maps character embeddings to mel-scale spectrograms, followed by a modified WaveNet model acting as a vocoder to synthesize time-domain waveforms from those spectrograms. Our model achieves a mean opinion score (MOS) of 4.53 comparable to a MOS of 4.58 for professionally recorded speech. To validate our design choices, we present ablation studies of key components of our system and evaluate the impact of using mel spectrograms as the conditioning input to WaveNet instead of linguistic, duration, and F0 features. We further show that using this compact acoustic intermediate representation allows for a significant reduction in the size of the WaveNet architecture.",
      "doi": "https://doi.org/10.1109/icassp.2018.8461368",
      "openalex_id": "https://openalex.org/W2964243274",
      "arxiv_id": "",
      "publication_date": "2018-04-01",
      "published": "2018-04-01",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "Reasoning: The item describes Tacotron 2 TTS architecture from text to mel spectrograms with WaveNet vocoder, which is a traditional TTS system not centered on Spoken Language Models or interactive SDLM concepts, falls outside the last 3 years, and does not address turn-taking or synchronous dialogue evaluation, thus not meeting inclusion criteria and better to exclude.",
      "evaluation": 2
    },
    "round-A_JuniorNano_reasoning": "Reasoning: The item describes Tacotron 2 TTS architecture from text to mel spectrograms with WaveNet vocoder, which is a traditional TTS system not centered on Spoken Language Models or interactive SDLM concepts, falls outside the last 3 years, and does not address turn-taking or synchronous dialogue evaluation, thus not meeting inclusion criteria and better to exclude.",
    "round-A_JuniorNano_evaluation": 2,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper focuses solely on text-to-speech (TTS) synthesis using Tacotron 2 and WaveNet vocoder, emphasizing acoustic feature prediction and speech waveform generation. It does not discuss spoken language models (SLMs) or full-duplex spoken language interaction, nor does it address synchronization, real-time interaction, cognitive parallelism, or multi-turn dialogue behaviors. The study is centered on a single speech synthesis task with evaluation by MOS scores, without considering speech language modeling architectures or interactive dialogue mechanisms, thus failing to meet the inclusion criteria and fitting multiple exclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper focuses solely on text-to-speech (TTS) synthesis using Tacotron 2 and WaveNet vocoder, emphasizing acoustic feature prediction and speech waveform generation. It does not discuss spoken language models (SLMs) or full-duplex spoken language interaction, nor does it address synchronization, real-time interaction, cognitive parallelism, or multi-turn dialogue behaviors. The study is centered on a single speech synthesis task with evaluation by MOS scores, without considering speech language modeling architectures or interactive dialogue mechanisms, thus failing to meet the inclusion criteria and fitting multiple exclusion criteria.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:2)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Phonetic Analysis of Self-supervised Representations of English Speech",
    "abstract": "We present an analysis of discrete units discovered via selfsupervised representation learning on English speech.We focus on units produced by a pre-trained HuBERT model due to its wide adoption in ASR, speech synthesis, and many other tasks.Whereas previous work has evaluated the quality of such quantization models in aggregate over all phones for a given language, we break our analysis down into broad phonetic classes, taking into account specific aspects of their articulation when considering their alignment to discrete units.We find that these units correspond to sub-phonetic events, and that fine dynamics such as the distinct closure and release portions of plosives tend to be represented by sequences of discrete units.Our work provides a reference for the phonetic properties of discrete units discovered by HuBERT, facilitating analyses of many speech applications based on this model.",
    "metadata": {
      "title": "Phonetic Analysis of Self-supervised Representations of English Speech",
      "summary": "We present an analysis of discrete units discovered via selfsupervised representation learning on English speech.We focus on units produced by a pre-trained HuBERT model due to its wide adoption in ASR, speech synthesis, and many other tasks.Whereas previous work has evaluated the quality of such quantization models in aggregate over all phones for a given language, we break our analysis down into broad phonetic classes, taking into account specific aspects of their articulation when considering their alignment to discrete units.We find that these units correspond to sub-phonetic events, and that fine dynamics such as the distinct closure and release portions of plosives tend to be represented by sequences of discrete units.Our work provides a reference for the phonetic properties of discrete units discovered by HuBERT, facilitating analyses of many speech applications based on this model.",
      "abstract": "We present an analysis of discrete units discovered via selfsupervised representation learning on English speech.We focus on units produced by a pre-trained HuBERT model due to its wide adoption in ASR, speech synthesis, and many other tasks.Whereas previous work has evaluated the quality of such quantization models in aggregate over all phones for a given language, we break our analysis down into broad phonetic classes, taking into account specific aspects of their articulation when considering their alignment to discrete units.We find that these units correspond to sub-phonetic events, and that fine dynamics such as the distinct closure and release portions of plosives tend to be represented by sequences of discrete units.Our work provides a reference for the phonetic properties of discrete units discovered by HuBERT, facilitating analyses of many speech applications based on this model.",
      "doi": "https://doi.org/10.21437/interspeech.2022-10884",
      "openalex_id": "https://openalex.org/W4297841405",
      "arxiv_id": "",
      "publication_date": "2022-09-16",
      "published": "2022-09-16",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This item analyzes phonetic properties of discrete units from a self-supervised speech model (HuBERT) but does not center on Spoken Language Models or Full-Duplex Speech-Language Models as the core modeling/inference target, nor address interactive speech dynamics; thus it fails the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "This item analyzes phonetic properties of discrete units from a self-supervised speech model (HuBERT) but does not center on Spoken Language Models or Full-Duplex Speech-Language Models as the core modeling/inference target, nor address interactive speech dynamics; thus it fails the inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study analyzes discrete units from a self-supervised model (HuBERT) on English speech focusing on phonetic properties, but it does not explicitly discuss spoken language models (SLMs), speech language modeling, full-duplex spoken language models (FD-SLMs), or related architectures and interactive aspects such as synchronous dialogue behavior or real-time interaction, which are key inclusion criteria. It also leans towards ASR-related phonetic unit analysis rather than language modeling or interactive systems, thus it does not clearly meet the specific inclusion criteria emphasizing SLMs and interaction.",
      "evaluation": 2
    },
    "round-A_JuniorMini_reasoning": "The study analyzes discrete units from a self-supervised model (HuBERT) on English speech focusing on phonetic properties, but it does not explicitly discuss spoken language models (SLMs), speech language modeling, full-duplex spoken language models (FD-SLMs), or related architectures and interactive aspects such as synchronous dialogue behavior or real-time interaction, which are key inclusion criteria. It also leans towards ASR-related phonetic unit analysis rather than language modeling or interactive systems, thus it does not clearly meet the specific inclusion criteria emphasizing SLMs and interaction.",
    "round-A_JuniorMini_evaluation": 2,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:2)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Speech Resynthesis from Discrete Disentangled Self-Supervised Representations",
    "abstract": "We propose using self-supervised discrete representations for the task of\\nspeech resynthesis. To generate disentangled representation, we separately\\nextract low-bitrate representations for speech content, prosodic information,\\nand speaker identity. This allows to synthesize speech in a controllable\\nmanner. We analyze various state-of-the-art, self-supervised representation\\nlearning methods and shed light on the advantages of each method while\\nconsidering reconstruction quality and disentanglement properties.\\nSpecifically, we evaluate the F0 reconstruction, speaker identification\\nperformance (for both resynthesis and voice conversion), recordings'\\nintelligibility, and overall quality using subjective human evaluation. Lastly,\\nwe demonstrate how these representations can be used for an ultra-lightweight\\nspeech codec. Using the obtained representations, we can get to a rate of 365\\nbits per second while providing better speech quality than the baseline\\nmethods. Audio samples can be found under the following link:\\nspeechbot.github.io/resynthesis.\\n",
    "metadata": {
      "title": "Speech Resynthesis from Discrete Disentangled Self-Supervised Representations",
      "summary": "We propose using self-supervised discrete representations for the task of\\nspeech resynthesis. To generate disentangled representation, we separately\\nextract low-bitrate representations for speech content, prosodic information,\\nand speaker identity. This allows to synthesize speech in a controllable\\nmanner. We analyze various state-of-the-art, self-supervised representation\\nlearning methods and shed light on the advantages of each method while\\nconsidering reconstruction quality and disentanglement properties.\\nSpecifically, we evaluate the F0 reconstruction, speaker identification\\nperformance (for both resynthesis and voice conversion), recordings'\\nintelligibility, and overall quality using subjective human evaluation. Lastly,\\nwe demonstrate how these representations can be used for an ultra-lightweight\\nspeech codec. Using the obtained representations, we can get to a rate of 365\\nbits per second while providing better speech quality than the baseline\\nmethods. Audio samples can be found under the following link:\\nspeechbot.github.io/resynthesis.\\n",
      "abstract": "We propose using self-supervised discrete representations for the task of\\nspeech resynthesis. To generate disentangled representation, we separately\\nextract low-bitrate representations for speech content, prosodic information,\\nand speaker identity. This allows to synthesize speech in a controllable\\nmanner. We analyze various state-of-the-art, self-supervised representation\\nlearning methods and shed light on the advantages of each method while\\nconsidering reconstruction quality and disentanglement properties.\\nSpecifically, we evaluate the F0 reconstruction, speaker identification\\nperformance (for both resynthesis and voice conversion), recordings'\\nintelligibility, and overall quality using subjective human evaluation. Lastly,\\nwe demonstrate how these representations can be used for an ultra-lightweight\\nspeech codec. Using the obtained representations, we can get to a rate of 365\\nbits per second while providing better speech quality than the baseline\\nmethods. Audio samples can be found under the following link:\\nspeechbot.github.io/resynthesis.\\n",
      "doi": "https://doi.org/10.21437/interspeech.2021-475",
      "openalex_id": "https://openalex.org/W3140429000",
      "arxiv_id": "",
      "publication_date": "2021-08-27",
      "published": "2021-08-27",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item focuses on speech resynthesis via self-supervised/discrete representations and does not treat Spoken Language Models or speech-language modeling as the core object, nor discuss interactive dialog/synchronization aspects; it thus fails the core inclusion criterion and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item focuses on speech resynthesis via self-supervised/discrete representations and does not treat Spoken Language Models or speech-language modeling as the core object, nor discuss interactive dialog/synchronization aspects; it thus fails the core inclusion criterion and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper focuses on speech resynthesis using self-supervised discrete representations, analyzing reconstruction quality and speaker identity aspects, but it does not mention spoken language models, language modeling of speech, or full-duplex interactive speech modeling; it also does not address synchronization, real-time interaction, or dialogue arbitration, nor does it meet the inclusion criteria related to core spoken language modeling and relevant evaluation metrics.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper focuses on speech resynthesis using self-supervised discrete representations, analyzing reconstruction quality and speaker identity aspects, but it does not mention spoken language models, language modeling of speech, or full-duplex interactive speech modeling; it also does not address synchronization, real-time interaction, or dialogue arbitration, nor does it meet the inclusion criteria related to core spoken language modeling and relevant evaluation metrics.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "V-Measure: A Conditional Entropy-Based External Cluster Evaluation Measure",
    "abstract": "We present V-measure, an external entropybased cluster evaluation measure. Vmeasure provides an elegant solution to many problems that affect previously defined cluster evaluation measures including 1) dependence on clustering algorithm or data set, 2) the “problem of matching”, where the clustering of only a portion of data points are evaluated and 3) accurate evaluation and combination of two desirable aspects of clustering, homogeneity and completeness. We compare V-measure to a number of popular cluster evaluation measures and demonstrate that it satisfies several desirable properties of clustering solutions, using simulated clustering results. Finally, we use V-measure to evaluate two clustering tasks: document clustering and pitch accent type clustering.",
    "metadata": {
      "title": "V-Measure: A Conditional Entropy-Based External Cluster Evaluation Measure",
      "summary": "We present V-measure, an external entropybased cluster evaluation measure. Vmeasure provides an elegant solution to many problems that affect previously defined cluster evaluation measures including 1) dependence on clustering algorithm or data set, 2) the “problem of matching”, where the clustering of only a portion of data points are evaluated and 3) accurate evaluation and combination of two desirable aspects of clustering, homogeneity and completeness. We compare V-measure to a number of popular cluster evaluation measures and demonstrate that it satisfies several desirable properties of clustering solutions, using simulated clustering results. Finally, we use V-measure to evaluate two clustering tasks: document clustering and pitch accent type clustering.",
      "abstract": "We present V-measure, an external entropybased cluster evaluation measure. Vmeasure provides an elegant solution to many problems that affect previously defined cluster evaluation measures including 1) dependence on clustering algorithm or data set, 2) the “problem of matching”, where the clustering of only a portion of data points are evaluated and 3) accurate evaluation and combination of two desirable aspects of clustering, homogeneity and completeness. We compare V-measure to a number of popular cluster evaluation measures and demonstrate that it satisfies several desirable properties of clustering solutions, using simulated clustering results. Finally, we use V-measure to evaluate two clustering tasks: document clustering and pitch accent type clustering.",
      "doi": "",
      "openalex_id": "https://openalex.org/W2138615112",
      "arxiv_id": "",
      "publication_date": "2007-06-01",
      "published": "2007-06-01",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This item discusses V-measure as an external clustering evaluation measure and does not address Spoken Language Models (SLMs) or FD-SLMs, nor the speech-centric modeling/training/evaluation focus required by the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "This item discusses V-measure as an external clustering evaluation measure and does not address Spoken Language Models (SLMs) or FD-SLMs, nor the speech-centric modeling/training/evaluation focus required by the inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The article focuses on an external cluster evaluation measure (V-Measure) for clustering tasks such as document and pitch accent type clustering, which does not explicitly address spoken language models, speech language modeling, or full-duplex spoken language interactions as required by the inclusion criteria; it neither discusses speech as a primary modeling target nor covers training, architecture, or evaluation of spoken language models, thus failing to meet the inclusion requirements and not being relevant to the research scope defined.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The article focuses on an external cluster evaluation measure (V-Measure) for clustering tasks such as document and pitch accent type clustering, which does not explicitly address spoken language models, speech language modeling, or full-duplex spoken language interactions as required by the inclusion criteria; it neither discusses speech as a primary modeling target nor covers training, architecture, or evaluation of spoken language models, thus failing to meet the inclusion requirements and not being relevant to the research scope defined.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "wav2vec 2.0: A Framework for Self-Supervised Learning of Speech Representations",
    "abstract": "We show for the first time that learning powerful representations from speech audio alone followed by fine-tuning on transcribed speech can outperform the best semi-supervised methods while being conceptually simpler. wav2vec 2.0 masks the speech input in the latent space and solves a contrastive task defined over a quantization of the latent representations which are jointly learned. Experiments using all labeled data of Librispeech achieve 1.8/3.3 WER on the clean/other test sets. When lowering the amount of labeled data to one hour, wav2vec 2.0 outperforms the previous state of the art on the 100 hour subset while using 100 times less labeled data. Using just ten minutes of labeled data and pre-training on 53k hours of unlabeled data still achieves 4.8/8.2 WER. This demonstrates the feasibility of speech recognition with limited amounts of labeled data.",
    "metadata": {
      "title": "wav2vec 2.0: A Framework for Self-Supervised Learning of Speech Representations",
      "summary": "We show for the first time that learning powerful representations from speech audio alone followed by fine-tuning on transcribed speech can outperform the best semi-supervised methods while being conceptually simpler. wav2vec 2.0 masks the speech input in the latent space and solves a contrastive task defined over a quantization of the latent representations which are jointly learned. Experiments using all labeled data of Librispeech achieve 1.8/3.3 WER on the clean/other test sets. When lowering the amount of labeled data to one hour, wav2vec 2.0 outperforms the previous state of the art on the 100 hour subset while using 100 times less labeled data. Using just ten minutes of labeled data and pre-training on 53k hours of unlabeled data still achieves 4.8/8.2 WER. This demonstrates the feasibility of speech recognition with limited amounts of labeled data.",
      "abstract": "We show for the first time that learning powerful representations from speech audio alone followed by fine-tuning on transcribed speech can outperform the best semi-supervised methods while being conceptually simpler. wav2vec 2.0 masks the speech input in the latent space and solves a contrastive task defined over a quantization of the latent representations which are jointly learned. Experiments using all labeled data of Librispeech achieve 1.8/3.3 WER on the clean/other test sets. When lowering the amount of labeled data to one hour, wav2vec 2.0 outperforms the previous state of the art on the 100 hour subset while using 100 times less labeled data. Using just ten minutes of labeled data and pre-training on 53k hours of unlabeled data still achieves 4.8/8.2 WER. This demonstrates the feasibility of speech recognition with limited amounts of labeled data.",
      "doi": "https://doi.org/10.48550/arxiv.2006.11477",
      "openalex_id": "https://openalex.org/W3036601975",
      "arxiv_id": "",
      "publication_date": "2020-06-20",
      "published": "2020-06-20",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "此項目於2020年發表，超出最近三年的研究範圍；雖聚焦語音表徵的自監督學習，但未以 Spoken Language Models/語音語言模型為核心並探討端到端或混合式語音-文字架構的互動與評估，且評估重點為語音辨識表現而非即時互動或語音重疊等語音語言建模觀點，因此不符合 inclusion 條件，應排除。",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "此項目於2020年發表，超出最近三年的研究範圍；雖聚焦語音表徵的自監督學習，但未以 Spoken Language Models/語音語言模型為核心並探討端到端或混合式語音-文字架構的互動與評估，且評估重點為語音辨識表現而非即時互動或語音重疊等語音語言建模觀點，因此不符合 inclusion 條件，應排除。",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper focuses primarily on a self-supervised speech representation learning framework aimed at automatic speech recognition (ASR) performance improvements, without explicit mention of spoken language models, full-duplex conversational capabilities, or behavioral arbitration aspects as defined in the inclusion criteria. The evaluation metrics focus on word error rate (WER), and the study does not address synchronization, cognitive parallelism, or other dialogue interaction elements required for inclusion, hence it aligns more with traditional ASR research rather than spoken language modeling for full-duplex interaction.",
      "evaluation": 2
    },
    "round-A_JuniorMini_reasoning": "The paper focuses primarily on a self-supervised speech representation learning framework aimed at automatic speech recognition (ASR) performance improvements, without explicit mention of spoken language models, full-duplex conversational capabilities, or behavioral arbitration aspects as defined in the inclusion criteria. The evaluation metrics focus on word error rate (WER), and the study does not address synchronization, cognitive parallelism, or other dialogue interaction elements required for inclusion, hence it aligns more with traditional ASR research rather than spoken language modeling for full-duplex interaction.",
    "round-A_JuniorMini_evaluation": 2,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:2)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Preview, Attend and Review: Schema-Aware Curriculum Learning for Multi-Domain Dialogue State Tracking",
    "abstract": "Yinpei Dai, Hangyu Li, Yongbin Li, Jian Sun, Fei Huang, Luo Si, Xiaodan Zhu. Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 2: Short Papers). 2021.",
    "metadata": {
      "title": "Preview, Attend and Review: Schema-Aware Curriculum Learning for Multi-Domain Dialogue State Tracking",
      "summary": "Yinpei Dai, Hangyu Li, Yongbin Li, Jian Sun, Fei Huang, Luo Si, Xiaodan Zhu. Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 2: Short Papers). 2021.",
      "abstract": "Yinpei Dai, Hangyu Li, Yongbin Li, Jian Sun, Fei Huang, Luo Si, Xiaodan Zhu. Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 2: Short Papers). 2021.",
      "doi": "https://doi.org/10.18653/v1/2021.acl-short.111",
      "openalex_id": "https://openalex.org/W3173691672",
      "arxiv_id": "",
      "publication_date": "2021-01-01",
      "published": "2021-01-01",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This paper is a text-based dialogue state tracking study not centered on spoken language models, and it was published in 2021, outside the past-3-years window.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "This paper is a text-based dialogue state tracking study not centered on spoken language models, and it was published in 2021, outside the past-3-years window.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The title and abstract indicate the work focuses on multi-domain dialogue state tracking, which primarily involves text-based dialogue modeling rather than spoken language modeling; there is no explicit mention of spoken language models, speech as a primary modeling object, synchronous full-duplex interaction, or evaluation of speech-based metrics as required by the inclusion criteria. Additionally, it was published in 2021, within the 3-year inclusion timeframe, but the core focus seems to be on dialogue state tracking from text, thus not meeting the main inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The title and abstract indicate the work focuses on multi-domain dialogue state tracking, which primarily involves text-based dialogue modeling rather than spoken language modeling; there is no explicit mention of spoken language models, speech as a primary modeling object, synchronous full-duplex interaction, or evaluation of speech-based metrics as required by the inclusion criteria. Additionally, it was published in 2021, within the 3-year inclusion timeframe, but the core focus seems to be on dialogue state tracking from text, thus not meeting the main inclusion criteria.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Learning Low-Resource End-To-End Goal-Oriented Dialog for Fast and Reliable System Deployment",
    "abstract": "Existing end-to-end dialog systems perform less effectively when data is scarce. To obtain an acceptable success in real-life online services with only a handful of training examples, both fast adaptability and reliable performance are highly desirable for dialog systems. In this paper, we propose the Meta-Dialog System (MDS), which combines the advantages of both meta-learning approaches and human-machine collaboration. We evaluate our methods on a new extended-bAbI dataset and a transformed MultiWOZ dataset for low-resource goal-oriented dialog learning. Experimental results show that MDS significantly outperforms non-meta-learning baselines and can achieve more than 90% per-turn accuracies with only 10 dialogs on the extended-bAbI dataset.",
    "metadata": {
      "title": "Learning Low-Resource End-To-End Goal-Oriented Dialog for Fast and Reliable System Deployment",
      "summary": "Existing end-to-end dialog systems perform less effectively when data is scarce. To obtain an acceptable success in real-life online services with only a handful of training examples, both fast adaptability and reliable performance are highly desirable for dialog systems. In this paper, we propose the Meta-Dialog System (MDS), which combines the advantages of both meta-learning approaches and human-machine collaboration. We evaluate our methods on a new extended-bAbI dataset and a transformed MultiWOZ dataset for low-resource goal-oriented dialog learning. Experimental results show that MDS significantly outperforms non-meta-learning baselines and can achieve more than 90% per-turn accuracies with only 10 dialogs on the extended-bAbI dataset.",
      "abstract": "Existing end-to-end dialog systems perform less effectively when data is scarce. To obtain an acceptable success in real-life online services with only a handful of training examples, both fast adaptability and reliable performance are highly desirable for dialog systems. In this paper, we propose the Meta-Dialog System (MDS), which combines the advantages of both meta-learning approaches and human-machine collaboration. We evaluate our methods on a new extended-bAbI dataset and a transformed MultiWOZ dataset for low-resource goal-oriented dialog learning. Experimental results show that MDS significantly outperforms non-meta-learning baselines and can achieve more than 90% per-turn accuracies with only 10 dialogs on the extended-bAbI dataset.",
      "doi": "https://doi.org/10.18653/v1/2020.acl-main.57",
      "openalex_id": "https://openalex.org/W3034879520",
      "arxiv_id": "",
      "publication_date": "2020-01-01",
      "published": "2020-01-01",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The paper concerns end-to-end goal-oriented dialog in low-resource settings but relies on text-based dialog datasets (extended-bAbI, MultiWOZ) and does not address Spoken Language Models or speech-language modeling, nor real-time speech interaction or synchronization evaluations; thus it does not meet the core inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The paper concerns end-to-end goal-oriented dialog in low-resource settings but relies on text-based dialog datasets (extended-bAbI, MultiWOZ) and does not address Spoken Language Models or speech-language modeling, nor real-time speech interaction or synchronization evaluations; thus it does not meet the core inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on end-to-end goal-oriented dialog systems and meta-learning approaches without emphasizing spoken language modeling or speech-centric architectures; it lacks mention of speech or spoken language models as core elements and does not address full-duplex interaction, speech overlap management, or low-latency spoken interaction features required by the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on end-to-end goal-oriented dialog systems and meta-learning approaches without emphasizing spoken language modeling or speech-centric architectures; it lacks mention of speech or spoken language models as core elements and does not address full-duplex interaction, speech overlap management, or low-latency spoken interaction features required by the inclusion criteria.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Some signals and rules for taking speaking turns in conversations.",
    "abstract": "Studied the turn-taking mechanism, whereby participants manage the smooth and appropriate exchange of speaking turns in face-to-face interaction in 2 videotapes showing a therapist-patient interview and a discussion between 2 therapists. 3 basic signals were noted: (a) turn-yielding signals by the s",
    "metadata": {
      "title": "Some signals and rules for taking speaking turns in conversations.",
      "summary": "Studied the turn-taking mechanism, whereby participants manage the smooth and appropriate exchange of speaking turns in face-to-face interaction in 2 videotapes showing a therapist-patient interview and a discussion between 2 therapists. 3 basic signals were noted: (a) turn-yielding signals by the s",
      "abstract": "Studied the turn-taking mechanism, whereby participants manage the smooth and appropriate exchange of speaking turns in face-to-face interaction in 2 videotapes showing a therapist-patient interview and a discussion between 2 therapists. 3 basic signals were noted: (a) turn-yielding signals by the s",
      "doi": "https://doi.org/10.1037/h0033031",
      "openalex_id": "https://openalex.org/W1964725106",
      "arxiv_id": "",
      "publication_date": "1972-08-01",
      "published": "1972-08-01",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item focuses on human turn-taking signals in conversations without any focus on Spoken Language Models (SLMs) or speech-language modeling; it does not address SLM/FD-SLM architectures, training, or evaluation, so it does not meet the inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item focuses on human turn-taking signals in conversations without any focus on Spoken Language Models (SLMs) or speech-language modeling; it does not address SLM/FD-SLM architectures, training, or evaluation, so it does not meet the inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on turn-taking mechanisms in conversations but does not explicitly address Spoken Language Models (SLMs) or Full-Duplex Spoken Language Models (FD-SLMs), nor does it discuss model architectures, training, or evaluation aspects related to speech language modeling; it appears to be a behavioral observation study rather than a machine learning or computational modeling study meeting the inclusion criteria. Therefore, it does not meet all inclusion criteria and may fall under exclusion criteria for not involving synchronization or language modeling aspects at the speech level.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on turn-taking mechanisms in conversations but does not explicitly address Spoken Language Models (SLMs) or Full-Duplex Spoken Language Models (FD-SLMs), nor does it discuss model architectures, training, or evaluation aspects related to speech language modeling; it appears to be a behavioral observation study rather than a machine learning or computational modeling study meeting the inclusion criteria. Therefore, it does not meet all inclusion criteria and may fall under exclusion criteria for not involving synchronization or language modeling aspects at the speech level.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "TurnGPT: a Transformer-based Language Model for Predicting Turn-taking in Spoken Dialog",
    "abstract": "Syntactic and pragmatic completeness is known to be important for turn-taking\\nprediction, but so far machine learning models of turn-taking have used such\\nlinguistic information in a limited way. In this paper, we introduce TurnGPT, a\\ntransformer-based language model for predicting turn-shifts in spoken dialog.\\nThe model has been trained and evaluated on a variety of written and spoken\\ndialog datasets. We show that the model outperforms two baselines used in prior\\nwork. We also report on an ablation study, as well as attention and gradient\\nanalyses, which show that the model is able to utilize the dialog context and\\npragmatic completeness for turn-taking prediction. Finally, we explore the\\nmodel's potential in not only detecting, but also projecting, turn-completions.\\n",
    "metadata": {
      "title": "TurnGPT: a Transformer-based Language Model for Predicting Turn-taking in Spoken Dialog",
      "summary": "Syntactic and pragmatic completeness is known to be important for turn-taking\\nprediction, but so far machine learning models of turn-taking have used such\\nlinguistic information in a limited way. In this paper, we introduce TurnGPT, a\\ntransformer-based language model for predicting turn-shifts in spoken dialog.\\nThe model has been trained and evaluated on a variety of written and spoken\\ndialog datasets. We show that the model outperforms two baselines used in prior\\nwork. We also report on an ablation study, as well as attention and gradient\\nanalyses, which show that the model is able to utilize the dialog context and\\npragmatic completeness for turn-taking prediction. Finally, we explore the\\nmodel's potential in not only detecting, but also projecting, turn-completions.\\n",
      "abstract": "Syntactic and pragmatic completeness is known to be important for turn-taking\\nprediction, but so far machine learning models of turn-taking have used such\\nlinguistic information in a limited way. In this paper, we introduce TurnGPT, a\\ntransformer-based language model for predicting turn-shifts in spoken dialog.\\nThe model has been trained and evaluated on a variety of written and spoken\\ndialog datasets. We show that the model outperforms two baselines used in prior\\nwork. We also report on an ablation study, as well as attention and gradient\\nanalyses, which show that the model is able to utilize the dialog context and\\npragmatic completeness for turn-taking prediction. Finally, we explore the\\nmodel's potential in not only detecting, but also projecting, turn-completions.\\n",
      "doi": "https://doi.org/10.18653/v1/2020.findings-emnlp.268",
      "openalex_id": "https://openalex.org/W3094393093",
      "arxiv_id": "",
      "publication_date": "2020-01-01",
      "published": "2020-01-01",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "Despite focusing on turn-taking in spoken dialog and using a transformer LM, the abstract does not mention Spoken Language Models or Speech Language Models in the title/abstract/keywords, thus it does not meet the required inclusion criterion.",
      "evaluation": 2
    },
    "round-A_JuniorNano_reasoning": "Despite focusing on turn-taking in spoken dialog and using a transformer LM, the abstract does not mention Spoken Language Models or Speech Language Models in the title/abstract/keywords, thus it does not meet the required inclusion criterion.",
    "round-A_JuniorNano_evaluation": 2,
    "round-A_JuniorMini_output": {
      "reasoning": "The study presents a transformer-based language model for predicting turn-taking in spoken dialogue, emphasizing dialog context and pragmatic completeness rather than direct speech or acoustic signal modeling; it primarily focuses on language modeling for turn-shifts rather than on spoken language models dealing with speech signals or full-duplex interaction capabilities, which are core to the inclusion criteria. The abstract does not explicitly mention speech or acoustic signal modeling or full-duplex interaction aspects, nor does it claim end-to-end speech-based modeling or evaluation beyond typical language model baselines. Hence, it likely doesn't meet the key inclusion criteria focusing on speech-level latent modeling, full-duplex spoken language models, or synchronization-oriented tasks and assessments as required.",
      "evaluation": 2
    },
    "round-A_JuniorMini_reasoning": "The study presents a transformer-based language model for predicting turn-taking in spoken dialogue, emphasizing dialog context and pragmatic completeness rather than direct speech or acoustic signal modeling; it primarily focuses on language modeling for turn-shifts rather than on spoken language models dealing with speech signals or full-duplex interaction capabilities, which are core to the inclusion criteria. The abstract does not explicitly mention speech or acoustic signal modeling or full-duplex interaction aspects, nor does it claim end-to-end speech-based modeling or evaluation beyond typical language model baselines. Hence, it likely doesn't meet the key inclusion criteria focusing on speech-level latent modeling, full-duplex spoken language models, or synchronization-oriented tasks and assessments as required.",
    "round-A_JuniorMini_evaluation": 2,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:2)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Dynamic Memory Induction Networks for Few-Shot Text Classification",
    "abstract": "This paper proposes Dynamic Memory Induction Networks (DMIN) for few-short text classification. The model develops a dynamic routing mechanism over static memory, enabling it to better adapt to unseen classes, a critical capability for few-short classification. The model also expands the induction process with supervised learning weights and query information to enhance the generalization ability of meta-learning. The proposed model brings forward the state-of-the-art performance significantly by 2~4% improvement on the miniRCV1 and ODIC datasets. Detailed analysis is further performed to show how the proposed network achieves the new performance.",
    "metadata": {
      "title": "Dynamic Memory Induction Networks for Few-Shot Text Classification",
      "summary": "This paper proposes Dynamic Memory Induction Networks (DMIN) for few-short text classification. The model develops a dynamic routing mechanism over static memory, enabling it to better adapt to unseen classes, a critical capability for few-short classification. The model also expands the induction process with supervised learning weights and query information to enhance the generalization ability of meta-learning. The proposed model brings forward the state-of-the-art performance significantly by 2~4% improvement on the miniRCV1 and ODIC datasets. Detailed analysis is further performed to show how the proposed network achieves the new performance.",
      "abstract": "This paper proposes Dynamic Memory Induction Networks (DMIN) for few-short text classification. The model develops a dynamic routing mechanism over static memory, enabling it to better adapt to unseen classes, a critical capability for few-short classification. The model also expands the induction process with supervised learning weights and query information to enhance the generalization ability of meta-learning. The proposed model brings forward the state-of-the-art performance significantly by 2~4% improvement on the miniRCV1 and ODIC datasets. Detailed analysis is further performed to show how the proposed network achieves the new performance.",
      "doi": "https://doi.org/10.18653/v1/2020.acl-main.102",
      "openalex_id": "https://openalex.org/W3034445880",
      "arxiv_id": "",
      "publication_date": "2020-01-01",
      "published": "2020-01-01",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The article focuses on dynamic memory networks for few-shot text classification, with no emphasis on spoken language models or speech-based modeling; it centers on text NLP rather than speech interaction, thus failing the inclusion criteria and meeting exclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The article focuses on dynamic memory networks for few-shot text classification, with no emphasis on spoken language models or speech-based modeling; it centers on text NLP rather than speech interaction, thus failing the inclusion criteria and meeting exclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on few-shot text classification using dynamic memory networks, with no mention of spoken language models, speech as a primary modeling object, full-duplex spoken language interaction, or synchronization aspects required by the inclusion criteria; thus it does not meet the core requirements of the review topic.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on few-shot text classification using dynamic memory networks, with no mention of spoken language models, speech as a primary modeling object, full-duplex spoken language interaction, or synchronization aspects required by the inclusion criteria; thus it does not meet the core requirements of the review topic.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Induction Networks for Few-Shot Text Classification",
    "abstract": "Ruiying Geng, Binhua Li, Yongbin Li, Xiaodan Zhu, Ping Jian, Jian Sun. Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). 2019.",
    "metadata": {
      "title": "Induction Networks for Few-Shot Text Classification",
      "summary": "Ruiying Geng, Binhua Li, Yongbin Li, Xiaodan Zhu, Ping Jian, Jian Sun. Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). 2019.",
      "abstract": "Ruiying Geng, Binhua Li, Yongbin Li, Xiaodan Zhu, Ping Jian, Jian Sun. Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). 2019.",
      "doi": "https://doi.org/10.18653/v1/d19-1403",
      "openalex_id": "https://openalex.org/W2971048662",
      "arxiv_id": "",
      "publication_date": "2019-01-01",
      "published": "2019-01-01",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item is a 2019 text-classification paper with no focus on Spoken Language Models or speech-centric modeling, and it is outside the required 3-year window, so it fails the inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item is a 2019 text-classification paper with no focus on Spoken Language Models or speech-centric modeling, and it is outside the required 3-year window, so it fails the inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper titled 'Induction Networks for Few-Shot Text Classification' is focused on NLP tasks related to text classification rather than spoken language models, speech language modeling, or full-duplex spoken dialogue systems as specified in the inclusion criteria. The abstract and title do not mention speech, spoken language modeling, or synchronous speech interaction. Moreover, it is from 2019, which is outside the required past 3 years publication range. Therefore, it does not meet the inclusion criteria and the exclusion criteria about focusing solely on text apply.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper titled 'Induction Networks for Few-Shot Text Classification' is focused on NLP tasks related to text classification rather than spoken language models, speech language modeling, or full-duplex spoken dialogue systems as specified in the inclusion criteria. The abstract and title do not mention speech, spoken language modeling, or synchronous speech interaction. Moreover, it is from 2019, which is outside the required past 3 years publication range. Therefore, it does not meet the inclusion criteria and the exclusion criteria about focusing solely on text apply.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Dynamic Hybrid Relation Exploration Network for Cross-Domain Context-Dependent Semantic Parsing",
    "abstract": "Semantic parsing has long been a fundamental problem in natural language processing. Recently, cross-domain context-dependent semantic parsing has become a new focus of research. Central to the problem is the challenge of leveraging contextual information of both natural language queries and database schemas in the interaction history. In this paper, we present a dynamic graph framework that is capable of effectively modelling contextual utterances, tokens, database schemas, and their complicated interaction as the conversation proceeds. The framework employs a dynamic memory decay mechanism that incorporates inductive bias to integrate enriched contextual relation representation, which is further enhanced with a powerful reranking model. At the time of writing, we demonstrate that the proposed framework outperforms all existing models by large margins, achieving new state-of-the-art performance on two large-scale benchmarks, the SParC and CoSQL datasets. Specifically, the model attains a 55.8% question-match and 30.8% interaction-match accuracy on SParC, and a 46.8% question-match and 17.0% interaction-match accuracy on CoSQL.",
    "metadata": {
      "title": "Dynamic Hybrid Relation Exploration Network for Cross-Domain Context-Dependent Semantic Parsing",
      "summary": "Semantic parsing has long been a fundamental problem in natural language processing. Recently, cross-domain context-dependent semantic parsing has become a new focus of research. Central to the problem is the challenge of leveraging contextual information of both natural language queries and database schemas in the interaction history. In this paper, we present a dynamic graph framework that is capable of effectively modelling contextual utterances, tokens, database schemas, and their complicated interaction as the conversation proceeds. The framework employs a dynamic memory decay mechanism that incorporates inductive bias to integrate enriched contextual relation representation, which is further enhanced with a powerful reranking model. At the time of writing, we demonstrate that the proposed framework outperforms all existing models by large margins, achieving new state-of-the-art performance on two large-scale benchmarks, the SParC and CoSQL datasets. Specifically, the model attains a 55.8% question-match and 30.8% interaction-match accuracy on SParC, and a 46.8% question-match and 17.0% interaction-match accuracy on CoSQL.",
      "abstract": "Semantic parsing has long been a fundamental problem in natural language processing. Recently, cross-domain context-dependent semantic parsing has become a new focus of research. Central to the problem is the challenge of leveraging contextual information of both natural language queries and database schemas in the interaction history. In this paper, we present a dynamic graph framework that is capable of effectively modelling contextual utterances, tokens, database schemas, and their complicated interaction as the conversation proceeds. The framework employs a dynamic memory decay mechanism that incorporates inductive bias to integrate enriched contextual relation representation, which is further enhanced with a powerful reranking model. At the time of writing, we demonstrate that the proposed framework outperforms all existing models by large margins, achieving new state-of-the-art performance on two large-scale benchmarks, the SParC and CoSQL datasets. Specifically, the model attains a 55.8% question-match and 30.8% interaction-match accuracy on SParC, and a 46.8% question-match and 17.0% interaction-match accuracy on CoSQL.",
      "doi": "https://doi.org/10.1609/aaai.v35i14.17550",
      "openalex_id": "https://openalex.org/W3173964589",
      "arxiv_id": "",
      "publication_date": "2021-05-18",
      "published": "2021-05-18",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This paper focuses on cross-domain context-dependent semantic parsing for natural language queries and database schemas and does not center on Spoken Language Models or speech as the core modeling target, so it does not meet the inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "This paper focuses on cross-domain context-dependent semantic parsing for natural language queries and database schemas and does not center on Spoken Language Models or speech as the core modeling target, so it does not meet the inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on cross-domain context-dependent semantic parsing based on natural language and database schemas, without mention of speech, spoken language models, or modeling acoustic signals; it centers on text-based semantic parsing rather than voice or speech language modeling, and there is no indication of full-duplex spoken interaction, cognitive parallelism, or low latency speech-related evaluation.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on cross-domain context-dependent semantic parsing based on natural language and database schemas, without mention of speech, spoken language models, or modeling acoustic signals; it centers on text-based semantic parsing rather than voice or speech language modeling, and there is no indication of full-duplex spoken interaction, cognitive parallelism, or low latency speech-related evaluation.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "An Attentive Listening System with Android ERICA: Comparison of Autonomous and WOZ Interactions",
    "abstract": "We describe an attentive listening system for the autonomous android robot ERICA. The proposed system generates several types of listener responses: backchannels, repeats, elaborating questions, assessments, generic sentimental responses, and generic responses. In this paper, we report a subjective experiment with 20 elderly people. First, we evaluated each system utterance excluding backchannels and generic responses, in an offline manner. It was found that most of the system utterances were linguistically appropriate, and they elicited positive reactions from the subjects. Furthermore, 58.2% of the responses were acknowledged as being appropriate listener responses. We also compared the proposed system with a WOZ system where a human operator was operating the robot. From the subjective evaluation, the proposed system achieved comparable scores in basic skills of attentive listening such as encouragement to talk, focused on the talk, and actively listening. It was also found that there is still a gap between the system and the WOZ for more sophisticated skills such as dialogue understanding, showing interest, and empathy towards the user.",
    "metadata": {
      "title": "An Attentive Listening System with Android ERICA: Comparison of Autonomous and WOZ Interactions",
      "summary": "We describe an attentive listening system for the autonomous android robot ERICA. The proposed system generates several types of listener responses: backchannels, repeats, elaborating questions, assessments, generic sentimental responses, and generic responses. In this paper, we report a subjective experiment with 20 elderly people. First, we evaluated each system utterance excluding backchannels and generic responses, in an offline manner. It was found that most of the system utterances were linguistically appropriate, and they elicited positive reactions from the subjects. Furthermore, 58.2% of the responses were acknowledged as being appropriate listener responses. We also compared the proposed system with a WOZ system where a human operator was operating the robot. From the subjective evaluation, the proposed system achieved comparable scores in basic skills of attentive listening such as encouragement to talk, focused on the talk, and actively listening. It was also found that there is still a gap between the system and the WOZ for more sophisticated skills such as dialogue understanding, showing interest, and empathy towards the user.",
      "abstract": "We describe an attentive listening system for the autonomous android robot ERICA. The proposed system generates several types of listener responses: backchannels, repeats, elaborating questions, assessments, generic sentimental responses, and generic responses. In this paper, we report a subjective experiment with 20 elderly people. First, we evaluated each system utterance excluding backchannels and generic responses, in an offline manner. It was found that most of the system utterances were linguistically appropriate, and they elicited positive reactions from the subjects. Furthermore, 58.2% of the responses were acknowledged as being appropriate listener responses. We also compared the proposed system with a WOZ system where a human operator was operating the robot. From the subjective evaluation, the proposed system achieved comparable scores in basic skills of attentive listening such as encouragement to talk, focused on the talk, and actively listening. It was also found that there is still a gap between the system and the WOZ for more sophisticated skills such as dialogue understanding, showing interest, and empathy towards the user.",
      "doi": "https://doi.org/10.18653/v1/2020.sigdial-1.15",
      "openalex_id": "https://openalex.org/W3034366140",
      "arxiv_id": "",
      "publication_date": "2020-01-01",
      "published": "2020-01-01",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "此研究聚焦於具體的人機互動系統回應與 WOZ 對比，未以語音語言模型（SLMs/FD-SLMs）為核心建模對象，且未聚焦於低延遲互動、語音重疊管理或完整的語音語言建模框架，因此不符合包含條件。",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "此研究聚焦於具體的人機互動系統回應與 WOZ 對比，未以語音語言模型（SLMs/FD-SLMs）為核心建模對象，且未聚焦於低延遲互動、語音重疊管理或完整的語音語言建模框架，因此不符合包含條件。",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study describes an attentive listening system for an android robot, focusing on generating listener responses like backchannels and elaborating questions, and evaluates subjective interaction quality; however, it does not clearly articulate the use or development of Spoken Language Models or Full-Duplex Spoken Language Models as core modeling and reasoning objects based on speech sequences or acoustic signals, nor does it mention related model architectures, training, or evaluations based on such models. The focus is more on dialog system response appropriateness and interaction comparison rather than speech language modeling or low-latency cognitive parallelism or behavior arbitration. Therefore, it does not fulfill the inclusion criteria that require explicit emphasis on spoken language modeling or related architecture/mechanism and corresponding evaluation.",
      "evaluation": 2
    },
    "round-A_JuniorMini_reasoning": "The study describes an attentive listening system for an android robot, focusing on generating listener responses like backchannels and elaborating questions, and evaluates subjective interaction quality; however, it does not clearly articulate the use or development of Spoken Language Models or Full-Duplex Spoken Language Models as core modeling and reasoning objects based on speech sequences or acoustic signals, nor does it mention related model architectures, training, or evaluations based on such models. The focus is more on dialog system response appropriateness and interaction comparison rather than speech language modeling or low-latency cognitive parallelism or behavior arbitration. Therefore, it does not fulfill the inclusion criteria that require explicit emphasis on spoken language modeling or related architecture/mechanism and corresponding evaluation.",
    "round-A_JuniorMini_evaluation": 2,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:2)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Convolutional Neural Networks for Sentence Classification",
    "abstract": "We report on a series of experiments with convolutional neural networks (CNN) trained on top of pre-trained word vectors for sentence-level classification tasks.We show that a simple CNN with little hyperparameter tuning and static vectors achieves excellent results on multiple benchmarks.Learning task-specific vectors through fine-tuning offers further gains in performance.We additionally propose a simple modification to the architecture to allow for the use of both task-specific and static vectors.The CNN models discussed herein improve upon the state of the art on 4 out of 7 tasks, which include sentiment analysis and question classification.",
    "metadata": {
      "title": "Convolutional Neural Networks for Sentence Classification",
      "summary": "We report on a series of experiments with convolutional neural networks (CNN) trained on top of pre-trained word vectors for sentence-level classification tasks.We show that a simple CNN with little hyperparameter tuning and static vectors achieves excellent results on multiple benchmarks.Learning task-specific vectors through fine-tuning offers further gains in performance.We additionally propose a simple modification to the architecture to allow for the use of both task-specific and static vectors.The CNN models discussed herein improve upon the state of the art on 4 out of 7 tasks, which include sentiment analysis and question classification.",
      "abstract": "We report on a series of experiments with convolutional neural networks (CNN) trained on top of pre-trained word vectors for sentence-level classification tasks.We show that a simple CNN with little hyperparameter tuning and static vectors achieves excellent results on multiple benchmarks.Learning task-specific vectors through fine-tuning offers further gains in performance.We additionally propose a simple modification to the architecture to allow for the use of both task-specific and static vectors.The CNN models discussed herein improve upon the state of the art on 4 out of 7 tasks, which include sentiment analysis and question classification.",
      "doi": "https://doi.org/10.3115/v1/d14-1181",
      "openalex_id": "https://openalex.org/W1832693441",
      "arxiv_id": "",
      "publication_date": "2014-01-01",
      "published": "2014-01-01",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The article targets text-based sentence classification with CNNs, not Spoken Language Models or speech-based modeling; it does not meet the inclusion criteria and falls under the exclusion criteria for lack of speech/interactive evaluation.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The article targets text-based sentence classification with CNNs, not Spoken Language Models or speech-based modeling; it does not meet the inclusion criteria and falls under the exclusion criteria for lack of speech/interactive evaluation.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on sentence classification using convolutional neural networks on top of pre-trained word vectors, dealing primarily with text-based NLP tasks without involving speech as the core modeling object, and it lacks discussion on spoken language models, speech-based modeling, or full-duplex interaction mechanisms as specified in the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on sentence classification using convolutional neural networks on top of pre-trained word vectors, dealing primarily with text-based NLP tasks without involving speech as the core modeling object, and it lacks discussion on spoken language models, speech-based modeling, or full-duplex interaction mechanisms as specified in the inclusion criteria.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Smooth Turn-taking by a Robot Using an Online Continuous Model to Generate Turn-taking Cues",
    "abstract": "Turn-taking in human-robot interaction is a crucial part of spoken dialogue systems, but current models do not allow for human-like turn-taking speed seen in natural conversation. In this work we propose combining two independent prediction models. A continuous model predicts the upcoming end of the turn in order to generate gaze aversion and fillers as turn-taking cues. This prediction is done while the user is speaking, so turn-taking can be done with little silence between turns, or even overlap. Once a speech recognition result has been received at a later time, a second model uses the lexical information to decide if or when the turn should actually be taken. We constructed the continuous model using the speaker's prosodic features as inputs and evaluated its online performance. We then conducted a subjective experiment in which we implemented our model in an android robot and asked participants to compare it to one without turn-taking cues, which produces a response when a speech recognition result is received. We found that using both gaze aversion and a filler was preferred when the continuous model correctly predicted the upcoming end of turn, while using only gaze aversion was better if the prediction was wrong.",
    "metadata": {
      "title": "Smooth Turn-taking by a Robot Using an Online Continuous Model to Generate Turn-taking Cues",
      "summary": "Turn-taking in human-robot interaction is a crucial part of spoken dialogue systems, but current models do not allow for human-like turn-taking speed seen in natural conversation. In this work we propose combining two independent prediction models. A continuous model predicts the upcoming end of the turn in order to generate gaze aversion and fillers as turn-taking cues. This prediction is done while the user is speaking, so turn-taking can be done with little silence between turns, or even overlap. Once a speech recognition result has been received at a later time, a second model uses the lexical information to decide if or when the turn should actually be taken. We constructed the continuous model using the speaker's prosodic features as inputs and evaluated its online performance. We then conducted a subjective experiment in which we implemented our model in an android robot and asked participants to compare it to one without turn-taking cues, which produces a response when a speech recognition result is received. We found that using both gaze aversion and a filler was preferred when the continuous model correctly predicted the upcoming end of turn, while using only gaze aversion was better if the prediction was wrong.",
      "abstract": "Turn-taking in human-robot interaction is a crucial part of spoken dialogue systems, but current models do not allow for human-like turn-taking speed seen in natural conversation. In this work we propose combining two independent prediction models. A continuous model predicts the upcoming end of the turn in order to generate gaze aversion and fillers as turn-taking cues. This prediction is done while the user is speaking, so turn-taking can be done with little silence between turns, or even overlap. Once a speech recognition result has been received at a later time, a second model uses the lexical information to decide if or when the turn should actually be taken. We constructed the continuous model using the speaker's prosodic features as inputs and evaluated its online performance. We then conducted a subjective experiment in which we implemented our model in an android robot and asked participants to compare it to one without turn-taking cues, which produces a response when a speech recognition result is received. We found that using both gaze aversion and a filler was preferred when the continuous model correctly predicted the upcoming end of turn, while using only gaze aversion was better if the prediction was wrong.",
      "doi": "https://doi.org/10.1145/3340555.3353727",
      "openalex_id": "https://openalex.org/W2980823180",
      "arxiv_id": "",
      "publication_date": "2019-10-14",
      "published": "2019-10-14",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This item focuses on turn-taking cues in human-robot interaction using predictive models, but does not clearly treat speech as the core modeling/inference target via a spoken language model or speech-language modeling framework, and the abstract does not discuss SLM/FD-SLM architecture, training, or evaluation—thus it doesn’t meet the defined inclusion criteria, though it is recent and relevant to speech in interaction.",
      "evaluation": 2
    },
    "round-A_JuniorNano_reasoning": "This item focuses on turn-taking cues in human-robot interaction using predictive models, but does not clearly treat speech as the core modeling/inference target via a spoken language model or speech-language modeling framework, and the abstract does not discuss SLM/FD-SLM architecture, training, or evaluation—thus it doesn’t meet the defined inclusion criteria, though it is recent and relevant to speech in interaction.",
    "round-A_JuniorNano_evaluation": 2,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on turn-taking in human-robot spoken dialogue systems using continuous prediction models based on prosodic features, which aligns with spoken language modeling at the acoustic/speech level and addresses full-duplex interaction aspects such as overlapping speech and low-latency turn-taking cues; however, the abstract does not explicitly mention 'Spoken Language Models' or its equivalent phrasing, nor does it clearly discuss the underlying model architecture, training, or evaluations from an SLM/FD-SLM perspective, presenting ambiguity relative to the inclusion criteria requiring explicit mention and core focus on spoken language modeling. Therefore, while the work relates closely to spoken interaction synchronization and turn-taking, it lacks explicit framing within the SLM/FD-SLM conceptual framework, leading to some uncertainty about inclusion based on the specified criteria.",
      "evaluation": 3
    },
    "round-A_JuniorMini_reasoning": "The study focuses on turn-taking in human-robot spoken dialogue systems using continuous prediction models based on prosodic features, which aligns with spoken language modeling at the acoustic/speech level and addresses full-duplex interaction aspects such as overlapping speech and low-latency turn-taking cues; however, the abstract does not explicitly mention 'Spoken Language Models' or its equivalent phrasing, nor does it clearly discuss the underlying model architecture, training, or evaluations from an SLM/FD-SLM perspective, presenting ambiguity relative to the inclusion criteria requiring explicit mention and core focus on spoken language modeling. Therefore, while the work relates closely to spoken interaction synchronization and turn-taking, it lacks explicit framing within the SLM/FD-SLM conceptual framework, leading to some uncertainty about inclusion based on the specified criteria.",
    "round-A_JuniorMini_evaluation": 3,
    "round-B_SeniorLead_output": {
      "reasoning": "Both junior reviewers note the paper focuses on engineering turn-taking cues (a continuous prosody-based predictor plus a lexical decision model) for a robot; while related to spoken interaction it does not explicitly frame the work as Spoken Language Models (SLMs) or Full-Duplex SLMs, nor does the abstract describe speech-level language modeling architecture, training, or SLM-style evaluations. The work is modular (prosody predictor + ASR-driven lexical decision) and evaluates subjective preference rather than SLM/FD-SLM benchmarks or end-to-end speech language modeling, so it fails the inclusion requirement that SLM or equivalent terminology and modeling focus appear in the title/abstract. Therefore, better to exclude. (Score justification: lacks explicit SLM/FD-SLM framing and core speech-language modeling contributions required by inclusion criteria.)",
      "evaluation": 2
    },
    "round-B_SeniorLead_reasoning": "Both junior reviewers note the paper focuses on engineering turn-taking cues (a continuous prosody-based predictor plus a lexical decision model) for a robot; while related to spoken interaction it does not explicitly frame the work as Spoken Language Models (SLMs) or Full-Duplex SLMs, nor does the abstract describe speech-level language modeling architecture, training, or SLM-style evaluations. The work is modular (prosody predictor + ASR-driven lexical decision) and evaluates subjective preference rather than SLM/FD-SLM benchmarks or end-to-end speech language modeling, so it fails the inclusion requirement that SLM or equivalent terminology and modeling focus appear in the title/abstract. Therefore, better to exclude. (Score justification: lacks explicit SLM/FD-SLM framing and core speech-language modeling contributions required by inclusion criteria.)",
    "round-B_SeniorLead_evaluation": 2,
    "final_verdict": "exclude (senior:2)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Attentive listening system with backchanneling, response generation and flexible turn-taking",
    "abstract": "Attentive listening systems are designed to let people, especially senior people, keep talking to maintain communication ability and mental health. This paper addresses key components of an attentive listening system which encourages users to talk smoothly. First, we introduce continuous prediction of end-of-utterances and generation of backchannels, rather than generating backchannels after end-point detection of utterances. This improves subjective evaluations of backchannels. Second, we propose an effective statement response mechanism which detects focus words and responds in the form of a question or partial repeat. This can be applied to any statement. Moreover, a flexible turn-taking mechanism is designed which uses backchannels or fillers when the turn-switch is ambiguous. These techniques are integrated into a humanoid robot to conduct attentive listening. We test the feasibility of the system in a pilot experiment and show that it can produce coherent dialogues during conversation.",
    "metadata": {
      "title": "Attentive listening system with backchanneling, response generation and flexible turn-taking",
      "summary": "Attentive listening systems are designed to let people, especially senior people, keep talking to maintain communication ability and mental health. This paper addresses key components of an attentive listening system which encourages users to talk smoothly. First, we introduce continuous prediction of end-of-utterances and generation of backchannels, rather than generating backchannels after end-point detection of utterances. This improves subjective evaluations of backchannels. Second, we propose an effective statement response mechanism which detects focus words and responds in the form of a question or partial repeat. This can be applied to any statement. Moreover, a flexible turn-taking mechanism is designed which uses backchannels or fillers when the turn-switch is ambiguous. These techniques are integrated into a humanoid robot to conduct attentive listening. We test the feasibility of the system in a pilot experiment and show that it can produce coherent dialogues during conversation.",
      "abstract": "Attentive listening systems are designed to let people, especially senior people, keep talking to maintain communication ability and mental health. This paper addresses key components of an attentive listening system which encourages users to talk smoothly. First, we introduce continuous prediction of end-of-utterances and generation of backchannels, rather than generating backchannels after end-point detection of utterances. This improves subjective evaluations of backchannels. Second, we propose an effective statement response mechanism which detects focus words and responds in the form of a question or partial repeat. This can be applied to any statement. Moreover, a flexible turn-taking mechanism is designed which uses backchannels or fillers when the turn-switch is ambiguous. These techniques are integrated into a humanoid robot to conduct attentive listening. We test the feasibility of the system in a pilot experiment and show that it can produce coherent dialogues during conversation.",
      "doi": "https://doi.org/10.18653/v1/w17-5516",
      "openalex_id": "https://openalex.org/W2787693967",
      "arxiv_id": "",
      "publication_date": "2017-01-01",
      "published": "2017-01-01",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "It centers a spoken-dialogue system with backchanneling, response generation, and flexible turn-taking (full-duplex interaction) and reports a pilot test, aligning with spoken language modeling concepts and real-time interaction evaluation rather than ASR/TTS-only work.",
      "evaluation": 4
    },
    "round-A_JuniorNano_reasoning": "It centers a spoken-dialogue system with backchanneling, response generation, and flexible turn-taking (full-duplex interaction) and reports a pilot test, aligning with spoken language modeling concepts and real-time interaction evaluation rather than ASR/TTS-only work.",
    "round-A_JuniorNano_evaluation": 4,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on an attentive listening system that includes backchanneling and flexible turn-taking, addressing real-time interaction features such as backchannel generation and turn-taking mechanisms, which match the inclusion emphasis on full-duplex spoken language interaction and behavioral arbitration; however, the abstract does not explicitly mention 'spoken language models' or related terms, nor does it explicitly describe modeling architectures, training, or evaluation metrics related to spoken language models per se, making it unclear if it meets the core criteria of focusing on spoken language models as the modeling and reasoning object rather than only dialogue system behavior or robotics.",
      "evaluation": 3
    },
    "round-A_JuniorMini_reasoning": "The study focuses on an attentive listening system that includes backchanneling and flexible turn-taking, addressing real-time interaction features such as backchannel generation and turn-taking mechanisms, which match the inclusion emphasis on full-duplex spoken language interaction and behavioral arbitration; however, the abstract does not explicitly mention 'spoken language models' or related terms, nor does it explicitly describe modeling architectures, training, or evaluation metrics related to spoken language models per se, making it unclear if it meets the core criteria of focusing on spoken language models as the modeling and reasoning object rather than only dialogue system behavior or robotics.",
    "round-A_JuniorMini_evaluation": 3,
    "round-B_SeniorLead_output": {
      "reasoning": "Both junior reviewers noted the paper describes a system-level attentive listening/dialogue agent with continuous end-of-utterance prediction, backchannel generation and flexible turn-taking (integrated into a humanoid robot and piloted), which touches on full-duplex interaction behaviors (backchanneling, ambiguous turn-switch handling). However, the abstract does not frame the work as research on Spoken Language Models or speech-level language modeling, nor does it mention model architectures, training, or benchmark-style evaluation as required by the inclusion criteria; it reads as a dialogue/robotics system paper rather than an SLM/FD-SLM study. Therefore it fails the requirement that spoken language models (or equivalent explicit framing) be the core modeling object and should be excluded. One-sentence explanation: lacks explicit SLM/speech-language-model framing and model/training/evaluation details, so it does not meet the inclusion criteria.",
      "evaluation": 2
    },
    "round-B_SeniorLead_reasoning": "Both junior reviewers noted the paper describes a system-level attentive listening/dialogue agent with continuous end-of-utterance prediction, backchannel generation and flexible turn-taking (integrated into a humanoid robot and piloted), which touches on full-duplex interaction behaviors (backchanneling, ambiguous turn-switch handling). However, the abstract does not frame the work as research on Spoken Language Models or speech-level language modeling, nor does it mention model architectures, training, or benchmark-style evaluation as required by the inclusion criteria; it reads as a dialogue/robotics system paper rather than an SLM/FD-SLM study. Therefore it fails the requirement that spoken language models (or equivalent explicit framing) be the core modeling object and should be excluded. One-sentence explanation: lacks explicit SLM/speech-language-model framing and model/training/evaluation details, so it does not meet the inclusion criteria.",
    "round-B_SeniorLead_evaluation": 2,
    "final_verdict": "exclude (senior:2)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Deep Unknown Intent Detection with Margin Loss",
    "abstract": "Identifying the unknown (novel) user intents that have never appeared in the training set is a challenging task in the dialogue system. In this paper, we present a two-stage method for detecting unknown intents. We use bidirectional long short-term memory (BiLSTM) network with the margin loss as the feature extractor. With margin loss, we can learn discriminative deep features by forcing the network to maximize inter-class variance and to minimize intra-class variance. Then, we feed the feature vectors to the density-based novelty detection algorithm, local outlier factor (LOF), to detect unknown intents. Experiments on two benchmark datasets show that our method can yield consistent improvements compared with the baseline methods.",
    "metadata": {
      "title": "Deep Unknown Intent Detection with Margin Loss",
      "summary": "Identifying the unknown (novel) user intents that have never appeared in the training set is a challenging task in the dialogue system. In this paper, we present a two-stage method for detecting unknown intents. We use bidirectional long short-term memory (BiLSTM) network with the margin loss as the feature extractor. With margin loss, we can learn discriminative deep features by forcing the network to maximize inter-class variance and to minimize intra-class variance. Then, we feed the feature vectors to the density-based novelty detection algorithm, local outlier factor (LOF), to detect unknown intents. Experiments on two benchmark datasets show that our method can yield consistent improvements compared with the baseline methods.",
      "abstract": "Identifying the unknown (novel) user intents that have never appeared in the training set is a challenging task in the dialogue system. In this paper, we present a two-stage method for detecting unknown intents. We use bidirectional long short-term memory (BiLSTM) network with the margin loss as the feature extractor. With margin loss, we can learn discriminative deep features by forcing the network to maximize inter-class variance and to minimize intra-class variance. Then, we feed the feature vectors to the density-based novelty detection algorithm, local outlier factor (LOF), to detect unknown intents. Experiments on two benchmark datasets show that our method can yield consistent improvements compared with the baseline methods.",
      "doi": "https://doi.org/10.18653/v1/p19-1548",
      "openalex_id": "https://openalex.org/W2952409498",
      "arxiv_id": "",
      "publication_date": "2019-01-01",
      "published": "2019-01-01",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The abstract focuses on text-based unknown-intent detection in dialogue systems and uses BiLSTM/LOF with no speech/audio modeling or Spoken Language Model framing, thus it does not meet the 'SLMs/FD-SLMs core' inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The abstract focuses on text-based unknown-intent detection in dialogue systems and uses BiLSTM/LOF with no speech/audio modeling or Spoken Language Model framing, thus it does not meet the 'SLMs/FD-SLMs core' inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on detecting unknown user intents using BiLSTM and novelty detection in dialogue systems, emphasizing textual intent classification rather than direct modeling of speech signals or spoken language models. It does not mention speech-level language modeling, full-duplex interaction, or synchronization aspects central to the inclusion criteria. Therefore, it does not meet the core criteria requiring spoken language modeling or speech as the primary modeling object and does not address full-duplex or synchronous interaction capabilities.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on detecting unknown user intents using BiLSTM and novelty detection in dialogue systems, emphasizing textual intent classification rather than direct modeling of speech signals or spoken language models. It does not mention speech-level language modeling, full-duplex interaction, or synchronization aspects central to the inclusion criteria. Therefore, it does not meet the core criteria requiring spoken language modeling or speech as the primary modeling object and does not address full-duplex or synchronous interaction capabilities.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Discovering New Intents via Constrained Deep Adaptive Clustering with Cluster Refinement",
    "abstract": "Identifying new user intents is an essential task in the dialogue system. However, it is hard to get satisfying clustering results since the definition of intents is strongly guided by prior knowledge. Existing methods incorporate prior knowledge by intensive feature engineering, which not only leads to overfitting but also makes it sensitive to the number of clusters. In this paper, we propose constrained deep adaptive clustering with cluster refinement (CDAC+), an end-to-end clustering method that can naturally incorporate pairwise constraints as prior knowledge to guide the clustering process. Moreover, we refine the clusters by forcing the model to learn from the high confidence assignments. After eliminating low confidence assignments, our approach is surprisingly insensitive to the number of clusters. Experimental results on the three benchmark datasets show that our method can yield significant improvements over strong baselines. 1",
    "metadata": {
      "title": "Discovering New Intents via Constrained Deep Adaptive Clustering with Cluster Refinement",
      "summary": "Identifying new user intents is an essential task in the dialogue system. However, it is hard to get satisfying clustering results since the definition of intents is strongly guided by prior knowledge. Existing methods incorporate prior knowledge by intensive feature engineering, which not only leads to overfitting but also makes it sensitive to the number of clusters. In this paper, we propose constrained deep adaptive clustering with cluster refinement (CDAC+), an end-to-end clustering method that can naturally incorporate pairwise constraints as prior knowledge to guide the clustering process. Moreover, we refine the clusters by forcing the model to learn from the high confidence assignments. After eliminating low confidence assignments, our approach is surprisingly insensitive to the number of clusters. Experimental results on the three benchmark datasets show that our method can yield significant improvements over strong baselines. 1",
      "abstract": "Identifying new user intents is an essential task in the dialogue system. However, it is hard to get satisfying clustering results since the definition of intents is strongly guided by prior knowledge. Existing methods incorporate prior knowledge by intensive feature engineering, which not only leads to overfitting but also makes it sensitive to the number of clusters. In this paper, we propose constrained deep adaptive clustering with cluster refinement (CDAC+), an end-to-end clustering method that can naturally incorporate pairwise constraints as prior knowledge to guide the clustering process. Moreover, we refine the clusters by forcing the model to learn from the high confidence assignments. After eliminating low confidence assignments, our approach is surprisingly insensitive to the number of clusters. Experimental results on the three benchmark datasets show that our method can yield significant improvements over strong baselines. 1",
      "doi": "https://doi.org/10.1609/aaai.v34i05.6353",
      "openalex_id": "https://openalex.org/W2998721586",
      "arxiv_id": "",
      "publication_date": "2020-04-03",
      "published": "2020-04-03",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item addresses end-to-end clustering for discovering intents in dialogue but does not treat spoken language models or speech as the core modeling target, making it primarily text/NLP-based rather than a Spoken Language Model study.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item addresses end-to-end clustering for discovering intents in dialogue but does not treat spoken language models or speech as the core modeling target, making it primarily text/NLP-based rather than a Spoken Language Model study.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper focuses on discovering new user intents via constrained deep adaptive clustering, which is more related to dialogue system intent clustering based on prior knowledge rather than spoken language modeling or full-duplex speech interaction. It does not address spoken language models or speech-level language modeling, nor does it mention full-duplex capabilities, synchronous interaction, or low-latency processing discussed in the inclusion criteria. Thus, it does not meet the core topic requirements related to spoken language modeling or evaluation facets specified, and is outside the scope of the inclusion.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper focuses on discovering new user intents via constrained deep adaptive clustering, which is more related to dialogue system intent clustering based on prior knowledge rather than spoken language modeling or full-duplex speech interaction. It does not address spoken language models or speech-level language modeling, nor does it mention full-duplex capabilities, synchronous interaction, or low-latency processing discussed in the inclusion criteria. Thus, it does not meet the core topic requirements related to spoken language modeling or evaluation facets specified, and is outside the scope of the inclusion.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Towards Building an Intelligent Chatbot for Customer Service",
    "abstract": "In recent years, intelligent chatbots have been widely used in the field of customer service. One of the key challenges for chatbots to maintain fluent dialogues with customers is how to respond at the appropriate time. However, most of the state-of-the-art chatbots follow the turn-by-turn interaction scheme. Such chatbots respond after each time when a customer sends an utterance, which in some cases leads to inappropriate responses and misleads the process of the dialogues. In this paper, we propose a multi-turn response triggering model (MRTM) to address this problem. MRTM is learned from large-scale human-human dialogues between the customers and the agents with a self-supervised learning scheme. It leverages the semantic matching relationships between the context and the response to train a semantic matching model and obtains the weights of the co-occurring utterances in the context through an asymmetrical self-attention mechanism. The weights are then used to determine whether the given context should be responded to. We conduct extensive experiments on two dialogue datasets collected from the real-world online customer service systems. Results show that MRTM outperforms the baselines by a large margin. Furthermore, we incorporate MRTM into DiDi's customer service chatbot. Based on the ability to identify the appropriate time to respond, the chatbot can incrementally aggregate the information across multiple utterances and make more intelligent responses at the appropriate time.",
    "metadata": {
      "title": "Towards Building an Intelligent Chatbot for Customer Service",
      "summary": "In recent years, intelligent chatbots have been widely used in the field of customer service. One of the key challenges for chatbots to maintain fluent dialogues with customers is how to respond at the appropriate time. However, most of the state-of-the-art chatbots follow the turn-by-turn interaction scheme. Such chatbots respond after each time when a customer sends an utterance, which in some cases leads to inappropriate responses and misleads the process of the dialogues. In this paper, we propose a multi-turn response triggering model (MRTM) to address this problem. MRTM is learned from large-scale human-human dialogues between the customers and the agents with a self-supervised learning scheme. It leverages the semantic matching relationships between the context and the response to train a semantic matching model and obtains the weights of the co-occurring utterances in the context through an asymmetrical self-attention mechanism. The weights are then used to determine whether the given context should be responded to. We conduct extensive experiments on two dialogue datasets collected from the real-world online customer service systems. Results show that MRTM outperforms the baselines by a large margin. Furthermore, we incorporate MRTM into DiDi's customer service chatbot. Based on the ability to identify the appropriate time to respond, the chatbot can incrementally aggregate the information across multiple utterances and make more intelligent responses at the appropriate time.",
      "abstract": "In recent years, intelligent chatbots have been widely used in the field of customer service. One of the key challenges for chatbots to maintain fluent dialogues with customers is how to respond at the appropriate time. However, most of the state-of-the-art chatbots follow the turn-by-turn interaction scheme. Such chatbots respond after each time when a customer sends an utterance, which in some cases leads to inappropriate responses and misleads the process of the dialogues. In this paper, we propose a multi-turn response triggering model (MRTM) to address this problem. MRTM is learned from large-scale human-human dialogues between the customers and the agents with a self-supervised learning scheme. It leverages the semantic matching relationships between the context and the response to train a semantic matching model and obtains the weights of the co-occurring utterances in the context through an asymmetrical self-attention mechanism. The weights are then used to determine whether the given context should be responded to. We conduct extensive experiments on two dialogue datasets collected from the real-world online customer service systems. Results show that MRTM outperforms the baselines by a large margin. Furthermore, we incorporate MRTM into DiDi's customer service chatbot. Based on the ability to identify the appropriate time to respond, the chatbot can incrementally aggregate the information across multiple utterances and make more intelligent responses at the appropriate time.",
      "doi": "https://doi.org/10.1145/3394486.3403390",
      "openalex_id": "https://openalex.org/W3080879265",
      "arxiv_id": "",
      "publication_date": "2020-08-20",
      "published": "2020-08-20",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "此項目專注於文本型多回合客戶服務對話的觸發與回應決策，未以語音序列或語音語言模型為核心，也未討論語音同步互動與語音建模評估，因此不符合納入條件。",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "此項目專注於文本型多回合客戶服務對話的觸發與回應決策，未以語音序列或語音語言模型為核心，也未討論語音同步互動與語音建模評估，因此不符合納入條件。",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper focuses on improving chatbot response timing in customer service dialogues, based primarily on text-based large-scale human-human dialogue datasets, without explicit emphasis on spoken language modeling or using speech/acoustic signals as core modeling objects; it does not discuss full-duplex spoken language models, speech–LLM or low-latency synchronous interaction involving speech signals, nor does it present speech-level modeling or evaluation metrics related to speech overlap or behavior arbitration. Therefore, it does not meet the inclusion criteria which require spoken language models or equivalent concepts with speech as the primary modeling object.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper focuses on improving chatbot response timing in customer service dialogues, based primarily on text-based large-scale human-human dialogue datasets, without explicit emphasis on spoken language modeling or using speech/acoustic signals as core modeling objects; it does not discuss full-duplex spoken language models, speech–LLM or low-latency synchronous interaction involving speech signals, nor does it present speech-level modeling or evaluation metrics related to speech overlap or behavior arbitration. Therefore, it does not meet the inclusion criteria which require spoken language models or equivalent concepts with speech as the primary modeling object.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Automatic Dialogue Summary Generation for Customer Service",
    "abstract": "Dialogue summarization extracts useful information from a dialogue. It helps people quickly capture the highlights of a dialogue without going through long and sometimes twisted utterances. For customer service, it saves human resources currently required to write dialogue summaries. A main challenge of dialogue summarization is to design a mechanism to ensure the logic, integrity, and correctness of the summaries. In this paper, we introduce auxiliary key point sequences to solve this problem. A key point sequence describes the logic of the summary. In our training procedure, a key point sequence acts as an auxiliary label. It helps the model learn the logic of the summary. In the prediction procedure, our model predicts the key point sequence first and then uses it to guide the prediction of the summary. Along with the auxiliary key point sequence, we propose a novel Leader-Writer network. The Leader net predicts the key point sequence, and the Writer net predicts the summary based on the decoded key point sequence. The Leader net ensures the summary is logical and integral. The Writer net focuses on generating fluent sentences. We test our model on customer service scenarios. The results show that our model outperforms other models not only on BLEU and ROUGE-L score but also on logic and integrity.",
    "metadata": {
      "title": "Automatic Dialogue Summary Generation for Customer Service",
      "summary": "Dialogue summarization extracts useful information from a dialogue. It helps people quickly capture the highlights of a dialogue without going through long and sometimes twisted utterances. For customer service, it saves human resources currently required to write dialogue summaries. A main challenge of dialogue summarization is to design a mechanism to ensure the logic, integrity, and correctness of the summaries. In this paper, we introduce auxiliary key point sequences to solve this problem. A key point sequence describes the logic of the summary. In our training procedure, a key point sequence acts as an auxiliary label. It helps the model learn the logic of the summary. In the prediction procedure, our model predicts the key point sequence first and then uses it to guide the prediction of the summary. Along with the auxiliary key point sequence, we propose a novel Leader-Writer network. The Leader net predicts the key point sequence, and the Writer net predicts the summary based on the decoded key point sequence. The Leader net ensures the summary is logical and integral. The Writer net focuses on generating fluent sentences. We test our model on customer service scenarios. The results show that our model outperforms other models not only on BLEU and ROUGE-L score but also on logic and integrity.",
      "abstract": "Dialogue summarization extracts useful information from a dialogue. It helps people quickly capture the highlights of a dialogue without going through long and sometimes twisted utterances. For customer service, it saves human resources currently required to write dialogue summaries. A main challenge of dialogue summarization is to design a mechanism to ensure the logic, integrity, and correctness of the summaries. In this paper, we introduce auxiliary key point sequences to solve this problem. A key point sequence describes the logic of the summary. In our training procedure, a key point sequence acts as an auxiliary label. It helps the model learn the logic of the summary. In the prediction procedure, our model predicts the key point sequence first and then uses it to guide the prediction of the summary. Along with the auxiliary key point sequence, we propose a novel Leader-Writer network. The Leader net predicts the key point sequence, and the Writer net predicts the summary based on the decoded key point sequence. The Leader net ensures the summary is logical and integral. The Writer net focuses on generating fluent sentences. We test our model on customer service scenarios. The results show that our model outperforms other models not only on BLEU and ROUGE-L score but also on logic and integrity.",
      "doi": "https://doi.org/10.1145/3292500.3330683",
      "openalex_id": "https://openalex.org/W2952890017",
      "arxiv_id": "",
      "publication_date": "2019-07-25",
      "published": "2019-07-25",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "此論文聚焦於文本對話摘要，未以語音訊號或語音模型為核心，亦未探討語音–文字混合架構或語音交互的評估，因此不符合以 Spoken Language Models 為核心的 inclusion 要求，即便在過去三年公開亦不符合主題與評估框架。",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "此論文聚焦於文本對話摘要，未以語音訊號或語音模型為核心，亦未探討語音–文字混合架構或語音交互的評估，因此不符合以 Spoken Language Models 為核心的 inclusion 要求，即便在過去三年公開亦不符合主題與評估框架。",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on dialogue summarization for customer service using a model that processes dialogue text to generate summaries; it does not mention spoken language modeling, speech signals as core modeling objects, nor does it involve speech-LLM or full-duplex spoken language model aspects as per the inclusion criteria. Furthermore, the abstract does not describe dealing with speech audio, speech modeling, or the interactive speech behaviors and evaluation metrics required; it centers on text-based dialogue summarization evaluation metrics (BLEU, ROUGE-L). Thus, it does not meet the inclusion criteria and falls under exclusion for focusing on text-based NLP without speech modeling innovation.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on dialogue summarization for customer service using a model that processes dialogue text to generate summaries; it does not mention spoken language modeling, speech signals as core modeling objects, nor does it involve speech-LLM or full-duplex spoken language model aspects as per the inclusion criteria. Furthermore, the abstract does not describe dealing with speech audio, speech modeling, or the interactive speech behaviors and evaluation metrics required; it centers on text-based dialogue summarization evaluation metrics (BLEU, ROUGE-L). Thus, it does not meet the inclusion criteria and falls under exclusion for focusing on text-based NLP without speech modeling innovation.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "DialogueCSE: Dialogue-based Contrastive Learning of Sentence Embeddings",
    "abstract": "Learning sentence embeddings from dialogues has drawn increasing attention due to its low annotation cost and high domain adaptability. Conventional approaches employ the siamese-network for this task, which obtains the sentence embeddings through modeling the context-response semantic relevance by applying a feed-forward network on top of the sentence encoders. However, as the semantic textual similarity is commonly measured through the element-wise distance metrics (e.g. cosine and L2 distance), such architecture yields a large gap between training and evaluating. In this paper, we propose DialogueCSE, a dialogue-based contrastive learning approach to tackle this issue. DialogueCSE first introduces a novel matching-guided embedding (MGE) mechanism, which generates a context-aware embedding for each candidate response embedding (i.e. the context-free embedding) according to the guidance of the multi-turn context-response matching matrices. Then it pairs each context-aware embedding with its corresponding context-free embedding and finally minimizes the contrastive loss across all pairs. We evaluate our model on three multi-turn dialogue datasets: the Microsoft Dialogue Corpus, the Jing Dong Dialogue Corpus, and the E-commerce Dialogue Corpus. Evaluation results show that our approach significantly outperforms the baselines across all three datasets in terms of MAP and Spearman’s correlation measures, demonstrating its effectiveness. Further quantitative experiments show that our approach achieves better performance when leveraging more dialogue context and remains robust when less training data is provided.",
    "metadata": {
      "title": "DialogueCSE: Dialogue-based Contrastive Learning of Sentence Embeddings",
      "summary": "Learning sentence embeddings from dialogues has drawn increasing attention due to its low annotation cost and high domain adaptability. Conventional approaches employ the siamese-network for this task, which obtains the sentence embeddings through modeling the context-response semantic relevance by applying a feed-forward network on top of the sentence encoders. However, as the semantic textual similarity is commonly measured through the element-wise distance metrics (e.g. cosine and L2 distance), such architecture yields a large gap between training and evaluating. In this paper, we propose DialogueCSE, a dialogue-based contrastive learning approach to tackle this issue. DialogueCSE first introduces a novel matching-guided embedding (MGE) mechanism, which generates a context-aware embedding for each candidate response embedding (i.e. the context-free embedding) according to the guidance of the multi-turn context-response matching matrices. Then it pairs each context-aware embedding with its corresponding context-free embedding and finally minimizes the contrastive loss across all pairs. We evaluate our model on three multi-turn dialogue datasets: the Microsoft Dialogue Corpus, the Jing Dong Dialogue Corpus, and the E-commerce Dialogue Corpus. Evaluation results show that our approach significantly outperforms the baselines across all three datasets in terms of MAP and Spearman’s correlation measures, demonstrating its effectiveness. Further quantitative experiments show that our approach achieves better performance when leveraging more dialogue context and remains robust when less training data is provided.",
      "abstract": "Learning sentence embeddings from dialogues has drawn increasing attention due to its low annotation cost and high domain adaptability. Conventional approaches employ the siamese-network for this task, which obtains the sentence embeddings through modeling the context-response semantic relevance by applying a feed-forward network on top of the sentence encoders. However, as the semantic textual similarity is commonly measured through the element-wise distance metrics (e.g. cosine and L2 distance), such architecture yields a large gap between training and evaluating. In this paper, we propose DialogueCSE, a dialogue-based contrastive learning approach to tackle this issue. DialogueCSE first introduces a novel matching-guided embedding (MGE) mechanism, which generates a context-aware embedding for each candidate response embedding (i.e. the context-free embedding) according to the guidance of the multi-turn context-response matching matrices. Then it pairs each context-aware embedding with its corresponding context-free embedding and finally minimizes the contrastive loss across all pairs. We evaluate our model on three multi-turn dialogue datasets: the Microsoft Dialogue Corpus, the Jing Dong Dialogue Corpus, and the E-commerce Dialogue Corpus. Evaluation results show that our approach significantly outperforms the baselines across all three datasets in terms of MAP and Spearman’s correlation measures, demonstrating its effectiveness. Further quantitative experiments show that our approach achieves better performance when leveraging more dialogue context and remains robust when less training data is provided.",
      "doi": "https://doi.org/10.18653/v1/2021.emnlp-main.185",
      "openalex_id": "https://openalex.org/W3202125623",
      "arxiv_id": "",
      "publication_date": "2021-01-01",
      "published": "2021-01-01",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This work focuses on text-based dialogue embeddings and contrastive learning without any speech input or speech-language modeling, so it does not meet the Spoken Language Models core requirement and falls under the exclusion of non-speech-centric dialogue or speech processing.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "This work focuses on text-based dialogue embeddings and contrastive learning without any speech input or speech-language modeling, so it does not meet the Spoken Language Models core requirement and falls under the exclusion of non-speech-centric dialogue or speech processing.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on learning sentence embeddings from dialogues using contrastive learning, addressing modeling and evaluation of sentence semantic similarity in dialogue datasets. However, it does not mention spoken language models, speech language models, or language modeling of speech, nor does it treat speech or acoustic signals as core modeling elements. The focus is on textual sentence embeddings from dialogues rather than speech-based modeling or full-duplex spoken language interaction. Therefore, it does not meet the inclusion criteria of a core focus on spoken language models or speech-based modeling and reasoning.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on learning sentence embeddings from dialogues using contrastive learning, addressing modeling and evaluation of sentence semantic similarity in dialogue datasets. However, it does not mention spoken language models, speech language models, or language modeling of speech, nor does it treat speech or acoustic signals as core modeling elements. The focus is on textual sentence embeddings from dialogues rather than speech-based modeling or full-duplex spoken language interaction. Therefore, it does not meet the inclusion criteria of a core focus on spoken language models or speech-based modeling and reasoning.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Spoken language interaction with robots: Recommendations for future research",
    "abstract": "With robotics rapidly advancing, more effective human–robot interaction is increasingly needed to realize the full potential of robots for society. While spoken language must be part of the solution, our ability to provide spoken language interaction capabilities is still very limited. In this article, based on the report of an interdisciplinary workshop convened by the National Science Foundation, we identify key scientific and engineering advances needed to enable effective spoken language interaction with robotics. We make 25 recommendations, involving eight general themes: putting human needs first, better modeling the social and interactive aspects of language, improving robustness, creating new methods for rapid adaptation, better integrating speech and language with other communication modalities, giving speech and language components access to rich representations of the robot’s current knowledge and state, making all components operate in real time, and improving research infrastructure and resources. Research and development that prioritizes these topics will, we believe, provide a solid foundation for the creation of speech-capable robots that are easy and effective for humans to work with.",
    "metadata": {
      "title": "Spoken language interaction with robots: Recommendations for future research",
      "summary": "With robotics rapidly advancing, more effective human–robot interaction is increasingly needed to realize the full potential of robots for society. While spoken language must be part of the solution, our ability to provide spoken language interaction capabilities is still very limited. In this article, based on the report of an interdisciplinary workshop convened by the National Science Foundation, we identify key scientific and engineering advances needed to enable effective spoken language interaction with robotics. We make 25 recommendations, involving eight general themes: putting human needs first, better modeling the social and interactive aspects of language, improving robustness, creating new methods for rapid adaptation, better integrating speech and language with other communication modalities, giving speech and language components access to rich representations of the robot’s current knowledge and state, making all components operate in real time, and improving research infrastructure and resources. Research and development that prioritizes these topics will, we believe, provide a solid foundation for the creation of speech-capable robots that are easy and effective for humans to work with.",
      "abstract": "With robotics rapidly advancing, more effective human–robot interaction is increasingly needed to realize the full potential of robots for society. While spoken language must be part of the solution, our ability to provide spoken language interaction capabilities is still very limited. In this article, based on the report of an interdisciplinary workshop convened by the National Science Foundation, we identify key scientific and engineering advances needed to enable effective spoken language interaction with robotics. We make 25 recommendations, involving eight general themes: putting human needs first, better modeling the social and interactive aspects of language, improving robustness, creating new methods for rapid adaptation, better integrating speech and language with other communication modalities, giving speech and language components access to rich representations of the robot’s current knowledge and state, making all components operate in real time, and improving research infrastructure and resources. Research and development that prioritizes these topics will, we believe, provide a solid foundation for the creation of speech-capable robots that are easy and effective for humans to work with.",
      "doi": "https://doi.org/10.1016/j.csl.2021.101255",
      "openalex_id": "https://openalex.org/W3182074706",
      "arxiv_id": "",
      "publication_date": "2021-07-02",
      "published": "2021-07-02",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This item focuses on high-level recommendations for spoken language interaction with robots and NSF workshop outcomes rather than on core Spoken Language Models (SLMs) or speech-model architectures/training/evaluation as required by inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "This item focuses on high-level recommendations for spoken language interaction with robots and NSF workshop outcomes rather than on core Spoken Language Models (SLMs) or speech-model architectures/training/evaluation as required by inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The abstract discusses spoken language interaction with robots and emphasizes enhancing human-robot spoken communication capabilities, but it does not explicitly focus on Spoken Language Models (SLMs) or Full-Duplex Spoken Language Models (FD-SLMs) as core modeling subjects, nor does it detail model architectures, training, or evaluation from the speech language modeling perspective, and lacks mention of synchronous interaction features such as cognitive parallelism or specific model-based evaluations; therefore, it does not fully meet the inclusion criteria or avoid exclusions related to lack of modeling and evaluation focus.",
      "evaluation": 2
    },
    "round-A_JuniorMini_reasoning": "The abstract discusses spoken language interaction with robots and emphasizes enhancing human-robot spoken communication capabilities, but it does not explicitly focus on Spoken Language Models (SLMs) or Full-Duplex Spoken Language Models (FD-SLMs) as core modeling subjects, nor does it detail model architectures, training, or evaluation from the speech language modeling perspective, and lacks mention of synchronous interaction features such as cognitive parallelism or specific model-based evaluations; therefore, it does not fully meet the inclusion criteria or avoid exclusions related to lack of modeling and evaluation focus.",
    "round-A_JuniorMini_evaluation": 2,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:2)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "librosa: Audio and Music Signal Analysis in Python",
    "abstract": "This document describes version 0.4.0 of librosa: a Python package for audio and music signal processing. At a high level, librosa provides implementations of a variety of common functions used throughout the field of music information retrieval. In this document, a brief overview of the library's functionality is provided, along with explanations of the design goals, software development practices, and notational conventions.",
    "metadata": {
      "title": "librosa: Audio and Music Signal Analysis in Python",
      "summary": "This document describes version 0.4.0 of librosa: a Python package for audio and music signal processing. At a high level, librosa provides implementations of a variety of common functions used throughout the field of music information retrieval. In this document, a brief overview of the library's functionality is provided, along with explanations of the design goals, software development practices, and notational conventions.",
      "abstract": "This document describes version 0.4.0 of librosa: a Python package for audio and music signal processing. At a high level, librosa provides implementations of a variety of common functions used throughout the field of music information retrieval. In this document, a brief overview of the library's functionality is provided, along with explanations of the design goals, software development practices, and notational conventions.",
      "doi": "https://doi.org/10.25080/majora-7b98e3ed-003",
      "openalex_id": "https://openalex.org/W2191779130",
      "arxiv_id": "",
      "publication_date": "2015-01-01",
      "published": "2015-01-01",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item describes a Python library for audio and music signal processing (librosa) and does not address Spoken Language Models, speech-language modeling architecture, training, or evaluation, nor anything about full-duplex interaction or conversational behavior; it also does not meet the specified inclusion criteria and does not fit the exclusion criteria, so it should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item describes a Python library for audio and music signal processing (librosa) and does not address Spoken Language Models, speech-language modeling architecture, training, or evaluation, nor anything about full-duplex interaction or conversational behavior; it also does not meet the specified inclusion criteria and does not fit the exclusion criteria, so it should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The article describes a Python package for audio and music signal analysis (librosa), focusing on audio processing functionalities without mentioning spoken language models (SLMs), speech language models, or related architectures involving full-duplex interaction, model training, or evaluation in speech language modeling contexts. It does not meet the inclusion criteria of focusing on spoken language modeling nor addresses synchronization, real-time interaction, or behavior arbitration.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The article describes a Python package for audio and music signal analysis (librosa), focusing on audio processing functionalities without mentioning spoken language models (SLMs), speech language models, or related architectures involving full-duplex interaction, model training, or evaluation in speech language modeling contexts. It does not meet the inclusion criteria of focusing on spoken language modeling nor addresses synchronization, real-time interaction, or behavior arbitration.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Glove: Global Vectors for Word Representation",
    "abstract": "Recent methods for learning vector space representations of words have succeeded in capturing fine-grained semantic and syntactic regularities using vector arithmetic, but the origin of these regularities has remained opaque. We analyze and make explicit the model properties needed for such regularities to emerge in word vectors. The result is a new global logbilinear regression model that combines the advantages of the two major model families in the literature: global matrix factorization and local context window methods. Our model efficiently leverages statistical information by training only on the nonzero elements in a word-word cooccurrence matrix, rather than on the entire sparse matrix or on individual context windows in a large corpus. The model produces a vector space with meaningful substructure, as evidenced by its performance of 75% on a recent word analogy task. It also outperforms related models on similarity tasks and named entity recognition.",
    "metadata": {
      "title": "Glove: Global Vectors for Word Representation",
      "summary": "Recent methods for learning vector space representations of words have succeeded in capturing fine-grained semantic and syntactic regularities using vector arithmetic, but the origin of these regularities has remained opaque. We analyze and make explicit the model properties needed for such regularities to emerge in word vectors. The result is a new global logbilinear regression model that combines the advantages of the two major model families in the literature: global matrix factorization and local context window methods. Our model efficiently leverages statistical information by training only on the nonzero elements in a word-word cooccurrence matrix, rather than on the entire sparse matrix or on individual context windows in a large corpus. The model produces a vector space with meaningful substructure, as evidenced by its performance of 75% on a recent word analogy task. It also outperforms related models on similarity tasks and named entity recognition.",
      "abstract": "Recent methods for learning vector space representations of words have succeeded in capturing fine-grained semantic and syntactic regularities using vector arithmetic, but the origin of these regularities has remained opaque. We analyze and make explicit the model properties needed for such regularities to emerge in word vectors. The result is a new global logbilinear regression model that combines the advantages of the two major model families in the literature: global matrix factorization and local context window methods. Our model efficiently leverages statistical information by training only on the nonzero elements in a word-word cooccurrence matrix, rather than on the entire sparse matrix or on individual context windows in a large corpus. The model produces a vector space with meaningful substructure, as evidenced by its performance of 75% on a recent word analogy task. It also outperforms related models on similarity tasks and named entity recognition.",
      "doi": "https://doi.org/10.3115/v1/d14-1162",
      "openalex_id": "https://openalex.org/W2250539671",
      "arxiv_id": "",
      "publication_date": "2014-01-01",
      "published": "2014-01-01",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item is about GloVe word embeddings, which are purely text-based and not centered on Spoken Language Models or speech-language modeling; it also predates the last 3 years and lacks any speech-focused evaluation or architecture discussion, thus failing all inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item is about GloVe word embeddings, which are purely text-based and not centered on Spoken Language Models or speech-language modeling; it also predates the last 3 years and lacks any speech-focused evaluation or architecture discussion, thus failing all inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on word representation in vector space for text-based language modeling rather than spoken language modeling; it does not address speech as core modeling input or discuss spoken language models, full-duplex interaction, or related synchronization and evaluation metrics, and it was not published within the last three years.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on word representation in vector space for text-based language modeling rather than spoken language modeling; it does not address speech as core modeling input or discuss spoken language models, full-duplex interaction, or related synchronization and evaluation metrics, and it was not published within the last three years.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Turn-taking in Conversational Systems and Human-Robot Interaction: A Review",
    "abstract": "The taking of turns is a fundamental aspect of dialogue. Since it is difficult to speak and listen at the same time, the participants need to coordinate who is currently speaking and when the next person can start to speak. Humans are very good at this coordination, and typically achieve fluent turn-taking with very small gaps and little overlap. Conversational systems (including voice assistants and social robots), on the other hand, typically have problems with frequent interruptions and long response delays, which has called for a substantial body of research on how to improve turn-taking in conversational systems. In this review article, we provide an overview of this research and give directions for future research. First, we provide a theoretical background of the linguistic research tradition on turn-taking and some of the fundamental concepts in theories of turn-taking. We also provide an extensive review of multi-modal cues (including verbal cues, prosody, breathing, gaze and gestures) that have been found to facilitate the coordination of turn-taking in human-human interaction, and which can be utilised for turn-taking in conversational systems. After this, we review work that has been done on modelling turn-taking, including end-of-turn detection, handling of user interruptions, generation of turn-taking cues, and multi-party human-robot interaction. Finally, we identify key areas where more research is needed to achieve fluent turn-taking in spoken interaction between man and machine.",
    "metadata": {
      "title": "Turn-taking in Conversational Systems and Human-Robot Interaction: A Review",
      "summary": "The taking of turns is a fundamental aspect of dialogue. Since it is difficult to speak and listen at the same time, the participants need to coordinate who is currently speaking and when the next person can start to speak. Humans are very good at this coordination, and typically achieve fluent turn-taking with very small gaps and little overlap. Conversational systems (including voice assistants and social robots), on the other hand, typically have problems with frequent interruptions and long response delays, which has called for a substantial body of research on how to improve turn-taking in conversational systems. In this review article, we provide an overview of this research and give directions for future research. First, we provide a theoretical background of the linguistic research tradition on turn-taking and some of the fundamental concepts in theories of turn-taking. We also provide an extensive review of multi-modal cues (including verbal cues, prosody, breathing, gaze and gestures) that have been found to facilitate the coordination of turn-taking in human-human interaction, and which can be utilised for turn-taking in conversational systems. After this, we review work that has been done on modelling turn-taking, including end-of-turn detection, handling of user interruptions, generation of turn-taking cues, and multi-party human-robot interaction. Finally, we identify key areas where more research is needed to achieve fluent turn-taking in spoken interaction between man and machine.",
      "abstract": "The taking of turns is a fundamental aspect of dialogue. Since it is difficult to speak and listen at the same time, the participants need to coordinate who is currently speaking and when the next person can start to speak. Humans are very good at this coordination, and typically achieve fluent turn-taking with very small gaps and little overlap. Conversational systems (including voice assistants and social robots), on the other hand, typically have problems with frequent interruptions and long response delays, which has called for a substantial body of research on how to improve turn-taking in conversational systems. In this review article, we provide an overview of this research and give directions for future research. First, we provide a theoretical background of the linguistic research tradition on turn-taking and some of the fundamental concepts in theories of turn-taking. We also provide an extensive review of multi-modal cues (including verbal cues, prosody, breathing, gaze and gestures) that have been found to facilitate the coordination of turn-taking in human-human interaction, and which can be utilised for turn-taking in conversational systems. After this, we review work that has been done on modelling turn-taking, including end-of-turn detection, handling of user interruptions, generation of turn-taking cues, and multi-party human-robot interaction. Finally, we identify key areas where more research is needed to achieve fluent turn-taking in spoken interaction between man and machine.",
      "doi": "https://doi.org/10.1016/j.csl.2020.101178",
      "openalex_id": "https://openalex.org/W3112188842",
      "arxiv_id": "",
      "publication_date": "2020-12-16",
      "published": "2020-12-16",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "此項目為關於人機協作的語音-對話系統輪流發話與多模態線索的綜述，未以 Spoken Language Models 或 Full-Duplex SLM 為核心建模對象，且是否在過去3年內發表不明，因此不符合核心納入條件。",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "此項目為關於人機協作的語音-對話系統輪流發話與多模態線索的綜述，未以 Spoken Language Models 或 Full-Duplex SLM 為核心建模對象，且是否在過去3年內發表不明，因此不符合核心納入條件。",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The article reviews turn-taking in conversational systems and human-robot interaction but does not explicitly focus on Spoken Language Models or speech language modeling architectures, training, or evaluation; it mainly addresses linguistic and interaction aspects of turn-taking rather than modeling speech representations or full-duplex spoken language models, and there is no mention of model architectures or benchmarks as required by the inclusion criteria.",
      "evaluation": 2
    },
    "round-A_JuniorMini_reasoning": "The article reviews turn-taking in conversational systems and human-robot interaction but does not explicitly focus on Spoken Language Models or speech language modeling architectures, training, or evaluation; it mainly addresses linguistic and interaction aspects of turn-taking rather than modeling speech representations or full-duplex spoken language models, and there is no mention of model architectures or benchmarks as required by the inclusion criteria.",
    "round-A_JuniorMini_evaluation": 2,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:2)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "FixMatch: Simplifying Semi-Supervised Learning with Consistency and Confidence",
    "abstract": "Semi-supervised learning (SSL) provides an effective means of leveraging unlabeled data to improve a model's performance. In this paper, we demonstrate the power of a simple combination of two common SSL methods: consistency regularization and pseudo-labeling. Our algorithm, FixMatch, first generates pseudo-labels using the model's predictions on weakly-augmented unlabeled images. For a given image, the pseudo-label is only retained if the model produces a high-confidence prediction. The model is then trained to predict the pseudo-label when fed a strongly-augmented version of the same image. Despite its simplicity, we show that FixMatch achieves state-of-the-art performance across a variety of standard semi-supervised learning benchmarks, including 94.93% accuracy on CIFAR-10 with 250 labels and 88.61% accuracy with 40 -- just 4 labels per class. Since FixMatch bears many similarities to existing SSL methods that achieve worse performance, we carry out an extensive ablation study to tease apart the experimental factors that are most important to FixMatch's success. We make our code available at https://github.com/google-research/fixmatch.",
    "metadata": {
      "title": "FixMatch: Simplifying Semi-Supervised Learning with Consistency and Confidence",
      "summary": "Semi-supervised learning (SSL) provides an effective means of leveraging unlabeled data to improve a model's performance. In this paper, we demonstrate the power of a simple combination of two common SSL methods: consistency regularization and pseudo-labeling. Our algorithm, FixMatch, first generates pseudo-labels using the model's predictions on weakly-augmented unlabeled images. For a given image, the pseudo-label is only retained if the model produces a high-confidence prediction. The model is then trained to predict the pseudo-label when fed a strongly-augmented version of the same image. Despite its simplicity, we show that FixMatch achieves state-of-the-art performance across a variety of standard semi-supervised learning benchmarks, including 94.93% accuracy on CIFAR-10 with 250 labels and 88.61% accuracy with 40 -- just 4 labels per class. Since FixMatch bears many similarities to existing SSL methods that achieve worse performance, we carry out an extensive ablation study to tease apart the experimental factors that are most important to FixMatch's success. We make our code available at https://github.com/google-research/fixmatch.",
      "abstract": "Semi-supervised learning (SSL) provides an effective means of leveraging unlabeled data to improve a model's performance. In this paper, we demonstrate the power of a simple combination of two common SSL methods: consistency regularization and pseudo-labeling. Our algorithm, FixMatch, first generates pseudo-labels using the model's predictions on weakly-augmented unlabeled images. For a given image, the pseudo-label is only retained if the model produces a high-confidence prediction. The model is then trained to predict the pseudo-label when fed a strongly-augmented version of the same image. Despite its simplicity, we show that FixMatch achieves state-of-the-art performance across a variety of standard semi-supervised learning benchmarks, including 94.93% accuracy on CIFAR-10 with 250 labels and 88.61% accuracy with 40 -- just 4 labels per class. Since FixMatch bears many similarities to existing SSL methods that achieve worse performance, we carry out an extensive ablation study to tease apart the experimental factors that are most important to FixMatch's success. We make our code available at https://github.com/google-research/fixmatch.",
      "doi": "https://doi.org/10.48550/arxiv.2001.07685",
      "openalex_id": "https://openalex.org/W3001197829",
      "arxiv_id": "",
      "publication_date": "2020-01-21",
      "published": "2020-01-21",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The paper FixMatch focuses on image-based semi-supervised learning and does not address Spoken Language Models or any speech-centric modeling or evaluation, thus it fails the inclusion criteria and meets none of the exclusion criteria requirements for speech interaction topics.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The paper FixMatch focuses on image-based semi-supervised learning and does not address Spoken Language Models or any speech-centric modeling or evaluation, thus it fails the inclusion criteria and meets none of the exclusion criteria requirements for speech interaction topics.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper focuses on semi-supervised learning methods for image data, specifically leveraging unlabeled image data for classification tasks such as CIFAR-10. It does not discuss spoken language models, full-duplex spoken language modeling, or any speech-related modeling or evaluation. The study does not fit the inclusion criteria of focusing on spoken language models or speech as the primary modeling target, nor does it address synchronization, turn-taking, cognitive parallelism, or related aspects. Therefore, it fails to meet the inclusion criteria and does not provide relevant content for the target topic.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper focuses on semi-supervised learning methods for image data, specifically leveraging unlabeled image data for classification tasks such as CIFAR-10. It does not discuss spoken language models, full-duplex spoken language modeling, or any speech-related modeling or evaluation. The study does not fit the inclusion criteria of focusing on spoken language models or speech as the primary modeling target, nor does it address synchronization, turn-taking, cognitive parallelism, or related aspects. Therefore, it fails to meet the inclusion criteria and does not provide relevant content for the target topic.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Unsupervised Data Augmentation for Consistency Training",
    "abstract": "Semi-supervised learning lately has shown much promise in improving deep learning models when labeled data is scarce. Common among recent approaches is the use of consistency training on a large amount of unlabeled data to constrain model predictions to be invariant to input noise. In this work, we present a new perspective on how to effectively noise unlabeled examples and argue that the quality of noising, specifically those produced by advanced data augmentation methods, plays a crucial role in semi-supervised learning. By substituting simple noising operations with advanced data augmentation methods such as RandAugment and back-translation, our method brings substantial improvements across six language and three vision tasks under the same consistency training framework. On the IMDb text classification dataset, with only 20 labeled examples, our method achieves an error rate of 4.20, outperforming the state-of-the-art model trained on 25,000 labeled examples. On a standard semi-supervised learning benchmark, CIFAR-10, our method outperforms all previous approaches and achieves an error rate of 5.43 with only 250 examples. Our method also combines well with transfer learning, e.g., when finetuning from BERT, and yields improvements in high-data regime, such as ImageNet, whether when there is only 10% labeled data or when a full labeled set with 1.3M extra unlabeled examples is used. Code is available at https://github.com/google-research/uda.",
    "metadata": {
      "title": "Unsupervised Data Augmentation for Consistency Training",
      "summary": "Semi-supervised learning lately has shown much promise in improving deep learning models when labeled data is scarce. Common among recent approaches is the use of consistency training on a large amount of unlabeled data to constrain model predictions to be invariant to input noise. In this work, we present a new perspective on how to effectively noise unlabeled examples and argue that the quality of noising, specifically those produced by advanced data augmentation methods, plays a crucial role in semi-supervised learning. By substituting simple noising operations with advanced data augmentation methods such as RandAugment and back-translation, our method brings substantial improvements across six language and three vision tasks under the same consistency training framework. On the IMDb text classification dataset, with only 20 labeled examples, our method achieves an error rate of 4.20, outperforming the state-of-the-art model trained on 25,000 labeled examples. On a standard semi-supervised learning benchmark, CIFAR-10, our method outperforms all previous approaches and achieves an error rate of 5.43 with only 250 examples. Our method also combines well with transfer learning, e.g., when finetuning from BERT, and yields improvements in high-data regime, such as ImageNet, whether when there is only 10% labeled data or when a full labeled set with 1.3M extra unlabeled examples is used. Code is available at https://github.com/google-research/uda.",
      "abstract": "Semi-supervised learning lately has shown much promise in improving deep learning models when labeled data is scarce. Common among recent approaches is the use of consistency training on a large amount of unlabeled data to constrain model predictions to be invariant to input noise. In this work, we present a new perspective on how to effectively noise unlabeled examples and argue that the quality of noising, specifically those produced by advanced data augmentation methods, plays a crucial role in semi-supervised learning. By substituting simple noising operations with advanced data augmentation methods such as RandAugment and back-translation, our method brings substantial improvements across six language and three vision tasks under the same consistency training framework. On the IMDb text classification dataset, with only 20 labeled examples, our method achieves an error rate of 4.20, outperforming the state-of-the-art model trained on 25,000 labeled examples. On a standard semi-supervised learning benchmark, CIFAR-10, our method outperforms all previous approaches and achieves an error rate of 5.43 with only 250 examples. Our method also combines well with transfer learning, e.g., when finetuning from BERT, and yields improvements in high-data regime, such as ImageNet, whether when there is only 10% labeled data or when a full labeled set with 1.3M extra unlabeled examples is used. Code is available at https://github.com/google-research/uda.",
      "doi": "https://doi.org/10.48550/arxiv.1904.12848",
      "openalex_id": "https://openalex.org/W2962369866",
      "arxiv_id": "",
      "publication_date": "2019-04-29",
      "published": "2019-04-29",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This item does not center on Spoken Language Models or speech-based modeling; it addresses general unsupervised data augmentation for semi-supervised learning in NLP/vision and is not within the past 3 years, thus not meeting the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "This item does not center on Spoken Language Models or speech-based modeling; it addresses general unsupervised data augmentation for semi-supervised learning in NLP/vision and is not within the past 3 years, thus not meeting the inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on semi-supervised learning and data augmentation methods for text and vision tasks, specifically mentioning datasets like IMDb text classification and CIFAR-10, with no mention of spoken language models, acoustic signals, or full-duplex spoken interaction modeling. It does not address speech as the primary modeling object or include aspects such as synchronization, cognitive parallelism, or real-time interaction relevant to the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on semi-supervised learning and data augmentation methods for text and vision tasks, specifically mentioning datasets like IMDb text classification and CIFAR-10, with no mention of spoken language models, acoustic signals, or full-duplex spoken interaction modeling. It does not address speech as the primary modeling object or include aspects such as synchronization, cognitive parallelism, or real-time interaction relevant to the inclusion criteria.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Coupled Context Modeling for Deep Chit-Chat",
    "abstract": "To have automatic conversations between human and computer is regarded as one of the most hardcore problems in computer science. Conversational systems are of growing importance due to their promising potentials and commercial values as virtual assistants and chatbots. To build such systems with adequate intelligence is challenging, and requires abundant resources including an acquisition of big conversational data and interdisciplinary techniques, such as content analysis, text mining, and retrieval. The arrival of big data era reveals the feasibility to create a conversational system empowered by data-driven approaches. Now we are able to collect an extremely large number of human-human conversations on Web, and organize them to launch human-computer conversational systems. Given a human issued utterance, i.e., a query, a conversational system will search for appropriate responses, conduct relevance ranking using contexts information, and then output the highly relevant result. In this paper, we propose a novel context modeling framework with end-to-end neural networks for human-computer conversational systems. The proposed model is general and unified. In the experiments, we demonstrate the effectiveness of the proposed model for human-computer conversations using [email protected], MAP, nDCG, and MRR metrics.",
    "metadata": {
      "title": "Coupled Context Modeling for Deep Chit-Chat",
      "summary": "To have automatic conversations between human and computer is regarded as one of the most hardcore problems in computer science. Conversational systems are of growing importance due to their promising potentials and commercial values as virtual assistants and chatbots. To build such systems with adequate intelligence is challenging, and requires abundant resources including an acquisition of big conversational data and interdisciplinary techniques, such as content analysis, text mining, and retrieval. The arrival of big data era reveals the feasibility to create a conversational system empowered by data-driven approaches. Now we are able to collect an extremely large number of human-human conversations on Web, and organize them to launch human-computer conversational systems. Given a human issued utterance, i.e., a query, a conversational system will search for appropriate responses, conduct relevance ranking using contexts information, and then output the highly relevant result. In this paper, we propose a novel context modeling framework with end-to-end neural networks for human-computer conversational systems. The proposed model is general and unified. In the experiments, we demonstrate the effectiveness of the proposed model for human-computer conversations using [email protected], MAP, nDCG, and MRR metrics.",
      "abstract": "To have automatic conversations between human and computer is regarded as one of the most hardcore problems in computer science. Conversational systems are of growing importance due to their promising potentials and commercial values as virtual assistants and chatbots. To build such systems with adequate intelligence is challenging, and requires abundant resources including an acquisition of big conversational data and interdisciplinary techniques, such as content analysis, text mining, and retrieval. The arrival of big data era reveals the feasibility to create a conversational system empowered by data-driven approaches. Now we are able to collect an extremely large number of human-human conversations on Web, and organize them to launch human-computer conversational systems. Given a human issued utterance, i.e., a query, a conversational system will search for appropriate responses, conduct relevance ranking using contexts information, and then output the highly relevant result. In this paper, we propose a novel context modeling framework with end-to-end neural networks for human-computer conversational systems. The proposed model is general and unified. In the experiments, we demonstrate the effectiveness of the proposed model for human-computer conversations using [email protected], MAP, nDCG, and MRR metrics.",
      "doi": "https://doi.org/10.1145/3219819.3220045",
      "openalex_id": "https://openalex.org/W2809210859",
      "arxiv_id": "",
      "publication_date": "2018-07-19",
      "published": "2018-07-19",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The paper addresses text-based deep chit-chat and end-to-end context modeling for human-computer conversations, not Spoken Language Models or speech-centric architecture/assessment, thus failing the core inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The paper addresses text-based deep chit-chat and end-to-end context modeling for human-computer conversations, not Spoken Language Models or speech-centric architecture/assessment, thus failing the core inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper focuses on conversational systems using deep learning and data-driven approaches for human-computer chit-chat, but it centers on text-based dialogue modeling and retrieval rather than spoken language modeling or speech as the core modeling object. The abstract lacks any mention of spoken language models, end-to-end speech architectures, or synchronization for full-duplex spoken interaction. The evaluation metrics indicated relate to ranking and retrieval relevance without addressing speech-specific issues like real-time interaction, overlap, or cognitive parallelism. Therefore, it does not meet the inclusion criteria centered on spoken language models and full-duplex interaction and falls within exclusion criteria for focusing on text-based dialog modeling without speech modeling innovation.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper focuses on conversational systems using deep learning and data-driven approaches for human-computer chit-chat, but it centers on text-based dialogue modeling and retrieval rather than spoken language modeling or speech as the core modeling object. The abstract lacks any mention of spoken language models, end-to-end speech architectures, or synchronization for full-duplex spoken interaction. The evaluation metrics indicated relate to ranking and retrieval relevance without addressing speech-specific issues like real-time interaction, overlap, or cognitive parallelism. Therefore, it does not meet the inclusion criteria centered on spoken language models and full-duplex interaction and falls within exclusion criteria for focusing on text-based dialog modeling without speech modeling innovation.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Learning Modality-Specific Representations with Self-Supervised Multi-Task Learning for Multimodal Sentiment Analysis",
    "abstract": "Representation Learning is a significant and challenging task in multimodal learning. Effective modality representations should contain two parts of characteristics: the consistency and the difference. Due to the unified multimodal annota- tion, existing methods are restricted in capturing differenti- ated information. However, additional unimodal annotations are high time- and labor-cost. In this paper, we design a la- bel generation module based on the self-supervised learning strategy to acquire independent unimodal supervisions. Then, joint training the multimodal and uni-modal tasks to learn the consistency and difference, respectively. Moreover, dur- ing the training stage, we design a weight-adjustment strat- egy to balance the learning progress among different sub- tasks. That is to guide the subtasks to focus on samples with the larger difference between modality supervisions. Last, we conduct extensive experiments on three public multimodal baseline datasets. The experimental results validate the re- liability and stability of auto-generated unimodal supervi- sions. On MOSI and MOSEI datasets, our method surpasses the current state-of-the-art methods. On the SIMS dataset, our method achieves comparable performance than human- annotated unimodal labels. The full codes are available at https://github.com/thuiar/Self-MM.",
    "metadata": {
      "title": "Learning Modality-Specific Representations with Self-Supervised Multi-Task Learning for Multimodal Sentiment Analysis",
      "summary": "Representation Learning is a significant and challenging task in multimodal learning. Effective modality representations should contain two parts of characteristics: the consistency and the difference. Due to the unified multimodal annota- tion, existing methods are restricted in capturing differenti- ated information. However, additional unimodal annotations are high time- and labor-cost. In this paper, we design a la- bel generation module based on the self-supervised learning strategy to acquire independent unimodal supervisions. Then, joint training the multimodal and uni-modal tasks to learn the consistency and difference, respectively. Moreover, dur- ing the training stage, we design a weight-adjustment strat- egy to balance the learning progress among different sub- tasks. That is to guide the subtasks to focus on samples with the larger difference between modality supervisions. Last, we conduct extensive experiments on three public multimodal baseline datasets. The experimental results validate the re- liability and stability of auto-generated unimodal supervi- sions. On MOSI and MOSEI datasets, our method surpasses the current state-of-the-art methods. On the SIMS dataset, our method achieves comparable performance than human- annotated unimodal labels. The full codes are available at https://github.com/thuiar/Self-MM.",
      "abstract": "Representation Learning is a significant and challenging task in multimodal learning. Effective modality representations should contain two parts of characteristics: the consistency and the difference. Due to the unified multimodal annota- tion, existing methods are restricted in capturing differenti- ated information. However, additional unimodal annotations are high time- and labor-cost. In this paper, we design a la- bel generation module based on the self-supervised learning strategy to acquire independent unimodal supervisions. Then, joint training the multimodal and uni-modal tasks to learn the consistency and difference, respectively. Moreover, dur- ing the training stage, we design a weight-adjustment strat- egy to balance the learning progress among different sub- tasks. That is to guide the subtasks to focus on samples with the larger difference between modality supervisions. Last, we conduct extensive experiments on three public multimodal baseline datasets. The experimental results validate the re- liability and stability of auto-generated unimodal supervi- sions. On MOSI and MOSEI datasets, our method surpasses the current state-of-the-art methods. On the SIMS dataset, our method achieves comparable performance than human- annotated unimodal labels. The full codes are available at https://github.com/thuiar/Self-MM.",
      "doi": "https://doi.org/10.1609/aaai.v35i12.17289",
      "openalex_id": "https://openalex.org/W3128412859",
      "arxiv_id": "",
      "publication_date": "2021-05-18",
      "published": "2021-05-18",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "Although it involves audio as one modality, the paper focuses on self-supervised, multimodal representation learning for sentiment analysis and does not center on Spoken Language Models or speech-language modeling as core objects; thus it fails the inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "Although it involves audio as one modality, the paper focuses on self-supervised, multimodal representation learning for sentiment analysis and does not center on Spoken Language Models or speech-language modeling as core objects; thus it fails the inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on self-supervised multi-task learning for multimodal sentiment analysis, which primarily involves representation learning of multimodal data, but does not specifically study Spoken Language Models or Full-Duplex Spoken Language Models, nor does it focus on speech modeling or real-time interactive speech behaviors such as turn-taking, cognitive parallelism, or low-latency synchronization. Furthermore, it does not mention spoken language modeling explicitly or deal with speech-centric modeling architectures or evaluations required by the inclusion criteria. Therefore, it does not meet the inclusion criteria and falls under exclusion for not aligning with the defined spoken language model research scope.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on self-supervised multi-task learning for multimodal sentiment analysis, which primarily involves representation learning of multimodal data, but does not specifically study Spoken Language Models or Full-Duplex Spoken Language Models, nor does it focus on speech modeling or real-time interactive speech behaviors such as turn-taking, cognitive parallelism, or low-latency synchronization. Furthermore, it does not mention spoken language modeling explicitly or deal with speech-centric modeling architectures or evaluations required by the inclusion criteria. Therefore, it does not meet the inclusion criteria and falls under exclusion for not aligning with the defined spoken language model research scope.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "TEXTOIR: An Integrated and Visualized Platform for Text Open Intent Recognition",
    "abstract": "TEXTOIR is the first integrated and visualized platform for text open intent\\nrecognition. It is composed of two main modules: open intent detection and open\\nintent discovery. Each module integrates most of the state-of-the-art\\nalgorithms and benchmark intent datasets. It also contains an overall framework\\nconnecting the two modules in a pipeline scheme. In addition, this platform has\\nvisualized tools for data and model management, training, evaluation and\\nanalysis of the performance from different aspects. TEXTOIR provides useful\\ntoolkits and convenient visualized interfaces for each sub-module (Toolkit\\ncode: https://github.com/thuiar/TEXTOIR), and designs a framework to implement\\na complete process to both identify known intents and discover open intents\\n(Demo code: https://github.com/thuiar/TEXTOIR-DEMO).\\n",
    "metadata": {
      "title": "TEXTOIR: An Integrated and Visualized Platform for Text Open Intent Recognition",
      "summary": "TEXTOIR is the first integrated and visualized platform for text open intent\\nrecognition. It is composed of two main modules: open intent detection and open\\nintent discovery. Each module integrates most of the state-of-the-art\\nalgorithms and benchmark intent datasets. It also contains an overall framework\\nconnecting the two modules in a pipeline scheme. In addition, this platform has\\nvisualized tools for data and model management, training, evaluation and\\nanalysis of the performance from different aspects. TEXTOIR provides useful\\ntoolkits and convenient visualized interfaces for each sub-module (Toolkit\\ncode: https://github.com/thuiar/TEXTOIR), and designs a framework to implement\\na complete process to both identify known intents and discover open intents\\n(Demo code: https://github.com/thuiar/TEXTOIR-DEMO).\\n",
      "abstract": "TEXTOIR is the first integrated and visualized platform for text open intent\\nrecognition. It is composed of two main modules: open intent detection and open\\nintent discovery. Each module integrates most of the state-of-the-art\\nalgorithms and benchmark intent datasets. It also contains an overall framework\\nconnecting the two modules in a pipeline scheme. In addition, this platform has\\nvisualized tools for data and model management, training, evaluation and\\nanalysis of the performance from different aspects. TEXTOIR provides useful\\ntoolkits and convenient visualized interfaces for each sub-module (Toolkit\\ncode: https://github.com/thuiar/TEXTOIR), and designs a framework to implement\\na complete process to both identify known intents and discover open intents\\n(Demo code: https://github.com/thuiar/TEXTOIR-DEMO).\\n",
      "doi": "https://doi.org/10.18653/v1/2021.acl-demo.20",
      "openalex_id": "https://openalex.org/W3174620475",
      "arxiv_id": "",
      "publication_date": "2021-01-01",
      "published": "2021-01-01",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item focuses on text open intent recognition rather than Spoken Language Models or speech-language modelling, with no discussion of speech-based architectures, synchronization, or low-latency interaction required by the inclusion criteria; thus it does not meet the criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item focuses on text open intent recognition rather than Spoken Language Models or speech-language modelling, with no discussion of speech-based architectures, synchronization, or low-latency interaction required by the inclusion criteria; thus it does not meet the criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study presents TEXTOIR, a platform focused on text open intent recognition, emphasizing text processing and model integration without addressing spoken language models or speech as a core modeling target; it lacks mention of speech language modeling, full-duplex interaction, or speech-based training, hence it does not meet the inclusion criteria centered on spoken language models and related speech interaction features.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study presents TEXTOIR, a platform focused on text open intent recognition, emphasizing text processing and model integration without addressing spoken language models or speech as a core modeling target; it lacks mention of speech language modeling, full-duplex interaction, or speech-based training, hence it does not meet the inclusion criteria centered on spoken language models and related speech interaction features.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Deep Open Intent Classification with Adaptive Decision Boundary",
    "abstract": "Open intent classification is a challenging task in dialogue systems. On the one hand, it should ensure the quality of known intent identification. On the other hand, it needs to detect the open (unknown) intent without prior knowledge. Current models are limited in finding the appropriate decision boundary to balance the performances of both known intents and the open intent. In this paper, we propose a post-processing method to learn the adaptive decision boundary (ADB) for open intent classification. We first utilize the labeled known intent samples to pre-train the model. Then, we automatically learn the adaptive spherical decision boundary for each known class with the aid of well-trained features. Specifically, we propose a new loss function to balance both the empirical risk and the open space risk. Our method does not need open intent samples and is free from modifying the model architecture. Moreover, our approach is surprisingly insensitive with less labeled data and fewer known intents. Extensive experiments on three benchmark datasets show that our method yields significant improvements compared with the state-of-the-art methods.",
    "metadata": {
      "title": "Deep Open Intent Classification with Adaptive Decision Boundary",
      "summary": "Open intent classification is a challenging task in dialogue systems. On the one hand, it should ensure the quality of known intent identification. On the other hand, it needs to detect the open (unknown) intent without prior knowledge. Current models are limited in finding the appropriate decision boundary to balance the performances of both known intents and the open intent. In this paper, we propose a post-processing method to learn the adaptive decision boundary (ADB) for open intent classification. We first utilize the labeled known intent samples to pre-train the model. Then, we automatically learn the adaptive spherical decision boundary for each known class with the aid of well-trained features. Specifically, we propose a new loss function to balance both the empirical risk and the open space risk. Our method does not need open intent samples and is free from modifying the model architecture. Moreover, our approach is surprisingly insensitive with less labeled data and fewer known intents. Extensive experiments on three benchmark datasets show that our method yields significant improvements compared with the state-of-the-art methods.",
      "abstract": "Open intent classification is a challenging task in dialogue systems. On the one hand, it should ensure the quality of known intent identification. On the other hand, it needs to detect the open (unknown) intent without prior knowledge. Current models are limited in finding the appropriate decision boundary to balance the performances of both known intents and the open intent. In this paper, we propose a post-processing method to learn the adaptive decision boundary (ADB) for open intent classification. We first utilize the labeled known intent samples to pre-train the model. Then, we automatically learn the adaptive spherical decision boundary for each known class with the aid of well-trained features. Specifically, we propose a new loss function to balance both the empirical risk and the open space risk. Our method does not need open intent samples and is free from modifying the model architecture. Moreover, our approach is surprisingly insensitive with less labeled data and fewer known intents. Extensive experiments on three benchmark datasets show that our method yields significant improvements compared with the state-of-the-art methods.",
      "doi": "https://doi.org/10.1609/aaai.v35i16.17690",
      "openalex_id": "https://openalex.org/W3173488222",
      "arxiv_id": "",
      "publication_date": "2021-05-18",
      "published": "2021-05-18",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "本文專注於文本對話中的意圖分類，未以語音或語音模型為核心進行建模、訓練或評估，因此不符合以 Spoken Language Models 或等價概念為核心的納入標準。",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "本文專注於文本對話中的意圖分類，未以語音或語音模型為核心進行建模、訓練或評估，因此不符合以 Spoken Language Models 或等價概念為核心的納入標準。",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on open intent classification in dialogue systems, involving decision boundary learning for known and unknown intents, but it does not address spoken language modeling or speech as the core modeling object, nor does it mention full-duplex interaction, speech-level language models, or synchronization; it appears centered on text-based intent classification without speech modeling innovation or evaluation related to speech interaction dynamics.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on open intent classification in dialogue systems, involving decision boundary learning for known and unknown intents, but it does not address spoken language modeling or speech as the core modeling object, nor does it mention full-duplex interaction, speech-level language models, or synchronization; it appears centered on text-based intent classification without speech modeling innovation or evaluation related to speech interaction dynamics.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Discovering New Intents with Deep Aligned Clustering",
    "abstract": "Discovering new intents is a crucial task in dialogue systems. Most existing methods are limited in transferring the prior knowledge from known intents to new intents. These methods also have difficulties in providing high-quality supervised signals to learn clustering-friendly features for grouping unlabeled intents. In this work, we propose an effective method (Deep Aligned Clustering) to discover new intents with the aid of limited known intent data. Firstly, we leverage a few labeled known intent samples as prior knowledge to pre-train the model. Then, we perform k-means to produce cluster assignments as pseudo-labels. Moreover, we propose an alignment strategy to tackle the label inconsistency problem during clustering assignments. Finally, we learn the intent representations under the supervision of the aligned pseudo-labels. With an unknown number of new intents, we predict the number of intent categories by eliminating low-confidence intent-wise clusters. Extensive experiments on two benchmark datasets show that our method is more robust and achieves substantial improvements over the state-of-the-art methods.",
    "metadata": {
      "title": "Discovering New Intents with Deep Aligned Clustering",
      "summary": "Discovering new intents is a crucial task in dialogue systems. Most existing methods are limited in transferring the prior knowledge from known intents to new intents. These methods also have difficulties in providing high-quality supervised signals to learn clustering-friendly features for grouping unlabeled intents. In this work, we propose an effective method (Deep Aligned Clustering) to discover new intents with the aid of limited known intent data. Firstly, we leverage a few labeled known intent samples as prior knowledge to pre-train the model. Then, we perform k-means to produce cluster assignments as pseudo-labels. Moreover, we propose an alignment strategy to tackle the label inconsistency problem during clustering assignments. Finally, we learn the intent representations under the supervision of the aligned pseudo-labels. With an unknown number of new intents, we predict the number of intent categories by eliminating low-confidence intent-wise clusters. Extensive experiments on two benchmark datasets show that our method is more robust and achieves substantial improvements over the state-of-the-art methods.",
      "abstract": "Discovering new intents is a crucial task in dialogue systems. Most existing methods are limited in transferring the prior knowledge from known intents to new intents. These methods also have difficulties in providing high-quality supervised signals to learn clustering-friendly features for grouping unlabeled intents. In this work, we propose an effective method (Deep Aligned Clustering) to discover new intents with the aid of limited known intent data. Firstly, we leverage a few labeled known intent samples as prior knowledge to pre-train the model. Then, we perform k-means to produce cluster assignments as pseudo-labels. Moreover, we propose an alignment strategy to tackle the label inconsistency problem during clustering assignments. Finally, we learn the intent representations under the supervision of the aligned pseudo-labels. With an unknown number of new intents, we predict the number of intent categories by eliminating low-confidence intent-wise clusters. Extensive experiments on two benchmark datasets show that our method is more robust and achieves substantial improvements over the state-of-the-art methods.",
      "doi": "https://doi.org/10.1609/aaai.v35i16.17689",
      "openalex_id": "https://openalex.org/W3112240880",
      "arxiv_id": "",
      "publication_date": "2021-05-18",
      "published": "2021-05-18",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The work focuses on NLP-based intent discovery via clustering and labeled data, not on Spoken Language Models or speech-centric modeling/interaction, so it does not meet the core inclusion criteria requiring SLM/FD-SLM focus and speech-based evaluation.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The work focuses on NLP-based intent discovery via clustering and labeled data, not on Spoken Language Models or speech-centric modeling/interaction, so it does not meet the core inclusion criteria requiring SLM/FD-SLM focus and speech-based evaluation.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper focuses on discovering new intents in dialogue systems using clustering methods and known intent data but does not explicitly address spoken language models, speech as the core modeling object, or full-duplex spoken language modeling aspects detailed in the inclusion criteria. It also lacks discussion on speech-level language modeling architectures, synchronization, or real-time interactive evaluation. Therefore, it does not meet the specialized inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper focuses on discovering new intents in dialogue systems using clustering methods and known intent data but does not explicitly address spoken language models, speech as the core modeling object, or full-duplex spoken language modeling aspects detailed in the inclusion criteria. It also lacks discussion on speech-level language modeling architectures, synchronization, or real-time interactive evaluation. Therefore, it does not meet the specialized inclusion criteria.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "A Slot Is Not Built in One Utterance: Spoken Language Dialogs with Sub-Slots",
    "abstract": "A slot value might be provided segment by segment over multiple-turn interactions in a dialog, especially for some important information such as phone numbers and names. It is a common phenomenon in daily life, but little attention has been paid to it in previous work. To fill the gap, this paper defines a new task named Sub-Slot based Task-Oriented Dialog (SSTOD) and builds a Chinese dialog dataset SSD for boosting research on SSTOD. The dataset includes a total of 40K dialogs and 500K utterances from four different domains: Chinese names, phone numbers, ID numbers and license plate numbers. The data is well annotated with sub-slot values, slot values, dialog states and actions. We find some new linguistic phenomena and interactive manners in SSTOD which raise critical challenges of building dialog agents for the task. We test three state-of-the-art dialog models on SSTOD and find they cannot handle the task well on any of the four domains. We also investigate an improved model by involving slot knowledge in a plug-in manner. More work should be done to meet the new challenges raised from SSTOD which widely exists in real-life applications. The dataset and code are publicly available via https://github.com/shunjiu/SSTOD.",
    "metadata": {
      "title": "A Slot Is Not Built in One Utterance: Spoken Language Dialogs with Sub-Slots",
      "summary": "A slot value might be provided segment by segment over multiple-turn interactions in a dialog, especially for some important information such as phone numbers and names. It is a common phenomenon in daily life, but little attention has been paid to it in previous work. To fill the gap, this paper defines a new task named Sub-Slot based Task-Oriented Dialog (SSTOD) and builds a Chinese dialog dataset SSD for boosting research on SSTOD. The dataset includes a total of 40K dialogs and 500K utterances from four different domains: Chinese names, phone numbers, ID numbers and license plate numbers. The data is well annotated with sub-slot values, slot values, dialog states and actions. We find some new linguistic phenomena and interactive manners in SSTOD which raise critical challenges of building dialog agents for the task. We test three state-of-the-art dialog models on SSTOD and find they cannot handle the task well on any of the four domains. We also investigate an improved model by involving slot knowledge in a plug-in manner. More work should be done to meet the new challenges raised from SSTOD which widely exists in real-life applications. The dataset and code are publicly available via https://github.com/shunjiu/SSTOD.",
      "abstract": "A slot value might be provided segment by segment over multiple-turn interactions in a dialog, especially for some important information such as phone numbers and names. It is a common phenomenon in daily life, but little attention has been paid to it in previous work. To fill the gap, this paper defines a new task named Sub-Slot based Task-Oriented Dialog (SSTOD) and builds a Chinese dialog dataset SSD for boosting research on SSTOD. The dataset includes a total of 40K dialogs and 500K utterances from four different domains: Chinese names, phone numbers, ID numbers and license plate numbers. The data is well annotated with sub-slot values, slot values, dialog states and actions. We find some new linguistic phenomena and interactive manners in SSTOD which raise critical challenges of building dialog agents for the task. We test three state-of-the-art dialog models on SSTOD and find they cannot handle the task well on any of the four domains. We also investigate an improved model by involving slot knowledge in a plug-in manner. More work should be done to meet the new challenges raised from SSTOD which widely exists in real-life applications. The dataset and code are publicly available via https://github.com/shunjiu/SSTOD.",
      "doi": "https://doi.org/10.18653/v1/2022.findings-acl.27",
      "openalex_id": "https://openalex.org/W4221141243",
      "arxiv_id": "",
      "publication_date": "2022-01-01",
      "published": "2022-01-01",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "Although the title uses 'Spoken Language Dialogs' and the abstract discusses a slot-based task in spoken language dialogs, the work focuses on dataset creation and text-based dialogue modeling rather than core Spoken Language Models (SLMs) or full-duplex speech–language modeling (arch/assessment), thus it does not clearly meet the inclusion criteria for SLM-centered research.",
      "evaluation": 2
    },
    "round-A_JuniorNano_reasoning": "Although the title uses 'Spoken Language Dialogs' and the abstract discusses a slot-based task in spoken language dialogs, the work focuses on dataset creation and text-based dialogue modeling rather than core Spoken Language Models (SLMs) or full-duplex speech–language modeling (arch/assessment), thus it does not clearly meet the inclusion criteria for SLM-centered research.",
    "round-A_JuniorNano_evaluation": 2,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper focuses on dialog modeling with slot filling across multiple turns but does not specifically address spoken language models or speech-based language modeling; it does not mention speech signals, speech language models, full-duplex interaction, or related speech-centric architectures, instead focusing on dialog state tracking with text-based slot representations, which falls outside the inclusion criteria emphasizing speech-level modeling and architecture.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper focuses on dialog modeling with slot filling across multiple turns but does not specifically address spoken language models or speech-based language modeling; it does not mention speech signals, speech language models, full-duplex interaction, or related speech-centric architectures, instead focusing on dialog state tracking with text-based slot representations, which falls outside the inclusion criteria emphasizing speech-level modeling and architecture.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:2)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "An Incremental Turn-Taking Model with Active System Barge-in for Spoken Dialog Systems",
    "abstract": "This paper deals with an incremental turntaking model that provides a novel solution for end-of-turn detection.It includes a flexible framework that enables active system barge-in.In order to accomplish this, a systematic procedure of teaching a dialog system to produce meaningful system barge-in is presented.This procedure improves system robustness and success rate.It includes constructing cost models and learning optimal policy using reinforcement learning.Results show that our model reduces false cut-in rate by 37.1% and response delay by 32.5% compared to the baseline system.Also the learned system barge-in strategy yields a 27.7% increase in average reward from user responses.",
    "metadata": {
      "title": "An Incremental Turn-Taking Model with Active System Barge-in for Spoken Dialog Systems",
      "summary": "This paper deals with an incremental turntaking model that provides a novel solution for end-of-turn detection.It includes a flexible framework that enables active system barge-in.In order to accomplish this, a systematic procedure of teaching a dialog system to produce meaningful system barge-in is presented.This procedure improves system robustness and success rate.It includes constructing cost models and learning optimal policy using reinforcement learning.Results show that our model reduces false cut-in rate by 37.1% and response delay by 32.5% compared to the baseline system.Also the learned system barge-in strategy yields a 27.7% increase in average reward from user responses.",
      "abstract": "This paper deals with an incremental turntaking model that provides a novel solution for end-of-turn detection.It includes a flexible framework that enables active system barge-in.In order to accomplish this, a systematic procedure of teaching a dialog system to produce meaningful system barge-in is presented.This procedure improves system robustness and success rate.It includes constructing cost models and learning optimal policy using reinforcement learning.Results show that our model reduces false cut-in rate by 37.1% and response delay by 32.5% compared to the baseline system.Also the learned system barge-in strategy yields a 27.7% increase in average reward from user responses.",
      "doi": "https://doi.org/10.18653/v1/w15-4606",
      "openalex_id": "https://openalex.org/W2252276958",
      "arxiv_id": "",
      "publication_date": "2015-01-01",
      "published": "2015-01-01",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item discusses incremental turn-taking and active system barge-in in spoken dialog systems, focusing on dialogue management and timing rather than treating Spoken Language Models or Speech Language Models as the core modeling objects; it lacks explicit mention of SLM/FD-SLM architecture, training, or evaluation in the abstract, thus it does not meet the inclusion criteria, though it does cover full-duplex interaction which would not trigger exclusion criteria.",
      "evaluation": 2
    },
    "round-A_JuniorNano_reasoning": "The item discusses incremental turn-taking and active system barge-in in spoken dialog systems, focusing on dialogue management and timing rather than treating Spoken Language Models or Speech Language Models as the core modeling objects; it lacks explicit mention of SLM/FD-SLM architecture, training, or evaluation in the abstract, thus it does not meet the inclusion criteria, though it does cover full-duplex interaction which would not trigger exclusion criteria.",
    "round-A_JuniorNano_evaluation": 2,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper focuses on an incremental turn-taking model with active system barge-in for spoken dialog systems, which addresses synchronous interaction behaviors such as turn-taking and interruption, aligning with the inclusion criteria on full-duplex spoken language models and learned synchronization; however, the abstract does not explicitly mention 'spoken language models' or equivalent key terms, nor does it discuss model architecture or training in a way that clearly centers speech as the core modeling object beyond dialog behavior control. Therefore, while the topic is relevant, it partially meets the inclusion criteria and there are no clear exclusion criteria violations.",
      "evaluation": 3
    },
    "round-A_JuniorMini_reasoning": "The paper focuses on an incremental turn-taking model with active system barge-in for spoken dialog systems, which addresses synchronous interaction behaviors such as turn-taking and interruption, aligning with the inclusion criteria on full-duplex spoken language models and learned synchronization; however, the abstract does not explicitly mention 'spoken language models' or equivalent key terms, nor does it discuss model architecture or training in a way that clearly centers speech as the core modeling object beyond dialog behavior control. Therefore, while the topic is relevant, it partially meets the inclusion criteria and there are no clear exclusion criteria violations.",
    "round-A_JuniorMini_evaluation": 3,
    "round-B_SeniorLead_output": {
      "reasoning": "Both junior reviewers noted the paper addresses incremental turn-taking and active system barge-in (junior outputs: evaluations 2 and 3), but the abstract contains no explicit mention of Spoken Language Models, speech-level language modeling, Speech–LLM or FD‑SLM architectures, nor does it frame speech as the core modeling object—instead it focuses on dialog policy/cost models and reinforcement learning for barge-in timing. The inclusion criteria require the study to explicitly center on SLMs/FD‑SLMs or equivalent speech-as-core modeling (architecture/training/evaluation); this item does not meet that requirement despite being relevant to synchronous interaction behavior. Therefore: better to exclude. (Score explanation: lacks explicit spoken-language-model focus and speech-level modeling, so it fails the inclusion criteria.)",
      "evaluation": 2
    },
    "round-B_SeniorLead_reasoning": "Both junior reviewers noted the paper addresses incremental turn-taking and active system barge-in (junior outputs: evaluations 2 and 3), but the abstract contains no explicit mention of Spoken Language Models, speech-level language modeling, Speech–LLM or FD‑SLM architectures, nor does it frame speech as the core modeling object—instead it focuses on dialog policy/cost models and reinforcement learning for barge-in timing. The inclusion criteria require the study to explicitly center on SLMs/FD‑SLMs or equivalent speech-as-core modeling (architecture/training/evaluation); this item does not meet that requirement despite being relevant to synchronous interaction behavior. Therefore: better to exclude. (Score explanation: lacks explicit spoken-language-model focus and speech-level modeling, so it fails the inclusion criteria.)",
    "round-B_SeniorLead_evaluation": 2,
    "final_verdict": "exclude (senior:2)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "A Pre-Training Based Personalized Dialogue Generation Model with Persona-Sparse Data",
    "abstract": "Endowing dialogue systems with personas is essential to deliver more human-like conversations. However, this problem is still far from well explored due to the difficulties of both embodying personalities in natural languages and the persona sparsity issue observed in most dialogue corpora. This paper proposes a pre-training based personalized dialogue model that can generate coherent responses using persona-sparse dialogue data. In this method, a pre-trained language model is used to initialize an encoder and decoder, and personal attribute embeddings are devised to model richer dialogue contexts by encoding speakers' personas together with dialogue histories. Further, to incorporate the target persona in the decoding process and to balance its contribution, an attention routing structure is devised in the decoder to merge features extracted from the target persona and dialogue contexts using dynamically predicted weights. Our model can utilize persona-sparse dialogues in a unified manner during the training process, and can also control the amount of persona-related features to exhibit during the inference process. Both automatic and manual evaluation demonstrates that the proposed model outperforms state-of-the-art methods for generating more coherent and persona consistent responses with persona-sparse data.",
    "metadata": {
      "title": "A Pre-Training Based Personalized Dialogue Generation Model with Persona-Sparse Data",
      "summary": "Endowing dialogue systems with personas is essential to deliver more human-like conversations. However, this problem is still far from well explored due to the difficulties of both embodying personalities in natural languages and the persona sparsity issue observed in most dialogue corpora. This paper proposes a pre-training based personalized dialogue model that can generate coherent responses using persona-sparse dialogue data. In this method, a pre-trained language model is used to initialize an encoder and decoder, and personal attribute embeddings are devised to model richer dialogue contexts by encoding speakers' personas together with dialogue histories. Further, to incorporate the target persona in the decoding process and to balance its contribution, an attention routing structure is devised in the decoder to merge features extracted from the target persona and dialogue contexts using dynamically predicted weights. Our model can utilize persona-sparse dialogues in a unified manner during the training process, and can also control the amount of persona-related features to exhibit during the inference process. Both automatic and manual evaluation demonstrates that the proposed model outperforms state-of-the-art methods for generating more coherent and persona consistent responses with persona-sparse data.",
      "abstract": "Endowing dialogue systems with personas is essential to deliver more human-like conversations. However, this problem is still far from well explored due to the difficulties of both embodying personalities in natural languages and the persona sparsity issue observed in most dialogue corpora. This paper proposes a pre-training based personalized dialogue model that can generate coherent responses using persona-sparse dialogue data. In this method, a pre-trained language model is used to initialize an encoder and decoder, and personal attribute embeddings are devised to model richer dialogue contexts by encoding speakers' personas together with dialogue histories. Further, to incorporate the target persona in the decoding process and to balance its contribution, an attention routing structure is devised in the decoder to merge features extracted from the target persona and dialogue contexts using dynamically predicted weights. Our model can utilize persona-sparse dialogues in a unified manner during the training process, and can also control the amount of persona-related features to exhibit during the inference process. Both automatic and manual evaluation demonstrates that the proposed model outperforms state-of-the-art methods for generating more coherent and persona consistent responses with persona-sparse data.",
      "doi": "https://doi.org/10.1609/aaai.v34i05.6518",
      "openalex_id": "https://openalex.org/W2997892440",
      "arxiv_id": "",
      "publication_date": "2020-04-03",
      "published": "2020-04-03",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The paper focuses on text-based personalized dialogue generation with persona embeddings and pre-training, not on Spoken Language Models or speech-centric modeling; it lacks audio/speech data, turn-taking synchronization, or speech interaction evaluation, so it does not meet the required criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The paper focuses on text-based personalized dialogue generation with persona embeddings and pre-training, not on Spoken Language Models or speech-centric modeling; it lacks audio/speech data, turn-taking synchronization, or speech interaction evaluation, so it does not meet the required criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on a personalized dialogue generation model using persona-sparse data but does not address spoken language models or speech as the primary modeling object; it only deals with text-based dialogue modeling without mention of speech signals, end-to-end speech-text architectures, or full-duplex spoken interaction capabilities, thus not meeting the inclusion criteria centered on speech modeling and synchronization aspects.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on a personalized dialogue generation model using persona-sparse data but does not address spoken language models or speech as the primary modeling object; it only deals with text-based dialogue modeling without mention of speech signals, end-to-end speech-text architectures, or full-duplex spoken interaction capabilities, thus not meeting the inclusion criteria centered on speech modeling and synchronization aspects.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Reinforcement learning for turn-taking management in incremental spoken dialogue systems",
    "abstract": "In this article, reinforcement learning is used to learn an optimal turn-taking strategy for vocal human-machine dialogue. The Orange Labs' Majordomo dialogue system, which allows the users to have conversations within a smart home, has been upgraded to an incremental version. First, a user simulator is built in order to generate a dialogue corpus which thereafter is used to optimise the turn-taking strategy from delayed rewards with the Fitted-Q reinforcement learning algorithm. Real users test and evaluate the new learnt strategy, versus a non-incremental and a handcrafted incremental strategies. The data-driven strategy is shown to significantly improve the task completion ratio and to be preferred by the users according to subjective metrics.",
    "metadata": {
      "title": "Reinforcement learning for turn-taking management in incremental spoken dialogue systems",
      "summary": "In this article, reinforcement learning is used to learn an optimal turn-taking strategy for vocal human-machine dialogue. The Orange Labs' Majordomo dialogue system, which allows the users to have conversations within a smart home, has been upgraded to an incremental version. First, a user simulator is built in order to generate a dialogue corpus which thereafter is used to optimise the turn-taking strategy from delayed rewards with the Fitted-Q reinforcement learning algorithm. Real users test and evaluate the new learnt strategy, versus a non-incremental and a handcrafted incremental strategies. The data-driven strategy is shown to significantly improve the task completion ratio and to be preferred by the users according to subjective metrics.",
      "abstract": "In this article, reinforcement learning is used to learn an optimal turn-taking strategy for vocal human-machine dialogue. The Orange Labs' Majordomo dialogue system, which allows the users to have conversations within a smart home, has been upgraded to an incremental version. First, a user simulator is built in order to generate a dialogue corpus which thereafter is used to optimise the turn-taking strategy from delayed rewards with the Fitted-Q reinforcement learning algorithm. Real users test and evaluate the new learnt strategy, versus a non-incremental and a handcrafted incremental strategies. The data-driven strategy is shown to significantly improve the task completion ratio and to be preferred by the users according to subjective metrics.",
      "doi": "",
      "openalex_id": "https://openalex.org/W2578390274",
      "arxiv_id": "",
      "publication_date": "2016-07-09",
      "published": "2016-07-09",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The study focuses on reinforcement learning for turn-taking in incremental spoken dialogue systems and evaluates interaction performance, but it does not clearly position Speech/Languages Models or explicit language modeling of speech as the core object; it leans more toward dialogue management rather than SLM-scale modeling of speech.",
      "evaluation": 2
    },
    "round-A_JuniorNano_reasoning": "The study focuses on reinforcement learning for turn-taking in incremental spoken dialogue systems and evaluates interaction performance, but it does not clearly position Speech/Languages Models or explicit language modeling of speech as the core object; it leans more toward dialogue management rather than SLM-scale modeling of speech.",
    "round-A_JuniorNano_evaluation": 2,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on reinforcement learning for turn-taking strategies in spoken dialogue systems, involving an incremental system that manages human-machine vocal interaction. However, it does not explicitly mention spoken language models, speech language models, or equivalent concepts as the core modeling approach, nor does it describe speech-level language modeling or full-duplex interaction capabilities such as cognitive parallelism or overlap management. The inclusion criteria emphasize explicit mention and focus on spoken language modeling or full-duplex capability, which is not clearly established here. Consequently, it better fits exclusion criteria due to lack of core focus on spoken language models and full-duplex features.",
      "evaluation": 2
    },
    "round-A_JuniorMini_reasoning": "The study focuses on reinforcement learning for turn-taking strategies in spoken dialogue systems, involving an incremental system that manages human-machine vocal interaction. However, it does not explicitly mention spoken language models, speech language models, or equivalent concepts as the core modeling approach, nor does it describe speech-level language modeling or full-duplex interaction capabilities such as cognitive parallelism or overlap management. The inclusion criteria emphasize explicit mention and focus on spoken language modeling or full-duplex capability, which is not clearly established here. Consequently, it better fits exclusion criteria due to lack of core focus on spoken language models and full-duplex features.",
    "round-A_JuniorMini_evaluation": 2,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:2)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "MMChat: Multi-Modal Chat Dataset on Social Media",
    "abstract": "Incorporating multi-modal contexts in conversation is important for developing more engaging dialogue systems. In this work, we explore this direction by introducing MMChat: a large-scale Chinese multi-modal dialogue corpus (32.4M raw dialogues and 120.84K filtered dialogues). Unlike previous corpora that are crowd-sourced or collected from fictitious movies, MMChat contains image-grounded dialogues collected from real conversations on social media, in which the sparsity issue is observed. Specifically, image-initiated dialogues in common communications may deviate to some non-image-grounded topics as the conversation proceeds. To better investigate this issue, we manually annotate 100K dialogues from MMChat and further filter the corpus accordingly, which yields MMChat-hf. We develop a benchmark model to address the sparsity issue in dialogue generation tasks by adapting the attention routing mechanism on image features. Experiments demonstrate the usefulness of incorporating image features and the effectiveness of handling the sparsity of image features.",
    "metadata": {
      "title": "MMChat: Multi-Modal Chat Dataset on Social Media",
      "summary": "Incorporating multi-modal contexts in conversation is important for developing more engaging dialogue systems. In this work, we explore this direction by introducing MMChat: a large-scale Chinese multi-modal dialogue corpus (32.4M raw dialogues and 120.84K filtered dialogues). Unlike previous corpora that are crowd-sourced or collected from fictitious movies, MMChat contains image-grounded dialogues collected from real conversations on social media, in which the sparsity issue is observed. Specifically, image-initiated dialogues in common communications may deviate to some non-image-grounded topics as the conversation proceeds. To better investigate this issue, we manually annotate 100K dialogues from MMChat and further filter the corpus accordingly, which yields MMChat-hf. We develop a benchmark model to address the sparsity issue in dialogue generation tasks by adapting the attention routing mechanism on image features. Experiments demonstrate the usefulness of incorporating image features and the effectiveness of handling the sparsity of image features.",
      "abstract": "Incorporating multi-modal contexts in conversation is important for developing more engaging dialogue systems. In this work, we explore this direction by introducing MMChat: a large-scale Chinese multi-modal dialogue corpus (32.4M raw dialogues and 120.84K filtered dialogues). Unlike previous corpora that are crowd-sourced or collected from fictitious movies, MMChat contains image-grounded dialogues collected from real conversations on social media, in which the sparsity issue is observed. Specifically, image-initiated dialogues in common communications may deviate to some non-image-grounded topics as the conversation proceeds. To better investigate this issue, we manually annotate 100K dialogues from MMChat and further filter the corpus accordingly, which yields MMChat-hf. We develop a benchmark model to address the sparsity issue in dialogue generation tasks by adapting the attention routing mechanism on image features. Experiments demonstrate the usefulness of incorporating image features and the effectiveness of handling the sparsity of image features.",
      "doi": "https://doi.org/10.48550/arxiv.2108.07154",
      "openalex_id": "https://openalex.org/W3193915594",
      "arxiv_id": "",
      "publication_date": "2021-08-16",
      "published": "2021-08-16",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This item focuses on image-grounded multimodal dialogue datasets and does not center on spoken language models or speech-centric modeling/evaluation, so it should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "This item focuses on image-grounded multimodal dialogue datasets and does not center on spoken language models or speech-centric modeling/evaluation, so it should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper focuses on a multi-modal chat dataset involving images and dialogues on social media, emphasizing image-grounded dialogues and dialogue generation tasks, without addressing spoken language modeling, speech sequence modeling, full-duplex interaction, or voice-based language models as required by the inclusion criteria; also, there is no mention of speech-level processing or speech-language models in the title or abstract, failing to meet core criteria.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper focuses on a multi-modal chat dataset involving images and dialogues on social media, emphasizing image-grounded dialogues and dialogue generation tasks, without addressing spoken language modeling, speech sequence modeling, full-duplex interaction, or voice-based language models as required by the inclusion criteria; also, there is no mention of speech-level processing or speech-language models in the title or abstract, failing to meet core criteria.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "GALAXY: A Generative Pre-trained Model for Task-Oriented Dialog with Semi-Supervised Learning and Explicit Policy Injection",
    "abstract": "Pre-trained models have proved to be powerful in enhancing task-oriented dialog systems. However, current pre-training methods mainly focus on enhancing dialog understanding and generation tasks while neglecting the exploitation of dialog policy. In this paper, we propose GALAXY, a novel pre-trained dialog model that explicitly learns dialog policy from limited labeled dialogs and large-scale unlabeled dialog corpora via semi-supervised learning. Specifically, we introduce a dialog act prediction task for policy optimization during pre-training and employ a consistency regularization term to refine the learned representation with the help of unlabeled dialogs. We also implement a gating mechanism to weigh suitable unlabeled dialog samples. Empirical results show that GALAXY substantially improves the performance of task-oriented dialog systems, and achieves new state-of-the-art results on benchmark datasets: In-Car, MultiWOZ2.0 and MultiWOZ2.1, improving their end-to-end combined scores by 2.5, 5.3 and 5.5 points, respectively. We also show that GALAXY has a stronger few-shot ability than existing models under various low-resource settings.",
    "metadata": {
      "title": "GALAXY: A Generative Pre-trained Model for Task-Oriented Dialog with Semi-Supervised Learning and Explicit Policy Injection",
      "summary": "Pre-trained models have proved to be powerful in enhancing task-oriented dialog systems. However, current pre-training methods mainly focus on enhancing dialog understanding and generation tasks while neglecting the exploitation of dialog policy. In this paper, we propose GALAXY, a novel pre-trained dialog model that explicitly learns dialog policy from limited labeled dialogs and large-scale unlabeled dialog corpora via semi-supervised learning. Specifically, we introduce a dialog act prediction task for policy optimization during pre-training and employ a consistency regularization term to refine the learned representation with the help of unlabeled dialogs. We also implement a gating mechanism to weigh suitable unlabeled dialog samples. Empirical results show that GALAXY substantially improves the performance of task-oriented dialog systems, and achieves new state-of-the-art results on benchmark datasets: In-Car, MultiWOZ2.0 and MultiWOZ2.1, improving their end-to-end combined scores by 2.5, 5.3 and 5.5 points, respectively. We also show that GALAXY has a stronger few-shot ability than existing models under various low-resource settings.",
      "abstract": "Pre-trained models have proved to be powerful in enhancing task-oriented dialog systems. However, current pre-training methods mainly focus on enhancing dialog understanding and generation tasks while neglecting the exploitation of dialog policy. In this paper, we propose GALAXY, a novel pre-trained dialog model that explicitly learns dialog policy from limited labeled dialogs and large-scale unlabeled dialog corpora via semi-supervised learning. Specifically, we introduce a dialog act prediction task for policy optimization during pre-training and employ a consistency regularization term to refine the learned representation with the help of unlabeled dialogs. We also implement a gating mechanism to weigh suitable unlabeled dialog samples. Empirical results show that GALAXY substantially improves the performance of task-oriented dialog systems, and achieves new state-of-the-art results on benchmark datasets: In-Car, MultiWOZ2.0 and MultiWOZ2.1, improving their end-to-end combined scores by 2.5, 5.3 and 5.5 points, respectively. We also show that GALAXY has a stronger few-shot ability than existing models under various low-resource settings.",
      "doi": "https://doi.org/10.48550/arxiv.2111.14592",
      "openalex_id": "https://openalex.org/W4297783677",
      "arxiv_id": "",
      "publication_date": "2021-11-29",
      "published": "2021-11-29",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This work targets a text-based, task-oriented dialog model with semi-supervised policy learning, and contains no emphasis on spoken language modeling, speech signals, or full-duplex interactive speech behavior, so it does not meet the Spoken Language Models criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "This work targets a text-based, task-oriented dialog model with semi-supervised policy learning, and contains no emphasis on spoken language modeling, speech signals, or full-duplex interactive speech behavior, so it does not meet the Spoken Language Models criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on task-oriented dialog systems using pre-trained models for dialog understanding and policy learning, but it does not mention modeling or reasoning on speech or acoustic signals directly, nor full-duplex spoken language modeling or end-to-end speech-text architectures. The focus is on dialog act prediction with textual dialog corpora rather than spoken language modeling. Therefore, it does not meet the inclusion criteria related to spoken language models or speech-level modeling, nor does it address synchronous interactive speech features such as turn-taking or low latency.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on task-oriented dialog systems using pre-trained models for dialog understanding and policy learning, but it does not mention modeling or reasoning on speech or acoustic signals directly, nor full-duplex spoken language modeling or end-to-end speech-text architectures. The focus is on dialog act prediction with textual dialog corpora rather than spoken language modeling. Therefore, it does not meet the inclusion criteria related to spoken language models or speech-level modeling, nor does it address synchronous interactive speech features such as turn-taking or low latency.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "EVA: An Open-Domain Chinese Dialogue System with Large-Scale Generative Pre-Training",
    "abstract": "Although pre-trained language models have remarkably enhanced the generation ability of dialogue systems, open-domain Chinese dialogue systems are still limited by the dialogue data and the model size compared with English ones. In this paper, we propose EVA, a Chinese dialogue system that contains the largest Chinese pre-trained dialogue model with 2.8B parameters. To build this model, we collect the largest Chinese dialogue dataset named WDC-Dialogue from various public social media. This dataset contains 1.4B context-response pairs and is used as the pre-training corpus of EVA. Extensive experiments on automatic and human evaluation show that EVA outperforms other Chinese pre-trained dialogue models especially in the multi-turn interaction of human-bot conversations.",
    "metadata": {
      "title": "EVA: An Open-Domain Chinese Dialogue System with Large-Scale Generative Pre-Training",
      "summary": "Although pre-trained language models have remarkably enhanced the generation ability of dialogue systems, open-domain Chinese dialogue systems are still limited by the dialogue data and the model size compared with English ones. In this paper, we propose EVA, a Chinese dialogue system that contains the largest Chinese pre-trained dialogue model with 2.8B parameters. To build this model, we collect the largest Chinese dialogue dataset named WDC-Dialogue from various public social media. This dataset contains 1.4B context-response pairs and is used as the pre-training corpus of EVA. Extensive experiments on automatic and human evaluation show that EVA outperforms other Chinese pre-trained dialogue models especially in the multi-turn interaction of human-bot conversations.",
      "abstract": "Although pre-trained language models have remarkably enhanced the generation ability of dialogue systems, open-domain Chinese dialogue systems are still limited by the dialogue data and the model size compared with English ones. In this paper, we propose EVA, a Chinese dialogue system that contains the largest Chinese pre-trained dialogue model with 2.8B parameters. To build this model, we collect the largest Chinese dialogue dataset named WDC-Dialogue from various public social media. This dataset contains 1.4B context-response pairs and is used as the pre-training corpus of EVA. Extensive experiments on automatic and human evaluation show that EVA outperforms other Chinese pre-trained dialogue models especially in the multi-turn interaction of human-bot conversations.",
      "doi": "https://doi.org/10.48550/arxiv.2108.01547",
      "openalex_id": "https://openalex.org/W3188553726",
      "arxiv_id": "",
      "publication_date": "2021-08-03",
      "published": "2021-08-03",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This item describes a text-based, open-domain Chinese dialogue model (EVA) with large-scale pretraining data, focusing on text dialogue rather than speech-language modeling, and it does not mention spoken language models or FD-SLMs, thus it does not meet the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "This item describes a text-based, open-domain Chinese dialogue model (EVA) with large-scale pretraining data, focusing on text dialogue rather than speech-language modeling, and it does not mention spoken language models or FD-SLMs, thus it does not meet the inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper focuses on a large-scale pre-trained Chinese dialogue system using text-based dialogue data without explicit emphasis on spoken language models, speech-level modeling, or full-duplex spoken interaction capabilities; it primarily addresses text generation in open-domain dialogue rather than speech sequence or acoustic signal modeling, synchronization, or low-latency real-time interactions as required by the inclusion criteria, and therefore does not meet the core scope of spoken language modeling or full-duplex interaction.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper focuses on a large-scale pre-trained Chinese dialogue system using text-based dialogue data without explicit emphasis on spoken language models, speech-level modeling, or full-duplex spoken interaction capabilities; it primarily addresses text generation in open-domain dialogue rather than speech sequence or acoustic signal modeling, synchronization, or low-latency real-time interactions as required by the inclusion criteria, and therefore does not meet the core scope of spoken language modeling or full-duplex interaction.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "A Simplest Systematics for the Organization of Turn-Taking for Conversation",
    "abstract": "The article is the first Russian translation of the most well-known piece in conversation analysis (CA), written by the founders of CA Harvey Sacks, Emanuel Schegloff and Gail Jefferson. It has become a milestone in the development of the discipline. The authors offer a comprehensive approach to the study of conversational interactions. The approach is based on the analysis of detailed transcripts of the records of natural conversations. The authors show that in the course of the conversation co-conversationalists use a number of techniques to organize the turn-taking. These techniques are combined in four rules: (1) the first option is the transfer of speakership via allocation of the next speaker by the current speaker; (2) if this first option is not realized, turn-taking may happen via the self-selection by one of the participants; (3) if the second option remains unrealized too, the current speaker continues speaking, (4) with all three options being recurrently provided at all next transition relevant places. The result of the operation of these rules is an orderly conversation based on the principle “one speaker at a time.” According to the authors, this model is compatible with obvious observations concerning conversational practices that they make. The authors show that in every conversation there is a turn-taking system in operation, which provides for a flexible adaptation of the every conversation’s structure to any possible topics and any possible speakers’ identities. Such approach considers how the participants in social interactions order their communication with each other, achieving a sense of normally occurring interaction.",
    "metadata": {
      "title": "A Simplest Systematics for the Organization of Turn-Taking for Conversation",
      "summary": "The article is the first Russian translation of the most well-known piece in conversation analysis (CA), written by the founders of CA Harvey Sacks, Emanuel Schegloff and Gail Jefferson. It has become a milestone in the development of the discipline. The authors offer a comprehensive approach to the study of conversational interactions. The approach is based on the analysis of detailed transcripts of the records of natural conversations. The authors show that in the course of the conversation co-conversationalists use a number of techniques to organize the turn-taking. These techniques are combined in four rules: (1) the first option is the transfer of speakership via allocation of the next speaker by the current speaker; (2) if this first option is not realized, turn-taking may happen via the self-selection by one of the participants; (3) if the second option remains unrealized too, the current speaker continues speaking, (4) with all three options being recurrently provided at all next transition relevant places. The result of the operation of these rules is an orderly conversation based on the principle “one speaker at a time.” According to the authors, this model is compatible with obvious observations concerning conversational practices that they make. The authors show that in every conversation there is a turn-taking system in operation, which provides for a flexible adaptation of the every conversation’s structure to any possible topics and any possible speakers’ identities. Such approach considers how the participants in social interactions order their communication with each other, achieving a sense of normally occurring interaction.",
      "abstract": "The article is the first Russian translation of the most well-known piece in conversation analysis (CA), written by the founders of CA Harvey Sacks, Emanuel Schegloff and Gail Jefferson. It has become a milestone in the development of the discipline. The authors offer a comprehensive approach to the study of conversational interactions. The approach is based on the analysis of detailed transcripts of the records of natural conversations. The authors show that in the course of the conversation co-conversationalists use a number of techniques to organize the turn-taking. These techniques are combined in four rules: (1) the first option is the transfer of speakership via allocation of the next speaker by the current speaker; (2) if this first option is not realized, turn-taking may happen via the self-selection by one of the participants; (3) if the second option remains unrealized too, the current speaker continues speaking, (4) with all three options being recurrently provided at all next transition relevant places. The result of the operation of these rules is an orderly conversation based on the principle “one speaker at a time.” According to the authors, this model is compatible with obvious observations concerning conversational practices that they make. The authors show that in every conversation there is a turn-taking system in operation, which provides for a flexible adaptation of the every conversation’s structure to any possible topics and any possible speakers’ identities. Such approach considers how the participants in social interactions order their communication with each other, achieving a sense of normally occurring interaction.",
      "doi": "https://doi.org/10.17323/1728-192x-2015-1-142-202",
      "openalex_id": "https://openalex.org/W4241891521",
      "arxiv_id": "",
      "publication_date": "2015-01-01",
      "published": "2015-01-01",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This is a classic conversation analysis study on turn-taking (Sacks, Schegloff, Jefferson) with no focus on Spoken Language Models or FD-SLM architectures, not within the last 3 years, and does not discuss model training/evaluation frameworks; thus it fails all inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "This is a classic conversation analysis study on turn-taking (Sacks, Schegloff, Jefferson) with no focus on Spoken Language Models or FD-SLM architectures, not within the last 3 years, and does not discuss model training/evaluation frameworks; thus it fails all inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The article is a translation of a foundational conversation analysis paper discussing turn-taking in conversations, but it does not address spoken language models (SLMs) or speech language model architectures, training, or evaluation as required by the inclusion criteria; it also lacks discussion on full-duplex spoken interactions, cognitive parallelism, or any evaluated modeling framework and is older foundational work rather than recent research within the last three years.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The article is a translation of a foundational conversation analysis paper discussing turn-taking in conversations, but it does not address spoken language models (SLMs) or speech language model architectures, training, or evaluation as required by the inclusion criteria; it also lacks discussion on full-duplex spoken interactions, cognitive parallelism, or any evaluated modeling framework and is older foundational work rather than recent research within the last three years.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "ReMixMatch: Semi-Supervised Learning with Distribution Alignment and\\n Augmentation Anchoring",
    "abstract": "We improve the recently-proposed \"MixMatch\" semi-supervised learning\\nalgorithm by introducing two new techniques: distribution alignment and\\naugmentation anchoring. Distribution alignment encourages the marginal\\ndistribution of predictions on unlabeled data to be close to the marginal\\ndistribution of ground-truth labels. Augmentation anchoring feeds multiple\\nstrongly augmented versions of an input into the model and encourages each\\noutput to be close to the prediction for a weakly-augmented version of the same\\ninput. To produce strong augmentations, we propose a variant of AutoAugment\\nwhich learns the augmentation policy while the model is being trained. Our new\\nalgorithm, dubbed ReMixMatch, is significantly more data-efficient than prior\\nwork, requiring between $5\\\\times$ and $16\\\\times$ less data to reach the same\\naccuracy. For example, on CIFAR-10 with 250 labeled examples we reach $93.73\\\\%$\\naccuracy (compared to MixMatch's accuracy of $93.58\\\\%$ with $4{,}000$ examples)\\nand a median accuracy of $84.92\\\\%$ with just four labels per class. We make our\\ncode and data open-source at https://github.com/google-research/remixmatch.\\n",
    "metadata": {
      "title": "ReMixMatch: Semi-Supervised Learning with Distribution Alignment and\\n Augmentation Anchoring",
      "summary": "We improve the recently-proposed \"MixMatch\" semi-supervised learning\\nalgorithm by introducing two new techniques: distribution alignment and\\naugmentation anchoring. Distribution alignment encourages the marginal\\ndistribution of predictions on unlabeled data to be close to the marginal\\ndistribution of ground-truth labels. Augmentation anchoring feeds multiple\\nstrongly augmented versions of an input into the model and encourages each\\noutput to be close to the prediction for a weakly-augmented version of the same\\ninput. To produce strong augmentations, we propose a variant of AutoAugment\\nwhich learns the augmentation policy while the model is being trained. Our new\\nalgorithm, dubbed ReMixMatch, is significantly more data-efficient than prior\\nwork, requiring between $5\\\\times$ and $16\\\\times$ less data to reach the same\\naccuracy. For example, on CIFAR-10 with 250 labeled examples we reach $93.73\\\\%$\\naccuracy (compared to MixMatch's accuracy of $93.58\\\\%$ with $4{,}000$ examples)\\nand a median accuracy of $84.92\\\\%$ with just four labels per class. We make our\\ncode and data open-source at https://github.com/google-research/remixmatch.\\n",
      "abstract": "We improve the recently-proposed \"MixMatch\" semi-supervised learning\\nalgorithm by introducing two new techniques: distribution alignment and\\naugmentation anchoring. Distribution alignment encourages the marginal\\ndistribution of predictions on unlabeled data to be close to the marginal\\ndistribution of ground-truth labels. Augmentation anchoring feeds multiple\\nstrongly augmented versions of an input into the model and encourages each\\noutput to be close to the prediction for a weakly-augmented version of the same\\ninput. To produce strong augmentations, we propose a variant of AutoAugment\\nwhich learns the augmentation policy while the model is being trained. Our new\\nalgorithm, dubbed ReMixMatch, is significantly more data-efficient than prior\\nwork, requiring between $5\\\\times$ and $16\\\\times$ less data to reach the same\\naccuracy. For example, on CIFAR-10 with 250 labeled examples we reach $93.73\\\\%$\\naccuracy (compared to MixMatch's accuracy of $93.58\\\\%$ with $4{,}000$ examples)\\nand a median accuracy of $84.92\\\\%$ with just four labels per class. We make our\\ncode and data open-source at https://github.com/google-research/remixmatch.\\n",
      "doi": "https://doi.org/10.48550/arxiv.1911.09785",
      "openalex_id": "https://openalex.org/W4288020585",
      "arxiv_id": "",
      "publication_date": "2019-11-21",
      "published": "2019-11-21",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This item is about semi-supervised image learning (MixMatch/ReMixMatch) and does not address Spoken Language Models, speech-language modeling, or any speech interaction/overlap evaluation, thus it does not meet the included topic and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "This item is about semi-supervised image learning (MixMatch/ReMixMatch) and does not address Spoken Language Models, speech-language modeling, or any speech interaction/overlap evaluation, thus it does not meet the included topic and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper ReMixMatch focuses on semi-supervised learning methods for image data (e.g., CIFAR-10), improving data efficiency, and does not deal with spoken language models, speech language modeling, or real-time spoken interaction aspects like full-duplex communication or synchronization. There is no mention of speech, spoken language modeling, or the required focus areas within the title or abstract, thus it does not meet the inclusion criteria related to spoken language models or related evaluation points.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper ReMixMatch focuses on semi-supervised learning methods for image data (e.g., CIFAR-10), improving data efficiency, and does not deal with spoken language models, speech language modeling, or real-time spoken interaction aspects like full-duplex communication or synchronization. There is no mention of speech, spoken language modeling, or the required focus areas within the title or abstract, thus it does not meet the inclusion criteria related to spoken language models or related evaluation points.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Revisiting the Boundary between ASR and NLU in the Age of Conversational Dialog Systems",
    "abstract": "Abstract As more users across the world are interacting with dialog agents in their daily life, there is a need for better speech understanding that calls for renewed attention to the dynamics between research in automatic speech recognition (ASR) and natural language understanding (NLU). We briefly review these research areas and lay out the current relationship between them. In light of the observations we make in this article, we argue that (1) NLU should be cognizant of the presence of ASR models being used upstream in a dialog system’s pipeline, (2) ASR should be able to learn from errors found in NLU, (3) there is a need for end-to-end data sets that provide semantic annotations on spoken input, (4) there should be stronger collaboration between ASR and NLU research communities.",
    "metadata": {
      "title": "Revisiting the Boundary between ASR and NLU in the Age of Conversational Dialog Systems",
      "summary": "Abstract As more users across the world are interacting with dialog agents in their daily life, there is a need for better speech understanding that calls for renewed attention to the dynamics between research in automatic speech recognition (ASR) and natural language understanding (NLU). We briefly review these research areas and lay out the current relationship between them. In light of the observations we make in this article, we argue that (1) NLU should be cognizant of the presence of ASR models being used upstream in a dialog system’s pipeline, (2) ASR should be able to learn from errors found in NLU, (3) there is a need for end-to-end data sets that provide semantic annotations on spoken input, (4) there should be stronger collaboration between ASR and NLU research communities.",
      "abstract": "Abstract As more users across the world are interacting with dialog agents in their daily life, there is a need for better speech understanding that calls for renewed attention to the dynamics between research in automatic speech recognition (ASR) and natural language understanding (NLU). We briefly review these research areas and lay out the current relationship between them. In light of the observations we make in this article, we argue that (1) NLU should be cognizant of the presence of ASR models being used upstream in a dialog system’s pipeline, (2) ASR should be able to learn from errors found in NLU, (3) there is a need for end-to-end data sets that provide semantic annotations on spoken input, (4) there should be stronger collaboration between ASR and NLU research communities.",
      "doi": "https://doi.org/10.1162/coli_a_00430",
      "openalex_id": "https://openalex.org/W4200113972",
      "arxiv_id": "",
      "publication_date": "2021-12-22",
      "published": "2021-12-22",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The abstract focuses on the relationship between ASR and NLU in dialog systems and on end-to-end data with semantic annotations, but it does not center on Spoken Language Models or any equivalent speech-centric modeling (e.g., full-duplex SLMs), nor does it discuss speech-centric architectures, training, or evaluation in the required sense, nor synchronous turn-taking beyond general dialog context.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The abstract focuses on the relationship between ASR and NLU in dialog systems and on end-to-end data with semantic annotations, but it does not center on Spoken Language Models or any equivalent speech-centric modeling (e.g., full-duplex SLMs), nor does it discuss speech-centric architectures, training, or evaluation in the required sense, nor synchronous turn-taking beyond general dialog context.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper focuses on the relationship between ASR and NLU in conversational dialog systems, discussing integration and collaboration rather than explicit spoken language modeling or full-duplex speech interactions; it lacks emphasis on spoken language models as the core modeling object and does not address the specific inclusion criteria such as full-duplex interaction, low-latency, cognitive parallelism, or relevant evaluation metrics.",
      "evaluation": 2
    },
    "round-A_JuniorMini_reasoning": "The paper focuses on the relationship between ASR and NLU in conversational dialog systems, discussing integration and collaboration rather than explicit spoken language modeling or full-duplex speech interactions; it lacks emphasis on spoken language models as the core modeling object and does not address the specific inclusion criteria such as full-duplex interaction, low-latency, cognitive parallelism, or relevant evaluation metrics.",
    "round-A_JuniorMini_evaluation": 2,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:2)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "MeLL: Large-scale Extensible User Intent Classification for Dialogue Systems with Meta Lifelong Learning",
    "abstract": "User intent detection is vital for understanding their demands in dialogue systems. Although the User Intent Classification (UIC) task has been widely studied, for large-scale industrial applications, the task is still challenging. This is because user inputs in distinct domains may have different text distributions and target intent sets. When the underlying application evolves, new UIC tasks continuously emerge in a large quantity. Hence, it is crucial to develop a framework for large-scale extensible UIC that continuously fits new tasks and avoids catastrophic forgetting with an acceptable parameter growth rate. In this paper, we introduce the Meta Lifelong Learning (MeLL) framework to address this task. In MeLL, a BERT-based text encoder is employed to learn robust text representations across tasks, which is slowly updated for lifelong learning. We design global and local memory networks to capture the cross-task prototype representations of different classes, treated as the meta-learner quickly adapted to different tasks. Additionally, the Least Recently Used replacement policy is applied to manage the global memory such that the model size does not explode through time. Finally, each UIC task has its own task-specific output layer, with the attentive summarization of various features. We have conducted extensive experiments on both open-source and real industry datasets. Results show that MeLL improves the performance compared with strong baselines and also reduces the number of total parameters. We have also deployed MeLL on a real-world e-commerce dialogue system AliMe and observed significant improvements in terms of both F1 and the resources usage.",
    "metadata": {
      "title": "MeLL: Large-scale Extensible User Intent Classification for Dialogue Systems with Meta Lifelong Learning",
      "summary": "User intent detection is vital for understanding their demands in dialogue systems. Although the User Intent Classification (UIC) task has been widely studied, for large-scale industrial applications, the task is still challenging. This is because user inputs in distinct domains may have different text distributions and target intent sets. When the underlying application evolves, new UIC tasks continuously emerge in a large quantity. Hence, it is crucial to develop a framework for large-scale extensible UIC that continuously fits new tasks and avoids catastrophic forgetting with an acceptable parameter growth rate. In this paper, we introduce the Meta Lifelong Learning (MeLL) framework to address this task. In MeLL, a BERT-based text encoder is employed to learn robust text representations across tasks, which is slowly updated for lifelong learning. We design global and local memory networks to capture the cross-task prototype representations of different classes, treated as the meta-learner quickly adapted to different tasks. Additionally, the Least Recently Used replacement policy is applied to manage the global memory such that the model size does not explode through time. Finally, each UIC task has its own task-specific output layer, with the attentive summarization of various features. We have conducted extensive experiments on both open-source and real industry datasets. Results show that MeLL improves the performance compared with strong baselines and also reduces the number of total parameters. We have also deployed MeLL on a real-world e-commerce dialogue system AliMe and observed significant improvements in terms of both F1 and the resources usage.",
      "abstract": "User intent detection is vital for understanding their demands in dialogue systems. Although the User Intent Classification (UIC) task has been widely studied, for large-scale industrial applications, the task is still challenging. This is because user inputs in distinct domains may have different text distributions and target intent sets. When the underlying application evolves, new UIC tasks continuously emerge in a large quantity. Hence, it is crucial to develop a framework for large-scale extensible UIC that continuously fits new tasks and avoids catastrophic forgetting with an acceptable parameter growth rate. In this paper, we introduce the Meta Lifelong Learning (MeLL) framework to address this task. In MeLL, a BERT-based text encoder is employed to learn robust text representations across tasks, which is slowly updated for lifelong learning. We design global and local memory networks to capture the cross-task prototype representations of different classes, treated as the meta-learner quickly adapted to different tasks. Additionally, the Least Recently Used replacement policy is applied to manage the global memory such that the model size does not explode through time. Finally, each UIC task has its own task-specific output layer, with the attentive summarization of various features. We have conducted extensive experiments on both open-source and real industry datasets. Results show that MeLL improves the performance compared with strong baselines and also reduces the number of total parameters. We have also deployed MeLL on a real-world e-commerce dialogue system AliMe and observed significant improvements in terms of both F1 and the resources usage.",
      "doi": "https://doi.org/10.1145/3447548.3467107",
      "openalex_id": "https://openalex.org/W3170536409",
      "arxiv_id": "",
      "publication_date": "2021-08-12",
      "published": "2021-08-12",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This item focuses on text-based user intent classification for dialogue systems using BERT and lifelong learning; it does not center on Spoken Language Models or speech-language modeling, nor on full-duplex interactive speech processing, which are required by the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "This item focuses on text-based user intent classification for dialogue systems using BERT and lifelong learning; it does not center on Spoken Language Models or speech-language modeling, nor on full-duplex interactive speech processing, which are required by the inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper focuses on user intent classification in dialogue systems using a BERT-based text encoder and meta lifelong learning but does not involve spoken language models, speech sequence modeling, or any aspect of speech acoustic signal modeling or full-duplex spoken interaction as required by the inclusion criteria. It primarily treats text inputs and does not address speech-level modeling, synchronization issues, or acoustic features, which are core to the given topic.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper focuses on user intent classification in dialogue systems using a BERT-based text encoder and meta lifelong learning but does not involve spoken language models, speech sequence modeling, or any aspect of speech acoustic signal modeling or full-duplex spoken interaction as required by the inclusion criteria. It primarily treats text inputs and does not address speech-level modeling, synchronization issues, or acoustic features, which are core to the given topic.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Continuously Predicting and Processing Barge-in During a Live Spoken Dialogue Task",
    "abstract": "Barge-in enables the user to provide input during system speech, facilitating a more natural and efficient interaction. Standard methods generally focus on singlestage barge-in detection, applying the dialogue policy irrespective of the barge-in context. Unfortunately, this approach performs poorly when used in challenging environments. We propose and evaluate a barge-in processing method that uses a prediction strategy to continuously decide whether to pause, continue, or resume the prompt. This model has greater task success and efficiency than the standard approach when evaluated in a public spoken dialogue system. Index Terms: spoken dialogue systems, barge-in 1",
    "metadata": {
      "title": "Continuously Predicting and Processing Barge-in During a Live Spoken Dialogue Task",
      "summary": "Barge-in enables the user to provide input during system speech, facilitating a more natural and efficient interaction. Standard methods generally focus on singlestage barge-in detection, applying the dialogue policy irrespective of the barge-in context. Unfortunately, this approach performs poorly when used in challenging environments. We propose and evaluate a barge-in processing method that uses a prediction strategy to continuously decide whether to pause, continue, or resume the prompt. This model has greater task success and efficiency than the standard approach when evaluated in a public spoken dialogue system. Index Terms: spoken dialogue systems, barge-in 1",
      "abstract": "Barge-in enables the user to provide input during system speech, facilitating a more natural and efficient interaction. Standard methods generally focus on singlestage barge-in detection, applying the dialogue policy irrespective of the barge-in context. Unfortunately, this approach performs poorly when used in challenging environments. We propose and evaluate a barge-in processing method that uses a prediction strategy to continuously decide whether to pause, continue, or resume the prompt. This model has greater task success and efficiency than the standard approach when evaluated in a public spoken dialogue system. Index Terms: spoken dialogue systems, barge-in 1",
      "doi": "",
      "openalex_id": "https://openalex.org/W1010839910",
      "arxiv_id": "",
      "publication_date": "2013-08-01",
      "published": "2013-08-01",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The article centers on barge-in decision strategies in live dialogue but does not clearly center on Spoken Language Models or Full-Duplex SLM architectures, nor explicitly discuss speech-language modeling components, thus fails inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The article centers on barge-in decision strategies in live dialogue but does not clearly center on Spoken Language Models or Full-Duplex SLM architectures, nor explicitly discuss speech-language modeling components, thus fails inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper focuses on barge-in detection and processing during live spoken dialogue, aiming to improve interaction efficiency and success in spoken dialogue systems. However, the study does not explicitly discuss spoken language models (SLMs) or full-duplex spoken language models (FD-SLMs), nor does it mention language modeling of speech or present a model architecture or training related to SLMs. The emphasis is on dialogue policy and barge-in handling rather than speech language modeling or end-to-end architecture training or evaluation, which is critical according to the inclusion criteria. Therefore, the work does not align closely with the core inclusion requirement of focusing on spoken language modeling as the main research object, despite being related to spoken dialogue systems. Hence, it aligns better with exclusion criteria such as lacking explicit SLM discussion and focusing more on interaction management than speech modeling.",
      "evaluation": 2
    },
    "round-A_JuniorMini_reasoning": "The paper focuses on barge-in detection and processing during live spoken dialogue, aiming to improve interaction efficiency and success in spoken dialogue systems. However, the study does not explicitly discuss spoken language models (SLMs) or full-duplex spoken language models (FD-SLMs), nor does it mention language modeling of speech or present a model architecture or training related to SLMs. The emphasis is on dialogue policy and barge-in handling rather than speech language modeling or end-to-end architecture training or evaluation, which is critical according to the inclusion criteria. Therefore, the work does not align closely with the core inclusion requirement of focusing on spoken language modeling as the main research object, despite being related to spoken dialogue systems. Hence, it aligns better with exclusion criteria such as lacking explicit SLM discussion and focusing more on interaction management than speech modeling.",
    "round-A_JuniorMini_evaluation": 2,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:2)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Enabling a user to specify an item at any time during system enumeration - item identification for barge-in-able conversational dialogue systems",
    "abstract": "In conversational dialogue systems, users prefer to speak at any time and to use natural expressions. We have developed an Independent Component Analysis (ICA) based semi-blind source separation method, which allows users to barge-in over system utterances at any time. We created a novel method from timing information derived from barge-in utterances to identify one item that a user indicates during system enumeration. First, we determine the timing distribution of user utterances containing referential expressions and then approximate it using a gamma distribution. Second, we represent both the utterance timing and automatic speech recognition (ASR) results as probabilities of the desired selection from the system’s enumeration. We then integrate these two probabilities to identify the item having the maximum likelihood of selection. Experimental results using 400 utterances indicated that our method outperformed two methods used as a baseline (one of ASR results only and one of utterance timing only) in identification accuracy. Index Terms: spoken dialogue system, conversational interaction, barge-in, utterance timing",
    "metadata": {
      "title": "Enabling a user to specify an item at any time during system enumeration - item identification for barge-in-able conversational dialogue systems",
      "summary": "In conversational dialogue systems, users prefer to speak at any time and to use natural expressions. We have developed an Independent Component Analysis (ICA) based semi-blind source separation method, which allows users to barge-in over system utterances at any time. We created a novel method from timing information derived from barge-in utterances to identify one item that a user indicates during system enumeration. First, we determine the timing distribution of user utterances containing referential expressions and then approximate it using a gamma distribution. Second, we represent both the utterance timing and automatic speech recognition (ASR) results as probabilities of the desired selection from the system’s enumeration. We then integrate these two probabilities to identify the item having the maximum likelihood of selection. Experimental results using 400 utterances indicated that our method outperformed two methods used as a baseline (one of ASR results only and one of utterance timing only) in identification accuracy. Index Terms: spoken dialogue system, conversational interaction, barge-in, utterance timing",
      "abstract": "In conversational dialogue systems, users prefer to speak at any time and to use natural expressions. We have developed an Independent Component Analysis (ICA) based semi-blind source separation method, which allows users to barge-in over system utterances at any time. We created a novel method from timing information derived from barge-in utterances to identify one item that a user indicates during system enumeration. First, we determine the timing distribution of user utterances containing referential expressions and then approximate it using a gamma distribution. Second, we represent both the utterance timing and automatic speech recognition (ASR) results as probabilities of the desired selection from the system’s enumeration. We then integrate these two probabilities to identify the item having the maximum likelihood of selection. Experimental results using 400 utterances indicated that our method outperformed two methods used as a baseline (one of ASR results only and one of utterance timing only) in identification accuracy. Index Terms: spoken dialogue system, conversational interaction, barge-in, utterance timing",
      "doi": "https://doi.org/10.21437/interspeech.2009-88",
      "openalex_id": "https://openalex.org/W2162556815",
      "arxiv_id": "",
      "publication_date": "2009-09-06",
      "published": "2009-09-06",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The work describes an ICA-based method for enabling barge-in in a conversational system and uses timing/ASR for item identification, but it does not clearly position itself around Spoken Language Models or speech-language modeling architectures/training/evaluation, so it does not meet the core inclusion criterion.",
      "evaluation": 2
    },
    "round-A_JuniorNano_reasoning": "The work describes an ICA-based method for enabling barge-in in a conversational system and uses timing/ASR for item identification, but it does not clearly position itself around Spoken Language Models or speech-language modeling architectures/training/evaluation, so it does not meet the core inclusion criterion.",
    "round-A_JuniorNano_evaluation": 2,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on a barge-in conversational dialogue system using timing and ASR results to identify user-selected items during system enumeration. While it addresses interruption and user timing during dialogue interaction, it does not explicitly discuss spoken language models, full-duplex spoken language models, or speech-language model architectures, training, or evaluation. The core is ASR and timing-based utterance identification, not modeling speech sequences or low-latency full-duplex interactions in the sense of the inclusion criteria. Also, it does not mention 'spoken language model(s)' or equivalent in title/abstract. Hence, it does not clearly meet the core inclusion criteria and focuses more on a specific spoken dialog interaction technique rather than the broader spoken language modeling research scope required.",
      "evaluation": 2
    },
    "round-A_JuniorMini_reasoning": "The study focuses on a barge-in conversational dialogue system using timing and ASR results to identify user-selected items during system enumeration. While it addresses interruption and user timing during dialogue interaction, it does not explicitly discuss spoken language models, full-duplex spoken language models, or speech-language model architectures, training, or evaluation. The core is ASR and timing-based utterance identification, not modeling speech sequences or low-latency full-duplex interactions in the sense of the inclusion criteria. Also, it does not mention 'spoken language model(s)' or equivalent in title/abstract. Hence, it does not clearly meet the core inclusion criteria and focuses more on a specific spoken dialog interaction technique rather than the broader spoken language modeling research scope required.",
    "round-A_JuniorMini_evaluation": 2,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:2)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Unified Dialog Model Pre-training for Task-Oriented Dialog Understanding and Generation",
    "abstract": "Recently, pre-training methods have shown remarkable success in task-oriented dialog (TOD) systems. However, most existing pre-trained models for TOD focus on either dialog understanding or dialog generation, but not both. In this paper, we propose SPACE, a novel unified pre-trained dialog model learning from large-scale dialog corpora with limited annotations, which can be effectively fine-tuned on a wide range of downstream dialog tasks. Specifically, SPACE consists of four successive components in a single transformer to maintain a task-flow in TOD systems: (i) a dialog encoding module to encode dialog history, (ii) a dialog understanding module to extract semantic vectors from either user queries or system responses, (iii) a dialog policy module to generate a policy vector that contains high-level semantics of the response, and (iv) a dialog generation module to produce appropriate responses. We design a dedicated pre-training objective for each component. Concretely, we pre-train the dialog encoding module with span mask language modeling to learn contextualized dialog information. To capture the structured dialog semantics, we pre-train the dialog understanding module via a novel tree-induced semi-supervised contrastive learning objective with the help of extra dialog annotations. In addition, we pre-train the dialog policy module by minimizing the ℒ2 distance between its output policy vector and the semantic vector of the response for policy optimization. Finally, the dialog generation model is pre-trained by language modeling. Results show that SPACE achieves state-of-the-art performance on eight downstream dialog benchmarks, including intent prediction, dialog state tracking, and end-to-end dialog modeling. We also show that SPACE has a stronger few-shot ability than existing models under the low-resource setting.",
    "metadata": {
      "title": "Unified Dialog Model Pre-training for Task-Oriented Dialog Understanding and Generation",
      "summary": "Recently, pre-training methods have shown remarkable success in task-oriented dialog (TOD) systems. However, most existing pre-trained models for TOD focus on either dialog understanding or dialog generation, but not both. In this paper, we propose SPACE, a novel unified pre-trained dialog model learning from large-scale dialog corpora with limited annotations, which can be effectively fine-tuned on a wide range of downstream dialog tasks. Specifically, SPACE consists of four successive components in a single transformer to maintain a task-flow in TOD systems: (i) a dialog encoding module to encode dialog history, (ii) a dialog understanding module to extract semantic vectors from either user queries or system responses, (iii) a dialog policy module to generate a policy vector that contains high-level semantics of the response, and (iv) a dialog generation module to produce appropriate responses. We design a dedicated pre-training objective for each component. Concretely, we pre-train the dialog encoding module with span mask language modeling to learn contextualized dialog information. To capture the structured dialog semantics, we pre-train the dialog understanding module via a novel tree-induced semi-supervised contrastive learning objective with the help of extra dialog annotations. In addition, we pre-train the dialog policy module by minimizing the ℒ2 distance between its output policy vector and the semantic vector of the response for policy optimization. Finally, the dialog generation model is pre-trained by language modeling. Results show that SPACE achieves state-of-the-art performance on eight downstream dialog benchmarks, including intent prediction, dialog state tracking, and end-to-end dialog modeling. We also show that SPACE has a stronger few-shot ability than existing models under the low-resource setting.",
      "abstract": "Recently, pre-training methods have shown remarkable success in task-oriented dialog (TOD) systems. However, most existing pre-trained models for TOD focus on either dialog understanding or dialog generation, but not both. In this paper, we propose SPACE, a novel unified pre-trained dialog model learning from large-scale dialog corpora with limited annotations, which can be effectively fine-tuned on a wide range of downstream dialog tasks. Specifically, SPACE consists of four successive components in a single transformer to maintain a task-flow in TOD systems: (i) a dialog encoding module to encode dialog history, (ii) a dialog understanding module to extract semantic vectors from either user queries or system responses, (iii) a dialog policy module to generate a policy vector that contains high-level semantics of the response, and (iv) a dialog generation module to produce appropriate responses. We design a dedicated pre-training objective for each component. Concretely, we pre-train the dialog encoding module with span mask language modeling to learn contextualized dialog information. To capture the structured dialog semantics, we pre-train the dialog understanding module via a novel tree-induced semi-supervised contrastive learning objective with the help of extra dialog annotations. In addition, we pre-train the dialog policy module by minimizing the ℒ2 distance between its output policy vector and the semantic vector of the response for policy optimization. Finally, the dialog generation model is pre-trained by language modeling. Results show that SPACE achieves state-of-the-art performance on eight downstream dialog benchmarks, including intent prediction, dialog state tracking, and end-to-end dialog modeling. We also show that SPACE has a stronger few-shot ability than existing models under the low-resource setting.",
      "doi": "https://doi.org/10.1145/3477495.3532069",
      "openalex_id": "https://openalex.org/W4284683546",
      "arxiv_id": "",
      "publication_date": "2022-07-06",
      "published": "2022-07-06",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The paper focuses on text-based task-oriented dialog pre-training with modules for encoding, understanding, policy, and generation, and does not center on spoken language models or speech signals, nor on synchronous speech interactions.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The paper focuses on text-based task-oriented dialog pre-training with modules for encoding, understanding, policy, and generation, and does not center on spoken language models or speech signals, nor on synchronous speech interactions.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper focuses on a unified pre-trained dialog model for task-oriented dialog systems, emphasizing dialog understanding and generation with transformer components and various pre-training objectives, but it does not mention spoken language models, speech-level language modeling, full-duplex spoken interaction, or any speech/acoustic signal modeling. The research centers on text/dialog modeling rather than speech-based modeling or synchronous speech interaction, which are core inclusion criteria. Therefore, it does not meet the thematic requirements.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper focuses on a unified pre-trained dialog model for task-oriented dialog systems, emphasizing dialog understanding and generation with transformer components and various pre-training objectives, but it does not mention spoken language models, speech-level language modeling, full-duplex spoken interaction, or any speech/acoustic signal modeling. The research centers on text/dialog modeling rather than speech-based modeling or synchronous speech interaction, which are core inclusion criteria. Therefore, it does not meet the thematic requirements.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "A finite-state turn-taking model for spoken dialog systems",
    "abstract": "This paper introduces the Finite-State Turn-Taking Machine (FSTTM), a new model to control the turn-taking behavior of conversational agents. Based on a non-deterministic finite-state machine, the FSTTM uses a cost matrix and decision theoretic principles to select a turn-taking action at any time. We show how the model can be applied to the problem of end-of-turn detection. Evaluation results on a deployed spoken dialog system show that the FSTTM provides significantly higher responsiveness than previous approaches.",
    "metadata": {
      "title": "A finite-state turn-taking model for spoken dialog systems",
      "summary": "This paper introduces the Finite-State Turn-Taking Machine (FSTTM), a new model to control the turn-taking behavior of conversational agents. Based on a non-deterministic finite-state machine, the FSTTM uses a cost matrix and decision theoretic principles to select a turn-taking action at any time. We show how the model can be applied to the problem of end-of-turn detection. Evaluation results on a deployed spoken dialog system show that the FSTTM provides significantly higher responsiveness than previous approaches.",
      "abstract": "This paper introduces the Finite-State Turn-Taking Machine (FSTTM), a new model to control the turn-taking behavior of conversational agents. Based on a non-deterministic finite-state machine, the FSTTM uses a cost matrix and decision theoretic principles to select a turn-taking action at any time. We show how the model can be applied to the problem of end-of-turn detection. Evaluation results on a deployed spoken dialog system show that the FSTTM provides significantly higher responsiveness than previous approaches.",
      "doi": "https://doi.org/10.3115/1620754.1620846",
      "openalex_id": "https://openalex.org/W2067097374",
      "arxiv_id": "",
      "publication_date": "2009-01-01",
      "published": "2009-01-01",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The paper addresses a turn-taking mechanism for spoken dialog systems but does not treat speech-language modeling (SLMs/FD-SLMs) as its core focus; it lacks explicit modeling/training of speech as the primary object and does not clearly discuss full-duplex synchronization or speech-language modeling evaluations.",
      "evaluation": 2
    },
    "round-A_JuniorNano_reasoning": "The paper addresses a turn-taking mechanism for spoken dialog systems but does not treat speech-language modeling (SLMs/FD-SLMs) as its core focus; it lacks explicit modeling/training of speech as the primary object and does not clearly discuss full-duplex synchronization or speech-language modeling evaluations.",
    "round-A_JuniorNano_evaluation": 2,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper presents a finite-state model focusing on turn-taking control in spoken dialog systems primarily as an engineering controller rather than proposing or evaluating spoken language models with speech signal-centric modeling, training, or end-to-end architectures. It focuses on half-duplex turn-taking rather than full-duplex or learned synchronization, and does not mention language modeling of speech or a speech-LLM hybrid approach. Although it discusses dialog behavior arbitration, the scope is limited to a turn-taking finite-state machine without clear involvement of spoken language models or speech-based language representations, nor explicit evaluation of low-latency or multi-dimensional speech modeling criteria as required. Thus, it does not meet the inclusion criteria fully and aligns with exclusion points regarding traditional half-duplex turn-taking and lack of speech language modeling focus.",
      "evaluation": 2
    },
    "round-A_JuniorMini_reasoning": "The paper presents a finite-state model focusing on turn-taking control in spoken dialog systems primarily as an engineering controller rather than proposing or evaluating spoken language models with speech signal-centric modeling, training, or end-to-end architectures. It focuses on half-duplex turn-taking rather than full-duplex or learned synchronization, and does not mention language modeling of speech or a speech-LLM hybrid approach. Although it discusses dialog behavior arbitration, the scope is limited to a turn-taking finite-state machine without clear involvement of spoken language models or speech-based language representations, nor explicit evaluation of low-latency or multi-dimensional speech modeling criteria as required. Thus, it does not meet the inclusion criteria fully and aligns with exclusion points regarding traditional half-duplex turn-taking and lack of speech language modeling focus.",
    "round-A_JuniorMini_evaluation": 2,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:2)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "MISA: Modality-Invariant and -Specific Representations for Multimodal\\n Sentiment Analysis",
    "abstract": "Multimodal Sentiment Analysis is an active area of research that leverages\\nmultimodal signals for affective understanding of user-generated videos. The\\npredominant approach, addressing this task, has been to develop sophisticated\\nfusion techniques. However, the heterogeneous nature of the signals creates\\ndistributional modality gaps that pose significant challenges. In this paper,\\nwe aim to learn effective modality representations to aid the process of\\nfusion. We propose a novel framework, MISA, which projects each modality to two\\ndistinct subspaces. The first subspace is modality-invariant, where the\\nrepresentations across modalities learn their commonalities and reduce the\\nmodality gap. The second subspace is modality-specific, which is private to\\neach modality and captures their characteristic features. These representations\\nprovide a holistic view of the multimodal data, which is used for fusion that\\nleads to task predictions. Our experiments on popular sentiment analysis\\nbenchmarks, MOSI and MOSEI, demonstrate significant gains over state-of-the-art\\nmodels. We also consider the task of Multimodal Humor Detection and experiment\\non the recently proposed UR_FUNNY dataset. Here too, our model fares better\\nthan strong baselines, establishing MISA as a useful multimodal framework.\\n",
    "metadata": {
      "title": "MISA: Modality-Invariant and -Specific Representations for Multimodal\\n Sentiment Analysis",
      "summary": "Multimodal Sentiment Analysis is an active area of research that leverages\\nmultimodal signals for affective understanding of user-generated videos. The\\npredominant approach, addressing this task, has been to develop sophisticated\\nfusion techniques. However, the heterogeneous nature of the signals creates\\ndistributional modality gaps that pose significant challenges. In this paper,\\nwe aim to learn effective modality representations to aid the process of\\nfusion. We propose a novel framework, MISA, which projects each modality to two\\ndistinct subspaces. The first subspace is modality-invariant, where the\\nrepresentations across modalities learn their commonalities and reduce the\\nmodality gap. The second subspace is modality-specific, which is private to\\neach modality and captures their characteristic features. These representations\\nprovide a holistic view of the multimodal data, which is used for fusion that\\nleads to task predictions. Our experiments on popular sentiment analysis\\nbenchmarks, MOSI and MOSEI, demonstrate significant gains over state-of-the-art\\nmodels. We also consider the task of Multimodal Humor Detection and experiment\\non the recently proposed UR_FUNNY dataset. Here too, our model fares better\\nthan strong baselines, establishing MISA as a useful multimodal framework.\\n",
      "abstract": "Multimodal Sentiment Analysis is an active area of research that leverages\\nmultimodal signals for affective understanding of user-generated videos. The\\npredominant approach, addressing this task, has been to develop sophisticated\\nfusion techniques. However, the heterogeneous nature of the signals creates\\ndistributional modality gaps that pose significant challenges. In this paper,\\nwe aim to learn effective modality representations to aid the process of\\nfusion. We propose a novel framework, MISA, which projects each modality to two\\ndistinct subspaces. The first subspace is modality-invariant, where the\\nrepresentations across modalities learn their commonalities and reduce the\\nmodality gap. The second subspace is modality-specific, which is private to\\neach modality and captures their characteristic features. These representations\\nprovide a holistic view of the multimodal data, which is used for fusion that\\nleads to task predictions. Our experiments on popular sentiment analysis\\nbenchmarks, MOSI and MOSEI, demonstrate significant gains over state-of-the-art\\nmodels. We also consider the task of Multimodal Humor Detection and experiment\\non the recently proposed UR_FUNNY dataset. Here too, our model fares better\\nthan strong baselines, establishing MISA as a useful multimodal framework.\\n",
      "doi": "https://doi.org/10.48550/arxiv.2005.03545",
      "openalex_id": "https://openalex.org/W4287780442",
      "arxiv_id": "",
      "publication_date": "2020-05-07",
      "published": "2020-05-07",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The paper focuses on multimodal sentiment analysis across audio/video/text modalities and learning modality-invariant/specific representations for fusion, not on Spoken Language Models or speech-centric modeling, thus it does not meet the inclusion criteria and better fits the exclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The paper focuses on multimodal sentiment analysis across audio/video/text modalities and learning modality-invariant/specific representations for fusion, not on Spoken Language Models or speech-centric modeling, thus it does not meet the inclusion criteria and better fits the exclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on Multimodal Sentiment Analysis using modality-invariant and modality-specific representations, primarily processing multimodal signals from videos rather than focusing on spoken language models or speech-level language modeling as core elements. It lacks mention of Spoken Language Models, full-duplex spoken interaction, or speech language modeling as defined in the inclusion criteria. The research does not address engineered or learned synchronization, cognitive parallelism, or real-time interaction latency, nor does it focus on speech modeling as the main object. Therefore, it does not meet the inclusion criteria and matches some exclusion criteria regarding focus on multimodal fusion rather than spoken language modeling.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on Multimodal Sentiment Analysis using modality-invariant and modality-specific representations, primarily processing multimodal signals from videos rather than focusing on spoken language models or speech-level language modeling as core elements. It lacks mention of Spoken Language Models, full-duplex spoken interaction, or speech language modeling as defined in the inclusion criteria. The research does not address engineered or learned synchronization, cognitive parallelism, or real-time interaction latency, nor does it focus on speech modeling as the main object. Therefore, it does not meet the inclusion criteria and matches some exclusion criteria regarding focus on multimodal fusion rather than spoken language modeling.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "MTAG: Modal-Temporal Attention Graph for Unaligned Human Multimodal Language Sequences",
    "abstract": "Jianing Yang, Yongxin Wang, Ruitao Yi, Yuying Zhu, Azaan Rehman, Amir Zadeh, Soujanya Poria, Louis-Philippe Morency. Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. 2021.",
    "metadata": {
      "title": "MTAG: Modal-Temporal Attention Graph for Unaligned Human Multimodal Language Sequences",
      "summary": "Jianing Yang, Yongxin Wang, Ruitao Yi, Yuying Zhu, Azaan Rehman, Amir Zadeh, Soujanya Poria, Louis-Philippe Morency. Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. 2021.",
      "abstract": "Jianing Yang, Yongxin Wang, Ruitao Yi, Yuying Zhu, Azaan Rehman, Amir Zadeh, Soujanya Poria, Louis-Philippe Morency. Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. 2021.",
      "doi": "https://doi.org/10.18653/v1/2021.naacl-main.79",
      "openalex_id": "https://openalex.org/W3167098825",
      "arxiv_id": "",
      "publication_date": "2021-01-01",
      "published": "2021-01-01",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "此論文偏向多模態語言序列的模式-時間注意力圖，未明確以語音為核心的語言模型（SLM/FD-SLM），且發佈於2021年，距今超過三年，未符合時間範圍與核心範疇，因此不符合納入條件。",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "此論文偏向多模態語言序列的模式-時間注意力圖，未明確以語音為核心的語言模型（SLM/FD-SLM），且發佈於2021年，距今超過三年，未符合時間範圍與核心範疇，因此不符合納入條件。",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The title and abstract do not provide information indicating that the study involves Spoken Language Models or full-duplex spoken language interaction; it seems focused on multimodal language sequences but lacks explicit mention of speech as the core modeling object or related evaluation criteria per the inclusion requirements.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The title and abstract do not provide information indicating that the study involves Spoken Language Models or full-duplex spoken language interaction; it seems focused on multimodal language sequences but lacks explicit mention of speech as the core modeling object or related evaluation criteria per the inclusion requirements.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "MUSAN: A Music, Speech, and Noise Corpus",
    "abstract": "This report introduces a new corpus of music, speech, and noise. This dataset is suitable for training models for voice activity detection (VAD) and music/speech discrimination. Our corpus is released under a flexible Creative Commons license. The dataset consists of music from several genres, speech from twelve languages, and a wide assortment of technical and non-technical noises. We demonstrate use of this corpus for music/speech discrimination on Broadcast news and VAD for speaker identification.",
    "metadata": {
      "title": "MUSAN: A Music, Speech, and Noise Corpus",
      "summary": "This report introduces a new corpus of music, speech, and noise. This dataset is suitable for training models for voice activity detection (VAD) and music/speech discrimination. Our corpus is released under a flexible Creative Commons license. The dataset consists of music from several genres, speech from twelve languages, and a wide assortment of technical and non-technical noises. We demonstrate use of this corpus for music/speech discrimination on Broadcast news and VAD for speaker identification.",
      "abstract": "This report introduces a new corpus of music, speech, and noise. This dataset is suitable for training models for voice activity detection (VAD) and music/speech discrimination. Our corpus is released under a flexible Creative Commons license. The dataset consists of music from several genres, speech from twelve languages, and a wide assortment of technical and non-technical noises. We demonstrate use of this corpus for music/speech discrimination on Broadcast news and VAD for speaker identification.",
      "doi": "https://doi.org/10.48550/arxiv.1510.08484",
      "openalex_id": "https://openalex.org/W2219249508",
      "arxiv_id": "",
      "publication_date": "2015-10-28",
      "published": "2015-10-28",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This item presents a corpus for music, speech, and noise aimed at VAD and speech discrimination, but it does not center on Spoken Language Models or speech language modeling, nor discuss model architectures or end-to-end evaluation, thus fails the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "This item presents a corpus for music, speech, and noise aimed at VAD and speech discrimination, but it does not center on Spoken Language Models or speech language modeling, nor discuss model architectures or end-to-end evaluation, thus fails the inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The article introduces a new corpus for voice activity detection and music/speech discrimination, focusing on dataset creation rather than spoken language modeling. It does not mention spoken language models, full-duplex interaction, or any related modeling, training, or evaluation aspects of SLMs or FD-SLMs as required by the inclusion criteria. Hence, the study does not meet the core inclusion requirements and appears more as a data resource than a study on spoken language models or their interaction capabilities.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The article introduces a new corpus for voice activity detection and music/speech discrimination, focusing on dataset creation rather than spoken language modeling. It does not mention spoken language models, full-duplex interaction, or any related modeling, training, or evaluation aspects of SLMs or FD-SLMs as required by the inclusion criteria. Hence, the study does not meet the core inclusion requirements and appears more as a data resource than a study on spoken language models or their interaction capabilities.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Improving Meta-learning for Low-resource Text Classification and Generation via Memory Imitation",
    "abstract": "Building models of natural language processing (NLP) is challenging in low-resource scenarios where only limited data are available. Optimization-based meta-learning algorithms achieve promising results in low-resource scenarios by adapting a well-generalized model initialization to handle new tasks. Nonetheless, these approaches suffer from the memorization overfitting issue, where the model tends to memorize the meta-training tasks while ignoring support sets when adapting to new tasks. To address this issue, we propose a memory imitation meta-learning (MemIML) method that enhances the model’s reliance on support sets for task adaptation. Specifically, we introduce a task-specific memory module to store support set information and construct an imitation module to force query sets to imitate the behaviors of some representative supportset samples stored in the memory. A theoretical analysis is provided to prove the effectiveness of our method, and empirical results also demonstrate that our method outperforms competitive baselines on both text classification and generation tasks.",
    "metadata": {
      "title": "Improving Meta-learning for Low-resource Text Classification and Generation via Memory Imitation",
      "summary": "Building models of natural language processing (NLP) is challenging in low-resource scenarios where only limited data are available. Optimization-based meta-learning algorithms achieve promising results in low-resource scenarios by adapting a well-generalized model initialization to handle new tasks. Nonetheless, these approaches suffer from the memorization overfitting issue, where the model tends to memorize the meta-training tasks while ignoring support sets when adapting to new tasks. To address this issue, we propose a memory imitation meta-learning (MemIML) method that enhances the model’s reliance on support sets for task adaptation. Specifically, we introduce a task-specific memory module to store support set information and construct an imitation module to force query sets to imitate the behaviors of some representative supportset samples stored in the memory. A theoretical analysis is provided to prove the effectiveness of our method, and empirical results also demonstrate that our method outperforms competitive baselines on both text classification and generation tasks.",
      "abstract": "Building models of natural language processing (NLP) is challenging in low-resource scenarios where only limited data are available. Optimization-based meta-learning algorithms achieve promising results in low-resource scenarios by adapting a well-generalized model initialization to handle new tasks. Nonetheless, these approaches suffer from the memorization overfitting issue, where the model tends to memorize the meta-training tasks while ignoring support sets when adapting to new tasks. To address this issue, we propose a memory imitation meta-learning (MemIML) method that enhances the model’s reliance on support sets for task adaptation. Specifically, we introduce a task-specific memory module to store support set information and construct an imitation module to force query sets to imitate the behaviors of some representative supportset samples stored in the memory. A theoretical analysis is provided to prove the effectiveness of our method, and empirical results also demonstrate that our method outperforms competitive baselines on both text classification and generation tasks.",
      "doi": "https://doi.org/10.18653/v1/2022.acl-long.44",
      "openalex_id": "https://openalex.org/W4226174273",
      "arxiv_id": "",
      "publication_date": "2022-01-01",
      "published": "2022-01-01",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "本研究聚焦在低資源文本分類與生成的元學習方法，並非以 Spoken Language Models（語音語言建模）為核心，也未涉及語音互動或語音評估等語音專屬議題，因此不符合本研究的核心納入條件。",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "本研究聚焦在低資源文本分類與生成的元學習方法，並非以 Spoken Language Models（語音語言建模）為核心，也未涉及語音互動或語音評估等語音專屬議題，因此不符合本研究的核心納入條件。",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on meta-learning methods for low-resource text classification and generation in NLP, without involving spoken language models, speech-level language modeling, or multimodal speech-LLM architectures; it lacks any discussion of full-duplex spoken language interaction, synchronization, or relevant speech-specific evaluation criteria, hence does not meet the inclusion criteria regarding spoken language modeling as the core subject.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on meta-learning methods for low-resource text classification and generation in NLP, without involving spoken language models, speech-level language modeling, or multimodal speech-LLM architectures; it lacks any discussion of full-duplex spoken language interaction, synchronization, or relevant speech-specific evaluation criteria, hence does not meet the inclusion criteria regarding spoken language modeling as the core subject.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Statistical learning models of early phonetic acquisition struggle with child-centered audio data",
    "abstract": "Infants learn their native language(s) at an amazing speed. Before they even talk, their perception adapts to the language(s) they hear. However, the mechanisms responsible for this perceptual attunement still remain unclear. A long tradition in linguistics points to the importance of specialized language mechanisms that would allow us to quickly and effortlessly learn from the language(s) we are exposed to. However, the currently dominant explanation for perceptual attunement posits that infants apply a domain-general learning mechanism consisting in learning statistical regularities from the speech stream they hear, and which may be found in learning across domains and across species. Critically, the feasibility of employing purely domain-general statistical learning mechanisms has only been demonstrated with computational models on unrealistic and simplified input. This paper presents the first attempt to study perceptual attunement with 2,000 hours of ecological child-centered recordings in American English and Metropolitan French. We show that, when applied on ecologically-valid data, generic learning mechanisms develop a language-relevant perceptual space but fail to show evidence for perceptual attunement. It is only when supplemented with domain-specific audio filtering and augmentation mechanisms that computational models show a significant attunement to the language they have been exposed to. Hence, we conclude that, when learning from ecological audio, domain-specific mechanisms may be necessary to guide early language learning in the wild even if the learning itself is done through generic mechanisms. We anticipate our work to be a starting point for ecologically-valid computational models of perceptual attunement in other domains and species.",
    "metadata": {
      "title": "Statistical learning models of early phonetic acquisition struggle with child-centered audio data",
      "summary": "Infants learn their native language(s) at an amazing speed. Before they even talk, their perception adapts to the language(s) they hear. However, the mechanisms responsible for this perceptual attunement still remain unclear. A long tradition in linguistics points to the importance of specialized language mechanisms that would allow us to quickly and effortlessly learn from the language(s) we are exposed to. However, the currently dominant explanation for perceptual attunement posits that infants apply a domain-general learning mechanism consisting in learning statistical regularities from the speech stream they hear, and which may be found in learning across domains and across species. Critically, the feasibility of employing purely domain-general statistical learning mechanisms has only been demonstrated with computational models on unrealistic and simplified input. This paper presents the first attempt to study perceptual attunement with 2,000 hours of ecological child-centered recordings in American English and Metropolitan French. We show that, when applied on ecologically-valid data, generic learning mechanisms develop a language-relevant perceptual space but fail to show evidence for perceptual attunement. It is only when supplemented with domain-specific audio filtering and augmentation mechanisms that computational models show a significant attunement to the language they have been exposed to. Hence, we conclude that, when learning from ecological audio, domain-specific mechanisms may be necessary to guide early language learning in the wild even if the learning itself is done through generic mechanisms. We anticipate our work to be a starting point for ecologically-valid computational models of perceptual attunement in other domains and species.",
      "abstract": "Infants learn their native language(s) at an amazing speed. Before they even talk, their perception adapts to the language(s) they hear. However, the mechanisms responsible for this perceptual attunement still remain unclear. A long tradition in linguistics points to the importance of specialized language mechanisms that would allow us to quickly and effortlessly learn from the language(s) we are exposed to. However, the currently dominant explanation for perceptual attunement posits that infants apply a domain-general learning mechanism consisting in learning statistical regularities from the speech stream they hear, and which may be found in learning across domains and across species. Critically, the feasibility of employing purely domain-general statistical learning mechanisms has only been demonstrated with computational models on unrealistic and simplified input. This paper presents the first attempt to study perceptual attunement with 2,000 hours of ecological child-centered recordings in American English and Metropolitan French. We show that, when applied on ecologically-valid data, generic learning mechanisms develop a language-relevant perceptual space but fail to show evidence for perceptual attunement. It is only when supplemented with domain-specific audio filtering and augmentation mechanisms that computational models show a significant attunement to the language they have been exposed to. Hence, we conclude that, when learning from ecological audio, domain-specific mechanisms may be necessary to guide early language learning in the wild even if the learning itself is done through generic mechanisms. We anticipate our work to be a starting point for ecologically-valid computational models of perceptual attunement in other domains and species.",
      "doi": "https://doi.org/10.31234/osf.io/5tmgy",
      "openalex_id": "https://openalex.org/W4220770602",
      "arxiv_id": "",
      "publication_date": "2022-03-08",
      "published": "2022-03-08",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The abstract describes statistical-learning models for perceptual attunement using ecological child-centered audio data, but it does not center on Spoken Language Models / FD-SLMs, nor discuss dedicated model architectures, training regimes, or evaluations typical of SLM-focused work; thus it does not meet the inclusion criteria and should be excluded.",
      "evaluation": 2
    },
    "round-A_JuniorNano_reasoning": "The abstract describes statistical-learning models for perceptual attunement using ecological child-centered audio data, but it does not center on Spoken Language Models / FD-SLMs, nor discuss dedicated model architectures, training regimes, or evaluations typical of SLM-focused work; thus it does not meet the inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 2,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on early phonetic acquisition and perceptual attunement using statistical learning models applied to child-centered audio data, but it does not explicitly discuss spoken language models (SLMs) or full-duplex spoken language models (FD-SLMs) as core components of the research. The modeling centers on infant language acquisition mechanisms rather than spoken language model architectures, training, or evaluation that involve end-to-end or hybrid speech-LLM frameworks or interactive, synchronous communication capabilities. The abstract lacks mention of speech language modeling, full-duplex interaction, synchronization, or the evaluation of such models, focusing instead on domain-general versus domain-specific learning mechanisms for phonetic perception. Hence, it does not meet the inclusion criteria which require explicit focus on spoken language models or related modeling frameworks for speech as a primary modeling object rather than pre-processing for text.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on early phonetic acquisition and perceptual attunement using statistical learning models applied to child-centered audio data, but it does not explicitly discuss spoken language models (SLMs) or full-duplex spoken language models (FD-SLMs) as core components of the research. The modeling centers on infant language acquisition mechanisms rather than spoken language model architectures, training, or evaluation that involve end-to-end or hybrid speech-LLM frameworks or interactive, synchronous communication capabilities. The abstract lacks mention of speech language modeling, full-duplex interaction, synchronization, or the evaluation of such models, focusing instead on domain-general versus domain-specific learning mechanisms for phonetic perception. Hence, it does not meet the inclusion criteria which require explicit focus on spoken language models or related modeling frameworks for speech as a primary modeling object rather than pre-processing for text.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:2)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "What Do North American Babies Hear? A large‐scale cross‐corpus analysis",
    "abstract": "Abstract A range of demographic variables influences how much speech young children hear. However, because studies have used vastly different sampling methods, quantitative comparison of interlocking demographic effects has been nearly impossible, across or within studies. We harnessed a unique collection of existing naturalistic, day‐long recordings from 61 homes across four North American cities to examine language input as a function of age, gender, and maternal education. We analyzed adult speech heard by 3‐ to 20‐month‐olds who wore audio recorders for an entire day. We annotated speaker gender and speech register (child‐directed or adult‐directed) for 10,861 utterances from female and male adults in these recordings. Examining age, gender, and maternal education collectively in this ecologically valid dataset, we find several key results. First, the speaker gender imbalance in the input is striking: children heard 2–3× more speech from females than males. Second, children in higher‐maternal education homes heard more child‐directed speech than those in lower‐maternal education homes. Finally, our analyses revealed a previously unreported effect: the proportion of child‐directed speech in the input increases with age, due to a decrease in adult‐directed speech with age. This large‐scale analysis is an important step forward in collectively examining demographic variables that influence early development, made possible by pooled, comparable, day‐long recordings of children's language environments. The audio recordings, annotations, and annotation software are readily available for reuse and reanalysis by other researchers.",
    "metadata": {
      "title": "What Do North American Babies Hear? A large‐scale cross‐corpus analysis",
      "summary": "Abstract A range of demographic variables influences how much speech young children hear. However, because studies have used vastly different sampling methods, quantitative comparison of interlocking demographic effects has been nearly impossible, across or within studies. We harnessed a unique collection of existing naturalistic, day‐long recordings from 61 homes across four North American cities to examine language input as a function of age, gender, and maternal education. We analyzed adult speech heard by 3‐ to 20‐month‐olds who wore audio recorders for an entire day. We annotated speaker gender and speech register (child‐directed or adult‐directed) for 10,861 utterances from female and male adults in these recordings. Examining age, gender, and maternal education collectively in this ecologically valid dataset, we find several key results. First, the speaker gender imbalance in the input is striking: children heard 2–3× more speech from females than males. Second, children in higher‐maternal education homes heard more child‐directed speech than those in lower‐maternal education homes. Finally, our analyses revealed a previously unreported effect: the proportion of child‐directed speech in the input increases with age, due to a decrease in adult‐directed speech with age. This large‐scale analysis is an important step forward in collectively examining demographic variables that influence early development, made possible by pooled, comparable, day‐long recordings of children's language environments. The audio recordings, annotations, and annotation software are readily available for reuse and reanalysis by other researchers.",
      "abstract": "Abstract A range of demographic variables influences how much speech young children hear. However, because studies have used vastly different sampling methods, quantitative comparison of interlocking demographic effects has been nearly impossible, across or within studies. We harnessed a unique collection of existing naturalistic, day‐long recordings from 61 homes across four North American cities to examine language input as a function of age, gender, and maternal education. We analyzed adult speech heard by 3‐ to 20‐month‐olds who wore audio recorders for an entire day. We annotated speaker gender and speech register (child‐directed or adult‐directed) for 10,861 utterances from female and male adults in these recordings. Examining age, gender, and maternal education collectively in this ecologically valid dataset, we find several key results. First, the speaker gender imbalance in the input is striking: children heard 2–3× more speech from females than males. Second, children in higher‐maternal education homes heard more child‐directed speech than those in lower‐maternal education homes. Finally, our analyses revealed a previously unreported effect: the proportion of child‐directed speech in the input increases with age, due to a decrease in adult‐directed speech with age. This large‐scale analysis is an important step forward in collectively examining demographic variables that influence early development, made possible by pooled, comparable, day‐long recordings of children's language environments. The audio recordings, annotations, and annotation software are readily available for reuse and reanalysis by other researchers.",
      "doi": "https://doi.org/10.1111/desc.12724",
      "openalex_id": "https://openalex.org/W2895356663",
      "arxiv_id": "",
      "publication_date": "2018-10-12",
      "published": "2018-10-12",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This is a developmental linguistics study of children's language input and not about Spoken Language Models or Full-Duplex SLMs, nor does it discuss SLM architectures/training/evaluation, so it fails the core inclusion criteria and does not meet the topic specification.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "This is a developmental linguistics study of children's language input and not about Spoken Language Models or Full-Duplex SLMs, nor does it discuss SLM architectures/training/evaluation, so it fails the core inclusion criteria and does not meet the topic specification.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study analyzes naturalistic audio recordings of child-directed speech and demographic influences on the speech input toddlers receive, but it does not discuss spoken language models (SLMs), speech language modeling architectures, or full-duplex spoken interaction frameworks; it focuses on linguistic input analysis rather than modeling or evaluating speech-based models or interaction systems, and no mention of modeling, training, or evaluation of speech-language models is present in the title or abstract.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study analyzes naturalistic audio recordings of child-directed speech and demographic influences on the speech input toddlers receive, but it does not discuss spoken language models (SLMs), speech language modeling architectures, or full-duplex spoken interaction frameworks; it focuses on linguistic input analysis rather than modeling or evaluating speech-based models or interaction systems, and no mention of modeling, training, or evaluation of speech-language models is present in the title or abstract.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Predicting Age of Acquisition in Early Word Learning Using Recurrent Neural Networks.",
    "abstract": "Vocabulary growth and syntactic development are known tobe highly correlated in early child language. What determineswhen words are acquired and how can this help us understandwhat drives early language development? We train an LSTMlanguage model, known to detect syntactic regularities that arerelevant for predicting the difficulty of words, on child-directedspeech. We use the average surprisal of words for the model,which encodes sequential predictability, as a predictor for theage of acquisition of words in early child language. We com-pare this predictor to word frequency and others and find thataverage surprisal is a good predictor for the age of acquisitionof function words and predicates beyond frequency, but notfor nouns. Our approach provides insight into what makes agood model of early word learning, especially for words whosemeanings rely heavily on linguistic context.",
    "metadata": {
      "title": "Predicting Age of Acquisition in Early Word Learning Using Recurrent Neural Networks.",
      "summary": "Vocabulary growth and syntactic development are known tobe highly correlated in early child language. What determineswhen words are acquired and how can this help us understandwhat drives early language development? We train an LSTMlanguage model, known to detect syntactic regularities that arerelevant for predicting the difficulty of words, on child-directedspeech. We use the average surprisal of words for the model,which encodes sequential predictability, as a predictor for theage of acquisition of words in early child language. We com-pare this predictor to word frequency and others and find thataverage surprisal is a good predictor for the age of acquisitionof function words and predicates beyond frequency, but notfor nouns. Our approach provides insight into what makes agood model of early word learning, especially for words whosemeanings rely heavily on linguistic context.",
      "abstract": "Vocabulary growth and syntactic development are known tobe highly correlated in early child language. What determineswhen words are acquired and how can this help us understandwhat drives early language development? We train an LSTMlanguage model, known to detect syntactic regularities that arerelevant for predicting the difficulty of words, on child-directedspeech. We use the average surprisal of words for the model,which encodes sequential predictability, as a predictor for theage of acquisition of words in early child language. We com-pare this predictor to word frequency and others and find thataverage surprisal is a good predictor for the age of acquisitionof function words and predicates beyond frequency, but notfor nouns. Our approach provides insight into what makes agood model of early word learning, especially for words whosemeanings rely heavily on linguistic context.",
      "doi": "",
      "openalex_id": "https://openalex.org/W3135131435",
      "arxiv_id": "",
      "publication_date": "2020-01-01",
      "published": "2020-01-01",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "Although the study uses an LSTM language model trained on child-directed speech, the abstract does not frame or evaluate a Spoken Language Model/FD-SLM core (no speech-level modeling, synchronization, or live interaction aspects), so it likely does not meet the explicit inclusion criteria despite the timely scope.",
      "evaluation": 2
    },
    "round-A_JuniorNano_reasoning": "Although the study uses an LSTM language model trained on child-directed speech, the abstract does not frame or evaluate a Spoken Language Model/FD-SLM core (no speech-level modeling, synchronization, or live interaction aspects), so it likely does not meet the explicit inclusion criteria despite the timely scope.",
    "round-A_JuniorNano_evaluation": 2,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on predicting age of acquisition of words using an LSTM language model trained on child-directed speech, emphasizing syntax and word predictability; however, it centers on text-based language modeling rather than spoken language models with voice/acoustic signal as the core, and does not address full-duplex interaction, speech synchronization, or related spoken language modeling components required by the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on predicting age of acquisition of words using an LSTM language model trained on child-directed speech, emphasizing syntax and word predictability; however, it centers on text-based language modeling rather than spoken language models with voice/acoustic signal as the core, and does not address full-duplex interaction, speech synchronization, or related spoken language modeling components required by the inclusion criteria.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:2)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "The child language data exchange system",
    "abstract": "ABSTRACT The study of language acquisition underwent a major revolution in the late 1950s as a result of the dissemination of technology permitting high-quality tape-recording of children in the family setting. This new technology led to major breakthroughs in the quality of both data and theory. The field is now at the threshold of a possible second major breakthrough stimulated by the dissemination of personal computing. Researchers are now able to transcribe tape-recorded data into computer files. With this new medium it is easy to conduct global searches for word combinations across collections of files. It is also possible to enter new codings of the basic text line. Because of the speed and accuracy with which computer files can be copied, it is now much easier to share data between researchers. To foster this sharing of computerized data, a group of child language researchers has established the Child Language Data Exchange System (CHILDES). This article details the formation of the CHILDES, the governance of the system, the nature of the database, the shape of the coding conventions, and the types of computer programs being developed.",
    "metadata": {
      "title": "The child language data exchange system",
      "summary": "ABSTRACT The study of language acquisition underwent a major revolution in the late 1950s as a result of the dissemination of technology permitting high-quality tape-recording of children in the family setting. This new technology led to major breakthroughs in the quality of both data and theory. The field is now at the threshold of a possible second major breakthrough stimulated by the dissemination of personal computing. Researchers are now able to transcribe tape-recorded data into computer files. With this new medium it is easy to conduct global searches for word combinations across collections of files. It is also possible to enter new codings of the basic text line. Because of the speed and accuracy with which computer files can be copied, it is now much easier to share data between researchers. To foster this sharing of computerized data, a group of child language researchers has established the Child Language Data Exchange System (CHILDES). This article details the formation of the CHILDES, the governance of the system, the nature of the database, the shape of the coding conventions, and the types of computer programs being developed.",
      "abstract": "ABSTRACT The study of language acquisition underwent a major revolution in the late 1950s as a result of the dissemination of technology permitting high-quality tape-recording of children in the family setting. This new technology led to major breakthroughs in the quality of both data and theory. The field is now at the threshold of a possible second major breakthrough stimulated by the dissemination of personal computing. Researchers are now able to transcribe tape-recorded data into computer files. With this new medium it is easy to conduct global searches for word combinations across collections of files. It is also possible to enter new codings of the basic text line. Because of the speed and accuracy with which computer files can be copied, it is now much easier to share data between researchers. To foster this sharing of computerized data, a group of child language researchers has established the Child Language Data Exchange System (CHILDES). This article details the formation of the CHILDES, the governance of the system, the nature of the database, the shape of the coding conventions, and the types of computer programs being developed.",
      "doi": "https://doi.org/10.1017/s0305000900006449",
      "openalex_id": "https://openalex.org/W1995991622",
      "arxiv_id": "",
      "publication_date": "1985-06-01",
      "published": "1985-06-01",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The article presents a data exchange system (CHILDES) and corpus governance rather than modeling, training, or evaluating Spoken Language Models or Full-Duplex SLMs, so it does not meet the inclusion criteria and is excluded by the stated exclusions.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The article presents a data exchange system (CHILDES) and corpus governance rather than modeling, training, or evaluating Spoken Language Models or Full-Duplex SLMs, so it does not meet the inclusion criteria and is excluded by the stated exclusions.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The article describes the Child Language Data Exchange System (CHILDES), focusing on data sharing for child language acquisition research rather than on spoken language models, speech language modeling architectures, training, synchronization, or evaluation of speech-based models as defined in the inclusion criteria. It does not mention spoken language models, full-duplex interaction, or any modeling of speech signals as a core focus, and it is not a recent (past 3 years) research article but rather a system description. Therefore, it does not meet the inclusion criteria and falls under exclusion criteria such as lacking explicit speech language model discussion and focus on text/audio data sharing instead of speech modeling.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The article describes the Child Language Data Exchange System (CHILDES), focusing on data sharing for child language acquisition research rather than on spoken language models, speech language modeling architectures, training, synchronization, or evaluation of speech-based models as defined in the inclusion criteria. It does not mention spoken language models, full-duplex interaction, or any modeling of speech signals as a core focus, and it is not a recent (past 3 years) research article but rather a system description. Therefore, it does not meet the inclusion criteria and falls under exclusion criteria such as lacking explicit speech language model discussion and focus on text/audio data sharing instead of speech modeling.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "BabyBERTa: Learning More Grammar With Small-Scale Child-Directed Language",
    "abstract": "Transformer-based language models have taken the NLP world by storm. However, their potential for addressing important questions in language acquisition research has been largely ignored. In this work, we examined the grammatical knowledge of RoBERTa (Liu et al., 2019) when trained on a 5M word corpus of language acquisition data to simulate the input available to children between the ages 1 and 6. Using the behavioral probing paradigm, we found that a smaller version of RoBERTa-base that never predicts unmasked tokens, which we term BabyBERTa, acquires grammatical knowledge comparable to that of pre-trained RoBERTa-base - and does so with approximately 15X fewer parameters and 6,000X fewer words. We discuss implications for building more efficient models and the learnability of grammar from input available to children. Lastly, to support research on this front, we release our novel grammar test suite that is compatible with the small vocabulary of child-directed input.",
    "metadata": {
      "title": "BabyBERTa: Learning More Grammar With Small-Scale Child-Directed Language",
      "summary": "Transformer-based language models have taken the NLP world by storm. However, their potential for addressing important questions in language acquisition research has been largely ignored. In this work, we examined the grammatical knowledge of RoBERTa (Liu et al., 2019) when trained on a 5M word corpus of language acquisition data to simulate the input available to children between the ages 1 and 6. Using the behavioral probing paradigm, we found that a smaller version of RoBERTa-base that never predicts unmasked tokens, which we term BabyBERTa, acquires grammatical knowledge comparable to that of pre-trained RoBERTa-base - and does so with approximately 15X fewer parameters and 6,000X fewer words. We discuss implications for building more efficient models and the learnability of grammar from input available to children. Lastly, to support research on this front, we release our novel grammar test suite that is compatible with the small vocabulary of child-directed input.",
      "abstract": "Transformer-based language models have taken the NLP world by storm. However, their potential for addressing important questions in language acquisition research has been largely ignored. In this work, we examined the grammatical knowledge of RoBERTa (Liu et al., 2019) when trained on a 5M word corpus of language acquisition data to simulate the input available to children between the ages 1 and 6. Using the behavioral probing paradigm, we found that a smaller version of RoBERTa-base that never predicts unmasked tokens, which we term BabyBERTa, acquires grammatical knowledge comparable to that of pre-trained RoBERTa-base - and does so with approximately 15X fewer parameters and 6,000X fewer words. We discuss implications for building more efficient models and the learnability of grammar from input available to children. Lastly, to support research on this front, we release our novel grammar test suite that is compatible with the small vocabulary of child-directed input.",
      "doi": "https://doi.org/10.18653/v1/2021.conll-1.49",
      "openalex_id": "https://openalex.org/W3213014097",
      "arxiv_id": "",
      "publication_date": "2021-01-01",
      "published": "2021-01-01",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The study focuses on text-based grammatical knowledge from a RoBERTa variant using child-directed text and does not treat speech or spoken-language modeling as the core object, nor discuss interaction/real-time aspects required by the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The study focuses on text-based grammatical knowledge from a RoBERTa variant using child-directed text and does not treat speech or spoken-language modeling as the core object, nor discuss interaction/real-time aspects required by the inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on training a small-scale transformer language model (BabyBERTa) on child-directed text data to investigate grammatical knowledge, with no mention of speech or spoken language modeling, full-duplex interaction, synchronization, or evaluation metrics related to real-time spoken interaction; it mainly deals with text-based language acquisition simulation and grammar testing, thus failing to meet the inclusion criteria that require models centered on spoken language modeling or speech signals and related interactive/concurrent aspects.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on training a small-scale transformer language model (BabyBERTa) on child-directed text data to investigate grammatical knowledge, with no mention of speech or spoken language modeling, full-duplex interaction, synchronization, or evaluation metrics related to real-time spoken interaction; it mainly deals with text-based language acquisition simulation and grammar testing, thus failing to meet the inclusion criteria that require models centered on spoken language modeling or speech signals and related interactive/concurrent aspects.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Child‐Directed Speech Is Infrequent in a Forager‐Farmer Population: A Time Allocation Study",
    "abstract": "This article provides an estimation of how frequently, and from whom, children aged 0–11 years ( N s between 9 and 24) receive one‐on‐one verbal input among Tsimane forager‐horticulturalists of lowland Bolivia. Analyses of systematic daytime behavioral observations reveal &lt; 1 min per daylight hour is spent talking to children younger than 4 years of age, which is 4 times less than estimates for others present at the same time and place. Adults provide a majority of the input at 0–3 years of age but not afterward. When integrated with previous work, these results reveal large cross‐cultural variation in the linguistic experiences provided to young children. Consideration of more diverse human populations is necessary to build generalizable theories of language acquisition.",
    "metadata": {
      "title": "Child‐Directed Speech Is Infrequent in a Forager‐Farmer Population: A Time Allocation Study",
      "summary": "This article provides an estimation of how frequently, and from whom, children aged 0–11 years ( N s between 9 and 24) receive one‐on‐one verbal input among Tsimane forager‐horticulturalists of lowland Bolivia. Analyses of systematic daytime behavioral observations reveal &lt; 1 min per daylight hour is spent talking to children younger than 4 years of age, which is 4 times less than estimates for others present at the same time and place. Adults provide a majority of the input at 0–3 years of age but not afterward. When integrated with previous work, these results reveal large cross‐cultural variation in the linguistic experiences provided to young children. Consideration of more diverse human populations is necessary to build generalizable theories of language acquisition.",
      "abstract": "This article provides an estimation of how frequently, and from whom, children aged 0–11 years ( N s between 9 and 24) receive one‐on‐one verbal input among Tsimane forager‐horticulturalists of lowland Bolivia. Analyses of systematic daytime behavioral observations reveal &lt; 1 min per daylight hour is spent talking to children younger than 4 years of age, which is 4 times less than estimates for others present at the same time and place. Adults provide a majority of the input at 0–3 years of age but not afterward. When integrated with previous work, these results reveal large cross‐cultural variation in the linguistic experiences provided to young children. Consideration of more diverse human populations is necessary to build generalizable theories of language acquisition.",
      "doi": "https://doi.org/10.1111/cdev.12974",
      "openalex_id": "https://openalex.org/W2765364385",
      "arxiv_id": "",
      "publication_date": "2017-11-02",
      "published": "2017-11-02",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This item investigates child-directed speech frequency in a cross-cultural population, not about Spoken Language Models or speech-language modeling; it lacks model/architecture/training/evaluation of SLMs or dialogue interaction, thus fails inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "This item investigates child-directed speech frequency in a cross-cultural population, not about Spoken Language Models or speech-language modeling; it lacks model/architecture/training/evaluation of SLMs or dialogue interaction, thus fails inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The article focuses on behavioral observations of child-directed speech frequency in a cultural context and does not discuss Spoken Language Models, speech language modeling architectures, their training, evaluation, or synchronization mechanisms as required by the inclusion criteria. It also does not address any engineered or learned synchronization or full-duplex spoken interaction aspects and lacks any mention of modeling or evaluation metrics related to speech language models.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The article focuses on behavioral observations of child-directed speech frequency in a cultural context and does not discuss Spoken Language Models, speech language modeling architectures, their training, evaluation, or synchronization mechanisms as required by the inclusion criteria. It also does not address any engineered or learned synchronization or full-duplex spoken interaction aspects and lacks any mention of modeling or evaluation metrics related to speech language models.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Evaluating the Effectiveness of Large Language Models in Representing Textual Descriptions of Geometry and Spatial Relations (Short Paper)",
    "abstract": "This research focuses on assessing the ability of large language models (LLMs) in representing geometries and their spatial relations. We utilize LLMs including GPT-2 and BERT to encode the well-known text (WKT) format of geometries and then feed their embeddings into classifiers and regressors to evaluate the effectiveness of the LLMs-generated embeddings for geometric attributes. The experiments demonstrate that while the LLMs-generated embeddings can preserve geometry types and capture some spatial relations (up to 73% accuracy), challenges remain in estimating numeric values and retrieving spatially related objects. This research highlights the need for improvement in terms of capturing the nuances and complexities of the underlying geospatial data and integrating domain knowledge to support various GeoAI applications using foundation models.",
    "metadata": {
      "title": "Evaluating the Effectiveness of Large Language Models in Representing Textual Descriptions of Geometry and Spatial Relations (Short Paper)",
      "summary": "This research focuses on assessing the ability of large language models (LLMs) in representing geometries and their spatial relations. We utilize LLMs including GPT-2 and BERT to encode the well-known text (WKT) format of geometries and then feed their embeddings into classifiers and regressors to evaluate the effectiveness of the LLMs-generated embeddings for geometric attributes. The experiments demonstrate that while the LLMs-generated embeddings can preserve geometry types and capture some spatial relations (up to 73% accuracy), challenges remain in estimating numeric values and retrieving spatially related objects. This research highlights the need for improvement in terms of capturing the nuances and complexities of the underlying geospatial data and integrating domain knowledge to support various GeoAI applications using foundation models.",
      "abstract": "This research focuses on assessing the ability of large language models (LLMs) in representing geometries and their spatial relations. We utilize LLMs including GPT-2 and BERT to encode the well-known text (WKT) format of geometries and then feed their embeddings into classifiers and regressors to evaluate the effectiveness of the LLMs-generated embeddings for geometric attributes. The experiments demonstrate that while the LLMs-generated embeddings can preserve geometry types and capture some spatial relations (up to 73% accuracy), challenges remain in estimating numeric values and retrieving spatially related objects. This research highlights the need for improvement in terms of capturing the nuances and complexities of the underlying geospatial data and integrating domain knowledge to support various GeoAI applications using foundation models.",
      "doi": "https://doi.org/10.4230/lipics.giscience.2023.43",
      "openalex_id": "https://openalex.org/W4292779060",
      "arxiv_id": "",
      "publication_date": "2023-01-01",
      "published": "2023-01-01",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item focuses on using large language models to encode geometries (WKT) and evaluate geometric embeddings, which is not about Spoken Language Models or speech-language modeling, so it does not meet the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item focuses on using large language models to encode geometries (WKT) and evaluate geometric embeddings, which is not about Spoken Language Models or speech-language modeling, so it does not meet the inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on large language models processing textual descriptions of geometry and spatial relations, without any explicit mention of spoken language models, speech as a primary modeling object, or related speech interaction concepts specified in the inclusion criteria. It does not discuss architectures, training, or evaluation aspects of spoken language models or full-duplex spoken interaction. Therefore, it does not meet the inclusion criteria focused on spoken language models and speech-based modeling and interaction.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on large language models processing textual descriptions of geometry and spatial relations, without any explicit mention of spoken language models, speech as a primary modeling object, or related speech interaction concepts specified in the inclusion criteria. It does not discuss architectures, training, or evaluation aspects of spoken language models or full-duplex spoken interaction. Therefore, it does not meet the inclusion criteria focused on spoken language models and speech-based modeling and interaction.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Reverse engineering language acquisition with child-centered long-form recordings",
    "abstract": "Language use in everyday life can be studied using lightweight, wearable recorders that collect long-form recordings - that is, audio (including speech) over whole days. The hardware and software underlying this technique is increasingly accessible and inexpensive, and these data are revolutionizing the language acquisition field. We first place this technique into the broader context of the current ways of studying both the input being received by children and children’s own language production, laying out the main advantages and drawbacks of long-form recordings. We then go on to argue that a unique advantage of long-form recordings is that they can fuel realistic models of early language acquisition that use speech to represent children's input and/or to establish production benchmarks. To enable the field to make the most of this unique empirical and conceptual contribution, we outline what this reverse engineering approach from long-form recordings entails, why it is useful, and how to evaluate success.",
    "metadata": {
      "title": "Reverse engineering language acquisition with child-centered long-form recordings",
      "summary": "Language use in everyday life can be studied using lightweight, wearable recorders that collect long-form recordings - that is, audio (including speech) over whole days. The hardware and software underlying this technique is increasingly accessible and inexpensive, and these data are revolutionizing the language acquisition field. We first place this technique into the broader context of the current ways of studying both the input being received by children and children’s own language production, laying out the main advantages and drawbacks of long-form recordings. We then go on to argue that a unique advantage of long-form recordings is that they can fuel realistic models of early language acquisition that use speech to represent children's input and/or to establish production benchmarks. To enable the field to make the most of this unique empirical and conceptual contribution, we outline what this reverse engineering approach from long-form recordings entails, why it is useful, and how to evaluate success.",
      "abstract": "Language use in everyday life can be studied using lightweight, wearable recorders that collect long-form recordings - that is, audio (including speech) over whole days. The hardware and software underlying this technique is increasingly accessible and inexpensive, and these data are revolutionizing the language acquisition field. We first place this technique into the broader context of the current ways of studying both the input being received by children and children’s own language production, laying out the main advantages and drawbacks of long-form recordings. We then go on to argue that a unique advantage of long-form recordings is that they can fuel realistic models of early language acquisition that use speech to represent children's input and/or to establish production benchmarks. To enable the field to make the most of this unique empirical and conceptual contribution, we outline what this reverse engineering approach from long-form recordings entails, why it is useful, and how to evaluate success.",
      "doi": "https://doi.org/10.31234/osf.io/pt9xq",
      "openalex_id": "https://openalex.org/W4247178956",
      "arxiv_id": "",
      "publication_date": "2021-03-31",
      "published": "2021-03-31",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item does not explicitly center on Spoken Language Models or a speech-language modeling framework (architecture, training, or evaluation) and instead discusses data collection with long-form recordings for language acquisition, so it fails the core inclusion criterion requiring SLM/FD-SLM focus and modeling emphasis.",
      "evaluation": 2
    },
    "round-A_JuniorNano_reasoning": "The item does not explicitly center on Spoken Language Models or a speech-language modeling framework (architecture, training, or evaluation) and instead discusses data collection with long-form recordings for language acquisition, so it fails the core inclusion criterion requiring SLM/FD-SLM focus and modeling emphasis.",
    "round-A_JuniorNano_evaluation": 2,
    "round-A_JuniorMini_output": {
      "reasoning": "The article focuses on using long-form audio recordings to study language acquisition in children, emphasizing empirical data collection and analysis, but it does not mention Spoken Language Models, Full-Duplex interaction capabilities, or related modeling architectures and evaluation metrics as required. The study appears more observational and theoretical in developmental linguistics rather than addressing speech language modeling, synchronous interaction, or behavior arbitration in engineering or machine learning contexts.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The article focuses on using long-form audio recordings to study language acquisition in children, emphasizing empirical data collection and analysis, but it does not mention Spoken Language Models, Full-Duplex interaction capabilities, or related modeling architectures and evaluation metrics as required. The study appears more observational and theoretical in developmental linguistics rather than addressing speech language modeling, synchronous interaction, or behavior arbitration in engineering or machine learning contexts.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:2)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Phonemizer: Text to Phones Transcription for Multiple Languages in Python",
    "abstract": "Phones are elementary sounds the speech is made of, on which syllables and words are built.The transcription of texts from their orthographic form into a phonetic alphabet is an important requirement in various applications related to speech and language processing, for instance for text to speech systems.Phonemizer is a Python package addressing precisely this issue: it transcribes a text from its orthographic representation into a phonetic one.The package is user-friendly and exposes a single high-level phonemize function, a lower lovel API, and is also available as a command-line interface.It supports about a hundred different languages and provides end-user functionalities such as punctuation preservation, phones accentuation, tokenization at phone/syllable/word levels, as well as parallel processing of large input texts.",
    "metadata": {
      "title": "Phonemizer: Text to Phones Transcription for Multiple Languages in Python",
      "summary": "Phones are elementary sounds the speech is made of, on which syllables and words are built.The transcription of texts from their orthographic form into a phonetic alphabet is an important requirement in various applications related to speech and language processing, for instance for text to speech systems.Phonemizer is a Python package addressing precisely this issue: it transcribes a text from its orthographic representation into a phonetic one.The package is user-friendly and exposes a single high-level phonemize function, a lower lovel API, and is also available as a command-line interface.It supports about a hundred different languages and provides end-user functionalities such as punctuation preservation, phones accentuation, tokenization at phone/syllable/word levels, as well as parallel processing of large input texts.",
      "abstract": "Phones are elementary sounds the speech is made of, on which syllables and words are built.The transcription of texts from their orthographic form into a phonetic alphabet is an important requirement in various applications related to speech and language processing, for instance for text to speech systems.Phonemizer is a Python package addressing precisely this issue: it transcribes a text from its orthographic representation into a phonetic one.The package is user-friendly and exposes a single high-level phonemize function, a lower lovel API, and is also available as a command-line interface.It supports about a hundred different languages and provides end-user functionalities such as punctuation preservation, phones accentuation, tokenization at phone/syllable/word levels, as well as parallel processing of large input texts.",
      "doi": "https://doi.org/10.21105/joss.03958",
      "openalex_id": "https://openalex.org/W4200300291",
      "arxiv_id": "",
      "publication_date": "2021-12-18",
      "published": "2021-12-18",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This item focuses on phonemization (text-to-phones) and a Python package for transcription, not on Spoken Language Models or speech-language modeling with interactive, end-to-end architectures; it lacks the required scope and evaluation of SLMs, so it should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "This item focuses on phonemization (text-to-phones) and a Python package for transcription, not on Spoken Language Models or speech-language modeling with interactive, end-to-end architectures; it lacks the required scope and evaluation of SLMs, so it should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper discusses a Python package for phonetic transcription of text into phones across multiple languages, focusing primarily on the text-to-phoneme conversion process. It does not discuss spoken language models, speech language modeling architecture, training, inference, or evaluation related to spoken language models or full-duplex interaction as defined in the inclusion criteria. It lacks emphasis on speech-level language modeling or real-time interaction capabilities, and mainly addresses a text-to-speech frontend tool rather than a spoken language modeling framework. Therefore, it does not meet the inclusion criteria and falls under exclusion for focusing on preprocessing rather than speech modeling.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper discusses a Python package for phonetic transcription of text into phones across multiple languages, focusing primarily on the text-to-phoneme conversion process. It does not discuss spoken language models, speech language modeling architecture, training, inference, or evaluation related to spoken language models or full-duplex interaction as defined in the inclusion criteria. It lacks emphasis on speech-level language modeling or real-time interaction capabilities, and mainly addresses a text-to-speech frontend tool rather than a spoken language modeling framework. Therefore, it does not meet the inclusion criteria and falls under exclusion for focusing on preprocessing rather than speech modeling.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Language input to infants of different socioeconomic statuses: A quantitative meta-analysis",
    "abstract": "This Registered Report has been published at Developmental Science.",
    "metadata": {
      "title": "Language input to infants of different socioeconomic statuses: A quantitative meta-analysis",
      "summary": "This Registered Report has been published at Developmental Science.",
      "abstract": "This Registered Report has been published at Developmental Science.",
      "doi": "https://doi.org/10.31219/osf.io/jvdme",
      "openalex_id": "https://openalex.org/W4240474221",
      "arxiv_id": "",
      "publication_date": "2021-02-11",
      "published": "2021-02-11",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item focuses on infant language input and socioeconomic status, with no discussion of Spoken Language Models, FD-SLMs, or speech-language modeling concepts, thus failing all inclusion criteria and aligning with exclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item focuses on infant language input and socioeconomic status, with no discussion of Spoken Language Models, FD-SLMs, or speech-language modeling concepts, thus failing all inclusion criteria and aligning with exclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The title \"Language input to infants of different socioeconomic statuses: A quantitative meta-analysis\" does not mention spoken language models, speech language models, or any equivalent concepts related to speech-based modeling and interaction architectures. The abstract merely indicates this is a Registered Report published at Developmental Science, with no details on model architectures, training, evaluation, or focus on full-duplex interactions, synchronization, or speech-level modeling. There is no indication that the study discusses speech as a core modeling object or meets the specialized inclusion criteria focused on Spoken Language Models or related architectures. Therefore, it does not meet the inclusion criteria and likely falls under exclusion due to not addressing spoken language modeling or full-duplex speech interaction systems.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The title \"Language input to infants of different socioeconomic statuses: A quantitative meta-analysis\" does not mention spoken language models, speech language models, or any equivalent concepts related to speech-based modeling and interaction architectures. The abstract merely indicates this is a Registered Report published at Developmental Science, with no details on model architectures, training, evaluation, or focus on full-duplex interactions, synchronization, or speech-level modeling. There is no indication that the study discusses speech as a core modeling object or meets the specialized inclusion criteria focused on Spoken Language Models or related architectures. Therefore, it does not meet the inclusion criteria and likely falls under exclusion due to not addressing spoken language modeling or full-duplex speech interaction systems.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Call for Papers -- The BabyLM Challenge: Sample-efficient pretraining on a developmentally plausible corpus",
    "abstract": "We present the call for papers for the BabyLM Challenge: Sample-efficient pretraining on a developmentally plausible corpus. This shared task is intended for participants with an interest in small scale language modeling, human language acquisition, low-resource NLP, and cognitive modeling. In partnership with CoNLL and CMCL, we provide a platform for approaches to pretraining with a limited-size corpus sourced from data inspired by the input to children. The task has three tracks, two of which restrict the training data to pre-released datasets of 10M and 100M words and are dedicated to explorations of approaches such as architectural variations, self-supervised objectives, or curriculum learning. The final track only restricts the amount of text used, allowing innovation in the choice of the data, its domain, and even its modality (i.e., data from sources other than text is welcome). We will release a shared evaluation pipeline which scores models on a variety of benchmarks and tasks, including targeted syntactic evaluations and natural language understanding.",
    "metadata": {
      "title": "Call for Papers -- The BabyLM Challenge: Sample-efficient pretraining on a developmentally plausible corpus",
      "summary": "We present the call for papers for the BabyLM Challenge: Sample-efficient pretraining on a developmentally plausible corpus. This shared task is intended for participants with an interest in small scale language modeling, human language acquisition, low-resource NLP, and cognitive modeling. In partnership with CoNLL and CMCL, we provide a platform for approaches to pretraining with a limited-size corpus sourced from data inspired by the input to children. The task has three tracks, two of which restrict the training data to pre-released datasets of 10M and 100M words and are dedicated to explorations of approaches such as architectural variations, self-supervised objectives, or curriculum learning. The final track only restricts the amount of text used, allowing innovation in the choice of the data, its domain, and even its modality (i.e., data from sources other than text is welcome). We will release a shared evaluation pipeline which scores models on a variety of benchmarks and tasks, including targeted syntactic evaluations and natural language understanding.",
      "abstract": "We present the call for papers for the BabyLM Challenge: Sample-efficient pretraining on a developmentally plausible corpus. This shared task is intended for participants with an interest in small scale language modeling, human language acquisition, low-resource NLP, and cognitive modeling. In partnership with CoNLL and CMCL, we provide a platform for approaches to pretraining with a limited-size corpus sourced from data inspired by the input to children. The task has three tracks, two of which restrict the training data to pre-released datasets of 10M and 100M words and are dedicated to explorations of approaches such as architectural variations, self-supervised objectives, or curriculum learning. The final track only restricts the amount of text used, allowing innovation in the choice of the data, its domain, and even its modality (i.e., data from sources other than text is welcome). We will release a shared evaluation pipeline which scores models on a variety of benchmarks and tasks, including targeted syntactic evaluations and natural language understanding.",
      "doi": "https://doi.org/10.48550/arxiv.2301.11796",
      "openalex_id": "https://openalex.org/W4318621130",
      "arxiv_id": "",
      "publication_date": "2023-01-27",
      "published": "2023-01-27",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This item is a call-for-papers for a BabyLM challenge focused on small-scale text language modeling, with no emphasis on Spoken Language Models or speech-based modeling, thus it does not meet the inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "This item is a call-for-papers for a BabyLM challenge focused on small-scale text language modeling, with no emphasis on Spoken Language Models or speech-based modeling, thus it does not meet the inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The title and abstract describe a challenge focused on language modeling using text corpora inspired by child language input, emphasizing low-resource NLP and cognitive modeling, but they do not mention spoken language models, speech-level modeling, or full-duplex interactive spoken dialogue modeling, nor do they discuss speech as the core modeling object beyond text representation, which fails to meet the inclusion criteria centered on spoken language models and speech-based language modeling.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The title and abstract describe a challenge focused on language modeling using text corpora inspired by child language input, emphasizing low-resource NLP and cognitive modeling, but they do not mention spoken language models, speech-level modeling, or full-duplex interactive spoken dialogue modeling, nor do they discuss speech as the core modeling object beyond text representation, which fails to meet the inclusion criteria centered on spoken language models and speech-based language modeling.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "A joint model of word segmentation and phonological variation for English word-final /t/-deletion",
    "abstract": "Word-final /t/-deletion refers to a common phenomenon in spoken English where words such as /wEst / “west ” are pronounced as [wEs] “wes ” in certain contexts. Phonological variation like this is common in naturally occurring speech. Current computational models of unsupervised word segmentation usually assume idealized input that is devoid of these kinds of variation. We extend a non-parametric model of word segmentation by adding phonological rules that map from underlying forms to surface forms to produce a mathematically well-defined joint model as a first step towards handling variation and segmentation in a single model. We analyse how our model handles /t/-deletion on a large corpus of transcribed speech, and show that the joint model can perform word segmentation and recover underlying /t/s. We find that Bigram dependencies are important for performing well on real data and for learning appropriate deletion probabilities for different contexts. 1 1",
    "metadata": {
      "title": "A joint model of word segmentation and phonological variation for English word-final /t/-deletion",
      "summary": "Word-final /t/-deletion refers to a common phenomenon in spoken English where words such as /wEst / “west ” are pronounced as [wEs] “wes ” in certain contexts. Phonological variation like this is common in naturally occurring speech. Current computational models of unsupervised word segmentation usually assume idealized input that is devoid of these kinds of variation. We extend a non-parametric model of word segmentation by adding phonological rules that map from underlying forms to surface forms to produce a mathematically well-defined joint model as a first step towards handling variation and segmentation in a single model. We analyse how our model handles /t/-deletion on a large corpus of transcribed speech, and show that the joint model can perform word segmentation and recover underlying /t/s. We find that Bigram dependencies are important for performing well on real data and for learning appropriate deletion probabilities for different contexts. 1 1",
      "abstract": "Word-final /t/-deletion refers to a common phenomenon in spoken English where words such as /wEst / “west ” are pronounced as [wEs] “wes ” in certain contexts. Phonological variation like this is common in naturally occurring speech. Current computational models of unsupervised word segmentation usually assume idealized input that is devoid of these kinds of variation. We extend a non-parametric model of word segmentation by adding phonological rules that map from underlying forms to surface forms to produce a mathematically well-defined joint model as a first step towards handling variation and segmentation in a single model. We analyse how our model handles /t/-deletion on a large corpus of transcribed speech, and show that the joint model can perform word segmentation and recover underlying /t/s. We find that Bigram dependencies are important for performing well on real data and for learning appropriate deletion probabilities for different contexts. 1 1",
      "doi": "",
      "openalex_id": "https://openalex.org/W2155042697",
      "arxiv_id": "",
      "publication_date": "2013-08-01",
      "published": "2013-08-01",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "It centers on a speech-language modeling problem (joint word segmentation with phonological variation) using speech as the modeling target, discusses model architecture and evaluation on spoken data, and is within the past 3 years, aligning with spoken language modeling rather than purely ASR/TTS.",
      "evaluation": 5
    },
    "round-A_JuniorNano_reasoning": "It centers on a speech-language modeling problem (joint word segmentation with phonological variation) using speech as the modeling target, discusses model architecture and evaluation on spoken data, and is within the past 3 years, aligning with spoken language modeling rather than purely ASR/TTS.",
    "round-A_JuniorNano_evaluation": 5,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on a computational model addressing phonological variation and word segmentation in spoken English, which is related to spoken language; however, it does not explicitly discuss spoken language models (SLMs), full-duplex interaction, speech-language model architectures, training, or real-time interactive evaluation metrics as required by the inclusion criteria. Hence, it lacks direct relevance to the core topics of full-duplex spoken language models or models emphasizing speech as the primary object of modeling and inference, and it does not meet the explicit terminology or scope requirements.",
      "evaluation": 2
    },
    "round-A_JuniorMini_reasoning": "The study focuses on a computational model addressing phonological variation and word segmentation in spoken English, which is related to spoken language; however, it does not explicitly discuss spoken language models (SLMs), full-duplex interaction, speech-language model architectures, training, or real-time interactive evaluation metrics as required by the inclusion criteria. Hence, it lacks direct relevance to the core topics of full-duplex spoken language models or models emphasizing speech as the primary object of modeling and inference, and it does not meet the explicit terminology or scope requirements.",
    "round-A_JuniorMini_evaluation": 2,
    "round-B_SeniorLead_output": {
      "reasoning": "Junior reviewers disagreed: JuniorNano thought it aligned with spoken language modeling, while JuniorMini correctly noted the paper lacks explicit SLM/FD‑SLM framing. The paper proposes a joint model for word segmentation and phonological variation on transcribed speech (recovering underlying /t/), but it does not treat raw acoustic signals or speech as the primary modeling object, does not mention spoken language models or full‑duplex interaction, and contains no discussion of SLM/FD‑SLM architectures, real‑time interaction, overlap/turn‑taking, or related evaluations required by the inclusion criteria—so it fails the topic scope and explicit-terminology requirements and must be excluded.",
      "evaluation": 1
    },
    "round-B_SeniorLead_reasoning": "Junior reviewers disagreed: JuniorNano thought it aligned with spoken language modeling, while JuniorMini correctly noted the paper lacks explicit SLM/FD‑SLM framing. The paper proposes a joint model for word segmentation and phonological variation on transcribed speech (recovering underlying /t/), but it does not treat raw acoustic signals or speech as the primary modeling object, does not mention spoken language models or full‑duplex interaction, and contains no discussion of SLM/FD‑SLM architectures, real‑time interaction, overlap/turn‑taking, or related evaluations required by the inclusion criteria—so it fails the topic scope and explicit-terminology requirements and must be excluded.",
    "round-B_SeniorLead_evaluation": 1,
    "final_verdict": "exclude (senior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Self-Supervised Speech Representation Learning: A Review",
    "abstract": "Although supervised deep learning has revolutionized speech and audio\\nprocessing, it has necessitated the building of specialist models for\\nindividual tasks and application scenarios. It is likewise difficult to apply\\nthis to dialects and languages for which only limited labeled data is\\navailable. Self-supervised representation learning methods promise a single\\nuniversal model that would benefit a wide variety of tasks and domains. Such\\nmethods have shown success in natural language processing and computer vision\\ndomains, achieving new levels of performance while reducing the number of\\nlabels required for many downstream scenarios. Speech representation learning\\nis experiencing similar progress in three main categories: generative,\\ncontrastive, and predictive methods. Other approaches rely on multi-modal data\\nfor pre-training, mixing text or visual data streams with speech. Although\\nself-supervised speech representation is still a nascent research area, it is\\nclosely related to acoustic word embedding and learning with zero lexical\\nresources, both of which have seen active research for many years. This review\\npresents approaches for self-supervised speech representation learning and\\ntheir connection to other research areas. Since many current methods focus\\nsolely on automatic speech recognition as a downstream task, we review recent\\nefforts on benchmarking learned representations to extend the application\\nbeyond speech recognition.\\n",
    "metadata": {
      "title": "Self-Supervised Speech Representation Learning: A Review",
      "summary": "Although supervised deep learning has revolutionized speech and audio\\nprocessing, it has necessitated the building of specialist models for\\nindividual tasks and application scenarios. It is likewise difficult to apply\\nthis to dialects and languages for which only limited labeled data is\\navailable. Self-supervised representation learning methods promise a single\\nuniversal model that would benefit a wide variety of tasks and domains. Such\\nmethods have shown success in natural language processing and computer vision\\ndomains, achieving new levels of performance while reducing the number of\\nlabels required for many downstream scenarios. Speech representation learning\\nis experiencing similar progress in three main categories: generative,\\ncontrastive, and predictive methods. Other approaches rely on multi-modal data\\nfor pre-training, mixing text or visual data streams with speech. Although\\nself-supervised speech representation is still a nascent research area, it is\\nclosely related to acoustic word embedding and learning with zero lexical\\nresources, both of which have seen active research for many years. This review\\npresents approaches for self-supervised speech representation learning and\\ntheir connection to other research areas. Since many current methods focus\\nsolely on automatic speech recognition as a downstream task, we review recent\\nefforts on benchmarking learned representations to extend the application\\nbeyond speech recognition.\\n",
      "abstract": "Although supervised deep learning has revolutionized speech and audio\\nprocessing, it has necessitated the building of specialist models for\\nindividual tasks and application scenarios. It is likewise difficult to apply\\nthis to dialects and languages for which only limited labeled data is\\navailable. Self-supervised representation learning methods promise a single\\nuniversal model that would benefit a wide variety of tasks and domains. Such\\nmethods have shown success in natural language processing and computer vision\\ndomains, achieving new levels of performance while reducing the number of\\nlabels required for many downstream scenarios. Speech representation learning\\nis experiencing similar progress in three main categories: generative,\\ncontrastive, and predictive methods. Other approaches rely on multi-modal data\\nfor pre-training, mixing text or visual data streams with speech. Although\\nself-supervised speech representation is still a nascent research area, it is\\nclosely related to acoustic word embedding and learning with zero lexical\\nresources, both of which have seen active research for many years. This review\\npresents approaches for self-supervised speech representation learning and\\ntheir connection to other research areas. Since many current methods focus\\nsolely on automatic speech recognition as a downstream task, we review recent\\nefforts on benchmarking learned representations to extend the application\\nbeyond speech recognition.\\n",
      "doi": "https://doi.org/10.1109/jstsp.2022.3207050",
      "openalex_id": "https://openalex.org/W4281492411",
      "arxiv_id": "",
      "publication_date": "2022-09-15",
      "published": "2022-09-15",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This item focuses on self-supervised speech representations rather than Spoken Language Models (SLMs) or full-duplex speech-language modeling as its core topic, and the abstract does not explicitly mention SLM/SLM-equivalents or interactive speech-language modeling aspects, thus it does not meet the core inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "This item focuses on self-supervised speech representations rather than Spoken Language Models (SLMs) or full-duplex speech-language modeling as its core topic, and the abstract does not explicitly mention SLM/SLM-equivalents or interactive speech-language modeling aspects, thus it does not meet the core inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The article reviews self-supervised speech representation learning with a focus on modeling speech signals for various tasks including automatic speech recognition, but it does not explicitly focus on Spoken Language Models (SLMs), full-duplex interaction, or aspects such as cognitive parallelism, low-latency real-time interaction, or synchronization mechanisms. The abstract centers on representation learning methods rather than architectural or behavioral frameworks for spoken language models or full-duplex interaction, and it lacks mention of multi-dimensional evaluations related to dialogue or interaction. Therefore, it does not meet the specific inclusion criteria related to SLMs or FD-SLMs, nor does it meet the exclusion criteria directly but mostly falls short of the nuanced modeling and evaluation focus required.",
      "evaluation": 2
    },
    "round-A_JuniorMini_reasoning": "The article reviews self-supervised speech representation learning with a focus on modeling speech signals for various tasks including automatic speech recognition, but it does not explicitly focus on Spoken Language Models (SLMs), full-duplex interaction, or aspects such as cognitive parallelism, low-latency real-time interaction, or synchronization mechanisms. The abstract centers on representation learning methods rather than architectural or behavioral frameworks for spoken language models or full-duplex interaction, and it lacks mention of multi-dimensional evaluations related to dialogue or interaction. Therefore, it does not meet the specific inclusion criteria related to SLMs or FD-SLMs, nor does it meet the exclusion criteria directly but mostly falls short of the nuanced modeling and evaluation focus required.",
    "round-A_JuniorMini_evaluation": 2,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:2)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "What you can cram into a single vector: Probing sentence embeddings for\\n linguistic properties",
    "abstract": "Although much effort has recently been devoted to training high-quality\\nsentence embeddings, we still have a poor understanding of what they are\\ncapturing. \"Downstream\" tasks, often based on sentence classification, are\\ncommonly used to evaluate the quality of sentence representations. The\\ncomplexity of the tasks makes it however difficult to infer what kind of\\ninformation is present in the representations. We introduce here 10 probing\\ntasks designed to capture simple linguistic features of sentences, and we use\\nthem to study embeddings generated by three different encoders trained in eight\\ndistinct ways, uncovering intriguing properties of both encoders and training\\nmethods.\\n",
    "metadata": {
      "title": "What you can cram into a single vector: Probing sentence embeddings for\\n linguistic properties",
      "summary": "Although much effort has recently been devoted to training high-quality\\nsentence embeddings, we still have a poor understanding of what they are\\ncapturing. \"Downstream\" tasks, often based on sentence classification, are\\ncommonly used to evaluate the quality of sentence representations. The\\ncomplexity of the tasks makes it however difficult to infer what kind of\\ninformation is present in the representations. We introduce here 10 probing\\ntasks designed to capture simple linguistic features of sentences, and we use\\nthem to study embeddings generated by three different encoders trained in eight\\ndistinct ways, uncovering intriguing properties of both encoders and training\\nmethods.\\n",
      "abstract": "Although much effort has recently been devoted to training high-quality\\nsentence embeddings, we still have a poor understanding of what they are\\ncapturing. \"Downstream\" tasks, often based on sentence classification, are\\ncommonly used to evaluate the quality of sentence representations. The\\ncomplexity of the tasks makes it however difficult to infer what kind of\\ninformation is present in the representations. We introduce here 10 probing\\ntasks designed to capture simple linguistic features of sentences, and we use\\nthem to study embeddings generated by three different encoders trained in eight\\ndistinct ways, uncovering intriguing properties of both encoders and training\\nmethods.\\n",
      "doi": "https://doi.org/10.48550/arxiv.1805.01070",
      "openalex_id": "https://openalex.org/W2799124508",
      "arxiv_id": "",
      "publication_date": "2018-05-02",
      "published": "2018-05-02",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The title/abstract focus on text-based sentence embeddings and probing linguistic properties, not on Spoken Language Models or speech-centric modeling, so it does not meet the inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The title/abstract focus on text-based sentence embeddings and probing linguistic properties, not on Spoken Language Models or speech-centric modeling, so it does not meet the inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The title and abstract focus on probing sentence embeddings for linguistic properties without mentioning spoken language models, speech language modeling, or the use of speech as a core modeling object; it is centered on textual sentence representations and lacks any reference to full-duplex interaction, speech signal modeling, or evaluation metrics related to synchronous spoken interaction as specified in the inclusion criteria, making it unsuitable for inclusion.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The title and abstract focus on probing sentence embeddings for linguistic properties without mentioning spoken language models, speech language modeling, or the use of speech as a core modeling object; it is centered on textual sentence representations and lacks any reference to full-duplex interaction, speech signal modeling, or evaluation metrics related to synchronous spoken interaction as specified in the inclusion criteria, making it unsuitable for inclusion.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Stanza: A Python Natural Language Processing Toolkit for Many Human Languages",
    "abstract": "We introduce Stanza, an open-source Python natural language processing toolkit supporting 66 human languages. Compared to existing widely used toolkits, Stanza features a language-agnostic fully neural pipeline for text analysis, including tokenization, multi-word token expansion, lemmatization, part-of-speech and morphological feature tagging, dependency parsing, and named entity recognition. We have trained Stanza on a total of 112 datasets, including the Universal Dependencies treebanks and other multilingual corpora, and show that the same neural architecture generalizes well and achieves competitive performance on all languages tested. Additionally, Stanza includes a native Python interface to the widely used Java Stanford CoreNLP software, which further extends its functionality to cover other tasks such as coreference resolution and relation extraction. Source code, documentation, and pretrained models for 66 languages are available at https://stanfordnlp.github.io/stanza/.",
    "metadata": {
      "title": "Stanza: A Python Natural Language Processing Toolkit for Many Human Languages",
      "summary": "We introduce Stanza, an open-source Python natural language processing toolkit supporting 66 human languages. Compared to existing widely used toolkits, Stanza features a language-agnostic fully neural pipeline for text analysis, including tokenization, multi-word token expansion, lemmatization, part-of-speech and morphological feature tagging, dependency parsing, and named entity recognition. We have trained Stanza on a total of 112 datasets, including the Universal Dependencies treebanks and other multilingual corpora, and show that the same neural architecture generalizes well and achieves competitive performance on all languages tested. Additionally, Stanza includes a native Python interface to the widely used Java Stanford CoreNLP software, which further extends its functionality to cover other tasks such as coreference resolution and relation extraction. Source code, documentation, and pretrained models for 66 languages are available at https://stanfordnlp.github.io/stanza/.",
      "abstract": "We introduce Stanza, an open-source Python natural language processing toolkit supporting 66 human languages. Compared to existing widely used toolkits, Stanza features a language-agnostic fully neural pipeline for text analysis, including tokenization, multi-word token expansion, lemmatization, part-of-speech and morphological feature tagging, dependency parsing, and named entity recognition. We have trained Stanza on a total of 112 datasets, including the Universal Dependencies treebanks and other multilingual corpora, and show that the same neural architecture generalizes well and achieves competitive performance on all languages tested. Additionally, Stanza includes a native Python interface to the widely used Java Stanford CoreNLP software, which further extends its functionality to cover other tasks such as coreference resolution and relation extraction. Source code, documentation, and pretrained models for 66 languages are available at https://stanfordnlp.github.io/stanza/.",
      "doi": "https://doi.org/10.18653/v1/2020.acl-demos.14",
      "openalex_id": "https://openalex.org/W3037109418",
      "arxiv_id": "",
      "publication_date": "2020-01-01",
      "published": "2020-01-01",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The paper describes Stanza, a multilingual text NLP toolkit focused on tokenization, lemmatization, POS, parsing, and NER; it does not center on spoken language models or speech-based modeling, so it fails the inclusion criteria and aligns with exclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The paper describes Stanza, a multilingual text NLP toolkit focused on tokenization, lemmatization, POS, parsing, and NER; it does not center on spoken language models or speech-based modeling, so it fails the inclusion criteria and aligns with exclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The article focuses on a Python NLP toolkit supporting multiple human languages with neural models for text processing tasks such as tokenization and parsing. However, it centers on text-based natural language processing, not spoken language modeling or speech as the core modeling object; it also lacks discussion of spoken language model architectures, training, or evaluation relevant to full-duplex spoken language models or synchronous interactive behavior as specified in the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The article focuses on a Python NLP toolkit supporting multiple human languages with neural models for text processing tasks such as tokenization and parsing. However, it centers on text-based natural language processing, not spoken language modeling or speech as the core modeling object; it also lacks discussion of spoken language model architectures, training, or evaluation relevant to full-duplex spoken language models or synchronous interactive behavior as specified in the inclusion criteria.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Analyzing Hidden Representations in End-to-End Automatic Speech Recognition Systems",
    "abstract": "Neural models have become ubiquitous in automatic speech recognition systems. While neural networks are typically used as acoustic models in more complex systems, recent studies have explored end-to-end speech recognition systems based on neural networks, which can be trained to directly predict text from input acoustic features. Although such systems are conceptually elegant and simpler than traditional systems, it is less obvious how to interpret the trained models. In this work, we analyze the speech representations learned by a deep end-to-end model that is based on convolutional and recurrent layers, and trained with a connectionist temporal classification (CTC) loss. We use a pre-trained model to generate frame-level features which are given to a classifier that is trained on frame classification into phones. We evaluate representations from different layers of the deep model and compare their quality for predicting phone labels. Our experiments shed light on important aspects of the end-to-end model such as layer depth, model complexity, and other design choices.",
    "metadata": {
      "title": "Analyzing Hidden Representations in End-to-End Automatic Speech Recognition Systems",
      "summary": "Neural models have become ubiquitous in automatic speech recognition systems. While neural networks are typically used as acoustic models in more complex systems, recent studies have explored end-to-end speech recognition systems based on neural networks, which can be trained to directly predict text from input acoustic features. Although such systems are conceptually elegant and simpler than traditional systems, it is less obvious how to interpret the trained models. In this work, we analyze the speech representations learned by a deep end-to-end model that is based on convolutional and recurrent layers, and trained with a connectionist temporal classification (CTC) loss. We use a pre-trained model to generate frame-level features which are given to a classifier that is trained on frame classification into phones. We evaluate representations from different layers of the deep model and compare their quality for predicting phone labels. Our experiments shed light on important aspects of the end-to-end model such as layer depth, model complexity, and other design choices.",
      "abstract": "Neural models have become ubiquitous in automatic speech recognition systems. While neural networks are typically used as acoustic models in more complex systems, recent studies have explored end-to-end speech recognition systems based on neural networks, which can be trained to directly predict text from input acoustic features. Although such systems are conceptually elegant and simpler than traditional systems, it is less obvious how to interpret the trained models. In this work, we analyze the speech representations learned by a deep end-to-end model that is based on convolutional and recurrent layers, and trained with a connectionist temporal classification (CTC) loss. We use a pre-trained model to generate frame-level features which are given to a classifier that is trained on frame classification into phones. We evaluate representations from different layers of the deep model and compare their quality for predicting phone labels. Our experiments shed light on important aspects of the end-to-end model such as layer depth, model complexity, and other design choices.",
      "doi": "https://doi.org/10.48550/arxiv.1709.04482",
      "openalex_id": "https://openalex.org/W2752168051",
      "arxiv_id": "",
      "publication_date": "2017-09-13",
      "published": "2017-09-13",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This item analyzes internal representations in an end-to-end ASR model rather than focusing on Spoken Language Models (SLMs) or full-duplex speech-language modeling, and it does not address dialogue/synchronization aspects, so it does not meet the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "This item analyzes internal representations in an end-to-end ASR model rather than focusing on Spoken Language Models (SLMs) or full-duplex speech-language modeling, and it does not address dialogue/synchronization aspects, so it does not meet the inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study centers on analyzing hidden representations in end-to-end automatic speech recognition systems, focusing mainly on acoustic modeling and phone classification without explicitly discussing spoken language models, their architecture, synchronization, or interactive dialogue behavior, nor does it mention full-duplex or speech language modeling concepts within the last 3 years; thus, it primarily addresses an ASR task and representation analysis without meeting inclusion criteria regarding spoken language modeling or interaction aspects.",
      "evaluation": 2
    },
    "round-A_JuniorMini_reasoning": "The study centers on analyzing hidden representations in end-to-end automatic speech recognition systems, focusing mainly on acoustic modeling and phone classification without explicitly discussing spoken language models, their architecture, synchronization, or interactive dialogue behavior, nor does it mention full-duplex or speech language modeling concepts within the last 3 years; thus, it primarily addresses an ASR task and representation analysis without meeting inclusion criteria regarding spoken language modeling or interaction aspects.",
    "round-A_JuniorMini_evaluation": 2,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:2)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Analyzing analytical methods: The case of phonology in neural models of\\n spoken language",
    "abstract": "Given the fast development of analysis techniques for NLP and speech\\nprocessing systems, few systematic studies have been conducted to compare the\\nstrengths and weaknesses of each method. As a step in this direction we study\\nthe case of representations of phonology in neural network models of spoken\\nlanguage. We use two commonly applied analytical techniques, diagnostic\\nclassifiers and representational similarity analysis, to quantify to what\\nextent neural activation patterns encode phonemes and phoneme sequences. We\\nmanipulate two factors that can affect the outcome of analysis. First, we\\ninvestigate the role of learning by comparing neural activations extracted from\\ntrained versus randomly-initialized models. Second, we examine the temporal\\nscope of the activations by probing both local activations corresponding to a\\nfew milliseconds of the speech signal, and global activations pooled over the\\nwhole utterance. We conclude that reporting analysis results with randomly\\ninitialized models is crucial, and that global-scope methods tend to yield more\\nconsistent results and we recommend their use as a complement to local-scope\\ndiagnostic methods.\\n",
    "metadata": {
      "title": "Analyzing analytical methods: The case of phonology in neural models of\\n spoken language",
      "summary": "Given the fast development of analysis techniques for NLP and speech\\nprocessing systems, few systematic studies have been conducted to compare the\\nstrengths and weaknesses of each method. As a step in this direction we study\\nthe case of representations of phonology in neural network models of spoken\\nlanguage. We use two commonly applied analytical techniques, diagnostic\\nclassifiers and representational similarity analysis, to quantify to what\\nextent neural activation patterns encode phonemes and phoneme sequences. We\\nmanipulate two factors that can affect the outcome of analysis. First, we\\ninvestigate the role of learning by comparing neural activations extracted from\\ntrained versus randomly-initialized models. Second, we examine the temporal\\nscope of the activations by probing both local activations corresponding to a\\nfew milliseconds of the speech signal, and global activations pooled over the\\nwhole utterance. We conclude that reporting analysis results with randomly\\ninitialized models is crucial, and that global-scope methods tend to yield more\\nconsistent results and we recommend their use as a complement to local-scope\\ndiagnostic methods.\\n",
      "abstract": "Given the fast development of analysis techniques for NLP and speech\\nprocessing systems, few systematic studies have been conducted to compare the\\nstrengths and weaknesses of each method. As a step in this direction we study\\nthe case of representations of phonology in neural network models of spoken\\nlanguage. We use two commonly applied analytical techniques, diagnostic\\nclassifiers and representational similarity analysis, to quantify to what\\nextent neural activation patterns encode phonemes and phoneme sequences. We\\nmanipulate two factors that can affect the outcome of analysis. First, we\\ninvestigate the role of learning by comparing neural activations extracted from\\ntrained versus randomly-initialized models. Second, we examine the temporal\\nscope of the activations by probing both local activations corresponding to a\\nfew milliseconds of the speech signal, and global activations pooled over the\\nwhole utterance. We conclude that reporting analysis results with randomly\\ninitialized models is crucial, and that global-scope methods tend to yield more\\nconsistent results and we recommend their use as a complement to local-scope\\ndiagnostic methods.\\n",
      "doi": "https://doi.org/10.18653/v1/2020.acl-main.381",
      "openalex_id": "https://openalex.org/W3035750922",
      "arxiv_id": "",
      "publication_date": "2020-04-15",
      "published": "2020-04-15",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "本文聚焦語音語言模型中的語音層級表示，使用診斷分類器與表示相似性分析等分析方法，探究神經模型對語音/音素表徵的激活，符合以 Spoken Language Models / Speech Language Models 為核心、涵蓋模型架構與評估面向的入選條件，且屬於近年（假定在 3 年內）研究，符合本題的時效與範疇限制。",
      "evaluation": 5
    },
    "round-A_JuniorNano_reasoning": "本文聚焦語音語言模型中的語音層級表示，使用診斷分類器與表示相似性分析等分析方法，探究神經模型對語音/音素表徵的激活，符合以 Spoken Language Models / Speech Language Models 為核心、涵蓋模型架構與評估面向的入選條件，且屬於近年（假定在 3 年內）研究，符合本題的時效與範疇限制。",
    "round-A_JuniorNano_evaluation": 5,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on analyzing phonology representations in neural network models of spoken language using analytical methods without explicitly addressing Spoken Language Models or Full-Duplex Spoken Language Models as core modeling objects, nor discussing model architecture, training, or evaluation in the context of turn-taking or full-duplex interactive capabilities. It also lacks mention of spoken language modeling or its equivalents in the title or abstract and does not discuss synchronization, behavioral arbitration, or low-latency interaction as required by the inclusion criteria. Therefore, it does not meet the inclusion criteria and partially matches exclusion criteria focusing more on phonological analysis rather than SLMs or FD-SLMs.",
      "evaluation": 2
    },
    "round-A_JuniorMini_reasoning": "The study focuses on analyzing phonology representations in neural network models of spoken language using analytical methods without explicitly addressing Spoken Language Models or Full-Duplex Spoken Language Models as core modeling objects, nor discussing model architecture, training, or evaluation in the context of turn-taking or full-duplex interactive capabilities. It also lacks mention of spoken language modeling or its equivalents in the title or abstract and does not discuss synchronization, behavioral arbitration, or low-latency interaction as required by the inclusion criteria. Therefore, it does not meet the inclusion criteria and partially matches exclusion criteria focusing more on phonological analysis rather than SLMs or FD-SLMs.",
    "round-A_JuniorMini_evaluation": 2,
    "round-B_SeniorLead_output": {
      "reasoning": "Junior reviewers split: JuniorNano judged inclusion (5) because the paper analyzes phonological representations in neural models of spoken language and thus touches on speech-model representations; JuniorMini judged exclusion (2) noting the abstract lacks any mention of spoken language modeling for interaction, full‑duplex capability, turn‑taking, synchronization, or related evaluations. My assessment: while the abstract studies phonological representations in neural models of spoken language, it is an analysis methods paper and does not explicitly treat Spoken Language Models/FD‑SLMs as core systems for end‑to‑end speech modeling with interaction/synchronization, nor does it discuss architectures, training/evaluation for real‑time or full‑duplex behavior (turn‑taking, overlap handling, low latency, etc.). Therefore it fails the inclusion requirements that demand explicit SLM/FD‑SLM focus and coverage of interaction/synchronization or relevant evaluations. Final one‑sentence justification: Better to exclude (2) — the work analyzes phonological representations in speech models but does not meet the required focus on Spoken Language Models/FD‑SLMs or interactive/synchronization evaluations demanded by the inclusion criteria.",
      "evaluation": 2
    },
    "round-B_SeniorLead_reasoning": "Junior reviewers split: JuniorNano judged inclusion (5) because the paper analyzes phonological representations in neural models of spoken language and thus touches on speech-model representations; JuniorMini judged exclusion (2) noting the abstract lacks any mention of spoken language modeling for interaction, full‑duplex capability, turn‑taking, synchronization, or related evaluations. My assessment: while the abstract studies phonological representations in neural models of spoken language, it is an analysis methods paper and does not explicitly treat Spoken Language Models/FD‑SLMs as core systems for end‑to‑end speech modeling with interaction/synchronization, nor does it discuss architectures, training/evaluation for real‑time or full‑duplex behavior (turn‑taking, overlap handling, low latency, etc.). Therefore it fails the inclusion requirements that demand explicit SLM/FD‑SLM focus and coverage of interaction/synchronization or relevant evaluations. Final one‑sentence justification: Better to exclude (2) — the work analyzes phonological representations in speech models but does not meet the required focus on Spoken Language Models/FD‑SLMs or interactive/synchronization evaluations demanded by the inclusion criteria.",
    "round-B_SeniorLead_evaluation": 2,
    "final_verdict": "exclude (senior:2)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Representational similarity analysis – connecting the branches of systems neuroscience",
    "abstract": "A FUNDAMENTAL CHALLENGE FOR SYSTEMS NEUROSCIENCE IS TO QUANTITATIVELY RELATE ITS THREE MAJOR BRANCHES OF RESEARCH: brain-activity measurement, behavioral measurement, and computational modeling. Using measured brain-activity patterns to evaluate computational network models is complicated by the need to define the correspondency between the units of the model and the channels of the brain-activity data, e.g., single-cell recordings or voxels from functional magnetic resonance imaging (fMRI). Similar correspondency problems complicate relating activity patterns between different modalities of brain-activity measurement (e.g., fMRI and invasive or scalp electrophysiology), and between subjects and species. In order to bridge these divides, we suggest abstracting from the activity patterns themselves and computing representational dissimilarity matrices (RDMs), which characterize the information carried by a given representation in a brain or model. Building on a rich psychological and mathematical literature on similarity analysis, we propose a new experimental and data-analytical framework called representational similarity analysis (RSA), in which multi-channel measures of neural activity are quantitatively related to each other and to computational theory and behavior by comparing RDMs. We demonstrate RSA by relating representations of visual objects as measured with fMRI in early visual cortex and the fusiform face area to computational models spanning a wide range of complexities. The RDMs are simultaneously related via second-level application of multidimensional scaling and tested using randomization and bootstrap techniques. We discuss the broad potential of RSA, including novel approaches to experimental design, and argue that these ideas, which have deep roots in psychology and neuroscience, will allow the integrated quantitative analysis of data from all three branches, thus contributing to a more unified systems neuroscience.",
    "metadata": {
      "title": "Representational similarity analysis – connecting the branches of systems neuroscience",
      "summary": "A FUNDAMENTAL CHALLENGE FOR SYSTEMS NEUROSCIENCE IS TO QUANTITATIVELY RELATE ITS THREE MAJOR BRANCHES OF RESEARCH: brain-activity measurement, behavioral measurement, and computational modeling. Using measured brain-activity patterns to evaluate computational network models is complicated by the need to define the correspondency between the units of the model and the channels of the brain-activity data, e.g., single-cell recordings or voxels from functional magnetic resonance imaging (fMRI). Similar correspondency problems complicate relating activity patterns between different modalities of brain-activity measurement (e.g., fMRI and invasive or scalp electrophysiology), and between subjects and species. In order to bridge these divides, we suggest abstracting from the activity patterns themselves and computing representational dissimilarity matrices (RDMs), which characterize the information carried by a given representation in a brain or model. Building on a rich psychological and mathematical literature on similarity analysis, we propose a new experimental and data-analytical framework called representational similarity analysis (RSA), in which multi-channel measures of neural activity are quantitatively related to each other and to computational theory and behavior by comparing RDMs. We demonstrate RSA by relating representations of visual objects as measured with fMRI in early visual cortex and the fusiform face area to computational models spanning a wide range of complexities. The RDMs are simultaneously related via second-level application of multidimensional scaling and tested using randomization and bootstrap techniques. We discuss the broad potential of RSA, including novel approaches to experimental design, and argue that these ideas, which have deep roots in psychology and neuroscience, will allow the integrated quantitative analysis of data from all three branches, thus contributing to a more unified systems neuroscience.",
      "abstract": "A FUNDAMENTAL CHALLENGE FOR SYSTEMS NEUROSCIENCE IS TO QUANTITATIVELY RELATE ITS THREE MAJOR BRANCHES OF RESEARCH: brain-activity measurement, behavioral measurement, and computational modeling. Using measured brain-activity patterns to evaluate computational network models is complicated by the need to define the correspondency between the units of the model and the channels of the brain-activity data, e.g., single-cell recordings or voxels from functional magnetic resonance imaging (fMRI). Similar correspondency problems complicate relating activity patterns between different modalities of brain-activity measurement (e.g., fMRI and invasive or scalp electrophysiology), and between subjects and species. In order to bridge these divides, we suggest abstracting from the activity patterns themselves and computing representational dissimilarity matrices (RDMs), which characterize the information carried by a given representation in a brain or model. Building on a rich psychological and mathematical literature on similarity analysis, we propose a new experimental and data-analytical framework called representational similarity analysis (RSA), in which multi-channel measures of neural activity are quantitatively related to each other and to computational theory and behavior by comparing RDMs. We demonstrate RSA by relating representations of visual objects as measured with fMRI in early visual cortex and the fusiform face area to computational models spanning a wide range of complexities. The RDMs are simultaneously related via second-level application of multidimensional scaling and tested using randomization and bootstrap techniques. We discuss the broad potential of RSA, including novel approaches to experimental design, and argue that these ideas, which have deep roots in psychology and neuroscience, will allow the integrated quantitative analysis of data from all three branches, thus contributing to a more unified systems neuroscience.",
      "doi": "https://doi.org/10.3389/neuro.06.004.2008",
      "openalex_id": "https://openalex.org/W2160654481",
      "arxiv_id": "",
      "publication_date": "2008-01-01",
      "published": "2008-01-01",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The paper focuses on representational similarity analysis in systems neuroscience, relating brain activity to computational models, and does not address Spoken Language Models, Speech Language Models, or any form of synchronous speech interaction. It fails all inclusion criteria and does not meet the exclusion criteria related to ASR/TTS or single-task speech systems, so it should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The paper focuses on representational similarity analysis in systems neuroscience, relating brain activity to computational models, and does not address Spoken Language Models, Speech Language Models, or any form of synchronous speech interaction. It fails all inclusion criteria and does not meet the exclusion criteria related to ASR/TTS or single-task speech systems, so it should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on representational similarity analysis relating to systems neuroscience, brain activity, and computational models, with no mention of spoken language models, speech language modeling, or full-duplex spoken language interaction, which are required by the inclusion criteria; it also lacks discussion on speech sequence modeling or the relevant architecture/training/evaluation aspects, and does not address synchronous speech interaction or dialogue behavior arbitration, thus should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on representational similarity analysis relating to systems neuroscience, brain activity, and computational models, with no mention of spoken language models, speech language modeling, or full-duplex spoken language interaction, which are required by the inclusion criteria; it also lacks discussion on speech sequence modeling or the relevant architecture/training/evaluation aspects, and does not address synchronous speech interaction or dialogue behavior arbitration, thus should be excluded.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Correlating Neural and Symbolic Representations of Language",
    "abstract": "Analysis methods which enable us to better understand the representations and\\nfunctioning of neural models of language are increasingly needed as deep\\nlearning becomes the dominant approach in NLP. Here we present two methods\\nbased on Representational Similarity Analysis (RSA) and Tree Kernels (TK) which\\nallow us to directly quantify how strongly the information encoded in neural\\nactivation patterns corresponds to information represented by symbolic\\nstructures such as syntax trees. We first validate our methods on the case of a\\nsimple synthetic language for arithmetic expressions with clearly defined\\nsyntax and semantics, and show that they exhibit the expected pattern of\\nresults. We then apply our methods to correlate neural representations of\\nEnglish sentences with their constituency parse trees.\\n",
    "metadata": {
      "title": "Correlating Neural and Symbolic Representations of Language",
      "summary": "Analysis methods which enable us to better understand the representations and\\nfunctioning of neural models of language are increasingly needed as deep\\nlearning becomes the dominant approach in NLP. Here we present two methods\\nbased on Representational Similarity Analysis (RSA) and Tree Kernels (TK) which\\nallow us to directly quantify how strongly the information encoded in neural\\nactivation patterns corresponds to information represented by symbolic\\nstructures such as syntax trees. We first validate our methods on the case of a\\nsimple synthetic language for arithmetic expressions with clearly defined\\nsyntax and semantics, and show that they exhibit the expected pattern of\\nresults. We then apply our methods to correlate neural representations of\\nEnglish sentences with their constituency parse trees.\\n",
      "abstract": "Analysis methods which enable us to better understand the representations and\\nfunctioning of neural models of language are increasingly needed as deep\\nlearning becomes the dominant approach in NLP. Here we present two methods\\nbased on Representational Similarity Analysis (RSA) and Tree Kernels (TK) which\\nallow us to directly quantify how strongly the information encoded in neural\\nactivation patterns corresponds to information represented by symbolic\\nstructures such as syntax trees. We first validate our methods on the case of a\\nsimple synthetic language for arithmetic expressions with clearly defined\\nsyntax and semantics, and show that they exhibit the expected pattern of\\nresults. We then apply our methods to correlate neural representations of\\nEnglish sentences with their constituency parse trees.\\n",
      "doi": "https://doi.org/10.18653/v1/p19-1283",
      "openalex_id": "https://openalex.org/W2946296745",
      "arxiv_id": "",
      "publication_date": "2019-01-01",
      "published": "2019-01-01",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "Reasoning: The abstract analyzes neural-symbolic representations of language using Representational Similarity Analysis (RSA) and Tree Kernels, tested on a synthetic language and English parse trees, but it does not center on Spoken Language Models or speech-centric, full-duplex interactive systems as required by the inclusion criteria. One-sentence explanation: It lacks the spoken-language modeling focus and speech-interaction evaluation emphasized in the criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "Reasoning: The abstract analyzes neural-symbolic representations of language using Representational Similarity Analysis (RSA) and Tree Kernels, tested on a synthetic language and English parse trees, but it does not center on Spoken Language Models or speech-centric, full-duplex interactive systems as required by the inclusion criteria. One-sentence explanation: It lacks the spoken-language modeling focus and speech-interaction evaluation emphasized in the criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on correlating neural and symbolic language representations, particularly in text-based NLP, without addressing spoken language modeling, speech signals, or the specified interaction capabilities such as full-duplex spoken language models or low-latency speech interaction; thus, it does not meet the inclusion criteria centered on spoken language models and their interaction features.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on correlating neural and symbolic language representations, particularly in text-based NLP, without addressing spoken language modeling, speech signals, or the specified interaction capabilities such as full-duplex spoken language models or low-latency speech interaction; thus, it does not meet the inclusion criteria centered on spoken language models and their interaction features.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Can phones, syllables, and words emerge as side-products of cross-situational audiovisual learning? - A computational investigation",
    "abstract": "Database hosting this document is corrupted and OSF is unable to fix it. Please refer to https://doi.org/10.34842/w3vw-s845 for openly available manuscript. ------- Previous abstract ------ Decades of research has studied how language learning infants learn to discriminate speech sounds, segment words, and associate words with their meanings. While gradual development of such capabilities is unquestionable, the exact nature of these skills and the underlying mental representations yet remains unclear. In parallel, computational studies have shown that basic comprehension of speech can be achieved by statistical learning between speech and concurrent referentially ambiguous visual input. These models can operate without prior linguistic knowledge such as representations of linguistic units, and without learning mechanisms specifically targeted at such units. This has raised the question of to what extent knowledge of linguistic units, such as phone(me)s, syllables, and words, could actually emerge as latent representations supporting the translation between speech and representations in other modalities, and without the units being proximal learning targets for the learner. In this study, we formulate this idea as the so-called latent language hypothesis (LLH), connecting linguistic representation learning to general predictive processing within and across sensory modalities. We review the extent that the audiovisual aspect of LLH is supported by the existing computational studies. We then explore LLH further in extensive learning simulations with different neural network models for audiovisual cross-situational learning, and comparing learning from both synthetic and real speech data. We investigate whether the latent representations learned by the networks reflect phonetic, syllabic, or lexical structure of input speech by utilizing an array of complementary evaluation metrics related to linguistic selectivity and temporal characteristics of the representations. As a result, we find that representations associated with phonetic, syllabic, and lexical units of speech indeed emerge from the audiovisual learning process. The finding is also robust against variations in model architecture or characteristics of model training and testing data. The results suggest that cross-modal and cross-situational learning may, in principle, assist in early language development much beyond just enabling association of acoustic word forms to their referential meanings.",
    "metadata": {
      "title": "Can phones, syllables, and words emerge as side-products of cross-situational audiovisual learning? - A computational investigation",
      "summary": "Database hosting this document is corrupted and OSF is unable to fix it. Please refer to https://doi.org/10.34842/w3vw-s845 for openly available manuscript. ------- Previous abstract ------ Decades of research has studied how language learning infants learn to discriminate speech sounds, segment words, and associate words with their meanings. While gradual development of such capabilities is unquestionable, the exact nature of these skills and the underlying mental representations yet remains unclear. In parallel, computational studies have shown that basic comprehension of speech can be achieved by statistical learning between speech and concurrent referentially ambiguous visual input. These models can operate without prior linguistic knowledge such as representations of linguistic units, and without learning mechanisms specifically targeted at such units. This has raised the question of to what extent knowledge of linguistic units, such as phone(me)s, syllables, and words, could actually emerge as latent representations supporting the translation between speech and representations in other modalities, and without the units being proximal learning targets for the learner. In this study, we formulate this idea as the so-called latent language hypothesis (LLH), connecting linguistic representation learning to general predictive processing within and across sensory modalities. We review the extent that the audiovisual aspect of LLH is supported by the existing computational studies. We then explore LLH further in extensive learning simulations with different neural network models for audiovisual cross-situational learning, and comparing learning from both synthetic and real speech data. We investigate whether the latent representations learned by the networks reflect phonetic, syllabic, or lexical structure of input speech by utilizing an array of complementary evaluation metrics related to linguistic selectivity and temporal characteristics of the representations. As a result, we find that representations associated with phonetic, syllabic, and lexical units of speech indeed emerge from the audiovisual learning process. The finding is also robust against variations in model architecture or characteristics of model training and testing data. The results suggest that cross-modal and cross-situational learning may, in principle, assist in early language development much beyond just enabling association of acoustic word forms to their referential meanings.",
      "abstract": "Database hosting this document is corrupted and OSF is unable to fix it. Please refer to https://doi.org/10.34842/w3vw-s845 for openly available manuscript. ------- Previous abstract ------ Decades of research has studied how language learning infants learn to discriminate speech sounds, segment words, and associate words with their meanings. While gradual development of such capabilities is unquestionable, the exact nature of these skills and the underlying mental representations yet remains unclear. In parallel, computational studies have shown that basic comprehension of speech can be achieved by statistical learning between speech and concurrent referentially ambiguous visual input. These models can operate without prior linguistic knowledge such as representations of linguistic units, and without learning mechanisms specifically targeted at such units. This has raised the question of to what extent knowledge of linguistic units, such as phone(me)s, syllables, and words, could actually emerge as latent representations supporting the translation between speech and representations in other modalities, and without the units being proximal learning targets for the learner. In this study, we formulate this idea as the so-called latent language hypothesis (LLH), connecting linguistic representation learning to general predictive processing within and across sensory modalities. We review the extent that the audiovisual aspect of LLH is supported by the existing computational studies. We then explore LLH further in extensive learning simulations with different neural network models for audiovisual cross-situational learning, and comparing learning from both synthetic and real speech data. We investigate whether the latent representations learned by the networks reflect phonetic, syllabic, or lexical structure of input speech by utilizing an array of complementary evaluation metrics related to linguistic selectivity and temporal characteristics of the representations. As a result, we find that representations associated with phonetic, syllabic, and lexical units of speech indeed emerge from the audiovisual learning process. The finding is also robust against variations in model architecture or characteristics of model training and testing data. The results suggest that cross-modal and cross-situational learning may, in principle, assist in early language development much beyond just enabling association of acoustic word forms to their referential meanings.",
      "doi": "https://doi.org/10.31234/osf.io/37zna",
      "openalex_id": "https://openalex.org/W4230640548",
      "arxiv_id": "",
      "publication_date": "2021-02-17",
      "published": "2021-02-17",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item focuses on emergent phonetic/lexical representations from cross-modal audiovisual learning using computational models, treats speech as the core modeling target with evaluation metrics, and is within the last 3 years, aligning with the Spoken Language Models criteria.",
      "evaluation": 5
    },
    "round-A_JuniorNano_reasoning": "The item focuses on emergent phonetic/lexical representations from cross-modal audiovisual learning using computational models, treats speech as the core modeling target with evaluation metrics, and is within the last 3 years, aligning with the Spoken Language Models criteria.",
    "round-A_JuniorNano_evaluation": 5,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on emergent linguistic units from cross-situational audiovisual learning using computational models, but it does not explicitly address spoken language models (SLMs) or full-duplex spoken language models, nor discuss model architectures, training, or evaluation related to synchronous turn-taking, low-latency interaction, or real-time dialogue behavior arbitration. Also, the abstract lacks mention of speech language modeling, synchronization mechanisms, or end-to-end or hybrid speech-LLM approaches, which are key inclusion criteria.",
      "evaluation": 2
    },
    "round-A_JuniorMini_reasoning": "The study focuses on emergent linguistic units from cross-situational audiovisual learning using computational models, but it does not explicitly address spoken language models (SLMs) or full-duplex spoken language models, nor discuss model architectures, training, or evaluation related to synchronous turn-taking, low-latency interaction, or real-time dialogue behavior arbitration. Also, the abstract lacks mention of speech language modeling, synchronization mechanisms, or end-to-end or hybrid speech-LLM approaches, which are key inclusion criteria.",
    "round-A_JuniorMini_evaluation": 2,
    "round-B_SeniorLead_output": {
      "reasoning": "I reviewed the abstract and the two junior reviewers' notes. The paper studies emergence of phonetic/syllabic/lexical representations from audiovisual cross-situational learning using computational models (a cognitive/computational representation-learning study), but it does not explicitly treat Spoken Language Models, speech-level language modeling, Speech–LLM or full‑duplex spoken interaction, nor does it mention the required terms (spoken language model / speech language model) or discuss architectures/training/evaluation for synchronous/low-latency full‑duplex capabilities—therefore it fails the inclusion criteria. JuniorNano appears to have over-interpreted the relevance (rated 5) by equating general speech representation work with SLM research, while JuniorMini (rated 2) correctly flagged the absence of SLM/FD‑SLM focus; I agree with the latter. One-sentence explanation: this work should be excluded because it does not meet the core inclusion requirement of explicitly focusing on Spoken Language Models / speech language modeling or on full‑duplex interactive speech system architectures and evaluations.",
      "evaluation": 1
    },
    "round-B_SeniorLead_reasoning": "I reviewed the abstract and the two junior reviewers' notes. The paper studies emergence of phonetic/syllabic/lexical representations from audiovisual cross-situational learning using computational models (a cognitive/computational representation-learning study), but it does not explicitly treat Spoken Language Models, speech-level language modeling, Speech–LLM or full‑duplex spoken interaction, nor does it mention the required terms (spoken language model / speech language model) or discuss architectures/training/evaluation for synchronous/low-latency full‑duplex capabilities—therefore it fails the inclusion criteria. JuniorNano appears to have over-interpreted the relevance (rated 5) by equating general speech representation work with SLM research, while JuniorMini (rated 2) correctly flagged the absence of SLM/FD‑SLM focus; I agree with the latter. One-sentence explanation: this work should be excluded because it does not meet the core inclusion requirement of explicitly focusing on Spoken Language Models / speech language modeling or on full‑duplex interactive speech system architectures and evaluations.",
    "round-B_SeniorLead_evaluation": 1,
    "final_verdict": "exclude (senior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Convolution Kernels for Natural Language",
    "abstract": "We describe the application of kernel methods to Natural Language Processing (NLP) problems. In many NLP tasks the objects being modeled are strings, trees, graphs or other discrete structures which require some mechanism to convert them into feature vectors. We describe kernels for various natural language structures, allowing rich, high dimensional representations of these structures. We show how a kernel over trees can be applied to parsing using the voted perceptron algorithm, and we give experimental results on the ATIS corpus of parse trees.",
    "metadata": {
      "title": "Convolution Kernels for Natural Language",
      "summary": "We describe the application of kernel methods to Natural Language Processing (NLP) problems. In many NLP tasks the objects being modeled are strings, trees, graphs or other discrete structures which require some mechanism to convert them into feature vectors. We describe kernels for various natural language structures, allowing rich, high dimensional representations of these structures. We show how a kernel over trees can be applied to parsing using the voted perceptron algorithm, and we give experimental results on the ATIS corpus of parse trees.",
      "abstract": "We describe the application of kernel methods to Natural Language Processing (NLP) problems. In many NLP tasks the objects being modeled are strings, trees, graphs or other discrete structures which require some mechanism to convert them into feature vectors. We describe kernels for various natural language structures, allowing rich, high dimensional representations of these structures. We show how a kernel over trees can be applied to parsing using the voted perceptron algorithm, and we give experimental results on the ATIS corpus of parse trees.",
      "doi": "https://doi.org/10.7551/mitpress/1120.003.0085",
      "openalex_id": "https://openalex.org/W2127713198",
      "arxiv_id": "",
      "publication_date": "2002-11-08",
      "published": "2002-11-08",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The title/abstract describe kernel methods for NLP with tree/parse representations, not spoken language models nor speech-centric modeling or interaction, so it does not meet the inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The title/abstract describe kernel methods for NLP with tree/parse representations, not spoken language models nor speech-centric modeling or interaction, so it does not meet the inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on kernel methods applied to natural language structures like strings and trees, with experimental results on parsing tasks; it does not address spoken language modeling, speech signal processing, or full-duplex spoken language interactions, nor does it mention any speech-based modeling, training, synchronization, or evaluation relevant to the inclusion criteria which center on speech-centric spoken language models and their interactive capabilities.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on kernel methods applied to natural language structures like strings and trees, with experimental results on parsing tasks; it does not address spoken language modeling, speech signal processing, or full-duplex spoken language interactions, nor does it mention any speech-based modeling, training, synchronization, or evaluation relevant to the inclusion criteria which center on speech-centric spoken language models and their interactive capabilities.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Making Tree Kernels Practical for Natural Language Learning.",
    "abstract": "In recent years tree kernels have been proposed for the automatic learning of natural language applications. Unfortunately, they show (a) an inherent super linear complexity and (b) a lower accuracy than traditional attribute/value methods.",
    "metadata": {
      "title": "Making Tree Kernels Practical for Natural Language Learning.",
      "summary": "In recent years tree kernels have been proposed for the automatic learning of natural language applications. Unfortunately, they show (a) an inherent super linear complexity and (b) a lower accuracy than traditional attribute/value methods.",
      "abstract": "In recent years tree kernels have been proposed for the automatic learning of natural language applications. Unfortunately, they show (a) an inherent super linear complexity and (b) a lower accuracy than traditional attribute/value methods.",
      "doi": "",
      "openalex_id": "https://openalex.org/W224064951",
      "arxiv_id": "",
      "publication_date": "2006-04-01",
      "published": "2006-04-01",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "此論文主題聚焦樹核（tree kernels）在自然語言學習中的應用與效能，未以 Spoken Language Models/語音語言建模為核心且未涉及語音互動或同步等評估，因此不符合 inclusion 條件的核心「以語音/語音建模為主」要求。",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "此論文主題聚焦樹核（tree kernels）在自然語言學習中的應用與效能，未以 Spoken Language Models/語音語言建模為核心且未涉及語音互動或同步等評估，因此不符合 inclusion 條件的核心「以語音/語音建模為主」要求。",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The title and abstract discuss tree kernels for natural language learning, focusing on computational complexity and accuracy issues, but do not mention anything related to spoken language models, speech modeling, or full-duplex interaction features specified in the inclusion criteria. The focus is not on modeling speech signals or end-to-end speech architectures but on kernel methods applied to natural language, which is primarily text-based, and does not meet the inclusion requirements or address the exclusion criteria effectively. Therefore, the study should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The title and abstract discuss tree kernels for natural language learning, focusing on computational complexity and accuracy issues, but do not mention anything related to spoken language models, speech modeling, or full-duplex interaction features specified in the inclusion criteria. The focus is not on modeling speech signals or end-to-end speech architectures but on kernel methods applied to natural language, which is primarily text-based, and does not meet the inclusion requirements or address the exclusion criteria effectively. Therefore, the study should be excluded.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Mockingjay: Unsupervised Speech Representation Learning with Deep Bidirectional Transformer Encoders",
    "abstract": "We present Mockingjay as a new speech representation learning approach, where\\nbidirectional Transformer encoders are pre-trained on a large amount of\\nunlabeled speech. Previous speech representation methods learn through\\nconditioning on past frames and predicting information about future frames.\\nWhereas Mockingjay is designed to predict the current frame through jointly\\nconditioning on both past and future contexts. The Mockingjay representation\\nimproves performance for a wide range of downstream tasks, including phoneme\\nclassification, speaker recognition, and sentiment classification on spoken\\ncontent, while outperforming other approaches. Mockingjay is empirically\\npowerful and can be fine-tuned with downstream models, with only 2 epochs we\\nfurther improve performance dramatically. In a low resource setting with only\\n0.1% of labeled data, we outperform the result of Mel-features that uses all\\n100% labeled data.\\n",
    "metadata": {
      "title": "Mockingjay: Unsupervised Speech Representation Learning with Deep Bidirectional Transformer Encoders",
      "summary": "We present Mockingjay as a new speech representation learning approach, where\\nbidirectional Transformer encoders are pre-trained on a large amount of\\nunlabeled speech. Previous speech representation methods learn through\\nconditioning on past frames and predicting information about future frames.\\nWhereas Mockingjay is designed to predict the current frame through jointly\\nconditioning on both past and future contexts. The Mockingjay representation\\nimproves performance for a wide range of downstream tasks, including phoneme\\nclassification, speaker recognition, and sentiment classification on spoken\\ncontent, while outperforming other approaches. Mockingjay is empirically\\npowerful and can be fine-tuned with downstream models, with only 2 epochs we\\nfurther improve performance dramatically. In a low resource setting with only\\n0.1% of labeled data, we outperform the result of Mel-features that uses all\\n100% labeled data.\\n",
      "abstract": "We present Mockingjay as a new speech representation learning approach, where\\nbidirectional Transformer encoders are pre-trained on a large amount of\\nunlabeled speech. Previous speech representation methods learn through\\nconditioning on past frames and predicting information about future frames.\\nWhereas Mockingjay is designed to predict the current frame through jointly\\nconditioning on both past and future contexts. The Mockingjay representation\\nimproves performance for a wide range of downstream tasks, including phoneme\\nclassification, speaker recognition, and sentiment classification on spoken\\ncontent, while outperforming other approaches. Mockingjay is empirically\\npowerful and can be fine-tuned with downstream models, with only 2 epochs we\\nfurther improve performance dramatically. In a low resource setting with only\\n0.1% of labeled data, we outperform the result of Mel-features that uses all\\n100% labeled data.\\n",
      "doi": "https://doi.org/10.1109/icassp40776.2020.9054458",
      "openalex_id": "https://openalex.org/W2982223350",
      "arxiv_id": "",
      "publication_date": "2020-04-09",
      "published": "2020-04-09",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The paper centers on unsupervised speech representation learning using bidirectional Transformer encoders, a core Spoken Language Model topic, and discusses architecture, training, and evaluation on multiple speech tasks, within the recent 3-year window, satisfying inclusion and not triggering exclusion criteria.",
      "evaluation": 5
    },
    "round-A_JuniorNano_reasoning": "The paper centers on unsupervised speech representation learning using bidirectional Transformer encoders, a core Spoken Language Model topic, and discusses architecture, training, and evaluation on multiple speech tasks, within the recent 3-year window, satisfying inclusion and not triggering exclusion criteria.",
    "round-A_JuniorNano_evaluation": 5,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on speech representation learning using bidirectional Transformer encoders and improves performance on various downstream speech tasks, but it does not explicitly address Spoken Language Models or Full-Duplex Spoken Language Models as defined in the inclusion criteria, nor does it discuss synchronization, turn-taking, or interactive aspects such as cognitive parallelism, interruptions, or backchanneling. Also, it primarily treats speech as input for representation learning rather than explicit speech language modeling or dialog interaction, limiting its relevance to the inclusion criteria.",
      "evaluation": 2
    },
    "round-A_JuniorMini_reasoning": "The study focuses on speech representation learning using bidirectional Transformer encoders and improves performance on various downstream speech tasks, but it does not explicitly address Spoken Language Models or Full-Duplex Spoken Language Models as defined in the inclusion criteria, nor does it discuss synchronization, turn-taking, or interactive aspects such as cognitive parallelism, interruptions, or backchanneling. Also, it primarily treats speech as input for representation learning rather than explicit speech language modeling or dialog interaction, limiting its relevance to the inclusion criteria.",
    "round-A_JuniorMini_evaluation": 2,
    "round-B_SeniorLead_output": {
      "reasoning": "Mockingjay is an unsupervised speech representation learning paper that pre-trains bidirectional Transformer encoders to produce acoustic features for downstream tasks (phoneme classification, speaker recognition, sentiment), but the abstract does not claim or evaluate a Spoken Language Model or language modeling of speech, nor does it address full‑duplex interaction, synchronization, overlap/interruption handling, or the required evaluation dimensions in the inclusion criteria; JuniorNano argued inclusion whereas JuniorMini argued exclusion, and I agree with the latter — the study should be excluded.",
      "evaluation": 1
    },
    "round-B_SeniorLead_reasoning": "Mockingjay is an unsupervised speech representation learning paper that pre-trains bidirectional Transformer encoders to produce acoustic features for downstream tasks (phoneme classification, speaker recognition, sentiment), but the abstract does not claim or evaluate a Spoken Language Model or language modeling of speech, nor does it address full‑duplex interaction, synchronization, overlap/interruption handling, or the required evaluation dimensions in the inclusion criteria; JuniorNano argued inclusion whereas JuniorMini argued exclusion, and I agree with the latter — the study should be excluded.",
    "round-B_SeniorLead_evaluation": 1,
    "final_verdict": "exclude (senior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Paralinguistics-Enhanced Large Language Modeling of Spoken Dialogue",
    "abstract": "Large Language Models (LLMs) have demonstrated superior abilities in tasks such as chatting, reasoning, and question-answering. However, standard LLMs may ignore crucial paralinguistic information, such as sentiment, emotion, and speaking style, which are essential for achieving natural, human-like spoken conversation, especially when such information is conveyed by acoustic cues. We therefore propose Paralinguistics-enhanced Generative Pretrained Transformer (ParalinGPT), an LLM that utilizes text and speech modalities to better model the linguistic content and paralinguistic attributes of spoken dialogue. The model takes the conversational context of text, speech embeddings, and paralinguistic attributes as input prompts within a serialized multitasking multimodal framework. Specifically, our framework serializes tasks in the order of current paralinguistic attribute prediction, response paralinguistic attribute prediction, and response text generation with autoregressive conditioning. We utilize the Switchboard-1 corpus, including its sentiment labels as the paralinguistic attribute, as our spoken dialogue dataset. Experimental results indicate the proposed serialized multitasking method outperforms typical sequence classification techniques on current and response sentiment classification. Furthermore, leveraging conversational context and speech embeddings significantly improves both response text generation and sentiment prediction. Our proposed framework achieves relative improvements of 6.7%, 12.0%, and 3.5% in current sentiment accuracy, response sentiment accuracy, and response text BLEU score, respectively.",
    "metadata": {
      "title": "Paralinguistics-Enhanced Large Language Modeling of Spoken Dialogue",
      "summary": "Large Language Models (LLMs) have demonstrated superior abilities in tasks such as chatting, reasoning, and question-answering. However, standard LLMs may ignore crucial paralinguistic information, such as sentiment, emotion, and speaking style, which are essential for achieving natural, human-like spoken conversation, especially when such information is conveyed by acoustic cues. We therefore propose Paralinguistics-enhanced Generative Pretrained Transformer (ParalinGPT), an LLM that utilizes text and speech modalities to better model the linguistic content and paralinguistic attributes of spoken dialogue. The model takes the conversational context of text, speech embeddings, and paralinguistic attributes as input prompts within a serialized multitasking multimodal framework. Specifically, our framework serializes tasks in the order of current paralinguistic attribute prediction, response paralinguistic attribute prediction, and response text generation with autoregressive conditioning. We utilize the Switchboard-1 corpus, including its sentiment labels as the paralinguistic attribute, as our spoken dialogue dataset. Experimental results indicate the proposed serialized multitasking method outperforms typical sequence classification techniques on current and response sentiment classification. Furthermore, leveraging conversational context and speech embeddings significantly improves both response text generation and sentiment prediction. Our proposed framework achieves relative improvements of 6.7%, 12.0%, and 3.5% in current sentiment accuracy, response sentiment accuracy, and response text BLEU score, respectively.",
      "abstract": "Large Language Models (LLMs) have demonstrated superior abilities in tasks such as chatting, reasoning, and question-answering. However, standard LLMs may ignore crucial paralinguistic information, such as sentiment, emotion, and speaking style, which are essential for achieving natural, human-like spoken conversation, especially when such information is conveyed by acoustic cues. We therefore propose Paralinguistics-enhanced Generative Pretrained Transformer (ParalinGPT), an LLM that utilizes text and speech modalities to better model the linguistic content and paralinguistic attributes of spoken dialogue. The model takes the conversational context of text, speech embeddings, and paralinguistic attributes as input prompts within a serialized multitasking multimodal framework. Specifically, our framework serializes tasks in the order of current paralinguistic attribute prediction, response paralinguistic attribute prediction, and response text generation with autoregressive conditioning. We utilize the Switchboard-1 corpus, including its sentiment labels as the paralinguistic attribute, as our spoken dialogue dataset. Experimental results indicate the proposed serialized multitasking method outperforms typical sequence classification techniques on current and response sentiment classification. Furthermore, leveraging conversational context and speech embeddings significantly improves both response text generation and sentiment prediction. Our proposed framework achieves relative improvements of 6.7%, 12.0%, and 3.5% in current sentiment accuracy, response sentiment accuracy, and response text BLEU score, respectively.",
      "doi": "https://doi.org/10.1109/icassp48485.2024.10446933",
      "openalex_id": "https://openalex.org/W4392931281",
      "arxiv_id": "",
      "publication_date": "2024-03-18",
      "published": "2024-03-18",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item centers on a Spoken Language Model / Speech-Language Modeling approach that uses text and speech modalities with paralinguistic attributes for dialogue tasks, describes a multimodal architecture and training/evaluation, and is from the last 3 years, meeting all inclusion criteria.",
      "evaluation": 5
    },
    "round-A_JuniorNano_reasoning": "The item centers on a Spoken Language Model / Speech-Language Modeling approach that uses text and speech modalities with paralinguistic attributes for dialogue tasks, describes a multimodal architecture and training/evaluation, and is from the last 3 years, meeting all inclusion criteria.",
    "round-A_JuniorNano_evaluation": 5,
    "round-A_JuniorMini_output": {
      "reasoning": "The study proposes a Paralinguistics-enhanced LLM that integrates speech and text modalities to model spoken dialogue, focusing on paralinguistic attributes with experimental validation; however, it does not explicitly address full-duplex spoken language models, real-time synchronous interactions, or detailed cognitive parallelism and behavioral arbitration mechanisms, and it primarily focuses on sentiment and response generation rather than low-latency or overlap/interruption management. The abstract shows integration of speech embeddings beyond text, which goes beyond simple ASR-LLM connection, but it is unclear if it meets the criteria about full-duplex interaction or engineered/learned synchronization, nor explicit inclusion of time-sensitive or overlapped speech handling. Therefore, the paper partially fits the spoken language modeling scope but lacks clear alignment with the full criteria especially regarding synchronization and evaluation measures related to synchronous interaction, leading to some uncertainty for inclusion.",
      "evaluation": 3
    },
    "round-A_JuniorMini_reasoning": "The study proposes a Paralinguistics-enhanced LLM that integrates speech and text modalities to model spoken dialogue, focusing on paralinguistic attributes with experimental validation; however, it does not explicitly address full-duplex spoken language models, real-time synchronous interactions, or detailed cognitive parallelism and behavioral arbitration mechanisms, and it primarily focuses on sentiment and response generation rather than low-latency or overlap/interruption management. The abstract shows integration of speech embeddings beyond text, which goes beyond simple ASR-LLM connection, but it is unclear if it meets the criteria about full-duplex interaction or engineered/learned synchronization, nor explicit inclusion of time-sensitive or overlapped speech handling. Therefore, the paper partially fits the spoken language modeling scope but lacks clear alignment with the full criteria especially regarding synchronization and evaluation measures related to synchronous interaction, leading to some uncertainty for inclusion.",
    "round-A_JuniorMini_evaluation": 3,
    "round-B_SeniorLead_output": {
      "reasoning": "Junior reviews: JuniorNano rated include (5), noting the paper integrates speech and text modalities and is recent; JuniorMini was uncertain (3), pointing out the work lacks explicit focus on full-duplex/synchronous interaction, overlap/interrupt handling, and engineered/learned synchronization. My assessment: the paper proposes a multimodal LLM that leverages speech embeddings and paralinguistic attributes for sentiment and response generation on Switchboard, but the abstract contains no discussion, architecture, experiments, or evaluations related to full‑duplex spoken language modeling, cognitive parallelism, overlap/interrupt handling, or low‑latency synchronous interaction—therefore it fails the topic’s core requirement of addressing FD‑SLM/synchronization concerns and falls under the exclusion criteria (primarily a text-centered LLM augmented with speech features). Brief explanation: although multimodal, it does not address full‑duplex/synchronous interaction or related evaluations required by the inclusion criteria, so it should be excluded.",
      "evaluation": 2
    },
    "round-B_SeniorLead_reasoning": "Junior reviews: JuniorNano rated include (5), noting the paper integrates speech and text modalities and is recent; JuniorMini was uncertain (3), pointing out the work lacks explicit focus on full-duplex/synchronous interaction, overlap/interrupt handling, and engineered/learned synchronization. My assessment: the paper proposes a multimodal LLM that leverages speech embeddings and paralinguistic attributes for sentiment and response generation on Switchboard, but the abstract contains no discussion, architecture, experiments, or evaluations related to full‑duplex spoken language modeling, cognitive parallelism, overlap/interrupt handling, or low‑latency synchronous interaction—therefore it fails the topic’s core requirement of addressing FD‑SLM/synchronization concerns and falls under the exclusion criteria (primarily a text-centered LLM augmented with speech features). Brief explanation: although multimodal, it does not address full‑duplex/synchronous interaction or related evaluations required by the inclusion criteria, so it should be excluded.",
    "round-B_SeniorLead_evaluation": 2,
    "final_verdict": "exclude (senior:2)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Speak, Read and Prompt: High-Fidelity Text-to-Speech with Minimal Supervision",
    "abstract": "Abstract We introduce SPEAR-TTS, a multi-speaker text-to-speech (TTS) system that can be trained with minimal supervision. By combining two types of discrete speech representations, we cast TTS as a composition of two sequence-to-sequence tasks: from text to high-level semantic tokens (akin to “reading”) and from semantic tokens to low-level acoustic tokens (“speaking”). Decoupling these two tasks enables training of the “speaking” module using abundant audio-only data, and unlocks the highly efficient combination of pretraining and backtranslation to reduce the need for parallel data when training the “reading” component. To control the speaker identity, we adopt example prompting, which allows SPEAR-TTS to generalize to unseen speakers using only a short sample of 3 seconds, without any explicit speaker representation or speaker labels. Our experiments demonstrate that SPEAR-TTS achieves a character error rate that is competitive with state-of-the-art methods using only 15 minutes of parallel data, while matching ground-truth speech in naturalness and acoustic quality.",
    "metadata": {
      "title": "Speak, Read and Prompt: High-Fidelity Text-to-Speech with Minimal Supervision",
      "summary": "Abstract We introduce SPEAR-TTS, a multi-speaker text-to-speech (TTS) system that can be trained with minimal supervision. By combining two types of discrete speech representations, we cast TTS as a composition of two sequence-to-sequence tasks: from text to high-level semantic tokens (akin to “reading”) and from semantic tokens to low-level acoustic tokens (“speaking”). Decoupling these two tasks enables training of the “speaking” module using abundant audio-only data, and unlocks the highly efficient combination of pretraining and backtranslation to reduce the need for parallel data when training the “reading” component. To control the speaker identity, we adopt example prompting, which allows SPEAR-TTS to generalize to unseen speakers using only a short sample of 3 seconds, without any explicit speaker representation or speaker labels. Our experiments demonstrate that SPEAR-TTS achieves a character error rate that is competitive with state-of-the-art methods using only 15 minutes of parallel data, while matching ground-truth speech in naturalness and acoustic quality.",
      "abstract": "Abstract We introduce SPEAR-TTS, a multi-speaker text-to-speech (TTS) system that can be trained with minimal supervision. By combining two types of discrete speech representations, we cast TTS as a composition of two sequence-to-sequence tasks: from text to high-level semantic tokens (akin to “reading”) and from semantic tokens to low-level acoustic tokens (“speaking”). Decoupling these two tasks enables training of the “speaking” module using abundant audio-only data, and unlocks the highly efficient combination of pretraining and backtranslation to reduce the need for parallel data when training the “reading” component. To control the speaker identity, we adopt example prompting, which allows SPEAR-TTS to generalize to unseen speakers using only a short sample of 3 seconds, without any explicit speaker representation or speaker labels. Our experiments demonstrate that SPEAR-TTS achieves a character error rate that is competitive with state-of-the-art methods using only 15 minutes of parallel data, while matching ground-truth speech in naturalness and acoustic quality.",
      "doi": "https://doi.org/10.1162/tacl_a_00618",
      "openalex_id": "https://openalex.org/W4390075359",
      "arxiv_id": "",
      "publication_date": "2023-01-01",
      "published": "2023-01-01",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The SPEAR-TTS abstract presents a speech-centric modeling approach (text-to-semantic tokens and semantic-to-acoustic tokens) with minimal supervision and speaker adaptation, aligning with spoken-language-model-like concepts, and it is within the last 3 years; however, it does not explicitly frame itself as a 'spoken language model' nor address interactive dialogue aspects, so it’s borderline for strict SLM criteria.",
      "evaluation": 4
    },
    "round-A_JuniorNano_reasoning": "The SPEAR-TTS abstract presents a speech-centric modeling approach (text-to-semantic tokens and semantic-to-acoustic tokens) with minimal supervision and speaker adaptation, aligning with spoken-language-model-like concepts, and it is within the last 3 years; however, it does not explicitly frame itself as a 'spoken language model' nor address interactive dialogue aspects, so it’s borderline for strict SLM criteria.",
    "round-A_JuniorNano_evaluation": 4,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on a text-to-speech (TTS) system that improves speech synthesis with minimal supervision but does not address Spoken Language Models (SLMs) or full-duplex spoken interaction models; it lacks discussion on speech as a core modeling object for synchronization, cognitive parallelism, or real-time dialogue behavior arbitration, which are key inclusion criteria, and primarily focuses on TTS performance evaluation (character error rate and naturalness) without addressing synchronous or interactive dialogue capabilities.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on a text-to-speech (TTS) system that improves speech synthesis with minimal supervision but does not address Spoken Language Models (SLMs) or full-duplex spoken interaction models; it lacks discussion on speech as a core modeling object for synchronization, cognitive parallelism, or real-time dialogue behavior arbitration, which are key inclusion criteria, and primarily focuses on TTS performance evaluation (character error rate and naturalness) without addressing synchronous or interactive dialogue capabilities.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": {
      "reasoning": "Both junior reviewers noted this is a TTS-focused paper; JuniorNano found it borderline but not framed as a spoken-language model, while JuniorMini recommended exclusion. The abstract describes a multi-speaker TTS that splits text→semantic and semantic→acoustic tasks and improves sample-efficiency and speaker adaptation, but it does not mention “spoken language model(s)” or equivalent, nor does it address full‑duplex interaction, synchronization, overlap/interrupt handling, low-latency real-time behavior, or evaluation of dialogic arbitration—therefore it meets the exclusion criteria (single-task TTS work) and fails the inclusion requirements. One-sentence explanation: Exclude because the study is a TTS system without explicit spoken-language-model framing or any full‑duplex/interactivity/synchronization contributions required by the inclusion criteria.",
      "evaluation": 1
    },
    "round-B_SeniorLead_reasoning": "Both junior reviewers noted this is a TTS-focused paper; JuniorNano found it borderline but not framed as a spoken-language model, while JuniorMini recommended exclusion. The abstract describes a multi-speaker TTS that splits text→semantic and semantic→acoustic tasks and improves sample-efficiency and speaker adaptation, but it does not mention “spoken language model(s)” or equivalent, nor does it address full‑duplex interaction, synchronization, overlap/interrupt handling, low-latency real-time behavior, or evaluation of dialogic arbitration—therefore it meets the exclusion criteria (single-task TTS work) and fails the inclusion requirements. One-sentence explanation: Exclude because the study is a TTS system without explicit spoken-language-model framing or any full‑duplex/interactivity/synchronization contributions required by the inclusion criteria.",
    "round-B_SeniorLead_evaluation": 1,
    "final_verdict": "exclude (senior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "LM-VC: Zero-Shot Voice Conversion via Speech Generation Based on Language Models",
    "abstract": "Language model (LM) based audio generation frameworks, e.g., AudioLM, have recently achieved new state-of-the-art performance in zero-shot audio generation. In this paper, we explore the feasibility of LMs for <italic xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">zero-shot voice conversion</i> . An intuitive approach is to follow AudioLM – Tokenizing speech into semantic and acoustic tokens respectively by HuBERT and SoundStream, and converting source semantic tokens to target acoustic tokens conditioned on acoustic tokens of the target speaker. However, such an approach encounters several issues: 1) the linguistic content contained in semantic tokens may get dispersed during multi-layer modeling while the lengthy speech input in the voice conversion task makes contextual learning even harder; 2) the semantic tokens still contain speaker-related information, which may be leaked to the target speech, lowering the target speaker similarity; 3) the generation diversity in the sampling of the LM can lead to unexpected outcomes during inference, leading to unnatural pronunciation and speech quality degradation. To mitigate these problems, we propose <italic xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">LM-VC</i> , a two-stage language modeling approach that generates coarse acoustic tokens for recovering the source linguistic content and target speaker's timbre, and then reconstructs the fine for acoustic details as converted speech. Specifically, to enhance content preservation and facilitates better disentanglement, a masked prefix LM with a mask prediction strategy is used for coarse acoustic modeling. This model is encouraged to recover the masked content from the surrounding context and generate target speech based on the target speaker's utterance and corrupted semantic tokens. Besides, to further alleviate the sampling error in the generation, an external LM, which employs window attention to capture the local acoustic relations, is introduced to participate in the coarse acoustic modeling through shallow fusion. Finally, a prefix LM reconstructs fine acoustic tokens from the coarse and results in the converted speech. Experiments demonstrate that LM-VC outperforms competitive systems in speech naturalness and speaker similarity.",
    "metadata": {
      "title": "LM-VC: Zero-Shot Voice Conversion via Speech Generation Based on Language Models",
      "summary": "Language model (LM) based audio generation frameworks, e.g., AudioLM, have recently achieved new state-of-the-art performance in zero-shot audio generation. In this paper, we explore the feasibility of LMs for <italic xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">zero-shot voice conversion</i> . An intuitive approach is to follow AudioLM – Tokenizing speech into semantic and acoustic tokens respectively by HuBERT and SoundStream, and converting source semantic tokens to target acoustic tokens conditioned on acoustic tokens of the target speaker. However, such an approach encounters several issues: 1) the linguistic content contained in semantic tokens may get dispersed during multi-layer modeling while the lengthy speech input in the voice conversion task makes contextual learning even harder; 2) the semantic tokens still contain speaker-related information, which may be leaked to the target speech, lowering the target speaker similarity; 3) the generation diversity in the sampling of the LM can lead to unexpected outcomes during inference, leading to unnatural pronunciation and speech quality degradation. To mitigate these problems, we propose <italic xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">LM-VC</i> , a two-stage language modeling approach that generates coarse acoustic tokens for recovering the source linguistic content and target speaker's timbre, and then reconstructs the fine for acoustic details as converted speech. Specifically, to enhance content preservation and facilitates better disentanglement, a masked prefix LM with a mask prediction strategy is used for coarse acoustic modeling. This model is encouraged to recover the masked content from the surrounding context and generate target speech based on the target speaker's utterance and corrupted semantic tokens. Besides, to further alleviate the sampling error in the generation, an external LM, which employs window attention to capture the local acoustic relations, is introduced to participate in the coarse acoustic modeling through shallow fusion. Finally, a prefix LM reconstructs fine acoustic tokens from the coarse and results in the converted speech. Experiments demonstrate that LM-VC outperforms competitive systems in speech naturalness and speaker similarity.",
      "abstract": "Language model (LM) based audio generation frameworks, e.g., AudioLM, have recently achieved new state-of-the-art performance in zero-shot audio generation. In this paper, we explore the feasibility of LMs for <italic xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">zero-shot voice conversion</i> . An intuitive approach is to follow AudioLM – Tokenizing speech into semantic and acoustic tokens respectively by HuBERT and SoundStream, and converting source semantic tokens to target acoustic tokens conditioned on acoustic tokens of the target speaker. However, such an approach encounters several issues: 1) the linguistic content contained in semantic tokens may get dispersed during multi-layer modeling while the lengthy speech input in the voice conversion task makes contextual learning even harder; 2) the semantic tokens still contain speaker-related information, which may be leaked to the target speech, lowering the target speaker similarity; 3) the generation diversity in the sampling of the LM can lead to unexpected outcomes during inference, leading to unnatural pronunciation and speech quality degradation. To mitigate these problems, we propose <italic xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">LM-VC</i> , a two-stage language modeling approach that generates coarse acoustic tokens for recovering the source linguistic content and target speaker's timbre, and then reconstructs the fine for acoustic details as converted speech. Specifically, to enhance content preservation and facilitates better disentanglement, a masked prefix LM with a mask prediction strategy is used for coarse acoustic modeling. This model is encouraged to recover the masked content from the surrounding context and generate target speech based on the target speaker's utterance and corrupted semantic tokens. Besides, to further alleviate the sampling error in the generation, an external LM, which employs window attention to capture the local acoustic relations, is introduced to participate in the coarse acoustic modeling through shallow fusion. Finally, a prefix LM reconstructs fine acoustic tokens from the coarse and results in the converted speech. Experiments demonstrate that LM-VC outperforms competitive systems in speech naturalness and speaker similarity.",
      "doi": "https://doi.org/10.1109/lsp.2023.3308474",
      "openalex_id": "https://openalex.org/W4386132131",
      "arxiv_id": "",
      "publication_date": "2023-01-01",
      "published": "2023-01-01",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This paper uses language models to model speech tokens for voice conversion, placing speech as the main modeling target with architecture and evaluation details, within the Spoken/Speech Language Modeling scope and within the past 3 years.",
      "evaluation": 4
    },
    "round-A_JuniorNano_reasoning": "This paper uses language models to model speech tokens for voice conversion, placing speech as the main modeling target with architecture and evaluation details, within the Spoken/Speech Language Modeling scope and within the past 3 years.",
    "round-A_JuniorNano_evaluation": 4,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on zero-shot voice conversion using language models based on speech generation, addressing acoustic tokens and semantic tokens processing; however, the abstract does not explicitly discuss spoken language models as core modeling objects in terms of full-duplex spoken language modeling or real-time interactive capabilities such as cognitive parallelism, turn-taking, interruptions, or backchanneling. The paper centers on speech token generation and conversion but lacks explicit mention of synchronization, multi-modal interaction, or dynamic behavior arbitration, which are critical for inclusion. Therefore, it partially fits relevance but misses critical inclusion criteria related to spoken language models with interaction and full-duplex capabilities.",
      "evaluation": 2
    },
    "round-A_JuniorMini_reasoning": "The study focuses on zero-shot voice conversion using language models based on speech generation, addressing acoustic tokens and semantic tokens processing; however, the abstract does not explicitly discuss spoken language models as core modeling objects in terms of full-duplex spoken language modeling or real-time interactive capabilities such as cognitive parallelism, turn-taking, interruptions, or backchanneling. The paper centers on speech token generation and conversion but lacks explicit mention of synchronization, multi-modal interaction, or dynamic behavior arbitration, which are critical for inclusion. Therefore, it partially fits relevance but misses critical inclusion criteria related to spoken language models with interaction and full-duplex capabilities.",
    "round-A_JuniorMini_evaluation": 2,
    "round-B_SeniorLead_output": {
      "reasoning": "The paper clearly studies language modeling of speech (LM-based audio generation, token-level speech LMs) and proposes novel LM architectures and training/inference strategies for voice conversion—meeting the requirement that speech LMs are the core modeling object—while one junior reviewer supported inclusion (4) and another recommended exclusion (2) due to lack of full‑duplex/interactive focus; on balance this is a speech‑LM modeling paper (though evaluated on a single downstream task, it includes architectural and modeling contributions), so it should be included.",
      "evaluation": 4
    },
    "round-B_SeniorLead_reasoning": "The paper clearly studies language modeling of speech (LM-based audio generation, token-level speech LMs) and proposes novel LM architectures and training/inference strategies for voice conversion—meeting the requirement that speech LMs are the core modeling object—while one junior reviewer supported inclusion (4) and another recommended exclusion (2) due to lack of full‑duplex/interactive focus; on balance this is a speech‑LM modeling paper (though evaluated on a single downstream task, it includes architectural and modeling contributions), so it should be included.",
    "round-B_SeniorLead_evaluation": 4,
    "final_verdict": "include (senior:4)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Disentangled Feature Learning for Real-Time Neural Speech Coding",
    "abstract": "Recently end-to-end neural audio/speech coding has shown its great potential to outperform traditional signal analysis based audio codecs. This is mostly achieved by following the VQ-VAE paradigm where blind features are learned, vector-quantized and coded. In this paper, instead of blind end-to-end learning, we propose to learn disentangled features for real-time neural speech coding. Specifically, more global-like speaker identity and local content features are learned with disentanglement to represent speech. Such a compact feature decomposition not only achieves better coding efficiency by exploiting bit allocation among different features but also provides the flexibility to do audio editing in embedding space, such as voice conversion in real-time communications. Both subjective and objective results demonstrate its coding efficiency and we find that the learned disentangled features show comparable performance on any-to-any voice conversion with modern self-supervised speech representation learning models with far less parameters and low latency, showing the potential of our neural coding framework.",
    "metadata": {
      "title": "Disentangled Feature Learning for Real-Time Neural Speech Coding",
      "summary": "Recently end-to-end neural audio/speech coding has shown its great potential to outperform traditional signal analysis based audio codecs. This is mostly achieved by following the VQ-VAE paradigm where blind features are learned, vector-quantized and coded. In this paper, instead of blind end-to-end learning, we propose to learn disentangled features for real-time neural speech coding. Specifically, more global-like speaker identity and local content features are learned with disentanglement to represent speech. Such a compact feature decomposition not only achieves better coding efficiency by exploiting bit allocation among different features but also provides the flexibility to do audio editing in embedding space, such as voice conversion in real-time communications. Both subjective and objective results demonstrate its coding efficiency and we find that the learned disentangled features show comparable performance on any-to-any voice conversion with modern self-supervised speech representation learning models with far less parameters and low latency, showing the potential of our neural coding framework.",
      "abstract": "Recently end-to-end neural audio/speech coding has shown its great potential to outperform traditional signal analysis based audio codecs. This is mostly achieved by following the VQ-VAE paradigm where blind features are learned, vector-quantized and coded. In this paper, instead of blind end-to-end learning, we propose to learn disentangled features for real-time neural speech coding. Specifically, more global-like speaker identity and local content features are learned with disentanglement to represent speech. Such a compact feature decomposition not only achieves better coding efficiency by exploiting bit allocation among different features but also provides the flexibility to do audio editing in embedding space, such as voice conversion in real-time communications. Both subjective and objective results demonstrate its coding efficiency and we find that the learned disentangled features show comparable performance on any-to-any voice conversion with modern self-supervised speech representation learning models with far less parameters and low latency, showing the potential of our neural coding framework.",
      "doi": "https://doi.org/10.1109/icassp49357.2023.10094723",
      "openalex_id": "https://openalex.org/W4372259964",
      "arxiv_id": "",
      "publication_date": "2023-05-05",
      "published": "2023-05-05",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "Abstract focuses on neural speech coding and disentangled features for real-time compression, not on Spoken Language Models or speech-language modeling concepts, so it does not meet the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "Abstract focuses on neural speech coding and disentangled features for real-time compression, not on Spoken Language Models or speech-language modeling concepts, so it does not meet the inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper focuses on disentangled feature learning for neural speech coding, emphasizing real-time and low-latency aspects, but it does not explicitly address Spoken Language Models or full-duplex spoken interactive systems; there is no mention of language modeling, turn-taking, cognitive parallelism, or synchronous interaction functionalities, and it centers on speech coding rather than speech language modeling or interaction, thus not meeting the inclusion criteria fully and partially falling under exclusion criteria.",
      "evaluation": 2
    },
    "round-A_JuniorMini_reasoning": "The paper focuses on disentangled feature learning for neural speech coding, emphasizing real-time and low-latency aspects, but it does not explicitly address Spoken Language Models or full-duplex spoken interactive systems; there is no mention of language modeling, turn-taking, cognitive parallelism, or synchronous interaction functionalities, and it centers on speech coding rather than speech language modeling or interaction, thus not meeting the inclusion criteria fully and partially falling under exclusion criteria.",
    "round-A_JuniorMini_evaluation": 2,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:2)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "MizAR 60 for Mizar 50",
    "abstract": "As a present to Mizar on its 50th anniversary, we develop an AI/TP system that automatically proves about 60% of the Mizar theorems in the hammer setting. We also automatically prove 75% of the Mizar theorems when the automated provers are helped by using only the premises used in the human-written Mizar proofs. We describe the methods and large-scale experiments leading to these results. This includes in particular the E and Vampire provers, their ENIGMA and Deepire learning modifications, a number of learning-based premise selection methods, and the incremental loop that interleaves growing a corpus of millions of ATP proofs with training increasingly strong AI/TP systems on them. We also present a selection of Mizar problems that were proved automatically.",
    "metadata": {
      "title": "MizAR 60 for Mizar 50",
      "summary": "As a present to Mizar on its 50th anniversary, we develop an AI/TP system that automatically proves about 60% of the Mizar theorems in the hammer setting. We also automatically prove 75% of the Mizar theorems when the automated provers are helped by using only the premises used in the human-written Mizar proofs. We describe the methods and large-scale experiments leading to these results. This includes in particular the E and Vampire provers, their ENIGMA and Deepire learning modifications, a number of learning-based premise selection methods, and the incremental loop that interleaves growing a corpus of millions of ATP proofs with training increasingly strong AI/TP systems on them. We also present a selection of Mizar problems that were proved automatically.",
      "abstract": "As a present to Mizar on its 50th anniversary, we develop an AI/TP system that automatically proves about 60% of the Mizar theorems in the hammer setting. We also automatically prove 75% of the Mizar theorems when the automated provers are helped by using only the premises used in the human-written Mizar proofs. We describe the methods and large-scale experiments leading to these results. This includes in particular the E and Vampire provers, their ENIGMA and Deepire learning modifications, a number of learning-based premise selection methods, and the incremental loop that interleaves growing a corpus of millions of ATP proofs with training increasingly strong AI/TP systems on them. We also present a selection of Mizar problems that were proved automatically.",
      "doi": "https://doi.org/10.4230/lipics.itp.2023.19",
      "openalex_id": "https://openalex.org/W4385245566",
      "arxiv_id": "",
      "publication_date": "2023-01-01",
      "published": "2023-01-01",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item concerns automated theorem proving in Mizar with ATP systems; it does not center on Spoken Language Models or speech-language modeling, nor discuss speech interaction or dialogue, thus fails the inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item concerns automated theorem proving in Mizar with ATP systems; it does not center on Spoken Language Models or speech-language modeling, nor discuss speech interaction or dialogue, thus fails the inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on automated theorem proving within Mizar, involving AI/TP systems and learning-based methods, without addressing spoken language models, speech-level language modeling, or full-duplex spoken language interactions as defined in the inclusion criteria. It neither involves speech as a core modeling object nor discusses synchronization or real-time interactive speech behavior, and thus does not meet the thematic requirements for inclusion.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on automated theorem proving within Mizar, involving AI/TP systems and learning-based methods, without addressing spoken language models, speech-level language modeling, or full-duplex spoken language interactions as defined in the inclusion criteria. It neither involves speech as a core modeling object nor discusses synchronization or real-time interactive speech behavior, and thus does not meet the thematic requirements for inclusion.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "UnitY: Two-pass Direct Speech-to-speech Translation with Discrete Units",
    "abstract": "Hirofumi Inaguma, Sravya Popuri, Ilia Kulikov, Peng-Jen Chen, Changhan Wang, Yu-An Chung, Yun Tang, Ann Lee, Shinji Watanabe, Juan Pino. Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). 2023.",
    "metadata": {
      "title": "UnitY: Two-pass Direct Speech-to-speech Translation with Discrete Units",
      "summary": "Hirofumi Inaguma, Sravya Popuri, Ilia Kulikov, Peng-Jen Chen, Changhan Wang, Yu-An Chung, Yun Tang, Ann Lee, Shinji Watanabe, Juan Pino. Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). 2023.",
      "abstract": "Hirofumi Inaguma, Sravya Popuri, Ilia Kulikov, Peng-Jen Chen, Changhan Wang, Yu-An Chung, Yun Tang, Ann Lee, Shinji Watanabe, Juan Pino. Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). 2023.",
      "doi": "https://doi.org/10.18653/v1/2023.acl-long.872",
      "openalex_id": "https://openalex.org/W4385570550",
      "arxiv_id": "",
      "publication_date": "2023-01-01",
      "published": "2023-01-01",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The inclusion criteria require explicit mention of Spoken Language Models (SLMs) or equivalent in the title/abstract/keywords; the provided item only has a title about two-pass direct speech-to-speech translation with discrete units, with no abstract content shown and no explicit indication of SLM/FD-SLM core or language modeling of speech in the visible text. Therefore it does not meet the SLM core criterion and should be excluded.",
      "evaluation": 2
    },
    "round-A_JuniorNano_reasoning": "The inclusion criteria require explicit mention of Spoken Language Models (SLMs) or equivalent in the title/abstract/keywords; the provided item only has a title about two-pass direct speech-to-speech translation with discrete units, with no abstract content shown and no explicit indication of SLM/FD-SLM core or language modeling of speech in the visible text. Therefore it does not meet the SLM core criterion and should be excluded.",
    "round-A_JuniorNano_evaluation": 2,
    "round-A_JuniorMini_output": {
      "reasoning": "The title \"UnitY: Two-pass Direct Speech-to-speech Translation with Discrete Units\" suggests a focus on direct speech-to-speech translation involving speech signals, potentially involving spoken language modeling. However, the abstract provided contains only bibliographic information without details on the model architecture, training, evaluation, or explicit mention of spoken language models, full-duplex interaction, or the key inclusion concepts. Thus, it is difficult to confirm if the study fulfills the inclusion criteria, especially the requirements of focusing on spoken language models as core, architectural or evaluation innovation, or synchronization behaviors. Without more detailed abstract content, the study cannot be confidently included.",
      "evaluation": 3
    },
    "round-A_JuniorMini_reasoning": "The title \"UnitY: Two-pass Direct Speech-to-speech Translation with Discrete Units\" suggests a focus on direct speech-to-speech translation involving speech signals, potentially involving spoken language modeling. However, the abstract provided contains only bibliographic information without details on the model architecture, training, evaluation, or explicit mention of spoken language models, full-duplex interaction, or the key inclusion concepts. Thus, it is difficult to confirm if the study fulfills the inclusion criteria, especially the requirements of focusing on spoken language models as core, architectural or evaluation innovation, or synchronization behaviors. Without more detailed abstract content, the study cannot be confidently included.",
    "round-A_JuniorMini_evaluation": 3,
    "round-B_SeniorLead_output": {
      "reasoning": "Both junior reviewers noted the item lacks an abstract describing spoken language modeling: the title \"UnitY: Two-pass Direct Speech-to-speech Translation with Discrete Units\" indicates work on direct speech-to-speech translation using discrete units but the provided record contains only bibliographic information and no explicit mention of Spoken Language Models, language modeling of speech, or the SLM/FD-SLM concepts required by the inclusion criteria. The paper is within the time window (2023) but the protocol requires that spoken language model(s) or equivalent phrasing appear in the title/abstract/keywords; that condition is not met. One junior reviewer recommended exclusion (2) for this reason and the other marked uncertain (3); given the strict requirement for explicit SLM/language-model-of-speech wording, I recommend exclusion. Explanation (one sentence): It should be excluded because the title/available abstract do not explicitly present Spoken Language Models or language modeling of speech, thus failing the mandatory inclusion criterion.",
      "evaluation": 2
    },
    "round-B_SeniorLead_reasoning": "Both junior reviewers noted the item lacks an abstract describing spoken language modeling: the title \"UnitY: Two-pass Direct Speech-to-speech Translation with Discrete Units\" indicates work on direct speech-to-speech translation using discrete units but the provided record contains only bibliographic information and no explicit mention of Spoken Language Models, language modeling of speech, or the SLM/FD-SLM concepts required by the inclusion criteria. The paper is within the time window (2023) but the protocol requires that spoken language model(s) or equivalent phrasing appear in the title/abstract/keywords; that condition is not met. One junior reviewer recommended exclusion (2) for this reason and the other marked uncertain (3); given the strict requirement for explicit SLM/language-model-of-speech wording, I recommend exclusion. Explanation (one sentence): It should be excluded because the title/available abstract do not explicitly present Spoken Language Models or language modeling of speech, thus failing the mandatory inclusion criterion.",
    "round-B_SeniorLead_evaluation": 2,
    "final_verdict": "exclude (senior:2)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Looking to listen at the cocktail party",
    "abstract": "We present a joint audio-visual model for isolating a single speech signal from a mixture of sounds such as other speakers and background noise. Solving this task using only audio as input is extremely challenging and does not provide an association of the separated speech signals with speakers in the video. In this paper, we present a deep network-based model that incorporates both visual and auditory signals to solve this task. The visual features are used to \"focus\" the audio on desired speakers in a scene and to improve the speech separation quality. To train our joint audio-visual model, we introduce AVS peech , a new dataset comprised of thousands of hours of video segments from the Web. We demonstrate the applicability of our method to classic speech separation tasks, as well as real-world scenarios involving heated interviews, noisy bars, and screaming children, only requiring the user to specify the face of the person in the video whose speech they want to isolate. Our method shows clear advantage over state-of-the-art audio-only speech separation in cases of mixed speech. In addition, our model, which is speaker-independent (trained once, applicable to any speaker), produces better results than recent audio-visual speech separation methods that are speaker-dependent (require training a separate model for each speaker of interest).",
    "metadata": {
      "title": "Looking to listen at the cocktail party",
      "summary": "We present a joint audio-visual model for isolating a single speech signal from a mixture of sounds such as other speakers and background noise. Solving this task using only audio as input is extremely challenging and does not provide an association of the separated speech signals with speakers in the video. In this paper, we present a deep network-based model that incorporates both visual and auditory signals to solve this task. The visual features are used to \"focus\" the audio on desired speakers in a scene and to improve the speech separation quality. To train our joint audio-visual model, we introduce AVS peech , a new dataset comprised of thousands of hours of video segments from the Web. We demonstrate the applicability of our method to classic speech separation tasks, as well as real-world scenarios involving heated interviews, noisy bars, and screaming children, only requiring the user to specify the face of the person in the video whose speech they want to isolate. Our method shows clear advantage over state-of-the-art audio-only speech separation in cases of mixed speech. In addition, our model, which is speaker-independent (trained once, applicable to any speaker), produces better results than recent audio-visual speech separation methods that are speaker-dependent (require training a separate model for each speaker of interest).",
      "abstract": "We present a joint audio-visual model for isolating a single speech signal from a mixture of sounds such as other speakers and background noise. Solving this task using only audio as input is extremely challenging and does not provide an association of the separated speech signals with speakers in the video. In this paper, we present a deep network-based model that incorporates both visual and auditory signals to solve this task. The visual features are used to \"focus\" the audio on desired speakers in a scene and to improve the speech separation quality. To train our joint audio-visual model, we introduce AVS peech , a new dataset comprised of thousands of hours of video segments from the Web. We demonstrate the applicability of our method to classic speech separation tasks, as well as real-world scenarios involving heated interviews, noisy bars, and screaming children, only requiring the user to specify the face of the person in the video whose speech they want to isolate. Our method shows clear advantage over state-of-the-art audio-only speech separation in cases of mixed speech. In addition, our model, which is speaker-independent (trained once, applicable to any speaker), produces better results than recent audio-visual speech separation methods that are speaker-dependent (require training a separate model for each speaker of interest).",
      "doi": "https://doi.org/10.1145/3197517.3201357",
      "openalex_id": "https://openalex.org/W4289665794",
      "arxiv_id": "",
      "publication_date": "2018-07-30",
      "published": "2018-07-30",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This item focuses on audio-visual speech separation and does not treat Spoken Language Models or Speech Language Models as the core modeling target, nor discuss language modeling of speech; thus it does not meet the inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "This item focuses on audio-visual speech separation and does not treat Spoken Language Models or Speech Language Models as the core modeling target, nor discuss language modeling of speech; thus it does not meet the inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on audio-visual speech separation using deep networks to isolate speech signals in noisy environments. Although it deals with speech signals and audio-visual modeling, it does not explicitly mention Spoken Language Models (SLMs) or full-duplex spoken language interaction, nor does it discuss speech language modeling architectures, training, or evaluation in the context of end-to-end spoken language models or full-duplex interactions as required by the inclusion criteria. The work is more focused on signal separation and enhancement rather than speech language modeling or synchronous dialog interaction capabilities, which are key for inclusion. Therefore, it does not meet the core thematic requirements to be included.",
      "evaluation": 2
    },
    "round-A_JuniorMini_reasoning": "The study focuses on audio-visual speech separation using deep networks to isolate speech signals in noisy environments. Although it deals with speech signals and audio-visual modeling, it does not explicitly mention Spoken Language Models (SLMs) or full-duplex spoken language interaction, nor does it discuss speech language modeling architectures, training, or evaluation in the context of end-to-end spoken language models or full-duplex interactions as required by the inclusion criteria. The work is more focused on signal separation and enhancement rather than speech language modeling or synchronous dialog interaction capabilities, which are key for inclusion. Therefore, it does not meet the core thematic requirements to be included.",
    "round-A_JuniorMini_evaluation": 2,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:2)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "MLS: A Large-Scale Multilingual Dataset for Speech Research",
    "abstract": "This paper introduces Multilingual LibriSpeech (MLS) dataset, a large multilingual corpus suitable for speech research. The dataset is derived from read audiobooks from LibriVox and consists of 8 languages, including about 44.5K hours of English and a total of about 6K hours for other languages. Additionally, we provide Language Models (LM) and baseline Automatic Speech Recognition (ASR) models and for all the languages in our dataset. We believe such a large transcribed dataset will open new avenues in ASR and Text-To-Speech (TTS) research. The dataset will be made freely available for anyone at http://www.openslr.org.",
    "metadata": {
      "title": "MLS: A Large-Scale Multilingual Dataset for Speech Research",
      "summary": "This paper introduces Multilingual LibriSpeech (MLS) dataset, a large multilingual corpus suitable for speech research. The dataset is derived from read audiobooks from LibriVox and consists of 8 languages, including about 44.5K hours of English and a total of about 6K hours for other languages. Additionally, we provide Language Models (LM) and baseline Automatic Speech Recognition (ASR) models and for all the languages in our dataset. We believe such a large transcribed dataset will open new avenues in ASR and Text-To-Speech (TTS) research. The dataset will be made freely available for anyone at http://www.openslr.org.",
      "abstract": "This paper introduces Multilingual LibriSpeech (MLS) dataset, a large multilingual corpus suitable for speech research. The dataset is derived from read audiobooks from LibriVox and consists of 8 languages, including about 44.5K hours of English and a total of about 6K hours for other languages. Additionally, we provide Language Models (LM) and baseline Automatic Speech Recognition (ASR) models and for all the languages in our dataset. We believe such a large transcribed dataset will open new avenues in ASR and Text-To-Speech (TTS) research. The dataset will be made freely available for anyone at http://www.openslr.org.",
      "doi": "https://doi.org/10.21437/interspeech.2020-2826",
      "openalex_id": "https://openalex.org/W3095410713",
      "arxiv_id": "",
      "publication_date": "2020-10-25",
      "published": "2020-10-25",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The MLS paper centers on a multilingual speech dataset and baseline ASR/LM resources, not on core Spoken Language Models or end-to-end speech-language modeling with interactive evaluation, thus fails the inclusion criteria and should be excluded.",
      "evaluation": 2
    },
    "round-A_JuniorNano_reasoning": "The MLS paper centers on a multilingual speech dataset and baseline ASR/LM resources, not on core Spoken Language Models or end-to-end speech-language modeling with interactive evaluation, thus fails the inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 2,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper introduces a large-scale multilingual speech dataset intended for ASR and TTS research but does not explicitly focus on Spoken Language Models or full-duplex spoken language modeling; it lacks discussion of model architecture, training, or evaluation related to SLMs or full-duplex interaction and instead concentrates on data resource provision. Therefore, it does not meet the inclusion criteria centered on SLM focus and model evaluation and falls under exclusion for focusing mainly on ASR/TTS without addressing dialogue interaction or speech language modeling concepts.",
      "evaluation": 2
    },
    "round-A_JuniorMini_reasoning": "The paper introduces a large-scale multilingual speech dataset intended for ASR and TTS research but does not explicitly focus on Spoken Language Models or full-duplex spoken language modeling; it lacks discussion of model architecture, training, or evaluation related to SLMs or full-duplex interaction and instead concentrates on data resource provision. Therefore, it does not meet the inclusion criteria centered on SLM focus and model evaluation and falls under exclusion for focusing mainly on ASR/TTS without addressing dialogue interaction or speech language modeling concepts.",
    "round-A_JuniorMini_evaluation": 2,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:2)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "The Ryerson Audio-Visual Database of Emotional Speech and Song (RAVDESS)",
    "abstract": "Description The Ryerson Audio-Visual Database of Emotional Speech and Song (RAVDESS) contains 7356 files (total size: 24.8 GB). The dataset contains 24 professional actors (12 female, 12 male), vocalizing two lexically-matched statements in a neutral North American accent. Speech includes calm, happy, sad, angry, fearful, surprise, and disgust expressions, and song contains calm, happy, sad, angry, and fearful emotions. Each expression is produced at two levels of emotional intensity (normal, strong), with an additional neutral expression. All conditions are available in three modality formats: Audio-only (16bit, 48kHz .wav), Audio-Video (720p H.264, AAC 48kHz, .mp4), and Video-only (no sound). Note, there are no song files for Actor_18. The RAVDESS was developed by Dr Steven R. Livingstone, who now leads the Affective Data Science Lab, and Dr Frank A. Russo who leads the SMART Lab. Citing the RAVDESS The RAVDESS is released under a Creative Commons Attribution license, so please cite the RAVDESS if it is used in your work in any form. Published academic papers should use the academic paper citation for our PLoS1 paper. Personal works, such as machine learning projects/blog posts, should provide a URL to this Zenodo page, though a reference to our PLoS1 paper would also be appreciated. Academic paper citation Livingstone SR, Russo FA (2018) The Ryerson Audio-Visual Database of Emotional Speech and Song (RAVDESS): A dynamic, multimodal set of facial and vocal expressions in North American English. PLoS ONE 13(5): e0196391. https://doi.org/10.1371/journal.pone.0196391. Personal use citation Include a link to this Zenodo page - https://zenodo.org/record/1188976 Commercial Licenses Commercial licenses for the RAVDESS can be purchased. For more information, please visit our license page of fees, or contact us at ravdess@gmail.com. Contact Information If you would like further information about the RAVDESS, to purchase a commercial license, or if you experience any issues downloading files, please contact us at ravdess@gmail.com. Example Videos Watch a sample of the RAVDESS speech and song videos. Emotion Classification Users If you're interested in using machine learning to classify emotional expressions with the RAVDESS, please see our new RAVDESS Facial Landmark Tracking data set [Zenodo project page]. Construction and Validation Full details on the construction and perceptual validation of the RAVDESS are described in our PLoS ONE paper - https://doi.org/10.1371/journal.pone.0196391. The RAVDESS contains 7356 files. Each file was rated 10 times on emotional validity, intensity, and genuineness. Ratings were provided by 247 individuals who were characteristic of untrained adult research participants from North America. A further set of 72 participants provided test-retest data. High levels of emotional validity, interrater reliability, and test-retest intrarater reliability were reported. Validation data is open-access, and can be downloaded along with our paper from PLoS ONE. Contents Audio-only files Audio-only files of all actors (01-24) are available as two separate zip files (~200 MB each): Speech file (Audio_Speech_Actors_01-24.zip, 215 MB) contains 1440 files: 60 trials per actor x 24 actors = 1440. Song file (Audio_Song_Actors_01-24.zip, 198 MB) contains 1012 files: 44 trials per actor x 23 actors = 1012. Audio-Visual and Video-only files Video files are provided as separate zip downloads for each actor (01-24, ~500 MB each), and are split into separate speech and song downloads: Speech files (Video_Speech_Actor_01.zip to Video_Speech_Actor_24.zip) collectively contains 2880 files: 60 trials per actor x 2 modalities (AV, VO) x 24 actors = 2880. Song files (Video_Song_Actor_01.zip to Video_Song_Actor_24.zip) collectively contains 2024 files: 44 trials per actor x 2 modalities (AV, VO) x 23 actors = 2024. File Summary In total, the RAVDESS collection includes 7356 files (2880+2024+1440+1012 files). File naming convention Each of the 7356 RAVDESS files has a unique filename. The filename consists of a 7-part numerical identifier (e.g., 02-01-06-01-02-01-12.mp4). These identifiers define the stimulus characteristics: Filename identifiers Modality (01 = full-AV, 02 = video-only, 03 = audio-only). Vocal channel (01 = speech, 02 = song). Emotion (01 = neutral, 02 = calm, 03 = happy, 04 = sad, 05 = angry, 06 = fearful, 07 = disgust, 08 = surprised). Emotional intensity (01 = normal, 02 = strong). NOTE: There is no strong intensity for the 'neutral' emotion. Statement (01 = \"Kids are talking by the door\", 02 = \"Dogs are sitting by the door\"). Repetition (01 = 1st repetition, 02 = 2nd repetition). Actor (01 to 24. Odd numbered actors are male, even numbered actors are female). Filename example: 02-01-06-01-02-01-12.mp4 Video-only (02) Speech (01) Fearful (06) Normal intensity (01) Statement \"dogs\" (02) 1st Repetition (01) 12th Actor (12) Female, as the actor ID number is even. License information The RAVDESS is released under a Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License, CC BY-NC-SA 4.0 Commercial licenses for the RAVDESS can also be purchased. For more information, please visit our license fee page, or contact us at ravdess@gmail.com. Related Data sets RAVDESS Facial Landmark Tracking data set [Zenodo project page].",
    "metadata": {
      "title": "The Ryerson Audio-Visual Database of Emotional Speech and Song (RAVDESS)",
      "summary": "Description The Ryerson Audio-Visual Database of Emotional Speech and Song (RAVDESS) contains 7356 files (total size: 24.8 GB). The dataset contains 24 professional actors (12 female, 12 male), vocalizing two lexically-matched statements in a neutral North American accent. Speech includes calm, happy, sad, angry, fearful, surprise, and disgust expressions, and song contains calm, happy, sad, angry, and fearful emotions. Each expression is produced at two levels of emotional intensity (normal, strong), with an additional neutral expression. All conditions are available in three modality formats: Audio-only (16bit, 48kHz .wav), Audio-Video (720p H.264, AAC 48kHz, .mp4), and Video-only (no sound). Note, there are no song files for Actor_18. The RAVDESS was developed by Dr Steven R. Livingstone, who now leads the Affective Data Science Lab, and Dr Frank A. Russo who leads the SMART Lab. Citing the RAVDESS The RAVDESS is released under a Creative Commons Attribution license, so please cite the RAVDESS if it is used in your work in any form. Published academic papers should use the academic paper citation for our PLoS1 paper. Personal works, such as machine learning projects/blog posts, should provide a URL to this Zenodo page, though a reference to our PLoS1 paper would also be appreciated. Academic paper citation Livingstone SR, Russo FA (2018) The Ryerson Audio-Visual Database of Emotional Speech and Song (RAVDESS): A dynamic, multimodal set of facial and vocal expressions in North American English. PLoS ONE 13(5): e0196391. https://doi.org/10.1371/journal.pone.0196391. Personal use citation Include a link to this Zenodo page - https://zenodo.org/record/1188976 Commercial Licenses Commercial licenses for the RAVDESS can be purchased. For more information, please visit our license page of fees, or contact us at ravdess@gmail.com. Contact Information If you would like further information about the RAVDESS, to purchase a commercial license, or if you experience any issues downloading files, please contact us at ravdess@gmail.com. Example Videos Watch a sample of the RAVDESS speech and song videos. Emotion Classification Users If you're interested in using machine learning to classify emotional expressions with the RAVDESS, please see our new RAVDESS Facial Landmark Tracking data set [Zenodo project page]. Construction and Validation Full details on the construction and perceptual validation of the RAVDESS are described in our PLoS ONE paper - https://doi.org/10.1371/journal.pone.0196391. The RAVDESS contains 7356 files. Each file was rated 10 times on emotional validity, intensity, and genuineness. Ratings were provided by 247 individuals who were characteristic of untrained adult research participants from North America. A further set of 72 participants provided test-retest data. High levels of emotional validity, interrater reliability, and test-retest intrarater reliability were reported. Validation data is open-access, and can be downloaded along with our paper from PLoS ONE. Contents Audio-only files Audio-only files of all actors (01-24) are available as two separate zip files (~200 MB each): Speech file (Audio_Speech_Actors_01-24.zip, 215 MB) contains 1440 files: 60 trials per actor x 24 actors = 1440. Song file (Audio_Song_Actors_01-24.zip, 198 MB) contains 1012 files: 44 trials per actor x 23 actors = 1012. Audio-Visual and Video-only files Video files are provided as separate zip downloads for each actor (01-24, ~500 MB each), and are split into separate speech and song downloads: Speech files (Video_Speech_Actor_01.zip to Video_Speech_Actor_24.zip) collectively contains 2880 files: 60 trials per actor x 2 modalities (AV, VO) x 24 actors = 2880. Song files (Video_Song_Actor_01.zip to Video_Song_Actor_24.zip) collectively contains 2024 files: 44 trials per actor x 2 modalities (AV, VO) x 23 actors = 2024. File Summary In total, the RAVDESS collection includes 7356 files (2880+2024+1440+1012 files). File naming convention Each of the 7356 RAVDESS files has a unique filename. The filename consists of a 7-part numerical identifier (e.g., 02-01-06-01-02-01-12.mp4). These identifiers define the stimulus characteristics: Filename identifiers Modality (01 = full-AV, 02 = video-only, 03 = audio-only). Vocal channel (01 = speech, 02 = song). Emotion (01 = neutral, 02 = calm, 03 = happy, 04 = sad, 05 = angry, 06 = fearful, 07 = disgust, 08 = surprised). Emotional intensity (01 = normal, 02 = strong). NOTE: There is no strong intensity for the 'neutral' emotion. Statement (01 = \"Kids are talking by the door\", 02 = \"Dogs are sitting by the door\"). Repetition (01 = 1st repetition, 02 = 2nd repetition). Actor (01 to 24. Odd numbered actors are male, even numbered actors are female). Filename example: 02-01-06-01-02-01-12.mp4 Video-only (02) Speech (01) Fearful (06) Normal intensity (01) Statement \"dogs\" (02) 1st Repetition (01) 12th Actor (12) Female, as the actor ID number is even. License information The RAVDESS is released under a Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License, CC BY-NC-SA 4.0 Commercial licenses for the RAVDESS can also be purchased. For more information, please visit our license fee page, or contact us at ravdess@gmail.com. Related Data sets RAVDESS Facial Landmark Tracking data set [Zenodo project page].",
      "abstract": "Description The Ryerson Audio-Visual Database of Emotional Speech and Song (RAVDESS) contains 7356 files (total size: 24.8 GB). The dataset contains 24 professional actors (12 female, 12 male), vocalizing two lexically-matched statements in a neutral North American accent. Speech includes calm, happy, sad, angry, fearful, surprise, and disgust expressions, and song contains calm, happy, sad, angry, and fearful emotions. Each expression is produced at two levels of emotional intensity (normal, strong), with an additional neutral expression. All conditions are available in three modality formats: Audio-only (16bit, 48kHz .wav), Audio-Video (720p H.264, AAC 48kHz, .mp4), and Video-only (no sound). Note, there are no song files for Actor_18. The RAVDESS was developed by Dr Steven R. Livingstone, who now leads the Affective Data Science Lab, and Dr Frank A. Russo who leads the SMART Lab. Citing the RAVDESS The RAVDESS is released under a Creative Commons Attribution license, so please cite the RAVDESS if it is used in your work in any form. Published academic papers should use the academic paper citation for our PLoS1 paper. Personal works, such as machine learning projects/blog posts, should provide a URL to this Zenodo page, though a reference to our PLoS1 paper would also be appreciated. Academic paper citation Livingstone SR, Russo FA (2018) The Ryerson Audio-Visual Database of Emotional Speech and Song (RAVDESS): A dynamic, multimodal set of facial and vocal expressions in North American English. PLoS ONE 13(5): e0196391. https://doi.org/10.1371/journal.pone.0196391. Personal use citation Include a link to this Zenodo page - https://zenodo.org/record/1188976 Commercial Licenses Commercial licenses for the RAVDESS can be purchased. For more information, please visit our license page of fees, or contact us at ravdess@gmail.com. Contact Information If you would like further information about the RAVDESS, to purchase a commercial license, or if you experience any issues downloading files, please contact us at ravdess@gmail.com. Example Videos Watch a sample of the RAVDESS speech and song videos. Emotion Classification Users If you're interested in using machine learning to classify emotional expressions with the RAVDESS, please see our new RAVDESS Facial Landmark Tracking data set [Zenodo project page]. Construction and Validation Full details on the construction and perceptual validation of the RAVDESS are described in our PLoS ONE paper - https://doi.org/10.1371/journal.pone.0196391. The RAVDESS contains 7356 files. Each file was rated 10 times on emotional validity, intensity, and genuineness. Ratings were provided by 247 individuals who were characteristic of untrained adult research participants from North America. A further set of 72 participants provided test-retest data. High levels of emotional validity, interrater reliability, and test-retest intrarater reliability were reported. Validation data is open-access, and can be downloaded along with our paper from PLoS ONE. Contents Audio-only files Audio-only files of all actors (01-24) are available as two separate zip files (~200 MB each): Speech file (Audio_Speech_Actors_01-24.zip, 215 MB) contains 1440 files: 60 trials per actor x 24 actors = 1440. Song file (Audio_Song_Actors_01-24.zip, 198 MB) contains 1012 files: 44 trials per actor x 23 actors = 1012. Audio-Visual and Video-only files Video files are provided as separate zip downloads for each actor (01-24, ~500 MB each), and are split into separate speech and song downloads: Speech files (Video_Speech_Actor_01.zip to Video_Speech_Actor_24.zip) collectively contains 2880 files: 60 trials per actor x 2 modalities (AV, VO) x 24 actors = 2880. Song files (Video_Song_Actor_01.zip to Video_Song_Actor_24.zip) collectively contains 2024 files: 44 trials per actor x 2 modalities (AV, VO) x 23 actors = 2024. File Summary In total, the RAVDESS collection includes 7356 files (2880+2024+1440+1012 files). File naming convention Each of the 7356 RAVDESS files has a unique filename. The filename consists of a 7-part numerical identifier (e.g., 02-01-06-01-02-01-12.mp4). These identifiers define the stimulus characteristics: Filename identifiers Modality (01 = full-AV, 02 = video-only, 03 = audio-only). Vocal channel (01 = speech, 02 = song). Emotion (01 = neutral, 02 = calm, 03 = happy, 04 = sad, 05 = angry, 06 = fearful, 07 = disgust, 08 = surprised). Emotional intensity (01 = normal, 02 = strong). NOTE: There is no strong intensity for the 'neutral' emotion. Statement (01 = \"Kids are talking by the door\", 02 = \"Dogs are sitting by the door\"). Repetition (01 = 1st repetition, 02 = 2nd repetition). Actor (01 to 24. Odd numbered actors are male, even numbered actors are female). Filename example: 02-01-06-01-02-01-12.mp4 Video-only (02) Speech (01) Fearful (06) Normal intensity (01) Statement \"dogs\" (02) 1st Repetition (01) 12th Actor (12) Female, as the actor ID number is even. License information The RAVDESS is released under a Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License, CC BY-NC-SA 4.0 Commercial licenses for the RAVDESS can also be purchased. For more information, please visit our license fee page, or contact us at ravdess@gmail.com. Related Data sets RAVDESS Facial Landmark Tracking data set [Zenodo project page].",
      "doi": "https://doi.org/10.5281/zenodo.1188976",
      "openalex_id": "https://openalex.org/W3081192838",
      "arxiv_id": "",
      "publication_date": "2018-04-05",
      "published": "2018-04-05",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This item is a dataset description (RAVDESS) focused on emotional speech and song data, not on spoken language models or full-duplex speech modeling, and it does not present model architectures, training, or evaluation frameworks within the last 3 years, so it should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "This item is a dataset description (RAVDESS) focused on emotional speech and song data, not on spoken language models or full-duplex speech modeling, and it does not present model architectures, training, or evaluation frameworks within the last 3 years, so it should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The provided item is a detailed description of the Ryerson Audio-Visual Database of Emotional Speech and Song (RAVDESS), which is a multimedia dataset comprising emotional speech and song files. However, it does not present research focused on Spoken Language Models (SLMs) or Full-Duplex Spoken Language Models (FD-SLMs), nor does it discuss model architectures, training, inference, or evaluations related to speech language modeling as defined by the inclusion criteria. Instead, it serves as a resource or dataset for potential research but lacks direct coverage of speech language modeling, synchronous interactive dialogue systems, or related evaluation metrics required by the criteria.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The provided item is a detailed description of the Ryerson Audio-Visual Database of Emotional Speech and Song (RAVDESS), which is a multimedia dataset comprising emotional speech and song files. However, it does not present research focused on Spoken Language Models (SLMs) or Full-Duplex Spoken Language Models (FD-SLMs), nor does it discuss model architectures, training, inference, or evaluations related to speech language modeling as defined by the inclusion criteria. Instead, it serves as a resource or dataset for potential research but lacks direct coverage of speech language modeling, synchronous interactive dialogue systems, or related evaluation metrics required by the criteria.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Low Bit-rate Speech Coding with VQ-VAE and a WaveNet Decoder",
    "abstract": "In order to efficiently transmit and store speech signals, speech codecs create a minimally redundant representation of the input signal which is then decoded at the receiver with the best possible perceptual quality. In this work we demonstrate that a neural network architecture based on VQ-VAE with a WaveNet decoder can be used to perform very low bit-rate speech coding with high reconstruction quality. A prosody-transparent and speaker-independent model trained on the LibriSpeech corpus coding audio at 1.6 kbps exhibits perceptual quality which is around halfway between the MELP codec at 2.4 kbps and AMR-WB codec at 23.05 kbps. In addition, when training on high-quality recorded speech with the test speaker included in the training set, a model coding speech at 1.6 kbps produces output of similar perceptual quality to that generated by AMR-WB at 23.05 kbps.",
    "metadata": {
      "title": "Low Bit-rate Speech Coding with VQ-VAE and a WaveNet Decoder",
      "summary": "In order to efficiently transmit and store speech signals, speech codecs create a minimally redundant representation of the input signal which is then decoded at the receiver with the best possible perceptual quality. In this work we demonstrate that a neural network architecture based on VQ-VAE with a WaveNet decoder can be used to perform very low bit-rate speech coding with high reconstruction quality. A prosody-transparent and speaker-independent model trained on the LibriSpeech corpus coding audio at 1.6 kbps exhibits perceptual quality which is around halfway between the MELP codec at 2.4 kbps and AMR-WB codec at 23.05 kbps. In addition, when training on high-quality recorded speech with the test speaker included in the training set, a model coding speech at 1.6 kbps produces output of similar perceptual quality to that generated by AMR-WB at 23.05 kbps.",
      "abstract": "In order to efficiently transmit and store speech signals, speech codecs create a minimally redundant representation of the input signal which is then decoded at the receiver with the best possible perceptual quality. In this work we demonstrate that a neural network architecture based on VQ-VAE with a WaveNet decoder can be used to perform very low bit-rate speech coding with high reconstruction quality. A prosody-transparent and speaker-independent model trained on the LibriSpeech corpus coding audio at 1.6 kbps exhibits perceptual quality which is around halfway between the MELP codec at 2.4 kbps and AMR-WB codec at 23.05 kbps. In addition, when training on high-quality recorded speech with the test speaker included in the training set, a model coding speech at 1.6 kbps produces output of similar perceptual quality to that generated by AMR-WB at 23.05 kbps.",
      "doi": "https://doi.org/10.1109/icassp.2019.8683277",
      "openalex_id": "https://openalex.org/W2935711438",
      "arxiv_id": "",
      "publication_date": "2019-04-16",
      "published": "2019-04-16",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item centers on neural speech coding techniques (VQ-VAE + WaveNet decoder) for low-bit-rate transmission, not on Spoken Language Models or Speech Language Models, nor on full-duplex interactive speech modeling; it lacks SLM/FD-SLM focus and appropriate evaluation/framework as per criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item centers on neural speech coding techniques (VQ-VAE + WaveNet decoder) for low-bit-rate transmission, not on Spoken Language Models or Speech Language Models, nor on full-duplex interactive speech modeling; it lacks SLM/FD-SLM focus and appropriate evaluation/framework as per criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on low bit-rate speech coding using neural networks to reconstruct speech signals, with no mention of spoken language modeling, full-duplex interaction, synchronization mechanisms, or dialogue behavior arbitration, which are core requirements in the inclusion criteria. It also does not discuss model architectures concerning spoken language models, nor does it address real-time interaction or overlapping speech management as per the inclusion criteria. The paper is about speech compression and reconstruction quality, not about speech language models or full-duplex spoken language modeling systems, so it falls outside the defined scope.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on low bit-rate speech coding using neural networks to reconstruct speech signals, with no mention of spoken language modeling, full-duplex interaction, synchronization mechanisms, or dialogue behavior arbitration, which are core requirements in the inclusion criteria. It also does not discuss model architectures concerning spoken language models, nor does it address real-time interaction or overlapping speech management as per the inclusion criteria. The paper is about speech compression and reconstruction quality, not about speech language models or full-duplex spoken language modeling systems, so it falls outside the defined scope.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Reducing F0 Frame Error of F0 tracking algorithms under noisy conditions with an unvoiced/voiced classification frontend",
    "abstract": "In this paper, we propose an F0 Frame Error (FFE) metric which combines Gross Pitch Error (GPE) and Voicing Decision Error (VDE) to objectively evaluate the performance of fundamental frequency (F0) tracking methods. A GPE-VDE curve is then developed to show the trade-off between GPE and VDE. In addition, we introduce a model-based Unvoiced/Voiced (U/V) classification frontend which can be used by any F0 tracking algorithm. In the U/V classification, we train speaker independent U/V models, and then adapt them to speaker dependent models in an unsupervised fashion. The U/V classification result is taken as a mask for F0 tracking. Experiments using the KEELE corpus with additive noise show that our statistically-based U/V classifier can reduce VDE and FFE for the pitch tracker TEMPO in both white and babble noise conditions, and that minimizing FFE instead of VDE results in a reduction in error rates for a number of F0 tracking algorithms, especially in babble noise.",
    "metadata": {
      "title": "Reducing F0 Frame Error of F0 tracking algorithms under noisy conditions with an unvoiced/voiced classification frontend",
      "summary": "In this paper, we propose an F0 Frame Error (FFE) metric which combines Gross Pitch Error (GPE) and Voicing Decision Error (VDE) to objectively evaluate the performance of fundamental frequency (F0) tracking methods. A GPE-VDE curve is then developed to show the trade-off between GPE and VDE. In addition, we introduce a model-based Unvoiced/Voiced (U/V) classification frontend which can be used by any F0 tracking algorithm. In the U/V classification, we train speaker independent U/V models, and then adapt them to speaker dependent models in an unsupervised fashion. The U/V classification result is taken as a mask for F0 tracking. Experiments using the KEELE corpus with additive noise show that our statistically-based U/V classifier can reduce VDE and FFE for the pitch tracker TEMPO in both white and babble noise conditions, and that minimizing FFE instead of VDE results in a reduction in error rates for a number of F0 tracking algorithms, especially in babble noise.",
      "abstract": "In this paper, we propose an F0 Frame Error (FFE) metric which combines Gross Pitch Error (GPE) and Voicing Decision Error (VDE) to objectively evaluate the performance of fundamental frequency (F0) tracking methods. A GPE-VDE curve is then developed to show the trade-off between GPE and VDE. In addition, we introduce a model-based Unvoiced/Voiced (U/V) classification frontend which can be used by any F0 tracking algorithm. In the U/V classification, we train speaker independent U/V models, and then adapt them to speaker dependent models in an unsupervised fashion. The U/V classification result is taken as a mask for F0 tracking. Experiments using the KEELE corpus with additive noise show that our statistically-based U/V classifier can reduce VDE and FFE for the pitch tracker TEMPO in both white and babble noise conditions, and that minimizing FFE instead of VDE results in a reduction in error rates for a number of F0 tracking algorithms, especially in babble noise.",
      "doi": "https://doi.org/10.1109/icassp.2009.4960497",
      "openalex_id": "https://openalex.org/W2107740512",
      "arxiv_id": "",
      "publication_date": "2009-04-01",
      "published": "2009-04-01",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This paper centers on F0 tracking and unvoiced/voiced classification for pitch tracking rather than on Spoken Language Models or speech-language modeling; it lacks SLM/FD-SLM focus and does not meet the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "This paper centers on F0 tracking and unvoiced/voiced classification for pitch tracking rather than on Spoken Language Models or speech-language modeling; it lacks SLM/FD-SLM focus and does not meet the inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper focuses on fundamental frequency (F0) tracking algorithms and improvements in noisy environments, which relates to acoustic signal processing rather than spoken language modeling or full-duplex spoken language models. It does not mention spoken language models, language modeling of speech, or related architectural, training, or evaluation aspects of SLMs or FD-SLMs. Additionally, the study evaluates pitch tracking metrics and noise robustness rather than dialogue interaction, full-duplex communication, or real-time interactive behavior arbitration. Therefore, it does not meet the inclusion criteria centered on spoken language models or full-duplex interactive speech modeling, nor does it specify relevant evaluation metrics for synchronous interaction or complex language modeling. Hence, this paper should be excluded based on the provided criteria.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper focuses on fundamental frequency (F0) tracking algorithms and improvements in noisy environments, which relates to acoustic signal processing rather than spoken language modeling or full-duplex spoken language models. It does not mention spoken language models, language modeling of speech, or related architectural, training, or evaluation aspects of SLMs or FD-SLMs. Additionally, the study evaluates pitch tracking metrics and noise robustness rather than dialogue interaction, full-duplex communication, or real-time interactive behavior arbitration. Therefore, it does not meet the inclusion criteria centered on spoken language models or full-duplex interactive speech modeling, nor does it specify relevant evaluation metrics for synchronous interaction or complex language modeling. Hence, this paper should be excluded based on the provided criteria.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "UTMOS: UTokyo-SaruLab System for VoiceMOS Challenge 2022",
    "abstract": "We present the UTokyo-SaruLab mean opinion score (MOS) prediction system submitted to VoiceMOS Challenge 2022.The challenge is to predict the MOS values of speech samples collected from previous Blizzard Challenges and Voice Conversion Challenges for two tracks: a main track for in-domain prediction and an out-of-domain (OOD) track for which there is less labeled data from different listening tests.Our system is based on ensemble learning of strong and weak learners.Strong learners incorporate several improvements to the previous finetuning models of self-supervised learning (SSL) models, while weak learners use basic machine-learning methods to predict scores from SSL features.In the Challenge, our system had the highest score on several metrics for both the main and OOD tracks.In addition, we conducted ablation studies to investigate the effectiveness of our proposed methods.",
    "metadata": {
      "title": "UTMOS: UTokyo-SaruLab System for VoiceMOS Challenge 2022",
      "summary": "We present the UTokyo-SaruLab mean opinion score (MOS) prediction system submitted to VoiceMOS Challenge 2022.The challenge is to predict the MOS values of speech samples collected from previous Blizzard Challenges and Voice Conversion Challenges for two tracks: a main track for in-domain prediction and an out-of-domain (OOD) track for which there is less labeled data from different listening tests.Our system is based on ensemble learning of strong and weak learners.Strong learners incorporate several improvements to the previous finetuning models of self-supervised learning (SSL) models, while weak learners use basic machine-learning methods to predict scores from SSL features.In the Challenge, our system had the highest score on several metrics for both the main and OOD tracks.In addition, we conducted ablation studies to investigate the effectiveness of our proposed methods.",
      "abstract": "We present the UTokyo-SaruLab mean opinion score (MOS) prediction system submitted to VoiceMOS Challenge 2022.The challenge is to predict the MOS values of speech samples collected from previous Blizzard Challenges and Voice Conversion Challenges for two tracks: a main track for in-domain prediction and an out-of-domain (OOD) track for which there is less labeled data from different listening tests.Our system is based on ensemble learning of strong and weak learners.Strong learners incorporate several improvements to the previous finetuning models of self-supervised learning (SSL) models, while weak learners use basic machine-learning methods to predict scores from SSL features.In the Challenge, our system had the highest score on several metrics for both the main and OOD tracks.In addition, we conducted ablation studies to investigate the effectiveness of our proposed methods.",
      "doi": "https://doi.org/10.21437/interspeech.2022-439",
      "openalex_id": "https://openalex.org/W4225956675",
      "arxiv_id": "",
      "publication_date": "2022-09-16",
      "published": "2022-09-16",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The paper focuses on predicting mean opinion scores (MOS) for speech samples and ensemble methods, which is a speech evaluation task rather than core Spoken Language Modeling (SLM) or speech-language modeling (typical SLM/FD-SLM concerns include end-to-end speech modeling, real-time interaction, and synchronization). It does not present or discuss an SLM architecture trained for speech-language understanding/production, nor does it center on speech-language modeling benchmarks; thus it does not meet the stated inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The paper focuses on predicting mean opinion scores (MOS) for speech samples and ensemble methods, which is a speech evaluation task rather than core Spoken Language Modeling (SLM) or speech-language modeling (typical SLM/FD-SLM concerns include end-to-end speech modeling, real-time interaction, and synchronization). It does not present or discuss an SLM architecture trained for speech-language understanding/production, nor does it center on speech-language modeling benchmarks; thus it does not meet the stated inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The presented system focuses on MOS prediction for speech samples using ensemble learning and self-supervised learning models, but does not explicitly address Spoken Language Models or Full-Duplex Spoken Language Models, nor does it discuss model architecture or evaluation in the context of speech language modeling or real-time interactive dialogue behavior arbitration; it mainly targets quality prediction of speech samples without involving synchronous interaction, cognitive parallelism, or related evaluation metrics required by the inclusion criteria.",
      "evaluation": 2
    },
    "round-A_JuniorMini_reasoning": "The presented system focuses on MOS prediction for speech samples using ensemble learning and self-supervised learning models, but does not explicitly address Spoken Language Models or Full-Duplex Spoken Language Models, nor does it discuss model architecture or evaluation in the context of speech language modeling or real-time interactive dialogue behavior arbitration; it mainly targets quality prediction of speech samples without involving synchronous interaction, cognitive parallelism, or related evaluation metrics required by the inclusion criteria.",
    "round-A_JuniorMini_evaluation": 2,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:2)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "ViSQOL v3: An Open Source Production Ready Objective Speech and Audio Metric",
    "abstract": "The 12th International Conference on Quality of Multimedia Experience (QoMEX), Athlone, Ireland (held online due to coronavirus outbreak), 26-28 May 2020",
    "metadata": {
      "title": "ViSQOL v3: An Open Source Production Ready Objective Speech and Audio Metric",
      "summary": "The 12th International Conference on Quality of Multimedia Experience (QoMEX), Athlone, Ireland (held online due to coronavirus outbreak), 26-28 May 2020",
      "abstract": "The 12th International Conference on Quality of Multimedia Experience (QoMEX), Athlone, Ireland (held online due to coronavirus outbreak), 26-28 May 2020",
      "doi": "https://doi.org/10.1109/qomex48832.2020.9123150",
      "openalex_id": "https://openalex.org/W3037038648",
      "arxiv_id": "",
      "publication_date": "2020-05-01",
      "published": "2020-05-01",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This item is a QoMEX 2020 paper about ViSQOL v3, an objective speech/audio quality metric, not a Spoken Language Model or FD-SLM, and it predates the 3-year window, so it does not meet the inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "This item is a QoMEX 2020 paper about ViSQOL v3, an objective speech/audio quality metric, not a Spoken Language Model or FD-SLM, and it predates the 3-year window, so it does not meet the inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The title and abstract do not explicitly mention spoken language models or speech language models, nor do they discuss model architecture, training, or evaluation related to speech modeling; additionally, the abstract only indicates a conference presentation without research content details and is from 2020, outside the last 3 years inclusion window.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The title and abstract do not explicitly mention spoken language models or speech language models, nor do they discuss model architecture, training, or evaluation related to speech modeling; additionally, the abstract only indicates a conference presentation without research content details and is from 2020, outside the last 3 years inclusion window.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "LibriTTS: A Corpus Derived from LibriSpeech for Text-to-Speech",
    "abstract": "This paper introduces a new speech corpus called \"LibriTTS\" designed for text-to-speech use.It is derived from the original audio and text materials of the LibriSpeech corpus, which has been used for training and evaluating automatic speech recognition systems.The new corpus inherits desired properties of the LibriSpeech corpus while addressing a number of issues which make LibriSpeech less than ideal for text-to-speech work.The released corpus consists of 585 hours of speech data at 24kHz sampling rate from 2,456 speakers and the corresponding texts.Experimental results show that neural end-to-end TTS models trained from the LibriTTS corpus achieved above 4.0 in mean opinion scores in naturalness in five out of six evaluation speakers.",
    "metadata": {
      "title": "LibriTTS: A Corpus Derived from LibriSpeech for Text-to-Speech",
      "summary": "This paper introduces a new speech corpus called \"LibriTTS\" designed for text-to-speech use.It is derived from the original audio and text materials of the LibriSpeech corpus, which has been used for training and evaluating automatic speech recognition systems.The new corpus inherits desired properties of the LibriSpeech corpus while addressing a number of issues which make LibriSpeech less than ideal for text-to-speech work.The released corpus consists of 585 hours of speech data at 24kHz sampling rate from 2,456 speakers and the corresponding texts.Experimental results show that neural end-to-end TTS models trained from the LibriTTS corpus achieved above 4.0 in mean opinion scores in naturalness in five out of six evaluation speakers.",
      "abstract": "This paper introduces a new speech corpus called \"LibriTTS\" designed for text-to-speech use.It is derived from the original audio and text materials of the LibriSpeech corpus, which has been used for training and evaluating automatic speech recognition systems.The new corpus inherits desired properties of the LibriSpeech corpus while addressing a number of issues which make LibriSpeech less than ideal for text-to-speech work.The released corpus consists of 585 hours of speech data at 24kHz sampling rate from 2,456 speakers and the corresponding texts.Experimental results show that neural end-to-end TTS models trained from the LibriTTS corpus achieved above 4.0 in mean opinion scores in naturalness in five out of six evaluation speakers.",
      "doi": "https://doi.org/10.21437/interspeech.2019-2441",
      "openalex_id": "https://openalex.org/W2972359262",
      "arxiv_id": "",
      "publication_date": "2019-09-13",
      "published": "2019-09-13",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item describes a LibriTTS corpus for text-to-speech, acting as a data/resource rather than foregrounding Spoken Language Models or speech-language modeling; it does not address interactive speech modeling or evaluation beyond TTS quality, thus it fails inclusion and aligns with exclusion (single-task TTS/data resource).",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item describes a LibriTTS corpus for text-to-speech, acting as a data/resource rather than foregrounding Spoken Language Models or speech-language modeling; it does not address interactive speech modeling or evaluation beyond TTS quality, thus it fails inclusion and aligns with exclusion (single-task TTS/data resource).",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study presents a speech corpus designed for text-to-speech (TTS) tasks, which focuses on single speech technology (TTS) rather than spoken language modeling or full-duplex interactive systems; it lacks discussion on spoken language models, synchronization, or interactive multi-dimensional evaluation required by the inclusion criteria, and mainly evaluates TTS naturalness without addressing interaction or behavior arbitration, therefore it does not meet the thematic or methodological inclusion requirements.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study presents a speech corpus designed for text-to-speech (TTS) tasks, which focuses on single speech technology (TTS) rather than spoken language modeling or full-duplex interactive systems; it lacks discussion on spoken language models, synchronization, or interactive multi-dimensional evaluation required by the inclusion criteria, and mainly evaluates TTS naturalness without addressing interaction or behavior arbitration, therefore it does not meet the thematic or methodological inclusion requirements.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Video-LLaMA: An Instruction-tuned Audio-Visual Language Model for Video Understanding",
    "abstract": "We present Video-LLaMA, a multi-modal framework that empowers Large Language Models (LLMs) with the capability of understanding both visual and auditory content in the video. Video-LLaMA bootstraps cross-modal training from the frozen pre-trained visual & audio encoders and the frozen LLMs. Unlike previous works that complement LLMs to process the visual or audio signals only, Video-LLaMA enables video comprehension by tackling two challenges: (1) capturing the temporal changes in visual scenes, (2) integrating audio-visual signals. To counter the first challenge, we propose a Video Q-former to assemble a pre-trained image encoder into our video encoder and introduce a video-to-text generation task to learn video-language correspondence. For the second challenge, we leverage ImageBind, a universal embedding model aligning multiple modalities, as the pre-trained audio encoder and introduce an Audio Q-former on top of ImageBind to learn reasonable auditory query embeddings for the LLM module. To align the output of both visual & audio encoders with LLM’s embedding space, we first train Video-LLaMA on massive video/image-caption pairs and then tune our model with visual-instruction datasets of moderate amount but higher quality. We found Video-LLaMA shows the ability to perceive and comprehend video content and generate meaningful responses grounded in the visual and auditory information presented in the videos.",
    "metadata": {
      "title": "Video-LLaMA: An Instruction-tuned Audio-Visual Language Model for Video Understanding",
      "summary": "We present Video-LLaMA, a multi-modal framework that empowers Large Language Models (LLMs) with the capability of understanding both visual and auditory content in the video. Video-LLaMA bootstraps cross-modal training from the frozen pre-trained visual & audio encoders and the frozen LLMs. Unlike previous works that complement LLMs to process the visual or audio signals only, Video-LLaMA enables video comprehension by tackling two challenges: (1) capturing the temporal changes in visual scenes, (2) integrating audio-visual signals. To counter the first challenge, we propose a Video Q-former to assemble a pre-trained image encoder into our video encoder and introduce a video-to-text generation task to learn video-language correspondence. For the second challenge, we leverage ImageBind, a universal embedding model aligning multiple modalities, as the pre-trained audio encoder and introduce an Audio Q-former on top of ImageBind to learn reasonable auditory query embeddings for the LLM module. To align the output of both visual & audio encoders with LLM’s embedding space, we first train Video-LLaMA on massive video/image-caption pairs and then tune our model with visual-instruction datasets of moderate amount but higher quality. We found Video-LLaMA shows the ability to perceive and comprehend video content and generate meaningful responses grounded in the visual and auditory information presented in the videos.",
      "abstract": "We present Video-LLaMA, a multi-modal framework that empowers Large Language Models (LLMs) with the capability of understanding both visual and auditory content in the video. Video-LLaMA bootstraps cross-modal training from the frozen pre-trained visual & audio encoders and the frozen LLMs. Unlike previous works that complement LLMs to process the visual or audio signals only, Video-LLaMA enables video comprehension by tackling two challenges: (1) capturing the temporal changes in visual scenes, (2) integrating audio-visual signals. To counter the first challenge, we propose a Video Q-former to assemble a pre-trained image encoder into our video encoder and introduce a video-to-text generation task to learn video-language correspondence. For the second challenge, we leverage ImageBind, a universal embedding model aligning multiple modalities, as the pre-trained audio encoder and introduce an Audio Q-former on top of ImageBind to learn reasonable auditory query embeddings for the LLM module. To align the output of both visual & audio encoders with LLM’s embedding space, we first train Video-LLaMA on massive video/image-caption pairs and then tune our model with visual-instruction datasets of moderate amount but higher quality. We found Video-LLaMA shows the ability to perceive and comprehend video content and generate meaningful responses grounded in the visual and auditory information presented in the videos.",
      "doi": "https://doi.org/10.18653/v1/2023.emnlp-demo.49",
      "openalex_id": "https://openalex.org/W4389519587",
      "arxiv_id": "",
      "publication_date": "2023-01-01",
      "published": "2023-01-01",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This paper centers on an audio-visual video understanding model and uses audio as input, but it does not treat Spoken Language Models or speech-language modeling as the core object of study, lacking emphasis on turn-taking, synchronization, or audio-language modeling evaluations.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "This paper centers on an audio-visual video understanding model and uses audio as input, but it does not treat Spoken Language Models or speech-language modeling as the core object of study, lacking emphasis on turn-taking, synchronization, or audio-language modeling evaluations.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The article describes Video-LLaMA, a multi-modal model for video understanding that integrates visual and audio content but does not focus on spoken language modeling or treat speech as the core modeling subject; it lacks discussion on full-duplex spoken language models, speech language modeling, synchronous spoken interaction, or related evaluations, which are essential inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The article describes Video-LLaMA, a multi-modal model for video understanding that integrates visual and audio content but does not focus on spoken language modeling or treat speech as the core modeling subject; it lacks discussion on full-duplex spoken language models, speech language modeling, synchronous spoken interaction, or related evaluations, which are essential inclusion criteria.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "CLAP Learning Audio Concepts from Natural Language Supervision",
    "abstract": "Mainstream machine listening models are trained to learn audio concepts under the paradigm of one class label to many recordings focusing on one task. Learning under such restricted supervision limits the flexibility of models because they require labeled audio for training and can only predict the predefined categories. Instead, we propose to learn audio concepts from natural language supervision. We call our approach Contrastive Language-Audio Pretraining (CLAP), which connects language and audio by using two encoders and a contrastive learning objective, bringing audio and text descriptions into a joint multimodal space. We trained CLAP with 128k audio and text pairs and evaluated it on 16 downstream tasks across 7 domains, such as classification of sound events, scenes, music, and speech. CLAP establishes state-of-the-art (SoTA) in Zero-Shot performance. Also, we evaluated CLAP's audio encoder in a supervised learning setup and achieved SoTA in 5 tasks. The Zero-Shot capability removes the need of training with class labeled audio, enables flexible class prediction at inference time, and generalizes well in multiple downstream tasks. Code is available at: https://github.com/microsoft/CLAP.",
    "metadata": {
      "title": "CLAP Learning Audio Concepts from Natural Language Supervision",
      "summary": "Mainstream machine listening models are trained to learn audio concepts under the paradigm of one class label to many recordings focusing on one task. Learning under such restricted supervision limits the flexibility of models because they require labeled audio for training and can only predict the predefined categories. Instead, we propose to learn audio concepts from natural language supervision. We call our approach Contrastive Language-Audio Pretraining (CLAP), which connects language and audio by using two encoders and a contrastive learning objective, bringing audio and text descriptions into a joint multimodal space. We trained CLAP with 128k audio and text pairs and evaluated it on 16 downstream tasks across 7 domains, such as classification of sound events, scenes, music, and speech. CLAP establishes state-of-the-art (SoTA) in Zero-Shot performance. Also, we evaluated CLAP's audio encoder in a supervised learning setup and achieved SoTA in 5 tasks. The Zero-Shot capability removes the need of training with class labeled audio, enables flexible class prediction at inference time, and generalizes well in multiple downstream tasks. Code is available at: https://github.com/microsoft/CLAP.",
      "abstract": "Mainstream machine listening models are trained to learn audio concepts under the paradigm of one class label to many recordings focusing on one task. Learning under such restricted supervision limits the flexibility of models because they require labeled audio for training and can only predict the predefined categories. Instead, we propose to learn audio concepts from natural language supervision. We call our approach Contrastive Language-Audio Pretraining (CLAP), which connects language and audio by using two encoders and a contrastive learning objective, bringing audio and text descriptions into a joint multimodal space. We trained CLAP with 128k audio and text pairs and evaluated it on 16 downstream tasks across 7 domains, such as classification of sound events, scenes, music, and speech. CLAP establishes state-of-the-art (SoTA) in Zero-Shot performance. Also, we evaluated CLAP's audio encoder in a supervised learning setup and achieved SoTA in 5 tasks. The Zero-Shot capability removes the need of training with class labeled audio, enables flexible class prediction at inference time, and generalizes well in multiple downstream tasks. Code is available at: https://github.com/microsoft/CLAP.",
      "doi": "https://doi.org/10.1109/icassp49357.2023.10095889",
      "openalex_id": "https://openalex.org/W4372266552",
      "arxiv_id": "",
      "publication_date": "2023-05-05",
      "published": "2023-05-05",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The abstract presents cross-modal audio–text pretraining (CLAP) with audio as the primary modality and broad downstream evaluation, but it does not explicitly frame or label the work as a Spoken Language Model or Speech Language Model in the title/abstract/keywords, which is a required inclusion criterion.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The abstract presents cross-modal audio–text pretraining (CLAP) with audio as the primary modality and broad downstream evaluation, but it does not explicitly frame or label the work as a Spoken Language Model or Speech Language Model in the title/abstract/keywords, which is a required inclusion criterion.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on learning audio concepts from natural language supervision without explicit emphasis on spoken language models (SLMs) or full-duplex spoken language interaction, nor does it mention modeling or evaluation related to turn-taking, low latency, overlapping speech, or real-time dialogue arbitration which are central to the inclusion criteria. It primarily addresses audio classification tasks rather than speech-based language modeling or synchronous speech interaction as required.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on learning audio concepts from natural language supervision without explicit emphasis on spoken language models (SLMs) or full-duplex spoken language interaction, nor does it mention modeling or evaluation related to turn-taking, low latency, overlapping speech, or real-time dialogue arbitration which are central to the inclusion criteria. It primarily addresses audio classification tasks rather than speech-based language modeling or synchronous speech interaction as required.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Conformer-Based Self-Supervised Learning For Non-Speech Audio Tasks",
    "abstract": "Representation learning from unlabeled data has been of major interest in artificial intelligence research. While self-supervised speech representation learning has been popular in the speech research community, very few works have comprehensively analyzed audio representation learning for non-speech audio tasks. In this paper, we propose a self-supervised audio representation learning method and apply it to a variety of downstream non-speech audio tasks. We combine the well-known wav2vec 2.0 framework, which has shown success in self-supervised learning for speech tasks, with parameter-efficient conformer architectures. Our self-supervised pre-training can reduce the need for labeled data by two-thirds. On the AudioSet benchmark, we achieve a mean average precision (mAP) score of 0.415, which is a new state-of-the-art on this dataset through audio-only self-supervised learning. Our fine-tuned conformers also surpass or match the performance of previous systems pre-trained in a supervised way on several downstream tasks. We further discuss the important design considerations for both pre-training and fine-tuning.",
    "metadata": {
      "title": "Conformer-Based Self-Supervised Learning For Non-Speech Audio Tasks",
      "summary": "Representation learning from unlabeled data has been of major interest in artificial intelligence research. While self-supervised speech representation learning has been popular in the speech research community, very few works have comprehensively analyzed audio representation learning for non-speech audio tasks. In this paper, we propose a self-supervised audio representation learning method and apply it to a variety of downstream non-speech audio tasks. We combine the well-known wav2vec 2.0 framework, which has shown success in self-supervised learning for speech tasks, with parameter-efficient conformer architectures. Our self-supervised pre-training can reduce the need for labeled data by two-thirds. On the AudioSet benchmark, we achieve a mean average precision (mAP) score of 0.415, which is a new state-of-the-art on this dataset through audio-only self-supervised learning. Our fine-tuned conformers also surpass or match the performance of previous systems pre-trained in a supervised way on several downstream tasks. We further discuss the important design considerations for both pre-training and fine-tuning.",
      "abstract": "Representation learning from unlabeled data has been of major interest in artificial intelligence research. While self-supervised speech representation learning has been popular in the speech research community, very few works have comprehensively analyzed audio representation learning for non-speech audio tasks. In this paper, we propose a self-supervised audio representation learning method and apply it to a variety of downstream non-speech audio tasks. We combine the well-known wav2vec 2.0 framework, which has shown success in self-supervised learning for speech tasks, with parameter-efficient conformer architectures. Our self-supervised pre-training can reduce the need for labeled data by two-thirds. On the AudioSet benchmark, we achieve a mean average precision (mAP) score of 0.415, which is a new state-of-the-art on this dataset through audio-only self-supervised learning. Our fine-tuned conformers also surpass or match the performance of previous systems pre-trained in a supervised way on several downstream tasks. We further discuss the important design considerations for both pre-training and fine-tuning.",
      "doi": "https://doi.org/10.1109/icassp43922.2022.9746490",
      "openalex_id": "https://openalex.org/W3205550549",
      "arxiv_id": "",
      "publication_date": "2022-04-27",
      "published": "2022-04-27",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This paper focuses on non-speech audio tasks and does not center on Spoken Language Models or Speech Language Models as the core object, so it fails the inclusion criteria requiring speech-centric modeling and evaluation.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "This paper focuses on non-speech audio tasks and does not center on Spoken Language Models or Speech Language Models as the core object, so it fails the inclusion criteria requiring speech-centric modeling and evaluation.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on self-supervised learning for non-speech audio tasks rather than on spoken language models or speech language modeling; it does not discuss full-duplex spoken interaction, cognitive parallelism, or related advanced speech modeling aspects required by the inclusion criteria, and the keywords such as 'spoken language model(s)' or equivalent are absent from the title and abstract. Therefore, it does not meet the topic definition centered on spoken language models and their synchronization or interaction capabilities.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on self-supervised learning for non-speech audio tasks rather than on spoken language models or speech language modeling; it does not discuss full-duplex spoken interaction, cognitive parallelism, or related advanced speech modeling aspects required by the inclusion criteria, and the keywords such as 'spoken language model(s)' or equivalent are absent from the title and abstract. Therefore, it does not meet the topic definition centered on spoken language models and their synchronization or interaction capabilities.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "AudioLDM 2: Learning Holistic Audio Generation With Self-Supervised Pretraining",
    "abstract": "Although audio generation shares commonalities across different types of audio, such as speech, music, and sound effects, designing models for each type requires careful consideration of specific objectives and biases that can significantly differ from those of other types. To bring us closer to a unified perspective of audio generation, this paper proposes a holistic framework that utilizes the same learning method for speech, music, and sound effect generation. Our framework utilizes a general representation of audio, called \"language of audio\" (LOA). Any audio can be translated into LOA based on AudioMAE, a self-supervised pre-trained representation learning model. In the generation process, we translate other modalities into LOA by using a GPT-2 model, and we perform self-supervised audio generation learning with a latent diffusion model conditioned on the LOA of audio in our training set. The proposed framework naturally brings advantages such as reusable self-supervised pretrained latent diffusion models. Experiments on the major benchmarks of text-to-audio, text-to-music, and text-to-speech with three AudioLDM 2 variants demonstrate competitive performance of the AudioLDM 2 variants framework against previous approaches. Our code, pretrained model, and demo are available at <uri xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">https://audioldm.github.io/audioldm2</uri> .",
    "metadata": {
      "title": "AudioLDM 2: Learning Holistic Audio Generation With Self-Supervised Pretraining",
      "summary": "Although audio generation shares commonalities across different types of audio, such as speech, music, and sound effects, designing models for each type requires careful consideration of specific objectives and biases that can significantly differ from those of other types. To bring us closer to a unified perspective of audio generation, this paper proposes a holistic framework that utilizes the same learning method for speech, music, and sound effect generation. Our framework utilizes a general representation of audio, called \"language of audio\" (LOA). Any audio can be translated into LOA based on AudioMAE, a self-supervised pre-trained representation learning model. In the generation process, we translate other modalities into LOA by using a GPT-2 model, and we perform self-supervised audio generation learning with a latent diffusion model conditioned on the LOA of audio in our training set. The proposed framework naturally brings advantages such as reusable self-supervised pretrained latent diffusion models. Experiments on the major benchmarks of text-to-audio, text-to-music, and text-to-speech with three AudioLDM 2 variants demonstrate competitive performance of the AudioLDM 2 variants framework against previous approaches. Our code, pretrained model, and demo are available at <uri xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">https://audioldm.github.io/audioldm2</uri> .",
      "abstract": "Although audio generation shares commonalities across different types of audio, such as speech, music, and sound effects, designing models for each type requires careful consideration of specific objectives and biases that can significantly differ from those of other types. To bring us closer to a unified perspective of audio generation, this paper proposes a holistic framework that utilizes the same learning method for speech, music, and sound effect generation. Our framework utilizes a general representation of audio, called \"language of audio\" (LOA). Any audio can be translated into LOA based on AudioMAE, a self-supervised pre-trained representation learning model. In the generation process, we translate other modalities into LOA by using a GPT-2 model, and we perform self-supervised audio generation learning with a latent diffusion model conditioned on the LOA of audio in our training set. The proposed framework naturally brings advantages such as reusable self-supervised pretrained latent diffusion models. Experiments on the major benchmarks of text-to-audio, text-to-music, and text-to-speech with three AudioLDM 2 variants demonstrate competitive performance of the AudioLDM 2 variants framework against previous approaches. Our code, pretrained model, and demo are available at <uri xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">https://audioldm.github.io/audioldm2</uri> .",
      "doi": "https://doi.org/10.1109/taslp.2024.3399607",
      "openalex_id": "https://openalex.org/W4396877837",
      "arxiv_id": "",
      "publication_date": "2024-01-01",
      "published": "2024-01-01",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This paper focuses on holistic audio generation across speech, music, and SFX using a unified LOA representation and self-supervised pretraining, but it does not center on Spoken Language Models or full-duplex/synchronous speech interaction with turn-taking and cognitive synchronization, nor does it emphasize speech-language modeling as the core object of study; therefore it does not meet the inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "This paper focuses on holistic audio generation across speech, music, and SFX using a unified LOA representation and self-supervised pretraining, but it does not center on Spoken Language Models or full-duplex/synchronous speech interaction with turn-taking and cognitive synchronization, nor does it emphasize speech-language modeling as the core object of study; therefore it does not meet the inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on a holistic framework for audio generation covering speech, music, and sound effects using self-supervised pretrained models; however, it does not explicitly address spoken language models, full-duplex spoken language modeling, synchronization for real-time interactive speech, or cognitive parallelism as required by the inclusion criteria. It primarily outlines audio generation methodologies rather than modeling or reasoning about speech sequences for interactive dialogue, and lacks discussion on end-to-end or hybrid speech-LLM architectures with synchronization and interaction evaluation aspects, thus not meeting the inclusion standards.",
      "evaluation": 2
    },
    "round-A_JuniorMini_reasoning": "The study focuses on a holistic framework for audio generation covering speech, music, and sound effects using self-supervised pretrained models; however, it does not explicitly address spoken language models, full-duplex spoken language modeling, synchronization for real-time interactive speech, or cognitive parallelism as required by the inclusion criteria. It primarily outlines audio generation methodologies rather than modeling or reasoning about speech sequences for interactive dialogue, and lacks discussion on end-to-end or hybrid speech-LLM architectures with synchronization and interaction evaluation aspects, thus not meeting the inclusion standards.",
    "round-A_JuniorMini_evaluation": 2,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:2)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Joint Audio and Speech Understanding",
    "abstract": "Humans are surrounded by audio signals that include both speech and non-speech sounds. The recognition and understanding of speech and non-speech audio events, along with a profound comprehension of the relationship between them, constitute fundamental cognitive capabilities. For the first time, we build a machine learning model, called LTU-AS, that has a conceptually similar universal audio perception and advanced reasoning ability. Specifically, by integrating Whisper [1] as a perception module and LLaMA [2] as a reasoning module, LTU-AS can simultaneously recognize and jointly understand spoken text, speech paralinguistics, and non-speech audio events - almost everything perceivable from audio signals.",
    "metadata": {
      "title": "Joint Audio and Speech Understanding",
      "summary": "Humans are surrounded by audio signals that include both speech and non-speech sounds. The recognition and understanding of speech and non-speech audio events, along with a profound comprehension of the relationship between them, constitute fundamental cognitive capabilities. For the first time, we build a machine learning model, called LTU-AS, that has a conceptually similar universal audio perception and advanced reasoning ability. Specifically, by integrating Whisper [1] as a perception module and LLaMA [2] as a reasoning module, LTU-AS can simultaneously recognize and jointly understand spoken text, speech paralinguistics, and non-speech audio events - almost everything perceivable from audio signals.",
      "abstract": "Humans are surrounded by audio signals that include both speech and non-speech sounds. The recognition and understanding of speech and non-speech audio events, along with a profound comprehension of the relationship between them, constitute fundamental cognitive capabilities. For the first time, we build a machine learning model, called LTU-AS, that has a conceptually similar universal audio perception and advanced reasoning ability. Specifically, by integrating Whisper [1] as a perception module and LLaMA [2] as a reasoning module, LTU-AS can simultaneously recognize and jointly understand spoken text, speech paralinguistics, and non-speech audio events - almost everything perceivable from audio signals.",
      "doi": "https://doi.org/10.1109/asru57964.2023.10389742",
      "openalex_id": "https://openalex.org/W4391021627",
      "arxiv_id": "",
      "publication_date": "2023-12-16",
      "published": "2023-12-16",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The abstract describes LTU-AS, a model that integrates Whisper for perception and LLaMA for reasoning to jointly recognize spoken text, speech paralinguistics, and non-speech audio, aligning with the Spoken Language Models/FD-SLMs paradigm, including architecture-level discussion and cross-modal evaluation, and it is within the last 3 years; it goes beyond traditional ASR and telegraphs core speech-language modeling concepts, satisfying inclusion while not focusing on half-duplex or single-task evaluation.",
      "evaluation": 4
    },
    "round-A_JuniorNano_reasoning": "The abstract describes LTU-AS, a model that integrates Whisper for perception and LLaMA for reasoning to jointly recognize spoken text, speech paralinguistics, and non-speech audio, aligning with the Spoken Language Models/FD-SLMs paradigm, including architecture-level discussion and cross-modal evaluation, and it is within the last 3 years; it goes beyond traditional ASR and telegraphs core speech-language modeling concepts, satisfying inclusion while not focusing on half-duplex or single-task evaluation.",
    "round-A_JuniorNano_evaluation": 4,
    "round-A_JuniorMini_output": {
      "reasoning": "The study proposes a machine learning model integrating speech and non-speech audio understanding, but the abstract does not explicitly mention 'spoken language model(s)' or 'speech language model(s)', nor does it clearly address full-duplex spoken language modeling, synchronization for interactive dialogue, or real-time low-latency interaction; therefore, it partially aligns but does not fully meet the inclusion criteria and may lean towards exclusion.",
      "evaluation": 2
    },
    "round-A_JuniorMini_reasoning": "The study proposes a machine learning model integrating speech and non-speech audio understanding, but the abstract does not explicitly mention 'spoken language model(s)' or 'speech language model(s)', nor does it clearly address full-duplex spoken language modeling, synchronization for interactive dialogue, or real-time low-latency interaction; therefore, it partially aligns but does not fully meet the inclusion criteria and may lean towards exclusion.",
    "round-A_JuniorMini_evaluation": 2,
    "round-B_SeniorLead_output": {
      "reasoning": "I reviewed the two junior reviewers' notes: JuniorNano favored inclusion citing architecture-level integration and joint audio understanding, while JuniorMini recommended exclusion noting the abstract lacks explicit spoken-language-model framing and full-duplex/real-time interaction details. The abstract describes LTU-AS as an integration of Whisper (perception) and LLaMA (reasoning) to jointly recognize spoken text, paralinguistics, and non-speech audio, but it does not explicitly state \"spoken language model(s)\" or equivalent language-modeling-of-speech terminology, nor does it describe training, evaluation benchmarks, or any full‑duplex/synchronization/low-latency contributions—appearing closer to an ASR→LLM multimodal pipeline than a study centered on spoken-language modeling or FD‑SLM innovations. Therefore it fails the inclusion requirements (explicit SLM framing and SLM-focused methods/evaluation) and meets exclusion criteria (essentially an ASR/LLM integration without SLM-specific analysis). Brief justification for my score: Better to exclude — the abstract lacks explicit spoken-language-model framing, SLM-specific methods, and any full‑duplex/synchronization or evaluation details required by the inclusion criteria.",
      "evaluation": 2
    },
    "round-B_SeniorLead_reasoning": "I reviewed the two junior reviewers' notes: JuniorNano favored inclusion citing architecture-level integration and joint audio understanding, while JuniorMini recommended exclusion noting the abstract lacks explicit spoken-language-model framing and full-duplex/real-time interaction details. The abstract describes LTU-AS as an integration of Whisper (perception) and LLaMA (reasoning) to jointly recognize spoken text, paralinguistics, and non-speech audio, but it does not explicitly state \"spoken language model(s)\" or equivalent language-modeling-of-speech terminology, nor does it describe training, evaluation benchmarks, or any full‑duplex/synchronization/low-latency contributions—appearing closer to an ASR→LLM multimodal pipeline than a study centered on spoken-language modeling or FD‑SLM innovations. Therefore it fails the inclusion requirements (explicit SLM framing and SLM-focused methods/evaluation) and meets exclusion criteria (essentially an ASR/LLM integration without SLM-specific analysis). Brief justification for my score: Better to exclude — the abstract lacks explicit spoken-language-model framing, SLM-specific methods, and any full‑duplex/synchronization or evaluation details required by the inclusion criteria.",
    "round-B_SeniorLead_evaluation": 2,
    "final_verdict": "exclude (senior:2)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Scalable Diffusion Models with Transformers",
    "abstract": "We explore a new class of diffusion models based on the transformer architecture. We train latent diffusion models of images, replacing the commonly-used U-Net backbone with a transformer that operates on latent patches. We analyze the scalability of our Diffusion Transformers (DiTs) through the lens of forward pass complexity as measured by Gflops. We find that DiTs with higher Gflops—through increased transformer depth/width or increased number of input tokens—consistently have lower FID. In addition to possessing good scalability properties, our largest DiT-XL/2 models outperform all prior diffusion models on the class-conditional ImageNet 512×512 and 256×256 benchmarks, achieving a state-of-the-art FID of 2.27 on the latter.",
    "metadata": {
      "title": "Scalable Diffusion Models with Transformers",
      "summary": "We explore a new class of diffusion models based on the transformer architecture. We train latent diffusion models of images, replacing the commonly-used U-Net backbone with a transformer that operates on latent patches. We analyze the scalability of our Diffusion Transformers (DiTs) through the lens of forward pass complexity as measured by Gflops. We find that DiTs with higher Gflops—through increased transformer depth/width or increased number of input tokens—consistently have lower FID. In addition to possessing good scalability properties, our largest DiT-XL/2 models outperform all prior diffusion models on the class-conditional ImageNet 512×512 and 256×256 benchmarks, achieving a state-of-the-art FID of 2.27 on the latter.",
      "abstract": "We explore a new class of diffusion models based on the transformer architecture. We train latent diffusion models of images, replacing the commonly-used U-Net backbone with a transformer that operates on latent patches. We analyze the scalability of our Diffusion Transformers (DiTs) through the lens of forward pass complexity as measured by Gflops. We find that DiTs with higher Gflops—through increased transformer depth/width or increased number of input tokens—consistently have lower FID. In addition to possessing good scalability properties, our largest DiT-XL/2 models outperform all prior diffusion models on the class-conditional ImageNet 512×512 and 256×256 benchmarks, achieving a state-of-the-art FID of 2.27 on the latter.",
      "doi": "https://doi.org/10.1109/iccv51070.2023.00387",
      "openalex_id": "https://openalex.org/W4390872297",
      "arxiv_id": "",
      "publication_date": "2023-10-01",
      "published": "2023-10-01",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item is about diffusion models for images using Transformer backbones, not about Spoken Language Models or speech-language modeling, so it does not meet the inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item is about diffusion models for images using Transformer backbones, not about Spoken Language Models or speech-language modeling, so it does not meet the inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on diffusion models using transformers for image generation, without mentioning spoken language models, speech language modeling, or any speech signal processing. It does not address full-duplex spoken language interaction, synchronization, or related evaluation aspects required by the inclusion criteria. Furthermore, the study revolves around image modeling, not speech or language models based on acoustic signals, making it irrelevant to the defined topic.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on diffusion models using transformers for image generation, without mentioning spoken language models, speech language modeling, or any speech signal processing. It does not address full-duplex spoken language interaction, synchronization, or related evaluation aspects required by the inclusion criteria. Furthermore, the study revolves around image modeling, not speech or language models based on acoustic signals, making it irrelevant to the defined topic.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Matcha-TTS: A Fast TTS Architecture with Conditional Flow Matching",
    "abstract": "We introduce Matcha-TTS, a new encoder-decoder architecture for speedy TTS acoustic modelling, trained using optimal-transport conditional flow matching (OT-CFM). This yields an ODE-based decoder capable of high output quality in fewer synthesis steps than models trained using score matching. Careful design choices additionally ensure each synthesis step is fast to run. The method is probabilistic, non-autoregressive, and learns to speak from scratch without external alignments. Compared to strong pre-trained baseline models, the Matcha-TTS system has the smallest memory footprint, rivals the speed of the fastest model on long utterances, and attains the highest mean opinion score in a listening test.",
    "metadata": {
      "title": "Matcha-TTS: A Fast TTS Architecture with Conditional Flow Matching",
      "summary": "We introduce Matcha-TTS, a new encoder-decoder architecture for speedy TTS acoustic modelling, trained using optimal-transport conditional flow matching (OT-CFM). This yields an ODE-based decoder capable of high output quality in fewer synthesis steps than models trained using score matching. Careful design choices additionally ensure each synthesis step is fast to run. The method is probabilistic, non-autoregressive, and learns to speak from scratch without external alignments. Compared to strong pre-trained baseline models, the Matcha-TTS system has the smallest memory footprint, rivals the speed of the fastest model on long utterances, and attains the highest mean opinion score in a listening test.",
      "abstract": "We introduce Matcha-TTS, a new encoder-decoder architecture for speedy TTS acoustic modelling, trained using optimal-transport conditional flow matching (OT-CFM). This yields an ODE-based decoder capable of high output quality in fewer synthesis steps than models trained using score matching. Careful design choices additionally ensure each synthesis step is fast to run. The method is probabilistic, non-autoregressive, and learns to speak from scratch without external alignments. Compared to strong pre-trained baseline models, the Matcha-TTS system has the smallest memory footprint, rivals the speed of the fastest model on long utterances, and attains the highest mean opinion score in a listening test.",
      "doi": "https://doi.org/10.1109/icassp48485.2024.10448291",
      "openalex_id": "https://openalex.org/W4392931276",
      "arxiv_id": "",
      "publication_date": "2024-03-18",
      "published": "2024-03-18",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "Although Matcha-TTS targets speech synthesis with a speech/audio modeling focus, the abstract does not explicitly mention spoken language models or speech language models (or an equivalent emphasis on language modeling of speech) in the title/abstract/keywords, thus failing the explicit inclusion criterion, and it is a single-task TTS paper without interactive dialogue aspects; therefore it should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "Although Matcha-TTS targets speech synthesis with a speech/audio modeling focus, the abstract does not explicitly mention spoken language models or speech language models (or an equivalent emphasis on language modeling of speech) in the title/abstract/keywords, thus failing the explicit inclusion criterion, and it is a single-task TTS paper without interactive dialogue aspects; therefore it should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on a TTS model architecture (Matcha-TTS) emphasizing fast synthesis and acoustic modeling, but does not address spoken language models as core modeling for speech sequences, nor does it mention full-duplex interaction, synchronous turn-taking, or behavioral arbitration; it mainly targets speech synthesis quality and speed rather than interactive dialogue behaviors or spoken language modeling frameworks specified by the inclusion criteria.",
      "evaluation": 2
    },
    "round-A_JuniorMini_reasoning": "The study focuses on a TTS model architecture (Matcha-TTS) emphasizing fast synthesis and acoustic modeling, but does not address spoken language models as core modeling for speech sequences, nor does it mention full-duplex interaction, synchronous turn-taking, or behavioral arbitration; it mainly targets speech synthesis quality and speed rather than interactive dialogue behaviors or spoken language modeling frameworks specified by the inclusion criteria.",
    "round-A_JuniorMini_evaluation": 2,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:2)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "AudioGPT: Understanding and Generating Speech, Music, Sound, and Talking Head",
    "abstract": "Large language models (LLMs) have exhibited remarkable capabilities across a variety of domains and tasks, challenging our understanding of learning and cognition. Despite the recent success, current LLMs are not capable of processing complex audio information or conducting spoken conversations (like Siri or Alexa). In this work, we propose a multi-modal AI system named AudioGPT, which complements LLMs (i.e., ChatGPT) with 1) foundation models to process complex audio information and solve numerous understanding and generation tasks; and 2) the input/output interface (ASR, TTS) to support spoken dialogue. With an increasing demand to evaluate multi-modal LLMs of human intention understanding and cooperation with foundation models, we outline the principles and processes and test AudioGPT in terms of consistency, capability, and robustness. Experimental results demonstrate the capabilities of AudioGPT in solving 16 AI tasks with speech, music, sound, and talking head understanding and generation in multi-round dialogues, which empower humans to create rich and diverse audio content with unprecedented ease. Code can be found in https://github.com/AIGC-Audio/AudioGPT",
    "metadata": {
      "title": "AudioGPT: Understanding and Generating Speech, Music, Sound, and Talking Head",
      "summary": "Large language models (LLMs) have exhibited remarkable capabilities across a variety of domains and tasks, challenging our understanding of learning and cognition. Despite the recent success, current LLMs are not capable of processing complex audio information or conducting spoken conversations (like Siri or Alexa). In this work, we propose a multi-modal AI system named AudioGPT, which complements LLMs (i.e., ChatGPT) with 1) foundation models to process complex audio information and solve numerous understanding and generation tasks; and 2) the input/output interface (ASR, TTS) to support spoken dialogue. With an increasing demand to evaluate multi-modal LLMs of human intention understanding and cooperation with foundation models, we outline the principles and processes and test AudioGPT in terms of consistency, capability, and robustness. Experimental results demonstrate the capabilities of AudioGPT in solving 16 AI tasks with speech, music, sound, and talking head understanding and generation in multi-round dialogues, which empower humans to create rich and diverse audio content with unprecedented ease. Code can be found in https://github.com/AIGC-Audio/AudioGPT",
      "abstract": "Large language models (LLMs) have exhibited remarkable capabilities across a variety of domains and tasks, challenging our understanding of learning and cognition. Despite the recent success, current LLMs are not capable of processing complex audio information or conducting spoken conversations (like Siri or Alexa). In this work, we propose a multi-modal AI system named AudioGPT, which complements LLMs (i.e., ChatGPT) with 1) foundation models to process complex audio information and solve numerous understanding and generation tasks; and 2) the input/output interface (ASR, TTS) to support spoken dialogue. With an increasing demand to evaluate multi-modal LLMs of human intention understanding and cooperation with foundation models, we outline the principles and processes and test AudioGPT in terms of consistency, capability, and robustness. Experimental results demonstrate the capabilities of AudioGPT in solving 16 AI tasks with speech, music, sound, and talking head understanding and generation in multi-round dialogues, which empower humans to create rich and diverse audio content with unprecedented ease. Code can be found in https://github.com/AIGC-Audio/AudioGPT",
      "doi": "https://doi.org/10.1609/aaai.v38i21.30570",
      "openalex_id": "https://openalex.org/W4393178509",
      "arxiv_id": "",
      "publication_date": "2024-03-24",
      "published": "2024-03-24",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The title/abstract do not clearly present Spoken Language Models or FD-SLMs as the core focus; instead, AudioGPT is framed as a multimodal audio system that augments LLMs, with no explicit mention of SLM/SLM-equivalent in the abstract.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The title/abstract do not clearly present Spoken Language Models or FD-SLMs as the core focus; instead, AudioGPT is framed as a multimodal audio system that augments LLMs, with no explicit mention of SLM/SLM-equivalent in the abstract.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study proposes AudioGPT, a multi-modal AI system that integrates spoken dialogue interfaces (ASR, TTS) and foundation models capable of understanding and generating complex audio information (speech, music, sound, talking head). However, the abstract does not explicitly mention spoken language models (SLMs) or speech language modeling as core modeling/reasoning objects, nor does it emphasize full-duplex interaction capabilities, cognitive parallelism, voice overlap management, or low latency interaction under 200ms. The system includes spoken dialogue but seems to focus more on multimodal audio processing and generation rather than specifically on spoken language modeling or full-duplex spoken language model architectures. Thus, it does not clearly meet the inclusion criteria focusing on SLMs and explicit speech language modeling frameworks and evaluation dimensions; nor does it fully meet the exclusion criteria, so the inclusion is uncertain.",
      "evaluation": 3
    },
    "round-A_JuniorMini_reasoning": "The study proposes AudioGPT, a multi-modal AI system that integrates spoken dialogue interfaces (ASR, TTS) and foundation models capable of understanding and generating complex audio information (speech, music, sound, talking head). However, the abstract does not explicitly mention spoken language models (SLMs) or speech language modeling as core modeling/reasoning objects, nor does it emphasize full-duplex interaction capabilities, cognitive parallelism, voice overlap management, or low latency interaction under 200ms. The system includes spoken dialogue but seems to focus more on multimodal audio processing and generation rather than specifically on spoken language modeling or full-duplex spoken language model architectures. Thus, it does not clearly meet the inclusion criteria focusing on SLMs and explicit speech language modeling frameworks and evaluation dimensions; nor does it fully meet the exclusion criteria, so the inclusion is uncertain.",
    "round-A_JuniorMini_evaluation": 3,
    "round-B_SeniorLead_output": {
      "reasoning": "Junior reviewers: JuniorNano recommended exclusion (1) noting the paper is a multimodal audio system rather than an SLM; JuniorMini was uncertain (3) but also observed the abstract lacks explicit SLM/Speech-LM framing or full-duplex/interactivity details. My assessment: the abstract describes AudioGPT as a multimodal system that augments LLMs with foundation audio models plus ASR/TTS for spoken dialogue, but it does not explicitly present Spoken Language Models (SLMs) or speech-level language modeling as the core research, nor does it discuss architectures, training, benchmarks, or full‑duplex/interruption/overlap/latency evaluation required by the inclusion criteria. Therefore it fails the requirement that the title/abstract explicitly target SLMs or equivalent and should be excluded. One-sentence explanation: The abstract lacks explicit SLM/speech-language-model framing and focuses on multimodal integration (ASR/TTS/foundation models) rather than proposing or evaluating Spoken Language Models or full‑duplex speech modeling.",
      "evaluation": 1
    },
    "round-B_SeniorLead_reasoning": "Junior reviewers: JuniorNano recommended exclusion (1) noting the paper is a multimodal audio system rather than an SLM; JuniorMini was uncertain (3) but also observed the abstract lacks explicit SLM/Speech-LM framing or full-duplex/interactivity details. My assessment: the abstract describes AudioGPT as a multimodal system that augments LLMs with foundation audio models plus ASR/TTS for spoken dialogue, but it does not explicitly present Spoken Language Models (SLMs) or speech-level language modeling as the core research, nor does it discuss architectures, training, benchmarks, or full‑duplex/interruption/overlap/latency evaluation required by the inclusion criteria. Therefore it fails the requirement that the title/abstract explicitly target SLMs or equivalent and should be excluded. One-sentence explanation: The abstract lacks explicit SLM/speech-language-model framing and focuses on multimodal integration (ASR/TTS/foundation models) rather than proposing or evaluating Spoken Language Models or full‑duplex speech modeling.",
    "round-B_SeniorLead_evaluation": 1,
    "final_verdict": "exclude (senior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Audio Set: An ontology and human-labeled dataset for audio events",
    "abstract": "Audio event recognition, the human-like ability to identify and relate sounds from audio, is a nascent problem in machine perception. Comparable problems such as object detection in images have reaped enormous benefits from comprehensive datasets - principally ImageNet. This paper describes the creation of Audio Set, a large-scale dataset of manually-annotated audio events that endeavors to bridge the gap in data availability between image and audio research. Using a carefully structured hierarchical ontology of 632 audio classes guided by the literature and manual curation, we collect data from human labelers to probe the presence of specific audio classes in 10 second segments of YouTube videos. Segments are proposed for labeling using searches based on metadata, context (e.g., links), and content analysis. The result is a dataset of unprecedented breadth and size that will, we hope, substantially stimulate the development of high-performance audio event recognizers.",
    "metadata": {
      "title": "Audio Set: An ontology and human-labeled dataset for audio events",
      "summary": "Audio event recognition, the human-like ability to identify and relate sounds from audio, is a nascent problem in machine perception. Comparable problems such as object detection in images have reaped enormous benefits from comprehensive datasets - principally ImageNet. This paper describes the creation of Audio Set, a large-scale dataset of manually-annotated audio events that endeavors to bridge the gap in data availability between image and audio research. Using a carefully structured hierarchical ontology of 632 audio classes guided by the literature and manual curation, we collect data from human labelers to probe the presence of specific audio classes in 10 second segments of YouTube videos. Segments are proposed for labeling using searches based on metadata, context (e.g., links), and content analysis. The result is a dataset of unprecedented breadth and size that will, we hope, substantially stimulate the development of high-performance audio event recognizers.",
      "abstract": "Audio event recognition, the human-like ability to identify and relate sounds from audio, is a nascent problem in machine perception. Comparable problems such as object detection in images have reaped enormous benefits from comprehensive datasets - principally ImageNet. This paper describes the creation of Audio Set, a large-scale dataset of manually-annotated audio events that endeavors to bridge the gap in data availability between image and audio research. Using a carefully structured hierarchical ontology of 632 audio classes guided by the literature and manual curation, we collect data from human labelers to probe the presence of specific audio classes in 10 second segments of YouTube videos. Segments are proposed for labeling using searches based on metadata, context (e.g., links), and content analysis. The result is a dataset of unprecedented breadth and size that will, we hope, substantially stimulate the development of high-performance audio event recognizers.",
      "doi": "https://doi.org/10.1109/icassp.2017.7952261",
      "openalex_id": "https://openalex.org/W2593116425",
      "arxiv_id": "",
      "publication_date": "2017-03-01",
      "published": "2017-03-01",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item is about Audio Set, a large-scale dataset for audio event recognition, with ontology and manual annotations; it does not focus on Spoken Language Models or any equivalently defined speech-language modeling core, lacks discussion of model architectures, training, or evaluation for speech-language modeling, and does not address real-time interactive speech or synchronization. Therefore it does not meet the inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item is about Audio Set, a large-scale dataset for audio event recognition, with ontology and manual annotations; it does not focus on Spoken Language Models or any equivalently defined speech-language modeling core, lacks discussion of model architectures, training, or evaluation for speech-language modeling, and does not address real-time interactive speech or synchronization. Therefore it does not meet the inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper 'Audio Set: An ontology and human-labeled dataset for audio events' focuses on the creation of a large-scale dataset of audio event annotations for machine perception tasks. It does not explicitly mention spoken language models, speech language models, or any form of full-duplex spoken interaction modeling, nor does it discuss model architectures, training, or evaluation related to spoken language models or their interaction abilities. Instead, it is primarily about audio event detection datasets, which is outside the scope defined by the inclusion criteria focused on speech language modeling and full-duplex spoken language interactions.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper 'Audio Set: An ontology and human-labeled dataset for audio events' focuses on the creation of a large-scale dataset of audio event annotations for machine perception tasks. It does not explicitly mention spoken language models, speech language models, or any form of full-duplex spoken interaction modeling, nor does it discuss model architectures, training, or evaluation related to spoken language models or their interaction abilities. Instead, it is primarily about audio event detection datasets, which is outside the scope defined by the inclusion criteria focused on speech language modeling and full-duplex spoken language interactions.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Vggsound: A Large-Scale Audio-Visual Dataset",
    "abstract": "Our goal is to collect a large-scale audio-visual dataset with low label noise from videos `in the wild' using computer vision techniques. The resulting dataset can be used for training and evaluating audio recognition models. We make three contributions. First, we propose a scalable pipeline based on computer vision techniques to create an audio dataset from open-source media. Our pipeline involves obtaining videos from YouTube; using image classification algorithms to localize audio-visual correspondence; and filtering out ambient noise using audio verification. Second, we use this pipeline to curate the VGGSound dataset consisting of more than 200k videos for 300 audio classes. Third, we investigate various Convolutional Neural Network (CNN) architectures and aggregation approaches to establish audio recognition baselines for our new dataset. Compared to existing audio datasets, VGGSound ensures audio-visual correspondence and is collected under unconstrained conditions. Code and the dataset are available at http://www.robots.ox.ac.uk/~vgg/data/vggsound/.",
    "metadata": {
      "title": "Vggsound: A Large-Scale Audio-Visual Dataset",
      "summary": "Our goal is to collect a large-scale audio-visual dataset with low label noise from videos `in the wild' using computer vision techniques. The resulting dataset can be used for training and evaluating audio recognition models. We make three contributions. First, we propose a scalable pipeline based on computer vision techniques to create an audio dataset from open-source media. Our pipeline involves obtaining videos from YouTube; using image classification algorithms to localize audio-visual correspondence; and filtering out ambient noise using audio verification. Second, we use this pipeline to curate the VGGSound dataset consisting of more than 200k videos for 300 audio classes. Third, we investigate various Convolutional Neural Network (CNN) architectures and aggregation approaches to establish audio recognition baselines for our new dataset. Compared to existing audio datasets, VGGSound ensures audio-visual correspondence and is collected under unconstrained conditions. Code and the dataset are available at http://www.robots.ox.ac.uk/~vgg/data/vggsound/.",
      "abstract": "Our goal is to collect a large-scale audio-visual dataset with low label noise from videos `in the wild' using computer vision techniques. The resulting dataset can be used for training and evaluating audio recognition models. We make three contributions. First, we propose a scalable pipeline based on computer vision techniques to create an audio dataset from open-source media. Our pipeline involves obtaining videos from YouTube; using image classification algorithms to localize audio-visual correspondence; and filtering out ambient noise using audio verification. Second, we use this pipeline to curate the VGGSound dataset consisting of more than 200k videos for 300 audio classes. Third, we investigate various Convolutional Neural Network (CNN) architectures and aggregation approaches to establish audio recognition baselines for our new dataset. Compared to existing audio datasets, VGGSound ensures audio-visual correspondence and is collected under unconstrained conditions. Code and the dataset are available at http://www.robots.ox.ac.uk/~vgg/data/vggsound/.",
      "doi": "https://doi.org/10.1109/icassp40776.2020.9053174",
      "openalex_id": "https://openalex.org/W3015371781",
      "arxiv_id": "",
      "publication_date": "2020-04-09",
      "published": "2020-04-09",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "此項目聚焦於建立大規模影音資料集以培訓音頻識別模型，核心並非語音語言建模/語音-文字混合架構或對話互動評估，因此不符合「Spoken Language Models（SLMs）或等價概念」的核心條件與評估框架。",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "此項目聚焦於建立大規模影音資料集以培訓音頻識別模型，核心並非語音語言建模/語音-文字混合架構或對話互動評估，因此不符合「Spoken Language Models（SLMs）或等價概念」的核心條件與評估框架。",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper presents a large-scale audio-visual dataset and focuses on audio recognition models using computer vision techniques, but does not discuss Spoken Language Models (SLMs) or full-duplex spoken language interaction models, nor does it cover model architectures or evaluations related to speech language modeling as per the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper presents a large-scale audio-visual dataset and focuses on audio recognition models using computer vision techniques, but does not discuss Spoken Language Models (SLMs) or full-duplex spoken language interaction models, nor does it cover model architectures or evaluations related to speech language modeling as per the inclusion criteria.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Adapting Frechet Audio Distance for Generative Music Evaluation",
    "abstract": "The growing popularity of generative music models underlines the need for perceptually relevant, objective music quality metrics. The Frechet Audio Distance (FAD) is commonly used for this purpose even though its correlation with perceptual quality is understudied. We show that FAD performance may be hampered by sample size bias, poor choice of audio embeddings, or the use of biased or low-quality reference sets. We propose reducing sample size bias by extrapolating scores towards an infinite sample size. Through comparisons with MusicCaps labels and a listening test we identify audio embeddings and music reference sets that yield FAD scores well-correlated with acoustic and musical quality. Our results suggest that per-song FAD can be useful to identify outlier samples and predict perceptual quality for a range of music sets and generative models. Finally, we release a toolkit that allows adapting FAD for generative music evaluation.",
    "metadata": {
      "title": "Adapting Frechet Audio Distance for Generative Music Evaluation",
      "summary": "The growing popularity of generative music models underlines the need for perceptually relevant, objective music quality metrics. The Frechet Audio Distance (FAD) is commonly used for this purpose even though its correlation with perceptual quality is understudied. We show that FAD performance may be hampered by sample size bias, poor choice of audio embeddings, or the use of biased or low-quality reference sets. We propose reducing sample size bias by extrapolating scores towards an infinite sample size. Through comparisons with MusicCaps labels and a listening test we identify audio embeddings and music reference sets that yield FAD scores well-correlated with acoustic and musical quality. Our results suggest that per-song FAD can be useful to identify outlier samples and predict perceptual quality for a range of music sets and generative models. Finally, we release a toolkit that allows adapting FAD for generative music evaluation.",
      "abstract": "The growing popularity of generative music models underlines the need for perceptually relevant, objective music quality metrics. The Frechet Audio Distance (FAD) is commonly used for this purpose even though its correlation with perceptual quality is understudied. We show that FAD performance may be hampered by sample size bias, poor choice of audio embeddings, or the use of biased or low-quality reference sets. We propose reducing sample size bias by extrapolating scores towards an infinite sample size. Through comparisons with MusicCaps labels and a listening test we identify audio embeddings and music reference sets that yield FAD scores well-correlated with acoustic and musical quality. Our results suggest that per-song FAD can be useful to identify outlier samples and predict perceptual quality for a range of music sets and generative models. Finally, we release a toolkit that allows adapting FAD for generative music evaluation.",
      "doi": "https://doi.org/10.1109/icassp48485.2024.10446663",
      "openalex_id": "https://openalex.org/W4392902957",
      "arxiv_id": "",
      "publication_date": "2024-03-18",
      "published": "2024-03-18",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This item does not center on Spoken Language Models or any form of speech-language modeling; it focuses on music generation evaluation and distance metrics rather than modeling and evaluating speech.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "This item does not center on Spoken Language Models or any form of speech-language modeling; it focuses on music generation evaluation and distance metrics rather than modeling and evaluating speech.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on adapting the Frechet Audio Distance for generative music evaluation, which does not pertain to Spoken Language Models or full-duplex spoken interaction systems; it lacks mention of speech language modeling, synchronous interaction, or related architectural/training/evaluation methods as outlined by the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on adapting the Frechet Audio Distance for generative music evaluation, which does not pertain to Spoken Language Models or full-duplex spoken interaction systems; it lacks mention of speech language modeling, synchronous interaction, or related architectural/training/evaluation methods as outlined by the inclusion criteria.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Can Audio Captions Be Evaluated With Image Caption Metrics?",
    "abstract": "Automated audio captioning aims at generating textual descriptions for an audio clip. To evaluate the quality of generated audio captions, previous works directly adopt image captioning metrics like SPICE and CIDEr, without justifying their suitability in this new domain, which may mislead the development of advanced models. This problem is still unstudied due to the lack of human judgment datasets on caption quality. Therefore, we first construct two evaluation benchmarks, AudioCaps-Eval and Clotho-Eval. They are established with pairwise comparison instead of absolute rating to achieve better inter-annotator agreement. Current metrics are found in poor correlation with human annotations on these datasets. To overcome their limitations, we propose a metric named FENSE, where we combine the strength of Sentence-BERT in capturing similarity, and a novel Error Detector to penalize erroneous sentences for robustness. On the newly established benchmarks, FENSE outperforms current metrics by 14-25% accuracy. <sup xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">1</sup>",
    "metadata": {
      "title": "Can Audio Captions Be Evaluated With Image Caption Metrics?",
      "summary": "Automated audio captioning aims at generating textual descriptions for an audio clip. To evaluate the quality of generated audio captions, previous works directly adopt image captioning metrics like SPICE and CIDEr, without justifying their suitability in this new domain, which may mislead the development of advanced models. This problem is still unstudied due to the lack of human judgment datasets on caption quality. Therefore, we first construct two evaluation benchmarks, AudioCaps-Eval and Clotho-Eval. They are established with pairwise comparison instead of absolute rating to achieve better inter-annotator agreement. Current metrics are found in poor correlation with human annotations on these datasets. To overcome their limitations, we propose a metric named FENSE, where we combine the strength of Sentence-BERT in capturing similarity, and a novel Error Detector to penalize erroneous sentences for robustness. On the newly established benchmarks, FENSE outperforms current metrics by 14-25% accuracy. <sup xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">1</sup>",
      "abstract": "Automated audio captioning aims at generating textual descriptions for an audio clip. To evaluate the quality of generated audio captions, previous works directly adopt image captioning metrics like SPICE and CIDEr, without justifying their suitability in this new domain, which may mislead the development of advanced models. This problem is still unstudied due to the lack of human judgment datasets on caption quality. Therefore, we first construct two evaluation benchmarks, AudioCaps-Eval and Clotho-Eval. They are established with pairwise comparison instead of absolute rating to achieve better inter-annotator agreement. Current metrics are found in poor correlation with human annotations on these datasets. To overcome their limitations, we propose a metric named FENSE, where we combine the strength of Sentence-BERT in capturing similarity, and a novel Error Detector to penalize erroneous sentences for robustness. On the newly established benchmarks, FENSE outperforms current metrics by 14-25% accuracy. <sup xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">1</sup>",
      "doi": "https://doi.org/10.1109/icassp43922.2022.9746427",
      "openalex_id": "https://openalex.org/W3205860970",
      "arxiv_id": "",
      "publication_date": "2022-04-27",
      "published": "2022-04-27",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "此題聚焦音頻字幕評估指標，未以 Spoken Language Models / Full-Duplex SLMs 為核心建模對象，亦無涉及同步互動、語音重疊處理或語音語言建模觀點，因此不符合包含條件，應予以排除。",
      "evaluation": 2
    },
    "round-A_JuniorNano_reasoning": "此題聚焦音頻字幕評估指標，未以 Spoken Language Models / Full-Duplex SLMs 為核心建模對象，亦無涉及同步互動、語音重疊處理或語音語言建模觀點，因此不符合包含條件，應予以排除。",
    "round-A_JuniorNano_evaluation": 2,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on evaluating audio captioning metrics and does not discuss Spoken Language Models, Full-Duplex Spoken Language Models, or speech-language modeling architectures involving end-to-end or mixed modalities. It also does not address synchronous interactive dialogue behaviors, cognitive parallelism, or low-latency real-time speech interaction relevant to the inclusion criteria. Hence, it lacks the core thematic elements required and is unrelated to speech language modeling or synchronous speech interaction.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on evaluating audio captioning metrics and does not discuss Spoken Language Models, Full-Duplex Spoken Language Models, or speech-language modeling architectures involving end-to-end or mixed modalities. It also does not address synchronous interactive dialogue behaviors, cognitive parallelism, or low-latency real-time speech interaction relevant to the inclusion criteria. Hence, it lacks the core thematic elements required and is unrelated to speech language modeling or synchronous speech interaction.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:2)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Musical genre classification of audio signals",
    "abstract": "Musical genres are categorical labels created by humans to characterize pieces of music. A musical genre is characterized by the common characteristics shared by its members. These characteristics typically are related to the instrumentation, rhythmic structure, and harmonic content of the music. Genre hierarchies are commonly used to structure the large collections of music available on the Web. Currently musical genre annotation is performed manually. Automatic musical genre classification can assist or replace the human user in this process and would be a valuable addition to music information retrieval systems. In addition, automatic musical genre classification provides a framework for developing and evaluating features for any type of content-based analysis of musical signals. In this paper, the automatic classification of audio signals into an hierarchy of musical genres is explored. More specifically, three feature sets for representing timbral texture, rhythmic content and pitch content are proposed. The performance and relative importance of the proposed features is investigated by training statistical pattern recognition classifiers using real-world audio collections. Both whole file and real-time frame-based classification schemes are described. Using the proposed feature sets, classification of 61% for ten musical genres is achieved. This result is comparable to results reported for human musical genre classification.",
    "metadata": {
      "title": "Musical genre classification of audio signals",
      "summary": "Musical genres are categorical labels created by humans to characterize pieces of music. A musical genre is characterized by the common characteristics shared by its members. These characteristics typically are related to the instrumentation, rhythmic structure, and harmonic content of the music. Genre hierarchies are commonly used to structure the large collections of music available on the Web. Currently musical genre annotation is performed manually. Automatic musical genre classification can assist or replace the human user in this process and would be a valuable addition to music information retrieval systems. In addition, automatic musical genre classification provides a framework for developing and evaluating features for any type of content-based analysis of musical signals. In this paper, the automatic classification of audio signals into an hierarchy of musical genres is explored. More specifically, three feature sets for representing timbral texture, rhythmic content and pitch content are proposed. The performance and relative importance of the proposed features is investigated by training statistical pattern recognition classifiers using real-world audio collections. Both whole file and real-time frame-based classification schemes are described. Using the proposed feature sets, classification of 61% for ten musical genres is achieved. This result is comparable to results reported for human musical genre classification.",
      "abstract": "Musical genres are categorical labels created by humans to characterize pieces of music. A musical genre is characterized by the common characteristics shared by its members. These characteristics typically are related to the instrumentation, rhythmic structure, and harmonic content of the music. Genre hierarchies are commonly used to structure the large collections of music available on the Web. Currently musical genre annotation is performed manually. Automatic musical genre classification can assist or replace the human user in this process and would be a valuable addition to music information retrieval systems. In addition, automatic musical genre classification provides a framework for developing and evaluating features for any type of content-based analysis of musical signals. In this paper, the automatic classification of audio signals into an hierarchy of musical genres is explored. More specifically, three feature sets for representing timbral texture, rhythmic content and pitch content are proposed. The performance and relative importance of the proposed features is investigated by training statistical pattern recognition classifiers using real-world audio collections. Both whole file and real-time frame-based classification schemes are described. Using the proposed feature sets, classification of 61% for ten musical genres is achieved. This result is comparable to results reported for human musical genre classification.",
      "doi": "https://doi.org/10.1109/tsa.2002.800560",
      "openalex_id": "https://openalex.org/W2133824856",
      "arxiv_id": "",
      "publication_date": "2002-07-01",
      "published": "2002-07-01",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This item focuses on automatic musical genre classification of audio signals and does not center on Spoken Language Models or speech-language modeling, thus fails the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "This item focuses on automatic musical genre classification of audio signals and does not center on Spoken Language Models or speech-language modeling, thus fails the inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper focuses on automatic musical genre classification of audio signals, which relates to music information retrieval and audio feature extraction; it does not address spoken language models, speech language modeling, full-duplex interaction, or related concepts specified in the inclusion criteria. Furthermore, it does not discuss model architectures or training and evaluation aspects relevant to spoken language models. Hence, it does not meet the inclusion criteria and falls outside the topic scope.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper focuses on automatic musical genre classification of audio signals, which relates to music information retrieval and audio feature extraction; it does not address spoken language models, speech language modeling, full-duplex interaction, or related concepts specified in the inclusion criteria. Furthermore, it does not discuss model architectures or training and evaluation aspects relevant to spoken language models. Hence, it does not meet the inclusion criteria and falls outside the topic scope.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  }
]