doi,title,abstract,included,decision_source,final_verdict,landing_url,pdf_url,year,source_anchor,source_term,metadata_json,arxiv_id,openalex_id
10.1145/3534678.3539209,Duplex Conversation: Towards Human-like Interaction in Spoken Dialogue Systems,"In this paper, we present Duplex Conversation, a multi-turn, multimodal spoken dialogue system that enables telephone-based agents to interact with customers like a human. We use the concept of full-duplex in telecommunication to demonstrate what a human-like interactive experience should be and how to achieve smooth turn-taking through three subtasks: user state detection, backchannel selection, and barge-in detection. Besides, we propose semi-supervised learning with multimodal data augmentation to leverage unlabeled data to increase model generalization. Experimental results on three sub-tasks show that the proposed method achieves consistent improvements compared with baselines. We deploy the Duplex Conversation to Alibaba intelligent customer service and share lessons learned in production. Online A/B experiments show that the proposed system can significantly reduce response latency by 50%.",1,,include (junior:4),https://arxiv.org/abs/2205.15060v4,https://arxiv.org/pdf/2205.15060v4,2022,turn-taking,true full-duplex,"{""arxiv_id"": ""2205.15060"", ""title"": ""Duplex Conversation: Towards Human-like Interaction in Spoken Dialogue Systems"", ""summary"": ""In this paper, we present Duplex Conversation, a multi-turn, multimodal spoken dialogue system that enables telephone-based agents to interact with customers like a human. We use the concept of full-duplex in telecommunication to demonstrate what a human-like interactive experience should be and how to achieve smooth turn-taking through three subtasks: user state detection, backchannel selection, and barge-in detection. Besides, we propose semi-supervised learning with multimodal data augmentation to leverage unlabeled data to increase model generalization. Experimental results on three sub-tasks show that the proposed method achieves consistent improvements compared with baselines. We deploy the Duplex Conversation to Alibaba intelligent customer service and share lessons learned in production. Online A/B experiments show that the proposed system can significantly reduce response latency by 50%."", ""authors"": [""Ting-En Lin"", ""Yuchuan Wu"", ""Fei Huang"", ""Luo Si"", ""Jian Sun"", ""Yongbin Li""], ""published"": ""2022-05-30T12:41:23Z"", ""updated"": ""2022-06-14T10:45:56Z"", ""categories"": [""cs.CL""], ""pdf_url"": ""https://arxiv.org/pdf/2205.15060v4"", ""landing_url"": ""https://arxiv.org/abs/2205.15060v4"", ""doi"": ""https://doi.org/10.1145/3534678.3539209""}",2205.1506,https://openalex.org/W4282030117
10.48550/arxiv.2209.15483,Augmentation Invariant Discrete Representation for Generative Spoken Language Modeling,"Generative Spoken Language Modeling research focuses on optimizing speech Language Models (LMs) using raw audio recordings without accessing any textual supervision. Such speech LMs usually operate over discrete units obtained from quantizing internal representations of self-supervised models. Although such units show impressive modeling results, their robustness capabilities have not been extensively investigated. This work focuses on improving the robustness of discrete input representations for generative spoken language modeling. First, we formally define how to measure the robustness of such representations to various signal variations that do not alter the spoken information (e.g., time-stretch). Next, we empirically demonstrate how current state-of-the-art representation models lack robustness to such variations. To overcome this, we propose an effective and efficient method to learn robust discrete speech representation for generative spoken language modeling. The proposed approach is based on applying a set of signal transformations to the speech signal and optimizing the model using an iterative pseudo-labeling scheme. Our method significantly improves over the evaluated baselines when considering encoding and modeling metrics. We additionally evaluate our method on the speech-to-speech translation task, considering Spanish-English and French-English translations, and show the proposed approach outperforms the evaluated baselines.",1,,include (senior:5),https://arxiv.org/abs/2209.15483v2,https://arxiv.org/pdf/2209.15483v2,2022,spoken language models,true full-duplex,"{""arxiv_id"": ""2209.15483"", ""title"": ""Augmentation Invariant Discrete Representation for Generative Spoken Language Modeling"", ""summary"": ""Generative Spoken Language Modeling research focuses on optimizing speech Language Models (LMs) using raw audio recordings without accessing any textual supervision. Such speech LMs usually operate over discrete units obtained from quantizing internal representations of self-supervised models. Although such units show impressive modeling results, their robustness capabilities have not been extensively investigated. This work focuses on improving the robustness of discrete input representations for generative spoken language modeling. First, we formally define how to measure the robustness of such representations to various signal variations that do not alter the spoken information (e.g., time-stretch). Next, we empirically demonstrate how current state-of-the-art representation models lack robustness to such variations. To overcome this, we propose an effective and efficient method to learn robust discrete speech representation for generative spoken language modeling. The proposed approach is based on applying a set of signal transformations to the speech signal and optimizing the model using an iterative pseudo-labeling scheme. Our method significantly improves over the evaluated baselines when considering encoding and modeling metrics. We additionally evaluate our method on the speech-to-speech translation task, considering Spanish-English and French-English translations, and show the proposed approach outperforms the evaluated baselines."", ""authors"": [""Itai Gat"", ""Felix Kreuk"", ""Tu Anh Nguyen"", ""Ann Lee"", ""Jade Copet"", ""Gabriel Synnaeve"", ""Emmanuel Dupoux"", ""Yossi Adi""], ""published"": ""2022-09-30T14:15:03Z"", ""updated"": ""2023-05-29T10:50:29Z"", ""categories"": [""cs.CL"", ""cs.LG"", ""eess.AS""], ""pdf_url"": ""https://arxiv.org/pdf/2209.15483v2"", ""landing_url"": ""https://arxiv.org/abs/2209.15483v2"", ""doi"": ""https://doi.org/10.48550/arXiv.2209.15483""}",2209.15483,https://openalex.org/W4300980462
10.1109/jstsp.2022.3206084,Self-supervised language learning from raw audio: Lessons from the Zero Resource Speech Challenge,"Recent progress in self-supervised or unsupervised machine learning has opened the possibility of building a full speech processing system from raw audio without using any textual representations or expert labels such as phonemes, dictionaries or parse trees. The contribution of the Zero Resource Speech Challenge series since 2015 has been to break down this long-term objective into four well-defined tasks -- Acoustic Unit Discovery, Spoken Term Discovery, Discrete Resynthesis, and Spoken Language Modeling -- and introduce associated metrics and benchmarks enabling model comparison and cumulative progress. We present an overview of the six editions of this challenge series since 2015, discuss the lessons learned, and outline the areas which need more work or give puzzling results.",1,,include (senior:4),https://arxiv.org/abs/2210.15759v1,https://arxiv.org/pdf/2210.15759v1,2022,spoken language models,true full-duplex,"{""arxiv_id"": ""2210.15759"", ""title"": ""Self-supervised language learning from raw audio: Lessons from the Zero Resource Speech Challenge"", ""summary"": ""Recent progress in self-supervised or unsupervised machine learning has opened the possibility of building a full speech processing system from raw audio without using any textual representations or expert labels such as phonemes, dictionaries or parse trees. The contribution of the Zero Resource Speech Challenge series since 2015 has been to break down this long-term objective into four well-defined tasks -- Acoustic Unit Discovery, Spoken Term Discovery, Discrete Resynthesis, and Spoken Language Modeling -- and introduce associated metrics and benchmarks enabling model comparison and cumulative progress. We present an overview of the six editions of this challenge series since 2015, discuss the lessons learned, and outline the areas which need more work or give puzzling results."", ""authors"": [""Ewan Dunbar"", ""Nicolas Hamilakis"", ""Emmanuel Dupoux""], ""published"": ""2022-10-27T20:32:41Z"", ""updated"": ""2022-10-27T20:32:41Z"", ""categories"": [""cs.CL"", ""cs.SD"", ""eess.AS""], ""pdf_url"": ""https://arxiv.org/pdf/2210.15759v1"", ""landing_url"": ""https://arxiv.org/abs/2210.15759v1"", ""doi"": ""https://doi.org/10.1109/JSTSP.2022.3206084""}",2210.15759,https://openalex.org/W4295308567
10.1109/icassp49357.2023.10097097,Analysing Discrete Self Supervised Speech Representation for Spoken Language Modeling,"This work profoundly analyzes discrete self-supervised speech representations (units) through the eyes of Generative Spoken Language Modeling (GSLM). Following the findings of such an analysis, we propose practical improvements to the discrete unit for the GSLM. First, we start comprehending these units by analyzing them in three axes: interpretation, visualization, and resynthesis. Our analysis finds a high correlation between the speech units to phonemes and phoneme families, while their correlation with speaker or gender is weaker. Additionally, we found redundancies in the extracted units and claim that one reason may be the units' context. Following this analysis, we propose a new, unsupervised metric to measure unit redundancies. Finally, we use this metric to develop new methods that improve the robustness of units' clustering and show significant improvement considering zero-resource speech metrics such as ABX. Code and analysis tools are available under the following link: https://github.com/slp-rl/SLM-Discrete-Representations",1,,include (junior:4),https://arxiv.org/abs/2301.00591v3,https://arxiv.org/pdf/2301.00591v3,2023,spoken language models,true full-duplex,"{""arxiv_id"": ""2301.00591"", ""title"": ""Analysing Discrete Self Supervised Speech Representation for Spoken Language Modeling"", ""summary"": ""This work profoundly analyzes discrete self-supervised speech representations (units) through the eyes of Generative Spoken Language Modeling (GSLM). Following the findings of such an analysis, we propose practical improvements to the discrete unit for the GSLM. First, we start comprehending these units by analyzing them in three axes: interpretation, visualization, and resynthesis. Our analysis finds a high correlation between the speech units to phonemes and phoneme families, while their correlation with speaker or gender is weaker. Additionally, we found redundancies in the extracted units and claim that one reason may be the units' context. Following this analysis, we propose a new, unsupervised metric to measure unit redundancies. Finally, we use this metric to develop new methods that improve the robustness of units' clustering and show significant improvement considering zero-resource speech metrics such as ABX. Code and analysis tools are available under the following link: https://github.com/slp-rl/SLM-Discrete-Representations"", ""authors"": [""Amitay Sicherman"", ""Yossi Adi""], ""published"": ""2023-01-02T10:36:40Z"", ""updated"": ""2023-03-01T09:59:54Z"", ""categories"": [""cs.CL"", ""cs.SD"", ""eess.AS""], ""pdf_url"": ""https://arxiv.org/pdf/2301.00591v3"", ""landing_url"": ""https://arxiv.org/abs/2301.00591v3"", ""doi"": ""https://doi.org/10.1109/ICASSP49357.2023.10097097""}",2301.00591,https://openalex.org/W4375868953
10.48550/arxiv.2303.03926,Speak Foreign Languages with Your Own Voice: Cross-Lingual Neural Codec Language Modeling,"We propose a cross-lingual neural codec language model, VALL-E X, for cross-lingual speech synthesis. Specifically, we extend VALL-E and train a multi-lingual conditional codec language model to predict the acoustic token sequences of the target language speech by using both the source language speech and the target language text as prompts. VALL-E X inherits strong in-context learning capabilities and can be applied for zero-shot cross-lingual text-to-speech synthesis and zero-shot speech-to-speech translation tasks. Experimental results show that it can generate high-quality speech in the target language via just one speech utterance in the source language as a prompt while preserving the unseen speaker's voice, emotion, and acoustic environment. Moreover, VALL-E X effectively alleviates the foreign accent problems, which can be controlled by a language ID. Audio samples are available at \url{https://aka.ms/vallex}.",1,,include (senior:4),https://arxiv.org/abs/2303.03926v1,https://arxiv.org/pdf/2303.03926v1,2023,spoken language models,neural codec,"{""arxiv_id"": ""2303.03926"", ""title"": ""Speak Foreign Languages with Your Own Voice: Cross-Lingual Neural Codec Language Modeling"", ""summary"": ""We propose a cross-lingual neural codec language model, VALL-E X, for cross-lingual speech synthesis. Specifically, we extend VALL-E and train a multi-lingual conditional codec language model to predict the acoustic token sequences of the target language speech by using both the source language speech and the target language text as prompts. VALL-E X inherits strong in-context learning capabilities and can be applied for zero-shot cross-lingual text-to-speech synthesis and zero-shot speech-to-speech translation tasks. Experimental results show that it can generate high-quality speech in the target language via just one speech utterance in the source language as a prompt while preserving the unseen speaker's voice, emotion, and acoustic environment. Moreover, VALL-E X effectively alleviates the foreign accent problems, which can be controlled by a language ID. Audio samples are available at \\url{https://aka.ms/vallex}."", ""authors"": [""Ziqiang Zhang"", ""Long Zhou"", ""Chengyi Wang"", ""Sanyuan Chen"", ""Yu Wu"", ""Shujie Liu"", ""Zhuo Chen"", ""Yanqing Liu"", ""Huaming Wang"", ""Jinyu Li"", ""Lei He"", ""Sheng Zhao"", ""Furu Wei""], ""published"": ""2023-03-07T14:31:55Z"", ""updated"": ""2023-03-07T14:31:55Z"", ""categories"": [""cs.CL"", ""cs.AI"", ""cs.SD"", ""eess.AS""], ""pdf_url"": ""https://arxiv.org/pdf/2303.03926v1"", ""landing_url"": ""https://arxiv.org/abs/2303.03926v1"", ""doi"": ""https://doi.org/10.48550/arXiv.2303.03926""}",2303.03926,https://openalex.org/W4323651091
10.48550/arxiv.2305.15255,Spoken Question Answering and Speech Continuation Using Spectrogram-Powered LLM,"We present Spectron, a novel approach to adapting pre-trained large language models (LLMs) to perform spoken question answering (QA) and speech continuation. By endowing the LLM with a pre-trained speech encoder, our model becomes able to take speech inputs and generate speech outputs. The entire system is trained end-to-end and operates directly on spectrograms, simplifying our architecture. Key to our approach is a training objective that jointly supervises speech recognition, text continuation, and speech synthesis using only paired speech-text pairs, enabling a `cross-modal' chain-of-thought within a single decoding pass. Our method surpasses existing spoken language models in speaker preservation and semantic coherence. Furthermore, the proposed model improves upon direct initialization in retaining the knowledge of the original LLM as demonstrated through spoken QA datasets. We release our audio samples (https://michelleramanovich.github.io/spectron/spectron) and spoken QA dataset (https://github.com/google-research-datasets/LLAMA1-Test-Set).",1,,include (junior:4),https://arxiv.org/abs/2305.15255v4,https://arxiv.org/pdf/2305.15255v4,2023,spoken language models,true full-duplex,"{""arxiv_id"": ""2305.15255"", ""title"": ""Spoken Question Answering and Speech Continuation Using Spectrogram-Powered LLM"", ""summary"": ""We present Spectron, a novel approach to adapting pre-trained large language models (LLMs) to perform spoken question answering (QA) and speech continuation. By endowing the LLM with a pre-trained speech encoder, our model becomes able to take speech inputs and generate speech outputs. The entire system is trained end-to-end and operates directly on spectrograms, simplifying our architecture. Key to our approach is a training objective that jointly supervises speech recognition, text continuation, and speech synthesis using only paired speech-text pairs, enabling a `cross-modal' chain-of-thought within a single decoding pass. Our method surpasses existing spoken language models in speaker preservation and semantic coherence. Furthermore, the proposed model improves upon direct initialization in retaining the knowledge of the original LLM as demonstrated through spoken QA datasets. We release our audio samples (https://michelleramanovich.github.io/spectron/spectron) and spoken QA dataset (https://github.com/google-research-datasets/LLAMA1-Test-Set)."", ""authors"": [""Eliya Nachmani"", ""Alon Levkovitch"", ""Roy Hirsch"", ""Julian Salazar"", ""Chulayuth Asawaroengchai"", ""Soroosh Mariooryad"", ""Ehud Rivlin"", ""RJ Skerry-Ryan"", ""Michelle Tadmor Ramanovich""], ""published"": ""2023-05-24T15:39:43Z"", ""updated"": ""2024-05-31T01:29:27Z"", ""categories"": [""cs.CL"", ""cs.LG"", ""cs.SD"", ""eess.AS""], ""pdf_url"": ""https://arxiv.org/pdf/2305.15255v4"", ""landing_url"": ""https://arxiv.org/abs/2305.15255v4"", ""doi"": ""https://doi.org/10.48550/arXiv.2305.15255""}",2305.15255,https://openalex.org/W4378510408
10.48550/arxiv.2305.16107,"VioLA: Unified Codec Language Models for Speech Recognition, Synthesis, and Translation","Recent research shows a big convergence in model architecture, training objectives, and inference methods across various tasks for different modalities. In this paper, we propose VioLA, a single auto-regressive Transformer decoder-only network that unifies various cross-modal tasks involving speech and text, such as speech-to-text, text-to-text, text-to-speech, and speech-to-speech tasks, as a conditional codec language model task via multi-task learning framework. To accomplish this, we first convert all the speech utterances to discrete tokens (similar to the textual data) using an offline neural codec encoder. In such a way, all these tasks are converted to token-based sequence conversion problems, which can be naturally handled with one conditional language model. We further integrate task IDs (TID) and language IDs (LID) into the proposed model to enhance the modeling capability of handling different languages and tasks. Experimental results demonstrate that the proposed VioLA model can support both single-modal and cross-modal tasks well, and the decoder-only model achieves a comparable and even better performance than the strong baselines.",1,,include (senior:5),https://arxiv.org/abs/2305.16107v1,https://arxiv.org/pdf/2305.16107v1,2023,synchronous dialogue,neural codec,"{""arxiv_id"": ""2305.16107"", ""title"": ""VioLA: Unified Codec Language Models for Speech Recognition, Synthesis, and Translation"", ""summary"": ""Recent research shows a big convergence in model architecture, training objectives, and inference methods across various tasks for different modalities. In this paper, we propose VioLA, a single auto-regressive Transformer decoder-only network that unifies various cross-modal tasks involving speech and text, such as speech-to-text, text-to-text, text-to-speech, and speech-to-speech tasks, as a conditional codec language model task via multi-task learning framework. To accomplish this, we first convert all the speech utterances to discrete tokens (similar to the textual data) using an offline neural codec encoder. In such a way, all these tasks are converted to token-based sequence conversion problems, which can be naturally handled with one conditional language model. We further integrate task IDs (TID) and language IDs (LID) into the proposed model to enhance the modeling capability of handling different languages and tasks. Experimental results demonstrate that the proposed VioLA model can support both single-modal and cross-modal tasks well, and the decoder-only model achieves a comparable and even better performance than the strong baselines."", ""authors"": [""Tianrui Wang"", ""Long Zhou"", ""Ziqiang Zhang"", ""Yu Wu"", ""Shujie Liu"", ""Yashesh Gaur"", ""Zhuo Chen"", ""Jinyu Li"", ""Furu Wei""], ""published"": ""2023-05-25T14:39:47Z"", ""updated"": ""2023-05-25T14:39:47Z"", ""categories"": [""cs.CL"", ""cs.SD"", ""eess.AS""], ""pdf_url"": ""https://arxiv.org/pdf/2305.16107v1"", ""landing_url"": ""https://arxiv.org/abs/2305.16107v1"", ""doi"": ""https://doi.org/10.48550/arXiv.2305.16107""}",2305.16107,https://openalex.org/W4378501656
10.21437/interspeech.2023-679,Wave to Syntax: Probing spoken language models for syntax,"Understanding which information is encoded in deep models of spoken and written language has been the focus of much research in recent years, as it is crucial for debugging and improving these architectures. Most previous work has focused on probing for speaker characteristics, acoustic and phonological information in models of spoken language, and for syntactic information in models of written language. Here we focus on the encoding of syntax in several self-supervised and visually grounded models of spoken language. We employ two complementary probing methods, combined with baselines and reference representations to quantify the degree to which syntactic structure is encoded in the activations of the target models. We show that syntax is captured most prominently in the middle layers of the networks, and more explicitly within models with more parameters.",1,,include (senior:5),https://arxiv.org/abs/2305.18957v1,https://arxiv.org/pdf/2305.18957v1,2023,spoken language models,true full-duplex,"{""arxiv_id"": ""2305.18957"", ""title"": ""Wave to Syntax: Probing spoken language models for syntax"", ""summary"": ""Understanding which information is encoded in deep models of spoken and written language has been the focus of much research in recent years, as it is crucial for debugging and improving these architectures. Most previous work has focused on probing for speaker characteristics, acoustic and phonological information in models of spoken language, and for syntactic information in models of written language. Here we focus on the encoding of syntax in several self-supervised and visually grounded models of spoken language. We employ two complementary probing methods, combined with baselines and reference representations to quantify the degree to which syntactic structure is encoded in the activations of the target models. We show that syntax is captured most prominently in the middle layers of the networks, and more explicitly within models with more parameters."", ""authors"": [""Gaofei Shen"", ""Afra Alishahi"", ""Arianna Bisazza"", ""Grzegorz Chrupała""], ""published"": ""2023-05-30T11:43:18Z"", ""updated"": ""2023-05-30T11:43:18Z"", ""categories"": [""cs.CL""], ""pdf_url"": ""https://arxiv.org/pdf/2305.18957v1"", ""landing_url"": ""https://arxiv.org/abs/2305.18957v1"", ""doi"": ""https://doi.org/10.21437/Interspeech.2023-679""}",2305.18957,https://openalex.org/W4385823328
10.48550/arxiv.2306.00697,How Generative Spoken Language Modeling Encodes Noisy Speech: Investigation from Phonetics to Syntactics,"We examine the speech modeling potential of generative spoken language modeling (GSLM), which involves using learned symbols derived from data rather than phonemes for speech analysis and synthesis. Since GSLM facilitates textless spoken language processing, exploring its effectiveness is critical for paving the way for novel paradigms in spoken-language processing. This paper presents the findings of GSLM's encoding and decoding effectiveness at the spoken-language and speech levels. Through speech resynthesis experiments, we revealed that resynthesis errors occur at the levels ranging from phonology to syntactics and GSLM frequently resynthesizes natural but content-altered speech.",1,,include (senior:5),https://arxiv.org/abs/2306.00697v1,https://arxiv.org/pdf/2306.00697v1,2023,spoken language models,true full-duplex,"{""arxiv_id"": ""2306.00697"", ""title"": ""How Generative Spoken Language Modeling Encodes Noisy Speech: Investigation from Phonetics to Syntactics"", ""summary"": ""We examine the speech modeling potential of generative spoken language modeling (GSLM), which involves using learned symbols derived from data rather than phonemes for speech analysis and synthesis. Since GSLM facilitates textless spoken language processing, exploring its effectiveness is critical for paving the way for novel paradigms in spoken-language processing. This paper presents the findings of GSLM's encoding and decoding effectiveness at the spoken-language and speech levels. Through speech resynthesis experiments, we revealed that resynthesis errors occur at the levels ranging from phonology to syntactics and GSLM frequently resynthesizes natural but content-altered speech."", ""authors"": [""Joonyong Park"", ""Shinnosuke Takamichi"", ""Tomohiko Nakamura"", ""Kentaro Seki"", ""Detai Xin"", ""Hiroshi Saruwatari""], ""published"": ""2023-06-01T14:07:19Z"", ""updated"": ""2023-06-01T14:07:19Z"", ""categories"": [""cs.CL"", ""cs.AI"", ""eess.AS""], ""pdf_url"": ""https://arxiv.org/pdf/2306.00697v1"", ""landing_url"": ""https://arxiv.org/abs/2306.00697v1"", ""doi"": ""https://doi.org/10.48550/arXiv.2306.00697""}",2306.00697,https://openalex.org/W4379258843
10.21437/interspeech.2023-978,BabySLM: language-acquisition-friendly benchmark of self-supervised spoken language models,"Self-supervised techniques for learning speech representations have been shown to develop linguistic competence from exposure to speech without the need for human labels. In order to fully realize the potential of these approaches and further our understanding of how infants learn language, simulations must closely emulate real-life situations by training on developmentally plausible corpora and benchmarking against appropriate test sets. To this end, we propose a language-acquisition-friendly benchmark to probe spoken language models at the lexical and syntactic levels, both of which are compatible with the vocabulary typical of children's language experiences. This paper introduces the benchmark and summarizes a range of experiments showing its usefulness. In addition, we highlight two exciting challenges that need to be addressed for further progress: bridging the gap between text and speech and between clean speech and in-the-wild speech.",1,,include (senior:4),https://arxiv.org/abs/2306.01506v2,https://arxiv.org/pdf/2306.01506v2,2023,spoken language models,true full-duplex,"{""arxiv_id"": ""2306.01506"", ""title"": ""BabySLM: language-acquisition-friendly benchmark of self-supervised spoken language models"", ""summary"": ""Self-supervised techniques for learning speech representations have been shown to develop linguistic competence from exposure to speech without the need for human labels. In order to fully realize the potential of these approaches and further our understanding of how infants learn language, simulations must closely emulate real-life situations by training on developmentally plausible corpora and benchmarking against appropriate test sets. To this end, we propose a language-acquisition-friendly benchmark to probe spoken language models at the lexical and syntactic levels, both of which are compatible with the vocabulary typical of children's language experiences. This paper introduces the benchmark and summarizes a range of experiments showing its usefulness. In addition, we highlight two exciting challenges that need to be addressed for further progress: bridging the gap between text and speech and between clean speech and in-the-wild speech."", ""authors"": [""Marvin Lavechin"", ""Yaya Sy"", ""Hadrien Titeux"", ""María Andrea Cruz Blandón"", ""Okko Räsänen"", ""Hervé Bredin"", ""Emmanuel Dupoux"", ""Alejandrina Cristia""], ""published"": ""2023-06-02T12:54:38Z"", ""updated"": ""2023-06-08T12:22:30Z"", ""categories"": [""cs.CL"", ""eess.AS"", ""stat.ML""], ""pdf_url"": ""https://arxiv.org/pdf/2306.01506v2"", ""landing_url"": ""https://arxiv.org/abs/2306.01506v2"", ""doi"": ""https://doi.org/10.21437/Interspeech.2023-978""}",2306.01506,https://openalex.org/W4385822936
10.48550/arxiv.2308.06873,SpeechX: Neural Codec Language Model as a Versatile Speech Transformer,"Recent advancements in generative speech models based on audio-text prompts have enabled remarkable innovations like high-quality zero-shot text-to-speech. However, existing models still face limitations in handling diverse audio-text speech generation tasks involving transforming input speech and processing audio captured in adverse acoustic conditions. This paper introduces SpeechX, a versatile speech generation model capable of zero-shot TTS and various speech transformation tasks, dealing with both clean and noisy signals. SpeechX combines neural codec language modeling with multi-task learning using task-dependent prompting, enabling unified and extensible modeling and providing a consistent way for leveraging textual input in speech enhancement and transformation tasks. Experimental results show SpeechX's efficacy in various tasks, including zero-shot TTS, noise suppression, target speaker extraction, speech removal, and speech editing with or without background noise, achieving comparable or superior performance to specialized models across tasks. See https://aka.ms/speechx for demo samples.",1,,include (senior:5),https://arxiv.org/abs/2308.06873v2,https://arxiv.org/pdf/2308.06873v2,2023,spoken language models,neural codec,"{""arxiv_id"": ""2308.06873"", ""title"": ""SpeechX: Neural Codec Language Model as a Versatile Speech Transformer"", ""summary"": ""Recent advancements in generative speech models based on audio-text prompts have enabled remarkable innovations like high-quality zero-shot text-to-speech. However, existing models still face limitations in handling diverse audio-text speech generation tasks involving transforming input speech and processing audio captured in adverse acoustic conditions. This paper introduces SpeechX, a versatile speech generation model capable of zero-shot TTS and various speech transformation tasks, dealing with both clean and noisy signals. SpeechX combines neural codec language modeling with multi-task learning using task-dependent prompting, enabling unified and extensible modeling and providing a consistent way for leveraging textual input in speech enhancement and transformation tasks. Experimental results show SpeechX's efficacy in various tasks, including zero-shot TTS, noise suppression, target speaker extraction, speech removal, and speech editing with or without background noise, achieving comparable or superior performance to specialized models across tasks. See https://aka.ms/speechx for demo samples."", ""authors"": [""Xiaofei Wang"", ""Manthan Thakker"", ""Zhuo Chen"", ""Naoyuki Kanda"", ""Sefik Emre Eskimez"", ""Sanyuan Chen"", ""Min Tang"", ""Shujie Liu"", ""Jinyu Li"", ""Takuya Yoshioka""], ""published"": ""2023-08-14T01:01:19Z"", ""updated"": ""2024-06-25T18:38:28Z"", ""categories"": [""eess.AS"", ""cs.CL"", ""cs.LG"", ""cs.SD""], ""pdf_url"": ""https://arxiv.org/pdf/2308.06873v2"", ""landing_url"": ""https://arxiv.org/abs/2308.06873v2"", ""doi"": ""https://doi.org/10.48550/arXiv.2308.06873""}",2308.06873,https://openalex.org/W4385848954
10.48550/arxiv.2309.08531,Towards Practical and Efficient Image-to-Speech Captioning with Vision-Language Pre-training and Multi-modal Tokens,"In this paper, we propose methods to build a powerful and efficient Image-to-Speech captioning (Im2Sp) model. To this end, we start with importing the rich knowledge related to image comprehension and language modeling from a large-scale pre-trained vision-language model into Im2Sp. We set the output of the proposed Im2Sp as discretized speech units, i.e., the quantized speech features of a self-supervised speech model. The speech units mainly contain linguistic information while suppressing other characteristics of speech. This allows us to incorporate the language modeling capability of the pre-trained vision-language model into the spoken language modeling of Im2Sp. With the vision-language pre-training strategy, we set new state-of-the-art Im2Sp performances on two widely used benchmark databases, COCO and Flickr8k. Then, we further improve the efficiency of the Im2Sp model. Similar to the speech unit case, we convert the original image into image units, which are derived through vector quantization of the raw image. With these image units, we can drastically reduce the required data storage for saving image data to just 0.8% when compared to the original image data in terms of bits. Demo page: https://ms-dot-k.github.io/Image-to-Speech-Captioning.",1,,include (senior:5),https://arxiv.org/abs/2309.08531v1,https://arxiv.org/pdf/2309.08531v1,2023,spoken language models,true full-duplex,"{""arxiv_id"": ""2309.08531"", ""title"": ""Towards Practical and Efficient Image-to-Speech Captioning with Vision-Language Pre-training and Multi-modal Tokens"", ""summary"": ""In this paper, we propose methods to build a powerful and efficient Image-to-Speech captioning (Im2Sp) model. To this end, we start with importing the rich knowledge related to image comprehension and language modeling from a large-scale pre-trained vision-language model into Im2Sp. We set the output of the proposed Im2Sp as discretized speech units, i.e., the quantized speech features of a self-supervised speech model. The speech units mainly contain linguistic information while suppressing other characteristics of speech. This allows us to incorporate the language modeling capability of the pre-trained vision-language model into the spoken language modeling of Im2Sp. With the vision-language pre-training strategy, we set new state-of-the-art Im2Sp performances on two widely used benchmark databases, COCO and Flickr8k. Then, we further improve the efficiency of the Im2Sp model. Similar to the speech unit case, we convert the original image into image units, which are derived through vector quantization of the raw image. With these image units, we can drastically reduce the required data storage for saving image data to just 0.8% when compared to the original image data in terms of bits. Demo page: https://ms-dot-k.github.io/Image-to-Speech-Captioning."", ""authors"": [""Minsu Kim"", ""Jeongsoo Choi"", ""Soumi Maiti"", ""Jeong Hun Yeo"", ""Shinji Watanabe"", ""Yong Man Ro""], ""published"": ""2023-09-15T16:48:34Z"", ""updated"": ""2023-09-15T16:48:34Z"", ""categories"": [""cs.CV"", ""cs.CL"", ""eess.AS"", ""eess.IV""], ""pdf_url"": ""https://arxiv.org/pdf/2309.08531v1"", ""landing_url"": ""https://arxiv.org/abs/2309.08531v1"", ""doi"": ""https://doi.org/10.48550/arXiv.2309.08531""}",2309.08531,https://openalex.org/W4386839802
10.48550/arxiv.2310.04673,"LauraGPT: Listen, Attend, Understand, and Regenerate Audio with GPT","Generative Pre-trained Transformer (GPT) models have achieved remarkable performance on various natural language processing tasks, and have shown great potential as backbones for audio-and-text large language models (LLMs). Previous mainstream audio-and-text LLMs use discrete audio tokens to represent both input and output audio; however, they suffer from performance degradation on tasks such as automatic speech recognition, speech-to-text translation, and speech enhancement over models using continuous speech features. In this paper, we propose LauraGPT, a novel unified audio-and-text GPT-based LLM for audio recognition, understanding, and generation. LauraGPT is a versatile LLM that can process both audio and text inputs and generate outputs in either modalities. We propose a novel data representation that combines continuous and discrete features for audio: LauraGPT encodes input audio into continuous representations using an audio encoder and generates output audio from discrete codec codes. We propose a one-step codec vocoder to overcome the prediction challenge caused by the multimodal distribution of codec tokens. We fine-tune LauraGPT using supervised multi-task learning. Extensive experiments show that LauraGPT consistently achieves comparable to superior performance compared to strong baselines on a wide range of audio tasks related to content, semantics, paralinguistics, and audio-signal analysis, such as automatic speech recognition, speech-to-text translation, text-to-speech synthesis, speech enhancement, automated audio captioning, speech emotion recognition, and spoken language understanding.",1,,include (senior:4),https://arxiv.org/abs/2310.04673v4,https://arxiv.org/pdf/2310.04673v4,2023,synchronous dialogue,audio tokens,"{""arxiv_id"": ""2310.04673"", ""title"": ""LauraGPT: Listen, Attend, Understand, and Regenerate Audio with GPT"", ""summary"": ""Generative Pre-trained Transformer (GPT) models have achieved remarkable performance on various natural language processing tasks, and have shown great potential as backbones for audio-and-text large language models (LLMs). Previous mainstream audio-and-text LLMs use discrete audio tokens to represent both input and output audio; however, they suffer from performance degradation on tasks such as automatic speech recognition, speech-to-text translation, and speech enhancement over models using continuous speech features. In this paper, we propose LauraGPT, a novel unified audio-and-text GPT-based LLM for audio recognition, understanding, and generation. LauraGPT is a versatile LLM that can process both audio and text inputs and generate outputs in either modalities. We propose a novel data representation that combines continuous and discrete features for audio: LauraGPT encodes input audio into continuous representations using an audio encoder and generates output audio from discrete codec codes. We propose a one-step codec vocoder to overcome the prediction challenge caused by the multimodal distribution of codec tokens. We fine-tune LauraGPT using supervised multi-task learning. Extensive experiments show that LauraGPT consistently achieves comparable to superior performance compared to strong baselines on a wide range of audio tasks related to content, semantics, paralinguistics, and audio-signal analysis, such as automatic speech recognition, speech-to-text translation, text-to-speech synthesis, speech enhancement, automated audio captioning, speech emotion recognition, and spoken language understanding."", ""authors"": [""Zhihao Du"", ""Jiaming Wang"", ""Qian Chen"", ""Yunfei Chu"", ""Zhifu Gao"", ""Zerui Li"", ""Kai Hu"", ""Xiaohuan Zhou"", ""Jin Xu"", ""Ziyang Ma"", ""Wen Wang"", ""Siqi Zheng"", ""Chang Zhou"", ""Zhijie Yan"", ""Shiliang Zhang""], ""published"": ""2023-10-07T03:17:59Z"", ""updated"": ""2024-07-03T02:38:03Z"", ""categories"": [""cs.SD"", ""cs.AI"", ""cs.LG"", ""cs.MM"", ""eess.AS""], ""pdf_url"": ""https://arxiv.org/pdf/2310.04673v4"", ""landing_url"": ""https://arxiv.org/abs/2310.04673v4"", ""doi"": ""https://doi.org/10.48550/arXiv.2310.04673""}",2310.04673,https://openalex.org/W4387595589
10.48550/arxiv.2310.05224,Generative Spoken Language Model based on continuous word-sized audio tokens,"In NLP, text language models based on words or subwords are known to outperform their character-based counterparts. Yet, in the speech community, the standard input of spoken LMs are 20ms or 40ms-long discrete units (shorter than a phoneme). Taking inspiration from word-based LM, we introduce a Generative Spoken Language Model (GSLM) based on word-size continuous-valued audio embeddings that can generate diverse and expressive language output. This is obtained by replacing lookup table for lexical types with a Lexical Embedding function, the cross entropy loss by a contrastive loss, and multinomial sampling by k-NN sampling. The resulting model is the first generative language model based on word-size continuous embeddings. Its performance is on par with discrete unit GSLMs regarding generation quality as measured by automatic metrics and subjective human judgements. Moreover, it is five times more memory efficient thanks to its large 200ms units. In addition, the embeddings before and after the Lexical Embedder are phonetically and semantically interpretable.",1,,include (senior:5),https://arxiv.org/abs/2310.05224v1,https://arxiv.org/pdf/2310.05224v1,2023,spoken language models,true full-duplex,"{""arxiv_id"": ""2310.05224"", ""title"": ""Generative Spoken Language Model based on continuous word-sized audio tokens"", ""summary"": ""In NLP, text language models based on words or subwords are known to outperform their character-based counterparts. Yet, in the speech community, the standard input of spoken LMs are 20ms or 40ms-long discrete units (shorter than a phoneme). Taking inspiration from word-based LM, we introduce a Generative Spoken Language Model (GSLM) based on word-size continuous-valued audio embeddings that can generate diverse and expressive language output. This is obtained by replacing lookup table for lexical types with a Lexical Embedding function, the cross entropy loss by a contrastive loss, and multinomial sampling by k-NN sampling. The resulting model is the first generative language model based on word-size continuous embeddings. Its performance is on par with discrete unit GSLMs regarding generation quality as measured by automatic metrics and subjective human judgements. Moreover, it is five times more memory efficient thanks to its large 200ms units. In addition, the embeddings before and after the Lexical Embedder are phonetically and semantically interpretable."", ""authors"": [""Robin Algayres"", ""Yossi Adi"", ""Tu Anh Nguyen"", ""Jade Copet"", ""Gabriel Synnaeve"", ""Benoit Sagot"", ""Emmanuel Dupoux""], ""published"": ""2023-10-08T16:46:14Z"", ""updated"": ""2023-10-08T16:46:14Z"", ""categories"": [""cs.CL"", ""cs.LG""], ""pdf_url"": ""https://arxiv.org/pdf/2310.05224v1"", ""landing_url"": ""https://arxiv.org/abs/2310.05224v1"", ""doi"": ""https://doi.org/10.48550/arXiv.2310.05224""}",2310.05224,https://openalex.org/W4387559820
10.48550/arxiv.2310.10803,SD-HuBERT: Sentence-Level Self-Distillation Induces Syllabic Organization in HuBERT,"Data-driven unit discovery in self-supervised learning (SSL) of speech has embarked on a new era of spoken language processing. Yet, the discovered units often remain in phonetic space and the units beyond phonemes are largely underexplored. Here, we demonstrate that a syllabic organization emerges in learning sentence-level representation of speech. In particular, we adopt ""self-distillation"" objective to fine-tune the pretrained HuBERT with an aggregator token that summarizes the entire sentence. Without any supervision, the resulting model draws definite boundaries in speech, and the representations across frames exhibit salient syllabic structures. We demonstrate that this emergent structure largely corresponds to the ground truth syllables. Furthermore, we propose a new benchmark task, Spoken Speech ABX, for evaluating sentence-level representation of speech. When compared to previous models, our model outperforms in both unsupervised syllable discovery and learning sentence-level representation. Together, we demonstrate that the self-distillation of HuBERT gives rise to syllabic organization without relying on external labels or modalities, and potentially provides novel data-driven units for spoken language modeling.",1,,include (senior:4),https://arxiv.org/abs/2310.10803v3,https://arxiv.org/pdf/2310.10803v3,2023,spoken language models,true full-duplex,"{""arxiv_id"": ""2310.10803"", ""title"": ""SD-HuBERT: Sentence-Level Self-Distillation Induces Syllabic Organization in HuBERT"", ""summary"": ""Data-driven unit discovery in self-supervised learning (SSL) of speech has embarked on a new era of spoken language processing. Yet, the discovered units often remain in phonetic space and the units beyond phonemes are largely underexplored. Here, we demonstrate that a syllabic organization emerges in learning sentence-level representation of speech. In particular, we adopt \""self-distillation\"" objective to fine-tune the pretrained HuBERT with an aggregator token that summarizes the entire sentence. Without any supervision, the resulting model draws definite boundaries in speech, and the representations across frames exhibit salient syllabic structures. We demonstrate that this emergent structure largely corresponds to the ground truth syllables. Furthermore, we propose a new benchmark task, Spoken Speech ABX, for evaluating sentence-level representation of speech. When compared to previous models, our model outperforms in both unsupervised syllable discovery and learning sentence-level representation. Together, we demonstrate that the self-distillation of HuBERT gives rise to syllabic organization without relying on external labels or modalities, and potentially provides novel data-driven units for spoken language modeling."", ""authors"": [""Cheol Jun Cho"", ""Abdelrahman Mohamed"", ""Shang-Wen Li"", ""Alan W Black"", ""Gopala K. Anumanchipalli""], ""published"": ""2023-10-16T20:05:36Z"", ""updated"": ""2025-04-10T11:20:55Z"", ""categories"": [""cs.CL"", ""eess.AS""], ""pdf_url"": ""https://arxiv.org/pdf/2310.10803v3"", ""landing_url"": ""https://arxiv.org/abs/2310.10803v3"", ""doi"": ""https://doi.org/10.48550/arXiv.2310.10803""}",2310.10803,https://openalex.org/W4387796678
10.48550/arxiv.2401.14717,Turn-taking and Backchannel Prediction with Acoustic and Large Language Model Fusion,"We propose an approach for continuous prediction of turn-taking and backchanneling locations in spoken dialogue by fusing a neural acoustic model with a large language model (LLM). Experiments on the Switchboard human-human conversation dataset demonstrate that our approach consistently outperforms the baseline models with single modality. We also develop a novel multi-task instruction fine-tuning strategy to further benefit from LLM-encoded knowledge for understanding the tasks and conversational contexts, leading to additional improvements. Our approach demonstrates the potential of combined LLMs and acoustic models for a more natural and conversational interaction between humans and speech-enabled AI agents.",1,,include (junior:4),https://arxiv.org/abs/2401.14717v1,https://arxiv.org/pdf/2401.14717v1,2024,turn-taking,true full-duplex,"{""arxiv_id"": ""2401.14717"", ""title"": ""Turn-taking and Backchannel Prediction with Acoustic and Large Language Model Fusion"", ""summary"": ""We propose an approach for continuous prediction of turn-taking and backchanneling locations in spoken dialogue by fusing a neural acoustic model with a large language model (LLM). Experiments on the Switchboard human-human conversation dataset demonstrate that our approach consistently outperforms the baseline models with single modality. We also develop a novel multi-task instruction fine-tuning strategy to further benefit from LLM-encoded knowledge for understanding the tasks and conversational contexts, leading to additional improvements. Our approach demonstrates the potential of combined LLMs and acoustic models for a more natural and conversational interaction between humans and speech-enabled AI agents."", ""authors"": [""Jinhan Wang"", ""Long Chen"", ""Aparna Khare"", ""Anirudh Raju"", ""Pranav Dheram"", ""Di He"", ""Minhua Wu"", ""Andreas Stolcke"", ""Venkatesh Ravichandran""], ""published"": ""2024-01-26T08:59:07Z"", ""updated"": ""2024-01-26T08:59:07Z"", ""categories"": [""cs.CL"", ""cs.AI"", ""cs.LG"", ""cs.SD"", ""eess.AS""], ""pdf_url"": ""https://arxiv.org/pdf/2401.14717v1"", ""landing_url"": ""https://arxiv.org/abs/2401.14717v1"", ""doi"": ""https://doi.org/10.48550/arXiv.2401.14717""}",2401.14717,https://openalex.org/W4391335220
10.48550/arxiv.2403.16865,Encoding of lexical tone in self-supervised models of spoken language,"Interpretability research has shown that self-supervised Spoken Language Models (SLMs) encode a wide variety of features in human speech from the acoustic, phonetic, phonological, syntactic and semantic levels, to speaker characteristics. The bulk of prior research on representations of phonology has focused on segmental features such as phonemes; the encoding of suprasegmental phonology (such as tone and stress patterns) in SLMs is not yet well understood. Tone is a suprasegmental feature that is present in more than half of the world's languages. This paper aims to analyze the tone encoding capabilities of SLMs, using Mandarin and Vietnamese as case studies. We show that SLMs encode lexical tone to a significant degree even when they are trained on data from non-tonal languages. We further find that SLMs behave similarly to native and non-native human participants in tone and consonant perception studies, but they do not follow the same developmental trajectory.",1,,include (senior:4),https://arxiv.org/abs/2403.16865v2,https://arxiv.org/pdf/2403.16865v2,2024,spoken language models,true full-duplex,"{""arxiv_id"": ""2403.16865"", ""title"": ""Encoding of lexical tone in self-supervised models of spoken language"", ""summary"": ""Interpretability research has shown that self-supervised Spoken Language Models (SLMs) encode a wide variety of features in human speech from the acoustic, phonetic, phonological, syntactic and semantic levels, to speaker characteristics. The bulk of prior research on representations of phonology has focused on segmental features such as phonemes; the encoding of suprasegmental phonology (such as tone and stress patterns) in SLMs is not yet well understood. Tone is a suprasegmental feature that is present in more than half of the world's languages. This paper aims to analyze the tone encoding capabilities of SLMs, using Mandarin and Vietnamese as case studies. We show that SLMs encode lexical tone to a significant degree even when they are trained on data from non-tonal languages. We further find that SLMs behave similarly to native and non-native human participants in tone and consonant perception studies, but they do not follow the same developmental trajectory."", ""authors"": [""Gaofei Shen"", ""Michaela Watkins"", ""Afra Alishahi"", ""Arianna Bisazza"", ""Grzegorz Chrupała""], ""published"": ""2024-03-25T15:28:38Z"", ""updated"": ""2024-04-03T12:59:20Z"", ""categories"": [""cs.CL"", ""eess.AS""], ""pdf_url"": ""https://arxiv.org/pdf/2403.16865v2"", ""landing_url"": ""https://arxiv.org/abs/2403.16865v2"", ""doi"": ""https://doi.org/10.48550/arXiv.2403.16865""}",2403.16865,https://openalex.org/W4393213707
10.48550/arxiv.2403.16973,VoiceCraft: Zero-Shot Speech Editing and Text-to-Speech in the Wild,"We introduce VoiceCraft, a token infilling neural codec language model, that achieves state-of-the-art performance on both speech editing and zero-shot text-to-speech (TTS) on audiobooks, internet videos, and podcasts. VoiceCraft employs a Transformer decoder architecture and introduces a token rearrangement procedure that combines causal masking and delayed stacking to enable generation within an existing sequence. On speech editing tasks, VoiceCraft produces edited speech that is nearly indistinguishable from unedited recordings in terms of naturalness, as evaluated by humans; for zero-shot TTS, our model outperforms prior SotA models including VALLE and the popular commercial model XTTS-v2. Crucially, the models are evaluated on challenging and realistic datasets, that consist of diverse accents, speaking styles, recording conditions, and background noise and music, and our model performs consistently well compared to other models and real recordings. In particular, for speech editing evaluation, we introduce a high quality, challenging, and realistic dataset named RealEdit. We encourage readers to listen to the demos at https://jasonppy.github.io/VoiceCraft_web.",1,,include (senior:5),https://arxiv.org/abs/2403.16973v3,https://arxiv.org/pdf/2403.16973v3,2024,synchronous dialogue,neural codec,"{""arxiv_id"": ""2403.16973"", ""title"": ""VoiceCraft: Zero-Shot Speech Editing and Text-to-Speech in the Wild"", ""summary"": ""We introduce VoiceCraft, a token infilling neural codec language model, that achieves state-of-the-art performance on both speech editing and zero-shot text-to-speech (TTS) on audiobooks, internet videos, and podcasts. VoiceCraft employs a Transformer decoder architecture and introduces a token rearrangement procedure that combines causal masking and delayed stacking to enable generation within an existing sequence. On speech editing tasks, VoiceCraft produces edited speech that is nearly indistinguishable from unedited recordings in terms of naturalness, as evaluated by humans; for zero-shot TTS, our model outperforms prior SotA models including VALLE and the popular commercial model XTTS-v2. Crucially, the models are evaluated on challenging and realistic datasets, that consist of diverse accents, speaking styles, recording conditions, and background noise and music, and our model performs consistently well compared to other models and real recordings. In particular, for speech editing evaluation, we introduce a high quality, challenging, and realistic dataset named RealEdit. We encourage readers to listen to the demos at https://jasonppy.github.io/VoiceCraft_web."", ""authors"": [""Puyuan Peng"", ""Po-Yao Huang"", ""Shang-Wen Li"", ""Abdelrahman Mohamed"", ""David Harwath""], ""published"": ""2024-03-25T17:38:32Z"", ""updated"": ""2024-06-14T00:29:46Z"", ""categories"": [""eess.AS"", ""cs.AI"", ""cs.CL"", ""cs.LG"", ""cs.SD""], ""pdf_url"": ""https://arxiv.org/pdf/2403.16973v3"", ""landing_url"": ""https://arxiv.org/abs/2403.16973v3"", ""doi"": ""https://doi.org/10.48550/arXiv.2403.16973""}",2403.16973,https://openalex.org/W4393213897
10.48550/arxiv.2404.05600,SpeechAlign: Aligning Speech Generation to Human Preferences,"Speech language models have significantly advanced in generating realistic speech, with neural codec language models standing out. However, the integration of human feedback to align speech outputs to human preferences is often neglected. This paper addresses this gap by first analyzing the distribution gap in codec language models, highlighting how it leads to discrepancies between the training and inference phases, which negatively affects performance. Then we explore leveraging learning from human feedback to bridge the distribution gap. We introduce SpeechAlign, an iterative self-improvement strategy that aligns speech language models to human preferences. SpeechAlign involves constructing a preference codec dataset contrasting golden codec tokens against synthetic tokens, followed by preference optimization to improve the codec language model. This cycle of improvement is carried out iteratively to steadily convert weak models to strong ones. Through both subjective and objective evaluations, we show that SpeechAlign can bridge the distribution gap and facilitating continuous self-improvement of the speech language model. Moreover, SpeechAlign exhibits robust generalization capabilities and works for smaller models. Code and models will be available at https://github.com/0nutation/SpeechGPT.",1,,include (senior:4),https://arxiv.org/abs/2404.05600v1,https://arxiv.org/pdf/2404.05600v1,2024,synchronous dialogue,neural codec,"{""arxiv_id"": ""2404.05600"", ""title"": ""SpeechAlign: Aligning Speech Generation to Human Preferences"", ""summary"": ""Speech language models have significantly advanced in generating realistic speech, with neural codec language models standing out. However, the integration of human feedback to align speech outputs to human preferences is often neglected. This paper addresses this gap by first analyzing the distribution gap in codec language models, highlighting how it leads to discrepancies between the training and inference phases, which negatively affects performance. Then we explore leveraging learning from human feedback to bridge the distribution gap. We introduce SpeechAlign, an iterative self-improvement strategy that aligns speech language models to human preferences. SpeechAlign involves constructing a preference codec dataset contrasting golden codec tokens against synthetic tokens, followed by preference optimization to improve the codec language model. This cycle of improvement is carried out iteratively to steadily convert weak models to strong ones. Through both subjective and objective evaluations, we show that SpeechAlign can bridge the distribution gap and facilitating continuous self-improvement of the speech language model. Moreover, SpeechAlign exhibits robust generalization capabilities and works for smaller models. Code and models will be available at https://github.com/0nutation/SpeechGPT."", ""authors"": [""Dong Zhang"", ""Zhaowei Li"", ""Shimin Li"", ""Xin Zhang"", ""Pengyu Wang"", ""Yaqian Zhou"", ""Xipeng Qiu""], ""published"": ""2024-04-08T15:21:17Z"", ""updated"": ""2024-04-08T15:21:17Z"", ""categories"": [""cs.CL"", ""cs.SD"", ""eess.AS""], ""pdf_url"": ""https://arxiv.org/pdf/2404.05600v1"", ""landing_url"": ""https://arxiv.org/abs/2404.05600v1"", ""doi"": ""https://doi.org/10.48550/arXiv.2404.05600""}",2404.056,https://openalex.org/W4394673462
10.48550/arxiv.2404.10419,MAD Speech: Measures of Acoustic Diversity of Speech,"Generative spoken language models produce speech in a wide range of voices, prosody, and recording conditions, seemingly approaching the diversity of natural speech. However, the extent to which generated speech is acoustically diverse remains unclear due to a lack of appropriate metrics. We address this gap by developing lightweight metrics of acoustic diversity, which we collectively refer to as MAD Speech. We focus on measuring five facets of acoustic diversity: voice, gender, emotion, accent, and background noise. We construct the metrics as a composition of specialized, per-facet embedding models and an aggregation function that measures diversity within the embedding space. Next, we build a series of datasets with a priori known diversity preferences for each facet. Using these datasets, we demonstrate that our proposed metrics achieve a stronger agreement with the ground-truth diversity than baselines. Finally, we showcase the applicability of our proposed metrics across several real-life evaluation scenarios. MAD Speech is made publicly accessible.",1,,include (senior:4),https://arxiv.org/abs/2404.10419v2,https://arxiv.org/pdf/2404.10419v2,2024,spoken language models,true full-duplex,"{""arxiv_id"": ""2404.10419"", ""title"": ""MAD Speech: Measures of Acoustic Diversity of Speech"", ""summary"": ""Generative spoken language models produce speech in a wide range of voices, prosody, and recording conditions, seemingly approaching the diversity of natural speech. However, the extent to which generated speech is acoustically diverse remains unclear due to a lack of appropriate metrics. We address this gap by developing lightweight metrics of acoustic diversity, which we collectively refer to as MAD Speech. We focus on measuring five facets of acoustic diversity: voice, gender, emotion, accent, and background noise. We construct the metrics as a composition of specialized, per-facet embedding models and an aggregation function that measures diversity within the embedding space. Next, we build a series of datasets with a priori known diversity preferences for each facet. Using these datasets, we demonstrate that our proposed metrics achieve a stronger agreement with the ground-truth diversity than baselines. Finally, we showcase the applicability of our proposed metrics across several real-life evaluation scenarios. MAD Speech is made publicly accessible."", ""authors"": [""Matthieu Futeral"", ""Andrea Agostinelli"", ""Marco Tagliasacchi"", ""Neil Zeghidour"", ""Eugene Kharitonov""], ""published"": ""2024-04-16T09:35:27Z"", ""updated"": ""2025-03-11T12:02:06Z"", ""categories"": [""eess.AS"", ""cs.CL""], ""pdf_url"": ""https://arxiv.org/pdf/2404.10419v2"", ""landing_url"": ""https://arxiv.org/abs/2404.10419v2"", ""doi"": ""https://doi.org/10.48550/arXiv.2404.10419""}",2404.10419,https://openalex.org/W4394906250
10.48550/arxiv.2406.08336,CoLM-DSR: Leveraging Neural Codec Language Modeling for Multi-Modal Dysarthric Speech Reconstruction,"Dysarthric speech reconstruction (DSR) aims to transform dysarthric speech into normal speech. It still suffers from low speaker similarity and poor prosody naturalness. In this paper, we propose a multi-modal DSR model by leveraging neural codec language modeling to improve the reconstruction results, especially for the speaker similarity and prosody naturalness. Our proposed model consists of: (i) a multi-modal content encoder to extract robust phoneme embeddings from dysarthric speech with auxiliary visual inputs; (ii) a speaker codec encoder to extract and normalize the speaker-aware codecs from the dysarthric speech, in order to provide original timbre and normal prosody; (iii) a codec language model based speech decoder to reconstruct the speech based on the extracted phoneme embeddings and normalized codecs. Evaluations on the commonly used UASpeech corpus show that our proposed model can achieve significant improvements in terms of speaker similarity and prosody naturalness.",1,,include (senior:4),https://arxiv.org/abs/2406.08336v2,https://arxiv.org/pdf/2406.08336v2,2024,spoken language models,neural codec,"{""arxiv_id"": ""2406.08336"", ""title"": ""CoLM-DSR: Leveraging Neural Codec Language Modeling for Multi-Modal Dysarthric Speech Reconstruction"", ""summary"": ""Dysarthric speech reconstruction (DSR) aims to transform dysarthric speech into normal speech. It still suffers from low speaker similarity and poor prosody naturalness. In this paper, we propose a multi-modal DSR model by leveraging neural codec language modeling to improve the reconstruction results, especially for the speaker similarity and prosody naturalness. Our proposed model consists of: (i) a multi-modal content encoder to extract robust phoneme embeddings from dysarthric speech with auxiliary visual inputs; (ii) a speaker codec encoder to extract and normalize the speaker-aware codecs from the dysarthric speech, in order to provide original timbre and normal prosody; (iii) a codec language model based speech decoder to reconstruct the speech based on the extracted phoneme embeddings and normalized codecs. Evaluations on the commonly used UASpeech corpus show that our proposed model can achieve significant improvements in terms of speaker similarity and prosody naturalness."", ""authors"": [""Xueyuan Chen"", ""Dongchao Yang"", ""Dingdong Wang"", ""Xixin Wu"", ""Zhiyong Wu"", ""Helen Meng""], ""published"": ""2024-06-12T15:42:21Z"", ""updated"": ""2024-06-24T06:09:42Z"", ""categories"": [""cs.SD"", ""cs.CV"", ""eess.AS""], ""pdf_url"": ""https://arxiv.org/pdf/2406.08336v2"", ""landing_url"": ""https://arxiv.org/abs/2406.08336v2"", ""doi"": ""https://doi.org/10.48550/arXiv.2406.08336""}",2406.08336,https://openalex.org/W4399657731
10.48550/arxiv.2406.11037,NAST: Noise Aware Speech Tokenization for Speech Language Models,"Speech tokenization is the task of representing speech signals as a sequence of discrete units. Such representations can be later used for various downstream tasks including automatic speech recognition, text-to-speech, etc. More relevant to this study, such representation serves as the basis of Speech Language Models. In this work, we tackle the task of speech tokenization under the noisy setup and present NAST: Noise Aware Speech Tokenization for Speech Language Models. NAST is composed of three main components: (i) a predictor; (ii) a residual encoder; and (iii) a decoder. We evaluate the efficiency of NAST considering several spoken language modeling tasks and show that NAST is superior to the evaluated baselines across all setups. Lastly, we analyze NAST and show its disentanglement properties and robustness to signal variations in the form of noise, reverberation, pitch-shift, and time-stretch. Code and pre-trained models are available at https://github.com/ShovalMessica/NAST.",1,,include (senior:5),https://arxiv.org/abs/2406.11037v1,https://arxiv.org/pdf/2406.11037v1,2024,spoken language models,true full-duplex,"{""arxiv_id"": ""2406.11037"", ""title"": ""NAST: Noise Aware Speech Tokenization for Speech Language Models"", ""summary"": ""Speech tokenization is the task of representing speech signals as a sequence of discrete units. Such representations can be later used for various downstream tasks including automatic speech recognition, text-to-speech, etc. More relevant to this study, such representation serves as the basis of Speech Language Models. In this work, we tackle the task of speech tokenization under the noisy setup and present NAST: Noise Aware Speech Tokenization for Speech Language Models. NAST is composed of three main components: (i) a predictor; (ii) a residual encoder; and (iii) a decoder. We evaluate the efficiency of NAST considering several spoken language modeling tasks and show that NAST is superior to the evaluated baselines across all setups. Lastly, we analyze NAST and show its disentanglement properties and robustness to signal variations in the form of noise, reverberation, pitch-shift, and time-stretch. Code and pre-trained models are available at https://github.com/ShovalMessica/NAST."", ""authors"": [""Shoval Messica"", ""Yossi Adi""], ""published"": ""2024-06-16T18:20:45Z"", ""updated"": ""2024-06-16T18:20:45Z"", ""categories"": [""cs.SD"", ""eess.AS""], ""pdf_url"": ""https://arxiv.org/pdf/2406.11037v1"", ""landing_url"": ""https://arxiv.org/abs/2406.11037v1"", ""doi"": ""https://doi.org/10.48550/arXiv.2406.11037""}",2406.11037,https://openalex.org/W4399794786
10.48550/arxiv.2407.01911,Investigating the Effects of Large-Scale Pseudo-Stereo Data and Different Speech Foundation Model on Dialogue Generative Spoken Language Model,"Recent efforts in Spoken Dialogue Modeling aim to synthesize spoken dialogue without the need for direct transcription, thereby preserving the wealth of non-textual information inherent in speech. However, this approach faces a challenge when speakers talk simultaneously, requiring stereo dialogue data with speakers recorded on separate channels, a notably scarce resource. To address this, we have developed an innovative pipeline capable of transforming single-channel dialogue data into pseudo-stereo data. This expanded our training dataset from a mere 2,000 to an impressive 17,600 hours, significantly enriching the diversity and quality of the training examples available. The inclusion of this pseudo-stereo data has proven to be effective in improving the performance of spoken dialogue language models. Additionally, we explored the use of discrete units of different speech foundation models for spoken dialogue generation.",1,,include (senior:4),https://arxiv.org/abs/2407.01911v1,https://arxiv.org/pdf/2407.01911v1,2024,spoken language models,true full-duplex,"{""arxiv_id"": ""2407.01911"", ""title"": ""Investigating the Effects of Large-Scale Pseudo-Stereo Data and Different Speech Foundation Model on Dialogue Generative Spoken Language Model"", ""summary"": ""Recent efforts in Spoken Dialogue Modeling aim to synthesize spoken dialogue without the need for direct transcription, thereby preserving the wealth of non-textual information inherent in speech. However, this approach faces a challenge when speakers talk simultaneously, requiring stereo dialogue data with speakers recorded on separate channels, a notably scarce resource. To address this, we have developed an innovative pipeline capable of transforming single-channel dialogue data into pseudo-stereo data. This expanded our training dataset from a mere 2,000 to an impressive 17,600 hours, significantly enriching the diversity and quality of the training examples available. The inclusion of this pseudo-stereo data has proven to be effective in improving the performance of spoken dialogue language models. Additionally, we explored the use of discrete units of different speech foundation models for spoken dialogue generation."", ""authors"": [""Yu-Kuan Fu"", ""Cheng-Kuang Lee"", ""Hsiu-Hsuan Wang"", ""Hung-yi Lee""], ""published"": ""2024-07-02T03:22:41Z"", ""updated"": ""2024-07-02T03:22:41Z"", ""categories"": [""cs.CL"", ""cs.HC"", ""cs.SD"", ""eess.AS""], ""pdf_url"": ""https://arxiv.org/pdf/2407.01911v1"", ""landing_url"": ""https://arxiv.org/abs/2407.01911v1"", ""doi"": ""https://doi.org/10.48550/arXiv.2407.01911""}",2407.01911,https://openalex.org/W4400377753
10.48550/arxiv.2407.15828,J-CHAT: Japanese Large-scale Spoken Dialogue Corpus for Spoken Dialogue Language Modeling,"Spoken dialogue plays a crucial role in human-AI interactions, necessitating dialogue-oriented spoken language models (SLMs). To develop versatile SLMs, large-scale and diverse speech datasets are essential. Additionally, to ensure hiqh-quality speech generation, the data must be spontaneous like in-wild data and must be acoustically clean with noise removed. Despite the critical need, no open-source corpus meeting all these criteria has been available. This study addresses this gap by constructing and releasing a large-scale spoken dialogue corpus, named Japanese Corpus for Human-AI Talks (J-CHAT), which is publicly accessible. Furthermore, this paper presents a language-independent method for corpus construction and describes experiments on dialogue generation using SLMs trained on J-CHAT. Experimental results indicate that the collected data from multiple domains by our method improve the naturalness and meaningfulness of dialogue generation.",1,,include (senior:4),https://arxiv.org/abs/2407.15828v1,https://arxiv.org/pdf/2407.15828v1,2024,spoken language models,true full-duplex,"{""arxiv_id"": ""2407.15828"", ""title"": ""J-CHAT: Japanese Large-scale Spoken Dialogue Corpus for Spoken Dialogue Language Modeling"", ""summary"": ""Spoken dialogue plays a crucial role in human-AI interactions, necessitating dialogue-oriented spoken language models (SLMs). To develop versatile SLMs, large-scale and diverse speech datasets are essential. Additionally, to ensure hiqh-quality speech generation, the data must be spontaneous like in-wild data and must be acoustically clean with noise removed. Despite the critical need, no open-source corpus meeting all these criteria has been available. This study addresses this gap by constructing and releasing a large-scale spoken dialogue corpus, named Japanese Corpus for Human-AI Talks (J-CHAT), which is publicly accessible. Furthermore, this paper presents a language-independent method for corpus construction and describes experiments on dialogue generation using SLMs trained on J-CHAT. Experimental results indicate that the collected data from multiple domains by our method improve the naturalness and meaningfulness of dialogue generation."", ""authors"": [""Wataru Nakata"", ""Kentaro Seki"", ""Hitomi Yanaka"", ""Yuki Saito"", ""Shinnosuke Takamichi"", ""Hiroshi Saruwatari""], ""published"": ""2024-07-22T17:46:50Z"", ""updated"": ""2024-07-22T17:46:50Z"", ""categories"": [""cs.CL"", ""cs.SD"", ""eess.AS""], ""pdf_url"": ""https://arxiv.org/pdf/2407.15828v1"", ""landing_url"": ""https://arxiv.org/abs/2407.15828v1"", ""doi"": ""https://doi.org/10.48550/arXiv.2407.15828""}",2407.15828,https://openalex.org/W4406073439
10.48550/arxiv.2408.02622,Language Model Can Listen While Speaking,"Dialogue serves as the most natural manner of human-computer interaction (HCI). Recent advancements in speech language models (SLM) have significantly enhanced speech-based conversational AI. However, these models are limited to turn-based conversation, lacking the ability to interact with humans in real-time spoken scenarios, for example, being interrupted when the generated content is not satisfactory. To address these limitations, we explore full duplex modeling (FDM) in interactive speech language models (iSLM), focusing on enhancing real-time interaction and, more explicitly, exploring the quintessential ability of interruption. We introduce a novel model design, namely listening-while-speaking language model (LSLM), an end-to-end system equipped with both listening and speaking channels. Our LSLM employs a token-based decoder-only TTS for speech generation and a streaming self-supervised learning (SSL) encoder for real-time audio input. LSLM fuses both channels for autoregressive generation and detects turn-taking in real time. Three fusion strategies -- early fusion, middle fusion, and late fusion -- are explored, with middle fusion achieving an optimal balance between speech generation and real-time interaction. Two experimental settings, command-based FDM and voice-based FDM, demonstrate LSLM's robustness to noise and sensitivity to diverse instructions. Our results highlight LSLM's capability to achieve duplex communication with minimal impact on existing systems. This study aims to advance the development of interactive speech dialogue systems, enhancing their applicability in real-world contexts.",1,,include (junior:5),https://arxiv.org/abs/2408.02622v1,https://arxiv.org/pdf/2408.02622v1,2024,spoken language models,real-time interaction,"{""arxiv_id"": ""2408.02622"", ""title"": ""Language Model Can Listen While Speaking"", ""summary"": ""Dialogue serves as the most natural manner of human-computer interaction (HCI). Recent advancements in speech language models (SLM) have significantly enhanced speech-based conversational AI. However, these models are limited to turn-based conversation, lacking the ability to interact with humans in real-time spoken scenarios, for example, being interrupted when the generated content is not satisfactory. To address these limitations, we explore full duplex modeling (FDM) in interactive speech language models (iSLM), focusing on enhancing real-time interaction and, more explicitly, exploring the quintessential ability of interruption. We introduce a novel model design, namely listening-while-speaking language model (LSLM), an end-to-end system equipped with both listening and speaking channels. Our LSLM employs a token-based decoder-only TTS for speech generation and a streaming self-supervised learning (SSL) encoder for real-time audio input. LSLM fuses both channels for autoregressive generation and detects turn-taking in real time. Three fusion strategies -- early fusion, middle fusion, and late fusion -- are explored, with middle fusion achieving an optimal balance between speech generation and real-time interaction. Two experimental settings, command-based FDM and voice-based FDM, demonstrate LSLM's robustness to noise and sensitivity to diverse instructions. Our results highlight LSLM's capability to achieve duplex communication with minimal impact on existing systems. This study aims to advance the development of interactive speech dialogue systems, enhancing their applicability in real-world contexts."", ""authors"": [""Ziyang Ma"", ""Yakun Song"", ""Chenpeng Du"", ""Jian Cong"", ""Zhuo Chen"", ""Yuping Wang"", ""Yuxuan Wang"", ""Xie Chen""], ""published"": ""2024-08-05T16:47:22Z"", ""updated"": ""2024-08-05T16:47:22Z"", ""categories"": [""cs.CL"", ""cs.AI"", ""cs.HC"", ""cs.SD"", ""eess.AS""], ""pdf_url"": ""https://arxiv.org/pdf/2408.02622v1"", ""landing_url"": ""https://arxiv.org/abs/2408.02622v1"", ""doi"": ""https://doi.org/10.48550/arXiv.2408.02622""}",2408.02622,https://openalex.org/W4403370726
10.48550/arxiv.2408.16725,"Mini-Omni: Language Models Can Hear, Talk While Thinking in Streaming","Recent advances in language models have achieved significant progress. GPT-4o, as a new milestone, has enabled real-time conversations with humans, demonstrating near-human natural fluency. Such human-computer interaction necessitates models with the capability to perform reasoning directly with the audio modality and generate output in streaming. However, this remains beyond the reach of current academic models, as they typically depend on extra TTS systems for speech synthesis, resulting in undesirable latency. This paper introduces the Mini-Omni, an audio-based end-to-end conversational model, capable of real-time speech interaction. To achieve this capability, we propose a text-instructed speech generation method, along with batch-parallel strategies during inference to further boost the performance. Our method also helps to retain the original model's language capabilities with minimal degradation, enabling other works to establish real-time interaction capabilities. We call this training method ""Any Model Can Talk"". We also introduce the VoiceAssistant-400K dataset to fine-tune models optimized for speech output. To our best knowledge, Mini-Omni is the first fully end-to-end, open-source model for real-time speech interaction, offering valuable potential for future research.",1,,include (junior:4),https://arxiv.org/abs/2408.16725v3,https://arxiv.org/pdf/2408.16725v3,2024,synchronous dialogue,real-time interaction,"{""arxiv_id"": ""2408.16725"", ""title"": ""Mini-Omni: Language Models Can Hear, Talk While Thinking in Streaming"", ""summary"": ""Recent advances in language models have achieved significant progress. GPT-4o, as a new milestone, has enabled real-time conversations with humans, demonstrating near-human natural fluency. Such human-computer interaction necessitates models with the capability to perform reasoning directly with the audio modality and generate output in streaming. However, this remains beyond the reach of current academic models, as they typically depend on extra TTS systems for speech synthesis, resulting in undesirable latency. This paper introduces the Mini-Omni, an audio-based end-to-end conversational model, capable of real-time speech interaction. To achieve this capability, we propose a text-instructed speech generation method, along with batch-parallel strategies during inference to further boost the performance. Our method also helps to retain the original model's language capabilities with minimal degradation, enabling other works to establish real-time interaction capabilities. We call this training method \""Any Model Can Talk\"". We also introduce the VoiceAssistant-400K dataset to fine-tune models optimized for speech output. To our best knowledge, Mini-Omni is the first fully end-to-end, open-source model for real-time speech interaction, offering valuable potential for future research."", ""authors"": [""Zhifei Xie"", ""Changqiao Wu""], ""published"": ""2024-08-29T17:18:53Z"", ""updated"": ""2024-11-05T02:24:18Z"", ""categories"": [""cs.AI"", ""cs.CL"", ""cs.HC"", ""cs.LG"", ""cs.SD"", ""eess.AS""], ""pdf_url"": ""https://arxiv.org/pdf/2408.16725v3"", ""landing_url"": ""https://arxiv.org/abs/2408.16725v3"", ""doi"": ""https://doi.org/10.48550/arXiv.2408.16725""}",2408.16725,https://openalex.org/W4403556012
10.48550/arxiv.2409.03701,LAST: Language Model Aware Speech Tokenization,"Speech tokenization serves as the foundation of speech language model (LM), enabling them to perform various tasks such as spoken language modeling, text-to-speech, speech-to-text, etc. Most speech tokenizers are trained independently of the LM training process, relying on separate acoustic models and quantization methods. Following such an approach may create a mismatch between the tokenization process and its usage afterward. In this study, we propose a novel approach to training a speech tokenizer by leveraging objectives from pre-trained textual LMs. We advocate for the integration of this objective into the process of learning discrete speech representations. Our aim is to transform features from a pre-trained speech model into a new feature space that enables better clustering for speech LMs. We empirically investigate the impact of various model design choices, including speech vocabulary size and text LM size. Our results demonstrate the proposed tokenization method outperforms the evaluated baselines considering both spoken language modeling and speech-to-text. More importantly, unlike prior work, the proposed method allows the utilization of a single pre-trained LM for processing both speech and text inputs, setting it apart from conventional tokenization approaches.",1,,include (senior:4),https://arxiv.org/abs/2409.03701v2,https://arxiv.org/pdf/2409.03701v2,2024,spoken language models,true full-duplex,"{""arxiv_id"": ""2409.03701"", ""title"": ""LAST: Language Model Aware Speech Tokenization"", ""summary"": ""Speech tokenization serves as the foundation of speech language model (LM), enabling them to perform various tasks such as spoken language modeling, text-to-speech, speech-to-text, etc. Most speech tokenizers are trained independently of the LM training process, relying on separate acoustic models and quantization methods. Following such an approach may create a mismatch between the tokenization process and its usage afterward. In this study, we propose a novel approach to training a speech tokenizer by leveraging objectives from pre-trained textual LMs. We advocate for the integration of this objective into the process of learning discrete speech representations. Our aim is to transform features from a pre-trained speech model into a new feature space that enables better clustering for speech LMs. We empirically investigate the impact of various model design choices, including speech vocabulary size and text LM size. Our results demonstrate the proposed tokenization method outperforms the evaluated baselines considering both spoken language modeling and speech-to-text. More importantly, unlike prior work, the proposed method allows the utilization of a single pre-trained LM for processing both speech and text inputs, setting it apart from conventional tokenization approaches."", ""authors"": [""Arnon Turetzky"", ""Yossi Adi""], ""published"": ""2024-09-05T16:57:39Z"", ""updated"": ""2024-09-10T14:45:15Z"", ""categories"": [""cs.CL"", ""cs.SD"", ""eess.AS""], ""pdf_url"": ""https://arxiv.org/pdf/2409.03701v2"", ""landing_url"": ""https://arxiv.org/abs/2409.03701v2"", ""doi"": ""https://doi.org/10.48550/arXiv.2409.03701""}",2409.03701,https://openalex.org/W4403590045
10.48550/arxiv.2409.07841,TSELM: Target Speaker Extraction using Discrete Tokens and Language Models,"We propose TSELM, a novel target speaker extraction network that leverages discrete tokens and language models. TSELM utilizes multiple discretized layers from WavLM as input tokens and incorporates cross-attention mechanisms to integrate target speaker information. Language models are employed to capture the sequence dependencies, while a scalable HiFi-GAN is used to reconstruct the audio from the tokens. By applying a cross-entropy loss, TSELM models the probability distribution of output tokens, thus converting the complex regression problem of audio generation into a classification task. Experimental results show that TSELM achieves excellent results in speech quality and comparable results in speech intelligibility.",1,,include (senior:4),https://arxiv.org/abs/2409.07841v3,https://arxiv.org/pdf/2409.07841v3,2024,spoken language models,discrete tokens,"{""arxiv_id"": ""2409.07841"", ""title"": ""TSELM: Target Speaker Extraction using Discrete Tokens and Language Models"", ""summary"": ""We propose TSELM, a novel target speaker extraction network that leverages discrete tokens and language models. TSELM utilizes multiple discretized layers from WavLM as input tokens and incorporates cross-attention mechanisms to integrate target speaker information. Language models are employed to capture the sequence dependencies, while a scalable HiFi-GAN is used to reconstruct the audio from the tokens. By applying a cross-entropy loss, TSELM models the probability distribution of output tokens, thus converting the complex regression problem of audio generation into a classification task. Experimental results show that TSELM achieves excellent results in speech quality and comparable results in speech intelligibility."", ""authors"": [""Beilong Tang"", ""Bang Zeng"", ""Ming Li""], ""published"": ""2024-09-12T08:41:07Z"", ""updated"": ""2024-09-17T01:41:32Z"", ""categories"": [""cs.SD"", ""cs.LG"", ""eess.AS""], ""pdf_url"": ""https://arxiv.org/pdf/2409.07841v3"", ""landing_url"": ""https://arxiv.org/abs/2409.07841v3"", ""doi"": ""https://doi.org/10.48550/arXiv.2409.07841""}",2409.07841,https://openalex.org/W4403703830
10.48550/arxiv.2409.15594,Beyond Turn-Based Interfaces: Synchronous LLMs as Full-Duplex Dialogue Agents,"Despite broad interest in modeling spoken dialogue agents, most approaches are inherently ""half-duplex"" -- restricted to turn-based interaction with responses requiring explicit prompting by the user or implicit tracking of interruption or silence events. Human dialogue, by contrast, is ""full-duplex"" allowing for rich synchronicity in the form of quick and dynamic turn-taking, overlapping speech, and backchanneling. Technically, the challenge of achieving full-duplex dialogue with LLMs lies in modeling synchrony as pre-trained LLMs do not have a sense of ""time"". To bridge this gap, we propose Synchronous LLMs for full-duplex spoken dialogue modeling. We design a novel mechanism to integrate time information into Llama3-8b so that they run synchronously with the real-world clock. We also introduce a training recipe that uses 212k hours of synthetic spoken dialogue data generated from text dialogue data to create a model that generates meaningful and natural spoken dialogue, with just 2k hours of real-world spoken dialogue data. Synchronous LLMs outperform state-of-the-art in dialogue meaningfulness while maintaining naturalness. Finally, we demonstrate the model's ability to participate in full-duplex dialogue by simulating interaction between two agents trained on different datasets, while considering Internet-scale latencies of up to 240 ms. Webpage: https://syncllm.cs.washington.edu/.",1,,include (junior:5),https://arxiv.org/abs/2409.15594v1,https://arxiv.org/pdf/2409.15594v1,2024,full-duplex,true full-duplex,"{""arxiv_id"": ""2409.15594"", ""title"": ""Beyond Turn-Based Interfaces: Synchronous LLMs as Full-Duplex Dialogue Agents"", ""summary"": ""Despite broad interest in modeling spoken dialogue agents, most approaches are inherently \""half-duplex\"" -- restricted to turn-based interaction with responses requiring explicit prompting by the user or implicit tracking of interruption or silence events. Human dialogue, by contrast, is \""full-duplex\"" allowing for rich synchronicity in the form of quick and dynamic turn-taking, overlapping speech, and backchanneling. Technically, the challenge of achieving full-duplex dialogue with LLMs lies in modeling synchrony as pre-trained LLMs do not have a sense of \""time\"". To bridge this gap, we propose Synchronous LLMs for full-duplex spoken dialogue modeling. We design a novel mechanism to integrate time information into Llama3-8b so that they run synchronously with the real-world clock. We also introduce a training recipe that uses 212k hours of synthetic spoken dialogue data generated from text dialogue data to create a model that generates meaningful and natural spoken dialogue, with just 2k hours of real-world spoken dialogue data. Synchronous LLMs outperform state-of-the-art in dialogue meaningfulness while maintaining naturalness. Finally, we demonstrate the model's ability to participate in full-duplex dialogue by simulating interaction between two agents trained on different datasets, while considering Internet-scale latencies of up to 240 ms. Webpage: https://syncllm.cs.washington.edu/."", ""authors"": [""Bandhav Veluri"", ""Benjamin N Peloquin"", ""Bokai Yu"", ""Hongyu Gong"", ""Shyamnath Gollakota""], ""published"": ""2024-09-23T23:01:31Z"", ""updated"": ""2024-09-23T23:01:31Z"", ""categories"": [""cs.CL"", ""cs.LG"", ""cs.SD"", ""eess.AS""], ""pdf_url"": ""https://arxiv.org/pdf/2409.15594v1"", ""landing_url"": ""https://arxiv.org/abs/2409.15594v1"", ""doi"": ""https://doi.org/10.48550/arXiv.2409.15594""}",2409.15594,https://openalex.org/W4403786381
10.48550/arxiv.2409.19283,Analyzing and Mitigating Inconsistency in Discrete Audio Tokens for Neural Codec Language Models,"Building upon advancements in Large Language Models (LLMs), the field of audio processing has seen increased interest in training audio generation tasks with discrete audio token sequences. However, directly discretizing audio by neural audio codecs often results in sequences that fundamentally differ from text sequences. Unlike text, where text token sequences are deterministic, discrete audio tokens can exhibit significant variability based on contextual factors, while still producing perceptually identical audio segments. We refer to this phenomenon as \textbf{Discrete Representation Inconsistency (DRI)}. This inconsistency can lead to a single audio segment being represented by multiple divergent sequences, which creates confusion in neural codec language models and results in omissions and repetitions during speech generation. In this paper, we quantitatively analyze the DRI phenomenon within popular audio tokenizers such as EnCodec. Our approach effectively mitigates the DRI phenomenon of the neural audio codec. Furthermore, extensive experiments on the neural codec language model over LibriTTS and large-scale MLS datases (44,000 hours) demonstrate the effectiveness and generality of our method. The demo of audio samples is available online~\footnote{\url{https://consistencyinneuralcodec.github.io}}.",1,,include (senior:5),https://arxiv.org/abs/2409.19283v2,https://arxiv.org/pdf/2409.19283v2,2024,full-duplex,audio tokens,"{""arxiv_id"": ""2409.19283"", ""title"": ""Analyzing and Mitigating Inconsistency in Discrete Audio Tokens for Neural Codec Language Models"", ""summary"": ""Building upon advancements in Large Language Models (LLMs), the field of audio processing has seen increased interest in training audio generation tasks with discrete audio token sequences. However, directly discretizing audio by neural audio codecs often results in sequences that fundamentally differ from text sequences. Unlike text, where text token sequences are deterministic, discrete audio tokens can exhibit significant variability based on contextual factors, while still producing perceptually identical audio segments. We refer to this phenomenon as \\textbf{Discrete Representation Inconsistency (DRI)}. This inconsistency can lead to a single audio segment being represented by multiple divergent sequences, which creates confusion in neural codec language models and results in omissions and repetitions during speech generation. In this paper, we quantitatively analyze the DRI phenomenon within popular audio tokenizers such as EnCodec. Our approach effectively mitigates the DRI phenomenon of the neural audio codec. Furthermore, extensive experiments on the neural codec language model over LibriTTS and large-scale MLS datases (44,000 hours) demonstrate the effectiveness and generality of our method. The demo of audio samples is available online~\\footnote{\\url{https://consistencyinneuralcodec.github.io}}."", ""authors"": [""Wenrui Liu"", ""Zhifang Guo"", ""Jin Xu"", ""Yuanjun Lv"", ""Yunfei Chu"", ""Zhou Zhao"", ""Junyang Lin""], ""published"": ""2024-09-28T08:36:44Z"", ""updated"": ""2024-10-04T22:34:38Z"", ""categories"": [""eess.AS"", ""cs.SD""], ""pdf_url"": ""https://arxiv.org/pdf/2409.19283v2"", ""landing_url"": ""https://arxiv.org/abs/2409.19283v2"", ""doi"": ""https://doi.org/10.48550/arXiv.2409.19283""}",2409.19283,https://openalex.org/W4403321974
10.48550/arxiv.2410.00025,Improving Spoken Language Modeling with Phoneme Classification: A Simple Fine-tuning Approach,"Recent progress in Spoken Language Modeling has shown that learning language directly from speech is feasible. Generating speech through a pipeline that operates at the text level typically loses nuances, intonations, and non-verbal vocalizations. Modeling directly from speech opens up the path to more natural and expressive systems. On the other hand, speech-only systems require up to three orders of magnitude more data to catch up to their text-based counterparts in terms of their semantic abilities. We show that fine-tuning speech representation models on phoneme classification leads to more context-invariant representations, and language models trained on these units achieve comparable lexical comprehension to ones trained on hundred times more data.",1,,include (senior:4),https://arxiv.org/abs/2410.00025v2,https://arxiv.org/pdf/2410.00025v2,2024,spoken language models,true full-duplex,"{""arxiv_id"": ""2410.00025"", ""title"": ""Improving Spoken Language Modeling with Phoneme Classification: A Simple Fine-tuning Approach"", ""summary"": ""Recent progress in Spoken Language Modeling has shown that learning language directly from speech is feasible. Generating speech through a pipeline that operates at the text level typically loses nuances, intonations, and non-verbal vocalizations. Modeling directly from speech opens up the path to more natural and expressive systems. On the other hand, speech-only systems require up to three orders of magnitude more data to catch up to their text-based counterparts in terms of their semantic abilities. We show that fine-tuning speech representation models on phoneme classification leads to more context-invariant representations, and language models trained on these units achieve comparable lexical comprehension to ones trained on hundred times more data."", ""authors"": [""Maxime Poli"", ""Emmanuel Chemla"", ""Emmanuel Dupoux""], ""published"": ""2024-09-16T10:29:15Z"", ""updated"": ""2024-10-30T17:46:22Z"", ""categories"": [""cs.CL"", ""cs.SD"", ""eess.AS""], ""pdf_url"": ""https://arxiv.org/pdf/2410.00025v2"", ""landing_url"": ""https://arxiv.org/abs/2410.00025v2"", ""doi"": ""https://doi.org/10.48550/arXiv.2410.00025""}",2410.00025,https://openalex.org/W4403818350
10.48550/arxiv.2410.00037,Moshi: a speech-text foundation model for real-time dialogue,"We introduce Moshi, a speech-text foundation model and full-duplex spoken dialogue framework. Current systems for spoken dialogue rely on pipelines of independent components, namely voice activity detection, speech recognition, textual dialogue and text-to-speech. Such frameworks cannot emulate the experience of real conversations. First, their complexity induces a latency of several seconds between interactions. Second, text being the intermediate modality for dialogue, non-linguistic information that modifies meaning -- such as emotion or non-speech sounds -- is lost in the interaction. Finally, they rely on a segmentation into speaker turns, which does not take into account overlapping speech, interruptions and interjections. Moshi solves these independent issues altogether by casting spoken dialogue as speech-to-speech generation. Starting from a text language model backbone, Moshi generates speech as tokens from the residual quantizer of a neural audio codec, while modeling separately its own speech and that of the user into parallel streams. This allows for the removal of explicit speaker turns, and the modeling of arbitrary conversational dynamics. We moreover extend the hierarchical semantic-to-acoustic token generation of previous work to first predict time-aligned text tokens as a prefix to audio tokens. Not only this ""Inner Monologue"" method significantly improves the linguistic quality of generated speech, but we also illustrate how it can provide streaming speech recognition and text-to-speech. Our resulting model is the first real-time full-duplex spoken large language model, with a theoretical latency of 160ms, 200ms in practice, and is available at https://github.com/kyutai-labs/moshi.",1,,include (junior:5),https://arxiv.org/abs/2410.00037v2,https://arxiv.org/pdf/2410.00037v2,2024,spoken language models,audio tokens,"{""arxiv_id"": ""2410.00037"", ""title"": ""Moshi: a speech-text foundation model for real-time dialogue"", ""summary"": ""We introduce Moshi, a speech-text foundation model and full-duplex spoken dialogue framework. Current systems for spoken dialogue rely on pipelines of independent components, namely voice activity detection, speech recognition, textual dialogue and text-to-speech. Such frameworks cannot emulate the experience of real conversations. First, their complexity induces a latency of several seconds between interactions. Second, text being the intermediate modality for dialogue, non-linguistic information that modifies meaning -- such as emotion or non-speech sounds -- is lost in the interaction. Finally, they rely on a segmentation into speaker turns, which does not take into account overlapping speech, interruptions and interjections. Moshi solves these independent issues altogether by casting spoken dialogue as speech-to-speech generation. Starting from a text language model backbone, Moshi generates speech as tokens from the residual quantizer of a neural audio codec, while modeling separately its own speech and that of the user into parallel streams. This allows for the removal of explicit speaker turns, and the modeling of arbitrary conversational dynamics. We moreover extend the hierarchical semantic-to-acoustic token generation of previous work to first predict time-aligned text tokens as a prefix to audio tokens. Not only this \""Inner Monologue\"" method significantly improves the linguistic quality of generated speech, but we also illustrate how it can provide streaming speech recognition and text-to-speech. Our resulting model is the first real-time full-duplex spoken large language model, with a theoretical latency of 160ms, 200ms in practice, and is available at https://github.com/kyutai-labs/moshi."", ""authors"": [""Alexandre Défossez"", ""Laurent Mazaré"", ""Manu Orsini"", ""Amélie Royer"", ""Patrick Pérez"", ""Hervé Jégou"", ""Edouard Grave"", ""Neil Zeghidour""], ""published"": ""2024-09-17T17:55:39Z"", ""updated"": ""2024-10-02T09:11:45Z"", ""categories"": [""eess.AS"", ""cs.AI"", ""cs.CL"", ""cs.LG"", ""cs.SD""], ""pdf_url"": ""https://arxiv.org/pdf/2410.00037v2"", ""landing_url"": ""https://arxiv.org/abs/2410.00037v2"", ""doi"": ""https://doi.org/10.48550/arXiv.2410.00037""}",2410.00037,https://openalex.org/W4403883071
10.48550/arxiv.2410.04029,SyllableLM: Learning Coarse Semantic Units for Speech Language Models,"Language models require tokenized inputs. However, tokenization strategies for continuous data like audio and vision are often based on simple heuristics such as fixed sized convolutions or discrete clustering, which do not necessarily align with the semantic structure of the data. For speech in particular, the high resolution of waveforms (16,000 samples/second or more) presents a significant challenge as speech-based language models have had to use several times more tokens per word than text-based language models. In this work, we introduce a controllable self-supervised technique to merge speech representations into coarser syllable-like units while still preserving semantic information. We do this by 1) extracting noisy boundaries through analyzing correlations in pretrained encoder losses and 2) iteratively improving model representations with a novel distillation technique. Our method produces controllable-rate semantic units at as low as 5Hz and 60bps and achieves SotA in syllabic segmentation and clustering. Using these coarse tokens, we successfully train SyllableLM, a Speech Language Model (SpeechLM) that matches or outperforms current SotA SpeechLMs on a range of spoken language modeling tasks. SyllableLM also achieves significant improvements in efficiency with a 30x reduction in training compute and a 4x wall-clock inference speedup.",1,,include (senior:5),https://arxiv.org/abs/2410.04029v1,https://arxiv.org/pdf/2410.04029v1,2024,spoken language models,true full-duplex,"{""arxiv_id"": ""2410.04029"", ""title"": ""SyllableLM: Learning Coarse Semantic Units for Speech Language Models"", ""summary"": ""Language models require tokenized inputs. However, tokenization strategies for continuous data like audio and vision are often based on simple heuristics such as fixed sized convolutions or discrete clustering, which do not necessarily align with the semantic structure of the data. For speech in particular, the high resolution of waveforms (16,000 samples/second or more) presents a significant challenge as speech-based language models have had to use several times more tokens per word than text-based language models. In this work, we introduce a controllable self-supervised technique to merge speech representations into coarser syllable-like units while still preserving semantic information. We do this by 1) extracting noisy boundaries through analyzing correlations in pretrained encoder losses and 2) iteratively improving model representations with a novel distillation technique. Our method produces controllable-rate semantic units at as low as 5Hz and 60bps and achieves SotA in syllabic segmentation and clustering. Using these coarse tokens, we successfully train SyllableLM, a Speech Language Model (SpeechLM) that matches or outperforms current SotA SpeechLMs on a range of spoken language modeling tasks. SyllableLM also achieves significant improvements in efficiency with a 30x reduction in training compute and a 4x wall-clock inference speedup."", ""authors"": [""Alan Baade"", ""Puyuan Peng"", ""David Harwath""], ""published"": ""2024-10-05T04:29:55Z"", ""updated"": ""2024-10-05T04:29:55Z"", ""categories"": [""cs.CL"", ""cs.AI"", ""eess.AS""], ""pdf_url"": ""https://arxiv.org/pdf/2410.04029v1"", ""landing_url"": ""https://arxiv.org/abs/2410.04029v1"", ""doi"": ""https://doi.org/10.48550/arXiv.2410.04029""}",2410.04029,https://openalex.org/W4403929295
10.48550/arxiv.2410.04380,HALL-E: Hierarchical Neural Codec Language Model for Minute-Long Zero-Shot Text-to-Speech Synthesis,"Recently, Text-to-speech (TTS) models based on large language models (LLMs) that translate natural language text into sequences of discrete audio tokens have gained great research attention, with advances in neural audio codec (NAC) models using residual vector quantization (RVQ). However, long-form speech synthesis remains a significant challenge due to the high frame rate, which increases the length of audio tokens and makes it difficult for autoregressive language models to generate audio tokens for even a minute of speech. To address this challenge, this paper introduces two novel post-training approaches: 1) Multi-Resolution Requantization (MReQ) and 2) HALL-E. MReQ is a framework to reduce the frame rate of pre-trained NAC models. Specifically, it incorporates multi-resolution residual vector quantization (MRVQ) module that hierarchically reorganizes discrete audio tokens through teacher-student distillation. HALL-E is an LLM-based TTS model designed to predict hierarchical tokens of MReQ. Specifically, it incorporates the technique of using MRVQ sub-modules and continues training from a pre-trained LLM-based TTS model. Furthermore, to promote TTS research, we create MinutesSpeech, a new benchmark dataset consisting of 40k hours of filtered speech data for training and evaluating speech synthesis ranging from 3s up to 180s. In experiments, we demonstrated the effectiveness of our approaches by applying our post-training framework to VALL-E. We achieved the frame rate down to as low as 8 Hz, enabling the stable minitue-long speech synthesis in a single inference step. Audio samples, dataset, codes and pre-trained models are available at https://yutonishimura-v2.github.io/HALL-E_DEMO/.",1,,include (senior:4),https://arxiv.org/abs/2410.04380v1,https://arxiv.org/pdf/2410.04380v1,2024,spoken language models,audio tokens,"{""arxiv_id"": ""2410.04380"", ""title"": ""HALL-E: Hierarchical Neural Codec Language Model for Minute-Long Zero-Shot Text-to-Speech Synthesis"", ""summary"": ""Recently, Text-to-speech (TTS) models based on large language models (LLMs) that translate natural language text into sequences of discrete audio tokens have gained great research attention, with advances in neural audio codec (NAC) models using residual vector quantization (RVQ). However, long-form speech synthesis remains a significant challenge due to the high frame rate, which increases the length of audio tokens and makes it difficult for autoregressive language models to generate audio tokens for even a minute of speech. To address this challenge, this paper introduces two novel post-training approaches: 1) Multi-Resolution Requantization (MReQ) and 2) HALL-E. MReQ is a framework to reduce the frame rate of pre-trained NAC models. Specifically, it incorporates multi-resolution residual vector quantization (MRVQ) module that hierarchically reorganizes discrete audio tokens through teacher-student distillation. HALL-E is an LLM-based TTS model designed to predict hierarchical tokens of MReQ. Specifically, it incorporates the technique of using MRVQ sub-modules and continues training from a pre-trained LLM-based TTS model. Furthermore, to promote TTS research, we create MinutesSpeech, a new benchmark dataset consisting of 40k hours of filtered speech data for training and evaluating speech synthesis ranging from 3s up to 180s. In experiments, we demonstrated the effectiveness of our approaches by applying our post-training framework to VALL-E. We achieved the frame rate down to as low as 8 Hz, enabling the stable minitue-long speech synthesis in a single inference step. Audio samples, dataset, codes and pre-trained models are available at https://yutonishimura-v2.github.io/HALL-E_DEMO/."", ""authors"": [""Yuto Nishimura"", ""Takumi Hirose"", ""Masanari Ohi"", ""Hideki Nakayama"", ""Nakamasa Inoue""], ""published"": ""2024-10-06T07:20:58Z"", ""updated"": ""2024-10-06T07:20:58Z"", ""categories"": [""eess.AS"", ""cs.SD""], ""pdf_url"": ""https://arxiv.org/pdf/2410.04380v1"", ""landing_url"": ""https://arxiv.org/abs/2410.04380v1"", ""doi"": ""https://doi.org/10.48550/arXiv.2410.04380""}",2410.0438,https://openalex.org/W4403963885
10.48550/arxiv.2410.07168,Sylber: Syllabic Embedding Representation of Speech from Raw Audio,"Syllables are compositional units of spoken language that efficiently structure human speech perception and production. However, current neural speech representations lack such structure, resulting in dense token sequences that are costly to process. To bridge this gap, we propose a new model, Sylber, that produces speech representations with clean and robust syllabic structure. Specifically, we propose a self-supervised learning (SSL) framework that bootstraps syllabic embeddings by distilling from its own initial unsupervised syllabic segmentation. This results in a highly structured representation of speech features, offering three key benefits: 1) a fast, linear-time syllable segmentation algorithm, 2) efficient syllabic tokenization with an average of 4.27 tokens per second, and 3) novel phonological units suited for efficient spoken language modeling. Our proposed segmentation method is highly robust and generalizes to out-of-domain data and unseen languages without any tuning. By training token-to-speech generative models, fully intelligible speech can be reconstructed from Sylber tokens with a significantly lower bitrate than baseline SSL tokens. This suggests that our model effectively compresses speech into a compact sequence of tokens with minimal information loss. Lastly, we demonstrate that categorical perception-a linguistic phenomenon in speech perception-emerges naturally in Sylber, making the embedding space more categorical and sparse than previous speech features and thus supporting the high efficiency of our tokenization. Together, we present a novel SSL approach for representing speech as syllables, with significant potential for efficient speech tokenization and spoken language modeling.",1,,include (senior:4),https://arxiv.org/abs/2410.07168v2,https://arxiv.org/pdf/2410.07168v2,2024,spoken language models,true full-duplex,"{""arxiv_id"": ""2410.07168"", ""title"": ""Sylber: Syllabic Embedding Representation of Speech from Raw Audio"", ""summary"": ""Syllables are compositional units of spoken language that efficiently structure human speech perception and production. However, current neural speech representations lack such structure, resulting in dense token sequences that are costly to process. To bridge this gap, we propose a new model, Sylber, that produces speech representations with clean and robust syllabic structure. Specifically, we propose a self-supervised learning (SSL) framework that bootstraps syllabic embeddings by distilling from its own initial unsupervised syllabic segmentation. This results in a highly structured representation of speech features, offering three key benefits: 1) a fast, linear-time syllable segmentation algorithm, 2) efficient syllabic tokenization with an average of 4.27 tokens per second, and 3) novel phonological units suited for efficient spoken language modeling. Our proposed segmentation method is highly robust and generalizes to out-of-domain data and unseen languages without any tuning. By training token-to-speech generative models, fully intelligible speech can be reconstructed from Sylber tokens with a significantly lower bitrate than baseline SSL tokens. This suggests that our model effectively compresses speech into a compact sequence of tokens with minimal information loss. Lastly, we demonstrate that categorical perception-a linguistic phenomenon in speech perception-emerges naturally in Sylber, making the embedding space more categorical and sparse than previous speech features and thus supporting the high efficiency of our tokenization. Together, we present a novel SSL approach for representing speech as syllables, with significant potential for efficient speech tokenization and spoken language modeling."", ""authors"": [""Cheol Jun Cho"", ""Nicholas Lee"", ""Akshat Gupta"", ""Dhruv Agarwal"", ""Ethan Chen"", ""Alan W Black"", ""Gopala K. Anumanchipalli""], ""published"": ""2024-10-09T17:59:04Z"", ""updated"": ""2025-03-02T09:16:05Z"", ""categories"": [""cs.CL"", ""cs.SD"", ""eess.AS""], ""pdf_url"": ""https://arxiv.org/pdf/2410.07168v2"", ""landing_url"": ""https://arxiv.org/abs/2410.07168v2"", ""doi"": ""https://doi.org/10.48550/arXiv.2410.07168""}",2410.07168,https://openalex.org/W4403345965
10.48550/arxiv.2410.24177,DC-Spin: A Speaker-invariant Speech Tokenizer for Spoken Language Models,"Spoken language models (SLMs) have gained increasing attention with advancements in text-based, decoder-only language models. SLMs process text and speech, enabling simultaneous speech understanding and generation. This paper presents Double-Codebook Speaker-invariant Clustering (DC-Spin), which aims to improve speech tokenization by bridging audio signals and SLM tokens. DC-Spin extracts speaker-invariant tokens rich in phonetic information and resilient to input variations, enhancing zero-shot SLM tasks and speech resynthesis. We propose a chunk-wise approach to enable streamable DC-Spin without retraining and degradation. Comparisons of tokenization methods (self-supervised and neural audio codecs), model scalability, and downstream task proxies show that tokens easily modeled by an n-gram LM or aligned with phonemes offer strong performance, providing insights for designing speech tokenizers for SLMs.",1,,include (senior:5),https://arxiv.org/abs/2410.24177v1,https://arxiv.org/pdf/2410.24177v1,2024,spoken language models,true full-duplex,"{""arxiv_id"": ""2410.24177"", ""title"": ""DC-Spin: A Speaker-invariant Speech Tokenizer for Spoken Language Models"", ""summary"": ""Spoken language models (SLMs) have gained increasing attention with advancements in text-based, decoder-only language models. SLMs process text and speech, enabling simultaneous speech understanding and generation. This paper presents Double-Codebook Speaker-invariant Clustering (DC-Spin), which aims to improve speech tokenization by bridging audio signals and SLM tokens. DC-Spin extracts speaker-invariant tokens rich in phonetic information and resilient to input variations, enhancing zero-shot SLM tasks and speech resynthesis. We propose a chunk-wise approach to enable streamable DC-Spin without retraining and degradation. Comparisons of tokenization methods (self-supervised and neural audio codecs), model scalability, and downstream task proxies show that tokens easily modeled by an n-gram LM or aligned with phonemes offer strong performance, providing insights for designing speech tokenizers for SLMs."", ""authors"": [""Heng-Jui Chang"", ""Hongyu Gong"", ""Changhan Wang"", ""James Glass"", ""Yu-An Chung""], ""published"": ""2024-10-31T17:43:13Z"", ""updated"": ""2024-10-31T17:43:13Z"", ""categories"": [""eess.AS"", ""cs.CL"", ""cs.LG"", ""cs.SD""], ""pdf_url"": ""https://arxiv.org/pdf/2410.24177v1"", ""landing_url"": ""https://arxiv.org/abs/2410.24177v1"", ""doi"": ""https://doi.org/10.48550/arXiv.2410.24177""}",2410.24177,https://openalex.org/W4404348734
10.48550/arxiv.2411.01834,Align-SLM: Textless Spoken Language Models with Reinforcement Learning from AI Feedback,"While textless Spoken Language Models (SLMs) have shown potential in end-to-end speech-to-speech modeling, they still lag behind text-based Large Language Models (LLMs) in terms of semantic coherence and relevance. This work introduces the Align-SLM framework, which leverages preference optimization inspired by Reinforcement Learning with AI Feedback (RLAIF) to enhance the semantic understanding of SLMs. Our approach generates multiple speech continuations from a given prompt and uses semantic metrics to create preference data for Direct Preference Optimization (DPO). We evaluate the framework using ZeroSpeech 2021 benchmarks for lexical and syntactic modeling, the spoken version of the StoryCloze dataset for semantic coherence, and other speech generation metrics, including the GPT4-o score and human evaluation. Experimental results show that our method achieves state-of-the-art performance for SLMs on most benchmarks, highlighting the importance of preference optimization to improve the semantics of SLMs.",1,,include (junior:4),https://arxiv.org/abs/2411.01834v2,https://arxiv.org/pdf/2411.01834v2,2024,spoken language models,true full-duplex,"{""arxiv_id"": ""2411.01834"", ""title"": ""Align-SLM: Textless Spoken Language Models with Reinforcement Learning from AI Feedback"", ""summary"": ""While textless Spoken Language Models (SLMs) have shown potential in end-to-end speech-to-speech modeling, they still lag behind text-based Large Language Models (LLMs) in terms of semantic coherence and relevance. This work introduces the Align-SLM framework, which leverages preference optimization inspired by Reinforcement Learning with AI Feedback (RLAIF) to enhance the semantic understanding of SLMs. Our approach generates multiple speech continuations from a given prompt and uses semantic metrics to create preference data for Direct Preference Optimization (DPO). We evaluate the framework using ZeroSpeech 2021 benchmarks for lexical and syntactic modeling, the spoken version of the StoryCloze dataset for semantic coherence, and other speech generation metrics, including the GPT4-o score and human evaluation. Experimental results show that our method achieves state-of-the-art performance for SLMs on most benchmarks, highlighting the importance of preference optimization to improve the semantics of SLMs."", ""authors"": [""Guan-Ting Lin"", ""Prashanth Gurunath Shivakumar"", ""Aditya Gourav"", ""Yile Gu"", ""Ankur Gandhe"", ""Hung-yi Lee"", ""Ivan Bulyko""], ""published"": ""2024-11-04T06:07:53Z"", ""updated"": ""2025-05-27T16:17:52Z"", ""categories"": [""cs.CL"", ""eess.AS""], ""pdf_url"": ""https://arxiv.org/pdf/2411.01834v2"", ""landing_url"": ""https://arxiv.org/abs/2411.01834v2"", ""doi"": ""https://doi.org/10.48550/arXiv.2411.01834""}",2411.01834,https://openalex.org/W4404352991
10.48550/arxiv.2411.05361,Dynamic-SUPERB Phase-2: A Collaboratively Expanding Benchmark for Measuring the Capabilities of Spoken Language Models with 180 Tasks,"Multimodal foundation models, such as Gemini and ChatGPT, have revolutionized human-machine interactions by seamlessly integrating various forms of data. Developing a universal spoken language model that comprehends a wide range of natural language instructions is critical for bridging communication gaps and facilitating more intuitive interactions. However, the absence of a comprehensive evaluation benchmark poses a significant challenge. We present Dynamic-SUPERB Phase-2, an open and evolving benchmark for the comprehensive evaluation of instruction-based universal speech models. Building upon the first generation, this second version incorporates 125 new tasks contributed collaboratively by the global research community, expanding the benchmark to a total of 180 tasks, making it the largest benchmark for speech and audio evaluation. While the first generation of Dynamic-SUPERB was limited to classification tasks, Dynamic-SUPERB Phase-2 broadens its evaluation capabilities by introducing a wide array of novel and diverse tasks, including regression and sequence generation, across speech, music, and environmental audio. Evaluation results show that no model performed well universally. SALMONN-13B excelled in English ASR and Qwen2-Audio-7B-Instruct showed high accuracy in emotion recognition, but current models still require further innovations to handle a broader range of tasks. We open-source all task data and the evaluation pipeline at https://github.com/dynamic-superb/dynamic-superb.",1,,include (senior:4),https://arxiv.org/abs/2411.05361v2,https://arxiv.org/pdf/2411.05361v2,2024,spoken language models,true full-duplex,"{""arxiv_id"": ""2411.05361"", ""title"": ""Dynamic-SUPERB Phase-2: A Collaboratively Expanding Benchmark for Measuring the Capabilities of Spoken Language Models with 180 Tasks"", ""summary"": ""Multimodal foundation models, such as Gemini and ChatGPT, have revolutionized human-machine interactions by seamlessly integrating various forms of data. Developing a universal spoken language model that comprehends a wide range of natural language instructions is critical for bridging communication gaps and facilitating more intuitive interactions. However, the absence of a comprehensive evaluation benchmark poses a significant challenge. We present Dynamic-SUPERB Phase-2, an open and evolving benchmark for the comprehensive evaluation of instruction-based universal speech models. Building upon the first generation, this second version incorporates 125 new tasks contributed collaboratively by the global research community, expanding the benchmark to a total of 180 tasks, making it the largest benchmark for speech and audio evaluation. While the first generation of Dynamic-SUPERB was limited to classification tasks, Dynamic-SUPERB Phase-2 broadens its evaluation capabilities by introducing a wide array of novel and diverse tasks, including regression and sequence generation, across speech, music, and environmental audio. Evaluation results show that no model performed well universally. SALMONN-13B excelled in English ASR and Qwen2-Audio-7B-Instruct showed high accuracy in emotion recognition, but current models still require further innovations to handle a broader range of tasks. We open-source all task data and the evaluation pipeline at https://github.com/dynamic-superb/dynamic-superb."", ""authors"": [""Chien-yu Huang"", ""Wei-Chih Chen"", ""Shu-wen Yang"", ""Andy T. Liu"", ""Chen-An Li"", ""Yu-Xiang Lin"", ""Wei-Cheng Tseng"", ""Anuj Diwan"", ""Yi-Jen Shih"", ""Jiatong Shi"", ""William Chen"", ""Chih-Kai Yang"", ""Wenze Ren"", ""Xuanjun Chen"", ""Chi-Yuan Hsiao"", ""Puyuan Peng"", ""Shih-Heng Wang"", ""Chun-Yi Kuan"", ""Ke-Han Lu"", ""Kai-Wei Chang"", ""Fabian Ritter-Gutierrez"", ""Kuan-Po Huang"", ""Siddhant Arora"", ""You-Kuan Lin"", ""Ming To Chuang"", ""Eunjung Yeo"", ""Kalvin Chang"", ""Chung-Ming Chien"", ""Kwanghee Choi"", ""Jun-You Wang"", ""Cheng-Hsiu Hsieh"", ""Yi-Cheng Lin"", ""Chee-En Yu"", ""I-Hsiang Chiu"", ""Heitor R. Guimarães"", ""Jionghao Han"", ""Tzu-Quan Lin"", ""Tzu-Yuan Lin"", ""Homu Chang"", ""Ting-Wu Chang"", ""Chun Wei Chen"", ""Shou-Jen Chen"", ""Yu-Hua Chen"", ""Hsi-Chun Cheng"", ""Kunal Dhawan"", ""Jia-Lin Fang"", ""Shi-Xin Fang"", ""Kuan-Yu Fang Chiang"", ""Chi An Fu"", ""Hsien-Fu Hsiao"", ""Ching Yu Hsu"", ""Shao-Syuan Huang"", ""Lee Chen Wei"", ""Hsi-Che Lin"", ""Hsuan-Hao Lin"", ""Hsuan-Ting Lin"", ""Jian-Ren Lin"", ""Ting-Chun Liu"", ""Li-Chun Lu"", ""Tsung-Min Pai"", ""Ankita Pasad"", ""Shih-Yun Shan Kuan"", ""Suwon Shon"", ""Yuxun Tang"", ""Yun-Shao Tsai"", ""Jui-Chiang Wei"", ""Tzu-Chieh Wei"", ""Chengxi Wu"", ""Dien-Ruei Wu"", ""Chao-Han Huck Yang"", ""Chieh-Chi Yang"", ""Jia Qi Yip"", ""Shao-Xiang Yuan"", ""Vahid Noroozi"", ""Zhehuai Chen"", ""Haibin Wu"", ""Karen Livescu"", ""David Harwath"", ""Shinji Watanabe"", ""Hung-yi Lee""], ""published"": ""2024-11-08T06:33:22Z"", ""updated"": ""2025-06-09T16:36:12Z"", ""categories"": [""cs.CL"", ""eess.AS""], ""pdf_url"": ""https://arxiv.org/pdf/2411.05361v2"", ""landing_url"": ""https://arxiv.org/abs/2411.05361v2"", ""doi"": ""https://doi.org/10.48550/arXiv.2411.05361""}",2411.05361,https://openalex.org/W4404389501
10.48550/arxiv.2411.07111,Building a Taiwanese Mandarin Spoken Language Model: A First Attempt,"This technical report presents our initial attempt to build a spoken large language model (LLM) for Taiwanese Mandarin, specifically tailored to enable real-time, speech-to-speech interaction in multi-turn conversations. Our end-to-end model incorporates a decoder-only transformer architecture and aims to achieve seamless interaction while preserving the conversational flow, including full-duplex capabilities allowing simultaneous speaking and listening. The paper also details the training process, including data preparation with synthesized dialogues and adjustments for real-time interaction. We also developed a platform to evaluate conversational fluency and response coherence in multi-turn dialogues. We hope the release of the report can contribute to the future development of spoken LLMs in Taiwanese Mandarin.",1,,include (junior:5),https://arxiv.org/abs/2411.07111v2,https://arxiv.org/pdf/2411.07111v2,2024,spoken language models,true full-duplex,"{""arxiv_id"": ""2411.07111"", ""title"": ""Building a Taiwanese Mandarin Spoken Language Model: A First Attempt"", ""summary"": ""This technical report presents our initial attempt to build a spoken large language model (LLM) for Taiwanese Mandarin, specifically tailored to enable real-time, speech-to-speech interaction in multi-turn conversations. Our end-to-end model incorporates a decoder-only transformer architecture and aims to achieve seamless interaction while preserving the conversational flow, including full-duplex capabilities allowing simultaneous speaking and listening. The paper also details the training process, including data preparation with synthesized dialogues and adjustments for real-time interaction. We also developed a platform to evaluate conversational fluency and response coherence in multi-turn dialogues. We hope the release of the report can contribute to the future development of spoken LLMs in Taiwanese Mandarin."", ""authors"": [""Chih-Kai Yang"", ""Yu-Kuan Fu"", ""Chen-An Li"", ""Yi-Cheng Lin"", ""Yu-Xiang Lin"", ""Wei-Chih Chen"", ""Ho Lam Chung"", ""Chun-Yi Kuan"", ""Wei-Ping Huang"", ""Ke-Han Lu"", ""Tzu-Quan Lin"", ""Hsiu-Hsuan Wang"", ""En-Pei Hu"", ""Chan-Jan Hsu"", ""Liang-Hsuan Tseng"", ""I-Hsiang Chiu"", ""Ulin Sanga"", ""Xuanjun Chen"", ""Po-chun Hsu"", ""Shu-wen Yang"", ""Hung-yi Lee""], ""published"": ""2024-11-11T16:37:40Z"", ""updated"": ""2024-12-27T07:29:19Z"", ""categories"": [""cs.CL"", ""cs.SD"", ""eess.AS""], ""pdf_url"": ""https://arxiv.org/pdf/2411.07111v2"", ""landing_url"": ""https://arxiv.org/abs/2411.07111v2"", ""doi"": ""https://doi.org/10.48550/arXiv.2411.07111""}",2411.07111,https://openalex.org/W4404402405
10.48550/arxiv.2411.08742,A Comparative Study of Discrete Speech Tokens for Semantic-Related Tasks with Large Language Models,"With the rise of Speech Large Language Models (Speech LLMs), there has been growing interest in discrete speech tokens for their ability to integrate with text-based tokens seamlessly. Compared to most studies that focus on continuous speech features, although discrete-token based LLMs have shown promising results on certain tasks, the performance gap between these two paradigms is rarely explored. In this paper, we present a fair and thorough comparison between discrete and continuous features across a variety of semantic-related tasks using a light-weight LLM (Qwen1.5-0.5B). Our findings reveal that continuous features generally outperform discrete tokens, particularly in tasks requiring fine-grained semantic understanding. Moreover, this study goes beyond surface-level comparison by identifying key factors behind the under-performance of discrete tokens, such as limited token granularity and inefficient information retention. To enhance the performance of discrete tokens, we explore potential aspects based on our analysis. We hope our results can offer new insights into the opportunities for advancing discrete speech tokens in Speech LLMs.",1,,include (senior:5),https://arxiv.org/abs/2411.08742v1,https://arxiv.org/pdf/2411.08742v1,2024,spoken language models,discrete tokens,"{""arxiv_id"": ""2411.08742"", ""title"": ""A Comparative Study of Discrete Speech Tokens for Semantic-Related Tasks with Large Language Models"", ""summary"": ""With the rise of Speech Large Language Models (Speech LLMs), there has been growing interest in discrete speech tokens for their ability to integrate with text-based tokens seamlessly. Compared to most studies that focus on continuous speech features, although discrete-token based LLMs have shown promising results on certain tasks, the performance gap between these two paradigms is rarely explored. In this paper, we present a fair and thorough comparison between discrete and continuous features across a variety of semantic-related tasks using a light-weight LLM (Qwen1.5-0.5B). Our findings reveal that continuous features generally outperform discrete tokens, particularly in tasks requiring fine-grained semantic understanding. Moreover, this study goes beyond surface-level comparison by identifying key factors behind the under-performance of discrete tokens, such as limited token granularity and inefficient information retention. To enhance the performance of discrete tokens, we explore potential aspects based on our analysis. We hope our results can offer new insights into the opportunities for advancing discrete speech tokens in Speech LLMs."", ""authors"": [""Dingdong Wang"", ""Mingyu Cui"", ""Dongchao Yang"", ""Xueyuan Chen"", ""Helen Meng""], ""published"": ""2024-11-13T16:20:20Z"", ""updated"": ""2024-11-13T16:20:20Z"", ""categories"": [""cs.CL"", ""cs.SD"", ""eess.AS""], ""pdf_url"": ""https://arxiv.org/pdf/2411.08742v1"", ""landing_url"": ""https://arxiv.org/abs/2411.08742v1"", ""doi"": ""https://doi.org/10.48550/arXiv.2411.08742""}",2411.08742,https://openalex.org/W4404408071
10.48550/arxiv.2411.18138,SALMONN-omni: A Codec-free LLM for Full-duplex Speech Understanding and Generation,"Full-duplex multimodal large language models (LLMs) provide a unified framework for addressing diverse speech understanding and generation tasks, enabling more natural and seamless human-machine conversations. Unlike traditional modularised conversational AI systems, which separate speech recognition, understanding, and text-to-speech generation into distinct components, multimodal LLMs operate as single end-to-end models. This streamlined design eliminates error propagation across components and fully leverages the rich non-verbal information embedded in input speech signals. We introduce SALMONN-omni, a codec-free, full-duplex speech understanding and generation model capable of simultaneously listening to its own generated speech and background sounds while speaking. To support this capability, we propose a novel duplex spoken dialogue framework incorporating a ``thinking'' mechanism that facilitates asynchronous text and speech generation relying on embeddings instead of codecs (quantized speech and audio tokens). Experimental results demonstrate SALMONN-omni's versatility across a broad range of streaming speech tasks, including speech recognition, speech enhancement, and spoken question answering. Additionally, SALMONN-omni excels at managing turn-taking, barge-in, and echo cancellation scenarios, establishing its potential as a robust prototype for full-duplex conversational AI systems. To the best of our knowledge, SALMONN-omni is the first codec-free model of its kind. A full technical report along with model checkpoints will be released soon.",1,,include (junior:5),https://arxiv.org/abs/2411.18138v1,https://arxiv.org/pdf/2411.18138v1,2024,full-duplex,audio tokens,"{""arxiv_id"": ""2411.18138"", ""title"": ""SALMONN-omni: A Codec-free LLM for Full-duplex Speech Understanding and Generation"", ""summary"": ""Full-duplex multimodal large language models (LLMs) provide a unified framework for addressing diverse speech understanding and generation tasks, enabling more natural and seamless human-machine conversations. Unlike traditional modularised conversational AI systems, which separate speech recognition, understanding, and text-to-speech generation into distinct components, multimodal LLMs operate as single end-to-end models. This streamlined design eliminates error propagation across components and fully leverages the rich non-verbal information embedded in input speech signals. We introduce SALMONN-omni, a codec-free, full-duplex speech understanding and generation model capable of simultaneously listening to its own generated speech and background sounds while speaking. To support this capability, we propose a novel duplex spoken dialogue framework incorporating a ``thinking'' mechanism that facilitates asynchronous text and speech generation relying on embeddings instead of codecs (quantized speech and audio tokens). Experimental results demonstrate SALMONN-omni's versatility across a broad range of streaming speech tasks, including speech recognition, speech enhancement, and spoken question answering. Additionally, SALMONN-omni excels at managing turn-taking, barge-in, and echo cancellation scenarios, establishing its potential as a robust prototype for full-duplex conversational AI systems. To the best of our knowledge, SALMONN-omni is the first codec-free model of its kind. A full technical report along with model checkpoints will be released soon."", ""authors"": [""Wenyi Yu"", ""Siyin Wang"", ""Xiaoyu Yang"", ""Xianzhao Chen"", ""Xiaohai Tian"", ""Jun Zhang"", ""Guangzhi Sun"", ""Lu Lu"", ""Yuxuan Wang"", ""Chao Zhang""], ""published"": ""2024-11-27T08:38:57Z"", ""updated"": ""2024-11-27T08:38:57Z"", ""categories"": [""eess.AS"", ""cs.AI"", ""cs.CL"", ""cs.SD""], ""pdf_url"": ""https://arxiv.org/pdf/2411.18138v1"", ""landing_url"": ""https://arxiv.org/abs/2411.18138v1"", ""doi"": ""https://doi.org/10.48550/arXiv.2411.18138""}",2411.18138,https://openalex.org/W4404990673
10.48550/arxiv.2412.01078,"Advancing Speech Language Models by Scaling Supervised Fine-Tuning with Over 60,000 Hours of Synthetic Speech Dialogue Data","The GPT-4o represents a significant milestone in enabling real-time interaction with large language models (LLMs) through speech, its remarkable low latency and high fluency not only capture attention but also stimulate research interest in the field. This real-time speech interaction is particularly valuable in scenarios requiring rapid feedback and immediate responses, dramatically enhancing user experience. However, there is a notable lack of research focused on real-time large speech language models, particularly for Chinese. In this work, we present KE-Omni, a seamless large speech language model built upon Ke-SpeechChat, a large-scale high-quality synthetic speech interaction dataset consisting of 7 million Chinese and English conversations, featuring 42,002 speakers, and totaling over 60,000 hours, This contributes significantly to the advancement of research and development in this field. The demos can be accessed at \url{https://huggingface.co/spaces/KE-Team/KE-Omni}.",1,,include (senior:5),https://arxiv.org/abs/2412.01078v2,https://arxiv.org/pdf/2412.01078v2,2024,synchronous dialogue,real-time interaction,"{""arxiv_id"": ""2412.01078"", ""title"": ""Advancing Speech Language Models by Scaling Supervised Fine-Tuning with Over 60,000 Hours of Synthetic Speech Dialogue Data"", ""summary"": ""The GPT-4o represents a significant milestone in enabling real-time interaction with large language models (LLMs) through speech, its remarkable low latency and high fluency not only capture attention but also stimulate research interest in the field. This real-time speech interaction is particularly valuable in scenarios requiring rapid feedback and immediate responses, dramatically enhancing user experience. However, there is a notable lack of research focused on real-time large speech language models, particularly for Chinese. In this work, we present KE-Omni, a seamless large speech language model built upon Ke-SpeechChat, a large-scale high-quality synthetic speech interaction dataset consisting of 7 million Chinese and English conversations, featuring 42,002 speakers, and totaling over 60,000 hours, This contributes significantly to the advancement of research and development in this field. The demos can be accessed at \\url{https://huggingface.co/spaces/KE-Team/KE-Omni}."", ""authors"": [""Shuaijiang Zhao"", ""Tingwei Guo"", ""Bajian Xiang"", ""Tongtang Wan"", ""Qiang Niu"", ""Wei Zou"", ""Xiangang Li""], ""published"": ""2024-12-02T03:31:46Z"", ""updated"": ""2024-12-03T02:59:43Z"", ""categories"": [""cs.CL"", ""cs.AI"", ""cs.HC""], ""pdf_url"": ""https://arxiv.org/pdf/2412.01078v2"", ""landing_url"": ""https://arxiv.org/abs/2412.01078v2"", ""doi"": ""https://doi.org/10.48550/arXiv.2412.01078""}",2412.01078,https://openalex.org/W4405033245
10.48550/arxiv.2412.11449,Whisper-GPT: A Hybrid Representation Audio Large Language Model,"We propose WHISPER-GPT: A generative large language model (LLM) for speech and music that allows us to work with continuous audio representations and discrete tokens simultaneously as part of a single architecture. There has been a huge surge in generative audio, speech, and music models that utilize discrete audio tokens derived from neural compression algorithms, e.g. ENCODEC. However, one of the major drawbacks of this approach is handling the context length. It blows up for high-fidelity generative architecture if one has to account for all the audio contents at various frequencies for the next token prediction. By combining continuous audio representation like the spectrogram and discrete acoustic tokens, we retain the best of both worlds: Have all the information needed from the audio at a specific time instance in a single token, yet allow LLM to predict the future token to allow for sampling and other benefits discrete space provides. We show how our architecture improves the perplexity and negative log-likelihood scores for the next token prediction compared to a token-based LLM for speech and music.",1,,include (senior:4),https://arxiv.org/abs/2412.11449v1,https://arxiv.org/pdf/2412.11449v1,2024,spoken language models,audio tokens,"{""arxiv_id"": ""2412.11449"", ""title"": ""Whisper-GPT: A Hybrid Representation Audio Large Language Model"", ""summary"": ""We propose WHISPER-GPT: A generative large language model (LLM) for speech and music that allows us to work with continuous audio representations and discrete tokens simultaneously as part of a single architecture. There has been a huge surge in generative audio, speech, and music models that utilize discrete audio tokens derived from neural compression algorithms, e.g. ENCODEC. However, one of the major drawbacks of this approach is handling the context length. It blows up for high-fidelity generative architecture if one has to account for all the audio contents at various frequencies for the next token prediction. By combining continuous audio representation like the spectrogram and discrete acoustic tokens, we retain the best of both worlds: Have all the information needed from the audio at a specific time instance in a single token, yet allow LLM to predict the future token to allow for sampling and other benefits discrete space provides. We show how our architecture improves the perplexity and negative log-likelihood scores for the next token prediction compared to a token-based LLM for speech and music."", ""authors"": [""Prateek Verma""], ""published"": ""2024-12-16T05:03:48Z"", ""updated"": ""2024-12-16T05:03:48Z"", ""categories"": [""cs.SD"", ""cs.AI"", ""cs.CL"", ""cs.LG"", ""eess.AS""], ""pdf_url"": ""https://arxiv.org/pdf/2412.11449v1"", ""landing_url"": ""https://arxiv.org/abs/2412.11449v1"", ""doi"": ""https://doi.org/10.48550/arXiv.2412.11449""}",2412.11449,https://openalex.org/W4405470171
10.48550/arxiv.2412.15649,SLAM-Omni: Timbre-Controllable Voice Interaction System with Single-Stage Training,"Recent advancements highlight the potential of end-to-end real-time spoken dialogue systems, showcasing their low latency and high quality. In this paper, we introduce SLAM-Omni, a timbre-controllable, end-to-end voice interaction system with single-stage training. SLAM-Omni achieves zero-shot timbre control by modeling spoken language with semantic tokens and decoupling speaker information to a vocoder. By predicting grouped speech semantic tokens at each step, our method significantly reduces the sequence length of audio tokens, accelerating both training and inference. Additionally, we propose historical text prompting to compress dialogue history, facilitating efficient multi-round interactions. Comprehensive evaluations reveal that SLAM-Omni outperforms prior models of similar scale, requiring only 15 hours of training on 4 GPUs with limited data. Notably, it is the first spoken dialogue system to achieve competitive performance with a single-stage training approach, eliminating the need for pre-training on TTS or ASR tasks. Further experiments validate its multilingual and multi-turn dialogue capabilities on larger datasets.",1,,include (senior:5),https://arxiv.org/abs/2412.15649v1,https://arxiv.org/pdf/2412.15649v1,2024,spoken language models,audio tokens,"{""arxiv_id"": ""2412.15649"", ""title"": ""SLAM-Omni: Timbre-Controllable Voice Interaction System with Single-Stage Training"", ""summary"": ""Recent advancements highlight the potential of end-to-end real-time spoken dialogue systems, showcasing their low latency and high quality. In this paper, we introduce SLAM-Omni, a timbre-controllable, end-to-end voice interaction system with single-stage training. SLAM-Omni achieves zero-shot timbre control by modeling spoken language with semantic tokens and decoupling speaker information to a vocoder. By predicting grouped speech semantic tokens at each step, our method significantly reduces the sequence length of audio tokens, accelerating both training and inference. Additionally, we propose historical text prompting to compress dialogue history, facilitating efficient multi-round interactions. Comprehensive evaluations reveal that SLAM-Omni outperforms prior models of similar scale, requiring only 15 hours of training on 4 GPUs with limited data. Notably, it is the first spoken dialogue system to achieve competitive performance with a single-stage training approach, eliminating the need for pre-training on TTS or ASR tasks. Further experiments validate its multilingual and multi-turn dialogue capabilities on larger datasets."", ""authors"": [""Wenxi Chen"", ""Ziyang Ma"", ""Ruiqi Yan"", ""Yuzhe Liang"", ""Xiquan Li"", ""Ruiyang Xu"", ""Zhikang Niu"", ""Yanqiao Zhu"", ""Yifan Yang"", ""Zhanxun Liu"", ""Kai Yu"", ""Yuxuan Hu"", ""Jinyu Li"", ""Yan Lu"", ""Shujie Liu"", ""Xie Chen""], ""published"": ""2024-12-20T08:05:55Z"", ""updated"": ""2024-12-20T08:05:55Z"", ""categories"": [""eess.AS""], ""pdf_url"": ""https://arxiv.org/pdf/2412.15649v1"", ""landing_url"": ""https://arxiv.org/abs/2412.15649v1"", ""doi"": ""https://doi.org/10.48550/arXiv.2412.15649""}",2412.15649,https://openalex.org/W4405714880
10.48550/arxiv.2412.17048,Why Do Speech Language Models Fail to Generate Semantically Coherent Outputs? A Modality Evolving Perspective,"Although text-based large language models exhibit human-level writing ability and remarkable intelligence, speech language models (SLMs) still struggle to generate semantically coherent outputs. There are several potential reasons for this performance degradation: (A) speech tokens mainly provide phonetic information rather than semantic information, (B) the length of speech sequences is much longer than that of text sequences, and (C) paralinguistic information, such as prosody, introduces additional complexity and variability. In this paper, we explore the influence of three key factors separately by transiting the modality from text to speech in an evolving manner. Our findings reveal that the impact of the three factors varies. Factor A has a relatively minor impact, factor B influences syntactical and semantic modeling more obviously, and factor C exerts the most significant impact, particularly in the basic lexical modeling. Based on these findings, we provide insights into the unique challenges of training SLMs and highlight pathways to develop more effective end-to-end SLMs.",1,,include (senior:5),https://arxiv.org/abs/2412.17048v1,https://arxiv.org/pdf/2412.17048v1,2024,spoken language models,semantic coherence,"{""arxiv_id"": ""2412.17048"", ""title"": ""Why Do Speech Language Models Fail to Generate Semantically Coherent Outputs? A Modality Evolving Perspective"", ""summary"": ""Although text-based large language models exhibit human-level writing ability and remarkable intelligence, speech language models (SLMs) still struggle to generate semantically coherent outputs. There are several potential reasons for this performance degradation: (A) speech tokens mainly provide phonetic information rather than semantic information, (B) the length of speech sequences is much longer than that of text sequences, and (C) paralinguistic information, such as prosody, introduces additional complexity and variability. In this paper, we explore the influence of three key factors separately by transiting the modality from text to speech in an evolving manner. Our findings reveal that the impact of the three factors varies. Factor A has a relatively minor impact, factor B influences syntactical and semantic modeling more obviously, and factor C exerts the most significant impact, particularly in the basic lexical modeling. Based on these findings, we provide insights into the unique challenges of training SLMs and highlight pathways to develop more effective end-to-end SLMs."", ""authors"": [""Hankun Wang"", ""Haoran Wang"", ""Yiwei Guo"", ""Zhihan Li"", ""Chenpeng Du"", ""Xie Chen"", ""Kai Yu""], ""published"": ""2024-12-22T14:59:19Z"", ""updated"": ""2024-12-22T14:59:19Z"", ""categories"": [""eess.AS"", ""cs.CL"", ""cs.SD""], ""pdf_url"": ""https://arxiv.org/pdf/2412.17048v1"", ""landing_url"": ""https://arxiv.org/abs/2412.17048v1"", ""doi"": ""https://doi.org/10.48550/arXiv.2412.17048""}",2412.17048,https://openalex.org/W4405767357
10.48550/arxiv.2412.18603,Long-Form Speech Generation with Spoken Language Models,"We consider the generative modeling of speech over multiple minutes, a requirement for long-form multimedia generation and audio-native voice assistants. However, textless spoken language models struggle to generate plausible speech past tens of seconds, due to high temporal resolution of speech tokens causing loss of coherence, architectural issues with long-sequence training or extrapolation, and memory costs at inference time. From these considerations we derive SpeechSSM, the first speech language model family to learn from and sample long-form spoken audio (e.g., 16 minutes of read or extemporaneous speech) in a single decoding session without text intermediates. SpeechSSMs leverage recent advances in linear-time sequence modeling to greatly surpass current Transformer spoken LMs in coherence and efficiency on multi-minute generations while still matching them at the utterance level. As we found current spoken language evaluations uninformative, especially in this new long-form setting, we also introduce: LibriSpeech-Long, a benchmark for long-form speech evaluation; new embedding-based and LLM-judged metrics; and quality measurements over length and time. Speech samples, the LibriSpeech-Long dataset, and any future code or model releases can be found at https://google.github.io/tacotron/publications/speechssm/.",1,,include (junior:4),https://arxiv.org/abs/2412.18603v2,https://arxiv.org/pdf/2412.18603v2,2024,spoken language models,true full-duplex,"{""arxiv_id"": ""2412.18603"", ""title"": ""Long-Form Speech Generation with Spoken Language Models"", ""summary"": ""We consider the generative modeling of speech over multiple minutes, a requirement for long-form multimedia generation and audio-native voice assistants. However, textless spoken language models struggle to generate plausible speech past tens of seconds, due to high temporal resolution of speech tokens causing loss of coherence, architectural issues with long-sequence training or extrapolation, and memory costs at inference time. From these considerations we derive SpeechSSM, the first speech language model family to learn from and sample long-form spoken audio (e.g., 16 minutes of read or extemporaneous speech) in a single decoding session without text intermediates. SpeechSSMs leverage recent advances in linear-time sequence modeling to greatly surpass current Transformer spoken LMs in coherence and efficiency on multi-minute generations while still matching them at the utterance level. As we found current spoken language evaluations uninformative, especially in this new long-form setting, we also introduce: LibriSpeech-Long, a benchmark for long-form speech evaluation; new embedding-based and LLM-judged metrics; and quality measurements over length and time. Speech samples, the LibriSpeech-Long dataset, and any future code or model releases can be found at https://google.github.io/tacotron/publications/speechssm/."", ""authors"": [""Se Jin Park"", ""Julian Salazar"", ""Aren Jansen"", ""Keisuke Kinoshita"", ""Yong Man Ro"", ""RJ Skerry-Ryan""], ""published"": ""2024-12-24T18:56:46Z"", ""updated"": ""2025-07-10T17:52:43Z"", ""categories"": [""cs.CL"", ""cs.SD"", ""eess.AS""], ""pdf_url"": ""https://arxiv.org/pdf/2412.18603v2"", ""landing_url"": ""https://arxiv.org/abs/2412.18603v2"", ""doi"": ""https://doi.org/10.48550/arXiv.2412.18603""}",2412.18603,https://openalex.org/W4405792755
10.48550/arxiv.2501.00805,SLIDE: Integrating Speech Language Model with LLM for Spontaneous Spoken Dialogue Generation,"Recently, ``textless"" speech language models (SLMs) based on speech units have made huge progress in generating naturalistic speech, including non-verbal vocalizations. However, the generated speech samples often lack semantic coherence. In this paper, we propose SLM and LLM Integration for spontaneous spoken Dialogue gEneration (SLIDE). Specifically, we first utilize an LLM to generate the textual content of spoken dialogue. Next, we convert the textual dialogues into phoneme sequences and use a two-tower transformer-based duration predictor to predict the duration of each phoneme. Finally, an SLM conditioned on the spoken phoneme sequences is used to vocalize the textual dialogue. Experimental results on the Fisher dataset demonstrate that our system can generate naturalistic spoken dialogue while maintaining high semantic coherence.",1,,include (senior:4),https://arxiv.org/abs/2501.00805v1,https://arxiv.org/pdf/2501.00805v1,2025,spoken language models,semantic coherence,"{""arxiv_id"": ""2501.00805"", ""title"": ""SLIDE: Integrating Speech Language Model with LLM for Spontaneous Spoken Dialogue Generation"", ""summary"": ""Recently, ``textless\"" speech language models (SLMs) based on speech units have made huge progress in generating naturalistic speech, including non-verbal vocalizations. However, the generated speech samples often lack semantic coherence. In this paper, we propose SLM and LLM Integration for spontaneous spoken Dialogue gEneration (SLIDE). Specifically, we first utilize an LLM to generate the textual content of spoken dialogue. Next, we convert the textual dialogues into phoneme sequences and use a two-tower transformer-based duration predictor to predict the duration of each phoneme. Finally, an SLM conditioned on the spoken phoneme sequences is used to vocalize the textual dialogue. Experimental results on the Fisher dataset demonstrate that our system can generate naturalistic spoken dialogue while maintaining high semantic coherence."", ""authors"": [""Haitian Lu"", ""Gaofeng Cheng"", ""Liuping Luo"", ""Leying Zhang"", ""Yanmin Qian"", ""Pengyuan Zhang""], ""published"": ""2025-01-01T11:11:07Z"", ""updated"": ""2025-01-01T11:11:07Z"", ""categories"": [""eess.AS"", ""cs.CL"", ""cs.SD""], ""pdf_url"": ""https://arxiv.org/pdf/2501.00805v1"", ""landing_url"": ""https://arxiv.org/abs/2501.00805v1"", ""doi"": ""https://doi.org/10.48550/arXiv.2501.00805""}",2501.00805,https://openalex.org/W4406032717
10.48550/arxiv.2501.04877,Real-Time Textless Dialogue Generation,"Recent advancements in large language models (LLMs) have led to significant progress in text-based dialogue systems. These systems can now generate high-quality responses that are accurate and coherent across a wide range of topics and tasks. However, spoken dialogue systems still lag behind in terms of naturalness. They tend to produce robotic interactions, with issues such as slow response times, overly generic or cautious replies, and a lack of natural rhythm and fluid turn-taking. This shortcoming is largely due to the over-reliance on the traditional cascaded design, which involve separate, sequential components, as well as the use of text as an intermediate representation. This paper propose a real-time, textless spoken dialogue generation model (RTTL-DG) that aims to overcome these challenges. Our system enables fluid turn-taking and generates responses with minimal delay by processing streaming spoken conversation directly. Additionally, our model incorporates backchannels, filters, laughter, and other paralinguistic signals, which are often absent in cascaded dialogue systems, to create more natural and human-like interactions. The implementations and generated samples are available in our repository: https://github.com/mailong25/rts2s-dg",1,,include (junior:4),https://arxiv.org/abs/2501.04877v1,https://arxiv.org/pdf/2501.04877v1,2025,turn-taking,true full-duplex,"{""arxiv_id"": ""2501.04877"", ""title"": ""Real-Time Textless Dialogue Generation"", ""summary"": ""Recent advancements in large language models (LLMs) have led to significant progress in text-based dialogue systems. These systems can now generate high-quality responses that are accurate and coherent across a wide range of topics and tasks. However, spoken dialogue systems still lag behind in terms of naturalness. They tend to produce robotic interactions, with issues such as slow response times, overly generic or cautious replies, and a lack of natural rhythm and fluid turn-taking. This shortcoming is largely due to the over-reliance on the traditional cascaded design, which involve separate, sequential components, as well as the use of text as an intermediate representation. This paper propose a real-time, textless spoken dialogue generation model (RTTL-DG) that aims to overcome these challenges. Our system enables fluid turn-taking and generates responses with minimal delay by processing streaming spoken conversation directly. Additionally, our model incorporates backchannels, filters, laughter, and other paralinguistic signals, which are often absent in cascaded dialogue systems, to create more natural and human-like interactions. The implementations and generated samples are available in our repository: https://github.com/mailong25/rts2s-dg"", ""authors"": [""Long Mai"", ""Julie Carson-Berndsen""], ""published"": ""2025-01-08T23:21:43Z"", ""updated"": ""2025-01-08T23:21:43Z"", ""categories"": [""cs.CL"", ""cs.AI"", ""cs.SD"", ""eess.AS""], ""pdf_url"": ""https://arxiv.org/pdf/2501.04877v1"", ""landing_url"": ""https://arxiv.org/abs/2501.04877v1"", ""doi"": ""https://doi.org/10.48550/arXiv.2501.04877""}",2501.04877,https://openalex.org/W4406271032
10.48550/arxiv.2502.02942,GenSE: Generative Speech Enhancement via Language Models using Hierarchical Modeling,"Semantic information refers to the meaning conveyed through words, phrases, and contextual relationships within a given linguistic structure. Humans can leverage semantic information, such as familiar linguistic patterns and contextual cues, to reconstruct incomplete or masked speech signals in noisy environments. However, existing speech enhancement (SE) approaches often overlook the rich semantic information embedded in speech, which is crucial for improving intelligibility, speaker consistency, and overall quality of enhanced speech signals. To enrich the SE model with semantic information, we employ language models as an efficient semantic learner and propose a comprehensive framework tailored for language model-based speech enhancement, called \textit{GenSE}. Specifically, we approach SE as a conditional language modeling task rather than a continuous signal regression problem defined in existing works. This is achieved by tokenizing speech signals into semantic tokens using a pre-trained self-supervised model and into acoustic tokens using a custom-designed single-quantizer neural codec model. To improve the stability of language model predictions, we propose a hierarchical modeling method that decouples the generation of clean semantic tokens and clean acoustic tokens into two distinct stages. Moreover, we introduce a token chain prompting mechanism during the acoustic token generation stage to ensure timbre consistency throughout the speech enhancement process. Experimental results on benchmark datasets demonstrate that our proposed approach outperforms state-of-the-art SE systems in terms of speech quality and generalization capability.",1,,include (senior:4),https://arxiv.org/abs/2502.02942v1,https://arxiv.org/pdf/2502.02942v1,2025,synchronous dialogue,neural codec,"{""arxiv_id"": ""2502.02942"", ""title"": ""GenSE: Generative Speech Enhancement via Language Models using Hierarchical Modeling"", ""summary"": ""Semantic information refers to the meaning conveyed through words, phrases, and contextual relationships within a given linguistic structure. Humans can leverage semantic information, such as familiar linguistic patterns and contextual cues, to reconstruct incomplete or masked speech signals in noisy environments. However, existing speech enhancement (SE) approaches often overlook the rich semantic information embedded in speech, which is crucial for improving intelligibility, speaker consistency, and overall quality of enhanced speech signals. To enrich the SE model with semantic information, we employ language models as an efficient semantic learner and propose a comprehensive framework tailored for language model-based speech enhancement, called \\textit{GenSE}. Specifically, we approach SE as a conditional language modeling task rather than a continuous signal regression problem defined in existing works. This is achieved by tokenizing speech signals into semantic tokens using a pre-trained self-supervised model and into acoustic tokens using a custom-designed single-quantizer neural codec model. To improve the stability of language model predictions, we propose a hierarchical modeling method that decouples the generation of clean semantic tokens and clean acoustic tokens into two distinct stages. Moreover, we introduce a token chain prompting mechanism during the acoustic token generation stage to ensure timbre consistency throughout the speech enhancement process. Experimental results on benchmark datasets demonstrate that our proposed approach outperforms state-of-the-art SE systems in terms of speech quality and generalization capability."", ""authors"": [""Jixun Yao"", ""Hexin Liu"", ""Chen Chen"", ""Yuchen Hu"", ""EngSiong Chng"", ""Lei Xie""], ""published"": ""2025-02-05T07:14:39Z"", ""updated"": ""2025-02-05T07:14:39Z"", ""categories"": [""eess.AS"", ""cs.SD""], ""pdf_url"": ""https://arxiv.org/pdf/2502.02942v1"", ""landing_url"": ""https://arxiv.org/abs/2502.02942v1"", ""doi"": ""https://doi.org/10.48550/arXiv.2502.02942""}",2502.02942,https://openalex.org/W4407212698
10.48550/arxiv.2502.03382,High-Fidelity Simultaneous Speech-To-Speech Translation,"We introduce Hibiki, a decoder-only model for simultaneous speech translation. Hibiki leverages a multistream language model to synchronously process source and target speech, and jointly produces text and audio tokens to perform speech-to-text and speech-to-speech translation. We furthermore address the fundamental challenge of simultaneous interpretation, which unlike its consecutive counterpart, where one waits for the end of the source utterance to start translating, adapts its flow to accumulate just enough context to produce a correct translation in real-time, chunk by chunk. To do so, we introduce a weakly-supervised method that leverages the perplexity of an off-the-shelf text translation system to identify optimal delays on a per-word basis and create aligned synthetic data. After supervised training, Hibiki performs adaptive, simultaneous speech translation with vanilla temperature sampling. On a French-English simultaneous speech translation task, Hibiki demonstrates state-of-the-art performance in translation quality, speaker fidelity and naturalness. Moreover, the simplicity of its inference process makes it compatible with batched translation and even real-time on-device deployment. We provide examples as well as models and inference code.",1,,include (senior:5),https://arxiv.org/abs/2502.03382v2,https://arxiv.org/pdf/2502.03382v2,2025,spoken language models,audio tokens,"{""arxiv_id"": ""2502.03382"", ""title"": ""High-Fidelity Simultaneous Speech-To-Speech Translation"", ""summary"": ""We introduce Hibiki, a decoder-only model for simultaneous speech translation. Hibiki leverages a multistream language model to synchronously process source and target speech, and jointly produces text and audio tokens to perform speech-to-text and speech-to-speech translation. We furthermore address the fundamental challenge of simultaneous interpretation, which unlike its consecutive counterpart, where one waits for the end of the source utterance to start translating, adapts its flow to accumulate just enough context to produce a correct translation in real-time, chunk by chunk. To do so, we introduce a weakly-supervised method that leverages the perplexity of an off-the-shelf text translation system to identify optimal delays on a per-word basis and create aligned synthetic data. After supervised training, Hibiki performs adaptive, simultaneous speech translation with vanilla temperature sampling. On a French-English simultaneous speech translation task, Hibiki demonstrates state-of-the-art performance in translation quality, speaker fidelity and naturalness. Moreover, the simplicity of its inference process makes it compatible with batched translation and even real-time on-device deployment. We provide examples as well as models and inference code."", ""authors"": [""Tom Labiausse"", ""Laurent Mazaré"", ""Edouard Grave"", ""Patrick Pérez"", ""Alexandre Défossez"", ""Neil Zeghidour""], ""published"": ""2025-02-05T17:18:55Z"", ""updated"": ""2025-02-26T09:31:58Z"", ""categories"": [""cs.CL"", ""cs.SD"", ""eess.AS""], ""pdf_url"": ""https://arxiv.org/pdf/2502.03382v2"", ""landing_url"": ""https://arxiv.org/abs/2502.03382v2"", ""doi"": ""https://doi.org/10.48550/arXiv.2502.03382""}",2502.03382,https://openalex.org/W4407231766
10.48550/arxiv.2502.14145,LLM-Enhanced Dialogue Management for Full-Duplex Spoken Dialogue Systems,"Achieving full-duplex communication in spoken dialogue systems (SDS) requires real-time coordination between listening, speaking, and thinking. This paper proposes a semantic voice activity detection (VAD) module as a dialogue manager (DM) to efficiently manage turn-taking in full-duplex SDS. Implemented as a lightweight (0.5B) LLM fine-tuned on full-duplex conversation data, the semantic VAD predicts four control tokens to regulate turn-switching and turn-keeping, distinguishing between intentional and unintentional barge-ins while detecting query completion for handling user pauses and hesitations. By processing input speech in short intervals, the semantic VAD enables real-time decision-making, while the core dialogue engine (CDE) is only activated for response generation, reducing computational overhead. This design allows independent DM optimization without retraining the CDE, balancing interaction accuracy and inference efficiency for scalable, next-generation full-duplex SDS.",1,,include (junior:4),https://arxiv.org/abs/2502.14145v2,https://arxiv.org/pdf/2502.14145v2,2025,full-duplex,true full-duplex,"{""arxiv_id"": ""2502.14145"", ""title"": ""LLM-Enhanced Dialogue Management for Full-Duplex Spoken Dialogue Systems"", ""summary"": ""Achieving full-duplex communication in spoken dialogue systems (SDS) requires real-time coordination between listening, speaking, and thinking. This paper proposes a semantic voice activity detection (VAD) module as a dialogue manager (DM) to efficiently manage turn-taking in full-duplex SDS. Implemented as a lightweight (0.5B) LLM fine-tuned on full-duplex conversation data, the semantic VAD predicts four control tokens to regulate turn-switching and turn-keeping, distinguishing between intentional and unintentional barge-ins while detecting query completion for handling user pauses and hesitations. By processing input speech in short intervals, the semantic VAD enables real-time decision-making, while the core dialogue engine (CDE) is only activated for response generation, reducing computational overhead. This design allows independent DM optimization without retraining the CDE, balancing interaction accuracy and inference efficiency for scalable, next-generation full-duplex SDS."", ""authors"": [""Hao Zhang"", ""Weiwei Li"", ""Rilin Chen"", ""Vinay Kothapally"", ""Meng Yu"", ""Dong Yu""], ""published"": ""2025-02-19T23:15:13Z"", ""updated"": ""2025-02-24T19:08:11Z"", ""categories"": [""cs.CL"", ""eess.AS""], ""pdf_url"": ""https://arxiv.org/pdf/2502.14145v2"", ""landing_url"": ""https://arxiv.org/abs/2502.14145v2"", ""doi"": ""https://doi.org/10.48550/arXiv.2502.14145""}",2502.14145,https://openalex.org/W4407806352
10.48550/arxiv.2502.17239,Baichuan-Audio: A Unified Framework for End-to-End Speech Interaction,"We introduce Baichuan-Audio, an end-to-end audio large language model that seamlessly integrates audio understanding and generation. It features a text-guided aligned speech generation mechanism, enabling real-time speech interaction with both comprehension and generation capabilities. Baichuan-Audio leverages a pre-trained ASR model, followed by multi-codebook discretization of speech at a frame rate of 12.5 Hz. This multi-codebook setup ensures that speech tokens retain both semantic and acoustic information. To further enhance modeling, an independent audio head is employed to process audio tokens, effectively capturing their unique characteristics. To mitigate the loss of intelligence during pre-training and preserve the original capabilities of the LLM, we propose a two-stage pre-training strategy that maintains language understanding while enhancing audio modeling. Following alignment, the model excels in real-time speech-based conversation and exhibits outstanding question-answering capabilities, demonstrating its versatility and efficiency. The proposed model demonstrates superior performance in real-time spoken dialogue and exhibits strong question-answering abilities. Our code, model and training data are available at https://github.com/baichuan-inc/Baichuan-Audio",1,,include (junior:4),https://arxiv.org/abs/2502.17239v1,https://arxiv.org/pdf/2502.17239v1,2025,spoken language models,audio tokens,"{""arxiv_id"": ""2502.17239"", ""title"": ""Baichuan-Audio: A Unified Framework for End-to-End Speech Interaction"", ""summary"": ""We introduce Baichuan-Audio, an end-to-end audio large language model that seamlessly integrates audio understanding and generation. It features a text-guided aligned speech generation mechanism, enabling real-time speech interaction with both comprehension and generation capabilities. Baichuan-Audio leverages a pre-trained ASR model, followed by multi-codebook discretization of speech at a frame rate of 12.5 Hz. This multi-codebook setup ensures that speech tokens retain both semantic and acoustic information. To further enhance modeling, an independent audio head is employed to process audio tokens, effectively capturing their unique characteristics. To mitigate the loss of intelligence during pre-training and preserve the original capabilities of the LLM, we propose a two-stage pre-training strategy that maintains language understanding while enhancing audio modeling. Following alignment, the model excels in real-time speech-based conversation and exhibits outstanding question-answering capabilities, demonstrating its versatility and efficiency. The proposed model demonstrates superior performance in real-time spoken dialogue and exhibits strong question-answering abilities. Our code, model and training data are available at https://github.com/baichuan-inc/Baichuan-Audio"", ""authors"": [""Tianpeng Li"", ""Jun Liu"", ""Tao Zhang"", ""Yuanbo Fang"", ""Da Pan"", ""Mingrui Wang"", ""Zheng Liang"", ""Zehuan Li"", ""Mingan Lin"", ""Guosheng Dong"", ""Jianhua Xu"", ""Haoze Sun"", ""Zenan Zhou"", ""Weipeng Chen""], ""published"": ""2025-02-24T15:16:34Z"", ""updated"": ""2025-02-24T15:16:34Z"", ""categories"": [""cs.CL"", ""cs.SD"", ""eess.AS""], ""pdf_url"": ""https://arxiv.org/pdf/2502.17239v1"", ""landing_url"": ""https://arxiv.org/abs/2502.17239v1"", ""doi"": ""https://doi.org/10.48550/arXiv.2502.17239""}",2502.17239,https://openalex.org/W4414849600
10.48550/arxiv.2503.01174,Talking Turns: Benchmarking Audio Foundation Models on Turn-Taking Dynamics,"The recent wave of audio foundation models (FMs) could provide new capabilities for conversational modeling. However, there have been limited efforts to evaluate these audio FMs comprehensively on their ability to have natural and interactive conversations. To engage in meaningful conversation with the end user, we would want the FMs to additionally perform a fluent succession of turns without too much overlapping speech or long stretches of silence. Inspired by this, we ask whether the recently proposed audio FMs can understand, predict, and perform turn-taking events? To answer this, we propose a novel evaluation protocol that can assess spoken dialog system's turn-taking capabilities using a supervised model as a judge that has been trained to predict turn-taking events in human-human conversations. Using this protocol, we present the first comprehensive user study that evaluates existing spoken dialogue systems on their ability to perform turn-taking events and reveal many interesting insights, such as they sometimes do not understand when to speak up, can interrupt too aggressively and rarely backchannel. We further evaluate multiple open-source and proprietary audio FMs accessible through APIs on carefully curated test benchmarks from Switchboard to measure their ability to understand and predict turn-taking events and identify significant room for improvement. We will open source our evaluation platform to promote the development of advanced conversational AI systems.",1,,include (junior:5),https://arxiv.org/abs/2503.01174v1,https://arxiv.org/pdf/2503.01174v1,2025,turn-taking,true full-duplex,"{""arxiv_id"": ""2503.01174"", ""title"": ""Talking Turns: Benchmarking Audio Foundation Models on Turn-Taking Dynamics"", ""summary"": ""The recent wave of audio foundation models (FMs) could provide new capabilities for conversational modeling. However, there have been limited efforts to evaluate these audio FMs comprehensively on their ability to have natural and interactive conversations. To engage in meaningful conversation with the end user, we would want the FMs to additionally perform a fluent succession of turns without too much overlapping speech or long stretches of silence. Inspired by this, we ask whether the recently proposed audio FMs can understand, predict, and perform turn-taking events? To answer this, we propose a novel evaluation protocol that can assess spoken dialog system's turn-taking capabilities using a supervised model as a judge that has been trained to predict turn-taking events in human-human conversations. Using this protocol, we present the first comprehensive user study that evaluates existing spoken dialogue systems on their ability to perform turn-taking events and reveal many interesting insights, such as they sometimes do not understand when to speak up, can interrupt too aggressively and rarely backchannel. We further evaluate multiple open-source and proprietary audio FMs accessible through APIs on carefully curated test benchmarks from Switchboard to measure their ability to understand and predict turn-taking events and identify significant room for improvement. We will open source our evaluation platform to promote the development of advanced conversational AI systems."", ""authors"": [""Siddhant Arora"", ""Zhiyun Lu"", ""Chung-Cheng Chiu"", ""Ruoming Pang"", ""Shinji Watanabe""], ""published"": ""2025-03-03T04:46:04Z"", ""updated"": ""2025-03-03T04:46:04Z"", ""categories"": [""cs.CL"", ""cs.SD"", ""eess.AS""], ""pdf_url"": ""https://arxiv.org/pdf/2503.01174v1"", ""landing_url"": ""https://arxiv.org/abs/2503.01174v1"", ""doi"": ""https://doi.org/10.48550/arXiv.2503.01174""}",2503.01174,https://openalex.org/W4415084350
10.48550/arxiv.2503.04721,Full-Duplex-Bench: A Benchmark to Evaluate Full-duplex Spoken Dialogue Models on Turn-taking Capabilities,"Spoken dialogue modeling poses challenges beyond text-based language modeling, requiring real-time interaction, turn-taking, and backchanneling. While most Spoken Dialogue Models (SDMs) operate in half-duplex mode-processing one turn at a time - emerging full-duplex SDMs can listen and speak simultaneously, enabling more natural conversations. However, current evaluations remain limited, focusing mainly on turn-based metrics or coarse corpus-level analyses. To address this, we introduce Full-Duplex-Bench, a benchmark that systematically evaluates key interactive behaviors: pause handling, backchanneling, turn-taking, and interruption management. Our framework uses automatic metrics for consistent, reproducible assessment and provides a fair, fast evaluation setup. By releasing our benchmark and code, we aim to advance spoken dialogue modeling and foster the development of more natural and engaging SDMs.",1,,include (junior:5),https://arxiv.org/abs/2503.04721v3,https://arxiv.org/pdf/2503.04721v3,2025,full-duplex,true full-duplex,"{""arxiv_id"": ""2503.04721"", ""title"": ""Full-Duplex-Bench: A Benchmark to Evaluate Full-duplex Spoken Dialogue Models on Turn-taking Capabilities"", ""summary"": ""Spoken dialogue modeling poses challenges beyond text-based language modeling, requiring real-time interaction, turn-taking, and backchanneling. While most Spoken Dialogue Models (SDMs) operate in half-duplex mode-processing one turn at a time - emerging full-duplex SDMs can listen and speak simultaneously, enabling more natural conversations. However, current evaluations remain limited, focusing mainly on turn-based metrics or coarse corpus-level analyses. To address this, we introduce Full-Duplex-Bench, a benchmark that systematically evaluates key interactive behaviors: pause handling, backchanneling, turn-taking, and interruption management. Our framework uses automatic metrics for consistent, reproducible assessment and provides a fair, fast evaluation setup. By releasing our benchmark and code, we aim to advance spoken dialogue modeling and foster the development of more natural and engaging SDMs."", ""authors"": [""Guan-Ting Lin"", ""Jiachen Lian"", ""Tingle Li"", ""Qirui Wang"", ""Gopala Anumanchipalli"", ""Alexander H. Liu"", ""Hung-yi Lee""], ""published"": ""2025-03-06T18:59:16Z"", ""updated"": ""2025-08-16T05:46:19Z"", ""categories"": [""cs.CL"", ""eess.AS""], ""pdf_url"": ""https://arxiv.org/pdf/2503.04721v3"", ""landing_url"": ""https://arxiv.org/abs/2503.04721v3"", ""doi"": ""https://doi.org/10.48550/arXiv.2503.04721""}",2503.04721,https://openalex.org/W4416114359
10.1109/jstsp.2024.3488557,Universal Speech Token Learning via Low-Bitrate Neural Codec and Pretrained Representations,"Current large speech language models are mainly based on semantic tokens from discretization of self-supervised learned representations and acoustic tokens from a neural codec, following a semantic-modeling and acoustic-synthesis paradigm. However, semantic tokens discard paralinguistic attributes of speakers that is important for natural spoken communication, while prompt-based acoustic synthesis from semantic tokens has limits in recovering paralinguistic details and suffers from robustness issues, especially when there are domain gaps between the prompt and the target. This paper unifies two types of tokens and proposes the UniCodec, a universal speech token learning that encapsulates all semantics of speech, including linguistic and paralinguistic information, into a compact and semantically-disentangled unified token. Such a unified token can not only benefit speech language models in understanding with paralinguistic hints but also help speech generation with high-quality output. A low-bitrate neural codec is leveraged to learn such disentangled discrete representations at global and local scales, with knowledge distilled from self-supervised learned features. Extensive evaluations on multilingual datasets demonstrate its effectiveness in generating natural, expressive and long-term consistent output quality with paralinguistic attributes well preserved in several speech processing tasks.",1,,include (senior:4),https://arxiv.org/abs/2503.12115v2,https://arxiv.org/pdf/2503.12115v2,2025,spoken language models,neural codec,"{""arxiv_id"": ""2503.12115"", ""title"": ""Universal Speech Token Learning via Low-Bitrate Neural Codec and Pretrained Representations"", ""summary"": ""Current large speech language models are mainly based on semantic tokens from discretization of self-supervised learned representations and acoustic tokens from a neural codec, following a semantic-modeling and acoustic-synthesis paradigm. However, semantic tokens discard paralinguistic attributes of speakers that is important for natural spoken communication, while prompt-based acoustic synthesis from semantic tokens has limits in recovering paralinguistic details and suffers from robustness issues, especially when there are domain gaps between the prompt and the target. This paper unifies two types of tokens and proposes the UniCodec, a universal speech token learning that encapsulates all semantics of speech, including linguistic and paralinguistic information, into a compact and semantically-disentangled unified token. Such a unified token can not only benefit speech language models in understanding with paralinguistic hints but also help speech generation with high-quality output. A low-bitrate neural codec is leveraged to learn such disentangled discrete representations at global and local scales, with knowledge distilled from self-supervised learned features. Extensive evaluations on multilingual datasets demonstrate its effectiveness in generating natural, expressive and long-term consistent output quality with paralinguistic attributes well preserved in several speech processing tasks."", ""authors"": [""Xue Jiang"", ""Xiulian Peng"", ""Yuan Zhang"", ""Yan Lu""], ""published"": ""2025-03-15T12:50:43Z"", ""updated"": ""2025-10-15T06:52:30Z"", ""categories"": [""cs.SD"", ""cs.AI"", ""eess.AS""], ""pdf_url"": ""https://arxiv.org/pdf/2503.12115v2"", ""landing_url"": ""https://arxiv.org/abs/2503.12115v2"", ""doi"": ""https://doi.org/10.1109/JSTSP.2024.3488557""}",2503.12115,https://openalex.org/W4403918744
10.48550/arxiv.2503.20215,Qwen2.5-Omni Technical Report,"In this report, we present Qwen2.5-Omni, an end-to-end multimodal model designed to perceive diverse modalities, including text, images, audio, and video, while simultaneously generating text and natural speech responses in a streaming manner. To enable the streaming of multimodal information inputs, both audio and visual encoders utilize a block-wise processing approach. To synchronize the timestamps of video inputs with audio, we organize the audio and video sequentially in an interleaved manner and propose a novel position embedding approach, named TMRoPE(Time-aligned Multimodal RoPE). To concurrently generate text and speech while avoiding interference between the two modalities, we propose \textbf{Thinker-Talker} architecture. In this framework, Thinker functions as a large language model tasked with text generation, while Talker is a dual-track autoregressive model that directly utilizes the hidden representations from the Thinker to produce audio tokens as output. Both the Thinker and Talker models are designed to be trained and inferred in an end-to-end manner. For decoding audio tokens in a streaming manner, we introduce a sliding-window DiT that restricts the receptive field, aiming to reduce the initial package delay. Qwen2.5-Omni is comparable with the similarly sized Qwen2.5-VL and outperforms Qwen2-Audio. Furthermore, Qwen2.5-Omni achieves state-of-the-art performance on multimodal benchmarks like Omni-Bench. Notably, Qwen2.5-Omni's performance in end-to-end speech instruction following is comparable to its capabilities with text inputs, as evidenced by benchmarks such as MMLU and GSM8K. As for speech generation, Qwen2.5-Omni's streaming Talker outperforms most existing streaming and non-streaming alternatives in robustness and naturalness.",1,,include (senior:4),https://arxiv.org/abs/2503.20215v1,https://arxiv.org/pdf/2503.20215v1,2025,spoken language models,audio tokens,"{""arxiv_id"": ""2503.20215"", ""title"": ""Qwen2.5-Omni Technical Report"", ""summary"": ""In this report, we present Qwen2.5-Omni, an end-to-end multimodal model designed to perceive diverse modalities, including text, images, audio, and video, while simultaneously generating text and natural speech responses in a streaming manner. To enable the streaming of multimodal information inputs, both audio and visual encoders utilize a block-wise processing approach. To synchronize the timestamps of video inputs with audio, we organize the audio and video sequentially in an interleaved manner and propose a novel position embedding approach, named TMRoPE(Time-aligned Multimodal RoPE). To concurrently generate text and speech while avoiding interference between the two modalities, we propose \\textbf{Thinker-Talker} architecture. In this framework, Thinker functions as a large language model tasked with text generation, while Talker is a dual-track autoregressive model that directly utilizes the hidden representations from the Thinker to produce audio tokens as output. Both the Thinker and Talker models are designed to be trained and inferred in an end-to-end manner. For decoding audio tokens in a streaming manner, we introduce a sliding-window DiT that restricts the receptive field, aiming to reduce the initial package delay. Qwen2.5-Omni is comparable with the similarly sized Qwen2.5-VL and outperforms Qwen2-Audio. Furthermore, Qwen2.5-Omni achieves state-of-the-art performance on multimodal benchmarks like Omni-Bench. Notably, Qwen2.5-Omni's performance in end-to-end speech instruction following is comparable to its capabilities with text inputs, as evidenced by benchmarks such as MMLU and GSM8K. As for speech generation, Qwen2.5-Omni's streaming Talker outperforms most existing streaming and non-streaming alternatives in robustness and naturalness."", ""authors"": [""Jin Xu"", ""Zhifang Guo"", ""Jinzheng He"", ""Hangrui Hu"", ""Ting He"", ""Shuai Bai"", ""Keqin Chen"", ""Jialin Wang"", ""Yang Fan"", ""Kai Dang"", ""Bin Zhang"", ""Xiong Wang"", ""Yunfei Chu"", ""Junyang Lin""], ""published"": ""2025-03-26T04:17:55Z"", ""updated"": ""2025-03-26T04:17:55Z"", ""categories"": [""cs.CL"", ""cs.CV"", ""cs.SD"", ""eess.AS""], ""pdf_url"": ""https://arxiv.org/pdf/2503.20215v1"", ""landing_url"": ""https://arxiv.org/abs/2503.20215v1"", ""doi"": ""https://doi.org/10.48550/arXiv.2503.20215""}",2503.20215,https://openalex.org/W4409051416
10.1109/icassp49660.2025.10888809,Make Some Noise: Towards LLM audio reasoning and generation using sound tokens,"Integrating audio comprehension and generation into large language models (LLMs) remains challenging due to the continuous nature of audio and the resulting high sampling rates. Here, we introduce a novel approach that combines Variational Quantization with Conditional Flow Matching to convert audio into ultra-low bitrate discrete tokens of 0.23kpbs, allowing for seamless integration with text tokens in LLMs. We fine-tuned a pretrained text-based LLM using Low-Rank Adaptation (LoRA) to assess its effectiveness in achieving true multimodal capabilities, i.e., audio comprehension and generation. Our tokenizer outperforms a traditional VQ-VAE across various datasets with diverse acoustic events. Despite the substantial loss of fine-grained details through audio tokenization, our multimodal LLM trained with discrete tokens achieves competitive results in audio comprehension with state-of-the-art methods, though audio generation is poor. Our results highlight the need for larger, more diverse datasets and improved evaluation metrics to advance multimodal LLM performance.",1,,include (senior:4),https://arxiv.org/abs/2503.22275v1,https://arxiv.org/pdf/2503.22275v1,2025,synchronous dialogue,discrete tokens,"{""arxiv_id"": ""2503.22275"", ""title"": ""Make Some Noise: Towards LLM audio reasoning and generation using sound tokens"", ""summary"": ""Integrating audio comprehension and generation into large language models (LLMs) remains challenging due to the continuous nature of audio and the resulting high sampling rates. Here, we introduce a novel approach that combines Variational Quantization with Conditional Flow Matching to convert audio into ultra-low bitrate discrete tokens of 0.23kpbs, allowing for seamless integration with text tokens in LLMs. We fine-tuned a pretrained text-based LLM using Low-Rank Adaptation (LoRA) to assess its effectiveness in achieving true multimodal capabilities, i.e., audio comprehension and generation. Our tokenizer outperforms a traditional VQ-VAE across various datasets with diverse acoustic events. Despite the substantial loss of fine-grained details through audio tokenization, our multimodal LLM trained with discrete tokens achieves competitive results in audio comprehension with state-of-the-art methods, though audio generation is poor. Our results highlight the need for larger, more diverse datasets and improved evaluation metrics to advance multimodal LLM performance."", ""authors"": [""Shivam Mehta"", ""Nebojsa Jojic"", ""Hannes Gamper""], ""published"": ""2025-03-28T09:43:47Z"", ""updated"": ""2025-03-28T09:43:47Z"", ""categories"": [""eess.AS"", ""cs.AI"", ""cs.SD""], ""pdf_url"": ""https://arxiv.org/pdf/2503.22275v1"", ""landing_url"": ""https://arxiv.org/abs/2503.22275v1"", ""doi"": ""https://doi.org/10.1109/ICASSP49660.2025.10888809""}",2503.22275,https://openalex.org/W4408354829
10.48550/arxiv.2504.07053,TASTE: Text-Aligned Speech Tokenization and Embedding for Spoken Language Modeling,"Recent efforts target spoken language models (SLMs) that not only listen but also speak for more natural human-LLM interaction. Joint speech-text modeling is a promising direction to achieve this. However, the effectiveness of recent speech tokens for joint modeling remains underexplored. To address this, we introduce Text-Aligned Speech Tokenization and Embedding (TASTE), a method that directly addresses the modality gap by aligning speech token with the corresponding text transcription during the tokenization stage. We propose a method that can achieve this through a attention-based aggregation mechanism and with speech reconstruction as the training objective. We conduct extensive experiments and show that TASTE can preserve essential paralinguistic information while dramatically reducing the token sequence length. With TASTE, we perform straightforward joint spoken language modeling by using Low-Rank Adaptation on the pre-trained text LLM. Experimental results show that TASTE-based SLMs perform comparable to previous work on SALMON and StoryCloze; while significantly outperform other pre-trained SLMs on speech continuation across subjective and objective evaluations. To our knowledge, TASTE is the first end-to-end approach that utilizes a reconstruction objective to automatically learn a text-aligned speech tokenization and embedding suitable for spoken language modeling. Our demo, code, and model are available at https://mtkresearch.github.io/TASTE-SpokenLM.github.io.",1,,include (junior:5),https://arxiv.org/abs/2504.07053v2,https://arxiv.org/pdf/2504.07053v2,2025,spoken language models,true full-duplex,"{""arxiv_id"": ""2504.07053"", ""title"": ""TASTE: Text-Aligned Speech Tokenization and Embedding for Spoken Language Modeling"", ""summary"": ""Recent efforts target spoken language models (SLMs) that not only listen but also speak for more natural human-LLM interaction. Joint speech-text modeling is a promising direction to achieve this. However, the effectiveness of recent speech tokens for joint modeling remains underexplored. To address this, we introduce Text-Aligned Speech Tokenization and Embedding (TASTE), a method that directly addresses the modality gap by aligning speech token with the corresponding text transcription during the tokenization stage. We propose a method that can achieve this through a attention-based aggregation mechanism and with speech reconstruction as the training objective. We conduct extensive experiments and show that TASTE can preserve essential paralinguistic information while dramatically reducing the token sequence length. With TASTE, we perform straightforward joint spoken language modeling by using Low-Rank Adaptation on the pre-trained text LLM. Experimental results show that TASTE-based SLMs perform comparable to previous work on SALMON and StoryCloze; while significantly outperform other pre-trained SLMs on speech continuation across subjective and objective evaluations. To our knowledge, TASTE is the first end-to-end approach that utilizes a reconstruction objective to automatically learn a text-aligned speech tokenization and embedding suitable for spoken language modeling. Our demo, code, and model are available at https://mtkresearch.github.io/TASTE-SpokenLM.github.io."", ""authors"": [""Liang-Hsuan Tseng"", ""Yi-Chang Chen"", ""Kuan-Yi Lee"", ""Da-Shan Shiu"", ""Hung-yi Lee""], ""published"": ""2025-04-09T17:14:33Z"", ""updated"": ""2025-05-22T14:49:03Z"", ""categories"": [""cs.CL"", ""cs.SD"", ""eess.AS""], ""pdf_url"": ""https://arxiv.org/pdf/2504.07053v2"", ""landing_url"": ""https://arxiv.org/abs/2504.07053v2"", ""doi"": ""https://doi.org/10.48550/arXiv.2504.07053""}",2504.07053,https://openalex.org/W4417248906
