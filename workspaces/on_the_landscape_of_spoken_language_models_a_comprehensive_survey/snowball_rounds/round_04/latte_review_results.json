[
  {
    "title": "Unraveling the landscape of large language models: a systematic review and future perspectives",
    "abstract": "Purpose The rapid rise of large language models (LLMs) has propelled them to the forefront of applications in natural language processing (NLP). This paper aims to present a comprehensive examination of the research landscape in LLMs, providing an overview of the prevailing themes and topics within this dynamic domain. Design/methodology/approach Drawing from an extensive corpus of 198 records published between 1996 to 2023 from the relevant academic database encompassing journal articles, books, book chapters, conference papers and selected working papers, this study delves deep into the multifaceted world of LLM research. In this study, the authors employed the BERTopic algorithm, a recent advancement in topic modeling, to conduct a comprehensive analysis of the data after it had been meticulously cleaned and preprocessed. BERTopic leverages the power of transformer-based language models like bidirectional encoder representations from transformers (BERT) to generate more meaningful and coherent topics. This approach facilitates the identification of hidden patterns within the data, enabling authors to uncover valuable insights that might otherwise have remained obscure. The analysis revealed four distinct clusters of topics in LLM research: “language and NLP”, “education and teaching”, “clinical and medical applications” and “speech and recognition techniques”. Each cluster embodies a unique aspect of LLM application and showcases the breadth of possibilities that LLM technology has to offer. In addition to presenting the research findings, this paper identifies key challenges and opportunities in the realm of LLMs. It underscores the necessity for further investigation in specific areas, including the paramount importance of addressing potential biases, transparency and explainability, data privacy and security, and responsible deployment of LLM technology. Findings The analysis revealed four distinct clusters of topics in LLM research: “language and NLP”, “education and teaching”, “clinical and medical applications” and “speech and recognition techniques”. Each cluster embodies a unique aspect of LLM application and showcases the breadth of possibilities that LLM technology has to offer. In addition to presenting the research findings, this paper identifies key challenges and opportunities in the realm of LLMs. It underscores the necessity for further investigation in specific areas, including the paramount importance of addressing potential biases, transparency and explainability, data privacy and security, and responsible deployment of LLM technology. Practical implications This classification offers practical guidance for researchers, developers, educators, and policymakers to focus efforts and resources. The study underscores the importance of addressing challenges in LLMs, including potential biases, transparency, data privacy, and responsible deployment. Policymakers can utilize this information to shape regulations, while developers can tailor technology development based on the diverse applications identified. The findings also emphasize the need for interdisciplinary collaboration and highlight ethical considerations, providing a roadmap for navigating the complex landscape of LLM research and applications. Originality/value This study stands out as the first to examine the evolution of LLMs across such a long time frame and across such diversified disciplines. It provides a unique perspective on the key areas of LLM research, highlighting the breadth and depth of LLM’s evolution.",
    "metadata": {
      "title": "Unraveling the landscape of large language models: a systematic review and future perspectives",
      "summary": "Purpose The rapid rise of large language models (LLMs) has propelled them to the forefront of applications in natural language processing (NLP). This paper aims to present a comprehensive examination of the research landscape in LLMs, providing an overview of the prevailing themes and topics within this dynamic domain. Design/methodology/approach Drawing from an extensive corpus of 198 records published between 1996 to 2023 from the relevant academic database encompassing journal articles, books, book chapters, conference papers and selected working papers, this study delves deep into the multifaceted world of LLM research. In this study, the authors employed the BERTopic algorithm, a recent advancement in topic modeling, to conduct a comprehensive analysis of the data after it had been meticulously cleaned and preprocessed. BERTopic leverages the power of transformer-based language models like bidirectional encoder representations from transformers (BERT) to generate more meaningful and coherent topics. This approach facilitates the identification of hidden patterns within the data, enabling authors to uncover valuable insights that might otherwise have remained obscure. The analysis revealed four distinct clusters of topics in LLM research: “language and NLP”, “education and teaching”, “clinical and medical applications” and “speech and recognition techniques”. Each cluster embodies a unique aspect of LLM application and showcases the breadth of possibilities that LLM technology has to offer. In addition to presenting the research findings, this paper identifies key challenges and opportunities in the realm of LLMs. It underscores the necessity for further investigation in specific areas, including the paramount importance of addressing potential biases, transparency and explainability, data privacy and security, and responsible deployment of LLM technology. Findings The analysis revealed four distinct clusters of topics in LLM research: “language and NLP”, “education and teaching”, “clinical and medical applications” and “speech and recognition techniques”. Each cluster embodies a unique aspect of LLM application and showcases the breadth of possibilities that LLM technology has to offer. In addition to presenting the research findings, this paper identifies key challenges and opportunities in the realm of LLMs. It underscores the necessity for further investigation in specific areas, including the paramount importance of addressing potential biases, transparency and explainability, data privacy and security, and responsible deployment of LLM technology. Practical implications This classification offers practical guidance for researchers, developers, educators, and policymakers to focus efforts and resources. The study underscores the importance of addressing challenges in LLMs, including potential biases, transparency, data privacy, and responsible deployment. Policymakers can utilize this information to shape regulations, while developers can tailor technology development based on the diverse applications identified. The findings also emphasize the need for interdisciplinary collaboration and highlight ethical considerations, providing a roadmap for navigating the complex landscape of LLM research and applications. Originality/value This study stands out as the first to examine the evolution of LLMs across such a long time frame and across such diversified disciplines. It provides a unique perspective on the key areas of LLM research, highlighting the breadth and depth of LLM’s evolution.",
      "abstract": "Purpose The rapid rise of large language models (LLMs) has propelled them to the forefront of applications in natural language processing (NLP). This paper aims to present a comprehensive examination of the research landscape in LLMs, providing an overview of the prevailing themes and topics within this dynamic domain. Design/methodology/approach Drawing from an extensive corpus of 198 records published between 1996 to 2023 from the relevant academic database encompassing journal articles, books, book chapters, conference papers and selected working papers, this study delves deep into the multifaceted world of LLM research. In this study, the authors employed the BERTopic algorithm, a recent advancement in topic modeling, to conduct a comprehensive analysis of the data after it had been meticulously cleaned and preprocessed. BERTopic leverages the power of transformer-based language models like bidirectional encoder representations from transformers (BERT) to generate more meaningful and coherent topics. This approach facilitates the identification of hidden patterns within the data, enabling authors to uncover valuable insights that might otherwise have remained obscure. The analysis revealed four distinct clusters of topics in LLM research: “language and NLP”, “education and teaching”, “clinical and medical applications” and “speech and recognition techniques”. Each cluster embodies a unique aspect of LLM application and showcases the breadth of possibilities that LLM technology has to offer. In addition to presenting the research findings, this paper identifies key challenges and opportunities in the realm of LLMs. It underscores the necessity for further investigation in specific areas, including the paramount importance of addressing potential biases, transparency and explainability, data privacy and security, and responsible deployment of LLM technology. Findings The analysis revealed four distinct clusters of topics in LLM research: “language and NLP”, “education and teaching”, “clinical and medical applications” and “speech and recognition techniques”. Each cluster embodies a unique aspect of LLM application and showcases the breadth of possibilities that LLM technology has to offer. In addition to presenting the research findings, this paper identifies key challenges and opportunities in the realm of LLMs. It underscores the necessity for further investigation in specific areas, including the paramount importance of addressing potential biases, transparency and explainability, data privacy and security, and responsible deployment of LLM technology. Practical implications This classification offers practical guidance for researchers, developers, educators, and policymakers to focus efforts and resources. The study underscores the importance of addressing challenges in LLMs, including potential biases, transparency, data privacy, and responsible deployment. Policymakers can utilize this information to shape regulations, while developers can tailor technology development based on the diverse applications identified. The findings also emphasize the need for interdisciplinary collaboration and highlight ethical considerations, providing a roadmap for navigating the complex landscape of LLM research and applications. Originality/value This study stands out as the first to examine the evolution of LLMs across such a long time frame and across such diversified disciplines. It provides a unique perspective on the key areas of LLM research, highlighting the breadth and depth of LLM’s evolution.",
      "doi": "https://doi.org/10.1108/jebde-08-2023-0015",
      "openalex_id": "https://openalex.org/W4389881233",
      "arxiv_id": "",
      "publication_date": "2023-12-18",
      "published": "2023-12-18",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item focuses on general large language models without centering on Spoken Language Models, speech-based modeling, or full-duplex interaction; it lacks required keywords and speech-centric evaluation, thus does not meet the inclusion criteria and does meet exclusion criteria by omission.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item focuses on general large language models without centering on Spoken Language Models, speech-based modeling, or full-duplex interaction; it lacks required keywords and speech-centric evaluation, thus does not meet the inclusion criteria and does meet exclusion criteria by omission.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study is a comprehensive survey of large language models across various application domains but does not focus on spoken language models or speech as the core modeling target; it does not address full-duplex spoken language modeling, synchronization for speech interaction, or detailed evaluations of speech-based interaction metrics as specified in the inclusion criteria. Additionally, the research covers a wide timeline (1996-2023) not limited to the past 3 years and does not specifically mention the key concepts or terminology required (e.g., Spoken Language Model, Speech Language Model). Therefore, it does not meet the core inclusion criteria related to spoken language models and speech-based modeling and interaction.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study is a comprehensive survey of large language models across various application domains but does not focus on spoken language models or speech as the core modeling target; it does not address full-duplex spoken language modeling, synchronization for speech interaction, or detailed evaluations of speech-based interaction metrics as specified in the inclusion criteria. Additionally, the research covers a wide timeline (1996-2023) not limited to the past 3 years and does not specifically mention the key concepts or terminology required (e.g., Spoken Language Model, Speech Language Model). Therefore, it does not meet the core inclusion criteria related to spoken language models and speech-based modeling and interaction.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "End-to-End Speech Recognition Contextualization with Large Language Models",
    "abstract": "In recent years, Large Language Models (LLMs) have garnered significant attention from the research community due to their exceptional performance and generalization capabilities. In this paper, we introduce a novel method for contextualizing speech recognition models incorporating LLMs. Our approach casts speech recognition as a mixed-modal language modeling task based on a pretrained LLM. We use audio features, along with optional text tokens for context, to train the system to complete transcriptions in a decoder-only fashion. As a result, the system implicitly learns how to leverage unstructured contextual information during training. Our empirical results demonstrate a significant improvement in performance, with a 6% WER reduction when additional textual context is provided. Moreover, we find that our method performs competitively, improving by 7.5% WER overall and 17% WER on rare words, compared to a baseline contextualized RNN-T system that has been trained on a speech dataset more than twenty-five times larger. Overall, we demonstrate that by adding only a handful of trainable parameters via adapters, we can unlock the contextualized speech recognition capability of the pretrained LLM while maintaining the same text-only input functionality.",
    "metadata": {
      "title": "End-to-End Speech Recognition Contextualization with Large Language Models",
      "summary": "In recent years, Large Language Models (LLMs) have garnered significant attention from the research community due to their exceptional performance and generalization capabilities. In this paper, we introduce a novel method for contextualizing speech recognition models incorporating LLMs. Our approach casts speech recognition as a mixed-modal language modeling task based on a pretrained LLM. We use audio features, along with optional text tokens for context, to train the system to complete transcriptions in a decoder-only fashion. As a result, the system implicitly learns how to leverage unstructured contextual information during training. Our empirical results demonstrate a significant improvement in performance, with a 6% WER reduction when additional textual context is provided. Moreover, we find that our method performs competitively, improving by 7.5% WER overall and 17% WER on rare words, compared to a baseline contextualized RNN-T system that has been trained on a speech dataset more than twenty-five times larger. Overall, we demonstrate that by adding only a handful of trainable parameters via adapters, we can unlock the contextualized speech recognition capability of the pretrained LLM while maintaining the same text-only input functionality.",
      "abstract": "In recent years, Large Language Models (LLMs) have garnered significant attention from the research community due to their exceptional performance and generalization capabilities. In this paper, we introduce a novel method for contextualizing speech recognition models incorporating LLMs. Our approach casts speech recognition as a mixed-modal language modeling task based on a pretrained LLM. We use audio features, along with optional text tokens for context, to train the system to complete transcriptions in a decoder-only fashion. As a result, the system implicitly learns how to leverage unstructured contextual information during training. Our empirical results demonstrate a significant improvement in performance, with a 6% WER reduction when additional textual context is provided. Moreover, we find that our method performs competitively, improving by 7.5% WER overall and 17% WER on rare words, compared to a baseline contextualized RNN-T system that has been trained on a speech dataset more than twenty-five times larger. Overall, we demonstrate that by adding only a handful of trainable parameters via adapters, we can unlock the contextualized speech recognition capability of the pretrained LLM while maintaining the same text-only input functionality.",
      "doi": "https://doi.org/10.1109/icassp48485.2024.10446898",
      "openalex_id": "https://openalex.org/W4392903288",
      "arxiv_id": "",
      "publication_date": "2024-03-18",
      "published": "2024-03-18",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The work treats speech recognition as a mixed-modal language modeling task using LLMs, explicitly framing speech-language modeling with audio inputs and contextualization, and discusses architecture/training/evaluation (WER), within the past 3 years, aligning with Spoken Language Models/FD-SLM concepts and excluding only traditional single-turn ASR; thus it should be included.",
      "evaluation": 5
    },
    "round-A_JuniorNano_reasoning": "The work treats speech recognition as a mixed-modal language modeling task using LLMs, explicitly framing speech-language modeling with audio inputs and contextualization, and discusses architecture/training/evaluation (WER), within the past 3 years, aligning with Spoken Language Models/FD-SLM concepts and excluding only traditional single-turn ASR; thus it should be included.",
    "round-A_JuniorNano_evaluation": 5,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on integrating Large Language Models with speech recognition models, presenting a mixed-modal language modeling approach that treats speech as a core modeling object and discusses model architecture and training. However, it emphasizes ASR improvements (WER reduction) without addressing full-duplex interactions, cognitive parallelism, real-time low latency interaction, or evaluation metrics related to synchronization or speech overlapping, and does not explicitly mention Spoken Language Models or equivalent terms in title or abstract. Therefore, while relevant to speech-LLM architectures, it does not meet all inclusion criteria, especially concerning the interactive, full-duplex spoken language modeling scope and evaluation requirements.",
      "evaluation": 2
    },
    "round-A_JuniorMini_reasoning": "The study focuses on integrating Large Language Models with speech recognition models, presenting a mixed-modal language modeling approach that treats speech as a core modeling object and discusses model architecture and training. However, it emphasizes ASR improvements (WER reduction) without addressing full-duplex interactions, cognitive parallelism, real-time low latency interaction, or evaluation metrics related to synchronization or speech overlapping, and does not explicitly mention Spoken Language Models or equivalent terms in title or abstract. Therefore, while relevant to speech-LLM architectures, it does not meet all inclusion criteria, especially concerning the interactive, full-duplex spoken language modeling scope and evaluation requirements.",
    "round-A_JuniorMini_evaluation": 2,
    "round-B_SeniorLead_output": {
      "reasoning": "JuniorNano judged it includable because the paper frames ASR as a mixed-modal language modeling task with LLMs (treating speech+text in a language-modeling setup), while JuniorMini judged it excludable because the work remains ASR-centric (WER improvements) and lacks explicit full‑duplex/spoken‑LM interaction, overlap handling, low‑latency or behavioral arbitration evaluations. I agree with JuniorMini: although the abstract phrases the approach as “mixed‑modal language modeling,” the study is primarily an ASR contextualization effort evaluated by WER and comparative ASR baselines, and it does not address core SLM/FD‑SLM concerns (full‑duplex interaction, overlap/interruption handling, cognitive parallelism, or required multi‑dimensional evaluations) required by the inclusion criteria—therefore it should be excluded.",
      "evaluation": 2
    },
    "round-B_SeniorLead_reasoning": "JuniorNano judged it includable because the paper frames ASR as a mixed-modal language modeling task with LLMs (treating speech+text in a language-modeling setup), while JuniorMini judged it excludable because the work remains ASR-centric (WER improvements) and lacks explicit full‑duplex/spoken‑LM interaction, overlap handling, low‑latency or behavioral arbitration evaluations. I agree with JuniorMini: although the abstract phrases the approach as “mixed‑modal language modeling,” the study is primarily an ASR contextualization effort evaluated by WER and comparative ASR baselines, and it does not address core SLM/FD‑SLM concerns (full‑duplex interaction, overlap/interruption handling, cognitive parallelism, or required multi‑dimensional evaluations) required by the inclusion criteria—therefore it should be excluded.",
    "round-B_SeniorLead_evaluation": 2,
    "final_verdict": "exclude (senior:2)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Translatotron 3: Speech to Speech Translation with Monolingual Data",
    "abstract": "This paper presents Translatotron 3, a novel approach to unsupervised direct speech-to-speech translation from monolingual speech-text datasets by combining masked autoencoder, unsupervised embedding mapping, and back-translation. Experimental results in speech-to-speech translation tasks between Spanish and English show that Translatotron 3 outperforms a baseline cascade system, reporting 18.14 BLEU points improvement on the synthesized Unpaired-Conversational dataset. In contrast to supervised approaches that necessitate real paired data, or specialized modeling to replicate para-/non-linguistic information such as pauses, speaking rates, and speaker identity, Translatotron 3 showcases its capability to retain it.",
    "metadata": {
      "title": "Translatotron 3: Speech to Speech Translation with Monolingual Data",
      "summary": "This paper presents Translatotron 3, a novel approach to unsupervised direct speech-to-speech translation from monolingual speech-text datasets by combining masked autoencoder, unsupervised embedding mapping, and back-translation. Experimental results in speech-to-speech translation tasks between Spanish and English show that Translatotron 3 outperforms a baseline cascade system, reporting 18.14 BLEU points improvement on the synthesized Unpaired-Conversational dataset. In contrast to supervised approaches that necessitate real paired data, or specialized modeling to replicate para-/non-linguistic information such as pauses, speaking rates, and speaker identity, Translatotron 3 showcases its capability to retain it.",
      "abstract": "This paper presents Translatotron 3, a novel approach to unsupervised direct speech-to-speech translation from monolingual speech-text datasets by combining masked autoencoder, unsupervised embedding mapping, and back-translation. Experimental results in speech-to-speech translation tasks between Spanish and English show that Translatotron 3 outperforms a baseline cascade system, reporting 18.14 BLEU points improvement on the synthesized Unpaired-Conversational dataset. In contrast to supervised approaches that necessitate real paired data, or specialized modeling to replicate para-/non-linguistic information such as pauses, speaking rates, and speaker identity, Translatotron 3 showcases its capability to retain it.",
      "doi": "https://doi.org/10.1109/icassp48485.2024.10448426",
      "openalex_id": "https://openalex.org/W4392903062",
      "arxiv_id": "",
      "publication_date": "2024-03-18",
      "published": "2024-03-18",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The paper focuses on unsupervised direct speech-to-speech translation and does not explicitly center on Spoken Language Models or speech-language modeling as the core concept, nor does it discuss full-duplex interactive speech modeling or evaluation beyond translation quality, so it does not meet the inclusion criteria.",
      "evaluation": 2
    },
    "round-A_JuniorNano_reasoning": "The paper focuses on unsupervised direct speech-to-speech translation and does not explicitly center on Spoken Language Models or speech-language modeling as the core concept, nor does it discuss full-duplex interactive speech modeling or evaluation beyond translation quality, so it does not meet the inclusion criteria.",
    "round-A_JuniorNano_evaluation": 2,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper proposes Translatotron 3 for unsupervised speech-to-speech translation using monolingual data, focusing mainly on direct speech translation rather than explicit spoken language modeling or full-duplex spoken language interaction capabilities; it does not mention spoken language models or language modeling of speech explicitly, nor does it discuss synchronization, real-time interaction, or behavior arbitration, thus it doesn't fully meet the core inclusion criteria focused on SLMs or FD-SLMs.",
      "evaluation": 2
    },
    "round-A_JuniorMini_reasoning": "The paper proposes Translatotron 3 for unsupervised speech-to-speech translation using monolingual data, focusing mainly on direct speech translation rather than explicit spoken language modeling or full-duplex spoken language interaction capabilities; it does not mention spoken language models or language modeling of speech explicitly, nor does it discuss synchronization, real-time interaction, or behavior arbitration, thus it doesn't fully meet the core inclusion criteria focused on SLMs or FD-SLMs.",
    "round-A_JuniorMini_evaluation": 2,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:2)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Adapting Large Language Model with Speech for Fully Formatted End-to-End Speech Recognition",
    "abstract": "Most end-to-end (E2E) speech recognition models are composed of encoder and decoder blocks that perform acoustic and language modeling functions. Pretrained large language models (LLMs) have the potential to improve the performance of E2E ASR. However, integrating a pretrained language model into an E2E speech recognition model has shown limited benefits due to the mismatches between text-based LLMs and those used in E2E ASR. In this paper, we explore an alternative approach by adapting a pretrained LLMs to speech. Our experiments on fully-formatted E2E ASR transcription tasks across various domains demonstrate that our approach can effectively leverage the strengths of pretrained LLMs to produce more readable ASR transcriptions. Our model, which is based on the pretrained large language models with either an encoder-decoder or decoder-only structure, surpasses strong ASR models such as Whisper <sup xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">1</sup> , in terms of recognition error rate, considering formats like punctuation and capitalization as well.",
    "metadata": {
      "title": "Adapting Large Language Model with Speech for Fully Formatted End-to-End Speech Recognition",
      "summary": "Most end-to-end (E2E) speech recognition models are composed of encoder and decoder blocks that perform acoustic and language modeling functions. Pretrained large language models (LLMs) have the potential to improve the performance of E2E ASR. However, integrating a pretrained language model into an E2E speech recognition model has shown limited benefits due to the mismatches between text-based LLMs and those used in E2E ASR. In this paper, we explore an alternative approach by adapting a pretrained LLMs to speech. Our experiments on fully-formatted E2E ASR transcription tasks across various domains demonstrate that our approach can effectively leverage the strengths of pretrained LLMs to produce more readable ASR transcriptions. Our model, which is based on the pretrained large language models with either an encoder-decoder or decoder-only structure, surpasses strong ASR models such as Whisper <sup xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">1</sup> , in terms of recognition error rate, considering formats like punctuation and capitalization as well.",
      "abstract": "Most end-to-end (E2E) speech recognition models are composed of encoder and decoder blocks that perform acoustic and language modeling functions. Pretrained large language models (LLMs) have the potential to improve the performance of E2E ASR. However, integrating a pretrained language model into an E2E speech recognition model has shown limited benefits due to the mismatches between text-based LLMs and those used in E2E ASR. In this paper, we explore an alternative approach by adapting a pretrained LLMs to speech. Our experiments on fully-formatted E2E ASR transcription tasks across various domains demonstrate that our approach can effectively leverage the strengths of pretrained LLMs to produce more readable ASR transcriptions. Our model, which is based on the pretrained large language models with either an encoder-decoder or decoder-only structure, surpasses strong ASR models such as Whisper <sup xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">1</sup> , in terms of recognition error rate, considering formats like punctuation and capitalization as well.",
      "doi": "https://doi.org/10.1109/icassp48485.2024.10448204",
      "openalex_id": "https://openalex.org/W4392910583",
      "arxiv_id": "",
      "publication_date": "2024-03-18",
      "published": "2024-03-18",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This paper explicitly centers on Spoken Language Models / speech-language modeling by adapting pretrained LLMs to speech for end-to-end ASR, discusses architecture, training and evaluation, and is within the past 3 years, so it meets inclusion criteria and is not excluded.",
      "evaluation": 5
    },
    "round-A_JuniorNano_reasoning": "This paper explicitly centers on Spoken Language Models / speech-language modeling by adapting pretrained LLMs to speech for end-to-end ASR, discusses architecture, training and evaluation, and is within the past 3 years, so it meets inclusion criteria and is not excluded.",
    "round-A_JuniorNano_evaluation": 5,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper focuses on adapting large language models to speech for end-to-end automatic speech recognition (ASR), primarily targeting improving ASR transcription quality by leveraging pretrained language models. However, it centers on ASR performance (recognition error rate) and formatting (punctuation and capitalization) but does not discuss spoken language modeling concepts such as spoken language models (SLMs) or full-duplex spoken language models (FD-SLMs), nor does it address synchronized turn-taking, cognitive parallelism, low-latency interaction, speech overlap management, or multimodal dialogue interaction. Moreover, it appears to treat speech mainly as input to improve textual output rather than as the core modeling and reasoning object; the evaluation focuses on recognition error rate, not on behaviors like real-time interaction or speech overlap handling. Therefore, it does not meet several inclusion criteria such as explicit spoken language model focus, multi-dimensional evaluation, and full-duplex interaction capabilities, and it aligns with exclusion criteria of focusing solely on ASR without broader spoken language modeling or interaction aspects.",
      "evaluation": 2
    },
    "round-A_JuniorMini_reasoning": "The paper focuses on adapting large language models to speech for end-to-end automatic speech recognition (ASR), primarily targeting improving ASR transcription quality by leveraging pretrained language models. However, it centers on ASR performance (recognition error rate) and formatting (punctuation and capitalization) but does not discuss spoken language modeling concepts such as spoken language models (SLMs) or full-duplex spoken language models (FD-SLMs), nor does it address synchronized turn-taking, cognitive parallelism, low-latency interaction, speech overlap management, or multimodal dialogue interaction. Moreover, it appears to treat speech mainly as input to improve textual output rather than as the core modeling and reasoning object; the evaluation focuses on recognition error rate, not on behaviors like real-time interaction or speech overlap handling. Therefore, it does not meet several inclusion criteria such as explicit spoken language model focus, multi-dimensional evaluation, and full-duplex interaction capabilities, and it aligns with exclusion criteria of focusing solely on ASR without broader spoken language modeling or interaction aspects.",
    "round-A_JuniorMini_evaluation": 2,
    "round-B_SeniorLead_output": {
      "reasoning": "I reviewed both junior assessments: JuniorNano favored inclusion, but JuniorMini correctly noted the paper focuses on improving E2E ASR (recognition error rate and formatting) rather than treating speech as the core spoken-language-modeling object or addressing full‑duplex interaction, overlap management, low‑latency behavior, or explicit SLM/FD‑SLM concepts. The title/abstract do not mention “spoken language model(s)” or equivalent, the contribution appears to be adapting text LLMs for better formatted ASR outputs (speech as input → better text), and evaluation is WER/formatting-based — therefore it meets the exclusion criteria (solely ASR-focused, speech used only as input, no spoken-language modeling or interaction claims). One‑sentence explanation: Exclude (score=2) because the paper is an ASR-focused work adapting LLMs for formatted transcription and does not meet the SLM/FD‑SLM inclusion requirements nor covers synchronization/interaction or speech-native language modeling.",
      "evaluation": 2
    },
    "round-B_SeniorLead_reasoning": "I reviewed both junior assessments: JuniorNano favored inclusion, but JuniorMini correctly noted the paper focuses on improving E2E ASR (recognition error rate and formatting) rather than treating speech as the core spoken-language-modeling object or addressing full‑duplex interaction, overlap management, low‑latency behavior, or explicit SLM/FD‑SLM concepts. The title/abstract do not mention “spoken language model(s)” or equivalent, the contribution appears to be adapting text LLMs for better formatted ASR outputs (speech as input → better text), and evaluation is WER/formatting-based — therefore it meets the exclusion criteria (solely ASR-focused, speech used only as input, no spoken-language modeling or interaction claims). One‑sentence explanation: Exclude (score=2) because the paper is an ASR-focused work adapting LLMs for formatted transcription and does not meet the SLM/FD‑SLM inclusion requirements nor covers synchronization/interaction or speech-native language modeling.",
    "round-B_SeniorLead_evaluation": 2,
    "final_verdict": "exclude (senior:2)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Cross-Modal Multi-Tasking for Speech-to-Text Translation via Hard Parameter Sharing",
    "abstract": "Recent works in end-to-end speech-to-text translation (ST) have proposed multi-tasking methods with soft parameter sharing which leverage machine translation (MT) data via secondary encoders that map text inputs to an eventual cross-modal representation. In this work, we instead propose a ST/MT multi-tasking framework with hard parameter sharing in which all model parameters are shared cross-modally. Our method reduces the speech-text modality gap via a pre-processing stage which converts speech and text inputs into two discrete token sequences of similar length – this allows models to indiscriminately process both modalities simply using a joint vocabulary. With experiments on MuST-C, we demonstrate that our multi-tasking framework improves attentional encoder-decoder, Connectionist Temporal Classification (CTC), transducer, and joint CTC/attention models by an average of +0.5 BLEU without any external MT data. Further, we show that this framework incorporates external MT data, yielding +0.8 BLEU, and also improves transfer learning from pre-trained textual models, yielding +1.8 BLEU. <sup xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">1</sup>",
    "metadata": {
      "title": "Cross-Modal Multi-Tasking for Speech-to-Text Translation via Hard Parameter Sharing",
      "summary": "Recent works in end-to-end speech-to-text translation (ST) have proposed multi-tasking methods with soft parameter sharing which leverage machine translation (MT) data via secondary encoders that map text inputs to an eventual cross-modal representation. In this work, we instead propose a ST/MT multi-tasking framework with hard parameter sharing in which all model parameters are shared cross-modally. Our method reduces the speech-text modality gap via a pre-processing stage which converts speech and text inputs into two discrete token sequences of similar length – this allows models to indiscriminately process both modalities simply using a joint vocabulary. With experiments on MuST-C, we demonstrate that our multi-tasking framework improves attentional encoder-decoder, Connectionist Temporal Classification (CTC), transducer, and joint CTC/attention models by an average of +0.5 BLEU without any external MT data. Further, we show that this framework incorporates external MT data, yielding +0.8 BLEU, and also improves transfer learning from pre-trained textual models, yielding +1.8 BLEU. <sup xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">1</sup>",
      "abstract": "Recent works in end-to-end speech-to-text translation (ST) have proposed multi-tasking methods with soft parameter sharing which leverage machine translation (MT) data via secondary encoders that map text inputs to an eventual cross-modal representation. In this work, we instead propose a ST/MT multi-tasking framework with hard parameter sharing in which all model parameters are shared cross-modally. Our method reduces the speech-text modality gap via a pre-processing stage which converts speech and text inputs into two discrete token sequences of similar length – this allows models to indiscriminately process both modalities simply using a joint vocabulary. With experiments on MuST-C, we demonstrate that our multi-tasking framework improves attentional encoder-decoder, Connectionist Temporal Classification (CTC), transducer, and joint CTC/attention models by an average of +0.5 BLEU without any external MT data. Further, we show that this framework incorporates external MT data, yielding +0.8 BLEU, and also improves transfer learning from pre-trained textual models, yielding +1.8 BLEU. <sup xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">1</sup>",
      "doi": "https://doi.org/10.1109/icassp48485.2024.10447926",
      "openalex_id": "https://openalex.org/W4392902778",
      "arxiv_id": "",
      "publication_date": "2024-03-18",
      "published": "2024-03-18",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "Although the paper addresses speech-to-text translation with cross-modal sharing, it does not explicitly frame itself as a Spoken Language Model or equivalent core concept per the stated criteria, so it should be excluded.",
      "evaluation": 2
    },
    "round-A_JuniorNano_reasoning": "Although the paper addresses speech-to-text translation with cross-modal sharing, it does not explicitly frame itself as a Spoken Language Model or equivalent core concept per the stated criteria, so it should be excluded.",
    "round-A_JuniorNano_evaluation": 2,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on speech-to-text translation via a multi-task learning framework sharing parameters cross-modally, improving models' performance with speech and text data. However, it centers on end-to-end speech translation tasks rather than explicit spoken language modeling or full-duplex spoken language interactive frameworks. The abstract does not mention spoken language models, full-duplex interaction, turn-taking, real-time interaction, or synchronization aspects, nor does it analyze the speech modeling itself beyond translation tasks. Therefore, it does not meet the inclusion criteria centered on spoken language models and interactive full-duplex capabilities, nor does it sufficiently address the research scope of the topic.",
      "evaluation": 2
    },
    "round-A_JuniorMini_reasoning": "The study focuses on speech-to-text translation via a multi-task learning framework sharing parameters cross-modally, improving models' performance with speech and text data. However, it centers on end-to-end speech translation tasks rather than explicit spoken language modeling or full-duplex spoken language interactive frameworks. The abstract does not mention spoken language models, full-duplex interaction, turn-taking, real-time interaction, or synchronization aspects, nor does it analyze the speech modeling itself beyond translation tasks. Therefore, it does not meet the inclusion criteria centered on spoken language models and interactive full-duplex capabilities, nor does it sufficiently address the research scope of the topic.",
    "round-A_JuniorMini_evaluation": 2,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:2)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Improving Multilingual and Code-Switching ASR Using Large Language Model Generated Text",
    "abstract": "We investigate using large language models (LLMs) to generate text-only training data for improving multilingual and code-switching automatic speech recognition (ASR) through a text injection method. In a multilingual setup or a low-resource scenario such as code-switching, we propose to generate text data using the state-of-the-art PaLM 2. To better match the generated text data with specific tasks, we use prompt tuning to adapt PaLM 2 to generate domain-relevant multilingual or code-switched text data for text injection. We can achieve significant improvements in Word Error Rate (WER) in both multilingual and code-switching scenarios. The multilingual experiment shows a $6.2 \\%$ relative WER reduction on average, i.e., from $11.25 \\%$ to $10.55 \\%$, compared to a baseline without text injection. The improvement is up to $23.1 \\%$ improvement for certain languages. While in the code-switching scenario, we use English-only prompts to generate Mandarin-English code-switching text and achieve a 3.6% relative WER reduction for a code-switching test set, as well as WER reductions in both English and Mandarin monolingual scenarios, $5.3 \\%$ and $8.5 \\%$ relative, respectively. Our findings demonstrate that leveraging LLMs for text generation and then injection benefits multilingual or code-switching ASR tasks.",
    "metadata": {
      "title": "Improving Multilingual and Code-Switching ASR Using Large Language Model Generated Text",
      "summary": "We investigate using large language models (LLMs) to generate text-only training data for improving multilingual and code-switching automatic speech recognition (ASR) through a text injection method. In a multilingual setup or a low-resource scenario such as code-switching, we propose to generate text data using the state-of-the-art PaLM 2. To better match the generated text data with specific tasks, we use prompt tuning to adapt PaLM 2 to generate domain-relevant multilingual or code-switched text data for text injection. We can achieve significant improvements in Word Error Rate (WER) in both multilingual and code-switching scenarios. The multilingual experiment shows a $6.2 \\%$ relative WER reduction on average, i.e., from $11.25 \\%$ to $10.55 \\%$, compared to a baseline without text injection. The improvement is up to $23.1 \\%$ improvement for certain languages. While in the code-switching scenario, we use English-only prompts to generate Mandarin-English code-switching text and achieve a 3.6% relative WER reduction for a code-switching test set, as well as WER reductions in both English and Mandarin monolingual scenarios, $5.3 \\%$ and $8.5 \\%$ relative, respectively. Our findings demonstrate that leveraging LLMs for text generation and then injection benefits multilingual or code-switching ASR tasks.",
      "abstract": "We investigate using large language models (LLMs) to generate text-only training data for improving multilingual and code-switching automatic speech recognition (ASR) through a text injection method. In a multilingual setup or a low-resource scenario such as code-switching, we propose to generate text data using the state-of-the-art PaLM 2. To better match the generated text data with specific tasks, we use prompt tuning to adapt PaLM 2 to generate domain-relevant multilingual or code-switched text data for text injection. We can achieve significant improvements in Word Error Rate (WER) in both multilingual and code-switching scenarios. The multilingual experiment shows a $6.2 \\%$ relative WER reduction on average, i.e., from $11.25 \\%$ to $10.55 \\%$, compared to a baseline without text injection. The improvement is up to $23.1 \\%$ improvement for certain languages. While in the code-switching scenario, we use English-only prompts to generate Mandarin-English code-switching text and achieve a 3.6% relative WER reduction for a code-switching test set, as well as WER reductions in both English and Mandarin monolingual scenarios, $5.3 \\%$ and $8.5 \\%$ relative, respectively. Our findings demonstrate that leveraging LLMs for text generation and then injection benefits multilingual or code-switching ASR tasks.",
      "doi": "https://doi.org/10.1109/asru57964.2023.10389644",
      "openalex_id": "https://openalex.org/W4391021723",
      "arxiv_id": "",
      "publication_date": "2023-12-16",
      "published": "2023-12-16",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "Although the work targets ASR in multilingual/code-switching contexts, it centers on text-data augmentation via LLMs rather than modeling speech as the primary object (no discussion of SLM/FD-SLM architectures or speech-centric evaluation beyond WER), thus failing the core inclusion criteria and meeting exclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "Although the work targets ASR in multilingual/code-switching contexts, it centers on text-data augmentation via LLMs rather than modeling speech as the primary object (no discussion of SLM/FD-SLM architectures or speech-centric evaluation beyond WER), thus failing the core inclusion criteria and meeting exclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on using large language models to generate text data to improve ASR performance for multilingual and code-switching scenarios. However, it does not explicitly discuss spoken language models (SLMs) or full-duplex spoken language models (FD-SLMs), nor does it address modeling or reasoning directly on acoustic speech data or synchronous conversational behaviors such as turn-taking, backchanneling, or low-latency interaction. The research primarily targets ASR performance improvement through text-based data augmentation without focusing on interaction or joint speech-text modeling, thus not meeting the inclusion criteria for spoken language models and synchronous interactive behaviors.",
      "evaluation": 2
    },
    "round-A_JuniorMini_reasoning": "The study focuses on using large language models to generate text data to improve ASR performance for multilingual and code-switching scenarios. However, it does not explicitly discuss spoken language models (SLMs) or full-duplex spoken language models (FD-SLMs), nor does it address modeling or reasoning directly on acoustic speech data or synchronous conversational behaviors such as turn-taking, backchanneling, or low-latency interaction. The research primarily targets ASR performance improvement through text-based data augmentation without focusing on interaction or joint speech-text modeling, thus not meeting the inclusion criteria for spoken language models and synchronous interactive behaviors.",
    "round-A_JuniorMini_evaluation": 2,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:2)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Unconstrained Dysfluency Modeling for Dysfluent Speech Transcription and Detection",
    "abstract": "Dysfluent speech modeling requires time-accurate and silence-aware transcription at both the word-level and phonetic-level. However, current research in dysfluency modeling primarily focuses on either transcription or detection, and the performance of each aspect remains limited. In this work, we present an unconstrained dysfluency modeling (UDM) approach that addresses both transcription and detection in an automatic and hierarchical manner. UDM eliminates the need for extensive manual annotation by providing a comprehensive solution. Furthermore, we introduce a simulated dysfluent dataset called VCTK<sup>++</sup> to enhance the capabilities of UDM in phonetic transcription. Our experimental results demonstrate the effectiveness and robustness of our proposed methods in both transcription and detection tasks.",
    "metadata": {
      "title": "Unconstrained Dysfluency Modeling for Dysfluent Speech Transcription and Detection",
      "summary": "Dysfluent speech modeling requires time-accurate and silence-aware transcription at both the word-level and phonetic-level. However, current research in dysfluency modeling primarily focuses on either transcription or detection, and the performance of each aspect remains limited. In this work, we present an unconstrained dysfluency modeling (UDM) approach that addresses both transcription and detection in an automatic and hierarchical manner. UDM eliminates the need for extensive manual annotation by providing a comprehensive solution. Furthermore, we introduce a simulated dysfluent dataset called VCTK<sup>++</sup> to enhance the capabilities of UDM in phonetic transcription. Our experimental results demonstrate the effectiveness and robustness of our proposed methods in both transcription and detection tasks.",
      "abstract": "Dysfluent speech modeling requires time-accurate and silence-aware transcription at both the word-level and phonetic-level. However, current research in dysfluency modeling primarily focuses on either transcription or detection, and the performance of each aspect remains limited. In this work, we present an unconstrained dysfluency modeling (UDM) approach that addresses both transcription and detection in an automatic and hierarchical manner. UDM eliminates the need for extensive manual annotation by providing a comprehensive solution. Furthermore, we introduce a simulated dysfluent dataset called VCTK<sup>++</sup> to enhance the capabilities of UDM in phonetic transcription. Our experimental results demonstrate the effectiveness and robustness of our proposed methods in both transcription and detection tasks.",
      "doi": "https://doi.org/10.1109/asru57964.2023.10389771",
      "openalex_id": "https://openalex.org/W4391021769",
      "arxiv_id": "",
      "publication_date": "2023-12-16",
      "published": "2023-12-16",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This item does not position Spoken Language Models as the core focus and lacks SLM/FD-SLM terminology in the title/abstract, thus it does not meet the inclusion criteria and should be excluded.",
      "evaluation": 2
    },
    "round-A_JuniorNano_reasoning": "This item does not position Spoken Language Models as the core focus and lacks SLM/FD-SLM terminology in the title/abstract, thus it does not meet the inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 2,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on dysfluency modeling in speech transcription and detection but does not mention spoken language models (SLMs) or speech language models explicitly, nor does it address full-duplex interaction, synchronization, or cognitive parallelism aspects essential to the inclusion criteria. The work appears centered on phonetic transcription and detection, which aligns more with ASR-type tasks rather than broader speech language modeling or interactive dialogue behavior arbitration. Therefore, it does not meet the inclusion criteria nor is it excluded solely for being traditional or incomplete; it simply falls outside the targeted scope.",
      "evaluation": 2
    },
    "round-A_JuniorMini_reasoning": "The study focuses on dysfluency modeling in speech transcription and detection but does not mention spoken language models (SLMs) or speech language models explicitly, nor does it address full-duplex interaction, synchronization, or cognitive parallelism aspects essential to the inclusion criteria. The work appears centered on phonetic transcription and detection, which aligns more with ASR-type tasks rather than broader speech language modeling or interactive dialogue behavior arbitration. Therefore, it does not meet the inclusion criteria nor is it excluded solely for being traditional or incomplete; it simply falls outside the targeted scope.",
    "round-A_JuniorMini_evaluation": 2,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:2)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "The Idiap Speech Synthesis System for the Blizzard Challenge 2023",
    "abstract": "This paper presents the text-to-speech (TTS) system submitted by Idiap Research Institute to the Blizzard Challenge 2023.Our system follows the conventional pipeline of text analysis, acoustic modeling (AM) and vocoding.For text analysis, opensource pretrained part-of-speech (POS) taggers and lemmatizers are utilized to provide more accurate grapheme-to-phoneme (G2P) conversion on top of the eSpeak backend.The rest of the system incorporates a fully diffusion-based approach which comprises a diffusion transformer-based acoustic model and FastDiff as the vocoder, both of which are trained only on the provided data to ensure high-quality synthesis.Our entry provides a baseline for the cascading diffusion AM-vocoder architecture since no extra design is adopted to enhance the naturalness of speech.Evaluation results have demonstrated high synthesis quality of our system and the effectiveness of the proposed phonemization pipeline.",
    "metadata": {
      "title": "The Idiap Speech Synthesis System for the Blizzard Challenge 2023",
      "summary": "This paper presents the text-to-speech (TTS) system submitted by Idiap Research Institute to the Blizzard Challenge 2023.Our system follows the conventional pipeline of text analysis, acoustic modeling (AM) and vocoding.For text analysis, opensource pretrained part-of-speech (POS) taggers and lemmatizers are utilized to provide more accurate grapheme-to-phoneme (G2P) conversion on top of the eSpeak backend.The rest of the system incorporates a fully diffusion-based approach which comprises a diffusion transformer-based acoustic model and FastDiff as the vocoder, both of which are trained only on the provided data to ensure high-quality synthesis.Our entry provides a baseline for the cascading diffusion AM-vocoder architecture since no extra design is adopted to enhance the naturalness of speech.Evaluation results have demonstrated high synthesis quality of our system and the effectiveness of the proposed phonemization pipeline.",
      "abstract": "This paper presents the text-to-speech (TTS) system submitted by Idiap Research Institute to the Blizzard Challenge 2023.Our system follows the conventional pipeline of text analysis, acoustic modeling (AM) and vocoding.For text analysis, opensource pretrained part-of-speech (POS) taggers and lemmatizers are utilized to provide more accurate grapheme-to-phoneme (G2P) conversion on top of the eSpeak backend.The rest of the system incorporates a fully diffusion-based approach which comprises a diffusion transformer-based acoustic model and FastDiff as the vocoder, both of which are trained only on the provided data to ensure high-quality synthesis.Our entry provides a baseline for the cascading diffusion AM-vocoder architecture since no extra design is adopted to enhance the naturalness of speech.Evaluation results have demonstrated high synthesis quality of our system and the effectiveness of the proposed phonemization pipeline.",
      "doi": "https://doi.org/10.21437/blizzard.2023-13",
      "openalex_id": "https://openalex.org/W4388272705",
      "arxiv_id": "",
      "publication_date": "2023-08-29",
      "published": "2023-08-29",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item describes a TTS system submission with a diffusion-based acoustic model and vocoder, focusing on speech synthesis without addressing Spoken Language Models or full-duplex interactive speech modeling; it does not meet the core inclusion criterion of SLM/FD-SLM focus or interactive speech reasoning, and thus should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item describes a TTS system submission with a diffusion-based acoustic model and vocoder, focusing on speech synthesis without addressing Spoken Language Models or full-duplex interactive speech modeling; it does not meet the core inclusion criterion of SLM/FD-SLM focus or interactive speech reasoning, and thus should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study presents a TTS system focused on speech synthesis quality and phonemization pipeline, without discussing spoken language models, full-duplex interaction, or any synchronization or behavioral arbitration mechanisms. It centers on a conventional TTS pipeline rather than end-to-end spoken language modeling or multi-modal speech-LLM architectures, and lacks mention of any evaluation involving real-time interaction dynamics or cognitive parallelism, thus not meeting the inclusion criteria and matching exclusion criteria related to single-task speech technology without modeling language or dialogue behaviors.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study presents a TTS system focused on speech synthesis quality and phonemization pipeline, without discussing spoken language models, full-duplex interaction, or any synchronization or behavioral arbitration mechanisms. It centers on a conventional TTS pipeline rather than end-to-end spoken language modeling or multi-modal speech-LLM architectures, and lacks mention of any evaluation involving real-time interaction dynamics or cognitive parallelism, thus not meeting the inclusion criteria and matching exclusion criteria related to single-task speech technology without modeling language or dialogue behaviors.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Deep Fake and Digital Forensics",
    "abstract": "Deepfakes have posed a significant challenge to digital forensics, and there is an increasing need for high accuracy deepfake (DF) detection models in real-world scenarios. This research examines and fine-tunes the MesoNet model to improve its performance on a large dataset of 140K authentic and manipulated images. The original MesoNet model achieved an accuracy of 87.1%. However, after fine-tuning and optimizing the model's weights, the accuracy improved to 96.20%. This was accompanied by a sensitivity of 97.48% and a specificity of 94.75%, indicating that the model is highly effective at detecting genuine images and accurately identifying forged ones. This research contributes to the advancement of DF detection mechanisms in real-world scenarios.",
    "metadata": {
      "title": "Deep Fake and Digital Forensics",
      "summary": "Deepfakes have posed a significant challenge to digital forensics, and there is an increasing need for high accuracy deepfake (DF) detection models in real-world scenarios. This research examines and fine-tunes the MesoNet model to improve its performance on a large dataset of 140K authentic and manipulated images. The original MesoNet model achieved an accuracy of 87.1%. However, after fine-tuning and optimizing the model's weights, the accuracy improved to 96.20%. This was accompanied by a sensitivity of 97.48% and a specificity of 94.75%, indicating that the model is highly effective at detecting genuine images and accurately identifying forged ones. This research contributes to the advancement of DF detection mechanisms in real-world scenarios.",
      "abstract": "Deepfakes have posed a significant challenge to digital forensics, and there is an increasing need for high accuracy deepfake (DF) detection models in real-world scenarios. This research examines and fine-tunes the MesoNet model to improve its performance on a large dataset of 140K authentic and manipulated images. The original MesoNet model achieved an accuracy of 87.1%. However, after fine-tuning and optimizing the model's weights, the accuracy improved to 96.20%. This was accompanied by a sensitivity of 97.48% and a specificity of 94.75%, indicating that the model is highly effective at detecting genuine images and accurately identifying forged ones. This research contributes to the advancement of DF detection mechanisms in real-world scenarios.",
      "doi": "https://doi.org/10.1109/uemcon59035.2023.10315974",
      "openalex_id": "https://openalex.org/W4388757580",
      "arxiv_id": "",
      "publication_date": "2023-10-12",
      "published": "2023-10-12",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item centers on deepfake detection using a vision-based model (MesoNet) on image data, with no discussion of spoken language models, speech modeling, or audio-interaction assessment, thus it fails all inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item centers on deepfake detection using a vision-based model (MesoNet) on image data, with no discussion of spoken language models, speech modeling, or audio-interaction assessment, thus it fails all inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on deepfake detection using the MesoNet model and does not address spoken language models or speech language modeling; it lacks discussion on full-duplex interaction, spoken language model architectures, or speech-based language modeling concepts required by the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on deepfake detection using the MesoNet model and does not address spoken language models or speech language modeling; it lacks discussion on full-duplex interaction, spoken language model architectures, or speech-based language modeling concepts required by the inclusion criteria.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Multiple Representation Transfer from Large Language Models to End-to-End ASR Systems",
    "abstract": "Transferring the knowledge of large language models (LLMs) is a promising technique to incorporate linguistic knowledge into end-to-end automatic speech recognition (ASR) systems. However, existing works only transfer a single representation of LLM (e.g. the last layer of pretrained BERT), while the representation of a text is inherently non-unique and can be obtained variously from different layers, contexts and models. In this work, we explore a wide range of techniques to obtain and transfer multiple representations of LLMs into a transducer-based ASR system. While being conceptually simple, we show that transferring multiple representations of LLMs can be an effective alternative to transferring only a single LLM representation.",
    "metadata": {
      "title": "Multiple Representation Transfer from Large Language Models to End-to-End ASR Systems",
      "summary": "Transferring the knowledge of large language models (LLMs) is a promising technique to incorporate linguistic knowledge into end-to-end automatic speech recognition (ASR) systems. However, existing works only transfer a single representation of LLM (e.g. the last layer of pretrained BERT), while the representation of a text is inherently non-unique and can be obtained variously from different layers, contexts and models. In this work, we explore a wide range of techniques to obtain and transfer multiple representations of LLMs into a transducer-based ASR system. While being conceptually simple, we show that transferring multiple representations of LLMs can be an effective alternative to transferring only a single LLM representation.",
      "abstract": "Transferring the knowledge of large language models (LLMs) is a promising technique to incorporate linguistic knowledge into end-to-end automatic speech recognition (ASR) systems. However, existing works only transfer a single representation of LLM (e.g. the last layer of pretrained BERT), while the representation of a text is inherently non-unique and can be obtained variously from different layers, contexts and models. In this work, we explore a wide range of techniques to obtain and transfer multiple representations of LLMs into a transducer-based ASR system. While being conceptually simple, we show that transferring multiple representations of LLMs can be an effective alternative to transferring only a single LLM representation.",
      "doi": "https://doi.org/10.1109/icassp48485.2024.10448022",
      "openalex_id": "https://openalex.org/W4392904094",
      "arxiv_id": "",
      "publication_date": "2024-03-18",
      "published": "2024-03-18",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "Although within the last 3 years and about ASR, it does not center on Spoken Language Models or speech-language modeling as the core object, instead focusing on LLM-to-ASR integration, thus does not meet inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "Although within the last 3 years and about ASR, it does not center on Spoken Language Models or speech-language modeling as the core object, instead focusing on LLM-to-ASR integration, thus does not meet inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on transferring knowledge from large language models to end-to-end ASR systems, emphasizing linguistic knowledge integration, but it primarily addresses ASR improvement without explicitly involving spoken language models, full-duplex interaction capabilities, synchronization, or multi-dimensional behavioral evaluation as per inclusion criteria. It also lacks mention of spoken language models or equivalent wording in the abstract and title, and does not discuss dialogue behavior arbitration or low-latency interactive aspects, which are crucial for inclusion. Therefore, it does not meet the core thematic inclusion criteria and falls into exclusion due to its focus on ASR only as a downstream task without covering broader spoken language modeling or synchronous interaction aspects.",
      "evaluation": 2
    },
    "round-A_JuniorMini_reasoning": "The study focuses on transferring knowledge from large language models to end-to-end ASR systems, emphasizing linguistic knowledge integration, but it primarily addresses ASR improvement without explicitly involving spoken language models, full-duplex interaction capabilities, synchronization, or multi-dimensional behavioral evaluation as per inclusion criteria. It also lacks mention of spoken language models or equivalent wording in the abstract and title, and does not discuss dialogue behavior arbitration or low-latency interactive aspects, which are crucial for inclusion. Therefore, it does not meet the core thematic inclusion criteria and falls into exclusion due to its focus on ASR only as a downstream task without covering broader spoken language modeling or synchronous interaction aspects.",
    "round-A_JuniorMini_evaluation": 2,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:2)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Direct Punjabi to English Speech Translation using Discrete Units",
    "abstract": "Speech-to-speech translation is yet to reach the same level of coverage as text-to-text translation systems. The current speech technology is highly limited in its coverage of over 7000 languages spoken worldwide, leaving more than half of the population deprived of such technology and shared experiences. With voice-assisted technology (such as social robots and speech-to-text apps) and auditory content (such as podcasts and lectures) on the rise, ensuring that the technology is available for all is more important than ever. Speech translation can play a vital role in mitigating technological disparity and creating a more inclusive society. With a motive to contribute towards speech translation research for low-resource languages, our work presents a direct speech-to-speech translation model for one of the Indic languages called Punjabi to English. Additionally, we explore the performance of using a discrete representation of speech called discrete acoustic units as input to the Transformer-based translation model. The model, abbreviated as Unit-to-Unit Translation (U2UT), takes a sequence of discrete units of the source language (the language being translated from) and outputs a sequence of discrete units of the target language (the language being translated to). Our results show that the U2UT model performs better than the Speechto-Unit Translation (S2UT) model by a 3.69 BLEU score.",
    "metadata": {
      "title": "Direct Punjabi to English Speech Translation using Discrete Units",
      "summary": "Speech-to-speech translation is yet to reach the same level of coverage as text-to-text translation systems. The current speech technology is highly limited in its coverage of over 7000 languages spoken worldwide, leaving more than half of the population deprived of such technology and shared experiences. With voice-assisted technology (such as social robots and speech-to-text apps) and auditory content (such as podcasts and lectures) on the rise, ensuring that the technology is available for all is more important than ever. Speech translation can play a vital role in mitigating technological disparity and creating a more inclusive society. With a motive to contribute towards speech translation research for low-resource languages, our work presents a direct speech-to-speech translation model for one of the Indic languages called Punjabi to English. Additionally, we explore the performance of using a discrete representation of speech called discrete acoustic units as input to the Transformer-based translation model. The model, abbreviated as Unit-to-Unit Translation (U2UT), takes a sequence of discrete units of the source language (the language being translated from) and outputs a sequence of discrete units of the target language (the language being translated to). Our results show that the U2UT model performs better than the Speechto-Unit Translation (S2UT) model by a 3.69 BLEU score.",
      "abstract": "Speech-to-speech translation is yet to reach the same level of coverage as text-to-text translation systems. The current speech technology is highly limited in its coverage of over 7000 languages spoken worldwide, leaving more than half of the population deprived of such technology and shared experiences. With voice-assisted technology (such as social robots and speech-to-text apps) and auditory content (such as podcasts and lectures) on the rise, ensuring that the technology is available for all is more important than ever. Speech translation can play a vital role in mitigating technological disparity and creating a more inclusive society. With a motive to contribute towards speech translation research for low-resource languages, our work presents a direct speech-to-speech translation model for one of the Indic languages called Punjabi to English. Additionally, we explore the performance of using a discrete representation of speech called discrete acoustic units as input to the Transformer-based translation model. The model, abbreviated as Unit-to-Unit Translation (U2UT), takes a sequence of discrete units of the source language (the language being translated from) and outputs a sequence of discrete units of the target language (the language being translated to). Our results show that the U2UT model performs better than the Speechto-Unit Translation (S2UT) model by a 3.69 BLEU score.",
      "doi": "https://doi.org/10.5121/ijci.2024.130201",
      "openalex_id": "https://openalex.org/W4392884616",
      "arxiv_id": "",
      "publication_date": "2024-03-10",
      "published": "2024-03-10",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "End-to-end direct speech-to-speech translation using discrete units treats speech as the modeling target and discusses architecture and evaluation, aligning with the Spoken Language Model scope within the last 3 years, though it does not explicitly use the exact phrase 'spoken language model' in the abstract.",
      "evaluation": 4
    },
    "round-A_JuniorNano_reasoning": "End-to-end direct speech-to-speech translation using discrete units treats speech as the modeling target and discusses architecture and evaluation, aligning with the Spoken Language Model scope within the last 3 years, though it does not explicitly use the exact phrase 'spoken language model' in the abstract.",
    "round-A_JuniorNano_evaluation": 4,
    "round-A_JuniorMini_output": {
      "reasoning": "The study presents a direct speech-to-speech translation model from Punjabi to English using discrete acoustic units, focusing on end-to-end speech translation. However, it does not explicitly discuss Spoken Language Models or Full-Duplex Spoken Language Models, cognitive parallelism, synchronization, or real-time interactive behaviors, nor does it include evaluation related to turn-taking, interruption, or backchanneling. The core focus is on speech translation performance rather than modeling speech sequences for interaction or conversational dynamics, thus it does not fully meet the inclusion criteria centered on spoken language modeling and synchronous interaction capabilities.",
      "evaluation": 2
    },
    "round-A_JuniorMini_reasoning": "The study presents a direct speech-to-speech translation model from Punjabi to English using discrete acoustic units, focusing on end-to-end speech translation. However, it does not explicitly discuss Spoken Language Models or Full-Duplex Spoken Language Models, cognitive parallelism, synchronization, or real-time interactive behaviors, nor does it include evaluation related to turn-taking, interruption, or backchanneling. The core focus is on speech translation performance rather than modeling speech sequences for interaction or conversational dynamics, thus it does not fully meet the inclusion criteria centered on spoken language modeling and synchronous interaction capabilities.",
    "round-A_JuniorMini_evaluation": 2,
    "round-B_SeniorLead_output": {
      "reasoning": "I reviewed the abstract and the two junior reviews: the paper presents an end-to-end Punjabi→English speech-to-speech translation using discrete acoustic units, but the abstract does not use the required explicit phrasing (e.g., \"Spoken Language Model(s)\" or equivalent), nor does it address full‑duplex interaction, synchronization, cognitive parallelism, overlap/interrupt handling, or low‑latency real‑time evaluation—therefore it fails the inclusion criteria and matches the exclusions. Score explanation: absolutely exclude (1) — it focuses on a single downstream translation task without presenting spoken language modeling or synchronous interaction contributions required by the scope.",
      "evaluation": 1
    },
    "round-B_SeniorLead_reasoning": "I reviewed the abstract and the two junior reviews: the paper presents an end-to-end Punjabi→English speech-to-speech translation using discrete acoustic units, but the abstract does not use the required explicit phrasing (e.g., \"Spoken Language Model(s)\" or equivalent), nor does it address full‑duplex interaction, synchronization, cognitive parallelism, overlap/interrupt handling, or low‑latency real‑time evaluation—therefore it fails the inclusion criteria and matches the exclusions. Score explanation: absolutely exclude (1) — it focuses on a single downstream translation task without presenting spoken language modeling or synchronous interaction contributions required by the scope.",
    "round-B_SeniorLead_evaluation": 1,
    "final_verdict": "exclude (senior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Towards ASR Robust Spoken Language Understanding Through in-Context Learning with Word Confusion Networks",
    "abstract": "In the realm of spoken language understanding (SLU). numerous natural language understanding (NLU) methodologies have been adapted by supplying large language models (LLMs) with transcribed speech instead of conventional written text. In real-world scenarios, prior to input into an LLM. an automated speech recognition (ASR) system generates an output transcript hypothesis, where inherent errors can degrade subsequent SLU tasks. Here we introduce a method that utilizes the ASR system's lattice output instead of relying solely on the top hypothesis, aiming to encapsulate speech ambiguities and enhance SLU outcomes. Our in-context learning experiments, covering spoken question answering and intent classification. underline the LLM's resilience to noisy speech transcripts with the help of word confusion networks from lattices, bridging the SLU performance gap between using the top ASR hypothesis and an oracle upper bound. Additionally, we delve into the LLM's robustness to varying ASR performance conditions and scrutinize the aspects of in-context learning which prove the most influential.",
    "metadata": {
      "title": "Towards ASR Robust Spoken Language Understanding Through in-Context Learning with Word Confusion Networks",
      "summary": "In the realm of spoken language understanding (SLU). numerous natural language understanding (NLU) methodologies have been adapted by supplying large language models (LLMs) with transcribed speech instead of conventional written text. In real-world scenarios, prior to input into an LLM. an automated speech recognition (ASR) system generates an output transcript hypothesis, where inherent errors can degrade subsequent SLU tasks. Here we introduce a method that utilizes the ASR system's lattice output instead of relying solely on the top hypothesis, aiming to encapsulate speech ambiguities and enhance SLU outcomes. Our in-context learning experiments, covering spoken question answering and intent classification. underline the LLM's resilience to noisy speech transcripts with the help of word confusion networks from lattices, bridging the SLU performance gap between using the top ASR hypothesis and an oracle upper bound. Additionally, we delve into the LLM's robustness to varying ASR performance conditions and scrutinize the aspects of in-context learning which prove the most influential.",
      "abstract": "In the realm of spoken language understanding (SLU). numerous natural language understanding (NLU) methodologies have been adapted by supplying large language models (LLMs) with transcribed speech instead of conventional written text. In real-world scenarios, prior to input into an LLM. an automated speech recognition (ASR) system generates an output transcript hypothesis, where inherent errors can degrade subsequent SLU tasks. Here we introduce a method that utilizes the ASR system's lattice output instead of relying solely on the top hypothesis, aiming to encapsulate speech ambiguities and enhance SLU outcomes. Our in-context learning experiments, covering spoken question answering and intent classification. underline the LLM's resilience to noisy speech transcripts with the help of word confusion networks from lattices, bridging the SLU performance gap between using the top ASR hypothesis and an oracle upper bound. Additionally, we delve into the LLM's robustness to varying ASR performance conditions and scrutinize the aspects of in-context learning which prove the most influential.",
      "doi": "https://doi.org/10.1109/icassp48485.2024.10447938",
      "openalex_id": "https://openalex.org/W4392909867",
      "arxiv_id": "",
      "publication_date": "2024-03-18",
      "published": "2024-03-18",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "根據題目，論文以語音輸入與語音層面的建模為核心，利用 ASR lattices 的語音不確定性提升 SLU，並結合大語言模型的在-context learning，且涵蓋模型設計/訓練/評估，符合近三年的 Spoken Language Model 研究主題與評估框架。",
      "evaluation": 5
    },
    "round-A_JuniorNano_reasoning": "根據題目，論文以語音輸入與語音層面的建模為核心，利用 ASR lattices 的語音不確定性提升 SLU，並結合大語言模型的在-context learning，且涵蓋模型設計/訓練/評估，符合近三年的 Spoken Language Model 研究主題與評估框架。",
    "round-A_JuniorNano_evaluation": 5,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on using ASR outputs and word confusion networks to improve spoken language understanding by feeding them into large language models, which aligns with SLU but mainly treats speech as input to LLMs rather than modeling speech sequences or acoustic signals directly as the core modeling/reasoning object. The paper does not mention full-duplex or synchronous interaction capabilities, cognitive parallelism, or engineered/learned synchronization, nor does it propose new spoken language model architectures but rather applies in-context learning to improve SLU robustness. Thus, it partially meets inclusion criteria but lacks key aspects of spoken language modeling as defined (e.g., end-to-end speech modeling, full-duplex interaction, multi-dimensional evaluation). Therefore, inclusion is uncertain without clear evidence of comprehensive spoken language model focus and synchronous interaction capabilities.",
      "evaluation": 3
    },
    "round-A_JuniorMini_reasoning": "The study focuses on using ASR outputs and word confusion networks to improve spoken language understanding by feeding them into large language models, which aligns with SLU but mainly treats speech as input to LLMs rather than modeling speech sequences or acoustic signals directly as the core modeling/reasoning object. The paper does not mention full-duplex or synchronous interaction capabilities, cognitive parallelism, or engineered/learned synchronization, nor does it propose new spoken language model architectures but rather applies in-context learning to improve SLU robustness. Thus, it partially meets inclusion criteria but lacks key aspects of spoken language modeling as defined (e.g., end-to-end speech modeling, full-duplex interaction, multi-dimensional evaluation). Therefore, inclusion is uncertain without clear evidence of comprehensive spoken language model focus and synchronous interaction capabilities.",
    "round-A_JuniorMini_evaluation": 3,
    "round-B_SeniorLead_output": {
      "reasoning": "Junior reviewers: JuniorNano argued the paper aligns with spoken-language research because it improves SLU using ASR lattice information with LLM in-context learning; JuniorMini correctly pointed out the work primarily treats speech as ASR output fed into an LLM and does not develop or evaluate Spoken Language Models, full‑duplex interaction, or speech-token/ acoustic-level modeling. Final judgment: this paper fails the inclusion criteria because it does not present spoken language models (or equivalent terminology) as the core modeling object, instead relying on ASR→LLM pipelines and SLU task robustness—thus it meets exclusion criteria (speech only as input; core work remains text/LLM integration). Score: 1 (absolutely exclude).",
      "evaluation": 1
    },
    "round-B_SeniorLead_reasoning": "Junior reviewers: JuniorNano argued the paper aligns with spoken-language research because it improves SLU using ASR lattice information with LLM in-context learning; JuniorMini correctly pointed out the work primarily treats speech as ASR output fed into an LLM and does not develop or evaluate Spoken Language Models, full‑duplex interaction, or speech-token/ acoustic-level modeling. Final judgment: this paper fails the inclusion criteria because it does not present spoken language models (or equivalent terminology) as the core modeling object, instead relying on ASR→LLM pipelines and SLU task robustness—thus it meets exclusion criteria (speech only as input; core work remains text/LLM integration). Score: 1 (absolutely exclude).",
    "round-B_SeniorLead_evaluation": 1,
    "final_verdict": "exclude (senior:1)",
    "review_skipped": false,
    "discard_reason": null
  }
]