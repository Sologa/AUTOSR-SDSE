openalex_id,doi,title,abstract,referenced_works,publication_date
https://openalex.org/W4388017359,https://doi.org/10.1109/taslp.2023.3328283,End-to-End Speech Recognition: A Survey,"In the last decade of automatic speech recognition (ASR) research, the introduction of deep learning has brought considerable reductions in word error rate of more than 50% relative, compared to modeling without deep learning. In the wake of this transition, a number of all-neural ASR architectures have been introduced. These so-called <italic xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">end-to-end</i> (E2E) models provide highly integrated, completely neural ASR models, which rely strongly on general machine learning knowledge, learn more consistently from data, with lower dependence on ASR domain-specific experience. The success and enthusiastic adoption of deep learning, accompanied by more generic model architectures has led to E2E models now becoming the prominent ASR approach. The goal of this survey is to provide a taxonomy of E2E ASR models and corresponding improvements, and to discuss their properties and their relationship to classical hidden Markov model (HMM) based ASR architectures. All relevant aspects of E2E ASR are covered in this work: modeling, training, decoding, and external language model integration, discussions of performance and deployment opportunities, as well as an outlook into potential future developments.","['https://openalex.org/W6715097780', 'https://openalex.org/W2125838338', 'https://openalex.org/W2394932179', 'https://openalex.org/W98857008', 'https://openalex.org/W2165712214', 'https://openalex.org/W1588735863', 'https://openalex.org/W6680532216', 'https://openalex.org/W2056590938', 'https://openalex.org/W2408093180', 'https://openalex.org/W2288217446', 'https://openalex.org/W2127141656', 'https://openalex.org/W6638749077', 'https://openalex.org/W6623517193', 'https://openalex.org/W2327501763', 'https://openalex.org/W2143564602', 'https://openalex.org/W6683738474', 'https://openalex.org/W6675365184', 'https://openalex.org/W2889129739', 'https://openalex.org/W2962760690', 'https://openalex.org/W2129545859', 'https://openalex.org/W2100180150', 'https://openalex.org/W6780218876', 'https://openalex.org/W3008762051', 'https://openalex.org/W2962699523', 'https://openalex.org/W2972889948', 'https://openalex.org/W2545177271', 'https://openalex.org/W2962784628', 'https://openalex.org/W6728811460', 'https://openalex.org/W6734588641', 'https://openalex.org/W2899879954', 'https://openalex.org/W3198455051', 'https://openalex.org/W2121879602', 'https://openalex.org/W2046932483', 'https://openalex.org/W2889187401', 'https://openalex.org/W3197991202', 'https://openalex.org/W2750499125', 'https://openalex.org/W2064675550', 'https://openalex.org/W2105482032', 'https://openalex.org/W4385245566', 'https://openalex.org/W3097777922', 'https://openalex.org/W2746192915', 'https://openalex.org/W3016234571', 'https://openalex.org/W2143612262', 'https://openalex.org/W3211040052', 'https://openalex.org/W6690026940', 'https://openalex.org/W3008174054', 'https://openalex.org/W3028545098', 'https://openalex.org/W2962826786', 'https://openalex.org/W6727690538', 'https://openalex.org/W2889504751', 'https://openalex.org/W2915977493', 'https://openalex.org/W2745439869', 'https://openalex.org/W3008898571', 'https://openalex.org/W6685711979', 'https://openalex.org/W6735706088', 'https://openalex.org/W6747158283', 'https://openalex.org/W2952992734', 'https://openalex.org/W2962742956', 'https://openalex.org/W2936123380', 'https://openalex.org/W2972995428', 'https://openalex.org/W6793472422', 'https://openalex.org/W4319862683', 'https://openalex.org/W3015671919', 'https://openalex.org/W2514741789', 'https://openalex.org/W6727336983', 'https://openalex.org/W66978610', 'https://openalex.org/W2748816379', 'https://openalex.org/W4319862408', 'https://openalex.org/W2963211739', 'https://openalex.org/W6640090968', 'https://openalex.org/W2608712415', 'https://openalex.org/W2577366047', 'https://openalex.org/W2530876040', 'https://openalex.org/W2963303028', 'https://openalex.org/W2964012862', 'https://openalex.org/W6754576867', 'https://openalex.org/W2889163603', 'https://openalex.org/W3008525923', 'https://openalex.org/W2131968858', 'https://openalex.org/W2606722458', 'https://openalex.org/W2766219058', 'https://openalex.org/W2973122799', 'https://openalex.org/W3011339933', 'https://openalex.org/W3163203022', 'https://openalex.org/W6784400248', 'https://openalex.org/W6784800133', 'https://openalex.org/W2972625221', 'https://openalex.org/W2886319145', 'https://openalex.org/W2886025712', 'https://openalex.org/W2937402758', 'https://openalex.org/W2889374926', 'https://openalex.org/W3095173472', 'https://openalex.org/W2892009249', 'https://openalex.org/W3016010032', 'https://openalex.org/W6839026989', 'https://openalex.org/W2972818416', 'https://openalex.org/W3163793923', 'https://openalex.org/W3197976839', 'https://openalex.org/W3015686596', 'https://openalex.org/W3160551958', 'https://openalex.org/W3161375121', 'https://openalex.org/W3202419788', 'https://openalex.org/W4319862418', 'https://openalex.org/W6853611000', 'https://openalex.org/W6853998256', 'https://openalex.org/W3008284571', 'https://openalex.org/W3015927303', 'https://openalex.org/W3015383801', 'https://openalex.org/W3198116002', 'https://openalex.org/W1806891645', 'https://openalex.org/W6910546390', 'https://openalex.org/W2105594594', 'https://openalex.org/W6679128827', 'https://openalex.org/W1991133427', 'https://openalex.org/W2155368638', 'https://openalex.org/W4224518768', 'https://openalex.org/W6796656850', 'https://openalex.org/W3202184514', 'https://openalex.org/W2024539680', 'https://openalex.org/W3198439131', 'https://openalex.org/W3206876927', 'https://openalex.org/W4372259859', 'https://openalex.org/W2033565080', 'https://openalex.org/W2157749010', 'https://openalex.org/W1979136262', 'https://openalex.org/W2114016253', 'https://openalex.org/W2001679125', 'https://openalex.org/W2953561564', 'https://openalex.org/W2963747784', 'https://openalex.org/W3095697114', 'https://openalex.org/W2900209846', 'https://openalex.org/W2963144852', 'https://openalex.org/W2892124901', 'https://openalex.org/W3095376166', 'https://openalex.org/W2136922672', 'https://openalex.org/W2110798204', 'https://openalex.org/W2471933213', 'https://openalex.org/W2799923439', 'https://openalex.org/W2799800213', 'https://openalex.org/W6757856092', 'https://openalex.org/W2963571336', 'https://openalex.org/W2951974815', 'https://openalex.org/W1501286448', 'https://openalex.org/W1975550806', 'https://openalex.org/W6752630080', 'https://openalex.org/W2963431393', 'https://openalex.org/W3096215352', 'https://openalex.org/W3096032230', 'https://openalex.org/W4210463634', 'https://openalex.org/W3204696009', 'https://openalex.org/W4226120743', 'https://openalex.org/W2033245860', 'https://openalex.org/W1587755118', 'https://openalex.org/W2000200144', 'https://openalex.org/W6757817989', 'https://openalex.org/W6745410505', 'https://openalex.org/W2963026768', 'https://openalex.org/W6863618527', 'https://openalex.org/W3163842339', 'https://openalex.org/W2151834591', 'https://openalex.org/W2296073425', 'https://openalex.org/W6687566353', 'https://openalex.org/W3097747488', 'https://openalex.org/W3017474798', 'https://openalex.org/W1988720110', 'https://openalex.org/W6604254268', 'https://openalex.org/W6631190155', 'https://openalex.org/W3198654230', 'https://openalex.org/W4206410067', 'https://openalex.org/W6681151457', 'https://openalex.org/W2145249131', 'https://openalex.org/W6692956712', 'https://openalex.org/W6640036494', 'https://openalex.org/W2618530766', 'https://openalex.org/W2331143823', 'https://openalex.org/W2972451902', 'https://openalex.org/W3162249256', 'https://openalex.org/W6600213771', 'https://openalex.org/W6714142977', 'https://openalex.org/W2183341477', 'https://openalex.org/W6621543089', 'https://openalex.org/W6749075489', 'https://openalex.org/W2057653135', 'https://openalex.org/W6749518548', 'https://openalex.org/W3163839574', 'https://openalex.org/W1989674786', 'https://openalex.org/W2407080277', 'https://openalex.org/W6675409298', 'https://openalex.org/W2937780860', 'https://openalex.org/W3015995734', 'https://openalex.org/W3095189764', 'https://openalex.org/W2883586237', 'https://openalex.org/W4225334634', 'https://openalex.org/W2964107261', 'https://openalex.org/W1583239513', 'https://openalex.org/W2050526637', 'https://openalex.org/W1986184096', 'https://openalex.org/W2516457973', 'https://openalex.org/W2808939837', 'https://openalex.org/W2127095586', 'https://openalex.org/W2914018192', 'https://openalex.org/W2888909726', 'https://openalex.org/W206545267', 'https://openalex.org/W4473315', 'https://openalex.org/W6793076252', 'https://openalex.org/W6785891320', 'https://openalex.org/W2944255943', 'https://openalex.org/W3026041220', 'https://openalex.org/W2936774411', 'https://openalex.org/W3015726069', 'https://openalex.org/W2291975472', 'https://openalex.org/W2971840980', 'https://openalex.org/W108866686', 'https://openalex.org/W3097882114', 'https://openalex.org/W6774835902', 'https://openalex.org/W3096160024', 'https://openalex.org/W3016167541', 'https://openalex.org/W3197140813', 'https://openalex.org/W3206573929', 'https://openalex.org/W3094713728', 'https://openalex.org/W2963506925', 'https://openalex.org/W2963260202', 'https://openalex.org/W2886180730', 'https://openalex.org/W6854240803', 'https://openalex.org/W4299649720', 'https://openalex.org/W2151058131', 'https://openalex.org/W2008554732', 'https://openalex.org/W2066378046', 'https://openalex.org/W6765658108', 'https://openalex.org/W3015190365', 'https://openalex.org/W1710082047', 'https://openalex.org/W3015974384', 'https://openalex.org/W6770245836', 'https://openalex.org/W3097973766', 'https://openalex.org/W3197304116', 'https://openalex.org/W2972977747', 'https://openalex.org/W3148654612', 'https://openalex.org/W2963022149', 'https://openalex.org/W2627092829', 'https://openalex.org/W2963240019', 'https://openalex.org/W3163300396', 'https://openalex.org/W2972780808', 'https://openalex.org/W2787663903', 'https://openalex.org/W3008912312', 'https://openalex.org/W2972837679', 'https://openalex.org/W2938348542', 'https://openalex.org/W3197478142', 'https://openalex.org/W6797037654', 'https://openalex.org/W2972953886', 'https://openalex.org/W3005302685', 'https://openalex.org/W6713134421', 'https://openalex.org/W2933138175', 'https://openalex.org/W6760633627', 'https://openalex.org/W2962728618', 'https://openalex.org/W2972630480', 'https://openalex.org/W2949975180', 'https://openalex.org/W3015369343', 'https://openalex.org/W2091981305', 'https://openalex.org/W2972528057', 'https://openalex.org/W3016053754', 'https://openalex.org/W2964103964', 'https://openalex.org/W2963382396', 'https://openalex.org/W2970692082', 'https://openalex.org/W2078354939', 'https://openalex.org/W2136617108', 'https://openalex.org/W3015501067', 'https://openalex.org/W2972799770', 'https://openalex.org/W2972621414', 'https://openalex.org/W6752334204', 'https://openalex.org/W2097927681', 'https://openalex.org/W179875071', 'https://openalex.org/W2402268235', 'https://openalex.org/W2566563465', 'https://openalex.org/W6731370813', 'https://openalex.org/W6757424787', 'https://openalex.org/W2940180244', 'https://openalex.org/W2963088785', 'https://openalex.org/W2943845043', 'https://openalex.org/W2964110616', 'https://openalex.org/W2150355110', 'https://openalex.org/W6770251742', 'https://openalex.org/W6640059789', 'https://openalex.org/W2888779557', 'https://openalex.org/W2939111082', 'https://openalex.org/W3008037978', 'https://openalex.org/W3152221657', 'https://openalex.org/W3205201903', 'https://openalex.org/W6755207826', 'https://openalex.org/W3024308166', 'https://openalex.org/W2962745521', 'https://openalex.org/W3197507772', 'https://openalex.org/W1966812932', 'https://openalex.org/W2014151772', 'https://openalex.org/W2080213370', 'https://openalex.org/W1985258458', 'https://openalex.org/W2396464458', 'https://openalex.org/W2166637769', 'https://openalex.org/W1494198834', 'https://openalex.org/W3008191852', 'https://openalex.org/W3209059054', 'https://openalex.org/W6770506093', 'https://openalex.org/W3147414526', 'https://openalex.org/W2995181338', 'https://openalex.org/W2981857663', 'https://openalex.org/W4319862255', 'https://openalex.org/W2972692349', 'https://openalex.org/W2962824709', 'https://openalex.org/W3007528493', 'https://openalex.org/W3094667432', 'https://openalex.org/W3048407879', 'https://openalex.org/W3162665866', 'https://openalex.org/W3161873870', 'https://openalex.org/W3015194534', 'https://openalex.org/W3160766462', 'https://openalex.org/W3198442913', 'https://openalex.org/W6803092890', 'https://openalex.org/W6810259195', 'https://openalex.org/W4223622550', 'https://openalex.org/W3148001440', 'https://openalex.org/W3205644108', 'https://openalex.org/W4225319488', 'https://openalex.org/W4319862474', 'https://openalex.org/W2963739817', 'https://openalex.org/W6735168207', 'https://openalex.org/W3211278025', 'https://openalex.org/W4221155340', 'https://openalex.org/W2079656678', 'https://openalex.org/W2952230511', 'https://openalex.org/W3034775979', 'https://openalex.org/W4394662461', 'https://openalex.org/W2792376130', 'https://openalex.org/W4297781872', 'https://openalex.org/W1922655562', 'https://openalex.org/W3207222250', 'https://openalex.org/W3147187328', 'https://openalex.org/W3007328579', 'https://openalex.org/W3151269043', 'https://openalex.org/W1508165687', 'https://openalex.org/W1904365287', 'https://openalex.org/W3170405627', 'https://openalex.org/W4381827575', 'https://openalex.org/W4378501656', 'https://openalex.org/W4383605108', 'https://openalex.org/W3167895882', 'https://openalex.org/W2987019345', 'https://openalex.org/W3100910367', 'https://openalex.org/W2525778437', 'https://openalex.org/W1915251500', 'https://openalex.org/W2242818861', 'https://openalex.org/W2411921399', 'https://openalex.org/W2808640845', 'https://openalex.org/W2520160253', 'https://openalex.org/W3092122846', 'https://openalex.org/W3105532142', 'https://openalex.org/W2928941594', 'https://openalex.org/W4288290348', 'https://openalex.org/W1553004968', 'https://openalex.org/W2904818793', 'https://openalex.org/W3094957294', 'https://openalex.org/W3103005696']",2023-10-30
https://openalex.org/W4402155831,https://doi.org/10.2196/59505,"Multimodal Large Language Models in Health Care: Applications, Challenges, and Future Outlook","In the complex and multidimensional field of medicine, multimodal data are prevalent and crucial for informed clinical decisions. Multimodal data span a broad spectrum of data types, including medical images (eg, MRI and CT scans), time-series data (eg, sensor data from wearable devices and electronic health records), audio recordings (eg, heart and respiratory sounds and patient interviews), text (eg, clinical notes and research articles), videos (eg, surgical procedures), and omics data (eg, genomics and proteomics). While advancements in large language models (LLMs) have enabled new applications for knowledge retrieval and processing in the medical field, most LLMs remain limited to processing unimodal data, typically text-based content, and often overlook the importance of integrating the diverse data modalities encountered in clinical practice. This paper aims to present a detailed, practical, and solution-oriented perspective on the use of multimodal LLMs (M-LLMs) in the medical field. Our investigation spanned M-LLM foundational principles, current and potential applications, technical and ethical challenges, and future research directions. By connecting these elements, we aimed to provide a comprehensive framework that links diverse aspects of M-LLMs, offering a unified vision for their future in health care. This approach aims to guide both future research and practical implementations of M-LLMs in health care, positioning them as a paradigm shift toward integrated, multimodal data–driven medical practice. We anticipate that this work will spark further discussion and inspire the development of innovative approaches in the next generation of medical M-LLM systems.","['https://openalex.org/W4387865285', 'https://openalex.org/W4387500346', 'https://openalex.org/W4386575491', 'https://openalex.org/W4385164288', 'https://openalex.org/W4384071683', 'https://openalex.org/W4387809804', 'https://openalex.org/W4377011132', 'https://openalex.org/W3122765966', 'https://openalex.org/W4285987711', 'https://openalex.org/W4221022534', 'https://openalex.org/W2064675550', 'https://openalex.org/W2983587580', 'https://openalex.org/W4391829954', 'https://openalex.org/W4310645210', 'https://openalex.org/W3017637887', 'https://openalex.org/W3129155125', 'https://openalex.org/W3193158708', 'https://openalex.org/W4389686112', 'https://openalex.org/W4385647263', 'https://openalex.org/W4388931647', 'https://openalex.org/W4321459182', 'https://openalex.org/W4387381389', 'https://openalex.org/W4378672794', 'https://openalex.org/W4386052426', 'https://openalex.org/W4387028645', 'https://openalex.org/W4386892967', 'https://openalex.org/W4386735541', 'https://openalex.org/W4387653139', 'https://openalex.org/W4386923268', 'https://openalex.org/W4387583347', 'https://openalex.org/W4385346108', 'https://openalex.org/W4308885870', 'https://openalex.org/W4366330503', 'https://openalex.org/W4376167553', 'https://openalex.org/W4379918953', 'https://openalex.org/W4389524500', 'https://openalex.org/W4378711593', 'https://openalex.org/W4376864533', 'https://openalex.org/W4281643269', 'https://openalex.org/W4386168937', 'https://openalex.org/W3094502228', 'https://openalex.org/W2962835968', 'https://openalex.org/W2194775991', 'https://openalex.org/W4382403009', 'https://openalex.org/W1922655562', 'https://openalex.org/W4388430464', 'https://openalex.org/W4388831047', 'https://openalex.org/W4389538721', 'https://openalex.org/W2889664156', 'https://openalex.org/W2345512687', 'https://openalex.org/W2952935243', 'https://openalex.org/W3177049011', 'https://openalex.org/W3009260245', 'https://openalex.org/W2972119347', 'https://openalex.org/W4299732308', 'https://openalex.org/W4389116614', 'https://openalex.org/W4390580666', 'https://openalex.org/W4386185600', 'https://openalex.org/W4391159225', 'https://openalex.org/W4390214291', 'https://openalex.org/W4389925873', 'https://openalex.org/W4389708725', 'https://openalex.org/W4388962905', 'https://openalex.org/W4388513209', 'https://openalex.org/W4388926400', 'https://openalex.org/W4388685775', 'https://openalex.org/W4393149524', 'https://openalex.org/W4383987918', 'https://openalex.org/W4388482284', 'https://openalex.org/W4379259189', 'https://openalex.org/W4377121462', 'https://openalex.org/W4385436420', 'https://openalex.org/W4390690017', 'https://openalex.org/W4390115208', 'https://openalex.org/W4312220150', 'https://openalex.org/W4387355843', 'https://openalex.org/W4385965979', 'https://openalex.org/W4387559560', 'https://openalex.org/W4389664922', 'https://openalex.org/W4387724855', 'https://openalex.org/W4387634898', 'https://openalex.org/W4378498682', 'https://openalex.org/W4381827575', 'https://openalex.org/W4381786045', 'https://openalex.org/W4377372369', 'https://openalex.org/W4393178509', 'https://openalex.org/W4401070302', 'https://openalex.org/W4387891768', 'https://openalex.org/W4386185396', 'https://openalex.org/W4392019855', 'https://openalex.org/W2911489562', 'https://openalex.org/W3046375318', 'https://openalex.org/W4380137126', 'https://openalex.org/W4389216607', 'https://openalex.org/W4382334257', 'https://openalex.org/W4386083024', 'https://openalex.org/W4385474169', 'https://openalex.org/W4386066385', 'https://openalex.org/W4378174011', 'https://openalex.org/W4389261102', 'https://openalex.org/W4390783938', 'https://openalex.org/W4387966979', 'https://openalex.org/W3127238141', 'https://openalex.org/W4382490702', 'https://openalex.org/W4384133826', 'https://openalex.org/W4296613150', 'https://openalex.org/W4288421316', 'https://openalex.org/W4319065545', 'https://openalex.org/W4319335178', 'https://openalex.org/W4225917625', 'https://openalex.org/W3195980265', 'https://openalex.org/W4387346412', 'https://openalex.org/W3177500196', 'https://openalex.org/W4318071656', 'https://openalex.org/W4318751307', 'https://openalex.org/W4205773061', 'https://openalex.org/W4366163632', 'https://openalex.org/W4391316987', 'https://openalex.org/W4323572061', 'https://openalex.org/W4387210470', 'https://openalex.org/W4386655647', 'https://openalex.org/W4380994269', 'https://openalex.org/W4389502051', 'https://openalex.org/W4388555312', 'https://openalex.org/W4390041933', 'https://openalex.org/W4392947532', 'https://openalex.org/W4386047824', 'https://openalex.org/W4386157633', 'https://openalex.org/W4292438865', 'https://openalex.org/W4376130909', 'https://openalex.org/W4313439128', 'https://openalex.org/W4387789684', 'https://openalex.org/W4308393531', 'https://openalex.org/W4386893702', 'https://openalex.org/W4361289277', 'https://openalex.org/W4387068110', 'https://openalex.org/W4220867361', 'https://openalex.org/W4390480840', 'https://openalex.org/W4390603852', 'https://openalex.org/W3168867926', 'https://openalex.org/W4378509449', 'https://openalex.org/W4297813615', 'https://openalex.org/W4386876368', 'https://openalex.org/W1821462560', 'https://openalex.org/W4322718246', 'https://openalex.org/W4313484371', 'https://openalex.org/W4387323474', 'https://openalex.org/W2766839578', 'https://openalex.org/W2606722458', 'https://openalex.org/W4390490761', 'https://openalex.org/W3038035611', 'https://openalex.org/W3174057701', 'https://openalex.org/W4377226909', 'https://openalex.org/W4392849937', 'https://openalex.org/W2963400886', 'https://openalex.org/W4395026179', 'https://openalex.org/W6908377304', 'https://openalex.org/W4389524012', 'https://openalex.org/W4395703766', 'https://openalex.org/W4376643691', 'https://openalex.org/W4382394524', 'https://openalex.org/W3018610833', 'https://openalex.org/W3107627743', 'https://openalex.org/W2895917461', 'https://openalex.org/W2905657479', 'https://openalex.org/W2168610667', 'https://openalex.org/W4226196842', 'https://openalex.org/W3128912454', 'https://openalex.org/W4394763992', 'https://openalex.org/W2996550193', 'https://openalex.org/W4225966403', 'https://openalex.org/W4380421649']",2024-08-20
https://openalex.org/W4390075359,https://doi.org/10.1162/tacl_a_00618,"Speak, Read and Prompt: High-Fidelity Text-to-Speech with Minimal Supervision","Abstract We introduce SPEAR-TTS, a multi-speaker text-to-speech (TTS) system that can be trained with minimal supervision. By combining two types of discrete speech representations, we cast TTS as a composition of two sequence-to-sequence tasks: from text to high-level semantic tokens (akin to “reading”) and from semantic tokens to low-level acoustic tokens (“speaking”). Decoupling these two tasks enables training of the “speaking” module using abundant audio-only data, and unlocks the highly efficient combination of pretraining and backtranslation to reduce the need for parallel data when training the “reading” component. To control the speaker identity, we adopt example prompting, which allows SPEAR-TTS to generalize to unseen speakers using only a short sample of 3 seconds, without any explicit speaker representation or speaker labels. Our experiments demonstrate that SPEAR-TTS achieves a character error rate that is competitive with state-of-the-art methods using only 15 minutes of parallel data, while matching ground-truth speech in naturalness and acoustic quality.","['https://openalex.org/W6849105126', 'https://openalex.org/W3205644108', 'https://openalex.org/W4381786045', 'https://openalex.org/W6778883912', 'https://openalex.org/W6805710207', 'https://openalex.org/W3209984917', 'https://openalex.org/W4226033575', 'https://openalex.org/W6800767084', 'https://openalex.org/W2889326796', 'https://openalex.org/W6638667902', 'https://openalex.org/W6917585676', 'https://openalex.org/W4380874786', 'https://openalex.org/W2995181338', 'https://openalex.org/W3198217962', 'https://openalex.org/W6804184754', 'https://openalex.org/W6849956205', 'https://openalex.org/W6783867762', 'https://openalex.org/W4385574033', 'https://openalex.org/W6790356757', 'https://openalex.org/W4296068981', 'https://openalex.org/W3034999214', 'https://openalex.org/W6810406373', 'https://openalex.org/W3015419784', 'https://openalex.org/W4307680525', 'https://openalex.org/W1494198834', 'https://openalex.org/W4285182272', 'https://openalex.org/W3140429000', 'https://openalex.org/W6769627184', 'https://openalex.org/W3161480375', 'https://openalex.org/W3033411150', 'https://openalex.org/W3016181583', 'https://openalex.org/W6853998256', 'https://openalex.org/W2963216553', 'https://openalex.org/W6751104502', 'https://openalex.org/W2964243274', 'https://openalex.org/W6677920722', 'https://openalex.org/W6739901393', 'https://openalex.org/W6848735303', 'https://openalex.org/W3215615641', 'https://openalex.org/W2972359262', 'https://openalex.org/W4394671563', 'https://openalex.org/W4288089799', 'https://openalex.org/W4381827575', 'https://openalex.org/W4296070453', 'https://openalex.org/W4320459320', 'https://openalex.org/W4318351475', 'https://openalex.org/W4385245566', 'https://openalex.org/W3198123200', 'https://openalex.org/W3215895588', 'https://openalex.org/W3092028330', 'https://openalex.org/W4313679638', 'https://openalex.org/W4292779060']",2023-01-01
https://openalex.org/W4391021666,https://doi.org/10.1109/asru57964.2023.10389705,On Decoder-Only Architecture For Speech-to-Text and Large Language Model Integration,"Large language models (LLMs) have achieved remarkable success in the field of natural language processing, enabling better human-computer interaction using natural language. However, the seamless integration of speech signals into LLMs has not been explored well. The ""decoder-only"" architecture has also not been well studied for speech processing tasks. In this research, we introduce Speech-LLaMA, a novel approach that effectively incorporates acoustic information into text-based large language models. Our method leverages Connectionist Temporal Classification and a simple audio encoder to map the compressed acoustic features to the continuous semantic space of the LLM. In addition, we further probe the decoder-only architecture for speech-to-text tasks by training a smaller scale randomly initialized speech-LLaMA model from speech-text paired data alone. We conduct experiments on multilingual speech-to-text translation tasks and demonstrate a significant improvement over strong baselines, highlighting the potential advantages of decoder-only models for speech-to-text conversion.","['https://openalex.org/W6778883912', 'https://openalex.org/W6810081322', 'https://openalex.org/W6850625674', 'https://openalex.org/W6739901393', 'https://openalex.org/W6851847159', 'https://openalex.org/W6851513886', 'https://openalex.org/W6852326057', 'https://openalex.org/W6852818750', 'https://openalex.org/W3211278025', 'https://openalex.org/W6851950068', 'https://openalex.org/W6851592950', 'https://openalex.org/W6852489829', 'https://openalex.org/W6810334672', 'https://openalex.org/W6853611000', 'https://openalex.org/W4389524500', 'https://openalex.org/W6853998256', 'https://openalex.org/W6848735303', 'https://openalex.org/W6850334629', 'https://openalex.org/W6851317108', 'https://openalex.org/W3153583341', 'https://openalex.org/W6796581206', 'https://openalex.org/W2901607128', 'https://openalex.org/W3034625919', 'https://openalex.org/W6846175339', 'https://openalex.org/W4224137820', 'https://openalex.org/W4385823495', 'https://openalex.org/W6780680273', 'https://openalex.org/W2963532001', 'https://openalex.org/W6769627184', 'https://openalex.org/W6732953234', 'https://openalex.org/W2605131327', 'https://openalex.org/W6847363464', 'https://openalex.org/W2739883972', 'https://openalex.org/W6773820404', 'https://openalex.org/W4388979610', 'https://openalex.org/W6757817989', 'https://openalex.org/W2963250244', 'https://openalex.org/W6854218657', 'https://openalex.org/W4366330503', 'https://openalex.org/W4393178509', 'https://openalex.org/W3168867926', 'https://openalex.org/W4391021781', 'https://openalex.org/W4361866031', 'https://openalex.org/W4323651091', 'https://openalex.org/W4292779060', 'https://openalex.org/W4381827575', 'https://openalex.org/W4301581299', 'https://openalex.org/W4375958083', 'https://openalex.org/W4322718191', 'https://openalex.org/W4288089799', 'https://openalex.org/W2908510526', 'https://openalex.org/W4385245566', 'https://openalex.org/W4367628410', 'https://openalex.org/W4391021457', 'https://openalex.org/W4377372369', 'https://openalex.org/W4224308101', 'https://openalex.org/W4313679638', 'https://openalex.org/W4225323055', 'https://openalex.org/W4366850747', 'https://openalex.org/W4364382977', 'https://openalex.org/W3043665049', 'https://openalex.org/W4378501656']",2023-12-16
https://openalex.org/W4389881233,https://doi.org/10.1108/jebde-08-2023-0015,Unraveling the landscape of large language models: a systematic review and future perspectives,"Purpose The rapid rise of large language models (LLMs) has propelled them to the forefront of applications in natural language processing (NLP). This paper aims to present a comprehensive examination of the research landscape in LLMs, providing an overview of the prevailing themes and topics within this dynamic domain. Design/methodology/approach Drawing from an extensive corpus of 198 records published between 1996 to 2023 from the relevant academic database encompassing journal articles, books, book chapters, conference papers and selected working papers, this study delves deep into the multifaceted world of LLM research. In this study, the authors employed the BERTopic algorithm, a recent advancement in topic modeling, to conduct a comprehensive analysis of the data after it had been meticulously cleaned and preprocessed. BERTopic leverages the power of transformer-based language models like bidirectional encoder representations from transformers (BERT) to generate more meaningful and coherent topics. This approach facilitates the identification of hidden patterns within the data, enabling authors to uncover valuable insights that might otherwise have remained obscure. The analysis revealed four distinct clusters of topics in LLM research: “language and NLP”, “education and teaching”, “clinical and medical applications” and “speech and recognition techniques”. Each cluster embodies a unique aspect of LLM application and showcases the breadth of possibilities that LLM technology has to offer. In addition to presenting the research findings, this paper identifies key challenges and opportunities in the realm of LLMs. It underscores the necessity for further investigation in specific areas, including the paramount importance of addressing potential biases, transparency and explainability, data privacy and security, and responsible deployment of LLM technology. Findings The analysis revealed four distinct clusters of topics in LLM research: “language and NLP”, “education and teaching”, “clinical and medical applications” and “speech and recognition techniques”. Each cluster embodies a unique aspect of LLM application and showcases the breadth of possibilities that LLM technology has to offer. In addition to presenting the research findings, this paper identifies key challenges and opportunities in the realm of LLMs. It underscores the necessity for further investigation in specific areas, including the paramount importance of addressing potential biases, transparency and explainability, data privacy and security, and responsible deployment of LLM technology. Practical implications This classification offers practical guidance for researchers, developers, educators, and policymakers to focus efforts and resources. The study underscores the importance of addressing challenges in LLMs, including potential biases, transparency, data privacy, and responsible deployment. Policymakers can utilize this information to shape regulations, while developers can tailor technology development based on the diverse applications identified. The findings also emphasize the need for interdisciplinary collaboration and highlight ethical considerations, providing a roadmap for navigating the complex landscape of LLM research and applications. Originality/value This study stands out as the first to examine the evolution of LLMs across such a long time frame and across such diversified disciplines. It provides a unique perspective on the key areas of LLM research, highlighting the breadth and depth of LLM’s evolution.","['https://openalex.org/W4287687124', 'https://openalex.org/W4321606060', 'https://openalex.org/W4283716065', 'https://openalex.org/W2558405088', 'https://openalex.org/W4311991106', 'https://openalex.org/W3015202090', 'https://openalex.org/W4319746602', 'https://openalex.org/W2109664771', 'https://openalex.org/W3217791385', 'https://openalex.org/W3030163527', 'https://openalex.org/W4383605161', 'https://openalex.org/W4280554344', 'https://openalex.org/W3213382547', 'https://openalex.org/W2896457183', 'https://openalex.org/W1673310716', 'https://openalex.org/W4362679052', 'https://openalex.org/W2114508388', 'https://openalex.org/W4221142221', 'https://openalex.org/W4379651111', 'https://openalex.org/W2295124130', 'https://openalex.org/W2008496475', 'https://openalex.org/W4318464200', 'https://openalex.org/W2466110500', 'https://openalex.org/W3111180042', 'https://openalex.org/W4313451803', 'https://openalex.org/W4365517040', 'https://openalex.org/W4367186868', 'https://openalex.org/W4286233477', 'https://openalex.org/W3116286104', 'https://openalex.org/W4311426581', 'https://openalex.org/W4309685935', 'https://openalex.org/W2601243251', 'https://openalex.org/W2786672974', 'https://openalex.org/W4226278401', 'https://openalex.org/W4280492102', 'https://openalex.org/W4381827575', 'https://openalex.org/W4321479909', 'https://openalex.org/W2131774270', 'https://openalex.org/W2250379827', 'https://openalex.org/W4360980513', 'https://openalex.org/W4323348223', 'https://openalex.org/W932413789', 'https://openalex.org/W4283026156', 'https://openalex.org/W4221143046', 'https://openalex.org/W4360891295', 'https://openalex.org/W4317547647']",2023-12-18
https://openalex.org/W4392903389,https://doi.org/10.1109/icassp48485.2024.10447523,"FunCodec: A Fundamental, Reproducible and Integrable Open-Source Toolkit for Neural Speech Codec","This paper presents FunCodec, a fundamental neural speech codec toolkit, which is an extension of the open-source speech processing toolkit FunASR. FunCodec provides reproducible training recipes and inference scripts for the latest neural speech codec models, such as SoundStream and Encodec. Thanks to the unified design with FunASR, FunCodec can be easily integrated into downstream tasks, such as speech recognition. Along with FunCodec, pretrained models are also provided, which can be used for academic or generalized purposes. Based on the toolkit, we further propose the frequency-domain codec models, FreqCodec, which can achieve comparable speech quality with much lower computation and parameter complexity. Experimental results show that, under the same compression ratio, FunCodec can achieve better reconstruction quality compared with other toolkits and released models. We also demonstrate that the pre-trained models are suitable for downstream tasks, including automatic speech recognition and personalized text-to-speech synthesis. This toolkit is publicly available at https://github.com/alibaba-damo-academy/FunCodec.","['https://openalex.org/W6639363673', 'https://openalex.org/W1481955708', 'https://openalex.org/W6767111847', 'https://openalex.org/W3215615641', 'https://openalex.org/W6783867762', 'https://openalex.org/W4307323391', 'https://openalex.org/W3095095816', 'https://openalex.org/W3163243746', 'https://openalex.org/W2064675550', 'https://openalex.org/W4385245566', 'https://openalex.org/W4372190822', 'https://openalex.org/W4375869380', 'https://openalex.org/W4372348514', 'https://openalex.org/W4372260101', 'https://openalex.org/W4375869436', 'https://openalex.org/W6848735303', 'https://openalex.org/W6853611000', 'https://openalex.org/W6853998256', 'https://openalex.org/W4372270198', 'https://openalex.org/W6853515095', 'https://openalex.org/W2752796333', 'https://openalex.org/W4221159457', 'https://openalex.org/W2798405286', 'https://openalex.org/W6631362777', 'https://openalex.org/W3209059054', 'https://openalex.org/W2972359262', 'https://openalex.org/W1494198834', 'https://openalex.org/W2963242190', 'https://openalex.org/W6754473786', 'https://openalex.org/W3203407300', 'https://openalex.org/W3198694222', 'https://openalex.org/W1728888090', 'https://openalex.org/W2889048668', 'https://openalex.org/W4313679638', 'https://openalex.org/W4205788663', 'https://openalex.org/W2963799213', 'https://openalex.org/W2970006822', 'https://openalex.org/W1524333225', 'https://openalex.org/W4378501656', 'https://openalex.org/W4381827575', 'https://openalex.org/W3092028330', 'https://openalex.org/W4380551955']",2024-03-18
https://openalex.org/W4392931626,https://doi.org/10.1109/icassp48485.2024.10445874,Connecting Speech Encoder and Large Language Model for ASR,"The impressive capability and versatility of large language models (LLMs) have aroused increasing attention in automatic speech recognition (ASR), with several pioneering studies attempting to build integrated ASR models by connecting a speech encoder with an LLM. This paper presents a comparative study of three commonly used structures as connectors, including fully connected layers, multi-head cross-attention, and Q-Former. Speech encoders from the Whisper model series as well as LLMs from the Vicuna model series with different model sizes were studied. Experiments were performed on the commonly used LibriSpeech, Common Voice, and GigaSpeech datasets, where the LLMs with Q-Formers demonstrated consistent and considerable word error rate (WER) reductions over LLMs with other connector structures. Q-Former-based LLMs can generalise well to out-of-domain datasets, where 12% relative WER reductions over the Whisper baseline ASR model were achieved on the Eval2000 test set without using any in-domain training data from Switchboard. Moreover, a novel segment-level Q-Former is proposed to enable LLMs to recognise speech segments with a duration exceeding the limitation of the encoders, which results in 17% relative WER reductions over other connector structures on 90-second-long speech data.","['https://openalex.org/W6778883912', 'https://openalex.org/W6852800892', 'https://openalex.org/W6850625674', 'https://openalex.org/W6794212170', 'https://openalex.org/W4393178509', 'https://openalex.org/W6851513886', 'https://openalex.org/W4389524500', 'https://openalex.org/W6853998256', 'https://openalex.org/W6852326057', 'https://openalex.org/W6855801281', 'https://openalex.org/W4391021666', 'https://openalex.org/W6855414158', 'https://openalex.org/W4391021457', 'https://openalex.org/W6854445763', 'https://openalex.org/W6855793536', 'https://openalex.org/W6847363464', 'https://openalex.org/W6853094705', 'https://openalex.org/W6849177959', 'https://openalex.org/W3209059054', 'https://openalex.org/W4226033575', 'https://openalex.org/W6850218400', 'https://openalex.org/W6810334672', 'https://openalex.org/W6851950068', 'https://openalex.org/W6853116092', 'https://openalex.org/W4389519587', 'https://openalex.org/W6852447913', 'https://openalex.org/W6853089504', 'https://openalex.org/W6853515732', 'https://openalex.org/W6853249747', 'https://openalex.org/W4392909390', 'https://openalex.org/W1494198834', 'https://openalex.org/W6771467084', 'https://openalex.org/W3198694222', 'https://openalex.org/W4385807435', 'https://openalex.org/W4386351448', 'https://openalex.org/W4402671548', 'https://openalex.org/W4366850747', 'https://openalex.org/W4378711593', 'https://openalex.org/W4375958083', 'https://openalex.org/W4376312115', 'https://openalex.org/W4323066695', 'https://openalex.org/W4380994269', 'https://openalex.org/W4378174011', 'https://openalex.org/W4361866031', 'https://openalex.org/W4383989131', 'https://openalex.org/W4377130946', 'https://openalex.org/W4392908891', 'https://openalex.org/W4392903956', 'https://openalex.org/W4322718191', 'https://openalex.org/W4381827575']",2024-03-18
https://openalex.org/W4392903288,https://doi.org/10.1109/icassp48485.2024.10446898,End-to-End Speech Recognition Contextualization with Large Language Models,"In recent years, Large Language Models (LLMs) have garnered significant attention from the research community due to their exceptional performance and generalization capabilities. In this paper, we introduce a novel method for contextualizing speech recognition models incorporating LLMs. Our approach casts speech recognition as a mixed-modal language modeling task based on a pretrained LLM. We use audio features, along with optional text tokens for context, to train the system to complete transcriptions in a decoder-only fashion. As a result, the system implicitly learns how to leverage unstructured contextual information during training. Our empirical results demonstrate a significant improvement in performance, with a 6% WER reduction when additional textual context is provided. Moreover, we find that our method performs competitively, improving by 7.5% WER overall and 17% WER on rare words, compared to a baseline contextualized RNN-T system that has been trained on a speech dataset more than twenty-five times larger. Overall, we demonstrate that by adding only a handful of trainable parameters via adapters, we can unlock the contextualized speech recognition capability of the pretrained LLM while maintaining the same text-only input functionality.","['https://openalex.org/W6850625674', 'https://openalex.org/W3097794466', 'https://openalex.org/W3198004110', 'https://openalex.org/W4224918838', 'https://openalex.org/W2886319145', 'https://openalex.org/W6811088048', 'https://openalex.org/W2972625221', 'https://openalex.org/W3140235797', 'https://openalex.org/W4372259757', 'https://openalex.org/W4385823371', 'https://openalex.org/W4391021666', 'https://openalex.org/W6853998256', 'https://openalex.org/W4392903956', 'https://openalex.org/W6847363464', 'https://openalex.org/W3097777922', 'https://openalex.org/W4388979610', 'https://openalex.org/W2127141656', 'https://openalex.org/W6796581206', 'https://openalex.org/W2936774411', 'https://openalex.org/W2933138175', 'https://openalex.org/W2407080277', 'https://openalex.org/W3015486229', 'https://openalex.org/W6639156005', 'https://openalex.org/W6847420240', 'https://openalex.org/W6854054124', 'https://openalex.org/W3213673948', 'https://openalex.org/W4383473945', 'https://openalex.org/W4322718191', 'https://openalex.org/W1855892484', 'https://openalex.org/W4381827575']",2024-03-18
https://openalex.org/W4391021623,https://doi.org/10.1109/asru57964.2023.10389703,SLM: Bridge the Thin Gap Between Speech and Text Foundation Models,"We present a joint Speech and Language Model (SLM), a multitask, multilingual, and dual-modal model that takes advantage of pretrained foundational speech and language models. SLM freezes the pretrained foundation models to maximally preserves their capabilities, and only trains a simple adapter with just 1% (156M) of the foundation models' parameters. This adaptation not only leads SLM to achieve strong performance on conventional tasks such as automatic speech recognition (ASR) and automatic speech translation (AST), but also unlocks the novel capability of zero-shot instruction-following for more diverse tasks. Given a speech input and a text instruction, SLM is able to perform unseen generation tasks including contextual biasing ASR using real-time context, dialog generation, speech continuation, and question answering. Our approach demonstrates that the representational gap between pretrained speech and language models is narrower than one would expect, and can be bridged by a simple adaptation mechanism. As a result, SLM is not only efficient to train, but also inherits strong capabilities already present in foundation models of different modalities.","['https://openalex.org/W6852800892', 'https://openalex.org/W6850218400', 'https://openalex.org/W6847363464', 'https://openalex.org/W6759579507', 'https://openalex.org/W4385822641', 'https://openalex.org/W4389524500', 'https://openalex.org/W3209059054', 'https://openalex.org/W6853998256', 'https://openalex.org/W4381786045', 'https://openalex.org/W6852818750', 'https://openalex.org/W4386071707', 'https://openalex.org/W6853249747', 'https://openalex.org/W4385822589', 'https://openalex.org/W3196509775', 'https://openalex.org/W3197324626', 'https://openalex.org/W6784545093', 'https://openalex.org/W6810673746', 'https://openalex.org/W6769627184', 'https://openalex.org/W6845943790', 'https://openalex.org/W6784577980', 'https://openalex.org/W6791904447', 'https://openalex.org/W3119308075', 'https://openalex.org/W4319862635', 'https://openalex.org/W38194800', 'https://openalex.org/W2101105183', 'https://openalex.org/W2963532001', 'https://openalex.org/W4319862232', 'https://openalex.org/W6810259195', 'https://openalex.org/W4226120743', 'https://openalex.org/W6847568899', 'https://openalex.org/W2912924812', 'https://openalex.org/W6796581206', 'https://openalex.org/W6810334672', 'https://openalex.org/W4385573964', 'https://openalex.org/W6847076894', 'https://openalex.org/W100623710', 'https://openalex.org/W6755207826', 'https://openalex.org/W3168867926', 'https://openalex.org/W3091928890', 'https://openalex.org/W4307079201', 'https://openalex.org/W4377372369', 'https://openalex.org/W3139918052', 'https://openalex.org/W4288089799', 'https://openalex.org/W2896457183', 'https://openalex.org/W4323066695', 'https://openalex.org/W4385571124', 'https://openalex.org/W3169483174', 'https://openalex.org/W4221155340', 'https://openalex.org/W4377130946', 'https://openalex.org/W4381827575']",2023-12-16
https://openalex.org/W4392902623,https://doi.org/10.1109/icassp48485.2024.10448257,"Dynamic-Superb: Towards a Dynamic, Collaborative, and Comprehensive Instruction-Tuning Benchmark For Speech","Text language models have shown remarkable zero-shot capability in generalizing to unseen tasks when provided with well-formulated instructions. However, existing studies in speech processing primarily focus on limited or specific tasks. Moreover, the lack of standardized benchmarks hinders a fair comparison across different approaches. Thus, we present Dynamic-SUPERB, a benchmark designed for building universal speech models capable of leveraging instruction tuning to perform multiple tasks in a zero-shot fashion. To achieve comprehensive coverage of diverse speech tasks and harness instruction tuning, we invite the community to collaborate and contribute, facilitating the dynamic growth of the benchmark. To initiate, Dynamic-SUPERB features 55 evaluation instances by combining 33 tasks and 22 datasets. This spans a broad spectrum of dimensions, providing a comprehensive platform for evaluation. Additionally, we propose several approaches to establish benchmark baselines. These include the utilization of speech models, text language models, and the multimodal encoder. Evaluation results indicate that while these baselines perform reasonably on seen tasks, they struggle with unseen ones. We release all materials to the public and welcome researchers to collaborate on the project, advancing technologies in the field together <sup xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">1</sup> .","['https://openalex.org/W2842511635', 'https://openalex.org/W6780218876', 'https://openalex.org/W3209059054', 'https://openalex.org/W4281492411', 'https://openalex.org/W4226162428', 'https://openalex.org/W6850477478', 'https://openalex.org/W6852859116', 'https://openalex.org/W3174770825', 'https://openalex.org/W4285247752', 'https://openalex.org/W4386187806', 'https://openalex.org/W4205991051', 'https://openalex.org/W3098267758', 'https://openalex.org/W4385822890', 'https://openalex.org/W4319862642', 'https://openalex.org/W6853249747', 'https://openalex.org/W4225410153', 'https://openalex.org/W3176693010', 'https://openalex.org/W6800875267', 'https://openalex.org/W6853998256', 'https://openalex.org/W4393178509', 'https://openalex.org/W6786696081', 'https://openalex.org/W3197580070', 'https://openalex.org/W4226103796', 'https://openalex.org/W3189296823', 'https://openalex.org/W6846004400', 'https://openalex.org/W4319862635', 'https://openalex.org/W4385822439', 'https://openalex.org/W4319862652', 'https://openalex.org/W6755207826', 'https://openalex.org/W6790356757', 'https://openalex.org/W6847363464', 'https://openalex.org/W4386071707', 'https://openalex.org/W6850625674', 'https://openalex.org/W6854866820', 'https://openalex.org/W1494198834', 'https://openalex.org/W6917585676', 'https://openalex.org/W6772349387', 'https://openalex.org/W4385807453', 'https://openalex.org/W6855594702', 'https://openalex.org/W6852489829', 'https://openalex.org/W6851948999', 'https://openalex.org/W4385823351', 'https://openalex.org/W4297808394', 'https://openalex.org/W4384918448', 'https://openalex.org/W4379539302', 'https://openalex.org/W4322718191', 'https://openalex.org/W4381827575', 'https://openalex.org/W4377130946', 'https://openalex.org/W4386555501', 'https://openalex.org/W4361229539', 'https://openalex.org/W4322825254', 'https://openalex.org/W3036601975', 'https://openalex.org/W4367628410', 'https://openalex.org/W4307322847', 'https://openalex.org/W2998572311', 'https://openalex.org/W4394671563']",2024-03-18
https://openalex.org/W4392909068,https://doi.org/10.1109/icassp48485.2024.10447929,"Exploring Speech Recognition, Translation, and Understanding with Discrete Speech Units: A Comparative Study","Speech signals, typically sampled at rates in the tens of thousands per second, contain redundancies, evoking inefficiencies in sequence modeling. High-dimensional speech features such as spectrograms are often used as the input for the subsequent model. However, they can still be redundant. Recent investigations proposed the use of discrete speech units derived from self-supervised learning representations, which significantly compresses the size of speech data. Applying various methods, such as de-duplication and subword modeling, can further compress the speech sequence length. Hence, training time is significantly reduced while retaining notable performance. In this study, we undertake a comprehensive and systematic exploration into the application of discrete units within end-to-end speech processing models. Experiments on 12 automatic speech recognition, 3 speech translation, and 1 spoken language understanding corpora demonstrate that discrete units achieve reasonably good results in almost all the settings. Our configurations and trained models are released in ESPnet to foster future research efforts.","['https://openalex.org/W2160815625', 'https://openalex.org/W2515753980', 'https://openalex.org/W2127141656', 'https://openalex.org/W2144499799', 'https://openalex.org/W6623517193', 'https://openalex.org/W4385245566', 'https://openalex.org/W3097777922', 'https://openalex.org/W3163793923', 'https://openalex.org/W4319862255', 'https://openalex.org/W2973049979', 'https://openalex.org/W6769196770', 'https://openalex.org/W6780218876', 'https://openalex.org/W3209059054', 'https://openalex.org/W3209984917', 'https://openalex.org/W6847363464', 'https://openalex.org/W2398826216', 'https://openalex.org/W4385822683', 'https://openalex.org/W3015356564', 'https://openalex.org/W4385570101', 'https://openalex.org/W4381786045', 'https://openalex.org/W4375869259', 'https://openalex.org/W2962780374', 'https://openalex.org/W2752796333', 'https://openalex.org/W3215615641', 'https://openalex.org/W1494198834', 'https://openalex.org/W2166637769', 'https://openalex.org/W3198694222', 'https://openalex.org/W2799473636', 'https://openalex.org/W6755559483', 'https://openalex.org/W3198587774', 'https://openalex.org/W6898634591', 'https://openalex.org/W2963242190', 'https://openalex.org/W6771467084', 'https://openalex.org/W4385822439', 'https://openalex.org/W4391021675', 'https://openalex.org/W4386566728', 'https://openalex.org/W4385565440', 'https://openalex.org/W3100460087', 'https://openalex.org/W2899274165', 'https://openalex.org/W3024605872', 'https://openalex.org/W4313679638', 'https://openalex.org/W854541894', 'https://openalex.org/W2963799213', 'https://openalex.org/W3036601975', 'https://openalex.org/W4307323391', 'https://openalex.org/W4381827575', 'https://openalex.org/W4200635400', 'https://openalex.org/W4385970143', 'https://openalex.org/W3101648800']",2024-03-18
https://openalex.org/W4392903872,https://doi.org/10.1109/icassp48485.2024.10447553,SALM: Speech-Augmented Language Model with in-Context Learning for Speech Recognition and Translation,"We present a novel Speech Augmented Language Model (SALM) with multitask and in-context learning capabilities. SALM comprises a frozen text LLM, a audio encoder, a modality adapter module, and LoRA layers to accommodate speech input and associated task instructions. The unified SALM not only achieves performance on par with task-specific Conformer baselines for Automatic Speech Recognition (ASR) and Speech Translation (AST), but also exhibits zero-shot in-context learning capabilities, demonstrated through keyword-boosting task for ASR and AST. Moreover, speech supervised in-context training is proposed to bridge the gap between LLM training and downstream speech tasks, which further boosts the in-context learning ability of speech-to-text models. Proposed model is open-sourced via NeMo toolkit <sup xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">1</sup> .","['https://openalex.org/W4393178509', 'https://openalex.org/W6852326057', 'https://openalex.org/W4372269772', 'https://openalex.org/W4385822949', 'https://openalex.org/W4389524500', 'https://openalex.org/W6853998256', 'https://openalex.org/W6853249747', 'https://openalex.org/W4392910583', 'https://openalex.org/W4391021666', 'https://openalex.org/W6855414158', 'https://openalex.org/W6767997687', 'https://openalex.org/W6767671539', 'https://openalex.org/W4385822953', 'https://openalex.org/W6848735303', 'https://openalex.org/W4252812408', 'https://openalex.org/W4391021773', 'https://openalex.org/W2889012072', 'https://openalex.org/W2886319145', 'https://openalex.org/W4225308107', 'https://openalex.org/W4226120743', 'https://openalex.org/W4392979802', 'https://openalex.org/W4391021542', 'https://openalex.org/W3097777922', 'https://openalex.org/W6796581206', 'https://openalex.org/W6800875267', 'https://openalex.org/W6846665566', 'https://openalex.org/W6761551260', 'https://openalex.org/W1494198834', 'https://openalex.org/W4385570170', 'https://openalex.org/W3092085609', 'https://openalex.org/W3005302685', 'https://openalex.org/W6857054612', 'https://openalex.org/W4387595589', 'https://openalex.org/W2963499882', 'https://openalex.org/W4387891768', 'https://openalex.org/W4375958083', 'https://openalex.org/W2938704169', 'https://openalex.org/W4392903956', 'https://openalex.org/W4381827575', 'https://openalex.org/W2974231335', 'https://openalex.org/W4313679638', 'https://openalex.org/W3168867926', 'https://openalex.org/W4310428868', 'https://openalex.org/W2973727699', 'https://openalex.org/W4377130946']",2024-03-18
https://openalex.org/W4392902647,https://doi.org/10.1109/icassp48485.2024.10446343,Extending Large Language Models for Speech and Audio Captioning,"Multimodal large language models (LLMs) have shown promising visual perception abilities by connecting with image encoders, but their performance on auditory tasks has not yet been widely investigated. Meanwhile, automatic speech recognition (ASR) and automatic audio captioning (AAC) are often achieved with separate systems, resulting in incomplete auditory perception abilities. To fill in these gaps, in this paper, we present the first study that achieves both ASR and AAC by connecting an LLM with auditory encoders. A dual auditory encoder structure is proposed, integrating the Whisper encoder for speech and the BEATs encoder for audio events with a high temporal resolution by using a Q-Former at the window level. Experiments for ASR and AAC are performed correspondingly on the widely used LibriSpeech, GigaSpeech, WavCaps, AudioCaps, and Clotho datasets and yield promising results. In particular, state-of-the-art results are achieved on GigaSpeech, AudioCaps and Clotho. Our model is also able to caption speech and audio events simultaneously from clips with mixed speech and background audio events, which is a step towards more complete machine auditory perception.","['https://openalex.org/W6850625674', 'https://openalex.org/W6810334672', 'https://openalex.org/W6849177959', 'https://openalex.org/W6853116092', 'https://openalex.org/W4392903956', 'https://openalex.org/W4391021666', 'https://openalex.org/W6852326057', 'https://openalex.org/W4389524500', 'https://openalex.org/W6853998256', 'https://openalex.org/W6853249747', 'https://openalex.org/W4389519587', 'https://openalex.org/W6852447913', 'https://openalex.org/W6853094705', 'https://openalex.org/W6854596735', 'https://openalex.org/W6855952517', 'https://openalex.org/W4224932123', 'https://openalex.org/W6847363464', 'https://openalex.org/W6848208918', 'https://openalex.org/W6846004400', 'https://openalex.org/W2949376505', 'https://openalex.org/W6851513886', 'https://openalex.org/W6851847159', 'https://openalex.org/W6796581206', 'https://openalex.org/W1494198834', 'https://openalex.org/W3198694222', 'https://openalex.org/W4400033239', 'https://openalex.org/W3015591594', 'https://openalex.org/W4377130946', 'https://openalex.org/W4375958083', 'https://openalex.org/W4378711593', 'https://openalex.org/W3103022576', 'https://openalex.org/W4380994269', 'https://openalex.org/W4322718191', 'https://openalex.org/W4393178509', 'https://openalex.org/W4392909390', 'https://openalex.org/W4384648427', 'https://openalex.org/W4381827575', 'https://openalex.org/W4376312115', 'https://openalex.org/W4361866031', 'https://openalex.org/W4307322847']",2024-03-18
https://openalex.org/W4392903062,https://doi.org/10.1109/icassp48485.2024.10448426,Translatotron 3: Speech to Speech Translation with Monolingual Data,"This paper presents Translatotron 3, a novel approach to unsupervised direct speech-to-speech translation from monolingual speech-text datasets by combining masked autoencoder, unsupervised embedding mapping, and back-translation. Experimental results in speech-to-speech translation tasks between Spanish and English show that Translatotron 3 outperforms a baseline cascade system, reporting 18.14 BLEU points improvement on the synthesized Unpaired-Conversational dataset. In contrast to supervised approaches that necessitate real paired data, or specialized modeling to replicate para-/non-linguistic information such as pauses, speaking rates, and speaker identity, Translatotron 3 showcases its capability to retain it.","['https://openalex.org/W6841035593', 'https://openalex.org/W2097203679', 'https://openalex.org/W2136545725', 'https://openalex.org/W2972495969', 'https://openalex.org/W3180374548', 'https://openalex.org/W2963216553', 'https://openalex.org/W6745740328', 'https://openalex.org/W6745388339', 'https://openalex.org/W4313156423', 'https://openalex.org/W2936774411', 'https://openalex.org/W6744957266', 'https://openalex.org/W3175871055', 'https://openalex.org/W4287854499', 'https://openalex.org/W6838789019', 'https://openalex.org/W4223622550', 'https://openalex.org/W4280601369', 'https://openalex.org/W3142316150', 'https://openalex.org/W4385893869', 'https://openalex.org/W6847247304', 'https://openalex.org/W6845958605', 'https://openalex.org/W3215615641', 'https://openalex.org/W4226033575', 'https://openalex.org/W4307323391', 'https://openalex.org/W6780218876', 'https://openalex.org/W3209059054', 'https://openalex.org/W3007068036', 'https://openalex.org/W6853998256', 'https://openalex.org/W4372349107', 'https://openalex.org/W6855650468', 'https://openalex.org/W4372191700', 'https://openalex.org/W4385570101', 'https://openalex.org/W3205644108', 'https://openalex.org/W4285112515', 'https://openalex.org/W4392979802', 'https://openalex.org/W2741602058', 'https://openalex.org/W2964266061', 'https://openalex.org/W2899134946', 'https://openalex.org/W2962793481', 'https://openalex.org/W2964161387', 'https://openalex.org/W6771467084', 'https://openalex.org/W4319862245', 'https://openalex.org/W6765510844', 'https://openalex.org/W6846600677', 'https://openalex.org/W3197324626', 'https://openalex.org/W6784545093', 'https://openalex.org/W6748409065', 'https://openalex.org/W6810701745', 'https://openalex.org/W3196509775', 'https://openalex.org/W4385970143', 'https://openalex.org/W4226444650', 'https://openalex.org/W4381827575', 'https://openalex.org/W4307783813', 'https://openalex.org/W3091928890', 'https://openalex.org/W4299579390', 'https://openalex.org/W3036601975', 'https://openalex.org/W4298393544', 'https://openalex.org/W4281789500', 'https://openalex.org/W4385570550']",2024-03-18
https://openalex.org/W4406385636,https://doi.org/10.1038/s41586-024-08359-z,Joint speech and text machine translation for up to 100 languages,"Creating the Babel Fish, a tool that helps individuals translate speech between any two languages, requires advanced technological innovation and linguistic expertise. Although conventional speech-to-speech translation systems composed of multiple subsystems performing translation in a cascaded fashion exist<sup>1-3</sup>, scalable and high-performing unified systems<sup>4,5</sup> remain underexplored. To address this gap, here we introduce SEAMLESSM4T-Massively Multilingual and Multimodal Machine Translation-a single model that supports speech-to-speech translation (101 to 36 languages), speech-to-text translation (from 101 to 96 languages), text-to-speech translation (from 96 to 36 languages), text-to-text translation (96 languages) and automatic speech recognition (96 languages). Built using a new multimodal corpus of automatically aligned speech translations and other publicly available data, SEAMLESSM4T is one of the first multilingual systems that can translate from and into English for both speech and text. Moreover, it outperforms the existing state-of-the-art cascaded systems, achieving up to 8% and 23% higher BLEU (Bilingual Evaluation Understudy) scores in speech-to-text and speech-to-speech tasks, respectively. Beyond quality, when tested for robustness, our system is, on average, approximately 50% more resilient against background noise and speaker variations in speech-to-text tasks than the previous state-of-the-art systems. We evaluated SEAMLESSM4T on added toxicity and gender bias to assess translation safety. For the former, we included two strategies for added toxicity mitigation working at either training or inference time. Finally, all contributions in this work are publicly available for non-commercial use to propel further research on inclusive speech translation technologies.","['https://openalex.org/W2097203679', 'https://openalex.org/W1538023239', 'https://openalex.org/W2136545725', 'https://openalex.org/W2936184970', 'https://openalex.org/W2972495969', 'https://openalex.org/W3180374548', 'https://openalex.org/W6798080464', 'https://openalex.org/W4399365598', 'https://openalex.org/W6855524384', 'https://openalex.org/W2250342921', 'https://openalex.org/W2101105183', 'https://openalex.org/W3024869864', 'https://openalex.org/W3139878283', 'https://openalex.org/W4385569960', 'https://openalex.org/W4385574194', 'https://openalex.org/W3039695075', 'https://openalex.org/W4281621399', 'https://openalex.org/W4319862635', 'https://openalex.org/W4323651091', 'https://openalex.org/W4311000453', 'https://openalex.org/W4381827575', 'https://openalex.org/W4200631896', 'https://openalex.org/W6852909395', 'https://openalex.org/W4404781204', 'https://openalex.org/W4309129001', 'https://openalex.org/W4385571229', 'https://openalex.org/W4280617721', 'https://openalex.org/W4303648927', 'https://openalex.org/W4389518873', 'https://openalex.org/W4297841625', 'https://openalex.org/W4390810311', 'https://openalex.org/W4402670286', 'https://openalex.org/W2952328691', 'https://openalex.org/W3197577761', 'https://openalex.org/W4283809028', 'https://openalex.org/W3035070478', 'https://openalex.org/W4377865270', 'https://openalex.org/W4389524081', 'https://openalex.org/W4403431383', 'https://openalex.org/W3012624518', 'https://openalex.org/W4312391725', 'https://openalex.org/W4394778654', 'https://openalex.org/W2032374598', 'https://openalex.org/W4285131748', 'https://openalex.org/W4307416501', 'https://openalex.org/W3196509775', 'https://openalex.org/W3127012371', 'https://openalex.org/W3197771105', 'https://openalex.org/W4323066695', 'https://openalex.org/W2798389157', 'https://openalex.org/W2962735107', 'https://openalex.org/W2973088264', 'https://openalex.org/W4308756394', 'https://openalex.org/W4385572318', 'https://openalex.org/W4286359908', 'https://openalex.org/W6861295083', 'https://openalex.org/W4311731008', 'https://openalex.org/W4385570550', 'https://openalex.org/W3007068036', 'https://openalex.org/W3213029956', 'https://openalex.org/W4384648564', 'https://openalex.org/W4226033575', 'https://openalex.org/W3025165719', 'https://openalex.org/W3097777922', 'https://openalex.org/W2963250244', 'https://openalex.org/W2962784628', 'https://openalex.org/W4385573923', 'https://openalex.org/W2899716505', 'https://openalex.org/W2964161387', 'https://openalex.org/W3096490862', 'https://openalex.org/W6739901393', 'https://openalex.org/W4283834483', 'https://openalex.org/W6778823374', 'https://openalex.org/W3194000401', 'https://openalex.org/W4311994673', 'https://openalex.org/W4385569956', 'https://openalex.org/W4385574250']",2025-01-15
https://openalex.org/W4392904154,https://doi.org/10.1109/icassp48485.2024.10446998,Discrete Audio Representation as an Alternative to Mel-Spectrograms for Speaker and Speech Recognition,"Discrete audio representation, aka audio tokenization, has seen renewed interest driven by its potential to facilitate the application of text language modeling approaches in audio domain. To this end, various compression and representation-learning based tokenization schemes have been proposed. However, there is limited investigation into the performance of compression-based audio tokens compared to well-established mel-spectrogram features across various speaker and speech related tasks. In this paper, we evaluate compression based audio tokens on three tasks: Speaker Verification, Diarization and (Multi-lingual) Speech Recognition. Our findings indicate that (i) the models trained on audio tokens perform competitively, on average within 1% of mel-spectrogram features for all the tasks considered, and do not surpass them yet. (ii) these models exhibit robustness for out-of-domain narrowband data, particularly in speaker tasks. (iii) audio tokens allow for compression to 20x compared to mel-spectrogram features with minimal loss of performance in speech and speaker related tasks, which is crucial for low bit-rate applications, and (iv) the examined Residual Vector Quantization (RVQ) based audio tokenizer exhibits a low-pass frequency response characteristic, offering a plausible explanation for the observed results, and providing insight for future tokenizer designs.","['https://openalex.org/W6778883912', 'https://openalex.org/W6854866820', 'https://openalex.org/W6847363464', 'https://openalex.org/W3209059054', 'https://openalex.org/W4226033575', 'https://openalex.org/W3209984917', 'https://openalex.org/W6850218400', 'https://openalex.org/W6848735303', 'https://openalex.org/W6850334629', 'https://openalex.org/W6853611000', 'https://openalex.org/W6849105126', 'https://openalex.org/W4381786045', 'https://openalex.org/W6853998256', 'https://openalex.org/W3215615641', 'https://openalex.org/W4307323391', 'https://openalex.org/W6853515095', 'https://openalex.org/W3205878676', 'https://openalex.org/W4391021542', 'https://openalex.org/W2969985801', 'https://openalex.org/W3097777922', 'https://openalex.org/W2963250244', 'https://openalex.org/W2808631503', 'https://openalex.org/W1494198834', 'https://openalex.org/W3095410713', 'https://openalex.org/W6678809451', 'https://openalex.org/W6770434673', 'https://openalex.org/W6711908931', 'https://openalex.org/W6771467084', 'https://openalex.org/W3119308075', 'https://openalex.org/W2936774411', 'https://openalex.org/W4381827575', 'https://openalex.org/W4378501656', 'https://openalex.org/W3030437843', 'https://openalex.org/W4313679638', 'https://openalex.org/W4320013820', 'https://openalex.org/W4323066695', 'https://openalex.org/W4311000453', 'https://openalex.org/W4323651091', 'https://openalex.org/W2622566932', 'https://openalex.org/W4292779060', 'https://openalex.org/W4318351475', 'https://openalex.org/W4380551955', 'https://openalex.org/W4384918448']",2024-03-18
https://openalex.org/W4392931281,https://doi.org/10.1109/icassp48485.2024.10446933,Paralinguistics-Enhanced Large Language Modeling of Spoken Dialogue,"Large Language Models (LLMs) have demonstrated superior abilities in tasks such as chatting, reasoning, and question-answering. However, standard LLMs may ignore crucial paralinguistic information, such as sentiment, emotion, and speaking style, which are essential for achieving natural, human-like spoken conversation, especially when such information is conveyed by acoustic cues. We therefore propose Paralinguistics-enhanced Generative Pretrained Transformer (ParalinGPT), an LLM that utilizes text and speech modalities to better model the linguistic content and paralinguistic attributes of spoken dialogue. The model takes the conversational context of text, speech embeddings, and paralinguistic attributes as input prompts within a serialized multitasking multimodal framework. Specifically, our framework serializes tasks in the order of current paralinguistic attribute prediction, response paralinguistic attribute prediction, and response text generation with autoregressive conditioning. We utilize the Switchboard-1 corpus, including its sentiment labels as the paralinguistic attribute, as our spoken dialogue dataset. Experimental results indicate the proposed serialized multitasking method outperforms typical sequence classification techniques on current and response sentiment classification. Furthermore, leveraging conversational context and speech embeddings significantly improves both response text generation and sentiment prediction. Our proposed framework achieves relative improvements of 6.7%, 12.0%, and 3.5% in current sentiment accuracy, response sentiment accuracy, and response text BLEU score, respectively.","['https://openalex.org/W6851678396', 'https://openalex.org/W6763494226', 'https://openalex.org/W2766833041', 'https://openalex.org/W4293714393', 'https://openalex.org/W2952088495', 'https://openalex.org/W3048664667', 'https://openalex.org/W3153025627', 'https://openalex.org/W3174716116', 'https://openalex.org/W2128970689', 'https://openalex.org/W2151083697', 'https://openalex.org/W2897636448', 'https://openalex.org/W6804073865', 'https://openalex.org/W4285273040', 'https://openalex.org/W2936162287', 'https://openalex.org/W6791303579', 'https://openalex.org/W2548264631', 'https://openalex.org/W2586286573', 'https://openalex.org/W4319862652', 'https://openalex.org/W2968228919', 'https://openalex.org/W3095607145', 'https://openalex.org/W4385823311', 'https://openalex.org/W6790356757', 'https://openalex.org/W3198217962', 'https://openalex.org/W4307680525', 'https://openalex.org/W4381786045', 'https://openalex.org/W6853998256', 'https://openalex.org/W4389524500', 'https://openalex.org/W3015489952', 'https://openalex.org/W3197236059', 'https://openalex.org/W6780218876', 'https://openalex.org/W3198771897', 'https://openalex.org/W4319862479', 'https://openalex.org/W2166637769', 'https://openalex.org/W6778490555', 'https://openalex.org/W4381827575', 'https://openalex.org/W3036601975', 'https://openalex.org/W4394671563', 'https://openalex.org/W3133903425', 'https://openalex.org/W4366559955', 'https://openalex.org/W3193377986', 'https://openalex.org/W4226422436']",2024-03-18
https://openalex.org/W4392910583,https://doi.org/10.1109/icassp48485.2024.10448204,Adapting Large Language Model with Speech for Fully Formatted End-to-End Speech Recognition,"Most end-to-end (E2E) speech recognition models are composed of encoder and decoder blocks that perform acoustic and language modeling functions. Pretrained large language models (LLMs) have the potential to improve the performance of E2E ASR. However, integrating a pretrained language model into an E2E speech recognition model has shown limited benefits due to the mismatches between text-based LLMs and those used in E2E ASR. In this paper, we explore an alternative approach by adapting a pretrained LLMs to speech. Our experiments on fully-formatted E2E ASR transcription tasks across various domains demonstrate that our approach can effectively leverage the strengths of pretrained LLMs to produce more readable ASR transcriptions. Our model, which is based on the pretrained large language models with either an encoder-decoder or decoder-only structure, surpasses strong ASR models such as Whisper <sup xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">1</sup> , in terms of recognition error rate, considering formats like punctuation and capitalization as well.","['https://openalex.org/W3198587774', 'https://openalex.org/W6847363464', 'https://openalex.org/W2577366047', 'https://openalex.org/W4372259777', 'https://openalex.org/W2888779557', 'https://openalex.org/W2963362078', 'https://openalex.org/W6640059789', 'https://openalex.org/W3162037819', 'https://openalex.org/W4223622550', 'https://openalex.org/W6803092890', 'https://openalex.org/W4226120743', 'https://openalex.org/W6850218400', 'https://openalex.org/W3173767661', 'https://openalex.org/W6853998256', 'https://openalex.org/W6846004400', 'https://openalex.org/W4292595872', 'https://openalex.org/W2970076840', 'https://openalex.org/W6778883912', 'https://openalex.org/W6850625674', 'https://openalex.org/W6796581206', 'https://openalex.org/W3198643121', 'https://openalex.org/W4372267461', 'https://openalex.org/W4385569799', 'https://openalex.org/W6779068807', 'https://openalex.org/W3168867926', 'https://openalex.org/W3122890974', 'https://openalex.org/W4292779060', 'https://openalex.org/W4307322847', 'https://openalex.org/W3207222250', 'https://openalex.org/W4323066695', 'https://openalex.org/W1915251500', 'https://openalex.org/W4322718191', 'https://openalex.org/W4381827575', 'https://openalex.org/W4311000453']",2024-03-18
https://openalex.org/W4392902656,https://doi.org/10.1109/icassp48485.2024.10447296,Loss Masking Is Not Needed In Decoder-Only Transformer For Discrete-Token-Based ASR,"Recently, unified speech-text models, such as SpeechGPT, VioLA, and AudioPaLM, have achieved remarkable performance on various speech tasks. These models discretize speech signals into tokens (speech discretization) and use a shared vocabulary for both text and speech tokens. Then they train a single decoder-only Transformer on a mixture of speech tasks. However, these models rely on the Loss Masking strategy for the ASR task, which ignores the dependency among speech tokens. In this paper, we propose to model speech tokens in an autoregressive way, similar to text. We find that applying the conventional cross-entropy loss on input speech tokens does not consistently improve the ASR performance over the Loss Masking approach. To address this issue, we propose a novel approach denoted Smoothed Label Distillation (SLD), which applies a KL divergence loss with smoothed labels on speech tokens. Our experiments show that SLD effectively models speech tokens and outperforms Loss Masking for decoder-only Transformers in ASR tasks with different speech discretization methods <sup xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">1</sup> .","['https://openalex.org/W6852800892', 'https://openalex.org/W6850625674', 'https://openalex.org/W4389524500', 'https://openalex.org/W6853611000', 'https://openalex.org/W6853998256', 'https://openalex.org/W6857968694', 'https://openalex.org/W6739901393', 'https://openalex.org/W6769196770', 'https://openalex.org/W6770514103', 'https://openalex.org/W4385822683', 'https://openalex.org/W4319862255', 'https://openalex.org/W3209984917', 'https://openalex.org/W3209059054', 'https://openalex.org/W4307323391', 'https://openalex.org/W6850218400', 'https://openalex.org/W2963979492', 'https://openalex.org/W2962784628', 'https://openalex.org/W2113158412', 'https://openalex.org/W6638523607', 'https://openalex.org/W2183341477', 'https://openalex.org/W1494198834', 'https://openalex.org/W2407080277', 'https://openalex.org/W4323066695', 'https://openalex.org/W4381827575', 'https://openalex.org/W2747329762', 'https://openalex.org/W4387595589', 'https://openalex.org/W4385245566', 'https://openalex.org/W4378501656', 'https://openalex.org/W1821462560', 'https://openalex.org/W4322718191', 'https://openalex.org/W2988736778']",2024-03-18
https://openalex.org/W4392902778,https://doi.org/10.1109/icassp48485.2024.10447926,Cross-Modal Multi-Tasking for Speech-to-Text Translation via Hard Parameter Sharing,"Recent works in end-to-end speech-to-text translation (ST) have proposed multi-tasking methods with soft parameter sharing which leverage machine translation (MT) data via secondary encoders that map text inputs to an eventual cross-modal representation. In this work, we instead propose a ST/MT multi-tasking framework with hard parameter sharing in which all model parameters are shared cross-modally. Our method reduces the speech-text modality gap via a pre-processing stage which converts speech and text inputs into two discrete token sequences of similar length – this allows models to indiscriminately process both modalities simply using a joint vocabulary. With experiments on MuST-C, we demonstrate that our multi-tasking framework improves attentional encoder-decoder, Connectionist Temporal Classification (CTC), transducer, and joint CTC/attention models by an average of +0.5 BLEU without any external MT data. Further, we show that this framework incorporates external MT data, yielding +0.8 BLEU, and also improves transfer learning from pre-trained textual models, yielding +1.8 BLEU. <sup xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">1</sup>","['https://openalex.org/W6769269733', 'https://openalex.org/W3168212167', 'https://openalex.org/W4221153524', 'https://openalex.org/W6739365718', 'https://openalex.org/W3205644108', 'https://openalex.org/W6810259195', 'https://openalex.org/W4223622550', 'https://openalex.org/W4287890956', 'https://openalex.org/W4221163209', 'https://openalex.org/W4226120743', 'https://openalex.org/W6846505686', 'https://openalex.org/W4385570757', 'https://openalex.org/W4391021666', 'https://openalex.org/W4385822683', 'https://openalex.org/W6853998256', 'https://openalex.org/W6777028661', 'https://openalex.org/W6790356757', 'https://openalex.org/W6848735303', 'https://openalex.org/W6852781825', 'https://openalex.org/W3180374548', 'https://openalex.org/W4372349107', 'https://openalex.org/W3209984917', 'https://openalex.org/W6898634591', 'https://openalex.org/W2512924740', 'https://openalex.org/W3001434439', 'https://openalex.org/W6781811055', 'https://openalex.org/W2752796333', 'https://openalex.org/W6798098866', 'https://openalex.org/W4307323391', 'https://openalex.org/W3209059054', 'https://openalex.org/W4375869259', 'https://openalex.org/W2963250244', 'https://openalex.org/W3035207248', 'https://openalex.org/W4388017359', 'https://openalex.org/W3089472875', 'https://openalex.org/W2936774411', 'https://openalex.org/W2964012862', 'https://openalex.org/W4225308107', 'https://openalex.org/W6679434410', 'https://openalex.org/W2327501763', 'https://openalex.org/W2127141656', 'https://openalex.org/W6638749077', 'https://openalex.org/W2766219058', 'https://openalex.org/W4386566728', 'https://openalex.org/W3037217258', 'https://openalex.org/W3153583341', 'https://openalex.org/W6839510803', 'https://openalex.org/W4385565440', 'https://openalex.org/W4319862255', 'https://openalex.org/W3173767661', 'https://openalex.org/W4385571303', 'https://openalex.org/W2963532001', 'https://openalex.org/W4285223043', 'https://openalex.org/W6838475196', 'https://openalex.org/W2758375579', 'https://openalex.org/W4385570550', 'https://openalex.org/W4377865046', 'https://openalex.org/W4221155340', 'https://openalex.org/W4313679638', 'https://openalex.org/W4394671563', 'https://openalex.org/W2624871570', 'https://openalex.org/W3046368065', 'https://openalex.org/W3215615641', 'https://openalex.org/W4381827575', 'https://openalex.org/W3024605872']",2024-03-18
https://openalex.org/W4391021723,https://doi.org/10.1109/asru57964.2023.10389644,Improving Multilingual and Code-Switching ASR Using Large Language Model Generated Text,"We investigate using large language models (LLMs) to generate text-only training data for improving multilingual and code-switching automatic speech recognition (ASR) through a text injection method. In a multilingual setup or a low-resource scenario such as code-switching, we propose to generate text data using the state-of-the-art PaLM 2. To better match the generated text data with specific tasks, we use prompt tuning to adapt PaLM 2 to generate domain-relevant multilingual or code-switched text data for text injection. We can achieve significant improvements in Word Error Rate (WER) in both multilingual and code-switching scenarios. The multilingual experiment shows a $6.2 \%$ relative WER reduction on average, i.e., from $11.25 \%$ to $10.55 \%$, compared to a baseline without text injection. The improvement is up to $23.1 \%$ improvement for certain languages. While in the code-switching scenario, we use English-only prompts to generate Mandarin-English code-switching text and achieve a 3.6% relative WER reduction for a code-switching test set, as well as WER reductions in both English and Mandarin monolingual scenarios, $5.3 \%$ and $8.5 \%$ relative, respectively. Our findings demonstrate that leveraging LLMs for text generation and then injection benefits multilingual or code-switching ASR tasks.","['https://openalex.org/W2577366047', 'https://openalex.org/W2963240019', 'https://openalex.org/W3198442913', 'https://openalex.org/W4372259777', 'https://openalex.org/W6803092890', 'https://openalex.org/W4223622550', 'https://openalex.org/W4225319488', 'https://openalex.org/W4226120743', 'https://openalex.org/W4319862474', 'https://openalex.org/W3163203022', 'https://openalex.org/W3171500670', 'https://openalex.org/W3176614736', 'https://openalex.org/W4372266937', 'https://openalex.org/W4319862717', 'https://openalex.org/W3034724424', 'https://openalex.org/W2939069254', 'https://openalex.org/W2963431393', 'https://openalex.org/W4225295099', 'https://openalex.org/W3097961301', 'https://openalex.org/W2972417954', 'https://openalex.org/W6755461288', 'https://openalex.org/W4281770669', 'https://openalex.org/W6852800892', 'https://openalex.org/W6805239564', 'https://openalex.org/W6850625674', 'https://openalex.org/W6853998256', 'https://openalex.org/W6852326057', 'https://openalex.org/W3174770825', 'https://openalex.org/W4205991051', 'https://openalex.org/W6796581206', 'https://openalex.org/W6850477478', 'https://openalex.org/W4296069150', 'https://openalex.org/W2791281350', 'https://openalex.org/W3016234571', 'https://openalex.org/W6739901393', 'https://openalex.org/W6810081322', 'https://openalex.org/W4385823078', 'https://openalex.org/W6638749077', 'https://openalex.org/W4381827575', 'https://openalex.org/W4224308101', 'https://openalex.org/W4385245566', 'https://openalex.org/W3168867926', 'https://openalex.org/W2899073901', 'https://openalex.org/W4375958083', 'https://openalex.org/W3207222250', 'https://openalex.org/W4322825254', 'https://openalex.org/W1828163288', 'https://openalex.org/W4322718191']",2023-12-16
https://openalex.org/W4391021769,https://doi.org/10.1109/asru57964.2023.10389771,Unconstrained Dysfluency Modeling for Dysfluent Speech Transcription and Detection,"Dysfluent speech modeling requires time-accurate and silence-aware transcription at both the word-level and phonetic-level. However, current research in dysfluency modeling primarily focuses on either transcription or detection, and the performance of each aspect remains limited. In this work, we present an unconstrained dysfluency modeling (UDM) approach that addresses both transcription and detection in an automatic and hierarchical manner. UDM eliminates the need for extensive manual annotation by providing a comprehensive solution. Furthermore, we introduce a simulated dysfluent dataset called VCTK<sup>++</sup> to enhance the capabilities of UDM in phonetic transcription. Our experimental results demonstrate the effectiveness and robustness of our proposed methods in both transcription and detection tasks.","['https://openalex.org/W1891932247', 'https://openalex.org/W6634147634', 'https://openalex.org/W2029812037', 'https://openalex.org/W2145410271', 'https://openalex.org/W4385822647', 'https://openalex.org/W6847363464', 'https://openalex.org/W6849786779', 'https://openalex.org/W6850218400', 'https://openalex.org/W6852909395', 'https://openalex.org/W6848194691', 'https://openalex.org/W4385822293', 'https://openalex.org/W3198298452', 'https://openalex.org/W2768920018', 'https://openalex.org/W2993334576', 'https://openalex.org/W4224932945', 'https://openalex.org/W4296068429', 'https://openalex.org/W6810467415', 'https://openalex.org/W3209984917', 'https://openalex.org/W3097777922', 'https://openalex.org/W2747874407', 'https://openalex.org/W6772349387', 'https://openalex.org/W4319862440', 'https://openalex.org/W4225329057', 'https://openalex.org/W4280552695', 'https://openalex.org/W4382371192', 'https://openalex.org/W2168510624', 'https://openalex.org/W3043783436', 'https://openalex.org/W3209059054', 'https://openalex.org/W4223430326', 'https://openalex.org/W4372260274', 'https://openalex.org/W4372340876', 'https://openalex.org/W4385823207', 'https://openalex.org/W6853249747', 'https://openalex.org/W6853998256', 'https://openalex.org/W1573083250', 'https://openalex.org/W2998572311', 'https://openalex.org/W4381827575', 'https://openalex.org/W4221147660', 'https://openalex.org/W4377130946', 'https://openalex.org/W4323066695', 'https://openalex.org/W4391021811', 'https://openalex.org/W4378105483']",2023-12-16
https://openalex.org/W4388272705,https://doi.org/10.21437/blizzard.2023-13,The Idiap Speech Synthesis System for the Blizzard Challenge 2023,"This paper presents the text-to-speech (TTS) system submitted by Idiap Research Institute to the Blizzard Challenge 2023.Our system follows the conventional pipeline of text analysis, acoustic modeling (AM) and vocoding.For text analysis, opensource pretrained part-of-speech (POS) taggers and lemmatizers are utilized to provide more accurate grapheme-to-phoneme (G2P) conversion on top of the eSpeak backend.The rest of the system incorporates a fully diffusion-based approach which comprises a diffusion transformer-based acoustic model and FastDiff as the vocoder, both of which are trained only on the provided data to ensure high-quality synthesis.Our entry provides a baseline for the cascading diffusion AM-vocoder architecture since no extra design is adopted to enhance the naturalness of speech.Evaluation results have demonstrated high synthesis quality of our system and the effectiveness of the proposed phonemization pipeline.","['https://openalex.org/W6736996214', 'https://openalex.org/W3026874504', 'https://openalex.org/W3169905056', 'https://openalex.org/W4287120591', 'https://openalex.org/W2777302760', 'https://openalex.org/W6778823374', 'https://openalex.org/W3015586278', 'https://openalex.org/W6792476516', 'https://openalex.org/W3162673269', 'https://openalex.org/W3087665158', 'https://openalex.org/W4226213470', 'https://openalex.org/W4285605725', 'https://openalex.org/W4313679638', 'https://openalex.org/W4381827575', 'https://openalex.org/W4366460484', 'https://openalex.org/W4382603054', 'https://openalex.org/W2747874407', 'https://openalex.org/W4224629880', 'https://openalex.org/W1593247906', 'https://openalex.org/W4309981764', 'https://openalex.org/W4385993880', 'https://openalex.org/W4303519914', 'https://openalex.org/W3033411150', 'https://openalex.org/W2964243274', 'https://openalex.org/W3129651364', 'https://openalex.org/W2963609956', 'https://openalex.org/W2519091744', 'https://openalex.org/W3198213150', 'https://openalex.org/W3172148458', 'https://openalex.org/W4287121924']",2023-08-29
https://openalex.org/W4388757580,https://doi.org/10.1109/uemcon59035.2023.10315974,Deep Fake and Digital Forensics,"Deepfakes have posed a significant challenge to digital forensics, and there is an increasing need for high accuracy deepfake (DF) detection models in real-world scenarios. This research examines and fine-tunes the MesoNet model to improve its performance on a large dataset of 140K authentic and manipulated images. The original MesoNet model achieved an accuracy of 87.1%. However, after fine-tuning and optimizing the model's weights, the accuracy improved to 96.20%. This was accompanied by a sensitivity of 97.48% and a specificity of 94.75%, indicating that the model is highly effective at detecting genuine images and accurately identifying forged ones. This research contributes to the advancement of DF detection mechanisms in real-world scenarios.","['https://openalex.org/W2012595795', 'https://openalex.org/W2785645041', 'https://openalex.org/W3211278025', 'https://openalex.org/W4386211255', 'https://openalex.org/W6853998256', 'https://openalex.org/W6755207826', 'https://openalex.org/W2891145043', 'https://openalex.org/W3114326827', 'https://openalex.org/W3128081663', 'https://openalex.org/W3154326567', 'https://openalex.org/W3158353280', 'https://openalex.org/W3086623482', 'https://openalex.org/W2982058372', 'https://openalex.org/W2301937176', 'https://openalex.org/W4315865725', 'https://openalex.org/W3097503270', 'https://openalex.org/W3011820288', 'https://openalex.org/W2587706859', 'https://openalex.org/W6784810453', 'https://openalex.org/W6810065566', 'https://openalex.org/W4206742848', 'https://openalex.org/W3034713808', 'https://openalex.org/W3034900344', 'https://openalex.org/W3174508664', 'https://openalex.org/W2794857359', 'https://openalex.org/W2962770929', 'https://openalex.org/W2009130368', 'https://openalex.org/W2603123944', 'https://openalex.org/W6631190155', 'https://openalex.org/W4287758545', 'https://openalex.org/W1522301498', 'https://openalex.org/W4320013936', 'https://openalex.org/W2896457183', 'https://openalex.org/W4381827575', 'https://openalex.org/W4221155330']",2023-10-12
https://openalex.org/W4392904094,https://doi.org/10.1109/icassp48485.2024.10448022,Multiple Representation Transfer from Large Language Models to End-to-End ASR Systems,"Transferring the knowledge of large language models (LLMs) is a promising technique to incorporate linguistic knowledge into end-to-end automatic speech recognition (ASR) systems. However, existing works only transfer a single representation of LLM (e.g. the last layer of pretrained BERT), while the representation of a text is inherently non-unique and can be obtained variously from different layers, contexts and models. In this work, we explore a wide range of techniques to obtain and transfer multiple representations of LLMs into a transducer-based ASR system. While being conceptually simple, we show that transferring multiple representations of LLMs can be an effective alternative to transferring only a single LLM representation.","['https://openalex.org/W6755207826', 'https://openalex.org/W3035390927', 'https://openalex.org/W6739901393', 'https://openalex.org/W2923014074', 'https://openalex.org/W6779271971', 'https://openalex.org/W6778883912', 'https://openalex.org/W6810738896', 'https://openalex.org/W3024308166', 'https://openalex.org/W6790163761', 'https://openalex.org/W3122931219', 'https://openalex.org/W4372260576', 'https://openalex.org/W6853998256', 'https://openalex.org/W6638523607', 'https://openalex.org/W3096297644', 'https://openalex.org/W3164692279', 'https://openalex.org/W4224916448', 'https://openalex.org/W6845595086', 'https://openalex.org/W4225299129', 'https://openalex.org/W4385823005', 'https://openalex.org/W2946417913', 'https://openalex.org/W2970454332', 'https://openalex.org/W4385567464', 'https://openalex.org/W3097777922', 'https://openalex.org/W2750499125', 'https://openalex.org/W2963362078', 'https://openalex.org/W3008037978', 'https://openalex.org/W6770528390', 'https://openalex.org/W3141961557', 'https://openalex.org/W4297841367', 'https://openalex.org/W3196313137', 'https://openalex.org/W4389010951', 'https://openalex.org/W3200601846', 'https://openalex.org/W4385567350', 'https://openalex.org/W3016240723', 'https://openalex.org/W2144499799', 'https://openalex.org/W3163300396', 'https://openalex.org/W4389519317', 'https://openalex.org/W3105966348', 'https://openalex.org/W3200786561', 'https://openalex.org/W6768851824', 'https://openalex.org/W4319862680', 'https://openalex.org/W4381827575', 'https://openalex.org/W1821462560', 'https://openalex.org/W3155427814', 'https://openalex.org/W3034775979', 'https://openalex.org/W2978017171']",2024-03-18
https://openalex.org/W4392884616,https://doi.org/10.5121/ijci.2024.130201,Direct Punjabi to English Speech Translation using Discrete Units,"Speech-to-speech translation is yet to reach the same level of coverage as text-to-text translation systems. The current speech technology is highly limited in its coverage of over 7000 languages spoken worldwide, leaving more than half of the population deprived of such technology and shared experiences. With voice-assisted technology (such as social robots and speech-to-text apps) and auditory content (such as podcasts and lectures) on the rise, ensuring that the technology is available for all is more important than ever. Speech translation can play a vital role in mitigating technological disparity and creating a more inclusive society. With a motive to contribute towards speech translation research for low-resource languages, our work presents a direct speech-to-speech translation model for one of the Indic languages called Punjabi to English. Additionally, we explore the performance of using a discrete representation of speech called discrete acoustic units as input to the Transformer-based translation model. The model, abbreviated as Unit-to-Unit Translation (U2UT), takes a sequence of discrete units of the source language (the language being translated from) and outputs a sequence of discrete units of the target language (the language being translated to). Our results show that the U2UT model performs better than the Speechto-Unit Translation (S2UT) model by a 3.69 BLEU score.","['https://openalex.org/W6761353324', 'https://openalex.org/W6796730497', 'https://openalex.org/W6852909395', 'https://openalex.org/W4309129001', 'https://openalex.org/W6798080464', 'https://openalex.org/W4221164184', 'https://openalex.org/W4378505287', 'https://openalex.org/W2924093092', 'https://openalex.org/W4378473793', 'https://openalex.org/W4283121045', 'https://openalex.org/W6842730932', 'https://openalex.org/W2936184970', 'https://openalex.org/W4387162606', 'https://openalex.org/W3036601975', 'https://openalex.org/W3169320628', 'https://openalex.org/W4387687030', 'https://openalex.org/W3025165719', 'https://openalex.org/W6739901393', 'https://openalex.org/W6810419249', 'https://openalex.org/W4226543485', 'https://openalex.org/W4384648564', 'https://openalex.org/W4311731008', 'https://openalex.org/W3142316150', 'https://openalex.org/W4381827575', 'https://openalex.org/W4386566860', 'https://openalex.org/W3215465553', 'https://openalex.org/W2980109192', 'https://openalex.org/W4310079959', 'https://openalex.org/W4311550865', 'https://openalex.org/W4285077564', 'https://openalex.org/W3095410713', 'https://openalex.org/W2995929068', 'https://openalex.org/W3054645415', 'https://openalex.org/W2292087804', 'https://openalex.org/W3106807794', 'https://openalex.org/W3119308075', 'https://openalex.org/W2966095117', 'https://openalex.org/W4308756394', 'https://openalex.org/W4281621399', 'https://openalex.org/W4293332626', 'https://openalex.org/W1494198834', 'https://openalex.org/W4311000453', 'https://openalex.org/W6898505805', 'https://openalex.org/W6750200984', 'https://openalex.org/W2972495969', 'https://openalex.org/W4382202628', 'https://openalex.org/W3097777922', 'https://openalex.org/W3180374548', 'https://openalex.org/W4306393960', 'https://openalex.org/W3209059054', 'https://openalex.org/W3174758275', 'https://openalex.org/W3139878283', 'https://openalex.org/W4385570009', 'https://openalex.org/W4389524529', 'https://openalex.org/W4296070387', 'https://openalex.org/W4226444650', 'https://openalex.org/W4385245566', 'https://openalex.org/W4385570550', 'https://openalex.org/W4287854499', 'https://openalex.org/W4386142098', 'https://openalex.org/W2963532001', 'https://openalex.org/W4378105483', 'https://openalex.org/W4385571229', 'https://openalex.org/W3030437843', 'https://openalex.org/W4389518827', 'https://openalex.org/W4301980136', 'https://openalex.org/W4385572318', 'https://openalex.org/W4287887366', 'https://openalex.org/W4319862635', 'https://openalex.org/W2937197076']",2024-03-10
https://openalex.org/W4410215894,https://doi.org/10.32622/ijrat.131202513,Survey On Monolingual Speech-to-Speech Translation,"Direct Speech-to-Speech (S2S) translation represents a significant goal in facilitating seamless cross- lingual communication, aiming to overcome the latency and error propagation issues inherent in traditional cascaded systems (ASRMT-TTS). However, the development of high- performing direct S2S models has been critically constrained by the scarcity of large-scale parallel S2S corpora. This survey details Translatotron 3, a pivotal direct S2S system introduced by Google Research in 2023. Translatotron 3 fundamentally shifts the paradigm by demonstrating, for the first time, the feasibility of training a high-quality, end-toend S2S model exclusively using readily available monolingual data resources: source/target speech and source/target text. Leveraging innovative techniques such as unsupervised utterance splitting, phoneme-based intermediate representations, speech-adapted back-translation, and a non auto regressive decoder for rapid inference, Translatotron 3 achieves strong translation quality and remarkable speaker voice preservation without requiring any parallel S2S examples. We critically review the technological context, dissect the model’s architecture and training methodology, analyze its reported performance benchmarks, and discuss its profound implications for advancing S2S translation, particularly for the vast number of low-resource languages previously underserved by data-hungry models.","['https://openalex.org/W2097203679', 'https://openalex.org/W2136545725', 'https://openalex.org/W2936184970', 'https://openalex.org/W4410215894', 'https://openalex.org/W3035707016', 'https://openalex.org/W6810419249', 'https://openalex.org/W4281789500', 'https://openalex.org/W4307006458', 'https://openalex.org/W4223622550', 'https://openalex.org/W4280601369', 'https://openalex.org/W4307932596', 'https://openalex.org/W6798098866', 'https://openalex.org/W3193521535', 'https://openalex.org/W6846857581', 'https://openalex.org/W3169320628', 'https://openalex.org/W6745740328', 'https://openalex.org/W2765961751', 'https://openalex.org/W2284660317', 'https://openalex.org/W2798908575', 'https://openalex.org/W6755567139', 'https://openalex.org/W6798080464', 'https://openalex.org/W2605287558', 'https://openalex.org/W3036601975', 'https://openalex.org/W3205644108', 'https://openalex.org/W4381827575', 'https://openalex.org/W6744957266', 'https://openalex.org/W4378765378']",2025-03-30
https://openalex.org/W4392931320,https://doi.org/10.1109/icassp48485.2024.10446991,A Study on the Adverse Impact of Synthetic Speech on Speech Recognition,"The high-quality synthetic speech by TTS has been widely used in the field of human-computer interaction, bringing users better experience. However, synthetic speech is prone to be mixed with real human speech as part of the noise and recorded by the microphone, which leads to performance decrease for speech recognition. To address this issue, we propose different methods to study the adverse impact of synthetic speech on speech recognition, thereby enhancing its robustness. On the one hand, we adopt the concept of fake audio detection and incorporate an additional module into speech recognition model to differentiate between real and synthetic speech. On the other hand, we propose various methods of incorporating prompt labels from a language semantics perspective to achieve differentiation. These prompt labels provide contextual cues that help speech recognition model to better understand the difference between the two types of speech. The experimental results demonstrate the acoustic modeling of ASR is capable of distinguishing between real and synthetic speech effectively. Putting the prompt labels at the beginning achieves the best performance in a clean synthetic data scenario, while emptying the transcripts of synthetic speech obtains the best performance in a noisy synthetic data scenario.","['https://openalex.org/W6783867762', 'https://openalex.org/W4297841605', 'https://openalex.org/W6848735303', 'https://openalex.org/W3006752097', 'https://openalex.org/W3016008406', 'https://openalex.org/W3080248383', 'https://openalex.org/W4281712850', 'https://openalex.org/W4225873749', 'https://openalex.org/W4385573729', 'https://openalex.org/W4298633873', 'https://openalex.org/W3134568285', 'https://openalex.org/W4372259861', 'https://openalex.org/W3013139777', 'https://openalex.org/W6778883912', 'https://openalex.org/W6810738896', 'https://openalex.org/W6847363464', 'https://openalex.org/W6853998256', 'https://openalex.org/W6853611000', 'https://openalex.org/W2963242190', 'https://openalex.org/W4283067311', 'https://openalex.org/W4385822407', 'https://openalex.org/W4313679638', 'https://openalex.org/W4292779060', 'https://openalex.org/W4226278401', 'https://openalex.org/W3092028330', 'https://openalex.org/W4378501656', 'https://openalex.org/W4381827575']",2024-03-18
https://openalex.org/W4411807528,https://doi.org/10.1049/ses2.70000,AI Large Models for Power System: A Survey and Outlook,"ABSTRACT In recent years, AI large models, also known as large pre‐trained models or foundational models, have achieved remarkable success in various tasks across multiple domains. These models leverage extensive unlabelled datasets from multiple fields and modalities, enabling them to generalize across tasks with minimal labelled data. Their ability has led to advancements in numerous domains. However, the application of large models in power systems remains in their early stages, and the potential of large models has not been fully explored. This paper aims to help researchers and engineers grasp the latest advances and trends in large models to foster the development and applications in the power industry. It traces the development stages of large models, introduces the concept and architecture of large models, and concludes the verified and remarkable capabilities of the large model. Additionally, by integrating existing research, this paper reviews recent advancements and potential applications of large models in power systems, with a focus on perception, planning, and control. Moreover, it summarizes the key enhancement technologies for optimizing the effectiveness of large models of power systems. Finally, the challenges and risks associated with developing large models including computing power requirements, reliability, and safety considerations for power systems are also discussed. Based on the survey, large models for power systems are proven to be a promising paradigm that can improve the efficiency and effectiveness of intelligent power systems, which contributes to the reference for the intelligent development of the power industry.","['https://openalex.org/W4285294723', 'https://openalex.org/W2618530766', 'https://openalex.org/W2160815625', 'https://openalex.org/W2153579005', 'https://openalex.org/W6678277124', 'https://openalex.org/W2116341502', 'https://openalex.org/W6739901393', 'https://openalex.org/W2896457183', 'https://openalex.org/W3030163527', 'https://openalex.org/W2981852735', 'https://openalex.org/W3119866685', 'https://openalex.org/W3001279689', 'https://openalex.org/W4384918448', 'https://openalex.org/W4367176035', 'https://openalex.org/W4381827575', 'https://openalex.org/W4386162736', 'https://openalex.org/W4404987470', 'https://openalex.org/W3094502228', 'https://openalex.org/W4402670135', 'https://openalex.org/W4399357038', 'https://openalex.org/W4396986140', 'https://openalex.org/W4385568059', 'https://openalex.org/W4392930030', 'https://openalex.org/W3176917966', 'https://openalex.org/W2759136286', 'https://openalex.org/W4283026156', 'https://openalex.org/W4405425107', 'https://openalex.org/W2895418405', 'https://openalex.org/W4306672754', 'https://openalex.org/W4392522494', 'https://openalex.org/W4393216334', 'https://openalex.org/W4401357982', 'https://openalex.org/W4390658530', 'https://openalex.org/W4403565951', 'https://openalex.org/W4308505112', 'https://openalex.org/W4312226181', 'https://openalex.org/W4403212011', 'https://openalex.org/W4391382616', 'https://openalex.org/W4312456426', 'https://openalex.org/W4401042907', 'https://openalex.org/W4400933678', 'https://openalex.org/W4393153123', 'https://openalex.org/W4403445359', 'https://openalex.org/W4387838674', 'https://openalex.org/W4391900164', 'https://openalex.org/W4388736304', 'https://openalex.org/W4389115798', 'https://openalex.org/W4400104205', 'https://openalex.org/W4398765090', 'https://openalex.org/W4402260162', 'https://openalex.org/W4392612897', 'https://openalex.org/W4387579585', 'https://openalex.org/W4408358718', 'https://openalex.org/W4402303475', 'https://openalex.org/W3198002980', 'https://openalex.org/W3168867926', 'https://openalex.org/W4205991051', 'https://openalex.org/W4392093999', 'https://openalex.org/W4400943524', 'https://openalex.org/W4404343884', 'https://openalex.org/W4221161695', 'https://openalex.org/W4377130677', 'https://openalex.org/W4401253046', 'https://openalex.org/W4403262324', 'https://openalex.org/W4392867353', 'https://openalex.org/W4404988064', 'https://openalex.org/W4393763811', 'https://openalex.org/W4214759040', 'https://openalex.org/W4379137333', 'https://openalex.org/W4385262276', 'https://openalex.org/W4392251124', 'https://openalex.org/W4389297203', 'https://openalex.org/W4403556333', 'https://openalex.org/W4281679115', 'https://openalex.org/W3035214886', 'https://openalex.org/W4367000527', 'https://openalex.org/W4294170691', 'https://openalex.org/W4385245566', 'https://openalex.org/W4386946669', 'https://openalex.org/W4404688462', 'https://openalex.org/W4320013936', 'https://openalex.org/W4388728292', 'https://openalex.org/W4379929801', 'https://openalex.org/W4288089799', 'https://openalex.org/W2121227244', 'https://openalex.org/W4287391717', 'https://openalex.org/W4383197616', 'https://openalex.org/W4387757449', 'https://openalex.org/W7108515395']",2025-06-01
https://openalex.org/W4392909571,https://doi.org/10.1109/icassp48485.2024.10447464,SELM: Speech Enhancement using Discrete Tokens and Language Models,"Language models (LMs) have recently shown superior performances in various speech generation tasks, demonstrating their powerful ability for semantic context modeling. Given the intrinsic similarity between speech generation and speech enhancement, harnessing semantic information is advantageous for speech enhancement tasks. In light of this, we propose SELM, a novel speech enhancement paradigm that integrates discrete tokens and leverages language models. SELM comprises three stages: encoding, modeling, and decoding. We transform continuous waveform signals into discrete tokens using pre-trained self-supervised learning (SSL) models and a k-means tokenizer. Language models then capture comprehensive contextual information within these tokens. Finally, a de-tokenizer and HiFi-GAN restore them into enhanced speech. Experimental results demonstrate that SELM achieves comparable performance in objective metrics and superior subjective perception results. Our demos are available <sup xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">1</sup> .","['https://openalex.org/W2980451698', 'https://openalex.org/W2963341071', 'https://openalex.org/W3131332223', 'https://openalex.org/W3174264304', 'https://openalex.org/W3011982609', 'https://openalex.org/W4221144097', 'https://openalex.org/W4384080510', 'https://openalex.org/W4297841790', 'https://openalex.org/W3209984917', 'https://openalex.org/W3197580070', 'https://openalex.org/W4319862462', 'https://openalex.org/W6790356757', 'https://openalex.org/W6848735303', 'https://openalex.org/W6852859116', 'https://openalex.org/W4400111385', 'https://openalex.org/W4385822255', 'https://openalex.org/W6853888607', 'https://openalex.org/W6857478255', 'https://openalex.org/W6678914141', 'https://openalex.org/W6783867762', 'https://openalex.org/W4381786045', 'https://openalex.org/W6778883912', 'https://openalex.org/W4385245566', 'https://openalex.org/W3097777922', 'https://openalex.org/W3197478142', 'https://openalex.org/W3198694222', 'https://openalex.org/W6777776875', 'https://openalex.org/W2094721231', 'https://openalex.org/W3197042120', 'https://openalex.org/W2972541922', 'https://openalex.org/W4232282348', 'https://openalex.org/W2696967604', 'https://openalex.org/W6784457260', 'https://openalex.org/W3161480375', 'https://openalex.org/W2144404214', 'https://openalex.org/W3024869864', 'https://openalex.org/W2952218014', 'https://openalex.org/W3097945073', 'https://openalex.org/W4372259751', 'https://openalex.org/W3027008958', 'https://openalex.org/W3093990297', 'https://openalex.org/W4387634372', 'https://openalex.org/W4382603054', 'https://openalex.org/W4379539302', 'https://openalex.org/W2299467264', 'https://openalex.org/W3099330747', 'https://openalex.org/W4394671563', 'https://openalex.org/W4313679638']",2024-03-18
https://openalex.org/W4391021530,https://doi.org/10.1109/asru57964.2023.10389731,Prompting and Adapter Tuning For Self-Supervised Encoder-Decoder Speech Model,"Prompting and adapter tuning have emerged as efficient alternatives to fine-tuning (FT) methods. However, existing studies on speech prompting focused on classification tasks and failed on more complex sequence generation tasks. Besides, adapter tuning is primarily applied with a focus on encoder-only self-supervised models. Our experiments show that prompting on Wav2Seq, a self-supervised encoder-decoder model, surpasses previous works in sequence generation tasks. It achieves a remarkable 53% relative improvement in word error rate for ASR and a 27% in F1 score for slot filling. Additionally, prompting competes with the FT method in the low-resource scenario. Moreover, we show the transferability of prompting and adapter tuning on Wav2Seq in cross-lingual ASR. When limited trainable parameters are involved, prompting and adapter tuning consistently outperform conventional FT across 7 languages. Notably, in the low-resource scenario, prompting consistently outperforms adapter tuning.","['https://openalex.org/W2842511635', 'https://openalex.org/W6780218876', 'https://openalex.org/W3209059054', 'https://openalex.org/W3197580070', 'https://openalex.org/W4281492411', 'https://openalex.org/W3129009457', 'https://openalex.org/W3198217962', 'https://openalex.org/W4377865046', 'https://openalex.org/W4372270126', 'https://openalex.org/W3185341429', 'https://openalex.org/W6759579507', 'https://openalex.org/W4226162428', 'https://openalex.org/W6850477478', 'https://openalex.org/W4372270069', 'https://openalex.org/W4319862642', 'https://openalex.org/W3209984917', 'https://openalex.org/W4226278833', 'https://openalex.org/W4381786045', 'https://openalex.org/W4281672148', 'https://openalex.org/W2970476646', 'https://openalex.org/W3098267758', 'https://openalex.org/W3174770825', 'https://openalex.org/W4285247752', 'https://openalex.org/W6852859116', 'https://openalex.org/W6752946794', 'https://openalex.org/W6809645183', 'https://openalex.org/W4372346241', 'https://openalex.org/W4372260195', 'https://openalex.org/W6738045163', 'https://openalex.org/W6796581206', 'https://openalex.org/W4225410153', 'https://openalex.org/W3172698324', 'https://openalex.org/W6846600677', 'https://openalex.org/W6750665317', 'https://openalex.org/W2972584841', 'https://openalex.org/W1494198834', 'https://openalex.org/W2995181338', 'https://openalex.org/W3161223924', 'https://openalex.org/W3095410713', 'https://openalex.org/W6802744804', 'https://openalex.org/W2963211188', 'https://openalex.org/W4297808394', 'https://openalex.org/W4298312696', 'https://openalex.org/W3036601975', 'https://openalex.org/W4394671563', 'https://openalex.org/W4379539302', 'https://openalex.org/W4385245566', 'https://openalex.org/W3168867926', 'https://openalex.org/W4307783813', 'https://openalex.org/W4322825254', 'https://openalex.org/W2797583228', 'https://openalex.org/W4221155122']",2023-12-16
https://openalex.org/W4392909867,https://doi.org/10.1109/icassp48485.2024.10447938,Towards ASR Robust Spoken Language Understanding Through in-Context Learning with Word Confusion Networks,"In the realm of spoken language understanding (SLU). numerous natural language understanding (NLU) methodologies have been adapted by supplying large language models (LLMs) with transcribed speech instead of conventional written text. In real-world scenarios, prior to input into an LLM. an automated speech recognition (ASR) system generates an output transcript hypothesis, where inherent errors can degrade subsequent SLU tasks. Here we introduce a method that utilizes the ASR system's lattice output instead of relying solely on the top hypothesis, aiming to encapsulate speech ambiguities and enhance SLU outcomes. Our in-context learning experiments, covering spoken question answering and intent classification. underline the LLM's resilience to noisy speech transcripts with the help of word confusion networks from lattices, bridging the SLU performance gap between using the top ASR hypothesis and an oracle upper bound. Additionally, we delve into the LLM's robustness to varying ASR performance conditions and scrutinize the aspects of in-context learning which prove the most influential.","['https://openalex.org/W3175061805', 'https://openalex.org/W2511962886', 'https://openalex.org/W3006901707', 'https://openalex.org/W2891539192', 'https://openalex.org/W3095711142', 'https://openalex.org/W6853888607', 'https://openalex.org/W6848735303', 'https://openalex.org/W4385823351', 'https://openalex.org/W3100460087', 'https://openalex.org/W6778883912', 'https://openalex.org/W1494198834', 'https://openalex.org/W6631362777', 'https://openalex.org/W1993721840', 'https://openalex.org/W4385571124', 'https://openalex.org/W6847118041', 'https://openalex.org/W4385572845', 'https://openalex.org/W3197876970', 'https://openalex.org/W6810738896', 'https://openalex.org/W6739585900', 'https://openalex.org/W4221146627', 'https://openalex.org/W2963748441', 'https://openalex.org/W2962854302', 'https://openalex.org/W4225080353', 'https://openalex.org/W6850477478', 'https://openalex.org/W6796456916', 'https://openalex.org/W6852859116', 'https://openalex.org/W6839692592', 'https://openalex.org/W2077302143', 'https://openalex.org/W4313679638', 'https://openalex.org/W4311642023', 'https://openalex.org/W4285070173', 'https://openalex.org/W4292779060', 'https://openalex.org/W4382603054', 'https://openalex.org/W4322825254', 'https://openalex.org/W4379539302', 'https://openalex.org/W2122364000', 'https://openalex.org/W4226278401']",2024-03-18
