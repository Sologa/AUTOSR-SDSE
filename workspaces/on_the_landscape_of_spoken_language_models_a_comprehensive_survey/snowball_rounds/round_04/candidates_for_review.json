[
  {
    "metadata": {
      "title": "End-to-End Speech Recognition: A Survey",
      "summary": "In the last decade of automatic speech recognition (ASR) research, the introduction of deep learning has brought considerable reductions in word error rate of more than 50% relative, compared to modeling without deep learning. In the wake of this transition, a number of all-neural ASR architectures have been introduced. These so-called <italic xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">end-to-end</i> (E2E) models provide highly integrated, completely neural ASR models, which rely strongly on general machine learning knowledge, learn more consistently from data, with lower dependence on ASR domain-specific experience. The success and enthusiastic adoption of deep learning, accompanied by more generic model architectures has led to E2E models now becoming the prominent ASR approach. The goal of this survey is to provide a taxonomy of E2E ASR models and corresponding improvements, and to discuss their properties and their relationship to classical hidden Markov model (HMM) based ASR architectures. All relevant aspects of E2E ASR are covered in this work: modeling, training, decoding, and external language model integration, discussions of performance and deployment opportunities, as well as an outlook into potential future developments.",
      "abstract": "In the last decade of automatic speech recognition (ASR) research, the introduction of deep learning has brought considerable reductions in word error rate of more than 50% relative, compared to modeling without deep learning. In the wake of this transition, a number of all-neural ASR architectures have been introduced. These so-called <italic xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">end-to-end</i> (E2E) models provide highly integrated, completely neural ASR models, which rely strongly on general machine learning knowledge, learn more consistently from data, with lower dependence on ASR domain-specific experience. The success and enthusiastic adoption of deep learning, accompanied by more generic model architectures has led to E2E models now becoming the prominent ASR approach. The goal of this survey is to provide a taxonomy of E2E ASR models and corresponding improvements, and to discuss their properties and their relationship to classical hidden Markov model (HMM) based ASR architectures. All relevant aspects of E2E ASR are covered in this work: modeling, training, decoding, and external language model integration, discussions of performance and deployment opportunities, as well as an outlook into potential future developments.",
      "doi": "https://doi.org/10.1109/taslp.2023.3328283",
      "openalex_id": "https://openalex.org/W4388017359",
      "arxiv_id": "",
      "publication_date": "2023-10-30",
      "published": "2023-10-30",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Unraveling the landscape of large language models: a systematic review and future perspectives",
      "summary": "Purpose The rapid rise of large language models (LLMs) has propelled them to the forefront of applications in natural language processing (NLP). This paper aims to present a comprehensive examination of the research landscape in LLMs, providing an overview of the prevailing themes and topics within this dynamic domain. Design/methodology/approach Drawing from an extensive corpus of 198 records published between 1996 to 2023 from the relevant academic database encompassing journal articles, books, book chapters, conference papers and selected working papers, this study delves deep into the multifaceted world of LLM research. In this study, the authors employed the BERTopic algorithm, a recent advancement in topic modeling, to conduct a comprehensive analysis of the data after it had been meticulously cleaned and preprocessed. BERTopic leverages the power of transformer-based language models like bidirectional encoder representations from transformers (BERT) to generate more meaningful and coherent topics. This approach facilitates the identification of hidden patterns within the data, enabling authors to uncover valuable insights that might otherwise have remained obscure. The analysis revealed four distinct clusters of topics in LLM research: “language and NLP”, “education and teaching”, “clinical and medical applications” and “speech and recognition techniques”. Each cluster embodies a unique aspect of LLM application and showcases the breadth of possibilities that LLM technology has to offer. In addition to presenting the research findings, this paper identifies key challenges and opportunities in the realm of LLMs. It underscores the necessity for further investigation in specific areas, including the paramount importance of addressing potential biases, transparency and explainability, data privacy and security, and responsible deployment of LLM technology. Findings The analysis revealed four distinct clusters of topics in LLM research: “language and NLP”, “education and teaching”, “clinical and medical applications” and “speech and recognition techniques”. Each cluster embodies a unique aspect of LLM application and showcases the breadth of possibilities that LLM technology has to offer. In addition to presenting the research findings, this paper identifies key challenges and opportunities in the realm of LLMs. It underscores the necessity for further investigation in specific areas, including the paramount importance of addressing potential biases, transparency and explainability, data privacy and security, and responsible deployment of LLM technology. Practical implications This classification offers practical guidance for researchers, developers, educators, and policymakers to focus efforts and resources. The study underscores the importance of addressing challenges in LLMs, including potential biases, transparency, data privacy, and responsible deployment. Policymakers can utilize this information to shape regulations, while developers can tailor technology development based on the diverse applications identified. The findings also emphasize the need for interdisciplinary collaboration and highlight ethical considerations, providing a roadmap for navigating the complex landscape of LLM research and applications. Originality/value This study stands out as the first to examine the evolution of LLMs across such a long time frame and across such diversified disciplines. It provides a unique perspective on the key areas of LLM research, highlighting the breadth and depth of LLM’s evolution.",
      "abstract": "Purpose The rapid rise of large language models (LLMs) has propelled them to the forefront of applications in natural language processing (NLP). This paper aims to present a comprehensive examination of the research landscape in LLMs, providing an overview of the prevailing themes and topics within this dynamic domain. Design/methodology/approach Drawing from an extensive corpus of 198 records published between 1996 to 2023 from the relevant academic database encompassing journal articles, books, book chapters, conference papers and selected working papers, this study delves deep into the multifaceted world of LLM research. In this study, the authors employed the BERTopic algorithm, a recent advancement in topic modeling, to conduct a comprehensive analysis of the data after it had been meticulously cleaned and preprocessed. BERTopic leverages the power of transformer-based language models like bidirectional encoder representations from transformers (BERT) to generate more meaningful and coherent topics. This approach facilitates the identification of hidden patterns within the data, enabling authors to uncover valuable insights that might otherwise have remained obscure. The analysis revealed four distinct clusters of topics in LLM research: “language and NLP”, “education and teaching”, “clinical and medical applications” and “speech and recognition techniques”. Each cluster embodies a unique aspect of LLM application and showcases the breadth of possibilities that LLM technology has to offer. In addition to presenting the research findings, this paper identifies key challenges and opportunities in the realm of LLMs. It underscores the necessity for further investigation in specific areas, including the paramount importance of addressing potential biases, transparency and explainability, data privacy and security, and responsible deployment of LLM technology. Findings The analysis revealed four distinct clusters of topics in LLM research: “language and NLP”, “education and teaching”, “clinical and medical applications” and “speech and recognition techniques”. Each cluster embodies a unique aspect of LLM application and showcases the breadth of possibilities that LLM technology has to offer. In addition to presenting the research findings, this paper identifies key challenges and opportunities in the realm of LLMs. It underscores the necessity for further investigation in specific areas, including the paramount importance of addressing potential biases, transparency and explainability, data privacy and security, and responsible deployment of LLM technology. Practical implications This classification offers practical guidance for researchers, developers, educators, and policymakers to focus efforts and resources. The study underscores the importance of addressing challenges in LLMs, including potential biases, transparency, data privacy, and responsible deployment. Policymakers can utilize this information to shape regulations, while developers can tailor technology development based on the diverse applications identified. The findings also emphasize the need for interdisciplinary collaboration and highlight ethical considerations, providing a roadmap for navigating the complex landscape of LLM research and applications. Originality/value This study stands out as the first to examine the evolution of LLMs across such a long time frame and across such diversified disciplines. It provides a unique perspective on the key areas of LLM research, highlighting the breadth and depth of LLM’s evolution.",
      "doi": "https://doi.org/10.1108/jebde-08-2023-0015",
      "openalex_id": "https://openalex.org/W4389881233",
      "arxiv_id": "",
      "publication_date": "2023-12-18",
      "published": "2023-12-18",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "End-to-End Speech Recognition Contextualization with Large Language Models",
      "summary": "In recent years, Large Language Models (LLMs) have garnered significant attention from the research community due to their exceptional performance and generalization capabilities. In this paper, we introduce a novel method for contextualizing speech recognition models incorporating LLMs. Our approach casts speech recognition as a mixed-modal language modeling task based on a pretrained LLM. We use audio features, along with optional text tokens for context, to train the system to complete transcriptions in a decoder-only fashion. As a result, the system implicitly learns how to leverage unstructured contextual information during training. Our empirical results demonstrate a significant improvement in performance, with a 6% WER reduction when additional textual context is provided. Moreover, we find that our method performs competitively, improving by 7.5% WER overall and 17% WER on rare words, compared to a baseline contextualized RNN-T system that has been trained on a speech dataset more than twenty-five times larger. Overall, we demonstrate that by adding only a handful of trainable parameters via adapters, we can unlock the contextualized speech recognition capability of the pretrained LLM while maintaining the same text-only input functionality.",
      "abstract": "In recent years, Large Language Models (LLMs) have garnered significant attention from the research community due to their exceptional performance and generalization capabilities. In this paper, we introduce a novel method for contextualizing speech recognition models incorporating LLMs. Our approach casts speech recognition as a mixed-modal language modeling task based on a pretrained LLM. We use audio features, along with optional text tokens for context, to train the system to complete transcriptions in a decoder-only fashion. As a result, the system implicitly learns how to leverage unstructured contextual information during training. Our empirical results demonstrate a significant improvement in performance, with a 6% WER reduction when additional textual context is provided. Moreover, we find that our method performs competitively, improving by 7.5% WER overall and 17% WER on rare words, compared to a baseline contextualized RNN-T system that has been trained on a speech dataset more than twenty-five times larger. Overall, we demonstrate that by adding only a handful of trainable parameters via adapters, we can unlock the contextualized speech recognition capability of the pretrained LLM while maintaining the same text-only input functionality.",
      "doi": "https://doi.org/10.1109/icassp48485.2024.10446898",
      "openalex_id": "https://openalex.org/W4392903288",
      "arxiv_id": "",
      "publication_date": "2024-03-18",
      "published": "2024-03-18",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Translatotron 3: Speech to Speech Translation with Monolingual Data",
      "summary": "This paper presents Translatotron 3, a novel approach to unsupervised direct speech-to-speech translation from monolingual speech-text datasets by combining masked autoencoder, unsupervised embedding mapping, and back-translation. Experimental results in speech-to-speech translation tasks between Spanish and English show that Translatotron 3 outperforms a baseline cascade system, reporting 18.14 BLEU points improvement on the synthesized Unpaired-Conversational dataset. In contrast to supervised approaches that necessitate real paired data, or specialized modeling to replicate para-/non-linguistic information such as pauses, speaking rates, and speaker identity, Translatotron 3 showcases its capability to retain it.",
      "abstract": "This paper presents Translatotron 3, a novel approach to unsupervised direct speech-to-speech translation from monolingual speech-text datasets by combining masked autoencoder, unsupervised embedding mapping, and back-translation. Experimental results in speech-to-speech translation tasks between Spanish and English show that Translatotron 3 outperforms a baseline cascade system, reporting 18.14 BLEU points improvement on the synthesized Unpaired-Conversational dataset. In contrast to supervised approaches that necessitate real paired data, or specialized modeling to replicate para-/non-linguistic information such as pauses, speaking rates, and speaker identity, Translatotron 3 showcases its capability to retain it.",
      "doi": "https://doi.org/10.1109/icassp48485.2024.10448426",
      "openalex_id": "https://openalex.org/W4392903062",
      "arxiv_id": "",
      "publication_date": "2024-03-18",
      "published": "2024-03-18",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Adapting Large Language Model with Speech for Fully Formatted End-to-End Speech Recognition",
      "summary": "Most end-to-end (E2E) speech recognition models are composed of encoder and decoder blocks that perform acoustic and language modeling functions. Pretrained large language models (LLMs) have the potential to improve the performance of E2E ASR. However, integrating a pretrained language model into an E2E speech recognition model has shown limited benefits due to the mismatches between text-based LLMs and those used in E2E ASR. In this paper, we explore an alternative approach by adapting a pretrained LLMs to speech. Our experiments on fully-formatted E2E ASR transcription tasks across various domains demonstrate that our approach can effectively leverage the strengths of pretrained LLMs to produce more readable ASR transcriptions. Our model, which is based on the pretrained large language models with either an encoder-decoder or decoder-only structure, surpasses strong ASR models such as Whisper <sup xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">1</sup> , in terms of recognition error rate, considering formats like punctuation and capitalization as well.",
      "abstract": "Most end-to-end (E2E) speech recognition models are composed of encoder and decoder blocks that perform acoustic and language modeling functions. Pretrained large language models (LLMs) have the potential to improve the performance of E2E ASR. However, integrating a pretrained language model into an E2E speech recognition model has shown limited benefits due to the mismatches between text-based LLMs and those used in E2E ASR. In this paper, we explore an alternative approach by adapting a pretrained LLMs to speech. Our experiments on fully-formatted E2E ASR transcription tasks across various domains demonstrate that our approach can effectively leverage the strengths of pretrained LLMs to produce more readable ASR transcriptions. Our model, which is based on the pretrained large language models with either an encoder-decoder or decoder-only structure, surpasses strong ASR models such as Whisper <sup xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">1</sup> , in terms of recognition error rate, considering formats like punctuation and capitalization as well.",
      "doi": "https://doi.org/10.1109/icassp48485.2024.10448204",
      "openalex_id": "https://openalex.org/W4392910583",
      "arxiv_id": "",
      "publication_date": "2024-03-18",
      "published": "2024-03-18",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Cross-Modal Multi-Tasking for Speech-to-Text Translation via Hard Parameter Sharing",
      "summary": "Recent works in end-to-end speech-to-text translation (ST) have proposed multi-tasking methods with soft parameter sharing which leverage machine translation (MT) data via secondary encoders that map text inputs to an eventual cross-modal representation. In this work, we instead propose a ST/MT multi-tasking framework with hard parameter sharing in which all model parameters are shared cross-modally. Our method reduces the speech-text modality gap via a pre-processing stage which converts speech and text inputs into two discrete token sequences of similar length – this allows models to indiscriminately process both modalities simply using a joint vocabulary. With experiments on MuST-C, we demonstrate that our multi-tasking framework improves attentional encoder-decoder, Connectionist Temporal Classification (CTC), transducer, and joint CTC/attention models by an average of +0.5 BLEU without any external MT data. Further, we show that this framework incorporates external MT data, yielding +0.8 BLEU, and also improves transfer learning from pre-trained textual models, yielding +1.8 BLEU. <sup xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">1</sup>",
      "abstract": "Recent works in end-to-end speech-to-text translation (ST) have proposed multi-tasking methods with soft parameter sharing which leverage machine translation (MT) data via secondary encoders that map text inputs to an eventual cross-modal representation. In this work, we instead propose a ST/MT multi-tasking framework with hard parameter sharing in which all model parameters are shared cross-modally. Our method reduces the speech-text modality gap via a pre-processing stage which converts speech and text inputs into two discrete token sequences of similar length – this allows models to indiscriminately process both modalities simply using a joint vocabulary. With experiments on MuST-C, we demonstrate that our multi-tasking framework improves attentional encoder-decoder, Connectionist Temporal Classification (CTC), transducer, and joint CTC/attention models by an average of +0.5 BLEU without any external MT data. Further, we show that this framework incorporates external MT data, yielding +0.8 BLEU, and also improves transfer learning from pre-trained textual models, yielding +1.8 BLEU. <sup xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">1</sup>",
      "doi": "https://doi.org/10.1109/icassp48485.2024.10447926",
      "openalex_id": "https://openalex.org/W4392902778",
      "arxiv_id": "",
      "publication_date": "2024-03-18",
      "published": "2024-03-18",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Improving Multilingual and Code-Switching ASR Using Large Language Model Generated Text",
      "summary": "We investigate using large language models (LLMs) to generate text-only training data for improving multilingual and code-switching automatic speech recognition (ASR) through a text injection method. In a multilingual setup or a low-resource scenario such as code-switching, we propose to generate text data using the state-of-the-art PaLM 2. To better match the generated text data with specific tasks, we use prompt tuning to adapt PaLM 2 to generate domain-relevant multilingual or code-switched text data for text injection. We can achieve significant improvements in Word Error Rate (WER) in both multilingual and code-switching scenarios. The multilingual experiment shows a $6.2 \\%$ relative WER reduction on average, i.e., from $11.25 \\%$ to $10.55 \\%$, compared to a baseline without text injection. The improvement is up to $23.1 \\%$ improvement for certain languages. While in the code-switching scenario, we use English-only prompts to generate Mandarin-English code-switching text and achieve a 3.6% relative WER reduction for a code-switching test set, as well as WER reductions in both English and Mandarin monolingual scenarios, $5.3 \\%$ and $8.5 \\%$ relative, respectively. Our findings demonstrate that leveraging LLMs for text generation and then injection benefits multilingual or code-switching ASR tasks.",
      "abstract": "We investigate using large language models (LLMs) to generate text-only training data for improving multilingual and code-switching automatic speech recognition (ASR) through a text injection method. In a multilingual setup or a low-resource scenario such as code-switching, we propose to generate text data using the state-of-the-art PaLM 2. To better match the generated text data with specific tasks, we use prompt tuning to adapt PaLM 2 to generate domain-relevant multilingual or code-switched text data for text injection. We can achieve significant improvements in Word Error Rate (WER) in both multilingual and code-switching scenarios. The multilingual experiment shows a $6.2 \\%$ relative WER reduction on average, i.e., from $11.25 \\%$ to $10.55 \\%$, compared to a baseline without text injection. The improvement is up to $23.1 \\%$ improvement for certain languages. While in the code-switching scenario, we use English-only prompts to generate Mandarin-English code-switching text and achieve a 3.6% relative WER reduction for a code-switching test set, as well as WER reductions in both English and Mandarin monolingual scenarios, $5.3 \\%$ and $8.5 \\%$ relative, respectively. Our findings demonstrate that leveraging LLMs for text generation and then injection benefits multilingual or code-switching ASR tasks.",
      "doi": "https://doi.org/10.1109/asru57964.2023.10389644",
      "openalex_id": "https://openalex.org/W4391021723",
      "arxiv_id": "",
      "publication_date": "2023-12-16",
      "published": "2023-12-16",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Unconstrained Dysfluency Modeling for Dysfluent Speech Transcription and Detection",
      "summary": "Dysfluent speech modeling requires time-accurate and silence-aware transcription at both the word-level and phonetic-level. However, current research in dysfluency modeling primarily focuses on either transcription or detection, and the performance of each aspect remains limited. In this work, we present an unconstrained dysfluency modeling (UDM) approach that addresses both transcription and detection in an automatic and hierarchical manner. UDM eliminates the need for extensive manual annotation by providing a comprehensive solution. Furthermore, we introduce a simulated dysfluent dataset called VCTK<sup>++</sup> to enhance the capabilities of UDM in phonetic transcription. Our experimental results demonstrate the effectiveness and robustness of our proposed methods in both transcription and detection tasks.",
      "abstract": "Dysfluent speech modeling requires time-accurate and silence-aware transcription at both the word-level and phonetic-level. However, current research in dysfluency modeling primarily focuses on either transcription or detection, and the performance of each aspect remains limited. In this work, we present an unconstrained dysfluency modeling (UDM) approach that addresses both transcription and detection in an automatic and hierarchical manner. UDM eliminates the need for extensive manual annotation by providing a comprehensive solution. Furthermore, we introduce a simulated dysfluent dataset called VCTK<sup>++</sup> to enhance the capabilities of UDM in phonetic transcription. Our experimental results demonstrate the effectiveness and robustness of our proposed methods in both transcription and detection tasks.",
      "doi": "https://doi.org/10.1109/asru57964.2023.10389771",
      "openalex_id": "https://openalex.org/W4391021769",
      "arxiv_id": "",
      "publication_date": "2023-12-16",
      "published": "2023-12-16",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "The Idiap Speech Synthesis System for the Blizzard Challenge 2023",
      "summary": "This paper presents the text-to-speech (TTS) system submitted by Idiap Research Institute to the Blizzard Challenge 2023.Our system follows the conventional pipeline of text analysis, acoustic modeling (AM) and vocoding.For text analysis, opensource pretrained part-of-speech (POS) taggers and lemmatizers are utilized to provide more accurate grapheme-to-phoneme (G2P) conversion on top of the eSpeak backend.The rest of the system incorporates a fully diffusion-based approach which comprises a diffusion transformer-based acoustic model and FastDiff as the vocoder, both of which are trained only on the provided data to ensure high-quality synthesis.Our entry provides a baseline for the cascading diffusion AM-vocoder architecture since no extra design is adopted to enhance the naturalness of speech.Evaluation results have demonstrated high synthesis quality of our system and the effectiveness of the proposed phonemization pipeline.",
      "abstract": "This paper presents the text-to-speech (TTS) system submitted by Idiap Research Institute to the Blizzard Challenge 2023.Our system follows the conventional pipeline of text analysis, acoustic modeling (AM) and vocoding.For text analysis, opensource pretrained part-of-speech (POS) taggers and lemmatizers are utilized to provide more accurate grapheme-to-phoneme (G2P) conversion on top of the eSpeak backend.The rest of the system incorporates a fully diffusion-based approach which comprises a diffusion transformer-based acoustic model and FastDiff as the vocoder, both of which are trained only on the provided data to ensure high-quality synthesis.Our entry provides a baseline for the cascading diffusion AM-vocoder architecture since no extra design is adopted to enhance the naturalness of speech.Evaluation results have demonstrated high synthesis quality of our system and the effectiveness of the proposed phonemization pipeline.",
      "doi": "https://doi.org/10.21437/blizzard.2023-13",
      "openalex_id": "https://openalex.org/W4388272705",
      "arxiv_id": "",
      "publication_date": "2023-08-29",
      "published": "2023-08-29",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Deep Fake and Digital Forensics",
      "summary": "Deepfakes have posed a significant challenge to digital forensics, and there is an increasing need for high accuracy deepfake (DF) detection models in real-world scenarios. This research examines and fine-tunes the MesoNet model to improve its performance on a large dataset of 140K authentic and manipulated images. The original MesoNet model achieved an accuracy of 87.1%. However, after fine-tuning and optimizing the model's weights, the accuracy improved to 96.20%. This was accompanied by a sensitivity of 97.48% and a specificity of 94.75%, indicating that the model is highly effective at detecting genuine images and accurately identifying forged ones. This research contributes to the advancement of DF detection mechanisms in real-world scenarios.",
      "abstract": "Deepfakes have posed a significant challenge to digital forensics, and there is an increasing need for high accuracy deepfake (DF) detection models in real-world scenarios. This research examines and fine-tunes the MesoNet model to improve its performance on a large dataset of 140K authentic and manipulated images. The original MesoNet model achieved an accuracy of 87.1%. However, after fine-tuning and optimizing the model's weights, the accuracy improved to 96.20%. This was accompanied by a sensitivity of 97.48% and a specificity of 94.75%, indicating that the model is highly effective at detecting genuine images and accurately identifying forged ones. This research contributes to the advancement of DF detection mechanisms in real-world scenarios.",
      "doi": "https://doi.org/10.1109/uemcon59035.2023.10315974",
      "openalex_id": "https://openalex.org/W4388757580",
      "arxiv_id": "",
      "publication_date": "2023-10-12",
      "published": "2023-10-12",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Multiple Representation Transfer from Large Language Models to End-to-End ASR Systems",
      "summary": "Transferring the knowledge of large language models (LLMs) is a promising technique to incorporate linguistic knowledge into end-to-end automatic speech recognition (ASR) systems. However, existing works only transfer a single representation of LLM (e.g. the last layer of pretrained BERT), while the representation of a text is inherently non-unique and can be obtained variously from different layers, contexts and models. In this work, we explore a wide range of techniques to obtain and transfer multiple representations of LLMs into a transducer-based ASR system. While being conceptually simple, we show that transferring multiple representations of LLMs can be an effective alternative to transferring only a single LLM representation.",
      "abstract": "Transferring the knowledge of large language models (LLMs) is a promising technique to incorporate linguistic knowledge into end-to-end automatic speech recognition (ASR) systems. However, existing works only transfer a single representation of LLM (e.g. the last layer of pretrained BERT), while the representation of a text is inherently non-unique and can be obtained variously from different layers, contexts and models. In this work, we explore a wide range of techniques to obtain and transfer multiple representations of LLMs into a transducer-based ASR system. While being conceptually simple, we show that transferring multiple representations of LLMs can be an effective alternative to transferring only a single LLM representation.",
      "doi": "https://doi.org/10.1109/icassp48485.2024.10448022",
      "openalex_id": "https://openalex.org/W4392904094",
      "arxiv_id": "",
      "publication_date": "2024-03-18",
      "published": "2024-03-18",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Direct Punjabi to English Speech Translation using Discrete Units",
      "summary": "Speech-to-speech translation is yet to reach the same level of coverage as text-to-text translation systems. The current speech technology is highly limited in its coverage of over 7000 languages spoken worldwide, leaving more than half of the population deprived of such technology and shared experiences. With voice-assisted technology (such as social robots and speech-to-text apps) and auditory content (such as podcasts and lectures) on the rise, ensuring that the technology is available for all is more important than ever. Speech translation can play a vital role in mitigating technological disparity and creating a more inclusive society. With a motive to contribute towards speech translation research for low-resource languages, our work presents a direct speech-to-speech translation model for one of the Indic languages called Punjabi to English. Additionally, we explore the performance of using a discrete representation of speech called discrete acoustic units as input to the Transformer-based translation model. The model, abbreviated as Unit-to-Unit Translation (U2UT), takes a sequence of discrete units of the source language (the language being translated from) and outputs a sequence of discrete units of the target language (the language being translated to). Our results show that the U2UT model performs better than the Speechto-Unit Translation (S2UT) model by a 3.69 BLEU score.",
      "abstract": "Speech-to-speech translation is yet to reach the same level of coverage as text-to-text translation systems. The current speech technology is highly limited in its coverage of over 7000 languages spoken worldwide, leaving more than half of the population deprived of such technology and shared experiences. With voice-assisted technology (such as social robots and speech-to-text apps) and auditory content (such as podcasts and lectures) on the rise, ensuring that the technology is available for all is more important than ever. Speech translation can play a vital role in mitigating technological disparity and creating a more inclusive society. With a motive to contribute towards speech translation research for low-resource languages, our work presents a direct speech-to-speech translation model for one of the Indic languages called Punjabi to English. Additionally, we explore the performance of using a discrete representation of speech called discrete acoustic units as input to the Transformer-based translation model. The model, abbreviated as Unit-to-Unit Translation (U2UT), takes a sequence of discrete units of the source language (the language being translated from) and outputs a sequence of discrete units of the target language (the language being translated to). Our results show that the U2UT model performs better than the Speechto-Unit Translation (S2UT) model by a 3.69 BLEU score.",
      "doi": "https://doi.org/10.5121/ijci.2024.130201",
      "openalex_id": "https://openalex.org/W4392884616",
      "arxiv_id": "",
      "publication_date": "2024-03-10",
      "published": "2024-03-10",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Survey On Monolingual Speech-to-Speech Translation",
      "summary": "Direct Speech-to-Speech (S2S) translation represents a significant goal in facilitating seamless cross- lingual communication, aiming to overcome the latency and error propagation issues inherent in traditional cascaded systems (ASRMT-TTS). However, the development of high- performing direct S2S models has been critically constrained by the scarcity of large-scale parallel S2S corpora. This survey details Translatotron 3, a pivotal direct S2S system introduced by Google Research in 2023. Translatotron 3 fundamentally shifts the paradigm by demonstrating, for the first time, the feasibility of training a high-quality, end-toend S2S model exclusively using readily available monolingual data resources: source/target speech and source/target text. Leveraging innovative techniques such as unsupervised utterance splitting, phoneme-based intermediate representations, speech-adapted back-translation, and a non auto regressive decoder for rapid inference, Translatotron 3 achieves strong translation quality and remarkable speaker voice preservation without requiring any parallel S2S examples. We critically review the technological context, dissect the model’s architecture and training methodology, analyze its reported performance benchmarks, and discuss its profound implications for advancing S2S translation, particularly for the vast number of low-resource languages previously underserved by data-hungry models.",
      "abstract": "Direct Speech-to-Speech (S2S) translation represents a significant goal in facilitating seamless cross- lingual communication, aiming to overcome the latency and error propagation issues inherent in traditional cascaded systems (ASRMT-TTS). However, the development of high- performing direct S2S models has been critically constrained by the scarcity of large-scale parallel S2S corpora. This survey details Translatotron 3, a pivotal direct S2S system introduced by Google Research in 2023. Translatotron 3 fundamentally shifts the paradigm by demonstrating, for the first time, the feasibility of training a high-quality, end-toend S2S model exclusively using readily available monolingual data resources: source/target speech and source/target text. Leveraging innovative techniques such as unsupervised utterance splitting, phoneme-based intermediate representations, speech-adapted back-translation, and a non auto regressive decoder for rapid inference, Translatotron 3 achieves strong translation quality and remarkable speaker voice preservation without requiring any parallel S2S examples. We critically review the technological context, dissect the model’s architecture and training methodology, analyze its reported performance benchmarks, and discuss its profound implications for advancing S2S translation, particularly for the vast number of low-resource languages previously underserved by data-hungry models.",
      "doi": "https://doi.org/10.32622/ijrat.131202513",
      "openalex_id": "https://openalex.org/W4410215894",
      "arxiv_id": "",
      "publication_date": "2025-03-30",
      "published": "2025-03-30",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "AI Large Models for Power System: A Survey and Outlook",
      "summary": "ABSTRACT In recent years, AI large models, also known as large pre‐trained models or foundational models, have achieved remarkable success in various tasks across multiple domains. These models leverage extensive unlabelled datasets from multiple fields and modalities, enabling them to generalize across tasks with minimal labelled data. Their ability has led to advancements in numerous domains. However, the application of large models in power systems remains in their early stages, and the potential of large models has not been fully explored. This paper aims to help researchers and engineers grasp the latest advances and trends in large models to foster the development and applications in the power industry. It traces the development stages of large models, introduces the concept and architecture of large models, and concludes the verified and remarkable capabilities of the large model. Additionally, by integrating existing research, this paper reviews recent advancements and potential applications of large models in power systems, with a focus on perception, planning, and control. Moreover, it summarizes the key enhancement technologies for optimizing the effectiveness of large models of power systems. Finally, the challenges and risks associated with developing large models including computing power requirements, reliability, and safety considerations for power systems are also discussed. Based on the survey, large models for power systems are proven to be a promising paradigm that can improve the efficiency and effectiveness of intelligent power systems, which contributes to the reference for the intelligent development of the power industry.",
      "abstract": "ABSTRACT In recent years, AI large models, also known as large pre‐trained models or foundational models, have achieved remarkable success in various tasks across multiple domains. These models leverage extensive unlabelled datasets from multiple fields and modalities, enabling them to generalize across tasks with minimal labelled data. Their ability has led to advancements in numerous domains. However, the application of large models in power systems remains in their early stages, and the potential of large models has not been fully explored. This paper aims to help researchers and engineers grasp the latest advances and trends in large models to foster the development and applications in the power industry. It traces the development stages of large models, introduces the concept and architecture of large models, and concludes the verified and remarkable capabilities of the large model. Additionally, by integrating existing research, this paper reviews recent advancements and potential applications of large models in power systems, with a focus on perception, planning, and control. Moreover, it summarizes the key enhancement technologies for optimizing the effectiveness of large models of power systems. Finally, the challenges and risks associated with developing large models including computing power requirements, reliability, and safety considerations for power systems are also discussed. Based on the survey, large models for power systems are proven to be a promising paradigm that can improve the efficiency and effectiveness of intelligent power systems, which contributes to the reference for the intelligent development of the power industry.",
      "doi": "https://doi.org/10.1049/ses2.70000",
      "openalex_id": "https://openalex.org/W4411807528",
      "arxiv_id": "",
      "publication_date": "2025-06-01",
      "published": "2025-06-01",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Towards ASR Robust Spoken Language Understanding Through in-Context Learning with Word Confusion Networks",
      "summary": "In the realm of spoken language understanding (SLU). numerous natural language understanding (NLU) methodologies have been adapted by supplying large language models (LLMs) with transcribed speech instead of conventional written text. In real-world scenarios, prior to input into an LLM. an automated speech recognition (ASR) system generates an output transcript hypothesis, where inherent errors can degrade subsequent SLU tasks. Here we introduce a method that utilizes the ASR system's lattice output instead of relying solely on the top hypothesis, aiming to encapsulate speech ambiguities and enhance SLU outcomes. Our in-context learning experiments, covering spoken question answering and intent classification. underline the LLM's resilience to noisy speech transcripts with the help of word confusion networks from lattices, bridging the SLU performance gap between using the top ASR hypothesis and an oracle upper bound. Additionally, we delve into the LLM's robustness to varying ASR performance conditions and scrutinize the aspects of in-context learning which prove the most influential.",
      "abstract": "In the realm of spoken language understanding (SLU). numerous natural language understanding (NLU) methodologies have been adapted by supplying large language models (LLMs) with transcribed speech instead of conventional written text. In real-world scenarios, prior to input into an LLM. an automated speech recognition (ASR) system generates an output transcript hypothesis, where inherent errors can degrade subsequent SLU tasks. Here we introduce a method that utilizes the ASR system's lattice output instead of relying solely on the top hypothesis, aiming to encapsulate speech ambiguities and enhance SLU outcomes. Our in-context learning experiments, covering spoken question answering and intent classification. underline the LLM's resilience to noisy speech transcripts with the help of word confusion networks from lattices, bridging the SLU performance gap between using the top ASR hypothesis and an oracle upper bound. Additionally, we delve into the LLM's robustness to varying ASR performance conditions and scrutinize the aspects of in-context learning which prove the most influential.",
      "doi": "https://doi.org/10.1109/icassp48485.2024.10447938",
      "openalex_id": "https://openalex.org/W4392909867",
      "arxiv_id": "",
      "publication_date": "2024-03-18",
      "published": "2024-03-18",
      "source": "openalex_snowball"
    }
  }
]