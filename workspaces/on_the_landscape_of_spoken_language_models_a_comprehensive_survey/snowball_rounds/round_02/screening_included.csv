doi,title,abstract,included,decision_source,final_verdict,landing_url,pdf_url,year,source_anchor,source_term,metadata_json,arxiv_id,openalex_id
10.1109/taslp.2024.3436618,SpeechPrompt: Prompting Speech Language Models for Speech Processing Tasks,"Prompting has become a practical method for utilizing pre-trained language\nmodels (LMs). This approach offers several advantages. It allows an LM to adapt\nto new tasks with minimal training and parameter updates, thus achieving\nefficiency in both storage and computation. Additionally, prompting modifies\nonly the LM's inputs and harnesses the generative capabilities of language\nmodels to address various downstream tasks in a unified manner. This\nsignificantly reduces the need for human labor in designing task-specific\nmodels. These advantages become even more evident as the number of tasks served\nby the LM scales up. Motivated by the strengths of prompting, we are the first\nto explore the potential of prompting speech LMs in the domain of speech\nprocessing. Recently, there has been a growing interest in converting speech\ninto discrete units for language modeling. Our pioneer research demonstrates\nthat these quantized speech units are highly versatile within our unified\nprompting framework. Not only can they serve as class labels, but they also\ncontain rich phonetic information that can be re-synthesized back into speech\nsignals for speech generation tasks. Specifically, we reformulate speech\nprocessing tasks into speech-to-unit generation tasks. As a result, we can\nseamlessly integrate tasks such as speech classification, sequence generation,\nand speech generation within a single, unified prompting framework. The\nexperiment results show that the prompting method can achieve competitive\nperformance compared to the strong fine-tuning method based on self-supervised\nlearning models with a similar number of trainable parameters. The prompting\nmethod also shows promising results in the few-shot setting. Moreover, with the\nadvanced speech LMs coming into the stage, the proposed prompting framework\nattains great potential.\n",1,,include (senior:5),,,2024,,,"{""title"": ""SpeechPrompt: Prompting Speech Language Models for Speech Processing Tasks"", ""summary"": ""Prompting has become a practical method for utilizing pre-trained language\\nmodels (LMs). This approach offers several advantages. It allows an LM to adapt\\nto new tasks with minimal training and parameter updates, thus achieving\\nefficiency in both storage and computation. Additionally, prompting modifies\\nonly the LM's inputs and harnesses the generative capabilities of language\\nmodels to address various downstream tasks in a unified manner. This\\nsignificantly reduces the need for human labor in designing task-specific\\nmodels. These advantages become even more evident as the number of tasks served\\nby the LM scales up. Motivated by the strengths of prompting, we are the first\\nto explore the potential of prompting speech LMs in the domain of speech\\nprocessing. Recently, there has been a growing interest in converting speech\\ninto discrete units for language modeling. Our pioneer research demonstrates\\nthat these quantized speech units are highly versatile within our unified\\nprompting framework. Not only can they serve as class labels, but they also\\ncontain rich phonetic information that can be re-synthesized back into speech\\nsignals for speech generation tasks. Specifically, we reformulate speech\\nprocessing tasks into speech-to-unit generation tasks. As a result, we can\\nseamlessly integrate tasks such as speech classification, sequence generation,\\nand speech generation within a single, unified prompting framework. The\\nexperiment results show that the prompting method can achieve competitive\\nperformance compared to the strong fine-tuning method based on self-supervised\\nlearning models with a similar number of trainable parameters. The prompting\\nmethod also shows promising results in the few-shot setting. Moreover, with the\\nadvanced speech LMs coming into the stage, the proposed prompting framework\\nattains great potential.\\n"", ""abstract"": ""Prompting has become a practical method for utilizing pre-trained language\\nmodels (LMs). This approach offers several advantages. It allows an LM to adapt\\nto new tasks with minimal training and parameter updates, thus achieving\\nefficiency in both storage and computation. Additionally, prompting modifies\\nonly the LM's inputs and harnesses the generative capabilities of language\\nmodels to address various downstream tasks in a unified manner. This\\nsignificantly reduces the need for human labor in designing task-specific\\nmodels. These advantages become even more evident as the number of tasks served\\nby the LM scales up. Motivated by the strengths of prompting, we are the first\\nto explore the potential of prompting speech LMs in the domain of speech\\nprocessing. Recently, there has been a growing interest in converting speech\\ninto discrete units for language modeling. Our pioneer research demonstrates\\nthat these quantized speech units are highly versatile within our unified\\nprompting framework. Not only can they serve as class labels, but they also\\ncontain rich phonetic information that can be re-synthesized back into speech\\nsignals for speech generation tasks. Specifically, we reformulate speech\\nprocessing tasks into speech-to-unit generation tasks. As a result, we can\\nseamlessly integrate tasks such as speech classification, sequence generation,\\nand speech generation within a single, unified prompting framework. The\\nexperiment results show that the prompting method can achieve competitive\\nperformance compared to the strong fine-tuning method based on self-supervised\\nlearning models with a similar number of trainable parameters. The prompting\\nmethod also shows promising results in the few-shot setting. Moreover, with the\\nadvanced speech LMs coming into the stage, the proposed prompting framework\\nattains great potential.\\n"", ""doi"": ""https://doi.org/10.1109/taslp.2024.3436618"", ""openalex_id"": ""https://openalex.org/W4401246677"", ""arxiv_id"": """", ""publication_date"": ""2024-01-01"", ""published"": ""2024-01-01"", ""source"": ""openalex_snowball""}",,https://openalex.org/W4401246677
10.18653/v1/2023.findings-emnlp.810,XLS-R fine-tuning on noisy word boundaries for unsupervised speech segmentation into words,"Due to the absence of explicit word boundaries in the speech stream, the task of segmenting spoken sentences into word units without text supervision is particularly challenging. In this work, we leverage the most recent self-supervised speech models that have proved to quickly adapt to new tasks through fine-tuning, even in low resource conditions. Taking inspiration from semi-supervised learning, we fine-tune an XLS-R model to predict word boundaries themselves produced by top-tier speech segmentation systems: DPDP, VG-HuBERT and DP-Parse. Once XLS-R is fine-tuned, it is used to infer new word boundary labels that are used in turn for another fine-tuning step. Our method consistently improves the performance of each system and set a new state-of-the-art that is, on average 130% higher than the previous one as measured by the F1 score on correctly discovered word tokens on five corpora featuring different languages. Finally, our system can segment speech from languages unseen during fine-tuning in a zero-shot fashion.",1,,include (senior:4),,,2023,,,"{""title"": ""XLS-R fine-tuning on noisy word boundaries for unsupervised speech segmentation into words"", ""summary"": ""Due to the absence of explicit word boundaries in the speech stream, the task of segmenting spoken sentences into word units without text supervision is particularly challenging. In this work, we leverage the most recent self-supervised speech models that have proved to quickly adapt to new tasks through fine-tuning, even in low resource conditions. Taking inspiration from semi-supervised learning, we fine-tune an XLS-R model to predict word boundaries themselves produced by top-tier speech segmentation systems: DPDP, VG-HuBERT and DP-Parse. Once XLS-R is fine-tuned, it is used to infer new word boundary labels that are used in turn for another fine-tuning step. Our method consistently improves the performance of each system and set a new state-of-the-art that is, on average 130% higher than the previous one as measured by the F1 score on correctly discovered word tokens on five corpora featuring different languages. Finally, our system can segment speech from languages unseen during fine-tuning in a zero-shot fashion."", ""abstract"": ""Due to the absence of explicit word boundaries in the speech stream, the task of segmenting spoken sentences into word units without text supervision is particularly challenging. In this work, we leverage the most recent self-supervised speech models that have proved to quickly adapt to new tasks through fine-tuning, even in low resource conditions. Taking inspiration from semi-supervised learning, we fine-tune an XLS-R model to predict word boundaries themselves produced by top-tier speech segmentation systems: DPDP, VG-HuBERT and DP-Parse. Once XLS-R is fine-tuned, it is used to infer new word boundary labels that are used in turn for another fine-tuning step. Our method consistently improves the performance of each system and set a new state-of-the-art that is, on average 130% higher than the previous one as measured by the F1 score on correctly discovered word tokens on five corpora featuring different languages. Finally, our system can segment speech from languages unseen during fine-tuning in a zero-shot fashion."", ""doi"": ""https://doi.org/10.18653/v1/2023.findings-emnlp.810"", ""openalex_id"": ""https://openalex.org/W4389524126"", ""arxiv_id"": """", ""publication_date"": ""2023-01-01"", ""published"": ""2023-01-01"", ""source"": ""openalex_snowball""}",,https://openalex.org/W4389524126
10.18653/v1/2023.findings-emnlp.1055,SpeechGPT: Empowering Large Language Models with Intrinsic Cross-Modal Conversational Abilities,"Multi-modal large language models are regarded as a crucial step towards Artificial General Intelligence (AGI) and have garnered significant interest with the emergence of ChatGPT. However, current speech-language models typically adopt the cascade paradigm, preventing inter-modal knowledge transfer. In this paper, we propose SpeechGPT, a large language model with intrinsic cross-modal conversational abilities, capable of perceiving and generating multi-modal content. With discrete speech representations, we construct SpeechInstruct, the first large-scale cross-modal speech instruction dataset. Additionally, we employ a three-stage training strategy that includes modality-adaptation pre-training, cross-modal instruction fine-tuning, and chain-of-modality instruction fine-tuning. The experimental results demonstrate that SpeechGPT has an impressive capacity to follow cross-modal human instructions and highlight the potential of handling multiple modalities with one model. Code and models are available in https://github.com/0nutation/SpeechGPT. Demos are shown in https://0nutation.github.io/SpeechGPT.github.io/.",1,,include (senior:4),,,2023,,,"{""title"": ""SpeechGPT: Empowering Large Language Models with Intrinsic Cross-Modal Conversational Abilities"", ""summary"": ""Multi-modal large language models are regarded as a crucial step towards Artificial General Intelligence (AGI) and have garnered significant interest with the emergence of ChatGPT. However, current speech-language models typically adopt the cascade paradigm, preventing inter-modal knowledge transfer. In this paper, we propose SpeechGPT, a large language model with intrinsic cross-modal conversational abilities, capable of perceiving and generating multi-modal content. With discrete speech representations, we construct SpeechInstruct, the first large-scale cross-modal speech instruction dataset. Additionally, we employ a three-stage training strategy that includes modality-adaptation pre-training, cross-modal instruction fine-tuning, and chain-of-modality instruction fine-tuning. The experimental results demonstrate that SpeechGPT has an impressive capacity to follow cross-modal human instructions and highlight the potential of handling multiple modalities with one model. Code and models are available in https://github.com/0nutation/SpeechGPT. Demos are shown in https://0nutation.github.io/SpeechGPT.github.io/."", ""abstract"": ""Multi-modal large language models are regarded as a crucial step towards Artificial General Intelligence (AGI) and have garnered significant interest with the emergence of ChatGPT. However, current speech-language models typically adopt the cascade paradigm, preventing inter-modal knowledge transfer. In this paper, we propose SpeechGPT, a large language model with intrinsic cross-modal conversational abilities, capable of perceiving and generating multi-modal content. With discrete speech representations, we construct SpeechInstruct, the first large-scale cross-modal speech instruction dataset. Additionally, we employ a three-stage training strategy that includes modality-adaptation pre-training, cross-modal instruction fine-tuning, and chain-of-modality instruction fine-tuning. The experimental results demonstrate that SpeechGPT has an impressive capacity to follow cross-modal human instructions and highlight the potential of handling multiple modalities with one model. Code and models are available in https://github.com/0nutation/SpeechGPT. Demos are shown in https://0nutation.github.io/SpeechGPT.github.io/."", ""doi"": ""https://doi.org/10.18653/v1/2023.findings-emnlp.1055"", ""openalex_id"": ""https://openalex.org/W4389524500"", ""arxiv_id"": """", ""publication_date"": ""2023-01-01"", ""published"": ""2023-01-01"", ""source"": ""openalex_snowball""}",,https://openalex.org/W4389524500
10.18653/v1/2023.emnlp-main.513,Homophone Disambiguation Reveals Patterns of Context Mixing in Speech Transformers,"Transformers have become a key architecture in speech processing, but our understanding of how they build up representations of acoustic and linguistic structure is limited. In this study, we address this gap by investigating how measures of ‘context-mixing’ developed for text models can be adapted and applied to models of spoken language. We identify a linguistic phenomenon that is ideal for such a case study: homophony in French (e.g. livre vs livres), where a speech recognition model has to attend to syntactic cues such as determiners and pronouns in order to disambiguate spoken words with identical pronunciations and transcribe them while respecting grammatical agreement. We perform a series of controlled experiments and probing analyses on Transformer-based speech models. Our findings reveal that representations in encoder-only models effectively incorporate these cues to identify the correct transcription, whereas encoders in encoder-decoder models mainly relegate the task of capturing contextual dependencies to decoder modules.",1,,include (senior:5),,,2023,,,"{""title"": ""Homophone Disambiguation Reveals Patterns of Context Mixing in Speech Transformers"", ""summary"": ""Transformers have become a key architecture in speech processing, but our understanding of how they build up representations of acoustic and linguistic structure is limited. In this study, we address this gap by investigating how measures of ‘context-mixing’ developed for text models can be adapted and applied to models of spoken language. We identify a linguistic phenomenon that is ideal for such a case study: homophony in French (e.g. livre vs livres), where a speech recognition model has to attend to syntactic cues such as determiners and pronouns in order to disambiguate spoken words with identical pronunciations and transcribe them while respecting grammatical agreement. We perform a series of controlled experiments and probing analyses on Transformer-based speech models. Our findings reveal that representations in encoder-only models effectively incorporate these cues to identify the correct transcription, whereas encoders in encoder-decoder models mainly relegate the task of capturing contextual dependencies to decoder modules."", ""abstract"": ""Transformers have become a key architecture in speech processing, but our understanding of how they build up representations of acoustic and linguistic structure is limited. In this study, we address this gap by investigating how measures of ‘context-mixing’ developed for text models can be adapted and applied to models of spoken language. We identify a linguistic phenomenon that is ideal for such a case study: homophony in French (e.g. livre vs livres), where a speech recognition model has to attend to syntactic cues such as determiners and pronouns in order to disambiguate spoken words with identical pronunciations and transcribe them while respecting grammatical agreement. We perform a series of controlled experiments and probing analyses on Transformer-based speech models. Our findings reveal that representations in encoder-only models effectively incorporate these cues to identify the correct transcription, whereas encoders in encoder-decoder models mainly relegate the task of capturing contextual dependencies to decoder modules."", ""doi"": ""https://doi.org/10.18653/v1/2023.emnlp-main.513"", ""openalex_id"": ""https://openalex.org/W4389520784"", ""arxiv_id"": """", ""publication_date"": ""2023-01-01"", ""published"": ""2023-01-01"", ""source"": ""openalex_snowball""}",,https://openalex.org/W4389520784
10.32388/758n37,Benchmarking Open-ended Audio Dialogue Understanding for Large Audio-Language Models,"Large Audio-Language Models (LALMs) have unclocked audio dialogue capabilities, where audio dialogues are a direct exchange of spoken language between LALMs and humans. Recent advances, such as GPT-4o, have enabled LALMs in back-and-forth audio dialogues with humans. This progression not only underscores the potential of LALMs but also broadens their applicability across a wide range of practical scenarios supported by audio dialogues. However, given these advancements, a comprehensive benchmark to evaluate the performance of LALMs in the open-ended audio dialogue understanding remains absent currently. To address this gap, we propose an Audio Dialogue Understanding Benchmark (ADU-Bench), which consists of 4 benchmark datasets. They assess the open-ended audio dialogue ability for LALMs in 3 general scenarios, 12 skills, 9 multilingual languages, and 4 categories of ambiguity handling. Notably, we _firstly propose the evaluation of ambiguity handling in audio dialogues_ that expresses different intentions beyond the same literal meaning of sentences, e.g., “Really!?” with different intonations. In summary, ADU-Bench includes over 20,000 open-ended audio dialogues for the assessment of LALMs. Through extensive experiments conducted on 13 LALMs, our analysis reveals that there is still considerable room for improvement in the audio dialogue understanding abilities of existing LALMs. In particular, they struggle with mathematical symbols and formulas, understanding human behavior such as roleplay, comprehending multiple languages, and handling audio dialogue ambiguities from different phonetic elements, such as intonations, pause positions, and homophones.",1,,include (senior:5),,,2024,,,"{""title"": ""Benchmarking Open-ended Audio Dialogue Understanding for Large Audio-Language Models"", ""summary"": ""Large Audio-Language Models (LALMs) have unclocked audio dialogue capabilities, where audio dialogues are a direct exchange of spoken language between LALMs and humans. Recent advances, such as GPT-4o, have enabled LALMs in back-and-forth audio dialogues with humans. This progression not only underscores the potential of LALMs but also broadens their applicability across a wide range of practical scenarios supported by audio dialogues. However, given these advancements, a comprehensive benchmark to evaluate the performance of LALMs in the open-ended audio dialogue understanding remains absent currently. To address this gap, we propose an Audio Dialogue Understanding Benchmark (ADU-Bench), which consists of 4 benchmark datasets. They assess the open-ended audio dialogue ability for LALMs in 3 general scenarios, 12 skills, 9 multilingual languages, and 4 categories of ambiguity handling. Notably, we _firstly propose the evaluation of ambiguity handling in audio dialogues_ that expresses different intentions beyond the same literal meaning of sentences, e.g., “Really!?” with different intonations. In summary, ADU-Bench includes over 20,000 open-ended audio dialogues for the assessment of LALMs. Through extensive experiments conducted on 13 LALMs, our analysis reveals that there is still considerable room for improvement in the audio dialogue understanding abilities of existing LALMs. In particular, they struggle with mathematical symbols and formulas, understanding human behavior such as roleplay, comprehending multiple languages, and handling audio dialogue ambiguities from different phonetic elements, such as intonations, pause positions, and homophones."", ""abstract"": ""Large Audio-Language Models (LALMs) have unclocked audio dialogue capabilities, where audio dialogues are a direct exchange of spoken language between LALMs and humans. Recent advances, such as GPT-4o, have enabled LALMs in back-and-forth audio dialogues with humans. This progression not only underscores the potential of LALMs but also broadens their applicability across a wide range of practical scenarios supported by audio dialogues. However, given these advancements, a comprehensive benchmark to evaluate the performance of LALMs in the open-ended audio dialogue understanding remains absent currently. To address this gap, we propose an Audio Dialogue Understanding Benchmark (ADU-Bench), which consists of 4 benchmark datasets. They assess the open-ended audio dialogue ability for LALMs in 3 general scenarios, 12 skills, 9 multilingual languages, and 4 categories of ambiguity handling. Notably, we _firstly propose the evaluation of ambiguity handling in audio dialogues_ that expresses different intentions beyond the same literal meaning of sentences, e.g., “Really!?” with different intonations. In summary, ADU-Bench includes over 20,000 open-ended audio dialogues for the assessment of LALMs. Through extensive experiments conducted on 13 LALMs, our analysis reveals that there is still considerable room for improvement in the audio dialogue understanding abilities of existing LALMs. In particular, they struggle with mathematical symbols and formulas, understanding human behavior such as roleplay, comprehending multiple languages, and handling audio dialogue ambiguities from different phonetic elements, such as intonations, pause positions, and homophones."", ""doi"": ""https://doi.org/10.32388/758n37"", ""openalex_id"": ""https://openalex.org/W4405547956"", ""arxiv_id"": """", ""publication_date"": ""2024-12-18"", ""published"": ""2024-12-18"", ""source"": ""openalex_snowball""}",,https://openalex.org/W4405547956
10.1109/lsp.2023.3308474,LM-VC: Zero-Shot Voice Conversion via Speech Generation Based on Language Models,"Language model (LM) based audio generation frameworks, e.g., AudioLM, have recently achieved new state-of-the-art performance in zero-shot audio generation. In this paper, we explore the feasibility of LMs for <italic xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">zero-shot voice conversion</i> . An intuitive approach is to follow AudioLM – Tokenizing speech into semantic and acoustic tokens respectively by HuBERT and SoundStream, and converting source semantic tokens to target acoustic tokens conditioned on acoustic tokens of the target speaker. However, such an approach encounters several issues: 1) the linguistic content contained in semantic tokens may get dispersed during multi-layer modeling while the lengthy speech input in the voice conversion task makes contextual learning even harder; 2) the semantic tokens still contain speaker-related information, which may be leaked to the target speech, lowering the target speaker similarity; 3) the generation diversity in the sampling of the LM can lead to unexpected outcomes during inference, leading to unnatural pronunciation and speech quality degradation. To mitigate these problems, we propose <italic xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">LM-VC</i> , a two-stage language modeling approach that generates coarse acoustic tokens for recovering the source linguistic content and target speaker's timbre, and then reconstructs the fine for acoustic details as converted speech. Specifically, to enhance content preservation and facilitates better disentanglement, a masked prefix LM with a mask prediction strategy is used for coarse acoustic modeling. This model is encouraged to recover the masked content from the surrounding context and generate target speech based on the target speaker's utterance and corrupted semantic tokens. Besides, to further alleviate the sampling error in the generation, an external LM, which employs window attention to capture the local acoustic relations, is introduced to participate in the coarse acoustic modeling through shallow fusion. Finally, a prefix LM reconstructs fine acoustic tokens from the coarse and results in the converted speech. Experiments demonstrate that LM-VC outperforms competitive systems in speech naturalness and speaker similarity.",1,,include (senior:4),,,2023,,,"{""title"": ""LM-VC: Zero-Shot Voice Conversion via Speech Generation Based on Language Models"", ""summary"": ""Language model (LM) based audio generation frameworks, e.g., AudioLM, have recently achieved new state-of-the-art performance in zero-shot audio generation. In this paper, we explore the feasibility of LMs for <italic xmlns:mml=\""http://www.w3.org/1998/Math/MathML\"" xmlns:xlink=\""http://www.w3.org/1999/xlink\"">zero-shot voice conversion</i> . An intuitive approach is to follow AudioLM – Tokenizing speech into semantic and acoustic tokens respectively by HuBERT and SoundStream, and converting source semantic tokens to target acoustic tokens conditioned on acoustic tokens of the target speaker. However, such an approach encounters several issues: 1) the linguistic content contained in semantic tokens may get dispersed during multi-layer modeling while the lengthy speech input in the voice conversion task makes contextual learning even harder; 2) the semantic tokens still contain speaker-related information, which may be leaked to the target speech, lowering the target speaker similarity; 3) the generation diversity in the sampling of the LM can lead to unexpected outcomes during inference, leading to unnatural pronunciation and speech quality degradation. To mitigate these problems, we propose <italic xmlns:mml=\""http://www.w3.org/1998/Math/MathML\"" xmlns:xlink=\""http://www.w3.org/1999/xlink\"">LM-VC</i> , a two-stage language modeling approach that generates coarse acoustic tokens for recovering the source linguistic content and target speaker's timbre, and then reconstructs the fine for acoustic details as converted speech. Specifically, to enhance content preservation and facilitates better disentanglement, a masked prefix LM with a mask prediction strategy is used for coarse acoustic modeling. This model is encouraged to recover the masked content from the surrounding context and generate target speech based on the target speaker's utterance and corrupted semantic tokens. Besides, to further alleviate the sampling error in the generation, an external LM, which employs window attention to capture the local acoustic relations, is introduced to participate in the coarse acoustic modeling through shallow fusion. Finally, a prefix LM reconstructs fine acoustic tokens from the coarse and results in the converted speech. Experiments demonstrate that LM-VC outperforms competitive systems in speech naturalness and speaker similarity."", ""abstract"": ""Language model (LM) based audio generation frameworks, e.g., AudioLM, have recently achieved new state-of-the-art performance in zero-shot audio generation. In this paper, we explore the feasibility of LMs for <italic xmlns:mml=\""http://www.w3.org/1998/Math/MathML\"" xmlns:xlink=\""http://www.w3.org/1999/xlink\"">zero-shot voice conversion</i> . An intuitive approach is to follow AudioLM – Tokenizing speech into semantic and acoustic tokens respectively by HuBERT and SoundStream, and converting source semantic tokens to target acoustic tokens conditioned on acoustic tokens of the target speaker. However, such an approach encounters several issues: 1) the linguistic content contained in semantic tokens may get dispersed during multi-layer modeling while the lengthy speech input in the voice conversion task makes contextual learning even harder; 2) the semantic tokens still contain speaker-related information, which may be leaked to the target speech, lowering the target speaker similarity; 3) the generation diversity in the sampling of the LM can lead to unexpected outcomes during inference, leading to unnatural pronunciation and speech quality degradation. To mitigate these problems, we propose <italic xmlns:mml=\""http://www.w3.org/1998/Math/MathML\"" xmlns:xlink=\""http://www.w3.org/1999/xlink\"">LM-VC</i> , a two-stage language modeling approach that generates coarse acoustic tokens for recovering the source linguistic content and target speaker's timbre, and then reconstructs the fine for acoustic details as converted speech. Specifically, to enhance content preservation and facilitates better disentanglement, a masked prefix LM with a mask prediction strategy is used for coarse acoustic modeling. This model is encouraged to recover the masked content from the surrounding context and generate target speech based on the target speaker's utterance and corrupted semantic tokens. Besides, to further alleviate the sampling error in the generation, an external LM, which employs window attention to capture the local acoustic relations, is introduced to participate in the coarse acoustic modeling through shallow fusion. Finally, a prefix LM reconstructs fine acoustic tokens from the coarse and results in the converted speech. Experiments demonstrate that LM-VC outperforms competitive systems in speech naturalness and speaker similarity."", ""doi"": ""https://doi.org/10.1109/lsp.2023.3308474"", ""openalex_id"": ""https://openalex.org/W4386132131"", ""arxiv_id"": """", ""publication_date"": ""2023-01-01"", ""published"": ""2023-01-01"", ""source"": ""openalex_snowball""}",,https://openalex.org/W4386132131
