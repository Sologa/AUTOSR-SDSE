[
  {
    "title": "Extracting the meaning of a word: an artificial intelligence approach",
    "abstract": "Abstract This article presents a strategy to extract the meaning of words in different contexts, using classification algorithms such as kNN, WiSARD, and 1NN, combined with a robust language model. The main objective is to investigate how the term “archive” is used in journalistic articles and how this usage reflects the value placed on the work of archivists. To achieve this, texts published in the newspaper “A Tribuna” between 2003 and 2017 were analyzed. The adopted method involves the automatic classification of sentences containing the term “archive,” dividing them into eleven categories that represent different interpretations of the term. The research was conducted through a classification algorithm, trained to identify semantic patterns in the sentences. This is a textual data analysis extracted from a digital collection of a periodical, without the direct participation of human subjects. The results indicate that combining the language model with the neural network significantly improves classification performance, surpassing traditional methods in metrics such as precision and recall. Additionally, the analysis showed that the term “archive” is widely used in different contexts by journalists, revealing multiple meanings and highlighting the importance of archivists in the process of organizing and documenting records. The proposed approach shows potential for application in other domains, contributing to the automation of semantic inference and the classification of large volumes of textual data.",
    "metadata": {
      "title": "Extracting the meaning of a word: an artificial intelligence approach",
      "summary": "Abstract This article presents a strategy to extract the meaning of words in different contexts, using classification algorithms such as kNN, WiSARD, and 1NN, combined with a robust language model. The main objective is to investigate how the term “archive” is used in journalistic articles and how this usage reflects the value placed on the work of archivists. To achieve this, texts published in the newspaper “A Tribuna” between 2003 and 2017 were analyzed. The adopted method involves the automatic classification of sentences containing the term “archive,” dividing them into eleven categories that represent different interpretations of the term. The research was conducted through a classification algorithm, trained to identify semantic patterns in the sentences. This is a textual data analysis extracted from a digital collection of a periodical, without the direct participation of human subjects. The results indicate that combining the language model with the neural network significantly improves classification performance, surpassing traditional methods in metrics such as precision and recall. Additionally, the analysis showed that the term “archive” is widely used in different contexts by journalists, revealing multiple meanings and highlighting the importance of archivists in the process of organizing and documenting records. The proposed approach shows potential for application in other domains, contributing to the automation of semantic inference and the classification of large volumes of textual data.",
      "abstract": "Abstract This article presents a strategy to extract the meaning of words in different contexts, using classification algorithms such as kNN, WiSARD, and 1NN, combined with a robust language model. The main objective is to investigate how the term “archive” is used in journalistic articles and how this usage reflects the value placed on the work of archivists. To achieve this, texts published in the newspaper “A Tribuna” between 2003 and 2017 were analyzed. The adopted method involves the automatic classification of sentences containing the term “archive,” dividing them into eleven categories that represent different interpretations of the term. The research was conducted through a classification algorithm, trained to identify semantic patterns in the sentences. This is a textual data analysis extracted from a digital collection of a periodical, without the direct participation of human subjects. The results indicate that combining the language model with the neural network significantly improves classification performance, surpassing traditional methods in metrics such as precision and recall. Additionally, the analysis showed that the term “archive” is widely used in different contexts by journalists, revealing multiple meanings and highlighting the importance of archivists in the process of organizing and documenting records. The proposed approach shows potential for application in other domains, contributing to the automation of semantic inference and the classification of large volumes of textual data.",
      "doi": "https://doi.org/10.1590/2318-0889202537e2514829",
      "openalex_id": "https://openalex.org/W4415283286",
      "arxiv_id": "",
      "publication_date": "2025-01-01",
      "published": "2025-01-01",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The abstract centers on textual semantic analysis and word meaning extraction using classification algorithms, with no focus on Spoken Language Models or speech-based modeling, thus it fails the core inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The abstract centers on textual semantic analysis and word meaning extraction using classification algorithms, with no focus on Spoken Language Models or speech-based modeling, thus it fails the core inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The article focuses on extracting the meaning of words using classification algorithms on textual data without involving speech or spoken language modeling; it does not mention any spoken language models, speech language modeling, or full-duplex spoken interaction aspects, nor does it cover voice as a primary modeling object or related synchronization or interaction capabilities. Therefore, it does not meet the inclusion criteria, which require studies centered on spoken language models or equivalent concepts with explicit modeling of speech sequences and interaction.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The article focuses on extracting the meaning of words using classification algorithms on textual data without involving speech or spoken language modeling; it does not mention any spoken language models, speech language modeling, or full-duplex spoken interaction aspects, nor does it cover voice as a primary modeling object or related synchronization or interaction capabilities. Therefore, it does not meet the inclusion criteria, which require studies centered on spoken language models or equivalent concepts with explicit modeling of speech sequences and interaction.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Mizo Automatic Speech Recognition: Leveraging Wav2vec 2.0 and XLS-R for Enhanced Accuracy in Low-Resource Language Processing",
    "abstract": "This study introduces a Mizo Automatic Speech Recognition (ASR) approach by fine-tuning Wav2vec 2.0 and XLS-R models. The research presents the newly developed Mizo speech dataset, MiZonal v1.0 which significantly contributes to the advancement of low-resource language processing and plays a crucial role in preserving the Mizo language, thereby enhancing the training and assessment of speech models for this underrepresented language. It focuses on evaluating the effectiveness of these models in handling Mizo speech data, with particular emphasis on their performance in converting numerical numbers into Mizo cardinal words, which have a positive effect on the Word Error Rate (WER). The findings reveal that while the Wav2vec-Base-Mizo-Lus model achieved a WER of 16.59%, the XLS-R-300M-Mizo-Lus model outperformed it significantly, achieving a WER of 11.84% and setting a new benchmark for accuracy in the Mizo language. This shows the importance of using large multilingual speech recognition models and the cross-lingual abilities of models such as XLS-R are essential for ASR tasks in low-resource languages, leading to progress in Mizo speech technology and its applications.",
    "metadata": {
      "title": "Mizo Automatic Speech Recognition: Leveraging Wav2vec 2.0 and XLS-R for Enhanced Accuracy in Low-Resource Language Processing",
      "summary": "This study introduces a Mizo Automatic Speech Recognition (ASR) approach by fine-tuning Wav2vec 2.0 and XLS-R models. The research presents the newly developed Mizo speech dataset, MiZonal v1.0 which significantly contributes to the advancement of low-resource language processing and plays a crucial role in preserving the Mizo language, thereby enhancing the training and assessment of speech models for this underrepresented language. It focuses on evaluating the effectiveness of these models in handling Mizo speech data, with particular emphasis on their performance in converting numerical numbers into Mizo cardinal words, which have a positive effect on the Word Error Rate (WER). The findings reveal that while the Wav2vec-Base-Mizo-Lus model achieved a WER of 16.59%, the XLS-R-300M-Mizo-Lus model outperformed it significantly, achieving a WER of 11.84% and setting a new benchmark for accuracy in the Mizo language. This shows the importance of using large multilingual speech recognition models and the cross-lingual abilities of models such as XLS-R are essential for ASR tasks in low-resource languages, leading to progress in Mizo speech technology and its applications.",
      "abstract": "This study introduces a Mizo Automatic Speech Recognition (ASR) approach by fine-tuning Wav2vec 2.0 and XLS-R models. The research presents the newly developed Mizo speech dataset, MiZonal v1.0 which significantly contributes to the advancement of low-resource language processing and plays a crucial role in preserving the Mizo language, thereby enhancing the training and assessment of speech models for this underrepresented language. It focuses on evaluating the effectiveness of these models in handling Mizo speech data, with particular emphasis on their performance in converting numerical numbers into Mizo cardinal words, which have a positive effect on the Word Error Rate (WER). The findings reveal that while the Wav2vec-Base-Mizo-Lus model achieved a WER of 16.59%, the XLS-R-300M-Mizo-Lus model outperformed it significantly, achieving a WER of 11.84% and setting a new benchmark for accuracy in the Mizo language. This shows the importance of using large multilingual speech recognition models and the cross-lingual abilities of models such as XLS-R are essential for ASR tasks in low-resource languages, leading to progress in Mizo speech technology and its applications.",
      "doi": "https://doi.org/10.1145/3746063",
      "openalex_id": "https://openalex.org/W4411757614",
      "arxiv_id": "",
      "publication_date": "2025-06-28",
      "published": "2025-06-28",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "此研究主題為低資源語言的自動語音辨識（ASR）性能提升，利用 Wav2vec 2.0/XLS-R 微調與新數據集，但未以 spoken language model 或語音語言模型為核心進行語音建模、端到端/語音-文字混合架構的設計與評估，且無涉及同步互動、語音重疊處理等評估，因此符合排除條件。",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "此研究主題為低資源語言的自動語音辨識（ASR）性能提升，利用 Wav2vec 2.0/XLS-R 微調與新數據集，但未以 spoken language model 或語音語言模型為核心進行語音建模、端到端/語音-文字混合架構的設計與評估，且無涉及同步互動、語音重疊處理等評估，因此符合排除條件。",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on automatic speech recognition (ASR) for the Mizo language, applying Wav2vec 2.0 and XLS-R models to a low-resource language dataset and evaluates performance mainly through word error rate (WER). However, it does not discuss spoken language models as core modeling objects, nor does it cover full-duplex interaction, cognitive parallelism, synchronization, or multi-dimensional behavioral arbitration; it is confined to a single downstream task (ASR) and focuses solely on performance metrics typical for ASR, lacking broader spoken language modeling or synchronization interaction aspects required by the inclusion criteria. Therefore, it fails to meet the inclusion criteria and matches several exclusion points.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on automatic speech recognition (ASR) for the Mizo language, applying Wav2vec 2.0 and XLS-R models to a low-resource language dataset and evaluates performance mainly through word error rate (WER). However, it does not discuss spoken language models as core modeling objects, nor does it cover full-duplex interaction, cognitive parallelism, synchronization, or multi-dimensional behavioral arbitration; it is confined to a single downstream task (ASR) and focuses solely on performance metrics typical for ASR, lacking broader spoken language modeling or synchronization interaction aspects required by the inclusion criteria. Therefore, it fails to meet the inclusion criteria and matches several exclusion points.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Multimodal Large Language Models in Health Care: Applications, Challenges, and Future Outlook",
    "abstract": "In the complex and multidimensional field of medicine, multimodal data are prevalent and crucial for informed clinical decisions. Multimodal data span a broad spectrum of data types, including medical images (eg, MRI and CT scans), time-series data (eg, sensor data from wearable devices and electronic health records), audio recordings (eg, heart and respiratory sounds and patient interviews), text (eg, clinical notes and research articles), videos (eg, surgical procedures), and omics data (eg, genomics and proteomics). While advancements in large language models (LLMs) have enabled new applications for knowledge retrieval and processing in the medical field, most LLMs remain limited to processing unimodal data, typically text-based content, and often overlook the importance of integrating the diverse data modalities encountered in clinical practice. This paper aims to present a detailed, practical, and solution-oriented perspective on the use of multimodal LLMs (M-LLMs) in the medical field. Our investigation spanned M-LLM foundational principles, current and potential applications, technical and ethical challenges, and future research directions. By connecting these elements, we aimed to provide a comprehensive framework that links diverse aspects of M-LLMs, offering a unified vision for their future in health care. This approach aims to guide both future research and practical implementations of M-LLMs in health care, positioning them as a paradigm shift toward integrated, multimodal data–driven medical practice. We anticipate that this work will spark further discussion and inspire the development of innovative approaches in the next generation of medical M-LLM systems.",
    "metadata": {
      "title": "Multimodal Large Language Models in Health Care: Applications, Challenges, and Future Outlook",
      "summary": "In the complex and multidimensional field of medicine, multimodal data are prevalent and crucial for informed clinical decisions. Multimodal data span a broad spectrum of data types, including medical images (eg, MRI and CT scans), time-series data (eg, sensor data from wearable devices and electronic health records), audio recordings (eg, heart and respiratory sounds and patient interviews), text (eg, clinical notes and research articles), videos (eg, surgical procedures), and omics data (eg, genomics and proteomics). While advancements in large language models (LLMs) have enabled new applications for knowledge retrieval and processing in the medical field, most LLMs remain limited to processing unimodal data, typically text-based content, and often overlook the importance of integrating the diverse data modalities encountered in clinical practice. This paper aims to present a detailed, practical, and solution-oriented perspective on the use of multimodal LLMs (M-LLMs) in the medical field. Our investigation spanned M-LLM foundational principles, current and potential applications, technical and ethical challenges, and future research directions. By connecting these elements, we aimed to provide a comprehensive framework that links diverse aspects of M-LLMs, offering a unified vision for their future in health care. This approach aims to guide both future research and practical implementations of M-LLMs in health care, positioning them as a paradigm shift toward integrated, multimodal data–driven medical practice. We anticipate that this work will spark further discussion and inspire the development of innovative approaches in the next generation of medical M-LLM systems.",
      "abstract": "In the complex and multidimensional field of medicine, multimodal data are prevalent and crucial for informed clinical decisions. Multimodal data span a broad spectrum of data types, including medical images (eg, MRI and CT scans), time-series data (eg, sensor data from wearable devices and electronic health records), audio recordings (eg, heart and respiratory sounds and patient interviews), text (eg, clinical notes and research articles), videos (eg, surgical procedures), and omics data (eg, genomics and proteomics). While advancements in large language models (LLMs) have enabled new applications for knowledge retrieval and processing in the medical field, most LLMs remain limited to processing unimodal data, typically text-based content, and often overlook the importance of integrating the diverse data modalities encountered in clinical practice. This paper aims to present a detailed, practical, and solution-oriented perspective on the use of multimodal LLMs (M-LLMs) in the medical field. Our investigation spanned M-LLM foundational principles, current and potential applications, technical and ethical challenges, and future research directions. By connecting these elements, we aimed to provide a comprehensive framework that links diverse aspects of M-LLMs, offering a unified vision for their future in health care. This approach aims to guide both future research and practical implementations of M-LLMs in health care, positioning them as a paradigm shift toward integrated, multimodal data–driven medical practice. We anticipate that this work will spark further discussion and inspire the development of innovative approaches in the next generation of medical M-LLM systems.",
      "doi": "https://doi.org/10.2196/59505",
      "openalex_id": "https://openalex.org/W4402155831",
      "arxiv_id": "",
      "publication_date": "2024-08-20",
      "published": "2024-08-20",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item focuses on multimodal LLMs in healthcare and is not centered on Spoken Language Models (SLMs) or Full-Duplex SLMs; it lacks emphasis on speech-centric modeling, training, or evaluation and does not address synchronous spoken interaction or speech-specific benchmarks, thus not meeting the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item focuses on multimodal LLMs in healthcare and is not centered on Spoken Language Models (SLMs) or Full-Duplex SLMs; it lacks emphasis on speech-centric modeling, training, or evaluation and does not address synchronous spoken interaction or speech-specific benchmarks, thus not meeting the inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper focuses on multimodal large language models in healthcare, covering various data modalities, including text and images, but it does not specifically discuss spoken language models, speech language modeling, or full-duplex spoken interaction, nor does it focus on speech as the core modeling object or detailed real-time interactive behaviors such as cognitive parallelism or backchanneling. The study mostly discusses multimodal LLMs in a broad medical context without aligning with the narrowly defined inclusion criteria related to spoken language models or associated architectures and evaluations; therefore, it does not meet the key inclusion criteria and aligns with exclusion points related to lacking spoken language modeling focus and interaction behavior arbitration relevant to full-duplex spoken language models.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper focuses on multimodal large language models in healthcare, covering various data modalities, including text and images, but it does not specifically discuss spoken language models, speech language modeling, or full-duplex spoken interaction, nor does it focus on speech as the core modeling object or detailed real-time interactive behaviors such as cognitive parallelism or backchanneling. The study mostly discusses multimodal LLMs in a broad medical context without aligning with the narrowly defined inclusion criteria related to spoken language models or associated architectures and evaluations; therefore, it does not meet the key inclusion criteria and aligns with exclusion points related to lacking spoken language modeling focus and interaction behavior arbitration relevant to full-duplex spoken language models.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Talk2Care: An LLM-based Voice Assistant for Communication between Healthcare Providers and Older Adults",
    "abstract": "Despite the plethora of telehealth applications to assist home-based older adults and healthcare providers, basic messaging and phone calls are still the most common communication methods, which suffer from limited availability, information loss, and process inefficiencies. One promising solution to facilitate patient-provider communication is to leverage large language models (LLMs) with their powerful natural conversation and summarization capability. However, there is a limited understanding of LLMs' role during the communication. We first conducted two interview studies with both older adults (N=10) and healthcare providers (N=9) to understand their needs and opportunities for LLMs in patient-provider asynchronous communication. Based on the insights, we built an LLM-powered communication system, Talk2Care, and designed interactive components for both groups: (1) For older adults, we leveraged the convenience and accessibility of voice assistants (VAs) and built an LLM-powered conversational interface for effective information collection. (2) For health providers, we built an LLM-based dashboard to summarize and present important health information based on older adults' conversations with the VA. We further conducted two user studies with older adults and providers to evaluate the usability of the system. The results showed that Talk2Care could facilitate the communication process, enrich the health information collected from older adults, and considerably save providers' efforts and time. We envision our work as an initial exploration of LLMs' capability in the intersection of healthcare and interpersonal communication.",
    "metadata": {
      "title": "Talk2Care: An LLM-based Voice Assistant for Communication between Healthcare Providers and Older Adults",
      "summary": "Despite the plethora of telehealth applications to assist home-based older adults and healthcare providers, basic messaging and phone calls are still the most common communication methods, which suffer from limited availability, information loss, and process inefficiencies. One promising solution to facilitate patient-provider communication is to leverage large language models (LLMs) with their powerful natural conversation and summarization capability. However, there is a limited understanding of LLMs' role during the communication. We first conducted two interview studies with both older adults (N=10) and healthcare providers (N=9) to understand their needs and opportunities for LLMs in patient-provider asynchronous communication. Based on the insights, we built an LLM-powered communication system, Talk2Care, and designed interactive components for both groups: (1) For older adults, we leveraged the convenience and accessibility of voice assistants (VAs) and built an LLM-powered conversational interface for effective information collection. (2) For health providers, we built an LLM-based dashboard to summarize and present important health information based on older adults' conversations with the VA. We further conducted two user studies with older adults and providers to evaluate the usability of the system. The results showed that Talk2Care could facilitate the communication process, enrich the health information collected from older adults, and considerably save providers' efforts and time. We envision our work as an initial exploration of LLMs' capability in the intersection of healthcare and interpersonal communication.",
      "abstract": "Despite the plethora of telehealth applications to assist home-based older adults and healthcare providers, basic messaging and phone calls are still the most common communication methods, which suffer from limited availability, information loss, and process inefficiencies. One promising solution to facilitate patient-provider communication is to leverage large language models (LLMs) with their powerful natural conversation and summarization capability. However, there is a limited understanding of LLMs' role during the communication. We first conducted two interview studies with both older adults (N=10) and healthcare providers (N=9) to understand their needs and opportunities for LLMs in patient-provider asynchronous communication. Based on the insights, we built an LLM-powered communication system, Talk2Care, and designed interactive components for both groups: (1) For older adults, we leveraged the convenience and accessibility of voice assistants (VAs) and built an LLM-powered conversational interface for effective information collection. (2) For health providers, we built an LLM-based dashboard to summarize and present important health information based on older adults' conversations with the VA. We further conducted two user studies with older adults and providers to evaluate the usability of the system. The results showed that Talk2Care could facilitate the communication process, enrich the health information collected from older adults, and considerably save providers' efforts and time. We envision our work as an initial exploration of LLMs' capability in the intersection of healthcare and interpersonal communication.",
      "doi": "https://doi.org/10.1145/3659625",
      "openalex_id": "https://openalex.org/W4396919238",
      "arxiv_id": "",
      "publication_date": "2024-05-13",
      "published": "2024-05-13",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item centers on an LLM-based voice assistant for healthcare communication and usability evaluation, but it does not explicitly focus on Spoken Language Models or full-duplex speech-language modeling, nor on architectural/training/evaluation aspects specific to SLM/FD-SLM; thus it likely does not meet the core inclusion criteria.",
      "evaluation": 2
    },
    "round-A_JuniorNano_reasoning": "The item centers on an LLM-based voice assistant for healthcare communication and usability evaluation, but it does not explicitly focus on Spoken Language Models or full-duplex speech-language modeling, nor on architectural/training/evaluation aspects specific to SLM/FD-SLM; thus it likely does not meet the core inclusion criteria.",
    "round-A_JuniorNano_evaluation": 2,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on an LLM-based voice assistant for healthcare communication, emphasizing conversational interfaces and summarization but does not explicitly discuss spoken language models as core modeling objects, nor does it mention full-duplex spoken language modeling, low-latency interaction, or synchronization aspects critical to inclusion criteria; it primarily treats speech as an input modality rather than the central modeling object, lacking detailed architecture or evaluation in speech language modeling terms, thus it doesn't meet all inclusion criteria and matches exclusion conditions about focusing on ASR and NLP without deep speech modeling innovation.",
      "evaluation": 2
    },
    "round-A_JuniorMini_reasoning": "The study focuses on an LLM-based voice assistant for healthcare communication, emphasizing conversational interfaces and summarization but does not explicitly discuss spoken language models as core modeling objects, nor does it mention full-duplex spoken language modeling, low-latency interaction, or synchronization aspects critical to inclusion criteria; it primarily treats speech as an input modality rather than the central modeling object, lacking detailed architecture or evaluation in speech language modeling terms, thus it doesn't meet all inclusion criteria and matches exclusion conditions about focusing on ASR and NLP without deep speech modeling innovation.",
    "round-A_JuniorMini_evaluation": 2,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:2)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "The Implementation of Multimodal Large Language Models for Hydrological Applications: A Comparative Study of GPT-4 Vision, Gemini, LLaVa, and Multimodal-GPT",
    "abstract": "Large Language Models (LLMs) combined with visual foundation models have demonstrated significant advancements, achieving intelligence levels comparable to human capabilities. This study analyzes the latest Multimodal LLMs (MLLMs), including Multimodal-GPT, GPT-4 Vision, Gemini, and LLaVa, with a focus on hydrological applications such as flood management, water level monitoring, agricultural water discharge, and water pollution management. We evaluated these MLLMs on hydrology-specific tasks, testing their response generation and real-time suitability in complex real-world scenarios. Prompts were designed to enhance the models’ visual inference capabilities and contextual comprehension from images. Our findings reveal that GPT-4 Vision demonstrated exceptional proficiency in interpreting visual data, providing accurate assessments of flood severity and water quality. Additionally, MLLMs showed potential in various hydrological applications, including drought prediction, streamflow forecasting, groundwater management, and wetland conservation. These models can optimize water resource management by predicting rainfall, evaporation rates, and soil moisture levels, thereby promoting sustainable agricultural practices. This research provides valuable insights into the potential applications of advanced AI models in addressing complex hydrological challenges and improving real-time decision-making in water resource management",
    "metadata": {
      "title": "The Implementation of Multimodal Large Language Models for Hydrological Applications: A Comparative Study of GPT-4 Vision, Gemini, LLaVa, and Multimodal-GPT",
      "summary": "Large Language Models (LLMs) combined with visual foundation models have demonstrated significant advancements, achieving intelligence levels comparable to human capabilities. This study analyzes the latest Multimodal LLMs (MLLMs), including Multimodal-GPT, GPT-4 Vision, Gemini, and LLaVa, with a focus on hydrological applications such as flood management, water level monitoring, agricultural water discharge, and water pollution management. We evaluated these MLLMs on hydrology-specific tasks, testing their response generation and real-time suitability in complex real-world scenarios. Prompts were designed to enhance the models’ visual inference capabilities and contextual comprehension from images. Our findings reveal that GPT-4 Vision demonstrated exceptional proficiency in interpreting visual data, providing accurate assessments of flood severity and water quality. Additionally, MLLMs showed potential in various hydrological applications, including drought prediction, streamflow forecasting, groundwater management, and wetland conservation. These models can optimize water resource management by predicting rainfall, evaporation rates, and soil moisture levels, thereby promoting sustainable agricultural practices. This research provides valuable insights into the potential applications of advanced AI models in addressing complex hydrological challenges and improving real-time decision-making in water resource management",
      "abstract": "Large Language Models (LLMs) combined with visual foundation models have demonstrated significant advancements, achieving intelligence levels comparable to human capabilities. This study analyzes the latest Multimodal LLMs (MLLMs), including Multimodal-GPT, GPT-4 Vision, Gemini, and LLaVa, with a focus on hydrological applications such as flood management, water level monitoring, agricultural water discharge, and water pollution management. We evaluated these MLLMs on hydrology-specific tasks, testing their response generation and real-time suitability in complex real-world scenarios. Prompts were designed to enhance the models’ visual inference capabilities and contextual comprehension from images. Our findings reveal that GPT-4 Vision demonstrated exceptional proficiency in interpreting visual data, providing accurate assessments of flood severity and water quality. Additionally, MLLMs showed potential in various hydrological applications, including drought prediction, streamflow forecasting, groundwater management, and wetland conservation. These models can optimize water resource management by predicting rainfall, evaporation rates, and soil moisture levels, thereby promoting sustainable agricultural practices. This research provides valuable insights into the potential applications of advanced AI models in addressing complex hydrological challenges and improving real-time decision-making in water resource management",
      "doi": "https://doi.org/10.3390/hydrology11090148",
      "openalex_id": "https://openalex.org/W4402448026",
      "arxiv_id": "",
      "publication_date": "2024-09-11",
      "published": "2024-09-11",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item focuses on multimodal vision-language models (GPT-4 Vision, Gemini, LLaVa) applied to hydrology, not on Spoken Language Models or speech-language modeling, and it does not address speech turn-taking, synchronization, or speech-centric evaluation; thus it does not meet the inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item focuses on multimodal vision-language models (GPT-4 Vision, Gemini, LLaVa) applied to hydrology, not on Spoken Language Models or speech-language modeling, and it does not address speech turn-taking, synchronization, or speech-centric evaluation; thus it does not meet the inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on multimodal large language models applied to hydrological tasks, without any mention of Spoken Language Models, speech-level language modeling, full-duplex interaction, or synchrony in speech-based interaction; it does not address speech as the primary modeling target nor discuss architecture or evaluation for speech language modeling, thus not meeting the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on multimodal large language models applied to hydrological tasks, without any mention of Spoken Language Models, speech-level language modeling, full-duplex interaction, or synchrony in speech-based interaction; it does not address speech as the primary modeling target nor discuss architecture or evaluation for speech language modeling, thus not meeting the inclusion criteria.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Connecting Speech Encoder and Large Language Model for ASR",
    "abstract": "The impressive capability and versatility of large language models (LLMs) have aroused increasing attention in automatic speech recognition (ASR), with several pioneering studies attempting to build integrated ASR models by connecting a speech encoder with an LLM. This paper presents a comparative study of three commonly used structures as connectors, including fully connected layers, multi-head cross-attention, and Q-Former. Speech encoders from the Whisper model series as well as LLMs from the Vicuna model series with different model sizes were studied. Experiments were performed on the commonly used LibriSpeech, Common Voice, and GigaSpeech datasets, where the LLMs with Q-Formers demonstrated consistent and considerable word error rate (WER) reductions over LLMs with other connector structures. Q-Former-based LLMs can generalise well to out-of-domain datasets, where 12% relative WER reductions over the Whisper baseline ASR model were achieved on the Eval2000 test set without using any in-domain training data from Switchboard. Moreover, a novel segment-level Q-Former is proposed to enable LLMs to recognise speech segments with a duration exceeding the limitation of the encoders, which results in 17% relative WER reductions over other connector structures on 90-second-long speech data.",
    "metadata": {
      "title": "Connecting Speech Encoder and Large Language Model for ASR",
      "summary": "The impressive capability and versatility of large language models (LLMs) have aroused increasing attention in automatic speech recognition (ASR), with several pioneering studies attempting to build integrated ASR models by connecting a speech encoder with an LLM. This paper presents a comparative study of three commonly used structures as connectors, including fully connected layers, multi-head cross-attention, and Q-Former. Speech encoders from the Whisper model series as well as LLMs from the Vicuna model series with different model sizes were studied. Experiments were performed on the commonly used LibriSpeech, Common Voice, and GigaSpeech datasets, where the LLMs with Q-Formers demonstrated consistent and considerable word error rate (WER) reductions over LLMs with other connector structures. Q-Former-based LLMs can generalise well to out-of-domain datasets, where 12% relative WER reductions over the Whisper baseline ASR model were achieved on the Eval2000 test set without using any in-domain training data from Switchboard. Moreover, a novel segment-level Q-Former is proposed to enable LLMs to recognise speech segments with a duration exceeding the limitation of the encoders, which results in 17% relative WER reductions over other connector structures on 90-second-long speech data.",
      "abstract": "The impressive capability and versatility of large language models (LLMs) have aroused increasing attention in automatic speech recognition (ASR), with several pioneering studies attempting to build integrated ASR models by connecting a speech encoder with an LLM. This paper presents a comparative study of three commonly used structures as connectors, including fully connected layers, multi-head cross-attention, and Q-Former. Speech encoders from the Whisper model series as well as LLMs from the Vicuna model series with different model sizes were studied. Experiments were performed on the commonly used LibriSpeech, Common Voice, and GigaSpeech datasets, where the LLMs with Q-Formers demonstrated consistent and considerable word error rate (WER) reductions over LLMs with other connector structures. Q-Former-based LLMs can generalise well to out-of-domain datasets, where 12% relative WER reductions over the Whisper baseline ASR model were achieved on the Eval2000 test set without using any in-domain training data from Switchboard. Moreover, a novel segment-level Q-Former is proposed to enable LLMs to recognise speech segments with a duration exceeding the limitation of the encoders, which results in 17% relative WER reductions over other connector structures on 90-second-long speech data.",
      "doi": "https://doi.org/10.1109/icassp48485.2024.10445874",
      "openalex_id": "https://openalex.org/W4392931626",
      "arxiv_id": "",
      "publication_date": "2024-03-18",
      "published": "2024-03-18",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This study explicitly analyzes a speech-encoder to LLM integration for ASR, comparing connector architectures and evaluating on multiple speech datasets, aligning with the Spoken Language Models / speech-language modeling criteria within the last 3 years.",
      "evaluation": 5
    },
    "round-A_JuniorNano_reasoning": "This study explicitly analyzes a speech-encoder to LLM integration for ASR, comparing connector architectures and evaluating on multiple speech datasets, aligning with the Spoken Language Models / speech-language modeling criteria within the last 3 years.",
    "round-A_JuniorNano_evaluation": 5,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on connecting speech encoders with large language models for automatic speech recognition (ASR), emphasizing word error rate improvements via different connector structures. However, it primarily addresses ASR performance rather than spoken language models or full-duplex spoken interaction modeling, lacks discussion of cognitive parallelism, behavioral arbitration, or low-latency real-time interaction aspects, and does not describe speech language modeling as core but rather as an input to LLMs. It also does not mention spoken language model terminology or comprehensive evaluation beyond WER, thus failing to meet the inclusion criteria and aligning with several exclusion criteria.",
      "evaluation": 2
    },
    "round-A_JuniorMini_reasoning": "The study focuses on connecting speech encoders with large language models for automatic speech recognition (ASR), emphasizing word error rate improvements via different connector structures. However, it primarily addresses ASR performance rather than spoken language models or full-duplex spoken interaction modeling, lacks discussion of cognitive parallelism, behavioral arbitration, or low-latency real-time interaction aspects, and does not describe speech language modeling as core but rather as an input to LLMs. It also does not mention spoken language model terminology or comprehensive evaluation beyond WER, thus failing to meet the inclusion criteria and aligning with several exclusion criteria.",
    "round-A_JuniorMini_evaluation": 2,
    "round-B_SeniorLead_output": {
      "reasoning": "Junior reviewers disagreed (JuniorNano: include; JuniorMini: exclude). After review I side with JuniorMini: the paper focuses on connector architectures for ASR and reports WER gains using speech encoders + LLMs, but it does not frame itself as a Spoken Language Model nor address speech-as-primary language modeling, full-duplex interaction, cognitive parallelism, or multi-dimensional interaction evaluation; therefore it fails the inclusion requirements and meets multiple exclusion criteria. Explanation (1 sentence): This is an ASR-focused study improving WER via speech-encoder-to-LLM connectors rather than research on Spoken Language Models or full-duplex spoken interaction, so it should be excluded.",
      "evaluation": 1
    },
    "round-B_SeniorLead_reasoning": "Junior reviewers disagreed (JuniorNano: include; JuniorMini: exclude). After review I side with JuniorMini: the paper focuses on connector architectures for ASR and reports WER gains using speech encoders + LLMs, but it does not frame itself as a Spoken Language Model nor address speech-as-primary language modeling, full-duplex interaction, cognitive parallelism, or multi-dimensional interaction evaluation; therefore it fails the inclusion requirements and meets multiple exclusion criteria. Explanation (1 sentence): This is an ASR-focused study improving WER via speech-encoder-to-LLM connectors rather than research on Spoken Language Models or full-duplex spoken interaction, so it should be excluded.",
    "round-B_SeniorLead_evaluation": 1,
    "final_verdict": "exclude (senior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "ML-LMCL: Mutual Learning and Large-Margin Contrastive Learning for Improving ASR Robustness in Spoken Language Understanding",
    "abstract": "Spoken language understanding (SLU) is a fundamental task in the task-oriented dialogue systems. However, the inevitable errors from automatic speech recognition (ASR) usually impair the understanding performance and lead to error propagation. Although there are some attempts to address this problem through contrastive learning, they (1) treat clean manual transcripts and ASR transcripts equally without discrimination in fine-tuning; (2) neglect the fact that the semantically similar pairs are still pushed away when applying contrastive learning; (3) suffer from the problem of Kullback–Leibler (KL) vanishing. In this paper, we propose Mutual Learning and Large-Margin Contrastive Learning (ML-LMCL), a novel framework for improving ASR robustness in SLU. Specifically, in fine-tuning, we apply mutual learning and train two SLU models on the manual transcripts and the ASR transcripts, respectively, aiming to iteratively share knowledge between these two models. We also introduce a distance polarization regularizer to avoid pushing away the intra-cluster pairs as much as possible. Moreover, we use a cyclical annealing schedule to mitigate KL vanishing issue. Experiments on three datasets show that ML-LMCL outperforms existing models and achieves new state-of-the-art performance.",
    "metadata": {
      "title": "ML-LMCL: Mutual Learning and Large-Margin Contrastive Learning for Improving ASR Robustness in Spoken Language Understanding",
      "summary": "Spoken language understanding (SLU) is a fundamental task in the task-oriented dialogue systems. However, the inevitable errors from automatic speech recognition (ASR) usually impair the understanding performance and lead to error propagation. Although there are some attempts to address this problem through contrastive learning, they (1) treat clean manual transcripts and ASR transcripts equally without discrimination in fine-tuning; (2) neglect the fact that the semantically similar pairs are still pushed away when applying contrastive learning; (3) suffer from the problem of Kullback–Leibler (KL) vanishing. In this paper, we propose Mutual Learning and Large-Margin Contrastive Learning (ML-LMCL), a novel framework for improving ASR robustness in SLU. Specifically, in fine-tuning, we apply mutual learning and train two SLU models on the manual transcripts and the ASR transcripts, respectively, aiming to iteratively share knowledge between these two models. We also introduce a distance polarization regularizer to avoid pushing away the intra-cluster pairs as much as possible. Moreover, we use a cyclical annealing schedule to mitigate KL vanishing issue. Experiments on three datasets show that ML-LMCL outperforms existing models and achieves new state-of-the-art performance.",
      "abstract": "Spoken language understanding (SLU) is a fundamental task in the task-oriented dialogue systems. However, the inevitable errors from automatic speech recognition (ASR) usually impair the understanding performance and lead to error propagation. Although there are some attempts to address this problem through contrastive learning, they (1) treat clean manual transcripts and ASR transcripts equally without discrimination in fine-tuning; (2) neglect the fact that the semantically similar pairs are still pushed away when applying contrastive learning; (3) suffer from the problem of Kullback–Leibler (KL) vanishing. In this paper, we propose Mutual Learning and Large-Margin Contrastive Learning (ML-LMCL), a novel framework for improving ASR robustness in SLU. Specifically, in fine-tuning, we apply mutual learning and train two SLU models on the manual transcripts and the ASR transcripts, respectively, aiming to iteratively share knowledge between these two models. We also introduce a distance polarization regularizer to avoid pushing away the intra-cluster pairs as much as possible. Moreover, we use a cyclical annealing schedule to mitigate KL vanishing issue. Experiments on three datasets show that ML-LMCL outperforms existing models and achieves new state-of-the-art performance.",
      "doi": "https://doi.org/10.18653/v1/2023.findings-acl.406",
      "openalex_id": "https://openalex.org/W4385572615",
      "arxiv_id": "",
      "publication_date": "2023-01-01",
      "published": "2023-01-01",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "本研究以 Spoken Language Understanding 為核心任務，探討在 ASR 與手動轉錄之間的互學與大間距對比學習，聚焦模型架構、訓練與評估，且屬於近3年內的研究，符合以語音為主要建模與推理對象的範疇與評估面向，並非僅限單向語音處理或單一語音技術的排除情形，因此建議納入。",
      "evaluation": 4
    },
    "round-A_JuniorNano_reasoning": "本研究以 Spoken Language Understanding 為核心任務，探討在 ASR 與手動轉錄之間的互學與大間距對比學習，聚焦模型架構、訓練與評估，且屬於近3年內的研究，符合以語音為主要建模與推理對象的範疇與評估面向，並非僅限單向語音處理或單一語音技術的排除情形，因此建議納入。",
    "round-A_JuniorNano_evaluation": 4,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper focuses on improving ASR robustness in spoken language understanding using mutual learning and contrastive learning techniques, which primarily address ASR transcript errors and model fine-tuning; however, it does not explicitly mention or focus on spoken language models or full-duplex spoken language models as core concepts, nor does it touch on synchronized or real-time interactive dialogue behaviors, cognitive parallelism, or the specific aspects of full-duplex architectures stated in the inclusion criteria. The research centers on SLU with ASR error mitigation rather than on spoken language modeling of speech signals as core modeling and reasoning objects. Hence, it does not fully meet the inclusion criteria and partially overlaps with the exclusion criteria focusing on single-task downstream ASR-related improvements without broader spoken language modeling or synchronization aspects.",
      "evaluation": 2
    },
    "round-A_JuniorMini_reasoning": "The paper focuses on improving ASR robustness in spoken language understanding using mutual learning and contrastive learning techniques, which primarily address ASR transcript errors and model fine-tuning; however, it does not explicitly mention or focus on spoken language models or full-duplex spoken language models as core concepts, nor does it touch on synchronized or real-time interactive dialogue behaviors, cognitive parallelism, or the specific aspects of full-duplex architectures stated in the inclusion criteria. The research centers on SLU with ASR error mitigation rather than on spoken language modeling of speech signals as core modeling and reasoning objects. Hence, it does not fully meet the inclusion criteria and partially overlaps with the exclusion criteria focusing on single-task downstream ASR-related improvements without broader spoken language modeling or synchronization aspects.",
    "round-A_JuniorMini_evaluation": 2,
    "round-B_SeniorLead_output": {
      "reasoning": "Junior reviewers disagreed: JuniorNano recommended inclusion (4) noting the paper studies SLU robustness with mutual learning between ASR and manual transcripts, while JuniorMini recommended exclusion (2) arguing the work does not center on Spoken Language Models or full‑duplex interaction. After review, the paper does NOT meet the inclusion criteria because its core is ASR-robust SLU (fine‑tuning on transcripts and contrastive learning) rather than research that treats speech/audio as the primary modeling/inference object (no mention of spoken language models, speech‑level language modeling, Speech–LLM architectures, or full‑duplex/synchronization capabilities), and it focuses on a single downstream task rather than general SLM design or evaluation—therefore it should be excluded. One-sentence explanation: The study targets ASR-robust SLU using transcript-based training rather than advancing Spoken Language Models or full‑duplex speech modeling, so it fails the inclusion requirements.",
      "evaluation": 2
    },
    "round-B_SeniorLead_reasoning": "Junior reviewers disagreed: JuniorNano recommended inclusion (4) noting the paper studies SLU robustness with mutual learning between ASR and manual transcripts, while JuniorMini recommended exclusion (2) arguing the work does not center on Spoken Language Models or full‑duplex interaction. After review, the paper does NOT meet the inclusion criteria because its core is ASR-robust SLU (fine‑tuning on transcripts and contrastive learning) rather than research that treats speech/audio as the primary modeling/inference object (no mention of spoken language models, speech‑level language modeling, Speech–LLM architectures, or full‑duplex/synchronization capabilities), and it focuses on a single downstream task rather than general SLM design or evaluation—therefore it should be excluded. One-sentence explanation: The study targets ASR-robust SLU using transcript-based training rather than advancing Spoken Language Models or full‑duplex speech modeling, so it fails the inclusion requirements.",
    "round-B_SeniorLead_evaluation": 2,
    "final_verdict": "exclude (senior:2)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "SLM: Bridge the Thin Gap Between Speech and Text Foundation Models",
    "abstract": "We present a joint Speech and Language Model (SLM), a multitask, multilingual, and dual-modal model that takes advantage of pretrained foundational speech and language models. SLM freezes the pretrained foundation models to maximally preserves their capabilities, and only trains a simple adapter with just 1% (156M) of the foundation models' parameters. This adaptation not only leads SLM to achieve strong performance on conventional tasks such as automatic speech recognition (ASR) and automatic speech translation (AST), but also unlocks the novel capability of zero-shot instruction-following for more diverse tasks. Given a speech input and a text instruction, SLM is able to perform unseen generation tasks including contextual biasing ASR using real-time context, dialog generation, speech continuation, and question answering. Our approach demonstrates that the representational gap between pretrained speech and language models is narrower than one would expect, and can be bridged by a simple adaptation mechanism. As a result, SLM is not only efficient to train, but also inherits strong capabilities already present in foundation models of different modalities.",
    "metadata": {
      "title": "SLM: Bridge the Thin Gap Between Speech and Text Foundation Models",
      "summary": "We present a joint Speech and Language Model (SLM), a multitask, multilingual, and dual-modal model that takes advantage of pretrained foundational speech and language models. SLM freezes the pretrained foundation models to maximally preserves their capabilities, and only trains a simple adapter with just 1% (156M) of the foundation models' parameters. This adaptation not only leads SLM to achieve strong performance on conventional tasks such as automatic speech recognition (ASR) and automatic speech translation (AST), but also unlocks the novel capability of zero-shot instruction-following for more diverse tasks. Given a speech input and a text instruction, SLM is able to perform unseen generation tasks including contextual biasing ASR using real-time context, dialog generation, speech continuation, and question answering. Our approach demonstrates that the representational gap between pretrained speech and language models is narrower than one would expect, and can be bridged by a simple adaptation mechanism. As a result, SLM is not only efficient to train, but also inherits strong capabilities already present in foundation models of different modalities.",
      "abstract": "We present a joint Speech and Language Model (SLM), a multitask, multilingual, and dual-modal model that takes advantage of pretrained foundational speech and language models. SLM freezes the pretrained foundation models to maximally preserves their capabilities, and only trains a simple adapter with just 1% (156M) of the foundation models' parameters. This adaptation not only leads SLM to achieve strong performance on conventional tasks such as automatic speech recognition (ASR) and automatic speech translation (AST), but also unlocks the novel capability of zero-shot instruction-following for more diverse tasks. Given a speech input and a text instruction, SLM is able to perform unseen generation tasks including contextual biasing ASR using real-time context, dialog generation, speech continuation, and question answering. Our approach demonstrates that the representational gap between pretrained speech and language models is narrower than one would expect, and can be bridged by a simple adaptation mechanism. As a result, SLM is not only efficient to train, but also inherits strong capabilities already present in foundation models of different modalities.",
      "doi": "https://doi.org/10.1109/asru57964.2023.10389703",
      "openalex_id": "https://openalex.org/W4391021623",
      "arxiv_id": "",
      "publication_date": "2023-12-16",
      "published": "2023-12-16",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item explicitly centers on Spoken Language Models / Speech–Language Models, treats speech as the core modality rather than just pre-processing, discusses architecture/training/evaluation for cross-modal speech–language modeling, is within the past 3 years (arXiv:2504.08528, 2025), and uses terms like SLM / speech-language model in the title/abstract.",
      "evaluation": 5
    },
    "round-A_JuniorNano_reasoning": "The item explicitly centers on Spoken Language Models / Speech–Language Models, treats speech as the core modality rather than just pre-processing, discusses architecture/training/evaluation for cross-modal speech–language modeling, is within the past 3 years (arXiv:2504.08528, 2025), and uses terms like SLM / speech-language model in the title/abstract.",
    "round-A_JuniorNano_evaluation": 5,
    "round-A_JuniorMini_output": {
      "reasoning": "The study presents a joint Speech and Language Model that leverages pretrained speech and language foundation models, addressing multitask and multilingual capabilities with efficient adaptation. However, it mainly focuses on tasks like ASR, AST, zero-shot instruction-following, and generation tasks such as dialog generation and question answering with a speech input and text instruction. The abstract does not explicitly mention core aspects like full-duplex spoken language modeling, cognitive parallelism, speech overlap handling, real-time low latency interaction (sub-200ms), engineered or learned synchronization mechanisms, or detailed behavioral arbitration assessments required by the inclusion criteria. Additionally, the emphasis seems more on bridging speech and text modalities through multitask learning rather than modeling speech sequences as the core modeling and reasoning object, aligned with Spoken Language Model definitions. Therefore, the study does not fully meet the inclusion criteria and partially aligns with some spoken language model concepts but lacks the key focus on real-time interactive spoken language modeling and detailed synchronization or evaluation aspects.",
      "evaluation": 2
    },
    "round-A_JuniorMini_reasoning": "The study presents a joint Speech and Language Model that leverages pretrained speech and language foundation models, addressing multitask and multilingual capabilities with efficient adaptation. However, it mainly focuses on tasks like ASR, AST, zero-shot instruction-following, and generation tasks such as dialog generation and question answering with a speech input and text instruction. The abstract does not explicitly mention core aspects like full-duplex spoken language modeling, cognitive parallelism, speech overlap handling, real-time low latency interaction (sub-200ms), engineered or learned synchronization mechanisms, or detailed behavioral arbitration assessments required by the inclusion criteria. Additionally, the emphasis seems more on bridging speech and text modalities through multitask learning rather than modeling speech sequences as the core modeling and reasoning object, aligned with Spoken Language Model definitions. Therefore, the study does not fully meet the inclusion criteria and partially aligns with some spoken language model concepts but lacks the key focus on real-time interactive spoken language modeling and detailed synchronization or evaluation aspects.",
    "round-A_JuniorMini_evaluation": 2,
    "round-B_SeniorLead_output": {
      "reasoning": "The title and abstract explicitly introduce a Speech and Language Model (SLM) and describe a joint dual-modal architecture that leverages pretrained speech and language foundation models (freezing them and training a lightweight adapter), thereby treating speech representations as a core modeling object rather than merely pre-processing for text; the abstract also reports training/architecture details (adapter size, freezing), multilingual multitask evaluation (ASR, AST) and broader capabilities beyond plain recognition (zero-shot instruction-following, dialog generation, speech continuation, QA, contextual biasing with real-time context). This satisfies the requirement that the work centers on a spoken/speech-language model and discusses model architecture, training, and evaluation, and it is within the required timeframe. Although the abstract does not explicitly claim full-duplex interaction, overlap handling, or sub-200ms latency, those are desirable but not mandatory for inclusion; the paper also is not limited to traditional ASR/TTS or purely text-NLP downstreams. Junior reviewer opinions: one recommended inclusion (5) and one recommended exclusion (2) due to absence of explicit full-duplex/low-latency discussion — I reconcile by noting the inclusion criteria require explicit SLM focus and architecture/evaluation, which this abstract provides, so it should be included.",
      "evaluation": 5
    },
    "round-B_SeniorLead_reasoning": "The title and abstract explicitly introduce a Speech and Language Model (SLM) and describe a joint dual-modal architecture that leverages pretrained speech and language foundation models (freezing them and training a lightweight adapter), thereby treating speech representations as a core modeling object rather than merely pre-processing for text; the abstract also reports training/architecture details (adapter size, freezing), multilingual multitask evaluation (ASR, AST) and broader capabilities beyond plain recognition (zero-shot instruction-following, dialog generation, speech continuation, QA, contextual biasing with real-time context). This satisfies the requirement that the work centers on a spoken/speech-language model and discusses model architecture, training, and evaluation, and it is within the required timeframe. Although the abstract does not explicitly claim full-duplex interaction, overlap handling, or sub-200ms latency, those are desirable but not mandatory for inclusion; the paper also is not limited to traditional ASR/TTS or purely text-NLP downstreams. Junior reviewer opinions: one recommended inclusion (5) and one recommended exclusion (2) due to absence of explicit full-duplex/low-latency discussion — I reconcile by noting the inclusion criteria require explicit SLM focus and architecture/evaluation, which this abstract provides, so it should be included.",
    "round-B_SeniorLead_evaluation": 5,
    "final_verdict": "include (senior:5)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Towards Multi-Intent Spoken Language Understanding via Hierarchical Attention and Optimal Transport",
    "abstract": "Multi-Intent spoken language understanding (SLU) can handle complicated utterances expressing multiple intents, which has attracted increasing attention from researchers. Although existing models have achieved promising performance, most of them still suffer from two leading problems: (1) each intent has its specific scope and the semantic information outside the scope might potentially hinder accurate predictions, i.e. scope barrier; (2) only the guidance from intent to slot is modeled but the guidance from slot to intent is often neglected, i.e. unidirectional guidance. In this paper, we propose a novel Multi-Intent SLU framework termed HAOT, which utilizes hierarchical attention to divide the scopes of each intent and applies optimal transport to achieve the mutual guidance between slot and intent. Experiments demonstrate that our model achieves state-of-the-art performance on two public Multi-Intent SLU datasets, obtaining the 3.4 improvement on MixATIS dataset compared to the previous best models in overall accuracy.",
    "metadata": {
      "title": "Towards Multi-Intent Spoken Language Understanding via Hierarchical Attention and Optimal Transport",
      "summary": "Multi-Intent spoken language understanding (SLU) can handle complicated utterances expressing multiple intents, which has attracted increasing attention from researchers. Although existing models have achieved promising performance, most of them still suffer from two leading problems: (1) each intent has its specific scope and the semantic information outside the scope might potentially hinder accurate predictions, i.e. scope barrier; (2) only the guidance from intent to slot is modeled but the guidance from slot to intent is often neglected, i.e. unidirectional guidance. In this paper, we propose a novel Multi-Intent SLU framework termed HAOT, which utilizes hierarchical attention to divide the scopes of each intent and applies optimal transport to achieve the mutual guidance between slot and intent. Experiments demonstrate that our model achieves state-of-the-art performance on two public Multi-Intent SLU datasets, obtaining the 3.4 improvement on MixATIS dataset compared to the previous best models in overall accuracy.",
      "abstract": "Multi-Intent spoken language understanding (SLU) can handle complicated utterances expressing multiple intents, which has attracted increasing attention from researchers. Although existing models have achieved promising performance, most of them still suffer from two leading problems: (1) each intent has its specific scope and the semantic information outside the scope might potentially hinder accurate predictions, i.e. scope barrier; (2) only the guidance from intent to slot is modeled but the guidance from slot to intent is often neglected, i.e. unidirectional guidance. In this paper, we propose a novel Multi-Intent SLU framework termed HAOT, which utilizes hierarchical attention to divide the scopes of each intent and applies optimal transport to achieve the mutual guidance between slot and intent. Experiments demonstrate that our model achieves state-of-the-art performance on two public Multi-Intent SLU datasets, obtaining the 3.4 improvement on MixATIS dataset compared to the previous best models in overall accuracy.",
      "doi": "https://doi.org/10.1609/aaai.v38i16.29738",
      "openalex_id": "https://openalex.org/W4393160357",
      "arxiv_id": "",
      "publication_date": "2024-03-24",
      "published": "2024-03-24",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The work targets Spoken Language Understanding with speech-oriented modeling (multi-intent SLU using hierarchy and transport), presents a dedicated SLU architecture, uses MixATIS as evaluation, and is published in 2025 (within the 3-year window), thus aligning with the spoken-language-model core and not being limited to ASR/TTS or text-only.",
      "evaluation": 5
    },
    "round-A_JuniorNano_reasoning": "The work targets Spoken Language Understanding with speech-oriented modeling (multi-intent SLU using hierarchy and transport), presents a dedicated SLU architecture, uses MixATIS as evaluation, and is published in 2025 (within the 3-year window), thus aligning with the spoken-language-model core and not being limited to ASR/TTS or text-only.",
    "round-A_JuniorNano_evaluation": 5,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on multi-intent spoken language understanding using hierarchical attention and optimal transport, but it does not explicitly address spoken language models or full-duplex interaction as defined in the inclusion criteria. It mainly treats spoken language understanding as text-level semantics rather than modeling acoustic sequences or speech as primary objects, nor does it mention full-duplex capabilities, engineered/learned synchronization, or low-latency interactive behaviors. Therefore, it does not fully meet the core requirements for inclusion.",
      "evaluation": 2
    },
    "round-A_JuniorMini_reasoning": "The study focuses on multi-intent spoken language understanding using hierarchical attention and optimal transport, but it does not explicitly address spoken language models or full-duplex interaction as defined in the inclusion criteria. It mainly treats spoken language understanding as text-level semantics rather than modeling acoustic sequences or speech as primary objects, nor does it mention full-duplex capabilities, engineered/learned synchronization, or low-latency interactive behaviors. Therefore, it does not fully meet the core requirements for inclusion.",
    "round-A_JuniorMini_evaluation": 2,
    "round-B_SeniorLead_output": {
      "reasoning": "Junior reviewers disagreed (5 vs 2); after inspection, the paper proposes a multi-intent SLU model (hierarchical attention + optimal transport) evaluated on MixATIS but the abstract contains no mention of spoken-language models, acoustic/speech-level modeling, speech–LLM or full‑duplex/synchronous interaction, and treats SLU as text-level slot/intents prediction. Therefore it fails the inclusion criteria and meets exclusion conditions (core research remains text-NLP/SLU rather than spoken-language modeling). Brief 1-sentence explanation: Exclude — the work is a text-level multi-intent SLU study and does not engage with speech-level language modeling or full‑duplex spoken interaction.",
      "evaluation": 1
    },
    "round-B_SeniorLead_reasoning": "Junior reviewers disagreed (5 vs 2); after inspection, the paper proposes a multi-intent SLU model (hierarchical attention + optimal transport) evaluated on MixATIS but the abstract contains no mention of spoken-language models, acoustic/speech-level modeling, speech–LLM or full‑duplex/synchronous interaction, and treats SLU as text-level slot/intents prediction. Therefore it fails the inclusion criteria and meets exclusion conditions (core research remains text-NLP/SLU rather than spoken-language modeling). Brief 1-sentence explanation: Exclude — the work is a text-level multi-intent SLU study and does not engage with speech-level language modeling or full‑duplex spoken interaction.",
    "round-B_SeniorLead_evaluation": 1,
    "final_verdict": "exclude (senior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Extending Large Language Models for Speech and Audio Captioning",
    "abstract": "Multimodal large language models (LLMs) have shown promising visual perception abilities by connecting with image encoders, but their performance on auditory tasks has not yet been widely investigated. Meanwhile, automatic speech recognition (ASR) and automatic audio captioning (AAC) are often achieved with separate systems, resulting in incomplete auditory perception abilities. To fill in these gaps, in this paper, we present the first study that achieves both ASR and AAC by connecting an LLM with auditory encoders. A dual auditory encoder structure is proposed, integrating the Whisper encoder for speech and the BEATs encoder for audio events with a high temporal resolution by using a Q-Former at the window level. Experiments for ASR and AAC are performed correspondingly on the widely used LibriSpeech, GigaSpeech, WavCaps, AudioCaps, and Clotho datasets and yield promising results. In particular, state-of-the-art results are achieved on GigaSpeech, AudioCaps and Clotho. Our model is also able to caption speech and audio events simultaneously from clips with mixed speech and background audio events, which is a step towards more complete machine auditory perception.",
    "metadata": {
      "title": "Extending Large Language Models for Speech and Audio Captioning",
      "summary": "Multimodal large language models (LLMs) have shown promising visual perception abilities by connecting with image encoders, but their performance on auditory tasks has not yet been widely investigated. Meanwhile, automatic speech recognition (ASR) and automatic audio captioning (AAC) are often achieved with separate systems, resulting in incomplete auditory perception abilities. To fill in these gaps, in this paper, we present the first study that achieves both ASR and AAC by connecting an LLM with auditory encoders. A dual auditory encoder structure is proposed, integrating the Whisper encoder for speech and the BEATs encoder for audio events with a high temporal resolution by using a Q-Former at the window level. Experiments for ASR and AAC are performed correspondingly on the widely used LibriSpeech, GigaSpeech, WavCaps, AudioCaps, and Clotho datasets and yield promising results. In particular, state-of-the-art results are achieved on GigaSpeech, AudioCaps and Clotho. Our model is also able to caption speech and audio events simultaneously from clips with mixed speech and background audio events, which is a step towards more complete machine auditory perception.",
      "abstract": "Multimodal large language models (LLMs) have shown promising visual perception abilities by connecting with image encoders, but their performance on auditory tasks has not yet been widely investigated. Meanwhile, automatic speech recognition (ASR) and automatic audio captioning (AAC) are often achieved with separate systems, resulting in incomplete auditory perception abilities. To fill in these gaps, in this paper, we present the first study that achieves both ASR and AAC by connecting an LLM with auditory encoders. A dual auditory encoder structure is proposed, integrating the Whisper encoder for speech and the BEATs encoder for audio events with a high temporal resolution by using a Q-Former at the window level. Experiments for ASR and AAC are performed correspondingly on the widely used LibriSpeech, GigaSpeech, WavCaps, AudioCaps, and Clotho datasets and yield promising results. In particular, state-of-the-art results are achieved on GigaSpeech, AudioCaps and Clotho. Our model is also able to caption speech and audio events simultaneously from clips with mixed speech and background audio events, which is a step towards more complete machine auditory perception.",
      "doi": "https://doi.org/10.1109/icassp48485.2024.10446343",
      "openalex_id": "https://openalex.org/W4392902647",
      "arxiv_id": "",
      "publication_date": "2024-03-18",
      "published": "2024-03-18",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item centers on Spoken Language Model concepts by integrating an LLM with auditory encoders for ASR and audio captioning, discusses architecture/training/evaluation, uses relevant datasets, and is within the past 3 years, meeting all inclusion criteria and none of the exclusions.",
      "evaluation": 5
    },
    "round-A_JuniorNano_reasoning": "The item centers on Spoken Language Model concepts by integrating an LLM with auditory encoders for ASR and audio captioning, discusses architecture/training/evaluation, uses relevant datasets, and is within the past 3 years, meeting all inclusion criteria and none of the exclusions.",
    "round-A_JuniorNano_evaluation": 5,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper focuses on extending large language models for speech and audio captioning, with a dual auditory encoder structure for ASR and AAC tasks, but does not explicitly discuss spoken language models (SLMs) or full-duplex spoken language models (FD-SLMs) as core modeling objects, nor does it mention synchronous interaction, real-time behavior arbitration, or low-latency interaction; also, the focus is on ASR and audio captioning benchmarks without explicit emphasis on speech language modeling or interactive dialogue behavior, which are key inclusion criteria. Hence, it does not meet the full inclusion criteria and overlaps with exclusion criteria such as focusing mainly on ASR and lacking speech language modeling innovation.",
      "evaluation": 2
    },
    "round-A_JuniorMini_reasoning": "The paper focuses on extending large language models for speech and audio captioning, with a dual auditory encoder structure for ASR and AAC tasks, but does not explicitly discuss spoken language models (SLMs) or full-duplex spoken language models (FD-SLMs) as core modeling objects, nor does it mention synchronous interaction, real-time behavior arbitration, or low-latency interaction; also, the focus is on ASR and audio captioning benchmarks without explicit emphasis on speech language modeling or interactive dialogue behavior, which are key inclusion criteria. Hence, it does not meet the full inclusion criteria and overlaps with exclusion criteria such as focusing mainly on ASR and lacking speech language modeling innovation.",
    "round-A_JuniorMini_evaluation": 2,
    "round-B_SeniorLead_output": {
      "reasoning": "I reviewed the two junior reviews and the abstract: while the paper describes integrating LLMs with auditory encoders for ASR and audio captioning and proposes an encoder architecture with evaluations, it does not mention or frame itself as a Spoken Language Model (SLM) or Full‑Duplex SLM, nor does it address synchronous/full‑duplex interaction, low‑latency real‑time arbitration, or speech‑level language modeling; therefore it fails the explicit inclusion requirement that the work be centered and labeled as SLM/speech language modeling and advances in synchronous spoken interaction. Score explanation (1 sentence): Does not meet inclusion criteria—the work focuses on ASR/AAC integration with LLMs rather than explicit Spoken Language Model research or full‑duplex/synchronous speech interaction.",
      "evaluation": 2
    },
    "round-B_SeniorLead_reasoning": "I reviewed the two junior reviews and the abstract: while the paper describes integrating LLMs with auditory encoders for ASR and audio captioning and proposes an encoder architecture with evaluations, it does not mention or frame itself as a Spoken Language Model (SLM) or Full‑Duplex SLM, nor does it address synchronous/full‑duplex interaction, low‑latency real‑time arbitration, or speech‑level language modeling; therefore it fails the explicit inclusion requirement that the work be centered and labeled as SLM/speech language modeling and advances in synchronous spoken interaction. Score explanation (1 sentence): Does not meet inclusion criteria—the work focuses on ASR/AAC integration with LLMs rather than explicit Spoken Language Model research or full‑duplex/synchronous speech interaction.",
    "round-B_SeniorLead_evaluation": 2,
    "final_verdict": "exclude (senior:2)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "A Comprehensive Overview and Analysis of Large Language Models: Trends and Challenges",
    "abstract": "Large Language Models (LLMs) have transformed numerous fields by offering innovative solutions that drive advancements across a wide range of applications. However, their widespread adoption presents several challenges, including variations in architectures, limitations in processing capabilities, and high computational resource demands for training. Addressing these challenges is crucial for maximizing the benefits of LLMs while ensuring their responsible and efficient use. This paper reviews LLMs, focusing on their key characteristics and the factors that influence their performance. It examines several prominent families of LLMs and provides a comparative analysis of their properties. In addition, it explores the classification of LLMs based on criteria such as availability, context window, and model size. In addition, the study explores advanced fine-tuning techniques, including Parameter-Efficient fine-tuning (PEFT) and Low-Rank Adaptation (LoRA), that enhance the performance and efficiency of models. Furthermore, it reviews the wide-ranging applications of LLMs and evaluates the methodologies used to evaluate their effectiveness.",
    "metadata": {
      "title": "A Comprehensive Overview and Analysis of Large Language Models: Trends and Challenges",
      "summary": "Large Language Models (LLMs) have transformed numerous fields by offering innovative solutions that drive advancements across a wide range of applications. However, their widespread adoption presents several challenges, including variations in architectures, limitations in processing capabilities, and high computational resource demands for training. Addressing these challenges is crucial for maximizing the benefits of LLMs while ensuring their responsible and efficient use. This paper reviews LLMs, focusing on their key characteristics and the factors that influence their performance. It examines several prominent families of LLMs and provides a comparative analysis of their properties. In addition, it explores the classification of LLMs based on criteria such as availability, context window, and model size. In addition, the study explores advanced fine-tuning techniques, including Parameter-Efficient fine-tuning (PEFT) and Low-Rank Adaptation (LoRA), that enhance the performance and efficiency of models. Furthermore, it reviews the wide-ranging applications of LLMs and evaluates the methodologies used to evaluate their effectiveness.",
      "abstract": "Large Language Models (LLMs) have transformed numerous fields by offering innovative solutions that drive advancements across a wide range of applications. However, their widespread adoption presents several challenges, including variations in architectures, limitations in processing capabilities, and high computational resource demands for training. Addressing these challenges is crucial for maximizing the benefits of LLMs while ensuring their responsible and efficient use. This paper reviews LLMs, focusing on their key characteristics and the factors that influence their performance. It examines several prominent families of LLMs and provides a comparative analysis of their properties. In addition, it explores the classification of LLMs based on criteria such as availability, context window, and model size. In addition, the study explores advanced fine-tuning techniques, including Parameter-Efficient fine-tuning (PEFT) and Low-Rank Adaptation (LoRA), that enhance the performance and efficiency of models. Furthermore, it reviews the wide-ranging applications of LLMs and evaluates the methodologies used to evaluate their effectiveness.",
      "doi": "https://doi.org/10.1109/access.2025.3573955",
      "openalex_id": "https://openalex.org/W4410737867",
      "arxiv_id": "",
      "publication_date": "2025-01-01",
      "published": "2025-01-01",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item is a general Large Language Model overview with no focus on Spoken Language Models or FD-SLMs, nor on speech-centric architectures or full-duplex interactions, thus not meeting the inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item is a general Large Language Model overview with no focus on Spoken Language Models or FD-SLMs, nor on speech-centric architectures or full-duplex interactions, thus not meeting the inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper focuses broadly on Large Language Models, covering aspects such as architectures, fine-tuning techniques, and applications, without emphasizing Spoken Language Models or speech as the core modeling object. It lacks specific mention of spoken language models, full-duplex interaction, speech-level modeling, or related synchronization and evaluation criteria. Therefore, it does not meet the inclusion criteria centered on spoken language modeling and synchronous interactive speech systems, and also lacks references to these key concepts in title or abstract.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper focuses broadly on Large Language Models, covering aspects such as architectures, fine-tuning techniques, and applications, without emphasizing Spoken Language Models or speech as the core modeling object. It lacks specific mention of spoken language models, full-duplex interaction, speech-level modeling, or related synchronization and evaluation criteria. Therefore, it does not meet the inclusion criteria centered on spoken language modeling and synchronous interactive speech systems, and also lacks references to these key concepts in title or abstract.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Foundation Models in Agriculture: A Comprehensive Review",
    "abstract": "This paper explores the transformative potential of Foundation Models (FMs) in agriculture, driven by the need for efficient and intelligent decision support systems in the face of growing global population and climate change. It begins by outlining the development history of FMs, including general FM training processes, application trends and challenges, before focusing on Agricultural Foundation Models (AFMs). The paper examines the diversity and applications of AFMs in areas like crop classification, pest detection, and crop image segmentation, and delves into specific use cases such as agricultural knowledge question-answering, image and video analysis, decision support, and robotics. Furthermore, it discusses the challenges faced by AFMs, including data acquisition, training efficiency, data shift, and practical application challenges. Finally, the paper discusses future development directions for AFMs, emphasizing multimodal applications, integrating AFMs across the agricultural and food sectors, and intelligent decision-making systems, ultimately aiming to promote the digitalization and intelligent transformation of agriculture.",
    "metadata": {
      "title": "Foundation Models in Agriculture: A Comprehensive Review",
      "summary": "This paper explores the transformative potential of Foundation Models (FMs) in agriculture, driven by the need for efficient and intelligent decision support systems in the face of growing global population and climate change. It begins by outlining the development history of FMs, including general FM training processes, application trends and challenges, before focusing on Agricultural Foundation Models (AFMs). The paper examines the diversity and applications of AFMs in areas like crop classification, pest detection, and crop image segmentation, and delves into specific use cases such as agricultural knowledge question-answering, image and video analysis, decision support, and robotics. Furthermore, it discusses the challenges faced by AFMs, including data acquisition, training efficiency, data shift, and practical application challenges. Finally, the paper discusses future development directions for AFMs, emphasizing multimodal applications, integrating AFMs across the agricultural and food sectors, and intelligent decision-making systems, ultimately aiming to promote the digitalization and intelligent transformation of agriculture.",
      "abstract": "This paper explores the transformative potential of Foundation Models (FMs) in agriculture, driven by the need for efficient and intelligent decision support systems in the face of growing global population and climate change. It begins by outlining the development history of FMs, including general FM training processes, application trends and challenges, before focusing on Agricultural Foundation Models (AFMs). The paper examines the diversity and applications of AFMs in areas like crop classification, pest detection, and crop image segmentation, and delves into specific use cases such as agricultural knowledge question-answering, image and video analysis, decision support, and robotics. Furthermore, it discusses the challenges faced by AFMs, including data acquisition, training efficiency, data shift, and practical application challenges. Finally, the paper discusses future development directions for AFMs, emphasizing multimodal applications, integrating AFMs across the agricultural and food sectors, and intelligent decision-making systems, ultimately aiming to promote the digitalization and intelligent transformation of agriculture.",
      "doi": "https://doi.org/10.3390/agriculture15080847",
      "openalex_id": "https://openalex.org/W4409431856",
      "arxiv_id": "",
      "publication_date": "2025-04-14",
      "published": "2025-04-14",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This item discusses Foundation Models in agriculture but does not focus on Spoken Language Models or speech-based modeling; it lacks core concepts (SLMs/FD-SLMs, speech signaling, interactive synchronization) and related evaluation frameworks, so it fails the inclusion criteria and does not align with the exclusion criteria either (i.e., not a speech-language modeling study).",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "This item discusses Foundation Models in agriculture but does not focus on Spoken Language Models or speech-based modeling; it lacks core concepts (SLMs/FD-SLMs, speech signaling, interactive synchronization) and related evaluation frameworks, so it fails the inclusion criteria and does not align with the exclusion criteria either (i.e., not a speech-language modeling study).",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper focuses on Foundation Models in agriculture, emphasizing applications in crop classification, pest detection, and decision support, without mentioning spoken language models, speech language model architectures, or real-time full-duplex speech interaction aspects. The core research is not centered on spoken language modeling or synchronous speech communication as defined in the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper focuses on Foundation Models in agriculture, emphasizing applications in crop classification, pest detection, and decision support, without mentioning spoken language models, speech language model architectures, or real-time full-duplex speech interaction aspects. The core research is not centered on spoken language modeling or synchronous speech communication as defined in the inclusion criteria.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Artificial Intelligence in the 21st Century",
    "abstract": "Artificial intelligence (AI) is the most important and interesting technology in the 21st Century due to its vast application. This review focuses on the evolution of AI techniques and their applications in recent decades. Deep learning algorithms/models, represented by Large Language Models (LLMs) have resulted in groundbreaking advancements, indicating that AI is evolving to improve its capacity to interact with and help people in various fields such as finance, medicine, and science research. The potential for research in AI is immense, and there is a need for scientific principles behind AI. Future perspectives on how machines can be developed to work with humans and to be compatible with human values and preferences are also discussed.",
    "metadata": {
      "title": "Artificial Intelligence in the 21st Century",
      "summary": "Artificial intelligence (AI) is the most important and interesting technology in the 21st Century due to its vast application. This review focuses on the evolution of AI techniques and their applications in recent decades. Deep learning algorithms/models, represented by Large Language Models (LLMs) have resulted in groundbreaking advancements, indicating that AI is evolving to improve its capacity to interact with and help people in various fields such as finance, medicine, and science research. The potential for research in AI is immense, and there is a need for scientific principles behind AI. Future perspectives on how machines can be developed to work with humans and to be compatible with human values and preferences are also discussed.",
      "abstract": "Artificial intelligence (AI) is the most important and interesting technology in the 21st Century due to its vast application. This review focuses on the evolution of AI techniques and their applications in recent decades. Deep learning algorithms/models, represented by Large Language Models (LLMs) have resulted in groundbreaking advancements, indicating that AI is evolving to improve its capacity to interact with and help people in various fields such as finance, medicine, and science research. The potential for research in AI is immense, and there is a need for scientific principles behind AI. Future perspectives on how machines can be developed to work with humans and to be compatible with human values and preferences are also discussed.",
      "doi": "https://doi.org/10.25082/rima.2023.01.002",
      "openalex_id": "https://openalex.org/W4392839128",
      "arxiv_id": "",
      "publication_date": "2023-03-25",
      "published": "2023-03-25",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The abstract is a broad AI review about deep learning/LLMs and future perspectives, with no focus on Spoken Language Models or speech-centric modeling, no discussion of synchronous dialogue or FD-SLMs, and no explicit evaluation framework for speech-language tasks, thus it fails the inclusion criteria and meets at least one exclusion criterion.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The abstract is a broad AI review about deep learning/LLMs and future perspectives, with no focus on Spoken Language Models or speech-centric modeling, no discussion of synchronous dialogue or FD-SLMs, and no explicit evaluation framework for speech-language tasks, thus it fails the inclusion criteria and meets at least one exclusion criterion.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The title and abstract discuss the broad evolution and applications of AI and deep learning models like LLMs but do not mention spoken language models, speech as a primary modeling object, or any specific topics related to full-duplex spoken language modeling, synchronization, or real-time interaction as required. The focus lacks the core thematic elements specified in the inclusion criteria and does not address speech language modeling or related evaluation frameworks; thus, it fails to meet the inclusion criteria and falls under exclusion categories.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The title and abstract discuss the broad evolution and applications of AI and deep learning models like LLMs but do not mention spoken language models, speech as a primary modeling object, or any specific topics related to full-duplex spoken language modeling, synchronization, or real-time interaction as required. The focus lacks the core thematic elements specified in the inclusion criteria and does not address speech language modeling or related evaluation frameworks; thus, it fails to meet the inclusion criteria and falls under exclusion categories.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Geosystems risk and uncertainty: The application of ChatGPT with targeted prompting",
    "abstract": "ChatGPT, a prominent large language model (LLM), is being increasingly used across a wide range of scientific fields. Geosystem engineers and researchers are also posed to leverage ChatGPT to find solutions to challenges encountered in various topics. This study evaluates the accuracy and reproducibility of ChatGPT in responding to different qualitative and quantitative questions, with a particular focus on risk and uncertainty (R&U) in both the Greenfield and Brownfield domains as an important area of interest. The results show the importance of prompting to considerably improve the ChatGPT's response accuracy and reproducibility. For example, prompting increases the accuracy of responses to qualitative and quantitative questions in the Greenfield domain by 10.4% and 41.8%, respectively. Additionally, prompting enhances the reproducibility of responses, with a 32.1% increase for qualitative questions and a 33.3% rise for quantitative questions in the Brownfield domain. The findings highlight that the greater the comprehensiveness of the prompts, the higher the accuracy and reproducibility of the responses to the questions. The study also acknowledges the potential limitations associated with the sources of information and the contextual influences on the reliability of the response.",
    "metadata": {
      "title": "Geosystems risk and uncertainty: The application of ChatGPT with targeted prompting",
      "summary": "ChatGPT, a prominent large language model (LLM), is being increasingly used across a wide range of scientific fields. Geosystem engineers and researchers are also posed to leverage ChatGPT to find solutions to challenges encountered in various topics. This study evaluates the accuracy and reproducibility of ChatGPT in responding to different qualitative and quantitative questions, with a particular focus on risk and uncertainty (R&U) in both the Greenfield and Brownfield domains as an important area of interest. The results show the importance of prompting to considerably improve the ChatGPT's response accuracy and reproducibility. For example, prompting increases the accuracy of responses to qualitative and quantitative questions in the Greenfield domain by 10.4% and 41.8%, respectively. Additionally, prompting enhances the reproducibility of responses, with a 32.1% increase for qualitative questions and a 33.3% rise for quantitative questions in the Brownfield domain. The findings highlight that the greater the comprehensiveness of the prompts, the higher the accuracy and reproducibility of the responses to the questions. The study also acknowledges the potential limitations associated with the sources of information and the contextual influences on the reliability of the response.",
      "abstract": "ChatGPT, a prominent large language model (LLM), is being increasingly used across a wide range of scientific fields. Geosystem engineers and researchers are also posed to leverage ChatGPT to find solutions to challenges encountered in various topics. This study evaluates the accuracy and reproducibility of ChatGPT in responding to different qualitative and quantitative questions, with a particular focus on risk and uncertainty (R&U) in both the Greenfield and Brownfield domains as an important area of interest. The results show the importance of prompting to considerably improve the ChatGPT's response accuracy and reproducibility. For example, prompting increases the accuracy of responses to qualitative and quantitative questions in the Greenfield domain by 10.4% and 41.8%, respectively. Additionally, prompting enhances the reproducibility of responses, with a 32.1% increase for qualitative questions and a 33.3% rise for quantitative questions in the Brownfield domain. The findings highlight that the greater the comprehensiveness of the prompts, the higher the accuracy and reproducibility of the responses to the questions. The study also acknowledges the potential limitations associated with the sources of information and the contextual influences on the reliability of the response.",
      "doi": "https://doi.org/10.1016/j.geoen.2024.212889",
      "openalex_id": "https://openalex.org/W4396781220",
      "arxiv_id": "",
      "publication_date": "2024-05-09",
      "published": "2024-05-09",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This item focuses on prompting ChatGPT for geosystem risk and uncertainty in text form and does not address Spoken Language Models or any speech-language modeling aspects (no audio input/processing, no turn-taking, no speech-specific evaluation), so it does not meet the core inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "This item focuses on prompting ChatGPT for geosystem risk and uncertainty in text form and does not address Spoken Language Models or any speech-language modeling aspects (no audio input/processing, no turn-taking, no speech-specific evaluation), so it does not meet the core inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on applying ChatGPT, a text-based large language model, to geosystems risk and uncertainty, and does not mention spoken language models, speech language modeling, or related architectures involving speech as the primary modeling target. It lacks discussion on full-duplex interaction, speech signal modeling, synchronization, or any speech-specific evaluation, and instead centers on text-based prompt engineering in a domain outside speech language modeling. Therefore, it does not meet the inclusion criteria and matches multiple exclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on applying ChatGPT, a text-based large language model, to geosystems risk and uncertainty, and does not mention spoken language models, speech language modeling, or related architectures involving speech as the primary modeling target. It lacks discussion on full-duplex interaction, speech signal modeling, synchronization, or any speech-specific evaluation, and instead centers on text-based prompt engineering in a domain outside speech language modeling. Therefore, it does not meet the inclusion criteria and matches multiple exclusion criteria.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "GPT VS. HUMAN FOR SCIENTIFIC REVIEWS: A DUAL SOURCE REVIEW ON APPLICATIONS OF CHATGPT IN SCIENCE",
    "abstract": "The new polymath large language models (LLMs) can greatly speed up scientific reviews, possibly using more unbiased quantitative metrics, facilitating cross-disciplinary connections, and identifying emerging trends and research gaps by analyzing large volumes of data. However, at the present time, they lack the required deep understanding of complex methodologies, they have difficulty in evaluating innovative claims, and they are unable to assess ethical issues and conflicts of interest. Herein, we consider 13 geotechnical parrot tales (GPT)-related papers across different scientific domains, reviewed by a human reviewer and SciSpace, a large language model, with the reviews evaluated by three distinct types of evaluators, namely GPT-3.5, a crowd panel, and GPT-4. We found that 50&amp;#37; of SciSpace's responses to objective questions align with those of a human reviewer, with GPT-4 (informed evaluator) often rating the human reviewer higher in accuracy, and SciSpace higher in structure, clarity, and completeness. In subjective questions, the uninformed evaluators (GPT-3.5 and crowd panel) showed varying preferences between SciSpace and human responses, with the crowd panel showing a preference for the human responses. However, GPT-4 rated them equally in accuracy and structure but favored SciSpace for completeness.",
    "metadata": {
      "title": "GPT VS. HUMAN FOR SCIENTIFIC REVIEWS: A DUAL SOURCE REVIEW ON APPLICATIONS OF CHATGPT IN SCIENCE",
      "summary": "The new polymath large language models (LLMs) can greatly speed up scientific reviews, possibly using more unbiased quantitative metrics, facilitating cross-disciplinary connections, and identifying emerging trends and research gaps by analyzing large volumes of data. However, at the present time, they lack the required deep understanding of complex methodologies, they have difficulty in evaluating innovative claims, and they are unable to assess ethical issues and conflicts of interest. Herein, we consider 13 geotechnical parrot tales (GPT)-related papers across different scientific domains, reviewed by a human reviewer and SciSpace, a large language model, with the reviews evaluated by three distinct types of evaluators, namely GPT-3.5, a crowd panel, and GPT-4. We found that 50&amp;#37; of SciSpace's responses to objective questions align with those of a human reviewer, with GPT-4 (informed evaluator) often rating the human reviewer higher in accuracy, and SciSpace higher in structure, clarity, and completeness. In subjective questions, the uninformed evaluators (GPT-3.5 and crowd panel) showed varying preferences between SciSpace and human responses, with the crowd panel showing a preference for the human responses. However, GPT-4 rated them equally in accuracy and structure but favored SciSpace for completeness.",
      "abstract": "The new polymath large language models (LLMs) can greatly speed up scientific reviews, possibly using more unbiased quantitative metrics, facilitating cross-disciplinary connections, and identifying emerging trends and research gaps by analyzing large volumes of data. However, at the present time, they lack the required deep understanding of complex methodologies, they have difficulty in evaluating innovative claims, and they are unable to assess ethical issues and conflicts of interest. Herein, we consider 13 geotechnical parrot tales (GPT)-related papers across different scientific domains, reviewed by a human reviewer and SciSpace, a large language model, with the reviews evaluated by three distinct types of evaluators, namely GPT-3.5, a crowd panel, and GPT-4. We found that 50&amp;#37; of SciSpace's responses to objective questions align with those of a human reviewer, with GPT-4 (informed evaluator) often rating the human reviewer higher in accuracy, and SciSpace higher in structure, clarity, and completeness. In subjective questions, the uninformed evaluators (GPT-3.5 and crowd panel) showed varying preferences between SciSpace and human responses, with the crowd panel showing a preference for the human responses. However, GPT-4 rated them equally in accuracy and structure but favored SciSpace for completeness.",
      "doi": "https://doi.org/10.1615/jmachlearnmodelcomput.2024052432",
      "openalex_id": "https://openalex.org/W4394844601",
      "arxiv_id": "",
      "publication_date": "2024-01-01",
      "published": "2024-01-01",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item concerns general large language models and evaluation of reviews, not Spoken Language Models or any speech-based modeling/synchronic interaction.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item concerns general large language models and evaluation of reviews, not Spoken Language Models or any speech-based modeling/synchronic interaction.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on evaluating ChatGPT and LLMs for scientific reviews, which is a text-based natural language processing application, not centered on spoken language modeling or speech signals; it does not discuss architectures or evaluations related to spoken language models or full-duplex spoken interactions, thus failing to meet inclusion criteria and falling under exclusion criteria for focusing on text-based LLM capabilities rather than speech modeling.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on evaluating ChatGPT and LLMs for scientific reviews, which is a text-based natural language processing application, not centered on spoken language modeling or speech signals; it does not discuss architectures or evaluations related to spoken language models or full-duplex spoken interactions, thus failing to meet inclusion criteria and falling under exclusion criteria for focusing on text-based LLM capabilities rather than speech modeling.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "MOSS-MED: A Family of Multimodal Models Serving Medical Image Analysis",
    "abstract": "The remarkable advancements in large language models (LLMs) and large-scale visual encoders have laid a solid foundation for enhancing the capabilities of artificial intelligence (AI) in various application scenarios. In this study, we introduce MOSS-MED, a suite of models designed for biomedical vision-language understandings. MOSS-MED includes two models currently: MOSS-MED-2.5B and MOSS-MED-LLaMA, with different scale of their underlying LLMs. We employ a two-stage training pipeline that involves visual-text alignment and visual-involved instruction fine-tuning from both general domains and the medical domain. We evaluate the performance of MOSS-MED family on medical visual question answering (VQA) benchmarks. The results and cases demonstrate the impressive proficiency in medical expertise that MOSS-MED embodies.",
    "metadata": {
      "title": "MOSS-MED: A Family of Multimodal Models Serving Medical Image Analysis",
      "summary": "The remarkable advancements in large language models (LLMs) and large-scale visual encoders have laid a solid foundation for enhancing the capabilities of artificial intelligence (AI) in various application scenarios. In this study, we introduce MOSS-MED, a suite of models designed for biomedical vision-language understandings. MOSS-MED includes two models currently: MOSS-MED-2.5B and MOSS-MED-LLaMA, with different scale of their underlying LLMs. We employ a two-stage training pipeline that involves visual-text alignment and visual-involved instruction fine-tuning from both general domains and the medical domain. We evaluate the performance of MOSS-MED family on medical visual question answering (VQA) benchmarks. The results and cases demonstrate the impressive proficiency in medical expertise that MOSS-MED embodies.",
      "abstract": "The remarkable advancements in large language models (LLMs) and large-scale visual encoders have laid a solid foundation for enhancing the capabilities of artificial intelligence (AI) in various application scenarios. In this study, we introduce MOSS-MED, a suite of models designed for biomedical vision-language understandings. MOSS-MED includes two models currently: MOSS-MED-2.5B and MOSS-MED-LLaMA, with different scale of their underlying LLMs. We employ a two-stage training pipeline that involves visual-text alignment and visual-involved instruction fine-tuning from both general domains and the medical domain. We evaluate the performance of MOSS-MED family on medical visual question answering (VQA) benchmarks. The results and cases demonstrate the impressive proficiency in medical expertise that MOSS-MED embodies.",
      "doi": "https://doi.org/10.1145/3688005",
      "openalex_id": "https://openalex.org/W4401764365",
      "arxiv_id": "",
      "publication_date": "2024-08-22",
      "published": "2024-08-22",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This item focuses on multimodal medical vision-language models for VQA and does not treat Spoken Language Models or speech as the core modeling target, so it does not meet the inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "This item focuses on multimodal medical vision-language models for VQA and does not treat Spoken Language Models or speech as the core modeling target, so it does not meet the inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The title and abstract focus entirely on multimodal models for medical image analysis using large language models and visual encoders, without any mention of spoken language models, speech as the core modeling target, or full-duplex speech interaction capabilities; thus, it does not meet the inclusion criteria centered on spoken language and speech-based model research.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The title and abstract focus entirely on multimodal models for medical image analysis using large language models and visual encoders, without any mention of spoken language models, speech as the core modeling target, or full-duplex speech interaction capabilities; thus, it does not meet the inclusion criteria centered on spoken language and speech-based model research.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "uTalk: Bridging the Gap Between Humans and AI",
    "abstract": "Large Language Models (LLMs) have revolutionized various industries by harnessing their power to improve productivity and facilitate learning across different fields. One intriguing application involves combining LLMs with visual models to create a novel approach to Human-Computer Interaction. The core idea of this system is to create a user-friendly platform that enables people to utilize ChatGPT's features in their everyday lives. uTalk is comprised of technologies like Whisper, ChatGPT, Microsoft Speech Services, and the state-of-the-art (SOTA) talking head system SadTalker. Users can engage in human-like conversation with a digital twin and receive answers to any questions. Also, uTalk could generate content by submitting an image and input (text or audio). This system is hosted on Streamlit, where users will be prompted to provide an image to serve as their AI assistant. Then, as the input (text or audio) is provided, a set of operations will produce a video of the avatar with the precise response. This paper outlines how SadTalker's run-time has been optimized by 27.69% based on 25 frames per second (FPS) generated videos and 38.38% compared to our 20FPS generated videos. Furthermore, the integration and parallelization of SadTalker and Streamlit have resulted in a 9.8% improvement compared to the initial performance of the system.",
    "metadata": {
      "title": "uTalk: Bridging the Gap Between Humans and AI",
      "summary": "Large Language Models (LLMs) have revolutionized various industries by harnessing their power to improve productivity and facilitate learning across different fields. One intriguing application involves combining LLMs with visual models to create a novel approach to Human-Computer Interaction. The core idea of this system is to create a user-friendly platform that enables people to utilize ChatGPT's features in their everyday lives. uTalk is comprised of technologies like Whisper, ChatGPT, Microsoft Speech Services, and the state-of-the-art (SOTA) talking head system SadTalker. Users can engage in human-like conversation with a digital twin and receive answers to any questions. Also, uTalk could generate content by submitting an image and input (text or audio). This system is hosted on Streamlit, where users will be prompted to provide an image to serve as their AI assistant. Then, as the input (text or audio) is provided, a set of operations will produce a video of the avatar with the precise response. This paper outlines how SadTalker's run-time has been optimized by 27.69% based on 25 frames per second (FPS) generated videos and 38.38% compared to our 20FPS generated videos. Furthermore, the integration and parallelization of SadTalker and Streamlit have resulted in a 9.8% improvement compared to the initial performance of the system.",
      "abstract": "Large Language Models (LLMs) have revolutionized various industries by harnessing their power to improve productivity and facilitate learning across different fields. One intriguing application involves combining LLMs with visual models to create a novel approach to Human-Computer Interaction. The core idea of this system is to create a user-friendly platform that enables people to utilize ChatGPT's features in their everyday lives. uTalk is comprised of technologies like Whisper, ChatGPT, Microsoft Speech Services, and the state-of-the-art (SOTA) talking head system SadTalker. Users can engage in human-like conversation with a digital twin and receive answers to any questions. Also, uTalk could generate content by submitting an image and input (text or audio). This system is hosted on Streamlit, where users will be prompted to provide an image to serve as their AI assistant. Then, as the input (text or audio) is provided, a set of operations will produce a video of the avatar with the precise response. This paper outlines how SadTalker's run-time has been optimized by 27.69% based on 25 frames per second (FPS) generated videos and 38.38% compared to our 20FPS generated videos. Furthermore, the integration and parallelization of SadTalker and Streamlit have resulted in a 9.8% improvement compared to the initial performance of the system.",
      "doi": "https://doi.org/10.1109/icce59016.2024.10444441",
      "openalex_id": "https://openalex.org/W4392248350",
      "arxiv_id": "",
      "publication_date": "2024-01-06",
      "published": "2024-01-06",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The abstract describes an HCI platform that integrates LLMs with a talking-head avatar and multimedia input/output; however, the core is system deployment and optimization rather than focusing on Spoken Language Models (SLMs) or Full-Duplex SLMs, including modeling, training, or evaluation of speech-as-a-model reasoning. It references speech components (Whisper, Speech Services) but does not address the essential criteria (speech-centric modeling, turn-taking, low-latency speech interaction analysis). Therefore it does not meet the inclusion criteria and should be excluded.",
      "evaluation": 2
    },
    "round-A_JuniorNano_reasoning": "The abstract describes an HCI platform that integrates LLMs with a talking-head avatar and multimedia input/output; however, the core is system deployment and optimization rather than focusing on Spoken Language Models (SLMs) or Full-Duplex SLMs, including modeling, training, or evaluation of speech-as-a-model reasoning. It references speech components (Whisper, Speech Services) but does not address the essential criteria (speech-centric modeling, turn-taking, low-latency speech interaction analysis). Therefore it does not meet the inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 2,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on a system combining various AI technologies including ChatGPT and visual talking head models, emphasizing human-like conversation and interaction, but it does not explicitly focus on spoken language models (SLMs) or full-duplex spoken language models (FD-SLMs) with voice as the core modeling target; it lacks discussion on speech language modeling architecture, training, or multi-dimensional evaluation related to real-time speech interactions. Therefore, it does not meet the inclusion criteria centered on speech as the primary modeling object and advanced spoken language modeling concepts and falls under exclusion criteria of not addressing synchronization or speech language modeling innovations.",
      "evaluation": 2
    },
    "round-A_JuniorMini_reasoning": "The study focuses on a system combining various AI technologies including ChatGPT and visual talking head models, emphasizing human-like conversation and interaction, but it does not explicitly focus on spoken language models (SLMs) or full-duplex spoken language models (FD-SLMs) with voice as the core modeling target; it lacks discussion on speech language modeling architecture, training, or multi-dimensional evaluation related to real-time speech interactions. Therefore, it does not meet the inclusion criteria centered on speech as the primary modeling object and advanced spoken language modeling concepts and falls under exclusion criteria of not addressing synchronization or speech language modeling innovations.",
    "round-A_JuniorMini_evaluation": 2,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:2)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Leveraging Large Language Models for Exploiting ASR Uncertainty",
    "abstract": "While large language models excel in a variety of natural language processing (NLP) tasks, to perform well on spoken language understanding (SLU) tasks, they must either rely on off-the-shelf automatic speech recognition (ASR) systems for transcription, or be equipped with an in-built speech modality. This work focuses on the former scenario, where LLM's accuracy on SLU tasks is constrained by the accuracy of a fixed ASR system on the spoken input. Specifically, we tackle speech-intent classification task, where a high word-error-rate can limit the LLM's ability to understand the spoken intent. Instead of chasing a high accuracy by designing complex or specialized architectures regardless of deployment costs, we seek to answer how far we can go without substantially changing the underlying ASR and LLM, which can potentially be shared by multiple unrelated tasks. To this end, we propose prompting the LLM with an n-best list of ASR hypotheses instead of only the error-prone 1-best hypothesis. We explore prompt-engineering to explain the concept of n-best lists to the LLM; followed by the finetuning of Low-Rank Adapters [1] on the downstream tasks. Our approach using n-best lists proves to be effective on a device-directed speech detection task as well as on a keyword spotting task, where systems using n-best list prompts outperform those using 1-best ASR hypothesis; thus paving the way for an efficient method to exploit ASR uncertainty via LLMs for speech-based applications.",
    "metadata": {
      "title": "Leveraging Large Language Models for Exploiting ASR Uncertainty",
      "summary": "While large language models excel in a variety of natural language processing (NLP) tasks, to perform well on spoken language understanding (SLU) tasks, they must either rely on off-the-shelf automatic speech recognition (ASR) systems for transcription, or be equipped with an in-built speech modality. This work focuses on the former scenario, where LLM's accuracy on SLU tasks is constrained by the accuracy of a fixed ASR system on the spoken input. Specifically, we tackle speech-intent classification task, where a high word-error-rate can limit the LLM's ability to understand the spoken intent. Instead of chasing a high accuracy by designing complex or specialized architectures regardless of deployment costs, we seek to answer how far we can go without substantially changing the underlying ASR and LLM, which can potentially be shared by multiple unrelated tasks. To this end, we propose prompting the LLM with an n-best list of ASR hypotheses instead of only the error-prone 1-best hypothesis. We explore prompt-engineering to explain the concept of n-best lists to the LLM; followed by the finetuning of Low-Rank Adapters [1] on the downstream tasks. Our approach using n-best lists proves to be effective on a device-directed speech detection task as well as on a keyword spotting task, where systems using n-best list prompts outperform those using 1-best ASR hypothesis; thus paving the way for an efficient method to exploit ASR uncertainty via LLMs for speech-based applications.",
      "abstract": "While large language models excel in a variety of natural language processing (NLP) tasks, to perform well on spoken language understanding (SLU) tasks, they must either rely on off-the-shelf automatic speech recognition (ASR) systems for transcription, or be equipped with an in-built speech modality. This work focuses on the former scenario, where LLM's accuracy on SLU tasks is constrained by the accuracy of a fixed ASR system on the spoken input. Specifically, we tackle speech-intent classification task, where a high word-error-rate can limit the LLM's ability to understand the spoken intent. Instead of chasing a high accuracy by designing complex or specialized architectures regardless of deployment costs, we seek to answer how far we can go without substantially changing the underlying ASR and LLM, which can potentially be shared by multiple unrelated tasks. To this end, we propose prompting the LLM with an n-best list of ASR hypotheses instead of only the error-prone 1-best hypothesis. We explore prompt-engineering to explain the concept of n-best lists to the LLM; followed by the finetuning of Low-Rank Adapters [1] on the downstream tasks. Our approach using n-best lists proves to be effective on a device-directed speech detection task as well as on a keyword spotting task, where systems using n-best list prompts outperform those using 1-best ASR hypothesis; thus paving the way for an efficient method to exploit ASR uncertainty via LLMs for speech-based applications.",
      "doi": "https://doi.org/10.1109/icassp48485.2024.10446132",
      "openalex_id": "https://openalex.org/W4392908891",
      "arxiv_id": "",
      "publication_date": "2024-03-18",
      "published": "2024-03-18",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "Although it discusses SLU with LLMs and ASR, the title/abstract do not include the required terms like 'spoken language model' (or equivalent) and it focuses on single-turn tasks rather than full-duplex conversational modeling, thus failing the core inclusion criteria and triggering exclusion.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "Although it discusses SLU with LLMs and ASR, the title/abstract do not include the required terms like 'spoken language model' (or equivalent) and it focuses on single-turn tasks rather than full-duplex conversational modeling, thus failing the core inclusion criteria and triggering exclusion.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on leveraging large language models with ASR hypotheses for speech-intent classification tasks, treating ASR output as text input without emphasizing primary modeling or inference on acoustic or speech token sequences. It does not address full-duplex spoken language modeling, cognitive parallelism, or interactive synchronization paradigms, nor does it propose architecture or evaluation relevant to spoken language models as defined. The research primarily explores prompt engineering for downstream tasks, with ASR as a fixed black box, and lacks broader modeling or multi-task generalization discussions; thus, it does not meet the inclusion criteria centered on spoken language modeling or full-duplex interaction and fails several exclusion criteria related to solely text-based processing of ASR output without voice modeling innovation.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on leveraging large language models with ASR hypotheses for speech-intent classification tasks, treating ASR output as text input without emphasizing primary modeling or inference on acoustic or speech token sequences. It does not address full-duplex spoken language modeling, cognitive parallelism, or interactive synchronization paradigms, nor does it propose architecture or evaluation relevant to spoken language models as defined. The research primarily explores prompt engineering for downstream tasks, with ASR as a fixed black box, and lacks broader modeling or multi-task generalization discussions; thus, it does not meet the inclusion criteria centered on spoken language modeling or full-duplex interaction and fails several exclusion criteria related to solely text-based processing of ASR output without voice modeling innovation.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "A Comprehensive Evaluation of Multimodal Large Language Models in Hydrological Applications",
    "abstract": "Large Language Models (LLMs) combined with visual foundation models have demonstrated remarkable advancements, achieving a level of intelligence comparable to human capabilities. In this study, we conduct an analysis of the latest Multimodal LLMs (MLLMs), specifically Multimodal-GPT, GPT-4 Vision, Gemini and LLaVa, focusing on their application in the hydrology domain. The hydrology domain holds significant relevance for AI intelligence applications, including flood management and response, water level monitoring, agricultural water discharge, and water pollution management. Our analysis involves testing these MLLMs on various hydrology-specific studies, evaluating their response generation, and assessing their suitability for real-time systems. We deliberately selected complex real-world scenarios to explore the potential of MLLMs in addressing hydrological challenges. Additionally, we carefully designed prompts to enhance the models' visual inference capabilities and their ability to comprehend context from image data. The findings from our analysis reveal effective human-computer interaction and inspire potential solutions for real-world hydrological inference systems that incorporate both textual and image data. Among the validated models, GPT-4 Vision stands out as the top performer among other MLLMs, showcasing unparalleled proficiency in inferring visual data. The results highlight the significant understanding, reasoning, and decision-making capabilities that multimodal foundation models bring to the domain of hydrology. This research contributes valuable insights into the potential applications of advanced AI models in addressing complex challenges within hydrological contexts.",
    "metadata": {
      "title": "A Comprehensive Evaluation of Multimodal Large Language Models in Hydrological Applications",
      "summary": "Large Language Models (LLMs) combined with visual foundation models have demonstrated remarkable advancements, achieving a level of intelligence comparable to human capabilities. In this study, we conduct an analysis of the latest Multimodal LLMs (MLLMs), specifically Multimodal-GPT, GPT-4 Vision, Gemini and LLaVa, focusing on their application in the hydrology domain. The hydrology domain holds significant relevance for AI intelligence applications, including flood management and response, water level monitoring, agricultural water discharge, and water pollution management. Our analysis involves testing these MLLMs on various hydrology-specific studies, evaluating their response generation, and assessing their suitability for real-time systems. We deliberately selected complex real-world scenarios to explore the potential of MLLMs in addressing hydrological challenges. Additionally, we carefully designed prompts to enhance the models' visual inference capabilities and their ability to comprehend context from image data. The findings from our analysis reveal effective human-computer interaction and inspire potential solutions for real-world hydrological inference systems that incorporate both textual and image data. Among the validated models, GPT-4 Vision stands out as the top performer among other MLLMs, showcasing unparalleled proficiency in inferring visual data. The results highlight the significant understanding, reasoning, and decision-making capabilities that multimodal foundation models bring to the domain of hydrology. This research contributes valuable insights into the potential applications of advanced AI models in addressing complex challenges within hydrological contexts.",
      "abstract": "Large Language Models (LLMs) combined with visual foundation models have demonstrated remarkable advancements, achieving a level of intelligence comparable to human capabilities. In this study, we conduct an analysis of the latest Multimodal LLMs (MLLMs), specifically Multimodal-GPT, GPT-4 Vision, Gemini and LLaVa, focusing on their application in the hydrology domain. The hydrology domain holds significant relevance for AI intelligence applications, including flood management and response, water level monitoring, agricultural water discharge, and water pollution management. Our analysis involves testing these MLLMs on various hydrology-specific studies, evaluating their response generation, and assessing their suitability for real-time systems. We deliberately selected complex real-world scenarios to explore the potential of MLLMs in addressing hydrological challenges. Additionally, we carefully designed prompts to enhance the models' visual inference capabilities and their ability to comprehend context from image data. The findings from our analysis reveal effective human-computer interaction and inspire potential solutions for real-world hydrological inference systems that incorporate both textual and image data. Among the validated models, GPT-4 Vision stands out as the top performer among other MLLMs, showcasing unparalleled proficiency in inferring visual data. The results highlight the significant understanding, reasoning, and decision-making capabilities that multimodal foundation models bring to the domain of hydrology. This research contributes valuable insights into the potential applications of advanced AI models in addressing complex challenges within hydrological contexts.",
      "doi": "https://doi.org/10.31223/x5tq37",
      "openalex_id": "https://openalex.org/W4399011256",
      "arxiv_id": "",
      "publication_date": "2024-05-25",
      "published": "2024-05-25",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The article analyzes multimodal LLMs with vision in hydrology and does not center on spoken language models (SLMs) or speech-based interaction, therefore it fails to meet the core inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The article analyzes multimodal LLMs with vision in hydrology and does not center on spoken language models (SLMs) or speech-based interaction, therefore it fails to meet the core inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on the application of Multimodal Large Language Models (MLLMs) in hydrological contexts, assessing their capabilities in processing both textual and visual data. It does not discuss Spoken Language Models or speech-centric modeling, training, or evaluation; nor does it address full-duplex speech interaction, speech synchronization, or speech-specific challenges related to real-time spoken communication. Therefore, it fails to meet the inclusion criteria centered on speech/sound-based language modeling and interaction architectures, and is outside the scope of the defined research area.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on the application of Multimodal Large Language Models (MLLMs) in hydrological contexts, assessing their capabilities in processing both textual and visual data. It does not discuss Spoken Language Models or speech-centric modeling, training, or evaluation; nor does it address full-duplex speech interaction, speech synchronization, or speech-specific challenges related to real-time spoken communication. Therefore, it fails to meet the inclusion criteria centered on speech/sound-based language modeling and interaction architectures, and is outside the scope of the defined research area.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Improving End-to-End Speech Processing by Efficient Text Data Utilization with Latent Synthesis",
    "abstract": "Training a high performance end-to-end speech (E2E) processing model requires an enormous amount of labeled speech data, especially in the era of data-centric artificial intelligence. However, labeled speech data are usually scarcer and more expensive for collection, compared to textual data. We propose Latent Synthesis (LaSyn), an efficient textual data utilization framework for E2E speech processing models. We train a latent synthesizer to convert textual data into an intermediate latent representation of a pre-trained speech model. These pseudo acoustic representations of textual data augment acoustic data for model training. We evaluate LaSyn on low-resource automatic speech recognition (ASR) and spoken language understanding (SLU) tasks. For ASR, LaSyn improves an E2E baseline trained on LibriSpeech train-clean-100, with relative word error rate reductions over 22.3% on different test sets. For SLU, LaSyn improves our E2E baseline by absolute 4.1% for intent classification accuracy and 3.8% for slot filling SLU-F1 on SLURP, and absolute 4.49% and 2.25% for exact match (EM) and EM-Tree accuracies on STOP respectively. With fewer parameters, the results of LaSyn are competitive to published state-of-the-art works. The results demonstrate the quality of the augmented training data.",
    "metadata": {
      "title": "Improving End-to-End Speech Processing by Efficient Text Data Utilization with Latent Synthesis",
      "summary": "Training a high performance end-to-end speech (E2E) processing model requires an enormous amount of labeled speech data, especially in the era of data-centric artificial intelligence. However, labeled speech data are usually scarcer and more expensive for collection, compared to textual data. We propose Latent Synthesis (LaSyn), an efficient textual data utilization framework for E2E speech processing models. We train a latent synthesizer to convert textual data into an intermediate latent representation of a pre-trained speech model. These pseudo acoustic representations of textual data augment acoustic data for model training. We evaluate LaSyn on low-resource automatic speech recognition (ASR) and spoken language understanding (SLU) tasks. For ASR, LaSyn improves an E2E baseline trained on LibriSpeech train-clean-100, with relative word error rate reductions over 22.3% on different test sets. For SLU, LaSyn improves our E2E baseline by absolute 4.1% for intent classification accuracy and 3.8% for slot filling SLU-F1 on SLURP, and absolute 4.49% and 2.25% for exact match (EM) and EM-Tree accuracies on STOP respectively. With fewer parameters, the results of LaSyn are competitive to published state-of-the-art works. The results demonstrate the quality of the augmented training data.",
      "abstract": "Training a high performance end-to-end speech (E2E) processing model requires an enormous amount of labeled speech data, especially in the era of data-centric artificial intelligence. However, labeled speech data are usually scarcer and more expensive for collection, compared to textual data. We propose Latent Synthesis (LaSyn), an efficient textual data utilization framework for E2E speech processing models. We train a latent synthesizer to convert textual data into an intermediate latent representation of a pre-trained speech model. These pseudo acoustic representations of textual data augment acoustic data for model training. We evaluate LaSyn on low-resource automatic speech recognition (ASR) and spoken language understanding (SLU) tasks. For ASR, LaSyn improves an E2E baseline trained on LibriSpeech train-clean-100, with relative word error rate reductions over 22.3% on different test sets. For SLU, LaSyn improves our E2E baseline by absolute 4.1% for intent classification accuracy and 3.8% for slot filling SLU-F1 on SLURP, and absolute 4.49% and 2.25% for exact match (EM) and EM-Tree accuracies on STOP respectively. With fewer parameters, the results of LaSyn are competitive to published state-of-the-art works. The results demonstrate the quality of the augmented training data.",
      "doi": "https://doi.org/10.18653/v1/2023.findings-emnlp.327",
      "openalex_id": "https://openalex.org/W4389519824",
      "arxiv_id": "",
      "publication_date": "2023-01-01",
      "published": "2023-01-01",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "Although the paper targets end-to-end speech processing with text data augmentation, it does not explicitly center on Spoken Language Models (SLMs) or Full-Duplex SLMs, nor does its title/abstract/keywords clearly use the required terminology (e.g., ‘spoken language model(s)’ or ‘language modeling of speech’), so it likely fails to meet ALL inclusion criteria and the explicit exclusions.",
      "evaluation": 2
    },
    "round-A_JuniorNano_reasoning": "Although the paper targets end-to-end speech processing with text data augmentation, it does not explicitly center on Spoken Language Models (SLMs) or Full-Duplex SLMs, nor does its title/abstract/keywords clearly use the required terminology (e.g., ‘spoken language model(s)’ or ‘language modeling of speech’), so it likely fails to meet ALL inclusion criteria and the explicit exclusions.",
    "round-A_JuniorNano_evaluation": 2,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on improving end-to-end speech processing models by utilizing textual data through latent synthesis, aiming at low-resource ASR and SLU tasks. However, it centers on standard speech recognition and understanding improvements, with no discussion or emphasis on spoken language models as core, full-duplex synchronous interaction, cognitive parallelism, or benchmark evaluation of dialogue behaviors, which are essential inclusion criteria. The evaluation metrics focus on WER, intent classification accuracy, and slot filling, none of which relate to full-duplex interaction or synchronization. Thus, it does not meet the thematic core of spoken language modeling with synchronous interaction and multi-dimensional evaluation required by the inclusion criteria, nor does it address the exclusion criteria aspects sufficiently. Therefore, it should be excluded.",
      "evaluation": 2
    },
    "round-A_JuniorMini_reasoning": "The study focuses on improving end-to-end speech processing models by utilizing textual data through latent synthesis, aiming at low-resource ASR and SLU tasks. However, it centers on standard speech recognition and understanding improvements, with no discussion or emphasis on spoken language models as core, full-duplex synchronous interaction, cognitive parallelism, or benchmark evaluation of dialogue behaviors, which are essential inclusion criteria. The evaluation metrics focus on WER, intent classification accuracy, and slot filling, none of which relate to full-duplex interaction or synchronization. Thus, it does not meet the thematic core of spoken language modeling with synchronous interaction and multi-dimensional evaluation required by the inclusion criteria, nor does it address the exclusion criteria aspects sufficiently. Therefore, it should be excluded.",
    "round-A_JuniorMini_evaluation": 2,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:2)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Multimodal Large Language Models in Health Care: Applications, Challenges, and Future Outlook (Preprint)",
    "abstract": "<sec> <title>UNSTRUCTURED</title> In the complex and multidimensional field of medicine, multimodal data are prevalent and crucial for informed clinical decisions. Multimodal data span a broad spectrum of data types, including medical images (eg, MRI and CT scans), time-series data (eg, sensor data from wearable devices and electronic health records), audio recordings (eg, heart and respiratory sounds and patient interviews), text (eg, clinical notes and research articles), videos (eg, surgical procedures), and omics data (eg, genomics and proteomics). While advancements in large language models (LLMs) have enabled new applications for knowledge retrieval and processing in the medical field, most LLMs remain limited to processing unimodal data, typically text-based content, and often overlook the importance of integrating the diverse data modalities encountered in clinical practice. This paper aims to present a detailed, practical, and solution-oriented perspective on the use of multimodal LLMs (M-LLMs) in the medical field. Our investigation spanned M-LLM foundational principles, current and potential applications, technical and ethical challenges, and future research directions. By connecting these elements, we aimed to provide a comprehensive framework that links diverse aspects of M-LLMs, offering a unified vision for their future in health care. This approach aims to guide both future research and practical implementations of M-LLMs in health care, positioning them as a paradigm shift toward integrated, multimodal data–driven medical practice. We anticipate that this work will spark further discussion and inspire the development of innovative approaches in the next generation of medical M-LLM systems. </sec>",
    "metadata": {
      "title": "Multimodal Large Language Models in Health Care: Applications, Challenges, and Future Outlook (Preprint)",
      "summary": "<sec> <title>UNSTRUCTURED</title> In the complex and multidimensional field of medicine, multimodal data are prevalent and crucial for informed clinical decisions. Multimodal data span a broad spectrum of data types, including medical images (eg, MRI and CT scans), time-series data (eg, sensor data from wearable devices and electronic health records), audio recordings (eg, heart and respiratory sounds and patient interviews), text (eg, clinical notes and research articles), videos (eg, surgical procedures), and omics data (eg, genomics and proteomics). While advancements in large language models (LLMs) have enabled new applications for knowledge retrieval and processing in the medical field, most LLMs remain limited to processing unimodal data, typically text-based content, and often overlook the importance of integrating the diverse data modalities encountered in clinical practice. This paper aims to present a detailed, practical, and solution-oriented perspective on the use of multimodal LLMs (M-LLMs) in the medical field. Our investigation spanned M-LLM foundational principles, current and potential applications, technical and ethical challenges, and future research directions. By connecting these elements, we aimed to provide a comprehensive framework that links diverse aspects of M-LLMs, offering a unified vision for their future in health care. This approach aims to guide both future research and practical implementations of M-LLMs in health care, positioning them as a paradigm shift toward integrated, multimodal data–driven medical practice. We anticipate that this work will spark further discussion and inspire the development of innovative approaches in the next generation of medical M-LLM systems. </sec>",
      "abstract": "<sec> <title>UNSTRUCTURED</title> In the complex and multidimensional field of medicine, multimodal data are prevalent and crucial for informed clinical decisions. Multimodal data span a broad spectrum of data types, including medical images (eg, MRI and CT scans), time-series data (eg, sensor data from wearable devices and electronic health records), audio recordings (eg, heart and respiratory sounds and patient interviews), text (eg, clinical notes and research articles), videos (eg, surgical procedures), and omics data (eg, genomics and proteomics). While advancements in large language models (LLMs) have enabled new applications for knowledge retrieval and processing in the medical field, most LLMs remain limited to processing unimodal data, typically text-based content, and often overlook the importance of integrating the diverse data modalities encountered in clinical practice. This paper aims to present a detailed, practical, and solution-oriented perspective on the use of multimodal LLMs (M-LLMs) in the medical field. Our investigation spanned M-LLM foundational principles, current and potential applications, technical and ethical challenges, and future research directions. By connecting these elements, we aimed to provide a comprehensive framework that links diverse aspects of M-LLMs, offering a unified vision for their future in health care. This approach aims to guide both future research and practical implementations of M-LLMs in health care, positioning them as a paradigm shift toward integrated, multimodal data–driven medical practice. We anticipate that this work will spark further discussion and inspire the development of innovative approaches in the next generation of medical M-LLM systems. </sec>",
      "doi": "https://doi.org/10.2196/preprints.59505",
      "openalex_id": "https://openalex.org/W4402830007",
      "arxiv_id": "",
      "publication_date": "2024-04-13",
      "published": "2024-04-13",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item discusses multimodal LLMs in healthcare but does not centralize Spoken Language Models or speech-language modeling concepts (no emphasis on SLM/FD-SLM, synchronization, or speech-centric evaluation). It is not aligned with inclusion criteria and is better excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item discusses multimodal LLMs in healthcare but does not centralize Spoken Language Models or speech-language modeling concepts (no emphasis on SLM/FD-SLM, synchronization, or speech-centric evaluation). It is not aligned with inclusion criteria and is better excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on multimodal large language models in health care, emphasizing integration of various data types including text, images, audio, and omics data. However, it does not specifically address spoken language models or speech as the core modeling and reasoning object, nor does it discuss full-duplex spoken language modeling, synchronization, or real-time interaction as required by the inclusion criteria. It is more about multimodal medical information processing rather than speech-language modeling within an integrated spoken language framework. Thus, it does not meet the inclusion criteria and falls under exclusion for focusing mainly on multimodal data broadly without specific spoken language model focus.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on multimodal large language models in health care, emphasizing integration of various data types including text, images, audio, and omics data. However, it does not specifically address spoken language models or speech as the core modeling and reasoning object, nor does it discuss full-duplex spoken language modeling, synchronization, or real-time interaction as required by the inclusion criteria. It is more about multimodal medical information processing rather than speech-language modeling within an integrated spoken language framework. Thus, it does not meet the inclusion criteria and falls under exclusion for focusing mainly on multimodal data broadly without specific spoken language model focus.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "StreamAdapter: Efficient Test Time Adaptation from Contextual Streams",
    "abstract": "In-context learning (ICL) allows large language models (LLMs) to adapt to new tasks directly from the given demonstrations without requiring gradient updates. While recent advances have expanded context windows to accommodate more demonstrations, this approach increases inference costs without necessarily improving performance. To mitigate these issues, We propose StreamAdapter, a novel approach that directly updates model parameters from context at test time, eliminating the need for explicit in-context demonstrations. StreamAdapter employs context mapping and weight absorption mechanisms to dynamically transform ICL demonstrations into parameter updates with minimal additional parameters. By reducing reliance on numerous in-context examples, StreamAdapter significantly reduce inference costs and allows for efficient inference with constant time complexity, regardless of demonstration count. Extensive experiments across diverse tasks and model architectures demonstrate that StreamAdapter achieves comparable or superior adaptation capability to ICL while requiring significantly fewer demonstrations. The superior task adaptation and context encoding capabilities of StreamAdapter on both language understanding and generation tasks provides a new perspective for adapting LLMs at test time using context, allowing for more efficient adaptation across scenarios and more cost-effective inference.",
    "metadata": {
      "title": "StreamAdapter: Efficient Test Time Adaptation from Contextual Streams",
      "summary": "In-context learning (ICL) allows large language models (LLMs) to adapt to new tasks directly from the given demonstrations without requiring gradient updates. While recent advances have expanded context windows to accommodate more demonstrations, this approach increases inference costs without necessarily improving performance. To mitigate these issues, We propose StreamAdapter, a novel approach that directly updates model parameters from context at test time, eliminating the need for explicit in-context demonstrations. StreamAdapter employs context mapping and weight absorption mechanisms to dynamically transform ICL demonstrations into parameter updates with minimal additional parameters. By reducing reliance on numerous in-context examples, StreamAdapter significantly reduce inference costs and allows for efficient inference with constant time complexity, regardless of demonstration count. Extensive experiments across diverse tasks and model architectures demonstrate that StreamAdapter achieves comparable or superior adaptation capability to ICL while requiring significantly fewer demonstrations. The superior task adaptation and context encoding capabilities of StreamAdapter on both language understanding and generation tasks provides a new perspective for adapting LLMs at test time using context, allowing for more efficient adaptation across scenarios and more cost-effective inference.",
      "abstract": "In-context learning (ICL) allows large language models (LLMs) to adapt to new tasks directly from the given demonstrations without requiring gradient updates. While recent advances have expanded context windows to accommodate more demonstrations, this approach increases inference costs without necessarily improving performance. To mitigate these issues, We propose StreamAdapter, a novel approach that directly updates model parameters from context at test time, eliminating the need for explicit in-context demonstrations. StreamAdapter employs context mapping and weight absorption mechanisms to dynamically transform ICL demonstrations into parameter updates with minimal additional parameters. By reducing reliance on numerous in-context examples, StreamAdapter significantly reduce inference costs and allows for efficient inference with constant time complexity, regardless of demonstration count. Extensive experiments across diverse tasks and model architectures demonstrate that StreamAdapter achieves comparable or superior adaptation capability to ICL while requiring significantly fewer demonstrations. The superior task adaptation and context encoding capabilities of StreamAdapter on both language understanding and generation tasks provides a new perspective for adapting LLMs at test time using context, allowing for more efficient adaptation across scenarios and more cost-effective inference.",
      "doi": "https://doi.org/10.32388/g5h95j",
      "openalex_id": "https://openalex.org/W4404865873",
      "arxiv_id": "",
      "publication_date": "2024-11-29",
      "published": "2024-11-29",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "此項著重於通用大語言模型的測試時參數更新與成本，未以語音或語音-語言混合（SLMs/FD-SLMs）為核心模型對象，亦未針對語音建模與語音互動進行評估，因此不符合「Spoken Language Models / 語音語言模型」主題與相關評估範疇；屬於與主題不符的研究。",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "此項著重於通用大語言模型的測試時參數更新與成本，未以語音或語音-語言混合（SLMs/FD-SLMs）為核心模型對象，亦未針對語音建模與語音互動進行評估，因此不符合「Spoken Language Models / 語音語言模型」主題與相關評估範疇；屬於與主題不符的研究。",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on test time adaptation for large language models using in-context learning, primarily addressing text-based language models and adaptation mechanisms; it does not discuss spoken language models, speech as a primary modeling object, or full-duplex spoken interaction as per inclusion criteria. The abstract lacks any mention of spoken language models, speech signals, or related concepts central to the defined topic, and does not address synchronous interaction features or speech modeling architectures. Thus, it does not meet the inclusion criteria and matches exclusion conditions related to focus solely on text-based NLP without speech language modeling innovation.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on test time adaptation for large language models using in-context learning, primarily addressing text-based language models and adaptation mechanisms; it does not discuss spoken language models, speech as a primary modeling object, or full-duplex spoken interaction as per inclusion criteria. The abstract lacks any mention of spoken language models, speech signals, or related concepts central to the defined topic, and does not address synchronous interaction features or speech modeling architectures. Thus, it does not meet the inclusion criteria and matches exclusion conditions related to focus solely on text-based NLP without speech language modeling innovation.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Foundation models for exploration geophysics",
    "abstract": "ABSTRACT Recently, large models, or foundation models, have exhibited remarkable performance, profoundly impacting research paradigms in diverse domains. Foundation models, trained on extensive and diverse data sets, provide exceptional generalization abilities, allowing for their straightforward application across various use cases and domains. Exploration geophysics entails processing vast, multimodal, and multitask data sets that have traditionally relied on expert experience and physical principles. These unique characteristics present substantial challenges and compelling opportunities for advancing geophysical foundation models (GeoFMs). However, the development of GeoFMs in exploration geophysics is still at an early stage. This paper provides an overview of the current state and future prospects of GeoFMs in exploration geophysics. We begin by reviewing the development and emergent capabilities of foundation models, emphasizing their growing relevance in this domain. Furthermore, we discuss the hierarchy of GeoFMs for exploration geophysics and the critical techniques used, providing a research workflow that serves as a reference for their development. We then examine how GeoFMs may support various exploration tasks, with first-arrival picking based on the segment anything model as an example. Finally, we summarize the challenges faced in developing GeoFMs, along with future trends and their potential impact on the field.",
    "metadata": {
      "title": "Foundation models for exploration geophysics",
      "summary": "ABSTRACT Recently, large models, or foundation models, have exhibited remarkable performance, profoundly impacting research paradigms in diverse domains. Foundation models, trained on extensive and diverse data sets, provide exceptional generalization abilities, allowing for their straightforward application across various use cases and domains. Exploration geophysics entails processing vast, multimodal, and multitask data sets that have traditionally relied on expert experience and physical principles. These unique characteristics present substantial challenges and compelling opportunities for advancing geophysical foundation models (GeoFMs). However, the development of GeoFMs in exploration geophysics is still at an early stage. This paper provides an overview of the current state and future prospects of GeoFMs in exploration geophysics. We begin by reviewing the development and emergent capabilities of foundation models, emphasizing their growing relevance in this domain. Furthermore, we discuss the hierarchy of GeoFMs for exploration geophysics and the critical techniques used, providing a research workflow that serves as a reference for their development. We then examine how GeoFMs may support various exploration tasks, with first-arrival picking based on the segment anything model as an example. Finally, we summarize the challenges faced in developing GeoFMs, along with future trends and their potential impact on the field.",
      "abstract": "ABSTRACT Recently, large models, or foundation models, have exhibited remarkable performance, profoundly impacting research paradigms in diverse domains. Foundation models, trained on extensive and diverse data sets, provide exceptional generalization abilities, allowing for their straightforward application across various use cases and domains. Exploration geophysics entails processing vast, multimodal, and multitask data sets that have traditionally relied on expert experience and physical principles. These unique characteristics present substantial challenges and compelling opportunities for advancing geophysical foundation models (GeoFMs). However, the development of GeoFMs in exploration geophysics is still at an early stage. This paper provides an overview of the current state and future prospects of GeoFMs in exploration geophysics. We begin by reviewing the development and emergent capabilities of foundation models, emphasizing their growing relevance in this domain. Furthermore, we discuss the hierarchy of GeoFMs for exploration geophysics and the critical techniques used, providing a research workflow that serves as a reference for their development. We then examine how GeoFMs may support various exploration tasks, with first-arrival picking based on the segment anything model as an example. Finally, we summarize the challenges faced in developing GeoFMs, along with future trends and their potential impact on the field.",
      "doi": "https://doi.org/10.1190/geo2024-0916.1",
      "openalex_id": "https://openalex.org/W4413058266",
      "arxiv_id": "",
      "publication_date": "2025-01-11",
      "published": "2025-01-11",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The article centers on foundation models for exploration geophysics, not on Spoken Language Models or speech-language modeling, so it does not meet the inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The article centers on foundation models for exploration geophysics, not on Spoken Language Models or speech-language modeling, so it does not meet the inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper focuses on foundation models in exploration geophysics and does not cover spoken language models or speech language models as defined by the inclusion criteria, nor does it address speech as the core modeling and reasoning object or discuss architecture, training, or evaluation of spoken language models; therefore, it falls outside the defined research scope.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper focuses on foundation models in exploration geophysics and does not cover spoken language models or speech language models as defined by the inclusion criteria, nor does it address speech as the core modeling and reasoning object or discuss architecture, training, or evaluation of spoken language models; therefore, it falls outside the defined research scope.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Large language models for PHM: a review of optimization techniques and applications",
    "abstract": "Abstract The rapid advancement of Large Language Models (LLMs) has created unprecedented opportunities for industrial automation, process optimization, and decision support systems. As industries seek to leverage LLMs for industrial tasks, understanding their architecture, deployment strategies, and fine-tuning methods becomes critical. In this review, we aim to summarize the challenges, key technologies, current status, and future directions of LLM in Prognostics and Health Management(PHM). First, this review introduces deep learning for PHM. We begin by analyzing the architectural considerations and deployment strategies for industrial environments, including acceleration techniques and quantization methods that enable efficient operation on resource-constrained industrial hardware. Second, we investigate Parameter Efficient Fine-Tuning (PEFT) techniques that allow industry-specific adaptation without prohibitive computational costs. Multi-modal capabilities extending LLMs beyond text to process sensor data, images, and time-series information are also discussed. Finally, we explore emerging PHM including anomaly detection systems that identify equipment malfunctions, fault diagnosis frameworks that determine root causes, and specialized question-answering systems that empower workers with instant domain expertise. We conclude by identifying key challenges and future research directions for LLM deployment in PHM. This review provides a timely resource for researchers, engineers, and decision-makers navigating the transformative potential of language models in industry 4.0 environments.",
    "metadata": {
      "title": "Large language models for PHM: a review of optimization techniques and applications",
      "summary": "Abstract The rapid advancement of Large Language Models (LLMs) has created unprecedented opportunities for industrial automation, process optimization, and decision support systems. As industries seek to leverage LLMs for industrial tasks, understanding their architecture, deployment strategies, and fine-tuning methods becomes critical. In this review, we aim to summarize the challenges, key technologies, current status, and future directions of LLM in Prognostics and Health Management(PHM). First, this review introduces deep learning for PHM. We begin by analyzing the architectural considerations and deployment strategies for industrial environments, including acceleration techniques and quantization methods that enable efficient operation on resource-constrained industrial hardware. Second, we investigate Parameter Efficient Fine-Tuning (PEFT) techniques that allow industry-specific adaptation without prohibitive computational costs. Multi-modal capabilities extending LLMs beyond text to process sensor data, images, and time-series information are also discussed. Finally, we explore emerging PHM including anomaly detection systems that identify equipment malfunctions, fault diagnosis frameworks that determine root causes, and specialized question-answering systems that empower workers with instant domain expertise. We conclude by identifying key challenges and future research directions for LLM deployment in PHM. This review provides a timely resource for researchers, engineers, and decision-makers navigating the transformative potential of language models in industry 4.0 environments.",
      "abstract": "Abstract The rapid advancement of Large Language Models (LLMs) has created unprecedented opportunities for industrial automation, process optimization, and decision support systems. As industries seek to leverage LLMs for industrial tasks, understanding their architecture, deployment strategies, and fine-tuning methods becomes critical. In this review, we aim to summarize the challenges, key technologies, current status, and future directions of LLM in Prognostics and Health Management(PHM). First, this review introduces deep learning for PHM. We begin by analyzing the architectural considerations and deployment strategies for industrial environments, including acceleration techniques and quantization methods that enable efficient operation on resource-constrained industrial hardware. Second, we investigate Parameter Efficient Fine-Tuning (PEFT) techniques that allow industry-specific adaptation without prohibitive computational costs. Multi-modal capabilities extending LLMs beyond text to process sensor data, images, and time-series information are also discussed. Finally, we explore emerging PHM including anomaly detection systems that identify equipment malfunctions, fault diagnosis frameworks that determine root causes, and specialized question-answering systems that empower workers with instant domain expertise. We conclude by identifying key challenges and future research directions for LLM deployment in PHM. This review provides a timely resource for researchers, engineers, and decision-makers navigating the transformative potential of language models in industry 4.0 environments.",
      "doi": "https://doi.org/10.1007/s43684-025-00100-5",
      "openalex_id": "https://openalex.org/W4413304852",
      "arxiv_id": "",
      "publication_date": "2025-08-19",
      "published": "2025-08-19",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This item focuses on general LLMs in PHM and multi-modal/industrial deployment, not on Spoken Language Models or speech-centric architecture, failing the core inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "This item focuses on general LLMs in PHM and multi-modal/industrial deployment, not on Spoken Language Models or speech-centric architecture, failing the core inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on Large Language Models applied to Prognostics and Health Management (PHM), discussing deep learning architectures, fine-tuning, and industrial applications; however, it does not address speech or spoken language modeling, full-duplex spoken interaction, or any speech-level language modeling aspects required by the inclusion criteria. It does not mention spoken language models, speech language models, nor relevant synchronization or real-time interaction features, and thus fails to meet the inclusion criteria and falls under exclusion criteria for not focusing on spoken language modeling or speech-centric tasks.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on Large Language Models applied to Prognostics and Health Management (PHM), discussing deep learning architectures, fine-tuning, and industrial applications; however, it does not address speech or spoken language modeling, full-duplex spoken interaction, or any speech-level language modeling aspects required by the inclusion criteria. It does not mention spoken language models, speech language models, nor relevant synchronization or real-time interaction features, and thus fails to meet the inclusion criteria and falls under exclusion criteria for not focusing on spoken language modeling or speech-centric tasks.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Symmetry-Aware Advances in Multimodal Large Language Models: Architectures, Training, and Evaluation",
    "abstract": "With the exponential growth of multimodal data, the limitations of traditional unimodal models in cross-modal understanding and complex scenario reasoning have become increasingly evident. Built upon the foundation of Large Language Models (LLMs), Multimodal Large Language Models (MLLMs) retain strong reasoning abilities and demonstrate unique capabilities in multimodal understanding. This survey provides a comprehensive overview of the current research landscape of MLLMs. It systematically analyzes mainstream model architectures, training, fine-tuning strategies, and task classifications, while offering a structured account of evaluation methodologies. Beyond synthesis, the paper highlights emerging trends that aim for balanced integration across modalities, tasks, and components, and critically examines key challenges together with potential solutions. The survey specifically emphasizes recent reasoning-oriented MLLMs, with a focus on DeepSeek-R1, analyzing their design paradigms and contributions from the perspective of symmetric reasoning capabilities. Overall, this work offers a comprehensive overview of cutting-edge advancements and lays a foundation for the future development of MLLMs, especially those guided by symmetry principles.",
    "metadata": {
      "title": "Symmetry-Aware Advances in Multimodal Large Language Models: Architectures, Training, and Evaluation",
      "summary": "With the exponential growth of multimodal data, the limitations of traditional unimodal models in cross-modal understanding and complex scenario reasoning have become increasingly evident. Built upon the foundation of Large Language Models (LLMs), Multimodal Large Language Models (MLLMs) retain strong reasoning abilities and demonstrate unique capabilities in multimodal understanding. This survey provides a comprehensive overview of the current research landscape of MLLMs. It systematically analyzes mainstream model architectures, training, fine-tuning strategies, and task classifications, while offering a structured account of evaluation methodologies. Beyond synthesis, the paper highlights emerging trends that aim for balanced integration across modalities, tasks, and components, and critically examines key challenges together with potential solutions. The survey specifically emphasizes recent reasoning-oriented MLLMs, with a focus on DeepSeek-R1, analyzing their design paradigms and contributions from the perspective of symmetric reasoning capabilities. Overall, this work offers a comprehensive overview of cutting-edge advancements and lays a foundation for the future development of MLLMs, especially those guided by symmetry principles.",
      "abstract": "With the exponential growth of multimodal data, the limitations of traditional unimodal models in cross-modal understanding and complex scenario reasoning have become increasingly evident. Built upon the foundation of Large Language Models (LLMs), Multimodal Large Language Models (MLLMs) retain strong reasoning abilities and demonstrate unique capabilities in multimodal understanding. This survey provides a comprehensive overview of the current research landscape of MLLMs. It systematically analyzes mainstream model architectures, training, fine-tuning strategies, and task classifications, while offering a structured account of evaluation methodologies. Beyond synthesis, the paper highlights emerging trends that aim for balanced integration across modalities, tasks, and components, and critically examines key challenges together with potential solutions. The survey specifically emphasizes recent reasoning-oriented MLLMs, with a focus on DeepSeek-R1, analyzing their design paradigms and contributions from the perspective of symmetric reasoning capabilities. Overall, this work offers a comprehensive overview of cutting-edge advancements and lays a foundation for the future development of MLLMs, especially those guided by symmetry principles.",
      "doi": "https://doi.org/10.3390/sym17091400",
      "openalex_id": "https://openalex.org/W4413796162",
      "arxiv_id": "",
      "publication_date": "2025-08-28",
      "published": "2025-08-28",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This item is a survey on Multimodal LLMs with a focus on symmetry and reasoning, but it does not center Spoken Language Models (SLMs) or speech-language modeling as the core object, nor does it emphasize synchronous speech interaction; thus it fails the inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "This item is a survey on Multimodal LLMs with a focus on symmetry and reasoning, but it does not center Spoken Language Models (SLMs) or speech-language modeling as the core object, nor does it emphasize synchronous speech interaction; thus it fails the inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The article focuses on Multimodal Large Language Models related to multimodal understanding and reasoning, without emphasizing Spoken Language Models or speech as the primary modeling object; it lacks discussion on full-duplex spoken language interaction, speech-centric architectures, or relevant benchmarks as required by the inclusion criteria, and appears to be outside the subject scope focusing on speech sequence modeling and synchronous interaction behaviors.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The article focuses on Multimodal Large Language Models related to multimodal understanding and reasoning, without emphasizing Spoken Language Models or speech as the primary modeling object; it lacks discussion on full-duplex spoken language interaction, speech-centric architectures, or relevant benchmarks as required by the inclusion criteria, and appears to be outside the subject scope focusing on speech sequence modeling and synchronous interaction behaviors.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "SoundTwin: A High-Similarity Fast Diffusion Autoregressive Speech Cloning Model Based on Local-Global Feature Fusion",
    "abstract": "<title>Abstract</title> In recent years, significant advances in deep learning have propelled text-to-speech (TTS) technology forward. However, there are still challenges in high-quality voice cloning with low-resource and low-latency conditions. Traditional autoregressive models suffer from high inference latency, while emerging diffusion models, although capable of generating high-fidelity speech, incur substantial computational overhead due to their multi-step sampling process. To mitigate these limitations, we propose SoundTwin, a novel speech synthesis framework integrating accelerated diffusion sampling with an autoregressive Transformer architecture. This approach significantly enhances synthesis efficiency without compromising speech naturalness. Furthermore, we design a Local-Global Squeeze-and-Excitation weighted Speaker embedding Network to efficiently extract fine-grained timbre features from limited reference audio, enabling rapid speaker adaptation. The model accepts target text, reference speech, and reference text as inputs, jointly modeling duration, pitch, and energy features to generate high-quality mel-spectrograms. Experimental results validate that our method achieves state-of-the-art speaker similarity and speech naturalness in zero-shot voice cloning tasks.",
    "metadata": {
      "title": "SoundTwin: A High-Similarity Fast Diffusion Autoregressive Speech Cloning Model Based on Local-Global Feature Fusion",
      "summary": "<title>Abstract</title> In recent years, significant advances in deep learning have propelled text-to-speech (TTS) technology forward. However, there are still challenges in high-quality voice cloning with low-resource and low-latency conditions. Traditional autoregressive models suffer from high inference latency, while emerging diffusion models, although capable of generating high-fidelity speech, incur substantial computational overhead due to their multi-step sampling process. To mitigate these limitations, we propose SoundTwin, a novel speech synthesis framework integrating accelerated diffusion sampling with an autoregressive Transformer architecture. This approach significantly enhances synthesis efficiency without compromising speech naturalness. Furthermore, we design a Local-Global Squeeze-and-Excitation weighted Speaker embedding Network to efficiently extract fine-grained timbre features from limited reference audio, enabling rapid speaker adaptation. The model accepts target text, reference speech, and reference text as inputs, jointly modeling duration, pitch, and energy features to generate high-quality mel-spectrograms. Experimental results validate that our method achieves state-of-the-art speaker similarity and speech naturalness in zero-shot voice cloning tasks.",
      "abstract": "<title>Abstract</title> In recent years, significant advances in deep learning have propelled text-to-speech (TTS) technology forward. However, there are still challenges in high-quality voice cloning with low-resource and low-latency conditions. Traditional autoregressive models suffer from high inference latency, while emerging diffusion models, although capable of generating high-fidelity speech, incur substantial computational overhead due to their multi-step sampling process. To mitigate these limitations, we propose SoundTwin, a novel speech synthesis framework integrating accelerated diffusion sampling with an autoregressive Transformer architecture. This approach significantly enhances synthesis efficiency without compromising speech naturalness. Furthermore, we design a Local-Global Squeeze-and-Excitation weighted Speaker embedding Network to efficiently extract fine-grained timbre features from limited reference audio, enabling rapid speaker adaptation. The model accepts target text, reference speech, and reference text as inputs, jointly modeling duration, pitch, and energy features to generate high-quality mel-spectrograms. Experimental results validate that our method achieves state-of-the-art speaker similarity and speech naturalness in zero-shot voice cloning tasks.",
      "doi": "https://doi.org/10.21203/rs.3.rs-7512915/v1",
      "openalex_id": "https://openalex.org/W4416754455",
      "arxiv_id": "",
      "publication_date": "2025-11-27",
      "published": "2025-11-27",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item centers on a speech-language modeling task (speech synthesis/voice cloning) with modeling and evaluation of speech, within the last 3 years, and it is not limited to ASR or single-turn dialogues; thus it matches the Spoken Language Model/FD-SLM focus and passes the exclusion criteria.",
      "evaluation": 5
    },
    "round-A_JuniorNano_reasoning": "The item centers on a speech-language modeling task (speech synthesis/voice cloning) with modeling and evaluation of speech, within the last 3 years, and it is not limited to ASR or single-turn dialogues; thus it matches the Spoken Language Model/FD-SLM focus and passes the exclusion criteria.",
    "round-A_JuniorNano_evaluation": 5,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on a speech cloning model leveraging a diffusion and autoregressive approach to generate high-quality speech, primarily addressing TTS and voice cloning efficiency and naturalness. However, it does not explicitly address Spoken Language Models or Full-Duplex Spoken Language Models, nor does it discuss synchronous interaction capabilities, cognitive parallelism, or behavior arbitration relevant to the inclusion criteria. The model is centered on speech synthesis rather than speech-level language modeling or synchronous spoken interaction, and the evaluation focuses on speaker similarity and naturalness without real-time interactive metrics.",
      "evaluation": 2
    },
    "round-A_JuniorMini_reasoning": "The study focuses on a speech cloning model leveraging a diffusion and autoregressive approach to generate high-quality speech, primarily addressing TTS and voice cloning efficiency and naturalness. However, it does not explicitly address Spoken Language Models or Full-Duplex Spoken Language Models, nor does it discuss synchronous interaction capabilities, cognitive parallelism, or behavior arbitration relevant to the inclusion criteria. The model is centered on speech synthesis rather than speech-level language modeling or synchronous spoken interaction, and the evaluation focuses on speaker similarity and naturalness without real-time interactive metrics.",
    "round-A_JuniorMini_evaluation": 2,
    "round-B_SeniorLead_output": {
      "reasoning": "The two junior reviewers disagreed: JuniorNano rated include (5) viewing it as speech-language modeling, while JuniorMini rated exclude (2) noting it is purely a TTS/voice-cloning work without SLM/FD-SLM or synchronous interaction content. After review, I agree with JuniorMini: the abstract describes a TTS/voice-cloning model (mel-spectrogram generation, speaker embedding, zero-shot cloning) and does not mention \"spoken language model(s)\" or equivalent, nor address full-duplex interaction, cognitive parallelism, overlap/interrupt handling, or low-latency interactive evaluation — therefore it fails the inclusion criteria and meets the exclusion criteria for single-task TTS. Score explanation: Exclude because the paper is focused on TTS/voice cloning rather than Spoken Language Models or full‑duplex spoken interaction research.",
      "evaluation": 1
    },
    "round-B_SeniorLead_reasoning": "The two junior reviewers disagreed: JuniorNano rated include (5) viewing it as speech-language modeling, while JuniorMini rated exclude (2) noting it is purely a TTS/voice-cloning work without SLM/FD-SLM or synchronous interaction content. After review, I agree with JuniorMini: the abstract describes a TTS/voice-cloning model (mel-spectrogram generation, speaker embedding, zero-shot cloning) and does not mention \"spoken language model(s)\" or equivalent, nor address full-duplex interaction, cognitive parallelism, overlap/interrupt handling, or low-latency interactive evaluation — therefore it fails the inclusion criteria and meets the exclusion criteria for single-task TTS. Score explanation: Exclude because the paper is focused on TTS/voice cloning rather than Spoken Language Models or full‑duplex spoken interaction research.",
    "round-B_SeniorLead_evaluation": 1,
    "final_verdict": "exclude (senior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "A Multi-modal LLM for Dynamic Protein-Ligand Interactions and Generative Molecular Design",
    "abstract": "Abstract BioDynaGen (Biological Dynamics and Generation) is a novel multi-modal framework unifying protein sequences, dynamic binding site conformations, small molecule ligand SMILES, and natural language text into a single discrete token representation. Built upon a general large language model, BioDynaGen employs continuous pre-training and instruction fine-tuning via next-token prediction to address critical gaps in modeling protein dynamics and ligand interactions. This framework enables a diverse range of tasks, including small molecule-protein binding prediction, dynamic pocket design, and ligand-assisted functional generation. By comprehensively integrating these modalities, BioDynaGen offers an advanced framework for understanding and designing complex biological molecular interactions.",
    "metadata": {
      "title": "A Multi-modal LLM for Dynamic Protein-Ligand Interactions and Generative Molecular Design",
      "summary": "Abstract BioDynaGen (Biological Dynamics and Generation) is a novel multi-modal framework unifying protein sequences, dynamic binding site conformations, small molecule ligand SMILES, and natural language text into a single discrete token representation. Built upon a general large language model, BioDynaGen employs continuous pre-training and instruction fine-tuning via next-token prediction to address critical gaps in modeling protein dynamics and ligand interactions. This framework enables a diverse range of tasks, including small molecule-protein binding prediction, dynamic pocket design, and ligand-assisted functional generation. By comprehensively integrating these modalities, BioDynaGen offers an advanced framework for understanding and designing complex biological molecular interactions.",
      "abstract": "Abstract BioDynaGen (Biological Dynamics and Generation) is a novel multi-modal framework unifying protein sequences, dynamic binding site conformations, small molecule ligand SMILES, and natural language text into a single discrete token representation. Built upon a general large language model, BioDynaGen employs continuous pre-training and instruction fine-tuning via next-token prediction to address critical gaps in modeling protein dynamics and ligand interactions. This framework enables a diverse range of tasks, including small molecule-protein binding prediction, dynamic pocket design, and ligand-assisted functional generation. By comprehensively integrating these modalities, BioDynaGen offers an advanced framework for understanding and designing complex biological molecular interactions.",
      "doi": "https://doi.org/10.64898/2025.12.01.691647",
      "openalex_id": "https://openalex.org/W4417116566",
      "arxiv_id": "",
      "publication_date": "2025-12-03",
      "published": "2025-12-03",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This item focuses on a multi-modal protein–ligand design framework and does not treat Spoken Language Models or speech as the core modeling/reasoning target, thus it fails the inclusion criterion requiring SLM/FD-SLM focus.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "This item focuses on a multi-modal protein–ligand design framework and does not treat Spoken Language Models or speech as the core modeling/reasoning target, thus it fails the inclusion criterion requiring SLM/FD-SLM focus.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper focuses on protein-ligand interactions using a multi-modal large language model integrating biological sequences and molecular representations, with no mention of spoken language models, speech language modeling, or related concepts focused on speech/audio processing as required by the inclusion criteria; it thus fails to meet the core topical requirement for spoken language modeling and synchronization in spoken interaction systems, and is unrelated to speech/audio-language modeling topics outlined.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper focuses on protein-ligand interactions using a multi-modal large language model integrating biological sequences and molecular representations, with no mention of spoken language models, speech language modeling, or related concepts focused on speech/audio processing as required by the inclusion criteria; it thus fails to meet the core topical requirement for spoken language modeling and synchronization in spoken interaction systems, and is unrelated to speech/audio-language modeling topics outlined.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Two-Stage Domain Adaptation for LLM-Based ASR by Decoupling Linguistic and Acoustic Factors",
    "abstract": "Large language models (LLMs) have been increasingly applied in Automatic Speech Recognition (ASR), achieving significant advancements. However, the performance of LLM-based ASR (LLM-ASR) models remains unsatisfactory when applied across domains due to domain shifts between acoustic and linguistic conditions. To address this challenge, we propose a decoupled two-stage domain adaptation framework that separates the adaptation process into text-only and audio-only stages. In the first stage, we leverage abundant text data from the target domain to refine the LLM component, thereby improving its contextual and linguistic alignment with the target domain. In the second stage, we employ a pseudo-labeling method with unlabeled audio data in the target domain and introduce two key enhancements: (1) incorporating decoupled auxiliary Connectionist Temporal Classification (CTC) loss to improve the robustness of the speech encoder under different acoustic conditions; (2) adopting a synchronous LLM tuning strategy, allowing the LLM to continuously learn linguistic alignment from pseudo-labeled transcriptions enriched with domain textual knowledge. The experimental results demonstrate that our proposed methods significantly improve the performance of LLM-ASR in the target domain, achieving a relative word error rate reduction of 19.2%.",
    "metadata": {
      "title": "Two-Stage Domain Adaptation for LLM-Based ASR by Decoupling Linguistic and Acoustic Factors",
      "summary": "Large language models (LLMs) have been increasingly applied in Automatic Speech Recognition (ASR), achieving significant advancements. However, the performance of LLM-based ASR (LLM-ASR) models remains unsatisfactory when applied across domains due to domain shifts between acoustic and linguistic conditions. To address this challenge, we propose a decoupled two-stage domain adaptation framework that separates the adaptation process into text-only and audio-only stages. In the first stage, we leverage abundant text data from the target domain to refine the LLM component, thereby improving its contextual and linguistic alignment with the target domain. In the second stage, we employ a pseudo-labeling method with unlabeled audio data in the target domain and introduce two key enhancements: (1) incorporating decoupled auxiliary Connectionist Temporal Classification (CTC) loss to improve the robustness of the speech encoder under different acoustic conditions; (2) adopting a synchronous LLM tuning strategy, allowing the LLM to continuously learn linguistic alignment from pseudo-labeled transcriptions enriched with domain textual knowledge. The experimental results demonstrate that our proposed methods significantly improve the performance of LLM-ASR in the target domain, achieving a relative word error rate reduction of 19.2%.",
      "abstract": "Large language models (LLMs) have been increasingly applied in Automatic Speech Recognition (ASR), achieving significant advancements. However, the performance of LLM-based ASR (LLM-ASR) models remains unsatisfactory when applied across domains due to domain shifts between acoustic and linguistic conditions. To address this challenge, we propose a decoupled two-stage domain adaptation framework that separates the adaptation process into text-only and audio-only stages. In the first stage, we leverage abundant text data from the target domain to refine the LLM component, thereby improving its contextual and linguistic alignment with the target domain. In the second stage, we employ a pseudo-labeling method with unlabeled audio data in the target domain and introduce two key enhancements: (1) incorporating decoupled auxiliary Connectionist Temporal Classification (CTC) loss to improve the robustness of the speech encoder under different acoustic conditions; (2) adopting a synchronous LLM tuning strategy, allowing the LLM to continuously learn linguistic alignment from pseudo-labeled transcriptions enriched with domain textual knowledge. The experimental results demonstrate that our proposed methods significantly improve the performance of LLM-ASR in the target domain, achieving a relative word error rate reduction of 19.2%.",
      "doi": "https://doi.org/10.3390/app16010060",
      "openalex_id": "https://openalex.org/W7116905298",
      "arxiv_id": "",
      "publication_date": "2025-12-20",
      "published": "2025-12-20",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The study centers on Spoken Language Modeling by integrating ASR with LLMs and discusses architecture, training, and evaluation (across acoustic and linguistic domains) within the last 3 years, aligning with the inclusion criteria and not clearly falling under the exclusion criteria.",
      "evaluation": 4
    },
    "round-A_JuniorNano_reasoning": "The study centers on Spoken Language Modeling by integrating ASR with LLMs and discusses architecture, training, and evaluation (across acoustic and linguistic domains) within the last 3 years, aligning with the inclusion criteria and not clearly falling under the exclusion criteria.",
    "round-A_JuniorNano_evaluation": 4,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on improving LLM-based ASR via domain adaptation, specifically addressing linguistic and acoustic factor separation, but it centers primarily on ASR performance enhancement without discussing spoken language models, full-duplex interaction, real-time conversational behavior, or language modeling of speech as a core modeling object; moreover, evaluation is based on word error rate without metrics related to interactive or overlapping speech handling, thus failing to meet the inclusion criteria related to spoken language modeling and synchronous interaction, while fitting several exclusion criteria.",
      "evaluation": 2
    },
    "round-A_JuniorMini_reasoning": "The study focuses on improving LLM-based ASR via domain adaptation, specifically addressing linguistic and acoustic factor separation, but it centers primarily on ASR performance enhancement without discussing spoken language models, full-duplex interaction, real-time conversational behavior, or language modeling of speech as a core modeling object; moreover, evaluation is based on word error rate without metrics related to interactive or overlapping speech handling, thus failing to meet the inclusion criteria related to spoken language modeling and synchronous interaction, while fitting several exclusion criteria.",
    "round-A_JuniorMini_evaluation": 2,
    "round-B_SeniorLead_output": {
      "reasoning": "兩位初審意見分歧（JuniorNano 建議納入，JuniorMini 建議排除）。經審查，該篇僅聚焦於 LLM 輔助的 ASR 領域適應（text-only 與 audio-only 的分階段微調與 pseudo-labeling），以降低 WER 為主要目標；摘要中未以 “Spoken Language Model(s)” 或等價描述為核心，也未探討全雙工/同步聽說、語音層級的 language modeling、語音重疊處理、即時延遲或行為仲裁等重點議題，評估僅以 WER 為主。故不符合納入標準且落入多項排除條件（僅為 ASR 技術改進、語音僅作為輸入/轉寫而非核心語音建模），應排除。 簡短說明：因此評分1（絕對排除）— 不符合本主題對 Spoken Language Models/Full-Duplex 能力與語音為核心建模之嚴格要求。",
      "evaluation": 1
    },
    "round-B_SeniorLead_reasoning": "兩位初審意見分歧（JuniorNano 建議納入，JuniorMini 建議排除）。經審查，該篇僅聚焦於 LLM 輔助的 ASR 領域適應（text-only 與 audio-only 的分階段微調與 pseudo-labeling），以降低 WER 為主要目標；摘要中未以 “Spoken Language Model(s)” 或等價描述為核心，也未探討全雙工/同步聽說、語音層級的 language modeling、語音重疊處理、即時延遲或行為仲裁等重點議題，評估僅以 WER 為主。故不符合納入標準且落入多項排除條件（僅為 ASR 技術改進、語音僅作為輸入/轉寫而非核心語音建模），應排除。 簡短說明：因此評分1（絕對排除）— 不符合本主題對 Spoken Language Models/Full-Duplex 能力與語音為核心建模之嚴格要求。",
    "round-B_SeniorLead_evaluation": 1,
    "final_verdict": "exclude (senior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "SEF-VC: Speaker Embedding Free Zero-Shot Voice Conversion with Cross Attention",
    "abstract": "Zero-shot voice conversion (VC) aims to transfer the source speaker timbre to arbitrary unseen target speaker timbre, while keeping the linguistic content unchanged. Although the voice of generated speech can be controlled by providing the speaker embedding of the target speaker, the speaker similarity still lags behind the ground truth recordings. In this paper, we propose SEF-VC, a speaker embedding free voice conversion model, which is designed to learn and incorporate speaker timbre from reference speech via a powerful position-agnostic cross-attention mechanism, and then reconstruct waveform from HuBERT semantic tokens in a non-autoregressive manner. The concise design of SEF-VC enhances its training stability and voice conversion performance. Objective and subjective evaluations demonstrate the superiority of SEF-VC to generate high-quality speech with better similarity to target reference than strong zero-shot VC baselines, even for very short reference speeches.",
    "metadata": {
      "title": "SEF-VC: Speaker Embedding Free Zero-Shot Voice Conversion with Cross Attention",
      "summary": "Zero-shot voice conversion (VC) aims to transfer the source speaker timbre to arbitrary unseen target speaker timbre, while keeping the linguistic content unchanged. Although the voice of generated speech can be controlled by providing the speaker embedding of the target speaker, the speaker similarity still lags behind the ground truth recordings. In this paper, we propose SEF-VC, a speaker embedding free voice conversion model, which is designed to learn and incorporate speaker timbre from reference speech via a powerful position-agnostic cross-attention mechanism, and then reconstruct waveform from HuBERT semantic tokens in a non-autoregressive manner. The concise design of SEF-VC enhances its training stability and voice conversion performance. Objective and subjective evaluations demonstrate the superiority of SEF-VC to generate high-quality speech with better similarity to target reference than strong zero-shot VC baselines, even for very short reference speeches.",
      "abstract": "Zero-shot voice conversion (VC) aims to transfer the source speaker timbre to arbitrary unseen target speaker timbre, while keeping the linguistic content unchanged. Although the voice of generated speech can be controlled by providing the speaker embedding of the target speaker, the speaker similarity still lags behind the ground truth recordings. In this paper, we propose SEF-VC, a speaker embedding free voice conversion model, which is designed to learn and incorporate speaker timbre from reference speech via a powerful position-agnostic cross-attention mechanism, and then reconstruct waveform from HuBERT semantic tokens in a non-autoregressive manner. The concise design of SEF-VC enhances its training stability and voice conversion performance. Objective and subjective evaluations demonstrate the superiority of SEF-VC to generate high-quality speech with better similarity to target reference than strong zero-shot VC baselines, even for very short reference speeches.",
      "doi": "https://doi.org/10.1109/icassp48485.2024.10446160",
      "openalex_id": "https://openalex.org/W4392902857",
      "arxiv_id": "",
      "publication_date": "2024-03-18",
      "published": "2024-03-18",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item focuses on zero-shot voice conversion and speaker timbre transfer without explicitly centering on Spoken Language Models or a speech-language modeling framework in its title/abstract/keywords, so it likely does not meet the inclusion criteria that require an explicit SLM/FD-SLM focus.",
      "evaluation": 2
    },
    "round-A_JuniorNano_reasoning": "The item focuses on zero-shot voice conversion and speaker timbre transfer without explicitly centering on Spoken Language Models or a speech-language modeling framework in its title/abstract/keywords, so it likely does not meet the inclusion criteria that require an explicit SLM/FD-SLM focus.",
    "round-A_JuniorNano_evaluation": 2,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on zero-shot voice conversion, aiming to transfer speaker timbre while preserving linguistic content. However, it does not explicitly address Spoken Language Models (SLMs), speech language modeling, or full-duplex spoken interaction capabilities such as synchronous turn-taking, cognitive parallelism, or low-latency interaction. It primarily emphasizes voice conversion quality and speaker embedding techniques without discussing model architectures or training paradigms related to speech language modeling or synchronization in dialogue.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on zero-shot voice conversion, aiming to transfer speaker timbre while preserving linguistic content. However, it does not explicitly address Spoken Language Models (SLMs), speech language modeling, or full-duplex spoken interaction capabilities such as synchronous turn-taking, cognitive parallelism, or low-latency interaction. It primarily emphasizes voice conversion quality and speaker embedding techniques without discussing model architectures or training paradigms related to speech language modeling or synchronization in dialogue.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:2)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Reimagining speech: a scoping review of deep learning-based methods for non-parallel voice conversion",
    "abstract": "Research on deep learning-powered voice conversion (VC) in speech-to-speech scenarios are gaining increasing popularity. Although many of the works in the field of voice conversion share a common global pipeline, there is considerable diversity in the underlying structures, methods, and neural sub-blocks used across research efforts. Thus, obtaining a comprehensive understanding of the reasons behind the choice of the different methods included when training voice conversion models can be challenging, and the actual hurdles in the proposed solutions are often unclear. To shed light on these aspects, this paper presents a scoping review that explores the use of deep learning in speech analysis, synthesis, and disentangled speech representation learning within modern voice conversion systems. We screened 628 publications from more than 38 venues between 2017 and 2023, followed by an in-depth review of a final database of 130 eligible studies. Based on the review, we summarise the most frequently used approaches to voice conversion based on deep learning and highlight common pitfalls. We condense the knowledge gathered to identify main challenges, supply solutions grounded in the analysis and provide recommendations for future research directions.",
    "metadata": {
      "title": "Reimagining speech: a scoping review of deep learning-based methods for non-parallel voice conversion",
      "summary": "Research on deep learning-powered voice conversion (VC) in speech-to-speech scenarios are gaining increasing popularity. Although many of the works in the field of voice conversion share a common global pipeline, there is considerable diversity in the underlying structures, methods, and neural sub-blocks used across research efforts. Thus, obtaining a comprehensive understanding of the reasons behind the choice of the different methods included when training voice conversion models can be challenging, and the actual hurdles in the proposed solutions are often unclear. To shed light on these aspects, this paper presents a scoping review that explores the use of deep learning in speech analysis, synthesis, and disentangled speech representation learning within modern voice conversion systems. We screened 628 publications from more than 38 venues between 2017 and 2023, followed by an in-depth review of a final database of 130 eligible studies. Based on the review, we summarise the most frequently used approaches to voice conversion based on deep learning and highlight common pitfalls. We condense the knowledge gathered to identify main challenges, supply solutions grounded in the analysis and provide recommendations for future research directions.",
      "abstract": "Research on deep learning-powered voice conversion (VC) in speech-to-speech scenarios are gaining increasing popularity. Although many of the works in the field of voice conversion share a common global pipeline, there is considerable diversity in the underlying structures, methods, and neural sub-blocks used across research efforts. Thus, obtaining a comprehensive understanding of the reasons behind the choice of the different methods included when training voice conversion models can be challenging, and the actual hurdles in the proposed solutions are often unclear. To shed light on these aspects, this paper presents a scoping review that explores the use of deep learning in speech analysis, synthesis, and disentangled speech representation learning within modern voice conversion systems. We screened 628 publications from more than 38 venues between 2017 and 2023, followed by an in-depth review of a final database of 130 eligible studies. Based on the review, we summarise the most frequently used approaches to voice conversion based on deep learning and highlight common pitfalls. We condense the knowledge gathered to identify main challenges, supply solutions grounded in the analysis and provide recommendations for future research directions.",
      "doi": "https://doi.org/10.3389/frsip.2024.1339159",
      "openalex_id": "https://openalex.org/W4401652690",
      "arxiv_id": "",
      "publication_date": "2024-08-16",
      "published": "2024-08-16",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item is a scoping review of deep learning-based non-parallel voice conversion, which does not center on Spoken Language Models or full-duplex speech interaction as defined, and its publication window (up to 2023) falls outside the past 3 years, so it fails both the core inclusion and timeframe criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item is a scoping review of deep learning-based non-parallel voice conversion, which does not center on Spoken Language Models or full-duplex speech interaction as defined, and its publication window (up to 2023) falls outside the past 3 years, so it fails both the core inclusion and timeframe criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The reviewed paper focuses on deep learning-based voice conversion methods in speech-to-speech scenarios. However, it does not explicitly address Spoken Language Models (SLMs) or full-duplex spoken interaction systems, nor does it highlight language modeling of speech, synchronous interaction behaviors, or low-latency full-duplex interaction capabilities. It also lacks discussion on turn-taking management, cognitive parallelism, or multi-dimensional evaluation aligned with the inclusion criteria. Instead, it primarily discusses voice conversion architectures and challenges, which are outside the scope of the inclusion criteria emphasizing Spoken Language Models and interactive speech language modeling.",
      "evaluation": 2
    },
    "round-A_JuniorMini_reasoning": "The reviewed paper focuses on deep learning-based voice conversion methods in speech-to-speech scenarios. However, it does not explicitly address Spoken Language Models (SLMs) or full-duplex spoken interaction systems, nor does it highlight language modeling of speech, synchronous interaction behaviors, or low-latency full-duplex interaction capabilities. It also lacks discussion on turn-taking management, cognitive parallelism, or multi-dimensional evaluation aligned with the inclusion criteria. Instead, it primarily discusses voice conversion architectures and challenges, which are outside the scope of the inclusion criteria emphasizing Spoken Language Models and interactive speech language modeling.",
    "round-A_JuniorMini_evaluation": 2,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:2)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Partial Fake Speech Attacks in the Real World Using Deepfake Audio",
    "abstract": "Advances in deep learning have led to dramatic improvements in generative synthetic speech, eliminating robotic speech patterns to create speech that is indistinguishable from a human voice. Although these advances are extremely useful in various applications, they also facilitate powerful attacks against both humans and machines. Recently, a new type of speech attack called partial fake (PF) speech has emerged. This paper studies how well humans and machines, including speaker recognition systems and existing fake-speech detection tools, can distinguish between human voice and computer-generated speech. Our study shows that both humans and machines can be easily deceived by PF speech, and the current defences against PF speech are insufficient. These findings emphasise the urgency of increasing awareness for humans and creating new automated defences against PF speech for machines.",
    "metadata": {
      "title": "Partial Fake Speech Attacks in the Real World Using Deepfake Audio",
      "summary": "Advances in deep learning have led to dramatic improvements in generative synthetic speech, eliminating robotic speech patterns to create speech that is indistinguishable from a human voice. Although these advances are extremely useful in various applications, they also facilitate powerful attacks against both humans and machines. Recently, a new type of speech attack called partial fake (PF) speech has emerged. This paper studies how well humans and machines, including speaker recognition systems and existing fake-speech detection tools, can distinguish between human voice and computer-generated speech. Our study shows that both humans and machines can be easily deceived by PF speech, and the current defences against PF speech are insufficient. These findings emphasise the urgency of increasing awareness for humans and creating new automated defences against PF speech for machines.",
      "abstract": "Advances in deep learning have led to dramatic improvements in generative synthetic speech, eliminating robotic speech patterns to create speech that is indistinguishable from a human voice. Although these advances are extremely useful in various applications, they also facilitate powerful attacks against both humans and machines. Recently, a new type of speech attack called partial fake (PF) speech has emerged. This paper studies how well humans and machines, including speaker recognition systems and existing fake-speech detection tools, can distinguish between human voice and computer-generated speech. Our study shows that both humans and machines can be easily deceived by PF speech, and the current defences against PF speech are insufficient. These findings emphasise the urgency of increasing awareness for humans and creating new automated defences against PF speech for machines.",
      "doi": "https://doi.org/10.3390/jcp5010006",
      "openalex_id": "https://openalex.org/W4407388754",
      "arxiv_id": "",
      "publication_date": "2025-02-08",
      "published": "2025-02-08",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "此論文聚焦於深偽語音攻擊與偵測，未以 Spoken Language Models 或 Full-Duplex Spoken Language Models 為核心，且缺乏對話同步與語音語言建模相關內容，因此建議排除。",
      "evaluation": 2
    },
    "round-A_JuniorNano_reasoning": "此論文聚焦於深偽語音攻擊與偵測，未以 Spoken Language Models 或 Full-Duplex Spoken Language Models 為核心，且缺乏對話同步與語音語言建模相關內容，因此建議排除。",
    "round-A_JuniorNano_evaluation": 2,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on deepfake audio attacks and their detection, which is related to speech synthesis and security, but it does not discuss Spoken Language Models or related language modeling architectures, training, or evaluation as required by the inclusion criteria. It also does not address full-duplex or synchronized interactive speech behaviors but rather concentrates on detection of fake speech; thus, it fails to meet the core thematic requirement and includes no mention of speech language modeling or related evaluation frameworks.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on deepfake audio attacks and their detection, which is related to speech synthesis and security, but it does not discuss Spoken Language Models or related language modeling architectures, training, or evaluation as required by the inclusion criteria. It also does not address full-duplex or synchronized interactive speech behaviors but rather concentrates on detection of fake speech; thus, it fails to meet the core thematic requirement and includes no mention of speech language modeling or related evaluation frameworks.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:2)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "DUB: Discrete Unit Back-translation for Speech Translation",
    "abstract": "How can speech-to-text translation (ST) perform as well as machine translation (MT)? The key point is to bridge the modality gap between speech and text so that useful MT techniques can be applied to ST.Recently, the approach of representing speech with unsupervised discrete units yields a new way to ease the modality problem. This motivates us to propose Discrete Unit Back-translation(DUB) to answer two questions (1) Is it better to represent speech with discrete units than with continuous features in direct ST? (2) How much benefit can useful MT techniques bring to ST? With DUB, the back-translation technique can successfully be applied on direct ST and obtains an average boost of 5.5 BLEU on MuST-C En-De/Fr/Es. In the low-resource language scenario, our method achieves comparable performance to existing methods that rely on large-scale external data. Code and models are available at https://anonymous.4open.science/r/DUB/.",
    "metadata": {
      "title": "DUB: Discrete Unit Back-translation for Speech Translation",
      "summary": "How can speech-to-text translation (ST) perform as well as machine translation (MT)? The key point is to bridge the modality gap between speech and text so that useful MT techniques can be applied to ST.Recently, the approach of representing speech with unsupervised discrete units yields a new way to ease the modality problem. This motivates us to propose Discrete Unit Back-translation(DUB) to answer two questions (1) Is it better to represent speech with discrete units than with continuous features in direct ST? (2) How much benefit can useful MT techniques bring to ST? With DUB, the back-translation technique can successfully be applied on direct ST and obtains an average boost of 5.5 BLEU on MuST-C En-De/Fr/Es. In the low-resource language scenario, our method achieves comparable performance to existing methods that rely on large-scale external data. Code and models are available at https://anonymous.4open.science/r/DUB/.",
      "abstract": "How can speech-to-text translation (ST) perform as well as machine translation (MT)? The key point is to bridge the modality gap between speech and text so that useful MT techniques can be applied to ST.Recently, the approach of representing speech with unsupervised discrete units yields a new way to ease the modality problem. This motivates us to propose Discrete Unit Back-translation(DUB) to answer two questions (1) Is it better to represent speech with discrete units than with continuous features in direct ST? (2) How much benefit can useful MT techniques bring to ST? With DUB, the back-translation technique can successfully be applied on direct ST and obtains an average boost of 5.5 BLEU on MuST-C En-De/Fr/Es. In the low-resource language scenario, our method achieves comparable performance to existing methods that rely on large-scale external data. Code and models are available at https://anonymous.4open.science/r/DUB/.",
      "doi": "https://doi.org/10.18653/v1/2023.findings-acl.447",
      "openalex_id": "https://openalex.org/W4385570101",
      "arxiv_id": "",
      "publication_date": "2023-01-01",
      "published": "2023-01-01",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The paper focuses on speech translation using discrete unit representations and back-translation, but does not treat speech as the primary object of modeling nor discuss Spoken Language Models or full-duplex speech interaction; it lacks emphasis on SLM/FD-SLM architecture, synchronization, or dialogue arbitration, thus does not meet the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The paper focuses on speech translation using discrete unit representations and back-translation, but does not treat speech as the primary object of modeling nor discuss Spoken Language Models or full-duplex speech interaction; it lacks emphasis on SLM/FD-SLM architecture, synchronization, or dialogue arbitration, thus does not meet the inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study proposes a method for speech-to-text translation employing discrete speech units and back-translation, focusing on speech representation and improving translation performance; however, it does not clearly address spoken language models as core concepts, full-duplex or synchronous speech interaction, or cognitive and synchronization aspects emphasized by the inclusion criteria, nor does it discuss turn-taking, real-time interaction, or behavior arbitration. It mainly focuses on speech translation performance improvement, which aligns more with conventional speech processing rather than the advanced spoken language model framework sought.",
      "evaluation": 2
    },
    "round-A_JuniorMini_reasoning": "The study proposes a method for speech-to-text translation employing discrete speech units and back-translation, focusing on speech representation and improving translation performance; however, it does not clearly address spoken language models as core concepts, full-duplex or synchronous speech interaction, or cognitive and synchronization aspects emphasized by the inclusion criteria, nor does it discuss turn-taking, real-time interaction, or behavior arbitration. It mainly focuses on speech translation performance improvement, which aligns more with conventional speech processing rather than the advanced spoken language model framework sought.",
    "round-A_JuniorMini_evaluation": 2,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:2)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "PaLM-E: An Embodied Multimodal Language Model",
    "abstract": "Large language models excel at a wide range of complex tasks. However, enabling general inference in the real world, e.g., for robotics problems, raises the challenge of grounding. We propose embodied language models to directly incorporate real-world continuous sensor modalities into language models and thereby establish the link between words and percepts. Input to our embodied language model are multi-modal sentences that interleave visual, continuous state estimation, and textual input encodings. We train these encodings end-to-end, in conjunction with a pre-trained large language model, for multiple embodied tasks including sequential robotic manipulation planning, visual question answering, and captioning. Our evaluations show that PaLM-E, a single large embodied multimodal model, can address a variety of embodied reasoning tasks, from a variety of observation modalities, on multiple embodiments, and further, exhibits positive transfer: the model benefits from diverse joint training across internet-scale language, vision, and visual-language domains. Our largest model, PaLM-E-562B with 562B parameters, in addition to being trained on robotics tasks, is a visual-language generalist with state-of-the-art performance on OK-VQA, and retains generalist language capabilities with increasing scale.",
    "metadata": {
      "title": "PaLM-E: An Embodied Multimodal Language Model",
      "summary": "Large language models excel at a wide range of complex tasks. However, enabling general inference in the real world, e.g., for robotics problems, raises the challenge of grounding. We propose embodied language models to directly incorporate real-world continuous sensor modalities into language models and thereby establish the link between words and percepts. Input to our embodied language model are multi-modal sentences that interleave visual, continuous state estimation, and textual input encodings. We train these encodings end-to-end, in conjunction with a pre-trained large language model, for multiple embodied tasks including sequential robotic manipulation planning, visual question answering, and captioning. Our evaluations show that PaLM-E, a single large embodied multimodal model, can address a variety of embodied reasoning tasks, from a variety of observation modalities, on multiple embodiments, and further, exhibits positive transfer: the model benefits from diverse joint training across internet-scale language, vision, and visual-language domains. Our largest model, PaLM-E-562B with 562B parameters, in addition to being trained on robotics tasks, is a visual-language generalist with state-of-the-art performance on OK-VQA, and retains generalist language capabilities with increasing scale.",
      "abstract": "Large language models excel at a wide range of complex tasks. However, enabling general inference in the real world, e.g., for robotics problems, raises the challenge of grounding. We propose embodied language models to directly incorporate real-world continuous sensor modalities into language models and thereby establish the link between words and percepts. Input to our embodied language model are multi-modal sentences that interleave visual, continuous state estimation, and textual input encodings. We train these encodings end-to-end, in conjunction with a pre-trained large language model, for multiple embodied tasks including sequential robotic manipulation planning, visual question answering, and captioning. Our evaluations show that PaLM-E, a single large embodied multimodal model, can address a variety of embodied reasoning tasks, from a variety of observation modalities, on multiple embodiments, and further, exhibits positive transfer: the model benefits from diverse joint training across internet-scale language, vision, and visual-language domains. Our largest model, PaLM-E-562B with 562B parameters, in addition to being trained on robotics tasks, is a visual-language generalist with state-of-the-art performance on OK-VQA, and retains generalist language capabilities with increasing scale.",
      "doi": "https://doi.org/10.48550/arxiv.2303.03378",
      "openalex_id": "https://openalex.org/W4323572061",
      "arxiv_id": "",
      "publication_date": "2023-03-06",
      "published": "2023-03-06",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This item describes PaLM-E as an embodied multimodal model with vision and sensor inputs, not centered on Spoken Language Models or speech-language modeling, thus failing the core inclusion criteria requiring SLM/Speech LM focus.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "This item describes PaLM-E as an embodied multimodal model with vision and sensor inputs, not centered on Spoken Language Models or speech-language modeling, thus failing the core inclusion criteria requiring SLM/Speech LM focus.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study titled 'PaLM-E: An Embodied Multimodal Language Model' focuses on embodied language models integrating visual, state estimation, and textual input for robotic and multimodal tasks, with no mention of spoken language models, core speech modeling, or full-duplex spoken interaction capacities as outlined in the inclusion criteria. It neither centers on speech as a primary modeling object nor discusses architecture or evaluations specific to spoken language or speech-based turn-taking, synchronization, or low-latency interaction; hence, it does not satisfy the inclusion criteria and aligns with multiple exclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study titled 'PaLM-E: An Embodied Multimodal Language Model' focuses on embodied language models integrating visual, state estimation, and textual input for robotic and multimodal tasks, with no mention of spoken language models, core speech modeling, or full-duplex spoken interaction capacities as outlined in the inclusion criteria. It neither centers on speech as a primary modeling object nor discusses architecture or evaluations specific to spoken language or speech-based turn-taking, synchronization, or low-latency interaction; hence, it does not satisfy the inclusion criteria and aligns with multiple exclusion criteria.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in Hugging Face",
    "abstract": "Solving complicated AI tasks with different domains and modalities is a key step toward artificial general intelligence. While there are numerous AI models available for various domains and modalities, they cannot handle complicated AI tasks autonomously. Considering large language models (LLMs) have exhibited exceptional abilities in language understanding, generation, interaction, and reasoning, we advocate that LLMs could act as a controller to manage existing AI models to solve complicated AI tasks, with language serving as a generic interface to empower this. Based on this philosophy, we present HuggingGPT, an LLM-powered agent that leverages LLMs (e.g., ChatGPT) to connect various AI models in machine learning communities (e.g., Hugging Face) to solve AI tasks. Specifically, we use ChatGPT to conduct task planning when receiving a user request, select models according to their function descriptions available in Hugging Face, execute each subtask with the selected AI model, and summarize the response according to the execution results. By leveraging the strong language capability of ChatGPT and abundant AI models in Hugging Face, HuggingGPT can tackle a wide range of sophisticated AI tasks spanning different modalities and domains and achieve impressive results in language, vision, speech, and other challenging tasks, which paves a new way towards the realization of artificial general intelligence.",
    "metadata": {
      "title": "HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in Hugging Face",
      "summary": "Solving complicated AI tasks with different domains and modalities is a key step toward artificial general intelligence. While there are numerous AI models available for various domains and modalities, they cannot handle complicated AI tasks autonomously. Considering large language models (LLMs) have exhibited exceptional abilities in language understanding, generation, interaction, and reasoning, we advocate that LLMs could act as a controller to manage existing AI models to solve complicated AI tasks, with language serving as a generic interface to empower this. Based on this philosophy, we present HuggingGPT, an LLM-powered agent that leverages LLMs (e.g., ChatGPT) to connect various AI models in machine learning communities (e.g., Hugging Face) to solve AI tasks. Specifically, we use ChatGPT to conduct task planning when receiving a user request, select models according to their function descriptions available in Hugging Face, execute each subtask with the selected AI model, and summarize the response according to the execution results. By leveraging the strong language capability of ChatGPT and abundant AI models in Hugging Face, HuggingGPT can tackle a wide range of sophisticated AI tasks spanning different modalities and domains and achieve impressive results in language, vision, speech, and other challenging tasks, which paves a new way towards the realization of artificial general intelligence.",
      "abstract": "Solving complicated AI tasks with different domains and modalities is a key step toward artificial general intelligence. While there are numerous AI models available for various domains and modalities, they cannot handle complicated AI tasks autonomously. Considering large language models (LLMs) have exhibited exceptional abilities in language understanding, generation, interaction, and reasoning, we advocate that LLMs could act as a controller to manage existing AI models to solve complicated AI tasks, with language serving as a generic interface to empower this. Based on this philosophy, we present HuggingGPT, an LLM-powered agent that leverages LLMs (e.g., ChatGPT) to connect various AI models in machine learning communities (e.g., Hugging Face) to solve AI tasks. Specifically, we use ChatGPT to conduct task planning when receiving a user request, select models according to their function descriptions available in Hugging Face, execute each subtask with the selected AI model, and summarize the response according to the execution results. By leveraging the strong language capability of ChatGPT and abundant AI models in Hugging Face, HuggingGPT can tackle a wide range of sophisticated AI tasks spanning different modalities and domains and achieve impressive results in language, vision, speech, and other challenging tasks, which paves a new way towards the realization of artificial general intelligence.",
      "doi": "https://doi.org/10.48550/arxiv.2303.17580",
      "openalex_id": "https://openalex.org/W4361866031",
      "arxiv_id": "",
      "publication_date": "2023-03-30",
      "published": "2023-03-30",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "Reasoning: The item centers on LLM-based task planning and orchestration of external models (HuggingFace tools) rather than Spoken Language Models or Speech Language Models, lacking focus on speech-centric modeling, synchronous interaction, or speech-based evaluation as required by the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "Reasoning: The item centers on LLM-based task planning and orchestration of external models (HuggingFace tools) rather than Spoken Language Models or Speech Language Models, lacking focus on speech-centric modeling, synchronous interaction, or speech-based evaluation as required by the inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on using ChatGPT as a controller to orchestrate various AI models across modalities to solve complex AI tasks, but it does not specifically focus on Spoken Language Models or speech as the primary modeling object; it lacks details on speech modeling architecture, training, or evaluation central to spoken language modeling or full-duplex speech interaction concepts. Therefore, it does not meet the inclusion criteria centered on spoken language modeling and full-duplex spoken language models.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on using ChatGPT as a controller to orchestrate various AI models across modalities to solve complex AI tasks, but it does not specifically focus on Spoken Language Models or speech as the primary modeling object; it lacks details on speech modeling architecture, training, or evaluation central to spoken language modeling or full-duplex speech interaction concepts. Therefore, it does not meet the inclusion criteria centered on spoken language modeling and full-duplex spoken language models.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Language Is Not All You Need: Aligning Perception with Language Models",
    "abstract": "A big convergence of language, multimodal perception, action, and world modeling is a key step toward artificial general intelligence. In this work, we introduce Kosmos-1, a Multimodal Large Language Model (MLLM) that can perceive general modalities, learn in context (i.e., few-shot), and follow instructions (i.e., zero-shot). Specifically, we train Kosmos-1 from scratch on web-scale multimodal corpora, including arbitrarily interleaved text and images, image-caption pairs, and text data. We evaluate various settings, including zero-shot, few-shot, and multimodal chain-of-thought prompting, on a wide range of tasks without any gradient updates or finetuning. Experimental results show that Kosmos-1 achieves impressive performance on (i) language understanding, generation, and even OCR-free NLP (directly fed with document images), (ii) perception-language tasks, including multimodal dialogue, image captioning, visual question answering, and (iii) vision tasks, such as image recognition with descriptions (specifying classification via text instructions). We also show that MLLMs can benefit from cross-modal transfer, i.e., transfer knowledge from language to multimodal, and from multimodal to language. In addition, we introduce a dataset of Raven IQ test, which diagnoses the nonverbal reasoning capability of MLLMs.",
    "metadata": {
      "title": "Language Is Not All You Need: Aligning Perception with Language Models",
      "summary": "A big convergence of language, multimodal perception, action, and world modeling is a key step toward artificial general intelligence. In this work, we introduce Kosmos-1, a Multimodal Large Language Model (MLLM) that can perceive general modalities, learn in context (i.e., few-shot), and follow instructions (i.e., zero-shot). Specifically, we train Kosmos-1 from scratch on web-scale multimodal corpora, including arbitrarily interleaved text and images, image-caption pairs, and text data. We evaluate various settings, including zero-shot, few-shot, and multimodal chain-of-thought prompting, on a wide range of tasks without any gradient updates or finetuning. Experimental results show that Kosmos-1 achieves impressive performance on (i) language understanding, generation, and even OCR-free NLP (directly fed with document images), (ii) perception-language tasks, including multimodal dialogue, image captioning, visual question answering, and (iii) vision tasks, such as image recognition with descriptions (specifying classification via text instructions). We also show that MLLMs can benefit from cross-modal transfer, i.e., transfer knowledge from language to multimodal, and from multimodal to language. In addition, we introduce a dataset of Raven IQ test, which diagnoses the nonverbal reasoning capability of MLLMs.",
      "abstract": "A big convergence of language, multimodal perception, action, and world modeling is a key step toward artificial general intelligence. In this work, we introduce Kosmos-1, a Multimodal Large Language Model (MLLM) that can perceive general modalities, learn in context (i.e., few-shot), and follow instructions (i.e., zero-shot). Specifically, we train Kosmos-1 from scratch on web-scale multimodal corpora, including arbitrarily interleaved text and images, image-caption pairs, and text data. We evaluate various settings, including zero-shot, few-shot, and multimodal chain-of-thought prompting, on a wide range of tasks without any gradient updates or finetuning. Experimental results show that Kosmos-1 achieves impressive performance on (i) language understanding, generation, and even OCR-free NLP (directly fed with document images), (ii) perception-language tasks, including multimodal dialogue, image captioning, visual question answering, and (iii) vision tasks, such as image recognition with descriptions (specifying classification via text instructions). We also show that MLLMs can benefit from cross-modal transfer, i.e., transfer knowledge from language to multimodal, and from multimodal to language. In addition, we introduce a dataset of Raven IQ test, which diagnoses the nonverbal reasoning capability of MLLMs.",
      "doi": "https://doi.org/10.48550/arxiv.2302.14045",
      "openalex_id": "https://openalex.org/W4322718246",
      "arxiv_id": "",
      "publication_date": "2023-02-27",
      "published": "2023-02-27",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "此論文本質聚焦於多模態大語言模型的感知與語言整合，未以 Spoken Language Models、語音語言建模為核心，亦未於摘要中強調語音互動、同步或語音特定評估，因此不符合納入標準中的核心條件。",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "此論文本質聚焦於多模態大語言模型的感知與語言整合，未以 Spoken Language Models、語音語言建模為核心，亦未於摘要中強調語音互動、同步或語音特定評估，因此不符合納入標準中的核心條件。",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The title and abstract describe a multimodal large language model focusing on language and visual modalities, without any mention of spoken language models, speech-level language modeling, or full-duplex spoken interaction features such as cognitive parallelism, overlapping speech, or low-latency interaction. The study centers on general multimodal perception and language tasks rather than speech as the core modeling target and does not address the inclusion criteria related to spoken language modeling or interactive turn-taking. Thus, it does not meet the inclusion criteria and fits several exclusion points, making it unsuitable for inclusion under the given criteria.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The title and abstract describe a multimodal large language model focusing on language and visual modalities, without any mention of spoken language models, speech-level language modeling, or full-duplex spoken interaction features such as cognitive parallelism, overlapping speech, or low-latency interaction. The study centers on general multimodal perception and language tasks rather than speech as the core modeling target and does not address the inclusion criteria related to spoken language modeling or interactive turn-taking. Thus, it does not meet the inclusion criteria and fits several exclusion points, making it unsuitable for inclusion under the given criteria.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "HuBERT: Self-Supervised Speech Representation Learning by Masked Prediction of Hidden Units",
    "abstract": "Self-supervised approaches for speech representation learning are challenged by three unique problems: (1) there are multiple sound units in each input utterance, (2) there is no lexicon of input sound units during the pre-training phase, and (3) sound units have variable lengths with no explicit segmentation. To deal with these three problems, we propose the Hidden-Unit BERT (HuBERT) approach for self-supervised speech representation learning, which utilizes an offline clustering step to provide aligned target labels for a BERT-like prediction loss. A key ingredient of our approach is applying the prediction loss over the masked regions only, which forces the model to learn a combined acoustic and language model over the continuous inputs. HuBERT relies primarily on the consistency of the unsupervised clustering step rather than the intrinsic quality of the assigned cluster labels. Starting with a simple k-means teacher of 100 clusters, and using two iterations of clustering, the HuBERT model either matches or improves upon the state-of-the-art wav2vec 2.0 performance on the Librispeech (960h) and Libri-light (60,000h) benchmarks with 10min, 1h, 10h, 100h, and 960h fine-tuning subsets. Using a 1B parameter model, HuBERT shows up to 19% and 13% relative WER reduction on the more challenging dev-other and test-other evaluation subsets.",
    "metadata": {
      "title": "HuBERT: Self-Supervised Speech Representation Learning by Masked Prediction of Hidden Units",
      "summary": "Self-supervised approaches for speech representation learning are challenged by three unique problems: (1) there are multiple sound units in each input utterance, (2) there is no lexicon of input sound units during the pre-training phase, and (3) sound units have variable lengths with no explicit segmentation. To deal with these three problems, we propose the Hidden-Unit BERT (HuBERT) approach for self-supervised speech representation learning, which utilizes an offline clustering step to provide aligned target labels for a BERT-like prediction loss. A key ingredient of our approach is applying the prediction loss over the masked regions only, which forces the model to learn a combined acoustic and language model over the continuous inputs. HuBERT relies primarily on the consistency of the unsupervised clustering step rather than the intrinsic quality of the assigned cluster labels. Starting with a simple k-means teacher of 100 clusters, and using two iterations of clustering, the HuBERT model either matches or improves upon the state-of-the-art wav2vec 2.0 performance on the Librispeech (960h) and Libri-light (60,000h) benchmarks with 10min, 1h, 10h, 100h, and 960h fine-tuning subsets. Using a 1B parameter model, HuBERT shows up to 19% and 13% relative WER reduction on the more challenging dev-other and test-other evaluation subsets.",
      "abstract": "Self-supervised approaches for speech representation learning are challenged by three unique problems: (1) there are multiple sound units in each input utterance, (2) there is no lexicon of input sound units during the pre-training phase, and (3) sound units have variable lengths with no explicit segmentation. To deal with these three problems, we propose the Hidden-Unit BERT (HuBERT) approach for self-supervised speech representation learning, which utilizes an offline clustering step to provide aligned target labels for a BERT-like prediction loss. A key ingredient of our approach is applying the prediction loss over the masked regions only, which forces the model to learn a combined acoustic and language model over the continuous inputs. HuBERT relies primarily on the consistency of the unsupervised clustering step rather than the intrinsic quality of the assigned cluster labels. Starting with a simple k-means teacher of 100 clusters, and using two iterations of clustering, the HuBERT model either matches or improves upon the state-of-the-art wav2vec 2.0 performance on the Librispeech (960h) and Libri-light (60,000h) benchmarks with 10min, 1h, 10h, 100h, and 960h fine-tuning subsets. Using a 1B parameter model, HuBERT shows up to 19% and 13% relative WER reduction on the more challenging dev-other and test-other evaluation subsets.",
      "doi": "https://doi.org/10.1109/taslp.2021.3122291",
      "openalex_id": "https://openalex.org/W3169320628",
      "arxiv_id": "",
      "publication_date": "2021-01-01",
      "published": "2021-01-01",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The paper is about self-supervised speech representation learning with language modeling elements, but it does not meet the past-3-years publication window required by inclusion criteria, so it should be excluded despite relevance to SLM concepts.",
      "evaluation": 2
    },
    "round-A_JuniorNano_reasoning": "The paper is about self-supervised speech representation learning with language modeling elements, but it does not meet the past-3-years publication window required by inclusion criteria, so it should be excluded despite relevance to SLM concepts.",
    "round-A_JuniorNano_evaluation": 2,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on self-supervised speech representation learning using acoustic signals, but it does not explicitly address spoken language models, full-duplex interaction, synchronization, or speech language modeling as defined in the inclusion criteria. Its evaluation centers on speech recognition benchmarks (WER), lacking discussion on interactive or behavioral arbitration aspects required for inclusion.",
      "evaluation": 2
    },
    "round-A_JuniorMini_reasoning": "The study focuses on self-supervised speech representation learning using acoustic signals, but it does not explicitly address spoken language models, full-duplex interaction, synchronization, or speech language modeling as defined in the inclusion criteria. Its evaluation centers on speech recognition benchmarks (WER), lacking discussion on interactive or behavioral arbitration aspects required for inclusion.",
    "round-A_JuniorMini_evaluation": 2,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:2)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "M3ST: Mix at Three Levels for Speech Translation",
    "abstract": "How to solve the data scarcity problem for end-to-end speech-to-text translation (ST)? It's well known that data augmentation is an efficient method to improve performance for many tasks by enlarging the dataset. In this paper, we propose Mix at three levels for Speech Translation (M^3ST) method to increase the diversity of the augmented training corpus. Specifically, we conduct two phases of fine-tuning based on a pre-trained model using external machine translation (MT) data. In the first stage of fine-tuning, we mix the training corpus at three levels, including word level, sentence level and frame level, and fine-tune the entire model with mixed data. At the second stage of fine-tuning, we take both original speech sequences and original text sequences in parallel into the model to fine-tune the network, and use Jensen-Shannon divergence to regularize their outputs. Experiments on MuST-C speech translation benchmark and analysis show that M^3ST outperforms current strong baselines and achieves state-of-the-art results on eight directions with an average BLEU of 29.9.",
    "metadata": {
      "title": "M3ST: Mix at Three Levels for Speech Translation",
      "summary": "How to solve the data scarcity problem for end-to-end speech-to-text translation (ST)? It's well known that data augmentation is an efficient method to improve performance for many tasks by enlarging the dataset. In this paper, we propose Mix at three levels for Speech Translation (M^3ST) method to increase the diversity of the augmented training corpus. Specifically, we conduct two phases of fine-tuning based on a pre-trained model using external machine translation (MT) data. In the first stage of fine-tuning, we mix the training corpus at three levels, including word level, sentence level and frame level, and fine-tune the entire model with mixed data. At the second stage of fine-tuning, we take both original speech sequences and original text sequences in parallel into the model to fine-tune the network, and use Jensen-Shannon divergence to regularize their outputs. Experiments on MuST-C speech translation benchmark and analysis show that M^3ST outperforms current strong baselines and achieves state-of-the-art results on eight directions with an average BLEU of 29.9.",
      "abstract": "How to solve the data scarcity problem for end-to-end speech-to-text translation (ST)? It's well known that data augmentation is an efficient method to improve performance for many tasks by enlarging the dataset. In this paper, we propose Mix at three levels for Speech Translation (M^3ST) method to increase the diversity of the augmented training corpus. Specifically, we conduct two phases of fine-tuning based on a pre-trained model using external machine translation (MT) data. In the first stage of fine-tuning, we mix the training corpus at three levels, including word level, sentence level and frame level, and fine-tune the entire model with mixed data. At the second stage of fine-tuning, we take both original speech sequences and original text sequences in parallel into the model to fine-tune the network, and use Jensen-Shannon divergence to regularize their outputs. Experiments on MuST-C speech translation benchmark and analysis show that M^3ST outperforms current strong baselines and achieves state-of-the-art results on eight directions with an average BLEU of 29.9.",
      "doi": "https://doi.org/10.48550/arxiv.2212.03657",
      "openalex_id": "https://openalex.org/W4310924890",
      "arxiv_id": "",
      "publication_date": "2022-12-07",
      "published": "2022-12-07",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "本論文聚焦端到端語音翻譯之資料增強，未在標題或摘要中明確出現 spoken language model(s) / 語音語言建模等等價描述，因此不符合必須出現語音語言模型的包含條件，故應排除。",
      "evaluation": 2
    },
    "round-A_JuniorNano_reasoning": "本論文聚焦端到端語音翻譯之資料增強，未在標題或摘要中明確出現 spoken language model(s) / 語音語言建模等等價描述，因此不符合必須出現語音語言模型的包含條件，故應排除。",
    "round-A_JuniorNano_evaluation": 2,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on data augmentation techniques for end-to-end speech-to-text translation using mixing methods at multiple levels, but it does not explicitly address Spoken Language Models (SLMs) or full-duplex spoken language modeling, nor does it mention synchronous interaction capabilities, behavioral arbitration, or low-latency real-time interaction which are key inclusion criteria. It treats speech primarily as input for translation rather than as the central modeling object, and it lacks discussion involving spoken language model architectures or evaluation metrics related to real-time interactive spoken language modeling.",
      "evaluation": 2
    },
    "round-A_JuniorMini_reasoning": "The study focuses on data augmentation techniques for end-to-end speech-to-text translation using mixing methods at multiple levels, but it does not explicitly address Spoken Language Models (SLMs) or full-duplex spoken language modeling, nor does it mention synchronous interaction capabilities, behavioral arbitration, or low-latency real-time interaction which are key inclusion criteria. It treats speech primarily as input for translation rather than as the central modeling object, and it lacks discussion involving spoken language model architectures or evaluation metrics related to real-time interactive spoken language modeling.",
    "round-A_JuniorMini_evaluation": 2,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:2)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "PaLM: Scaling Language Modeling with Pathways",
    "abstract": "Large language models have been shown to achieve remarkable performance across a variety of natural language tasks using few-shot learning, which drastically reduces the number of task-specific training examples needed to adapt the model to a particular application. To further our understanding of the impact of scale on few-shot learning, we trained a 540-billion parameter, densely activated, Transformer language model, which we call Pathways Language Model PaLM. We trained PaLM on 6144 TPU v4 chips using Pathways, a new ML system which enables highly efficient training across multiple TPU Pods. We demonstrate continued benefits of scaling by achieving state-of-the-art few-shot learning results on hundreds of language understanding and generation benchmarks. On a number of these tasks, PaLM 540B achieves breakthrough performance, outperforming the finetuned state-of-the-art on a suite of multi-step reasoning tasks, and outperforming average human performance on the recently released BIG-bench benchmark. A significant number of BIG-bench tasks showed discontinuous improvements from model scale, meaning that performance steeply increased as we scaled to our largest model. PaLM also has strong capabilities in multilingual tasks and source code generation, which we demonstrate on a wide array of benchmarks. We additionally provide a comprehensive analysis on bias and toxicity, and study the extent of training data memorization with respect to model scale. Finally, we discuss the ethical considerations related to large language models and discuss potential mitigation strategies.",
    "metadata": {
      "title": "PaLM: Scaling Language Modeling with Pathways",
      "summary": "Large language models have been shown to achieve remarkable performance across a variety of natural language tasks using few-shot learning, which drastically reduces the number of task-specific training examples needed to adapt the model to a particular application. To further our understanding of the impact of scale on few-shot learning, we trained a 540-billion parameter, densely activated, Transformer language model, which we call Pathways Language Model PaLM. We trained PaLM on 6144 TPU v4 chips using Pathways, a new ML system which enables highly efficient training across multiple TPU Pods. We demonstrate continued benefits of scaling by achieving state-of-the-art few-shot learning results on hundreds of language understanding and generation benchmarks. On a number of these tasks, PaLM 540B achieves breakthrough performance, outperforming the finetuned state-of-the-art on a suite of multi-step reasoning tasks, and outperforming average human performance on the recently released BIG-bench benchmark. A significant number of BIG-bench tasks showed discontinuous improvements from model scale, meaning that performance steeply increased as we scaled to our largest model. PaLM also has strong capabilities in multilingual tasks and source code generation, which we demonstrate on a wide array of benchmarks. We additionally provide a comprehensive analysis on bias and toxicity, and study the extent of training data memorization with respect to model scale. Finally, we discuss the ethical considerations related to large language models and discuss potential mitigation strategies.",
      "abstract": "Large language models have been shown to achieve remarkable performance across a variety of natural language tasks using few-shot learning, which drastically reduces the number of task-specific training examples needed to adapt the model to a particular application. To further our understanding of the impact of scale on few-shot learning, we trained a 540-billion parameter, densely activated, Transformer language model, which we call Pathways Language Model PaLM. We trained PaLM on 6144 TPU v4 chips using Pathways, a new ML system which enables highly efficient training across multiple TPU Pods. We demonstrate continued benefits of scaling by achieving state-of-the-art few-shot learning results on hundreds of language understanding and generation benchmarks. On a number of these tasks, PaLM 540B achieves breakthrough performance, outperforming the finetuned state-of-the-art on a suite of multi-step reasoning tasks, and outperforming average human performance on the recently released BIG-bench benchmark. A significant number of BIG-bench tasks showed discontinuous improvements from model scale, meaning that performance steeply increased as we scaled to our largest model. PaLM also has strong capabilities in multilingual tasks and source code generation, which we demonstrate on a wide array of benchmarks. We additionally provide a comprehensive analysis on bias and toxicity, and study the extent of training data memorization with respect to model scale. Finally, we discuss the ethical considerations related to large language models and discuss potential mitigation strategies.",
      "doi": "https://doi.org/10.48550/arxiv.2204.02311",
      "openalex_id": "https://openalex.org/W4224308101",
      "arxiv_id": "",
      "publication_date": "2022-04-05",
      "published": "2022-04-05",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "PaLM is a text-focused language-modeling paper with no discussion of Spoken Language Models or speech-centric architectures or evaluations, so it does not meet the inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "PaLM is a text-focused language-modeling paper with no discussion of Spoken Language Models or speech-centric architectures or evaluations, so it does not meet the inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper discusses a large-scale language model focusing on natural language tasks and few-shot learning using text-based Transformer models without addressing spoken language modeling, speech as a core modeling object, full-duplex interaction, or any speech-specific architectures or evaluations as required by the inclusion criteria; thus it does not meet the inclusion requirements regarding spoken language or speech-based modeling.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper discusses a large-scale language model focusing on natural language tasks and few-shot learning using text-based Transformer models without addressing spoken language modeling, speech as a core modeling object, full-duplex interaction, or any speech-specific architectures or evaluations as required by the inclusion criteria; thus it does not meet the inclusion requirements regarding spoken language or speech-based modeling.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale",
    "abstract": "While the Transformer architecture has become the de-facto standard for natural language processing tasks, its applications to computer vision remain limited. In vision, attention is either applied in conjunction with convolutional networks, or used to replace certain components of convolutional networks while keeping their overall structure in place. We show that this reliance on CNNs is not necessary and a pure transformer applied directly to sequences of image patches can perform very well on image classification tasks. When pre-trained on large amounts of data and transferred to multiple mid-sized or small image recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision Transformer (ViT) attains excellent results compared to state-of-the-art convolutional networks while requiring substantially fewer computational resources to train.",
    "metadata": {
      "title": "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale",
      "summary": "While the Transformer architecture has become the de-facto standard for natural language processing tasks, its applications to computer vision remain limited. In vision, attention is either applied in conjunction with convolutional networks, or used to replace certain components of convolutional networks while keeping their overall structure in place. We show that this reliance on CNNs is not necessary and a pure transformer applied directly to sequences of image patches can perform very well on image classification tasks. When pre-trained on large amounts of data and transferred to multiple mid-sized or small image recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision Transformer (ViT) attains excellent results compared to state-of-the-art convolutional networks while requiring substantially fewer computational resources to train.",
      "abstract": "While the Transformer architecture has become the de-facto standard for natural language processing tasks, its applications to computer vision remain limited. In vision, attention is either applied in conjunction with convolutional networks, or used to replace certain components of convolutional networks while keeping their overall structure in place. We show that this reliance on CNNs is not necessary and a pure transformer applied directly to sequences of image patches can perform very well on image classification tasks. When pre-trained on large amounts of data and transferred to multiple mid-sized or small image recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision Transformer (ViT) attains excellent results compared to state-of-the-art convolutional networks while requiring substantially fewer computational resources to train.",
      "doi": "https://doi.org/10.48550/arxiv.2010.11929",
      "openalex_id": "https://openalex.org/W3094502228",
      "arxiv_id": "",
      "publication_date": "2020-10-22",
      "published": "2020-10-22",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "Reasoning: The item is a Vision Transformer paper focused on image recognition, not on Spoken Language Models or speech-language modeling; it does not meet the inclusion criteria and fails the core topic requirement.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "Reasoning: The item is a Vision Transformer paper focused on image recognition, not on Spoken Language Models or speech-language modeling; it does not meet the inclusion criteria and fails the core topic requirement.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper focuses on the Vision Transformer for image classification tasks, dealing with image patches and computer vision, with no mention of spoken language models, speech signals, or any related full-duplex spoken interaction capabilities required by the inclusion criteria. It does not address speech language modeling, speech-LLM hybrid models, or any synchronization and interaction features in speech, thus failing the core inclusion criteria and falling under exclusion for being unrelated to the topic.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper focuses on the Vision Transformer for image classification tasks, dealing with image patches and computer vision, with no mention of spoken language models, speech signals, or any related full-duplex spoken interaction capabilities required by the inclusion criteria. It does not address speech language modeling, speech-LLM hybrid models, or any synchronization and interaction features in speech, thus failing the core inclusion criteria and falling under exclusion for being unrelated to the topic.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "LoRA: Low-Rank Adaptation of Large Language Models",
    "abstract": "An important paradigm of natural language processing consists of large-scale pre-training on general domain data and adaptation to particular tasks or domains. As we pre-train larger models, full fine-tuning, which retrains all model parameters, becomes less feasible. Using GPT-3 175B as an example -- deploying independent instances of fine-tuned models, each with 175B parameters, is prohibitively expensive. We propose Low-Rank Adaptation, or LoRA, which freezes the pre-trained model weights and injects trainable rank decomposition matrices into each layer of the Transformer architecture, greatly reducing the number of trainable parameters for downstream tasks. Compared to GPT-3 175B fine-tuned with Adam, LoRA can reduce the number of trainable parameters by 10,000 times and the GPU memory requirement by 3 times. LoRA performs on-par or better than fine-tuning in model quality on RoBERTa, DeBERTa, GPT-2, and GPT-3, despite having fewer trainable parameters, a higher training throughput, and, unlike adapters, no additional inference latency. We also provide an empirical investigation into rank-deficiency in language model adaptation, which sheds light on the efficacy of LoRA. We release a package that facilitates the integration of LoRA with PyTorch models and provide our implementations and model checkpoints for RoBERTa, DeBERTa, and GPT-2 at https://github.com/microsoft/LoRA.",
    "metadata": {
      "title": "LoRA: Low-Rank Adaptation of Large Language Models",
      "summary": "An important paradigm of natural language processing consists of large-scale pre-training on general domain data and adaptation to particular tasks or domains. As we pre-train larger models, full fine-tuning, which retrains all model parameters, becomes less feasible. Using GPT-3 175B as an example -- deploying independent instances of fine-tuned models, each with 175B parameters, is prohibitively expensive. We propose Low-Rank Adaptation, or LoRA, which freezes the pre-trained model weights and injects trainable rank decomposition matrices into each layer of the Transformer architecture, greatly reducing the number of trainable parameters for downstream tasks. Compared to GPT-3 175B fine-tuned with Adam, LoRA can reduce the number of trainable parameters by 10,000 times and the GPU memory requirement by 3 times. LoRA performs on-par or better than fine-tuning in model quality on RoBERTa, DeBERTa, GPT-2, and GPT-3, despite having fewer trainable parameters, a higher training throughput, and, unlike adapters, no additional inference latency. We also provide an empirical investigation into rank-deficiency in language model adaptation, which sheds light on the efficacy of LoRA. We release a package that facilitates the integration of LoRA with PyTorch models and provide our implementations and model checkpoints for RoBERTa, DeBERTa, and GPT-2 at https://github.com/microsoft/LoRA.",
      "abstract": "An important paradigm of natural language processing consists of large-scale pre-training on general domain data and adaptation to particular tasks or domains. As we pre-train larger models, full fine-tuning, which retrains all model parameters, becomes less feasible. Using GPT-3 175B as an example -- deploying independent instances of fine-tuned models, each with 175B parameters, is prohibitively expensive. We propose Low-Rank Adaptation, or LoRA, which freezes the pre-trained model weights and injects trainable rank decomposition matrices into each layer of the Transformer architecture, greatly reducing the number of trainable parameters for downstream tasks. Compared to GPT-3 175B fine-tuned with Adam, LoRA can reduce the number of trainable parameters by 10,000 times and the GPU memory requirement by 3 times. LoRA performs on-par or better than fine-tuning in model quality on RoBERTa, DeBERTa, GPT-2, and GPT-3, despite having fewer trainable parameters, a higher training throughput, and, unlike adapters, no additional inference latency. We also provide an empirical investigation into rank-deficiency in language model adaptation, which sheds light on the efficacy of LoRA. We release a package that facilitates the integration of LoRA with PyTorch models and provide our implementations and model checkpoints for RoBERTa, DeBERTa, and GPT-2 at https://github.com/microsoft/LoRA.",
      "doi": "https://doi.org/10.48550/arxiv.2106.09685",
      "openalex_id": "https://openalex.org/W3168867926",
      "arxiv_id": "",
      "publication_date": "2021-06-17",
      "published": "2021-06-17",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item focuses on Low-Rank Adaptation for large language models and general fine-tuning efficiency, with no emphasis on Spoken Language Models or speech-language modeling, and it predates the past 3 years, thus failing the core inclusion criteria and meeting at least one exclusion criterion.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item focuses on Low-Rank Adaptation for large language models and general fine-tuning efficiency, with no emphasis on Spoken Language Models or speech-language modeling, and it predates the past 3 years, thus failing the core inclusion criteria and meeting at least one exclusion criterion.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper primarily focuses on LoRA as a parameter-efficient fine-tuning method for large language models such as GPT-3, RoBERTa, and GPT-2, with no emphasis on spoken language modeling or speech as the primary modeling object. The abstract does not mention any aspects related to spoken language models (SLMs), full-duplex interaction, speech signals, or evaluation on speech-related tasks, and it mainly addresses textual NLP model adaptation challenges rather than speech processing or speech-language models. Therefore, it does not meet the inclusion criteria concerning spoken language modeling or speech language models and is not relevant for inclusion.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper primarily focuses on LoRA as a parameter-efficient fine-tuning method for large language models such as GPT-3, RoBERTa, and GPT-2, with no emphasis on spoken language modeling or speech as the primary modeling object. The abstract does not mention any aspects related to spoken language models (SLMs), full-duplex interaction, speech signals, or evaluation on speech-related tasks, and it mainly addresses textual NLP model adaptation challenges rather than speech processing or speech-language models. Therefore, it does not meet the inclusion criteria concerning spoken language modeling or speech language models and is not relevant for inclusion.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "X-LLM: Bootstrapping Advanced Large Language Models by Treating Multi-Modalities as Foreign Languages",
    "abstract": "Large language models (LLMs) have demonstrated remarkable language abilities. GPT-4, based on advanced LLMs, exhibits extraordinary multimodal capabilities beyond previous visual language models. We attribute this to the use of more advanced LLMs compared with previous multimodal models. Unfortunately, the model architecture and training strategies of GPT-4 are unknown. To endow LLMs with multimodal capabilities, we propose X-LLM, which converts Multi-modalities (images, speech, videos) into foreign languages using X2L interfaces and inputs them into a large Language model (ChatGLM). Specifically, X-LLM aligns multiple frozen single-modal encoders and a frozen LLM using X2L interfaces, where ``X'' denotes multi-modalities such as image, speech, and videos, and ``L'' denotes languages. X-LLM's training consists of three stages: (1) Converting Multimodal Information: The first stage trains each X2L interface to align with its respective single-modal encoder separately to convert multimodal information into languages. (2) Aligning X2L representations with the LLM: single-modal encoders are aligned with the LLM through X2L interfaces independently. (3) Integrating multiple modalities: all single-modal encoders are aligned with the LLM through X2L interfaces to integrate multimodal capabilities into the LLM. Our experiments show that X-LLM demonstrates impressive multimodel chat abilities, sometimes exhibiting the behaviors of multimodal GPT-4 on unseen images/instructions, and yields a 84.5\\% relative score compared with GPT-4 on a synthetic multimodal instruction-following dataset. And we also conduct quantitative tests on using LLM for ASR and multimodal ASR, hoping to promote the era of LLM-based speech recognition.",
    "metadata": {
      "title": "X-LLM: Bootstrapping Advanced Large Language Models by Treating Multi-Modalities as Foreign Languages",
      "summary": "Large language models (LLMs) have demonstrated remarkable language abilities. GPT-4, based on advanced LLMs, exhibits extraordinary multimodal capabilities beyond previous visual language models. We attribute this to the use of more advanced LLMs compared with previous multimodal models. Unfortunately, the model architecture and training strategies of GPT-4 are unknown. To endow LLMs with multimodal capabilities, we propose X-LLM, which converts Multi-modalities (images, speech, videos) into foreign languages using X2L interfaces and inputs them into a large Language model (ChatGLM). Specifically, X-LLM aligns multiple frozen single-modal encoders and a frozen LLM using X2L interfaces, where ``X'' denotes multi-modalities such as image, speech, and videos, and ``L'' denotes languages. X-LLM's training consists of three stages: (1) Converting Multimodal Information: The first stage trains each X2L interface to align with its respective single-modal encoder separately to convert multimodal information into languages. (2) Aligning X2L representations with the LLM: single-modal encoders are aligned with the LLM through X2L interfaces independently. (3) Integrating multiple modalities: all single-modal encoders are aligned with the LLM through X2L interfaces to integrate multimodal capabilities into the LLM. Our experiments show that X-LLM demonstrates impressive multimodel chat abilities, sometimes exhibiting the behaviors of multimodal GPT-4 on unseen images/instructions, and yields a 84.5\\% relative score compared with GPT-4 on a synthetic multimodal instruction-following dataset. And we also conduct quantitative tests on using LLM for ASR and multimodal ASR, hoping to promote the era of LLM-based speech recognition.",
      "abstract": "Large language models (LLMs) have demonstrated remarkable language abilities. GPT-4, based on advanced LLMs, exhibits extraordinary multimodal capabilities beyond previous visual language models. We attribute this to the use of more advanced LLMs compared with previous multimodal models. Unfortunately, the model architecture and training strategies of GPT-4 are unknown. To endow LLMs with multimodal capabilities, we propose X-LLM, which converts Multi-modalities (images, speech, videos) into foreign languages using X2L interfaces and inputs them into a large Language model (ChatGLM). Specifically, X-LLM aligns multiple frozen single-modal encoders and a frozen LLM using X2L interfaces, where ``X'' denotes multi-modalities such as image, speech, and videos, and ``L'' denotes languages. X-LLM's training consists of three stages: (1) Converting Multimodal Information: The first stage trains each X2L interface to align with its respective single-modal encoder separately to convert multimodal information into languages. (2) Aligning X2L representations with the LLM: single-modal encoders are aligned with the LLM through X2L interfaces independently. (3) Integrating multiple modalities: all single-modal encoders are aligned with the LLM through X2L interfaces to integrate multimodal capabilities into the LLM. Our experiments show that X-LLM demonstrates impressive multimodel chat abilities, sometimes exhibiting the behaviors of multimodal GPT-4 on unseen images/instructions, and yields a 84.5\\% relative score compared with GPT-4 on a synthetic multimodal instruction-following dataset. And we also conduct quantitative tests on using LLM for ASR and multimodal ASR, hoping to promote the era of LLM-based speech recognition.",
      "doi": "https://doi.org/10.48550/arxiv.2305.04160",
      "openalex_id": "https://openalex.org/W4375958083",
      "arxiv_id": "",
      "publication_date": "2023-05-07",
      "published": "2023-05-07",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "Core focus is on multimodal LLMs rather than Spoken Language Models or speech-centric modeling, so it does not meet the inclusion criteria for SLM/FD-SLM emphasis and cannot be included.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "Core focus is on multimodal LLMs rather than Spoken Language Models or speech-centric modeling, so it does not meet the inclusion criteria for SLM/FD-SLM emphasis and cannot be included.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on integrating multimodal inputs (images, speech, videos) into large language models but treats speech only as a component converted into language tokens, without emphasizing spoken language modeling or full-duplex spoken interaction capabilities; the work does not explicitly address spoken language models as the core research, real-time interaction, or behavioral arbitration related to speech, and mainly focuses on multimodal instruction following and ASR performance without integration of speech language modeling or synchronous interaction.",
      "evaluation": 2
    },
    "round-A_JuniorMini_reasoning": "The study focuses on integrating multimodal inputs (images, speech, videos) into large language models but treats speech only as a component converted into language tokens, without emphasizing spoken language modeling or full-duplex spoken interaction capabilities; the work does not explicitly address spoken language models as the core research, real-time interaction, or behavioral arbitration related to speech, and mainly focuses on multimodal instruction following and ASR performance without integration of speech language modeling or synchronous interaction.",
    "round-A_JuniorMini_evaluation": 2,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:2)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "LLaMA-Adapter: Efficient Fine-tuning of Language Models with Zero-init Attention",
    "abstract": "We present LLaMA-Adapter, a lightweight adaption method to efficiently fine-tune LLaMA into an instruction-following model. Using 52K self-instruct demonstrations, LLaMA-Adapter only introduces 1.2M learnable parameters upon the frozen LLaMA 7B model, and costs less than one hour for fine-tuning on 8 A100 GPUs. Specifically, we adopt a set of learnable adaption prompts, and prepend them to the word tokens at higher transformer layers. Then, a zero-initialized attention mechanism with zero gating is proposed, which adaptively injects the new instructional cues into LLaMA, while effectively preserves its pre-trained knowledge. With our efficient training, LLaMA-Adapter can generate high-quality responses, comparable to Alpaca with fully fine-tuned 7B parameters. Besides language commands, our approach can be simply extended to multi-modal instructions for learning image-conditioned LLaMA model, which achieves superior reasoning performance on ScienceQA and COCO Caption benchmarks. Furthermore, we also evaluate the zero-initialized attention mechanism for fine-tuning other pre-trained models (ViT, RoBERTa) on traditional vision and language tasks, demonstrating the superior generalization capacity of our approach. Code is released at https://github.com/OpenGVLab/LLaMA-Adapter.",
    "metadata": {
      "title": "LLaMA-Adapter: Efficient Fine-tuning of Language Models with Zero-init Attention",
      "summary": "We present LLaMA-Adapter, a lightweight adaption method to efficiently fine-tune LLaMA into an instruction-following model. Using 52K self-instruct demonstrations, LLaMA-Adapter only introduces 1.2M learnable parameters upon the frozen LLaMA 7B model, and costs less than one hour for fine-tuning on 8 A100 GPUs. Specifically, we adopt a set of learnable adaption prompts, and prepend them to the word tokens at higher transformer layers. Then, a zero-initialized attention mechanism with zero gating is proposed, which adaptively injects the new instructional cues into LLaMA, while effectively preserves its pre-trained knowledge. With our efficient training, LLaMA-Adapter can generate high-quality responses, comparable to Alpaca with fully fine-tuned 7B parameters. Besides language commands, our approach can be simply extended to multi-modal instructions for learning image-conditioned LLaMA model, which achieves superior reasoning performance on ScienceQA and COCO Caption benchmarks. Furthermore, we also evaluate the zero-initialized attention mechanism for fine-tuning other pre-trained models (ViT, RoBERTa) on traditional vision and language tasks, demonstrating the superior generalization capacity of our approach. Code is released at https://github.com/OpenGVLab/LLaMA-Adapter.",
      "abstract": "We present LLaMA-Adapter, a lightweight adaption method to efficiently fine-tune LLaMA into an instruction-following model. Using 52K self-instruct demonstrations, LLaMA-Adapter only introduces 1.2M learnable parameters upon the frozen LLaMA 7B model, and costs less than one hour for fine-tuning on 8 A100 GPUs. Specifically, we adopt a set of learnable adaption prompts, and prepend them to the word tokens at higher transformer layers. Then, a zero-initialized attention mechanism with zero gating is proposed, which adaptively injects the new instructional cues into LLaMA, while effectively preserves its pre-trained knowledge. With our efficient training, LLaMA-Adapter can generate high-quality responses, comparable to Alpaca with fully fine-tuned 7B parameters. Besides language commands, our approach can be simply extended to multi-modal instructions for learning image-conditioned LLaMA model, which achieves superior reasoning performance on ScienceQA and COCO Caption benchmarks. Furthermore, we also evaluate the zero-initialized attention mechanism for fine-tuning other pre-trained models (ViT, RoBERTa) on traditional vision and language tasks, demonstrating the superior generalization capacity of our approach. Code is released at https://github.com/OpenGVLab/LLaMA-Adapter.",
      "doi": "https://doi.org/10.48550/arxiv.2303.16199",
      "openalex_id": "https://openalex.org/W4361229539",
      "arxiv_id": "",
      "publication_date": "2023-03-28",
      "published": "2023-03-28",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This item focuses on text-only fine-tuning of language models with adapters (LLaMA-Adapter) and does not address Spoken Language Models or any speech-centric evaluation or architecture, thus not meeting the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "This item focuses on text-only fine-tuning of language models with adapters (LLaMA-Adapter) and does not address Spoken Language Models or any speech-centric evaluation or architecture, thus not meeting the inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on efficient fine-tuning of a large language model (LLaMA) primarily on text-based instruction-following tasks and multi-modal instructions extending to image-conditioned models, without emphasizing spoken language models or speech-based modeling as core components. The abstract does not mention spoken language models, speech language modeling, or speech as a primary object of modeling and inference; it lacks discussion on synchronous interaction, full-duplex spoken language modeling, or related evaluation criteria. Therefore, it does not meet the inclusion criteria focused on spoken language or full-duplex speech interaction modeling, nor does it avoid exclusion criteria related to solely text-based approaches without speech modeling innovation.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on efficient fine-tuning of a large language model (LLaMA) primarily on text-based instruction-following tasks and multi-modal instructions extending to image-conditioned models, without emphasizing spoken language models or speech-based modeling as core components. The abstract does not mention spoken language models, speech language modeling, or speech as a primary object of modeling and inference; it lacks discussion on synchronous interaction, full-duplex spoken language modeling, or related evaluation criteria. Therefore, it does not meet the inclusion criteria focused on spoken language or full-duplex speech interaction modeling, nor does it avoid exclusion criteria related to solely text-based approaches without speech modeling innovation.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "SpeechTokenizer: Unified Speech Tokenizer for Speech Large Language Models",
    "abstract": "Current speech large language models build upon discrete speech representations, which can be categorized into semantic tokens and acoustic tokens. However, existing speech tokens are not specifically designed for speech language modeling. To assess the suitability of speech tokens for building speech language models, we established the first benchmark, SLMTokBench. Our results indicate that neither semantic nor acoustic tokens are ideal for this purpose. Therefore, we propose SpeechTokenizer, a unified speech tokenizer for speech large language models. SpeechTokenizer adopts the Encoder-Decoder architecture with residual vector quantization (RVQ). Unifying semantic and acoustic tokens, SpeechTokenizer disentangles different aspects of speech information hierarchically across different RVQ layers. Furthermore, We construct a Unified Speech Language Model (USLM) leveraging SpeechTokenizer. Experiments show that SpeechTokenizer performs comparably to EnCodec in speech reconstruction and demonstrates strong performance on the SLMTokBench benchmark. Also, USLM outperforms VALL-E in zero-shot Text-to-Speech tasks. Code and models are available at https://github.com/ZhangXInFD/SpeechTokenizer/.",
    "metadata": {
      "title": "SpeechTokenizer: Unified Speech Tokenizer for Speech Large Language Models",
      "summary": "Current speech large language models build upon discrete speech representations, which can be categorized into semantic tokens and acoustic tokens. However, existing speech tokens are not specifically designed for speech language modeling. To assess the suitability of speech tokens for building speech language models, we established the first benchmark, SLMTokBench. Our results indicate that neither semantic nor acoustic tokens are ideal for this purpose. Therefore, we propose SpeechTokenizer, a unified speech tokenizer for speech large language models. SpeechTokenizer adopts the Encoder-Decoder architecture with residual vector quantization (RVQ). Unifying semantic and acoustic tokens, SpeechTokenizer disentangles different aspects of speech information hierarchically across different RVQ layers. Furthermore, We construct a Unified Speech Language Model (USLM) leveraging SpeechTokenizer. Experiments show that SpeechTokenizer performs comparably to EnCodec in speech reconstruction and demonstrates strong performance on the SLMTokBench benchmark. Also, USLM outperforms VALL-E in zero-shot Text-to-Speech tasks. Code and models are available at https://github.com/ZhangXInFD/SpeechTokenizer/.",
      "abstract": "Current speech large language models build upon discrete speech representations, which can be categorized into semantic tokens and acoustic tokens. However, existing speech tokens are not specifically designed for speech language modeling. To assess the suitability of speech tokens for building speech language models, we established the first benchmark, SLMTokBench. Our results indicate that neither semantic nor acoustic tokens are ideal for this purpose. Therefore, we propose SpeechTokenizer, a unified speech tokenizer for speech large language models. SpeechTokenizer adopts the Encoder-Decoder architecture with residual vector quantization (RVQ). Unifying semantic and acoustic tokens, SpeechTokenizer disentangles different aspects of speech information hierarchically across different RVQ layers. Furthermore, We construct a Unified Speech Language Model (USLM) leveraging SpeechTokenizer. Experiments show that SpeechTokenizer performs comparably to EnCodec in speech reconstruction and demonstrates strong performance on the SLMTokBench benchmark. Also, USLM outperforms VALL-E in zero-shot Text-to-Speech tasks. Code and models are available at https://github.com/ZhangXInFD/SpeechTokenizer/.",
      "doi": "https://doi.org/10.48550/arxiv.2308.16692",
      "openalex_id": "https://openalex.org/W4386384714",
      "arxiv_id": "",
      "publication_date": "2023-08-31",
      "published": "2023-08-31",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item centers on Speech Language Models/Spoken Language Models through a unified speech tokenizer and a USLM, with architecture, training/evaluation, and a dedicated benchmark, and it falls within the past 3 years and is not solely ASR/TTS or non-academic, meeting all inclusion criteria without triggering any exclusions.",
      "evaluation": 5
    },
    "round-A_JuniorNano_reasoning": "The item centers on Speech Language Models/Spoken Language Models through a unified speech tokenizer and a USLM, with architecture, training/evaluation, and a dedicated benchmark, and it falls within the past 3 years and is not solely ASR/TTS or non-academic, meeting all inclusion criteria without triggering any exclusions.",
    "round-A_JuniorNano_evaluation": 5,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper proposes SpeechTokenizer, a unified speech tokenizer geared towards improving speech large language models with a focus on modeling discrete speech representations; however, it lacks explicit mention of full-duplex interaction capabilities, real-time interaction, cognitive parallelism, or arbitration behaviors in the abstract, and primarily reports benchmark and reconstruction performance without addressing synchronous interaction or dialogue behavior arbitration; thus, while it fits within speech language modeling, it does not fully satisfy the criteria emphasizing full-duplex spoken language models or synchronization evaluation.",
      "evaluation": 3
    },
    "round-A_JuniorMini_reasoning": "The paper proposes SpeechTokenizer, a unified speech tokenizer geared towards improving speech large language models with a focus on modeling discrete speech representations; however, it lacks explicit mention of full-duplex interaction capabilities, real-time interaction, cognitive parallelism, or arbitration behaviors in the abstract, and primarily reports benchmark and reconstruction performance without addressing synchronous interaction or dialogue behavior arbitration; thus, while it fits within speech language modeling, it does not fully satisfy the criteria emphasizing full-duplex spoken language models or synchronization evaluation.",
    "round-A_JuniorMini_evaluation": 3,
    "round-B_SeniorLead_output": {
      "reasoning": "The title and abstract explicitly target Speech Large Language Models (mentions SpeechTokenizer for speech large language models and a Unified Speech Language Model, plus a new benchmark SLMTokBench and modeling/evaluation of speech tokens), focus on speech-level token modeling rather than only ASR/TTS, and present methods and benchmarks—while they don’t specifically study full‑duplex interaction, that is not a strict exclusion; both junior reviews noted this (one included, one hesitated due to full‑duplex absence), but the item meets the core inclusion requirement of being centered on spoken/speech language modeling and provides concrete modeling and evaluation contributions, so it should be included.",
      "evaluation": 5
    },
    "round-B_SeniorLead_reasoning": "The title and abstract explicitly target Speech Large Language Models (mentions SpeechTokenizer for speech large language models and a Unified Speech Language Model, plus a new benchmark SLMTokBench and modeling/evaluation of speech tokens), focus on speech-level token modeling rather than only ASR/TTS, and present methods and benchmarks—while they don’t specifically study full‑duplex interaction, that is not a strict exclusion; both junior reviews noted this (one included, one hesitated due to full‑duplex absence), but the item meets the core inclusion requirement of being centered on spoken/speech language modeling and provides concrete modeling and evaluation contributions, so it should be included.",
    "round-B_SeniorLead_evaluation": 5,
    "final_verdict": "include (senior:5)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "LIMA: Less Is More for Alignment",
    "abstract": "Large language models are trained in two stages: (1) unsupervised pretraining from raw text, to learn general-purpose representations, and (2) large scale instruction tuning and reinforcement learning, to better align to end tasks and user preferences. We measure the relative importance of these two stages by training LIMA, a 65B parameter LLaMa language model fine-tuned with the standard supervised loss on only 1,000 carefully curated prompts and responses, without any reinforcement learning or human preference modeling. LIMA demonstrates remarkably strong performance, learning to follow specific response formats from only a handful of examples in the training data, including complex queries that range from planning trip itineraries to speculating about alternate history. Moreover, the model tends to generalize well to unseen tasks that did not appear in the training data. In a controlled human study, responses from LIMA are either equivalent or strictly preferred to GPT-4 in 43% of cases; this statistic is as high as 58% when compared to Bard and 65% versus DaVinci003, which was trained with human feedback. Taken together, these results strongly suggest that almost all knowledge in large language models is learned during pretraining, and only limited instruction tuning data is necessary to teach models to produce high quality output.",
    "metadata": {
      "title": "LIMA: Less Is More for Alignment",
      "summary": "Large language models are trained in two stages: (1) unsupervised pretraining from raw text, to learn general-purpose representations, and (2) large scale instruction tuning and reinforcement learning, to better align to end tasks and user preferences. We measure the relative importance of these two stages by training LIMA, a 65B parameter LLaMa language model fine-tuned with the standard supervised loss on only 1,000 carefully curated prompts and responses, without any reinforcement learning or human preference modeling. LIMA demonstrates remarkably strong performance, learning to follow specific response formats from only a handful of examples in the training data, including complex queries that range from planning trip itineraries to speculating about alternate history. Moreover, the model tends to generalize well to unseen tasks that did not appear in the training data. In a controlled human study, responses from LIMA are either equivalent or strictly preferred to GPT-4 in 43% of cases; this statistic is as high as 58% when compared to Bard and 65% versus DaVinci003, which was trained with human feedback. Taken together, these results strongly suggest that almost all knowledge in large language models is learned during pretraining, and only limited instruction tuning data is necessary to teach models to produce high quality output.",
      "abstract": "Large language models are trained in two stages: (1) unsupervised pretraining from raw text, to learn general-purpose representations, and (2) large scale instruction tuning and reinforcement learning, to better align to end tasks and user preferences. We measure the relative importance of these two stages by training LIMA, a 65B parameter LLaMa language model fine-tuned with the standard supervised loss on only 1,000 carefully curated prompts and responses, without any reinforcement learning or human preference modeling. LIMA demonstrates remarkably strong performance, learning to follow specific response formats from only a handful of examples in the training data, including complex queries that range from planning trip itineraries to speculating about alternate history. Moreover, the model tends to generalize well to unseen tasks that did not appear in the training data. In a controlled human study, responses from LIMA are either equivalent or strictly preferred to GPT-4 in 43% of cases; this statistic is as high as 58% when compared to Bard and 65% versus DaVinci003, which was trained with human feedback. Taken together, these results strongly suggest that almost all knowledge in large language models is learned during pretraining, and only limited instruction tuning data is necessary to teach models to produce high quality output.",
      "doi": "https://doi.org/10.48550/arxiv.2305.11206",
      "openalex_id": "https://openalex.org/W4377297670",
      "arxiv_id": "",
      "publication_date": "2023-05-18",
      "published": "2023-05-18",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This abstract is about a text-based LLM alignment (LIMA) and has no focus on spoken-language models, speech interactions, or speech-language modeling, so it does not meet the core inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "This abstract is about a text-based LLM alignment (LIMA) and has no focus on spoken-language models, speech interactions, or speech-language modeling, so it does not meet the core inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on instruction tuning of a large language model on text prompts and responses without involving spoken language or acoustic signals; it does not address spoken language modeling, full-duplex spoken interaction, or speech-based architectures as required by the inclusion criteria. Therefore, it does not meet the core topic and modeling requirements.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on instruction tuning of a large language model on text prompts and responses without involving spoken language or acoustic signals; it does not address spoken language modeling, full-duplex spoken interaction, or speech-based architectures as required by the inclusion criteria. Therefore, it does not meet the core topic and modeling requirements.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "LLaMA: Open and Efficient Foundation Language Models",
    "abstract": "We introduce LLaMA, a collection of foundation language models ranging from 7B to 65B parameters. We train our models on trillions of tokens, and show that it is possible to train state-of-the-art models using publicly available datasets exclusively, without resorting to proprietary and inaccessible datasets. In particular, LLaMA-13B outperforms GPT-3 (175B) on most benchmarks, and LLaMA-65B is competitive with the best models, Chinchilla-70B and PaLM-540B. We release all our models to the research community.",
    "metadata": {
      "title": "LLaMA: Open and Efficient Foundation Language Models",
      "summary": "We introduce LLaMA, a collection of foundation language models ranging from 7B to 65B parameters. We train our models on trillions of tokens, and show that it is possible to train state-of-the-art models using publicly available datasets exclusively, without resorting to proprietary and inaccessible datasets. In particular, LLaMA-13B outperforms GPT-3 (175B) on most benchmarks, and LLaMA-65B is competitive with the best models, Chinchilla-70B and PaLM-540B. We release all our models to the research community.",
      "abstract": "We introduce LLaMA, a collection of foundation language models ranging from 7B to 65B parameters. We train our models on trillions of tokens, and show that it is possible to train state-of-the-art models using publicly available datasets exclusively, without resorting to proprietary and inaccessible datasets. In particular, LLaMA-13B outperforms GPT-3 (175B) on most benchmarks, and LLaMA-65B is competitive with the best models, Chinchilla-70B and PaLM-540B. We release all our models to the research community.",
      "doi": "https://doi.org/10.48550/arxiv.2302.13971",
      "openalex_id": "https://openalex.org/W4322718191",
      "arxiv_id": "",
      "publication_date": "2023-02-27",
      "published": "2023-02-27",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "此論文著重於以文本為主的基礎語言模型（LLaMA），未以 Spoken Language Models 或 Full-Duplex SLM 為核心，亦缺乏語音端到端建模、同步互動與語音處理的評估，因此不符合本研究的納入條件。",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "此論文著重於以文本為主的基礎語言模型（LLaMA），未以 Spoken Language Models 或 Full-Duplex SLM 為核心，亦缺乏語音端到端建模、同步互動與語音處理的評估，因此不符合本研究的納入條件。",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The title and abstract describe LLaMA, a collection of foundation language models focused on large-scale language model training with publicly available text datasets, but there is no indication that the study centers on spoken language modeling, speech as the core modeling object, full-duplex interaction, or any of the specific spoken language model characteristics required in the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The title and abstract describe LLaMA, a collection of foundation language models focused on large-scale language model training with publicly available text datasets, but there is no indication that the study centers on spoken language modeling, speech as the core modeling object, full-duplex interaction, or any of the specific spoken language model characteristics required in the inclusion criteria.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Phonetic posteriorgrams for many-to-one voice conversion without parallel data training",
    "abstract": "This paper proposes a novel approach to voice conversion with non-parallel training data. The idea is to bridge between speakers by means of Phonetic PosteriorGrams (PPGs) obtained from a speaker-independent automatic speech recognition (SI-ASR) system. It is assumed that these PPGs can represent articulation of speech sounds in a speaker-normalized space and correspond to spoken content speaker-independently. The proposed approach first obtains PPGs of target speech. Then, a Deep Bidirectional Long Short-Term Memory based Recurrent Neural Network (DBLSTM) structure is used to model the relationships between the PPGs and acoustic features of the target speech. To convert arbitrary source speech, we obtain its PPGs from the same SI-ASR and feed them into the trained DBLSTM for generating converted speech. Our approach has two main advantages: 1) no parallel training data is required; 2) a trained model can be applied to any other source speaker for a fixed target speaker (i.e., many-to-one conversion). Experiments show that our approach performs equally well or better than state-of-the-art systems in both speech quality and speaker similarity.",
    "metadata": {
      "title": "Phonetic posteriorgrams for many-to-one voice conversion without parallel data training",
      "summary": "This paper proposes a novel approach to voice conversion with non-parallel training data. The idea is to bridge between speakers by means of Phonetic PosteriorGrams (PPGs) obtained from a speaker-independent automatic speech recognition (SI-ASR) system. It is assumed that these PPGs can represent articulation of speech sounds in a speaker-normalized space and correspond to spoken content speaker-independently. The proposed approach first obtains PPGs of target speech. Then, a Deep Bidirectional Long Short-Term Memory based Recurrent Neural Network (DBLSTM) structure is used to model the relationships between the PPGs and acoustic features of the target speech. To convert arbitrary source speech, we obtain its PPGs from the same SI-ASR and feed them into the trained DBLSTM for generating converted speech. Our approach has two main advantages: 1) no parallel training data is required; 2) a trained model can be applied to any other source speaker for a fixed target speaker (i.e., many-to-one conversion). Experiments show that our approach performs equally well or better than state-of-the-art systems in both speech quality and speaker similarity.",
      "abstract": "This paper proposes a novel approach to voice conversion with non-parallel training data. The idea is to bridge between speakers by means of Phonetic PosteriorGrams (PPGs) obtained from a speaker-independent automatic speech recognition (SI-ASR) system. It is assumed that these PPGs can represent articulation of speech sounds in a speaker-normalized space and correspond to spoken content speaker-independently. The proposed approach first obtains PPGs of target speech. Then, a Deep Bidirectional Long Short-Term Memory based Recurrent Neural Network (DBLSTM) structure is used to model the relationships between the PPGs and acoustic features of the target speech. To convert arbitrary source speech, we obtain its PPGs from the same SI-ASR and feed them into the trained DBLSTM for generating converted speech. Our approach has two main advantages: 1) no parallel training data is required; 2) a trained model can be applied to any other source speaker for a fixed target speaker (i.e., many-to-one conversion). Experiments show that our approach performs equally well or better than state-of-the-art systems in both speech quality and speaker similarity.",
      "doi": "https://doi.org/10.1109/icme.2016.7552917",
      "openalex_id": "https://openalex.org/W2518172956",
      "arxiv_id": "",
      "publication_date": "2016-07-01",
      "published": "2016-07-01",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This paper centers on voice conversion using phonetic posteriorgrams and an SI-ASR pipeline, not on Spoken Language Models or speech-language modeling, and it likely falls outside the required 3-year window and lacks interactive SLM evaluation.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "This paper centers on voice conversion using phonetic posteriorgrams and an SI-ASR pipeline, not on Spoken Language Models or speech-language modeling, and it likely falls outside the required 3-year window and lacks interactive SLM evaluation.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper focuses on voice conversion using phonetic posteriorgrams and acoustic modeling but does not explicitly address spoken language modeling, full-duplex interaction, or the synchronous dialogue behaviors outlined in the inclusion criteria. The study centers on a single speech processing task (voice conversion) without discussing language modeling or end-to-end architectures encompassing both speech and text. Therefore, it does not meet the core inclusion criteria emphasizing spoken language models and synchronized interactive capabilities, nor addresses the multi-dimensional evaluation required.",
      "evaluation": 2
    },
    "round-A_JuniorMini_reasoning": "The paper focuses on voice conversion using phonetic posteriorgrams and acoustic modeling but does not explicitly address spoken language modeling, full-duplex interaction, or the synchronous dialogue behaviors outlined in the inclusion criteria. The study centers on a single speech processing task (voice conversion) without discussing language modeling or end-to-end architectures encompassing both speech and text. Therefore, it does not meet the core inclusion criteria emphasizing spoken language models and synchronized interactive capabilities, nor addresses the multi-dimensional evaluation required.",
    "round-A_JuniorMini_evaluation": 2,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:2)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "One-Shot Voice Conversion by Separating Speaker and Content Representations with Instance Normalization",
    "abstract": "Recently, voice conversion (VC) without parallel data has been successfully adapted to multi-target scenario in which a single model is trained to convert the input voice to many different speakers.However, such model suffers from the limitation that it can only convert the voice to the speakers in the training data, which narrows down the applicable scenario of VC.In this paper, we proposed a novel one-shot VC approach which is able to perform VC by only an example utterance from source and target speaker respectively, and the source and target speaker do not even need to be seen during training.This is achieved by disentangling speaker and content representations with instance normalization (IN).Objective and subjective evaluation shows that our model is able to generate the voice similar to target speaker.In addition to the performance measurement, we also demonstrate that this model is able to learn meaningful speaker representations without any supervision.",
    "metadata": {
      "title": "One-Shot Voice Conversion by Separating Speaker and Content Representations with Instance Normalization",
      "summary": "Recently, voice conversion (VC) without parallel data has been successfully adapted to multi-target scenario in which a single model is trained to convert the input voice to many different speakers.However, such model suffers from the limitation that it can only convert the voice to the speakers in the training data, which narrows down the applicable scenario of VC.In this paper, we proposed a novel one-shot VC approach which is able to perform VC by only an example utterance from source and target speaker respectively, and the source and target speaker do not even need to be seen during training.This is achieved by disentangling speaker and content representations with instance normalization (IN).Objective and subjective evaluation shows that our model is able to generate the voice similar to target speaker.In addition to the performance measurement, we also demonstrate that this model is able to learn meaningful speaker representations without any supervision.",
      "abstract": "Recently, voice conversion (VC) without parallel data has been successfully adapted to multi-target scenario in which a single model is trained to convert the input voice to many different speakers.However, such model suffers from the limitation that it can only convert the voice to the speakers in the training data, which narrows down the applicable scenario of VC.In this paper, we proposed a novel one-shot VC approach which is able to perform VC by only an example utterance from source and target speaker respectively, and the source and target speaker do not even need to be seen during training.This is achieved by disentangling speaker and content representations with instance normalization (IN).Objective and subjective evaluation shows that our model is able to generate the voice similar to target speaker.In addition to the performance measurement, we also demonstrate that this model is able to learn meaningful speaker representations without any supervision.",
      "doi": "https://doi.org/10.21437/interspeech.2019-2663",
      "openalex_id": "https://openalex.org/W2972659941",
      "arxiv_id": "",
      "publication_date": "2019-09-13",
      "published": "2019-09-13",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item centers on one-shot voice conversion and speaker/content disentanglement, not on Spoken Language Models or speech-language modeling as the core object of study, and it does not address dialogue interaction or SLM-focused evaluation.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item centers on one-shot voice conversion and speaker/content disentanglement, not on Spoken Language Models or speech-language modeling as the core object of study, and it does not address dialogue interaction or SLM-focused evaluation.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on one-shot voice conversion by separating speaker and content representations, but does not address spoken language models (SLMs) or full-duplex spoken language interactions, nor does it discuss speech language modeling, synchronization, cognitive parallelism, or low-latency interactive capabilities. It primarily concerns voice conversion technology rather than speech language modeling or multi-turn interaction mechanisms as required by the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on one-shot voice conversion by separating speaker and content representations, but does not address spoken language models (SLMs) or full-duplex spoken language interactions, nor does it discuss speech language modeling, synchronization, cognitive parallelism, or low-latency interactive capabilities. It primarily concerns voice conversion technology rather than speech language modeling or multi-turn interaction mechanisms as required by the inclusion criteria.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "VQMIVC: Vector Quantization and Mutual Information-Based Unsupervised Speech Representation Disentanglement for One-Shot Voice Conversion",
    "abstract": "One-shot voice conversion (VC), which performs conversion across arbitrary speakers with only a single target-speaker utterance for reference, can be effectively achieved by speech representation disentanglement.Existing work generally ignores the correlation between different speech representations during training, which causes leakage of content information into the speaker representation and thus degrades VC performance.To alleviate this issue, we employ vector quantization (VQ) for content encoding and introduce mutual information (MI) as the correlation metric during training, to achieve proper disentanglement of content, speaker and pitch representations, by reducing their inter-dependencies in an unsupervised manner.Experimental results reflect the superiority of the proposed method in learning effective disentangled speech representations for retaining source linguistic content and intonation variations, while capturing target speaker characteristics.In doing so, the proposed approach achieves higher speech naturalness and speaker similarity than current state-of-the-art one-shot VC systems.Our code, pre-trained models and demo are available at https://github.com/Wendison/VQMIVC.",
    "metadata": {
      "title": "VQMIVC: Vector Quantization and Mutual Information-Based Unsupervised Speech Representation Disentanglement for One-Shot Voice Conversion",
      "summary": "One-shot voice conversion (VC), which performs conversion across arbitrary speakers with only a single target-speaker utterance for reference, can be effectively achieved by speech representation disentanglement.Existing work generally ignores the correlation between different speech representations during training, which causes leakage of content information into the speaker representation and thus degrades VC performance.To alleviate this issue, we employ vector quantization (VQ) for content encoding and introduce mutual information (MI) as the correlation metric during training, to achieve proper disentanglement of content, speaker and pitch representations, by reducing their inter-dependencies in an unsupervised manner.Experimental results reflect the superiority of the proposed method in learning effective disentangled speech representations for retaining source linguistic content and intonation variations, while capturing target speaker characteristics.In doing so, the proposed approach achieves higher speech naturalness and speaker similarity than current state-of-the-art one-shot VC systems.Our code, pre-trained models and demo are available at https://github.com/Wendison/VQMIVC.",
      "abstract": "One-shot voice conversion (VC), which performs conversion across arbitrary speakers with only a single target-speaker utterance for reference, can be effectively achieved by speech representation disentanglement.Existing work generally ignores the correlation between different speech representations during training, which causes leakage of content information into the speaker representation and thus degrades VC performance.To alleviate this issue, we employ vector quantization (VQ) for content encoding and introduce mutual information (MI) as the correlation metric during training, to achieve proper disentanglement of content, speaker and pitch representations, by reducing their inter-dependencies in an unsupervised manner.Experimental results reflect the superiority of the proposed method in learning effective disentangled speech representations for retaining source linguistic content and intonation variations, while capturing target speaker characteristics.In doing so, the proposed approach achieves higher speech naturalness and speaker similarity than current state-of-the-art one-shot VC systems.Our code, pre-trained models and demo are available at https://github.com/Wendison/VQMIVC.",
      "doi": "https://doi.org/10.21437/interspeech.2021-283",
      "openalex_id": "https://openalex.org/W3197659778",
      "arxiv_id": "",
      "publication_date": "2021-08-27",
      "published": "2021-08-27",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The paper centers on one-shot voice conversion and disentangled speech representations, but it does not mention Spoken Language Models or Speech Language Models in the title/abstract and does not address SLM-based end-to-end or hybrid architectures or full-duplex/dialogue aspects, thus failing the stated inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The paper centers on one-shot voice conversion and disentangled speech representations, but it does not mention Spoken Language Models or Speech Language Models in the title/abstract and does not address SLM-based end-to-end or hybrid architectures or full-duplex/dialogue aspects, thus failing the stated inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on one-shot voice conversion through disentangled speech representations but does not explicitly address Spoken Language Models (SLMs) or Full-Duplex Spoken Language Models (FD-SLMs) as core topics, nor does it discuss speech language modeling architectures, training, or evaluation from a spoken language modeling perspective. It mainly addresses speech representation learning for voice conversion without involving synchronization, turn-taking, cognitive parallelism, or full-duplex interactions central to the inclusion criteria. Therefore, it does not meet the inclusion criteria centered on spoken language modeling.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on one-shot voice conversion through disentangled speech representations but does not explicitly address Spoken Language Models (SLMs) or Full-Duplex Spoken Language Models (FD-SLMs) as core topics, nor does it discuss speech language modeling architectures, training, or evaluation from a spoken language modeling perspective. It mainly addresses speech representation learning for voice conversion without involving synchronization, turn-taking, cognitive parallelism, or full-duplex interactions central to the inclusion criteria. Therefore, it does not meet the inclusion criteria centered on spoken language modeling.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Adversarially Learning Disentangled Speech Representations for Robust Multi-Factor Voice Conversion",
    "abstract": "Factorizing speech as disentangled speech representations is vital to achieve highly controllable style transfer in voice conversion (VC).Conventional speech representation learning methods in VC only factorize speech as speaker and content, lacking controllability on other prosody-related factors.State-ofthe-art speech representation learning methods for more speech factors are using primary disentangle algorithms such as random resampling and ad-hoc bottleneck layer size adjustment, which however is hard to ensure robust speech representation disentanglement.To increase the robustness of highly controllable style transfer on multiple factors in VC, we propose a disentangled speech representation learning framework based on adversarial learning.Four speech representations characterizing content, timbre, rhythm and pitch are extracted, and further disentangled by an adversarial Mask-And-Predict (MAP) network inspired by BERT.The adversarial network is used to minimize the correlations between the speech representations, by randomly masking and predicting one of the representations from the others.Experimental results show that the proposed framework significantly improves the robustness of VC on multiple factors by increasing the speech quality MOS from 2.79 to 3.30 and decreasing the MCD from 3.89 to 3.58.",
    "metadata": {
      "title": "Adversarially Learning Disentangled Speech Representations for Robust Multi-Factor Voice Conversion",
      "summary": "Factorizing speech as disentangled speech representations is vital to achieve highly controllable style transfer in voice conversion (VC).Conventional speech representation learning methods in VC only factorize speech as speaker and content, lacking controllability on other prosody-related factors.State-ofthe-art speech representation learning methods for more speech factors are using primary disentangle algorithms such as random resampling and ad-hoc bottleneck layer size adjustment, which however is hard to ensure robust speech representation disentanglement.To increase the robustness of highly controllable style transfer on multiple factors in VC, we propose a disentangled speech representation learning framework based on adversarial learning.Four speech representations characterizing content, timbre, rhythm and pitch are extracted, and further disentangled by an adversarial Mask-And-Predict (MAP) network inspired by BERT.The adversarial network is used to minimize the correlations between the speech representations, by randomly masking and predicting one of the representations from the others.Experimental results show that the proposed framework significantly improves the robustness of VC on multiple factors by increasing the speech quality MOS from 2.79 to 3.30 and decreasing the MCD from 3.89 to 3.58.",
      "abstract": "Factorizing speech as disentangled speech representations is vital to achieve highly controllable style transfer in voice conversion (VC).Conventional speech representation learning methods in VC only factorize speech as speaker and content, lacking controllability on other prosody-related factors.State-ofthe-art speech representation learning methods for more speech factors are using primary disentangle algorithms such as random resampling and ad-hoc bottleneck layer size adjustment, which however is hard to ensure robust speech representation disentanglement.To increase the robustness of highly controllable style transfer on multiple factors in VC, we propose a disentangled speech representation learning framework based on adversarial learning.Four speech representations characterizing content, timbre, rhythm and pitch are extracted, and further disentangled by an adversarial Mask-And-Predict (MAP) network inspired by BERT.The adversarial network is used to minimize the correlations between the speech representations, by randomly masking and predicting one of the representations from the others.Experimental results show that the proposed framework significantly improves the robustness of VC on multiple factors by increasing the speech quality MOS from 2.79 to 3.30 and decreasing the MCD from 3.89 to 3.58.",
      "doi": "https://doi.org/10.21437/interspeech.2021-1990",
      "openalex_id": "https://openalex.org/W3198082505",
      "arxiv_id": "",
      "publication_date": "2021-08-27",
      "published": "2021-08-27",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This paper centers on disentangled speech representations for voice conversion and does not address Spoken Language Models (SLMs) or Full-Duplex SLMs as core modeling objects, nor discuss interactive dialogue/speech-language modeling architecture, so it fails the inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "This paper centers on disentangled speech representations for voice conversion and does not address Spoken Language Models (SLMs) or Full-Duplex SLMs as core modeling objects, nor discuss interactive dialogue/speech-language modeling architecture, so it fails the inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on disentangled speech representation learning for voice conversion, which involves speech signal processing and acoustic feature factorization, but it does not center on spoken language models or full-duplex spoken interaction modeling; it lacks discussion on language modeling of speech, synchronization for dialogue, or interaction behaviors, and evaluation does not address interactive or real-time dialogue aspects, only speech quality metrics. Thus, it does not meet the core inclusion criteria relating to spoken language models and interactive dialogue modeling.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on disentangled speech representation learning for voice conversion, which involves speech signal processing and acoustic feature factorization, but it does not center on spoken language models or full-duplex spoken interaction modeling; it lacks discussion on language modeling of speech, synchronization for dialogue, or interaction behaviors, and evaluation does not address interactive or real-time dialogue aspects, only speech quality metrics. Thus, it does not meet the core inclusion criteria relating to spoken language models and interactive dialogue modeling.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Avqvc: One-Shot Voice Conversion By Vector Quantization With Applying Contrastive Learning",
    "abstract": "Voice Conversion(VC) refers to changing the timbre of a speech while retaining the discourse content. Recently, many works have focused on disentangle-based learning techniques to separate the timbre and the linguistic content information from a speech signal. Once successful, voice conversion will be feasible and straightforward. This paper proposed a novel one-shot voice conversion framework based on vector quantization voice conversion (VQVC) and AutoVC, called AVQVC. A new training method is applied to VQVC to separate content and timbre information from speech more effectively. The result shows that this approach has better performance than VQVC in separating content and timbre to improve the sound quality of generated speech.",
    "metadata": {
      "title": "Avqvc: One-Shot Voice Conversion By Vector Quantization With Applying Contrastive Learning",
      "summary": "Voice Conversion(VC) refers to changing the timbre of a speech while retaining the discourse content. Recently, many works have focused on disentangle-based learning techniques to separate the timbre and the linguistic content information from a speech signal. Once successful, voice conversion will be feasible and straightforward. This paper proposed a novel one-shot voice conversion framework based on vector quantization voice conversion (VQVC) and AutoVC, called AVQVC. A new training method is applied to VQVC to separate content and timbre information from speech more effectively. The result shows that this approach has better performance than VQVC in separating content and timbre to improve the sound quality of generated speech.",
      "abstract": "Voice Conversion(VC) refers to changing the timbre of a speech while retaining the discourse content. Recently, many works have focused on disentangle-based learning techniques to separate the timbre and the linguistic content information from a speech signal. Once successful, voice conversion will be feasible and straightforward. This paper proposed a novel one-shot voice conversion framework based on vector quantization voice conversion (VQVC) and AutoVC, called AVQVC. A new training method is applied to VQVC to separate content and timbre information from speech more effectively. The result shows that this approach has better performance than VQVC in separating content and timbre to improve the sound quality of generated speech.",
      "doi": "https://doi.org/10.1109/icassp43922.2022.9746369",
      "openalex_id": "https://openalex.org/W4221141917",
      "arxiv_id": "",
      "publication_date": "2022-04-27",
      "published": "2022-04-27",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The article investigates one-shot voice conversion using vector quantization and contrastive learning, focusing on disentangling content and timbre; there is no discussion of Spoken Language Models or Full-Duplex SLMs, nor explicit emphasis on speech-language modeling, architecture, training, or evaluation for interactive speech-LM systems. Therefore it does not meet the core inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The article investigates one-shot voice conversion using vector quantization and contrastive learning, focusing on disentangling content and timbre; there is no discussion of Spoken Language Models or Full-Duplex SLMs, nor explicit emphasis on speech-language modeling, architecture, training, or evaluation for interactive speech-LM systems. Therefore it does not meet the core inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on voice conversion using vector quantization and contrastive learning but does not address Spoken Language Models (SLMs) or full-duplex spoken language interactive capabilities, nor does it discuss model architectures or evaluation metrics related to synchronous interaction or behavior arbitration. It also lacks mention of language modeling of speech or speech language models explicitly in the title or abstract, and it appears to concentrate on a specific voice conversion task without broader spoken language modeling implications.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on voice conversion using vector quantization and contrastive learning but does not address Spoken Language Models (SLMs) or full-duplex spoken language interactive capabilities, nor does it discuss model architectures or evaluation metrics related to synchronous interaction or behavior arbitration. It also lacks mention of language modeling of speech or speech language models explicitly in the title or abstract, and it appears to concentrate on a specific voice conversion task without broader spoken language modeling implications.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "GLM: General Language Model Pretraining with Autoregressive Blank Infilling",
    "abstract": "Zhengxiao Du, Yujie Qian, Xiao Liu, Ming Ding, Jiezhong Qiu, Zhilin Yang, Jie Tang. Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). 2022.",
    "metadata": {
      "title": "GLM: General Language Model Pretraining with Autoregressive Blank Infilling",
      "summary": "Zhengxiao Du, Yujie Qian, Xiao Liu, Ming Ding, Jiezhong Qiu, Zhilin Yang, Jie Tang. Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). 2022.",
      "abstract": "Zhengxiao Du, Yujie Qian, Xiao Liu, Ming Ding, Jiezhong Qiu, Zhilin Yang, Jie Tang. Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). 2022.",
      "doi": "https://doi.org/10.18653/v1/2022.acl-long.26",
      "openalex_id": "https://openalex.org/W4285294723",
      "arxiv_id": "",
      "publication_date": "2022-01-01",
      "published": "2022-01-01",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This item centers on a general text-based language model pretraining method (GLM) and does not treat speech as the modeling target nor address spoken language interaction or real-time dialogue mechanics, thus it does not meet the inclusion criteria and aligns with an exclusion.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "This item centers on a general text-based language model pretraining method (GLM) and does not treat speech as the modeling target nor address spoken language interaction or real-time dialogue mechanics, thus it does not meet the inclusion criteria and aligns with an exclusion.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The title and abstract provided do not explicitly mention spoken language models, speech as primary modeling objects, or related keywords such as full-duplex interaction or speech-language modeling. The reference is to a general language model pretraining paper without evidence of focusing on spoken language modeling, synchronous speech interaction, or low-latency speech behaviors. Therefore, it does not meet the inclusion criteria centered on spoken language models and related speech-specific modeling and evaluation requirements.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The title and abstract provided do not explicitly mention spoken language models, speech as primary modeling objects, or related keywords such as full-duplex interaction or speech-language modeling. The reference is to a general language model pretraining paper without evidence of focusing on spoken language modeling, synchronous speech interaction, or low-latency speech behaviors. Therefore, it does not meet the inclusion criteria centered on spoken language models and related speech-specific modeling and evaluation requirements.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "ECAPA-TDNN: Emphasized Channel Attention, Propagation and Aggregation in TDNN Based Speaker Verification",
    "abstract": "Current speaker verification techniques rely on a neural network to extract speaker representations. The successful x-vector architecture is a Time Delay Neural Network (TDNN) that applies statistics pooling to project variable-length utterances into fixed-length speaker characterizing embeddings. In this paper, we propose multiple enhancements to this architecture based on recent trends in the related fields of face verification and computer vision. Firstly, the initial frame layers can be restructured into 1-dimensional Res2Net modules with impactful skip connections. Similarly to SE-ResNet, we introduce Squeeze-and-Excitation blocks in these modules to explicitly model channel interdependencies. The SE block expands the temporal context of the frame layer by rescaling the channels according to global properties of the recording. Secondly, neural networks are known to learn hierarchical features, with each layer operating on a different level of complexity. To leverage this complementary information, we aggregate and propagate features of different hierarchical levels. Finally, we improve the statistics pooling module with channel-dependent frame attention. This enables the network to focus on different subsets of frames during each of the channel’s statistics estimation. The proposed ECAPA-TDNN architecture significantly outperforms state-of-the-art TDNN based systems on the VoxCeleb test sets and the 2019 VoxCeleb Speaker Recognition Challenge.",
    "metadata": {
      "title": "ECAPA-TDNN: Emphasized Channel Attention, Propagation and Aggregation in TDNN Based Speaker Verification",
      "summary": "Current speaker verification techniques rely on a neural network to extract speaker representations. The successful x-vector architecture is a Time Delay Neural Network (TDNN) that applies statistics pooling to project variable-length utterances into fixed-length speaker characterizing embeddings. In this paper, we propose multiple enhancements to this architecture based on recent trends in the related fields of face verification and computer vision. Firstly, the initial frame layers can be restructured into 1-dimensional Res2Net modules with impactful skip connections. Similarly to SE-ResNet, we introduce Squeeze-and-Excitation blocks in these modules to explicitly model channel interdependencies. The SE block expands the temporal context of the frame layer by rescaling the channels according to global properties of the recording. Secondly, neural networks are known to learn hierarchical features, with each layer operating on a different level of complexity. To leverage this complementary information, we aggregate and propagate features of different hierarchical levels. Finally, we improve the statistics pooling module with channel-dependent frame attention. This enables the network to focus on different subsets of frames during each of the channel’s statistics estimation. The proposed ECAPA-TDNN architecture significantly outperforms state-of-the-art TDNN based systems on the VoxCeleb test sets and the 2019 VoxCeleb Speaker Recognition Challenge.",
      "abstract": "Current speaker verification techniques rely on a neural network to extract speaker representations. The successful x-vector architecture is a Time Delay Neural Network (TDNN) that applies statistics pooling to project variable-length utterances into fixed-length speaker characterizing embeddings. In this paper, we propose multiple enhancements to this architecture based on recent trends in the related fields of face verification and computer vision. Firstly, the initial frame layers can be restructured into 1-dimensional Res2Net modules with impactful skip connections. Similarly to SE-ResNet, we introduce Squeeze-and-Excitation blocks in these modules to explicitly model channel interdependencies. The SE block expands the temporal context of the frame layer by rescaling the channels according to global properties of the recording. Secondly, neural networks are known to learn hierarchical features, with each layer operating on a different level of complexity. To leverage this complementary information, we aggregate and propagate features of different hierarchical levels. Finally, we improve the statistics pooling module with channel-dependent frame attention. This enables the network to focus on different subsets of frames during each of the channel’s statistics estimation. The proposed ECAPA-TDNN architecture significantly outperforms state-of-the-art TDNN based systems on the VoxCeleb test sets and the 2019 VoxCeleb Speaker Recognition Challenge.",
      "doi": "https://doi.org/10.21437/interspeech.2020-2650",
      "openalex_id": "https://openalex.org/W3024869864",
      "arxiv_id": "",
      "publication_date": "2020-10-25",
      "published": "2020-10-25",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "Reasoning: The item centers on ECAPA-TDNN improvements for speaker verification and does not treat Spoken Language Models or Speech Language Models as the core focus; it also predates the 3-year inclusion window and does not discuss turn-taking, conversational arbitration, or full-duplex speech modeling.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "Reasoning: The item centers on ECAPA-TDNN improvements for speaker verification and does not treat Spoken Language Models or Speech Language Models as the core focus; it also predates the 3-year inclusion window and does not discuss turn-taking, conversational arbitration, or full-duplex speech modeling.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper presents improvements to TDNN-based speaker verification focusing on speaker representation extraction and hierarchical feature aggregation, not on spoken language modeling or full-duplex spoken language models as defined in the inclusion criteria. It does not address core topics like speech language modeling, end-to-end architectures for spoken language understanding, nor synchronization or dialogue interaction capabilities. It is more aligned with speaker verification technology rather than the specified research domain involving speech language models or interactive spoken language systems.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper presents improvements to TDNN-based speaker verification focusing on speaker representation extraction and hierarchical feature aggregation, not on spoken language modeling or full-duplex spoken language models as defined in the inclusion criteria. It does not address core topics like speech language modeling, end-to-end architectures for spoken language understanding, nor synchronization or dialogue interaction capabilities. It is more aligned with speaker verification technology rather than the specified research domain involving speech language models or interactive spoken language systems.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Case-based Reasoning for Natural Language Queries over Knowledge Bases",
    "abstract": "Rajarshi Das, Manzil Zaheer, Dung Thai, Ameya Godbole, Ethan Perez, Jay Yoon Lee, Lizhen Tan, Lazaros Polymenakos, Andrew McCallum. Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing. 2021.",
    "metadata": {
      "title": "Case-based Reasoning for Natural Language Queries over Knowledge Bases",
      "summary": "Rajarshi Das, Manzil Zaheer, Dung Thai, Ameya Godbole, Ethan Perez, Jay Yoon Lee, Lizhen Tan, Lazaros Polymenakos, Andrew McCallum. Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing. 2021.",
      "abstract": "Rajarshi Das, Manzil Zaheer, Dung Thai, Ameya Godbole, Ethan Perez, Jay Yoon Lee, Lizhen Tan, Lazaros Polymenakos, Andrew McCallum. Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing. 2021.",
      "doi": "https://doi.org/10.18653/v1/2021.emnlp-main.755",
      "openalex_id": "https://openalex.org/W3152740956",
      "arxiv_id": "",
      "publication_date": "2021-01-01",
      "published": "2021-01-01",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "此論文於2021年發表，主題非以 Spoken Language Models 為核心，且不在過去三年內公開，因此符合排除條件。",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "此論文於2021年發表，主題非以 Spoken Language Models 為核心，且不在過去三年內公開，因此符合排除條件。",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper focuses on case-based reasoning for natural language queries over knowledge bases, which does not align with the core inclusion criteria centered on spoken language models that handle speech sequences and full-duplex interaction capabilities. There is no mention of spoken language models, speech language models, or related topics in the title or abstract, nor does it appear to discuss speech as the primary modeling object or real-time interaction aspects. Therefore, it fails to meet key inclusion requirements and aligns more with traditional text-based NLP approaches.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper focuses on case-based reasoning for natural language queries over knowledge bases, which does not align with the core inclusion criteria centered on spoken language models that handle speech sequences and full-duplex interaction capabilities. There is no mention of spoken language models, speech language models, or related topics in the title or abstract, nor does it appear to discuss speech as the primary modeling object or real-time interaction aspects. Therefore, it fails to meet key inclusion requirements and aligns more with traditional text-based NLP approaches.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "ASVspoof 2021: accelerating progress in spoofed and deepfake speech detection",
    "abstract": "ASVspoof 2021 is the forth edition in the series of bi-annual challenges which aim to promote the study of spoofing and the design of countermeasures to protect automatic speaker verification systems from manipulation. In addition to a continued focus upon logical and physical access tasks in which there are a number of advances compared to previous editions, ASVspoof 2021 introduces a new task involving deepfake speech detection. This paper describes all three tasks, the new databases for each of them, the evaluation metrics, four challenge baselines, the evaluation platform and a summary of challenge results. Despite the introduction of channel and compression variability which compound the difficulty, results for the logical access and deepfake tasks are close to those from previous ASVspoof editions. Results for the physical access task show the difficulty in detecting attacks in real, variable physical spaces. With ASVspoof 2021 being the first edition for which participants were not provided with any matched training or development data and with this reflecting real conditions in which the nature of spoofed and deepfake speech can never be predicated with confidence, the results are extremely encouraging and demonstrate the substantial progress made in the field in recent years.",
    "metadata": {
      "title": "ASVspoof 2021: accelerating progress in spoofed and deepfake speech detection",
      "summary": "ASVspoof 2021 is the forth edition in the series of bi-annual challenges which aim to promote the study of spoofing and the design of countermeasures to protect automatic speaker verification systems from manipulation. In addition to a continued focus upon logical and physical access tasks in which there are a number of advances compared to previous editions, ASVspoof 2021 introduces a new task involving deepfake speech detection. This paper describes all three tasks, the new databases for each of them, the evaluation metrics, four challenge baselines, the evaluation platform and a summary of challenge results. Despite the introduction of channel and compression variability which compound the difficulty, results for the logical access and deepfake tasks are close to those from previous ASVspoof editions. Results for the physical access task show the difficulty in detecting attacks in real, variable physical spaces. With ASVspoof 2021 being the first edition for which participants were not provided with any matched training or development data and with this reflecting real conditions in which the nature of spoofed and deepfake speech can never be predicated with confidence, the results are extremely encouraging and demonstrate the substantial progress made in the field in recent years.",
      "abstract": "ASVspoof 2021 is the forth edition in the series of bi-annual challenges which aim to promote the study of spoofing and the design of countermeasures to protect automatic speaker verification systems from manipulation. In addition to a continued focus upon logical and physical access tasks in which there are a number of advances compared to previous editions, ASVspoof 2021 introduces a new task involving deepfake speech detection. This paper describes all three tasks, the new databases for each of them, the evaluation metrics, four challenge baselines, the evaluation platform and a summary of challenge results. Despite the introduction of channel and compression variability which compound the difficulty, results for the logical access and deepfake tasks are close to those from previous ASVspoof editions. Results for the physical access task show the difficulty in detecting attacks in real, variable physical spaces. With ASVspoof 2021 being the first edition for which participants were not provided with any matched training or development data and with this reflecting real conditions in which the nature of spoofed and deepfake speech can never be predicated with confidence, the results are extremely encouraging and demonstrate the substantial progress made in the field in recent years.",
      "doi": "https://doi.org/10.21437/asvspoof.2021-8",
      "openalex_id": "https://openalex.org/W3197358873",
      "arxiv_id": "",
      "publication_date": "2021-09-10",
      "published": "2021-09-10",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This item is about spoofing/anti-spoofing in speaker verification (ASVspoof 2021) and deepfake speech detection, not centered on Spoken Language Models or full-duplex speech modeling; it also predates the last 3 years, thus fails the core inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "This item is about spoofing/anti-spoofing in speaker verification (ASVspoof 2021) and deepfake speech detection, not centered on Spoken Language Models or full-duplex speech modeling; it also predates the last 3 years, thus fails the core inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper focuses on spoofed and deepfake speech detection to protect speaker verification systems and does not address Spoken Language Models (SLMs) or Full-Duplex Spoken Language Models (FD-SLMs), nor does it discuss model architectures or evaluation related to speech language modeling; it mainly deals with security aspects rather than speech modeling or interactive/dialogue capabilities as required by the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper focuses on spoofed and deepfake speech detection to protect speaker verification systems and does not address Spoken Language Models (SLMs) or Full-Duplex Spoken Language Models (FD-SLMs), nor does it discuss model architectures or evaluation related to speech language modeling; it mainly deals with security aspects rather than speech modeling or interactive/dialogue capabilities as required by the inclusion criteria.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "MEGABYTE: Predicting Million-byte Sequences with Multiscale Transformers",
    "abstract": "Autoregressive transformers are spectacular models for short sequences but scale poorly to long sequences such as high-resolution images, podcasts, code, or books. We proposed Megabyte, a multi-scale decoder architecture that enables end-to-end differentiable modeling of sequences of over one million bytes. Megabyte segments sequences into patches and uses a local submodel within patches and a global model between patches. This enables sub-quadratic self-attention, much larger feedforward layers for the same compute, and improved parallelism during decoding -- unlocking better performance at reduced cost for both training and generation. Extensive experiments show that Megabyte allows byte-level models to perform competitively with subword models on long context language modeling, achieve state-of-the-art density estimation on ImageNet, and model audio from raw files. Together, these results establish the viability of tokenization-free autoregressive sequence modeling at scale.",
    "metadata": {
      "title": "MEGABYTE: Predicting Million-byte Sequences with Multiscale Transformers",
      "summary": "Autoregressive transformers are spectacular models for short sequences but scale poorly to long sequences such as high-resolution images, podcasts, code, or books. We proposed Megabyte, a multi-scale decoder architecture that enables end-to-end differentiable modeling of sequences of over one million bytes. Megabyte segments sequences into patches and uses a local submodel within patches and a global model between patches. This enables sub-quadratic self-attention, much larger feedforward layers for the same compute, and improved parallelism during decoding -- unlocking better performance at reduced cost for both training and generation. Extensive experiments show that Megabyte allows byte-level models to perform competitively with subword models on long context language modeling, achieve state-of-the-art density estimation on ImageNet, and model audio from raw files. Together, these results establish the viability of tokenization-free autoregressive sequence modeling at scale.",
      "abstract": "Autoregressive transformers are spectacular models for short sequences but scale poorly to long sequences such as high-resolution images, podcasts, code, or books. We proposed Megabyte, a multi-scale decoder architecture that enables end-to-end differentiable modeling of sequences of over one million bytes. Megabyte segments sequences into patches and uses a local submodel within patches and a global model between patches. This enables sub-quadratic self-attention, much larger feedforward layers for the same compute, and improved parallelism during decoding -- unlocking better performance at reduced cost for both training and generation. Extensive experiments show that Megabyte allows byte-level models to perform competitively with subword models on long context language modeling, achieve state-of-the-art density estimation on ImageNet, and model audio from raw files. Together, these results establish the viability of tokenization-free autoregressive sequence modeling at scale.",
      "doi": "https://doi.org/10.48550/arxiv.2305.07185",
      "openalex_id": "https://openalex.org/W4376632433",
      "arxiv_id": "",
      "publication_date": "2023-05-12",
      "published": "2023-05-12",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The Megabyte paper targets long-sequence byte-level autoregressive modeling and multi-scale Transformer architecture for general data (bytes/images/audio) rather than Spoken Language Models or speech-centric language modeling. There is no emphasis on turn-taking, synchronization, live speech interaction, or speech-specific evaluation, and the abstract does not frame speech-language modeling as the core object. Therefore it does not meet the inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The Megabyte paper targets long-sequence byte-level autoregressive modeling and multi-scale Transformer architecture for general data (bytes/images/audio) rather than Spoken Language Models or speech-centric language modeling. There is no emphasis on turn-taking, synchronization, live speech interaction, or speech-specific evaluation, and the abstract does not frame speech-language modeling as the core object. Therefore it does not meet the inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The title and abstract describe a transformer model for very long sequences, focusing on byte-level sequence modeling applicable to images, audio, and text, but it does not specifically address spoken language models or speech language modeling as core subjects. There is no mention of full-duplex spoken interaction, speech-specific architectures, end-to-end speech modeling, or evaluations related to speech turn-taking, backchanneling, or real-time spoken interaction; rather, the focus is on general autoregressive modeling of sequences including raw audio data, without explicit speech language modeling or dialogue interaction perspectives. Therefore, the study does not meet the inclusion criteria centered on spoken language models or full-duplex spoken language interaction, nor does it cover the required speech-specific modeling or evaluation aspects.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The title and abstract describe a transformer model for very long sequences, focusing on byte-level sequence modeling applicable to images, audio, and text, but it does not specifically address spoken language models or speech language modeling as core subjects. There is no mention of full-duplex spoken interaction, speech-specific architectures, end-to-end speech modeling, or evaluations related to speech turn-taking, backchanneling, or real-time spoken interaction; rather, the focus is on general autoregressive modeling of sequences including raw audio data, without explicit speech language modeling or dialogue interaction perspectives. Therefore, the study does not meet the inclusion criteria centered on spoken language models or full-duplex spoken language interaction, nor does it cover the required speech-specific modeling or evaluation aspects.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Multi-level Temporal-channel Speaker Retrieval for Zero-shot Voice Conversion",
    "abstract": "Zero-shot voice conversion (VC) converts source speech into the voice of any desired speaker using only one utterance of the speaker without requiring additional model updates. Typical methods use a speaker representation from a pre-trained speaker verification (SV) model or learn speaker representation during VC training to achieve zero-shot VC. However, existing speaker modeling methods overlook the variation of speaker information richness in temporal and frequency channel dimensions of speech. This insufficient speaker modeling hampers the ability of the VC model to accurately represent unseen speakers who are not in the training dataset. In this study, we present a robust zero-shot VC model with multi-level temporal-channel retrieval, referred to as MTCR-VC. Specifically, to flexibly adapt to the dynamic-variant speaker characteristic in the temporal and channel axis of the speech, we propose a novel fine-grained speaker modeling method, called temporal-channel retrieval (TCR), to find out when and where speaker information appears in speech. It retrieves variable-length speaker representation from both temporal and channel dimensions under the guidance of a pre-trained SV model. Besides, inspired by the hierarchical process of human speech production, the MTCR speaker module stacks several TCR blocks to extract speaker representations from multi-granularity levels. Furthermore, to achieve better speech disentanglement and reconstruction, we introduce a cycle-based training strategy to simulate zero-shot inference recurrently. We adopt perpetual constraints on three aspects, including content, style, and speaker, to drive this process. Experiments demonstrate that MTCR-VC is superior to the previous zero-shot VC methods in modeling speaker timbre while maintaining good speech naturalness.",
    "metadata": {
      "title": "Multi-level Temporal-channel Speaker Retrieval for Zero-shot Voice Conversion",
      "summary": "Zero-shot voice conversion (VC) converts source speech into the voice of any desired speaker using only one utterance of the speaker without requiring additional model updates. Typical methods use a speaker representation from a pre-trained speaker verification (SV) model or learn speaker representation during VC training to achieve zero-shot VC. However, existing speaker modeling methods overlook the variation of speaker information richness in temporal and frequency channel dimensions of speech. This insufficient speaker modeling hampers the ability of the VC model to accurately represent unseen speakers who are not in the training dataset. In this study, we present a robust zero-shot VC model with multi-level temporal-channel retrieval, referred to as MTCR-VC. Specifically, to flexibly adapt to the dynamic-variant speaker characteristic in the temporal and channel axis of the speech, we propose a novel fine-grained speaker modeling method, called temporal-channel retrieval (TCR), to find out when and where speaker information appears in speech. It retrieves variable-length speaker representation from both temporal and channel dimensions under the guidance of a pre-trained SV model. Besides, inspired by the hierarchical process of human speech production, the MTCR speaker module stacks several TCR blocks to extract speaker representations from multi-granularity levels. Furthermore, to achieve better speech disentanglement and reconstruction, we introduce a cycle-based training strategy to simulate zero-shot inference recurrently. We adopt perpetual constraints on three aspects, including content, style, and speaker, to drive this process. Experiments demonstrate that MTCR-VC is superior to the previous zero-shot VC methods in modeling speaker timbre while maintaining good speech naturalness.",
      "abstract": "Zero-shot voice conversion (VC) converts source speech into the voice of any desired speaker using only one utterance of the speaker without requiring additional model updates. Typical methods use a speaker representation from a pre-trained speaker verification (SV) model or learn speaker representation during VC training to achieve zero-shot VC. However, existing speaker modeling methods overlook the variation of speaker information richness in temporal and frequency channel dimensions of speech. This insufficient speaker modeling hampers the ability of the VC model to accurately represent unseen speakers who are not in the training dataset. In this study, we present a robust zero-shot VC model with multi-level temporal-channel retrieval, referred to as MTCR-VC. Specifically, to flexibly adapt to the dynamic-variant speaker characteristic in the temporal and channel axis of the speech, we propose a novel fine-grained speaker modeling method, called temporal-channel retrieval (TCR), to find out when and where speaker information appears in speech. It retrieves variable-length speaker representation from both temporal and channel dimensions under the guidance of a pre-trained SV model. Besides, inspired by the hierarchical process of human speech production, the MTCR speaker module stacks several TCR blocks to extract speaker representations from multi-granularity levels. Furthermore, to achieve better speech disentanglement and reconstruction, we introduce a cycle-based training strategy to simulate zero-shot inference recurrently. We adopt perpetual constraints on three aspects, including content, style, and speaker, to drive this process. Experiments demonstrate that MTCR-VC is superior to the previous zero-shot VC methods in modeling speaker timbre while maintaining good speech naturalness.",
      "doi": "https://doi.org/10.48550/arxiv.2305.07204",
      "openalex_id": "https://openalex.org/W4376632463",
      "arxiv_id": "",
      "publication_date": "2023-05-12",
      "published": "2023-05-12",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item focuses on zero-shot voice conversion and speaker representation rather than Spoken Language Models / speech-language modeling as the core, and does not address full-duplex interactive speech modeling or evaluation; it fails the inclusion criteria even though it is recent.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item focuses on zero-shot voice conversion and speaker representation rather than Spoken Language Models / speech-language modeling as the core, and does not address full-duplex interactive speech modeling or evaluation; it fails the inclusion criteria even though it is recent.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on zero-shot voice conversion by improving speaker representation through multi-level temporal-channel retrieval, which is fundamentally about speech signal processing for voice transformation; however, it does not discuss Spoken Language Models or speech language modeling as core components, nor does it address full-duplex or synchronous spoken interaction aspects, cognitive parallelism, or real-time interactive behaviors required by the inclusion criteria. It also lacks explicit discussion of model training or evaluation in the context of spoken language modeling or full-duplex dialogue management. Therefore, it does not meet the inclusion criteria related to spoken language models and full-duplex spoken interaction.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on zero-shot voice conversion by improving speaker representation through multi-level temporal-channel retrieval, which is fundamentally about speech signal processing for voice transformation; however, it does not discuss Spoken Language Models or speech language modeling as core components, nor does it address full-duplex or synchronous spoken interaction aspects, cognitive parallelism, or real-time interactive behaviors required by the inclusion criteria. It also lacks explicit discussion of model training or evaluation in the context of spoken language modeling or full-duplex dialogue management. Therefore, it does not meet the inclusion criteria related to spoken language models and full-duplex spoken interaction.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "The EMIME Bilingual Database",
    "abstract": "This paper describes the collection of a bilingual database of Finnish/English and German/English data. In addition, the accents of the talkers in the database have been rated. English, German and Finnish listeners assessed the English, German and Finnish talkers ’ degree of foreign accent in English. Native English listeners showed higher inter-listener agreement than non-native listeners. Further analyses showed that non-native listeners judged Finnish and German female talkers to be significantly less accented than do English listeners. German males are judged less accented by Finnish listeners than they are by English and German listeners and there is no difference between listeners as to how they judge the accent of Finnish males. Finally, all English talkers are judged more accented by non-native listeners than they are by native English listeners. Index Terms: evaluation, accent rating, cross-lingual",
    "metadata": {
      "title": "The EMIME Bilingual Database",
      "summary": "This paper describes the collection of a bilingual database of Finnish/English and German/English data. In addition, the accents of the talkers in the database have been rated. English, German and Finnish listeners assessed the English, German and Finnish talkers ’ degree of foreign accent in English. Native English listeners showed higher inter-listener agreement than non-native listeners. Further analyses showed that non-native listeners judged Finnish and German female talkers to be significantly less accented than do English listeners. German males are judged less accented by Finnish listeners than they are by English and German listeners and there is no difference between listeners as to how they judge the accent of Finnish males. Finally, all English talkers are judged more accented by non-native listeners than they are by native English listeners. Index Terms: evaluation, accent rating, cross-lingual",
      "abstract": "This paper describes the collection of a bilingual database of Finnish/English and German/English data. In addition, the accents of the talkers in the database have been rated. English, German and Finnish listeners assessed the English, German and Finnish talkers ’ degree of foreign accent in English. Native English listeners showed higher inter-listener agreement than non-native listeners. Further analyses showed that non-native listeners judged Finnish and German female talkers to be significantly less accented than do English listeners. German males are judged less accented by Finnish listeners than they are by English and German listeners and there is no difference between listeners as to how they judge the accent of Finnish males. Finally, all English talkers are judged more accented by non-native listeners than they are by native English listeners. Index Terms: evaluation, accent rating, cross-lingual",
      "doi": "",
      "openalex_id": "https://openalex.org/W1572989473",
      "arxiv_id": "",
      "publication_date": "2010-01-01",
      "published": "2010-01-01",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The EMIME Bilingual Database focuses on collecting a bilingual speech dataset and accent ratings, not on Spoken Language Models or speech-language modeling architectures/training/evaluation, so it fails the inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The EMIME Bilingual Database focuses on collecting a bilingual speech dataset and accent ratings, not on Spoken Language Models or speech-language modeling architectures/training/evaluation, so it fails the inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper focuses on the collection and evaluation of a bilingual database with accent ratings, without addressing Spoken Language Models (SLMs), full-duplex spoken language modeling, or related architectures, training, or evaluations as required by the inclusion criteria. It primarily deals with accent perception and rating rather than speech language modeling or synchronous interactive dialogue behaviors, and does not discuss model architecture, training, or evaluation related to end-to-end speech language models or low-latency interactive features.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper focuses on the collection and evaluation of a bilingual database with accent ratings, without addressing Spoken Language Models (SLMs), full-duplex spoken language modeling, or related architectures, training, or evaluations as required by the inclusion criteria. It primarily deals with accent perception and rating rather than speech language modeling or synchronous interactive dialogue behaviors, and does not discuss model architecture, training, or evaluation related to end-to-end speech language models or low-latency interactive features.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "FlashAttention-2: Faster Attention with Better Parallelism and Work Partitioning",
    "abstract": "Scaling Transformers to longer sequence lengths has been a major problem in the last several years, promising to improve performance in language modeling and high-resolution image understanding, as well as to unlock new applications in code, audio, and video generation. The attention layer is the main bottleneck in scaling to longer sequences, as its runtime and memory increase quadratically in the sequence length. FlashAttention exploits the asymmetric GPU memory hierarchy to bring significant memory saving (linear instead of quadratic) and runtime speedup (2-4$\\times$ compared to optimized baselines), with no approximation. However, FlashAttention is still not nearly as fast as optimized matrix-multiply (GEMM) operations, reaching only 25-40\\% of the theoretical maximum FLOPs/s. We observe that the inefficiency is due to suboptimal work partitioning between different thread blocks and warps on the GPU, causing either low-occupancy or unnecessary shared memory reads/writes. We propose FlashAttention-2, with better work partitioning to address these issues. In particular, we (1) tweak the algorithm to reduce the number of non-matmul FLOPs (2) parallelize the attention computation, even for a single head, across different thread blocks to increase occupancy, and (3) within each thread block, distribute the work between warps to reduce communication through shared memory. These yield around 2$\\times$ speedup compared to FlashAttention, reaching 50-73\\% of the theoretical maximum FLOPs/s on A100 and getting close to the efficiency of GEMM operations. We empirically validate that when used end-to-end to train GPT-style models, FlashAttention-2 reaches training speed of up to 225 TFLOPs/s per A100 GPU (72\\% model FLOPs utilization).",
    "metadata": {
      "title": "FlashAttention-2: Faster Attention with Better Parallelism and Work Partitioning",
      "summary": "Scaling Transformers to longer sequence lengths has been a major problem in the last several years, promising to improve performance in language modeling and high-resolution image understanding, as well as to unlock new applications in code, audio, and video generation. The attention layer is the main bottleneck in scaling to longer sequences, as its runtime and memory increase quadratically in the sequence length. FlashAttention exploits the asymmetric GPU memory hierarchy to bring significant memory saving (linear instead of quadratic) and runtime speedup (2-4$\\times$ compared to optimized baselines), with no approximation. However, FlashAttention is still not nearly as fast as optimized matrix-multiply (GEMM) operations, reaching only 25-40\\% of the theoretical maximum FLOPs/s. We observe that the inefficiency is due to suboptimal work partitioning between different thread blocks and warps on the GPU, causing either low-occupancy or unnecessary shared memory reads/writes. We propose FlashAttention-2, with better work partitioning to address these issues. In particular, we (1) tweak the algorithm to reduce the number of non-matmul FLOPs (2) parallelize the attention computation, even for a single head, across different thread blocks to increase occupancy, and (3) within each thread block, distribute the work between warps to reduce communication through shared memory. These yield around 2$\\times$ speedup compared to FlashAttention, reaching 50-73\\% of the theoretical maximum FLOPs/s on A100 and getting close to the efficiency of GEMM operations. We empirically validate that when used end-to-end to train GPT-style models, FlashAttention-2 reaches training speed of up to 225 TFLOPs/s per A100 GPU (72\\% model FLOPs utilization).",
      "abstract": "Scaling Transformers to longer sequence lengths has been a major problem in the last several years, promising to improve performance in language modeling and high-resolution image understanding, as well as to unlock new applications in code, audio, and video generation. The attention layer is the main bottleneck in scaling to longer sequences, as its runtime and memory increase quadratically in the sequence length. FlashAttention exploits the asymmetric GPU memory hierarchy to bring significant memory saving (linear instead of quadratic) and runtime speedup (2-4$\\times$ compared to optimized baselines), with no approximation. However, FlashAttention is still not nearly as fast as optimized matrix-multiply (GEMM) operations, reaching only 25-40\\% of the theoretical maximum FLOPs/s. We observe that the inefficiency is due to suboptimal work partitioning between different thread blocks and warps on the GPU, causing either low-occupancy or unnecessary shared memory reads/writes. We propose FlashAttention-2, with better work partitioning to address these issues. In particular, we (1) tweak the algorithm to reduce the number of non-matmul FLOPs (2) parallelize the attention computation, even for a single head, across different thread blocks to increase occupancy, and (3) within each thread block, distribute the work between warps to reduce communication through shared memory. These yield around 2$\\times$ speedup compared to FlashAttention, reaching 50-73\\% of the theoretical maximum FLOPs/s on A100 and getting close to the efficiency of GEMM operations. We empirically validate that when used end-to-end to train GPT-style models, FlashAttention-2 reaches training speed of up to 225 TFLOPs/s per A100 GPU (72\\% model FLOPs utilization).",
      "doi": "https://doi.org/10.48550/arxiv.2307.08691",
      "openalex_id": "https://openalex.org/W4384648639",
      "arxiv_id": "",
      "publication_date": "2023-07-17",
      "published": "2023-07-17",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This item focuses on optimizing transformer attention (FlashAttention-2) for faster hardware performance and does not address Spoken Language Models or speech-centric modeling/evaluation, hence it falls outside the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "This item focuses on optimizing transformer attention (FlashAttention-2) for faster hardware performance and does not address Spoken Language Models or speech-centric modeling/evaluation, hence it falls outside the inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The article focuses on improving the computational efficiency of transformer attention mechanisms, which relates to model architectures and performance optimization but does not mention spoken language models, speech as a primary modeling object, or related inclusion criteria such as full-duplex interaction, cognitive parallelism, or evaluation for spoken language tasks. It is more concerned with general transformer scaling for language and vision applications rather than the spoken language modeling domain specified.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The article focuses on improving the computational efficiency of transformer attention mechanisms, which relates to model architectures and performance optimization but does not mention spoken language models, speech as a primary modeling object, or related inclusion criteria such as full-duplex interaction, cognitive parallelism, or evaluation for spoken language tasks. It is more concerned with general transformer scaling for language and vision applications rather than the spoken language modeling domain specified.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "On Using Monolingual Corpora in Neural Machine Translation",
    "abstract": "Recent work on end-to-end neural network-based architectures for machine translation has shown promising results for En-Fr and En-De translation. Arguably, one of the major factors behind this success has been the availability of high quality parallel corpora. In this work, we investigate how to leverage abundant monolingual corpora for neural machine translation. Compared to a phrase-based and hierarchical baseline, we obtain up to $1.96$ BLEU improvement on the low-resource language pair Turkish-English, and $1.59$ BLEU on the focused domain task of Chinese-English chat messages. While our method was initially targeted toward such tasks with less parallel data, we show that it also extends to high resource languages such as Cs-En and De-En where we obtain an improvement of $0.39$ and $0.47$ BLEU scores over the neural machine translation baselines, respectively.",
    "metadata": {
      "title": "On Using Monolingual Corpora in Neural Machine Translation",
      "summary": "Recent work on end-to-end neural network-based architectures for machine translation has shown promising results for En-Fr and En-De translation. Arguably, one of the major factors behind this success has been the availability of high quality parallel corpora. In this work, we investigate how to leverage abundant monolingual corpora for neural machine translation. Compared to a phrase-based and hierarchical baseline, we obtain up to $1.96$ BLEU improvement on the low-resource language pair Turkish-English, and $1.59$ BLEU on the focused domain task of Chinese-English chat messages. While our method was initially targeted toward such tasks with less parallel data, we show that it also extends to high resource languages such as Cs-En and De-En where we obtain an improvement of $0.39$ and $0.47$ BLEU scores over the neural machine translation baselines, respectively.",
      "abstract": "Recent work on end-to-end neural network-based architectures for machine translation has shown promising results for En-Fr and En-De translation. Arguably, one of the major factors behind this success has been the availability of high quality parallel corpora. In this work, we investigate how to leverage abundant monolingual corpora for neural machine translation. Compared to a phrase-based and hierarchical baseline, we obtain up to $1.96$ BLEU improvement on the low-resource language pair Turkish-English, and $1.59$ BLEU on the focused domain task of Chinese-English chat messages. While our method was initially targeted toward such tasks with less parallel data, we show that it also extends to high resource languages such as Cs-En and De-En where we obtain an improvement of $0.39$ and $0.47$ BLEU scores over the neural machine translation baselines, respectively.",
      "doi": "https://doi.org/10.48550/arxiv.1503.03535",
      "openalex_id": "https://openalex.org/W1915251500",
      "arxiv_id": "",
      "publication_date": "2015-03-11",
      "published": "2015-03-11",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "本論文聚焦於使用單語文本語料訓練神經機器翻譯，核心為文字級語言模型與資料，未以 Spoken Language Models 或語音為核心建模/推理對象，亦無語音互動或即時評估，故不符合本研究的納入條件。",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "本論文聚焦於使用單語文本語料訓練神經機器翻譯，核心為文字級語言模型與資料，未以 Spoken Language Models 或語音為核心建模/推理對象，亦無語音互動或即時評估，故不符合本研究的納入條件。",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper focuses on leveraging monolingual corpora for neural machine translation, which is purely text-based NLP and does not discuss spoken language models, speech-level language modeling, or any full-duplex spoken interaction capabilities. There is no mention of speech as a core modeling object, synchronous interaction, or relevant evaluation metrics like low latency or speech overlap management, thus it does not meet the inclusion criteria centered on speech and spoken language modeling aspects.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper focuses on leveraging monolingual corpora for neural machine translation, which is purely text-based NLP and does not discuss spoken language models, speech-level language modeling, or any full-duplex spoken interaction capabilities. There is no mention of speech as a core modeling object, synchronous interaction, or relevant evaluation metrics like low latency or speech overlap management, thus it does not meet the inclusion criteria centered on speech and spoken language modeling aspects.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "MediumVC: Any-to-any voice conversion using synthetic specific-speaker speeches as intermedium features",
    "abstract": "To realize any-to-any (A2A) voice conversion (VC), most methods are to perform symmetric self-supervised reconstruction tasks (Xi to Xi), which usually results in inefficient performances due to inadequate feature decoupling, especially for unseen speakers. We propose a two-stage reconstruction task (Xi to Yi to Xi) using synthetic specific-speaker speeches as intermedium features, where A2A VC is divided into two stages: any-to-one (A2O) and one-to-Any (O2A). In the A2O stage, we propose a new A2O method: SingleVC, by employing a noval data augment strategy(pitch-shifted and duration-remained, PSDR) to accomplish Xi to Yi. In the O2A stage, MediumVC is proposed based on pre-trained SingleVC to conduct Yi to Xi. Through such asymmetrical reconstruction tasks (Xi to Yi in SingleVC and Yi to Xi in MediumVC), the models are to capture robust disentangled features purposefully. Experiments indicate MediumVC can enhance the similarity of converted speeches while maintaining a high degree of naturalness.",
    "metadata": {
      "title": "MediumVC: Any-to-any voice conversion using synthetic specific-speaker speeches as intermedium features",
      "summary": "To realize any-to-any (A2A) voice conversion (VC), most methods are to perform symmetric self-supervised reconstruction tasks (Xi to Xi), which usually results in inefficient performances due to inadequate feature decoupling, especially for unseen speakers. We propose a two-stage reconstruction task (Xi to Yi to Xi) using synthetic specific-speaker speeches as intermedium features, where A2A VC is divided into two stages: any-to-one (A2O) and one-to-Any (O2A). In the A2O stage, we propose a new A2O method: SingleVC, by employing a noval data augment strategy(pitch-shifted and duration-remained, PSDR) to accomplish Xi to Yi. In the O2A stage, MediumVC is proposed based on pre-trained SingleVC to conduct Yi to Xi. Through such asymmetrical reconstruction tasks (Xi to Yi in SingleVC and Yi to Xi in MediumVC), the models are to capture robust disentangled features purposefully. Experiments indicate MediumVC can enhance the similarity of converted speeches while maintaining a high degree of naturalness.",
      "abstract": "To realize any-to-any (A2A) voice conversion (VC), most methods are to perform symmetric self-supervised reconstruction tasks (Xi to Xi), which usually results in inefficient performances due to inadequate feature decoupling, especially for unseen speakers. We propose a two-stage reconstruction task (Xi to Yi to Xi) using synthetic specific-speaker speeches as intermedium features, where A2A VC is divided into two stages: any-to-one (A2O) and one-to-Any (O2A). In the A2O stage, we propose a new A2O method: SingleVC, by employing a noval data augment strategy(pitch-shifted and duration-remained, PSDR) to accomplish Xi to Yi. In the O2A stage, MediumVC is proposed based on pre-trained SingleVC to conduct Yi to Xi. Through such asymmetrical reconstruction tasks (Xi to Yi in SingleVC and Yi to Xi in MediumVC), the models are to capture robust disentangled features purposefully. Experiments indicate MediumVC can enhance the similarity of converted speeches while maintaining a high degree of naturalness.",
      "doi": "https://doi.org/10.48550/arxiv.2110.02500",
      "openalex_id": "https://openalex.org/W3202267900",
      "arxiv_id": "",
      "publication_date": "2021-10-06",
      "published": "2021-10-06",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "此研究雖是語音轉換技術，但未以 Spoken Language Models 或等價語音語言建模為核心，且摘要未以語音層級語言建模、端到端訓練或聽說互動控制為重點；因此不符合核心納入條件，並且符合排除條件的可能性較高。",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "此研究雖是語音轉換技術，但未以 Spoken Language Models 或等價語音語言建模為核心，且摘要未以語音層級語言建模、端到端訓練或聽說互動控制為重點；因此不符合核心納入條件，並且符合排除條件的可能性較高。",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on voice conversion techniques using synthetic speaker speech as intermediate features, which relates mainly to speech signal processing and generation rather than spoken language modeling or full-duplex spoken language interactions. It does not mention spoken language models, speech language models, or language modeling of speech as core components, nor does it cover multi-turn interaction, cognitive parallelism, synchronization, or dialogue behavior arbitration, which are key to the inclusion criteria. Additionally, the evaluation focuses on similarity and naturalness of voice conversion rather than interactive or turn-taking metrics. Therefore, it does not meet the required inclusion criteria and falls into the exclusion categories of focusing solely on speech synthesis/conversion without addressing dialogue or interactive language modeling.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on voice conversion techniques using synthetic speaker speech as intermediate features, which relates mainly to speech signal processing and generation rather than spoken language modeling or full-duplex spoken language interactions. It does not mention spoken language models, speech language models, or language modeling of speech as core components, nor does it cover multi-turn interaction, cognitive parallelism, synchronization, or dialogue behavior arbitration, which are key to the inclusion criteria. Additionally, the evaluation focuses on similarity and naturalness of voice conversion rather than interactive or turn-taking metrics. Therefore, it does not meet the required inclusion criteria and falls into the exclusion categories of focusing solely on speech synthesis/conversion without addressing dialogue or interactive language modeling.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Language model fusion for streaming end to end speech recognition",
    "abstract": "Streaming processing of speech audio is required for many contemporary practical speech recognition tasks. Even with the large corpora of manually transcribed speech data available today, it is impossible for such corpora to cover adequately the long tail of linguistic content that's important for tasks such as open-ended dictation and voice search. We seek to address both the streaming and the tail recognition challenges by using a language model (LM) trained on unpaired text data to enhance the end-to-end (E2E) model. We extend shallow fusion and cold fusion approaches to streaming Recurrent Neural Network Transducer (RNNT), and also propose two new competitive fusion approaches that further enhance the RNNT architecture. Our results on multiple languages with varying training set sizes show that these fusion methods improve streaming RNNT performance through introducing extra linguistic features. Cold fusion works consistently better on streaming RNNT with up to a 8.5% WER improvement.",
    "metadata": {
      "title": "Language model fusion for streaming end to end speech recognition",
      "summary": "Streaming processing of speech audio is required for many contemporary practical speech recognition tasks. Even with the large corpora of manually transcribed speech data available today, it is impossible for such corpora to cover adequately the long tail of linguistic content that's important for tasks such as open-ended dictation and voice search. We seek to address both the streaming and the tail recognition challenges by using a language model (LM) trained on unpaired text data to enhance the end-to-end (E2E) model. We extend shallow fusion and cold fusion approaches to streaming Recurrent Neural Network Transducer (RNNT), and also propose two new competitive fusion approaches that further enhance the RNNT architecture. Our results on multiple languages with varying training set sizes show that these fusion methods improve streaming RNNT performance through introducing extra linguistic features. Cold fusion works consistently better on streaming RNNT with up to a 8.5% WER improvement.",
      "abstract": "Streaming processing of speech audio is required for many contemporary practical speech recognition tasks. Even with the large corpora of manually transcribed speech data available today, it is impossible for such corpora to cover adequately the long tail of linguistic content that's important for tasks such as open-ended dictation and voice search. We seek to address both the streaming and the tail recognition challenges by using a language model (LM) trained on unpaired text data to enhance the end-to-end (E2E) model. We extend shallow fusion and cold fusion approaches to streaming Recurrent Neural Network Transducer (RNNT), and also propose two new competitive fusion approaches that further enhance the RNNT architecture. Our results on multiple languages with varying training set sizes show that these fusion methods improve streaming RNNT performance through introducing extra linguistic features. Cold fusion works consistently better on streaming RNNT with up to a 8.5% WER improvement.",
      "doi": "https://doi.org/10.48550/arxiv.2104.04487",
      "openalex_id": "https://openalex.org/W3154308281",
      "arxiv_id": "",
      "publication_date": "2021-04-09",
      "published": "2021-04-09",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This item centers on integrating language models with a streaming end-to-end speech recognition system (LM fusion for RNNT), addressing speech-language modeling in an end-to-end setting, within the last 3 years, and mentions language modeling of speech as part of the approach.",
      "evaluation": 5
    },
    "round-A_JuniorNano_reasoning": "This item centers on integrating language models with a streaming end-to-end speech recognition system (LM fusion for RNNT), addressing speech-language modeling in an end-to-end setting, within the last 3 years, and mentions language modeling of speech as part of the approach.",
    "round-A_JuniorNano_evaluation": 5,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on integrating language models with an end-to-end streaming speech recognition model (RNNT), addressing improvement in speech recognition accuracy by leveraging unpaired text data for fusion approaches. However, it primarily concerns ASR performance enhancement (WER improvements) and does not explicitly discuss spoken language models (SLMs), full-duplex interaction capabilities, cognitive parallelism, interruption management, low latency real-time interaction, or synchronization mechanisms as required by the inclusion criteria. The abstract lacks mention of spoken language modeling as the central research object or discussion of speech modeling beyond ASR, nor does it address evaluation metrics related to interactive dialogue or speech overlap handling. Therefore, it does not meet the inclusion criteria of focusing on spoken language models or full-duplex spoken language modeling, nor does it avoid exclusion criteria related to single-task ASR focus and limited evaluation metrics.",
      "evaluation": 2
    },
    "round-A_JuniorMini_reasoning": "The study focuses on integrating language models with an end-to-end streaming speech recognition model (RNNT), addressing improvement in speech recognition accuracy by leveraging unpaired text data for fusion approaches. However, it primarily concerns ASR performance enhancement (WER improvements) and does not explicitly discuss spoken language models (SLMs), full-duplex interaction capabilities, cognitive parallelism, interruption management, low latency real-time interaction, or synchronization mechanisms as required by the inclusion criteria. The abstract lacks mention of spoken language modeling as the central research object or discussion of speech modeling beyond ASR, nor does it address evaluation metrics related to interactive dialogue or speech overlap handling. Therefore, it does not meet the inclusion criteria of focusing on spoken language models or full-duplex spoken language modeling, nor does it avoid exclusion criteria related to single-task ASR focus and limited evaluation metrics.",
    "round-A_JuniorMini_evaluation": 2,
    "round-B_SeniorLead_output": {
      "reasoning": "This paper focuses on LM fusion to improve streaming RNNT ASR (WER gains) and does not present spoken language models or full‑duplex SLM capabilities, does not treat speech itself as the core modeling/inference object beyond ASR, and lacks required keywords (e.g., \"spoken language model\"), so it fails the inclusion criteria and meets exclusion criteria for ASR‑only work; Junior reviewers disagreed (one mistakenly favored inclusion, the other correctly excluded), but the evidence supports exclusion.",
      "evaluation": 1
    },
    "round-B_SeniorLead_reasoning": "This paper focuses on LM fusion to improve streaming RNNT ASR (WER gains) and does not present spoken language models or full‑duplex SLM capabilities, does not treat speech itself as the core modeling/inference object beyond ASR, and lacks required keywords (e.g., \"spoken language model\"), so it fails the inclusion criteria and meets exclusion criteria for ASR‑only work; Junior reviewers disagreed (one mistakenly favored inclusion, the other correctly excluded), but the evidence supports exclusion.",
    "round-B_SeniorLead_evaluation": 1,
    "final_verdict": "exclude (senior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "LeBenchmark: A Reproducible Framework for Assessing Self-Supervised\\n Representation Learning from Speech",
    "abstract": "Self-Supervised Learning (SSL) using huge unlabeled data has been\\nsuccessfully explored for image and natural language processing. Recent works\\nalso investigated SSL from speech. They were notably successful to improve\\nperformance on downstream tasks such as automatic speech recognition (ASR).\\nWhile these works suggest it is possible to reduce dependence on labeled data\\nfor building efficient speech systems, their evaluation was mostly made on ASR\\nand using multiple and heterogeneous experimental settings (most of them for\\nEnglish). This questions the objective comparison of SSL approaches and the\\nevaluation of their impact on building speech systems. In this paper, we\\npropose LeBenchmark: a reproducible framework for assessing SSL from speech. It\\nnot only includes ASR (high and low resource) tasks but also spoken language\\nunderstanding, speech translation and emotion recognition. We also focus on\\nspeech technologies in a language different than English: French. SSL models of\\ndifferent sizes are trained from carefully sourced and documented datasets.\\nExperiments show that SSL is beneficial for most but not all tasks which\\nconfirms the need for exhaustive and reliable benchmarks to evaluate its real\\nimpact. LeBenchmark is shared with the scientific community for reproducible\\nresearch in SSL from speech.\\n",
    "metadata": {
      "title": "LeBenchmark: A Reproducible Framework for Assessing Self-Supervised\\n Representation Learning from Speech",
      "summary": "Self-Supervised Learning (SSL) using huge unlabeled data has been\\nsuccessfully explored for image and natural language processing. Recent works\\nalso investigated SSL from speech. They were notably successful to improve\\nperformance on downstream tasks such as automatic speech recognition (ASR).\\nWhile these works suggest it is possible to reduce dependence on labeled data\\nfor building efficient speech systems, their evaluation was mostly made on ASR\\nand using multiple and heterogeneous experimental settings (most of them for\\nEnglish). This questions the objective comparison of SSL approaches and the\\nevaluation of their impact on building speech systems. In this paper, we\\npropose LeBenchmark: a reproducible framework for assessing SSL from speech. It\\nnot only includes ASR (high and low resource) tasks but also spoken language\\nunderstanding, speech translation and emotion recognition. We also focus on\\nspeech technologies in a language different than English: French. SSL models of\\ndifferent sizes are trained from carefully sourced and documented datasets.\\nExperiments show that SSL is beneficial for most but not all tasks which\\nconfirms the need for exhaustive and reliable benchmarks to evaluate its real\\nimpact. LeBenchmark is shared with the scientific community for reproducible\\nresearch in SSL from speech.\\n",
      "abstract": "Self-Supervised Learning (SSL) using huge unlabeled data has been\\nsuccessfully explored for image and natural language processing. Recent works\\nalso investigated SSL from speech. They were notably successful to improve\\nperformance on downstream tasks such as automatic speech recognition (ASR).\\nWhile these works suggest it is possible to reduce dependence on labeled data\\nfor building efficient speech systems, their evaluation was mostly made on ASR\\nand using multiple and heterogeneous experimental settings (most of them for\\nEnglish). This questions the objective comparison of SSL approaches and the\\nevaluation of their impact on building speech systems. In this paper, we\\npropose LeBenchmark: a reproducible framework for assessing SSL from speech. It\\nnot only includes ASR (high and low resource) tasks but also spoken language\\nunderstanding, speech translation and emotion recognition. We also focus on\\nspeech technologies in a language different than English: French. SSL models of\\ndifferent sizes are trained from carefully sourced and documented datasets.\\nExperiments show that SSL is beneficial for most but not all tasks which\\nconfirms the need for exhaustive and reliable benchmarks to evaluate its real\\nimpact. LeBenchmark is shared with the scientific community for reproducible\\nresearch in SSL from speech.\\n",
      "doi": "https://doi.org/10.21437/interspeech.2021-556",
      "openalex_id": "https://openalex.org/W3189296823",
      "arxiv_id": "",
      "publication_date": "2021-04-23",
      "published": "2021-04-23",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "Although the item is recent and covers multiple speech tasks, it centers on benchmarking self-supervised learning for speech representations rather than treating Spoken Language Models or Speech Language Models as the core modeling object; the abstract does not frame SLM/FD-SLM or speech-language modeling as the main focus, so it likely does not meet the core inclusion criterion and should be excluded.",
      "evaluation": 2
    },
    "round-A_JuniorNano_reasoning": "Although the item is recent and covers multiple speech tasks, it centers on benchmarking self-supervised learning for speech representations rather than treating Spoken Language Models or Speech Language Models as the core modeling object; the abstract does not frame SLM/FD-SLM or speech-language modeling as the main focus, so it likely does not meet the core inclusion criterion and should be excluded.",
    "round-A_JuniorNano_evaluation": 2,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper focuses on a reproducible framework to evaluate self-supervised learning methods for speech representations, covering several downstream tasks including ASR, spoken language understanding, translation, and emotion recognition, but it does not explicitly address core Spoken Language Models (SLMs) or Full-Duplex Spoken Language Models (FD-SLMs), nor does it discuss synchronous interactive dialogue behaviors such as cognitive parallelism, speech overlap, or real-time interruption/backchanneling; it also lacks mention of modeling speech as a primary modeling and reasoning object rather than preprocessing input.",
      "evaluation": 2
    },
    "round-A_JuniorMini_reasoning": "The paper focuses on a reproducible framework to evaluate self-supervised learning methods for speech representations, covering several downstream tasks including ASR, spoken language understanding, translation, and emotion recognition, but it does not explicitly address core Spoken Language Models (SLMs) or Full-Duplex Spoken Language Models (FD-SLMs), nor does it discuss synchronous interactive dialogue behaviors such as cognitive parallelism, speech overlap, or real-time interruption/backchanneling; it also lacks mention of modeling speech as a primary modeling and reasoning object rather than preprocessing input.",
    "round-A_JuniorMini_evaluation": 2,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:2)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "AutoPrompt: Eliciting Knowledge from Language Models with Automatically Generated Prompts",
    "abstract": "The remarkable success of pretrained language models has motivated the study of what kinds of knowledge these models learn during pretraining. Reformulating tasks as fill-in-the-blanks problems (e.g., cloze tests) is a natural approach for gauging such knowledge, however, its usage is limited by the manual effort and guesswork required to write suitable prompts. To address this, we develop AutoPrompt, an automated method to create prompts for a diverse set of tasks, based on a gradient-guided search. Using AutoPrompt, we show that masked language models (MLMs) have an inherent capability to perform sentiment analysis and natural language inference without additional parameters or finetuning, sometimes achieving performance on par with recent state-of-the-art supervised models. We also show that our prompts elicit more accurate factual knowledge from MLMs than the manually created prompts on the LAMA benchmark, and that MLMs can be used as relation extractors more effectively than supervised relation extraction models. These results demonstrate that automatically generated prompts are a viable parameter-free alternative to existing probing methods, and as pretrained LMs become more sophisticated and capable, potentially a replacement for finetuning.",
    "metadata": {
      "title": "AutoPrompt: Eliciting Knowledge from Language Models with Automatically Generated Prompts",
      "summary": "The remarkable success of pretrained language models has motivated the study of what kinds of knowledge these models learn during pretraining. Reformulating tasks as fill-in-the-blanks problems (e.g., cloze tests) is a natural approach for gauging such knowledge, however, its usage is limited by the manual effort and guesswork required to write suitable prompts. To address this, we develop AutoPrompt, an automated method to create prompts for a diverse set of tasks, based on a gradient-guided search. Using AutoPrompt, we show that masked language models (MLMs) have an inherent capability to perform sentiment analysis and natural language inference without additional parameters or finetuning, sometimes achieving performance on par with recent state-of-the-art supervised models. We also show that our prompts elicit more accurate factual knowledge from MLMs than the manually created prompts on the LAMA benchmark, and that MLMs can be used as relation extractors more effectively than supervised relation extraction models. These results demonstrate that automatically generated prompts are a viable parameter-free alternative to existing probing methods, and as pretrained LMs become more sophisticated and capable, potentially a replacement for finetuning.",
      "abstract": "The remarkable success of pretrained language models has motivated the study of what kinds of knowledge these models learn during pretraining. Reformulating tasks as fill-in-the-blanks problems (e.g., cloze tests) is a natural approach for gauging such knowledge, however, its usage is limited by the manual effort and guesswork required to write suitable prompts. To address this, we develop AutoPrompt, an automated method to create prompts for a diverse set of tasks, based on a gradient-guided search. Using AutoPrompt, we show that masked language models (MLMs) have an inherent capability to perform sentiment analysis and natural language inference without additional parameters or finetuning, sometimes achieving performance on par with recent state-of-the-art supervised models. We also show that our prompts elicit more accurate factual knowledge from MLMs than the manually created prompts on the LAMA benchmark, and that MLMs can be used as relation extractors more effectively than supervised relation extraction models. These results demonstrate that automatically generated prompts are a viable parameter-free alternative to existing probing methods, and as pretrained LMs become more sophisticated and capable, potentially a replacement for finetuning.",
      "doi": "https://doi.org/10.18653/v1/2020.emnlp-main.346",
      "openalex_id": "https://openalex.org/W3098267758",
      "arxiv_id": "",
      "publication_date": "2020-01-01",
      "published": "2020-01-01",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item centers on AutoPrompt for general language models and probing capabilities, not on Spoken Language Models or speech-based modeling/interaction, and is not within the past 3 years, so it does not meet core inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item centers on AutoPrompt for general language models and probing capabilities, not on Spoken Language Models or speech-based modeling/interaction, and is not within the past 3 years, so it does not meet core inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on automatically generating prompts for pretrained language models to elicit knowledge, addressing tasks like sentiment analysis and relation extraction, but it centers on text-based language models rather than spoken language models or speech as the core modeling target; it does not discuss speech-level modeling, full-duplex spoken interaction, or synchronicity behaviors, nor does it meet the inclusion criteria emphasizing spoken language modeling and real-time interaction capabilities.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on automatically generating prompts for pretrained language models to elicit knowledge, addressing tasks like sentiment analysis and relation extraction, but it centers on text-based language models rather than spoken language models or speech as the core modeling target; it does not discuss speech-level modeling, full-duplex spoken interaction, or synchronicity behaviors, nor does it meet the inclusion criteria emphasizing spoken language modeling and real-time interaction capabilities.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Knowledgeable Prompt-tuning: Incorporating Knowledge into Prompt Verbalizer for Text Classification",
    "abstract": "Shengding Hu, Ning Ding, Huadong Wang, Zhiyuan Liu, Jingang Wang, Juanzi Li, Wei Wu, Maosong Sun. Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). 2022.",
    "metadata": {
      "title": "Knowledgeable Prompt-tuning: Incorporating Knowledge into Prompt Verbalizer for Text Classification",
      "summary": "Shengding Hu, Ning Ding, Huadong Wang, Zhiyuan Liu, Jingang Wang, Juanzi Li, Wei Wu, Maosong Sun. Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). 2022.",
      "abstract": "Shengding Hu, Ning Ding, Huadong Wang, Zhiyuan Liu, Jingang Wang, Juanzi Li, Wei Wu, Maosong Sun. Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). 2022.",
      "doi": "https://doi.org/10.18653/v1/2022.acl-long.158",
      "openalex_id": "https://openalex.org/W3188542058",
      "arxiv_id": "",
      "publication_date": "2022-01-01",
      "published": "2022-01-01",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The title/abstract describe text-only prompt-tuning for text classification and do not address Spoken Language Models or speech-based modeling; it does not meet the inclusion criteria and aligns with the exclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The title/abstract describe text-only prompt-tuning for text classification and do not address Spoken Language Models or speech-based modeling; it does not meet the inclusion criteria and aligns with the exclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The provided title and abstract information focus on prompt tuning and knowledge incorporation for text classification, which is related to NLP but does not mention Spoken Language Models, speech language modeling, or speech as a core modeling object. There is no indication of speech signal processing, full-duplex spoken interaction, or synchronization aspects required by the inclusion criteria. Additionally, the document is from 2022, which is within the last 3 years, but it clearly centers on text classification rather than speech modeling or spoken language models. Therefore, it does not meet the inclusion criteria and falls under exclusion for not focusing on speech-based models and interaction.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The provided title and abstract information focus on prompt tuning and knowledge incorporation for text classification, which is related to NLP but does not mention Spoken Language Models, speech language modeling, or speech as a core modeling object. There is no indication of speech signal processing, full-duplex spoken interaction, or synchronization aspects required by the inclusion criteria. Additionally, the document is from 2022, which is within the last 3 years, but it clearly centers on text classification rather than speech modeling or spoken language models. Therefore, it does not meet the inclusion criteria and falls under exclusion for not focusing on speech-based models and interaction.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Parameter-efficient fine-tuning of large-scale pre-trained language models",
    "abstract": "Abstract With the prevalence of pre-trained language models (PLMs) and the pre-training–fine-tuning paradigm, it has been continuously shown that larger models tend to yield better performance. However, as PLMs scale up, fine-tuning and storing all the parameters is prohibitively costly and eventually becomes practically infeasible. This necessitates a new branch of research focusing on the parameter-efficient adaptation of PLMs, which optimizes a small portion of the model parameters while keeping the rest fixed, drastically cutting down computation and storage costs. In general, it demonstrates that large-scale models could be effectively stimulated by the optimization of a few parameters. Despite the various designs, here we discuss and analyse the approaches under a more consistent and accessible term ‘delta-tuning’, where ‘delta’ a mathematical notation often used to denote changes, is borrowed to refer to the portion of parameters that are ‘changed’ during training. We formally describe the problem and propose a unified categorization criterion for existing delta-tuning methods to explore their correlations and differences. We also discuss the theoretical principles underlying the effectiveness of delta-tuning and interpret them from the perspectives of optimization and optimal control. Furthermore, we provide a holistic empirical study on over 100 natural language processing tasks and investigate various aspects of delta-tuning. With comprehensive study and analysis, our research demonstrates the theoretical and practical properties of delta-tuning in the adaptation of PLMs.",
    "metadata": {
      "title": "Parameter-efficient fine-tuning of large-scale pre-trained language models",
      "summary": "Abstract With the prevalence of pre-trained language models (PLMs) and the pre-training–fine-tuning paradigm, it has been continuously shown that larger models tend to yield better performance. However, as PLMs scale up, fine-tuning and storing all the parameters is prohibitively costly and eventually becomes practically infeasible. This necessitates a new branch of research focusing on the parameter-efficient adaptation of PLMs, which optimizes a small portion of the model parameters while keeping the rest fixed, drastically cutting down computation and storage costs. In general, it demonstrates that large-scale models could be effectively stimulated by the optimization of a few parameters. Despite the various designs, here we discuss and analyse the approaches under a more consistent and accessible term ‘delta-tuning’, where ‘delta’ a mathematical notation often used to denote changes, is borrowed to refer to the portion of parameters that are ‘changed’ during training. We formally describe the problem and propose a unified categorization criterion for existing delta-tuning methods to explore their correlations and differences. We also discuss the theoretical principles underlying the effectiveness of delta-tuning and interpret them from the perspectives of optimization and optimal control. Furthermore, we provide a holistic empirical study on over 100 natural language processing tasks and investigate various aspects of delta-tuning. With comprehensive study and analysis, our research demonstrates the theoretical and practical properties of delta-tuning in the adaptation of PLMs.",
      "abstract": "Abstract With the prevalence of pre-trained language models (PLMs) and the pre-training–fine-tuning paradigm, it has been continuously shown that larger models tend to yield better performance. However, as PLMs scale up, fine-tuning and storing all the parameters is prohibitively costly and eventually becomes practically infeasible. This necessitates a new branch of research focusing on the parameter-efficient adaptation of PLMs, which optimizes a small portion of the model parameters while keeping the rest fixed, drastically cutting down computation and storage costs. In general, it demonstrates that large-scale models could be effectively stimulated by the optimization of a few parameters. Despite the various designs, here we discuss and analyse the approaches under a more consistent and accessible term ‘delta-tuning’, where ‘delta’ a mathematical notation often used to denote changes, is borrowed to refer to the portion of parameters that are ‘changed’ during training. We formally describe the problem and propose a unified categorization criterion for existing delta-tuning methods to explore their correlations and differences. We also discuss the theoretical principles underlying the effectiveness of delta-tuning and interpret them from the perspectives of optimization and optimal control. Furthermore, we provide a holistic empirical study on over 100 natural language processing tasks and investigate various aspects of delta-tuning. With comprehensive study and analysis, our research demonstrates the theoretical and practical properties of delta-tuning in the adaptation of PLMs.",
      "doi": "https://doi.org/10.1038/s42256-023-00626-4",
      "openalex_id": "https://openalex.org/W4322766882",
      "arxiv_id": "",
      "publication_date": "2023-03-02",
      "published": "2023-03-02",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "It addresses parameter-efficient fine-tuning for large-scale pre-trained language models in NLP, with no focus on spoken language modeling, speech-based interaction, or evaluation of speech/synchronization; therefore it does not meet the inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "It addresses parameter-efficient fine-tuning for large-scale pre-trained language models in NLP, with no focus on spoken language modeling, speech-based interaction, or evaluation of speech/synchronization; therefore it does not meet the inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on parameter-efficient fine-tuning of large pre-trained language models with an emphasis on delta-tuning, optimization, and empirical studies on NLP tasks but does not specifically address spoken language models, speech as core modeling data, full-duplex interaction, or related synchronization and interaction criteria required by the inclusion standards. The abstract does not mention speech, spoken language modeling, or voice as a primary modeling target, thus it does not fit the specific inclusion criteria and matches some exclusions such as focusing on text-based NLP rather than speech.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on parameter-efficient fine-tuning of large pre-trained language models with an emphasis on delta-tuning, optimization, and empirical studies on NLP tasks but does not specifically address spoken language models, speech as core modeling data, full-duplex interaction, or related synchronization and interaction criteria required by the inclusion standards. The abstract does not mention speech, spoken language modeling, or voice as a primary modeling target, thus it does not fit the specific inclusion criteria and matches some exclusions such as focusing on text-based NLP rather than speech.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "An Exploration of Prompt Tuning on Generative Spoken Language Model for Speech Processing Tasks",
    "abstract": "Speech representations learned from Self-supervised learning (SSL) models can benefit various speech processing tasks. However, utilizing SSL representations usually requires fine-tuning the pre-trained models or designing task-specific downstream models and loss functions, causing much memory usage and human labor. Recently, prompting in Natural Language Processing (NLP) has been found to be an efficient technique to leverage pre-trained language models (LMs). Specifically, prompt tuning optimizes a limited number of task-specific parameters with a fixed pre-trained model; as a result, only a small set of parameters is needed to be stored for each task. Prompt tuning improves computation and memory efficiency by leveraging the pre-trained LM's prediction ability. Nevertheless, such a paradigm is little studied in the speech community. We report in this paper the first exploration of the prompt tuning paradigm for speech processing tasks based on Generative Spoken Language Model (GSLM). Experiment results show that the prompt tuning technique achieves competitive performance in speech classification tasks with fewer trainable parameters than fine-tuning specialized downstream models. We further study the technique in challenging sequence generation tasks. Prompt tuning also demonstrates its potential, while the limitation and possible research directions are discussed in this paper. The source code is available on https://github.com/ga642381/SpeechPrompt.",
    "metadata": {
      "title": "An Exploration of Prompt Tuning on Generative Spoken Language Model for Speech Processing Tasks",
      "summary": "Speech representations learned from Self-supervised learning (SSL) models can benefit various speech processing tasks. However, utilizing SSL representations usually requires fine-tuning the pre-trained models or designing task-specific downstream models and loss functions, causing much memory usage and human labor. Recently, prompting in Natural Language Processing (NLP) has been found to be an efficient technique to leverage pre-trained language models (LMs). Specifically, prompt tuning optimizes a limited number of task-specific parameters with a fixed pre-trained model; as a result, only a small set of parameters is needed to be stored for each task. Prompt tuning improves computation and memory efficiency by leveraging the pre-trained LM's prediction ability. Nevertheless, such a paradigm is little studied in the speech community. We report in this paper the first exploration of the prompt tuning paradigm for speech processing tasks based on Generative Spoken Language Model (GSLM). Experiment results show that the prompt tuning technique achieves competitive performance in speech classification tasks with fewer trainable parameters than fine-tuning specialized downstream models. We further study the technique in challenging sequence generation tasks. Prompt tuning also demonstrates its potential, while the limitation and possible research directions are discussed in this paper. The source code is available on https://github.com/ga642381/SpeechPrompt.",
      "abstract": "Speech representations learned from Self-supervised learning (SSL) models can benefit various speech processing tasks. However, utilizing SSL representations usually requires fine-tuning the pre-trained models or designing task-specific downstream models and loss functions, causing much memory usage and human labor. Recently, prompting in Natural Language Processing (NLP) has been found to be an efficient technique to leverage pre-trained language models (LMs). Specifically, prompt tuning optimizes a limited number of task-specific parameters with a fixed pre-trained model; as a result, only a small set of parameters is needed to be stored for each task. Prompt tuning improves computation and memory efficiency by leveraging the pre-trained LM's prediction ability. Nevertheless, such a paradigm is little studied in the speech community. We report in this paper the first exploration of the prompt tuning paradigm for speech processing tasks based on Generative Spoken Language Model (GSLM). Experiment results show that the prompt tuning technique achieves competitive performance in speech classification tasks with fewer trainable parameters than fine-tuning specialized downstream models. We further study the technique in challenging sequence generation tasks. Prompt tuning also demonstrates its potential, while the limitation and possible research directions are discussed in this paper. The source code is available on https://github.com/ga642381/SpeechPrompt.",
      "doi": "https://doi.org/10.21437/interspeech.2022-10610",
      "openalex_id": "https://openalex.org/W4226162428",
      "arxiv_id": "",
      "publication_date": "2022-09-16",
      "published": "2022-09-16",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This paper centers on Generative Spoken Language Models (GSLMs) for speech processing and discusses prompt-tuning—an architectural/training technique—aligning with the SLM/Speech-LM focus, includes model training/evaluation, and is within the last three years; it is not limited to ASR/TTS or text-NLP-only.",
      "evaluation": 5
    },
    "round-A_JuniorNano_reasoning": "This paper centers on Generative Spoken Language Models (GSLMs) for speech processing and discusses prompt-tuning—an architectural/training technique—aligning with the SLM/Speech-LM focus, includes model training/evaluation, and is within the last three years; it is not limited to ASR/TTS or text-NLP-only.",
    "round-A_JuniorNano_evaluation": 5,
    "round-A_JuniorMini_output": {
      "reasoning": "The study explores prompt tuning techniques on Generative Spoken Language Models (GSLMs), which is a form of Spoken Language Models focusing on speech representations and modeling rather than just text or ASR/TTS tasks. It discusses model adaptation methods and evaluates performance with fewer trainable parameters, indicating consideration of model architecture and efficiency. The topic aligns with spoken language modeling, though it does not explicitly mention full-duplex interaction, cognitive parallelism, or real-time interaction metrics like sub-200ms latency or turn-taking behaviors, which are critical criteria in the inclusion. The evaluation is on speech classification and sequence generation tasks rather than interaction or behavior arbitration, and the abstract shows genetic research methods and available source code, indicating completeness. Therefore, the study partially meets the inclusion criteria but lacks direct focus on full-duplex or synchronous interactive capabilities as required, and does not address low-latency or multi-dimensional interaction evaluations.",
      "evaluation": 3
    },
    "round-A_JuniorMini_reasoning": "The study explores prompt tuning techniques on Generative Spoken Language Models (GSLMs), which is a form of Spoken Language Models focusing on speech representations and modeling rather than just text or ASR/TTS tasks. It discusses model adaptation methods and evaluates performance with fewer trainable parameters, indicating consideration of model architecture and efficiency. The topic aligns with spoken language modeling, though it does not explicitly mention full-duplex interaction, cognitive parallelism, or real-time interaction metrics like sub-200ms latency or turn-taking behaviors, which are critical criteria in the inclusion. The evaluation is on speech classification and sequence generation tasks rather than interaction or behavior arbitration, and the abstract shows genetic research methods and available source code, indicating completeness. Therefore, the study partially meets the inclusion criteria but lacks direct focus on full-duplex or synchronous interactive capabilities as required, and does not address low-latency or multi-dimensional interaction evaluations.",
    "round-A_JuniorMini_evaluation": 3,
    "round-B_SeniorLead_output": {
      "reasoning": "The paper does center on a Generative Spoken Language Model and prompt tuning (so it satisfies the requirement of SLM being the core modeling object, as noted by junior reviewer A), but it only evaluates classification and sequence generation efficiency and does not address full‑duplex/synchronous interaction issues (turn‑taking → full‑duplex, cognitive parallelism, overlap/interruption handling, low‑latency metrics or interactive arbitration) — therefore it falls into the exclusion criterion of not covering synchronization/interactive behavior; recommend exclusion.",
      "evaluation": 2
    },
    "round-B_SeniorLead_reasoning": "The paper does center on a Generative Spoken Language Model and prompt tuning (so it satisfies the requirement of SLM being the core modeling object, as noted by junior reviewer A), but it only evaluates classification and sequence generation efficiency and does not address full‑duplex/synchronous interaction issues (turn‑taking → full‑duplex, cognitive parallelism, overlap/interruption handling, low‑latency metrics or interactive arbitration) — therefore it falls into the exclusion criterion of not covering synchronization/interactive behavior; recommend exclusion.",
    "round-B_SeniorLead_evaluation": 2,
    "final_verdict": "exclude (senior:2)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Preserving privacy in speaker and speech characterisation",
    "abstract": "Speech recordings are a rich source of personal, sensitive data that can be used to support a plethora of diverse applications, from health profiling to biometric recognition. It is therefore essential that speech recordings are adequately protected so that they cannot be misused. Such protection, in the form of privacy-preserving technologies, is required to ensure that: (i) the biometric profiles of a given individual (e.g., across different biometric service operators) are unlinkable; (ii) leaked, encrypted biometric information is irreversible, and that (iii) biometric references are renewable. Whereas many privacy-preserving technologies have been developed for other biometric characteristics, very few solutions have been proposed to protect privacy in the case of speech signals. Despite privacy preservation this is now being mandated by recent European and international data protection regulations. With the aim of fostering progress and collaboration between researchers in the speech, biometrics and applied cryptography communities, this survey article provides an introduction to the field, starting with a legal perspective on privacy preservation in the case of speech data. It then establishes the requirements for effective privacy preservation, reviews generic cryptography-based solutions, followed by specific techniques that are applicable to speaker characterisation (biometric applications) and speech characterisation (non-biometric applications). Glancing at non-biometrics, methods are presented to avoid function creep, preventing the exploitation of biometric information, e.g., to single out an identity in speech-assisted health care via speaker characterisation. In promoting harmonised research, the article also outlines common, empirical evaluation metrics for the assessment of privacy-preserving technologies for speech data.",
    "metadata": {
      "title": "Preserving privacy in speaker and speech characterisation",
      "summary": "Speech recordings are a rich source of personal, sensitive data that can be used to support a plethora of diverse applications, from health profiling to biometric recognition. It is therefore essential that speech recordings are adequately protected so that they cannot be misused. Such protection, in the form of privacy-preserving technologies, is required to ensure that: (i) the biometric profiles of a given individual (e.g., across different biometric service operators) are unlinkable; (ii) leaked, encrypted biometric information is irreversible, and that (iii) biometric references are renewable. Whereas many privacy-preserving technologies have been developed for other biometric characteristics, very few solutions have been proposed to protect privacy in the case of speech signals. Despite privacy preservation this is now being mandated by recent European and international data protection regulations. With the aim of fostering progress and collaboration between researchers in the speech, biometrics and applied cryptography communities, this survey article provides an introduction to the field, starting with a legal perspective on privacy preservation in the case of speech data. It then establishes the requirements for effective privacy preservation, reviews generic cryptography-based solutions, followed by specific techniques that are applicable to speaker characterisation (biometric applications) and speech characterisation (non-biometric applications). Glancing at non-biometrics, methods are presented to avoid function creep, preventing the exploitation of biometric information, e.g., to single out an identity in speech-assisted health care via speaker characterisation. In promoting harmonised research, the article also outlines common, empirical evaluation metrics for the assessment of privacy-preserving technologies for speech data.",
      "abstract": "Speech recordings are a rich source of personal, sensitive data that can be used to support a plethora of diverse applications, from health profiling to biometric recognition. It is therefore essential that speech recordings are adequately protected so that they cannot be misused. Such protection, in the form of privacy-preserving technologies, is required to ensure that: (i) the biometric profiles of a given individual (e.g., across different biometric service operators) are unlinkable; (ii) leaked, encrypted biometric information is irreversible, and that (iii) biometric references are renewable. Whereas many privacy-preserving technologies have been developed for other biometric characteristics, very few solutions have been proposed to protect privacy in the case of speech signals. Despite privacy preservation this is now being mandated by recent European and international data protection regulations. With the aim of fostering progress and collaboration between researchers in the speech, biometrics and applied cryptography communities, this survey article provides an introduction to the field, starting with a legal perspective on privacy preservation in the case of speech data. It then establishes the requirements for effective privacy preservation, reviews generic cryptography-based solutions, followed by specific techniques that are applicable to speaker characterisation (biometric applications) and speech characterisation (non-biometric applications). Glancing at non-biometrics, methods are presented to avoid function creep, preventing the exploitation of biometric information, e.g., to single out an identity in speech-assisted health care via speaker characterisation. In promoting harmonised research, the article also outlines common, empirical evaluation metrics for the assessment of privacy-preserving technologies for speech data.",
      "doi": "https://doi.org/10.1016/j.csl.2019.06.001",
      "openalex_id": "https://openalex.org/W2951082691",
      "arxiv_id": "",
      "publication_date": "2019-06-07",
      "published": "2019-06-07",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "此論文是一篇關於語音資料的隱私保護與加密技術的綜述，重點在於資料保護與法規合規，並非以 Spoken Language Models 或等價語音語言建模的架構、訓練與評估為核心，因此不符合 inclusion 條件且屬於排除情境。",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "此論文是一篇關於語音資料的隱私保護與加密技術的綜述，重點在於資料保護與法規合規，並非以 Spoken Language Models 或等價語音語言建模的架構、訓練與評估為核心，因此不符合 inclusion 條件且屬於排除情境。",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The article focuses on privacy preservation in speech recordings, encompassing biometric and non-biometric applications, but does not address Spoken Language Models (SLMs) or Full-Duplex Spoken Language Models (FD-SLMs), nor does it focus on language modeling or interactive synchronous speech capabilities as required by the inclusion criteria. The study is more about privacy and cryptography in speech data rather than speech language modeling, synchronization, or interactive behaviors. Hence, it does not meet the inclusion criteria and falls into areas excluded by the criteria that require core research on spoken language models and their synchronous interactive behaviors.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The article focuses on privacy preservation in speech recordings, encompassing biometric and non-biometric applications, but does not address Spoken Language Models (SLMs) or Full-Duplex Spoken Language Models (FD-SLMs), nor does it focus on language modeling or interactive synchronous speech capabilities as required by the inclusion criteria. The study is more about privacy and cryptography in speech data rather than speech language modeling, synchronization, or interactive behaviors. Hence, it does not meet the inclusion criteria and falls into areas excluded by the criteria that require core research on spoken language models and their synchronous interactive behaviors.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "textless-lib: a Library for Textless Spoken Language Processing",
    "abstract": "Eugene Kharitonov, Jade Copet, Kushal Lakhotia, Tu Anh Nguyen, Paden Tomasello, Ann Lee, Ali Elkahky, Wei-Ning Hsu, Abdelrahman Mohamed, Emmanuel Dupoux, Yossi Adi. Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies: System Demonstrations. 2022.",
    "metadata": {
      "title": "textless-lib: a Library for Textless Spoken Language Processing",
      "summary": "Eugene Kharitonov, Jade Copet, Kushal Lakhotia, Tu Anh Nguyen, Paden Tomasello, Ann Lee, Ali Elkahky, Wei-Ning Hsu, Abdelrahman Mohamed, Emmanuel Dupoux, Yossi Adi. Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies: System Demonstrations. 2022.",
      "abstract": "Eugene Kharitonov, Jade Copet, Kushal Lakhotia, Tu Anh Nguyen, Paden Tomasello, Ann Lee, Ali Elkahky, Wei-Ning Hsu, Abdelrahman Mohamed, Emmanuel Dupoux, Yossi Adi. Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies: System Demonstrations. 2022.",
      "doi": "https://doi.org/10.18653/v1/2022.naacl-demo.1",
      "openalex_id": "https://openalex.org/W4287887366",
      "arxiv_id": "",
      "publication_date": "2022-01-01",
      "published": "2022-01-01",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The title explicitly targets textless spoken language processing, which centers speech as the core object—consistent with Spoken Language Models / FD-SLMs—and the work appears within the 3-year window, suggesting modeling/architecture and evaluation aspects rather than only ASR or text post-processing.",
      "evaluation": 4
    },
    "round-A_JuniorNano_reasoning": "The title explicitly targets textless spoken language processing, which centers speech as the core object—consistent with Spoken Language Models / FD-SLMs—and the work appears within the 3-year window, suggesting modeling/architecture and evaluation aspects rather than only ASR or text post-processing.",
    "round-A_JuniorNano_evaluation": 4,
    "round-A_JuniorMini_output": {
      "reasoning": "The title and abstract indicate a system demonstration paper about a textless library for spoken language processing, focusing on spoken language as the modeling object rather than text; however, the abstract does not explicitly mention core terminology like 'spoken language models' or detailed discussion on model architecture, training, or evaluation aspects related to full-duplex interaction, cognitive parallelism, or synchronization, nor does it clarify evaluation metrics beyond general system demonstration. Given the lack of explicit evidence meeting all inclusion criteria and no clear exclusion criteria triggered, the evaluation is uncertain.",
      "evaluation": 3
    },
    "round-A_JuniorMini_reasoning": "The title and abstract indicate a system demonstration paper about a textless library for spoken language processing, focusing on spoken language as the modeling object rather than text; however, the abstract does not explicitly mention core terminology like 'spoken language models' or detailed discussion on model architecture, training, or evaluation aspects related to full-duplex interaction, cognitive parallelism, or synchronization, nor does it clarify evaluation metrics beyond general system demonstration. Given the lack of explicit evidence meeting all inclusion criteria and no clear exclusion criteria triggered, the evaluation is uncertain.",
    "round-A_JuniorMini_evaluation": 3,
    "round-B_SeniorLead_output": {
      "reasoning": "I reviewed the two junior reviews (one scored 4 noting the title targets textless spoken language processing and likely aligns with SLM/FD-SLM interests; the other scored 3 citing lack of explicit mention of SLMs, architectures, training, or evaluations). The title “textless-lib: a Library for Textless Spoken Language Processing” suggests relevance to speech-centric processing but the provided citation/abstract lacks explicit mention of “spoken language model(s)” or clear details on model architectures, training, evaluation, or full-duplex interaction, and the paper is a 2022 system demonstration (potentially outside the stated 3-year window), so inclusion is uncertain. Score explanation: uncertain inclusion because the title implies relevance but there is insufficient explicit evidence in the abstract/citation to confirm it meets all inclusion criteria (explicit SLM focus, model/train/eval details, and recency).",
      "evaluation": 3
    },
    "round-B_SeniorLead_reasoning": "I reviewed the two junior reviews (one scored 4 noting the title targets textless spoken language processing and likely aligns with SLM/FD-SLM interests; the other scored 3 citing lack of explicit mention of SLMs, architectures, training, or evaluations). The title “textless-lib: a Library for Textless Spoken Language Processing” suggests relevance to speech-centric processing but the provided citation/abstract lacks explicit mention of “spoken language model(s)” or clear details on model architectures, training, evaluation, or full-duplex interaction, and the paper is a 2022 system demonstration (potentially outside the stated 3-year window), so inclusion is uncertain. Score explanation: uncertain inclusion because the title implies relevance but there is insufficient explicit evidence in the abstract/citation to confirm it meets all inclusion criteria (explicit SLM focus, model/train/eval details, and recency).",
    "round-B_SeniorLead_evaluation": 3,
    "final_verdict": "maybe (senior:3)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Speech SimCLR: Combining Contrastive and Reconstruction Objective for Self-Supervised Speech Representation Learning",
    "abstract": "Self-supervised visual pretraining has shown significant progress recently.Among those methods, SimCLR greatly advanced the state of the art in self-supervised and semisupervised learning on ImageNet.The input feature representations for speech and visual tasks are both continuous, so it is natural to consider applying similar objective on speech representation learning.In this paper, we propose Speech SimCLR, a new self-supervised objective for speech representation learning.During training, Speech SimCLR applies augmentation on raw speech and its spectrogram.Its objective is the combination of contrastive loss that maximizes agreement between differently augmented samples in the latent space and reconstruction loss of input representation.The proposed method achieved competitive results on speech emotion recognition and speech recognition.",
    "metadata": {
      "title": "Speech SimCLR: Combining Contrastive and Reconstruction Objective for Self-Supervised Speech Representation Learning",
      "summary": "Self-supervised visual pretraining has shown significant progress recently.Among those methods, SimCLR greatly advanced the state of the art in self-supervised and semisupervised learning on ImageNet.The input feature representations for speech and visual tasks are both continuous, so it is natural to consider applying similar objective on speech representation learning.In this paper, we propose Speech SimCLR, a new self-supervised objective for speech representation learning.During training, Speech SimCLR applies augmentation on raw speech and its spectrogram.Its objective is the combination of contrastive loss that maximizes agreement between differently augmented samples in the latent space and reconstruction loss of input representation.The proposed method achieved competitive results on speech emotion recognition and speech recognition.",
      "abstract": "Self-supervised visual pretraining has shown significant progress recently.Among those methods, SimCLR greatly advanced the state of the art in self-supervised and semisupervised learning on ImageNet.The input feature representations for speech and visual tasks are both continuous, so it is natural to consider applying similar objective on speech representation learning.In this paper, we propose Speech SimCLR, a new self-supervised objective for speech representation learning.During training, Speech SimCLR applies augmentation on raw speech and its spectrogram.Its objective is the combination of contrastive loss that maximizes agreement between differently augmented samples in the latent space and reconstruction loss of input representation.The proposed method achieved competitive results on speech emotion recognition and speech recognition.",
      "doi": "https://doi.org/10.21437/interspeech.2021-391",
      "openalex_id": "https://openalex.org/W3198608154",
      "arxiv_id": "",
      "publication_date": "2021-08-27",
      "published": "2021-08-27",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The paper centers on self-supervised speech representation learning rather than explicitly on Spoken Language Models or full-duplex speech/dialogue capabilities, and it does not mention SLMs or speech-language modeling terms in the title/abstract, thus it does not meet the inclusion criteria and should be excluded.",
      "evaluation": 2
    },
    "round-A_JuniorNano_reasoning": "The paper centers on self-supervised speech representation learning rather than explicitly on Spoken Language Models or full-duplex speech/dialogue capabilities, and it does not mention SLMs or speech-language modeling terms in the title/abstract, thus it does not meet the inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 2,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on self-supervised speech representation learning through contrastive and reconstruction objectives but does not explicitly discuss Spoken Language Models (SLMs) or Full-Duplex Spoken Language Models (FD-SLMs), nor does it address model architectures or synchronization for interactive turn-taking, cognitive parallelism, or low-latency speech interactions, and primarily evaluates on speech emotion recognition and speech recognition without addressing synchronous or interactive dialogue behaviors; hence, it fails to meet the inclusion criteria focused on SLMs and interactive full-duplex speech modeling.",
      "evaluation": 2
    },
    "round-A_JuniorMini_reasoning": "The study focuses on self-supervised speech representation learning through contrastive and reconstruction objectives but does not explicitly discuss Spoken Language Models (SLMs) or Full-Duplex Spoken Language Models (FD-SLMs), nor does it address model architectures or synchronization for interactive turn-taking, cognitive parallelism, or low-latency speech interactions, and primarily evaluates on speech emotion recognition and speech recognition without addressing synchronous or interactive dialogue behaviors; hence, it fails to meet the inclusion criteria focused on SLMs and interactive full-duplex speech modeling.",
    "round-A_JuniorMini_evaluation": 2,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:2)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Generative Pre-Training for Speech with Autoregressive Predictive Coding",
    "abstract": "Learning meaningful and general representations from unannotated speech that are applicable to a wide range of tasks remains challenging. In this paper we propose to use autoregressive predictive coding (APC), a recently proposed self-supervised objective, as a generative pre-training approach for learning meaningful, non-specific, and transferable speech representations. We pre-train APC on large-scale unlabeled data and conduct transfer learning experiments on three speech applications that require different information about speech characteristics to perform well: speech recognition, speech translation, and speaker identification. Extensive experiments show that APC not only outperforms surface features (e.g., log Mel spectrograms) and other popular representation learning methods on all three tasks, but is also effective at reducing downstream labeled data size and model parameters. We also investigate the use of Transformers for modeling APC and find it superior to RNNs.",
    "metadata": {
      "title": "Generative Pre-Training for Speech with Autoregressive Predictive Coding",
      "summary": "Learning meaningful and general representations from unannotated speech that are applicable to a wide range of tasks remains challenging. In this paper we propose to use autoregressive predictive coding (APC), a recently proposed self-supervised objective, as a generative pre-training approach for learning meaningful, non-specific, and transferable speech representations. We pre-train APC on large-scale unlabeled data and conduct transfer learning experiments on three speech applications that require different information about speech characteristics to perform well: speech recognition, speech translation, and speaker identification. Extensive experiments show that APC not only outperforms surface features (e.g., log Mel spectrograms) and other popular representation learning methods on all three tasks, but is also effective at reducing downstream labeled data size and model parameters. We also investigate the use of Transformers for modeling APC and find it superior to RNNs.",
      "abstract": "Learning meaningful and general representations from unannotated speech that are applicable to a wide range of tasks remains challenging. In this paper we propose to use autoregressive predictive coding (APC), a recently proposed self-supervised objective, as a generative pre-training approach for learning meaningful, non-specific, and transferable speech representations. We pre-train APC on large-scale unlabeled data and conduct transfer learning experiments on three speech applications that require different information about speech characteristics to perform well: speech recognition, speech translation, and speaker identification. Extensive experiments show that APC not only outperforms surface features (e.g., log Mel spectrograms) and other popular representation learning methods on all three tasks, but is also effective at reducing downstream labeled data size and model parameters. We also investigate the use of Transformers for modeling APC and find it superior to RNNs.",
      "doi": "https://doi.org/10.1109/icassp40776.2020.9054438",
      "openalex_id": "https://openalex.org/W3016011332",
      "arxiv_id": "",
      "publication_date": "2020-04-09",
      "published": "2020-04-09",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "此論文未在標題、摘要或關鍵詞中出現 spoken language model(s)、speech language model(s) 或等價描述，因此未滿足「主題定義」中的核心概念要求，儘管是語音自監督表示學習並涵蓋多個下游任務，仍不符合該研究主題的嚴格納入條件；因此應排除。",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "此論文未在標題、摘要或關鍵詞中出現 spoken language model(s)、speech language model(s) 或等價描述，因此未滿足「主題定義」中的核心概念要求，儘管是語音自監督表示學習並涵蓋多個下游任務，仍不符合該研究主題的嚴格納入條件；因此應排除。",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on learning speech representations via autoregressive predictive coding and evaluates these representations on speech recognition, speech translation, and speaker identification, but it does not explicitly mention spoken language models, speech language modeling, or full-duplex spoken language interactive capabilities, nor does it address synchronous interactive dialogue behaviors or cognitive parallelism. It mainly centers on representation learning and transfer learning for multiple speech tasks without emphasizing the core research theme of spoken language modeling or full-duplex interaction as stipulated in the inclusion criteria. Therefore, it does not sufficiently meet the inclusion definitions and partially aligns with excluded topics such as focusing on single downstream tasks (ASR, for instance) without covering dialogue interaction or behavioral arbitration.",
      "evaluation": 2
    },
    "round-A_JuniorMini_reasoning": "The study focuses on learning speech representations via autoregressive predictive coding and evaluates these representations on speech recognition, speech translation, and speaker identification, but it does not explicitly mention spoken language models, speech language modeling, or full-duplex spoken language interactive capabilities, nor does it address synchronous interactive dialogue behaviors or cognitive parallelism. It mainly centers on representation learning and transfer learning for multiple speech tasks without emphasizing the core research theme of spoken language modeling or full-duplex interaction as stipulated in the inclusion criteria. Therefore, it does not sufficiently meet the inclusion definitions and partially aligns with excluded topics such as focusing on single downstream tasks (ASR, for instance) without covering dialogue interaction or behavioral arbitration.",
    "round-A_JuniorMini_evaluation": 2,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:2)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Deep Contextualized Acoustic Representations for Semi-Supervised Speech Recognition",
    "abstract": "We propose a novel approach to semi-supervised automatic speech recognition (ASR). We first exploit a large amount of unlabeled audio data via representation learning, where we reconstruct a temporal slice of filterbank features from past and future context frames. The resulting deep contextualized acoustic representations (DeCoAR) are then used to train a CTC-based end-to-end ASR system using a smaller amount of labeled audio data. In our experiments, we show that systems trained on DeCoAR consistently outperform ones trained on conventional filterbank features, giving 42% and 19% relative improvement over the baseline on WSJ eval92 and LibriSpeech test-clean, respectively. Our approach can drastically reduce the amount of labeled data required; unsupervised training on LibriSpeech then supervision with 100 hours of labeled data achieves performance on par with training on all 960 hours directly. Pre-trained models and code will be released online.",
    "metadata": {
      "title": "Deep Contextualized Acoustic Representations for Semi-Supervised Speech Recognition",
      "summary": "We propose a novel approach to semi-supervised automatic speech recognition (ASR). We first exploit a large amount of unlabeled audio data via representation learning, where we reconstruct a temporal slice of filterbank features from past and future context frames. The resulting deep contextualized acoustic representations (DeCoAR) are then used to train a CTC-based end-to-end ASR system using a smaller amount of labeled audio data. In our experiments, we show that systems trained on DeCoAR consistently outperform ones trained on conventional filterbank features, giving 42% and 19% relative improvement over the baseline on WSJ eval92 and LibriSpeech test-clean, respectively. Our approach can drastically reduce the amount of labeled data required; unsupervised training on LibriSpeech then supervision with 100 hours of labeled data achieves performance on par with training on all 960 hours directly. Pre-trained models and code will be released online.",
      "abstract": "We propose a novel approach to semi-supervised automatic speech recognition (ASR). We first exploit a large amount of unlabeled audio data via representation learning, where we reconstruct a temporal slice of filterbank features from past and future context frames. The resulting deep contextualized acoustic representations (DeCoAR) are then used to train a CTC-based end-to-end ASR system using a smaller amount of labeled audio data. In our experiments, we show that systems trained on DeCoAR consistently outperform ones trained on conventional filterbank features, giving 42% and 19% relative improvement over the baseline on WSJ eval92 and LibriSpeech test-clean, respectively. Our approach can drastically reduce the amount of labeled data required; unsupervised training on LibriSpeech then supervision with 100 hours of labeled data achieves performance on par with training on all 960 hours directly. Pre-trained models and code will be released online.",
      "doi": "https://doi.org/10.1109/icassp40776.2020.9053176",
      "openalex_id": "https://openalex.org/W3015265920",
      "arxiv_id": "",
      "publication_date": "2020-04-09",
      "published": "2020-04-09",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "Although it deals with speech recognition and learned acoustic representations, it does not center on Spoken Language Models or a speech-language modeling framework, and its primary focus is ASR feature learning (with uncertain timeframe), so it likely does not meet the inclusion criteria.",
      "evaluation": 2
    },
    "round-A_JuniorNano_reasoning": "Although it deals with speech recognition and learned acoustic representations, it does not center on Spoken Language Models or a speech-language modeling framework, and its primary focus is ASR feature learning (with uncertain timeframe), so it likely does not meet the inclusion criteria.",
    "round-A_JuniorNano_evaluation": 2,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on semi-supervised ASR using deep contextualized acoustic representations but does not explicitly involve Spoken Language Models (SLMs) or Full-Duplex Spoken Language Models (FD-SLMs), nor does it discuss end-to-end modeling of speech as a core object or synchronous multi-turn interaction capabilities; it is primarily about improving ASR performance with representation learning, which aligns with traditional ASR tasks and evaluation metrics like WER without addressing dialogue interaction, synchronization, or behavioral arbitration as specified in the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on semi-supervised ASR using deep contextualized acoustic representations but does not explicitly involve Spoken Language Models (SLMs) or Full-Duplex Spoken Language Models (FD-SLMs), nor does it discuss end-to-end modeling of speech as a core object or synchronous multi-turn interaction capabilities; it is primarily about improving ASR performance with representation learning, which aligns with traditional ASR tasks and evaluation metrics like WER without addressing dialogue interaction, synchronization, or behavioral arbitration as specified in the inclusion criteria.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:2)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "TERA: Self-Supervised Learning of Transformer Encoder Representation for Speech",
    "abstract": "We introduce a self-supervised speech pre-training method called TERA, which\\nstands for Transformer Encoder Representations from Alteration. Recent\\napproaches often learn by using a single auxiliary task like contrastive\\nprediction, autoregressive prediction, or masked reconstruction. Unlike\\nprevious methods, we use alteration along three orthogonal axes to pre-train\\nTransformer Encoders on a large amount of unlabeled speech. The model learns\\nthrough the reconstruction of acoustic frames from their altered counterpart,\\nwhere we use a stochastic policy to alter along various dimensions: time,\\nfrequency, and magnitude. TERA can be used for speech representations\\nextraction or fine-tuning with downstream models. We evaluate TERA on several\\ndownstream tasks, including phoneme classification, keyword spotting, speaker\\nrecognition, and speech recognition. We present a large-scale comparison of\\nvarious self-supervised models. TERA achieves strong performance in the\\ncomparison by improving upon surface features and outperforming previous\\nmodels. In our experiments, we study the effect of applying different\\nalteration techniques, pre-training on more data, and pre-training on various\\nfeatures. We analyze different model sizes and find that smaller models are\\nstrong representation learners than larger models, while larger models are more\\neffective for downstream fine-tuning than smaller models. Furthermore, we show\\nthe proposed method is transferable to downstream datasets not used in\\npre-training.\\n",
    "metadata": {
      "title": "TERA: Self-Supervised Learning of Transformer Encoder Representation for Speech",
      "summary": "We introduce a self-supervised speech pre-training method called TERA, which\\nstands for Transformer Encoder Representations from Alteration. Recent\\napproaches often learn by using a single auxiliary task like contrastive\\nprediction, autoregressive prediction, or masked reconstruction. Unlike\\nprevious methods, we use alteration along three orthogonal axes to pre-train\\nTransformer Encoders on a large amount of unlabeled speech. The model learns\\nthrough the reconstruction of acoustic frames from their altered counterpart,\\nwhere we use a stochastic policy to alter along various dimensions: time,\\nfrequency, and magnitude. TERA can be used for speech representations\\nextraction or fine-tuning with downstream models. We evaluate TERA on several\\ndownstream tasks, including phoneme classification, keyword spotting, speaker\\nrecognition, and speech recognition. We present a large-scale comparison of\\nvarious self-supervised models. TERA achieves strong performance in the\\ncomparison by improving upon surface features and outperforming previous\\nmodels. In our experiments, we study the effect of applying different\\nalteration techniques, pre-training on more data, and pre-training on various\\nfeatures. We analyze different model sizes and find that smaller models are\\nstrong representation learners than larger models, while larger models are more\\neffective for downstream fine-tuning than smaller models. Furthermore, we show\\nthe proposed method is transferable to downstream datasets not used in\\npre-training.\\n",
      "abstract": "We introduce a self-supervised speech pre-training method called TERA, which\\nstands for Transformer Encoder Representations from Alteration. Recent\\napproaches often learn by using a single auxiliary task like contrastive\\nprediction, autoregressive prediction, or masked reconstruction. Unlike\\nprevious methods, we use alteration along three orthogonal axes to pre-train\\nTransformer Encoders on a large amount of unlabeled speech. The model learns\\nthrough the reconstruction of acoustic frames from their altered counterpart,\\nwhere we use a stochastic policy to alter along various dimensions: time,\\nfrequency, and magnitude. TERA can be used for speech representations\\nextraction or fine-tuning with downstream models. We evaluate TERA on several\\ndownstream tasks, including phoneme classification, keyword spotting, speaker\\nrecognition, and speech recognition. We present a large-scale comparison of\\nvarious self-supervised models. TERA achieves strong performance in the\\ncomparison by improving upon surface features and outperforming previous\\nmodels. In our experiments, we study the effect of applying different\\nalteration techniques, pre-training on more data, and pre-training on various\\nfeatures. We analyze different model sizes and find that smaller models are\\nstrong representation learners than larger models, while larger models are more\\neffective for downstream fine-tuning than smaller models. Furthermore, we show\\nthe proposed method is transferable to downstream datasets not used in\\npre-training.\\n",
      "doi": "https://doi.org/10.1109/taslp.2021.3095662",
      "openalex_id": "https://openalex.org/W3041561163",
      "arxiv_id": "",
      "publication_date": "2021-01-01",
      "published": "2021-01-01",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The work clearly targets speech representation learning and downstream speech tasks (consistent with Speech/SLM goals), but the abstract does not explicitly mention 'spoken language model(s)' or a direct equivalent phrase, so it is uncertain whether it meets the explicit inclusion keyword criterion.",
      "evaluation": 3
    },
    "round-A_JuniorNano_reasoning": "The work clearly targets speech representation learning and downstream speech tasks (consistent with Speech/SLM goals), but the abstract does not explicitly mention 'spoken language model(s)' or a direct equivalent phrase, so it is uncertain whether it meets the explicit inclusion keyword criterion.",
    "round-A_JuniorNano_evaluation": 3,
    "round-A_JuniorMini_output": {
      "reasoning": "The study presents a self-supervised learning method for speech representation using Transformer Encoders on acoustic frames, focusing on speech sequence modeling; however, the abstract does not mention spoken language models, full-duplex interaction, synchronization, or dialogue behavior arbitration, which are essential inclusion criteria. Additionally, it evaluates on standard speech tasks (phoneme classification, keyword spotting, etc.) without addressing multi-turn or full-duplex spoken language modeling or interactive dialogue capabilities, which suggests it falls short of the thematic focus required.",
      "evaluation": 2
    },
    "round-A_JuniorMini_reasoning": "The study presents a self-supervised learning method for speech representation using Transformer Encoders on acoustic frames, focusing on speech sequence modeling; however, the abstract does not mention spoken language models, full-duplex interaction, synchronization, or dialogue behavior arbitration, which are essential inclusion criteria. Additionally, it evaluates on standard speech tasks (phoneme classification, keyword spotting, etc.) without addressing multi-turn or full-duplex spoken language modeling or interactive dialogue capabilities, which suggests it falls short of the thematic focus required.",
    "round-A_JuniorMini_evaluation": 2,
    "round-B_SeniorLead_output": {
      "reasoning": "Both junior reviewers correctly note this paper focuses on self-supervised speech representation learning (pre-training a Transformer encoder with altered acoustic frames) and evaluates on phoneme classification, keyword spotting, speaker recognition, and ASR. However, the abstract does not mention Spoken Language Models, speech-level language modeling, Speech–LLM or any full‑duplex/interactive synchronization or behavior-arbitration aspects required by the inclusion criteria; instead it targets general speech feature learning and downstream ASR-style tasks, which fall squarely under the exclusion criteria. Therefore it should be excluded. One-sentence explanation: The paper is about self-supervised speech representations and downstream ASR-style evaluations, not about Spoken Language Models or full‑duplex interactive speech modeling required for inclusion.",
      "evaluation": 1
    },
    "round-B_SeniorLead_reasoning": "Both junior reviewers correctly note this paper focuses on self-supervised speech representation learning (pre-training a Transformer encoder with altered acoustic frames) and evaluates on phoneme classification, keyword spotting, speaker recognition, and ASR. However, the abstract does not mention Spoken Language Models, speech-level language modeling, Speech–LLM or any full‑duplex/interactive synchronization or behavior-arbitration aspects required by the inclusion criteria; instead it targets general speech feature learning and downstream ASR-style tasks, which fall squarely under the exclusion criteria. Therefore it should be excluded. One-sentence explanation: The paper is about self-supervised speech representations and downstream ASR-style evaluations, not about Spoken Language Models or full‑duplex interactive speech modeling required for inclusion.",
    "round-B_SeniorLead_evaluation": 1,
    "final_verdict": "exclude (senior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Exploring Speech Recognition, Translation, and Understanding with Discrete Speech Units: A Comparative Study",
    "abstract": "Speech signals, typically sampled at rates in the tens of thousands per second, contain redundancies, evoking inefficiencies in sequence modeling. High-dimensional speech features such as spectrograms are often used as the input for the subsequent model. However, they can still be redundant. Recent investigations proposed the use of discrete speech units derived from self-supervised learning representations, which significantly compresses the size of speech data. Applying various methods, such as de-duplication and subword modeling, can further compress the speech sequence length. Hence, training time is significantly reduced while retaining notable performance. In this study, we undertake a comprehensive and systematic exploration into the application of discrete units within end-to-end speech processing models. Experiments on 12 automatic speech recognition, 3 speech translation, and 1 spoken language understanding corpora demonstrate that discrete units achieve reasonably good results in almost all the settings. Our configurations and trained models are released in ESPnet to foster future research efforts.",
    "metadata": {
      "title": "Exploring Speech Recognition, Translation, and Understanding with Discrete Speech Units: A Comparative Study",
      "summary": "Speech signals, typically sampled at rates in the tens of thousands per second, contain redundancies, evoking inefficiencies in sequence modeling. High-dimensional speech features such as spectrograms are often used as the input for the subsequent model. However, they can still be redundant. Recent investigations proposed the use of discrete speech units derived from self-supervised learning representations, which significantly compresses the size of speech data. Applying various methods, such as de-duplication and subword modeling, can further compress the speech sequence length. Hence, training time is significantly reduced while retaining notable performance. In this study, we undertake a comprehensive and systematic exploration into the application of discrete units within end-to-end speech processing models. Experiments on 12 automatic speech recognition, 3 speech translation, and 1 spoken language understanding corpora demonstrate that discrete units achieve reasonably good results in almost all the settings. Our configurations and trained models are released in ESPnet to foster future research efforts.",
      "abstract": "Speech signals, typically sampled at rates in the tens of thousands per second, contain redundancies, evoking inefficiencies in sequence modeling. High-dimensional speech features such as spectrograms are often used as the input for the subsequent model. However, they can still be redundant. Recent investigations proposed the use of discrete speech units derived from self-supervised learning representations, which significantly compresses the size of speech data. Applying various methods, such as de-duplication and subword modeling, can further compress the speech sequence length. Hence, training time is significantly reduced while retaining notable performance. In this study, we undertake a comprehensive and systematic exploration into the application of discrete units within end-to-end speech processing models. Experiments on 12 automatic speech recognition, 3 speech translation, and 1 spoken language understanding corpora demonstrate that discrete units achieve reasonably good results in almost all the settings. Our configurations and trained models are released in ESPnet to foster future research efforts.",
      "doi": "https://doi.org/10.1109/icassp48485.2024.10447929",
      "openalex_id": "https://openalex.org/W4392909068",
      "arxiv_id": "",
      "publication_date": "2024-03-18",
      "published": "2024-03-18",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The work studies end-to-end speech processing with discrete speech units across ASR/translation/SLU, but it does not explicitly present Spoken Language Models or equivalent as the core framework in the title/abstract/keywords, so it does not satisfy the primary inclusion criterion, though it aligns with some aspects.",
      "evaluation": 2
    },
    "round-A_JuniorNano_reasoning": "The work studies end-to-end speech processing with discrete speech units across ASR/translation/SLU, but it does not explicitly present Spoken Language Models or equivalent as the core framework in the title/abstract/keywords, so it does not satisfy the primary inclusion criterion, though it aligns with some aspects.",
    "round-A_JuniorNano_evaluation": 2,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on using discrete speech units for end-to-end speech processing models including speech recognition, translation, and understanding, but it does not explicitly mention 'spoken language models' or full-duplex capabilities, cognitive parallelism, or synchronous interaction features required by the inclusion criteria; it mainly targets ASR and related tasks without detailed discussion of dialogue behavior arbitration or low-latency real-time interaction, thus it doesn't fully meet the inclusion criteria nor the advanced interaction focus specified.",
      "evaluation": 2
    },
    "round-A_JuniorMini_reasoning": "The study focuses on using discrete speech units for end-to-end speech processing models including speech recognition, translation, and understanding, but it does not explicitly mention 'spoken language models' or full-duplex capabilities, cognitive parallelism, or synchronous interaction features required by the inclusion criteria; it mainly targets ASR and related tasks without detailed discussion of dialogue behavior arbitration or low-latency real-time interaction, thus it doesn't fully meet the inclusion criteria nor the advanced interaction focus specified.",
    "round-A_JuniorMini_evaluation": 2,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:2)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Neural Model Reprogramming with Similarity Based Mapping for Low-Resource Spoken Command Recognition",
    "abstract": "We propose a novel adversarial reprogramming (AR) approach for low-resource spoken command recognition (SCR) and build an AR-SCR system. The AR procedure aims at repurposing a pretrained SCR model (from the source domain) to modify the acoustic signals (from the target domain). To solve the label mismatches between source and target domains and further improve the stability of AR, we propose a novel similarity-based label mapping technique to align classes. In addition, the transfer learning (TL) technique is combined with the original AR process to improve the model adaptation capability. We evaluate the proposed AR-SCR system on three low-resource SCR datasets, including Arabic, Lithuanian, and dysarthric Mandarin speech. Experimental results show that with a pretrained acoustic model trained on a large-scale English dataset, the proposed AR-SCR system outperforms the current state-of-the-art results on Lithuanian and Arabic datasets, with only a limited amount of training data.",
    "metadata": {
      "title": "Neural Model Reprogramming with Similarity Based Mapping for Low-Resource Spoken Command Recognition",
      "summary": "We propose a novel adversarial reprogramming (AR) approach for low-resource spoken command recognition (SCR) and build an AR-SCR system. The AR procedure aims at repurposing a pretrained SCR model (from the source domain) to modify the acoustic signals (from the target domain). To solve the label mismatches between source and target domains and further improve the stability of AR, we propose a novel similarity-based label mapping technique to align classes. In addition, the transfer learning (TL) technique is combined with the original AR process to improve the model adaptation capability. We evaluate the proposed AR-SCR system on three low-resource SCR datasets, including Arabic, Lithuanian, and dysarthric Mandarin speech. Experimental results show that with a pretrained acoustic model trained on a large-scale English dataset, the proposed AR-SCR system outperforms the current state-of-the-art results on Lithuanian and Arabic datasets, with only a limited amount of training data.",
      "abstract": "We propose a novel adversarial reprogramming (AR) approach for low-resource spoken command recognition (SCR) and build an AR-SCR system. The AR procedure aims at repurposing a pretrained SCR model (from the source domain) to modify the acoustic signals (from the target domain). To solve the label mismatches between source and target domains and further improve the stability of AR, we propose a novel similarity-based label mapping technique to align classes. In addition, the transfer learning (TL) technique is combined with the original AR process to improve the model adaptation capability. We evaluate the proposed AR-SCR system on three low-resource SCR datasets, including Arabic, Lithuanian, and dysarthric Mandarin speech. Experimental results show that with a pretrained acoustic model trained on a large-scale English dataset, the proposed AR-SCR system outperforms the current state-of-the-art results on Lithuanian and Arabic datasets, with only a limited amount of training data.",
      "doi": "https://doi.org/10.21437/interspeech.2023-1086",
      "openalex_id": "https://openalex.org/W4385823335",
      "arxiv_id": "",
      "publication_date": "2023-08-14",
      "published": "2023-08-14",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "未在標題/摘要中出現 spoken language model(s) 或等價描述，且核心聚焦於對語音識別的對抗式重新編程與跨域映射，未涵蓋語音語言模型架構或端到端語音語言建模，因此不符合核心主題（SLMs / FD-SLMs）與評估框架要求，應排除。",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "未在標題/摘要中出現 spoken language model(s) 或等價描述，且核心聚焦於對語音識別的對抗式重新編程與跨域映射，未涵蓋語音語言模型架構或端到端語音語言建模，因此不符合核心主題（SLMs / FD-SLMs）與評估框架要求，應排除。",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on adversarial reprogramming and transfer learning for low-resource spoken command recognition using pretrained acoustic models but does not explicitly address spoken language models or full-duplex spoken language models, nor does it emphasize end-to-end or hybrid speech-LLM architectures, cognitive parallelism, or multi-dimensional evaluation beyond recognition accuracy; it mainly targets a single downstream task (spoken command recognition) with traditional low-resource speech recognition methods and evaluates performance primarily by recognition accuracy, thus not meeting the inclusion criteria focused on speech language modeling and synchronous interaction capabilities.",
      "evaluation": 2
    },
    "round-A_JuniorMini_reasoning": "The study focuses on adversarial reprogramming and transfer learning for low-resource spoken command recognition using pretrained acoustic models but does not explicitly address spoken language models or full-duplex spoken language models, nor does it emphasize end-to-end or hybrid speech-LLM architectures, cognitive parallelism, or multi-dimensional evaluation beyond recognition accuracy; it mainly targets a single downstream task (spoken command recognition) with traditional low-resource speech recognition methods and evaluates performance primarily by recognition accuracy, thus not meeting the inclusion criteria focused on speech language modeling and synchronous interaction capabilities.",
    "round-A_JuniorMini_evaluation": 2,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:2)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Rethinking the Role of Demonstrations: What Makes In-Context Learning Work?",
    "abstract": "Large language models (LMs) are able to in-context learn—perform a new task via inference alone by conditioning on a few input-label pairs (demonstrations) and making predictions for new inputs. However, there has been little understanding of how the model learns and which aspects of the demonstrations contribute to end task performance. In this paper, we show that ground truth demonstrations are in fact not required—randomly replacing labels in the demonstrations barely hurts performance on a range of classification and multi-choce tasks, consistently over 12 different models including GPT-3. Instead, we find that other aspects of the demonstrations are the key drivers of endtask performance, including the fact that they provide a few examples of (1) the label space, (2) the distribution of the input text, and (3) the overall format of the sequence. Together, our analysis provides a new way of understanding how and why in-context learning works, while opening up new questions about how much can be learned from large language models through inference alone.",
    "metadata": {
      "title": "Rethinking the Role of Demonstrations: What Makes In-Context Learning Work?",
      "summary": "Large language models (LMs) are able to in-context learn—perform a new task via inference alone by conditioning on a few input-label pairs (demonstrations) and making predictions for new inputs. However, there has been little understanding of how the model learns and which aspects of the demonstrations contribute to end task performance. In this paper, we show that ground truth demonstrations are in fact not required—randomly replacing labels in the demonstrations barely hurts performance on a range of classification and multi-choce tasks, consistently over 12 different models including GPT-3. Instead, we find that other aspects of the demonstrations are the key drivers of endtask performance, including the fact that they provide a few examples of (1) the label space, (2) the distribution of the input text, and (3) the overall format of the sequence. Together, our analysis provides a new way of understanding how and why in-context learning works, while opening up new questions about how much can be learned from large language models through inference alone.",
      "abstract": "Large language models (LMs) are able to in-context learn—perform a new task via inference alone by conditioning on a few input-label pairs (demonstrations) and making predictions for new inputs. However, there has been little understanding of how the model learns and which aspects of the demonstrations contribute to end task performance. In this paper, we show that ground truth demonstrations are in fact not required—randomly replacing labels in the demonstrations barely hurts performance on a range of classification and multi-choce tasks, consistently over 12 different models including GPT-3. Instead, we find that other aspects of the demonstrations are the key drivers of endtask performance, including the fact that they provide a few examples of (1) the label space, (2) the distribution of the input text, and (3) the overall format of the sequence. Together, our analysis provides a new way of understanding how and why in-context learning works, while opening up new questions about how much can be learned from large language models through inference alone.",
      "doi": "https://doi.org/10.18653/v1/2022.emnlp-main.759",
      "openalex_id": "https://openalex.org/W4385567149",
      "arxiv_id": "",
      "publication_date": "2022-01-01",
      "published": "2022-01-01",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The abstract centers on in-context learning for general LLMs with demonstrations and does not address Spoken Language Models, speech-language modeling, or audio-centric synchronization/discourse issues, thus failing the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The abstract centers on in-context learning for general LLMs with demonstrations and does not address Spoken Language Models, speech-language modeling, or audio-centric synchronization/discourse issues, thus failing the inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on in-context learning in large language models, specifically discussing text input-label demonstration pairs and model prediction abilities; it does not address spoken language modeling, speech as a primary modeling object, full-duplex spoken language models, synchronization, or speech-based evaluation metrics, and thus does not meet the inclusion criteria related to speech-centric modeling and interaction.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on in-context learning in large language models, specifically discussing text input-label demonstration pairs and model prediction abilities; it does not address spoken language modeling, speech as a primary modeling object, full-duplex spoken language models, synchronization, or speech-based evaluation metrics, and thus does not meet the inclusion criteria related to speech-centric modeling and interaction.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Improving End-to-End Speech-to-Intent Classification with Reptile",
    "abstract": "End-to-end spoken language understanding (SLU) systems have many advantages over conventional pipeline systems, but collecting in-domain speech data to train an end-to-end system is costly and time consuming. One question arises from this: how to train an end-to-end SLU with limited amounts of data? Many researchers have explored approaches that make use of other related data resources, typically by pre-training parts of the model on high-resource speech recognition. In this paper, we suggest improving the generalization performance of SLU models with a non-standard learning algorithm, Reptile. Though Reptile was originally proposed for model-agnostic meta learning, we argue that it can also be used to directly learn a target task and result in better generalization than conventional gradient descent. In this work, we employ Reptile to the task of end-to-end spoken intent classification. Experiments on four datasets of different languages and domains show improvement of intent prediction accuracy, both when Reptile is used alone and used in addition to pre-training.",
    "metadata": {
      "title": "Improving End-to-End Speech-to-Intent Classification with Reptile",
      "summary": "End-to-end spoken language understanding (SLU) systems have many advantages over conventional pipeline systems, but collecting in-domain speech data to train an end-to-end system is costly and time consuming. One question arises from this: how to train an end-to-end SLU with limited amounts of data? Many researchers have explored approaches that make use of other related data resources, typically by pre-training parts of the model on high-resource speech recognition. In this paper, we suggest improving the generalization performance of SLU models with a non-standard learning algorithm, Reptile. Though Reptile was originally proposed for model-agnostic meta learning, we argue that it can also be used to directly learn a target task and result in better generalization than conventional gradient descent. In this work, we employ Reptile to the task of end-to-end spoken intent classification. Experiments on four datasets of different languages and domains show improvement of intent prediction accuracy, both when Reptile is used alone and used in addition to pre-training.",
      "abstract": "End-to-end spoken language understanding (SLU) systems have many advantages over conventional pipeline systems, but collecting in-domain speech data to train an end-to-end system is costly and time consuming. One question arises from this: how to train an end-to-end SLU with limited amounts of data? Many researchers have explored approaches that make use of other related data resources, typically by pre-training parts of the model on high-resource speech recognition. In this paper, we suggest improving the generalization performance of SLU models with a non-standard learning algorithm, Reptile. Though Reptile was originally proposed for model-agnostic meta learning, we argue that it can also be used to directly learn a target task and result in better generalization than conventional gradient descent. In this work, we employ Reptile to the task of end-to-end spoken intent classification. Experiments on four datasets of different languages and domains show improvement of intent prediction accuracy, both when Reptile is used alone and used in addition to pre-training.",
      "doi": "https://doi.org/10.21437/interspeech.2020-1160",
      "openalex_id": "https://openalex.org/W3096251052",
      "arxiv_id": "",
      "publication_date": "2020-10-25",
      "published": "2020-10-25",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item presents an end-to-end speech-to-intent SLU study with a focus on architecture/training/evaluation and uses speech as the primary signal, aligning with the SLM/SLU-literature; it is within the last 3 years and is academic. The abstract does not explicitly use the term 'spoken language model' but it clearly targets speech-based modeling and evaluation, not just ASR or single-turn NLP.",
      "evaluation": 4
    },
    "round-A_JuniorNano_reasoning": "The item presents an end-to-end speech-to-intent SLU study with a focus on architecture/training/evaluation and uses speech as the primary signal, aligning with the SLM/SLU-literature; it is within the last 3 years and is academic. The abstract does not explicitly use the term 'spoken language model' but it clearly targets speech-based modeling and evaluation, not just ASR or single-turn NLP.",
    "round-A_JuniorNano_evaluation": 4,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on end-to-end spoken language understanding with intent classification, discussing model training improvements using Reptile in the context of speech data. However, it does not explicitly mention spoken language models (SLMs) or speech language models, nor does it address full-duplex spoken language modeling, synchronous interaction, or multi-dimensional evaluation related to turn-taking or cognitive parallelism. The core task is intent classification, which may be considered a downstream task rather than general speech language modeling. Thus, the study only partially meets the thematic inclusion criteria and misses key aspects such as synchronous interaction or full-duplex modeling.",
      "evaluation": 2
    },
    "round-A_JuniorMini_reasoning": "The study focuses on end-to-end spoken language understanding with intent classification, discussing model training improvements using Reptile in the context of speech data. However, it does not explicitly mention spoken language models (SLMs) or speech language models, nor does it address full-duplex spoken language modeling, synchronous interaction, or multi-dimensional evaluation related to turn-taking or cognitive parallelism. The core task is intent classification, which may be considered a downstream task rather than general speech language modeling. Thus, the study only partially meets the thematic inclusion criteria and misses key aspects such as synchronous interaction or full-duplex modeling.",
    "round-A_JuniorMini_evaluation": 2,
    "round-B_SeniorLead_output": {
      "reasoning": "Reasoning: I reviewed both junior reviewers' notes — the paper addresses end-to-end spoken intent classification and training improvements with Reptile (so it operates on speech as input and discusses training/evaluation), but the abstract does not use the required explicit terminology (spoken language model / speech language model or equivalent), focuses on a single downstream task (intent classification) without discussing full‑duplex interaction, synchronization, overlap handling, or broader speech language modeling perspectives, and thus fails the stated inclusion requirements and falls under the exclusion criteria. Score: 2. One-sentence explanation: It works on speech and training methods but lacks the explicit SLM framing and FD‑SLM/interaction evaluations required by the inclusion criteria, so it should be excluded.",
      "evaluation": 2
    },
    "round-B_SeniorLead_reasoning": "Reasoning: I reviewed both junior reviewers' notes — the paper addresses end-to-end spoken intent classification and training improvements with Reptile (so it operates on speech as input and discusses training/evaluation), but the abstract does not use the required explicit terminology (spoken language model / speech language model or equivalent), focuses on a single downstream task (intent classification) without discussing full‑duplex interaction, synchronization, overlap handling, or broader speech language modeling perspectives, and thus fails the stated inclusion requirements and falls under the exclusion criteria. Score: 2. One-sentence explanation: It works on speech and training methods but lacks the explicit SLM framing and FD‑SLM/interaction evaluations required by the inclusion criteria, so it should be excluded.",
    "round-B_SeniorLead_evaluation": 2,
    "final_verdict": "exclude (senior:2)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Unsupervised Pre-Training for Voice Activation",
    "abstract": "The problem of voice activation is to find a pre-defined word in the audio stream. Solutions such as keyword spotter “Ok, Google” for Android devices or keyword spotter “Alexa” for Amazon devices use tens of thousands to millions of keyword examples in training. In this paper, we explore the possibility of using pre-trained audio features to build voice activation with a small number of keyword examples. The contribution of this article consists of two parts. First, we investigate the dependence of the quality of the voice activation system on the number of examples in training for English and Russian and show that the use of pre-trained audio features, such as wav2vec, increases the accuracy of the system by up to 10% if only seven examples are available for each keyword during training. At the same time, the benefits of such features become less and disappear as the dataset size increases. Secondly, we prepare and provide for general use a dataset for training and testing voice activation for the Lithuanian language. We also provide training results on this dataset.",
    "metadata": {
      "title": "Unsupervised Pre-Training for Voice Activation",
      "summary": "The problem of voice activation is to find a pre-defined word in the audio stream. Solutions such as keyword spotter “Ok, Google” for Android devices or keyword spotter “Alexa” for Amazon devices use tens of thousands to millions of keyword examples in training. In this paper, we explore the possibility of using pre-trained audio features to build voice activation with a small number of keyword examples. The contribution of this article consists of two parts. First, we investigate the dependence of the quality of the voice activation system on the number of examples in training for English and Russian and show that the use of pre-trained audio features, such as wav2vec, increases the accuracy of the system by up to 10% if only seven examples are available for each keyword during training. At the same time, the benefits of such features become less and disappear as the dataset size increases. Secondly, we prepare and provide for general use a dataset for training and testing voice activation for the Lithuanian language. We also provide training results on this dataset.",
      "abstract": "The problem of voice activation is to find a pre-defined word in the audio stream. Solutions such as keyword spotter “Ok, Google” for Android devices or keyword spotter “Alexa” for Amazon devices use tens of thousands to millions of keyword examples in training. In this paper, we explore the possibility of using pre-trained audio features to build voice activation with a small number of keyword examples. The contribution of this article consists of two parts. First, we investigate the dependence of the quality of the voice activation system on the number of examples in training for English and Russian and show that the use of pre-trained audio features, such as wav2vec, increases the accuracy of the system by up to 10% if only seven examples are available for each keyword during training. At the same time, the benefits of such features become less and disappear as the dataset size increases. Secondly, we prepare and provide for general use a dataset for training and testing voice activation for the Lithuanian language. We also provide training results on this dataset.",
      "doi": "https://doi.org/10.3390/app10238643",
      "openalex_id": "https://openalex.org/W3108231750",
      "arxiv_id": "",
      "publication_date": "2020-12-03",
      "published": "2020-12-03",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The paper focuses on unsupervised pre-training for a keyword spotting/voice activation task, evaluating pre-trained audio features (e.g., wav2vec) for a small set of examples; it centers on a single-task ASR-style objective rather than Spoken Language Models or full-duplex, interactive speech-language modeling, and lacks discussion of turn-taking, real-time synchronization, or broader speech-language modeling architectures, thus not meeting the inclusion criteria and failing the exclusion criteria that disallow single-task or ASR-centric work.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The paper focuses on unsupervised pre-training for a keyword spotting/voice activation task, evaluating pre-trained audio features (e.g., wav2vec) for a small set of examples; it centers on a single-task ASR-style objective rather than Spoken Language Models or full-duplex, interactive speech-language modeling, and lacks discussion of turn-taking, real-time synchronization, or broader speech-language modeling architectures, thus not meeting the inclusion criteria and failing the exclusion criteria that disallow single-task or ASR-centric work.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper focuses on voice activation using pre-trained audio features but does not explicitly discuss Spoken Language Models or Full-Duplex Spoken Language Models as defined in the inclusion criteria. It mainly addresses keyword spotting with a small dataset and does not mention model architecture, training, or evaluation related to end-to-end spoken language modeling, nor does it involve synchronous conversational interaction or behavioral arbitration. Therefore, it does not meet the core inclusion requirements or address multiple listed key concepts, and it focuses on a single downstream task without general language modeling discussion.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper focuses on voice activation using pre-trained audio features but does not explicitly discuss Spoken Language Models or Full-Duplex Spoken Language Models as defined in the inclusion criteria. It mainly addresses keyword spotting with a small dataset and does not mention model architecture, training, or evaluation related to end-to-end spoken language modeling, nor does it involve synchronous conversational interaction or behavioral arbitration. Therefore, it does not meet the core inclusion requirements or address multiple listed key concepts, and it focuses on a single downstream task without general language modeling discussion.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "A Speech Command Control-Based Recognition System for Dysarthric Patients Based on Deep Learning Technology",
    "abstract": "Voice control is an important way of controlling mobile devices; however, using it remains a challenge for dysarthric patients. Currently, there are many approaches, such as automatic speech recognition (ASR) systems, being used to help dysarthric patients control mobile devices. However, the large computation power requirement for the ASR system increases implementation costs. To alleviate this problem, this study proposed a convolution neural network (CNN) with a phonetic posteriorgram (PPG) speech feature system to recognize speech commands, called CNN–PPG; meanwhile, the CNN model with Mel-frequency cepstral coefficient (CNN–MFCC model) and ASR-based systems were used for comparison. The experiment results show that the CNN–PPG system provided 93.49% accuracy, better than the CNN–MFCC (65.67%) and ASR-based systems (89.59%). Additionally, the CNN–PPG used a smaller model size comprising only 54% parameter numbers compared with the ASR-based system; hence, the proposed system could reduce implementation costs for users. These findings suggest that the CNN–PPG system could augment a communication device to help dysarthric patients control the mobile device via speech commands in the future.",
    "metadata": {
      "title": "A Speech Command Control-Based Recognition System for Dysarthric Patients Based on Deep Learning Technology",
      "summary": "Voice control is an important way of controlling mobile devices; however, using it remains a challenge for dysarthric patients. Currently, there are many approaches, such as automatic speech recognition (ASR) systems, being used to help dysarthric patients control mobile devices. However, the large computation power requirement for the ASR system increases implementation costs. To alleviate this problem, this study proposed a convolution neural network (CNN) with a phonetic posteriorgram (PPG) speech feature system to recognize speech commands, called CNN–PPG; meanwhile, the CNN model with Mel-frequency cepstral coefficient (CNN–MFCC model) and ASR-based systems were used for comparison. The experiment results show that the CNN–PPG system provided 93.49% accuracy, better than the CNN–MFCC (65.67%) and ASR-based systems (89.59%). Additionally, the CNN–PPG used a smaller model size comprising only 54% parameter numbers compared with the ASR-based system; hence, the proposed system could reduce implementation costs for users. These findings suggest that the CNN–PPG system could augment a communication device to help dysarthric patients control the mobile device via speech commands in the future.",
      "abstract": "Voice control is an important way of controlling mobile devices; however, using it remains a challenge for dysarthric patients. Currently, there are many approaches, such as automatic speech recognition (ASR) systems, being used to help dysarthric patients control mobile devices. However, the large computation power requirement for the ASR system increases implementation costs. To alleviate this problem, this study proposed a convolution neural network (CNN) with a phonetic posteriorgram (PPG) speech feature system to recognize speech commands, called CNN–PPG; meanwhile, the CNN model with Mel-frequency cepstral coefficient (CNN–MFCC model) and ASR-based systems were used for comparison. The experiment results show that the CNN–PPG system provided 93.49% accuracy, better than the CNN–MFCC (65.67%) and ASR-based systems (89.59%). Additionally, the CNN–PPG used a smaller model size comprising only 54% parameter numbers compared with the ASR-based system; hence, the proposed system could reduce implementation costs for users. These findings suggest that the CNN–PPG system could augment a communication device to help dysarthric patients control the mobile device via speech commands in the future.",
      "doi": "https://doi.org/10.3390/app11062477",
      "openalex_id": "https://openalex.org/W3134187040",
      "arxiv_id": "",
      "publication_date": "2021-03-10",
      "published": "2021-03-10",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The study centers on CNN-PPG based speech command recognition for dysarthric users and compares ASR-based systems, with no emphasis on Spoken Language Models or speech-language modeling as core concepts; it does not discuss SLM/FD-SLM architectures or multi-turn interaction, thus does not meet the inclusion criteria requiring SLM-centric modeling and evaluation.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The study centers on CNN-PPG based speech command recognition for dysarthric users and compares ASR-based systems, with no emphasis on Spoken Language Models or speech-language modeling as core concepts; it does not discuss SLM/FD-SLM architectures or multi-turn interaction, thus does not meet the inclusion criteria requiring SLM-centric modeling and evaluation.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on a CNN-based speech command recognition system for dysarthric patients, emphasizing improved accuracy and reduced implementation costs. However, it centers on ASR technology for a specific downstream task (speech command recognition) without addressing core spoken language modeling, full-duplex interaction, synchronous behavior arbitration, or advanced speech language model architectures as required by the inclusion criteria. There is no discussion of speech language modeling as a distinct research focus or evaluation related to synchronization, cognitive parallelism, or interaction beyond ASR accuracy. Therefore, it does not meet the core inclusion criteria and matches exclusion points regarding a narrow application of speech recognition technology without broader spoken language model perspectives.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on a CNN-based speech command recognition system for dysarthric patients, emphasizing improved accuracy and reduced implementation costs. However, it centers on ASR technology for a specific downstream task (speech command recognition) without addressing core spoken language modeling, full-duplex interaction, synchronous behavior arbitration, or advanced speech language model architectures as required by the inclusion criteria. There is no discussion of speech language modeling as a distinct research focus or evaluation related to synchronization, cognitive parallelism, or interaction beyond ASR accuracy. Therefore, it does not meet the core inclusion criteria and matches exclusion points regarding a narrow application of speech recognition technology without broader spoken language model perspectives.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "MarbleNet: Deep 1D Time-Channel Separable Convolutional Neural Network for Voice Activity Detection",
    "abstract": "We present MarbleNet, an end-to-end neural network for Voice Activity Detection (VAD). MarbleNet is a deep residual network composed from blocks of 1D time-channel separable convolution, batch-normalization, ReLU and dropout layers. When compared to a state-of-the-art VAD model, MarbleNet is able to achieve similar performance with roughly 1/10-th the parameter cost. We further conduct extensive ablation studies on different training methods and choices of parameters in order to study the robustness of MarbleNet in real-world VAD tasks.",
    "metadata": {
      "title": "MarbleNet: Deep 1D Time-Channel Separable Convolutional Neural Network for Voice Activity Detection",
      "summary": "We present MarbleNet, an end-to-end neural network for Voice Activity Detection (VAD). MarbleNet is a deep residual network composed from blocks of 1D time-channel separable convolution, batch-normalization, ReLU and dropout layers. When compared to a state-of-the-art VAD model, MarbleNet is able to achieve similar performance with roughly 1/10-th the parameter cost. We further conduct extensive ablation studies on different training methods and choices of parameters in order to study the robustness of MarbleNet in real-world VAD tasks.",
      "abstract": "We present MarbleNet, an end-to-end neural network for Voice Activity Detection (VAD). MarbleNet is a deep residual network composed from blocks of 1D time-channel separable convolution, batch-normalization, ReLU and dropout layers. When compared to a state-of-the-art VAD model, MarbleNet is able to achieve similar performance with roughly 1/10-th the parameter cost. We further conduct extensive ablation studies on different training methods and choices of parameters in order to study the robustness of MarbleNet in real-world VAD tasks.",
      "doi": "https://doi.org/10.1109/icassp39728.2021.9414470",
      "openalex_id": "https://openalex.org/W3160747466",
      "arxiv_id": "",
      "publication_date": "2021-05-13",
      "published": "2021-05-13",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This MarbleNet VAD work targets voice activity detection and 1D CNN architectures, not Spoken Language Modeling or full-duplex speech interaction, so it does not meet the inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "This MarbleNet VAD work targets voice activity detection and 1D CNN architectures, not Spoken Language Modeling or full-duplex speech interaction, so it does not meet the inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on a neural network for Voice Activity Detection (VAD), which is a speech processing task primarily aimed at detecting voice presence, without explicit mention of spoken language modeling, language modeling of speech, or full-duplex spoken language interaction. The abstract does not discuss spoken language models, synchronous interactive dialogue behaviors, or relevant evaluation metrics aligned with the inclusion criteria. Therefore, it likely does not meet the core inclusion requirement of focusing on spoken language models or full-duplex spoken language model research.",
      "evaluation": 2
    },
    "round-A_JuniorMini_reasoning": "The study focuses on a neural network for Voice Activity Detection (VAD), which is a speech processing task primarily aimed at detecting voice presence, without explicit mention of spoken language modeling, language modeling of speech, or full-duplex spoken language interaction. The abstract does not discuss spoken language models, synchronous interactive dialogue behaviors, or relevant evaluation metrics aligned with the inclusion criteria. Therefore, it likely does not meet the core inclusion requirement of focusing on spoken language models or full-duplex spoken language model research.",
    "round-A_JuniorMini_evaluation": 2,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:2)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Semi-Supervised Spoken Language Understanding via Self-Supervised Speech and Language Model Pretraining",
    "abstract": "Much recent work on Spoken Language Understanding (SLU) is limited in at least one of three ways: models were trained on oracle text input and neglected ASR errors, models were trained to predict only intents without the slot values, or models were trained on a large amount of in-house data. In this paper, we propose a clean and general framework to learn semantics directly from speech with semi-supervision from transcribed or untranscribed speech to address these issues. Our framework is built upon pretrained end-to-end (E2E) ASR and self-supervised language models, such as BERT, and fine-tuned on a limited amount of target SLU data. We study two semi-supervised settings for the ASR component: supervised pretraining on transcribed speech, and unsupervised pretraining by replacing the ASR encoder with self-supervised speech representations, such as wav2vec. In parallel, we identify two essential criteria for evaluating SLU models: environmental noise-robustness and E2E semantics evaluation. Experiments on ATIS show that our SLU framework with speech as input can perform on par with those using oracle text as input in semantics understanding, even though environmental noise is present and a limited amount of labeled semantics data is available for training.",
    "metadata": {
      "title": "Semi-Supervised Spoken Language Understanding via Self-Supervised Speech and Language Model Pretraining",
      "summary": "Much recent work on Spoken Language Understanding (SLU) is limited in at least one of three ways: models were trained on oracle text input and neglected ASR errors, models were trained to predict only intents without the slot values, or models were trained on a large amount of in-house data. In this paper, we propose a clean and general framework to learn semantics directly from speech with semi-supervision from transcribed or untranscribed speech to address these issues. Our framework is built upon pretrained end-to-end (E2E) ASR and self-supervised language models, such as BERT, and fine-tuned on a limited amount of target SLU data. We study two semi-supervised settings for the ASR component: supervised pretraining on transcribed speech, and unsupervised pretraining by replacing the ASR encoder with self-supervised speech representations, such as wav2vec. In parallel, we identify two essential criteria for evaluating SLU models: environmental noise-robustness and E2E semantics evaluation. Experiments on ATIS show that our SLU framework with speech as input can perform on par with those using oracle text as input in semantics understanding, even though environmental noise is present and a limited amount of labeled semantics data is available for training.",
      "abstract": "Much recent work on Spoken Language Understanding (SLU) is limited in at least one of three ways: models were trained on oracle text input and neglected ASR errors, models were trained to predict only intents without the slot values, or models were trained on a large amount of in-house data. In this paper, we propose a clean and general framework to learn semantics directly from speech with semi-supervision from transcribed or untranscribed speech to address these issues. Our framework is built upon pretrained end-to-end (E2E) ASR and self-supervised language models, such as BERT, and fine-tuned on a limited amount of target SLU data. We study two semi-supervised settings for the ASR component: supervised pretraining on transcribed speech, and unsupervised pretraining by replacing the ASR encoder with self-supervised speech representations, such as wav2vec. In parallel, we identify two essential criteria for evaluating SLU models: environmental noise-robustness and E2E semantics evaluation. Experiments on ATIS show that our SLU framework with speech as input can perform on par with those using oracle text as input in semantics understanding, even though environmental noise is present and a limited amount of labeled semantics data is available for training.",
      "doi": "https://doi.org/10.1109/icassp39728.2021.9414922",
      "openalex_id": "https://openalex.org/W3161223924",
      "arxiv_id": "",
      "publication_date": "2021-05-13",
      "published": "2021-05-13",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item explicitly targets Spoken Language Understanding from speech, leveraging speech-centric models and semi-supervised training, discusses architecture/training/evaluation, and is within the last 3 years with keywords around spoken language models.",
      "evaluation": 5
    },
    "round-A_JuniorNano_reasoning": "The item explicitly targets Spoken Language Understanding from speech, leveraging speech-centric models and semi-supervised training, discusses architecture/training/evaluation, and is within the last 3 years with keywords around spoken language models.",
    "round-A_JuniorNano_evaluation": 5,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on semi-supervised spoken language understanding leveraging pretrained speech and language models, addressing speech as input rather than mere text, and involves model training and evaluation, but it does not explicitly address full-duplex interaction, cognitive parallelism, or real-time overlapping speech management, which are key for full inclusion; thus, it partially meets inclusion criteria but lacks some core aspects of the targeted research scope.",
      "evaluation": 3
    },
    "round-A_JuniorMini_reasoning": "The study focuses on semi-supervised spoken language understanding leveraging pretrained speech and language models, addressing speech as input rather than mere text, and involves model training and evaluation, but it does not explicitly address full-duplex interaction, cognitive parallelism, or real-time overlapping speech management, which are key for full inclusion; thus, it partially meets inclusion criteria but lacks some core aspects of the targeted research scope.",
    "round-A_JuniorMini_evaluation": 3,
    "round-B_SeniorLead_output": {
      "reasoning": "Both junior reviewers were consulted: JuniorNano recommended inclusion (5) but appears to have conflated SLU with spoken language modeling; JuniorMini was uncertain (3) noting the paper focuses on speech input and model training but lacks FD-SLM aspects. The paper addresses Spoken Language Understanding from speech using pretrained ASR and text/self-supervised models (wav2vec, BERT) and evaluates SLU on ATIS, but it does not claim or develop Spoken Language Models/ speech-level language modeling nor discuss full‑duplex interaction, overlap handling, low-latency synchronization, or related evaluation—thus it fails the topic requirement that SLMs/FD‑SLMs be the core focus and meets several exclusion criteria (primarily an SLU task treating speech mainly as input and relying on text LM components). Therefore: better to exclude.",
      "evaluation": 2
    },
    "round-B_SeniorLead_reasoning": "Both junior reviewers were consulted: JuniorNano recommended inclusion (5) but appears to have conflated SLU with spoken language modeling; JuniorMini was uncertain (3) noting the paper focuses on speech input and model training but lacks FD-SLM aspects. The paper addresses Spoken Language Understanding from speech using pretrained ASR and text/self-supervised models (wav2vec, BERT) and evaluates SLU on ATIS, but it does not claim or develop Spoken Language Models/ speech-level language modeling nor discuss full‑duplex interaction, overlap handling, low-latency synchronization, or related evaluation—thus it fails the topic requirement that SLMs/FD‑SLMs be the core focus and meets several exclusion criteria (primarily an SLU task treating speech mainly as input and relying on text LM components). Therefore: better to exclude.",
    "round-B_SeniorLead_evaluation": 2,
    "final_verdict": "exclude (senior:2)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Torchaudio-Squim: Reference-Less Speech Quality and Intelligibility Measures in Torchaudio",
    "abstract": "Measuring quality and intelligibility of a speech signal is usually a critical step in development of speech processing systems. To enable this, a variety of metrics to measure quality and intelligibility under different assumptions have been developed. Through this paper, we introduce tools and a set of models to estimate such known metrics using deep neural networks. These models are made available in the well-established TorchAudio library, the core audio and speech processing library within the PyTorch deep learning framework. We refer to it as TorchAudio-Squim, TorchAudio-Speech QUality and Intelligibility Measures. More specifically, in the current version of TorchAudio-squim, we establish and release models for estimating PESQ, STOI and SI-SDR among objective metrics and MOS among subjective metrics. We develop a novel approach for objective metric estimation and use a recently developed approach for subjective metric estimation. These models operate in a \"referenceless\" manner, that is they do not require the corresponding clean speech as reference for speech assessment. Given the unavailability of clean speech and the effortful process of subjective evaluation in real-world situations, such easy-to-use tools would greatly benefit speech processing research and development.",
    "metadata": {
      "title": "Torchaudio-Squim: Reference-Less Speech Quality and Intelligibility Measures in Torchaudio",
      "summary": "Measuring quality and intelligibility of a speech signal is usually a critical step in development of speech processing systems. To enable this, a variety of metrics to measure quality and intelligibility under different assumptions have been developed. Through this paper, we introduce tools and a set of models to estimate such known metrics using deep neural networks. These models are made available in the well-established TorchAudio library, the core audio and speech processing library within the PyTorch deep learning framework. We refer to it as TorchAudio-Squim, TorchAudio-Speech QUality and Intelligibility Measures. More specifically, in the current version of TorchAudio-squim, we establish and release models for estimating PESQ, STOI and SI-SDR among objective metrics and MOS among subjective metrics. We develop a novel approach for objective metric estimation and use a recently developed approach for subjective metric estimation. These models operate in a \"referenceless\" manner, that is they do not require the corresponding clean speech as reference for speech assessment. Given the unavailability of clean speech and the effortful process of subjective evaluation in real-world situations, such easy-to-use tools would greatly benefit speech processing research and development.",
      "abstract": "Measuring quality and intelligibility of a speech signal is usually a critical step in development of speech processing systems. To enable this, a variety of metrics to measure quality and intelligibility under different assumptions have been developed. Through this paper, we introduce tools and a set of models to estimate such known metrics using deep neural networks. These models are made available in the well-established TorchAudio library, the core audio and speech processing library within the PyTorch deep learning framework. We refer to it as TorchAudio-Squim, TorchAudio-Speech QUality and Intelligibility Measures. More specifically, in the current version of TorchAudio-squim, we establish and release models for estimating PESQ, STOI and SI-SDR among objective metrics and MOS among subjective metrics. We develop a novel approach for objective metric estimation and use a recently developed approach for subjective metric estimation. These models operate in a \"referenceless\" manner, that is they do not require the corresponding clean speech as reference for speech assessment. Given the unavailability of clean speech and the effortful process of subjective evaluation in real-world situations, such easy-to-use tools would greatly benefit speech processing research and development.",
      "doi": "https://doi.org/10.1109/icassp49357.2023.10096680",
      "openalex_id": "https://openalex.org/W4372260337",
      "arxiv_id": "",
      "publication_date": "2023-05-05",
      "published": "2023-05-05",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "Reasoning: This entry centers on reference-less metrics for speech quality (PESQ/STOI/SI-SDR/MOS) rather than modeling or evaluating Spoken Language Models (SLMs/FD-SLMs) as core objects, so it does not meet the inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "Reasoning: This entry centers on reference-less metrics for speech quality (PESQ/STOI/SI-SDR/MOS) rather than modeling or evaluating Spoken Language Models (SLMs/FD-SLMs) as core objects, so it does not meet the inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper introduces neural network models for reference-less speech quality and intelligibility estimation integrated into the TorchAudio library. However, it focuses on speech quality/intelligibility metrics rather than modeling spoken language itself or speech language models. There is no mention of spoken language model architectures, training, inference, or full-duplex dialogue interaction. The research centers on signal processing evaluation metrics rather than SLM or FD-SLM aspects as required.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper introduces neural network models for reference-less speech quality and intelligibility estimation integrated into the TorchAudio library. However, it focuses on speech quality/intelligibility metrics rather than modeling spoken language itself or speech language models. There is no mention of spoken language model architectures, training, inference, or full-duplex dialogue interaction. The research centers on signal processing evaluation metrics rather than SLM or FD-SLM aspects as required.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Prompting and Adapter Tuning For Self-Supervised Encoder-Decoder Speech Model",
    "abstract": "Prompting and adapter tuning have emerged as efficient alternatives to fine-tuning (FT) methods. However, existing studies on speech prompting focused on classification tasks and failed on more complex sequence generation tasks. Besides, adapter tuning is primarily applied with a focus on encoder-only self-supervised models. Our experiments show that prompting on Wav2Seq, a self-supervised encoder-decoder model, surpasses previous works in sequence generation tasks. It achieves a remarkable 53% relative improvement in word error rate for ASR and a 27% in F1 score for slot filling. Additionally, prompting competes with the FT method in the low-resource scenario. Moreover, we show the transferability of prompting and adapter tuning on Wav2Seq in cross-lingual ASR. When limited trainable parameters are involved, prompting and adapter tuning consistently outperform conventional FT across 7 languages. Notably, in the low-resource scenario, prompting consistently outperforms adapter tuning.",
    "metadata": {
      "title": "Prompting and Adapter Tuning For Self-Supervised Encoder-Decoder Speech Model",
      "summary": "Prompting and adapter tuning have emerged as efficient alternatives to fine-tuning (FT) methods. However, existing studies on speech prompting focused on classification tasks and failed on more complex sequence generation tasks. Besides, adapter tuning is primarily applied with a focus on encoder-only self-supervised models. Our experiments show that prompting on Wav2Seq, a self-supervised encoder-decoder model, surpasses previous works in sequence generation tasks. It achieves a remarkable 53% relative improvement in word error rate for ASR and a 27% in F1 score for slot filling. Additionally, prompting competes with the FT method in the low-resource scenario. Moreover, we show the transferability of prompting and adapter tuning on Wav2Seq in cross-lingual ASR. When limited trainable parameters are involved, prompting and adapter tuning consistently outperform conventional FT across 7 languages. Notably, in the low-resource scenario, prompting consistently outperforms adapter tuning.",
      "abstract": "Prompting and adapter tuning have emerged as efficient alternatives to fine-tuning (FT) methods. However, existing studies on speech prompting focused on classification tasks and failed on more complex sequence generation tasks. Besides, adapter tuning is primarily applied with a focus on encoder-only self-supervised models. Our experiments show that prompting on Wav2Seq, a self-supervised encoder-decoder model, surpasses previous works in sequence generation tasks. It achieves a remarkable 53% relative improvement in word error rate for ASR and a 27% in F1 score for slot filling. Additionally, prompting competes with the FT method in the low-resource scenario. Moreover, we show the transferability of prompting and adapter tuning on Wav2Seq in cross-lingual ASR. When limited trainable parameters are involved, prompting and adapter tuning consistently outperform conventional FT across 7 languages. Notably, in the low-resource scenario, prompting consistently outperforms adapter tuning.",
      "doi": "https://doi.org/10.1109/asru57964.2023.10389731",
      "openalex_id": "https://openalex.org/W4391021530",
      "arxiv_id": "",
      "publication_date": "2023-12-16",
      "published": "2023-12-16",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item centers on Spoken Language Models with speech as the core modeling target, discusses architecture/training/evaluation (prompting and adapters for a self-supervised encoder–decoder speech model) and reports cross-lingual ASR results within the past 3 years, meeting all inclusion criteria and none of the exclusions.",
      "evaluation": 5
    },
    "round-A_JuniorNano_reasoning": "The item centers on Spoken Language Models with speech as the core modeling target, discusses architecture/training/evaluation (prompting and adapters for a self-supervised encoder–decoder speech model) and reports cross-lingual ASR results within the past 3 years, meeting all inclusion criteria and none of the exclusions.",
    "round-A_JuniorNano_evaluation": 5,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on prompting and adapter tuning for a self-supervised encoder-decoder speech model (Wav2Seq) in tasks like ASR and slot filling, primarily emphasizing word error rate and F1 score improvements. It concerns speech sequence generation and low-resource scenarios, but it does not explicitly discuss spoken language models or full-duplex spoken language models, nor does it address synchronous interaction, cognitive parallelism, or overlapping speech management. The evaluation focuses on ASR and slot filling performance metrics without involving real-time interaction or behavior arbitration. Therefore, while relevant to speech modeling, it lacks the core focus on spoken language models or synchronous interactive behaviors and does not meet all inclusion criteria nor avoids exclusion criteria perfectly.",
      "evaluation": 2
    },
    "round-A_JuniorMini_reasoning": "The study focuses on prompting and adapter tuning for a self-supervised encoder-decoder speech model (Wav2Seq) in tasks like ASR and slot filling, primarily emphasizing word error rate and F1 score improvements. It concerns speech sequence generation and low-resource scenarios, but it does not explicitly discuss spoken language models or full-duplex spoken language models, nor does it address synchronous interaction, cognitive parallelism, or overlapping speech management. The evaluation focuses on ASR and slot filling performance metrics without involving real-time interaction or behavior arbitration. Therefore, while relevant to speech modeling, it lacks the core focus on spoken language models or synchronous interactive behaviors and does not meet all inclusion criteria nor avoids exclusion criteria perfectly.",
    "round-A_JuniorMini_evaluation": 2,
    "round-B_SeniorLead_output": {
      "reasoning": "Junior reviewers disagreed: JuniorNano rated 5 arguing the work centers on speech encoder–decoder modeling, while JuniorMini rated 2 noting it lacks SLM/full‑duplex focus — after integrating both views, I conclude exclusion. The abstract describes prompting and adapter tuning for Wav2Seq targeting ASR and slot‑filling with WER/F1 gains and cross‑lingual ASR, but it does not explicitly present the work as a \"Spoken Language Model\" or discuss full‑duplex/synchronous interaction, cognitive parallelism, overlap management, low‑latency interaction, or multi‑dimensional interactive evaluation; it therefore fails the inclusion requirement for explicit SLM framing and falls into the exclusion categories (single‑task ASR/slot focus and metrics limited to WER/F1). One‑sentence explanation: Because the paper lacks explicit spoken‑language‑model framing and does not address synchronous/full‑duplex interaction or related evaluations, it should be excluded.",
      "evaluation": 1
    },
    "round-B_SeniorLead_reasoning": "Junior reviewers disagreed: JuniorNano rated 5 arguing the work centers on speech encoder–decoder modeling, while JuniorMini rated 2 noting it lacks SLM/full‑duplex focus — after integrating both views, I conclude exclusion. The abstract describes prompting and adapter tuning for Wav2Seq targeting ASR and slot‑filling with WER/F1 gains and cross‑lingual ASR, but it does not explicitly present the work as a \"Spoken Language Model\" or discuss full‑duplex/synchronous interaction, cognitive parallelism, overlap management, low‑latency interaction, or multi‑dimensional interactive evaluation; it therefore fails the inclusion requirement for explicit SLM framing and falls into the exclusion categories (single‑task ASR/slot focus and metrics limited to WER/F1). One‑sentence explanation: Because the paper lacks explicit spoken‑language‑model framing and does not address synchronous/full‑duplex interaction or related evaluations, it should be excluded.",
    "round-B_SeniorLead_evaluation": 1,
    "final_verdict": "exclude (senior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Wav2Seq: Pre-Training Speech-to-Text Encoder-Decoder Models Using Pseudo Languages",
    "abstract": "We introduce Wav2Seq, the first self-supervised approach to pre-train both parts of encoder-decoder models for speech data. We induce a pseudo language as a compact discrete representation, and formulate a self-supervised pseudo speech recognition task — transcribing audio inputs into pseudo subword sequences. This process stands on its own, or can be applied as low-cost second-stage pre-training. We experiment with automatic speech recognition (ASR), spoken named entity recognition, and speech-to-text translation. We set new state-of-the-art results for end-to-end spoken named entity recognition, and show consistent improvements on 8 language pairs for speech-to-text translation, even when competing methods use additional text data for training. On ASR, our approach enables encoder-decoder methods to benefit from pre-training for all parts of the network, and shows comparable performance to highly optimized recent methods.",
    "metadata": {
      "title": "Wav2Seq: Pre-Training Speech-to-Text Encoder-Decoder Models Using Pseudo Languages",
      "summary": "We introduce Wav2Seq, the first self-supervised approach to pre-train both parts of encoder-decoder models for speech data. We induce a pseudo language as a compact discrete representation, and formulate a self-supervised pseudo speech recognition task — transcribing audio inputs into pseudo subword sequences. This process stands on its own, or can be applied as low-cost second-stage pre-training. We experiment with automatic speech recognition (ASR), spoken named entity recognition, and speech-to-text translation. We set new state-of-the-art results for end-to-end spoken named entity recognition, and show consistent improvements on 8 language pairs for speech-to-text translation, even when competing methods use additional text data for training. On ASR, our approach enables encoder-decoder methods to benefit from pre-training for all parts of the network, and shows comparable performance to highly optimized recent methods.",
      "abstract": "We introduce Wav2Seq, the first self-supervised approach to pre-train both parts of encoder-decoder models for speech data. We induce a pseudo language as a compact discrete representation, and formulate a self-supervised pseudo speech recognition task — transcribing audio inputs into pseudo subword sequences. This process stands on its own, or can be applied as low-cost second-stage pre-training. We experiment with automatic speech recognition (ASR), spoken named entity recognition, and speech-to-text translation. We set new state-of-the-art results for end-to-end spoken named entity recognition, and show consistent improvements on 8 language pairs for speech-to-text translation, even when competing methods use additional text data for training. On ASR, our approach enables encoder-decoder methods to benefit from pre-training for all parts of the network, and shows comparable performance to highly optimized recent methods.",
      "doi": "https://doi.org/10.1109/icassp49357.2023.10096988",
      "openalex_id": "https://openalex.org/W4372270126",
      "arxiv_id": "",
      "publication_date": "2023-05-05",
      "published": "2023-05-05",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The paper centers on speech data and encoder–decoder pretraining with tasks like ASR, speech-to-text translation, and spoken NER, effectively modeling speech-language tasks and (via pseudo-language tokens) language modeling of speech, within the last 3 years, and it is not limited to single-task ASR or traditional NLP only, thus meeting inclusion and not exclusion.",
      "evaluation": 5
    },
    "round-A_JuniorNano_reasoning": "The paper centers on speech data and encoder–decoder pretraining with tasks like ASR, speech-to-text translation, and spoken NER, effectively modeling speech-language tasks and (via pseudo-language tokens) language modeling of speech, within the last 3 years, and it is not limited to single-task ASR or traditional NLP only, thus meeting inclusion and not exclusion.",
    "round-A_JuniorNano_evaluation": 5,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on pre-training encoder-decoder models for speech data using pseudo languages, primarily targeting ASR, spoken named entity recognition, and speech-to-text translation; however, it lacks explicit discussion on spoken language models or full-duplex spoken language models, does not address turn-taking, behavior arbitration, or low-latency real-time interaction, and mainly evaluates performance improvements on downstream tasks without emphasizing the core modeling of speech as a language or synchronization aspects required by the inclusion criteria.",
      "evaluation": 2
    },
    "round-A_JuniorMini_reasoning": "The study focuses on pre-training encoder-decoder models for speech data using pseudo languages, primarily targeting ASR, spoken named entity recognition, and speech-to-text translation; however, it lacks explicit discussion on spoken language models or full-duplex spoken language models, does not address turn-taking, behavior arbitration, or low-latency real-time interaction, and mainly evaluates performance improvements on downstream tasks without emphasizing the core modeling of speech as a language or synchronization aspects required by the inclusion criteria.",
    "round-A_JuniorMini_evaluation": 2,
    "round-B_SeniorLead_output": {
      "reasoning": "Junior reviewers: JuniorNano recommended inclusion (5), highlighting the paper's speech-focused encoder–decoder pretraining using pseudo-language tokens; JuniorMini recommended exclusion (2), noting the absence of explicit spoken-language-model framing and interactive/synchronization aspects. My assessment: although Wav2Seq clearly advances speech encoder–decoder pretraining and introduces discrete pseudo-languages for speech, it does not explicitly claim or frame itself as a Spoken Language Model (SLM) or Full‑Duplex SLM, nor does it address turn-taking, overlap management, low-latency/full‑duplex interaction, or provide evaluations of interactive synchronization—requirements in the inclusion criteria. Therefore I score it as better to exclude. 1-sentence explanation: The work is a strong speech pretraining contribution but lacks explicit SLM/FD‑SLM framing and any treatment or evaluation of interactive/full‑duplex synchronization behaviors required for inclusion.",
      "evaluation": 2
    },
    "round-B_SeniorLead_reasoning": "Junior reviewers: JuniorNano recommended inclusion (5), highlighting the paper's speech-focused encoder–decoder pretraining using pseudo-language tokens; JuniorMini recommended exclusion (2), noting the absence of explicit spoken-language-model framing and interactive/synchronization aspects. My assessment: although Wav2Seq clearly advances speech encoder–decoder pretraining and introduces discrete pseudo-languages for speech, it does not explicitly claim or frame itself as a Spoken Language Model (SLM) or Full‑Duplex SLM, nor does it address turn-taking, overlap management, low-latency/full‑duplex interaction, or provide evaluations of interactive synchronization—requirements in the inclusion criteria. Therefore I score it as better to exclude. 1-sentence explanation: The work is a strong speech pretraining contribution but lacks explicit SLM/FD‑SLM framing and any treatment or evaluation of interactive/full‑duplex synchronization behaviors required for inclusion.",
    "round-B_SeniorLead_evaluation": 2,
    "final_verdict": "exclude (senior:2)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Dynamic-Superb: Towards a Dynamic, Collaborative, and Comprehensive Instruction-Tuning Benchmark For Speech",
    "abstract": "Text language models have shown remarkable zero-shot capability in generalizing to unseen tasks when provided with well-formulated instructions. However, existing studies in speech processing primarily focus on limited or specific tasks. Moreover, the lack of standardized benchmarks hinders a fair comparison across different approaches. Thus, we present Dynamic-SUPERB, a benchmark designed for building universal speech models capable of leveraging instruction tuning to perform multiple tasks in a zero-shot fashion. To achieve comprehensive coverage of diverse speech tasks and harness instruction tuning, we invite the community to collaborate and contribute, facilitating the dynamic growth of the benchmark. To initiate, Dynamic-SUPERB features 55 evaluation instances by combining 33 tasks and 22 datasets. This spans a broad spectrum of dimensions, providing a comprehensive platform for evaluation. Additionally, we propose several approaches to establish benchmark baselines. These include the utilization of speech models, text language models, and the multimodal encoder. Evaluation results indicate that while these baselines perform reasonably on seen tasks, they struggle with unseen ones. We release all materials to the public and welcome researchers to collaborate on the project, advancing technologies in the field together <sup xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">1</sup> .",
    "metadata": {
      "title": "Dynamic-Superb: Towards a Dynamic, Collaborative, and Comprehensive Instruction-Tuning Benchmark For Speech",
      "summary": "Text language models have shown remarkable zero-shot capability in generalizing to unseen tasks when provided with well-formulated instructions. However, existing studies in speech processing primarily focus on limited or specific tasks. Moreover, the lack of standardized benchmarks hinders a fair comparison across different approaches. Thus, we present Dynamic-SUPERB, a benchmark designed for building universal speech models capable of leveraging instruction tuning to perform multiple tasks in a zero-shot fashion. To achieve comprehensive coverage of diverse speech tasks and harness instruction tuning, we invite the community to collaborate and contribute, facilitating the dynamic growth of the benchmark. To initiate, Dynamic-SUPERB features 55 evaluation instances by combining 33 tasks and 22 datasets. This spans a broad spectrum of dimensions, providing a comprehensive platform for evaluation. Additionally, we propose several approaches to establish benchmark baselines. These include the utilization of speech models, text language models, and the multimodal encoder. Evaluation results indicate that while these baselines perform reasonably on seen tasks, they struggle with unseen ones. We release all materials to the public and welcome researchers to collaborate on the project, advancing technologies in the field together <sup xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">1</sup> .",
      "abstract": "Text language models have shown remarkable zero-shot capability in generalizing to unseen tasks when provided with well-formulated instructions. However, existing studies in speech processing primarily focus on limited or specific tasks. Moreover, the lack of standardized benchmarks hinders a fair comparison across different approaches. Thus, we present Dynamic-SUPERB, a benchmark designed for building universal speech models capable of leveraging instruction tuning to perform multiple tasks in a zero-shot fashion. To achieve comprehensive coverage of diverse speech tasks and harness instruction tuning, we invite the community to collaborate and contribute, facilitating the dynamic growth of the benchmark. To initiate, Dynamic-SUPERB features 55 evaluation instances by combining 33 tasks and 22 datasets. This spans a broad spectrum of dimensions, providing a comprehensive platform for evaluation. Additionally, we propose several approaches to establish benchmark baselines. These include the utilization of speech models, text language models, and the multimodal encoder. Evaluation results indicate that while these baselines perform reasonably on seen tasks, they struggle with unseen ones. We release all materials to the public and welcome researchers to collaborate on the project, advancing technologies in the field together <sup xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">1</sup> .",
      "doi": "https://doi.org/10.1109/icassp48485.2024.10448257",
      "openalex_id": "https://openalex.org/W4392902623",
      "arxiv_id": "",
      "publication_date": "2024-03-18",
      "published": "2024-03-18",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item describes a benchmark for universal speech models using instruction tuning across multiple tasks, with evaluation baselines and public data, aligning with the focus on speech models and benchmarking in recent years; it discusses model evaluation and architecture-related aspects (benchmarks, baselines) rather than being limited to a single ASR/TTS task or purely NLP, though it does not explicitly use the phrase 'spoken language model' in the abstract, the concept of 'universal speech models' and instruction-tuning constitutes an equivalent framing to SLMs. Therefore, it should be included.",
      "evaluation": 4
    },
    "round-A_JuniorNano_reasoning": "The item describes a benchmark for universal speech models using instruction tuning across multiple tasks, with evaluation baselines and public data, aligning with the focus on speech models and benchmarking in recent years; it discusses model evaluation and architecture-related aspects (benchmarks, baselines) rather than being limited to a single ASR/TTS task or purely NLP, though it does not explicitly use the phrase 'spoken language model' in the abstract, the concept of 'universal speech models' and instruction-tuning constitutes an equivalent framing to SLMs. Therefore, it should be included.",
    "round-A_JuniorNano_evaluation": 4,
    "round-A_JuniorMini_output": {
      "reasoning": "The study presents Dynamic-SUPERB, a speech benchmark involving instruction tuning for multiple speech tasks, focusing on universal speech models with zero-shot capabilities. However, the abstract does not explicitly mention Spoken Language Models or equivalent concepts as the core modeling target, nor does it discuss full-duplex or synchronous interactive capabilities, cognitive parallelism, or low-latency real-time interaction features central to the inclusion criteria. The focus is more on benchmarking multiple diverse speech tasks rather than on modeling architectures involving speech as primary objects or the dynamics of speech interaction. Additionally, there is no mention of addressing synchronization, turn-taking beyond half-duplex, or behavioral arbitration. Therefore, it fails to meet the strict inclusion criteria and partially aligns with exclusion due to not emphasizing spoken language modeling or synchronous speech interaction.",
      "evaluation": 2
    },
    "round-A_JuniorMini_reasoning": "The study presents Dynamic-SUPERB, a speech benchmark involving instruction tuning for multiple speech tasks, focusing on universal speech models with zero-shot capabilities. However, the abstract does not explicitly mention Spoken Language Models or equivalent concepts as the core modeling target, nor does it discuss full-duplex or synchronous interactive capabilities, cognitive parallelism, or low-latency real-time interaction features central to the inclusion criteria. The focus is more on benchmarking multiple diverse speech tasks rather than on modeling architectures involving speech as primary objects or the dynamics of speech interaction. Additionally, there is no mention of addressing synchronization, turn-taking beyond half-duplex, or behavioral arbitration. Therefore, it fails to meet the strict inclusion criteria and partially aligns with exclusion due to not emphasizing spoken language modeling or synchronous speech interaction.",
    "round-A_JuniorMini_evaluation": 2,
    "round-B_SeniorLead_output": {
      "reasoning": "I reviewed both junior reviewers: JuniorNano recommended inclusion noting the benchmark focuses on speech models and benchmarking rather than a single ASR/TTS task; JuniorMini recommended exclusion citing lack of explicit mention of ‘Spoken Language Models’ and absence of full-duplex/synchronous interaction aspects. The abstract uses the phrase “universal speech models” and describes benchmark design, baselines (speech models, text LMs, multimodal encoder), and evaluations across many speech tasks — which satisfies the core inclusion requirement that the work treat speech as the primary modeling object and discuss model/benchmark evaluation even though it does not address full-duplex interaction or overlap/cognitive-parallelism. Therefore it meets the main inclusion criteria (SLM-equivalent focus and evaluation/architecture discussion) but lacks FD-SLM-specific contributions, so I lean to include. (Explanation) Although it does not address full-duplex interaction, the paper explicitly targets “universal speech models” with architecture/baseline/evaluation work, meeting the core inclusion criteria.",
      "evaluation": 4
    },
    "round-B_SeniorLead_reasoning": "I reviewed both junior reviewers: JuniorNano recommended inclusion noting the benchmark focuses on speech models and benchmarking rather than a single ASR/TTS task; JuniorMini recommended exclusion citing lack of explicit mention of ‘Spoken Language Models’ and absence of full-duplex/synchronous interaction aspects. The abstract uses the phrase “universal speech models” and describes benchmark design, baselines (speech models, text LMs, multimodal encoder), and evaluations across many speech tasks — which satisfies the core inclusion requirement that the work treat speech as the primary modeling object and discuss model/benchmark evaluation even though it does not address full-duplex interaction or overlap/cognitive-parallelism. Therefore it meets the main inclusion criteria (SLM-equivalent focus and evaluation/architecture discussion) but lacks FD-SLM-specific contributions, so I lean to include. (Explanation) Although it does not address full-duplex interaction, the paper explicitly targets “universal speech models” with architecture/baseline/evaluation work, meeting the core inclusion criteria.",
    "round-B_SeniorLead_evaluation": 4,
    "final_verdict": "include (senior:4)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text\\n Transformer",
    "abstract": "Transfer learning, where a model is first pre-trained on a data-rich task\\nbefore being fine-tuned on a downstream task, has emerged as a powerful\\ntechnique in natural language processing (NLP). The effectiveness of transfer\\nlearning has given rise to a diversity of approaches, methodology, and\\npractice. In this paper, we explore the landscape of transfer learning\\ntechniques for NLP by introducing a unified framework that converts all\\ntext-based language problems into a text-to-text format. Our systematic study\\ncompares pre-training objectives, architectures, unlabeled data sets, transfer\\napproaches, and other factors on dozens of language understanding tasks. By\\ncombining the insights from our exploration with scale and our new ``Colossal\\nClean Crawled Corpus'', we achieve state-of-the-art results on many benchmarks\\ncovering summarization, question answering, text classification, and more. To\\nfacilitate future work on transfer learning for NLP, we release our data set,\\npre-trained models, and code.\\n",
    "metadata": {
      "title": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text\\n Transformer",
      "summary": "Transfer learning, where a model is first pre-trained on a data-rich task\\nbefore being fine-tuned on a downstream task, has emerged as a powerful\\ntechnique in natural language processing (NLP). The effectiveness of transfer\\nlearning has given rise to a diversity of approaches, methodology, and\\npractice. In this paper, we explore the landscape of transfer learning\\ntechniques for NLP by introducing a unified framework that converts all\\ntext-based language problems into a text-to-text format. Our systematic study\\ncompares pre-training objectives, architectures, unlabeled data sets, transfer\\napproaches, and other factors on dozens of language understanding tasks. By\\ncombining the insights from our exploration with scale and our new ``Colossal\\nClean Crawled Corpus'', we achieve state-of-the-art results on many benchmarks\\ncovering summarization, question answering, text classification, and more. To\\nfacilitate future work on transfer learning for NLP, we release our data set,\\npre-trained models, and code.\\n",
      "abstract": "Transfer learning, where a model is first pre-trained on a data-rich task\\nbefore being fine-tuned on a downstream task, has emerged as a powerful\\ntechnique in natural language processing (NLP). The effectiveness of transfer\\nlearning has given rise to a diversity of approaches, methodology, and\\npractice. In this paper, we explore the landscape of transfer learning\\ntechniques for NLP by introducing a unified framework that converts all\\ntext-based language problems into a text-to-text format. Our systematic study\\ncompares pre-training objectives, architectures, unlabeled data sets, transfer\\napproaches, and other factors on dozens of language understanding tasks. By\\ncombining the insights from our exploration with scale and our new ``Colossal\\nClean Crawled Corpus'', we achieve state-of-the-art results on many benchmarks\\ncovering summarization, question answering, text classification, and more. To\\nfacilitate future work on transfer learning for NLP, we release our data set,\\npre-trained models, and code.\\n",
      "doi": "https://doi.org/10.48550/arxiv.1910.10683",
      "openalex_id": "https://openalex.org/W4288089799",
      "arxiv_id": "",
      "publication_date": "2019-10-23",
      "published": "2019-10-23",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "Reasoning: The item centers on transfer learning using a unified text-to-text transformer for NLP, with speech/spoken language modeling not as the core focus, and no evaluation of interactive speech behavior; thus it does not meet the Spoken Language Model inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "Reasoning: The item centers on transfer learning using a unified text-to-text transformer for NLP, with speech/spoken language modeling not as the core focus, and no evaluation of interactive speech behavior; thus it does not meet the Spoken Language Model inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The title and abstract focus on transfer learning for NLP with text-to-text transformers, and do not mention spoken language models, speech-level language modeling, or full-duplex spoken interaction capabilities; thus, the study does not satisfy the core inclusion criterion requiring explicit focus on speech as the main modeling object rather than just text processing.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The title and abstract focus on transfer learning for NLP with text-to-text transformers, and do not mention spoken language models, speech-level language modeling, or full-duplex spoken interaction capabilities; thus, the study does not satisfy the core inclusion criterion requiring explicit focus on speech as the main modeling object rather than just text processing.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Pyannote.Audio: Neural Building Blocks for Speaker Diarization",
    "abstract": "We introduce pyannote.audio, an open-source toolkit written in Python for speaker diarization. Based on PyTorch machine learning framework, it provides a set of trainable end-to-end neural building blocks that can be combined and jointly optimized to build speaker diarization pipelines. pyannote.audio also comes with pre-trained models covering a wide range of domains for voice activity detection, speaker change detection, overlapped speech detection, and speaker embedding -- reaching state-of-the-art performance for most of them.",
    "metadata": {
      "title": "Pyannote.Audio: Neural Building Blocks for Speaker Diarization",
      "summary": "We introduce pyannote.audio, an open-source toolkit written in Python for speaker diarization. Based on PyTorch machine learning framework, it provides a set of trainable end-to-end neural building blocks that can be combined and jointly optimized to build speaker diarization pipelines. pyannote.audio also comes with pre-trained models covering a wide range of domains for voice activity detection, speaker change detection, overlapped speech detection, and speaker embedding -- reaching state-of-the-art performance for most of them.",
      "abstract": "We introduce pyannote.audio, an open-source toolkit written in Python for speaker diarization. Based on PyTorch machine learning framework, it provides a set of trainable end-to-end neural building blocks that can be combined and jointly optimized to build speaker diarization pipelines. pyannote.audio also comes with pre-trained models covering a wide range of domains for voice activity detection, speaker change detection, overlapped speech detection, and speaker embedding -- reaching state-of-the-art performance for most of them.",
      "doi": "https://doi.org/10.1109/icassp40776.2020.9052974",
      "openalex_id": "https://openalex.org/W2985913104",
      "arxiv_id": "",
      "publication_date": "2020-04-09",
      "published": "2020-04-09",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "該論文聚焦於 pyannote.audio 及說話人分段工具，屬於語音辨識與分段模組的工程實作與預訓練模型，並未以 Spoken Language Models 或等價概念為核心，亦未討論全雙工互動、對話行為仲裁等評估，因此不符合包含條件，應排除。",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "該論文聚焦於 pyannote.audio 及說話人分段工具，屬於語音辨識與分段模組的工程實作與預訓練模型，並未以 Spoken Language Models 或等價概念為核心，亦未討論全雙工互動、對話行為仲裁等評估，因此不符合包含條件，應排除。",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper presents a toolkit for speaker diarization using neural building blocks but does not mention 'spoken language models' or equivalent terms explicitly in the title or abstract. Moreover, it focuses on speaker diarization and voice activity detection rather than direct speech language modeling or full-duplex spoken language interaction capabilities as required by the inclusion criteria. It also lacks discussion on model architecture, training, or evaluation in the context of spoken language modeling, and does not address features like cognitive parallelism or low-latency full-duplex interaction.",
      "evaluation": 2
    },
    "round-A_JuniorMini_reasoning": "The paper presents a toolkit for speaker diarization using neural building blocks but does not mention 'spoken language models' or equivalent terms explicitly in the title or abstract. Moreover, it focuses on speaker diarization and voice activity detection rather than direct speech language modeling or full-duplex spoken language interaction capabilities as required by the inclusion criteria. It also lacks discussion on model architecture, training, or evaluation in the context of spoken language modeling, and does not address features like cognitive parallelism or low-latency full-duplex interaction.",
    "round-A_JuniorMini_evaluation": 2,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:2)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Distilling the Knowledge in a Neural Network",
    "abstract": "A very simple way to improve the performance of almost any machine learning algorithm is to train many different models on the same data and then to average their predictions. Unfortunately, making predictions using a whole ensemble of models is cumbersome and may be too computationally expensive to allow deployment to a large number of users, especially if the individual models are large neural nets. Caruana and his collaborators have shown that it is possible to compress the knowledge in an ensemble into a single model which is much easier to deploy and we develop this approach further using a different compression technique. We achieve some surprising results on MNIST and we show that we can significantly improve the acoustic model of a heavily used commercial system by distilling the knowledge in an ensemble of models into a single model. We also introduce a new type of ensemble composed of one or more full models and many specialist models which learn to distinguish fine-grained classes that the full models confuse. Unlike a mixture of experts, these specialist models can be trained rapidly and in parallel.",
    "metadata": {
      "title": "Distilling the Knowledge in a Neural Network",
      "summary": "A very simple way to improve the performance of almost any machine learning algorithm is to train many different models on the same data and then to average their predictions. Unfortunately, making predictions using a whole ensemble of models is cumbersome and may be too computationally expensive to allow deployment to a large number of users, especially if the individual models are large neural nets. Caruana and his collaborators have shown that it is possible to compress the knowledge in an ensemble into a single model which is much easier to deploy and we develop this approach further using a different compression technique. We achieve some surprising results on MNIST and we show that we can significantly improve the acoustic model of a heavily used commercial system by distilling the knowledge in an ensemble of models into a single model. We also introduce a new type of ensemble composed of one or more full models and many specialist models which learn to distinguish fine-grained classes that the full models confuse. Unlike a mixture of experts, these specialist models can be trained rapidly and in parallel.",
      "abstract": "A very simple way to improve the performance of almost any machine learning algorithm is to train many different models on the same data and then to average their predictions. Unfortunately, making predictions using a whole ensemble of models is cumbersome and may be too computationally expensive to allow deployment to a large number of users, especially if the individual models are large neural nets. Caruana and his collaborators have shown that it is possible to compress the knowledge in an ensemble into a single model which is much easier to deploy and we develop this approach further using a different compression technique. We achieve some surprising results on MNIST and we show that we can significantly improve the acoustic model of a heavily used commercial system by distilling the knowledge in an ensemble of models into a single model. We also introduce a new type of ensemble composed of one or more full models and many specialist models which learn to distinguish fine-grained classes that the full models confuse. Unlike a mixture of experts, these specialist models can be trained rapidly and in parallel.",
      "doi": "https://doi.org/10.48550/arxiv.1503.02531",
      "openalex_id": "https://openalex.org/W1821462560",
      "arxiv_id": "",
      "publication_date": "2015-03-09",
      "published": "2015-03-09",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "Not about Spoken Language Models or full-duplex speech interaction; it's a general knowledge-distillation ML paper with only incidental reference to acoustic models and is outside the past 3 years, so it does not meet the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "Not about Spoken Language Models or full-duplex speech interaction; it's a general knowledge-distillation ML paper with only incidental reference to acoustic models and is outside the past 3 years, so it does not meet the inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The abstract discusses knowledge distillation to compress ensembles of neural networks, focusing on improving performance for acoustic models and introduces specialist models for fine-grained classification. However, it neither explicitly focuses on spoken language models or full-duplex spoken language models nor discusses core topics such as speech-level language modeling, speech-LLM architectures, synchronous interaction, or evaluation metrics related to real-time interaction, cognitive parallelism, or speech overlap management. It also does not mention spoken language modeling as central, primarily discussing model compression techniques in acoustic modeling, which falls short of the inclusion criteria emphasizing speech language modeling and synchronous interactive systems.",
      "evaluation": 2
    },
    "round-A_JuniorMini_reasoning": "The abstract discusses knowledge distillation to compress ensembles of neural networks, focusing on improving performance for acoustic models and introduces specialist models for fine-grained classification. However, it neither explicitly focuses on spoken language models or full-duplex spoken language models nor discusses core topics such as speech-level language modeling, speech-LLM architectures, synchronous interaction, or evaluation metrics related to real-time interaction, cognitive parallelism, or speech overlap management. It also does not mention spoken language modeling as central, primarily discussing model compression techniques in acoustic modeling, which falls short of the inclusion criteria emphasizing speech language modeling and synchronous interactive systems.",
    "round-A_JuniorMini_evaluation": 2,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:2)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "XLS-R: Self-supervised Cross-lingual Speech Representation Learning at Scale",
    "abstract": "This paper presents XLS-R, a large-scale model for cross-lingual speech representation learning based on wav2vec 2.0.We train models with up to 2B parameters on nearly half a million hours of publicly available speech audio in 128 languages, an order of magnitude more public data than the largest known prior work.Our evaluation covers a wide range of tasks, domains, data regimes and languages, both high and low-resource.On the CoVoST-2 speech translation benchmark, we improve the previous state of the art by an average of 7.4 BLEU over 21 translation directions into English.For speech recognition, XLS-R improves over the best known prior work on BABEL, MLS, CommonVoice as well as VoxPopuli, lowering error rates by 14-34% relative on average.XLS-R also sets a new state of the art on VoxLin-gua107 language identification.Moreover, we show that with sufficient model size, cross-lingual pretraining can perform as well as English-only pretraining when translating English speech into other languages, a setting which favors monolingual pretraining.We hope XLS-R can help to improve speech processing tasks for many more languages of the world.Models and code are available at www.github.",
    "metadata": {
      "title": "XLS-R: Self-supervised Cross-lingual Speech Representation Learning at Scale",
      "summary": "This paper presents XLS-R, a large-scale model for cross-lingual speech representation learning based on wav2vec 2.0.We train models with up to 2B parameters on nearly half a million hours of publicly available speech audio in 128 languages, an order of magnitude more public data than the largest known prior work.Our evaluation covers a wide range of tasks, domains, data regimes and languages, both high and low-resource.On the CoVoST-2 speech translation benchmark, we improve the previous state of the art by an average of 7.4 BLEU over 21 translation directions into English.For speech recognition, XLS-R improves over the best known prior work on BABEL, MLS, CommonVoice as well as VoxPopuli, lowering error rates by 14-34% relative on average.XLS-R also sets a new state of the art on VoxLin-gua107 language identification.Moreover, we show that with sufficient model size, cross-lingual pretraining can perform as well as English-only pretraining when translating English speech into other languages, a setting which favors monolingual pretraining.We hope XLS-R can help to improve speech processing tasks for many more languages of the world.Models and code are available at www.github.",
      "abstract": "This paper presents XLS-R, a large-scale model for cross-lingual speech representation learning based on wav2vec 2.0.We train models with up to 2B parameters on nearly half a million hours of publicly available speech audio in 128 languages, an order of magnitude more public data than the largest known prior work.Our evaluation covers a wide range of tasks, domains, data regimes and languages, both high and low-resource.On the CoVoST-2 speech translation benchmark, we improve the previous state of the art by an average of 7.4 BLEU over 21 translation directions into English.For speech recognition, XLS-R improves over the best known prior work on BABEL, MLS, CommonVoice as well as VoxPopuli, lowering error rates by 14-34% relative on average.XLS-R also sets a new state of the art on VoxLin-gua107 language identification.Moreover, we show that with sufficient model size, cross-lingual pretraining can perform as well as English-only pretraining when translating English speech into other languages, a setting which favors monolingual pretraining.We hope XLS-R can help to improve speech processing tasks for many more languages of the world.Models and code are available at www.github.",
      "doi": "https://doi.org/10.21437/interspeech.2022-143",
      "openalex_id": "https://openalex.org/W3213029956",
      "arxiv_id": "",
      "publication_date": "2022-09-16",
      "published": "2022-09-16",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "Although XLS-R is a large-scale cross-lingual speech representation model evaluated on speech tasks, the abstract/title do not explicitly mention Spoken Language Models or speech-language modeling terminology, failing the explicit keyword criterion; thus better to exclude.",
      "evaluation": 2
    },
    "round-A_JuniorNano_reasoning": "Although XLS-R is a large-scale cross-lingual speech representation model evaluated on speech tasks, the abstract/title do not explicitly mention Spoken Language Models or speech-language modeling terminology, failing the explicit keyword criterion; thus better to exclude.",
    "round-A_JuniorNano_evaluation": 2,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper presents XLS-R, a large-scale cross-lingual speech representation learning model focusing on speech data and achieving state-of-the-art results in speech recognition, translation, and language identification. However, the study does not explicitly mention spoken language models (SLMs) or concepts related to full-duplex interaction, cognitive parallelism, or synchronization in spoken dialogue systems as required by inclusion criteria. It primarily discusses speech representation learning for downstream tasks such as ASR and speech translation without addressing synchronous interaction or behavioral arbitration. Therefore, it does not fully meet the inclusion criteria nor does it violate exclusion criteria strongly enough to exclude outright, but lacks key aspects central to the research theme specified.",
      "evaluation": 2
    },
    "round-A_JuniorMini_reasoning": "The paper presents XLS-R, a large-scale cross-lingual speech representation learning model focusing on speech data and achieving state-of-the-art results in speech recognition, translation, and language identification. However, the study does not explicitly mention spoken language models (SLMs) or concepts related to full-duplex interaction, cognitive parallelism, or synchronization in spoken dialogue systems as required by inclusion criteria. It primarily discusses speech representation learning for downstream tasks such as ASR and speech translation without addressing synchronous interaction or behavioral arbitration. Therefore, it does not fully meet the inclusion criteria nor does it violate exclusion criteria strongly enough to exclude outright, but lacks key aspects central to the research theme specified.",
    "round-A_JuniorMini_evaluation": 2,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:2)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Word Discovery in Visually Grounded, Self-Supervised Speech Models",
    "abstract": "OursFigure 1: HuBERT: sum of attention weights each frame receives from other frames.Ours (VG-HuBERT3): attention weights each frame receives from the [CLS A] token.Attention weights from different attention heads are coded with different colors.",
    "metadata": {
      "title": "Word Discovery in Visually Grounded, Self-Supervised Speech Models",
      "summary": "OursFigure 1: HuBERT: sum of attention weights each frame receives from other frames.Ours (VG-HuBERT3): attention weights each frame receives from the [CLS A] token.Attention weights from different attention heads are coded with different colors.",
      "abstract": "OursFigure 1: HuBERT: sum of attention weights each frame receives from other frames.Ours (VG-HuBERT3): attention weights each frame receives from the [CLS A] token.Attention weights from different attention heads are coded with different colors.",
      "doi": "https://doi.org/10.21437/interspeech.2022-10652",
      "openalex_id": "https://openalex.org/W4224875474",
      "arxiv_id": "",
      "publication_date": "2022-09-16",
      "published": "2022-09-16",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The title clearly targets spoken language models (visually grounded self‑supervised speech models) within the last 3 years and implies discussion of architecture/training/evaluation, aligning with inclusion criteria and not triggering the exclusion criteria.",
      "evaluation": 4
    },
    "round-A_JuniorNano_reasoning": "The title clearly targets spoken language models (visually grounded self‑supervised speech models) within the last 3 years and implies discussion of architecture/training/evaluation, aligning with inclusion criteria and not triggering the exclusion criteria.",
    "round-A_JuniorNano_evaluation": 4,
    "round-A_JuniorMini_output": {
      "reasoning": "The title \"Word Discovery in Visually Grounded, Self-Supervised Speech Models\" implies a focus on speech models, but the abstract only describes attention weights visualization in a model named HuBERT and mentions visually grounded aspects without explicitly addressing spoken language modeling, full-duplex interactive behaviors, or evaluation criteria related to synchronization, turn-taking, or low-latency interaction. The abstract lacks details on model architecture, training or evaluation related to the inclusion criteria, and it does not mention spoken language models or equivalent concepts explicitly, nor does it cover synchronization or interaction capabilities as required. Therefore, it fails to meet the inclusion criteria and falls under exclusion criteria for lack of explicit spoken language modeling and interaction evaluation.",
      "evaluation": 2
    },
    "round-A_JuniorMini_reasoning": "The title \"Word Discovery in Visually Grounded, Self-Supervised Speech Models\" implies a focus on speech models, but the abstract only describes attention weights visualization in a model named HuBERT and mentions visually grounded aspects without explicitly addressing spoken language modeling, full-duplex interactive behaviors, or evaluation criteria related to synchronization, turn-taking, or low-latency interaction. The abstract lacks details on model architecture, training or evaluation related to the inclusion criteria, and it does not mention spoken language models or equivalent concepts explicitly, nor does it cover synchronization or interaction capabilities as required. Therefore, it fails to meet the inclusion criteria and falls under exclusion criteria for lack of explicit spoken language modeling and interaction evaluation.",
    "round-A_JuniorMini_evaluation": 2,
    "round-B_SeniorLead_output": {
      "reasoning": "Junior reviewers split: one (4) noted the title targets speech models; the other (2) pointed out the abstract contains only attention-visualization figure text (HuBERT/VG-HuBERT3) and lacks any explicit description of spoken language modeling, architecture, training, benchmarks, or the FD-SLM synchronization/interaction aspects required by the inclusion criteria. While the title mentions “Speech Models,” the provided abstract does not demonstrate that the study treats speech as the core modeling object with explicit model/training/evaluation details or addresses full-duplex/synchronization concerns, therefore it fails the requirement that the manuscript explicitly present spoken language modeling and concrete evaluation—so it should be excluded. One-sentence explanation: I score 2 because the abstract lacks sufficient methodological and evaluation detail to meet the inclusion criteria for Spoken Language Models and omits the required interaction/synchronization aspects.",
      "evaluation": 2
    },
    "round-B_SeniorLead_reasoning": "Junior reviewers split: one (4) noted the title targets speech models; the other (2) pointed out the abstract contains only attention-visualization figure text (HuBERT/VG-HuBERT3) and lacks any explicit description of spoken language modeling, architecture, training, benchmarks, or the FD-SLM synchronization/interaction aspects required by the inclusion criteria. While the title mentions “Speech Models,” the provided abstract does not demonstrate that the study treats speech as the core modeling object with explicit model/training/evaluation details or addresses full-duplex/synchronization concerns, therefore it fails the requirement that the manuscript explicitly present spoken language modeling and concrete evaluation—so it should be excluded. One-sentence explanation: I score 2 because the abstract lacks sufficient methodological and evaluation detail to meet the inclusion criteria for Spoken Language Models and omits the required interaction/synchronization aspects.",
    "round-B_SeniorLead_evaluation": 2,
    "final_verdict": "exclude (senior:2)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Billion-scale semi-supervised learning for image classification",
    "abstract": "This paper presents a study of semi-supervised learning with large convolutional networks. We propose a pipeline, based on a teacher/student paradigm, that leverages a large collection of unlabelled images (up to 1 billion). Our main goal is to improve the performance for a given target architecture, like ResNet-50 or ResNext. We provide an extensive analysis of the success factors of our approach, which leads us to formulate some recommendations to produce high-accuracy models for image classification with semi-supervised learning. As a result, our approach brings important gains to standard architectures for image, video and fine-grained classification. For instance, by leveraging one billion unlabelled images, our learned vanilla ResNet-50 achieves 81.2% top-1 accuracy on the ImageNet benchmark.",
    "metadata": {
      "title": "Billion-scale semi-supervised learning for image classification",
      "summary": "This paper presents a study of semi-supervised learning with large convolutional networks. We propose a pipeline, based on a teacher/student paradigm, that leverages a large collection of unlabelled images (up to 1 billion). Our main goal is to improve the performance for a given target architecture, like ResNet-50 or ResNext. We provide an extensive analysis of the success factors of our approach, which leads us to formulate some recommendations to produce high-accuracy models for image classification with semi-supervised learning. As a result, our approach brings important gains to standard architectures for image, video and fine-grained classification. For instance, by leveraging one billion unlabelled images, our learned vanilla ResNet-50 achieves 81.2% top-1 accuracy on the ImageNet benchmark.",
      "abstract": "This paper presents a study of semi-supervised learning with large convolutional networks. We propose a pipeline, based on a teacher/student paradigm, that leverages a large collection of unlabelled images (up to 1 billion). Our main goal is to improve the performance for a given target architecture, like ResNet-50 or ResNext. We provide an extensive analysis of the success factors of our approach, which leads us to formulate some recommendations to produce high-accuracy models for image classification with semi-supervised learning. As a result, our approach brings important gains to standard architectures for image, video and fine-grained classification. For instance, by leveraging one billion unlabelled images, our learned vanilla ResNet-50 achieves 81.2% top-1 accuracy on the ImageNet benchmark.",
      "doi": "https://doi.org/10.48550/arxiv.1905.00546",
      "openalex_id": "https://openalex.org/W2943152387",
      "arxiv_id": "",
      "publication_date": "2019-05-02",
      "published": "2019-05-02",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item studies image classification with semi-supervised learning on large CNNs; it does not address Spoken Language Models or any speech-language modeling concepts, so it fails the inclusion criteria and aligns with exclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item studies image classification with semi-supervised learning on large CNNs; it does not address Spoken Language Models or any speech-language modeling concepts, so it fails the inclusion criteria and aligns with exclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper focuses on semi-supervised learning methods for image classification using large convolutional networks, which is unrelated to the field of spoken language models or speech language modeling; it neither addresses speech as core modeling data nor discusses full-duplex spoken language interactions or synchronization aspects required by the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper focuses on semi-supervised learning methods for image classification using large convolutional networks, which is unrelated to the field of spoken language models or speech language modeling; it neither addresses speech as core modeling data nor discusses full-duplex spoken language interactions or synchronization aspects required by the inclusion criteria.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Probability of error of some adaptive pattern-recognition machines",
    "abstract": "A simple taught pattern-recognition machine for detecting an unknown, fixed, randomly occurring pattern is derived using a Bayes' approach, and its probability of error is analyzed. It is shown that with probability one, the machine converges to the optimal detector (a matched filter) for the unknown pattern, that the asymptotic decision function statistics are Gaussian, and that for an important class of problems, the central-limit theorem can be invoked to calculate the approximate probability of error at any stage of convergence. An untaught adaptive pattern-recognition machine may be made from the taught machine by using its own output instead of a teacher, and the asymptotic probability of error of this device is derived. It is shown that it does not converge to a matched filter for the unknown pattern, but that in any practical case it performs almost as well. Finally, the results of an experimental simulation of both machines are presented as curves of the relative frequency of error vs. time, and are compared with the values calculated by theory.",
    "metadata": {
      "title": "Probability of error of some adaptive pattern-recognition machines",
      "summary": "A simple taught pattern-recognition machine for detecting an unknown, fixed, randomly occurring pattern is derived using a Bayes' approach, and its probability of error is analyzed. It is shown that with probability one, the machine converges to the optimal detector (a matched filter) for the unknown pattern, that the asymptotic decision function statistics are Gaussian, and that for an important class of problems, the central-limit theorem can be invoked to calculate the approximate probability of error at any stage of convergence. An untaught adaptive pattern-recognition machine may be made from the taught machine by using its own output instead of a teacher, and the asymptotic probability of error of this device is derived. It is shown that it does not converge to a matched filter for the unknown pattern, but that in any practical case it performs almost as well. Finally, the results of an experimental simulation of both machines are presented as curves of the relative frequency of error vs. time, and are compared with the values calculated by theory.",
      "abstract": "A simple taught pattern-recognition machine for detecting an unknown, fixed, randomly occurring pattern is derived using a Bayes' approach, and its probability of error is analyzed. It is shown that with probability one, the machine converges to the optimal detector (a matched filter) for the unknown pattern, that the asymptotic decision function statistics are Gaussian, and that for an important class of problems, the central-limit theorem can be invoked to calculate the approximate probability of error at any stage of convergence. An untaught adaptive pattern-recognition machine may be made from the taught machine by using its own output instead of a teacher, and the asymptotic probability of error of this device is derived. It is shown that it does not converge to a matched filter for the unknown pattern, but that in any practical case it performs almost as well. Finally, the results of an experimental simulation of both machines are presented as curves of the relative frequency of error vs. time, and are compared with the values calculated by theory.",
      "doi": "https://doi.org/10.1109/tit.1965.1053799",
      "openalex_id": "https://openalex.org/W2111316763",
      "arxiv_id": "",
      "publication_date": "1965-07-01",
      "published": "1965-07-01",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This paper deals with general adaptive pattern-recognition theory and does not address Spoken Language Models, speech modeling, or full-duplex interactive dialogue, and it is not within the last 3 years.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "This paper deals with general adaptive pattern-recognition theory and does not address Spoken Language Models, speech modeling, or full-duplex interactive dialogue, and it is not within the last 3 years.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The title and abstract focus on adaptive pattern-recognition machines based on Bayesian detection and probability of error analysis, without any mention of spoken language models, speech language modeling, or voice-based modeling or interaction as per the inclusion criteria; the study does not address full-duplex speech interaction, speech-LLM hybrid models, cognitive parallelism, or low-latency speech communication, and it is not within the specified topical or temporal scope.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The title and abstract focus on adaptive pattern-recognition machines based on Bayesian detection and probability of error analysis, without any mention of spoken language models, speech language modeling, or voice-based modeling or interaction as per the inclusion criteria; the study does not address full-duplex speech interaction, speech-LLM hybrid models, cognitive parallelism, or low-latency speech communication, and it is not within the specified topical or temporal scope.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "A Correspondence Variational Autoencoder for Unsupervised Acoustic Word Embeddings",
    "abstract": "We propose a new unsupervised model for mapping a variable-duration speech segment to a fixed-dimensional representation. The resulting acoustic word embeddings can form the basis of search, discovery, and indexing systems for low- and zero-resource languages. Our model, which we refer to as a maximal sampling correspondence variational autoencoder (MCVAE), is a recurrent neural network (RNN) trained with a novel self-supervised correspondence loss that encourages consistency between embeddings of different instances of the same word. Our training scheme improves on previous correspondence training approaches through the use and comparison of multiple samples from the approximate posterior distribution. In the zero-resource setting, the MCVAE can be trained in an unsupervised way, without any ground-truth word pairs, by using the word-like segments discovered via an unsupervised term discovery system. In both this setting and a semi-supervised low-resource setting (with a limited set of ground-truth word pairs), the MCVAE outperforms previous state-of-the-art models, such as Siamese-, CAE- and VAE-based RNNs.",
    "metadata": {
      "title": "A Correspondence Variational Autoencoder for Unsupervised Acoustic Word Embeddings",
      "summary": "We propose a new unsupervised model for mapping a variable-duration speech segment to a fixed-dimensional representation. The resulting acoustic word embeddings can form the basis of search, discovery, and indexing systems for low- and zero-resource languages. Our model, which we refer to as a maximal sampling correspondence variational autoencoder (MCVAE), is a recurrent neural network (RNN) trained with a novel self-supervised correspondence loss that encourages consistency between embeddings of different instances of the same word. Our training scheme improves on previous correspondence training approaches through the use and comparison of multiple samples from the approximate posterior distribution. In the zero-resource setting, the MCVAE can be trained in an unsupervised way, without any ground-truth word pairs, by using the word-like segments discovered via an unsupervised term discovery system. In both this setting and a semi-supervised low-resource setting (with a limited set of ground-truth word pairs), the MCVAE outperforms previous state-of-the-art models, such as Siamese-, CAE- and VAE-based RNNs.",
      "abstract": "We propose a new unsupervised model for mapping a variable-duration speech segment to a fixed-dimensional representation. The resulting acoustic word embeddings can form the basis of search, discovery, and indexing systems for low- and zero-resource languages. Our model, which we refer to as a maximal sampling correspondence variational autoencoder (MCVAE), is a recurrent neural network (RNN) trained with a novel self-supervised correspondence loss that encourages consistency between embeddings of different instances of the same word. Our training scheme improves on previous correspondence training approaches through the use and comparison of multiple samples from the approximate posterior distribution. In the zero-resource setting, the MCVAE can be trained in an unsupervised way, without any ground-truth word pairs, by using the word-like segments discovered via an unsupervised term discovery system. In both this setting and a semi-supervised low-resource setting (with a limited set of ground-truth word pairs), the MCVAE outperforms previous state-of-the-art models, such as Siamese-, CAE- and VAE-based RNNs.",
      "doi": "https://doi.org/10.48550/arxiv.2012.02221",
      "openalex_id": "https://openalex.org/W3110761489",
      "arxiv_id": "",
      "publication_date": "2020-12-03",
      "published": "2020-12-03",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The paper treats speech as the core modeling object, presenting an unsupervised acoustic word embedding model (SLM-oriented) with architecture, training, and evaluation, published in 2025 (within 3 years). It does not focus on ASR/TTS alone or dialogue interaction, aligning with the Spoken Language Model scope.",
      "evaluation": 5
    },
    "round-A_JuniorNano_reasoning": "The paper treats speech as the core modeling object, presenting an unsupervised acoustic word embedding model (SLM-oriented) with architecture, training, and evaluation, published in 2025 (within 3 years). It does not focus on ASR/TTS alone or dialogue interaction, aligning with the Spoken Language Model scope.",
    "round-A_JuniorNano_evaluation": 5,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on unsupervised acoustic word embeddings using a variational autoencoder model for speech segments, contributing to speech representation learning; however, it does not explicitly address Spoken Language Models (SLMs), full-duplex interaction, synchronization, or interaction capabilities like turn-taking or backchanneling. It primarily targets acoustic representation for low-resource languages without discussion of end-to-end or hybrid speech-LLM architectures or interactive speech behaviors, which are required for inclusion. Therefore, it does not meet the key thematic inclusion criteria.",
      "evaluation": 2
    },
    "round-A_JuniorMini_reasoning": "The study focuses on unsupervised acoustic word embeddings using a variational autoencoder model for speech segments, contributing to speech representation learning; however, it does not explicitly address Spoken Language Models (SLMs), full-duplex interaction, synchronization, or interaction capabilities like turn-taking or backchanneling. It primarily targets acoustic representation for low-resource languages without discussion of end-to-end or hybrid speech-LLM architectures or interactive speech behaviors, which are required for inclusion. Therefore, it does not meet the key thematic inclusion criteria.",
    "round-A_JuniorMini_evaluation": 2,
    "round-B_SeniorLead_output": {
      "reasoning": "JuniorNano argued inclusion because the paper focuses on speech representation learning (acoustic word embeddings), but JuniorMini correctly noted the abstract does not mention Spoken Language Models, full‑duplex interaction, or other SLM-specific concepts. The paper describes an unsupervised VAE for acoustic word embeddings and applications like search/indexing for low-resource languages, but it does not claim or evaluate any Spoken Language Model, speech-level language modeling, or interactive/full‑duplex capabilities, nor does it include the required keywords (spoken language model / speech language model) in title or abstract. Therefore it fails the stated inclusion criteria and falls under the exclusion rules (focus on a single speech technology / representation without SLM or interaction emphasis).",
      "evaluation": 1
    },
    "round-B_SeniorLead_reasoning": "JuniorNano argued inclusion because the paper focuses on speech representation learning (acoustic word embeddings), but JuniorMini correctly noted the abstract does not mention Spoken Language Models, full‑duplex interaction, or other SLM-specific concepts. The paper describes an unsupervised VAE for acoustic word embeddings and applications like search/indexing for low-resource languages, but it does not claim or evaluate any Spoken Language Model, speech-level language modeling, or interactive/full‑duplex capabilities, nor does it include the required keywords (spoken language model / speech language model) in title or abstract. Therefore it fails the stated inclusion criteria and falls under the exclusion rules (focus on a single speech technology / representation without SLM or interaction emphasis).",
    "round-B_SeniorLead_evaluation": 1,
    "final_verdict": "exclude (senior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Unsupervised Lexicon Discovery from Acoustic Input",
    "abstract": "We present a model of unsupervised phonological lexicon discovery—the problem of simultaneously learning phoneme-like and word-like units from acoustic input. Our model builds on earlier models of unsupervised phone-like unit discovery from acoustic data (Lee and Glass, 2012), and unsupervised symbolic lexicon discovery using the Adaptor Grammar framework (Johnson et al., 2006), integrating these earlier approaches using a probabilistic model of phonological variation. We show that the model is competitive with state-of-the-art spoken term discovery systems, and present analyses exploring the model’s behavior and the kinds of linguistic structures it learns.",
    "metadata": {
      "title": "Unsupervised Lexicon Discovery from Acoustic Input",
      "summary": "We present a model of unsupervised phonological lexicon discovery—the problem of simultaneously learning phoneme-like and word-like units from acoustic input. Our model builds on earlier models of unsupervised phone-like unit discovery from acoustic data (Lee and Glass, 2012), and unsupervised symbolic lexicon discovery using the Adaptor Grammar framework (Johnson et al., 2006), integrating these earlier approaches using a probabilistic model of phonological variation. We show that the model is competitive with state-of-the-art spoken term discovery systems, and present analyses exploring the model’s behavior and the kinds of linguistic structures it learns.",
      "abstract": "We present a model of unsupervised phonological lexicon discovery—the problem of simultaneously learning phoneme-like and word-like units from acoustic input. Our model builds on earlier models of unsupervised phone-like unit discovery from acoustic data (Lee and Glass, 2012), and unsupervised symbolic lexicon discovery using the Adaptor Grammar framework (Johnson et al., 2006), integrating these earlier approaches using a probabilistic model of phonological variation. We show that the model is competitive with state-of-the-art spoken term discovery systems, and present analyses exploring the model’s behavior and the kinds of linguistic structures it learns.",
      "doi": "https://doi.org/10.1162/tacl_a_00146",
      "openalex_id": "https://openalex.org/W1778492285",
      "arxiv_id": "",
      "publication_date": "2015-12-01",
      "published": "2015-12-01",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The work does not explicitly frame itself as a Spoken Language Model nor discuss end-to-end speech–language modeling or interactive speech phenomena, and there is no clear indication it was published within the last 3 years.",
      "evaluation": 2
    },
    "round-A_JuniorNano_reasoning": "The work does not explicitly frame itself as a Spoken Language Model nor discuss end-to-end speech–language modeling or interactive speech phenomena, and there is no clear indication it was published within the last 3 years.",
    "round-A_JuniorNano_evaluation": 2,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on unsupervised phonological lexicon discovery from acoustic input, involving learning phoneme-like and word-like units from speech, which relates to modeling speech signals. However, the work does not explicitly mention spoken language models, full-duplex interaction, speech-LLM hybrid architectures, or evaluation related to synchronous interaction or cognitive parallelism; it also seems more aligned with lexicon discovery and phonological unit learning than with end-to-end spoken language modeling or interactive speech systems as defined in the inclusion criteria. Therefore, it does not clearly meet all the inclusion criteria and might partially overlap with excluded categories such as focusing on phonetic modeling rather than full spoken language modeling or real-time interaction.",
      "evaluation": 2
    },
    "round-A_JuniorMini_reasoning": "The study focuses on unsupervised phonological lexicon discovery from acoustic input, involving learning phoneme-like and word-like units from speech, which relates to modeling speech signals. However, the work does not explicitly mention spoken language models, full-duplex interaction, speech-LLM hybrid architectures, or evaluation related to synchronous interaction or cognitive parallelism; it also seems more aligned with lexicon discovery and phonological unit learning than with end-to-end spoken language modeling or interactive speech systems as defined in the inclusion criteria. Therefore, it does not clearly meet all the inclusion criteria and might partially overlap with excluded categories such as focusing on phonetic modeling rather than full spoken language modeling or real-time interaction.",
    "round-A_JuniorMini_evaluation": 2,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:2)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "w2v-BERT: Combining Contrastive Learning and Masked Language Modeling for Self-Supervised Speech Pre-Training",
    "abstract": "Motivated by the success of masked language modeling (MLM) in pre-training natural language processing models, we propose w2v-BERT that explores MLM for self-supervised speech representation learning. w2v-BERT is a framework that combines contrastive learning and MLM, where the former trains the model to discretize input continuous speech signals into a finite set of discriminative speech tokens, and the latter trains the model to learn contextualized speech representations via solving a masked prediction task consuming the discretized tokens. In contrast to existing MLM-based speech pre-training frameworks such as HuBERT, which relies on an iterative re-clustering and re-training process, or vq-wav2vec, which concatenates two separately trained modules, w2v-BERT can be optimized in an end-to-end fashion by solving the two self-supervised tasks (the contrastive task and MLM) simultaneously. Our experiments show that w2v-BERT achieves competitive results compared to current state-of-the-art pre-trained models on the LibriSpeech benchmarks when using the Libri-Light 60k corpus as the unsupervised data. In particular, when compared to published models such as conformer-based wav2vec 2.0 and HuBERT, our model shows 5% to 10% relative WER reduction on the test-clean and test-other subsets. When applied to the Google's Voice Search traffic dataset, w2v-BERT outperforms our internal conformer-based wav2vec 2.0 by more than 30% relatively.",
    "metadata": {
      "title": "w2v-BERT: Combining Contrastive Learning and Masked Language Modeling for Self-Supervised Speech Pre-Training",
      "summary": "Motivated by the success of masked language modeling (MLM) in pre-training natural language processing models, we propose w2v-BERT that explores MLM for self-supervised speech representation learning. w2v-BERT is a framework that combines contrastive learning and MLM, where the former trains the model to discretize input continuous speech signals into a finite set of discriminative speech tokens, and the latter trains the model to learn contextualized speech representations via solving a masked prediction task consuming the discretized tokens. In contrast to existing MLM-based speech pre-training frameworks such as HuBERT, which relies on an iterative re-clustering and re-training process, or vq-wav2vec, which concatenates two separately trained modules, w2v-BERT can be optimized in an end-to-end fashion by solving the two self-supervised tasks (the contrastive task and MLM) simultaneously. Our experiments show that w2v-BERT achieves competitive results compared to current state-of-the-art pre-trained models on the LibriSpeech benchmarks when using the Libri-Light 60k corpus as the unsupervised data. In particular, when compared to published models such as conformer-based wav2vec 2.0 and HuBERT, our model shows 5% to 10% relative WER reduction on the test-clean and test-other subsets. When applied to the Google's Voice Search traffic dataset, w2v-BERT outperforms our internal conformer-based wav2vec 2.0 by more than 30% relatively.",
      "abstract": "Motivated by the success of masked language modeling (MLM) in pre-training natural language processing models, we propose w2v-BERT that explores MLM for self-supervised speech representation learning. w2v-BERT is a framework that combines contrastive learning and MLM, where the former trains the model to discretize input continuous speech signals into a finite set of discriminative speech tokens, and the latter trains the model to learn contextualized speech representations via solving a masked prediction task consuming the discretized tokens. In contrast to existing MLM-based speech pre-training frameworks such as HuBERT, which relies on an iterative re-clustering and re-training process, or vq-wav2vec, which concatenates two separately trained modules, w2v-BERT can be optimized in an end-to-end fashion by solving the two self-supervised tasks (the contrastive task and MLM) simultaneously. Our experiments show that w2v-BERT achieves competitive results compared to current state-of-the-art pre-trained models on the LibriSpeech benchmarks when using the Libri-Light 60k corpus as the unsupervised data. In particular, when compared to published models such as conformer-based wav2vec 2.0 and HuBERT, our model shows 5% to 10% relative WER reduction on the test-clean and test-other subsets. When applied to the Google's Voice Search traffic dataset, w2v-BERT outperforms our internal conformer-based wav2vec 2.0 by more than 30% relatively.",
      "doi": "https://doi.org/10.1109/asru51503.2021.9688253",
      "openalex_id": "https://openalex.org/W4226033575",
      "arxiv_id": "",
      "publication_date": "2021-12-13",
      "published": "2021-12-13",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The paper targets speech-language modeling by combining contrastive learning with masked language modeling for self-supervised speech pre-training, focusing on speech as the modeling target and presenting architecture, training tasks, and evaluation on speech benchmarks; it’s within the past 3 years and goes beyond pure ASR or NLP-adjacent tasks. The abstract doesn’t explicitly use the term 'spoken language model,' but the methodology aligns with speech-language modeling concepts (language-modeling of speech) and end-to-end training across modalities.",
      "evaluation": 4
    },
    "round-A_JuniorNano_reasoning": "The paper targets speech-language modeling by combining contrastive learning with masked language modeling for self-supervised speech pre-training, focusing on speech as the modeling target and presenting architecture, training tasks, and evaluation on speech benchmarks; it’s within the past 3 years and goes beyond pure ASR or NLP-adjacent tasks. The abstract doesn’t explicitly use the term 'spoken language model,' but the methodology aligns with speech-language modeling concepts (language-modeling of speech) and end-to-end training across modalities.",
    "round-A_JuniorNano_evaluation": 4,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on self-supervised speech representation learning combining contrastive learning and masked language modeling, targeting spoken language model development at the acoustic signal level. However, it emphasizes pre-training and speech recognition performance (WER), with no explicit mention of full-duplex spoken language models, synchronous interaction capabilities (e.g., turn-taking, interruption, backchanneling), or multi-dimensional evaluation criteria related to real-time interactive behaviors. It also lacks discussions on learned synchronization or engineered synchronization, and real-time interaction benchmarks. Therefore, while the study partially overlaps with spoken language modeling, it does not fully satisfy the inclusion criteria focused on full-duplex and interactive spoken language model capabilities and evaluations, nor does it meet exclusion criteria fully but suggests incomplete scope regarding synchronous interaction and behavioral arbitration, warranting cautious evaluation.",
      "evaluation": 3
    },
    "round-A_JuniorMini_reasoning": "The study focuses on self-supervised speech representation learning combining contrastive learning and masked language modeling, targeting spoken language model development at the acoustic signal level. However, it emphasizes pre-training and speech recognition performance (WER), with no explicit mention of full-duplex spoken language models, synchronous interaction capabilities (e.g., turn-taking, interruption, backchanneling), or multi-dimensional evaluation criteria related to real-time interactive behaviors. It also lacks discussions on learned synchronization or engineered synchronization, and real-time interaction benchmarks. Therefore, while the study partially overlaps with spoken language modeling, it does not fully satisfy the inclusion criteria focused on full-duplex and interactive spoken language model capabilities and evaluations, nor does it meet exclusion criteria fully but suggests incomplete scope regarding synchronous interaction and behavioral arbitration, warranting cautious evaluation.",
    "round-A_JuniorMini_evaluation": 3,
    "round-B_SeniorLead_output": {
      "reasoning": "整合兩位初審意見：該工作確實把語音視為主要建模對象（透過對連續語音信號離散化為語音 token 並以 masked language modeling 學習語音層級的上下文表示），且在標題/摘要明確提出「language modeling of speech」等等價描述，滿足「以語音為核心的語言模型」之關鍵納入條件；不過論文實作與評估重心為自監督預訓練以提升 ASR 性能（以 WER 與 LibriSpeech / Voice Search 指標呈現），並未探討 full‑duplex 互動、同步/重疊處理、低延遲或行為仲裁等本主題強調的全雙工與即時互動面向，因此觸及排除準則（僅聚焦 ASR、僅以 WER 評估）。 評分理由（1 句）：雖屬語音層級的 language‑model 研究，但因主要定位於提升 ASR/WER 並未涵蓋全雙工/即時互動或同步行為分析，故應排除。",
      "evaluation": 2
    },
    "round-B_SeniorLead_reasoning": "整合兩位初審意見：該工作確實把語音視為主要建模對象（透過對連續語音信號離散化為語音 token 並以 masked language modeling 學習語音層級的上下文表示），且在標題/摘要明確提出「language modeling of speech」等等價描述，滿足「以語音為核心的語言模型」之關鍵納入條件；不過論文實作與評估重心為自監督預訓練以提升 ASR 性能（以 WER 與 LibriSpeech / Voice Search 指標呈現），並未探討 full‑duplex 互動、同步/重疊處理、低延遲或行為仲裁等本主題強調的全雙工與即時互動面向，因此觸及排除準則（僅聚焦 ASR、僅以 WER 評估）。 評分理由（1 句）：雖屬語音層級的 language‑model 研究，但因主要定位於提升 ASR/WER 並未涵蓋全雙工/即時互動或同步行為分析，故應排除。",
    "round-B_SeniorLead_evaluation": 2,
    "final_verdict": "exclude (senior:2)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Adam: A Method for Stochastic Optimization",
    "abstract": "We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments. The method is straightforward to implement, is computationally efficient, has little memory requirements, is invariant to diagonal rescaling of the gradients, and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. Empirical results demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. Finally, we discuss AdaMax, a variant of Adam based on the infinity norm.",
    "metadata": {
      "title": "Adam: A Method for Stochastic Optimization",
      "summary": "We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments. The method is straightforward to implement, is computationally efficient, has little memory requirements, is invariant to diagonal rescaling of the gradients, and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. Empirical results demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. Finally, we discuss AdaMax, a variant of Adam based on the infinity norm.",
      "abstract": "We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments. The method is straightforward to implement, is computationally efficient, has little memory requirements, is invariant to diagonal rescaling of the gradients, and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. Empirical results demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. Finally, we discuss AdaMax, a variant of Adam based on the infinity norm.",
      "doi": "https://doi.org/10.48550/arxiv.1412.6980",
      "openalex_id": "https://openalex.org/W1522301498",
      "arxiv_id": "",
      "publication_date": "2014-12-22",
      "published": "2014-12-22",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item describes the Adam optimizer paper, which focuses on generic stochastic optimization rather than Spoken Language Models or speech-based dialogue/synchronization, thus it does not meet the inclusion criteria and does not align with the exclusion criteria for this topic.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item describes the Adam optimizer paper, which focuses on generic stochastic optimization rather than Spoken Language Models or speech-based dialogue/synchronization, thus it does not meet the inclusion criteria and does not align with the exclusion criteria for this topic.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper introduces Adam, an optimization algorithm for stochastic objective functions, with no mention of spoken language models, speech language models, or related speech-based modeling. The focus is on optimization methods rather than end-to-end speech modeling, real-time interaction, or full-duplex speech interaction capabilities. Therefore, it does not meet the inclusion criteria requiring research on spoken language models as the core and the specific modeling aspects described.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper introduces Adam, an optimization algorithm for stochastic objective functions, with no mention of spoken language models, speech language models, or related speech-based modeling. The focus is on optimization methods rather than end-to-end speech modeling, real-time interaction, or full-duplex speech interaction capabilities. Therefore, it does not meet the inclusion criteria requiring research on spoken language models as the core and the specific modeling aspects described.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Scaling Speech Technology to 1,000+ Languages",
    "abstract": "Expanding the language coverage of speech technology has the potential to improve access to information for many more people. However, current speech technology is restricted to about one hundred languages which is a small fraction of the over 7,000 languages spoken around the world. The Massively Multilingual Speech (MMS) project increases the number of supported languages by 10-40x, depending on the task. The main ingredients are a new dataset based on readings of publicly available religious texts and effectively leveraging self-supervised learning. We built pre-trained wav2vec 2.0 models covering 1,406 languages, a single multilingual automatic speech recognition model for 1,107 languages, speech synthesis models for the same number of languages, as well as a language identification model for 4,017 languages. Experiments show that our multilingual speech recognition model more than halves the word error rate of Whisper on 54 languages of the FLEURS benchmark while being trained on a small fraction of the labeled data.",
    "metadata": {
      "title": "Scaling Speech Technology to 1,000+ Languages",
      "summary": "Expanding the language coverage of speech technology has the potential to improve access to information for many more people. However, current speech technology is restricted to about one hundred languages which is a small fraction of the over 7,000 languages spoken around the world. The Massively Multilingual Speech (MMS) project increases the number of supported languages by 10-40x, depending on the task. The main ingredients are a new dataset based on readings of publicly available religious texts and effectively leveraging self-supervised learning. We built pre-trained wav2vec 2.0 models covering 1,406 languages, a single multilingual automatic speech recognition model for 1,107 languages, speech synthesis models for the same number of languages, as well as a language identification model for 4,017 languages. Experiments show that our multilingual speech recognition model more than halves the word error rate of Whisper on 54 languages of the FLEURS benchmark while being trained on a small fraction of the labeled data.",
      "abstract": "Expanding the language coverage of speech technology has the potential to improve access to information for many more people. However, current speech technology is restricted to about one hundred languages which is a small fraction of the over 7,000 languages spoken around the world. The Massively Multilingual Speech (MMS) project increases the number of supported languages by 10-40x, depending on the task. The main ingredients are a new dataset based on readings of publicly available religious texts and effectively leveraging self-supervised learning. We built pre-trained wav2vec 2.0 models covering 1,406 languages, a single multilingual automatic speech recognition model for 1,107 languages, speech synthesis models for the same number of languages, as well as a language identification model for 4,017 languages. Experiments show that our multilingual speech recognition model more than halves the word error rate of Whisper on 54 languages of the FLEURS benchmark while being trained on a small fraction of the labeled data.",
      "doi": "https://doi.org/10.48550/arxiv.2305.13516",
      "openalex_id": "https://openalex.org/W4378105483",
      "arxiv_id": "",
      "publication_date": "2023-05-22",
      "published": "2023-05-22",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "本項目雖涉多語言語音模型與自監督學習，但摘要未出現 spoken language model(s) 或 speech language model(s) 的等價描述，也未聚焦於以語音為核心的端到端或混合式語音-文字模型的架構訓練與評估，因此不符合包含條件。",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "本項目雖涉多語言語音模型與自監督學習，但摘要未出現 spoken language model(s) 或 speech language model(s) 的等價描述，也未聚焦於以語音為核心的端到端或混合式語音-文字模型的架構訓練與評估，因此不符合包含條件。",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on massively multilingual speech technology covering over a thousand languages, primarily dealing with ASR, TTS, and language identification models. However, it does not explicitly mention spoken language models (SLMs) or full-duplex spoken language models (FD-SLMs), nor does it discuss synchronous interactive dialogue behaviors such as cognitive parallelism, speech overlap handling, or low-latency feedback which are core inclusion criteria. The evaluation metrics mentioned focus on word error rate (WER), indicative of traditional ASR tasks rather than interaction or behavioral evaluation. Therefore, it aligns more with traditional speech technology rather than the advanced spoken language modeling and interaction focus required for inclusion.",
      "evaluation": 2
    },
    "round-A_JuniorMini_reasoning": "The study focuses on massively multilingual speech technology covering over a thousand languages, primarily dealing with ASR, TTS, and language identification models. However, it does not explicitly mention spoken language models (SLMs) or full-duplex spoken language models (FD-SLMs), nor does it discuss synchronous interactive dialogue behaviors such as cognitive parallelism, speech overlap handling, or low-latency feedback which are core inclusion criteria. The evaluation metrics mentioned focus on word error rate (WER), indicative of traditional ASR tasks rather than interaction or behavioral evaluation. Therefore, it aligns more with traditional speech technology rather than the advanced spoken language modeling and interaction focus required for inclusion.",
    "round-A_JuniorMini_evaluation": 2,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:2)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Bootstrap your own latent: A new approach to self-supervised Learning",
    "abstract": "We introduce Bootstrap Your Own Latent (BYOL), a new approach to self-supervised image representation learning. BYOL relies on two neural networks, referred to as online and target networks, that interact and learn from each other. From an augmented view of an image, we train the online network to predict the target network representation of the same image under a different augmented view. At the same time, we update the target network with a slow-moving average of the online network. While state-of-the art methods rely on negative pairs, BYOL achieves a new state of the art without them. BYOL reaches $74.3\\%$ top-1 classification accuracy on ImageNet using a linear evaluation with a ResNet-50 architecture and $79.6\\%$ with a larger ResNet. We show that BYOL performs on par or better than the current state of the art on both transfer and semi-supervised benchmarks. Our implementation and pretrained models are given on GitHub.",
    "metadata": {
      "title": "Bootstrap your own latent: A new approach to self-supervised Learning",
      "summary": "We introduce Bootstrap Your Own Latent (BYOL), a new approach to self-supervised image representation learning. BYOL relies on two neural networks, referred to as online and target networks, that interact and learn from each other. From an augmented view of an image, we train the online network to predict the target network representation of the same image under a different augmented view. At the same time, we update the target network with a slow-moving average of the online network. While state-of-the art methods rely on negative pairs, BYOL achieves a new state of the art without them. BYOL reaches $74.3\\%$ top-1 classification accuracy on ImageNet using a linear evaluation with a ResNet-50 architecture and $79.6\\%$ with a larger ResNet. We show that BYOL performs on par or better than the current state of the art on both transfer and semi-supervised benchmarks. Our implementation and pretrained models are given on GitHub.",
      "abstract": "We introduce Bootstrap Your Own Latent (BYOL), a new approach to self-supervised image representation learning. BYOL relies on two neural networks, referred to as online and target networks, that interact and learn from each other. From an augmented view of an image, we train the online network to predict the target network representation of the same image under a different augmented view. At the same time, we update the target network with a slow-moving average of the online network. While state-of-the art methods rely on negative pairs, BYOL achieves a new state of the art without them. BYOL reaches $74.3\\%$ top-1 classification accuracy on ImageNet using a linear evaluation with a ResNet-50 architecture and $79.6\\%$ with a larger ResNet. We show that BYOL performs on par or better than the current state of the art on both transfer and semi-supervised benchmarks. Our implementation and pretrained models are given on GitHub.",
      "doi": "https://doi.org/10.48550/arxiv.2006.07733",
      "openalex_id": "https://openalex.org/W3035060554",
      "arxiv_id": "",
      "publication_date": "2020-06-13",
      "published": "2020-06-13",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "Not relevant to Spoken Language Models or speech-based interactions; it is a vision self-supervised learning paper (BYOL) with no speech modeling or audio evaluation.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "Not relevant to Spoken Language Models or speech-based interactions; it is a vision self-supervised learning paper (BYOL) with no speech modeling or audio evaluation.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The presented article focuses on a self-supervised method for image representation learning (BYOL) and does not address Spoken Language Models (SLMs) or Full-Duplex Spoken Language Models (FD-SLMs), nor does it involve speech or acoustic signal modeling, synchronous interaction, or evaluated behaviors related to speech modeling as required by the inclusion criteria. Thus, it fails to meet the core topic requirements and focuses on a different modality (images) rather than speech or spoken language modeling.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The presented article focuses on a self-supervised method for image representation learning (BYOL) and does not address Spoken Language Models (SLMs) or Full-Duplex Spoken Language Models (FD-SLMs), nor does it involve speech or acoustic signal modeling, synchronous interaction, or evaluated behaviors related to speech modeling as required by the inclusion criteria. Thus, it fails to meet the core topic requirements and focuses on a different modality (images) rather than speech or spoken language modeling.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "A Nonparametric Bayesian Approach to Acoustic Model Discovery",
    "abstract": "We investigate the problem of acoustic modeling in which prior language-specific knowledge and transcribed data are unavailable. We present an unsupervised model that simultaneously segments the speech, discovers a proper set of sub-word units (e.g., phones) and learns a Hidden Markov Model (HMM) for each induced acoustic unit. Our approach is formulated as a Dirichlet process mixture model in which each mixture is an HMM that represents a sub-word unit. We apply our model to the TIMIT corpus, and the results demonstrate that our model discovers sub-word units that are highly correlated with English phones and also produces better segmentation than the state-of-the-art unsupervised baseline. We test the quality of the learned acoustic models on a spoken term detection task. Compared to the baselines, our model improves the relative precision of top hits by at least 22.1 % and outperforms a language-mismatched acoustic model. 1",
    "metadata": {
      "title": "A Nonparametric Bayesian Approach to Acoustic Model Discovery",
      "summary": "We investigate the problem of acoustic modeling in which prior language-specific knowledge and transcribed data are unavailable. We present an unsupervised model that simultaneously segments the speech, discovers a proper set of sub-word units (e.g., phones) and learns a Hidden Markov Model (HMM) for each induced acoustic unit. Our approach is formulated as a Dirichlet process mixture model in which each mixture is an HMM that represents a sub-word unit. We apply our model to the TIMIT corpus, and the results demonstrate that our model discovers sub-word units that are highly correlated with English phones and also produces better segmentation than the state-of-the-art unsupervised baseline. We test the quality of the learned acoustic models on a spoken term detection task. Compared to the baselines, our model improves the relative precision of top hits by at least 22.1 % and outperforms a language-mismatched acoustic model. 1",
      "abstract": "We investigate the problem of acoustic modeling in which prior language-specific knowledge and transcribed data are unavailable. We present an unsupervised model that simultaneously segments the speech, discovers a proper set of sub-word units (e.g., phones) and learns a Hidden Markov Model (HMM) for each induced acoustic unit. Our approach is formulated as a Dirichlet process mixture model in which each mixture is an HMM that represents a sub-word unit. We apply our model to the TIMIT corpus, and the results demonstrate that our model discovers sub-word units that are highly correlated with English phones and also produces better segmentation than the state-of-the-art unsupervised baseline. We test the quality of the learned acoustic models on a spoken term detection task. Compared to the baselines, our model improves the relative precision of top hits by at least 22.1 % and outperforms a language-mismatched acoustic model. 1",
      "doi": "",
      "openalex_id": "https://openalex.org/W2100768664",
      "arxiv_id": "",
      "publication_date": "2012-07-08",
      "published": "2012-07-08",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The work focuses on unsupervised acoustic modeling and sub-word unit discovery but does not position itself as a Spoken Language Model (SLM) or discuss speech-language modeling as the core, nor does it mention SLMs in title/abstract, though it appears within the last 3 years; therefore it does not meet the inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The work focuses on unsupervised acoustic modeling and sub-word unit discovery but does not position itself as a Spoken Language Model (SLM) or discuss speech-language modeling as the core, nor does it mention SLMs in title/abstract, though it appears within the last 3 years; therefore it does not meet the inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on acoustic model discovery and unsupervised segmentation of speech at the sub-word unit level, improving spoken term detection precision; however, it does not explicitly address spoken language models (SLMs), full-duplex interaction, or core language modeling of speech, nor discuss synchronization, cognitive parallelism, or real-time interactive evaluation that the inclusion criteria specify.",
      "evaluation": 2
    },
    "round-A_JuniorMini_reasoning": "The study focuses on acoustic model discovery and unsupervised segmentation of speech at the sub-word unit level, improving spoken term detection precision; however, it does not explicitly address spoken language models (SLMs), full-duplex interaction, or core language modeling of speech, nor discuss synchronization, cognitive parallelism, or real-time interactive evaluation that the inclusion criteria specify.",
    "round-A_JuniorMini_evaluation": 2,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:2)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Self-Training With Noisy Student Improves ImageNet Classification",
    "abstract": "We present a simple self-training method that achieves 88.4% top-1 accuracy on ImageNet, which is 2.0% better than the state-of-the-art model that requires 3.5B weakly labeled Instagram images. On robustness test sets, it improves ImageNet-A top-1 accuracy from 61.0% to 83.7%, reduces ImageNet-C mean corruption error from 45.7 to 28.3, and reduces ImageNet-P mean flip rate from 27.8 to 12.2. To achieve this result, we first train an EfficientNet model on labeled ImageNet images and use it as a teacher to generate pseudo labels on 300M unlabeled images. We then train a larger EfficientNet as a student model on the combination of labeled and pseudo labeled images. We iterate this process by putting back the student as the teacher. During the generation of the pseudo labels, the teacher is not noised so that the pseudo labels are as accurate as possible. However, during the learning of the student, we inject noise such as dropout, stochastic depth and data augmentation via RandAugment to the student so that the student generalizes better than the teacher.",
    "metadata": {
      "title": "Self-Training With Noisy Student Improves ImageNet Classification",
      "summary": "We present a simple self-training method that achieves 88.4% top-1 accuracy on ImageNet, which is 2.0% better than the state-of-the-art model that requires 3.5B weakly labeled Instagram images. On robustness test sets, it improves ImageNet-A top-1 accuracy from 61.0% to 83.7%, reduces ImageNet-C mean corruption error from 45.7 to 28.3, and reduces ImageNet-P mean flip rate from 27.8 to 12.2. To achieve this result, we first train an EfficientNet model on labeled ImageNet images and use it as a teacher to generate pseudo labels on 300M unlabeled images. We then train a larger EfficientNet as a student model on the combination of labeled and pseudo labeled images. We iterate this process by putting back the student as the teacher. During the generation of the pseudo labels, the teacher is not noised so that the pseudo labels are as accurate as possible. However, during the learning of the student, we inject noise such as dropout, stochastic depth and data augmentation via RandAugment to the student so that the student generalizes better than the teacher.",
      "abstract": "We present a simple self-training method that achieves 88.4% top-1 accuracy on ImageNet, which is 2.0% better than the state-of-the-art model that requires 3.5B weakly labeled Instagram images. On robustness test sets, it improves ImageNet-A top-1 accuracy from 61.0% to 83.7%, reduces ImageNet-C mean corruption error from 45.7 to 28.3, and reduces ImageNet-P mean flip rate from 27.8 to 12.2. To achieve this result, we first train an EfficientNet model on labeled ImageNet images and use it as a teacher to generate pseudo labels on 300M unlabeled images. We then train a larger EfficientNet as a student model on the combination of labeled and pseudo labeled images. We iterate this process by putting back the student as the teacher. During the generation of the pseudo labels, the teacher is not noised so that the pseudo labels are as accurate as possible. However, during the learning of the student, we inject noise such as dropout, stochastic depth and data augmentation via RandAugment to the student so that the student generalizes better than the teacher.",
      "doi": "https://doi.org/10.1109/cvpr42600.2020.01070",
      "openalex_id": "https://openalex.org/W3035160371",
      "arxiv_id": "",
      "publication_date": "2020-06-01",
      "published": "2020-06-01",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This work centers on image classification via self-training with pseudo labels and contains no spoken language models, speech-language modeling, or evaluation of dialog/interactive speech tasks, thus it does not meet the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "This work centers on image classification via self-training with pseudo labels and contains no spoken language models, speech-language modeling, or evaluation of dialog/interactive speech tasks, thus it does not meet the inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses entirely on image classification using self-training methods on large-scale image datasets without any mention or discussion of spoken language models, speech-based modeling, full-duplex interaction, or any speech-related architectures and evaluations as required by the inclusion criteria. It does not meet the core topic of Spoken Language Models or Full-Duplex Spoken Language Models and fails to address any speech-related modeling or interaction aspects.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses entirely on image classification using self-training methods on large-scale image datasets without any mention or discussion of spoken language models, speech-based modeling, full-duplex interaction, or any speech-related architectures and evaluations as required by the inclusion criteria. It does not meet the core topic of Spoken Language Models or Full-Duplex Spoken Language Models and fails to address any speech-related modeling or interaction aspects.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Just Say No to Single Embeddings: Why Your AI Needs Multiple Perspectives",
    "abstract": "Note: This is a work in progress document This exploratory work analyzes 229 multi-agent AI dialogues byprojecting them into five different embedding spaces (transformer-based and classical) and measuring geometric properties. We find astriking dichotomy: global geometric patterns (distance matrices, tra-jectory shapes) show remarkable consistency across embeddings (corre-lations 0.52-0.96), while local features (phase boundaries) exhibit highvariability (F1: 0.08-0.36). This global consistency paired with localvariability suggests different embedding models may capture distinctprojections of conversational structure. We discuss possible interpre-tations and implications for understanding conversational dynamics inmulti-agent systems",
    "metadata": {
      "title": "Just Say No to Single Embeddings: Why Your AI Needs Multiple Perspectives",
      "summary": "Note: This is a work in progress document This exploratory work analyzes 229 multi-agent AI dialogues byprojecting them into five different embedding spaces (transformer-based and classical) and measuring geometric properties. We find astriking dichotomy: global geometric patterns (distance matrices, tra-jectory shapes) show remarkable consistency across embeddings (corre-lations 0.52-0.96), while local features (phase boundaries) exhibit highvariability (F1: 0.08-0.36). This global consistency paired with localvariability suggests different embedding models may capture distinctprojections of conversational structure. We discuss possible interpre-tations and implications for understanding conversational dynamics inmulti-agent systems",
      "abstract": "Note: This is a work in progress document This exploratory work analyzes 229 multi-agent AI dialogues byprojecting them into five different embedding spaces (transformer-based and classical) and measuring geometric properties. We find astriking dichotomy: global geometric patterns (distance matrices, tra-jectory shapes) show remarkable consistency across embeddings (corre-lations 0.52-0.96), while local features (phase boundaries) exhibit highvariability (F1: 0.08-0.36). This global consistency paired with localvariability suggests different embedding models may capture distinctprojections of conversational structure. We discuss possible interpre-tations and implications for understanding conversational dynamics inmulti-agent systems",
      "doi": "https://doi.org/10.5281/zenodo.16740292",
      "openalex_id": "https://openalex.org/W2948771346",
      "arxiv_id": "",
      "publication_date": "2025-08-04",
      "published": "2025-08-04",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item analyzes multi-agent dialogue embeddings but does not center on Spoken Language Models or speech-based modeling; it fails to satisfy the inclusion criteria and does not address the requested speech/syncing aspects.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item analyzes multi-agent dialogue embeddings but does not center on Spoken Language Models or speech-based modeling; it fails to satisfy the inclusion criteria and does not address the requested speech/syncing aspects.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study analyzes multi-agent AI dialogues using embedding spaces without focusing on Spoken Language Models (SLMs) or handling speech as a core modeling object. It lacks discussion on full-duplex spoken language models, speech-level language modeling, synchronization in speech interactions, or direct modeling of speech signals, which are required by inclusion criteria. Therefore, it does not meet the inclusion requirements related to spoken language modeling or full-duplex conversational capabilities and focuses more on text/dialog embeddings rather than speech or speech-text hybrid models.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study analyzes multi-agent AI dialogues using embedding spaces without focusing on Spoken Language Models (SLMs) or handling speech as a core modeling object. It lacks discussion on full-duplex spoken language models, speech-level language modeling, synchronization in speech interactions, or direct modeling of speech signals, which are required by inclusion criteria. Therefore, it does not meet the inclusion requirements related to spoken language modeling or full-duplex conversational capabilities and focuses more on text/dialog embeddings rather than speech or speech-text hybrid models.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Analysis of Self-Attention Head Diversity for Conformer-based Automatic Speech Recognition",
    "abstract": "Attention layers are an integral part of modern end-to-end automatic speech recognition systems, for instance as part of the Transformer or Conformer architecture.Attention is typically multi-headed, where each head has an independent set of learned parameters and operates on the same input feature sequence.The output of multi-headed attention is a fusion of the outputs from the individual heads.We empirically analyze the diversity between representations produced by the different attention heads and demonstrate that the heads become highly correlated during the course of training.We investigate a few approaches to increasing attention head diversity, including using different attention mechanisms for each head and auxiliary training loss functions to promote head diversity.We show that introducing diversity-promoting auxiliary loss functions during training is a more effective approach, and obtain WER improvements of up to 6% relative on the Librispeech corpus.Finally, we draw a connection between the diversity of attention heads and the similarity of the gradients of head parameters.",
    "metadata": {
      "title": "Analysis of Self-Attention Head Diversity for Conformer-based Automatic Speech Recognition",
      "summary": "Attention layers are an integral part of modern end-to-end automatic speech recognition systems, for instance as part of the Transformer or Conformer architecture.Attention is typically multi-headed, where each head has an independent set of learned parameters and operates on the same input feature sequence.The output of multi-headed attention is a fusion of the outputs from the individual heads.We empirically analyze the diversity between representations produced by the different attention heads and demonstrate that the heads become highly correlated during the course of training.We investigate a few approaches to increasing attention head diversity, including using different attention mechanisms for each head and auxiliary training loss functions to promote head diversity.We show that introducing diversity-promoting auxiliary loss functions during training is a more effective approach, and obtain WER improvements of up to 6% relative on the Librispeech corpus.Finally, we draw a connection between the diversity of attention heads and the similarity of the gradients of head parameters.",
      "abstract": "Attention layers are an integral part of modern end-to-end automatic speech recognition systems, for instance as part of the Transformer or Conformer architecture.Attention is typically multi-headed, where each head has an independent set of learned parameters and operates on the same input feature sequence.The output of multi-headed attention is a fusion of the outputs from the individual heads.We empirically analyze the diversity between representations produced by the different attention heads and demonstrate that the heads become highly correlated during the course of training.We investigate a few approaches to increasing attention head diversity, including using different attention mechanisms for each head and auxiliary training loss functions to promote head diversity.We show that introducing diversity-promoting auxiliary loss functions during training is a more effective approach, and obtain WER improvements of up to 6% relative on the Librispeech corpus.Finally, we draw a connection between the diversity of attention heads and the similarity of the gradients of head parameters.",
      "doi": "https://doi.org/10.21437/interspeech.2022-10560",
      "openalex_id": "https://openalex.org/W4297841458",
      "arxiv_id": "",
      "publication_date": "2022-09-16",
      "published": "2022-09-16",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "Although it analyzes Conformer-based ASR and reports WER improvements, it does not clearly position Spoken Language Models as the core focus and relies solely on WER as an evaluation metric, which triggers exclusion under the criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "Although it analyzes Conformer-based ASR and reports WER improvements, it does not clearly position Spoken Language Models as the core focus and relies solely on WER as an evaluation metric, which triggers exclusion under the criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on analysis and improvements of multi-headed attention mechanisms in Conformer-based ASR systems, which is primarily about speech recognition accuracy (WER improvement) without mentioning spoken language models, speech language modeling, full-duplex interaction, cognitive parallelism, or synchronized spoken interaction capabilities; it centers on ASR performance and does not address the core themes such as Spoken Language Models or full-duplex spoken interaction stipulated in the inclusion criteria, nor does it consider interaction or behavioral arbitration; hence it fails inclusion criteria related to the research scope and evaluation focus.",
      "evaluation": 2
    },
    "round-A_JuniorMini_reasoning": "The study focuses on analysis and improvements of multi-headed attention mechanisms in Conformer-based ASR systems, which is primarily about speech recognition accuracy (WER improvement) without mentioning spoken language models, speech language modeling, full-duplex interaction, cognitive parallelism, or synchronized spoken interaction capabilities; it centers on ASR performance and does not address the core themes such as Spoken Language Models or full-duplex spoken interaction stipulated in the inclusion criteria, nor does it consider interaction or behavioral arbitration; hence it fails inclusion criteria related to the research scope and evaluation focus.",
    "round-A_JuniorMini_evaluation": 2,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:2)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "How Much Does Attention Actually Attend? Questioning the Importance of Attention in Pretrained Transformers",
    "abstract": "The attention mechanism is considered the backbone of the widely-used Transformer architecture. It contextualizes the input by computing input-specific attention matrices. We find that this mechanism, while powerful and elegant, is not as important as typically thought for pretrained language models. We introduce PAPA, a new probing method that replaces the input-dependent attention matrices with constant ones—the average attention weights over multiple inputs. We use PAPA to analyze several established pretrained Transformers on six downstream tasks. We find that without any input-dependent attention, all models achieve competitive performance—an average relative drop of only 8% from the probing baseline. Further, little or no performance drop is observed when replacing half of the input-dependent attention matrices with constant (input-independent) ones. Interestingly, we show that better-performing models lose more from applying our method than weaker models, suggesting that the utilization of the input-dependent attention mechanism might be a factor in their success. Our results motivate research on simpler alternatives to input-dependent attention, as well as on methods for better utilization of this mechanism in the Transformer architecture.",
    "metadata": {
      "title": "How Much Does Attention Actually Attend? Questioning the Importance of Attention in Pretrained Transformers",
      "summary": "The attention mechanism is considered the backbone of the widely-used Transformer architecture. It contextualizes the input by computing input-specific attention matrices. We find that this mechanism, while powerful and elegant, is not as important as typically thought for pretrained language models. We introduce PAPA, a new probing method that replaces the input-dependent attention matrices with constant ones—the average attention weights over multiple inputs. We use PAPA to analyze several established pretrained Transformers on six downstream tasks. We find that without any input-dependent attention, all models achieve competitive performance—an average relative drop of only 8% from the probing baseline. Further, little or no performance drop is observed when replacing half of the input-dependent attention matrices with constant (input-independent) ones. Interestingly, we show that better-performing models lose more from applying our method than weaker models, suggesting that the utilization of the input-dependent attention mechanism might be a factor in their success. Our results motivate research on simpler alternatives to input-dependent attention, as well as on methods for better utilization of this mechanism in the Transformer architecture.",
      "abstract": "The attention mechanism is considered the backbone of the widely-used Transformer architecture. It contextualizes the input by computing input-specific attention matrices. We find that this mechanism, while powerful and elegant, is not as important as typically thought for pretrained language models. We introduce PAPA, a new probing method that replaces the input-dependent attention matrices with constant ones—the average attention weights over multiple inputs. We use PAPA to analyze several established pretrained Transformers on six downstream tasks. We find that without any input-dependent attention, all models achieve competitive performance—an average relative drop of only 8% from the probing baseline. Further, little or no performance drop is observed when replacing half of the input-dependent attention matrices with constant (input-independent) ones. Interestingly, we show that better-performing models lose more from applying our method than weaker models, suggesting that the utilization of the input-dependent attention mechanism might be a factor in their success. Our results motivate research on simpler alternatives to input-dependent attention, as well as on methods for better utilization of this mechanism in the Transformer architecture.",
      "doi": "https://doi.org/10.18653/v1/2022.findings-emnlp.101",
      "openalex_id": "https://openalex.org/W4385574383",
      "arxiv_id": "",
      "publication_date": "2022-01-01",
      "published": "2022-01-01",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item analyzes attention in pretrained language models for text, not Spoken Language Models or speech-centric architectures, and contains no speech modeling, evaluation, or dialogue/synchronization aspects.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item analyzes attention in pretrained language models for text, not Spoken Language Models or speech-centric architectures, and contains no speech modeling, evaluation, or dialogue/synchronization aspects.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper focuses on pretrained Transformer language models and their attention mechanism without addressing spoken language models, speech data, or full-duplex spoken language interactions; it does not discuss speech as a core modeling object nor synchronization aspects central to the inclusion criteria, thus not meeting the scope of spoken language modeling or full-duplex interaction as required.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper focuses on pretrained Transformer language models and their attention mechanism without addressing spoken language models, speech data, or full-duplex spoken language interactions; it does not discuss speech as a core modeling object nor synchronization aspects central to the inclusion criteria, thus not meeting the scope of spoken language modeling or full-duplex interaction as required.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Explaining by Removing: A Unified Framework for Model Explanation",
    "abstract": "Researchers have proposed a wide variety of model explanation approaches, but it remains unclear how most methods are related or when one method is preferable to another. We describe a new unified class of methods, removal-based explanations, that are based on the principle of simulating feature removal to quantify each feature's influence. These methods vary in several respects, so we develop a framework that characterizes each method along three dimensions: 1) how the method removes features, 2) what model behavior the method explains, and 3) how the method summarizes each feature's influence. Our framework unifies 26 existing methods, including several of the most widely used approaches: SHAP, LIME, Meaningful Perturbations, and permutation tests. This newly understood class of explanation methods has rich connections that we examine using tools that have been largely overlooked by the explainability literature. To anchor removal-based explanations in cognitive psychology, we show that feature removal is a simple application of subtractive counterfactual reasoning. Ideas from cooperative game theory shed light on the relationships and trade-offs among different methods, and we derive conditions under which all removal-based explanations have information-theoretic interpretations. Through this analysis, we develop a unified framework that helps practitioners better understand model explanation tools, and that offers a strong theoretical foundation upon which future explainability research can build.",
    "metadata": {
      "title": "Explaining by Removing: A Unified Framework for Model Explanation",
      "summary": "Researchers have proposed a wide variety of model explanation approaches, but it remains unclear how most methods are related or when one method is preferable to another. We describe a new unified class of methods, removal-based explanations, that are based on the principle of simulating feature removal to quantify each feature's influence. These methods vary in several respects, so we develop a framework that characterizes each method along three dimensions: 1) how the method removes features, 2) what model behavior the method explains, and 3) how the method summarizes each feature's influence. Our framework unifies 26 existing methods, including several of the most widely used approaches: SHAP, LIME, Meaningful Perturbations, and permutation tests. This newly understood class of explanation methods has rich connections that we examine using tools that have been largely overlooked by the explainability literature. To anchor removal-based explanations in cognitive psychology, we show that feature removal is a simple application of subtractive counterfactual reasoning. Ideas from cooperative game theory shed light on the relationships and trade-offs among different methods, and we derive conditions under which all removal-based explanations have information-theoretic interpretations. Through this analysis, we develop a unified framework that helps practitioners better understand model explanation tools, and that offers a strong theoretical foundation upon which future explainability research can build.",
      "abstract": "Researchers have proposed a wide variety of model explanation approaches, but it remains unclear how most methods are related or when one method is preferable to another. We describe a new unified class of methods, removal-based explanations, that are based on the principle of simulating feature removal to quantify each feature's influence. These methods vary in several respects, so we develop a framework that characterizes each method along three dimensions: 1) how the method removes features, 2) what model behavior the method explains, and 3) how the method summarizes each feature's influence. Our framework unifies 26 existing methods, including several of the most widely used approaches: SHAP, LIME, Meaningful Perturbations, and permutation tests. This newly understood class of explanation methods has rich connections that we examine using tools that have been largely overlooked by the explainability literature. To anchor removal-based explanations in cognitive psychology, we show that feature removal is a simple application of subtractive counterfactual reasoning. Ideas from cooperative game theory shed light on the relationships and trade-offs among different methods, and we derive conditions under which all removal-based explanations have information-theoretic interpretations. Through this analysis, we develop a unified framework that helps practitioners better understand model explanation tools, and that offers a strong theoretical foundation upon which future explainability research can build.",
      "doi": "https://doi.org/10.48550/arxiv.2011.14878",
      "openalex_id": "https://openalex.org/W3107600318",
      "arxiv_id": "",
      "publication_date": "2020-11-21",
      "published": "2020-11-21",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item discusses removal-based explanations for general ML models and does not address Spoken Language Models or speech-language modeling, so it fails the core inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item discusses removal-based explanations for general ML models and does not address Spoken Language Models or speech-language modeling, so it fails the core inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The title and abstract focus on model explanation methods and their theoretical unification without any mention of spoken language models, speech acoustic signal modeling, full-duplex interactive capabilities, or related evaluation criteria specified in the inclusion criteria; thus, it falls outside the thematic scope required for inclusion.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The title and abstract focus on model explanation methods and their theoretical unification without any mention of spoken language models, speech acoustic signal modeling, full-duplex interactive capabilities, or related evaluation criteria specified in the inclusion criteria; thus, it falls outside the thematic scope required for inclusion.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Putting Natural in Natural Language Processing",
    "abstract": "Human language is firstly spoken and only secondarily written. Text, however, is a very convenient and efficient representation oflanguage, and modern civilization has made it ubiquitous. Thus the field of NLP has overwhelmingly focused on processing written ratherthan spoken language. Work on spoken language, on the other hand, has been siloed off within the largely separate speech processingcommunity which has been inordinately preoccupied with transcribing speech into text. Recent advances in deep learning have led to afortuitous convergence in methods between speech processing and mainstream NLP. Arguably, the time is ripe for a unification of thesetwo fields, and for starting to take spoken language seriously as the primary mode of human communication. Truly natural language processingcould lead to better integration with the rest of language science and could lead to systems which are more data-efficient and morehuman-like, and which can communicate beyond the textual modality.",
    "metadata": {
      "title": "Putting Natural in Natural Language Processing",
      "summary": "Human language is firstly spoken and only secondarily written. Text, however, is a very convenient and efficient representation oflanguage, and modern civilization has made it ubiquitous. Thus the field of NLP has overwhelmingly focused on processing written ratherthan spoken language. Work on spoken language, on the other hand, has been siloed off within the largely separate speech processingcommunity which has been inordinately preoccupied with transcribing speech into text. Recent advances in deep learning have led to afortuitous convergence in methods between speech processing and mainstream NLP. Arguably, the time is ripe for a unification of thesetwo fields, and for starting to take spoken language seriously as the primary mode of human communication. Truly natural language processingcould lead to better integration with the rest of language science and could lead to systems which are more data-efficient and morehuman-like, and which can communicate beyond the textual modality.",
      "abstract": "Human language is firstly spoken and only secondarily written. Text, however, is a very convenient and efficient representation oflanguage, and modern civilization has made it ubiquitous. Thus the field of NLP has overwhelmingly focused on processing written ratherthan spoken language. Work on spoken language, on the other hand, has been siloed off within the largely separate speech processingcommunity which has been inordinately preoccupied with transcribing speech into text. Recent advances in deep learning have led to afortuitous convergence in methods between speech processing and mainstream NLP. Arguably, the time is ripe for a unification of thesetwo fields, and for starting to take spoken language seriously as the primary mode of human communication. Truly natural language processingcould lead to better integration with the rest of language science and could lead to systems which are more data-efficient and morehuman-like, and which can communicate beyond the textual modality.",
      "doi": "https://doi.org/10.18653/v1/2023.findings-acl.495",
      "openalex_id": "https://openalex.org/W4385570829",
      "arxiv_id": "",
      "publication_date": "2023-01-01",
      "published": "2023-01-01",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "Although the piece discusses spoken language in NLP, it does not frame Spoken Language Models (SLMs) or FD-SLMs as the core focus nor specify architecture/training/evaluation toward speech-language modeling, so it should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "Although the piece discusses spoken language in NLP, it does not frame Spoken Language Models (SLMs) or FD-SLMs as the core focus nor specify architecture/training/evaluation toward speech-language modeling, so it should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The abstract discusses the convergence of speech processing and NLP, emphasizing spoken language as the primary mode of communication and advocating for more integrated, natural language processing systems. However, it lacks explicit mention of spoken language models, full-duplex interaction, or detailed model architectures, training, or evaluations targeting spoken language modeling per the inclusion criteria. It also does not address specific advanced features such as cognitive parallelism, low-latency interaction, or behavioral arbitration. Therefore, while relevant in theme, it does not fully meet the strict inclusion criteria for modeling and evaluation of spoken language models with focus on speech sequences and interactive dynamics.",
      "evaluation": 2
    },
    "round-A_JuniorMini_reasoning": "The abstract discusses the convergence of speech processing and NLP, emphasizing spoken language as the primary mode of communication and advocating for more integrated, natural language processing systems. However, it lacks explicit mention of spoken language models, full-duplex interaction, or detailed model architectures, training, or evaluations targeting spoken language modeling per the inclusion criteria. It also does not address specific advanced features such as cognitive parallelism, low-latency interaction, or behavioral arbitration. Therefore, while relevant in theme, it does not fully meet the strict inclusion criteria for modeling and evaluation of spoken language models with focus on speech sequences and interactive dynamics.",
    "round-A_JuniorMini_evaluation": 2,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:2)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Measuring the Mixing of Contextual Information in the Transformer",
    "abstract": "The Transformer architecture aggregates input information through the self-attention mechanism, but there is no clear understanding of how this information is mixed across the entire model. Additionally, recent works have demonstrated that attention weights alone are not enough to describe the flow of information. In this paper, we consider the whole attention block --multi-head attention, residual connection, and layer normalization-- and define a metric to measure token-to-token interactions within each layer. Then, we aggregate layer-wise interpretations to provide input attribution scores for model predictions. Experimentally, we show that our method, ALTI (Aggregation of Layer-wise Token-to-token Interactions), provides more faithful explanations and increased robustness than gradient-based methods.",
    "metadata": {
      "title": "Measuring the Mixing of Contextual Information in the Transformer",
      "summary": "The Transformer architecture aggregates input information through the self-attention mechanism, but there is no clear understanding of how this information is mixed across the entire model. Additionally, recent works have demonstrated that attention weights alone are not enough to describe the flow of information. In this paper, we consider the whole attention block --multi-head attention, residual connection, and layer normalization-- and define a metric to measure token-to-token interactions within each layer. Then, we aggregate layer-wise interpretations to provide input attribution scores for model predictions. Experimentally, we show that our method, ALTI (Aggregation of Layer-wise Token-to-token Interactions), provides more faithful explanations and increased robustness than gradient-based methods.",
      "abstract": "The Transformer architecture aggregates input information through the self-attention mechanism, but there is no clear understanding of how this information is mixed across the entire model. Additionally, recent works have demonstrated that attention weights alone are not enough to describe the flow of information. In this paper, we consider the whole attention block --multi-head attention, residual connection, and layer normalization-- and define a metric to measure token-to-token interactions within each layer. Then, we aggregate layer-wise interpretations to provide input attribution scores for model predictions. Experimentally, we show that our method, ALTI (Aggregation of Layer-wise Token-to-token Interactions), provides more faithful explanations and increased robustness than gradient-based methods.",
      "doi": "https://doi.org/10.18653/v1/2022.emnlp-main.595",
      "openalex_id": "https://openalex.org/W4385574263",
      "arxiv_id": "",
      "publication_date": "2022-01-01",
      "published": "2022-01-01",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The paper analyzes Transformer attention and token-to-token interactions for explanations in a general NLP setting, not centered on Spoken Language Models or speech-language modeling; thus it does not meet the inclusion criteria (and there are no explicit exclusion criteria that would apply here).",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The paper analyzes Transformer attention and token-to-token interactions for explanations in a general NLP setting, not centered on Spoken Language Models or speech-language modeling; thus it does not meet the inclusion criteria (and there are no explicit exclusion criteria that would apply here).",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper focuses on Transformer architectures and measuring information flow inside attention mechanisms but does not mention any spoken language models, speech language models, or related concepts involving speech as core inputs or full-duplex spoken interaction. It does not cover speech-based modeling, end-to-end or hybrid Speech-LLM architectures, or synchronous interaction capabilities, which are key inclusion criteria. Hence, it fails to meet the domain specificity of spoken language or speech-based language modeling required.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper focuses on Transformer architectures and measuring information flow inside attention mechanisms but does not mention any spoken language models, speech language models, or related concepts involving speech as core inputs or full-duplex spoken interaction. It does not cover speech-based modeling, end-to-end or hybrid Speech-LLM architectures, or synchronous interaction capabilities, which are key inclusion criteria. Hence, it fails to meet the domain specificity of spoken language or speech-based language modeling required.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "GlobEnc: Quantifying Global Token Attribution by Incorporating the Whole Encoder Layer in Transformers",
    "abstract": "Ali Modarressi, Mohsen Fayyaz, Yadollah Yaghoobzadeh, Mohammad Taher Pilehvar. Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. 2022.",
    "metadata": {
      "title": "GlobEnc: Quantifying Global Token Attribution by Incorporating the Whole Encoder Layer in Transformers",
      "summary": "Ali Modarressi, Mohsen Fayyaz, Yadollah Yaghoobzadeh, Mohammad Taher Pilehvar. Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. 2022.",
      "abstract": "Ali Modarressi, Mohsen Fayyaz, Yadollah Yaghoobzadeh, Mohammad Taher Pilehvar. Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. 2022.",
      "doi": "https://doi.org/10.18653/v1/2022.naacl-main.19",
      "openalex_id": "https://openalex.org/W4287887174",
      "arxiv_id": "",
      "publication_date": "2022-01-01",
      "published": "2022-01-01",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This item centers on global token attribution in transformer models for NLP and does not address Spoken Language Models or any speech-language modeling, dialogue interaction, or speech-focused evaluation.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "This item centers on global token attribution in transformer models for NLP and does not address Spoken Language Models or any speech-language modeling, dialogue interaction, or speech-focused evaluation.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The title and abstract do not explicitly mention spoken language models, speech language models, or related descriptions involving speech as the primary modeling focus; instead, it focuses on transformer encoder layers, likely in a text or token context, without indication of speech or full-duplex spoken language interaction, thus failing the inclusion criteria and likely falling under exclusion.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The title and abstract do not explicitly mention spoken language models, speech language models, or related descriptions involving speech as the primary modeling focus; instead, it focuses on transformer encoder layers, likely in a text or token context, without indication of speech or full-duplex spoken language interaction, thus failing the inclusion criteria and likely falling under exclusion.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Quantifying Context Mixing in Transformers",
    "abstract": "Self-attention weights and their transformed variants have been the main source of information for analyzing token-to-token interactions in Transformer-based models. But despite their ease of interpretation, these weights are not faithful to the models’ decisions as they are only one part of an encoder, and other components in the encoder layer can have considerable impact on information mixing in the output representations. In this work, by expanding the scope of analysis to the whole encoder block, we propose Value Zeroing, a novel context mixing score customized for Transformers that provides us with a deeper understanding of how information is mixed at each encoder layer. We demonstrate the superiority of our context mixing score over other analysis methods through a series of complementary evaluations with different viewpoints based on linguistically informed rationales, probing, and faithfulness analysis.",
    "metadata": {
      "title": "Quantifying Context Mixing in Transformers",
      "summary": "Self-attention weights and their transformed variants have been the main source of information for analyzing token-to-token interactions in Transformer-based models. But despite their ease of interpretation, these weights are not faithful to the models’ decisions as they are only one part of an encoder, and other components in the encoder layer can have considerable impact on information mixing in the output representations. In this work, by expanding the scope of analysis to the whole encoder block, we propose Value Zeroing, a novel context mixing score customized for Transformers that provides us with a deeper understanding of how information is mixed at each encoder layer. We demonstrate the superiority of our context mixing score over other analysis methods through a series of complementary evaluations with different viewpoints based on linguistically informed rationales, probing, and faithfulness analysis.",
      "abstract": "Self-attention weights and their transformed variants have been the main source of information for analyzing token-to-token interactions in Transformer-based models. But despite their ease of interpretation, these weights are not faithful to the models’ decisions as they are only one part of an encoder, and other components in the encoder layer can have considerable impact on information mixing in the output representations. In this work, by expanding the scope of analysis to the whole encoder block, we propose Value Zeroing, a novel context mixing score customized for Transformers that provides us with a deeper understanding of how information is mixed at each encoder layer. We demonstrate the superiority of our context mixing score over other analysis methods through a series of complementary evaluations with different viewpoints based on linguistically informed rationales, probing, and faithfulness analysis.",
      "doi": "https://doi.org/10.18653/v1/2023.eacl-main.245",
      "openalex_id": "https://openalex.org/W4386566794",
      "arxiv_id": "",
      "publication_date": "2023-01-01",
      "published": "2023-01-01",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item discusses general Transformer context mixing and self-attention analysis without focusing on Spoken Language Models or speech-centric architectures, thus it fails the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item discusses general Transformer context mixing and self-attention analysis without focusing on Spoken Language Models or speech-centric architectures, thus it fails the inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on analyzing context mixing within Transformer encoder layers, a topic centered on model architecture interpretation and token interactions, with no indication of speech or spoken language modeling, nor any mention of full-duplex spoken language interactions or speech signal processing as a core modeling concept. It lacks reference to spoken language models, speech language models, or speech-specific language modeling, and does not address synchronization, full-duplex interactive behavior, or evaluation metrics related to spoken dialogue or audio signal processing, thus failing to meet the inclusion criteria and falling under exclusions due to focus on text modeling and lack of speech-related modeling aspects.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on analyzing context mixing within Transformer encoder layers, a topic centered on model architecture interpretation and token interactions, with no indication of speech or spoken language modeling, nor any mention of full-duplex spoken language interactions or speech signal processing as a core modeling concept. It lacks reference to spoken language models, speech language models, or speech-specific language modeling, and does not address synchronization, full-duplex interactive behavior, or evaluation metrics related to spoken dialogue or audio signal processing, thus failing to meet the inclusion criteria and falling under exclusions due to focus on text modeling and lack of speech-related modeling aspects.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Attention is Not Only a Weight: Analyzing Transformers with Vector Norms",
    "abstract": "Attention is a key component of Transformers, which have recently achieved considerable success in natural language processing. Hence, attention is being extensively studied to investigate various linguistic capabilities of Transformers, focusing on analyzing the parallels between attention weights and specific linguistic phenomena. This paper shows that attention weights alone are only one of the two factors that determine the output of attention and proposes a norm-based analysis that incorporates the second factor, the norm of the transformed input vectors. The findings of our norm-based analyses of BERT and a Transformer-based neural machine translation system include the following: (i) contrary to previous studies, BERT pays poor attention to special tokens, and (ii) reasonable word alignment can be extracted from attention mechanisms of Transformer. These findings provide insights into the inner workings of Transformers.",
    "metadata": {
      "title": "Attention is Not Only a Weight: Analyzing Transformers with Vector Norms",
      "summary": "Attention is a key component of Transformers, which have recently achieved considerable success in natural language processing. Hence, attention is being extensively studied to investigate various linguistic capabilities of Transformers, focusing on analyzing the parallels between attention weights and specific linguistic phenomena. This paper shows that attention weights alone are only one of the two factors that determine the output of attention and proposes a norm-based analysis that incorporates the second factor, the norm of the transformed input vectors. The findings of our norm-based analyses of BERT and a Transformer-based neural machine translation system include the following: (i) contrary to previous studies, BERT pays poor attention to special tokens, and (ii) reasonable word alignment can be extracted from attention mechanisms of Transformer. These findings provide insights into the inner workings of Transformers.",
      "abstract": "Attention is a key component of Transformers, which have recently achieved considerable success in natural language processing. Hence, attention is being extensively studied to investigate various linguistic capabilities of Transformers, focusing on analyzing the parallels between attention weights and specific linguistic phenomena. This paper shows that attention weights alone are only one of the two factors that determine the output of attention and proposes a norm-based analysis that incorporates the second factor, the norm of the transformed input vectors. The findings of our norm-based analyses of BERT and a Transformer-based neural machine translation system include the following: (i) contrary to previous studies, BERT pays poor attention to special tokens, and (ii) reasonable word alignment can be extracted from attention mechanisms of Transformer. These findings provide insights into the inner workings of Transformers.",
      "doi": "https://doi.org/10.18653/v1/2020.emnlp-main.574",
      "openalex_id": "https://openalex.org/W3099143320",
      "arxiv_id": "",
      "publication_date": "2020-01-01",
      "published": "2020-01-01",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item analyzes Transformer attention with norm-based analysis for NLP (not spoken language modeling or speech-centric architectures), and it does not address turn-taking, real-time interaction, or speech-specific evaluation frameworks required by the inclusion criteria; thus it does not meet the inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item analyzes Transformer attention with norm-based analysis for NLP (not spoken language modeling or speech-centric architectures), and it does not address turn-taking, real-time interaction, or speech-specific evaluation frameworks required by the inclusion criteria; thus it does not meet the inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper focuses on analyzing attention mechanisms within Transformers applied to NLP tasks, with no indication that it involves spoken language models or speech as the core modeling object, nor does it address full-duplex spoken language interaction or any related synchronization, low latency, or dialogue behavior arbitration aspects specified in the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper focuses on analyzing attention mechanisms within Transformers applied to NLP tasks, with no indication that it involves spoken language models or speech as the core modeling object, nor does it address full-duplex spoken language interaction or any related synchronization, low latency, or dialogue behavior arbitration aspects specified in the inclusion criteria.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Incorporating Residual and Normalization Layers into Analysis of Masked Language Models",
    "abstract": "Transformer architecture has become ubiquitous in the natural language processing field. To interpret the Transformer-based models, their attention patterns have been extensively analyzed. However, the Transformer architecture is not only composed of the multi-head attention; other components can also contribute to Transformers’ progressive performance. In this study, we extended the scope of the analysis of Transformers from solely the attention patterns to the whole attention block, i.e., multi-head attention, residual connection, and layer normalization. Our analysis of Transformer-based masked language models shows that the token-to-token interaction performed via attention has less impact on the intermediate representations than previously assumed. These results provide new intuitive explanations of existing reports; for example, discarding the learned attention patterns tends not to adversely affect the performance. The codes of our experiments are publicly available.",
    "metadata": {
      "title": "Incorporating Residual and Normalization Layers into Analysis of Masked Language Models",
      "summary": "Transformer architecture has become ubiquitous in the natural language processing field. To interpret the Transformer-based models, their attention patterns have been extensively analyzed. However, the Transformer architecture is not only composed of the multi-head attention; other components can also contribute to Transformers’ progressive performance. In this study, we extended the scope of the analysis of Transformers from solely the attention patterns to the whole attention block, i.e., multi-head attention, residual connection, and layer normalization. Our analysis of Transformer-based masked language models shows that the token-to-token interaction performed via attention has less impact on the intermediate representations than previously assumed. These results provide new intuitive explanations of existing reports; for example, discarding the learned attention patterns tends not to adversely affect the performance. The codes of our experiments are publicly available.",
      "abstract": "Transformer architecture has become ubiquitous in the natural language processing field. To interpret the Transformer-based models, their attention patterns have been extensively analyzed. However, the Transformer architecture is not only composed of the multi-head attention; other components can also contribute to Transformers’ progressive performance. In this study, we extended the scope of the analysis of Transformers from solely the attention patterns to the whole attention block, i.e., multi-head attention, residual connection, and layer normalization. Our analysis of Transformer-based masked language models shows that the token-to-token interaction performed via attention has less impact on the intermediate representations than previously assumed. These results provide new intuitive explanations of existing reports; for example, discarding the learned attention patterns tends not to adversely affect the performance. The codes of our experiments are publicly available.",
      "doi": "https://doi.org/10.18653/v1/2021.emnlp-main.373",
      "openalex_id": "https://openalex.org/W3200704197",
      "arxiv_id": "",
      "publication_date": "2021-01-01",
      "published": "2021-01-01",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The study analyzes Transformer-based textual masked language models and their architectural components (attention, residual connections, layer normalization) without involving Spoken Language Models or speech data, hence it does not meet the core domain and evaluation criteria for spoken-language modeling and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The study analyzes Transformer-based textual masked language models and their architectural components (attention, residual connections, layer normalization) without involving Spoken Language Models or speech data, hence it does not meet the core domain and evaluation criteria for spoken-language modeling and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on analyzing Transformer-based masked language models at a component level, primarily concerning text-based modeling; it neither centers on spoken language models nor treats speech data as the core modeling object, thus not meeting the inclusion criteria focused on spoken and full-duplex language models with speech-centric modeling and interaction features.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on analyzing Transformer-based masked language models at a component level, primarily concerning text-based modeling; it neither centers on spoken language models nor treats speech data as the core modeling object, thus not meeting the inclusion criteria focused on spoken and full-duplex language models with speech-centric modeling and interaction features.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Conformer: Convolution-augmented Transformer for Speech Recognition",
    "abstract": "Recently Transformer and Convolution neural network (CNN) based models have shown promising results in Automatic Speech Recognition (ASR), outperforming Recurrent neural networks (RNNs).Transformer models are good at capturing content-based global interactions, while CNNs exploit local features effectively.In this work, we achieve the best of both worlds by studying how to combine convolution neural networks and transformers to model both local and global dependencies of an audio sequence in a parameter-efficient way.To this regard, we propose the convolution-augmented transformer for speech recognition, named Conformer.Conformer significantly outperforms the previous Transformer and CNN based models achieving state-of-the-art accuracies.On the widely used LibriSpeech benchmark, our model achieves WER of 2.1%/4.3%without using a language model and 1.9%/3.9%with an external language model on test/testother.We also observe competitive performance of 2.7%/6.3%with a small model of only 10M parameters.",
    "metadata": {
      "title": "Conformer: Convolution-augmented Transformer for Speech Recognition",
      "summary": "Recently Transformer and Convolution neural network (CNN) based models have shown promising results in Automatic Speech Recognition (ASR), outperforming Recurrent neural networks (RNNs).Transformer models are good at capturing content-based global interactions, while CNNs exploit local features effectively.In this work, we achieve the best of both worlds by studying how to combine convolution neural networks and transformers to model both local and global dependencies of an audio sequence in a parameter-efficient way.To this regard, we propose the convolution-augmented transformer for speech recognition, named Conformer.Conformer significantly outperforms the previous Transformer and CNN based models achieving state-of-the-art accuracies.On the widely used LibriSpeech benchmark, our model achieves WER of 2.1%/4.3%without using a language model and 1.9%/3.9%with an external language model on test/testother.We also observe competitive performance of 2.7%/6.3%with a small model of only 10M parameters.",
      "abstract": "Recently Transformer and Convolution neural network (CNN) based models have shown promising results in Automatic Speech Recognition (ASR), outperforming Recurrent neural networks (RNNs).Transformer models are good at capturing content-based global interactions, while CNNs exploit local features effectively.In this work, we achieve the best of both worlds by studying how to combine convolution neural networks and transformers to model both local and global dependencies of an audio sequence in a parameter-efficient way.To this regard, we propose the convolution-augmented transformer for speech recognition, named Conformer.Conformer significantly outperforms the previous Transformer and CNN based models achieving state-of-the-art accuracies.On the widely used LibriSpeech benchmark, our model achieves WER of 2.1%/4.3%without using a language model and 1.9%/3.9%with an external language model on test/testother.We also observe competitive performance of 2.7%/6.3%with a small model of only 10M parameters.",
      "doi": "https://doi.org/10.21437/interspeech.2020-3015",
      "openalex_id": "https://openalex.org/W3097777922",
      "arxiv_id": "",
      "publication_date": "2020-10-25",
      "published": "2020-10-25",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This item is an ASR architecture paper (Conformer) from 2020 and not within the last 3 years, and it does not center on Spoken Language Models or full-duplex spoken language modeling; instead it focuses on a speech recognition model without addressing dialogue, interaction, or speech-language modeling as defined by the inclusions.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "This item is an ASR architecture paper (Conformer) from 2020 and not within the last 3 years, and it does not center on Spoken Language Models or full-duplex spoken language modeling; instead it focuses on a speech recognition model without addressing dialogue, interaction, or speech-language modeling as defined by the inclusions.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on a convolution-augmented Transformer model for speech recognition (ASR), emphasizing ASR performance metrics like WER without mentioning spoken language modeling, dialogue interaction, full-duplex interaction, or synchronization aspects mandated by the inclusion criteria. It treats speech largely as input acoustic features rather than modeling speech as core for language modeling or synchronous turn-taking behavior; therefore, it aligns with exclusion criteria of focusing solely on ASR without modeling spoken language aspects or interactive behaviors.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on a convolution-augmented Transformer model for speech recognition (ASR), emphasizing ASR performance metrics like WER without mentioning spoken language modeling, dialogue interaction, full-duplex interaction, or synchronization aspects mandated by the inclusion criteria. It treats speech largely as input acoustic features rather than modeling speech as core for language modeling or synchronous turn-taking behavior; therefore, it aligns with exclusion criteria of focusing solely on ASR without modeling spoken language aspects or interactive behaviors.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "What All Do Audio Transformer Models Hear? Probing Acoustic Representations for Language Delivery and Its Structure",
    "abstract": "In recent times, BERT based transformer models have become an inseparable part of the 'tech stack' of text processing models. Similar progress is being observed in the speech domain with a multitude of models observing state-of-the-art results by using audio transformer models to encode speech. This begs the question of what are these audio transformer models learning. Moreover, although the standard methodology is to choose the last layer embedding for any downstream task, but is it the optimal choice? We try to answer these questions for the two recent audio transformer models, Mockingjay and wave2vec2.0. We compare them on a comprehensive set of language delivery and structure features including audio, fluency and pronunciation features. Additionally, we probe the audio models' understanding of textual surface, syntax, and semantic features and compare them to BERT. We do this over exhaustive settings for native, non-native, synthetic, read and spontaneous speech datasets",
    "metadata": {
      "title": "What All Do Audio Transformer Models Hear? Probing Acoustic Representations for Language Delivery and Its Structure",
      "summary": "In recent times, BERT based transformer models have become an inseparable part of the 'tech stack' of text processing models. Similar progress is being observed in the speech domain with a multitude of models observing state-of-the-art results by using audio transformer models to encode speech. This begs the question of what are these audio transformer models learning. Moreover, although the standard methodology is to choose the last layer embedding for any downstream task, but is it the optimal choice? We try to answer these questions for the two recent audio transformer models, Mockingjay and wave2vec2.0. We compare them on a comprehensive set of language delivery and structure features including audio, fluency and pronunciation features. Additionally, we probe the audio models' understanding of textual surface, syntax, and semantic features and compare them to BERT. We do this over exhaustive settings for native, non-native, synthetic, read and spontaneous speech datasets",
      "abstract": "In recent times, BERT based transformer models have become an inseparable part of the 'tech stack' of text processing models. Similar progress is being observed in the speech domain with a multitude of models observing state-of-the-art results by using audio transformer models to encode speech. This begs the question of what are these audio transformer models learning. Moreover, although the standard methodology is to choose the last layer embedding for any downstream task, but is it the optimal choice? We try to answer these questions for the two recent audio transformer models, Mockingjay and wave2vec2.0. We compare them on a comprehensive set of language delivery and structure features including audio, fluency and pronunciation features. Additionally, we probe the audio models' understanding of textual surface, syntax, and semantic features and compare them to BERT. We do this over exhaustive settings for native, non-native, synthetic, read and spontaneous speech datasets",
      "doi": "https://doi.org/10.20944/preprints202101.0081.v1",
      "openalex_id": "https://openalex.org/W3121914243",
      "arxiv_id": "",
      "publication_date": "2021-01-05",
      "published": "2021-01-05",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The abstract analyzes acoustic representations learned by audio transformer models for speech, probes linguistic features (surface, syntax, semantics), and compares to text models, which aligns with the Spoken Language Models focus on speech as the modeling target rather than treating speech as only preprocessing; though it does not address full-duplex interaction or turn-taking, it fits the core theme of speech-centric language modeling.",
      "evaluation": 4
    },
    "round-A_JuniorNano_reasoning": "The abstract analyzes acoustic representations learned by audio transformer models for speech, probes linguistic features (surface, syntax, semantics), and compares to text models, which aligns with the Spoken Language Models focus on speech as the modeling target rather than treating speech as only preprocessing; though it does not address full-duplex interaction or turn-taking, it fits the core theme of speech-centric language modeling.",
    "round-A_JuniorNano_evaluation": 4,
    "round-A_JuniorMini_output": {
      "reasoning": "The study investigates audio transformer models focusing on acoustic representations for language delivery and structure, comparing them to textual models like BERT; however, it primarily examines feature probing and embeddings rather than explicit spoken language modeling or full-duplex interaction paradigms as described in the inclusion criteria, and does not mention speech language models, speech-LLM, full-duplex models, or synchronization aspects explicitly, nor does it address multi-dimensional evaluation related to synchronous interaction behaviors or real-time dialogue arbitration.",
      "evaluation": 2
    },
    "round-A_JuniorMini_reasoning": "The study investigates audio transformer models focusing on acoustic representations for language delivery and structure, comparing them to textual models like BERT; however, it primarily examines feature probing and embeddings rather than explicit spoken language modeling or full-duplex interaction paradigms as described in the inclusion criteria, and does not mention speech language models, speech-LLM, full-duplex models, or synchronization aspects explicitly, nor does it address multi-dimensional evaluation related to synchronous interaction behaviors or real-time dialogue arbitration.",
    "round-A_JuniorMini_evaluation": 2,
    "round-B_SeniorLead_output": {
      "reasoning": "Both junior reviewers provide useful perspectives: JuniorNano favored inclusion because the paper analyzes acoustic representations of audio transformer models (speech as the modeling target), while JuniorMini recommended exclusion noting the work probes embeddings/features rather than presenting spoken language models, full‑duplex interaction, or synchronization aspects required by the theme. Against the inclusion criteria, the abstract does not explicitly claim or study “Spoken Language Model(s)” or equivalent language‑modeling‑of‑speech formulations, and instead focuses on probing audio transformer embeddings (Mockingjay, wav2vec2.0) and comparing them to BERT—i.e., analysis of representations rather than SLM architectures, training, or interaction/evaluation for full‑duplex spoken language modeling. Therefore it fails the requirement that the study explicitly present spoken language models or equivalent and discuss architecture/training/evaluation in that context. Brief 1‑sentence explanation: Because the abstract lacks explicit SLM/speech‑LM framing and focuses on probing audio embeddings rather than on spoken language model architectures, training, or synchronous interaction evaluation, it should be excluded.",
      "evaluation": 2
    },
    "round-B_SeniorLead_reasoning": "Both junior reviewers provide useful perspectives: JuniorNano favored inclusion because the paper analyzes acoustic representations of audio transformer models (speech as the modeling target), while JuniorMini recommended exclusion noting the work probes embeddings/features rather than presenting spoken language models, full‑duplex interaction, or synchronization aspects required by the theme. Against the inclusion criteria, the abstract does not explicitly claim or study “Spoken Language Model(s)” or equivalent language‑modeling‑of‑speech formulations, and instead focuses on probing audio transformer embeddings (Mockingjay, wav2vec2.0) and comparing them to BERT—i.e., analysis of representations rather than SLM architectures, training, or interaction/evaluation for full‑duplex spoken language modeling. Therefore it fails the requirement that the study explicitly present spoken language models or equivalent and discuss architecture/training/evaluation in that context. Brief 1‑sentence explanation: Because the abstract lacks explicit SLM/speech‑LM framing and focuses on probing audio embeddings rather than on spoken language model architectures, training, or synchronous interaction evaluation, it should be excluded.",
    "round-B_SeniorLead_evaluation": 2,
    "final_verdict": "exclude (senior:2)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "BLSP: Bootstrapping Language-Speech Pre-training via Behavior Alignment of Continuation Writing",
    "abstract": "The emergence of large language models (LLMs) has sparked significant interest in extending their remarkable language capabilities to speech. However, modality alignment between speech and text still remains an open problem. Current solutions can be categorized into two strategies. One is a cascaded approach where outputs (tokens or states) of a separately trained speech recognition system are used as inputs for LLMs, which limits their potential in modeling alignment between speech and text. The other is an end-to-end approach that relies on speech instruction data, which is very difficult to collect in large quantities. In this paper, we address these issues and propose the BLSP approach that Bootstraps Language-Speech Pre-training via behavior alignment of continuation writing. We achieve this by learning a lightweight modality adapter between a frozen speech encoder and an LLM, ensuring that the LLM exhibits the same generation behavior regardless of the modality of input: a speech segment or its transcript. The training process can be divided into two steps. The first step prompts an LLM to generate texts with speech transcripts as prefixes, obtaining text continuations. In the second step, these continuations are used as supervised signals to train the modality adapter in an end-to-end manner. We demonstrate that this straightforward process can extend the capabilities of LLMs to speech, enabling speech recognition, speech translation, spoken language understanding, and speech conversation, even in zero-shot cross-lingual scenarios.",
    "metadata": {
      "title": "BLSP: Bootstrapping Language-Speech Pre-training via Behavior Alignment of Continuation Writing",
      "summary": "The emergence of large language models (LLMs) has sparked significant interest in extending their remarkable language capabilities to speech. However, modality alignment between speech and text still remains an open problem. Current solutions can be categorized into two strategies. One is a cascaded approach where outputs (tokens or states) of a separately trained speech recognition system are used as inputs for LLMs, which limits their potential in modeling alignment between speech and text. The other is an end-to-end approach that relies on speech instruction data, which is very difficult to collect in large quantities. In this paper, we address these issues and propose the BLSP approach that Bootstraps Language-Speech Pre-training via behavior alignment of continuation writing. We achieve this by learning a lightweight modality adapter between a frozen speech encoder and an LLM, ensuring that the LLM exhibits the same generation behavior regardless of the modality of input: a speech segment or its transcript. The training process can be divided into two steps. The first step prompts an LLM to generate texts with speech transcripts as prefixes, obtaining text continuations. In the second step, these continuations are used as supervised signals to train the modality adapter in an end-to-end manner. We demonstrate that this straightforward process can extend the capabilities of LLMs to speech, enabling speech recognition, speech translation, spoken language understanding, and speech conversation, even in zero-shot cross-lingual scenarios.",
      "abstract": "The emergence of large language models (LLMs) has sparked significant interest in extending their remarkable language capabilities to speech. However, modality alignment between speech and text still remains an open problem. Current solutions can be categorized into two strategies. One is a cascaded approach where outputs (tokens or states) of a separately trained speech recognition system are used as inputs for LLMs, which limits their potential in modeling alignment between speech and text. The other is an end-to-end approach that relies on speech instruction data, which is very difficult to collect in large quantities. In this paper, we address these issues and propose the BLSP approach that Bootstraps Language-Speech Pre-training via behavior alignment of continuation writing. We achieve this by learning a lightweight modality adapter between a frozen speech encoder and an LLM, ensuring that the LLM exhibits the same generation behavior regardless of the modality of input: a speech segment or its transcript. The training process can be divided into two steps. The first step prompts an LLM to generate texts with speech transcripts as prefixes, obtaining text continuations. In the second step, these continuations are used as supervised signals to train the modality adapter in an end-to-end manner. We demonstrate that this straightforward process can extend the capabilities of LLMs to speech, enabling speech recognition, speech translation, spoken language understanding, and speech conversation, even in zero-shot cross-lingual scenarios.",
      "doi": "https://doi.org/10.48550/arxiv.2309.00916",
      "openalex_id": "https://openalex.org/W4386552581",
      "arxiv_id": "",
      "publication_date": "2023-09-02",
      "published": "2023-09-02",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The paper targets Spoken Language Models / Speech-Language Model integration by training a modality adapter to align LLM generation across speech input and transcripts, enabling speech recognition, translation, spoken language understanding, and speech conversation, i.e., a full SLM/FD-SLM oriented approach with cross-modal evaluation, within the recent 3-year window.",
      "evaluation": 5
    },
    "round-A_JuniorNano_reasoning": "The paper targets Spoken Language Models / Speech-Language Model integration by training a modality adapter to align LLM generation across speech input and transcripts, enabling speech recognition, translation, spoken language understanding, and speech conversation, i.e., a full SLM/FD-SLM oriented approach with cross-modal evaluation, within the recent 3-year window.",
    "round-A_JuniorNano_evaluation": 5,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper proposes a method to bootstrap language-speech pre-training by aligning continuation writing behavior between speech and text modalities, involving a modality adapter between speech encoder and LLM, targeting multiple downstream tasks including speech recognition, translation, spoken language understanding, and conversation. However, the study focuses on extending LLMs to speech input but does not explicitly position itself as studying Spoken Language Models (SLMs) or Full-Duplex Spoken Language Models (FD-SLMs) with considerations of synchronous turn-taking, cognitive parallelism, or low-latency interactive behavior as required in the inclusion criteria. It addresses speech-text modality alignment and multi-task capabilities rather than full-duplex spoken language modeling or learned synchronization for real-time interaction. The core contribution is more toward modality adaptation for LLMs rather than detailed speech-level language modeling or dynamic interaction arbitration. Also, the abstract does not mention specific evaluations beyond zero-shot cross-lingual tasks, and no mention of dynamic interaction or latency metrics is visible. Therefore, while related to speech and LLM integration, it falls short of all required inclusion aspects and key concepts, and thus should not be included.",
      "evaluation": 2
    },
    "round-A_JuniorMini_reasoning": "The paper proposes a method to bootstrap language-speech pre-training by aligning continuation writing behavior between speech and text modalities, involving a modality adapter between speech encoder and LLM, targeting multiple downstream tasks including speech recognition, translation, spoken language understanding, and conversation. However, the study focuses on extending LLMs to speech input but does not explicitly position itself as studying Spoken Language Models (SLMs) or Full-Duplex Spoken Language Models (FD-SLMs) with considerations of synchronous turn-taking, cognitive parallelism, or low-latency interactive behavior as required in the inclusion criteria. It addresses speech-text modality alignment and multi-task capabilities rather than full-duplex spoken language modeling or learned synchronization for real-time interaction. The core contribution is more toward modality adaptation for LLMs rather than detailed speech-level language modeling or dynamic interaction arbitration. Also, the abstract does not mention specific evaluations beyond zero-shot cross-lingual tasks, and no mention of dynamic interaction or latency metrics is visible. Therefore, while related to speech and LLM integration, it falls short of all required inclusion aspects and key concepts, and thus should not be included.",
    "round-A_JuniorMini_evaluation": 2,
    "round-B_SeniorLead_output": {
      "reasoning": "Both junior reviews considered: JuniorNano rated inclusion (5) focusing on multi-task speech-LLM integration, while JuniorMini (2) correctly noted the paper centers on modality adaptation rather than explicit Spoken Language Models or full‑duplex capabilities. The abstract does not use the required terminology (e.g., “Spoken Language Models” or equivalent), treats speech primarily as an alternative input to a frozen LLM via a modality adapter (i.e., effectively ASR→LLM style alignment), and lacks any discussion or evaluation of full‑duplex interaction, learned/engineered synchronization, low‑latency real‑time behavior, or speech‑level language modeling—therefore it fails the inclusion criteria and matches multiple exclusion criteria (core research remains text‑centric/modal alignment rather than SLM/FD‑SLM modeling).",
      "evaluation": 2
    },
    "round-B_SeniorLead_reasoning": "Both junior reviews considered: JuniorNano rated inclusion (5) focusing on multi-task speech-LLM integration, while JuniorMini (2) correctly noted the paper centers on modality adaptation rather than explicit Spoken Language Models or full‑duplex capabilities. The abstract does not use the required terminology (e.g., “Spoken Language Models” or equivalent), treats speech primarily as an alternative input to a frozen LLM via a modality adapter (i.e., effectively ASR→LLM style alignment), and lacks any discussion or evaluation of full‑duplex interaction, learned/engineered synchronization, low‑latency real‑time behavior, or speech‑level language modeling—therefore it fails the inclusion criteria and matches multiple exclusion criteria (core research remains text‑centric/modal alignment rather than SLM/FD‑SLM modeling).",
    "round-B_SeniorLead_evaluation": 2,
    "final_verdict": "exclude (senior:2)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "SALMONN: Towards Generic Hearing Abilities for Large Language Models",
    "abstract": "Hearing is arguably an essential ability of artificial intelligence (AI) agents in the physical world, which refers to the perception and understanding of general auditory information consisting of at least three types of sounds: speech, audio events, and music. In this paper, we propose SALMONN, a speech audio language music open neural network, built by integrating a pre-trained text-based large language model (LLM) with speech and audio encoders into a single multimodal model. SALMONN enables the LLM to directly process and understand general audio inputs and achieve competitive performances on a number of speech and audio tasks used in training, such as automatic speech recognition and translation, auditory-information-based question answering, emotion recognition, speaker verification, and music and audio captioning etc. SALMONN also has a diverse set of emergent abilities unseen in the training, which includes but is not limited to speech translation to untrained languages, speech-based slot filling, spoken-query-based question answering, audio-based storytelling, and speech audio co-reasoning etc. The presence of cross-modal emergent abilities is studied, and a novel few-shot activation tuning approach is proposed to activate such abilities. To our knowledge, SALMONN is the first model of its type and can be regarded as a step towards AI with generic hearing abilities. The source code, model checkpoints and data are available at https://github.com/bytedance/SALMONN.",
    "metadata": {
      "title": "SALMONN: Towards Generic Hearing Abilities for Large Language Models",
      "summary": "Hearing is arguably an essential ability of artificial intelligence (AI) agents in the physical world, which refers to the perception and understanding of general auditory information consisting of at least three types of sounds: speech, audio events, and music. In this paper, we propose SALMONN, a speech audio language music open neural network, built by integrating a pre-trained text-based large language model (LLM) with speech and audio encoders into a single multimodal model. SALMONN enables the LLM to directly process and understand general audio inputs and achieve competitive performances on a number of speech and audio tasks used in training, such as automatic speech recognition and translation, auditory-information-based question answering, emotion recognition, speaker verification, and music and audio captioning etc. SALMONN also has a diverse set of emergent abilities unseen in the training, which includes but is not limited to speech translation to untrained languages, speech-based slot filling, spoken-query-based question answering, audio-based storytelling, and speech audio co-reasoning etc. The presence of cross-modal emergent abilities is studied, and a novel few-shot activation tuning approach is proposed to activate such abilities. To our knowledge, SALMONN is the first model of its type and can be regarded as a step towards AI with generic hearing abilities. The source code, model checkpoints and data are available at https://github.com/bytedance/SALMONN.",
      "abstract": "Hearing is arguably an essential ability of artificial intelligence (AI) agents in the physical world, which refers to the perception and understanding of general auditory information consisting of at least three types of sounds: speech, audio events, and music. In this paper, we propose SALMONN, a speech audio language music open neural network, built by integrating a pre-trained text-based large language model (LLM) with speech and audio encoders into a single multimodal model. SALMONN enables the LLM to directly process and understand general audio inputs and achieve competitive performances on a number of speech and audio tasks used in training, such as automatic speech recognition and translation, auditory-information-based question answering, emotion recognition, speaker verification, and music and audio captioning etc. SALMONN also has a diverse set of emergent abilities unseen in the training, which includes but is not limited to speech translation to untrained languages, speech-based slot filling, spoken-query-based question answering, audio-based storytelling, and speech audio co-reasoning etc. The presence of cross-modal emergent abilities is studied, and a novel few-shot activation tuning approach is proposed to activate such abilities. To our knowledge, SALMONN is the first model of its type and can be regarded as a step towards AI with generic hearing abilities. The source code, model checkpoints and data are available at https://github.com/bytedance/SALMONN.",
      "doi": "https://doi.org/10.48550/arxiv.2310.13289",
      "openalex_id": "https://openalex.org/W4387891768",
      "arxiv_id": "",
      "publication_date": "2023-10-20",
      "published": "2023-10-20",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The paper is centered on Spoken Language Models / FD-SLMs, integrating speech encoders with a pre-trained LLM to process general audio and perform multi-task evaluation (ASR, translation, emotion recognition, etc.), published within the last three years, thus meeting all inclusion criteria and addressing interactive speech-language modeling rather than single-task ASR/TTS as exclusions.",
      "evaluation": 5
    },
    "round-A_JuniorNano_reasoning": "The paper is centered on Spoken Language Models / FD-SLMs, integrating speech encoders with a pre-trained LLM to process general audio and perform multi-task evaluation (ASR, translation, emotion recognition, etc.), published within the last three years, thus meeting all inclusion criteria and addressing interactive speech-language modeling rather than single-task ASR/TTS as exclusions.",
    "round-A_JuniorNano_evaluation": 5,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper SALMONN focuses on integrating a large language model with speech and audio encoders for multimodal auditory understanding, covering speech and audio tasks such as ASR, translation, question answering, and emotion recognition. However, it does not explicitly emphasize spoken language modeling or language modeling of speech as a core concept, nor does it mention full-duplex interaction, synchronization, cognitive parallelism, real-time low-latency interaction, or behavioral arbitration, which are key inclusion criteria; it also seems to treat audio/speech largely as inputs to enhance the LLM rather than centering on speech sequence modeling itself. Therefore, it does not fully meet the stringent criteria requiring spoken language model focus and synchronous dialogue interaction features.",
      "evaluation": 2
    },
    "round-A_JuniorMini_reasoning": "The paper SALMONN focuses on integrating a large language model with speech and audio encoders for multimodal auditory understanding, covering speech and audio tasks such as ASR, translation, question answering, and emotion recognition. However, it does not explicitly emphasize spoken language modeling or language modeling of speech as a core concept, nor does it mention full-duplex interaction, synchronization, cognitive parallelism, real-time low-latency interaction, or behavioral arbitration, which are key inclusion criteria; it also seems to treat audio/speech largely as inputs to enhance the LLM rather than centering on speech sequence modeling itself. Therefore, it does not fully meet the stringent criteria requiring spoken language model focus and synchronous dialogue interaction features.",
    "round-A_JuniorMini_evaluation": 2,
    "round-B_SeniorLead_output": {
      "reasoning": "Junior reviewers disagreed (one included, one excluded); after review: SALMONN describes integrating speech/audio encoders with a text LLM to handle many audio tasks, but the abstract does not explicitly frame the work as a Spoken Language Model or language modeling of speech, nor does it address full‑duplex/synchronous interaction, low‑latency, overlap handling, or behavioral arbitration—so it fails the requirement that speech be the core modeling/inference object and that FD‑SLM aspects be discussed.  Therefore, better to exclude.",
      "evaluation": 2
    },
    "round-B_SeniorLead_reasoning": "Junior reviewers disagreed (one included, one excluded); after review: SALMONN describes integrating speech/audio encoders with a text LLM to handle many audio tasks, but the abstract does not explicitly frame the work as a Spoken Language Model or language modeling of speech, nor does it address full‑duplex/synchronous interaction, low‑latency, overlap handling, or behavioral arbitration—so it fails the requirement that speech be the core modeling/inference object and that FD‑SLM aspects be discussed.  Therefore, better to exclude.",
    "round-B_SeniorLead_evaluation": 2,
    "final_verdict": "exclude (senior:2)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Audio Flamingo: A Novel Audio Language Model with Few-Shot Learning and Dialogue Abilities",
    "abstract": "Augmenting large language models (LLMs) to understand audio -- including non-speech sounds and non-verbal speech -- is critically important for diverse real-world applications of LLMs. In this paper, we propose Audio Flamingo, a novel audio language model with 1) strong audio understanding abilities, 2) the ability to quickly adapt to unseen tasks via in-context learning and retrieval, and 3) strong multi-turn dialogue abilities. We introduce a series of training techniques, architecture design, and data strategies to enhance our model with these abilities. Extensive evaluations across various audio understanding tasks confirm the efficacy of our method, setting new state-of-the-art benchmarks. Our demo website is https://audioflamingo.github.io/ and the code is open-sourced at https://github.com/NVIDIA/audio-flamingo.",
    "metadata": {
      "title": "Audio Flamingo: A Novel Audio Language Model with Few-Shot Learning and Dialogue Abilities",
      "summary": "Augmenting large language models (LLMs) to understand audio -- including non-speech sounds and non-verbal speech -- is critically important for diverse real-world applications of LLMs. In this paper, we propose Audio Flamingo, a novel audio language model with 1) strong audio understanding abilities, 2) the ability to quickly adapt to unseen tasks via in-context learning and retrieval, and 3) strong multi-turn dialogue abilities. We introduce a series of training techniques, architecture design, and data strategies to enhance our model with these abilities. Extensive evaluations across various audio understanding tasks confirm the efficacy of our method, setting new state-of-the-art benchmarks. Our demo website is https://audioflamingo.github.io/ and the code is open-sourced at https://github.com/NVIDIA/audio-flamingo.",
      "abstract": "Augmenting large language models (LLMs) to understand audio -- including non-speech sounds and non-verbal speech -- is critically important for diverse real-world applications of LLMs. In this paper, we propose Audio Flamingo, a novel audio language model with 1) strong audio understanding abilities, 2) the ability to quickly adapt to unseen tasks via in-context learning and retrieval, and 3) strong multi-turn dialogue abilities. We introduce a series of training techniques, architecture design, and data strategies to enhance our model with these abilities. Extensive evaluations across various audio understanding tasks confirm the efficacy of our method, setting new state-of-the-art benchmarks. Our demo website is https://audioflamingo.github.io/ and the code is open-sourced at https://github.com/NVIDIA/audio-flamingo.",
      "doi": "https://doi.org/10.48550/arxiv.2402.01831",
      "openalex_id": "https://openalex.org/W4391591726",
      "arxiv_id": "",
      "publication_date": "2024-02-02",
      "published": "2024-02-02",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This item is clearly a spoken-language-model-focused work with audio as the core modeling target, discusses architecture/training/evaluation, demonstrates full-duplex dialogue capabilities, is within the last 3 years, and explicitly mentions an audio language model in the title/abstract, meeting all inclusions and none of the exclusions.",
      "evaluation": 5
    },
    "round-A_JuniorNano_reasoning": "This item is clearly a spoken-language-model-focused work with audio as the core modeling target, discusses architecture/training/evaluation, demonstrates full-duplex dialogue capabilities, is within the last 3 years, and explicitly mentions an audio language model in the title/abstract, meeting all inclusions and none of the exclusions.",
    "round-A_JuniorNano_evaluation": 5,
    "round-A_JuniorMini_output": {
      "reasoning": "The study presents 'Audio Flamingo,' a novel audio language model with strong audio understanding, few-shot learning, and multi-turn dialogue capabilities, which aligns with research on Spoken Language Models (SLMs) and addresses modeling based on audio signals rather than just text preprocessing. However, the abstract does not explicitly mention aspects of full-duplex spoken language modeling, specific synchronization mechanisms, or detailed evaluation on cognitive parallelism, low-latency interactions, or behavior arbitration, which are critical inclusion focus points. Also, the inclusion criteria require explicit mention of spoken language models or equivalent terms, which the abstract lacks; while the model is an audio language model, it does not explicitly claim to be a spoken language model, nor does it specify end-to-end or hybrid speech-LLM architectures in detail. Given these ambiguities and lack of explicit alignment with core inclusion criteria, the study falls into a gray area with promising subject relevance but insufficient clarity for full inclusion.",
      "evaluation": 3
    },
    "round-A_JuniorMini_reasoning": "The study presents 'Audio Flamingo,' a novel audio language model with strong audio understanding, few-shot learning, and multi-turn dialogue capabilities, which aligns with research on Spoken Language Models (SLMs) and addresses modeling based on audio signals rather than just text preprocessing. However, the abstract does not explicitly mention aspects of full-duplex spoken language modeling, specific synchronization mechanisms, or detailed evaluation on cognitive parallelism, low-latency interactions, or behavior arbitration, which are critical inclusion focus points. Also, the inclusion criteria require explicit mention of spoken language models or equivalent terms, which the abstract lacks; while the model is an audio language model, it does not explicitly claim to be a spoken language model, nor does it specify end-to-end or hybrid speech-LLM architectures in detail. Given these ambiguities and lack of explicit alignment with core inclusion criteria, the study falls into a gray area with promising subject relevance but insufficient clarity for full inclusion.",
    "round-A_JuniorMini_evaluation": 3,
    "round-B_SeniorLead_output": {
      "reasoning": "I reviewed both junior reviewers: one recommended inclusion (5) and the other flagged ambiguity about explicit 'spoken language model' phrasing and lack of full‑duplex/overlap evaluation (3). The paper clearly treats audio as the primary modeling target, proposes an audio language model with architecture/training/data details, and demonstrates multi‑turn dialogue and few‑shot learning — which aligns with Spoken Language Models / Speech–LLM research (the term 'audio language model' is a reasonable equivalent), but it does not explicitly claim full‑duplex synchronous interaction or evaluate overlap/low‑latency behaviors, so it is relevant and should be included though not an unequivocal match for full‑duplex FD‑SLM work. One‑sentence explanation for the score: it meets the SLM/speech‑modeling inclusion by treating audio as the core modeling object and providing architecture/training/evaluation, but lacks explicit full‑duplex/overlap/latency analyses, so I recommend inclusion but not as an absolute must‑include.",
      "evaluation": 4
    },
    "round-B_SeniorLead_reasoning": "I reviewed both junior reviewers: one recommended inclusion (5) and the other flagged ambiguity about explicit 'spoken language model' phrasing and lack of full‑duplex/overlap evaluation (3). The paper clearly treats audio as the primary modeling target, proposes an audio language model with architecture/training/data details, and demonstrates multi‑turn dialogue and few‑shot learning — which aligns with Spoken Language Models / Speech–LLM research (the term 'audio language model' is a reasonable equivalent), but it does not explicitly claim full‑duplex synchronous interaction or evaluate overlap/low‑latency behaviors, so it is relevant and should be included though not an unequivocal match for full‑duplex FD‑SLM work. One‑sentence explanation for the score: it meets the SLM/speech‑modeling inclusion by treating audio as the core modeling object and providing architecture/training/evaluation, but lacks explicit full‑duplex/overlap/latency analyses, so I recommend inclusion but not as an absolute must‑include.",
    "round-B_SeniorLead_evaluation": 4,
    "final_verdict": "include (senior:4)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "CVSS Corpus and Massively Multilingual Speech-to-Speech Translation",
    "abstract": "We introduce CVSS, a massively multilingual-to-English speech-to-speech translation (S2ST) corpus, covering sentence-level parallel S2ST pairs from 21 languages into English. CVSS is derived from the Common Voice speech corpus and the CoVoST 2 speech-to-text translation (ST) corpus, by synthesizing the translation text from CoVoST 2 into speech using state-of-the-art TTS systems. Two versions of translation speeches are provided: 1) CVSS-C: All the translation speeches are in a single high-quality canonical voice; 2) CVSS-T: The translation speeches are in voices transferred from the corresponding source speeches. In addition, CVSS provides normalized translation text which matches the pronunciation in the translation speech. On each version of CVSS, we built baseline multilingual direct S2ST models and cascade S2ST models, verifying the effectiveness of the corpus. To build strong cascade S2ST baselines, we trained an ST model on CoVoST 2, which outperforms the previous state-of-the-art trained on the corpus without extra data by 5.8 BLEU. Nevertheless, the performance of the direct S2ST models approaches the strong cascade baselines when trained from scratch, and with only 0.1 or 0.7 BLEU difference on ASR transcribed translation when initialized from matching ST models.",
    "metadata": {
      "title": "CVSS Corpus and Massively Multilingual Speech-to-Speech Translation",
      "summary": "We introduce CVSS, a massively multilingual-to-English speech-to-speech translation (S2ST) corpus, covering sentence-level parallel S2ST pairs from 21 languages into English. CVSS is derived from the Common Voice speech corpus and the CoVoST 2 speech-to-text translation (ST) corpus, by synthesizing the translation text from CoVoST 2 into speech using state-of-the-art TTS systems. Two versions of translation speeches are provided: 1) CVSS-C: All the translation speeches are in a single high-quality canonical voice; 2) CVSS-T: The translation speeches are in voices transferred from the corresponding source speeches. In addition, CVSS provides normalized translation text which matches the pronunciation in the translation speech. On each version of CVSS, we built baseline multilingual direct S2ST models and cascade S2ST models, verifying the effectiveness of the corpus. To build strong cascade S2ST baselines, we trained an ST model on CoVoST 2, which outperforms the previous state-of-the-art trained on the corpus without extra data by 5.8 BLEU. Nevertheless, the performance of the direct S2ST models approaches the strong cascade baselines when trained from scratch, and with only 0.1 or 0.7 BLEU difference on ASR transcribed translation when initialized from matching ST models.",
      "abstract": "We introduce CVSS, a massively multilingual-to-English speech-to-speech translation (S2ST) corpus, covering sentence-level parallel S2ST pairs from 21 languages into English. CVSS is derived from the Common Voice speech corpus and the CoVoST 2 speech-to-text translation (ST) corpus, by synthesizing the translation text from CoVoST 2 into speech using state-of-the-art TTS systems. Two versions of translation speeches are provided: 1) CVSS-C: All the translation speeches are in a single high-quality canonical voice; 2) CVSS-T: The translation speeches are in voices transferred from the corresponding source speeches. In addition, CVSS provides normalized translation text which matches the pronunciation in the translation speech. On each version of CVSS, we built baseline multilingual direct S2ST models and cascade S2ST models, verifying the effectiveness of the corpus. To build strong cascade S2ST baselines, we trained an ST model on CoVoST 2, which outperforms the previous state-of-the-art trained on the corpus without extra data by 5.8 BLEU. Nevertheless, the performance of the direct S2ST models approaches the strong cascade baselines when trained from scratch, and with only 0.1 or 0.7 BLEU difference on ASR transcribed translation when initialized from matching ST models.",
      "doi": "https://doi.org/10.48550/arxiv.2201.03713",
      "openalex_id": "https://openalex.org/W4226444650",
      "arxiv_id": "",
      "publication_date": "2022-01-11",
      "published": "2022-01-11",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "CVSS centers on speech-to-speech translation with speech as the core modeling object and discusses end-to-end/cascade S2ST architectures and evaluation using a multilingual corpus in the last 3 years, which aligns with the inclusion criteria, though it does not explicitly use the term 'spoken language model' in the title/abstract.",
      "evaluation": 4
    },
    "round-A_JuniorNano_reasoning": "CVSS centers on speech-to-speech translation with speech as the core modeling object and discusses end-to-end/cascade S2ST architectures and evaluation using a multilingual corpus in the last 3 years, which aligns with the inclusion criteria, though it does not explicitly use the term 'spoken language model' in the title/abstract.",
    "round-A_JuniorNano_evaluation": 4,
    "round-A_JuniorMini_output": {
      "reasoning": "The study introduces a multilingual speech-to-speech translation corpus and baseline models focusing on speech translation tasks. However, it does not explicitly address spoken language models (SLMs) or full-duplex spoken language models (FD-SLMs), nor does it focus on modeling speech as the primary unit for language modeling or discuss synchronization, cognitive parallelism, or real-time interactive features required by the inclusion criteria. The research centers around dataset creation and evaluation of translation models rather than spoken language modeling or dialogue behavior arbitration. Therefore, it does not meet the specified inclusion criteria and partially aligns with the exclusion criteria.",
      "evaluation": 2
    },
    "round-A_JuniorMini_reasoning": "The study introduces a multilingual speech-to-speech translation corpus and baseline models focusing on speech translation tasks. However, it does not explicitly address spoken language models (SLMs) or full-duplex spoken language models (FD-SLMs), nor does it focus on modeling speech as the primary unit for language modeling or discuss synchronization, cognitive parallelism, or real-time interactive features required by the inclusion criteria. The research centers around dataset creation and evaluation of translation models rather than spoken language modeling or dialogue behavior arbitration. Therefore, it does not meet the specified inclusion criteria and partially aligns with the exclusion criteria.",
    "round-A_JuniorMini_evaluation": 2,
    "round-B_SeniorLead_output": {
      "reasoning": "Both junior reviewers were consulted (JuniorNano scored 4 noting relevance to speech-to-speech modeling, JuniorMini scored 2 noting lack of SLM/FD‑SLM focus). The paper describes a multilingual S2ST corpus and baseline translation models (derived via TTS) and does not explicitly frame work as Spoken Language Models or discuss speech-centric language modeling, full‑duplex interaction, synchronization, cognitive parallelism, low‑latency interactive evaluation, or related benchmarks—therefore it meets the exclusion criteria and fails the strict inclusion requirement that the title/abstract explicitly present SLMs or equivalent concepts. Score: 1 (exclude).",
      "evaluation": 1
    },
    "round-B_SeniorLead_reasoning": "Both junior reviewers were consulted (JuniorNano scored 4 noting relevance to speech-to-speech modeling, JuniorMini scored 2 noting lack of SLM/FD‑SLM focus). The paper describes a multilingual S2ST corpus and baseline translation models (derived via TTS) and does not explicitly frame work as Spoken Language Models or discuss speech-centric language modeling, full‑duplex interaction, synchronization, cognitive parallelism, low‑latency interactive evaluation, or related benchmarks—therefore it meets the exclusion criteria and fails the strict inclusion requirement that the title/abstract explicitly present SLMs or equivalent concepts. Score: 1 (exclude).",
    "round-B_SeniorLead_evaluation": 1,
    "final_verdict": "exclude (senior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "The Ryerson Audio-Visual Database of Emotional Speech and Song (RAVDESS): A dynamic, multimodal set of facial and vocal expressions in North American English",
    "abstract": "The RAVDESS is a validated multimodal database of emotional speech and song. The database is gender balanced consisting of 24 professional actors, vocalizing lexically-matched statements in a neutral North American accent. Speech includes calm, happy, sad, angry, fearful, surprise, and disgust expressions, and song contains calm, happy, sad, angry, and fearful emotions. Each expression is produced at two levels of emotional intensity, with an additional neutral expression. All conditions are available in face-and-voice, face-only, and voice-only formats. The set of 7356 recordings were each rated 10 times on emotional validity, intensity, and genuineness. Ratings were provided by 247 individuals who were characteristic of untrained research participants from North America. A further set of 72 participants provided test-retest data. High levels of emotional validity and test-retest intrarater reliability were reported. Corrected accuracy and composite \"goodness\" measures are presented to assist researchers in the selection of stimuli. All recordings are made freely available under a Creative Commons license and can be downloaded at https://doi.org/10.5281/zenodo.1188976.",
    "metadata": {
      "title": "The Ryerson Audio-Visual Database of Emotional Speech and Song (RAVDESS): A dynamic, multimodal set of facial and vocal expressions in North American English",
      "summary": "The RAVDESS is a validated multimodal database of emotional speech and song. The database is gender balanced consisting of 24 professional actors, vocalizing lexically-matched statements in a neutral North American accent. Speech includes calm, happy, sad, angry, fearful, surprise, and disgust expressions, and song contains calm, happy, sad, angry, and fearful emotions. Each expression is produced at two levels of emotional intensity, with an additional neutral expression. All conditions are available in face-and-voice, face-only, and voice-only formats. The set of 7356 recordings were each rated 10 times on emotional validity, intensity, and genuineness. Ratings were provided by 247 individuals who were characteristic of untrained research participants from North America. A further set of 72 participants provided test-retest data. High levels of emotional validity and test-retest intrarater reliability were reported. Corrected accuracy and composite \"goodness\" measures are presented to assist researchers in the selection of stimuli. All recordings are made freely available under a Creative Commons license and can be downloaded at https://doi.org/10.5281/zenodo.1188976.",
      "abstract": "The RAVDESS is a validated multimodal database of emotional speech and song. The database is gender balanced consisting of 24 professional actors, vocalizing lexically-matched statements in a neutral North American accent. Speech includes calm, happy, sad, angry, fearful, surprise, and disgust expressions, and song contains calm, happy, sad, angry, and fearful emotions. Each expression is produced at two levels of emotional intensity, with an additional neutral expression. All conditions are available in face-and-voice, face-only, and voice-only formats. The set of 7356 recordings were each rated 10 times on emotional validity, intensity, and genuineness. Ratings were provided by 247 individuals who were characteristic of untrained research participants from North America. A further set of 72 participants provided test-retest data. High levels of emotional validity and test-retest intrarater reliability were reported. Corrected accuracy and composite \"goodness\" measures are presented to assist researchers in the selection of stimuli. All recordings are made freely available under a Creative Commons license and can be downloaded at https://doi.org/10.5281/zenodo.1188976.",
      "doi": "https://doi.org/10.1371/journal.pone.0196391",
      "openalex_id": "https://openalex.org/W2803193013",
      "arxiv_id": "",
      "publication_date": "2018-05-16",
      "published": "2018-05-16",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This item is a dataset/resource about emotional speech and song (RAVDESS) focusing on data collection and ratings; it does not address Spoken Language Models or full-duplex speech language modeling, nor model training/evaluation, so it fails the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "This item is a dataset/resource about emotional speech and song (RAVDESS) focusing on data collection and ratings; it does not address Spoken Language Models or full-duplex speech language modeling, nor model training/evaluation, so it fails the inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The title and abstract describe a multimodal emotional speech and song database designed for research on emotional expressions with no explicit mention of spoken language models, speech language modeling, model architectures, training, evaluation, or concepts like full-duplex interaction, cognitive parallelism, or synchronization. The focus is on dataset creation and validation rather than spoken language modeling or synchronous speech interaction systems. Thus, it does not meet the inclusion criteria emphasizing spoken language model research with modeling, training, or evaluation, nor does it align with the exclusion criteria related to ASR or TTS focused tasks because it is neither. Therefore, it does not qualify for inclusion based on the provided criteria and timeframe.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The title and abstract describe a multimodal emotional speech and song database designed for research on emotional expressions with no explicit mention of spoken language models, speech language modeling, model architectures, training, evaluation, or concepts like full-duplex interaction, cognitive parallelism, or synchronization. The focus is on dataset creation and validation rather than spoken language modeling or synchronous speech interaction systems. Thus, it does not meet the inclusion criteria emphasizing spoken language model research with modeling, training, or evaluation, nor does it align with the exclusion criteria related to ASR or TTS focused tasks because it is neither. Therefore, it does not qualify for inclusion based on the provided criteria and timeframe.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Natural Questions: A Benchmark for Question Answering Research",
    "abstract": "We present the Natural Questions corpus, a question answering data set. Questions consist of real anonymized, aggregated queries issued to the Google search engine. An annotator is presented with a question along with a Wikipedia page from the top 5 search results, and annotates a long answer (typically a paragraph) and a short answer (one or more entities) if present on the page, or marks null if no long/short answer is present. The public release consists of 307,373 training examples with single annotations; 7,830 examples with 5-way annotations for development data; and a further 7,842 examples with 5-way annotated sequestered as test data. We present experiments validating quality of the data. We also describe analysis of 25-way annotations on 302 examples, giving insights into human variability on the annotation task. We introduce robust metrics for the purposes of evaluating question answering systems; demonstrate high human upper bounds on these metrics; and establish baseline results using competitive methods drawn from related literature.",
    "metadata": {
      "title": "Natural Questions: A Benchmark for Question Answering Research",
      "summary": "We present the Natural Questions corpus, a question answering data set. Questions consist of real anonymized, aggregated queries issued to the Google search engine. An annotator is presented with a question along with a Wikipedia page from the top 5 search results, and annotates a long answer (typically a paragraph) and a short answer (one or more entities) if present on the page, or marks null if no long/short answer is present. The public release consists of 307,373 training examples with single annotations; 7,830 examples with 5-way annotations for development data; and a further 7,842 examples with 5-way annotated sequestered as test data. We present experiments validating quality of the data. We also describe analysis of 25-way annotations on 302 examples, giving insights into human variability on the annotation task. We introduce robust metrics for the purposes of evaluating question answering systems; demonstrate high human upper bounds on these metrics; and establish baseline results using competitive methods drawn from related literature.",
      "abstract": "We present the Natural Questions corpus, a question answering data set. Questions consist of real anonymized, aggregated queries issued to the Google search engine. An annotator is presented with a question along with a Wikipedia page from the top 5 search results, and annotates a long answer (typically a paragraph) and a short answer (one or more entities) if present on the page, or marks null if no long/short answer is present. The public release consists of 307,373 training examples with single annotations; 7,830 examples with 5-way annotations for development data; and a further 7,842 examples with 5-way annotated sequestered as test data. We present experiments validating quality of the data. We also describe analysis of 25-way annotations on 302 examples, giving insights into human variability on the annotation task. We introduce robust metrics for the purposes of evaluating question answering systems; demonstrate high human upper bounds on these metrics; and establish baseline results using competitive methods drawn from related literature.",
      "doi": "https://doi.org/10.1162/tacl_a_00276",
      "openalex_id": "https://openalex.org/W2912924812",
      "arxiv_id": "",
      "publication_date": "2019-08-02",
      "published": "2019-08-02",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item focuses on a question answering, text-based NLP dataset (Natural Questions) with no emphasis on Spoken Language Models or full-duplex speech interaction, so it does not meet the core inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item focuses on a question answering, text-based NLP dataset (Natural Questions) with no emphasis on Spoken Language Models or full-duplex speech interaction, so it does not meet the core inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper focuses on a question answering dataset derived from Google search queries and Wikipedia pages, emphasizing text-based question answering and evaluation metrics for such systems. It does not address spoken language models, speech language models, or full-duplex spoken language interactions involving acoustic or speech-level modeling, synchronization, or real-time interaction. Hence, it does not meet the inclusion criteria centered on spoken language models or speech-centric modeling and interaction, nor avoids the exclusion criteria about focusing solely on text-based models without speech modeling.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper focuses on a question answering dataset derived from Google search queries and Wikipedia pages, emphasizing text-based question answering and evaluation metrics for such systems. It does not address spoken language models, speech language models, or full-duplex spoken language interactions involving acoustic or speech-level modeling, synchronization, or real-time interaction. Hence, it does not meet the inclusion criteria centered on spoken language models or speech-centric modeling and interaction, nor avoids the exclusion criteria about focusing solely on text-based models without speech modeling.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "WebGLM: Towards An Efficient Web-Enhanced Question Answering System with Human Preferences",
    "abstract": "We present WebGLM, a web-enhanced question-answering system based on the General Language Model (GLM). Its goal is to augment a pre-trained large language model (LLM) with web search and retrieval capabilities while being efficient for real-world deployments. To achieve this, we develop WebGLM with strategies for the LLM-augmented retriever, bootstrapped generator, and human preference-aware scorer. Specifically, we identify and address the limitations of WebGPT (OpenAI), through which WebGLM is enabled with accuracy, efficiency, and cost-effectiveness advantages. In addition, we propose systematic criteria for evaluating web-enhanced QA systems. We conduct multi-dimensional human evaluation and quantitative ablation studies, which suggest the outperformance of the proposed WebGLM designs over existing systems. WebGLM with the 10-billion-parameter GLM (10B) is shown to perform better than the similar-sized WebGPT (13B) and even comparably to WebGPT (175B) in human evaluation. The code, demo, and data are at https://github.com/THUDM/WebGLM.",
    "metadata": {
      "title": "WebGLM: Towards An Efficient Web-Enhanced Question Answering System with Human Preferences",
      "summary": "We present WebGLM, a web-enhanced question-answering system based on the General Language Model (GLM). Its goal is to augment a pre-trained large language model (LLM) with web search and retrieval capabilities while being efficient for real-world deployments. To achieve this, we develop WebGLM with strategies for the LLM-augmented retriever, bootstrapped generator, and human preference-aware scorer. Specifically, we identify and address the limitations of WebGPT (OpenAI), through which WebGLM is enabled with accuracy, efficiency, and cost-effectiveness advantages. In addition, we propose systematic criteria for evaluating web-enhanced QA systems. We conduct multi-dimensional human evaluation and quantitative ablation studies, which suggest the outperformance of the proposed WebGLM designs over existing systems. WebGLM with the 10-billion-parameter GLM (10B) is shown to perform better than the similar-sized WebGPT (13B) and even comparably to WebGPT (175B) in human evaluation. The code, demo, and data are at https://github.com/THUDM/WebGLM.",
      "abstract": "We present WebGLM, a web-enhanced question-answering system based on the General Language Model (GLM). Its goal is to augment a pre-trained large language model (LLM) with web search and retrieval capabilities while being efficient for real-world deployments. To achieve this, we develop WebGLM with strategies for the LLM-augmented retriever, bootstrapped generator, and human preference-aware scorer. Specifically, we identify and address the limitations of WebGPT (OpenAI), through which WebGLM is enabled with accuracy, efficiency, and cost-effectiveness advantages. In addition, we propose systematic criteria for evaluating web-enhanced QA systems. We conduct multi-dimensional human evaluation and quantitative ablation studies, which suggest the outperformance of the proposed WebGLM designs over existing systems. WebGLM with the 10-billion-parameter GLM (10B) is shown to perform better than the similar-sized WebGPT (13B) and even comparably to WebGPT (175B) in human evaluation. The code, demo, and data are at https://github.com/THUDM/WebGLM.",
      "doi": "https://doi.org/10.1145/3580305.3599931",
      "openalex_id": "https://openalex.org/W4385568240",
      "arxiv_id": "",
      "publication_date": "2023-08-04",
      "published": "2023-08-04",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The paper targets text-based web-enhanced QA using GLM with web search and human preference scorer; it does not address Spoken Language Models, speech signals, or synchronous speech interactions, so it does not meet the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The paper targets text-based web-enhanced QA using GLM with web search and human preference scorer; it does not address Spoken Language Models, speech signals, or synchronous speech interactions, so it does not meet the inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on a web-enhanced question answering system based on a large language model with human preference scoring but does not mention Spoken Language Models (SLMs), speech-based language modeling, or full-duplex spoken interaction capabilities. It centers around text-based retrieval and generation rather than modeling or processing acoustic speech signals or synchronous spoken interaction. Thus, it does not satisfy the inclusion criteria which require spoken language model-centric research with speech as the core modeling object and discussions on speech-based architectures, training, or evaluation. It also does not fall into the exclusion criteria beyond that, but the lack of speech-centric focus is sufficient for exclusion.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on a web-enhanced question answering system based on a large language model with human preference scoring but does not mention Spoken Language Models (SLMs), speech-based language modeling, or full-duplex spoken interaction capabilities. It centers around text-based retrieval and generation rather than modeling or processing acoustic speech signals or synchronous spoken interaction. Thus, it does not satisfy the inclusion criteria which require spoken language model-centric research with speech as the core modeling object and discussions on speech-based architectures, training, or evaluation. It also does not fall into the exclusion criteria beyond that, but the lack of speech-centric focus is sufficient for exclusion.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "SLUE Phase-2: A Benchmark Suite of Diverse Spoken Language Understanding Tasks",
    "abstract": "Suwon Shon, Siddhant Arora, Chyi-Jiunn Lin, Ankita Pasad, Felix Wu, Roshan S Sharma, Wei-Lun Wu, Hung-yi Lee, Karen Livescu, Shinji Watanabe. Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). 2023.",
    "metadata": {
      "title": "SLUE Phase-2: A Benchmark Suite of Diverse Spoken Language Understanding Tasks",
      "summary": "Suwon Shon, Siddhant Arora, Chyi-Jiunn Lin, Ankita Pasad, Felix Wu, Roshan S Sharma, Wei-Lun Wu, Hung-yi Lee, Karen Livescu, Shinji Watanabe. Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). 2023.",
      "abstract": "Suwon Shon, Siddhant Arora, Chyi-Jiunn Lin, Ankita Pasad, Felix Wu, Roshan S Sharma, Wei-Lun Wu, Hung-yi Lee, Karen Livescu, Shinji Watanabe. Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). 2023.",
      "doi": "https://doi.org/10.18653/v1/2023.acl-long.496",
      "openalex_id": "https://openalex.org/W4385571440",
      "arxiv_id": "",
      "publication_date": "2023-01-01",
      "published": "2023-01-01",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This item is a SLU benchmark focused on evaluating spoken language understanding tasks; it does not clearly present Spoken Language Models as the core object of modeling with end-to-end or hybrid architectures, so it likely does not meet the inclusion criteria.",
      "evaluation": 2
    },
    "round-A_JuniorNano_reasoning": "This item is a SLU benchmark focused on evaluating spoken language understanding tasks; it does not clearly present Spoken Language Models as the core object of modeling with end-to-end or hybrid architectures, so it likely does not meet the inclusion criteria.",
    "round-A_JuniorNano_evaluation": 2,
    "round-A_JuniorMini_output": {
      "reasoning": "The title and abstract mention a benchmark suite for diverse spoken language understanding tasks, which implies evaluation and possibly model architectures involving spoken language; however, the abstract lacks explicit mention of spoken language models or full-duplex interaction capabilities, and it is unclear if the work addresses speech as a core modeling object or synchronization aspects as required by the inclusion criteria. Furthermore, the abstract does not detail evaluation metrics related to low-latency interaction, cognitive parallelism, or speech overlap management, making it uncertain whether the study fully meets all the inclusion criteria without violating any exclusion criteria.",
      "evaluation": 3
    },
    "round-A_JuniorMini_reasoning": "The title and abstract mention a benchmark suite for diverse spoken language understanding tasks, which implies evaluation and possibly model architectures involving spoken language; however, the abstract lacks explicit mention of spoken language models or full-duplex interaction capabilities, and it is unclear if the work addresses speech as a core modeling object or synchronization aspects as required by the inclusion criteria. Furthermore, the abstract does not detail evaluation metrics related to low-latency interaction, cognitive parallelism, or speech overlap management, making it uncertain whether the study fully meets all the inclusion criteria without violating any exclusion criteria.",
    "round-A_JuniorMini_evaluation": 3,
    "round-B_SeniorLead_output": {
      "reasoning": "Both junior reviewers noted uncertainty and leaned toward exclusion; integrating their feedback, the title indicates a benchmark suite for Spoken Language Understanding (SLU) tasks but the provided abstract contains no methodological detail and does not explicitly mention spoken language models, language modeling of speech, or any full‑duplex/real‑time synchronization aspects required by the inclusion criteria—thus it fails the requirement that the paper explicitly center on Spoken Language Models (or equivalent phrasing) and describe architectures/training/evaluation in that scope.",
      "evaluation": 2
    },
    "round-B_SeniorLead_reasoning": "Both junior reviewers noted uncertainty and leaned toward exclusion; integrating their feedback, the title indicates a benchmark suite for Spoken Language Understanding (SLU) tasks but the provided abstract contains no methodological detail and does not explicitly mention spoken language models, language modeling of speech, or any full‑duplex/real‑time synchronization aspects required by the inclusion criteria—thus it fails the requirement that the paper explicitly center on Spoken Language Models (or equivalent phrasing) and describe architectures/training/evaluation in that scope.",
    "round-B_SeniorLead_evaluation": 2,
    "final_verdict": "exclude (senior:2)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Common Voice: A Massively-Multilingual Speech Corpus",
    "abstract": "The Common Voice corpus is a massively-multilingual collection of transcribed speech intended for speech technology research and development. Common Voice is designed for Automatic Speech Recognition purposes but can be useful in other domains (e.g. language identification). To achieve scale and sustainability, the Common Voice project employs crowdsourcing for both data collection and data validation. The most recent release includes 29 languages, and as of November 2019 there are a total of 38 languages collecting data. Over 50,000 individuals have participated so far, resulting in 2,500 hours of collected audio. To our knowledge this is the largest audio corpus in the public domain for speech recognition, both in terms of number of hours and number of languages. As an example use case for Common Voice, we present speech recognition experiments using Mozilla's DeepSpeech Speech-to-Text toolkit. By applying transfer learning from a source English model, we find an average Character Error Rate improvement of 5.99 +/- 5.48 for twelve target languages (German, French, Italian, Turkish, Catalan, Slovenian, Welsh, Irish, Breton, Tatar, Chuvash, and Kabyle). For most of these languages, these are the first ever published results on end-to-end Automatic Speech Recognition.",
    "metadata": {
      "title": "Common Voice: A Massively-Multilingual Speech Corpus",
      "summary": "The Common Voice corpus is a massively-multilingual collection of transcribed speech intended for speech technology research and development. Common Voice is designed for Automatic Speech Recognition purposes but can be useful in other domains (e.g. language identification). To achieve scale and sustainability, the Common Voice project employs crowdsourcing for both data collection and data validation. The most recent release includes 29 languages, and as of November 2019 there are a total of 38 languages collecting data. Over 50,000 individuals have participated so far, resulting in 2,500 hours of collected audio. To our knowledge this is the largest audio corpus in the public domain for speech recognition, both in terms of number of hours and number of languages. As an example use case for Common Voice, we present speech recognition experiments using Mozilla's DeepSpeech Speech-to-Text toolkit. By applying transfer learning from a source English model, we find an average Character Error Rate improvement of 5.99 +/- 5.48 for twelve target languages (German, French, Italian, Turkish, Catalan, Slovenian, Welsh, Irish, Breton, Tatar, Chuvash, and Kabyle). For most of these languages, these are the first ever published results on end-to-end Automatic Speech Recognition.",
      "abstract": "The Common Voice corpus is a massively-multilingual collection of transcribed speech intended for speech technology research and development. Common Voice is designed for Automatic Speech Recognition purposes but can be useful in other domains (e.g. language identification). To achieve scale and sustainability, the Common Voice project employs crowdsourcing for both data collection and data validation. The most recent release includes 29 languages, and as of November 2019 there are a total of 38 languages collecting data. Over 50,000 individuals have participated so far, resulting in 2,500 hours of collected audio. To our knowledge this is the largest audio corpus in the public domain for speech recognition, both in terms of number of hours and number of languages. As an example use case for Common Voice, we present speech recognition experiments using Mozilla's DeepSpeech Speech-to-Text toolkit. By applying transfer learning from a source English model, we find an average Character Error Rate improvement of 5.99 +/- 5.48 for twelve target languages (German, French, Italian, Turkish, Catalan, Slovenian, Welsh, Irish, Breton, Tatar, Chuvash, and Kabyle). For most of these languages, these are the first ever published results on end-to-end Automatic Speech Recognition.",
      "doi": "https://doi.org/10.48550/arxiv.1912.06670",
      "openalex_id": "https://openalex.org/W2995929068",
      "arxiv_id": "",
      "publication_date": "2019-12-13",
      "published": "2019-12-13",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item is about a multilingual speech corpus for ASR and does not centralize Spoken Language Models/FD-SLMs or dialog-style, full-duplex speech modeling; it is a data/resource paper rather than modeling, training, or evaluation of speech-language models, thus it should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item is about a multilingual speech corpus for ASR and does not centralize Spoken Language Models/FD-SLMs or dialog-style, full-duplex speech modeling; it is a data/resource paper rather than modeling, training, or evaluation of speech-language models, thus it should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study describes a large multilingual speech corpus intended primarily for Automatic Speech Recognition (ASR), focusing on data collection and ASR model training and evaluation; it does not explicitly focus on Spoken Language Models (SLMs) or Full-Duplex Spoken Language Models (FD-SLMs), nor does it discuss model architectures or evaluations related to synchronous interactive spoken language modeling or relevant behaviors such as cognitive parallelism or low-latency interaction. It mainly provides ASR results, thus not meeting the inclusion criteria focused on SLMs and synchronous dialogue interaction, and fits exclusion criteria emphasizing ASR-only focus without behavioral arbitration or synchronous interaction.",
      "evaluation": 2
    },
    "round-A_JuniorMini_reasoning": "The study describes a large multilingual speech corpus intended primarily for Automatic Speech Recognition (ASR), focusing on data collection and ASR model training and evaluation; it does not explicitly focus on Spoken Language Models (SLMs) or Full-Duplex Spoken Language Models (FD-SLMs), nor does it discuss model architectures or evaluations related to synchronous interactive spoken language modeling or relevant behaviors such as cognitive parallelism or low-latency interaction. It mainly provides ASR results, thus not meeting the inclusion criteria focused on SLMs and synchronous dialogue interaction, and fits exclusion criteria emphasizing ASR-only focus without behavioral arbitration or synchronous interaction.",
    "round-A_JuniorMini_evaluation": 2,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:2)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "WizardLM: Empowering large pre-trained language models to follow complex instructions",
    "abstract": "Training large language models (LLMs) with open-domain instruction following data brings colossal success. However, manually creating such instruction data is very time-consuming and labor-intensive. Moreover, humans may struggle to produce high-complexity instructions. In this paper, we show an avenue for creating large amounts of instruction data with varying levels of complexity using LLM instead of humans. Starting with an initial set of instructions, we use our proposed Evol-Instruct to rewrite them step by step into more complex instructions. Then, we mix all generated instruction data to fine-tune LLaMA. We call the resulting model WizardLM. Human evaluations on a complexity-balanced test bed and Vicuna's testset show that instructions from Evol-Instruct are superior to human-created ones. By analyzing the human evaluation results of the high complexity part, we demonstrate that outputs from our WizardLM are preferred to outputs from OpenAI ChatGPT. In GPT-4 automatic evaluation, WizardLM achieves more than 90\\% capacity of ChatGPT on 17 out of 29 skills. Even though WizardLM still lags behind ChatGPT in some aspects, our findings suggest that fine-tuning with AI-evolved instructions is a promising direction for enhancing LLMs. Our code and data are public at https://github.com/nlpxucan/WizardLM",
    "metadata": {
      "title": "WizardLM: Empowering large pre-trained language models to follow complex instructions",
      "summary": "Training large language models (LLMs) with open-domain instruction following data brings colossal success. However, manually creating such instruction data is very time-consuming and labor-intensive. Moreover, humans may struggle to produce high-complexity instructions. In this paper, we show an avenue for creating large amounts of instruction data with varying levels of complexity using LLM instead of humans. Starting with an initial set of instructions, we use our proposed Evol-Instruct to rewrite them step by step into more complex instructions. Then, we mix all generated instruction data to fine-tune LLaMA. We call the resulting model WizardLM. Human evaluations on a complexity-balanced test bed and Vicuna's testset show that instructions from Evol-Instruct are superior to human-created ones. By analyzing the human evaluation results of the high complexity part, we demonstrate that outputs from our WizardLM are preferred to outputs from OpenAI ChatGPT. In GPT-4 automatic evaluation, WizardLM achieves more than 90\\% capacity of ChatGPT on 17 out of 29 skills. Even though WizardLM still lags behind ChatGPT in some aspects, our findings suggest that fine-tuning with AI-evolved instructions is a promising direction for enhancing LLMs. Our code and data are public at https://github.com/nlpxucan/WizardLM",
      "abstract": "Training large language models (LLMs) with open-domain instruction following data brings colossal success. However, manually creating such instruction data is very time-consuming and labor-intensive. Moreover, humans may struggle to produce high-complexity instructions. In this paper, we show an avenue for creating large amounts of instruction data with varying levels of complexity using LLM instead of humans. Starting with an initial set of instructions, we use our proposed Evol-Instruct to rewrite them step by step into more complex instructions. Then, we mix all generated instruction data to fine-tune LLaMA. We call the resulting model WizardLM. Human evaluations on a complexity-balanced test bed and Vicuna's testset show that instructions from Evol-Instruct are superior to human-created ones. By analyzing the human evaluation results of the high complexity part, we demonstrate that outputs from our WizardLM are preferred to outputs from OpenAI ChatGPT. In GPT-4 automatic evaluation, WizardLM achieves more than 90\\% capacity of ChatGPT on 17 out of 29 skills. Even though WizardLM still lags behind ChatGPT in some aspects, our findings suggest that fine-tuning with AI-evolved instructions is a promising direction for enhancing LLMs. Our code and data are public at https://github.com/nlpxucan/WizardLM",
      "doi": "https://doi.org/10.48550/arxiv.2304.12244",
      "openalex_id": "https://openalex.org/W4367000491",
      "arxiv_id": "",
      "publication_date": "2023-04-24",
      "published": "2023-04-24",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This paper focuses on text-based instruction fine-tuning of LLMs and does not address Spoken Language Models, speech modeling, or real-time speech interaction, so it should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "This paper focuses on text-based instruction fine-tuning of LLMs and does not address Spoken Language Models, speech modeling, or real-time speech interaction, so it should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper focuses on fine-tuning large language models using AI-generated textual instructions to improve instruction-following ability; it does not address spoken language models, speech as a modeling object, synchronous speech interaction, or speech-language modeling architectures as specified in the inclusion criteria. It primarily centers on text-based LLM training and evaluation without involving speech signals, full-duplex interaction, or related benchmarks, thus failing the core thematic requirements.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper focuses on fine-tuning large language models using AI-generated textual instructions to improve instruction-following ability; it does not address spoken language models, speech as a modeling object, synchronous speech interaction, or speech-language modeling architectures as specified in the inclusion criteria. It primarily centers on text-based LLM training and evaluation without involving speech signals, full-duplex interaction, or related benchmarks, thus failing the core thematic requirements.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "HotpotQA: A Dataset for Diverse, Explainable Multi-hop Question Answering",
    "abstract": "Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William Cohen, Ruslan Salakhutdinov, Christopher D. Manning. Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing. 2018.",
    "metadata": {
      "title": "HotpotQA: A Dataset for Diverse, Explainable Multi-hop Question Answering",
      "summary": "Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William Cohen, Ruslan Salakhutdinov, Christopher D. Manning. Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing. 2018.",
      "abstract": "Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William Cohen, Ruslan Salakhutdinov, Christopher D. Manning. Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing. 2018.",
      "doi": "https://doi.org/10.18653/v1/d18-1259",
      "openalex_id": "https://openalex.org/W2889787757",
      "arxiv_id": "",
      "publication_date": "2018-01-01",
      "published": "2018-01-01",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This item is a 2018 text-based QA dataset (HotpotQA) with no focus on Spoken Language Models or speech-language modeling, and it predates the 3-year window, so it does not meet the inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "This item is a 2018 text-based QA dataset (HotpotQA) with no focus on Spoken Language Models or speech-language modeling, and it predates the 3-year window, so it does not meet the inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The title and abstract do not mention any focus on Spoken Language Models or full-duplex spoken language interactions, nor do they discuss speech as the primary modeling target; additionally, the study is from 2018 which is outside the 3-year inclusion window, and the abstract does not indicate any relevant model architecture, training, or evaluation related to speech language models as defined.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The title and abstract do not mention any focus on Spoken Language Models or full-duplex spoken language interactions, nor do they discuss speech as the primary modeling target; additionally, the study is from 2018 which is outside the 3-year inclusion window, and the abstract does not indicate any relevant model architecture, training, or evaluation related to speech language models as defined.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "<i>Did Aristotle Use a Laptop?</i>A Question Answering Benchmark with Implicit Reasoning Strategies",
    "abstract": "Abstract A key limitation in current datasets for multi-hop reasoning is that the required steps for answering the question are mentioned in it explicitly. In this work, we introduce StrategyQA, a question answering (QA) benchmark where the required reasoning steps are implicit in the question, and should be inferred using a strategy. A fundamental challenge in this setup is how to elicit such creative questions from crowdsourcing workers, while covering a broad range of potential strategies. We propose a data collection procedure that combines term-based priming to inspire annotators, careful control over the annotator population, and adversarial filtering for eliminating reasoning shortcuts. Moreover, we annotate each question with (1) a decomposition into reasoning steps for answering it, and (2) Wikipedia paragraphs that contain the answers to each step. Overall, StrategyQA includes 2,780 examples, each consisting of a strategy question, its decomposition, and evidence paragraphs. Analysis shows that questions in StrategyQA are short, topic-diverse, and cover a wide range of strategies. Empirically, we show that humans perform well (87%) on this task, while our best baseline reaches an accuracy of ∼ 66%.",
    "metadata": {
      "title": "<i>Did Aristotle Use a Laptop?</i>A Question Answering Benchmark with Implicit Reasoning Strategies",
      "summary": "Abstract A key limitation in current datasets for multi-hop reasoning is that the required steps for answering the question are mentioned in it explicitly. In this work, we introduce StrategyQA, a question answering (QA) benchmark where the required reasoning steps are implicit in the question, and should be inferred using a strategy. A fundamental challenge in this setup is how to elicit such creative questions from crowdsourcing workers, while covering a broad range of potential strategies. We propose a data collection procedure that combines term-based priming to inspire annotators, careful control over the annotator population, and adversarial filtering for eliminating reasoning shortcuts. Moreover, we annotate each question with (1) a decomposition into reasoning steps for answering it, and (2) Wikipedia paragraphs that contain the answers to each step. Overall, StrategyQA includes 2,780 examples, each consisting of a strategy question, its decomposition, and evidence paragraphs. Analysis shows that questions in StrategyQA are short, topic-diverse, and cover a wide range of strategies. Empirically, we show that humans perform well (87%) on this task, while our best baseline reaches an accuracy of ∼ 66%.",
      "abstract": "Abstract A key limitation in current datasets for multi-hop reasoning is that the required steps for answering the question are mentioned in it explicitly. In this work, we introduce StrategyQA, a question answering (QA) benchmark where the required reasoning steps are implicit in the question, and should be inferred using a strategy. A fundamental challenge in this setup is how to elicit such creative questions from crowdsourcing workers, while covering a broad range of potential strategies. We propose a data collection procedure that combines term-based priming to inspire annotators, careful control over the annotator population, and adversarial filtering for eliminating reasoning shortcuts. Moreover, we annotate each question with (1) a decomposition into reasoning steps for answering it, and (2) Wikipedia paragraphs that contain the answers to each step. Overall, StrategyQA includes 2,780 examples, each consisting of a strategy question, its decomposition, and evidence paragraphs. Analysis shows that questions in StrategyQA are short, topic-diverse, and cover a wide range of strategies. Empirically, we show that humans perform well (87%) on this task, while our best baseline reaches an accuracy of ∼ 66%.",
      "doi": "https://doi.org/10.1162/tacl_a_00370",
      "openalex_id": "https://openalex.org/W3159959439",
      "arxiv_id": "",
      "publication_date": "2021-01-01",
      "published": "2021-01-01",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "本項目是 StrategyQA 的文字多步推理問答基準，未以語音/語音模型為核心，且與語音互動評估無關，故不符合納入條件。",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "本項目是 StrategyQA 的文字多步推理問答基準，未以語音/語音模型為核心，且與語音互動評估無關，故不符合納入條件。",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The title and abstract focus on a question answering benchmark involving implicit reasoning strategies with no mention of spoken language models, speech language modeling, or full-duplex spoken language interaction, nor do they address any speech or acoustic signal modeling or end-to-end speech-text architectures. The work is centered purely on text-based multi-hop reasoning, which falls outside of the inclusion criteria that require the research to explicitly target spoken language models or speech as the main modeling object.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The title and abstract focus on a question answering benchmark involving implicit reasoning strategies with no mention of spoken language models, speech language modeling, or full-duplex spoken language interaction, nor do they address any speech or acoustic signal modeling or end-to-end speech-text architectures. The work is centered purely on text-based multi-hop reasoning, which falls outside of the inclusion criteria that require the research to explicitly target spoken language models or speech as the main modeling object.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Llama 2: Open Foundation and Fine-Tuned Chat Models",
    "abstract": "In this work, we develop and release Llama 2, a collection of pretrained and fine-tuned large language models (LLMs) ranging in scale from 7 billion to 70 billion parameters. Our fine-tuned LLMs, called Llama 2-Chat, are optimized for dialogue use cases. Our models outperform open-source chat models on most benchmarks we tested, and based on our human evaluations for helpfulness and safety, may be a suitable substitute for closed-source models. We provide a detailed description of our approach to fine-tuning and safety improvements of Llama 2-Chat in order to enable the community to build on our work and contribute to the responsible development of LLMs.",
    "metadata": {
      "title": "Llama 2: Open Foundation and Fine-Tuned Chat Models",
      "summary": "In this work, we develop and release Llama 2, a collection of pretrained and fine-tuned large language models (LLMs) ranging in scale from 7 billion to 70 billion parameters. Our fine-tuned LLMs, called Llama 2-Chat, are optimized for dialogue use cases. Our models outperform open-source chat models on most benchmarks we tested, and based on our human evaluations for helpfulness and safety, may be a suitable substitute for closed-source models. We provide a detailed description of our approach to fine-tuning and safety improvements of Llama 2-Chat in order to enable the community to build on our work and contribute to the responsible development of LLMs.",
      "abstract": "In this work, we develop and release Llama 2, a collection of pretrained and fine-tuned large language models (LLMs) ranging in scale from 7 billion to 70 billion parameters. Our fine-tuned LLMs, called Llama 2-Chat, are optimized for dialogue use cases. Our models outperform open-source chat models on most benchmarks we tested, and based on our human evaluations for helpfulness and safety, may be a suitable substitute for closed-source models. We provide a detailed description of our approach to fine-tuning and safety improvements of Llama 2-Chat in order to enable the community to build on our work and contribute to the responsible development of LLMs.",
      "doi": "https://doi.org/10.48550/arxiv.2307.09288",
      "openalex_id": "https://openalex.org/W4384918448",
      "arxiv_id": "",
      "publication_date": "2023-07-18",
      "published": "2023-07-18",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item centers on large language models (Llama 2) for text dialogue and does not address Spoken Language Models, speech-language modeling, or full-duplex interactive speech, thus it fails the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item centers on large language models (Llama 2) for text dialogue and does not address Spoken Language Models, speech-language modeling, or full-duplex interactive speech, thus it fails the inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The title and abstract describe the development of large language models (LLMs) optimized for dialogue, but they do not indicate any focus on spoken language modeling, acoustic signal processing, or full-duplex interactive speech capabilities as required. The study emphasizes text-based fine-tuning and evaluation, without addressing core speech modeling, speech–LLM architectures, or synchronous speech interaction features mandated by the inclusion criteria. Thus, it fails to meet the key aspects of spoken language modeling central to the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The title and abstract describe the development of large language models (LLMs) optimized for dialogue, but they do not indicate any focus on spoken language modeling, acoustic signal processing, or full-duplex interactive speech capabilities as required. The study emphasizes text-based fine-tuning and evaluation, without addressing core speech modeling, speech–LLM architectures, or synchronous speech interaction features mandated by the inclusion criteria. Thus, it fails to meet the key aspects of spoken language modeling central to the inclusion criteria.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Efficient Memory Management for Large Language Model Serving with PagedAttention",
    "abstract": "High throughput serving of large language models (LLMs) requires batching sufficiently many requests at a time. However, existing systems struggle because the key-value cache (KV cache) memory for each request is huge and grows and shrinks dynamically. When managed inefficiently, this memory can be significantly wasted by fragmentation and redundant duplication, limiting the batch size. To address this problem, we propose PagedAttention, an attention algorithm inspired by the classical virtual memory and paging techniques in operating systems. On top of it, we build vLLM, an LLM serving system that achieves (1) near-zero waste in KV cache memory and (2) flexible sharing of KV cache within and across requests to further reduce memory usage. Our evaluations show that vLLM improves the throughput of popular LLMs by 2--4× with the same level of latency compared to the state-of-the-art systems, such as FasterTransformer and Orca. The improvement is more pronounced with longer sequences, larger models, and more complex decoding algorithms. vLLM's source code is publicly available at https://github.com/vllm-project/vllm.",
    "metadata": {
      "title": "Efficient Memory Management for Large Language Model Serving with PagedAttention",
      "summary": "High throughput serving of large language models (LLMs) requires batching sufficiently many requests at a time. However, existing systems struggle because the key-value cache (KV cache) memory for each request is huge and grows and shrinks dynamically. When managed inefficiently, this memory can be significantly wasted by fragmentation and redundant duplication, limiting the batch size. To address this problem, we propose PagedAttention, an attention algorithm inspired by the classical virtual memory and paging techniques in operating systems. On top of it, we build vLLM, an LLM serving system that achieves (1) near-zero waste in KV cache memory and (2) flexible sharing of KV cache within and across requests to further reduce memory usage. Our evaluations show that vLLM improves the throughput of popular LLMs by 2--4× with the same level of latency compared to the state-of-the-art systems, such as FasterTransformer and Orca. The improvement is more pronounced with longer sequences, larger models, and more complex decoding algorithms. vLLM's source code is publicly available at https://github.com/vllm-project/vllm.",
      "abstract": "High throughput serving of large language models (LLMs) requires batching sufficiently many requests at a time. However, existing systems struggle because the key-value cache (KV cache) memory for each request is huge and grows and shrinks dynamically. When managed inefficiently, this memory can be significantly wasted by fragmentation and redundant duplication, limiting the batch size. To address this problem, we propose PagedAttention, an attention algorithm inspired by the classical virtual memory and paging techniques in operating systems. On top of it, we build vLLM, an LLM serving system that achieves (1) near-zero waste in KV cache memory and (2) flexible sharing of KV cache within and across requests to further reduce memory usage. Our evaluations show that vLLM improves the throughput of popular LLMs by 2--4× with the same level of latency compared to the state-of-the-art systems, such as FasterTransformer and Orca. The improvement is more pronounced with longer sequences, larger models, and more complex decoding algorithms. vLLM's source code is publicly available at https://github.com/vllm-project/vllm.",
      "doi": "https://doi.org/10.1145/3600006.3613165",
      "openalex_id": "https://openalex.org/W4387321091",
      "arxiv_id": "",
      "publication_date": "2023-10-03",
      "published": "2023-10-03",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "Reasoning: The inclusion criteria require focus on Spoken Language Models or speech-language modeling with core emphasis on audio/speech as modeling target and evaluation of speech-related tasks or interactive speech/dialogue aspects; the provided title/abstract discuss memory management for LLM inference (PagedAttention/vLLM) with no speech/speech-vision modeling focus, no dialogue interaction or real-time speech considerations, and no speech-centric evaluation. Therefore it does not meet the inclusion criteria and should be excluded per the exclusion criteria as well (not addressing speech-language modeling).",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "Reasoning: The inclusion criteria require focus on Spoken Language Models or speech-language modeling with core emphasis on audio/speech as modeling target and evaluation of speech-related tasks or interactive speech/dialogue aspects; the provided title/abstract discuss memory management for LLM inference (PagedAttention/vLLM) with no speech/speech-vision modeling focus, no dialogue interaction or real-time speech considerations, and no speech-centric evaluation. Therefore it does not meet the inclusion criteria and should be excluded per the exclusion criteria as well (not addressing speech-language modeling).",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on efficient memory management for serving large language models and does not address spoken language modeling, speech as a core modeling object, or full-duplex spoken interaction capabilities as required by the inclusion criteria. Instead, it centers on computation and system level optimization without involvement in speech language modeling, synchronization, or evaluation of interactive speech behaviors.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on efficient memory management for serving large language models and does not address spoken language modeling, speech as a core modeling object, or full-duplex spoken interaction capabilities as required by the inclusion criteria. Instead, it centers on computation and system level optimization without involvement in speech language modeling, synchronization, or evaluation of interactive speech behaviors.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Improving the Robustness of Capsule Networks to Image Affine Transformations",
    "abstract": "Convolutional neural networks (CNNs) achieve translational invariance by using pooling operations. However, the operations do not preserve the spatial relationships in the learned representations. Hence, CNNs cannot extrapolate to various geometric transformations of inputs. Recently, Capsule Networks (CapsNets) have been proposed to tackle this problem. In CapsNets, each entity is represented by a vector and routed to high-level entity representations by a dynamic routing algorithm. CapsNets have been shown to be more robust than CNNs to affine transformations of inputs. However, there is still a huge gap between their performance on transformed inputs compared to untransformed versions. In this work, we first revisit the routing procedure by (un)rolling its forward and backward passes. Our investigation reveals that the routing procedure contributes neither to the generalization ability nor to the affine robustness of the CapsNets. Furthermore, we explore the limitations of capsule transformations and propose affine CapsNets (Aff-CapsNets), which are more robust to affine transformations. On our benchmark task, where models are trained on the MNIST dataset and tested on the AffNIST dataset, our Aff-CapsNets improve the benchmark performance by a large margin (from 79% to 93.21%), without using any routing mechanism.",
    "metadata": {
      "title": "Improving the Robustness of Capsule Networks to Image Affine Transformations",
      "summary": "Convolutional neural networks (CNNs) achieve translational invariance by using pooling operations. However, the operations do not preserve the spatial relationships in the learned representations. Hence, CNNs cannot extrapolate to various geometric transformations of inputs. Recently, Capsule Networks (CapsNets) have been proposed to tackle this problem. In CapsNets, each entity is represented by a vector and routed to high-level entity representations by a dynamic routing algorithm. CapsNets have been shown to be more robust than CNNs to affine transformations of inputs. However, there is still a huge gap between their performance on transformed inputs compared to untransformed versions. In this work, we first revisit the routing procedure by (un)rolling its forward and backward passes. Our investigation reveals that the routing procedure contributes neither to the generalization ability nor to the affine robustness of the CapsNets. Furthermore, we explore the limitations of capsule transformations and propose affine CapsNets (Aff-CapsNets), which are more robust to affine transformations. On our benchmark task, where models are trained on the MNIST dataset and tested on the AffNIST dataset, our Aff-CapsNets improve the benchmark performance by a large margin (from 79% to 93.21%), without using any routing mechanism.",
      "abstract": "Convolutional neural networks (CNNs) achieve translational invariance by using pooling operations. However, the operations do not preserve the spatial relationships in the learned representations. Hence, CNNs cannot extrapolate to various geometric transformations of inputs. Recently, Capsule Networks (CapsNets) have been proposed to tackle this problem. In CapsNets, each entity is represented by a vector and routed to high-level entity representations by a dynamic routing algorithm. CapsNets have been shown to be more robust than CNNs to affine transformations of inputs. However, there is still a huge gap between their performance on transformed inputs compared to untransformed versions. In this work, we first revisit the routing procedure by (un)rolling its forward and backward passes. Our investigation reveals that the routing procedure contributes neither to the generalization ability nor to the affine robustness of the CapsNets. Furthermore, we explore the limitations of capsule transformations and propose affine CapsNets (Aff-CapsNets), which are more robust to affine transformations. On our benchmark task, where models are trained on the MNIST dataset and tested on the AffNIST dataset, our Aff-CapsNets improve the benchmark performance by a large margin (from 79% to 93.21%), without using any routing mechanism.",
      "doi": "https://doi.org/10.1109/cvpr42600.2020.00731",
      "openalex_id": "https://openalex.org/W3034302278",
      "arxiv_id": "",
      "publication_date": "2020-06-01",
      "published": "2020-06-01",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item focuses on Capsule Networks for image transformations (CV), with no emphasis on spoken language modeling or speech as the core modeling object, thus it does not meet the spoken-language-model inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item focuses on Capsule Networks for image transformations (CV), with no emphasis on spoken language modeling or speech as the core modeling object, thus it does not meet the spoken-language-model inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper focuses on improving Capsule Networks for image affine transformations, a topic in computer vision and deep learning, with no mention of spoken language models, speech language models, or any speech-related modeling, interaction, or evaluation from the abstract or title. It does not meet the inclusion criteria centered on spoken language models or full-duplex spoken interaction modeling and also does not address any related speech processing or interaction aspects, hence it fails the inclusion criteria entirely.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper focuses on improving Capsule Networks for image affine transformations, a topic in computer vision and deep learning, with no mention of spoken language models, speech language models, or any speech-related modeling, interaction, or evaluation from the abstract or title. It does not meet the inclusion criteria centered on spoken language models or full-duplex spoken interaction modeling and also does not address any related speech processing or interaction aspects, hence it fails the inclusion criteria entirely.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Video Watermarking: Safeguarding Your Video from (Unauthorized) Annotations by Video-based LLMs",
    "abstract": "The advent of video-based Large Language Models (LLMs) has significantly enhanced video understanding. However, it has also raised some safety concerns regarding data protection, as videos can be more easily annotated, even without authorization. This paper introduces Video Watermarking, a novel technique to protect videos from unauthorized annotations by such video-based LLMs, especially concerning the video content and description, in response to specific queries. By imperceptibly embedding watermarks into key video frames with multi-modal flow-based losses, our method preserves the viewing experience while preventing misuse by video-based LLMs. Extensive experiments show that Video Watermarking significantly reduces the comprehensibility of videos with various video-based LLMs, demonstrating both stealth and robustness. In essence, our method provides a solution for securing video content, ensuring its integrity and confidentiality in the face of evolving video-based LLMs technologies.",
    "metadata": {
      "title": "Video Watermarking: Safeguarding Your Video from (Unauthorized) Annotations by Video-based LLMs",
      "summary": "The advent of video-based Large Language Models (LLMs) has significantly enhanced video understanding. However, it has also raised some safety concerns regarding data protection, as videos can be more easily annotated, even without authorization. This paper introduces Video Watermarking, a novel technique to protect videos from unauthorized annotations by such video-based LLMs, especially concerning the video content and description, in response to specific queries. By imperceptibly embedding watermarks into key video frames with multi-modal flow-based losses, our method preserves the viewing experience while preventing misuse by video-based LLMs. Extensive experiments show that Video Watermarking significantly reduces the comprehensibility of videos with various video-based LLMs, demonstrating both stealth and robustness. In essence, our method provides a solution for securing video content, ensuring its integrity and confidentiality in the face of evolving video-based LLMs technologies.",
      "abstract": "The advent of video-based Large Language Models (LLMs) has significantly enhanced video understanding. However, it has also raised some safety concerns regarding data protection, as videos can be more easily annotated, even without authorization. This paper introduces Video Watermarking, a novel technique to protect videos from unauthorized annotations by such video-based LLMs, especially concerning the video content and description, in response to specific queries. By imperceptibly embedding watermarks into key video frames with multi-modal flow-based losses, our method preserves the viewing experience while preventing misuse by video-based LLMs. Extensive experiments show that Video Watermarking significantly reduces the comprehensibility of videos with various video-based LLMs, demonstrating both stealth and robustness. In essence, our method provides a solution for securing video content, ensuring its integrity and confidentiality in the face of evolving video-based LLMs technologies.",
      "doi": "https://doi.org/10.48550/arxiv.2407.02411",
      "openalex_id": "https://openalex.org/W4400375155",
      "arxiv_id": "",
      "publication_date": "2024-07-02",
      "published": "2024-07-02",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item centers on video watermarking to protect videos from annotations by video-based LLMs and does not treat Spoken Language Models or speech-language modeling as the core object of study, thus not meeting the inclusion criteria for SLM/FD-SLM focus.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item centers on video watermarking to protect videos from annotations by video-based LLMs and does not treat Spoken Language Models or speech-language modeling as the core object of study, thus not meeting the inclusion criteria for SLM/FD-SLM focus.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on video watermarking to prevent unauthorized annotations by video-based LLMs and does not address spoken language models or speech language modeling. It lacks discussion on speech as the primary modeling object or any relevant spoken language modeling architecture, training, or evaluation. Therefore, it falls outside the inclusion criteria and does not discuss synchronous speech interaction, behavior arbitration, or related assessments. Hence, it should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on video watermarking to prevent unauthorized annotations by video-based LLMs and does not address spoken language models or speech language modeling. It lacks discussion on speech as the primary modeling object or any relevant spoken language modeling architecture, training, or evaluation. Therefore, it falls outside the inclusion criteria and does not discuss synchronous speech interaction, behavior arbitration, or related assessments. Hence, it should be excluded.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Adversarial Robustness for Visual Grounding of Multimodal Large Language Models",
    "abstract": "Multi-modal Large Language Models (MLLMs) have recently achieved enhanced performance across various vision-language tasks including visual grounding capabilities. However, the adversarial robustness of visual grounding remains unexplored in MLLMs. To fill this gap, we use referring expression comprehension (REC) as an example task in visual grounding and propose three adversarial attack paradigms as follows. Firstly, untargeted adversarial attacks induce MLLMs to generate incorrect bounding boxes for each object. Besides, exclusive targeted adversarial attacks cause all generated outputs to the same target bounding box. In addition, permuted targeted adversarial attacks aim to permute all bounding boxes among different objects within a single image. Extensive experiments demonstrate that the proposed methods can successfully attack visual grounding capabilities of MLLMs. Our methods not only provide a new perspective for designing novel attacks but also serve as a strong baseline for improving the adversarial robustness for visual grounding of MLLMs.",
    "metadata": {
      "title": "Adversarial Robustness for Visual Grounding of Multimodal Large Language Models",
      "summary": "Multi-modal Large Language Models (MLLMs) have recently achieved enhanced performance across various vision-language tasks including visual grounding capabilities. However, the adversarial robustness of visual grounding remains unexplored in MLLMs. To fill this gap, we use referring expression comprehension (REC) as an example task in visual grounding and propose three adversarial attack paradigms as follows. Firstly, untargeted adversarial attacks induce MLLMs to generate incorrect bounding boxes for each object. Besides, exclusive targeted adversarial attacks cause all generated outputs to the same target bounding box. In addition, permuted targeted adversarial attacks aim to permute all bounding boxes among different objects within a single image. Extensive experiments demonstrate that the proposed methods can successfully attack visual grounding capabilities of MLLMs. Our methods not only provide a new perspective for designing novel attacks but also serve as a strong baseline for improving the adversarial robustness for visual grounding of MLLMs.",
      "abstract": "Multi-modal Large Language Models (MLLMs) have recently achieved enhanced performance across various vision-language tasks including visual grounding capabilities. However, the adversarial robustness of visual grounding remains unexplored in MLLMs. To fill this gap, we use referring expression comprehension (REC) as an example task in visual grounding and propose three adversarial attack paradigms as follows. Firstly, untargeted adversarial attacks induce MLLMs to generate incorrect bounding boxes for each object. Besides, exclusive targeted adversarial attacks cause all generated outputs to the same target bounding box. In addition, permuted targeted adversarial attacks aim to permute all bounding boxes among different objects within a single image. Extensive experiments demonstrate that the proposed methods can successfully attack visual grounding capabilities of MLLMs. Our methods not only provide a new perspective for designing novel attacks but also serve as a strong baseline for improving the adversarial robustness for visual grounding of MLLMs.",
      "doi": "https://doi.org/10.48550/arxiv.2405.09981",
      "openalex_id": "https://openalex.org/W4397028323",
      "arxiv_id": "",
      "publication_date": "2024-05-16",
      "published": "2024-05-16",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This item concerns adversarial robustness for visual grounding in multimodal LLMs and does not center Spoken Language Models or speech-language modeling, thus should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "This item concerns adversarial robustness for visual grounding in multimodal LLMs and does not center Spoken Language Models or speech-language modeling, thus should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on adversarial robustness for visual grounding in multi-modal large language models, which primarily involves vision-language tasks, without addressing any spoken language modeling or speech-centric architecture, training, or evaluation aspects relevant to the inclusion criteria; thus, it does not meet the thematic focus on spoken language models or full-duplex speech interaction as specified.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on adversarial robustness for visual grounding in multi-modal large language models, which primarily involves vision-language tasks, without addressing any spoken language modeling or speech-centric architecture, training, or evaluation aspects relevant to the inclusion criteria; thus, it does not meet the thematic focus on spoken language models or full-duplex speech interaction as specified.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Imperceptible and Robust Backdoor Attack in 3D Point Cloud",
    "abstract": "With the thriving of deep learning in processing point cloud data, recent works show that backdoor attacks pose a severe security threat to 3D vision applications. The attacker injects the backdoor into the 3D model by poisoning a few training samples with trigger, such that the backdoored model performs well on clean samples but behaves maliciously when the trigger pattern appears. Existing attacks often insert some additional points into the point cloud as the trigger, or utilize a linear transformation (e.g., rotation) to construct the poisoned point cloud. However, the effects of these poisoned samples are likely to be weakened or even eliminated by some commonly used pre-processing techniques for 3D point cloud, e.g., outlier removal or rotation augmentation. In this paper, we propose a novel imperceptible and robust backdoor attack (IRBA) to tackle this challenge. We utilize a nonlinear and local transformation, called weighted local transformation (WLT), to construct poisoned samples with unique transformations. As there are several hyper-parameters and randomness in WLT, it is difficult to produce two similar transformations. Consequently, poisoned samples with unique transformations are likely to be resistant to aforementioned pre-processing techniques. Besides, the distortion caused by a fixed WLT is both controllable and smooth, resulting in the generated poisoned samples that are imperceptible to human inspection. Extensive experiments on three benchmark datasets and four models show that IRBA achieves <inline-formula xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"> <tex-math notation=\"LaTeX\">$80\\%+$ </tex-math></inline-formula> attack success rate (ASR) in most cases even with pre-processing techniques, which is significantly higher than previous state-of-the-art attacks. Our code is available at <uri xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">https://github.com/KuofengGao/IRBA</uri> .",
    "metadata": {
      "title": "Imperceptible and Robust Backdoor Attack in 3D Point Cloud",
      "summary": "With the thriving of deep learning in processing point cloud data, recent works show that backdoor attacks pose a severe security threat to 3D vision applications. The attacker injects the backdoor into the 3D model by poisoning a few training samples with trigger, such that the backdoored model performs well on clean samples but behaves maliciously when the trigger pattern appears. Existing attacks often insert some additional points into the point cloud as the trigger, or utilize a linear transformation (e.g., rotation) to construct the poisoned point cloud. However, the effects of these poisoned samples are likely to be weakened or even eliminated by some commonly used pre-processing techniques for 3D point cloud, e.g., outlier removal or rotation augmentation. In this paper, we propose a novel imperceptible and robust backdoor attack (IRBA) to tackle this challenge. We utilize a nonlinear and local transformation, called weighted local transformation (WLT), to construct poisoned samples with unique transformations. As there are several hyper-parameters and randomness in WLT, it is difficult to produce two similar transformations. Consequently, poisoned samples with unique transformations are likely to be resistant to aforementioned pre-processing techniques. Besides, the distortion caused by a fixed WLT is both controllable and smooth, resulting in the generated poisoned samples that are imperceptible to human inspection. Extensive experiments on three benchmark datasets and four models show that IRBA achieves <inline-formula xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"> <tex-math notation=\"LaTeX\">$80\\%+$ </tex-math></inline-formula> attack success rate (ASR) in most cases even with pre-processing techniques, which is significantly higher than previous state-of-the-art attacks. Our code is available at <uri xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">https://github.com/KuofengGao/IRBA</uri> .",
      "abstract": "With the thriving of deep learning in processing point cloud data, recent works show that backdoor attacks pose a severe security threat to 3D vision applications. The attacker injects the backdoor into the 3D model by poisoning a few training samples with trigger, such that the backdoored model performs well on clean samples but behaves maliciously when the trigger pattern appears. Existing attacks often insert some additional points into the point cloud as the trigger, or utilize a linear transformation (e.g., rotation) to construct the poisoned point cloud. However, the effects of these poisoned samples are likely to be weakened or even eliminated by some commonly used pre-processing techniques for 3D point cloud, e.g., outlier removal or rotation augmentation. In this paper, we propose a novel imperceptible and robust backdoor attack (IRBA) to tackle this challenge. We utilize a nonlinear and local transformation, called weighted local transformation (WLT), to construct poisoned samples with unique transformations. As there are several hyper-parameters and randomness in WLT, it is difficult to produce two similar transformations. Consequently, poisoned samples with unique transformations are likely to be resistant to aforementioned pre-processing techniques. Besides, the distortion caused by a fixed WLT is both controllable and smooth, resulting in the generated poisoned samples that are imperceptible to human inspection. Extensive experiments on three benchmark datasets and four models show that IRBA achieves <inline-formula xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"> <tex-math notation=\"LaTeX\">$80\\%+$ </tex-math></inline-formula> attack success rate (ASR) in most cases even with pre-processing techniques, which is significantly higher than previous state-of-the-art attacks. Our code is available at <uri xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">https://github.com/KuofengGao/IRBA</uri> .",
      "doi": "https://doi.org/10.1109/tifs.2023.3333687",
      "openalex_id": "https://openalex.org/W4388726687",
      "arxiv_id": "",
      "publication_date": "2023-11-16",
      "published": "2023-11-16",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "此項目聚焦於3D點雲的後門攻擊，屬於計算機視覺/安全領域，與語音語言模型（SLMs/FD-SLMs）及語音建模的納入條件無關，因此不符合納入標準。",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "此項目聚焦於3D點雲的後門攻擊，屬於計算機視覺/安全領域，與語音語言模型（SLMs/FD-SLMs）及語音建模的納入條件無關，因此不符合納入標準。",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on backdoor attacks in 3D point cloud deep learning models, which is unrelated to Spoken Language Models or full-duplex spoken language interaction as required by the inclusion criteria; it neither discusses speech language modeling nor any relevant architectures, training, or evaluation concerning speech or voice modeling.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on backdoor attacks in 3D point cloud deep learning models, which is unrelated to Spoken Language Models or full-duplex spoken language interaction as required by the inclusion criteria; it neither discusses speech language modeling nor any relevant architectures, training, or evaluation concerning speech or voice modeling.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Inducing High Energy-Latency of Large Vision-Language Models with Verbose Images",
    "abstract": "Large vision-language models (VLMs) such as GPT-4 have achieved exceptional performance across various multi-modal tasks. However, the deployment of VLMs necessitates substantial energy consumption and computational resources. Once attackers maliciously induce high energy consumption and latency time (energy-latency cost) during inference of VLMs, it will exhaust computational resources. In this paper, we explore this attack surface about availability of VLMs and aim to induce high energy-latency cost during inference of VLMs. We find that high energy-latency cost during inference of VLMs can be manipulated by maximizing the length of generated sequences. To this end, we propose verbose images, with the goal of crafting an imperceptible perturbation to induce VLMs to generate long sentences during inference. Concretely, we design three loss objectives. First, a loss is proposed to delay the occurrence of end-of-sequence (EOS) token, where EOS token is a signal for VLMs to stop generating further tokens. Moreover, an uncertainty loss and a token diversity loss are proposed to increase the uncertainty over each generated token and the diversity among all tokens of the whole generated sequence, respectively, which can break output dependency at token-level and sequence-level. Furthermore, a temporal weight adjustment algorithm is proposed, which can effectively balance these losses. Extensive experiments demonstrate that our verbose images can increase the length of generated sequences by 7.87 times and 8.56 times compared to original images on MS-COCO and ImageNet datasets, which presents potential challenges for various applications. Our code is available at https://github.com/KuofengGao/Verbose_Images.",
    "metadata": {
      "title": "Inducing High Energy-Latency of Large Vision-Language Models with Verbose Images",
      "summary": "Large vision-language models (VLMs) such as GPT-4 have achieved exceptional performance across various multi-modal tasks. However, the deployment of VLMs necessitates substantial energy consumption and computational resources. Once attackers maliciously induce high energy consumption and latency time (energy-latency cost) during inference of VLMs, it will exhaust computational resources. In this paper, we explore this attack surface about availability of VLMs and aim to induce high energy-latency cost during inference of VLMs. We find that high energy-latency cost during inference of VLMs can be manipulated by maximizing the length of generated sequences. To this end, we propose verbose images, with the goal of crafting an imperceptible perturbation to induce VLMs to generate long sentences during inference. Concretely, we design three loss objectives. First, a loss is proposed to delay the occurrence of end-of-sequence (EOS) token, where EOS token is a signal for VLMs to stop generating further tokens. Moreover, an uncertainty loss and a token diversity loss are proposed to increase the uncertainty over each generated token and the diversity among all tokens of the whole generated sequence, respectively, which can break output dependency at token-level and sequence-level. Furthermore, a temporal weight adjustment algorithm is proposed, which can effectively balance these losses. Extensive experiments demonstrate that our verbose images can increase the length of generated sequences by 7.87 times and 8.56 times compared to original images on MS-COCO and ImageNet datasets, which presents potential challenges for various applications. Our code is available at https://github.com/KuofengGao/Verbose_Images.",
      "abstract": "Large vision-language models (VLMs) such as GPT-4 have achieved exceptional performance across various multi-modal tasks. However, the deployment of VLMs necessitates substantial energy consumption and computational resources. Once attackers maliciously induce high energy consumption and latency time (energy-latency cost) during inference of VLMs, it will exhaust computational resources. In this paper, we explore this attack surface about availability of VLMs and aim to induce high energy-latency cost during inference of VLMs. We find that high energy-latency cost during inference of VLMs can be manipulated by maximizing the length of generated sequences. To this end, we propose verbose images, with the goal of crafting an imperceptible perturbation to induce VLMs to generate long sentences during inference. Concretely, we design three loss objectives. First, a loss is proposed to delay the occurrence of end-of-sequence (EOS) token, where EOS token is a signal for VLMs to stop generating further tokens. Moreover, an uncertainty loss and a token diversity loss are proposed to increase the uncertainty over each generated token and the diversity among all tokens of the whole generated sequence, respectively, which can break output dependency at token-level and sequence-level. Furthermore, a temporal weight adjustment algorithm is proposed, which can effectively balance these losses. Extensive experiments demonstrate that our verbose images can increase the length of generated sequences by 7.87 times and 8.56 times compared to original images on MS-COCO and ImageNet datasets, which presents potential challenges for various applications. Our code is available at https://github.com/KuofengGao/Verbose_Images.",
      "doi": "https://doi.org/10.48550/arxiv.2401.11170",
      "openalex_id": "https://openalex.org/W4391157527",
      "arxiv_id": "",
      "publication_date": "2024-01-20",
      "published": "2024-01-20",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The article investigates energy-latency attacks on large vision-language models using verbose images; it targets VLMs, not Spoken Language Models or speech-language modeling, and does not address speech-related architectures or evaluation. Therefore it does not meet the inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The article investigates energy-latency attacks on large vision-language models using verbose images; it targets VLMs, not Spoken Language Models or speech-language modeling, and does not address speech-related architectures or evaluation. Therefore it does not meet the inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The article focuses on vision-language models and the induced energy-latency attacks by manipulating the length of generated sequences using verbose images; it does not involve spoken language models, speech language modeling, or any of the key concepts related to speech/audio-based language modeling or full-duplex spoken interaction as specified in the inclusion criteria. Therefore, it falls outside the defined scope of spoken language model research and does not meet the inclusion criteria, making it not relevant to the topic.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The article focuses on vision-language models and the induced energy-latency attacks by manipulating the length of generated sequences using verbose images; it does not involve spoken language models, speech language modeling, or any of the key concepts related to speech/audio-based language modeling or full-duplex spoken interaction as specified in the inclusion criteria. Therefore, it falls outside the defined scope of spoken language model research and does not meet the inclusion criteria, making it not relevant to the topic.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Energy-Latency Manipulation of Multi-modal Large Language Models via Verbose Samples",
    "abstract": "Despite the exceptional performance of multi-modal large language models (MLLMs), their deployment requires substantial computational resources. Once malicious users induce high energy consumption and latency time (energy-latency cost), it will exhaust computational resources and harm availability of service. In this paper, we investigate this vulnerability for MLLMs, particularly image-based and video-based ones, and aim to induce high energy-latency cost during inference by crafting an imperceptible perturbation. We find that high energy-latency cost can be manipulated by maximizing the length of generated sequences, which motivates us to propose verbose samples, including verbose images and videos. Concretely, two modality non-specific losses are proposed, including a loss to delay end-of-sequence (EOS) token and an uncertainty loss to increase the uncertainty over each generated token. In addition, improving diversity is important to encourage longer responses by increasing the complexity, which inspires the following modality specific loss. For verbose images, a token diversity loss is proposed to promote diverse hidden states. For verbose videos, a frame feature diversity loss is proposed to increase the feature diversity among frames. To balance these losses, we propose a temporal weight adjustment algorithm. Experiments demonstrate that our verbose samples can largely extend the length of generated sequences.",
    "metadata": {
      "title": "Energy-Latency Manipulation of Multi-modal Large Language Models via Verbose Samples",
      "summary": "Despite the exceptional performance of multi-modal large language models (MLLMs), their deployment requires substantial computational resources. Once malicious users induce high energy consumption and latency time (energy-latency cost), it will exhaust computational resources and harm availability of service. In this paper, we investigate this vulnerability for MLLMs, particularly image-based and video-based ones, and aim to induce high energy-latency cost during inference by crafting an imperceptible perturbation. We find that high energy-latency cost can be manipulated by maximizing the length of generated sequences, which motivates us to propose verbose samples, including verbose images and videos. Concretely, two modality non-specific losses are proposed, including a loss to delay end-of-sequence (EOS) token and an uncertainty loss to increase the uncertainty over each generated token. In addition, improving diversity is important to encourage longer responses by increasing the complexity, which inspires the following modality specific loss. For verbose images, a token diversity loss is proposed to promote diverse hidden states. For verbose videos, a frame feature diversity loss is proposed to increase the feature diversity among frames. To balance these losses, we propose a temporal weight adjustment algorithm. Experiments demonstrate that our verbose samples can largely extend the length of generated sequences.",
      "abstract": "Despite the exceptional performance of multi-modal large language models (MLLMs), their deployment requires substantial computational resources. Once malicious users induce high energy consumption and latency time (energy-latency cost), it will exhaust computational resources and harm availability of service. In this paper, we investigate this vulnerability for MLLMs, particularly image-based and video-based ones, and aim to induce high energy-latency cost during inference by crafting an imperceptible perturbation. We find that high energy-latency cost can be manipulated by maximizing the length of generated sequences, which motivates us to propose verbose samples, including verbose images and videos. Concretely, two modality non-specific losses are proposed, including a loss to delay end-of-sequence (EOS) token and an uncertainty loss to increase the uncertainty over each generated token. In addition, improving diversity is important to encourage longer responses by increasing the complexity, which inspires the following modality specific loss. For verbose images, a token diversity loss is proposed to promote diverse hidden states. For verbose videos, a frame feature diversity loss is proposed to increase the feature diversity among frames. To balance these losses, we propose a temporal weight adjustment algorithm. Experiments demonstrate that our verbose samples can largely extend the length of generated sequences.",
      "doi": "https://doi.org/10.48550/arxiv.2404.16557",
      "openalex_id": "https://openalex.org/W4395687052",
      "arxiv_id": "",
      "publication_date": "2024-04-25",
      "published": "2024-04-25",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This item focuses on energy-latency manipulation in multimodal LLMs and does not center on Spoken Language Models or Speech Language Modeling, nor on real-time speech interaction or latency-aware dialogue; it fails the inclusion criteria and aligns with exclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "This item focuses on energy-latency manipulation in multimodal LLMs and does not center on Spoken Language Models or Speech Language Modeling, nor on real-time speech interaction or latency-aware dialogue; it fails the inclusion criteria and aligns with exclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on manipulating energy-latency costs in multi-modal large language models with image and video inputs and does not focus on spoken language models or speech as core input for modeling and inference; it also lacks discussion on full-duplex spoken interaction, synchronization, or spoken language modeling. Therefore, it does not meet the inclusion criteria centered on Spoken Language Models with speech as core modeling objects and corresponding evaluation.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on manipulating energy-latency costs in multi-modal large language models with image and video inputs and does not focus on spoken language models or speech as core input for modeling and inference; it also lacks discussion on full-duplex spoken interaction, synchronization, or spoken language modeling. Therefore, it does not meet the inclusion criteria centered on Spoken Language Models with speech as core modeling objects and corresponding evaluation.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Denial-of-Service Poisoning Attacks against Large Language Models",
    "abstract": "Recent studies have shown that LLMs are vulnerable to denial-of-service (DoS) attacks, where adversarial inputs like spelling errors or non-semantic prompts trigger endless outputs without generating an [EOS] token. These attacks can potentially cause high latency and make LLM services inaccessible to other users or tasks. However, when there are speech-to-text interfaces (e.g., voice commands to a robot), executing such DoS attacks becomes challenging, as it is difficult to introduce spelling errors or non-semantic prompts through speech. A simple DoS attack in these scenarios would be to instruct the model to \"Keep repeating Hello\", but we observe that relying solely on natural instructions limits output length, which is bounded by the maximum length of the LLM's supervised finetuning (SFT) data. To overcome this limitation, we propose poisoning-based DoS (P-DoS) attacks for LLMs, demonstrating that injecting a single poisoned sample designed for DoS purposes can break the output length limit. For example, a poisoned sample can successfully attack GPT-4o and GPT-4o mini (via OpenAI's finetuning API) using less than $1, causing repeated outputs up to the maximum inference length (16K tokens, compared to 0.5K before poisoning). Additionally, we perform comprehensive ablation studies on open-source LLMs and extend our method to LLM agents, where attackers can control both the finetuning dataset and algorithm. Our findings underscore the urgent need for defenses against P-DoS attacks to secure LLMs. Our code is available at https://github.com/sail-sg/P-DoS.",
    "metadata": {
      "title": "Denial-of-Service Poisoning Attacks against Large Language Models",
      "summary": "Recent studies have shown that LLMs are vulnerable to denial-of-service (DoS) attacks, where adversarial inputs like spelling errors or non-semantic prompts trigger endless outputs without generating an [EOS] token. These attacks can potentially cause high latency and make LLM services inaccessible to other users or tasks. However, when there are speech-to-text interfaces (e.g., voice commands to a robot), executing such DoS attacks becomes challenging, as it is difficult to introduce spelling errors or non-semantic prompts through speech. A simple DoS attack in these scenarios would be to instruct the model to \"Keep repeating Hello\", but we observe that relying solely on natural instructions limits output length, which is bounded by the maximum length of the LLM's supervised finetuning (SFT) data. To overcome this limitation, we propose poisoning-based DoS (P-DoS) attacks for LLMs, demonstrating that injecting a single poisoned sample designed for DoS purposes can break the output length limit. For example, a poisoned sample can successfully attack GPT-4o and GPT-4o mini (via OpenAI's finetuning API) using less than $1, causing repeated outputs up to the maximum inference length (16K tokens, compared to 0.5K before poisoning). Additionally, we perform comprehensive ablation studies on open-source LLMs and extend our method to LLM agents, where attackers can control both the finetuning dataset and algorithm. Our findings underscore the urgent need for defenses against P-DoS attacks to secure LLMs. Our code is available at https://github.com/sail-sg/P-DoS.",
      "abstract": "Recent studies have shown that LLMs are vulnerable to denial-of-service (DoS) attacks, where adversarial inputs like spelling errors or non-semantic prompts trigger endless outputs without generating an [EOS] token. These attacks can potentially cause high latency and make LLM services inaccessible to other users or tasks. However, when there are speech-to-text interfaces (e.g., voice commands to a robot), executing such DoS attacks becomes challenging, as it is difficult to introduce spelling errors or non-semantic prompts through speech. A simple DoS attack in these scenarios would be to instruct the model to \"Keep repeating Hello\", but we observe that relying solely on natural instructions limits output length, which is bounded by the maximum length of the LLM's supervised finetuning (SFT) data. To overcome this limitation, we propose poisoning-based DoS (P-DoS) attacks for LLMs, demonstrating that injecting a single poisoned sample designed for DoS purposes can break the output length limit. For example, a poisoned sample can successfully attack GPT-4o and GPT-4o mini (via OpenAI's finetuning API) using less than $1, causing repeated outputs up to the maximum inference length (16K tokens, compared to 0.5K before poisoning). Additionally, we perform comprehensive ablation studies on open-source LLMs and extend our method to LLM agents, where attackers can control both the finetuning dataset and algorithm. Our findings underscore the urgent need for defenses against P-DoS attacks to secure LLMs. Our code is available at https://github.com/sail-sg/P-DoS.",
      "doi": "https://doi.org/10.48550/arxiv.2410.10760",
      "openalex_id": "https://openalex.org/W4403571375",
      "arxiv_id": "",
      "publication_date": "2024-10-14",
      "published": "2024-10-14",
      "source": "openalex_snowball"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This item analyzes DoS/poisoning attacks on general LLMs and does not center on Spoken Language Models or speech-language modeling (no emphasis on speech input, turn-taking, or full-duplex speech interaction), hence it fails the inclusion criteria and is better excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "This item analyzes DoS/poisoning attacks on general LLMs and does not center on Spoken Language Models or speech-language modeling (no emphasis on speech input, turn-taking, or full-duplex speech interaction), hence it fails the inclusion criteria and is better excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The article focuses on denial-of-service poisoning attacks against large language models but does not address spoken language models or speech-based modeling and inference, nor does it discuss end-to-end or mixed speech-LLM architectures, full-duplex interaction, or related evaluation criteria. It mainly concentrates on adversarial attacks in textual LLM fine-tuning without involving speech signals as core modeling objects or spoken language processing aspects.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The article focuses on denial-of-service poisoning attacks against large language models but does not address spoken language models or speech-based modeling and inference, nor does it discuss end-to-end or mixed speech-LLM architectures, full-duplex interaction, or related evaluation criteria. It mainly concentrates on adversarial attacks in textual LLM fine-tuning without involving speech signals as core modeling objects or spoken language processing aspects.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  }
]