openalex_id,doi,title,abstract,referenced_works,publication_date
https://openalex.org/W4415283286,https://doi.org/10.1590/2318-0889202537e2514829,Extracting the meaning of a word: an artificial intelligence approach,"Abstract This article presents a strategy to extract the meaning of words in different contexts, using classification algorithms such as kNN, WiSARD, and 1NN, combined with a robust language model. The main objective is to investigate how the term “archive” is used in journalistic articles and how this usage reflects the value placed on the work of archivists. To achieve this, texts published in the newspaper “A Tribuna” between 2003 and 2017 were analyzed. The adopted method involves the automatic classification of sentences containing the term “archive,” dividing them into eleven categories that represent different interpretations of the term. The research was conducted through a classification algorithm, trained to identify semantic patterns in the sentences. This is a textual data analysis extracted from a digital collection of a periodical, without the direct participation of human subjects. The results indicate that combining the language model with the neural network significantly improves classification performance, surpassing traditional methods in metrics such as precision and recall. Additionally, the analysis showed that the term “archive” is widely used in different contexts by journalists, revealing multiple meanings and highlighting the importance of archivists in the process of organizing and documenting records. The proposed approach shows potential for application in other domains, contributing to the automation of semantic inference and the classification of large volumes of textual data.","['https://openalex.org/W4401246677', 'https://openalex.org/W1692308832', 'https://openalex.org/W2300264992', 'https://openalex.org/W3134574975', 'https://openalex.org/W4226418765', 'https://openalex.org/W3045445851', 'https://openalex.org/W3137208771', 'https://openalex.org/W2775790280', 'https://openalex.org/W4205971908', 'https://openalex.org/W4383426985', 'https://openalex.org/W4362700315', 'https://openalex.org/W4244061993', 'https://openalex.org/W1660390307']",2025-01-01
https://openalex.org/W4411757614,https://doi.org/10.1145/3746063,Mizo Automatic Speech Recognition: Leveraging Wav2vec 2.0 and XLS-R for Enhanced Accuracy in Low-Resource Language Processing,"This study introduces a Mizo Automatic Speech Recognition (ASR) approach by fine-tuning Wav2vec 2.0 and XLS-R models. The research presents the newly developed Mizo speech dataset, MiZonal v1.0 which significantly contributes to the advancement of low-resource language processing and plays a crucial role in preserving the Mizo language, thereby enhancing the training and assessment of speech models for this underrepresented language. It focuses on evaluating the effectiveness of these models in handling Mizo speech data, with particular emphasis on their performance in converting numerical numbers into Mizo cardinal words, which have a positive effect on the Word Error Rate (WER). The findings reveal that while the Wav2vec-Base-Mizo-Lus model achieved a WER of 16.59%, the XLS-R-300M-Mizo-Lus model outperformed it significantly, achieving a WER of 11.84% and setting a new benchmark for accuracy in the Mizo language. This shows the importance of using large multilingual speech recognition models and the cross-lingual abilities of models such as XLS-R are essential for ASR tasks in low-resource languages, leading to progress in Mizo speech technology and its applications.","['https://openalex.org/W4389524126', 'https://openalex.org/W4315645596', 'https://openalex.org/W3213029956', 'https://openalex.org/W4385572536', 'https://openalex.org/W3095173472', 'https://openalex.org/W2769205094', 'https://openalex.org/W2799473636', 'https://openalex.org/W4388562892', 'https://openalex.org/W2080213370', 'https://openalex.org/W4399017786', 'https://openalex.org/W1494198834', 'https://openalex.org/W4392355216', 'https://openalex.org/W2979826702', 'https://openalex.org/W4385571034', 'https://openalex.org/W3101648800', 'https://openalex.org/W2915722758', 'https://openalex.org/W2620949368', 'https://openalex.org/W2604292070', 'https://openalex.org/W116617985']",2025-06-28
https://openalex.org/W4389519587,https://doi.org/10.18653/v1/2023.emnlp-demo.49,Video-LLaMA: An Instruction-tuned Audio-Visual Language Model for Video Understanding,"We present Video-LLaMA, a multi-modal framework that empowers Large Language Models (LLMs) with the capability of understanding both visual and auditory content in the video. Video-LLaMA bootstraps cross-modal training from the frozen pre-trained visual & audio encoders and the frozen LLMs. Unlike previous works that complement LLMs to process the visual or audio signals only, Video-LLaMA enables video comprehension by tackling two challenges: (1) capturing the temporal changes in visual scenes, (2) integrating audio-visual signals. To counter the first challenge, we propose a Video Q-former to assemble a pre-trained image encoder into our video encoder and introduce a video-to-text generation task to learn video-language correspondence. For the second challenge, we leverage ImageBind, a universal embedding model aligning multiple modalities, as the pre-trained audio encoder and introduce an Audio Q-former on top of ImageBind to learn reasonable auditory query embeddings for the LLM module. To align the output of both visual & audio encoders with LLM’s embedding space, we first train Video-LLaMA on massive video/image-caption pairs and then tune our model with visual-instruction datasets of moderate amount but higher quality. We found Video-LLaMA shows the ability to perceive and comprehend video content and generate meaningful responses grounded in the visual and auditory information presented in the videos.","['https://openalex.org/W4361866031', 'https://openalex.org/W4323717348', 'https://openalex.org/W4375869762', 'https://openalex.org/W4366330503', 'https://openalex.org/W4389524372', 'https://openalex.org/W4378711593', 'https://openalex.org/W4383987498', 'https://openalex.org/W4224308101', 'https://openalex.org/W4319049530', 'https://openalex.org/W4311642023', 'https://openalex.org/W4311991106', 'https://openalex.org/W4287113019', 'https://openalex.org/W4285225959', 'https://openalex.org/W4327810158', 'https://openalex.org/W4380559123', 'https://openalex.org/W4386071707', 'https://openalex.org/W4376167553', 'https://openalex.org/W4367367040', 'https://openalex.org/W4229005866', 'https://openalex.org/W4322718246', 'https://openalex.org/W4366850747', 'https://openalex.org/W4402671548', 'https://openalex.org/W4225323055', 'https://openalex.org/W4367061106', 'https://openalex.org/W4367628410', 'https://openalex.org/W4389524500', 'https://openalex.org/W4386076522', 'https://openalex.org/W4384112212', 'https://openalex.org/W4322718191', 'https://openalex.org/W2886641317', 'https://openalex.org/W4382132560', 'https://openalex.org/W3204588463', 'https://openalex.org/W4318718936']",2023-01-01
https://openalex.org/W4389977189,https://doi.org/10.1016/j.nlp.2023.100048,A survey of GPT-3 family large language models including ChatGPT and GPT-4,"Large language models (LLMs) are a special class of pretrained language models (PLMs) obtained by scaling model size, pretraining corpus and computation. LLMs, because of their large size and pretraining on large volumes of text data, exhibit special abilities which allow them to achieve remarkable performances without any task-specific training in many of the natural language processing tasks. The era of LLMs started with OpenAI's GPT-3 model, and the popularity of LLMs has increased exponentially after the introduction of models like ChatGPT and GPT4. We refer to GPT-3 and its successor OpenAI models, including ChatGPT and GPT4, as GPT-3 family large language models (GLLMs). With the ever-rising popularity of GLLMs, especially in the research community, there is a strong need for a comprehensive survey which summarizes the recent research progress in multiple dimensions and can guide the research community with insightful future research directions. We start the survey paper with foundation concepts like transformers, transfer learning, self-supervised learning, pretrained language models and large language models. We then present a brief overview of GLLMs and discuss the performances of GLLMs in various downstream tasks, specific domains and multiple languages. We also discuss the data labelling and data augmentation abilities of GLLMs, the robustness of GLLMs, the effectiveness of GLLMs as evaluators, and finally, conclude with multiple insightful future research directions. To summarize, this comprehensive survey paper will serve as a good resource for both academic and industry people to stay updated with the latest research related to GLLMs.","['https://openalex.org/W2171960770', 'https://openalex.org/W4385573087', 'https://openalex.org/W3170092793', 'https://openalex.org/W4367353919', 'https://openalex.org/W4368340908', 'https://openalex.org/W3197510562', 'https://openalex.org/W2133564696', 'https://openalex.org/W2123301721', 'https://openalex.org/W6784452597', 'https://openalex.org/W3180181113', 'https://openalex.org/W2767899794', 'https://openalex.org/W2163302275', 'https://openalex.org/W2493916176', 'https://openalex.org/W4378189609', 'https://openalex.org/W6778883912', 'https://openalex.org/W6805220586', 'https://openalex.org/W4321372824', 'https://openalex.org/W2013942451', 'https://openalex.org/W6784117431', 'https://openalex.org/W3204112174', 'https://openalex.org/W4205508242', 'https://openalex.org/W4285107543', 'https://openalex.org/W6797362744', 'https://openalex.org/W2979864122', 'https://openalex.org/W6683258052', 'https://openalex.org/W1924770834', 'https://openalex.org/W3174519801', 'https://openalex.org/W2996035354', 'https://openalex.org/W3035390927', 'https://openalex.org/W6767737316', 'https://openalex.org/W6849081288', 'https://openalex.org/W6685158001', 'https://openalex.org/W3200787399', 'https://openalex.org/W4319777976', 'https://openalex.org/W6676297131', 'https://openalex.org/W3036247427', 'https://openalex.org/W3102925419', 'https://openalex.org/W6805239564', 'https://openalex.org/W3174828871', 'https://openalex.org/W6773936880', 'https://openalex.org/W4367051110', 'https://openalex.org/W4385571397', 'https://openalex.org/W2017561954', 'https://openalex.org/W6810009397', 'https://openalex.org/W4389403907', 'https://openalex.org/W4226321975', 'https://openalex.org/W6601399257', 'https://openalex.org/W4367668444', 'https://openalex.org/W4221165572', 'https://openalex.org/W4380319827', 'https://openalex.org/W3198659451', 'https://openalex.org/W4285210452', 'https://openalex.org/W3211686893', 'https://openalex.org/W3111372071', 'https://openalex.org/W3033187248', 'https://openalex.org/W2194775991', 'https://openalex.org/W4321276774', 'https://openalex.org/W4320920036', 'https://openalex.org/W2064675550', 'https://openalex.org/W4384484700', 'https://openalex.org/W2963026768', 'https://openalex.org/W3034287667', 'https://openalex.org/W6768086466', 'https://openalex.org/W2156723666', 'https://openalex.org/W6785668629', 'https://openalex.org/W2120615054', 'https://openalex.org/W4220967417', 'https://openalex.org/W3099035367', 'https://openalex.org/W3103272618', 'https://openalex.org/W3166508187', 'https://openalex.org/W4313563813', 'https://openalex.org/W6638733343', 'https://openalex.org/W3186081172', 'https://openalex.org/W6684191040', 'https://openalex.org/W6811037100', 'https://openalex.org/W4319662928', 'https://openalex.org/W2975059944', 'https://openalex.org/W2911489562', 'https://openalex.org/W4372219079', 'https://openalex.org/W3034999214', 'https://openalex.org/W4311887664', 'https://openalex.org/W3201915713', 'https://openalex.org/W6811297763', 'https://openalex.org/W6682631176', 'https://openalex.org/W3201174429', 'https://openalex.org/W2762550985', 'https://openalex.org/W4306955484', 'https://openalex.org/W3001434439', 'https://openalex.org/W3035101152', 'https://openalex.org/W4225934689', 'https://openalex.org/W3164540570', 'https://openalex.org/W6785247586', 'https://openalex.org/W6797132756', 'https://openalex.org/W6602680078', 'https://openalex.org/W4221166835', 'https://openalex.org/W6737947904', 'https://openalex.org/W1902237438', 'https://openalex.org/W4377010595', 'https://openalex.org/W4221159406', 'https://openalex.org/W4298152421', 'https://openalex.org/W4382318449', 'https://openalex.org/W3167228455', 'https://openalex.org/W4385893874', 'https://openalex.org/W6765122864', 'https://openalex.org/W2888482885', 'https://openalex.org/W3104186312', 'https://openalex.org/W4226485558', 'https://openalex.org/W6774753621', 'https://openalex.org/W3019166713', 'https://openalex.org/W6810738896', 'https://openalex.org/W2605035112', 'https://openalex.org/W2165698076', 'https://openalex.org/W2101105183', 'https://openalex.org/W2250539671', 'https://openalex.org/W6847593320', 'https://openalex.org/W4385894687', 'https://openalex.org/W6748634344', 'https://openalex.org/W3183962691', 'https://openalex.org/W6801857207', 'https://openalex.org/W4224215750', 'https://openalex.org/W3011574394', 'https://openalex.org/W6769627184', 'https://openalex.org/W3085807072', 'https://openalex.org/W4307309259', 'https://openalex.org/W4206256378', 'https://openalex.org/W6851654777', 'https://openalex.org/W4385292963', 'https://openalex.org/W2963216553', 'https://openalex.org/W2810821963', 'https://openalex.org/W4386065596', 'https://openalex.org/W4385572606', 'https://openalex.org/W4385573460', 'https://openalex.org/W2954996726', 'https://openalex.org/W6637373629', 'https://openalex.org/W6848237822', 'https://openalex.org/W6839193947', 'https://openalex.org/W3089472875', 'https://openalex.org/W4287887942', 'https://openalex.org/W2984500026', 'https://openalex.org/W3034457371', 'https://openalex.org/W4285159263', 'https://openalex.org/W6679436768', 'https://openalex.org/W2097117768', 'https://openalex.org/W3133652505', 'https://openalex.org/W7054915730', 'https://openalex.org/W3128553165', 'https://openalex.org/W2984353870', 'https://openalex.org/W6739901393', 'https://openalex.org/W4385572879', 'https://openalex.org/W4385002382', 'https://openalex.org/W3174150157', 'https://openalex.org/W4205737716', 'https://openalex.org/W4385572845', 'https://openalex.org/W2923014074', 'https://openalex.org/W3198685994', 'https://openalex.org/W2251658415', 'https://openalex.org/W6838461927', 'https://openalex.org/W6809646742', 'https://openalex.org/W2971296908', 'https://openalex.org/W4385567065', 'https://openalex.org/W2984452801', 'https://openalex.org/W6839710751', 'https://openalex.org/W3169483174', 'https://openalex.org/W6856454741', 'https://openalex.org/W6763701032', 'https://openalex.org/W3199693760', 'https://openalex.org/W4285305471', 'https://openalex.org/W2884001105', 'https://openalex.org/W6797816506', 'https://openalex.org/W3034379969', 'https://openalex.org/W6781533629', 'https://openalex.org/W4294833327', 'https://openalex.org/W6761205521', 'https://openalex.org/W3141797743', 'https://openalex.org/W6685053522', 'https://openalex.org/W6771915120', 'https://openalex.org/W6855083172', 'https://openalex.org/W2970785793', 'https://openalex.org/W6791446462', 'https://openalex.org/W3041133507', 'https://openalex.org/W4385572075', 'https://openalex.org/W4402670429', 'https://openalex.org/W4389520455', 'https://openalex.org/W4319773014', 'https://openalex.org/W2965373594', 'https://openalex.org/W4389519239', 'https://openalex.org/W4362511131', 'https://openalex.org/W4404783199', 'https://openalex.org/W4366735603', 'https://openalex.org/W4323709074', 'https://openalex.org/W4386794666', 'https://openalex.org/W3172427031', 'https://openalex.org/W4319915529', 'https://openalex.org/W4367628408', 'https://openalex.org/W4382654294', 'https://openalex.org/W4361230617', 'https://openalex.org/W3094446431', 'https://openalex.org/W4361019453', 'https://openalex.org/W4377090033', 'https://openalex.org/W3173151551', 'https://openalex.org/W4320009668', 'https://openalex.org/W4378711639', 'https://openalex.org/W2914120296', 'https://openalex.org/W4386721790', 'https://openalex.org/W3101118213', 'https://openalex.org/W4376311940', 'https://openalex.org/W4385565111', 'https://openalex.org/W4401880391', 'https://openalex.org/W4389523710', 'https://openalex.org/W4386044169', 'https://openalex.org/W4392846385', 'https://openalex.org/W4386361581', 'https://openalex.org/W4388694002', 'https://openalex.org/W4376653844', 'https://openalex.org/W4205403018', 'https://openalex.org/W4327522884', 'https://openalex.org/W4323650985', 'https://openalex.org/W4387294579', 'https://openalex.org/W4389524534', 'https://openalex.org/W4404356490', 'https://openalex.org/W4362679230', 'https://openalex.org/W4404869543', 'https://openalex.org/W4378420555', 'https://openalex.org/W4379259169', 'https://openalex.org/W4226399820', 'https://openalex.org/W4385571124', 'https://openalex.org/W4376652802', 'https://openalex.org/W4385572217', 'https://openalex.org/W4399750638', 'https://openalex.org/W4389921502', 'https://openalex.org/W2163605009', 'https://openalex.org/W4385890371', 'https://openalex.org/W4379540389', 'https://openalex.org/W4362706980', 'https://openalex.org/W4368755500', 'https://openalex.org/W2978017171', 'https://openalex.org/W4321605784', 'https://openalex.org/W4386555464', 'https://openalex.org/W4380353763', 'https://openalex.org/W4311253308', 'https://openalex.org/W4402683894', 'https://openalex.org/W4377161541', 'https://openalex.org/W4362679631', 'https://openalex.org/W4389520264', 'https://openalex.org/W4300506197', 'https://openalex.org/W2970597249', 'https://openalex.org/W4393178509', 'https://openalex.org/W4405138382', 'https://openalex.org/W4366815612', 'https://openalex.org/W4312205996', 'https://openalex.org/W3046368065', 'https://openalex.org/W4385569799', 'https://openalex.org/W2896457183', 'https://openalex.org/W4383473003', 'https://openalex.org/W4372272770', 'https://openalex.org/W4386228889', 'https://openalex.org/W4391876565', 'https://openalex.org/W4400033239', 'https://openalex.org/W4385570966', 'https://openalex.org/W4362679254', 'https://openalex.org/W4366460253', 'https://openalex.org/W4320879257', 'https://openalex.org/W3177813494', 'https://openalex.org/W4367000100', 'https://openalex.org/W4377164366', 'https://openalex.org/W4380715566', 'https://openalex.org/W3009431628', 'https://openalex.org/W4378174003', 'https://openalex.org/W4395101324', 'https://openalex.org/W4386721614', 'https://openalex.org/W4317553041', 'https://openalex.org/W4402671766', 'https://openalex.org/W4401043863', 'https://openalex.org/W4353113046', 'https://openalex.org/W4388926382', 'https://openalex.org/W4322760121', 'https://openalex.org/W4378510289', 'https://openalex.org/W4385571211', 'https://openalex.org/W4386729282', 'https://openalex.org/W4389518754', 'https://openalex.org/W4402671909', 'https://openalex.org/W4366342860', 'https://openalex.org/W3005724337', 'https://openalex.org/W3176456866', 'https://openalex.org/W4389524500', 'https://openalex.org/W4322718421', 'https://openalex.org/W4362706813', 'https://openalex.org/W4362598291', 'https://openalex.org/W4384662964', 'https://openalex.org/W4378942574', 'https://openalex.org/W4389520065', 'https://openalex.org/W4362678982', 'https://openalex.org/W4288089799', 'https://openalex.org/W4365601444', 'https://openalex.org/W4367860052', 'https://openalex.org/W4322208207', 'https://openalex.org/W4387299960', 'https://openalex.org/W4323717348', 'https://openalex.org/W4378465458', 'https://openalex.org/W4385474580', 'https://openalex.org/W4381586770', 'https://openalex.org/W4385571451', 'https://openalex.org/W4322759378', 'https://openalex.org/W3015468748', 'https://openalex.org/W4307079201', 'https://openalex.org/W4364320763', 'https://openalex.org/W4285600327', 'https://openalex.org/W2170240176', 'https://openalex.org/W4360887049', 'https://openalex.org/W4385570369', 'https://openalex.org/W4384389809', 'https://openalex.org/W4379958452', 'https://openalex.org/W4376163431', 'https://openalex.org/W3037252472', 'https://openalex.org/W4389519448', 'https://openalex.org/W3088409176', 'https://openalex.org/W4322718191', 'https://openalex.org/W4389523957', 'https://openalex.org/W4289145467', 'https://openalex.org/W4321524280', 'https://openalex.org/W4281944818', 'https://openalex.org/W4388926640', 'https://openalex.org/W4385569985', 'https://openalex.org/W4385965989', 'https://openalex.org/W4366808937', 'https://openalex.org/W4280543132', 'https://openalex.org/W3093871477', 'https://openalex.org/W4367000196', 'https://openalex.org/W4389217180', 'https://openalex.org/W4388184238', 'https://openalex.org/W4320005767', 'https://openalex.org/W4392669753', 'https://openalex.org/W4308504405', 'https://openalex.org/W2606347107', 'https://openalex.org/W4387596586', 'https://openalex.org/W4385571437', 'https://openalex.org/W4386185625', 'https://openalex.org/W4392919908', 'https://openalex.org/W4384920109', 'https://openalex.org/W4285077564', 'https://openalex.org/W4380994495', 'https://openalex.org/W4385571776', 'https://openalex.org/W4388926496', 'https://openalex.org/W4366330426', 'https://openalex.org/W4387324089', 'https://openalex.org/W4385570027', 'https://openalex.org/W4376653914', 'https://openalex.org/W4376633008', 'https://openalex.org/W4367000321', 'https://openalex.org/W4389520787', 'https://openalex.org/W4389665836', 'https://openalex.org/W4363671827', 'https://openalex.org/W4386303029', 'https://openalex.org/W2130942839', 'https://openalex.org/W4376122773', 'https://openalex.org/W4385571765', 'https://openalex.org/W2952729433', 'https://openalex.org/W4386365131', 'https://openalex.org/W4389524484', 'https://openalex.org/W2962862931', 'https://openalex.org/W4315498228', 'https://openalex.org/W4321854923', 'https://openalex.org/W2953894827', 'https://openalex.org/W4389524467', 'https://openalex.org/W4362707064', 'https://openalex.org/W4402665833', 'https://openalex.org/W4321472057', 'https://openalex.org/W4389519553', 'https://openalex.org/W4361806442', 'https://openalex.org/W4389519352', 'https://openalex.org/W4378469337', 'https://openalex.org/W4361230777', 'https://openalex.org/W4379089779', 'https://openalex.org/W4362508319', 'https://openalex.org/W4402671800', 'https://openalex.org/W4324134461', 'https://openalex.org/W4377009756', 'https://openalex.org/W4322832290', 'https://openalex.org/W4385262268', 'https://openalex.org/W4389636360', 'https://openalex.org/W4287887264', 'https://openalex.org/W4385571689', 'https://openalex.org/W4320559489', 'https://openalex.org/W4366208482', 'https://openalex.org/W4377372285', 'https://openalex.org/W4385571667', 'https://openalex.org/W4377864601', 'https://openalex.org/W4402683897', 'https://openalex.org/W4389520222', 'https://openalex.org/W4388335799', 'https://openalex.org/W4368755703', 'https://openalex.org/W4404918643', 'https://openalex.org/W4378770589', 'https://openalex.org/W4392616472', 'https://openalex.org/W4226278401', 'https://openalex.org/W4320167623', 'https://openalex.org/W4361866125', 'https://openalex.org/W4401042993', 'https://openalex.org/W4384918448', 'https://openalex.org/W4367628401', 'https://openalex.org/W4360891421', 'https://openalex.org/W4362515116', 'https://openalex.org/W4361193535', 'https://openalex.org/W4366559955', 'https://openalex.org/W4387323850', 'https://openalex.org/W4404783461', 'https://openalex.org/W4389521054', 'https://openalex.org/W4319991848', 'https://openalex.org/W4389518624', 'https://openalex.org/W4330337479', 'https://openalex.org/W4367860087', 'https://openalex.org/W3172480024', 'https://openalex.org/W4391136507', 'https://openalex.org/W4382567344', 'https://openalex.org/W4361865428', 'https://openalex.org/W4384263525', 'https://openalex.org/W4221143046', 'https://openalex.org/W4361230825', 'https://openalex.org/W4365211688', 'https://openalex.org/W4389519086', 'https://openalex.org/W4386729443', 'https://openalex.org/W4377130775', 'https://openalex.org/W4402671827', 'https://openalex.org/W3196850540', 'https://openalex.org/W4376311878', 'https://openalex.org/W4226364033', 'https://openalex.org/W4313483544', 'https://openalex.org/W4385694164', 'https://openalex.org/W3008374555', 'https://openalex.org/W4206802281', 'https://openalex.org/W4221141415', 'https://openalex.org/W4364383757', 'https://openalex.org/W4365799834', 'https://openalex.org/W4226305955', 'https://openalex.org/W4386501849', 'https://openalex.org/W4382998379', 'https://openalex.org/W4377864459', 'https://openalex.org/W4385570102', 'https://openalex.org/W4318903120', 'https://openalex.org/W4366999773', 'https://openalex.org/W4281690148', 'https://openalex.org/W4387560241', 'https://openalex.org/W4226418765', 'https://openalex.org/W4321009772', 'https://openalex.org/W4360891289', 'https://openalex.org/W4366851162', 'https://openalex.org/W4318719686', 'https://openalex.org/W4361866031', 'https://openalex.org/W4238634189', 'https://openalex.org/W4321018785', 'https://openalex.org/W4362655849', 'https://openalex.org/W4288727348', 'https://openalex.org/W4386566629', 'https://openalex.org/W4401042461', 'https://openalex.org/W4389519219', 'https://openalex.org/W4368250038', 'https://openalex.org/W4296413526', 'https://openalex.org/W4385572269', 'https://openalex.org/W4367190671', 'https://openalex.org/W4384071683', 'https://openalex.org/W4393161039', 'https://openalex.org/W4384642442', 'https://openalex.org/W4375870219', 'https://openalex.org/W4361193179', 'https://openalex.org/W4320732451', 'https://openalex.org/W4364385323', 'https://openalex.org/W4379539668', 'https://openalex.org/W4389261566', 'https://openalex.org/W4388788748', 'https://openalex.org/W4386081064', 'https://openalex.org/W4292779060', 'https://openalex.org/W4385571713', 'https://openalex.org/W4366999665', 'https://openalex.org/W4366999185', 'https://openalex.org/W4224308101', 'https://openalex.org/W4388928023', 'https://openalex.org/W4300485781', 'https://openalex.org/W4287025617', 'https://openalex.org/W4385570813', 'https://openalex.org/W4362598952', 'https://openalex.org/W4362679551', 'https://openalex.org/W4367175507', 'https://openalex.org/W4318351452', 'https://openalex.org/W4376632689', 'https://openalex.org/W1614298861', 'https://openalex.org/W4385571411', 'https://openalex.org/W4385570752', 'https://openalex.org/W4366989525', 'https://openalex.org/W4225591000', 'https://openalex.org/W4402671590', 'https://openalex.org/W4376122390', 'https://openalex.org/W4385774833', 'https://openalex.org/W4377009978', 'https://openalex.org/W4385734111', 'https://openalex.org/W4379259189', 'https://openalex.org/W4387356888', 'https://openalex.org/W4283026156', 'https://openalex.org/W4366733439', 'https://openalex.org/W4391758837', 'https://openalex.org/W3188301284', 'https://openalex.org/W4401042394', 'https://openalex.org/W4281486672', 'https://openalex.org/W4298181573', 'https://openalex.org/W4389519817', 'https://openalex.org/W3046375318', 'https://openalex.org/W4311642023', 'https://openalex.org/W4367628242', 'https://openalex.org/W4360836968', 'https://openalex.org/W4327811957', 'https://openalex.org/W4382323141', 'https://openalex.org/W4386719008', 'https://openalex.org/W4307933740', 'https://openalex.org/W4321524373', 'https://openalex.org/W4286985375', 'https://openalex.org/W2781626870', 'https://openalex.org/W4376652984', 'https://openalex.org/W4386566590', 'https://openalex.org/W4389518784', 'https://openalex.org/W4391473457', 'https://openalex.org/W4353007316', 'https://openalex.org/W4389523873', 'https://openalex.org/W4221167110', 'https://openalex.org/W4382618460', 'https://openalex.org/W4376167329', 'https://openalex.org/W4378498608', 'https://openalex.org/W4249142012', 'https://openalex.org/W3085139254', 'https://openalex.org/W3033529678', 'https://openalex.org/W4385245566', 'https://openalex.org/W4229005866', 'https://openalex.org/W4367189652', 'https://openalex.org/W4385570594', 'https://openalex.org/W4379933518', 'https://openalex.org/W4366733551', 'https://openalex.org/W4287257982', 'https://openalex.org/W4389519421', 'https://openalex.org/W4378501696', 'https://openalex.org/W4380687058', 'https://openalex.org/W4389520302']",2023-12-19
https://openalex.org/W4402155831,https://doi.org/10.2196/59505,"Multimodal Large Language Models in Health Care: Applications, Challenges, and Future Outlook","In the complex and multidimensional field of medicine, multimodal data are prevalent and crucial for informed clinical decisions. Multimodal data span a broad spectrum of data types, including medical images (eg, MRI and CT scans), time-series data (eg, sensor data from wearable devices and electronic health records), audio recordings (eg, heart and respiratory sounds and patient interviews), text (eg, clinical notes and research articles), videos (eg, surgical procedures), and omics data (eg, genomics and proteomics). While advancements in large language models (LLMs) have enabled new applications for knowledge retrieval and processing in the medical field, most LLMs remain limited to processing unimodal data, typically text-based content, and often overlook the importance of integrating the diverse data modalities encountered in clinical practice. This paper aims to present a detailed, practical, and solution-oriented perspective on the use of multimodal LLMs (M-LLMs) in the medical field. Our investigation spanned M-LLM foundational principles, current and potential applications, technical and ethical challenges, and future research directions. By connecting these elements, we aimed to provide a comprehensive framework that links diverse aspects of M-LLMs, offering a unified vision for their future in health care. This approach aims to guide both future research and practical implementations of M-LLMs in health care, positioning them as a paradigm shift toward integrated, multimodal data–driven medical practice. We anticipate that this work will spark further discussion and inspire the development of innovative approaches in the next generation of medical M-LLM systems.","['https://openalex.org/W4387865285', 'https://openalex.org/W4387500346', 'https://openalex.org/W4386575491', 'https://openalex.org/W4385164288', 'https://openalex.org/W4384071683', 'https://openalex.org/W4387809804', 'https://openalex.org/W4377011132', 'https://openalex.org/W3122765966', 'https://openalex.org/W4285987711', 'https://openalex.org/W4221022534', 'https://openalex.org/W2064675550', 'https://openalex.org/W2983587580', 'https://openalex.org/W4391829954', 'https://openalex.org/W4310645210', 'https://openalex.org/W3017637887', 'https://openalex.org/W3129155125', 'https://openalex.org/W3193158708', 'https://openalex.org/W4389686112', 'https://openalex.org/W4385647263', 'https://openalex.org/W4388931647', 'https://openalex.org/W4321459182', 'https://openalex.org/W4387381389', 'https://openalex.org/W4378672794', 'https://openalex.org/W4386052426', 'https://openalex.org/W4387028645', 'https://openalex.org/W4386892967', 'https://openalex.org/W4386735541', 'https://openalex.org/W4387653139', 'https://openalex.org/W4386923268', 'https://openalex.org/W4387583347', 'https://openalex.org/W4385346108', 'https://openalex.org/W4308885870', 'https://openalex.org/W4366330503', 'https://openalex.org/W4376167553', 'https://openalex.org/W4379918953', 'https://openalex.org/W4389524500', 'https://openalex.org/W4378711593', 'https://openalex.org/W4376864533', 'https://openalex.org/W4281643269', 'https://openalex.org/W4386168937', 'https://openalex.org/W3094502228', 'https://openalex.org/W2962835968', 'https://openalex.org/W2194775991', 'https://openalex.org/W4382403009', 'https://openalex.org/W1922655562', 'https://openalex.org/W4388430464', 'https://openalex.org/W4388831047', 'https://openalex.org/W4389538721', 'https://openalex.org/W2889664156', 'https://openalex.org/W2345512687', 'https://openalex.org/W2952935243', 'https://openalex.org/W3177049011', 'https://openalex.org/W3009260245', 'https://openalex.org/W2972119347', 'https://openalex.org/W4299732308', 'https://openalex.org/W4389116614', 'https://openalex.org/W4390580666', 'https://openalex.org/W4386185600', 'https://openalex.org/W4391159225', 'https://openalex.org/W4390214291', 'https://openalex.org/W4389925873', 'https://openalex.org/W4389708725', 'https://openalex.org/W4388962905', 'https://openalex.org/W4388513209', 'https://openalex.org/W4388926400', 'https://openalex.org/W4388685775', 'https://openalex.org/W4393149524', 'https://openalex.org/W4383987918', 'https://openalex.org/W4388482284', 'https://openalex.org/W4379259189', 'https://openalex.org/W4377121462', 'https://openalex.org/W4385436420', 'https://openalex.org/W4390690017', 'https://openalex.org/W4390115208', 'https://openalex.org/W4312220150', 'https://openalex.org/W4387355843', 'https://openalex.org/W4385965979', 'https://openalex.org/W4387559560', 'https://openalex.org/W4389664922', 'https://openalex.org/W4387724855', 'https://openalex.org/W4387634898', 'https://openalex.org/W4378498682', 'https://openalex.org/W4381827575', 'https://openalex.org/W4381786045', 'https://openalex.org/W4377372369', 'https://openalex.org/W4393178509', 'https://openalex.org/W4401070302', 'https://openalex.org/W4387891768', 'https://openalex.org/W4386185396', 'https://openalex.org/W4392019855', 'https://openalex.org/W2911489562', 'https://openalex.org/W3046375318', 'https://openalex.org/W4380137126', 'https://openalex.org/W4389216607', 'https://openalex.org/W4382334257', 'https://openalex.org/W4386083024', 'https://openalex.org/W4385474169', 'https://openalex.org/W4386066385', 'https://openalex.org/W4378174011', 'https://openalex.org/W4389261102', 'https://openalex.org/W4390783938', 'https://openalex.org/W4387966979', 'https://openalex.org/W3127238141', 'https://openalex.org/W4382490702', 'https://openalex.org/W4384133826', 'https://openalex.org/W4296613150', 'https://openalex.org/W4288421316', 'https://openalex.org/W4319065545', 'https://openalex.org/W4319335178', 'https://openalex.org/W4225917625', 'https://openalex.org/W3195980265', 'https://openalex.org/W4387346412', 'https://openalex.org/W3177500196', 'https://openalex.org/W4318071656', 'https://openalex.org/W4318751307', 'https://openalex.org/W4205773061', 'https://openalex.org/W4366163632', 'https://openalex.org/W4391316987', 'https://openalex.org/W4323572061', 'https://openalex.org/W4387210470', 'https://openalex.org/W4386655647', 'https://openalex.org/W4380994269', 'https://openalex.org/W4389502051', 'https://openalex.org/W4388555312', 'https://openalex.org/W4390041933', 'https://openalex.org/W4392947532', 'https://openalex.org/W4386047824', 'https://openalex.org/W4386157633', 'https://openalex.org/W4292438865', 'https://openalex.org/W4376130909', 'https://openalex.org/W4313439128', 'https://openalex.org/W4387789684', 'https://openalex.org/W4308393531', 'https://openalex.org/W4386893702', 'https://openalex.org/W4361289277', 'https://openalex.org/W4387068110', 'https://openalex.org/W4220867361', 'https://openalex.org/W4390480840', 'https://openalex.org/W4390603852', 'https://openalex.org/W3168867926', 'https://openalex.org/W4378509449', 'https://openalex.org/W4297813615', 'https://openalex.org/W4386876368', 'https://openalex.org/W1821462560', 'https://openalex.org/W4322718246', 'https://openalex.org/W4313484371', 'https://openalex.org/W4387323474', 'https://openalex.org/W2766839578', 'https://openalex.org/W2606722458', 'https://openalex.org/W4390490761', 'https://openalex.org/W3038035611', 'https://openalex.org/W3174057701', 'https://openalex.org/W4377226909', 'https://openalex.org/W4392849937', 'https://openalex.org/W2963400886', 'https://openalex.org/W4395026179', 'https://openalex.org/W6908377304', 'https://openalex.org/W4389524012', 'https://openalex.org/W4395703766', 'https://openalex.org/W4376643691', 'https://openalex.org/W4382394524', 'https://openalex.org/W3018610833', 'https://openalex.org/W3107627743', 'https://openalex.org/W2895917461', 'https://openalex.org/W2905657479', 'https://openalex.org/W2168610667', 'https://openalex.org/W4226196842', 'https://openalex.org/W3128912454', 'https://openalex.org/W4394763992', 'https://openalex.org/W2996550193', 'https://openalex.org/W4225966403', 'https://openalex.org/W4380421649']",2024-08-20
https://openalex.org/W4403203873,https://doi.org/10.1007/s44336-024-00009-2,"A survey on LLM-based multi-agent systems: workflow, infrastructure, and challenges","Abstract The pursuit of more intelligent and credible autonomous systems, akin to human society, has been a long-standing endeavor for humans. Leveraging the exceptional reasoning and planning capabilities of large language models (LLMs), LLM-based agents have been proposed and have achieved remarkable success across a wide array of tasks. Notably, LLM-based multi-agent systems (MAS) are considered a promising pathway towards realizing general artificial intelligence that is equivalent to or surpasses human-level intelligence. In this paper, we present a comprehensive survey of these studies, offering a systematic review of LLM-based MAS. Adhering to the workflow of LLM-based multi-agent systems, we synthesize a general structure encompassing five key components: profile, perception, self-action, mutual interaction, and evolution. This unified framework encapsulates much of the previous work in the field. Furthermore, we illuminate the extensive applications of LLM-based MAS in two principal areas: problem-solving and world simulation. Finally, we discuss in detail several contemporary challenges and provide insights into potential future directions in this domain.","['https://openalex.org/W4402669738', 'https://openalex.org/W4387567460', 'https://openalex.org/W6600538214', 'https://openalex.org/W4397004462', 'https://openalex.org/W4396723768', 'https://openalex.org/W4388720459', 'https://openalex.org/W4402508150', 'https://openalex.org/W4388744821', 'https://openalex.org/W6607077144', 'https://openalex.org/W4393065402', 'https://openalex.org/W4401023556', 'https://openalex.org/W6601144205', 'https://openalex.org/W6822005569', 'https://openalex.org/W3153865577', 'https://openalex.org/W6600169857', 'https://openalex.org/W6600407735', 'https://openalex.org/W2147492008', 'https://openalex.org/W2049318755', 'https://openalex.org/W4387835442', 'https://openalex.org/W4402684050', 'https://openalex.org/W4399557965', 'https://openalex.org/W4401415431', 'https://openalex.org/W4401416363', 'https://openalex.org/W4389519889', 'https://openalex.org/W4402670791', 'https://openalex.org/W4307475457', 'https://openalex.org/W4390772308', 'https://openalex.org/W6600751047', 'https://openalex.org/W4402671292', 'https://openalex.org/W4400525295', 'https://openalex.org/W4396758674', 'https://openalex.org/W4388718403', 'https://openalex.org/W6600175266', 'https://openalex.org/W6600402959', 'https://openalex.org/W4393147219', 'https://openalex.org/W6601065604', 'https://openalex.org/W6604666239', 'https://openalex.org/W4386200967', 'https://openalex.org/W6600109629', 'https://openalex.org/W6600493712', 'https://openalex.org/W6600001191', 'https://openalex.org/W4321455981', 'https://openalex.org/W2116360511', 'https://openalex.org/W2144730066', 'https://openalex.org/W6838461927', 'https://openalex.org/W4385573990', 'https://openalex.org/W4309663019', 'https://openalex.org/W4382991350', 'https://openalex.org/W2488290758', 'https://openalex.org/W4360612519', 'https://openalex.org/W4386576794', 'https://openalex.org/W4390874280', 'https://openalex.org/W4392669753', 'https://openalex.org/W6600005967', 'https://openalex.org/W4402951641', 'https://openalex.org/W6838557027', 'https://openalex.org/W4205474609', 'https://openalex.org/W3034655362', 'https://openalex.org/W6601897980', 'https://openalex.org/W6675775337', 'https://openalex.org/W4221143046', 'https://openalex.org/W6600575157', 'https://openalex.org/W4225537626', 'https://openalex.org/W4298427539', 'https://openalex.org/W4210268062', 'https://openalex.org/W6600213211', 'https://openalex.org/W6795140394', 'https://openalex.org/W6600234944', 'https://openalex.org/W6607923624', 'https://openalex.org/W6601804787', 'https://openalex.org/W3173972567', 'https://openalex.org/W4225323055', 'https://openalex.org/W6600577311', 'https://openalex.org/W6602670149', 'https://openalex.org/W4381786045', 'https://openalex.org/W6601204288', 'https://openalex.org/W4389519587', 'https://openalex.org/W6604381581', 'https://openalex.org/W6600336938', 'https://openalex.org/W3196974791', 'https://openalex.org/W3209059054', 'https://openalex.org/W4387969495', 'https://openalex.org/W4393178509', 'https://openalex.org/W6600274115', 'https://openalex.org/W4386076655', 'https://openalex.org/W4390874304', 'https://openalex.org/W4402703090', 'https://openalex.org/W6600042225', 'https://openalex.org/W4393160747', 'https://openalex.org/W6635750829', 'https://openalex.org/W4393147158', 'https://openalex.org/W6600263792', 'https://openalex.org/W4225727438', 'https://openalex.org/W4389524599', 'https://openalex.org/W4385571459', 'https://openalex.org/W6602097856', 'https://openalex.org/W4387993371', 'https://openalex.org/W6633512021', 'https://openalex.org/W4385572983', 'https://openalex.org/W3197792581', 'https://openalex.org/W6600129504', 'https://openalex.org/W6600669965', 'https://openalex.org/W4389524500', 'https://openalex.org/W6817328786', 'https://openalex.org/W4392931626', 'https://openalex.org/W4386251907', 'https://openalex.org/W4384071683', 'https://openalex.org/W6840334356', 'https://openalex.org/W4392044798', 'https://openalex.org/W4281763794', 'https://openalex.org/W4385567216', 'https://openalex.org/W4399177300', 'https://openalex.org/W6810081322', 'https://openalex.org/W6605656274', 'https://openalex.org/W6727391228', 'https://openalex.org/W6601899773', 'https://openalex.org/W6600798642', 'https://openalex.org/W3027879771', 'https://openalex.org/W6606511648', 'https://openalex.org/W6600586173', 'https://openalex.org/W6602789111', 'https://openalex.org/W4389520468', 'https://openalex.org/W4385571271', 'https://openalex.org/W4389519118', 'https://openalex.org/W6600210674', 'https://openalex.org/W4389520486', 'https://openalex.org/W4389518782', 'https://openalex.org/W4385573027', 'https://openalex.org/W4385572256', 'https://openalex.org/W2560647685', 'https://openalex.org/W6607643177', 'https://openalex.org/W4389520370', 'https://openalex.org/W4393147125', 'https://openalex.org/W6730126202', 'https://openalex.org/W3172669006', 'https://openalex.org/W6600195515', 'https://openalex.org/W4309674289', 'https://openalex.org/W3196268181', 'https://openalex.org/W6628632905', 'https://openalex.org/W4402670429', 'https://openalex.org/W6600599538', 'https://openalex.org/W6610928041', 'https://openalex.org/W4226278401', 'https://openalex.org/W6604662147', 'https://openalex.org/W3201174429', 'https://openalex.org/W4393160124', 'https://openalex.org/W4401042371', 'https://openalex.org/W3016970897', 'https://openalex.org/W3156470785', 'https://openalex.org/W4390043316', 'https://openalex.org/W4385569978', 'https://openalex.org/W4402667038', 'https://openalex.org/W1997480541', 'https://openalex.org/W1986884366', 'https://openalex.org/W4385571689', 'https://openalex.org/W6607167723', 'https://openalex.org/W4392637287', 'https://openalex.org/W6601211009', 'https://openalex.org/W6609703661', 'https://openalex.org/W6600696048', 'https://openalex.org/W4401042703', 'https://openalex.org/W4393160302', 'https://openalex.org/W6600473871', 'https://openalex.org/W4389520747', 'https://openalex.org/W6600212061', 'https://openalex.org/W6778883912', 'https://openalex.org/W6600558321', 'https://openalex.org/W6600553734', 'https://openalex.org/W6600310816', 'https://openalex.org/W4388626886', 'https://openalex.org/W6600223405', 'https://openalex.org/W4401042446', 'https://openalex.org/W6637231638', 'https://openalex.org/W2944408871', 'https://openalex.org/W6602063408', 'https://openalex.org/W6600351811', 'https://openalex.org/W6600002382', 'https://openalex.org/W6603321306', 'https://openalex.org/W6600134738', 'https://openalex.org/W6605438980', 'https://openalex.org/W6602855854', 'https://openalex.org/W6603816838', 'https://openalex.org/W4401042992', 'https://openalex.org/W6608181014', 'https://openalex.org/W6602838585', 'https://openalex.org/W6604583463', 'https://openalex.org/W4393160233', 'https://openalex.org/W6604641075', 'https://openalex.org/W6600882715', 'https://openalex.org/W6600424091', 'https://openalex.org/W6601697411', 'https://openalex.org/W3176828726', 'https://openalex.org/W6602666306', 'https://openalex.org/W3101498587', 'https://openalex.org/W4389524317', 'https://openalex.org/W6826116265', 'https://openalex.org/W6600103761', 'https://openalex.org/W4385570973', 'https://openalex.org/W4205991051', 'https://openalex.org/W2970476646', 'https://openalex.org/W6600655081', 'https://openalex.org/W6605475740', 'https://openalex.org/W3098267758', 'https://openalex.org/W6648260465', 'https://openalex.org/W6601141708', 'https://openalex.org/W4385567201', 'https://openalex.org/W4401024643', 'https://openalex.org/W6600075759', 'https://openalex.org/W4393305455', 'https://openalex.org/W4384345748', 'https://openalex.org/W6667296330', 'https://openalex.org/W4363624465', 'https://openalex.org/W6600120041', 'https://openalex.org/W4386728933', 'https://openalex.org/W4392846385', 'https://openalex.org/W6600062020', 'https://openalex.org/W4368755500', 'https://openalex.org/W6818330543', 'https://openalex.org/W6604009900', 'https://openalex.org/W4392367398', 'https://openalex.org/W4402623742', 'https://openalex.org/W4402727853', 'https://openalex.org/W2798730128', 'https://openalex.org/W4400135851', 'https://openalex.org/W4385570481', 'https://openalex.org/W4399528455', 'https://openalex.org/W4386302153', 'https://openalex.org/W4389519898', 'https://openalex.org/W3128232076', 'https://openalex.org/W4385573981', 'https://openalex.org/W4382318960', 'https://openalex.org/W3117655171', 'https://openalex.org/W3154086708', 'https://openalex.org/W4287887760', 'https://openalex.org/W3204561666', 'https://openalex.org/W6702248584', 'https://openalex.org/W4389519982', 'https://openalex.org/W4388537645', 'https://openalex.org/W4392667162', 'https://openalex.org/W4385507608', 'https://openalex.org/W4387432528', 'https://openalex.org/W4377234252', 'https://openalex.org/W4385572293', 'https://openalex.org/W4225858632', 'https://openalex.org/W6600728650', 'https://openalex.org/W3006630858', 'https://openalex.org/W4319661244', 'https://openalex.org/W6742787474', 'https://openalex.org/W2991046523', 'https://openalex.org/W2012513346', 'https://openalex.org/W6602167274', 'https://openalex.org/W4385768250', 'https://openalex.org/W4225702709', 'https://openalex.org/W3103934428']",2024-10-08
https://openalex.org/W4396877837,https://doi.org/10.1109/taslp.2024.3399607,AudioLDM 2: Learning Holistic Audio Generation With Self-Supervised Pretraining,"Although audio generation shares commonalities across different types of audio, such as speech, music, and sound effects, designing models for each type requires careful consideration of specific objectives and biases that can significantly differ from those of other types. To bring us closer to a unified perspective of audio generation, this paper proposes a holistic framework that utilizes the same learning method for speech, music, and sound effect generation. Our framework utilizes a general representation of audio, called ""language of audio"" (LOA). Any audio can be translated into LOA based on AudioMAE, a self-supervised pre-trained representation learning model. In the generation process, we translate other modalities into LOA by using a GPT-2 model, and we perform self-supervised audio generation learning with a latent diffusion model conditioned on the LOA of audio in our training set. The proposed framework naturally brings advantages such as reusable self-supervised pretrained latent diffusion models. Experiments on the major benchmarks of text-to-audio, text-to-music, and text-to-speech with three AudioLDM 2 variants demonstrate competitive performance of the AudioLDM 2 variants framework against previous approaches. Our code, pretrained model, and demo are available at <uri xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">https://audioldm.github.io/audioldm2</uri> .","['https://openalex.org/W4391020683', 'https://openalex.org/W6845479124', 'https://openalex.org/W6849109464', 'https://openalex.org/W4389524500', 'https://openalex.org/W6855691466', 'https://openalex.org/W6849105126', 'https://openalex.org/W6610228593', 'https://openalex.org/W6771763809', 'https://openalex.org/W6778823374', 'https://openalex.org/W2559726422', 'https://openalex.org/W6852824296', 'https://openalex.org/W4367359628', 'https://openalex.org/W6849416043', 'https://openalex.org/W6802017037', 'https://openalex.org/W6810007534', 'https://openalex.org/W4386071707', 'https://openalex.org/W6852871851', 'https://openalex.org/W3127705815', 'https://openalex.org/W4312933868', 'https://openalex.org/W6851775633', 'https://openalex.org/W3215615641', 'https://openalex.org/W4381786045', 'https://openalex.org/W6917585676', 'https://openalex.org/W4372348103', 'https://openalex.org/W6802805937', 'https://openalex.org/W6795261426', 'https://openalex.org/W6777694618', 'https://openalex.org/W6849635556', 'https://openalex.org/W6853096648', 'https://openalex.org/W4226033575', 'https://openalex.org/W6779823529', 'https://openalex.org/W6786375611', 'https://openalex.org/W6795288823', 'https://openalex.org/W6809885388', 'https://openalex.org/W6838639034', 'https://openalex.org/W3155072588', 'https://openalex.org/W6782760101', 'https://openalex.org/W6783182287', 'https://openalex.org/W6838844135', 'https://openalex.org/W6844305113', 'https://openalex.org/W6845281891', 'https://openalex.org/W6848482659', 'https://openalex.org/W6799642162', 'https://openalex.org/W6797095309', 'https://openalex.org/W4224931676', 'https://openalex.org/W4387969125', 'https://openalex.org/W4372266890', 'https://openalex.org/W4393157029', 'https://openalex.org/W4393161149', 'https://openalex.org/W6780218876', 'https://openalex.org/W6640963894', 'https://openalex.org/W3094502228', 'https://openalex.org/W2593116425', 'https://openalex.org/W6853393314', 'https://openalex.org/W3209059054', 'https://openalex.org/W2973049979', 'https://openalex.org/W3201143670', 'https://openalex.org/W6783867762', 'https://openalex.org/W2052666245', 'https://openalex.org/W3046747294', 'https://openalex.org/W6769915798', 'https://openalex.org/W6728610325', 'https://openalex.org/W2066334462', 'https://openalex.org/W4372260310', 'https://openalex.org/W4372260340', 'https://openalex.org/W6847076894', 'https://openalex.org/W6769627184', 'https://openalex.org/W4378602476', 'https://openalex.org/W6739901393', 'https://openalex.org/W6840815571', 'https://openalex.org/W6810940779', 'https://openalex.org/W6850843143', 'https://openalex.org/W3015371781', 'https://openalex.org/W6732646663', 'https://openalex.org/W6633499030', 'https://openalex.org/W3198694222', 'https://openalex.org/W4372259760', 'https://openalex.org/W2526050071', 'https://openalex.org/W3205475937', 'https://openalex.org/W6633724138', 'https://openalex.org/W4375869413', 'https://openalex.org/W6783713337', 'https://openalex.org/W6757817989', 'https://openalex.org/W6852971826', 'https://openalex.org/W6849517043', 'https://openalex.org/W4313447020', 'https://openalex.org/W4224035735', 'https://openalex.org/W4376632781', 'https://openalex.org/W4318351475', 'https://openalex.org/W2984284833', 'https://openalex.org/W4318718630', 'https://openalex.org/W4288089799', 'https://openalex.org/W1959608418', 'https://openalex.org/W1560729591', 'https://openalex.org/W3203491020', 'https://openalex.org/W4379251869', 'https://openalex.org/W4400033239', 'https://openalex.org/W4362515116', 'https://openalex.org/W2187089797', 'https://openalex.org/W4378942405', 'https://openalex.org/W4385328213', 'https://openalex.org/W4319989813', 'https://openalex.org/W4303440777']",2024-01-01
https://openalex.org/W4388725733,https://doi.org/10.36227/techrxiv.23589741.v4,"Large Language Models: A Comprehensive Survey of its Applications, Challenges, Limitations, and Future Prospects","&lt;p&gt;Within the vast expanse of computerized language processing, a revolutionary entity known as Large Language Models (LLMs) has emerged, wielding immense power in its capacity to comprehend intricate linguistic patterns and conjure coherent and contextually fitting responses. Large language models (LLMs) are a type of artificial intelligence (AI) that have emerged as powerful tools for a wide range of tasks, including natural language processing (NLP), machine translation, and question-answering. This survey paper provides a comprehensive overview of LLMs, including their history, architecture, training methods, applications, and challenges. The paper begins by discussing the fundamental concepts of generative AI and the architecture of generative pre- trained transformers (GPT). It then provides an overview of the history of LLMs, their evolution over time, and the different training methods that have been used to train them. The paper then discusses the wide range of applications of LLMs, including medical, education, finance, and engineering. It also discusses how LLMs are shaping the future of AI and how they can be used to solve real-world problems. The paper then discusses the challenges associated with deploying LLMs in real-world scenarios, including ethical considerations, model biases, interpretability, and computational resource requirements. It also highlights techniques for enhancing the robustness and controllability of LLMs, and addressing bias, fairness, and generation quality issues. Finally, the paper concludes by highlighting the future of LLM research and the challenges that need to be addressed in order to make LLMs more reliable and useful. This survey paper is intended to provide researchers, practitioners, and enthusiasts with a comprehensive understanding of LLMs, their evolution, applications, and challenges. By consolidating the state-of-the-art knowledge in the field, this survey serves as a valuable resource for further advancements in the development and utilization of LLMs for a wide range of real-world applications. The GitHub repo for this project is available at https://github.com/anas-zafar/LLM-Survey&lt;/p&gt;","['https://openalex.org/W6681458419', 'https://openalex.org/W3016399951', 'https://openalex.org/W3168584517', 'https://openalex.org/W3209721572', 'https://openalex.org/W4220665399', 'https://openalex.org/W3176574162', 'https://openalex.org/W6678277124', 'https://openalex.org/W2807079745', 'https://openalex.org/W4308793915', 'https://openalex.org/W3197876970', 'https://openalex.org/W6775855368', 'https://openalex.org/W6838461927', 'https://openalex.org/W4281690148', 'https://openalex.org/W4322718191', 'https://openalex.org/W3202773593', 'https://openalex.org/W6850810733', 'https://openalex.org/W2981089724', 'https://openalex.org/W4385327621', 'https://openalex.org/W4379918953', 'https://openalex.org/W3034875620', 'https://openalex.org/W4384648427', 'https://openalex.org/W4312091691', 'https://openalex.org/W2099547633', 'https://openalex.org/W2020073413', 'https://openalex.org/W1592871157', 'https://openalex.org/W2114858359', 'https://openalex.org/W2100506586', 'https://openalex.org/W6634534149', 'https://openalex.org/W2118714763', 'https://openalex.org/W2997833324', 'https://openalex.org/W6785197332', 'https://openalex.org/W3023211159', 'https://openalex.org/W1851422430', 'https://openalex.org/W2025478229', 'https://openalex.org/W2804243481', 'https://openalex.org/W2587019100', 'https://openalex.org/W6607333740', 'https://openalex.org/W2099257174', 'https://openalex.org/W6739901393', 'https://openalex.org/W2789541106', 'https://openalex.org/W4243640523', 'https://openalex.org/W2896457183', 'https://openalex.org/W6774952039', 'https://openalex.org/W3103891289', 'https://openalex.org/W6741042381', 'https://openalex.org/W6851896007', 'https://openalex.org/W4385572310', 'https://openalex.org/W4318146376', 'https://openalex.org/W1816313093', 'https://openalex.org/W4318719209', 'https://openalex.org/W4281651027', 'https://openalex.org/W4385374425', 'https://openalex.org/W6851024552', 'https://openalex.org/W4385970120', 'https://openalex.org/W4392384010', 'https://openalex.org/W4387323685', 'https://openalex.org/W4385833954', 'https://openalex.org/W6804900007', 'https://openalex.org/W2975059944', 'https://openalex.org/W2965373594', 'https://openalex.org/W4385490607', 'https://openalex.org/W6809838524', 'https://openalex.org/W4382394716', 'https://openalex.org/W4281250694', 'https://openalex.org/W4363676214', 'https://openalex.org/W4386148654', 'https://openalex.org/W4321649710', 'https://openalex.org/W4365512576', 'https://openalex.org/W6945930829', 'https://openalex.org/W6851013863', 'https://openalex.org/W4226278401', 'https://openalex.org/W4322718246', 'https://openalex.org/W6851775633', 'https://openalex.org/W6810527279', 'https://openalex.org/W6849898756', 'https://openalex.org/W4328092166', 'https://openalex.org/W4382393690', 'https://openalex.org/W4383988334', 'https://openalex.org/W6854475153', 'https://openalex.org/W4318464200', 'https://openalex.org/W4321479909', 'https://openalex.org/W6774110404', 'https://openalex.org/W4323706279', 'https://openalex.org/W2970771982', 'https://openalex.org/W6795072802', 'https://openalex.org/W6850820320', 'https://openalex.org/W4330336443', 'https://openalex.org/W6854908924', 'https://openalex.org/W6853458019', 'https://openalex.org/W3177813494', 'https://openalex.org/W4352977000', 'https://openalex.org/W4327810667', 'https://openalex.org/W6854027870', 'https://openalex.org/W4386942358', 'https://openalex.org/W4366988971', 'https://openalex.org/W4319304408', 'https://openalex.org/W4386184788', 'https://openalex.org/W4377865170', 'https://openalex.org/W6855786032', 'https://openalex.org/W6851269602', 'https://openalex.org/W6854751326', 'https://openalex.org/W4309953446', 'https://openalex.org/W4285178079', 'https://openalex.org/W4313384121', 'https://openalex.org/W4220962633', 'https://openalex.org/W6855011245', 'https://openalex.org/W4367602258', 'https://openalex.org/W2096352448', 'https://openalex.org/W2789264883', 'https://openalex.org/W2884367402', 'https://openalex.org/W6785245291', 'https://openalex.org/W4205805845', 'https://openalex.org/W4353115070', 'https://openalex.org/W4360890968', 'https://openalex.org/W3011628670', 'https://openalex.org/W3095698190', 'https://openalex.org/W4283781401', 'https://openalex.org/W4377143197', 'https://openalex.org/W4378510289', 'https://openalex.org/W4318566811', 'https://openalex.org/W3199004261', 'https://openalex.org/W6800751262', 'https://openalex.org/W6847389933', 'https://openalex.org/W2978017171', 'https://openalex.org/W4312048837', 'https://openalex.org/W2579277680', 'https://openalex.org/W6745929367', 'https://openalex.org/W4223908421', 'https://openalex.org/W3097340282', 'https://openalex.org/W4361002760', 'https://openalex.org/W2755577605', 'https://openalex.org/W4205608468', 'https://openalex.org/W4320343009', 'https://openalex.org/W3113143462', 'https://openalex.org/W4383959108', 'https://openalex.org/W6637568146', 'https://openalex.org/W2753252132', 'https://openalex.org/W6761100157', 'https://openalex.org/W2890462169', 'https://openalex.org/W3206916870', 'https://openalex.org/W3034820610', 'https://openalex.org/W2898827701', 'https://openalex.org/W3033067316', 'https://openalex.org/W2134243390', 'https://openalex.org/W3188522443', 'https://openalex.org/W4386250786', 'https://openalex.org/W4297835768', 'https://openalex.org/W4308733509', 'https://openalex.org/W2951768877', 'https://openalex.org/W4385416665', 'https://openalex.org/W6745907911', 'https://openalex.org/W4378945283', 'https://openalex.org/W3174622437', 'https://openalex.org/W3138478053', 'https://openalex.org/W4381436420', 'https://openalex.org/W4382318191', 'https://openalex.org/W6854515434', 'https://openalex.org/W3194309076', 'https://openalex.org/W4297630849', 'https://openalex.org/W6853986968', 'https://openalex.org/W4225292424', 'https://openalex.org/W6846907077', 'https://openalex.org/W4387323999', 'https://openalex.org/W4378506863', 'https://openalex.org/W4327525501', 'https://openalex.org/W6852798437', 'https://openalex.org/W4297161808', 'https://openalex.org/W4377371519', 'https://openalex.org/W4221143046', 'https://openalex.org/W4321855256', 'https://openalex.org/W4385474073', 'https://openalex.org/W6848909144', 'https://openalex.org/W6838865847', 'https://openalex.org/W4312091811', 'https://openalex.org/W6852357257', 'https://openalex.org/W4362656079', 'https://openalex.org/W4378760015', 'https://openalex.org/W4312824283', 'https://openalex.org/W6846173857', 'https://openalex.org/W4309953141', 'https://openalex.org/W4313484371', 'https://openalex.org/W4309805206', 'https://openalex.org/W6843258702', 'https://openalex.org/W6810601475', 'https://openalex.org/W4378473867', 'https://openalex.org/W4287891012', 'https://openalex.org/W4353007621', 'https://openalex.org/W4225591000', 'https://openalex.org/W4377864123', 'https://openalex.org/W6891809357', 'https://openalex.org/W6674571003', 'https://openalex.org/W6679336733', 'https://openalex.org/W2888794009', 'https://openalex.org/W7030290986', 'https://openalex.org/W3013669377', 'https://openalex.org/W187290754', 'https://openalex.org/W3213080957', 'https://openalex.org/W6778485988', 'https://openalex.org/W2979826702', 'https://openalex.org/W6778883912', 'https://openalex.org/W3095319910', 'https://openalex.org/W2907283777', 'https://openalex.org/W6776215604', 'https://openalex.org/W1539309091', 'https://openalex.org/W4362655426', 'https://openalex.org/W4281481109', 'https://openalex.org/W4221159672', 'https://openalex.org/W2056351363', 'https://openalex.org/W2911109671', 'https://openalex.org/W4327518740', 'https://openalex.org/W6641017118', 'https://openalex.org/W2150355110', 'https://openalex.org/W4367669558', 'https://openalex.org/W6779431784', 'https://openalex.org/W4286982826', 'https://openalex.org/W4320830073', 'https://openalex.org/W6763238093', 'https://openalex.org/W4315779850', 'https://openalex.org/W3205734318', 'https://openalex.org/W4362679052', 'https://openalex.org/W4292119927', 'https://openalex.org/W4387995158', 'https://openalex.org/W2950813464', 'https://openalex.org/W3039729104', 'https://openalex.org/W3028754898', 'https://openalex.org/W2734774145', 'https://openalex.org/W2560622264', 'https://openalex.org/W6800957439', 'https://openalex.org/W2981852735', 'https://openalex.org/W3126464137', 'https://openalex.org/W2973049837', 'https://openalex.org/W2616957565', 'https://openalex.org/W2986836624', 'https://openalex.org/W4382173325', 'https://openalex.org/W4313197536', 'https://openalex.org/W4379387190', 'https://openalex.org/W4221152111', 'https://openalex.org/W4366850747', 'https://openalex.org/W6780673046', 'https://openalex.org/W2993398598', 'https://openalex.org/W6749450065', 'https://openalex.org/W4362702134', 'https://openalex.org/W4387428001', 'https://openalex.org/W4367860087', 'https://openalex.org/W4317547647', 'https://openalex.org/W6852564843', 'https://openalex.org/W6797822351', 'https://openalex.org/W4378465262', 'https://openalex.org/W4362678805', 'https://openalex.org/W4385570624', 'https://openalex.org/W4322718421', 'https://openalex.org/W4386128198', 'https://openalex.org/W4387075390', 'https://openalex.org/W4320858112', 'https://openalex.org/W4376864573', 'https://openalex.org/W4380559123', 'https://openalex.org/W4376122554', 'https://openalex.org/W4321524373', 'https://openalex.org/W4293499155', 'https://openalex.org/W4323697341', 'https://openalex.org/W4311000453', 'https://openalex.org/W6847076894', 'https://openalex.org/W4306808908', 'https://openalex.org/W6798182279', 'https://openalex.org/W4376653782', 'https://openalex.org/W4377864408', 'https://openalex.org/W4360836968', 'https://openalex.org/W6852800892', 'https://openalex.org/W4376652621', 'https://openalex.org/W4323316496', 'https://openalex.org/W4318925155', 'https://openalex.org/W4313451803', 'https://openalex.org/W4327946446', 'https://openalex.org/W4323050332', 'https://openalex.org/W4379919409', 'https://openalex.org/W4319341091', 'https://openalex.org/W4318389352', 'https://openalex.org/W4319868628', 'https://openalex.org/W4387485634', 'https://openalex.org/W4366703942', 'https://openalex.org/W2982702361', 'https://openalex.org/W2966555834', 'https://openalex.org/W4380715596', 'https://openalex.org/W6852542133', 'https://openalex.org/W4382652046', 'https://openalex.org/W4365143687', 'https://openalex.org/W4372272282', 'https://openalex.org/W3158196282', 'https://openalex.org/W4366489368', 'https://openalex.org/W4323256981', 'https://openalex.org/W4366783381', 'https://openalex.org/W4381431616', 'https://openalex.org/W4387323762', 'https://openalex.org/W6849792168', 'https://openalex.org/W4317390716', 'https://openalex.org/W3189189854', 'https://openalex.org/W4323045163', 'https://openalex.org/W4318014888', 'https://openalex.org/W4385019067', 'https://openalex.org/W4318716950', 'https://openalex.org/W4364320763', 'https://openalex.org/W3037252472', 'https://openalex.org/W4385572185', 'https://openalex.org/W7047380062', 'https://openalex.org/W3028022888', 'https://openalex.org/W4380373983', 'https://openalex.org/W4386081573', 'https://openalex.org/W4367694135', 'https://openalex.org/W4379507362', 'https://openalex.org/W4308884325', 'https://openalex.org/W6854595581', 'https://openalex.org/W4324316215', 'https://openalex.org/W4313678819', 'https://openalex.org/W6840155464', 'https://openalex.org/W3087761062', 'https://openalex.org/W6848955896', 'https://openalex.org/W3211939470', 'https://openalex.org/W6858385244', 'https://openalex.org/W4229012946', 'https://openalex.org/W4210314826', 'https://openalex.org/W4318977924', 'https://openalex.org/W4377290676', 'https://openalex.org/W4382618722', 'https://openalex.org/W4297840560', 'https://openalex.org/W4310829037', 'https://openalex.org/W6849422333', 'https://openalex.org/W4311253308', 'https://openalex.org/W3112136262', 'https://openalex.org/W4363678768', 'https://openalex.org/W3128384299', 'https://openalex.org/W4318763789', 'https://openalex.org/W4322494770', 'https://openalex.org/W3081095512', 'https://openalex.org/W3126625714', 'https://openalex.org/W3018262553', 'https://openalex.org/W3164960502', 'https://openalex.org/W2950738594', 'https://openalex.org/W4353007316', 'https://openalex.org/W4386092526', 'https://openalex.org/W4380687197', 'https://openalex.org/W4376623565', 'https://openalex.org/W4381705785', 'https://openalex.org/W4386161583', 'https://openalex.org/W3165524250', 'https://openalex.org/W6744658717', 'https://openalex.org/W2914120296', 'https://openalex.org/W4376133327', 'https://openalex.org/W4383473194', 'https://openalex.org/W4366989878', 'https://openalex.org/W6853544979', 'https://openalex.org/W6811129797', 'https://openalex.org/W4321276130', 'https://openalex.org/W6850655640', 'https://openalex.org/W4367595583', 'https://openalex.org/W4327500636', 'https://openalex.org/W6785900487', 'https://openalex.org/W2213201020', 'https://openalex.org/W4378464434', 'https://openalex.org/W4381616695', 'https://openalex.org/W4379781108', 'https://openalex.org/W4365795311', 'https://openalex.org/W4383045293', 'https://openalex.org/W4366976243', 'https://openalex.org/W4294754026', 'https://openalex.org/W3175528029', 'https://openalex.org/W4285156045', 'https://openalex.org/W2395987157', 'https://openalex.org/W2033178790', 'https://openalex.org/W2766151966', 'https://openalex.org/W4377191407', 'https://openalex.org/W3162926177', 'https://openalex.org/W4226125322', 'https://openalex.org/W6804122398', 'https://openalex.org/W4200633008', 'https://openalex.org/W6779823529', 'https://openalex.org/W4226283196', 'https://openalex.org/W4387156645', 'https://openalex.org/W6786167053', 'https://openalex.org/W3047275688', 'https://openalex.org/W4298185919', 'https://openalex.org/W6791338928', 'https://openalex.org/W4221057950', 'https://openalex.org/W4366773818', 'https://openalex.org/W4360980141', 'https://openalex.org/W6853436546', 'https://openalex.org/W4361270914', 'https://openalex.org/W4383473036', 'https://openalex.org/W4366851162', 'https://openalex.org/W4283775237', 'https://openalex.org/W4386185625', 'https://openalex.org/W4378697700', 'https://openalex.org/W4361866031', 'https://openalex.org/W4386080660', 'https://openalex.org/W4366850928', 'https://openalex.org/W4385015470', 'https://openalex.org/W4324370640', 'https://openalex.org/W6810081322', 'https://openalex.org/W6912954327', 'https://openalex.org/W4376654497', 'https://openalex.org/W7006481656', 'https://openalex.org/W2395693197', 'https://openalex.org/W1627539022', 'https://openalex.org/W3162385798', 'https://openalex.org/W4226142803', 'https://openalex.org/W2137446469', 'https://openalex.org/W4294982692', 'https://openalex.org/W4376312626', 'https://openalex.org/W4361193535', 'https://openalex.org/W4378513190', 'https://openalex.org/W4378446728', 'https://openalex.org/W4366731740', 'https://openalex.org/W6851841283', 'https://openalex.org/W4377111802', 'https://openalex.org/W4320711939', 'https://openalex.org/W6851861569', 'https://openalex.org/W4365503878', 'https://openalex.org/W4327810480', 'https://openalex.org/W4304195511', 'https://openalex.org/W4362597962', 'https://openalex.org/W4376649177', 'https://openalex.org/W4318069287', 'https://openalex.org/W4372283945', 'https://openalex.org/W4366330500', 'https://openalex.org/W4367365458', 'https://openalex.org/W4386495671', 'https://openalex.org/W3043399508', 'https://openalex.org/W3046375318', 'https://openalex.org/W6854859551', 'https://openalex.org/W3180299705', 'https://openalex.org/W4315882015', 'https://openalex.org/W4366850566', 'https://openalex.org/W6850495340', 'https://openalex.org/W4323348223', 'https://openalex.org/W4376640725', 'https://openalex.org/W4368755092', 'https://openalex.org/W4321276803', 'https://openalex.org/W6772383348', 'https://openalex.org/W4384920109', 'https://openalex.org/W4381247876', 'https://openalex.org/W6766152879', 'https://openalex.org/W3119438769', 'https://openalex.org/W4307225507', 'https://openalex.org/W6759579507', 'https://openalex.org/W6796581206', 'https://openalex.org/W4386978016', 'https://openalex.org/W6853251322', 'https://openalex.org/W4379469208', 'https://openalex.org/W4376122773', 'https://openalex.org/W4382603041', 'https://openalex.org/W3045923047', 'https://openalex.org/W4225727438', 'https://openalex.org/W6796753453', 'https://openalex.org/W6852652685', 'https://openalex.org/W4313442864', 'https://openalex.org/W4377865177', 'https://openalex.org/W4220993274', 'https://openalex.org/W4382618460', 'https://openalex.org/W6856572447', 'https://openalex.org/W6810242208', 'https://openalex.org/W2951286828', 'https://openalex.org/W2885421725', 'https://openalex.org/W2789566302', 'https://openalex.org/W3153594481', 'https://openalex.org/W4361193179', 'https://openalex.org/W4379538661', 'https://openalex.org/W3164846705', 'https://openalex.org/W4285105932', 'https://openalex.org/W6777239595', 'https://openalex.org/W3093033010', 'https://openalex.org/W4361806395', 'https://openalex.org/W6992309071', 'https://openalex.org/W4367701241', 'https://openalex.org/W6851467109', 'https://openalex.org/W4309395891', 'https://openalex.org/W4380353722', 'https://openalex.org/W1487801014', 'https://openalex.org/W4308244910', 'https://openalex.org/W4321175700', 'https://openalex.org/W3156891177', 'https://openalex.org/W6851086451', 'https://openalex.org/W6846037595', 'https://openalex.org/W4321610465', 'https://openalex.org/W4315651764', 'https://openalex.org/W6852423663', 'https://openalex.org/W4366770586', 'https://openalex.org/W4385481791', 'https://openalex.org/W4324302594', 'https://openalex.org/W4226112058', 'https://openalex.org/W4221167743', 'https://openalex.org/W2001829221', 'https://openalex.org/W4311283029', 'https://openalex.org/W6854293084', 'https://openalex.org/W4303448941', 'https://openalex.org/W3010293452', 'https://openalex.org/W3010108619', 'https://openalex.org/W3109805546', 'https://openalex.org/W4317951304', 'https://openalex.org/W4312205996', 'https://openalex.org/W6848992219', 'https://openalex.org/W4382654530', 'https://openalex.org/W4384025135', 'https://openalex.org/W4380302185', 'https://openalex.org/W4381733607', 'https://openalex.org/W4382567209', 'https://openalex.org/W4318751399', 'https://openalex.org/W4297164521', 'https://openalex.org/W2017561954', 'https://openalex.org/W4382563481', 'https://openalex.org/W4384268409', 'https://openalex.org/W2966081953', 'https://openalex.org/W3174939560', 'https://openalex.org/W6833361363', 'https://openalex.org/W4366228214', 'https://openalex.org/W4229033625', 'https://openalex.org/W4385571232', 'https://openalex.org/W4385570090', 'https://openalex.org/W4380994473', 'https://openalex.org/W4285546139', 'https://openalex.org/W4385571689', 'https://openalex.org/W2792593939', 'https://openalex.org/W4385245566', 'https://openalex.org/W4385570439', 'https://openalex.org/W4318263917', 'https://openalex.org/W4362655923', 'https://openalex.org/W4360995249', 'https://openalex.org/W4360891295', 'https://openalex.org/W4360979863', 'https://openalex.org/W4389520065', 'https://openalex.org/W4390315357', 'https://openalex.org/W4226146865', 'https://openalex.org/W4364384540', 'https://openalex.org/W4318149317', 'https://openalex.org/W4321610533', 'https://openalex.org/W4383987374', 'https://openalex.org/W3100732527', 'https://openalex.org/W4385495736', 'https://openalex.org/W4390189960', 'https://openalex.org/W2953320089', 'https://openalex.org/W4320009668', 'https://openalex.org/W4287829148', 'https://openalex.org/W4389539730', 'https://openalex.org/W4293775665', 'https://openalex.org/W4367701198', 'https://openalex.org/W4322719791', 'https://openalex.org/W4378603221', 'https://openalex.org/W4399410307', 'https://openalex.org/W4383046813', 'https://openalex.org/W4385535562', 'https://openalex.org/W4205977514', 'https://openalex.org/W4386794230', 'https://openalex.org/W3100789280', 'https://openalex.org/W4388720459', 'https://openalex.org/W2963034797', 'https://openalex.org/W4385216086', 'https://openalex.org/W4385468994', 'https://openalex.org/W1575384945', 'https://openalex.org/W4389544300', 'https://openalex.org/W2996428491', 'https://openalex.org/W4288363925', 'https://openalex.org/W1996235486', 'https://openalex.org/W4319915529', 'https://openalex.org/W4386566629', 'https://openalex.org/W4380536977', 'https://openalex.org/W4362659486', 'https://openalex.org/W4385571451', 'https://openalex.org/W4283026156', 'https://openalex.org/W4226033602', 'https://openalex.org/W4383648142', 'https://openalex.org/W4389519352', 'https://openalex.org/W4385570965', 'https://openalex.org/W4309581942', 'https://openalex.org/W4362714560', 'https://openalex.org/W4286371766', 'https://openalex.org/W3195784889', 'https://openalex.org/W4321855128', 'https://openalex.org/W4205203579', 'https://openalex.org/W4387195417', 'https://openalex.org/W3167118264', 'https://openalex.org/W4362597819', 'https://openalex.org/W3168867926', 'https://openalex.org/W4322760070', 'https://openalex.org/W4384389802', 'https://openalex.org/W1915786104', 'https://openalex.org/W3036167779', 'https://openalex.org/W2980282514', 'https://openalex.org/W4380319827', 'https://openalex.org/W4368370665', 'https://openalex.org/W4384264317', 'https://openalex.org/W4399203759', 'https://openalex.org/W4286899907', 'https://openalex.org/W4306672343', 'https://openalex.org/W4385006003', 'https://openalex.org/W4327811957', 'https://openalex.org/W3212516020', 'https://openalex.org/W4366811103', 'https://openalex.org/W4362707075', 'https://openalex.org/W4366562918', 'https://openalex.org/W4362582720', 'https://openalex.org/W3044058576', 'https://openalex.org/W4287204036', 'https://openalex.org/W2312579046', 'https://openalex.org/W2997195635', 'https://openalex.org/W4324299222', 'https://openalex.org/W4385570320', 'https://openalex.org/W4388708748', 'https://openalex.org/W4389518968', 'https://openalex.org/W3011718307', 'https://openalex.org/W2145946200', 'https://openalex.org/W3112085823', 'https://openalex.org/W2970597249', 'https://openalex.org/W4382240029', 'https://openalex.org/W4381743266', 'https://openalex.org/W2121227244', 'https://openalex.org/W2097333193', 'https://openalex.org/W4390188013', 'https://openalex.org/W3001279689', 'https://openalex.org/W4289313402', 'https://openalex.org/W3096609285', 'https://openalex.org/W4389520370', 'https://openalex.org/W3162966461', 'https://openalex.org/W4287813862', 'https://openalex.org/W4389520703', 'https://openalex.org/W4321472144', 'https://openalex.org/W4367000547', 'https://openalex.org/W4320712008', 'https://openalex.org/W4387947082', 'https://openalex.org/W2964061924', 'https://openalex.org/W3174770825', 'https://openalex.org/W4360884927', 'https://openalex.org/W4385764034', 'https://openalex.org/W4386075538', 'https://openalex.org/W4293328703', 'https://openalex.org/W3016339201', 'https://openalex.org/W2131256974', 'https://openalex.org/W2611669587', 'https://openalex.org/W2963925437', 'https://openalex.org/W3028999579', 'https://openalex.org/W4361807050', 'https://openalex.org/W2963961878', 'https://openalex.org/W3111025719', 'https://openalex.org/W4383108457', 'https://openalex.org/W4281557260', 'https://openalex.org/W4307079201', 'https://openalex.org/W3034885317', 'https://openalex.org/W4382998379', 'https://openalex.org/W4386114105', 'https://openalex.org/W4241742952', 'https://openalex.org/W4385565485', 'https://openalex.org/W4382132796', 'https://openalex.org/W2765340009', 'https://openalex.org/W2963211188', 'https://openalex.org/W4389524555', 'https://openalex.org/W4389519587', 'https://openalex.org/W4312480274', 'https://openalex.org/W4377865136', 'https://openalex.org/W4288089799', 'https://openalex.org/W2765811365', 'https://openalex.org/W2963761396', 'https://openalex.org/W3197782487', 'https://openalex.org/W4307653761', 'https://openalex.org/W4380360954', 'https://openalex.org/W4382246105', 'https://openalex.org/W4309674289', 'https://openalex.org/W4380769213', 'https://openalex.org/W3135777491', 'https://openalex.org/W3158631574', 'https://openalex.org/W4385262268', 'https://openalex.org/W4377866403', 'https://openalex.org/W4396853307', 'https://openalex.org/W3034775979', 'https://openalex.org/W4384071683', 'https://openalex.org/W2525778437', 'https://openalex.org/W4384155024', 'https://openalex.org/W4310513840', 'https://openalex.org/W4385571157', 'https://openalex.org/W2962784628', 'https://openalex.org/W3101612813', 'https://openalex.org/W4205206005', 'https://openalex.org/W3193917007', 'https://openalex.org/W3169012807', 'https://openalex.org/W4387937197', 'https://openalex.org/W4224308101', 'https://openalex.org/W4385965642', 'https://openalex.org/W4384933271', 'https://openalex.org/W2964303773', 'https://openalex.org/W4387582980', 'https://openalex.org/W4366548330', 'https://openalex.org/W4389519818', 'https://openalex.org/W4361806442', 'https://openalex.org/W4287721977', 'https://openalex.org/W4378509449', 'https://openalex.org/W4312740349', 'https://openalex.org/W1955857676', 'https://openalex.org/W4366490174', 'https://openalex.org/W4292779060', 'https://openalex.org/W4388488349', 'https://openalex.org/W3195577433', 'https://openalex.org/W4299590935', 'https://openalex.org/W4389519409', 'https://openalex.org/W4389524317', 'https://openalex.org/W4386499220', 'https://openalex.org/W4321177655', 'https://openalex.org/W4378715452', 'https://openalex.org/W4283330306', 'https://openalex.org/W4287829318', 'https://openalex.org/W4384615697', 'https://openalex.org/W4226399820', 'https://openalex.org/W4367672983', 'https://openalex.org/W4392297945', 'https://openalex.org/W4384561707', 'https://openalex.org/W2963649795', 'https://openalex.org/W4388325436', 'https://openalex.org/W4323655724', 'https://openalex.org/W4384028668', 'https://openalex.org/W4384807943', 'https://openalex.org/W4376653844', 'https://openalex.org/W2964110616', 'https://openalex.org/W4229506649', 'https://openalex.org/W4323363618', 'https://openalex.org/W2952673310', 'https://openalex.org/W4387323848', 'https://openalex.org/W4318239126', 'https://openalex.org/W4404826238', 'https://openalex.org/W4389524500', 'https://openalex.org/W4287165115', 'https://openalex.org/W4385570507', 'https://openalex.org/W4313483544', 'https://openalex.org/W3037013468', 'https://openalex.org/W4286336838', 'https://openalex.org/W4384345748', 'https://openalex.org/W2955240493', 'https://openalex.org/W4294434188', 'https://openalex.org/W4386242298', 'https://openalex.org/W4200057797', 'https://openalex.org/W4366115126', 'https://openalex.org/W4319083882', 'https://openalex.org/W4384154861', 'https://openalex.org/W4405105409', 'https://openalex.org/W4385567113', 'https://openalex.org/W4386113271', 'https://openalex.org/W3197828817', 'https://openalex.org/W3126434301', 'https://openalex.org/W3120341153', 'https://openalex.org/W4321499561', 'https://openalex.org/W4318823652', 'https://openalex.org/W3010387158', 'https://openalex.org/W4285042490', 'https://openalex.org/W3173256823', 'https://openalex.org/W4389523721', 'https://openalex.org/W3200565935', 'https://openalex.org/W4377121468', 'https://openalex.org/W4306698380', 'https://openalex.org/W4281763794', 'https://openalex.org/W4378498682', 'https://openalex.org/W179875071', 'https://openalex.org/W2980708516', 'https://openalex.org/W4362515116', 'https://openalex.org/W4361866125', 'https://openalex.org/W4389520455']",2023-11-16
https://openalex.org/W4396919238,https://doi.org/10.1145/3659625,Talk2Care: An LLM-based Voice Assistant for Communication between Healthcare Providers and Older Adults,"Despite the plethora of telehealth applications to assist home-based older adults and healthcare providers, basic messaging and phone calls are still the most common communication methods, which suffer from limited availability, information loss, and process inefficiencies. One promising solution to facilitate patient-provider communication is to leverage large language models (LLMs) with their powerful natural conversation and summarization capability. However, there is a limited understanding of LLMs' role during the communication. We first conducted two interview studies with both older adults (N=10) and healthcare providers (N=9) to understand their needs and opportunities for LLMs in patient-provider asynchronous communication. Based on the insights, we built an LLM-powered communication system, Talk2Care, and designed interactive components for both groups: (1) For older adults, we leveraged the convenience and accessibility of voice assistants (VAs) and built an LLM-powered conversational interface for effective information collection. (2) For health providers, we built an LLM-based dashboard to summarize and present important health information based on older adults' conversations with the VA. We further conducted two user studies with older adults and providers to evaluate the usability of the system. The results showed that Talk2Care could facilitate the communication process, enrich the health information collected from older adults, and considerably save providers' efforts and time. We envision our work as an initial exploration of LLMs' capability in the intersection of healthcare and interpersonal communication.","['https://openalex.org/W4385573087', 'https://openalex.org/W4321105470', 'https://openalex.org/W4306801625', 'https://openalex.org/W4366547364', 'https://openalex.org/W4224979805', 'https://openalex.org/W2144976926', 'https://openalex.org/W3004483087', 'https://openalex.org/W3216972663', 'https://openalex.org/W2986874029', 'https://openalex.org/W3047954551', 'https://openalex.org/W1977604318', 'https://openalex.org/W4286273849', 'https://openalex.org/W2589953586', 'https://openalex.org/W3160029355', 'https://openalex.org/W4323050332', 'https://openalex.org/W2939803556', 'https://openalex.org/W2750766457', 'https://openalex.org/W2290835535', 'https://openalex.org/W2925512010', 'https://openalex.org/W2190965524', 'https://openalex.org/W2183607324', 'https://openalex.org/W4366590507', 'https://openalex.org/W2984385779', 'https://openalex.org/W2981276052', 'https://openalex.org/W4307444957', 'https://openalex.org/W4206502347', 'https://openalex.org/W2123508139', 'https://openalex.org/W4206580551', 'https://openalex.org/W2000435401', 'https://openalex.org/W2971722558', 'https://openalex.org/W4225163270', 'https://openalex.org/W2514862440', 'https://openalex.org/W2798849947', 'https://openalex.org/W4366549000', 'https://openalex.org/W2907049732', 'https://openalex.org/W4366591012', 'https://openalex.org/W4205494537', 'https://openalex.org/W4319662095', 'https://openalex.org/W4225001143', 'https://openalex.org/W4313433335', 'https://openalex.org/W1967487362', 'https://openalex.org/W3080958753', 'https://openalex.org/W4366548761', 'https://openalex.org/W3102775160', 'https://openalex.org/W4385572383', 'https://openalex.org/W2756371034', 'https://openalex.org/W4366769280', 'https://openalex.org/W2766863773', 'https://openalex.org/W3205235034', 'https://openalex.org/W3158465421', 'https://openalex.org/W4229979157', 'https://openalex.org/W4385847487', 'https://openalex.org/W3042276730', 'https://openalex.org/W3088440420', 'https://openalex.org/W4385570813', 'https://openalex.org/W1983080599', 'https://openalex.org/W2946006801', 'https://openalex.org/W3194914904', 'https://openalex.org/W3047975279', 'https://openalex.org/W3126844691', 'https://openalex.org/W4321013654', 'https://openalex.org/W3094018806', 'https://openalex.org/W2589021616', 'https://openalex.org/W3160135300', 'https://openalex.org/W3100983593', 'https://openalex.org/W4224326482', 'https://openalex.org/W3042239274', 'https://openalex.org/W3130337437', 'https://openalex.org/W3032205432', 'https://openalex.org/W4249329670', 'https://openalex.org/W2068146554', 'https://openalex.org/W4366549767', 'https://openalex.org/W3012134120', 'https://openalex.org/W3120083395', 'https://openalex.org/W4380559074', 'https://openalex.org/W4396219010', 'https://openalex.org/W2103268859', 'https://openalex.org/W4360991106', 'https://openalex.org/W4360978668', 'https://openalex.org/W4288080268', 'https://openalex.org/W2972890723', 'https://openalex.org/W4319837253', 'https://openalex.org/W4392503764', 'https://openalex.org/W4308610353', 'https://openalex.org/W4225164728', 'https://openalex.org/W4288359828', 'https://openalex.org/W4389524500', 'https://openalex.org/W3160278099']",2024-05-13
https://openalex.org/W4391021666,https://doi.org/10.1109/asru57964.2023.10389705,On Decoder-Only Architecture For Speech-to-Text and Large Language Model Integration,"Large language models (LLMs) have achieved remarkable success in the field of natural language processing, enabling better human-computer interaction using natural language. However, the seamless integration of speech signals into LLMs has not been explored well. The ""decoder-only"" architecture has also not been well studied for speech processing tasks. In this research, we introduce Speech-LLaMA, a novel approach that effectively incorporates acoustic information into text-based large language models. Our method leverages Connectionist Temporal Classification and a simple audio encoder to map the compressed acoustic features to the continuous semantic space of the LLM. In addition, we further probe the decoder-only architecture for speech-to-text tasks by training a smaller scale randomly initialized speech-LLaMA model from speech-text paired data alone. We conduct experiments on multilingual speech-to-text translation tasks and demonstrate a significant improvement over strong baselines, highlighting the potential advantages of decoder-only models for speech-to-text conversion.","['https://openalex.org/W6778883912', 'https://openalex.org/W6810081322', 'https://openalex.org/W6850625674', 'https://openalex.org/W6739901393', 'https://openalex.org/W6851847159', 'https://openalex.org/W6851513886', 'https://openalex.org/W6852326057', 'https://openalex.org/W6852818750', 'https://openalex.org/W3211278025', 'https://openalex.org/W6851950068', 'https://openalex.org/W6851592950', 'https://openalex.org/W6852489829', 'https://openalex.org/W6810334672', 'https://openalex.org/W6853611000', 'https://openalex.org/W4389524500', 'https://openalex.org/W6853998256', 'https://openalex.org/W6848735303', 'https://openalex.org/W6850334629', 'https://openalex.org/W6851317108', 'https://openalex.org/W3153583341', 'https://openalex.org/W6796581206', 'https://openalex.org/W2901607128', 'https://openalex.org/W3034625919', 'https://openalex.org/W6846175339', 'https://openalex.org/W4224137820', 'https://openalex.org/W4385823495', 'https://openalex.org/W6780680273', 'https://openalex.org/W2963532001', 'https://openalex.org/W6769627184', 'https://openalex.org/W6732953234', 'https://openalex.org/W2605131327', 'https://openalex.org/W6847363464', 'https://openalex.org/W2739883972', 'https://openalex.org/W6773820404', 'https://openalex.org/W4388979610', 'https://openalex.org/W6757817989', 'https://openalex.org/W2963250244', 'https://openalex.org/W6854218657', 'https://openalex.org/W4366330503', 'https://openalex.org/W4393178509', 'https://openalex.org/W3168867926', 'https://openalex.org/W4391021781', 'https://openalex.org/W4361866031', 'https://openalex.org/W4323651091', 'https://openalex.org/W4292779060', 'https://openalex.org/W4381827575', 'https://openalex.org/W4301581299', 'https://openalex.org/W4375958083', 'https://openalex.org/W4322718191', 'https://openalex.org/W4288089799', 'https://openalex.org/W2908510526', 'https://openalex.org/W4385245566', 'https://openalex.org/W4367628410', 'https://openalex.org/W4391021457', 'https://openalex.org/W4377372369', 'https://openalex.org/W4224308101', 'https://openalex.org/W4313679638', 'https://openalex.org/W4225323055', 'https://openalex.org/W4366850747', 'https://openalex.org/W4364382977', 'https://openalex.org/W3043665049', 'https://openalex.org/W4378501656']",2023-12-16
https://openalex.org/W4402448026,https://doi.org/10.3390/hydrology11090148,"The Implementation of Multimodal Large Language Models for Hydrological Applications: A Comparative Study of GPT-4 Vision, Gemini, LLaVa, and Multimodal-GPT","Large Language Models (LLMs) combined with visual foundation models have demonstrated significant advancements, achieving intelligence levels comparable to human capabilities. This study analyzes the latest Multimodal LLMs (MLLMs), including Multimodal-GPT, GPT-4 Vision, Gemini, and LLaVa, with a focus on hydrological applications such as flood management, water level monitoring, agricultural water discharge, and water pollution management. We evaluated these MLLMs on hydrology-specific tasks, testing their response generation and real-time suitability in complex real-world scenarios. Prompts were designed to enhance the models’ visual inference capabilities and contextual comprehension from images. Our findings reveal that GPT-4 Vision demonstrated exceptional proficiency in interpreting visual data, providing accurate assessments of flood severity and water quality. Additionally, MLLMs showed potential in various hydrological applications, including drought prediction, streamflow forecasting, groundwater management, and wetland conservation. These models can optimize water resource management by predicting rainfall, evaporation rates, and soil moisture levels, thereby promoting sustainable agricultural practices. This research provides valuable insights into the potential applications of advanced AI models in addressing complex hydrological challenges and improving real-time decision-making in water resource management","['https://openalex.org/W4388490623', 'https://openalex.org/W3092962781', 'https://openalex.org/W4393255021', 'https://openalex.org/W4307704580', 'https://openalex.org/W4392820477', 'https://openalex.org/W4387437512', 'https://openalex.org/W4388281482', 'https://openalex.org/W4388831665', 'https://openalex.org/W4311963991', 'https://openalex.org/W3205607555', 'https://openalex.org/W2811064840', 'https://openalex.org/W4396542473', 'https://openalex.org/W4396816283', 'https://openalex.org/W4401524370', 'https://openalex.org/W4392759165', 'https://openalex.org/W4389279079', 'https://openalex.org/W4382457527', 'https://openalex.org/W4225323055', 'https://openalex.org/W4226182655', 'https://openalex.org/W4389520252', 'https://openalex.org/W4386075985', 'https://openalex.org/W4390874324', 'https://openalex.org/W2966683369', 'https://openalex.org/W2964067226', 'https://openalex.org/W4389519587', 'https://openalex.org/W4389524500', 'https://openalex.org/W2073302931', 'https://openalex.org/W6809646742', 'https://openalex.org/W6838865847', 'https://openalex.org/W6811284106', 'https://openalex.org/W2745461083', 'https://openalex.org/W2966715458', 'https://openalex.org/W2970869018', 'https://openalex.org/W2998356391', 'https://openalex.org/W2970231061', 'https://openalex.org/W2969876226', 'https://openalex.org/W2997591391', 'https://openalex.org/W3090449556', 'https://openalex.org/W4387592425', 'https://openalex.org/W2108598243', 'https://openalex.org/W2295107390', 'https://openalex.org/W6676948653', 'https://openalex.org/W4361988710', 'https://openalex.org/W6868279600', 'https://openalex.org/W4385175044', 'https://openalex.org/W4306851324', 'https://openalex.org/W4281557260', 'https://openalex.org/W4226369848', 'https://openalex.org/W2113496055', 'https://openalex.org/W4398220429', 'https://openalex.org/W4221143046']",2024-09-11
https://openalex.org/W4391021627,https://doi.org/10.1109/asru57964.2023.10389742,Joint Audio and Speech Understanding,"Humans are surrounded by audio signals that include both speech and non-speech sounds. The recognition and understanding of speech and non-speech audio events, along with a profound comprehension of the relationship between them, constitute fundamental cognitive capabilities. For the first time, we build a machine learning model, called LTU-AS, that has a conceptually similar universal audio perception and advanced reasoning ability. Specifically, by integrating Whisper [1] as a perception module and LLaMA [2] as a reasoning module, LTU-AS can simultaneously recognize and jointly understand spoken text, speech paralinguistics, and non-speech audio events - almost everything perceivable from audio signals.","['https://openalex.org/W6847363464', 'https://openalex.org/W6850625674', 'https://openalex.org/W4385807453', 'https://openalex.org/W6778883912', 'https://openalex.org/W6853249747', 'https://openalex.org/W3196974791', 'https://openalex.org/W4375869243', 'https://openalex.org/W3095738461', 'https://openalex.org/W4224932123', 'https://openalex.org/W6809947431', 'https://openalex.org/W4297841687', 'https://openalex.org/W6851847159', 'https://openalex.org/W6850477478', 'https://openalex.org/W4389524500', 'https://openalex.org/W4390874621', 'https://openalex.org/W4385823034', 'https://openalex.org/W6739901393', 'https://openalex.org/W6677258307', 'https://openalex.org/W6796581206', 'https://openalex.org/W2963096510', 'https://openalex.org/W6768028577', 'https://openalex.org/W2146334809', 'https://openalex.org/W2883409523', 'https://openalex.org/W2808631503', 'https://openalex.org/W4210267911', 'https://openalex.org/W2191779130', 'https://openalex.org/W6732646663', 'https://openalex.org/W3162999565', 'https://openalex.org/W2593116425', 'https://openalex.org/W3015371781', 'https://openalex.org/W4205689591', 'https://openalex.org/W2982554818', 'https://openalex.org/W3205743929', 'https://openalex.org/W2052666245', 'https://openalex.org/W1494198834', 'https://openalex.org/W6785932716', 'https://openalex.org/W4226442948', 'https://openalex.org/W3204696009', 'https://openalex.org/W3209984917', 'https://openalex.org/W3097525302', 'https://openalex.org/W3176445421', 'https://openalex.org/W4372266552', 'https://openalex.org/W2964067969', 'https://openalex.org/W4311000453', 'https://openalex.org/W4292779060', 'https://openalex.org/W4377130946', 'https://openalex.org/W4300957348', 'https://openalex.org/W4385245566', 'https://openalex.org/W4221150524', 'https://openalex.org/W3168867926', 'https://openalex.org/W4322718191', 'https://openalex.org/W2973049837', 'https://openalex.org/W4322825254', 'https://openalex.org/W4393178509']",2023-12-16
https://openalex.org/W4392931626,https://doi.org/10.1109/icassp48485.2024.10445874,Connecting Speech Encoder and Large Language Model for ASR,"The impressive capability and versatility of large language models (LLMs) have aroused increasing attention in automatic speech recognition (ASR), with several pioneering studies attempting to build integrated ASR models by connecting a speech encoder with an LLM. This paper presents a comparative study of three commonly used structures as connectors, including fully connected layers, multi-head cross-attention, and Q-Former. Speech encoders from the Whisper model series as well as LLMs from the Vicuna model series with different model sizes were studied. Experiments were performed on the commonly used LibriSpeech, Common Voice, and GigaSpeech datasets, where the LLMs with Q-Formers demonstrated consistent and considerable word error rate (WER) reductions over LLMs with other connector structures. Q-Former-based LLMs can generalise well to out-of-domain datasets, where 12% relative WER reductions over the Whisper baseline ASR model were achieved on the Eval2000 test set without using any in-domain training data from Switchboard. Moreover, a novel segment-level Q-Former is proposed to enable LLMs to recognise speech segments with a duration exceeding the limitation of the encoders, which results in 17% relative WER reductions over other connector structures on 90-second-long speech data.","['https://openalex.org/W6778883912', 'https://openalex.org/W6852800892', 'https://openalex.org/W6850625674', 'https://openalex.org/W6794212170', 'https://openalex.org/W4393178509', 'https://openalex.org/W6851513886', 'https://openalex.org/W4389524500', 'https://openalex.org/W6853998256', 'https://openalex.org/W6852326057', 'https://openalex.org/W6855801281', 'https://openalex.org/W4391021666', 'https://openalex.org/W6855414158', 'https://openalex.org/W4391021457', 'https://openalex.org/W6854445763', 'https://openalex.org/W6855793536', 'https://openalex.org/W6847363464', 'https://openalex.org/W6853094705', 'https://openalex.org/W6849177959', 'https://openalex.org/W3209059054', 'https://openalex.org/W4226033575', 'https://openalex.org/W6850218400', 'https://openalex.org/W6810334672', 'https://openalex.org/W6851950068', 'https://openalex.org/W6853116092', 'https://openalex.org/W4389519587', 'https://openalex.org/W6852447913', 'https://openalex.org/W6853089504', 'https://openalex.org/W6853515732', 'https://openalex.org/W6853249747', 'https://openalex.org/W4392909390', 'https://openalex.org/W1494198834', 'https://openalex.org/W6771467084', 'https://openalex.org/W3198694222', 'https://openalex.org/W4385807435', 'https://openalex.org/W4386351448', 'https://openalex.org/W4402671548', 'https://openalex.org/W4366850747', 'https://openalex.org/W4378711593', 'https://openalex.org/W4375958083', 'https://openalex.org/W4376312115', 'https://openalex.org/W4323066695', 'https://openalex.org/W4380994269', 'https://openalex.org/W4378174011', 'https://openalex.org/W4361866031', 'https://openalex.org/W4383989131', 'https://openalex.org/W4377130946', 'https://openalex.org/W4392908891', 'https://openalex.org/W4392903956', 'https://openalex.org/W4322718191', 'https://openalex.org/W4381827575']",2024-03-18
https://openalex.org/W4392904805,https://doi.org/10.1109/icassp48485.2024.10447112,"VoxtLM: Unified Decoder-Only Models for Consolidating Speech Recognition, Synthesis and Speech, Text Continuation Tasks","We propose a decoder-only language model, VoxtLM, that can perform four tasks: speech recognition, speech synthesis, text generation, and speech continuation. VoxtLM integrates text vocabulary with discrete speech tokens from self-supervised speech features and uses special tokens to enable multitask learning. Compared to a single-task model, VoxtLM exhibits a significant improvement in speech synthesis, with improvements in both speech intelligibility from 28.9 to 5.6 and objective quality from 2.68 to 3.90. VoxtLM also improves speech generation and speech recognition performance over the single-task counterpart. Further, VoxtLM is trained with publicly available data and training recipes and model checkpoints are open-sourced to make fully reproducible work.","['https://openalex.org/W6778883912', 'https://openalex.org/W6790356757', 'https://openalex.org/W4381786045', 'https://openalex.org/W6777028661', 'https://openalex.org/W6848735303', 'https://openalex.org/W4388017359', 'https://openalex.org/W2964243274', 'https://openalex.org/W6778823374', 'https://openalex.org/W6802465204', 'https://openalex.org/W4226120743', 'https://openalex.org/W3209059054', 'https://openalex.org/W4226033575', 'https://openalex.org/W3215615641', 'https://openalex.org/W4307323391', 'https://openalex.org/W6803092890', 'https://openalex.org/W6852381208', 'https://openalex.org/W6853611000', 'https://openalex.org/W4389524500', 'https://openalex.org/W6811340617', 'https://openalex.org/W6852781825', 'https://openalex.org/W2963250244', 'https://openalex.org/W2962784628', 'https://openalex.org/W2963979492', 'https://openalex.org/W4385822683', 'https://openalex.org/W6783867762', 'https://openalex.org/W2890964092', 'https://openalex.org/W6786696081', 'https://openalex.org/W2972394484', 'https://openalex.org/W3202278141', 'https://openalex.org/W2995181338', 'https://openalex.org/W1494198834', 'https://openalex.org/W3095410713', 'https://openalex.org/W2972359262', 'https://openalex.org/W2998572311', 'https://openalex.org/W6631190155', 'https://openalex.org/W6796464841', 'https://openalex.org/W4319862255', 'https://openalex.org/W4313679638', 'https://openalex.org/W2899575547', 'https://openalex.org/W3024605872', 'https://openalex.org/W4378501656', 'https://openalex.org/W3207222250', 'https://openalex.org/W4379540238', 'https://openalex.org/W4394671563', 'https://openalex.org/W4229005866', 'https://openalex.org/W4377865046']",2024-03-18
https://openalex.org/W4385572615,https://doi.org/10.18653/v1/2023.findings-acl.406,ML-LMCL: Mutual Learning and Large-Margin Contrastive Learning for Improving ASR Robustness in Spoken Language Understanding,"Spoken language understanding (SLU) is a fundamental task in the task-oriented dialogue systems. However, the inevitable errors from automatic speech recognition (ASR) usually impair the understanding performance and lead to error propagation. Although there are some attempts to address this problem through contrastive learning, they (1) treat clean manual transcripts and ASR transcripts equally without discrimination in fine-tuning; (2) neglect the fact that the semantically similar pairs are still pushed away when applying contrastive learning; (3) suffer from the problem of Kullback–Leibler (KL) vanishing. In this paper, we propose Mutual Learning and Large-Margin Contrastive Learning (ML-LMCL), a novel framework for improving ASR robustness in SLU. Specifically, in fine-tuning, we apply mutual learning and train two SLU models on the manual transcripts and the ASR transcripts, respectively, aiming to iteratively share knowledge between these two models. We also introduce a distance polarization regularizer to avoid pushing away the intra-cluster pairs as much as possible. Moreover, we use a cyclical annealing schedule to mitigate KL vanishing issue. Experiments on three datasets show that ML-LMCL outperforms existing models and achieves new state-of-the-art performance.","['https://openalex.org/W4385571007', 'https://openalex.org/W3186064588', 'https://openalex.org/W2070246124', 'https://openalex.org/W2618039218', 'https://openalex.org/W3108541407', 'https://openalex.org/W4385571399', 'https://openalex.org/W3016256870', 'https://openalex.org/W3100460087', 'https://openalex.org/W3199926081', 'https://openalex.org/W3212972574', 'https://openalex.org/W4225403296', 'https://openalex.org/W4221165114', 'https://openalex.org/W4226353593', 'https://openalex.org/W4285141652', 'https://openalex.org/W3167478203', 'https://openalex.org/W3009561768', 'https://openalex.org/W3118062200', 'https://openalex.org/W2951670304', 'https://openalex.org/W4324314566', 'https://openalex.org/W4285296644', 'https://openalex.org/W3101706601', 'https://openalex.org/W4298093396', 'https://openalex.org/W2962680827', 'https://openalex.org/W2896457183', 'https://openalex.org/W3099206234', 'https://openalex.org/W4385570101', 'https://openalex.org/W4353114848', 'https://openalex.org/W4205879700', 'https://openalex.org/W1965555277', 'https://openalex.org/W2157364932', 'https://openalex.org/W1522301498', 'https://openalex.org/W3156636935', 'https://openalex.org/W4310924890', 'https://openalex.org/W1821462560', 'https://openalex.org/W3035524453', 'https://openalex.org/W2555897561', 'https://openalex.org/W4285821234', 'https://openalex.org/W4285144860', 'https://openalex.org/W4287666709', 'https://openalex.org/W3176967746', 'https://openalex.org/W4226031686', 'https://openalex.org/W3173190788', 'https://openalex.org/W3197744084', 'https://openalex.org/W2962717182', 'https://openalex.org/W1975244201', 'https://openalex.org/W3214254540', 'https://openalex.org/W4375869190', 'https://openalex.org/W2963066655', 'https://openalex.org/W3005680577', 'https://openalex.org/W4285222370', 'https://openalex.org/W4287552504', 'https://openalex.org/W2948957608', 'https://openalex.org/W4214768891', 'https://openalex.org/W3114505362', 'https://openalex.org/W3093312917', 'https://openalex.org/W4389524500', 'https://openalex.org/W648947103', 'https://openalex.org/W2077302143', 'https://openalex.org/W4287890956', 'https://openalex.org/W3141583950', 'https://openalex.org/W3175362188', 'https://openalex.org/W2951585248', 'https://openalex.org/W2894913190', 'https://openalex.org/W4229080094', 'https://openalex.org/W3173783447', 'https://openalex.org/W3035030897', 'https://openalex.org/W2965373594']",2023-01-01
https://openalex.org/W4391021623,https://doi.org/10.1109/asru57964.2023.10389703,SLM: Bridge the Thin Gap Between Speech and Text Foundation Models,"We present a joint Speech and Language Model (SLM), a multitask, multilingual, and dual-modal model that takes advantage of pretrained foundational speech and language models. SLM freezes the pretrained foundation models to maximally preserves their capabilities, and only trains a simple adapter with just 1% (156M) of the foundation models' parameters. This adaptation not only leads SLM to achieve strong performance on conventional tasks such as automatic speech recognition (ASR) and automatic speech translation (AST), but also unlocks the novel capability of zero-shot instruction-following for more diverse tasks. Given a speech input and a text instruction, SLM is able to perform unseen generation tasks including contextual biasing ASR using real-time context, dialog generation, speech continuation, and question answering. Our approach demonstrates that the representational gap between pretrained speech and language models is narrower than one would expect, and can be bridged by a simple adaptation mechanism. As a result, SLM is not only efficient to train, but also inherits strong capabilities already present in foundation models of different modalities.","['https://openalex.org/W6852800892', 'https://openalex.org/W6850218400', 'https://openalex.org/W6847363464', 'https://openalex.org/W6759579507', 'https://openalex.org/W4385822641', 'https://openalex.org/W4389524500', 'https://openalex.org/W3209059054', 'https://openalex.org/W6853998256', 'https://openalex.org/W4381786045', 'https://openalex.org/W6852818750', 'https://openalex.org/W4386071707', 'https://openalex.org/W6853249747', 'https://openalex.org/W4385822589', 'https://openalex.org/W3196509775', 'https://openalex.org/W3197324626', 'https://openalex.org/W6784545093', 'https://openalex.org/W6810673746', 'https://openalex.org/W6769627184', 'https://openalex.org/W6845943790', 'https://openalex.org/W6784577980', 'https://openalex.org/W6791904447', 'https://openalex.org/W3119308075', 'https://openalex.org/W4319862635', 'https://openalex.org/W38194800', 'https://openalex.org/W2101105183', 'https://openalex.org/W2963532001', 'https://openalex.org/W4319862232', 'https://openalex.org/W6810259195', 'https://openalex.org/W4226120743', 'https://openalex.org/W6847568899', 'https://openalex.org/W2912924812', 'https://openalex.org/W6796581206', 'https://openalex.org/W6810334672', 'https://openalex.org/W4385573964', 'https://openalex.org/W6847076894', 'https://openalex.org/W100623710', 'https://openalex.org/W6755207826', 'https://openalex.org/W3168867926', 'https://openalex.org/W3091928890', 'https://openalex.org/W4307079201', 'https://openalex.org/W4377372369', 'https://openalex.org/W3139918052', 'https://openalex.org/W4288089799', 'https://openalex.org/W2896457183', 'https://openalex.org/W4323066695', 'https://openalex.org/W4385571124', 'https://openalex.org/W3169483174', 'https://openalex.org/W4221155340', 'https://openalex.org/W4377130946', 'https://openalex.org/W4381827575']",2023-12-16
https://openalex.org/W4392903872,https://doi.org/10.1109/icassp48485.2024.10447553,SALM: Speech-Augmented Language Model with in-Context Learning for Speech Recognition and Translation,"We present a novel Speech Augmented Language Model (SALM) with multitask and in-context learning capabilities. SALM comprises a frozen text LLM, a audio encoder, a modality adapter module, and LoRA layers to accommodate speech input and associated task instructions. The unified SALM not only achieves performance on par with task-specific Conformer baselines for Automatic Speech Recognition (ASR) and Speech Translation (AST), but also exhibits zero-shot in-context learning capabilities, demonstrated through keyword-boosting task for ASR and AST. Moreover, speech supervised in-context training is proposed to bridge the gap between LLM training and downstream speech tasks, which further boosts the in-context learning ability of speech-to-text models. Proposed model is open-sourced via NeMo toolkit <sup xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">1</sup> .","['https://openalex.org/W4393178509', 'https://openalex.org/W6852326057', 'https://openalex.org/W4372269772', 'https://openalex.org/W4385822949', 'https://openalex.org/W4389524500', 'https://openalex.org/W6853998256', 'https://openalex.org/W6853249747', 'https://openalex.org/W4392910583', 'https://openalex.org/W4391021666', 'https://openalex.org/W6855414158', 'https://openalex.org/W6767997687', 'https://openalex.org/W6767671539', 'https://openalex.org/W4385822953', 'https://openalex.org/W6848735303', 'https://openalex.org/W4252812408', 'https://openalex.org/W4391021773', 'https://openalex.org/W2889012072', 'https://openalex.org/W2886319145', 'https://openalex.org/W4225308107', 'https://openalex.org/W4226120743', 'https://openalex.org/W4392979802', 'https://openalex.org/W4391021542', 'https://openalex.org/W3097777922', 'https://openalex.org/W6796581206', 'https://openalex.org/W6800875267', 'https://openalex.org/W6846665566', 'https://openalex.org/W6761551260', 'https://openalex.org/W1494198834', 'https://openalex.org/W4385570170', 'https://openalex.org/W3092085609', 'https://openalex.org/W3005302685', 'https://openalex.org/W6857054612', 'https://openalex.org/W4387595589', 'https://openalex.org/W2963499882', 'https://openalex.org/W4387891768', 'https://openalex.org/W4375958083', 'https://openalex.org/W2938704169', 'https://openalex.org/W4392903956', 'https://openalex.org/W4381827575', 'https://openalex.org/W2974231335', 'https://openalex.org/W4313679638', 'https://openalex.org/W3168867926', 'https://openalex.org/W4310428868', 'https://openalex.org/W2973727699', 'https://openalex.org/W4377130946']",2024-03-18
https://openalex.org/W4393160357,https://doi.org/10.1609/aaai.v38i16.29738,Towards Multi-Intent Spoken Language Understanding via Hierarchical Attention and Optimal Transport,"Multi-Intent spoken language understanding (SLU) can handle complicated utterances expressing multiple intents, which has attracted increasing attention from researchers. Although existing models have achieved promising performance, most of them still suffer from two leading problems: (1) each intent has its specific scope and the semantic information outside the scope might potentially hinder accurate predictions, i.e. scope barrier; (2) only the guidance from intent to slot is modeled but the guidance from slot to intent is often neglected, i.e. unidirectional guidance. In this paper, we propose a novel Multi-Intent SLU framework termed HAOT, which utilizes hierarchical attention to divide the scopes of each intent and applies optimal transport to achieve the mutual guidance between slot and intent. Experiments demonstrate that our model achieves state-of-the-art performance on two public Multi-Intent SLU datasets, obtaining the 3.4 improvement on MixATIS dataset compared to the previous best models in overall accuracy.","['https://openalex.org/W2739748921', 'https://openalex.org/W2133564696', 'https://openalex.org/W3092241499', 'https://openalex.org/W2910092637', 'https://openalex.org/W3193282380', 'https://openalex.org/W3020257313', 'https://openalex.org/W3135235529', 'https://openalex.org/W4309873553', 'https://openalex.org/W4385572615', 'https://openalex.org/W6846505686', 'https://openalex.org/W4387848696', 'https://openalex.org/W2158131535', 'https://openalex.org/W2896457183', 'https://openalex.org/W2949325756', 'https://openalex.org/W2946085385', 'https://openalex.org/W4323570427', 'https://openalex.org/W6669997747', 'https://openalex.org/W6774632908', 'https://openalex.org/W2474111273', 'https://openalex.org/W6631190155', 'https://openalex.org/W6621906925', 'https://openalex.org/W2891533927', 'https://openalex.org/W3212491726', 'https://openalex.org/W4321392589', 'https://openalex.org/W4308671340', 'https://openalex.org/W4224240992', 'https://openalex.org/W2509005166', 'https://openalex.org/W2965373594', 'https://openalex.org/W4386875681', 'https://openalex.org/W2971167298', 'https://openalex.org/W3168129742', 'https://openalex.org/W3092032275', 'https://openalex.org/W4255839052', 'https://openalex.org/W4385567178', 'https://openalex.org/W6853419591', 'https://openalex.org/W648947103', 'https://openalex.org/W6739901393', 'https://openalex.org/W6745537798', 'https://openalex.org/W6756667841', 'https://openalex.org/W2971351900', 'https://openalex.org/W2804945011', 'https://openalex.org/W2505121268', 'https://openalex.org/W2807744099', 'https://openalex.org/W4307008434', 'https://openalex.org/W4307008566', 'https://openalex.org/W4380994395', 'https://openalex.org/W2094472029', 'https://openalex.org/W2950813464', 'https://openalex.org/W3206809722', 'https://openalex.org/W4377143197', 'https://openalex.org/W4377371498', 'https://openalex.org/W3188008942', 'https://openalex.org/W3035690777', 'https://openalex.org/W4378472942', 'https://openalex.org/W2559542432', 'https://openalex.org/W4389519101', 'https://openalex.org/W4385571399', 'https://openalex.org/W4308670365', 'https://openalex.org/W4393160936', 'https://openalex.org/W4303580677', 'https://openalex.org/W4375869235', 'https://openalex.org/W4382119366', 'https://openalex.org/W2963066655', 'https://openalex.org/W3170264707', 'https://openalex.org/W4379256118', 'https://openalex.org/W4382202608', 'https://openalex.org/W2963974889', 'https://openalex.org/W4385573070', 'https://openalex.org/W4288629163', 'https://openalex.org/W4310924890', 'https://openalex.org/W4297733535', 'https://openalex.org/W4375868795', 'https://openalex.org/W3098800734', 'https://openalex.org/W3039256091', 'https://openalex.org/W4286905522', 'https://openalex.org/W1522301498', 'https://openalex.org/W4385245566', 'https://openalex.org/W4385570154', 'https://openalex.org/W4385570101', 'https://openalex.org/W3173794693', 'https://openalex.org/W3090449556', 'https://openalex.org/W2904551248', 'https://openalex.org/W658020064', 'https://openalex.org/W4389524500', 'https://openalex.org/W4385573163', 'https://openalex.org/W2077302143', 'https://openalex.org/W2970597249', 'https://openalex.org/W4388084955']",2024-03-24
https://openalex.org/W4392902647,https://doi.org/10.1109/icassp48485.2024.10446343,Extending Large Language Models for Speech and Audio Captioning,"Multimodal large language models (LLMs) have shown promising visual perception abilities by connecting with image encoders, but their performance on auditory tasks has not yet been widely investigated. Meanwhile, automatic speech recognition (ASR) and automatic audio captioning (AAC) are often achieved with separate systems, resulting in incomplete auditory perception abilities. To fill in these gaps, in this paper, we present the first study that achieves both ASR and AAC by connecting an LLM with auditory encoders. A dual auditory encoder structure is proposed, integrating the Whisper encoder for speech and the BEATs encoder for audio events with a high temporal resolution by using a Q-Former at the window level. Experiments for ASR and AAC are performed correspondingly on the widely used LibriSpeech, GigaSpeech, WavCaps, AudioCaps, and Clotho datasets and yield promising results. In particular, state-of-the-art results are achieved on GigaSpeech, AudioCaps and Clotho. Our model is also able to caption speech and audio events simultaneously from clips with mixed speech and background audio events, which is a step towards more complete machine auditory perception.","['https://openalex.org/W6850625674', 'https://openalex.org/W6810334672', 'https://openalex.org/W6849177959', 'https://openalex.org/W6853116092', 'https://openalex.org/W4392903956', 'https://openalex.org/W4391021666', 'https://openalex.org/W6852326057', 'https://openalex.org/W4389524500', 'https://openalex.org/W6853998256', 'https://openalex.org/W6853249747', 'https://openalex.org/W4389519587', 'https://openalex.org/W6852447913', 'https://openalex.org/W6853094705', 'https://openalex.org/W6854596735', 'https://openalex.org/W6855952517', 'https://openalex.org/W4224932123', 'https://openalex.org/W6847363464', 'https://openalex.org/W6848208918', 'https://openalex.org/W6846004400', 'https://openalex.org/W2949376505', 'https://openalex.org/W6851513886', 'https://openalex.org/W6851847159', 'https://openalex.org/W6796581206', 'https://openalex.org/W1494198834', 'https://openalex.org/W3198694222', 'https://openalex.org/W4400033239', 'https://openalex.org/W3015591594', 'https://openalex.org/W4377130946', 'https://openalex.org/W4375958083', 'https://openalex.org/W4378711593', 'https://openalex.org/W3103022576', 'https://openalex.org/W4380994269', 'https://openalex.org/W4322718191', 'https://openalex.org/W4393178509', 'https://openalex.org/W4392909390', 'https://openalex.org/W4384648427', 'https://openalex.org/W4381827575', 'https://openalex.org/W4376312115', 'https://openalex.org/W4361866031', 'https://openalex.org/W4307322847']",2024-03-18
https://openalex.org/W4410737867,https://doi.org/10.1109/access.2025.3573955,A Comprehensive Overview and Analysis of Large Language Models: Trends and Challenges,"Large Language Models (LLMs) have transformed numerous fields by offering innovative solutions that drive advancements across a wide range of applications. However, their widespread adoption presents several challenges, including variations in architectures, limitations in processing capabilities, and high computational resource demands for training. Addressing these challenges is crucial for maximizing the benefits of LLMs while ensuring their responsible and efficient use. This paper reviews LLMs, focusing on their key characteristics and the factors that influence their performance. It examines several prominent families of LLMs and provides a comparative analysis of their properties. In addition, it explores the classification of LLMs based on criteria such as availability, context window, and model size. In addition, the study explores advanced fine-tuning techniques, including Parameter-Efficient fine-tuning (PEFT) and Low-Rank Adaptation (LoRA), that enhance the performance and efficiency of models. Furthermore, it reviews the wide-ranging applications of LLMs and evaluates the methodologies used to evaluate their effectiveness.","['https://openalex.org/W3211971560', 'https://openalex.org/W3088930327', 'https://openalex.org/W3153990350', 'https://openalex.org/W4322721962', 'https://openalex.org/W3026646233', 'https://openalex.org/W6767707592', 'https://openalex.org/W2971875451', 'https://openalex.org/W2885195348', 'https://openalex.org/W2982145560', 'https://openalex.org/W4385245566', 'https://openalex.org/W4387827703', 'https://openalex.org/W4318677156', 'https://openalex.org/W6904038855', 'https://openalex.org/W4387827708', 'https://openalex.org/W4394828356', 'https://openalex.org/W3194782062', 'https://openalex.org/W6866303338', 'https://openalex.org/W6810220367', 'https://openalex.org/W4297253404', 'https://openalex.org/W4390874728', 'https://openalex.org/W6796581206', 'https://openalex.org/W4395065100', 'https://openalex.org/W4391855109', 'https://openalex.org/W4391136507', 'https://openalex.org/W6854475153', 'https://openalex.org/W6861581687', 'https://openalex.org/W6874660474', 'https://openalex.org/W6862481324', 'https://openalex.org/W3128376221', 'https://openalex.org/W4394902775', 'https://openalex.org/W4404781833', 'https://openalex.org/W6875731310', 'https://openalex.org/W6798398338', 'https://openalex.org/W6851317108', 'https://openalex.org/W6855917509', 'https://openalex.org/W4375850424', 'https://openalex.org/W6860305778', 'https://openalex.org/W6862308860', 'https://openalex.org/W2964199361', 'https://openalex.org/W4392547100', 'https://openalex.org/W3012526672', 'https://openalex.org/W6849594959', 'https://openalex.org/W6847478871', 'https://openalex.org/W4399757647', 'https://openalex.org/W2970476646', 'https://openalex.org/W4390490761', 'https://openalex.org/W4401955911', 'https://openalex.org/W3176634681', 'https://openalex.org/W4382490953', 'https://openalex.org/W4398187660', 'https://openalex.org/W6853336233', 'https://openalex.org/W6858076467', 'https://openalex.org/W6861859833', 'https://openalex.org/W4399588135', 'https://openalex.org/W6763701032', 'https://openalex.org/W3162462834', 'https://openalex.org/W6872781962', 'https://openalex.org/W6856308150', 'https://openalex.org/W6810081322', 'https://openalex.org/W4399571668', 'https://openalex.org/W4362659486', 'https://openalex.org/W6860746138', 'https://openalex.org/W6854159973', 'https://openalex.org/W6874247354', 'https://openalex.org/W6857114289', 'https://openalex.org/W4391792094', 'https://openalex.org/W4402290834', 'https://openalex.org/W6874379582', 'https://openalex.org/W6852777819', 'https://openalex.org/W4394997764', 'https://openalex.org/W4200057797', 'https://openalex.org/W4402969043', 'https://openalex.org/W6849928207', 'https://openalex.org/W4393971298', 'https://openalex.org/W6850625674', 'https://openalex.org/W4401416311', 'https://openalex.org/W4401383097', 'https://openalex.org/W2551516232', 'https://openalex.org/W4385900159', 'https://openalex.org/W6869114962', 'https://openalex.org/W6860299787', 'https://openalex.org/W3159351344', 'https://openalex.org/W2035996538', 'https://openalex.org/W4401643692', 'https://openalex.org/W6855755907', 'https://openalex.org/W6851579256', 'https://openalex.org/W4312045629', 'https://openalex.org/W6769263558', 'https://openalex.org/W4362707004', 'https://openalex.org/W6859273674', 'https://openalex.org/W6869815381', 'https://openalex.org/W6866223645', 'https://openalex.org/W4387596573', 'https://openalex.org/W6873292650', 'https://openalex.org/W6850071225', 'https://openalex.org/W6861501026', 'https://openalex.org/W6856434366', 'https://openalex.org/W4389524500', 'https://openalex.org/W4410027467', 'https://openalex.org/W4386075639', 'https://openalex.org/W2067956654', 'https://openalex.org/W4386071707', 'https://openalex.org/W4385572405', 'https://openalex.org/W3192806337', 'https://openalex.org/W6855055401', 'https://openalex.org/W6838557027', 'https://openalex.org/W4402753507', 'https://openalex.org/W6868922684', 'https://openalex.org/W6875195164', 'https://openalex.org/W6770088130', 'https://openalex.org/W4385572699', 'https://openalex.org/W4393406391', 'https://openalex.org/W6860976335', 'https://openalex.org/W6867729607', 'https://openalex.org/W4402684113', 'https://openalex.org/W4405957912', 'https://openalex.org/W6873388803', 'https://openalex.org/W6873013216', 'https://openalex.org/W4321071716', 'https://openalex.org/W6856529530', 'https://openalex.org/W6674849843', 'https://openalex.org/W4404039263', 'https://openalex.org/W1916385248', 'https://openalex.org/W4383605036', 'https://openalex.org/W6852489829', 'https://openalex.org/W6850503672', 'https://openalex.org/W4405561424', 'https://openalex.org/W4389524004', 'https://openalex.org/W6852800892', 'https://openalex.org/W2596358769', 'https://openalex.org/W2071794553', 'https://openalex.org/W4399367438', 'https://openalex.org/W2899935160', 'https://openalex.org/W4308760226', 'https://openalex.org/W6850617041', 'https://openalex.org/W4310418566', 'https://openalex.org/W6846496255', 'https://openalex.org/W4401916332', 'https://openalex.org/W3079406089', 'https://openalex.org/W6780805062', 'https://openalex.org/W4402671950', 'https://openalex.org/W6810737565', 'https://openalex.org/W1929550892', 'https://openalex.org/W3213153945', 'https://openalex.org/W3102659883', 'https://openalex.org/W6846844205', 'https://openalex.org/W6903577382', 'https://openalex.org/W4396888196', 'https://openalex.org/W3100107515', 'https://openalex.org/W4385571452', 'https://openalex.org/W6784577980', 'https://openalex.org/W4389977189', 'https://openalex.org/W4390227545', 'https://openalex.org/W4387508266', 'https://openalex.org/W2057515938', 'https://openalex.org/W3095319910', 'https://openalex.org/W4327545654', 'https://openalex.org/W6851960618', 'https://openalex.org/W4396831262', 'https://openalex.org/W6852667213', 'https://openalex.org/W6851950068', 'https://openalex.org/W4287889356', 'https://openalex.org/W4382998379', 'https://openalex.org/W6846143867', 'https://openalex.org/W6859018328', 'https://openalex.org/W4403619946', 'https://openalex.org/W6859375428', 'https://openalex.org/W6859410210', 'https://openalex.org/W4388409567', 'https://openalex.org/W6855036103', 'https://openalex.org/W6856222308', 'https://openalex.org/W6849079612', 'https://openalex.org/W6842601026', 'https://openalex.org/W4327738695', 'https://openalex.org/W4386187806', 'https://openalex.org/W6703049675', 'https://openalex.org/W6769470841', 'https://openalex.org/W3080406710', 'https://openalex.org/W2108598243', 'https://openalex.org/W3198659451', 'https://openalex.org/W6810156098', 'https://openalex.org/W2395579298', 'https://openalex.org/W3015120339', 'https://openalex.org/W2910453440', 'https://openalex.org/W3034942609', 'https://openalex.org/W2115733720', 'https://openalex.org/W4404782964', 'https://openalex.org/W4406696488', 'https://openalex.org/W4402701012', 'https://openalex.org/W2346062110', 'https://openalex.org/W6738957339', 'https://openalex.org/W4386076014', 'https://openalex.org/W4283766873', 'https://openalex.org/W3119438769', 'https://openalex.org/W4304192668', 'https://openalex.org/W4393851611', 'https://openalex.org/W4390548144', 'https://openalex.org/W4389524317', 'https://openalex.org/W6857192551', 'https://openalex.org/W6873668390', 'https://openalex.org/W4385570414', 'https://openalex.org/W6872771408', 'https://openalex.org/W6755843862', 'https://openalex.org/W6873399866', 'https://openalex.org/W6861355443', 'https://openalex.org/W6837789219', 'https://openalex.org/W2811485728', 'https://openalex.org/W2896725878', 'https://openalex.org/W3086927767', 'https://openalex.org/W6857453862', 'https://openalex.org/W3199566863', 'https://openalex.org/W6861489518', 'https://openalex.org/W6861443732', 'https://openalex.org/W4385570973', 'https://openalex.org/W6787411158', 'https://openalex.org/W4389523703', 'https://openalex.org/W6873962276', 'https://openalex.org/W6869036422', 'https://openalex.org/W3199258042', 'https://openalex.org/W6874037583', 'https://openalex.org/W6853576525', 'https://openalex.org/W6967316506', 'https://openalex.org/W4402671999', 'https://openalex.org/W6873131017', 'https://openalex.org/W4409363989', 'https://openalex.org/W4399269331', 'https://openalex.org/W6865009708', 'https://openalex.org/W6849730489', 'https://openalex.org/W6852636140', 'https://openalex.org/W4387782591', 'https://openalex.org/W6861084191', 'https://openalex.org/W4389520065', 'https://openalex.org/W6849780505', 'https://openalex.org/W4385571762', 'https://openalex.org/W4402670020', 'https://openalex.org/W6855748715', 'https://openalex.org/W6856639600', 'https://openalex.org/W6873922114', 'https://openalex.org/W4390682215', 'https://openalex.org/W4400127166', 'https://openalex.org/W6784628404', 'https://openalex.org/W4205767597', 'https://openalex.org/W3106109117', 'https://openalex.org/W4409733514', 'https://openalex.org/W2963026768', 'https://openalex.org/W4405003679', 'https://openalex.org/W4402352366', 'https://openalex.org/W6857673048', 'https://openalex.org/W4404784123', 'https://openalex.org/W4385932627', 'https://openalex.org/W4385571552', 'https://openalex.org/W4403535530', 'https://openalex.org/W6851854424', 'https://openalex.org/W4401042286', 'https://openalex.org/W4312673029', 'https://openalex.org/W6767182473', 'https://openalex.org/W4293071540', 'https://openalex.org/W3197236059', 'https://openalex.org/W3190460602', 'https://openalex.org/W4387435980', 'https://openalex.org/W4392859372', 'https://openalex.org/W6858039225', 'https://openalex.org/W6871588728', 'https://openalex.org/W4386065596', 'https://openalex.org/W2979300423', 'https://openalex.org/W4285020859', 'https://openalex.org/W4403791697', 'https://openalex.org/W2986836624', 'https://openalex.org/W3013838212', 'https://openalex.org/W6769125834', 'https://openalex.org/W3183866186', 'https://openalex.org/W4406152279', 'https://openalex.org/W6853557167', 'https://openalex.org/W3172335055', 'https://openalex.org/W4364320763', 'https://openalex.org/W6870115707', 'https://openalex.org/W4394956892', 'https://openalex.org/W4405373413', 'https://openalex.org/W4392740847', 'https://openalex.org/W4393119065']",2025-01-01
https://openalex.org/W4409431856,https://doi.org/10.3390/agriculture15080847,Foundation Models in Agriculture: A Comprehensive Review,"This paper explores the transformative potential of Foundation Models (FMs) in agriculture, driven by the need for efficient and intelligent decision support systems in the face of growing global population and climate change. It begins by outlining the development history of FMs, including general FM training processes, application trends and challenges, before focusing on Agricultural Foundation Models (AFMs). The paper examines the diversity and applications of AFMs in areas like crop classification, pest detection, and crop image segmentation, and delves into specific use cases such as agricultural knowledge question-answering, image and video analysis, decision support, and robotics. Furthermore, it discusses the challenges faced by AFMs, including data acquisition, training efficiency, data shift, and practical application challenges. Finally, the paper discusses future development directions for AFMs, emphasizing multimodal applications, integrating AFMs across the agricultural and food sectors, and intelligent decision-making systems, ultimately aiming to promote the digitalization and intelligent transformation of agriculture.","['https://openalex.org/W4407353911', 'https://openalex.org/W4399116883', 'https://openalex.org/W3153990350', 'https://openalex.org/W2565516711', 'https://openalex.org/W6778883912', 'https://openalex.org/W6810738896', 'https://openalex.org/W4312933868', 'https://openalex.org/W4390874575', 'https://openalex.org/W4386076458', 'https://openalex.org/W2919115771', 'https://openalex.org/W6684191040', 'https://openalex.org/W2108598243', 'https://openalex.org/W2963428668', 'https://openalex.org/W2902907165', 'https://openalex.org/W4383737134', 'https://openalex.org/W2097117768', 'https://openalex.org/W2194775991', 'https://openalex.org/W639708223', 'https://openalex.org/W2963037989', 'https://openalex.org/W2963150697', 'https://openalex.org/W3094502228', 'https://openalex.org/W6790978476', 'https://openalex.org/W4391094120', 'https://openalex.org/W6846002521', 'https://openalex.org/W2896457183', 'https://openalex.org/W6769627184', 'https://openalex.org/W6810081322', 'https://openalex.org/W6846496255', 'https://openalex.org/W6851592950', 'https://openalex.org/W4318718936', 'https://openalex.org/W6810334672', 'https://openalex.org/W4402660140', 'https://openalex.org/W4312567437', 'https://openalex.org/W4389519587', 'https://openalex.org/W6850503672', 'https://openalex.org/W6852447913', 'https://openalex.org/W4389524500', 'https://openalex.org/W6797716411', 'https://openalex.org/W3112689365', 'https://openalex.org/W2962784628', 'https://openalex.org/W2121879602', 'https://openalex.org/W2963979492', 'https://openalex.org/W4311887664', 'https://openalex.org/W6739901393', 'https://openalex.org/W3034999214', 'https://openalex.org/W6685158001', 'https://openalex.org/W1978106292', 'https://openalex.org/W2581624817', 'https://openalex.org/W4393138529', 'https://openalex.org/W4402671897', 'https://openalex.org/W4386437475', 'https://openalex.org/W6682631176', 'https://openalex.org/W2101105183', 'https://openalex.org/W6791376898', 'https://openalex.org/W2799054028', 'https://openalex.org/W2943552823', 'https://openalex.org/W2110485445', 'https://openalex.org/W6803845853', 'https://openalex.org/W6777615688', 'https://openalex.org/W6838865847', 'https://openalex.org/W1970381522', 'https://openalex.org/W4405096461', 'https://openalex.org/W3177828909', 'https://openalex.org/W4405827891', 'https://openalex.org/W3046764764', 'https://openalex.org/W4402626862', 'https://openalex.org/W3216221236', 'https://openalex.org/W6770616967', 'https://openalex.org/W4285601618', 'https://openalex.org/W4401656847', 'https://openalex.org/W4381800089', 'https://openalex.org/W4386002638', 'https://openalex.org/W4322502529', 'https://openalex.org/W4401008913', 'https://openalex.org/W4403929549', 'https://openalex.org/W4390175035', 'https://openalex.org/W6839507019', 'https://openalex.org/W4394866292', 'https://openalex.org/W6749533040', 'https://openalex.org/W2342065169', 'https://openalex.org/W6790737376', 'https://openalex.org/W4402297782', 'https://openalex.org/W2994662879', 'https://openalex.org/W6775792889', 'https://openalex.org/W4390031375', 'https://openalex.org/W4391018724', 'https://openalex.org/W4310184709', 'https://openalex.org/W4205165346', 'https://openalex.org/W4391531549', 'https://openalex.org/W3193491759', 'https://openalex.org/W7062369860', 'https://openalex.org/W4381712983', 'https://openalex.org/W4392821265', 'https://openalex.org/W3164820310', 'https://openalex.org/W4226135234', 'https://openalex.org/W4320890312', 'https://openalex.org/W6800686989', 'https://openalex.org/W4386134587', 'https://openalex.org/W4387429503', 'https://openalex.org/W2983552694', 'https://openalex.org/W4386950326', 'https://openalex.org/W4210590066', 'https://openalex.org/W4389332935', 'https://openalex.org/W4317609244', 'https://openalex.org/W4377233488', 'https://openalex.org/W4243138043', 'https://openalex.org/W4391932758', 'https://openalex.org/W2909550368', 'https://openalex.org/W2532936747', 'https://openalex.org/W2586383982', 'https://openalex.org/W2317801119', 'https://openalex.org/W4403391534', 'https://openalex.org/W6766191923', 'https://openalex.org/W6806462576', 'https://openalex.org/W3123459983', 'https://openalex.org/W4289516210', 'https://openalex.org/W4388954779', 'https://openalex.org/W3079760979', 'https://openalex.org/W6803272178', 'https://openalex.org/W2742878349', 'https://openalex.org/W2604086375', 'https://openalex.org/W2015386604', 'https://openalex.org/W2757734040', 'https://openalex.org/W2610544234', 'https://openalex.org/W2578363764', 'https://openalex.org/W2963523428', 'https://openalex.org/W2581094540', 'https://openalex.org/W2620742659', 'https://openalex.org/W2292390047', 'https://openalex.org/W4402915908', 'https://openalex.org/W4402933835', 'https://openalex.org/W6775726355', 'https://openalex.org/W2969742726', 'https://openalex.org/W4401781615', 'https://openalex.org/W2144499799', 'https://openalex.org/W2967983510', 'https://openalex.org/W3119189390', 'https://openalex.org/W3106433869', 'https://openalex.org/W4213428251', 'https://openalex.org/W4402124901', 'https://openalex.org/W4386252066', 'https://openalex.org/W4289947944', 'https://openalex.org/W3007918442', 'https://openalex.org/W3096000579', 'https://openalex.org/W4366549991', 'https://openalex.org/W4206553819', 'https://openalex.org/W4391778053', 'https://openalex.org/W2894218174', 'https://openalex.org/W4312912593', 'https://openalex.org/W4402445794', 'https://openalex.org/W4393002303', 'https://openalex.org/W6839964927', 'https://openalex.org/W4401684051', 'https://openalex.org/W4226278401', 'https://openalex.org/W2163605009', 'https://openalex.org/W4285506259', 'https://openalex.org/W4225323055', 'https://openalex.org/W4285322827', 'https://openalex.org/W4206405745', 'https://openalex.org/W2990272218', 'https://openalex.org/W4255701931', 'https://openalex.org/W3027879771', 'https://openalex.org/W3155739706', 'https://openalex.org/W3127736148', 'https://openalex.org/W3134642945', 'https://openalex.org/W4308245305', 'https://openalex.org/W4239338967', 'https://openalex.org/W4281557260', 'https://openalex.org/W2952729433', 'https://openalex.org/W4244977072', 'https://openalex.org/W4287113019', 'https://openalex.org/W4366330503', 'https://openalex.org/W4288089799', 'https://openalex.org/W4292779060', 'https://openalex.org/W4239131715', 'https://openalex.org/W3016084700', 'https://openalex.org/W4224308101', 'https://openalex.org/W4323572061']",2025-04-14
https://openalex.org/W4392839128,https://doi.org/10.25082/rima.2023.01.002,Artificial Intelligence in the 21st Century,"Artificial intelligence (AI) is the most important and interesting technology in the 21st Century due to its vast application. This review focuses on the evolution of AI techniques and their applications in recent decades. Deep learning algorithms/models, represented by Large Language Models (LLMs) have resulted in groundbreaking advancements, indicating that AI is evolving to improve its capacity to interact with and help people in various fields such as finance, medicine, and science research. The potential for research in AI is immense, and there is a need for scientific principles behind AI. Future perspectives on how machines can be developed to work with humans and to be compatible with human values and preferences are also discussed.","['https://openalex.org/W1901616594', 'https://openalex.org/W2901340108', 'https://openalex.org/W4288754168', 'https://openalex.org/W2991541823', 'https://openalex.org/W3146944767', 'https://openalex.org/W1589747210', 'https://openalex.org/W2139565295', 'https://openalex.org/W3168997536', 'https://openalex.org/W3195820203', 'https://openalex.org/W3200614847', 'https://openalex.org/W2953641512', 'https://openalex.org/W3185766304', 'https://openalex.org/W2116148009', 'https://openalex.org/W4212875990', 'https://openalex.org/W4250720725', 'https://openalex.org/W2919115771', 'https://openalex.org/W2910166370', 'https://openalex.org/W3195577433', 'https://openalex.org/W3215626407', 'https://openalex.org/W4312910992', 'https://openalex.org/W4360836968', 'https://openalex.org/W4361866031', 'https://openalex.org/W4365601026', 'https://openalex.org/W4304194220', 'https://openalex.org/W4379539370', 'https://openalex.org/W4389524458', 'https://openalex.org/W4385570090', 'https://openalex.org/W4389524500', 'https://openalex.org/W4323717348', 'https://openalex.org/W4321106177', 'https://openalex.org/W4384648121', 'https://openalex.org/W4323697696', 'https://openalex.org/W4385490607', 'https://openalex.org/W3161200675', 'https://openalex.org/W4214519789', 'https://openalex.org/W3177828909', 'https://openalex.org/W4378212018', 'https://openalex.org/W3211495999', 'https://openalex.org/W3151042244', 'https://openalex.org/W4387443463', 'https://openalex.org/W4362655746', 'https://openalex.org/W4388202310', 'https://openalex.org/W4390045971', 'https://openalex.org/W4386847565', 'https://openalex.org/W4367852612', 'https://openalex.org/W2766447205', 'https://openalex.org/W2902907165', 'https://openalex.org/W4322758918', 'https://openalex.org/W4307563196', 'https://openalex.org/W4390090453', 'https://openalex.org/W4361228074', 'https://openalex.org/W3092151159', 'https://openalex.org/W3197901813', 'https://openalex.org/W2981731882', 'https://openalex.org/W2995523160', 'https://openalex.org/W3036453007', 'https://openalex.org/W2999765337', 'https://openalex.org/W3189411510', 'https://openalex.org/W4384891029', 'https://openalex.org/W4319653583', 'https://openalex.org/W4380366109', 'https://openalex.org/W3104847483']",2023-03-25
https://openalex.org/W4392931281,https://doi.org/10.1109/icassp48485.2024.10446933,Paralinguistics-Enhanced Large Language Modeling of Spoken Dialogue,"Large Language Models (LLMs) have demonstrated superior abilities in tasks such as chatting, reasoning, and question-answering. However, standard LLMs may ignore crucial paralinguistic information, such as sentiment, emotion, and speaking style, which are essential for achieving natural, human-like spoken conversation, especially when such information is conveyed by acoustic cues. We therefore propose Paralinguistics-enhanced Generative Pretrained Transformer (ParalinGPT), an LLM that utilizes text and speech modalities to better model the linguistic content and paralinguistic attributes of spoken dialogue. The model takes the conversational context of text, speech embeddings, and paralinguistic attributes as input prompts within a serialized multitasking multimodal framework. Specifically, our framework serializes tasks in the order of current paralinguistic attribute prediction, response paralinguistic attribute prediction, and response text generation with autoregressive conditioning. We utilize the Switchboard-1 corpus, including its sentiment labels as the paralinguistic attribute, as our spoken dialogue dataset. Experimental results indicate the proposed serialized multitasking method outperforms typical sequence classification techniques on current and response sentiment classification. Furthermore, leveraging conversational context and speech embeddings significantly improves both response text generation and sentiment prediction. Our proposed framework achieves relative improvements of 6.7%, 12.0%, and 3.5% in current sentiment accuracy, response sentiment accuracy, and response text BLEU score, respectively.","['https://openalex.org/W6851678396', 'https://openalex.org/W6763494226', 'https://openalex.org/W2766833041', 'https://openalex.org/W4293714393', 'https://openalex.org/W2952088495', 'https://openalex.org/W3048664667', 'https://openalex.org/W3153025627', 'https://openalex.org/W3174716116', 'https://openalex.org/W2128970689', 'https://openalex.org/W2151083697', 'https://openalex.org/W2897636448', 'https://openalex.org/W6804073865', 'https://openalex.org/W4285273040', 'https://openalex.org/W2936162287', 'https://openalex.org/W6791303579', 'https://openalex.org/W2548264631', 'https://openalex.org/W2586286573', 'https://openalex.org/W4319862652', 'https://openalex.org/W2968228919', 'https://openalex.org/W3095607145', 'https://openalex.org/W4385823311', 'https://openalex.org/W6790356757', 'https://openalex.org/W3198217962', 'https://openalex.org/W4307680525', 'https://openalex.org/W4381786045', 'https://openalex.org/W6853998256', 'https://openalex.org/W4389524500', 'https://openalex.org/W3015489952', 'https://openalex.org/W3197236059', 'https://openalex.org/W6780218876', 'https://openalex.org/W3198771897', 'https://openalex.org/W4319862479', 'https://openalex.org/W2166637769', 'https://openalex.org/W6778490555', 'https://openalex.org/W4381827575', 'https://openalex.org/W3036601975', 'https://openalex.org/W4394671563', 'https://openalex.org/W3133903425', 'https://openalex.org/W4366559955', 'https://openalex.org/W3193377986', 'https://openalex.org/W4226422436']",2024-03-18
https://openalex.org/W4396781220,https://doi.org/10.1016/j.geoen.2024.212889,Geosystems risk and uncertainty: The application of ChatGPT with targeted prompting,"ChatGPT, a prominent large language model (LLM), is being increasingly used across a wide range of scientific fields. Geosystem engineers and researchers are also posed to leverage ChatGPT to find solutions to challenges encountered in various topics. This study evaluates the accuracy and reproducibility of ChatGPT in responding to different qualitative and quantitative questions, with a particular focus on risk and uncertainty (R&U) in both the Greenfield and Brownfield domains as an important area of interest. The results show the importance of prompting to considerably improve the ChatGPT's response accuracy and reproducibility. For example, prompting increases the accuracy of responses to qualitative and quantitative questions in the Greenfield domain by 10.4% and 41.8%, respectively. Additionally, prompting enhances the reproducibility of responses, with a 32.1% increase for qualitative questions and a 33.3% rise for quantitative questions in the Brownfield domain. The findings highlight that the greater the comprehensiveness of the prompts, the higher the accuracy and reproducibility of the responses to the questions. The study also acknowledges the potential limitations associated with the sources of information and the contextual influences on the reliability of the response.","['https://openalex.org/W4362581834', 'https://openalex.org/W4366991169', 'https://openalex.org/W4389437528', 'https://openalex.org/W4322719791', 'https://openalex.org/W3043899419', 'https://openalex.org/W4362508044', 'https://openalex.org/W4362608470', 'https://openalex.org/W4391326723', 'https://openalex.org/W3196974329', 'https://openalex.org/W6859856437', 'https://openalex.org/W4390176157', 'https://openalex.org/W4387131402', 'https://openalex.org/W4383818129', 'https://openalex.org/W4376866708', 'https://openalex.org/W4372319207', 'https://openalex.org/W3034385753', 'https://openalex.org/W2982049746', 'https://openalex.org/W4319662928', 'https://openalex.org/W4324130227', 'https://openalex.org/W4361289889', 'https://openalex.org/W4385988359', 'https://openalex.org/W6798272402', 'https://openalex.org/W4386161539', 'https://openalex.org/W4384135372', 'https://openalex.org/W2781661799', 'https://openalex.org/W4288680556', 'https://openalex.org/W3200970655', 'https://openalex.org/W6852072056', 'https://openalex.org/W4381982883', 'https://openalex.org/W6849870824', 'https://openalex.org/W6855116115', 'https://openalex.org/W4388666397', 'https://openalex.org/W6790898272', 'https://openalex.org/W6793879856', 'https://openalex.org/W4384561707', 'https://openalex.org/W4387106968', 'https://openalex.org/W4200311620', 'https://openalex.org/W6850653426', 'https://openalex.org/W4385235884', 'https://openalex.org/W4323717348', 'https://openalex.org/W3179485843', 'https://openalex.org/W4283319085', 'https://openalex.org/W4379473918', 'https://openalex.org/W4389524500', 'https://openalex.org/W4328049434', 'https://openalex.org/W4321524280', 'https://openalex.org/W4321499561', 'https://openalex.org/W4367189652', 'https://openalex.org/W2971089469', 'https://openalex.org/W4376167553', 'https://openalex.org/W4390671307', 'https://openalex.org/W4365211688', 'https://openalex.org/W4388490623', 'https://openalex.org/W3132033745', 'https://openalex.org/W4402665833', 'https://openalex.org/W2889098040', 'https://openalex.org/W4384389802', 'https://openalex.org/W4360891289']",2024-05-09
https://openalex.org/W4392902656,https://doi.org/10.1109/icassp48485.2024.10447296,Loss Masking Is Not Needed In Decoder-Only Transformer For Discrete-Token-Based ASR,"Recently, unified speech-text models, such as SpeechGPT, VioLA, and AudioPaLM, have achieved remarkable performance on various speech tasks. These models discretize speech signals into tokens (speech discretization) and use a shared vocabulary for both text and speech tokens. Then they train a single decoder-only Transformer on a mixture of speech tasks. However, these models rely on the Loss Masking strategy for the ASR task, which ignores the dependency among speech tokens. In this paper, we propose to model speech tokens in an autoregressive way, similar to text. We find that applying the conventional cross-entropy loss on input speech tokens does not consistently improve the ASR performance over the Loss Masking approach. To address this issue, we propose a novel approach denoted Smoothed Label Distillation (SLD), which applies a KL divergence loss with smoothed labels on speech tokens. Our experiments show that SLD effectively models speech tokens and outperforms Loss Masking for decoder-only Transformers in ASR tasks with different speech discretization methods <sup xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">1</sup> .","['https://openalex.org/W6852800892', 'https://openalex.org/W6850625674', 'https://openalex.org/W4389524500', 'https://openalex.org/W6853611000', 'https://openalex.org/W6853998256', 'https://openalex.org/W6857968694', 'https://openalex.org/W6739901393', 'https://openalex.org/W6769196770', 'https://openalex.org/W6770514103', 'https://openalex.org/W4385822683', 'https://openalex.org/W4319862255', 'https://openalex.org/W3209984917', 'https://openalex.org/W3209059054', 'https://openalex.org/W4307323391', 'https://openalex.org/W6850218400', 'https://openalex.org/W2963979492', 'https://openalex.org/W2962784628', 'https://openalex.org/W2113158412', 'https://openalex.org/W6638523607', 'https://openalex.org/W2183341477', 'https://openalex.org/W1494198834', 'https://openalex.org/W2407080277', 'https://openalex.org/W4323066695', 'https://openalex.org/W4381827575', 'https://openalex.org/W2747329762', 'https://openalex.org/W4387595589', 'https://openalex.org/W4385245566', 'https://openalex.org/W4378501656', 'https://openalex.org/W1821462560', 'https://openalex.org/W4322718191', 'https://openalex.org/W2988736778']",2024-03-18
https://openalex.org/W4394844601,https://doi.org/10.1615/jmachlearnmodelcomput.2024052432,GPT VS. HUMAN FOR SCIENTIFIC REVIEWS: A DUAL SOURCE REVIEW ON APPLICATIONS OF CHATGPT IN SCIENCE,"The new polymath large language models (LLMs) can greatly speed up scientific reviews, possibly using more unbiased quantitative metrics, facilitating cross-disciplinary connections, and identifying emerging trends and research gaps by analyzing large volumes of data. However, at the present time, they lack the required deep understanding of complex methodologies, they have difficulty in evaluating innovative claims, and they are unable to assess ethical issues and conflicts of interest. Herein, we consider 13 geotechnical parrot tales (GPT)-related papers across different scientific domains, reviewed by a human reviewer and SciSpace, a large language model, with the reviews evaluated by three distinct types of evaluators, namely GPT-3.5, a crowd panel, and GPT-4. We found that 50&amp;#37; of SciSpace's responses to objective questions align with those of a human reviewer, with GPT-4 (informed evaluator) often rating the human reviewer higher in accuracy, and SciSpace higher in structure, clarity, and completeness. In subjective questions, the uninformed evaluators (GPT-3.5 and crowd panel) showed varying preferences between SciSpace and human responses, with the crowd panel showing a preference for the human responses. However, GPT-4 rated them equally in accuracy and structure but favored SciSpace for completeness.","['https://openalex.org/W4362581834', 'https://openalex.org/W4389437528', 'https://openalex.org/W4388551749', 'https://openalex.org/W4322719791', 'https://openalex.org/W4324304837', 'https://openalex.org/W4383312437', 'https://openalex.org/W4384464487', 'https://openalex.org/W6600373357', 'https://openalex.org/W4366783381', 'https://openalex.org/W6609581451', 'https://openalex.org/W4383499296', 'https://openalex.org/W4362608470', 'https://openalex.org/W4387954002', 'https://openalex.org/W4383818129', 'https://openalex.org/W6607775392', 'https://openalex.org/W4385571325', 'https://openalex.org/W4360938283', 'https://openalex.org/W4372319207', 'https://openalex.org/W4366563389', 'https://openalex.org/W4378966181', 'https://openalex.org/W6601446089', 'https://openalex.org/W4327519588', 'https://openalex.org/W4378647938', 'https://openalex.org/W4388550745', 'https://openalex.org/W4387245383', 'https://openalex.org/W4324130227', 'https://openalex.org/W4361289889', 'https://openalex.org/W4366835631', 'https://openalex.org/W4377232911', 'https://openalex.org/W4327550249', 'https://openalex.org/W4385988359', 'https://openalex.org/W3179485843', 'https://openalex.org/W6606698219', 'https://openalex.org/W4384135372', 'https://openalex.org/W4386827094', 'https://openalex.org/W4388490623', 'https://openalex.org/W4381982883', 'https://openalex.org/W4385175044', 'https://openalex.org/W4321499561', 'https://openalex.org/W6600274115', 'https://openalex.org/W4365814901', 'https://openalex.org/W4385235884', 'https://openalex.org/W4381149642', 'https://openalex.org/W4384561707', 'https://openalex.org/W4366368023', 'https://openalex.org/W6600339963', 'https://openalex.org/W6600696048', 'https://openalex.org/W4389524500', 'https://openalex.org/W4399730268', 'https://openalex.org/W4391494845', 'https://openalex.org/W3211728297']",2024-01-01
https://openalex.org/W4401764365,https://doi.org/10.1145/3688005,MOSS-MED: A Family of Multimodal Models Serving Medical Image Analysis,"The remarkable advancements in large language models (LLMs) and large-scale visual encoders have laid a solid foundation for enhancing the capabilities of artificial intelligence (AI) in various application scenarios. In this study, we introduce MOSS-MED, a suite of models designed for biomedical vision-language understandings. MOSS-MED includes two models currently: MOSS-MED-2.5B and MOSS-MED-LLaMA, with different scale of their underlying LLMs. We employ a two-stage training pipeline that involves visual-text alignment and visual-involved instruction fine-tuning from both general domains and the medical domain. We evaluate the performance of MOSS-MED family on medical visual question answering (VQA) benchmarks. The results and cases demonstrate the impressive proficiency in medical expertise that MOSS-MED embodies.","['https://openalex.org/W4328120750', 'https://openalex.org/W4390873519', 'https://openalex.org/W3176641147', 'https://openalex.org/W4386566421', 'https://openalex.org/W4366327625', 'https://openalex.org/W4313156423', 'https://openalex.org/W4393178509', 'https://openalex.org/W4365606386', 'https://openalex.org/W4386352883', 'https://openalex.org/W4381930847', 'https://openalex.org/W3132611748', 'https://openalex.org/W4379660316', 'https://openalex.org/W4386722137', 'https://openalex.org/W4367000547', 'https://openalex.org/W4400904844', 'https://openalex.org/W4388725043', 'https://openalex.org/W3025935268', 'https://openalex.org/W4397028377', 'https://openalex.org/W4402727946', 'https://openalex.org/W2886641317', 'https://openalex.org/W6867456739', 'https://openalex.org/W4380715596', 'https://openalex.org/W4384918448', 'https://openalex.org/W4387211791', 'https://openalex.org/W4388685765', 'https://openalex.org/W4387775965', 'https://openalex.org/W4394782456', 'https://openalex.org/W4385645306', 'https://openalex.org/W4386794445', 'https://openalex.org/W4388093101', 'https://openalex.org/W4389524500', 'https://openalex.org/W4378718543', 'https://openalex.org/W4380353817', 'https://openalex.org/W4323066559', 'https://openalex.org/W2747329762', 'https://openalex.org/W4234592442', 'https://openalex.org/W2560674852', 'https://openalex.org/W398859631', 'https://openalex.org/W4387580644', 'https://openalex.org/W4386566590', 'https://openalex.org/W4301045096']",2024-08-22
https://openalex.org/W4392248350,https://doi.org/10.1109/icce59016.2024.10444441,uTalk: Bridging the Gap Between Humans and AI,"Large Language Models (LLMs) have revolutionized various industries by harnessing their power to improve productivity and facilitate learning across different fields. One intriguing application involves combining LLMs with visual models to create a novel approach to Human-Computer Interaction. The core idea of this system is to create a user-friendly platform that enables people to utilize ChatGPT's features in their everyday lives. uTalk is comprised of technologies like Whisper, ChatGPT, Microsoft Speech Services, and the state-of-the-art (SOTA) talking head system SadTalker. Users can engage in human-like conversation with a digital twin and receive answers to any questions. Also, uTalk could generate content by submitting an image and input (text or audio). This system is hosted on Streamlit, where users will be prompted to provide an image to serve as their AI assistant. Then, as the input (text or audio) is provided, a set of operations will produce a video of the avatar with the precise response. This paper outlines how SadTalker's run-time has been optimized by 27.69% based on 25 frames per second (FPS) generated videos and 38.38% compared to our 20FPS generated videos. Furthermore, the integration and parallelization of SadTalker and Streamlit have resulted in a 9.8% improvement compared to the initial performance of the system.","['https://openalex.org/W6850625674', 'https://openalex.org/W4389524500', 'https://openalex.org/W6851847159', 'https://openalex.org/W6851086418', 'https://openalex.org/W2886253469', 'https://openalex.org/W6847363464', 'https://openalex.org/W4386072021', 'https://openalex.org/W4322718191', 'https://openalex.org/W4324299499', 'https://openalex.org/W4393178509', 'https://openalex.org/W4311000453']",2024-01-06
https://openalex.org/W4392908891,https://doi.org/10.1109/icassp48485.2024.10446132,Leveraging Large Language Models for Exploiting ASR Uncertainty,"While large language models excel in a variety of natural language processing (NLP) tasks, to perform well on spoken language understanding (SLU) tasks, they must either rely on off-the-shelf automatic speech recognition (ASR) systems for transcription, or be equipped with an in-built speech modality. This work focuses on the former scenario, where LLM's accuracy on SLU tasks is constrained by the accuracy of a fixed ASR system on the spoken input. Specifically, we tackle speech-intent classification task, where a high word-error-rate can limit the LLM's ability to understand the spoken intent. Instead of chasing a high accuracy by designing complex or specialized architectures regardless of deployment costs, we seek to answer how far we can go without substantially changing the underlying ASR and LLM, which can potentially be shared by multiple unrelated tasks. To this end, we propose prompting the LLM with an n-best list of ASR hypotheses instead of only the error-prone 1-best hypothesis. We explore prompt-engineering to explain the concept of n-best lists to the LLM; followed by the finetuning of Low-Rank Adapters [1] on the downstream tasks. Our approach using n-best lists proves to be effective on a device-directed speech detection task as well as on a keyword spotting task, where systems using n-best list prompts outperform those using 1-best ASR hypothesis; thus paving the way for an efficient method to exploit ASR uncertainty via LLMs for speech-based applications.","['https://openalex.org/W6796581206', 'https://openalex.org/W6778883912', 'https://openalex.org/W6810738896', 'https://openalex.org/W4392669753', 'https://openalex.org/W4389523957', 'https://openalex.org/W4385567149', 'https://openalex.org/W6809646742', 'https://openalex.org/W6853249747', 'https://openalex.org/W4392903956', 'https://openalex.org/W4389524500', 'https://openalex.org/W6852818750', 'https://openalex.org/W4393178509', 'https://openalex.org/W6851513886', 'https://openalex.org/W4385823351', 'https://openalex.org/W6854445763', 'https://openalex.org/W4205385624', 'https://openalex.org/W6850625674', 'https://openalex.org/W6750665317', 'https://openalex.org/W4297841617', 'https://openalex.org/W2511962886', 'https://openalex.org/W3196496149', 'https://openalex.org/W3198035615', 'https://openalex.org/W3119913666', 'https://openalex.org/W3163571828', 'https://openalex.org/W2046932483', 'https://openalex.org/W2132991150', 'https://openalex.org/W3097777922', 'https://openalex.org/W6852874933', 'https://openalex.org/W3081168214', 'https://openalex.org/W2936167188', 'https://openalex.org/W4286769130', 'https://openalex.org/W4361866031', 'https://openalex.org/W4380353763', 'https://openalex.org/W4322718191', 'https://openalex.org/W4221143046', 'https://openalex.org/W2981022124', 'https://openalex.org/W2797583228', 'https://openalex.org/W4377130946', 'https://openalex.org/W3177813494', 'https://openalex.org/W4377372369', 'https://openalex.org/W3170405627', 'https://openalex.org/W4226278401', 'https://openalex.org/W4383989131', 'https://openalex.org/W4212774754']",2024-03-18
https://openalex.org/W4399011256,https://doi.org/10.31223/x5tq37,A Comprehensive Evaluation of Multimodal Large Language Models in Hydrological Applications,"Large Language Models (LLMs) combined with visual foundation models have demonstrated remarkable advancements, achieving a level of intelligence comparable to human capabilities. In this study, we conduct an analysis of the latest Multimodal LLMs (MLLMs), specifically Multimodal-GPT, GPT-4 Vision, Gemini and LLaVa, focusing on their application in the hydrology domain. The hydrology domain holds significant relevance for AI intelligence applications, including flood management and response, water level monitoring, agricultural water discharge, and water pollution management. Our analysis involves testing these MLLMs on various hydrology-specific studies, evaluating their response generation, and assessing their suitability for real-time systems. We deliberately selected complex real-world scenarios to explore the potential of MLLMs in addressing hydrological challenges. Additionally, we carefully designed prompts to enhance the models' visual inference capabilities and their ability to comprehend context from image data. The findings from our analysis reveal effective human-computer interaction and inspire potential solutions for real-world hydrological inference systems that incorporate both textual and image data. Among the validated models, GPT-4 Vision stands out as the top performer among other MLLMs, showcasing unparalleled proficiency in inferring visual data. The results highlight the significant understanding, reasoning, and decision-making capabilities that multimodal foundation models bring to the domain of hydrology. This research contributes valuable insights into the potential applications of advanced AI models in addressing complex challenges within hydrological contexts.","['https://openalex.org/W2998356391', 'https://openalex.org/W2964067226', 'https://openalex.org/W2970869018', 'https://openalex.org/W4396541396', 'https://openalex.org/W2969876226', 'https://openalex.org/W2896457183', 'https://openalex.org/W4309591663', 'https://openalex.org/W4389279079', 'https://openalex.org/W2975501350', 'https://openalex.org/W2970231061', 'https://openalex.org/W4389520252', 'https://openalex.org/W4378711593', 'https://openalex.org/W2745461083', 'https://openalex.org/W4389524500', 'https://openalex.org/W4389519587', 'https://openalex.org/W4366850747', 'https://openalex.org/W2968124245', 'https://openalex.org/W4226182655', 'https://openalex.org/W4281557623', 'https://openalex.org/W2966715458', 'https://openalex.org/W1889081078', 'https://openalex.org/W4327810158', 'https://openalex.org/W4319165821', 'https://openalex.org/W2073302931', 'https://openalex.org/W4366330503', 'https://openalex.org/W4307079201', 'https://openalex.org/W4322718191', 'https://openalex.org/W4226369848', 'https://openalex.org/W2113496055', 'https://openalex.org/W4239072543', 'https://openalex.org/W4387437512', 'https://openalex.org/W4323717348', 'https://openalex.org/W4324321291', 'https://openalex.org/W4309805219', 'https://openalex.org/W4376122449', 'https://openalex.org/W1933349210', 'https://openalex.org/W4311963991', 'https://openalex.org/W2811064840', 'https://openalex.org/W4405371303', 'https://openalex.org/W4225323055', 'https://openalex.org/W3092962781', 'https://openalex.org/W2295107390', 'https://openalex.org/W4225791671', 'https://openalex.org/W4386907041', 'https://openalex.org/W2966683369', 'https://openalex.org/W4385175044', 'https://openalex.org/W2966752525', 'https://openalex.org/W4322718246', 'https://openalex.org/W3001555892', 'https://openalex.org/W4382457527', 'https://openalex.org/W4307704580', 'https://openalex.org/W4304194220', 'https://openalex.org/W4388490623', 'https://openalex.org/W3205607555', 'https://openalex.org/W4281557260', 'https://openalex.org/W4286970294', 'https://openalex.org/W4320165837', 'https://openalex.org/W4385550033', 'https://openalex.org/W4372273323', 'https://openalex.org/W4361866031', 'https://openalex.org/W2997591391', 'https://openalex.org/W4386075985', 'https://openalex.org/W4396542473', 'https://openalex.org/W4377372281', 'https://openalex.org/W3181795509', 'https://openalex.org/W4221143046', 'https://openalex.org/W4380994269', 'https://openalex.org/W4323572061', 'https://openalex.org/W2963758027', 'https://openalex.org/W2937845937', 'https://openalex.org/W4387592425']",2024-05-25
https://openalex.org/W4403918744,https://doi.org/10.1109/jstsp.2024.3488557,Universal Speech Token Learning via Low-Bitrate Neural Codec and Pretrained Representations,"Current large speech language models are mainly based on semantic tokens from discretization of self-supervised learned representations and acoustic tokens from a neural codec, following a semantic-modeling and acoustic-synthesis paradigm. However, semantic tokens discard paralinguistic attributes of speakers that is important for natural spoken communication, while prompt-based acoustic synthesis from semantic tokens has limits in recovering paralinguistic details and suffers from robustness issues, especially when there are domain gaps between the prompt and the target. This paper unifies two types of tokens and proposes the UniCodec, a universal speech token learning that encapsulates all semantics of speech, including linguistic and paralinguistic information, into a compact and semantically-disentangled unified token. Such a unified token can not only benefit speech language models in understanding with paralinguistic hints but also help speech generation with high-quality output. A low-bitrate neural codec is leveraged to learn such disentangled discrete representations at global and local scales, with knowledge distilled from self-supervised learned features. Extensive evaluations on multilingual datasets demonstrate its effectiveness in generating natural, expressive and long-term consistent output quality with paralinguistic attributes well preserved in several speech processing tasks.","['https://openalex.org/W6853998256', 'https://openalex.org/W4389524500', 'https://openalex.org/W6790356757', 'https://openalex.org/W6780218876', 'https://openalex.org/W3209059054', 'https://openalex.org/W2151083697', 'https://openalex.org/W4392931281', 'https://openalex.org/W3198217962', 'https://openalex.org/W3215615641', 'https://openalex.org/W4307323391', 'https://openalex.org/W4381786045', 'https://openalex.org/W6848735303', 'https://openalex.org/W4390075359', 'https://openalex.org/W4386132131', 'https://openalex.org/W4392902611', 'https://openalex.org/W2752796333', 'https://openalex.org/W6855885476', 'https://openalex.org/W4372259964', 'https://openalex.org/W4287854499', 'https://openalex.org/W4377231659', 'https://openalex.org/W4385245566', 'https://openalex.org/W6864145738', 'https://openalex.org/W3180374548', 'https://openalex.org/W4385570550', 'https://openalex.org/W2995181338', 'https://openalex.org/W3163464523', 'https://openalex.org/W4289665794', 'https://openalex.org/W3095410713', 'https://openalex.org/W3081192838', 'https://openalex.org/W2935711438', 'https://openalex.org/W6853515095', 'https://openalex.org/W2130086727', 'https://openalex.org/W2107740512', 'https://openalex.org/W3196475561', 'https://openalex.org/W4225956675', 'https://openalex.org/W3037038648', 'https://openalex.org/W6805710207', 'https://openalex.org/W1494198834', 'https://openalex.org/W2030931454', 'https://openalex.org/W2972359262', 'https://openalex.org/W6810701745', 'https://openalex.org/W6783867762']",2024-10-30
https://openalex.org/W4389519824,https://doi.org/10.18653/v1/2023.findings-emnlp.327,Improving End-to-End Speech Processing by Efficient Text Data Utilization with Latent Synthesis,"Training a high performance end-to-end speech (E2E) processing model requires an enormous amount of labeled speech data, especially in the era of data-centric artificial intelligence. However, labeled speech data are usually scarcer and more expensive for collection, compared to textual data. We propose Latent Synthesis (LaSyn), an efficient textual data utilization framework for E2E speech processing models. We train a latent synthesizer to convert textual data into an intermediate latent representation of a pre-trained speech model. These pseudo acoustic representations of textual data augment acoustic data for model training. We evaluate LaSyn on low-resource automatic speech recognition (ASR) and spoken language understanding (SLU) tasks. For ASR, LaSyn improves an E2E baseline trained on LibriSpeech train-clean-100, with relative word error rate reductions over 22.3% on different test sets. For SLU, LaSyn improves our E2E baseline by absolute 4.1% for intent classification accuracy and 3.8% for slot filling SLU-F1 on SLURP, and absolute 4.49% and 2.25% for exact match (EM) and EM-Tree accuracies on STOP respectively. With fewer parameters, the results of LaSyn are competitive to published state-of-the-art works. The results demonstrate the quality of the augmented training data.","['https://openalex.org/W2997771882', 'https://openalex.org/W3090702983', 'https://openalex.org/W4225272718', 'https://openalex.org/W2915816387', 'https://openalex.org/W3096251052', 'https://openalex.org/W3161302809', 'https://openalex.org/W4226120743', 'https://openalex.org/W4319653946', 'https://openalex.org/W4221162932', 'https://openalex.org/W3173156538', 'https://openalex.org/W3211224152', 'https://openalex.org/W2804648901', 'https://openalex.org/W4300980246', 'https://openalex.org/W3016010032', 'https://openalex.org/W4288089799', 'https://openalex.org/W2976556660', 'https://openalex.org/W3173110665', 'https://openalex.org/W3207222250', 'https://openalex.org/W2729190387', 'https://openalex.org/W3034999214', 'https://openalex.org/W4225308107', 'https://openalex.org/W3015412890', 'https://openalex.org/W3175976195', 'https://openalex.org/W3107298252', 'https://openalex.org/W3045703328', 'https://openalex.org/W4385573012', 'https://openalex.org/W4319862713', 'https://openalex.org/W4214968481', 'https://openalex.org/W1901129140', 'https://openalex.org/W3036167779', 'https://openalex.org/W3100460087', 'https://openalex.org/W3217767527', 'https://openalex.org/W3026041220', 'https://openalex.org/W4389524500', 'https://openalex.org/W3102854726', 'https://openalex.org/W2896457183', 'https://openalex.org/W3096686110', 'https://openalex.org/W4312933868', 'https://openalex.org/W2952087486', 'https://openalex.org/W4281485151', 'https://openalex.org/W3138431698', 'https://openalex.org/W3095173472', 'https://openalex.org/W4385245566', 'https://openalex.org/W1828163288', 'https://openalex.org/W4288099666', 'https://openalex.org/W1494198834', 'https://openalex.org/W2786860129', 'https://openalex.org/W2986193249', 'https://openalex.org/W4226125322', 'https://openalex.org/W2963718112', 'https://openalex.org/W3082619646', 'https://openalex.org/W3004786215', 'https://openalex.org/W2097550833', 'https://openalex.org/W3163044982', 'https://openalex.org/W3103005696', 'https://openalex.org/W2760505947', 'https://openalex.org/W4389009452', 'https://openalex.org/W4281969232', 'https://openalex.org/W3045492832', 'https://openalex.org/W3162313915', 'https://openalex.org/W3148338017', 'https://openalex.org/W3172148458', 'https://openalex.org/W4297841852', 'https://openalex.org/W4308349017', 'https://openalex.org/W4213178287', 'https://openalex.org/W3015995734', 'https://openalex.org/W3110257065', 'https://openalex.org/W3169320628', 'https://openalex.org/W3015885816', 'https://openalex.org/W4288088457', 'https://openalex.org/W3036601975']",2023-01-01
https://openalex.org/W4402830007,https://doi.org/10.2196/preprints.59505,"Multimodal Large Language Models in Health Care: Applications, Challenges, and Future Outlook (Preprint)","<sec> <title>UNSTRUCTURED</title> In the complex and multidimensional field of medicine, multimodal data are prevalent and crucial for informed clinical decisions. Multimodal data span a broad spectrum of data types, including medical images (eg, MRI and CT scans), time-series data (eg, sensor data from wearable devices and electronic health records), audio recordings (eg, heart and respiratory sounds and patient interviews), text (eg, clinical notes and research articles), videos (eg, surgical procedures), and omics data (eg, genomics and proteomics). While advancements in large language models (LLMs) have enabled new applications for knowledge retrieval and processing in the medical field, most LLMs remain limited to processing unimodal data, typically text-based content, and often overlook the importance of integrating the diverse data modalities encountered in clinical practice. This paper aims to present a detailed, practical, and solution-oriented perspective on the use of multimodal LLMs (M-LLMs) in the medical field. Our investigation spanned M-LLM foundational principles, current and potential applications, technical and ethical challenges, and future research directions. By connecting these elements, we aimed to provide a comprehensive framework that links diverse aspects of M-LLMs, offering a unified vision for their future in health care. This approach aims to guide both future research and practical implementations of M-LLMs in health care, positioning them as a paradigm shift toward integrated, multimodal data–driven medical practice. We anticipate that this work will spark further discussion and inspire the development of innovative approaches in the next generation of medical M-LLM systems. </sec>","['https://openalex.org/W4387865285', 'https://openalex.org/W4387500346', 'https://openalex.org/W4386575491', 'https://openalex.org/W4384071683', 'https://openalex.org/W4377011132', 'https://openalex.org/W3122765966', 'https://openalex.org/W4285987711', 'https://openalex.org/W4221022534', 'https://openalex.org/W2064675550', 'https://openalex.org/W4310645210', 'https://openalex.org/W3017637887', 'https://openalex.org/W3129155125', 'https://openalex.org/W3193158708', 'https://openalex.org/W4389686112', 'https://openalex.org/W4385647263', 'https://openalex.org/W4321459182', 'https://openalex.org/W4378672794', 'https://openalex.org/W4386052426', 'https://openalex.org/W4387028645', 'https://openalex.org/W4386892967', 'https://openalex.org/W4386735541', 'https://openalex.org/W4387653139', 'https://openalex.org/W4386923268', 'https://openalex.org/W4387583347', 'https://openalex.org/W4385346108', 'https://openalex.org/W4308885870', 'https://openalex.org/W4281643269', 'https://openalex.org/W4386168937', 'https://openalex.org/W2194775991', 'https://openalex.org/W4382403009', 'https://openalex.org/W2952030198', 'https://openalex.org/W4388430464', 'https://openalex.org/W4388831047', 'https://openalex.org/W2889664156', 'https://openalex.org/W2345512687', 'https://openalex.org/W3009260245', 'https://openalex.org/W2964010806', 'https://openalex.org/W4390580666', 'https://openalex.org/W4393149524', 'https://openalex.org/W4390115208', 'https://openalex.org/W4312220150', 'https://openalex.org/W4389664922', 'https://openalex.org/W4381786045', 'https://openalex.org/W4393178509', 'https://openalex.org/W4389524500', 'https://openalex.org/W2911489562', 'https://openalex.org/W3046375318', 'https://openalex.org/W4386066385', 'https://openalex.org/W4387966979', 'https://openalex.org/W3127238141', 'https://openalex.org/W4384133826', 'https://openalex.org/W4296613150', 'https://openalex.org/W4319065545', 'https://openalex.org/W4319335178', 'https://openalex.org/W4225917625', 'https://openalex.org/W3195980265', 'https://openalex.org/W4387346412', 'https://openalex.org/W3177500196', 'https://openalex.org/W4318071656', 'https://openalex.org/W4205773061', 'https://openalex.org/W4366163632', 'https://openalex.org/W4391316987', 'https://openalex.org/W4292438865', 'https://openalex.org/W4313439128', 'https://openalex.org/W4387789684', 'https://openalex.org/W4308393531', 'https://openalex.org/W4386893702', 'https://openalex.org/W4361289277', 'https://openalex.org/W4387068110', 'https://openalex.org/W4220867361', 'https://openalex.org/W4390480840', 'https://openalex.org/W4390603852', 'https://openalex.org/W4390490761', 'https://openalex.org/W3038035611', 'https://openalex.org/W3174057701', 'https://openalex.org/W4377226909', 'https://openalex.org/W4385573270', 'https://openalex.org/W4392849937', 'https://openalex.org/W2963400886', 'https://openalex.org/W4376643691', 'https://openalex.org/W4382394524', 'https://openalex.org/W3107627743', 'https://openalex.org/W2350778671', 'https://openalex.org/W2895917461', 'https://openalex.org/W2905657479', 'https://openalex.org/W2168610667']",2024-04-13
https://openalex.org/W4404865873,https://doi.org/10.32388/g5h95j,StreamAdapter: Efficient Test Time Adaptation from Contextual Streams,"In-context learning (ICL) allows large language models (LLMs) to adapt to new tasks directly from the given demonstrations without requiring gradient updates. While recent advances have expanded context windows to accommodate more demonstrations, this approach increases inference costs without necessarily improving performance. To mitigate these issues, We propose StreamAdapter, a novel approach that directly updates model parameters from context at test time, eliminating the need for explicit in-context demonstrations. StreamAdapter employs context mapping and weight absorption mechanisms to dynamically transform ICL demonstrations into parameter updates with minimal additional parameters. By reducing reliance on numerous in-context examples, StreamAdapter significantly reduce inference costs and allows for efficient inference with constant time complexity, regardless of demonstration count. Extensive experiments across diverse tasks and model architectures demonstrate that StreamAdapter achieves comparable or superior adaptation capability to ICL while requiring significantly fewer demonstrations. The superior task adaptation and context encoding capabilities of StreamAdapter on both language understanding and generation tasks provides a new perspective for adapting LLMs at test time using context, allowing for more efficient adaptation across scenarios and more cost-effective inference.","['https://openalex.org/W4221055872', 'https://openalex.org/W4378942327', 'https://openalex.org/W4389524500', 'https://openalex.org/W4391631327', 'https://openalex.org/W6778883912', 'https://openalex.org/W4221143046', 'https://openalex.org/W6864255096', 'https://openalex.org/W4391833078', 'https://openalex.org/W4392090070', 'https://openalex.org/W4395444087', 'https://openalex.org/W4382319938', 'https://openalex.org/W6853344003', 'https://openalex.org/W4385571886', 'https://openalex.org/W4311726128', 'https://openalex.org/W6864208733', 'https://openalex.org/W6843370592', 'https://openalex.org/W4389518997', 'https://openalex.org/W4399453612', 'https://openalex.org/W6863816985', 'https://openalex.org/W6803096969', 'https://openalex.org/W4396788162', 'https://openalex.org/W4377130677', 'https://openalex.org/W6631621112', 'https://openalex.org/W3166702123', 'https://openalex.org/W6846558788', 'https://openalex.org/W4391158319', 'https://openalex.org/W6796581206', 'https://openalex.org/W4400435102', 'https://openalex.org/W3118216348', 'https://openalex.org/W4330338093', 'https://openalex.org/W6861489518', 'https://openalex.org/W2064675550', 'https://openalex.org/W4389326242', 'https://openalex.org/W6866443539', 'https://openalex.org/W4389713725', 'https://openalex.org/W3036224891', 'https://openalex.org/W4395474395', 'https://openalex.org/W2953271402', 'https://openalex.org/W2145755360', 'https://openalex.org/W2251939518', 'https://openalex.org/W2986922898', 'https://openalex.org/W2794325560', 'https://openalex.org/W2998617917', 'https://openalex.org/W6893007900']",2024-11-29
https://openalex.org/W4405547956,https://doi.org/10.32388/758n37,Benchmarking Open-ended Audio Dialogue Understanding for Large Audio-Language Models,"Large Audio-Language Models (LALMs) have unclocked audio dialogue capabilities, where audio dialogues are a direct exchange of spoken language between LALMs and humans. Recent advances, such as GPT-4o, have enabled LALMs in back-and-forth audio dialogues with humans. This progression not only underscores the potential of LALMs but also broadens their applicability across a wide range of practical scenarios supported by audio dialogues. However, given these advancements, a comprehensive benchmark to evaluate the performance of LALMs in the open-ended audio dialogue understanding remains absent currently. To address this gap, we propose an Audio Dialogue Understanding Benchmark (ADU-Bench), which consists of 4 benchmark datasets. They assess the open-ended audio dialogue ability for LALMs in 3 general scenarios, 12 skills, 9 multilingual languages, and 4 categories of ambiguity handling. Notably, we _firstly propose the evaluation of ambiguity handling in audio dialogues_ that expresses different intentions beyond the same literal meaning of sentences, e.g., “Really!?” with different intonations. In summary, ADU-Bench includes over 20,000 open-ended audio dialogues for the assessment of LALMs. Through extensive experiments conducted on 13 LALMs, our analysis reveals that there is still considerable room for improvement in the audio dialogue understanding abilities of existing LALMs. In particular, they struggle with mathematical symbols and formulas, understanding human behavior such as roleplay, comprehending multiple languages, and handling audio dialogue ambiguities from different phonetic elements, such as intonations, pause positions, and homophones.","['https://openalex.org/W4389524500', 'https://openalex.org/W4386552581', 'https://openalex.org/W4388718054', 'https://openalex.org/W4387891768', 'https://openalex.org/W4391021666', 'https://openalex.org/W4391591726', 'https://openalex.org/W4402683976', 'https://openalex.org/W4403556012', 'https://openalex.org/W4402386435', 'https://openalex.org/W3196509775', 'https://openalex.org/W4226444650', 'https://openalex.org/W2030931454', 'https://openalex.org/W2803193013', 'https://openalex.org/W4402684170', 'https://openalex.org/W6852874933', 'https://openalex.org/W4402671584', 'https://openalex.org/W6852447913', 'https://openalex.org/W6856794988', 'https://openalex.org/W4392902623', 'https://openalex.org/W2912924812', 'https://openalex.org/W4385568240', 'https://openalex.org/W4385571440', 'https://openalex.org/W2995929068', 'https://openalex.org/W6803096969', 'https://openalex.org/W4367000491', 'https://openalex.org/W6800166007', 'https://openalex.org/W3083410900', 'https://openalex.org/W2889787757', 'https://openalex.org/W3159959439', 'https://openalex.org/W4384918448', 'https://openalex.org/W1985975449', 'https://openalex.org/W4387321091', 'https://openalex.org/W3034302278', 'https://openalex.org/W3217804443', 'https://openalex.org/W4400375155', 'https://openalex.org/W4397028323', 'https://openalex.org/W4386076050', 'https://openalex.org/W4388726687', 'https://openalex.org/W4391157527', 'https://openalex.org/W4395687052', 'https://openalex.org/W4403571375', 'https://openalex.org/W4399554777']",2024-12-18
https://openalex.org/W4413058266,https://doi.org/10.1190/geo2024-0916.1,Foundation models for exploration geophysics,"ABSTRACT Recently, large models, or foundation models, have exhibited remarkable performance, profoundly impacting research paradigms in diverse domains. Foundation models, trained on extensive and diverse data sets, provide exceptional generalization abilities, allowing for their straightforward application across various use cases and domains. Exploration geophysics entails processing vast, multimodal, and multitask data sets that have traditionally relied on expert experience and physical principles. These unique characteristics present substantial challenges and compelling opportunities for advancing geophysical foundation models (GeoFMs). However, the development of GeoFMs in exploration geophysics is still at an early stage. This paper provides an overview of the current state and future prospects of GeoFMs in exploration geophysics. We begin by reviewing the development and emergent capabilities of foundation models, emphasizing their growing relevance in this domain. Furthermore, we discuss the hierarchy of GeoFMs for exploration geophysics and the critical techniques used, providing a research workflow that serves as a reference for their development. We then examine how GeoFMs may support various exploration tasks, with first-arrival picking based on the segment anything model as an example. Finally, we summarize the challenges faced in developing GeoFMs, along with future trends and their potential impact on the field.","['https://openalex.org/W4327810158', 'https://openalex.org/W3199948257', 'https://openalex.org/W4323257008', 'https://openalex.org/W3206907172', 'https://openalex.org/W2776585113', 'https://openalex.org/W4308902180', 'https://openalex.org/W4402660160', 'https://openalex.org/W2923222994', 'https://openalex.org/W4383218913', 'https://openalex.org/W4365597205', 'https://openalex.org/W3096609285', 'https://openalex.org/W4327564965', 'https://openalex.org/W4402916309', 'https://openalex.org/W2964309882', 'https://openalex.org/W4407129041', 'https://openalex.org/W4392384758', 'https://openalex.org/W2896457183', 'https://openalex.org/W2888770834', 'https://openalex.org/W6911020276', 'https://openalex.org/W2079724595', 'https://openalex.org/W4312995631', 'https://openalex.org/W4376866255', 'https://openalex.org/W4225410153', 'https://openalex.org/W6891980377', 'https://openalex.org/W4403665980', 'https://openalex.org/W4403186394', 'https://openalex.org/W3035029089', 'https://openalex.org/W3199258042', 'https://openalex.org/W4386071707', 'https://openalex.org/W3096831136', 'https://openalex.org/W4389326242', 'https://openalex.org/W4391673318', 'https://openalex.org/W4402667899', 'https://openalex.org/W4408952473', 'https://openalex.org/W4399594884', 'https://openalex.org/W4402758632', 'https://openalex.org/W4395465275', 'https://openalex.org/W4310856925', 'https://openalex.org/W4399186008', 'https://openalex.org/W4407809412', 'https://openalex.org/W4313156423', 'https://openalex.org/W3035524453', 'https://openalex.org/W2170860899', 'https://openalex.org/W2970419158', 'https://openalex.org/W4389524317', 'https://openalex.org/W4386825235', 'https://openalex.org/W3001279689', 'https://openalex.org/W1905882502', 'https://openalex.org/W2963775778', 'https://openalex.org/W4390874575', 'https://openalex.org/W4402776460', 'https://openalex.org/W4399159743', 'https://openalex.org/W4366734819', 'https://openalex.org/W4205991051', 'https://openalex.org/W4388726199', 'https://openalex.org/W4404694762', 'https://openalex.org/W3204221554', 'https://openalex.org/W4392309198', 'https://openalex.org/W3174770825', 'https://openalex.org/W4388994228', 'https://openalex.org/W4394606408', 'https://openalex.org/W3035652667', 'https://openalex.org/W6929520013', 'https://openalex.org/W4404533930', 'https://openalex.org/W4394867153', 'https://openalex.org/W4390528775', 'https://openalex.org/W4394938913', 'https://openalex.org/W4401832906', 'https://openalex.org/W4388074445', 'https://openalex.org/W4404088947', 'https://openalex.org/W4399567248', 'https://openalex.org/W2890883963', 'https://openalex.org/W1977341879', 'https://openalex.org/W4400451184', 'https://openalex.org/W4365143687', 'https://openalex.org/W3047855151', 'https://openalex.org/W3116651605', 'https://openalex.org/W4206410067', 'https://openalex.org/W4384389802', 'https://openalex.org/W4375853269', 'https://openalex.org/W2141953966', 'https://openalex.org/W4388129777', 'https://openalex.org/W2955412401', 'https://openalex.org/W2968260241', 'https://openalex.org/W2942977592', 'https://openalex.org/W4389519620', 'https://openalex.org/W4386973901', 'https://openalex.org/W4224035735', 'https://openalex.org/W4312933868', 'https://openalex.org/W1901129140', 'https://openalex.org/W2798828763', 'https://openalex.org/W2625710583', 'https://openalex.org/W2141423041', 'https://openalex.org/W3212516020', 'https://openalex.org/W4405527282', 'https://openalex.org/W3214333957', 'https://openalex.org/W4390873217', 'https://openalex.org/W4402451536', 'https://openalex.org/W1981607562', 'https://openalex.org/W4378711593', 'https://openalex.org/W3045911340', 'https://openalex.org/W4387449471', 'https://openalex.org/W4402855151', 'https://openalex.org/W4405469347', 'https://openalex.org/W4226399820', 'https://openalex.org/W2964184826', 'https://openalex.org/W4296027312', 'https://openalex.org/W4384918448', 'https://openalex.org/W2145167443', 'https://openalex.org/W2964051877', 'https://openalex.org/W4391766594', 'https://openalex.org/W4389040963', 'https://openalex.org/W4390023570', 'https://openalex.org/W4400524387', 'https://openalex.org/W6891906141', 'https://openalex.org/W4385573131', 'https://openalex.org/W4283026156', 'https://openalex.org/W4284969399', 'https://openalex.org/W4361866125', 'https://openalex.org/W4385681582', 'https://openalex.org/W4376226279', 'https://openalex.org/W4388624446', 'https://openalex.org/W2912052494', 'https://openalex.org/W4362496355', 'https://openalex.org/W4406032915', 'https://openalex.org/W4402816858', 'https://openalex.org/W3147262955', 'https://openalex.org/W4287117014', 'https://openalex.org/W4391988217', 'https://openalex.org/W4322707256', 'https://openalex.org/W4389524500', 'https://openalex.org/W4389519587', 'https://openalex.org/W4404809811', 'https://openalex.org/W4229005866', 'https://openalex.org/W4383197616', 'https://openalex.org/W4384644092', 'https://openalex.org/W4362640917', 'https://openalex.org/W4324378502', 'https://openalex.org/W4362515116']",2025-01-11
https://openalex.org/W4413304852,https://doi.org/10.1007/s43684-025-00100-5,Large language models for PHM: a review of optimization techniques and applications,"Abstract The rapid advancement of Large Language Models (LLMs) has created unprecedented opportunities for industrial automation, process optimization, and decision support systems. As industries seek to leverage LLMs for industrial tasks, understanding their architecture, deployment strategies, and fine-tuning methods becomes critical. In this review, we aim to summarize the challenges, key technologies, current status, and future directions of LLM in Prognostics and Health Management(PHM). First, this review introduces deep learning for PHM. We begin by analyzing the architectural considerations and deployment strategies for industrial environments, including acceleration techniques and quantization methods that enable efficient operation on resource-constrained industrial hardware. Second, we investigate Parameter Efficient Fine-Tuning (PEFT) techniques that allow industry-specific adaptation without prohibitive computational costs. Multi-modal capabilities extending LLMs beyond text to process sensor data, images, and time-series information are also discussed. Finally, we explore emerging PHM including anomaly detection systems that identify equipment malfunctions, fault diagnosis frameworks that determine root causes, and specialized question-answering systems that empower workers with instant domain expertise. We conclude by identifying key challenges and future research directions for LLM deployment in PHM. This review provides a timely resource for researchers, engineers, and decision-makers navigating the transformative potential of language models in industry 4.0 environments.","['https://openalex.org/W2060304859', 'https://openalex.org/W2028119131', 'https://openalex.org/W2107074288', 'https://openalex.org/W2070803528', 'https://openalex.org/W2139833307', 'https://openalex.org/W2296172419', 'https://openalex.org/W2919115771', 'https://openalex.org/W4391514872', 'https://openalex.org/W3168997536', 'https://openalex.org/W3005641657', 'https://openalex.org/W3008130297', 'https://openalex.org/W6810081322', 'https://openalex.org/W4225591000', 'https://openalex.org/W4283026156', 'https://openalex.org/W4389326242', 'https://openalex.org/W4384648484', 'https://openalex.org/W4390723197', 'https://openalex.org/W4386364964', 'https://openalex.org/W4387321091', 'https://openalex.org/W4288028629', 'https://openalex.org/W4389518760', 'https://openalex.org/W4402670692', 'https://openalex.org/W4307934016', 'https://openalex.org/W4393146966', 'https://openalex.org/W4384919461', 'https://openalex.org/W4388032210', 'https://openalex.org/W4399837985', 'https://openalex.org/W6600821145', 'https://openalex.org/W4385571124', 'https://openalex.org/W4322766882', 'https://openalex.org/W2616957565', 'https://openalex.org/W4386187806', 'https://openalex.org/W4205991051', 'https://openalex.org/W3044438666', 'https://openalex.org/W3098267758', 'https://openalex.org/W3174770825', 'https://openalex.org/W4285247752', 'https://openalex.org/W4386566659', 'https://openalex.org/W6818723395', 'https://openalex.org/W3215626407', 'https://openalex.org/W6600617704', 'https://openalex.org/W6600383830', 'https://openalex.org/W6608025442', 'https://openalex.org/W6600234944', 'https://openalex.org/W4403002096', 'https://openalex.org/W4388718054', 'https://openalex.org/W4389524500', 'https://openalex.org/W6819060087', 'https://openalex.org/W3209059054', 'https://openalex.org/W6600704668', 'https://openalex.org/W4372260310', 'https://openalex.org/W4402670135', 'https://openalex.org/W4393178509', 'https://openalex.org/W6600577311', 'https://openalex.org/W2962835968', 'https://openalex.org/W6600351811', 'https://openalex.org/W4311415873', 'https://openalex.org/W3040266635', 'https://openalex.org/W4393158476', 'https://openalex.org/W4402753378', 'https://openalex.org/W4391940723', 'https://openalex.org/W4403024522', 'https://openalex.org/W3214066484', 'https://openalex.org/W3006342871', 'https://openalex.org/W2952516470', 'https://openalex.org/W4407263578', 'https://openalex.org/W4387005290', 'https://openalex.org/W2896457183', 'https://openalex.org/W4384345635', 'https://openalex.org/W4394946189', 'https://openalex.org/W4387596421', 'https://openalex.org/W4386080541', 'https://openalex.org/W4388132131', 'https://openalex.org/W4404782930', 'https://openalex.org/W4389984066', 'https://openalex.org/W4403796472', 'https://openalex.org/W4406032994', 'https://openalex.org/W1583837637', 'https://openalex.org/W2612690371', 'https://openalex.org/W2990138404', 'https://openalex.org/W3135550350', 'https://openalex.org/W4236965008']",2025-08-19
https://openalex.org/W4413796162,https://doi.org/10.3390/sym17091400,"Symmetry-Aware Advances in Multimodal Large Language Models: Architectures, Training, and Evaluation","With the exponential growth of multimodal data, the limitations of traditional unimodal models in cross-modal understanding and complex scenario reasoning have become increasingly evident. Built upon the foundation of Large Language Models (LLMs), Multimodal Large Language Models (MLLMs) retain strong reasoning abilities and demonstrate unique capabilities in multimodal understanding. This survey provides a comprehensive overview of the current research landscape of MLLMs. It systematically analyzes mainstream model architectures, training, fine-tuning strategies, and task classifications, while offering a structured account of evaluation methodologies. Beyond synthesis, the paper highlights emerging trends that aim for balanced integration across modalities, tasks, and components, and critically examines key challenges together with potential solutions. The survey specifically emphasizes recent reasoning-oriented MLLMs, with a focus on DeepSeek-R1, analyzing their design paradigms and contributions from the perspective of symmetric reasoning capabilities. Overall, this work offers a comprehensive overview of cutting-edge advancements and lays a foundation for the future development of MLLMs, especially those guided by symmetry principles.","['https://openalex.org/W4400275494', 'https://openalex.org/W4402727558', 'https://openalex.org/W4386065512', 'https://openalex.org/W4402716330', 'https://openalex.org/W4372266552', 'https://openalex.org/W4386071707', 'https://openalex.org/W4402727764', 'https://openalex.org/W4402716423', 'https://openalex.org/W4402727885', 'https://openalex.org/W4402753519', 'https://openalex.org/W4393149524', 'https://openalex.org/W4402727669', 'https://openalex.org/W3110446398', 'https://openalex.org/W2886641317', 'https://openalex.org/W2760103357', 'https://openalex.org/W3090449556', 'https://openalex.org/W4410536678', 'https://openalex.org/W4402727516', 'https://openalex.org/W4401024018', 'https://openalex.org/W4386071687', 'https://openalex.org/W4404575065', 'https://openalex.org/W1933349210', 'https://openalex.org/W1905882502', 'https://openalex.org/W4385572634', 'https://openalex.org/W4401043272', 'https://openalex.org/W4393157525', 'https://openalex.org/W4386187806', 'https://openalex.org/W4402670859', 'https://openalex.org/W2947312908', 'https://openalex.org/W2963115613', 'https://openalex.org/W4285255856', 'https://openalex.org/W4402716477', 'https://openalex.org/W3035524453', 'https://openalex.org/W4390872723', 'https://openalex.org/W4390889801', 'https://openalex.org/W4386071604', 'https://openalex.org/W4402753379', 'https://openalex.org/W4402727613', 'https://openalex.org/W4413147134', 'https://openalex.org/W4402727018', 'https://openalex.org/W4402728164', 'https://openalex.org/W4386076215', 'https://openalex.org/W4406266806', 'https://openalex.org/W2789713147', 'https://openalex.org/W3157677932', 'https://openalex.org/W1956340063', 'https://openalex.org/W4402716381', 'https://openalex.org/W4389523832', 'https://openalex.org/W4405596328', 'https://openalex.org/W4402727405', 'https://openalex.org/W4402753774', 'https://openalex.org/W4402704633', 'https://openalex.org/W639708223', 'https://openalex.org/W4386066536', 'https://openalex.org/W4404879652', 'https://openalex.org/W4402754134', 'https://openalex.org/W4402753874', 'https://openalex.org/W4389524500', 'https://openalex.org/W4402716264', 'https://openalex.org/W4403791574', 'https://openalex.org/W4379259189', 'https://openalex.org/W4378768739', 'https://openalex.org/W4388482029', 'https://openalex.org/W4225323055', 'https://openalex.org/W4377372369', 'https://openalex.org/W4403081466', 'https://openalex.org/W4378509449', 'https://openalex.org/W3208314443', 'https://openalex.org/W4366330503', 'https://openalex.org/W4382763281', 'https://openalex.org/W3168867926', 'https://openalex.org/W4376312115', 'https://openalex.org/W4387800253', 'https://openalex.org/W4402699879', 'https://openalex.org/W4307079201', 'https://openalex.org/W4292779060', 'https://openalex.org/W4393399871', 'https://openalex.org/W4378942772', 'https://openalex.org/W3184735396', 'https://openalex.org/W4377164404']",2025-08-28
https://openalex.org/W4416754455,https://doi.org/10.21203/rs.3.rs-7512915/v1,SoundTwin: A High-Similarity Fast Diffusion Autoregressive Speech Cloning Model Based on Local-Global Feature Fusion,"<title>Abstract</title> In recent years, significant advances in deep learning have propelled text-to-speech (TTS) technology forward. However, there are still challenges in high-quality voice cloning with low-resource and low-latency conditions. Traditional autoregressive models suffer from high inference latency, while emerging diffusion models, although capable of generating high-fidelity speech, incur substantial computational overhead due to their multi-step sampling process. To mitigate these limitations, we propose SoundTwin, a novel speech synthesis framework integrating accelerated diffusion sampling with an autoregressive Transformer architecture. This approach significantly enhances synthesis efficiency without compromising speech naturalness. Furthermore, we design a Local-Global Squeeze-and-Excitation weighted Speaker embedding Network to efficiently extract fine-grained timbre features from limited reference audio, enabling rapid speaker adaptation. The model accepts target text, reference speech, and reference text as inputs, jointly modeling duration, pitch, and energy features to generate high-quality mel-spectrograms. Experimental results validate that our method achieves state-of-the-art speaker similarity and speech naturalness in zero-shot voice cloning tasks.","['https://openalex.org/W2129142580', 'https://openalex.org/W2964243274', 'https://openalex.org/W4297841267', 'https://openalex.org/W4389524500', 'https://openalex.org/W4392902773', 'https://openalex.org/W4402671538', 'https://openalex.org/W4404589953', 'https://openalex.org/W4403999409', 'https://openalex.org/W2747874407', 'https://openalex.org/W4281569374']",2025-11-27
https://openalex.org/W4417116566,https://doi.org/10.64898/2025.12.01.691647,A Multi-modal LLM for Dynamic Protein-Ligand Interactions and Generative Molecular Design,"Abstract BioDynaGen (Biological Dynamics and Generation) is a novel multi-modal framework unifying protein sequences, dynamic binding site conformations, small molecule ligand SMILES, and natural language text into a single discrete token representation. Built upon a general large language model, BioDynaGen employs continuous pre-training and instruction fine-tuning via next-token prediction to address critical gaps in modeling protein dynamics and ligand interactions. This framework enables a diverse range of tasks, including small molecule-protein binding prediction, dynamic pocket design, and ligand-assisted functional generation. By comprehensively integrating these modalities, BioDynaGen offers an advanced framework for understanding and designing complex biological molecular interactions.","['https://openalex.org/W4412490415', 'https://openalex.org/W4390659288', 'https://openalex.org/W4408047635', 'https://openalex.org/W4411446240', 'https://openalex.org/W4400006123', 'https://openalex.org/W3123208292', 'https://openalex.org/W4399527522', 'https://openalex.org/W4402955984', 'https://openalex.org/W4393140058', 'https://openalex.org/W4402716286', 'https://openalex.org/W4389524500', 'https://openalex.org/W4404456192', 'https://openalex.org/W4414834876', 'https://openalex.org/W4392210695', 'https://openalex.org/W4399363752', 'https://openalex.org/W4402900199', 'https://openalex.org/W4298003199', 'https://openalex.org/W4414433136', 'https://openalex.org/W4408345593']",2025-12-03
https://openalex.org/W7116905298,https://doi.org/10.3390/app16010060,Two-Stage Domain Adaptation for LLM-Based ASR by Decoupling Linguistic and Acoustic Factors,"Large language models (LLMs) have been increasingly applied in Automatic Speech Recognition (ASR), achieving significant advancements. However, the performance of LLM-based ASR (LLM-ASR) models remains unsatisfactory when applied across domains due to domain shifts between acoustic and linguistic conditions. To address this challenge, we propose a decoupled two-stage domain adaptation framework that separates the adaptation process into text-only and audio-only stages. In the first stage, we leverage abundant text data from the target domain to refine the LLM component, thereby improving its contextual and linguistic alignment with the target domain. In the second stage, we employ a pseudo-labeling method with unlabeled audio data in the target domain and introduce two key enhancements: (1) incorporating decoupled auxiliary Connectionist Temporal Classification (CTC) loss to improve the robustness of the speech encoder under different acoustic conditions; (2) adopting a synchronous LLM tuning strategy, allowing the LLM to continuously learn linguistic alignment from pseudo-labeled transcriptions enriched with domain textual knowledge. The experimental results demonstrate that our proposed methods significantly improve the performance of LLM-ASR in the target domain, achieving a relative word error rate reduction of 19.2%.","['https://openalex.org/W4212772209', 'https://openalex.org/W4319990555', 'https://openalex.org/W4391215800', 'https://openalex.org/W4403863362', 'https://openalex.org/W4402112425', 'https://openalex.org/W6959349094', 'https://openalex.org/W4410771025', 'https://openalex.org/W2963057973', 'https://openalex.org/W3161143478', 'https://openalex.org/W4406461904', 'https://openalex.org/W3096338464', 'https://openalex.org/W4408352929', 'https://openalex.org/W4386025763', 'https://openalex.org/W2766219058', 'https://openalex.org/W3015457435', 'https://openalex.org/W4389524500', 'https://openalex.org/W3209059054', 'https://openalex.org/W3198098585', 'https://openalex.org/W3163464943', 'https://openalex.org/W4385656656', 'https://openalex.org/W1494198834', 'https://openalex.org/W2962780374', 'https://openalex.org/W2127141656']",2025-12-20
https://openalex.org/W4400376370,https://doi.org/10.21437/interspeech.2024-2490,Human-like Linguistic Biases in Neural Speech Models: Phonetic Categorization and Phonotactic Constraints in Wav2Vec2.0,"What do deep neural speech models know about phonology? Existing work has\nexamined the encoding of individual linguistic units such as phonemes in these\nmodels. Here we investigate interactions between units. Inspired by classic\nexperiments on human speech perception, we study how Wav2Vec2 resolves\nphonotactic constraints. We synthesize sounds on an acoustic continuum between\n/l/ and /r/ and embed them in controlled contexts where only /l/, only /r/, or\nneither occur in English. Like humans, Wav2Vec2 models show a bias towards the\nphonotactically admissable category in processing such ambiguous sounds. Using\nsimple measures to analyze model internals on the level of individual stimuli,\nwe find that this bias emerges in early layers of the model's Transformer\nmodule. This effect is amplified by ASR finetuning but also present in fully\nself-supervised models. Our approach demonstrates how controlled stimulus\ndesigns can help localize specific linguistic knowledge in neural speech\nmodels.\n","['https://openalex.org/W4321764341', 'https://openalex.org/W1977722422', 'https://openalex.org/W3035750922', 'https://openalex.org/W3036601975', 'https://openalex.org/W2127141656', 'https://openalex.org/W2133590932', 'https://openalex.org/W3105148948', 'https://openalex.org/W4256720429', 'https://openalex.org/W2471520273', 'https://openalex.org/W4221102486', 'https://openalex.org/W4401043052', 'https://openalex.org/W2038110455', 'https://openalex.org/W4385822337', 'https://openalex.org/W4385823223', 'https://openalex.org/W4389520784', 'https://openalex.org/W3216401400', 'https://openalex.org/W4390465217', 'https://openalex.org/W4246695671', 'https://openalex.org/W1494198834', 'https://openalex.org/W2979826702', 'https://openalex.org/W4283332789', 'https://openalex.org/W4385823328', 'https://openalex.org/W3127686677']",2024-09-01
https://openalex.org/W4392902857,https://doi.org/10.1109/icassp48485.2024.10446160,SEF-VC: Speaker Embedding Free Zero-Shot Voice Conversion with Cross Attention,"Zero-shot voice conversion (VC) aims to transfer the source speaker timbre to arbitrary unseen target speaker timbre, while keeping the linguistic content unchanged. Although the voice of generated speech can be controlled by providing the speaker embedding of the target speaker, the speaker similarity still lags behind the ground truth recordings. In this paper, we propose SEF-VC, a speaker embedding free voice conversion model, which is designed to learn and incorporate speaker timbre from reference speech via a powerful position-agnostic cross-attention mechanism, and then reconstruct waveform from HuBERT semantic tokens in a non-autoregressive manner. The concise design of SEF-VC enhances its training stability and voice conversion performance. Objective and subjective evaluations demonstrate the superiority of SEF-VC to generate high-quality speech with better similarity to target reference than strong zero-shot VC baselines, even for very short reference speeches.","['https://openalex.org/W2532494225', 'https://openalex.org/W6762533536', 'https://openalex.org/W6776390925', 'https://openalex.org/W6810585344', 'https://openalex.org/W4221146610', 'https://openalex.org/W6752910514', 'https://openalex.org/W3196584150', 'https://openalex.org/W4386133927', 'https://openalex.org/W6805710207', 'https://openalex.org/W6769196770', 'https://openalex.org/W3209059054', 'https://openalex.org/W3140429000', 'https://openalex.org/W3197763626', 'https://openalex.org/W4224916404', 'https://openalex.org/W3161695192', 'https://openalex.org/W2890964092', 'https://openalex.org/W2804998325', 'https://openalex.org/W4280552695', 'https://openalex.org/W4225329057', 'https://openalex.org/W4224916568', 'https://openalex.org/W3160950295', 'https://openalex.org/W4381786045', 'https://openalex.org/W4386132131', 'https://openalex.org/W4393147067', 'https://openalex.org/W6783867762', 'https://openalex.org/W3097777922', 'https://openalex.org/W2972359262', 'https://openalex.org/W2972659941', 'https://openalex.org/W4375869015', 'https://openalex.org/W6631362777', 'https://openalex.org/W4378945745', 'https://openalex.org/W3154134465']",2024-03-18
https://openalex.org/W4401652690,https://doi.org/10.3389/frsip.2024.1339159,Reimagining speech: a scoping review of deep learning-based methods for non-parallel voice conversion,"Research on deep learning-powered voice conversion (VC) in speech-to-speech scenarios are gaining increasing popularity. Although many of the works in the field of voice conversion share a common global pipeline, there is considerable diversity in the underlying structures, methods, and neural sub-blocks used across research efforts. Thus, obtaining a comprehensive understanding of the reasons behind the choice of the different methods included when training voice conversion models can be challenging, and the actual hurdles in the proposed solutions are often unclear. To shed light on these aspects, this paper presents a scoping review that explores the use of deep learning in speech analysis, synthesis, and disentangled speech representation learning within modern voice conversion systems. We screened 628 publications from more than 38 venues between 2017 and 2023, followed by an in-depth review of a final database of 130 eligible studies. Based on the review, we summarise the most frequently used approaches to voice conversion based on deep learning and highlight common pitfalls. We condense the knowledge gathered to identify main challenges, supply solutions grounded in the analysis and provide recommendations for future research directions.","['https://openalex.org/W2118850452', 'https://openalex.org/W3195390317', 'https://openalex.org/W2075950485', 'https://openalex.org/W3113738713', 'https://openalex.org/W6846042280', 'https://openalex.org/W6780218876', 'https://openalex.org/W6800638409', 'https://openalex.org/W4379522932', 'https://openalex.org/W3095930733', 'https://openalex.org/W3097692357', 'https://openalex.org/W6810709333', 'https://openalex.org/W2091425152', 'https://openalex.org/W3136699727', 'https://openalex.org/W6803547063', 'https://openalex.org/W4309395027', 'https://openalex.org/W4385823264', 'https://openalex.org/W6850489759', 'https://openalex.org/W2091538045', 'https://openalex.org/W6810298122', 'https://openalex.org/W3158484908', 'https://openalex.org/W6847224110', 'https://openalex.org/W6851260135', 'https://openalex.org/W4286747238', 'https://openalex.org/W6771763809', 'https://openalex.org/W3113603766', 'https://openalex.org/W6807756524', 'https://openalex.org/W2079283960', 'https://openalex.org/W6849542685', 'https://openalex.org/W3097777922', 'https://openalex.org/W3196643119', 'https://openalex.org/W4312994048', 'https://openalex.org/W3135780427', 'https://openalex.org/W3209059054', 'https://openalex.org/W4297841676', 'https://openalex.org/W6803398443', 'https://openalex.org/W3002433751', 'https://openalex.org/W6784059072', 'https://openalex.org/W2973135352', 'https://openalex.org/W6848319342', 'https://openalex.org/W6748409065', 'https://openalex.org/W2946555236', 'https://openalex.org/W6756504009', 'https://openalex.org/W6761382815', 'https://openalex.org/W2972667718', 'https://openalex.org/W3010732772', 'https://openalex.org/W2049686551', 'https://openalex.org/W6748253163', 'https://openalex.org/W6792630005', 'https://openalex.org/W3132220150', 'https://openalex.org/W6850317796', 'https://openalex.org/W6846967393', 'https://openalex.org/W4280552695', 'https://openalex.org/W6850217783', 'https://openalex.org/W6769593479', 'https://openalex.org/W3215590351', 'https://openalex.org/W2793667044', 'https://openalex.org/W6852418399', 'https://openalex.org/W3204457821', 'https://openalex.org/W2972399707', 'https://openalex.org/W2471520273', 'https://openalex.org/W2901669506', 'https://openalex.org/W3096697361', 'https://openalex.org/W6702590400', 'https://openalex.org/W6796756982', 'https://openalex.org/W4296069266', 'https://openalex.org/W4226251144', 'https://openalex.org/W6857604550', 'https://openalex.org/W4385823126', 'https://openalex.org/W4367291458', 'https://openalex.org/W6631362777', 'https://openalex.org/W4384500817', 'https://openalex.org/W6776440307', 'https://openalex.org/W6776390925', 'https://openalex.org/W6762533536', 'https://openalex.org/W6839738141', 'https://openalex.org/W2988491238', 'https://openalex.org/W3163498938', 'https://openalex.org/W2973049979', 'https://openalex.org/W6810375697', 'https://openalex.org/W3098557217', 'https://openalex.org/W2972853549', 'https://openalex.org/W2083806749', 'https://openalex.org/W2129082420', 'https://openalex.org/W6675215834', 'https://openalex.org/W2156142001', 'https://openalex.org/W4313455347', 'https://openalex.org/W2518172956', 'https://openalex.org/W2130942839', 'https://openalex.org/W4376622218', 'https://openalex.org/W6794884141', 'https://openalex.org/W6849388766', 'https://openalex.org/W4221141917', 'https://openalex.org/W2891378911', 'https://openalex.org/W6640989505', 'https://openalex.org/W2519091744', 'https://openalex.org/W3210530853', 'https://openalex.org/W4322629454', 'https://openalex.org/W3197659778', 'https://openalex.org/W3198082505', 'https://openalex.org/W6809846428', 'https://openalex.org/W4312732823', 'https://openalex.org/W6810428317', 'https://openalex.org/W4386132131', 'https://openalex.org/W6785711547', 'https://openalex.org/W3096524539', 'https://openalex.org/W6775421620', 'https://openalex.org/W3041738652', 'https://openalex.org/W6732251480', 'https://openalex.org/W6839145123', 'https://openalex.org/W4296068989', 'https://openalex.org/W4376619349', 'https://openalex.org/W4296068587', 'https://openalex.org/W6860313990', 'https://openalex.org/W4390912423', 'https://openalex.org/W6849266739', 'https://openalex.org/W6803539737', 'https://openalex.org/W2996414377', 'https://openalex.org/W4388348985', 'https://openalex.org/W3097952294', 'https://openalex.org/W2973142754', 'https://openalex.org/W6774197554', 'https://openalex.org/W6785164032', 'https://openalex.org/W6784688130', 'https://openalex.org/W3167106704', 'https://openalex.org/W3100378519', 'https://openalex.org/W4221146610', 'https://openalex.org/W4327767739', 'https://openalex.org/W3196700074', 'https://openalex.org/W4224928197', 'https://openalex.org/W2596397435', 'https://openalex.org/W4224920040', 'https://openalex.org/W3204602440', 'https://openalex.org/W3105549447', 'https://openalex.org/W3144417991', 'https://openalex.org/W4319862675', 'https://openalex.org/W2937579788', 'https://openalex.org/W3135654121', 'https://openalex.org/W4312613101', 'https://openalex.org/W3015805741', 'https://openalex.org/W4312096673', 'https://openalex.org/W3168719651', 'https://openalex.org/W4392910523', 'https://openalex.org/W3005784640', 'https://openalex.org/W4310895557', 'https://openalex.org/W2105698384', 'https://openalex.org/W3160950295', 'https://openalex.org/W4319309700', 'https://openalex.org/W4206934353', 'https://openalex.org/W4224085515', 'https://openalex.org/W4376624754', 'https://openalex.org/W4224916404', 'https://openalex.org/W4392909950', 'https://openalex.org/W3101689408', 'https://openalex.org/W3130168980', 'https://openalex.org/W4328027527', 'https://openalex.org/W3213785244', 'https://openalex.org/W2990138404', 'https://openalex.org/W2982223350', 'https://openalex.org/W3015434413', 'https://openalex.org/W3163573274', 'https://openalex.org/W4319586549', 'https://openalex.org/W1524333225', 'https://openalex.org/W3161695192', 'https://openalex.org/W2902070858', 'https://openalex.org/W4407601776', 'https://openalex.org/W4319862691', 'https://openalex.org/W2100649345', 'https://openalex.org/W4407774794', 'https://openalex.org/W4312806563', 'https://openalex.org/W4224916425', 'https://openalex.org/W1583837637', 'https://openalex.org/W4200027410', 'https://openalex.org/W4312097414', 'https://openalex.org/W3142644187']",2024-08-16
https://openalex.org/W4407388754,https://doi.org/10.3390/jcp5010006,Partial Fake Speech Attacks in the Real World Using Deepfake Audio,"Advances in deep learning have led to dramatic improvements in generative synthetic speech, eliminating robotic speech patterns to create speech that is indistinguishable from a human voice. Although these advances are extremely useful in various applications, they also facilitate powerful attacks against both humans and machines. Recently, a new type of speech attack called partial fake (PF) speech has emerged. This paper studies how well humans and machines, including speaker recognition systems and existing fake-speech detection tools, can distinguish between human voice and computer-generated speech. Our study shows that both humans and machines can be easily deceived by PF speech, and the current defences against PF speech are insufficient. These findings emphasise the urgency of increasing awareness for humans and creating new automated defences against PF speech for machines.","['https://openalex.org/W2125245899', 'https://openalex.org/W4200631896', 'https://openalex.org/W4385815354', 'https://openalex.org/W4385764360', 'https://openalex.org/W6779871621', 'https://openalex.org/W4372270423', 'https://openalex.org/W4377865285', 'https://openalex.org/W4372337821', 'https://openalex.org/W4382999039', 'https://openalex.org/W4389166059', 'https://openalex.org/W3201506562', 'https://openalex.org/W4388980081', 'https://openalex.org/W3146945401', 'https://openalex.org/W4379193822', 'https://openalex.org/W3187722398', 'https://openalex.org/W4388903318', 'https://openalex.org/W2168959749', 'https://openalex.org/W4390517533', 'https://openalex.org/W4385993905', 'https://openalex.org/W4391021739', 'https://openalex.org/W4385993937', 'https://openalex.org/W2165848216', 'https://openalex.org/W3154451338', 'https://openalex.org/W4377079685', 'https://openalex.org/W4225329057', 'https://openalex.org/W4386132131', 'https://openalex.org/W4225746985', 'https://openalex.org/W3210530853', 'https://openalex.org/W4372260053', 'https://openalex.org/W3083423753', 'https://openalex.org/W4221138880', 'https://openalex.org/W4226264925', 'https://openalex.org/W3196584150', 'https://openalex.org/W4319862431', 'https://openalex.org/W4385823472', 'https://openalex.org/W2808706139', 'https://openalex.org/W4402600203', 'https://openalex.org/W3032558098', 'https://openalex.org/W2998572311', 'https://openalex.org/W3208526032', 'https://openalex.org/W4386730427', 'https://openalex.org/W3183869905', 'https://openalex.org/W4313306150', 'https://openalex.org/W4225527248', 'https://openalex.org/W4385823361', 'https://openalex.org/W4388856757', 'https://openalex.org/W4400678935', 'https://openalex.org/W3165478005']",2025-02-08
https://openalex.org/W4367061106,https://doi.org/10.48550/arxiv.2304.12995,"AudioGPT: Understanding and Generating Speech, Music, Sound, and Talking Head","Large language models (LLMs) have exhibited remarkable capabilities across a variety of domains and tasks, challenging our understanding of learning and cognition. Despite the recent success, current LLMs are not capable of processing complex audio information or conducting spoken conversations (like Siri or Alexa). In this work, we propose a multi-modal AI system named AudioGPT, which complements LLMs (i.e., ChatGPT) with 1) foundation models to process complex audio information and solve numerous understanding and generation tasks; and 2) the input/output interface (ASR, TTS) to support spoken dialogue. With an increasing demand to evaluate multi-modal LLMs of human intention understanding and cooperation with foundation models, we outline the principles and processes and test AudioGPT in terms of consistency, capability, and robustness. Experimental results demonstrate the capabilities of AudioGPT in solving AI tasks with speech, music, sound, and talking head understanding and generation in multi-round dialogues, which empower humans to create rich and diverse audio content with unprecedented ease. Our system is publicly available at \url{https://github.com/AIGC-Audio/AudioGPT}.",[],2023-04-25
https://openalex.org/W4381786045,https://doi.org/10.1109/taslp.2023.3288409,AudioLM: A Language Modeling Approach to Audio Generation,"We introduce AudioLM, a framework for high-quality audio generation with long-term consistency. AudioLM maps the input audio to a sequence of discrete tokens and casts audio generation as a language modeling task in this representation space. We show how existing audio tokenizers provide different trade-offs between reconstruction quality and long-term structure, and we propose a hybrid tokenization scheme to achieve both objectives. Namely, we leverage the discretized activations of a masked language model pre-trained on audio to capture long-term structure and the discrete codes produced by a neural audio codec to achieve high-quality synthesis. By training on large corpora of raw audio waveforms, AudioLM learns to generate natural and coherent continuations given short prompts. When trained on speech, and without any transcript or annotation, AudioLM generates syntactically and semantically plausible speech continuations while also maintaining speaker identity and prosody for unseen speakers. Furthermore, we demonstrate how our approach extends beyond speech by generating coherent piano music continuations, despite being trained without any symbolic representation of music.","['https://openalex.org/W3197259906', 'https://openalex.org/W6840487619', 'https://openalex.org/W6839643428', 'https://openalex.org/W3160799772', 'https://openalex.org/W6739901393', 'https://openalex.org/W6809593508', 'https://openalex.org/W6790356757', 'https://openalex.org/W4292825791', 'https://openalex.org/W2995181338', 'https://openalex.org/W6734901337', 'https://openalex.org/W6802517614', 'https://openalex.org/W3148101939', 'https://openalex.org/W6810081322', 'https://openalex.org/W1494198834', 'https://openalex.org/W4226033575', 'https://openalex.org/W3215615641', 'https://openalex.org/W6755207826', 'https://openalex.org/W2395899413', 'https://openalex.org/W3037038648', 'https://openalex.org/W4287887366', 'https://openalex.org/W3198217962', 'https://openalex.org/W4296068815', 'https://openalex.org/W3097777922', 'https://openalex.org/W3180355996', 'https://openalex.org/W6810311916', 'https://openalex.org/W6776218486', 'https://openalex.org/W4312633146', 'https://openalex.org/W1728888090', 'https://openalex.org/W6778883912', 'https://openalex.org/W6782760101', 'https://openalex.org/W6783867762', 'https://openalex.org/W6767111847', 'https://openalex.org/W6783182287', 'https://openalex.org/W3095095816', 'https://openalex.org/W6783944145', 'https://openalex.org/W4226380987', 'https://openalex.org/W6771324808', 'https://openalex.org/W6798182279', 'https://openalex.org/W3144810982', 'https://openalex.org/W3162391496', 'https://openalex.org/W3209059054', 'https://openalex.org/W6769196770', 'https://openalex.org/W6748409065', 'https://openalex.org/W3131922516', 'https://openalex.org/W6769627184', 'https://openalex.org/W6735849998', 'https://openalex.org/W6762931180', 'https://openalex.org/W2972354707', 'https://openalex.org/W2963182577', 'https://openalex.org/W6843673214', 'https://openalex.org/W6805710207', 'https://openalex.org/W6800767084', 'https://openalex.org/W2752796333', 'https://openalex.org/W6768435317', 'https://openalex.org/W6844194202', 'https://openalex.org/W3186609711', 'https://openalex.org/W6780218876', 'https://openalex.org/W6755182157', 'https://openalex.org/W3198815374', 'https://openalex.org/W4292779060', 'https://openalex.org/W4283388932', 'https://openalex.org/W2604231067', 'https://openalex.org/W4221161768', 'https://openalex.org/W2896457183', 'https://openalex.org/W2996286887', 'https://openalex.org/W2519091744', 'https://openalex.org/W4224308101', 'https://openalex.org/W2995359496', 'https://openalex.org/W4288348042', 'https://openalex.org/W3123097577', 'https://openalex.org/W2963799213', 'https://openalex.org/W4287802874', 'https://openalex.org/W2950547518', 'https://openalex.org/W3129651364', 'https://openalex.org/W4385245566', 'https://openalex.org/W3092028330', 'https://openalex.org/W3177813494', 'https://openalex.org/W2979476256', 'https://openalex.org/W3206395542', 'https://openalex.org/W2593779438', 'https://openalex.org/W4226275767', 'https://openalex.org/W2970006822', 'https://openalex.org/W4309793872', 'https://openalex.org/W3036601975', 'https://openalex.org/W4225680573', 'https://openalex.org/W4394671563', 'https://openalex.org/W4288089799', 'https://openalex.org/W3198123200', 'https://openalex.org/W4294619240', 'https://openalex.org/W4298580827', 'https://openalex.org/W2971074500', 'https://openalex.org/W4297808394']",2023-01-01
https://openalex.org/W4385570101,https://doi.org/10.18653/v1/2023.findings-acl.447,DUB: Discrete Unit Back-translation for Speech Translation,"How can speech-to-text translation (ST) perform as well as machine translation (MT)? The key point is to bridge the modality gap between speech and text so that useful MT techniques can be applied to ST.Recently, the approach of representing speech with unsupervised discrete units yields a new way to ease the modality problem. This motivates us to propose Discrete Unit Back-translation(DUB) to answer two questions (1) Is it better to represent speech with discrete units than with continuous features in direct ST? (2) How much benefit can useful MT techniques bring to ST? With DUB, the back-translation technique can successfully be applied on direct ST and obtains an average boost of 5.5 BLEU on MuST-C En-De/Fr/Es. In the low-resource language scenario, our method achieves comparable performance to existing methods that rely on large-scale external data. Code and models are available at https://anonymous.4open.science/r/DUB/.",[],2023-01-01
https://openalex.org/W4323572061,https://doi.org/10.48550/arxiv.2303.03378,PaLM-E: An Embodied Multimodal Language Model,"Large language models excel at a wide range of complex tasks. However, enabling general inference in the real world, e.g., for robotics problems, raises the challenge of grounding. We propose embodied language models to directly incorporate real-world continuous sensor modalities into language models and thereby establish the link between words and percepts. Input to our embodied language model are multi-modal sentences that interleave visual, continuous state estimation, and textual input encodings. We train these encodings end-to-end, in conjunction with a pre-trained large language model, for multiple embodied tasks including sequential robotic manipulation planning, visual question answering, and captioning. Our evaluations show that PaLM-E, a single large embodied multimodal model, can address a variety of embodied reasoning tasks, from a variety of observation modalities, on multiple embodiments, and further, exhibits positive transfer: the model benefits from diverse joint training across internet-scale language, vision, and visual-language domains. Our largest model, PaLM-E-562B with 562B parameters, in addition to being trained on robotics tasks, is a visual-language generalist with state-of-the-art performance on OK-VQA, and retains generalist language capabilities with increasing scale.",[],2023-03-06
https://openalex.org/W4361866031,https://doi.org/10.48550/arxiv.2303.17580,HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in Hugging Face,"Solving complicated AI tasks with different domains and modalities is a key step toward artificial general intelligence. While there are numerous AI models available for various domains and modalities, they cannot handle complicated AI tasks autonomously. Considering large language models (LLMs) have exhibited exceptional abilities in language understanding, generation, interaction, and reasoning, we advocate that LLMs could act as a controller to manage existing AI models to solve complicated AI tasks, with language serving as a generic interface to empower this. Based on this philosophy, we present HuggingGPT, an LLM-powered agent that leverages LLMs (e.g., ChatGPT) to connect various AI models in machine learning communities (e.g., Hugging Face) to solve AI tasks. Specifically, we use ChatGPT to conduct task planning when receiving a user request, select models according to their function descriptions available in Hugging Face, execute each subtask with the selected AI model, and summarize the response according to the execution results. By leveraging the strong language capability of ChatGPT and abundant AI models in Hugging Face, HuggingGPT can tackle a wide range of sophisticated AI tasks spanning different modalities and domains and achieve impressive results in language, vision, speech, and other challenging tasks, which paves a new way towards the realization of artificial general intelligence.",[],2023-03-30
https://openalex.org/W3140429000,https://doi.org/10.21437/interspeech.2021-475,Speech Resynthesis from Discrete Disentangled Self-Supervised Representations,"We propose using self-supervised discrete representations for the task of\nspeech resynthesis. To generate disentangled representation, we separately\nextract low-bitrate representations for speech content, prosodic information,\nand speaker identity. This allows to synthesize speech in a controllable\nmanner. We analyze various state-of-the-art, self-supervised representation\nlearning methods and shed light on the advantages of each method while\nconsidering reconstruction quality and disentanglement properties.\nSpecifically, we evaluate the F0 reconstruction, speaker identification\nperformance (for both resynthesis and voice conversion), recordings'\nintelligibility, and overall quality using subjective human evaluation. Lastly,\nwe demonstrate how these representations can be used for an ultra-lightweight\nspeech codec. Using the obtained representations, we can get to a rate of 365\nbits per second while providing better speech quality than the baseline\nmethods. Audio samples can be found under the following link:\nspeechbot.github.io/resynthesis.\n","['https://openalex.org/W2963618559', 'https://openalex.org/W2890983311', 'https://openalex.org/W2963403868', 'https://openalex.org/W2963799213', 'https://openalex.org/W2973049979', 'https://openalex.org/W2963300588', 'https://openalex.org/W2097645910', 'https://openalex.org/W3095948607', 'https://openalex.org/W2944079609', 'https://openalex.org/W2995181338', 'https://openalex.org/W2292235217', 'https://openalex.org/W3096216486', 'https://openalex.org/W2972867623', 'https://openalex.org/W2964167449', 'https://openalex.org/W2935711438', 'https://openalex.org/W2940544976', 'https://openalex.org/W1494198834', 'https://openalex.org/W3210177631', 'https://openalex.org/W2808631503', 'https://openalex.org/W2120847449', 'https://openalex.org/W3021164770', 'https://openalex.org/W3096656254', 'https://openalex.org/W3148101939', 'https://openalex.org/W1885680957', 'https://openalex.org/W2115098197', 'https://openalex.org/W1498609987', 'https://openalex.org/W3093427098', 'https://openalex.org/W3016098186', 'https://openalex.org/W3099782249', 'https://openalex.org/W3095361818', 'https://openalex.org/W2130086727', 'https://openalex.org/W2527729766', 'https://openalex.org/W2842511635', 'https://openalex.org/W3025878903', 'https://openalex.org/W2924551963', 'https://openalex.org/W3096323553', 'https://openalex.org/W2949382160', 'https://openalex.org/W2750248772', 'https://openalex.org/W3163296124', 'https://openalex.org/W3098403858', 'https://openalex.org/W2963341956', 'https://openalex.org/W2775336875', 'https://openalex.org/W1959608418', 'https://openalex.org/W3160799772', 'https://openalex.org/W2114925438', 'https://openalex.org/W2963091184', 'https://openalex.org/W2107740512']",2021-08-27
https://openalex.org/W4322718246,https://doi.org/10.48550/arxiv.2302.14045,Language Is Not All You Need: Aligning Perception with Language Models,"A big convergence of language, multimodal perception, action, and world modeling is a key step toward artificial general intelligence. In this work, we introduce Kosmos-1, a Multimodal Large Language Model (MLLM) that can perceive general modalities, learn in context (i.e., few-shot), and follow instructions (i.e., zero-shot). Specifically, we train Kosmos-1 from scratch on web-scale multimodal corpora, including arbitrarily interleaved text and images, image-caption pairs, and text data. We evaluate various settings, including zero-shot, few-shot, and multimodal chain-of-thought prompting, on a wide range of tasks without any gradient updates or finetuning. Experimental results show that Kosmos-1 achieves impressive performance on (i) language understanding, generation, and even OCR-free NLP (directly fed with document images), (ii) perception-language tasks, including multimodal dialogue, image captioning, visual question answering, and (iii) vision tasks, such as image recognition with descriptions (specifying classification via text instructions). We also show that MLLMs can benefit from cross-modal transfer, i.e., transfer knowledge from language to multimodal, and from multimodal to language. In addition, we introduce a dataset of Raven IQ test, which diagnoses the nonverbal reasoning capability of MLLMs.",[],2023-02-27
https://openalex.org/W3169320628,https://doi.org/10.1109/taslp.2021.3122291,HuBERT: Self-Supervised Speech Representation Learning by Masked Prediction of Hidden Units,"Self-supervised approaches for speech representation learning are challenged by three unique problems: (1) there are multiple sound units in each input utterance, (2) there is no lexicon of input sound units during the pre-training phase, and (3) sound units have variable lengths with no explicit segmentation. To deal with these three problems, we propose the Hidden-Unit BERT (HuBERT) approach for self-supervised speech representation learning, which utilizes an offline clustering step to provide aligned target labels for a BERT-like prediction loss. A key ingredient of our approach is applying the prediction loss over the masked regions only, which forces the model to learn a combined acoustic and language model over the continuous inputs. HuBERT relies primarily on the consistency of the unsupervised clustering step rather than the intrinsic quality of the assigned cluster labels. Starting with a simple k-means teacher of 100 clusters, and using two iterations of clustering, the HuBERT model either matches or improves upon the state-of-the-art wav2vec 2.0 performance on the Librispeech (960h) and Libri-light (60,000h) benchmarks with 10min, 1h, 10h, 100h, and 960h fine-tuning subsets. Using a 1B parameter model, HuBERT shows up to 19% and 13% relative WER reduction on the more challenging dev-other and test-other evaluation subsets.","['https://openalex.org/W2937090315', 'https://openalex.org/W6790168579', 'https://openalex.org/W6745117592', 'https://openalex.org/W2962850167', 'https://openalex.org/W2155273149', 'https://openalex.org/W2032210463', 'https://openalex.org/W3011411500', 'https://openalex.org/W3033038061', 'https://openalex.org/W6947929050', 'https://openalex.org/W3026041220', 'https://openalex.org/W2933138175', 'https://openalex.org/W3097777922', 'https://openalex.org/W2347098582', 'https://openalex.org/W6784637704', 'https://openalex.org/W6675022971', 'https://openalex.org/W3008525923', 'https://openalex.org/W2750248772', 'https://openalex.org/W6844194202', 'https://openalex.org/W3160799772', 'https://openalex.org/W6755207826', 'https://openalex.org/W2962739339', 'https://openalex.org/W6771917389', 'https://openalex.org/W2883725317', 'https://openalex.org/W3035524453', 'https://openalex.org/W2995181338', 'https://openalex.org/W1494198834', 'https://openalex.org/W6777232839', 'https://openalex.org/W6786669483', 'https://openalex.org/W6631190155', 'https://openalex.org/W2150593711', 'https://openalex.org/W6675354045', 'https://openalex.org/W6784436999', 'https://openalex.org/W2973157397', 'https://openalex.org/W6783797576', 'https://openalex.org/W6770514103', 'https://openalex.org/W3015522062', 'https://openalex.org/W6772883055', 'https://openalex.org/W4254197176', 'https://openalex.org/W3161101519', 'https://openalex.org/W3096338464', 'https://openalex.org/W6779997284', 'https://openalex.org/W6786614245', 'https://openalex.org/W6779326418', 'https://openalex.org/W6778883912', 'https://openalex.org/W6766673545', 'https://openalex.org/W6769311223', 'https://openalex.org/W6780483730', 'https://openalex.org/W2973049979', 'https://openalex.org/W6769196770', 'https://openalex.org/W2972943112', 'https://openalex.org/W6780218876', 'https://openalex.org/W2982223350', 'https://openalex.org/W2110073835', 'https://openalex.org/W3035202887', 'https://openalex.org/W3016011332', 'https://openalex.org/W3003875258', 'https://openalex.org/W3015265920', 'https://openalex.org/W2127141656', 'https://openalex.org/W6784614252', 'https://openalex.org/W2888911345', 'https://openalex.org/W2752796333', 'https://openalex.org/W3093788532', 'https://openalex.org/W2164579587', 'https://openalex.org/W2996383576', 'https://openalex.org/W3025035610', 'https://openalex.org/W2965373594', 'https://openalex.org/W2101234009', 'https://openalex.org/W3135676170', 'https://openalex.org/W2963341956', 'https://openalex.org/W2963618559', 'https://openalex.org/W3036601975', 'https://openalex.org/W3036224891', 'https://openalex.org/W3025165719', 'https://openalex.org/W3126816608', 'https://openalex.org/W2988736778', 'https://openalex.org/W3027083471', 'https://openalex.org/W3099782249', 'https://openalex.org/W2998532468', 'https://openalex.org/W3144810982', 'https://openalex.org/W3093579165', 'https://openalex.org/W2953190524', 'https://openalex.org/W2926827382', 'https://openalex.org/W2073459066', 'https://openalex.org/W1522301498', 'https://openalex.org/W2982399380', 'https://openalex.org/W3030163527', 'https://openalex.org/W3125709657', 'https://openalex.org/W3035060554', 'https://openalex.org/W3013571468', 'https://openalex.org/W3107668149', 'https://openalex.org/W3093533780', 'https://openalex.org/W3112034174', 'https://openalex.org/W2842511635', 'https://openalex.org/W2963799213', 'https://openalex.org/W2100768664', 'https://openalex.org/W1553004968']",2021-01-01
https://openalex.org/W4310924890,https://doi.org/10.48550/arxiv.2212.03657,M3ST: Mix at Three Levels for Speech Translation,"How to solve the data scarcity problem for end-to-end speech-to-text translation (ST)? It's well known that data augmentation is an efficient method to improve performance for many tasks by enlarging the dataset. In this paper, we propose Mix at three levels for Speech Translation (M^3ST) method to increase the diversity of the augmented training corpus. Specifically, we conduct two phases of fine-tuning based on a pre-trained model using external machine translation (MT) data. In the first stage of fine-tuning, we mix the training corpus at three levels, including word level, sentence level and frame level, and fine-tune the entire model with mixed data. At the second stage of fine-tuning, we take both original speech sequences and original text sequences in parallel into the model to fine-tune the network, and use Jensen-Shannon divergence to regularize their outputs. Experiments on MuST-C speech translation benchmark and analysis show that M^3ST outperforms current strong baselines and achieves state-of-the-art results on eight directions with an average BLEU of 29.9.",[],2022-12-07
https://openalex.org/W2995181338,https://doi.org/10.1109/icassp40776.2020.9052942,Libri-Light: A Benchmark for ASR with Limited or No Supervision,"We introduce a new collection of spoken English audio suitable for training\nspeech recognition systems under limited or no supervision. It is derived from\nopen-source audio books from the LibriVox project. It contains over 60K hours\nof audio, which is, to our knowledge, the largest freely-available corpus of\nspeech. The audio has been segmented using voice activity detection and is\ntagged with SNR, speaker ID and genre descriptions. Additionally, we provide\nbaseline systems and evaluation metrics working under three settings: (1) the\nzero resource/unsupervised setting (ABX), (2) the semi-supervised setting (PER,\nCER) and (3) the distant supervision setting (WER). Settings (2) and (3) use\nlimited textual resources (10 minutes to 10 hours) aligned with the speech.\nSetting (3) uses large amounts of unaligned text. They are evaluated on the\nstandard LibriSpeech dev and test sets for comparison with the supervised\nstate-of-the-art.\n","['https://openalex.org/W2963620343', 'https://openalex.org/W2940544976', 'https://openalex.org/W6629717138', 'https://openalex.org/W2937197076', 'https://openalex.org/W6712444837', 'https://openalex.org/W2592866267', 'https://openalex.org/W2953190524', 'https://openalex.org/W3005511757', 'https://openalex.org/W6751433836', 'https://openalex.org/W6770514103', 'https://openalex.org/W6756326128', 'https://openalex.org/W1970890968', 'https://openalex.org/W6656619859', 'https://openalex.org/W2127141656', 'https://openalex.org/W2973049979', 'https://openalex.org/W2842511635', 'https://openalex.org/W6775452034', 'https://openalex.org/W2346964103', 'https://openalex.org/W4234016251', 'https://openalex.org/W6747270024', 'https://openalex.org/W6679855610', 'https://openalex.org/W2944255943', 'https://openalex.org/W6748342566', 'https://openalex.org/W2972630480', 'https://openalex.org/W2963425185', 'https://openalex.org/W2161482971', 'https://openalex.org/W4288107125', 'https://openalex.org/W2979476256', 'https://openalex.org/W2963340922', 'https://openalex.org/W2899377381', 'https://openalex.org/W4297818305', 'https://openalex.org/W2134800885', 'https://openalex.org/W2593779438', 'https://openalex.org/W3103005696', 'https://openalex.org/W3016181583', 'https://openalex.org/W4297808394', 'https://openalex.org/W2988736778', 'https://openalex.org/W2804648901', 'https://openalex.org/W2025198378', 'https://openalex.org/W2161391345', 'https://openalex.org/W2787447541', 'https://openalex.org/W2926063217', 'https://openalex.org/W2794753807', 'https://openalex.org/W4300047444', 'https://openalex.org/W3015522062', 'https://openalex.org/W2395899413', 'https://openalex.org/W1494198834', 'https://openalex.org/W2996383576', 'https://openalex.org/W2973026522', 'https://openalex.org/W2781384251']",2020-04-09
https://openalex.org/W4313679638,https://doi.org/10.48550/arxiv.2301.02111,Neural Codec Language Models are Zero-Shot Text to Speech Synthesizers,"We introduce a language modeling approach for text to speech synthesis (TTS). Specifically, we train a neural codec language model (called Vall-E) using discrete codes derived from an off-the-shelf neural audio codec model, and regard TTS as a conditional language modeling task rather than continuous signal regression as in previous work. During the pre-training stage, we scale up the TTS training data to 60K hours of English speech which is hundreds of times larger than existing systems. Vall-E emerges in-context learning capabilities and can be used to synthesize high-quality personalized speech with only a 3-second enrolled recording of an unseen speaker as an acoustic prompt. Experiment results show that Vall-E significantly outperforms the state-of-the-art zero-shot TTS system in terms of speech naturalness and speaker similarity. In addition, we find Vall-E could preserve the speaker's emotion and acoustic environment of the acoustic prompt in synthesis. See https://aka.ms/valle for demos of our work.",[],2023-01-05
https://openalex.org/W3036601975,https://doi.org/10.48550/arxiv.2006.11477,wav2vec 2.0: A Framework for Self-Supervised Learning of Speech Representations,"We show for the first time that learning powerful representations from speech audio alone followed by fine-tuning on transcribed speech can outperform the best semi-supervised methods while being conceptually simpler. wav2vec 2.0 masks the speech input in the latent space and solves a contrastive task defined over a quantization of the latent representations which are jointly learned. Experiments using all labeled data of Librispeech achieve 1.8/3.3 WER on the clean/other test sets. When lowering the amount of labeled data to one hour, wav2vec 2.0 outperforms the previous state of the art on the 100 hour subset while using 100 times less labeled data. Using just ten minutes of labeled data and pre-training on 53k hours of unlabeled data still achieves 4.8/8.2 WER. This demonstrates the feasibility of speech recognition with limited amounts of labeled data.","['https://openalex.org/W273093436', 'https://openalex.org/W3005680577', 'https://openalex.org/W3107298252', 'https://openalex.org/W3016181583', 'https://openalex.org/W2908336025', 'https://openalex.org/W2936295285', 'https://openalex.org/W2973049979', 'https://openalex.org/W2953190524', 'https://openalex.org/W2941814890', 'https://openalex.org/W2962901777', 'https://openalex.org/W2971155163', 'https://openalex.org/W2936774411', 'https://openalex.org/W3025165719', 'https://openalex.org/W2127141656', 'https://openalex.org/W2996383576', 'https://openalex.org/W2995181338', 'https://openalex.org/W2996159613', 'https://openalex.org/W3004728855', 'https://openalex.org/W2124509324', 'https://openalex.org/W2995680346', 'https://openalex.org/W10548402', 'https://openalex.org/W3026041220', 'https://openalex.org/W2896457183', 'https://openalex.org/W3103005696', 'https://openalex.org/W2121879602', 'https://openalex.org/W2794209590', 'https://openalex.org/W2991213871', 'https://openalex.org/W2944828972', 'https://openalex.org/W2949892913', 'https://openalex.org/W2994536315', 'https://openalex.org/W3021469861', 'https://openalex.org/W2152790380', 'https://openalex.org/W2964121744', 'https://openalex.org/W2547875792', 'https://openalex.org/W2962942158', 'https://openalex.org/W3127686677', 'https://openalex.org/W2963799213', 'https://openalex.org/W3003875258', 'https://openalex.org/W2963807318', 'https://openalex.org/W3027083471', 'https://openalex.org/W2899663614', 'https://openalex.org/W2296701362', 'https://openalex.org/W2965373594', 'https://openalex.org/W2963403868', 'https://openalex.org/W3002741552', 'https://openalex.org/W2981991061', 'https://openalex.org/W2962739339', 'https://openalex.org/W2952509486', 'https://openalex.org/W2988736778', 'https://openalex.org/W3035524453', 'https://openalex.org/W3037932933', 'https://openalex.org/W2963631907', 'https://openalex.org/W2842511635', 'https://openalex.org/W1494198834', 'https://openalex.org/W3015419784', 'https://openalex.org/W2972943112', 'https://openalex.org/W2972374322']",2020-06-20
https://openalex.org/W4224308101,https://doi.org/10.48550/arxiv.2204.02311,PaLM: Scaling Language Modeling with Pathways,"Large language models have been shown to achieve remarkable performance across a variety of natural language tasks using few-shot learning, which drastically reduces the number of task-specific training examples needed to adapt the model to a particular application. To further our understanding of the impact of scale on few-shot learning, we trained a 540-billion parameter, densely activated, Transformer language model, which we call Pathways Language Model PaLM. We trained PaLM on 6144 TPU v4 chips using Pathways, a new ML system which enables highly efficient training across multiple TPU Pods. We demonstrate continued benefits of scaling by achieving state-of-the-art few-shot learning results on hundreds of language understanding and generation benchmarks. On a number of these tasks, PaLM 540B achieves breakthrough performance, outperforming the finetuned state-of-the-art on a suite of multi-step reasoning tasks, and outperforming average human performance on the recently released BIG-bench benchmark. A significant number of BIG-bench tasks showed discontinuous improvements from model scale, meaning that performance steeply increased as we scaled to our largest model. PaLM also has strong capabilities in multilingual tasks and source code generation, which we demonstrate on a wide array of benchmarks. We additionally provide a comprehensive analysis on bias and toxicity, and study the extent of training data memorization with respect to model scale. Finally, we discuss the ethical considerations related to large language models and discuss potential mitigation strategies.",[],2022-04-05
https://openalex.org/W3094502228,https://doi.org/10.48550/arxiv.2010.11929,An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale,"While the Transformer architecture has become the de-facto standard for natural language processing tasks, its applications to computer vision remain limited. In vision, attention is either applied in conjunction with convolutional networks, or used to replace certain components of convolutional networks while keeping their overall structure in place. We show that this reliance on CNNs is not necessary and a pure transformer applied directly to sequences of image patches can perform very well on image classification tasks. When pre-trained on large amounts of data and transferred to multiple mid-sized or small image recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision Transformer (ViT) attains excellent results compared to state-of-the-art convolutional networks while requiring substantially fewer computational resources to train.","['https://openalex.org/W2799269579', 'https://openalex.org/W2147800946', 'https://openalex.org/W2964080601', 'https://openalex.org/W2194775991', 'https://openalex.org/W2968124245', 'https://openalex.org/W3097065222', 'https://openalex.org/W2108598243', 'https://openalex.org/W2971315489', 'https://openalex.org/W3034445277', 'https://openalex.org/W2963091558', 'https://openalex.org/W3008526508', 'https://openalex.org/W2981689412', 'https://openalex.org/W3034885317', 'https://openalex.org/W2962843773', 'https://openalex.org/W3096609285', 'https://openalex.org/W3102696055', 'https://openalex.org/W2284050935', 'https://openalex.org/W2998108143', 'https://openalex.org/W3097217077', 'https://openalex.org/W2948798935', 'https://openalex.org/W2963341956', 'https://openalex.org/W2964121744', 'https://openalex.org/W2949117887', 'https://openalex.org/W2937843571', 'https://openalex.org/W3034978746', 'https://openalex.org/W3035058308', 'https://openalex.org/W2970608575', 'https://openalex.org/W2963631907', 'https://openalex.org/W2981413347', 'https://openalex.org/W2931316642', 'https://openalex.org/W3012324658', 'https://openalex.org/W3035422918', 'https://openalex.org/W3033210410', 'https://openalex.org/W3035282577', 'https://openalex.org/W2940744433', 'https://openalex.org/W2950739196', 'https://openalex.org/W2163605009', 'https://openalex.org/W3030163527', 'https://openalex.org/W1977295328', 'https://openalex.org/W2994760783', 'https://openalex.org/W3040573126', 'https://openalex.org/W2533598788', 'https://openalex.org/W3040304705', 'https://openalex.org/W2983446232', 'https://openalex.org/W2086161653', 'https://openalex.org/W3035524453', 'https://openalex.org/W2626778328', 'https://openalex.org/W3035160371', 'https://openalex.org/W2994759459', 'https://openalex.org/W3042608254', 'https://openalex.org/W2970389371', 'https://openalex.org/W3090449556', 'https://openalex.org/W3118608800']",2020-10-22
https://openalex.org/W1494198834,https://doi.org/10.1109/icassp.2015.7178964,Librispeech: An ASR corpus based on public domain audio books,"This paper introduces a new corpus of read English speech, suitable for training and evaluating speech recognition systems. The LibriSpeech corpus is derived from audiobooks that are part of the LibriVox project, and contains 1000 hours of speech sampled at 16 kHz. We have made the corpus freely available for download, along with separately prepared language-model training data and pre-built language models. We show that acoustic models trained on LibriSpeech give lower error rate on the Wall Street Journal (WSJ) test sets than models trained on WSJ itself. We are also releasing Kaldi scripts that make it easy to build these systems.","['https://openalex.org/W2113641473', 'https://openalex.org/W2090755665', 'https://openalex.org/W2148154194', 'https://openalex.org/W2106554350', 'https://openalex.org/W2087064593', 'https://openalex.org/W1647671624', 'https://openalex.org/W1517939602', 'https://openalex.org/W2037740282', 'https://openalex.org/W1599512239', 'https://openalex.org/W6631362777', 'https://openalex.org/W6603477829', 'https://openalex.org/W2024490156', 'https://openalex.org/W2164107060', 'https://openalex.org/W6712802073', 'https://openalex.org/W6677973343', 'https://openalex.org/W6636811518', 'https://openalex.org/W6738902873', 'https://openalex.org/W2097927681', 'https://openalex.org/W2026369565', 'https://openalex.org/W2330075180', 'https://openalex.org/W2950186769', 'https://openalex.org/W2125234026', 'https://openalex.org/W1524333225', 'https://openalex.org/W1934041838', 'https://openalex.org/W1631260214', 'https://openalex.org/W2397159106', 'https://openalex.org/W2620757702', 'https://openalex.org/W85707815', 'https://openalex.org/W2916535084']",2015-04-01
https://openalex.org/W3168867926,https://doi.org/10.48550/arxiv.2106.09685,LoRA: Low-Rank Adaptation of Large Language Models,"An important paradigm of natural language processing consists of large-scale pre-training on general domain data and adaptation to particular tasks or domains. As we pre-train larger models, full fine-tuning, which retrains all model parameters, becomes less feasible. Using GPT-3 175B as an example -- deploying independent instances of fine-tuned models, each with 175B parameters, is prohibitively expensive. We propose Low-Rank Adaptation, or LoRA, which freezes the pre-trained model weights and injects trainable rank decomposition matrices into each layer of the Transformer architecture, greatly reducing the number of trainable parameters for downstream tasks. Compared to GPT-3 175B fine-tuned with Adam, LoRA can reduce the number of trainable parameters by 10,000 times and the GPU memory requirement by 3 times. LoRA performs on-par or better than fine-tuning in model quality on RoBERTa, DeBERTa, GPT-2, and GPT-3, despite having fewer trainable parameters, a higher training throughput, and, unlike adapters, no additional inference latency. We also provide an empirical investigation into rank-deficiency in language model adaptation, which sheds light on the efficacy of LoRA. We release a package that facilitates the integration of LoRA with PyTorch models and provide our implementations and model checkpoints for RoBERTa, DeBERTa, and GPT-2 at https://github.com/microsoft/LoRA.","['https://openalex.org/W3037180580', 'https://openalex.org/W2964035320', 'https://openalex.org/W2949960976', 'https://openalex.org/W2963403868', 'https://openalex.org/W2090958120', 'https://openalex.org/W2786660442', 'https://openalex.org/W2899748887', 'https://openalex.org/W3039127676', 'https://openalex.org/W3030163527', 'https://openalex.org/W2908510526', 'https://openalex.org/W1967077133', 'https://openalex.org/W2251939518', 'https://openalex.org/W3033187248', 'https://openalex.org/W2971169274', 'https://openalex.org/W2964121744', 'https://openalex.org/W2952899695', 'https://openalex.org/W2616957565', 'https://openalex.org/W3152956381', 'https://openalex.org/W2950967261', 'https://openalex.org/W3214715529', 'https://openalex.org/W3166140588', 'https://openalex.org/W3174784402', 'https://openalex.org/W2058641082', 'https://openalex.org/W2949819354', 'https://openalex.org/W2751448157', 'https://openalex.org/W2798836595', 'https://openalex.org/W3028525609', 'https://openalex.org/W2732004306', 'https://openalex.org/W3176828726', 'https://openalex.org/W3119438769', 'https://openalex.org/W2126017757', 'https://openalex.org/W2888867175', 'https://openalex.org/W2117130368', 'https://openalex.org/W2964031251', 'https://openalex.org/W3098824823', 'https://openalex.org/W3104033643', 'https://openalex.org/W3000127803', 'https://openalex.org/W2963477238', 'https://openalex.org/W3118216348', 'https://openalex.org/W2965373594', 'https://openalex.org/W131533222', 'https://openalex.org/W2103972604', 'https://openalex.org/W3181384422', 'https://openalex.org/W2990704537', 'https://openalex.org/W2973727699', 'https://openalex.org/W2394882406', 'https://openalex.org/W3158697128', 'https://openalex.org/W2963341956', 'https://openalex.org/W2963846996', 'https://openalex.org/W2886067286', 'https://openalex.org/W3040573126', 'https://openalex.org/W2913946806', 'https://openalex.org/W3139080614', 'https://openalex.org/W2806120502', 'https://openalex.org/W2899063268', 'https://openalex.org/W2595741664', 'https://openalex.org/W2963310665']",2021-06-17
https://openalex.org/W4375958083,https://doi.org/10.48550/arxiv.2305.04160,X-LLM: Bootstrapping Advanced Large Language Models by Treating Multi-Modalities as Foreign Languages,"Large language models (LLMs) have demonstrated remarkable language abilities. GPT-4, based on advanced LLMs, exhibits extraordinary multimodal capabilities beyond previous visual language models. We attribute this to the use of more advanced LLMs compared with previous multimodal models. Unfortunately, the model architecture and training strategies of GPT-4 are unknown. To endow LLMs with multimodal capabilities, we propose X-LLM, which converts Multi-modalities (images, speech, videos) into foreign languages using X2L interfaces and inputs them into a large Language model (ChatGLM). Specifically, X-LLM aligns multiple frozen single-modal encoders and a frozen LLM using X2L interfaces, where ``X'' denotes multi-modalities such as image, speech, and videos, and ``L'' denotes languages. X-LLM's training consists of three stages: (1) Converting Multimodal Information: The first stage trains each X2L interface to align with its respective single-modal encoder separately to convert multimodal information into languages. (2) Aligning X2L representations with the LLM: single-modal encoders are aligned with the LLM through X2L interfaces independently. (3) Integrating multiple modalities: all single-modal encoders are aligned with the LLM through X2L interfaces to integrate multimodal capabilities into the LLM. Our experiments show that X-LLM demonstrates impressive multimodel chat abilities, sometimes exhibiting the behaviors of multimodal GPT-4 on unseen images/instructions, and yields a 84.5\% relative score compared with GPT-4 on a synthetic multimodal instruction-following dataset. And we also conduct quantitative tests on using LLM for ASR and multimodal ASR, hoping to promote the era of LLM-based speech recognition.",[],2023-05-07
https://openalex.org/W4361229539,https://doi.org/10.48550/arxiv.2303.16199,LLaMA-Adapter: Efficient Fine-tuning of Language Models with Zero-init Attention,"We present LLaMA-Adapter, a lightweight adaption method to efficiently fine-tune LLaMA into an instruction-following model. Using 52K self-instruct demonstrations, LLaMA-Adapter only introduces 1.2M learnable parameters upon the frozen LLaMA 7B model, and costs less than one hour for fine-tuning on 8 A100 GPUs. Specifically, we adopt a set of learnable adaption prompts, and prepend them to the word tokens at higher transformer layers. Then, a zero-initialized attention mechanism with zero gating is proposed, which adaptively injects the new instructional cues into LLaMA, while effectively preserves its pre-trained knowledge. With our efficient training, LLaMA-Adapter can generate high-quality responses, comparable to Alpaca with fully fine-tuned 7B parameters. Besides language commands, our approach can be simply extended to multi-modal instructions for learning image-conditioned LLaMA model, which achieves superior reasoning performance on ScienceQA and COCO Caption benchmarks. Furthermore, we also evaluate the zero-initialized attention mechanism for fine-tuning other pre-trained models (ViT, RoBERTa) on traditional vision and language tasks, demonstrating the superior generalization capacity of our approach. Code is released at https://github.com/OpenGVLab/LLaMA-Adapter.",[],2023-03-28
https://openalex.org/W4386384714,https://doi.org/10.48550/arxiv.2308.16692,SpeechTokenizer: Unified Speech Tokenizer for Speech Large Language Models,"Current speech large language models build upon discrete speech representations, which can be categorized into semantic tokens and acoustic tokens. However, existing speech tokens are not specifically designed for speech language modeling. To assess the suitability of speech tokens for building speech language models, we established the first benchmark, SLMTokBench. Our results indicate that neither semantic nor acoustic tokens are ideal for this purpose. Therefore, we propose SpeechTokenizer, a unified speech tokenizer for speech large language models. SpeechTokenizer adopts the Encoder-Decoder architecture with residual vector quantization (RVQ). Unifying semantic and acoustic tokens, SpeechTokenizer disentangles different aspects of speech information hierarchically across different RVQ layers. Furthermore, We construct a Unified Speech Language Model (USLM) leveraging SpeechTokenizer. Experiments show that SpeechTokenizer performs comparably to EnCodec in speech reconstruction and demonstrates strong performance on the SLMTokBench benchmark. Also, USLM outperforms VALL-E in zero-shot Text-to-Speech tasks. Code and models are available at https://github.com/ZhangXInFD/SpeechTokenizer/.",[],2023-08-31
https://openalex.org/W4377297670,https://doi.org/10.48550/arxiv.2305.11206,LIMA: Less Is More for Alignment,"Large language models are trained in two stages: (1) unsupervised pretraining from raw text, to learn general-purpose representations, and (2) large scale instruction tuning and reinforcement learning, to better align to end tasks and user preferences. We measure the relative importance of these two stages by training LIMA, a 65B parameter LLaMa language model fine-tuned with the standard supervised loss on only 1,000 carefully curated prompts and responses, without any reinforcement learning or human preference modeling. LIMA demonstrates remarkably strong performance, learning to follow specific response formats from only a handful of examples in the training data, including complex queries that range from planning trip itineraries to speculating about alternate history. Moreover, the model tends to generalize well to unseen tasks that did not appear in the training data. In a controlled human study, responses from LIMA are either equivalent or strictly preferred to GPT-4 in 43% of cases; this statistic is as high as 58% when compared to Bard and 65% versus DaVinci003, which was trained with human feedback. Taken together, these results strongly suggest that almost all knowledge in large language models is learned during pretraining, and only limited instruction tuning data is necessary to teach models to produce high quality output.",[],2023-05-18
https://openalex.org/W4323651091,https://doi.org/10.48550/arxiv.2303.03926,Speak Foreign Languages with Your Own Voice: Cross-Lingual Neural Codec Language Modeling,"We propose a cross-lingual neural codec language model, VALL-E X, for cross-lingual speech synthesis. Specifically, we extend VALL-E and train a multi-lingual conditional codec language model to predict the acoustic token sequences of the target language speech by using both the source language speech and the target language text as prompts. VALL-E X inherits strong in-context learning capabilities and can be applied for zero-shot cross-lingual text-to-speech synthesis and zero-shot speech-to-speech translation tasks. Experimental results show that it can generate high-quality speech in the target language via just one speech utterance in the source language as a prompt while preserving the unseen speaker's voice, emotion, and acoustic environment. Moreover, VALL-E X effectively alleviates the foreign accent problems, which can be controlled by a language ID. Audio samples are available at \url{https://aka.ms/vallex}.",[],2023-03-07
https://openalex.org/W4322718191,https://doi.org/10.48550/arxiv.2302.13971,LLaMA: Open and Efficient Foundation Language Models,"We introduce LLaMA, a collection of foundation language models ranging from 7B to 65B parameters. We train our models on trillions of tokens, and show that it is possible to train state-of-the-art models using publicly available datasets exclusively, without resorting to proprietary and inaccessible datasets. In particular, LLaMA-13B outperforms GPT-3 (175B) on most benchmarks, and LLaMA-65B is competitive with the best models, Chinchilla-70B and PaLM-540B. We release all our models to the research community.",[],2023-02-27
https://openalex.org/W2518172956,https://doi.org/10.1109/icme.2016.7552917,Phonetic posteriorgrams for many-to-one voice conversion without parallel data training,"This paper proposes a novel approach to voice conversion with non-parallel training data. The idea is to bridge between speakers by means of Phonetic PosteriorGrams (PPGs) obtained from a speaker-independent automatic speech recognition (SI-ASR) system. It is assumed that these PPGs can represent articulation of speech sounds in a speaker-normalized space and correspond to spoken content speaker-independently. The proposed approach first obtains PPGs of target speech. Then, a Deep Bidirectional Long Short-Term Memory based Recurrent Neural Network (DBLSTM) structure is used to model the relationships between the PPGs and acoustic features of the target speech. To convert arbitrary source speech, we obtain its PPGs from the same SI-ASR and feed them into the trained DBLSTM for generating converted speech. Our approach has two main advantages: 1) no parallel training data is required; 2) a trained model can be applied to any other source speaker for a fixed target speaker (i.e., many-to-one conversion). Experiments show that our approach performs equally well or better than state-of-the-art systems in both speech quality and speaker similarity.","['https://openalex.org/W6711872879', 'https://openalex.org/W2171019095', 'https://openalex.org/W2401207139', 'https://openalex.org/W2115055618', 'https://openalex.org/W2049686551', 'https://openalex.org/W2096980176', 'https://openalex.org/W6603838645', 'https://openalex.org/W6631362777', 'https://openalex.org/W6789826613', 'https://openalex.org/W6610843619', 'https://openalex.org/W6696767757', 'https://openalex.org/W6732251480', 'https://openalex.org/W2161476805', 'https://openalex.org/W1509691205', 'https://openalex.org/W2404109308', 'https://openalex.org/W2135832479', 'https://openalex.org/W2120605154', 'https://openalex.org/W2156142001', 'https://openalex.org/W2032130465', 'https://openalex.org/W2294351487', 'https://openalex.org/W95152782', 'https://openalex.org/W3127686677', 'https://openalex.org/W2395403337', 'https://openalex.org/W2577042574', 'https://openalex.org/W1524333225', 'https://openalex.org/W304834817']",2016-07-01
https://openalex.org/W2972659941,https://doi.org/10.21437/interspeech.2019-2663,One-Shot Voice Conversion by Separating Speaker and Content Representations with Instance Normalization,"Recently, voice conversion (VC) without parallel data has been successfully adapted to multi-target scenario in which a single model is trained to convert the input voice to many different speakers.However, such model suffers from the limitation that it can only convert the voice to the speakers in the training data, which narrows down the applicable scenario of VC.In this paper, we proposed a novel one-shot VC approach which is able to perform VC by only an example utterance from source and target speaker respectively, and the source and target speaker do not even need to be seen during training.This is achieved by disentangling speaker and content representations with instance normalization (IN).Objective and subjective evaluation shows that our model is able to generate the voice similar to target speaker.In addition to the performance measurement, we also demonstrate that this model is able to learn meaningful speaker representations without any supervision.","['https://openalex.org/W2887264325', 'https://openalex.org/W2603777577', 'https://openalex.org/W2518312472', 'https://openalex.org/W2608207374', 'https://openalex.org/W2910577860', 'https://openalex.org/W2972689158', 'https://openalex.org/W2892743843', 'https://openalex.org/W2794490148', 'https://openalex.org/W2502312327', 'https://openalex.org/W2396025094', 'https://openalex.org/W2962896155', 'https://openalex.org/W2885820941', 'https://openalex.org/W2907262790', 'https://openalex.org/W2547364378', 'https://openalex.org/W2507912506', 'https://openalex.org/W2808706139', 'https://openalex.org/W2127520494', 'https://openalex.org/W2651834199', 'https://openalex.org/W2902751174', 'https://openalex.org/W2805669069', 'https://openalex.org/W2476548250', 'https://openalex.org/W2527729766', 'https://openalex.org/W2666408839', 'https://openalex.org/W2000513720', 'https://openalex.org/W1959608418', 'https://openalex.org/W4295731579', 'https://openalex.org/W2972894903', 'https://openalex.org/W2963808252', 'https://openalex.org/W2963830550', 'https://openalex.org/W2086796102', 'https://openalex.org/W2057609679', 'https://openalex.org/W1509691205', 'https://openalex.org/W2963796886', 'https://openalex.org/W2774848319', 'https://openalex.org/W4320013936']",2019-09-13
https://openalex.org/W3197659778,https://doi.org/10.21437/interspeech.2021-283,VQMIVC: Vector Quantization and Mutual Information-Based Unsupervised Speech Representation Disentanglement for One-Shot Voice Conversion,"One-shot voice conversion (VC), which performs conversion across arbitrary speakers with only a single target-speaker utterance for reference, can be effectively achieved by speech representation disentanglement.Existing work generally ignores the correlation between different speech representations during training, which causes leakage of content information into the speaker representation and thus degrades VC performance.To alleviate this issue, we employ vector quantization (VQ) for content encoding and introduce mutual information (MI) as the correlation metric during training, to achieve proper disentanglement of content, speaker and pitch representations, by reducing their inter-dependencies in an unsupervised manner.Experimental results reflect the superiority of the proposed method in learning effective disentangled speech representations for retaining source linguistic content and intonation variations, while capturing target speaker characteristics.In doing so, the proposed approach achieves higher speech naturalness and speaker similarity than current state-of-the-art one-shot VC systems.Our code, pre-trained models and demo are available at https://github.com/Wendison/VQMIVC.","['https://openalex.org/W3015338123', 'https://openalex.org/W2962896155', 'https://openalex.org/W3015805741', 'https://openalex.org/W3097256596', 'https://openalex.org/W2152790380', 'https://openalex.org/W1522301498', 'https://openalex.org/W2124641009', 'https://openalex.org/W2105160541', 'https://openalex.org/W2972894903', 'https://openalex.org/W2973154337', 'https://openalex.org/W2963539064', 'https://openalex.org/W4297808394', 'https://openalex.org/W1862426464', 'https://openalex.org/W3034794073', 'https://openalex.org/W2803832867', 'https://openalex.org/W3126283728', 'https://openalex.org/W4235132546', 'https://openalex.org/W3125709657', 'https://openalex.org/W2979476256', 'https://openalex.org/W2527729766', 'https://openalex.org/W2774848319', 'https://openalex.org/W2888922217', 'https://openalex.org/W3096524539', 'https://openalex.org/W3163475957', 'https://openalex.org/W2945478979', 'https://openalex.org/W2972659941', 'https://openalex.org/W3095361818', 'https://openalex.org/W3015212100', 'https://openalex.org/W3036928441', 'https://openalex.org/W2576309025', 'https://openalex.org/W2120605154', 'https://openalex.org/W2166944917', 'https://openalex.org/W2518172956', 'https://openalex.org/W2973215447', 'https://openalex.org/W2161476805', 'https://openalex.org/W2787748320']",2021-08-27
https://openalex.org/W3198082505,https://doi.org/10.21437/interspeech.2021-1990,Adversarially Learning Disentangled Speech Representations for Robust Multi-Factor Voice Conversion,"Factorizing speech as disentangled speech representations is vital to achieve highly controllable style transfer in voice conversion (VC).Conventional speech representation learning methods in VC only factorize speech as speaker and content, lacking controllability on other prosody-related factors.State-ofthe-art speech representation learning methods for more speech factors are using primary disentangle algorithms such as random resampling and ad-hoc bottleneck layer size adjustment, which however is hard to ensure robust speech representation disentanglement.To increase the robustness of highly controllable style transfer on multiple factors in VC, we propose a disentangled speech representation learning framework based on adversarial learning.Four speech representations characterizing content, timbre, rhythm and pitch are extracted, and further disentangled by an adversarial Mask-And-Predict (MAP) network inspired by BERT.The adversarial network is used to minimize the correlations between the speech representations, by randomly masking and predicting one of the representations from the others.Experimental results show that the proposed framework significantly improves the robustness of VC on multiple factors by increasing the speech quality MOS from 2.79 to 3.30 and decreasing the MCD from 3.89 to 3.58.","['https://openalex.org/W3168542456', 'https://openalex.org/W2889061305', 'https://openalex.org/W3134921434', 'https://openalex.org/W2972659941', 'https://openalex.org/W4299812321', 'https://openalex.org/W2963226019', 'https://openalex.org/W2896457183', 'https://openalex.org/W2963830550', 'https://openalex.org/W3034303964', 'https://openalex.org/W2970006822', 'https://openalex.org/W3163475957', 'https://openalex.org/W3034794073', 'https://openalex.org/W4287690582', 'https://openalex.org/W3199364993', 'https://openalex.org/W2794490148', 'https://openalex.org/W2527729766', 'https://openalex.org/W2982223350', 'https://openalex.org/W3095035471', 'https://openalex.org/W2937020545', 'https://openalex.org/W1731081199', 'https://openalex.org/W3197411683', 'https://openalex.org/W3126283728', 'https://openalex.org/W3133667170', 'https://openalex.org/W3030987249', 'https://openalex.org/W3201143670', 'https://openalex.org/W3095361818', 'https://openalex.org/W4295731579', 'https://openalex.org/W4232345992', 'https://openalex.org/W3015645837', 'https://openalex.org/W4287614031', 'https://openalex.org/W2947445680', 'https://openalex.org/W3015434413', 'https://openalex.org/W3015699566', 'https://openalex.org/W2972921407']",2021-08-27
https://openalex.org/W4221141917,https://doi.org/10.1109/icassp43922.2022.9746369,Avqvc: One-Shot Voice Conversion By Vector Quantization With Applying Contrastive Learning,"Voice Conversion(VC) refers to changing the timbre of a speech while retaining the discourse content. Recently, many works have focused on disentangle-based learning techniques to separate the timbre and the linguistic content information from a speech signal. Once successful, voice conversion will be feasible and straightforward. This paper proposed a novel one-shot voice conversion framework based on vector quantization voice conversion (VQVC) and AutoVC, called AVQVC. A new training method is applied to VQVC to separate content and timbre information from speech more effectively. The result shows that this approach has better performance than VQVC in separating content and timbre to improve the sound quality of generated speech.","['https://openalex.org/W3100270690', 'https://openalex.org/W3015434413', 'https://openalex.org/W3096524539', 'https://openalex.org/W2963425185', 'https://openalex.org/W2842511635', 'https://openalex.org/W6769196770', 'https://openalex.org/W6797314988', 'https://openalex.org/W2972849140', 'https://openalex.org/W3095361818', 'https://openalex.org/W2962788625', 'https://openalex.org/W2889061305', 'https://openalex.org/W2532494225', 'https://openalex.org/W2962896155', 'https://openalex.org/W2963539064', 'https://openalex.org/W6756504009', 'https://openalex.org/W2973216307', 'https://openalex.org/W2156142001', 'https://openalex.org/W2972667718', 'https://openalex.org/W6752888775', 'https://openalex.org/W6727697161', 'https://openalex.org/W4210282084', 'https://openalex.org/W3160950295', 'https://openalex.org/W2979476256', 'https://openalex.org/W4297808394', 'https://openalex.org/W3102628737', 'https://openalex.org/W3175212059', 'https://openalex.org/W4320013936', 'https://openalex.org/W2527729766', 'https://openalex.org/W2902070858', 'https://openalex.org/W2808706139']",2022-04-27
https://openalex.org/W4390075359,https://doi.org/10.1162/tacl_a_00618,"Speak, Read and Prompt: High-Fidelity Text-to-Speech with Minimal Supervision","Abstract We introduce SPEAR-TTS, a multi-speaker text-to-speech (TTS) system that can be trained with minimal supervision. By combining two types of discrete speech representations, we cast TTS as a composition of two sequence-to-sequence tasks: from text to high-level semantic tokens (akin to “reading”) and from semantic tokens to low-level acoustic tokens (“speaking”). Decoupling these two tasks enables training of the “speaking” module using abundant audio-only data, and unlocks the highly efficient combination of pretraining and backtranslation to reduce the need for parallel data when training the “reading” component. To control the speaker identity, we adopt example prompting, which allows SPEAR-TTS to generalize to unseen speakers using only a short sample of 3 seconds, without any explicit speaker representation or speaker labels. Our experiments demonstrate that SPEAR-TTS achieves a character error rate that is competitive with state-of-the-art methods using only 15 minutes of parallel data, while matching ground-truth speech in naturalness and acoustic quality.","['https://openalex.org/W6849105126', 'https://openalex.org/W3205644108', 'https://openalex.org/W4381786045', 'https://openalex.org/W6778883912', 'https://openalex.org/W6805710207', 'https://openalex.org/W3209984917', 'https://openalex.org/W4226033575', 'https://openalex.org/W6800767084', 'https://openalex.org/W2889326796', 'https://openalex.org/W6638667902', 'https://openalex.org/W6917585676', 'https://openalex.org/W4380874786', 'https://openalex.org/W2995181338', 'https://openalex.org/W3198217962', 'https://openalex.org/W6804184754', 'https://openalex.org/W6849956205', 'https://openalex.org/W6783867762', 'https://openalex.org/W4385574033', 'https://openalex.org/W6790356757', 'https://openalex.org/W4296068981', 'https://openalex.org/W3034999214', 'https://openalex.org/W6810406373', 'https://openalex.org/W3015419784', 'https://openalex.org/W4307680525', 'https://openalex.org/W1494198834', 'https://openalex.org/W4285182272', 'https://openalex.org/W3140429000', 'https://openalex.org/W6769627184', 'https://openalex.org/W3161480375', 'https://openalex.org/W3033411150', 'https://openalex.org/W3016181583', 'https://openalex.org/W6853998256', 'https://openalex.org/W2963216553', 'https://openalex.org/W6751104502', 'https://openalex.org/W2964243274', 'https://openalex.org/W6677920722', 'https://openalex.org/W6739901393', 'https://openalex.org/W6848735303', 'https://openalex.org/W3215615641', 'https://openalex.org/W2972359262', 'https://openalex.org/W4394671563', 'https://openalex.org/W4288089799', 'https://openalex.org/W4381827575', 'https://openalex.org/W4296070453', 'https://openalex.org/W4320459320', 'https://openalex.org/W4318351475', 'https://openalex.org/W4385245566', 'https://openalex.org/W3198123200', 'https://openalex.org/W3215895588', 'https://openalex.org/W3092028330', 'https://openalex.org/W4313679638', 'https://openalex.org/W4292779060']",2023-01-01
https://openalex.org/W4385245566,https://doi.org/10.4230/lipics.itp.2023.19,MizAR 60 for Mizar 50,"As a present to Mizar on its 50th anniversary, we develop an AI/TP system that automatically proves about 60% of the Mizar theorems in the hammer setting. We also automatically prove 75% of the Mizar theorems when the automated provers are helped by using only the premises used in the human-written Mizar proofs. We describe the methods and large-scale experiments leading to these results. This includes in particular the E and Vampire provers, their ENIGMA and Deepire learning modifications, a number of learning-based premise selection methods, and the incremental loop that interleaves growing a corpus of millions of ATP proofs with training increasingly strong AI/TP systems on them. We also present a selection of Mizar problems that were proved automatically.",[],2023-01-01
https://openalex.org/W4285294723,https://doi.org/10.18653/v1/2022.acl-long.26,GLM: General Language Model Pretraining with Autoregressive Blank Infilling,"Zhengxiao Du, Yujie Qian, Xiao Liu, Ming Ding, Jiezhong Qiu, Zhilin Yang, Jie Tang. Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). 2022.","['https://openalex.org/W2996428491', 'https://openalex.org/W3081168214', 'https://openalex.org/W2973727699', 'https://openalex.org/W2963015836', 'https://openalex.org/W2962965405', 'https://openalex.org/W2805206884', 'https://openalex.org/W3034999214', 'https://openalex.org/W4394651747', 'https://openalex.org/W3024131638', 'https://openalex.org/W2970419734', 'https://openalex.org/W2923014074', 'https://openalex.org/W3172642864', 'https://openalex.org/W2133459682', 'https://openalex.org/W2943552823', 'https://openalex.org/W2462831000', 'https://openalex.org/W1566289585', 'https://openalex.org/W2606974598', 'https://openalex.org/W2963323070', 'https://openalex.org/W2963846996', 'https://openalex.org/W3103753836', 'https://openalex.org/W3011411500', 'https://openalex.org/W2963600562', 'https://openalex.org/W2945260553', 'https://openalex.org/W2154652894', 'https://openalex.org/W3106340866', 'https://openalex.org/W4297798436', 'https://openalex.org/W2130158090', 'https://openalex.org/W2965373594', 'https://openalex.org/W3007759824', 'https://openalex.org/W2888482885', 'https://openalex.org/W3121525843', 'https://openalex.org/W3106339673', 'https://openalex.org/W2963748441', 'https://openalex.org/W3153427360', 'https://openalex.org/W4313908941', 'https://openalex.org/W2970597249', 'https://openalex.org/W4288089799', 'https://openalex.org/W2251939518', 'https://openalex.org/W2952468927', 'https://openalex.org/W2962717047', 'https://openalex.org/W3122890974', 'https://openalex.org/W4292779060', 'https://openalex.org/W2996264288', 'https://openalex.org/W2896457183', 'https://openalex.org/W3094045953', 'https://openalex.org/W4287824654']",2022-01-01
https://openalex.org/W2972359262,https://doi.org/10.21437/interspeech.2019-2441,LibriTTS: A Corpus Derived from LibriSpeech for Text-to-Speech,"This paper introduces a new speech corpus called ""LibriTTS"" designed for text-to-speech use.It is derived from the original audio and text materials of the LibriSpeech corpus, which has been used for training and evaluating automatic speech recognition systems.The new corpus inherits desired properties of the LibriSpeech corpus while addressing a number of issues which make LibriSpeech less than ideal for text-to-speech work.The released corpus consists of 585 hours of speech data at 24kHz sampling rate from 2,456 speakers and the corresponding texts.Experimental results show that neural end-to-end TTS models trained from the LibriTTS corpus achieved above 4.0 in mean opinion scores in naturalness in five out of six evaluation speakers.","['https://openalex.org/W2892620417', 'https://openalex.org/W4232345992', 'https://openalex.org/W2794490148', 'https://openalex.org/W2182214061', 'https://openalex.org/W2808706139', 'https://openalex.org/W2129142580', 'https://openalex.org/W4298174729', 'https://openalex.org/W2105961775', 'https://openalex.org/W2103085228', 'https://openalex.org/W4294619240', 'https://openalex.org/W2963827314', 'https://openalex.org/W2604184139', 'https://openalex.org/W1597121597', 'https://openalex.org/W1522301498', 'https://openalex.org/W2964243274', 'https://openalex.org/W2788357188', 'https://openalex.org/W2736900972', 'https://openalex.org/W2901997113', 'https://openalex.org/W2885800352', 'https://openalex.org/W2033256038', 'https://openalex.org/W4289383906', 'https://openalex.org/W3177989406', 'https://openalex.org/W2747681982', 'https://openalex.org/W4298580827', 'https://openalex.org/W1574170747', 'https://openalex.org/W2892140764', 'https://openalex.org/W2293634267', 'https://openalex.org/W4298240696', 'https://openalex.org/W4295731579', 'https://openalex.org/W1494198834', 'https://openalex.org/W2889028433']",2019-09-13
https://openalex.org/W3024869864,https://doi.org/10.21437/interspeech.2020-2650,"ECAPA-TDNN: Emphasized Channel Attention, Propagation and Aggregation in TDNN Based Speaker Verification","Current speaker verification techniques rely on a neural network to extract speaker representations. The successful x-vector architecture is a Time Delay Neural Network (TDNN) that applies statistics pooling to project variable-length utterances into fixed-length speaker characterizing embeddings. In this paper, we propose multiple enhancements to this architecture based on recent trends in the related fields of face verification and computer vision. Firstly, the initial frame layers can be restructured into 1-dimensional Res2Net modules with impactful skip connections. Similarly to SE-ResNet, we introduce Squeeze-and-Excitation blocks in these modules to explicitly model channel interdependencies. The SE block expands the temporal context of the frame layer by rescaling the channels according to global properties of the recording. Secondly, neural networks are known to learn hierarchical features, with each layer operating on a different level of complexity. To leverage this complementary information, we aggregate and propagate features of different hierarchical levels. Finally, we improve the statistics pooling module with channel-dependent frame attention. This enables the network to focus on different subsets of frames during each of the channel’s statistics estimation. The proposed ECAPA-TDNN architecture significantly outperforms state-of-the-art TDNN based systems on the VoxCeleb test sets and the 2019 VoxCeleb Speaker Recognition Challenge.","['https://openalex.org/W2696967604', 'https://openalex.org/W4288091954', 'https://openalex.org/W2808631503', 'https://openalex.org/W2888867175', 'https://openalex.org/W2936774411', 'https://openalex.org/W3015368919', 'https://openalex.org/W2972441390', 'https://openalex.org/W4309845474', 'https://openalex.org/W2713263280', 'https://openalex.org/W2964054038', 'https://openalex.org/W2981461916', 'https://openalex.org/W2219249508', 'https://openalex.org/W2889519245', 'https://openalex.org/W3027908062', 'https://openalex.org/W2938358845', 'https://openalex.org/W2991951409', 'https://openalex.org/W1589137271', 'https://openalex.org/W2794506738', 'https://openalex.org/W2726515241', 'https://openalex.org/W2951534110', 'https://openalex.org/W1836465849', 'https://openalex.org/W2969985801', 'https://openalex.org/W2963420686', 'https://openalex.org/W2972552635', 'https://openalex.org/W3042801391', 'https://openalex.org/W2747165665', 'https://openalex.org/W4300685083', 'https://openalex.org/W2928165649', 'https://openalex.org/W2890964092', 'https://openalex.org/W3010925296', 'https://openalex.org/W125553504', 'https://openalex.org/W2964121744', 'https://openalex.org/W2194775991', 'https://openalex.org/W2157161740', 'https://openalex.org/W1522301498', 'https://openalex.org/W2996132597']",2020-10-25
https://openalex.org/W3152740956,https://doi.org/10.18653/v1/2021.emnlp-main.755,Case-based Reasoning for Natural Language Queries over Knowledge Bases,"Rajarshi Das, Manzil Zaheer, Dung Thai, Ameya Godbole, Ethan Perez, Jay Yoon Lee, Lizhen Tan, Lazaros Polymenakos, Andrew McCallum. Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing. 2021.","['https://openalex.org/W2890431379', 'https://openalex.org/W3093630174', 'https://openalex.org/W2951352467', 'https://openalex.org/W4253990296', 'https://openalex.org/W2751448157', 'https://openalex.org/W2940154139', 'https://openalex.org/W3100436891', 'https://openalex.org/W3102935281', 'https://openalex.org/W1989549063', 'https://openalex.org/W2094728533', 'https://openalex.org/W2798463315', 'https://openalex.org/W4288025992', 'https://openalex.org/W3043281046', 'https://openalex.org/W4287704453', 'https://openalex.org/W2963899988', 'https://openalex.org/W4287671158', 'https://openalex.org/W2022591466', 'https://openalex.org/W2912624765', 'https://openalex.org/W2963794306', 'https://openalex.org/W4297733535', 'https://openalex.org/W2127795553', 'https://openalex.org/W2903433945', 'https://openalex.org/W2949434543', 'https://openalex.org/W1525136198', 'https://openalex.org/W3103802158', 'https://openalex.org/W3083835029', 'https://openalex.org/W580074167', 'https://openalex.org/W3043172396', 'https://openalex.org/W3153094109', 'https://openalex.org/W2952842591', 'https://openalex.org/W2794443436', 'https://openalex.org/W3103801878', 'https://openalex.org/W2964118342', 'https://openalex.org/W2963829526', 'https://openalex.org/W2546950329', 'https://openalex.org/W2887970879', 'https://openalex.org/W24106767', 'https://openalex.org/W2949134692', 'https://openalex.org/W2077735941', 'https://openalex.org/W2163274265', 'https://openalex.org/W2963858333', 'https://openalex.org/W2127426251', 'https://openalex.org/W2126385963', 'https://openalex.org/W2251960799', 'https://openalex.org/W2971155257', 'https://openalex.org/W2560647685', 'https://openalex.org/W3034456481', 'https://openalex.org/W3082274269', 'https://openalex.org/W2982399380', 'https://openalex.org/W4288087322', 'https://openalex.org/W2966747703', 'https://openalex.org/W2890397703', 'https://openalex.org/W2252136820', 'https://openalex.org/W2963545046', 'https://openalex.org/W2805003733', 'https://openalex.org/W2964120615', 'https://openalex.org/W1811976956', 'https://openalex.org/W2953014395', 'https://openalex.org/W4288089799', 'https://openalex.org/W2962985038', 'https://openalex.org/W4289494028', 'https://openalex.org/W1554190925', 'https://openalex.org/W3034999214', 'https://openalex.org/W2919115771', 'https://openalex.org/W3099700870', 'https://openalex.org/W2788330850', 'https://openalex.org/W2995154514', 'https://openalex.org/W1551773846', 'https://openalex.org/W2885421725', 'https://openalex.org/W4287649493', 'https://openalex.org/W2798542795', 'https://openalex.org/W2566402689', 'https://openalex.org/W3029732686', 'https://openalex.org/W3104616515', 'https://openalex.org/W3037105888', 'https://openalex.org/W2963341956', 'https://openalex.org/W2282821441', 'https://openalex.org/W2965373594', 'https://openalex.org/W4288601872', 'https://openalex.org/W3027879771', 'https://openalex.org/W1682403713', 'https://openalex.org/W2963813662', 'https://openalex.org/W3126425262', 'https://openalex.org/W2171278097', 'https://openalex.org/W1525859397', 'https://openalex.org/W2950632879', 'https://openalex.org/W2951200021', 'https://openalex.org/W3016828850', 'https://openalex.org/W2964022985', 'https://openalex.org/W2250184916', 'https://openalex.org/W2511149293', 'https://openalex.org/W3035172316', 'https://openalex.org/W2740646481', 'https://openalex.org/W3034881347', 'https://openalex.org/W2996132992', 'https://openalex.org/W205829674', 'https://openalex.org/W2909137510', 'https://openalex.org/W3047026265', 'https://openalex.org/W3034273250', 'https://openalex.org/W3034862985', 'https://openalex.org/W3045733172', 'https://openalex.org/W3007672467', 'https://openalex.org/W3173274550', 'https://openalex.org/W2890961898', 'https://openalex.org/W2949428332', 'https://openalex.org/W4287898313']",2021-01-01
https://openalex.org/W3197358873,https://doi.org/10.21437/asvspoof.2021-8,ASVspoof 2021: accelerating progress in spoofed and deepfake speech detection,"ASVspoof 2021 is the forth edition in the series of bi-annual challenges which aim to promote the study of spoofing and the design of countermeasures to protect automatic speaker verification systems from manipulation. In addition to a continued focus upon logical and physical access tasks in which there are a number of advances compared to previous editions, ASVspoof 2021 introduces a new task involving deepfake speech detection. This paper describes all three tasks, the new databases for each of them, the evaluation metrics, four challenge baselines, the evaluation platform and a summary of challenge results. Despite the introduction of channel and compression variability which compound the difficulty, results for the logical access and deepfake tasks are close to those from previous ASVspoof editions. Results for the physical access task show the difficulty in detecting attacks in real, variable physical spaces. With ASVspoof 2021 being the first edition for which participants were not provided with any matched training or development data and with this reflecting real conditions in which the nature of spoofed and deepfake speech can never be predicated with confidence, the results are extremely encouraging and demonstrate the substantial progress made in the field in recent years.","['https://openalex.org/W2799053639', 'https://openalex.org/W2295634712', 'https://openalex.org/W3143280635', 'https://openalex.org/W2807325376', 'https://openalex.org/W2936802426', 'https://openalex.org/W3198329097', 'https://openalex.org/W2745896134', 'https://openalex.org/W3041816239', 'https://openalex.org/W2019915615', 'https://openalex.org/W2972414786', 'https://openalex.org/W3196723471', 'https://openalex.org/W1524333225', 'https://openalex.org/W3153893842', 'https://openalex.org/W2121812409', 'https://openalex.org/W2176804518', 'https://openalex.org/W2972811785', 'https://openalex.org/W2590129515', 'https://openalex.org/W3096084197', 'https://openalex.org/W4287262583', 'https://openalex.org/W2890964092', 'https://openalex.org/W3131786367', 'https://openalex.org/W2588445447', 'https://openalex.org/W2726515241', 'https://openalex.org/W2808631503', 'https://openalex.org/W3026777299']",2021-09-10
https://openalex.org/W4376632433,https://doi.org/10.48550/arxiv.2305.07185,MEGABYTE: Predicting Million-byte Sequences with Multiscale Transformers,"Autoregressive transformers are spectacular models for short sequences but scale poorly to long sequences such as high-resolution images, podcasts, code, or books. We proposed Megabyte, a multi-scale decoder architecture that enables end-to-end differentiable modeling of sequences of over one million bytes. Megabyte segments sequences into patches and uses a local submodel within patches and a global model between patches. This enables sub-quadratic self-attention, much larger feedforward layers for the same compute, and improved parallelism during decoding -- unlocking better performance at reduced cost for both training and generation. Extensive experiments show that Megabyte allows byte-level models to perform competitively with subword models on long context language modeling, achieve state-of-the-art density estimation on ImageNet, and model audio from raw files. Together, these results establish the viability of tokenization-free autoregressive sequence modeling at scale.",[],2023-05-12
https://openalex.org/W4297808394,https://doi.org/10.48550/arxiv.1807.03748,Representation Learning with Contrastive Predictive Coding,"While supervised learning has enabled great progress in many applications, unsupervised learning has not seen such widespread adoption, and remains an important and challenging endeavor for artificial intelligence. In this work, we propose a universal unsupervised learning approach to extract useful representations from high-dimensional data, which we call Contrastive Predictive Coding. The key insight of our model is to learn such representations by predicting the future in latent space by using powerful autoregressive models. We use a probabilistic contrastive loss which induces the latent space to capture information that is maximally useful to predict future samples. It also makes the model tractable by using negative sampling. While most prior work has focused on evaluating representations for a particular modality, we demonstrate that our approach is able to learn useful representations achieving strong performance on four distinct domains: speech, images, text and reinforcement learning in 3D environments.",[],2018-07-10
https://openalex.org/W4376632463,https://doi.org/10.48550/arxiv.2305.07204,Multi-level Temporal-channel Speaker Retrieval for Zero-shot Voice Conversion,"Zero-shot voice conversion (VC) converts source speech into the voice of any desired speaker using only one utterance of the speaker without requiring additional model updates. Typical methods use a speaker representation from a pre-trained speaker verification (SV) model or learn speaker representation during VC training to achieve zero-shot VC. However, existing speaker modeling methods overlook the variation of speaker information richness in temporal and frequency channel dimensions of speech. This insufficient speaker modeling hampers the ability of the VC model to accurately represent unseen speakers who are not in the training dataset. In this study, we present a robust zero-shot VC model with multi-level temporal-channel retrieval, referred to as MTCR-VC. Specifically, to flexibly adapt to the dynamic-variant speaker characteristic in the temporal and channel axis of the speech, we propose a novel fine-grained speaker modeling method, called temporal-channel retrieval (TCR), to find out when and where speaker information appears in speech. It retrieves variable-length speaker representation from both temporal and channel dimensions under the guidance of a pre-trained SV model. Besides, inspired by the hierarchical process of human speech production, the MTCR speaker module stacks several TCR blocks to extract speaker representations from multi-granularity levels. Furthermore, to achieve better speech disentanglement and reconstruction, we introduce a cycle-based training strategy to simulate zero-shot inference recurrently. We adopt perpetual constraints on three aspects, including content, style, and speaker, to drive this process. Experiments demonstrate that MTCR-VC is superior to the previous zero-shot VC methods in modeling speaker timbre while maintaining good speech naturalness.",[],2023-05-12
https://openalex.org/W1572989473,,The EMIME Bilingual Database,"This paper describes the collection of a bilingual database of Finnish/English and German/English data. In addition, the accents of the talkers in the database have been rated. English, German and Finnish listeners assessed the English, German and Finnish talkers ’ degree of foreign accent in English. Native English listeners showed higher inter-listener agreement than non-native listeners. Further analyses showed that non-native listeners judged Finnish and German female talkers to be significantly less accented than do English listeners. German males are judged less accented by Finnish listeners than they are by English and German listeners and there is no difference between listeners as to how they judge the accent of Finnish males. Finally, all English talkers are judged more accented by non-native listeners than they are by native English listeners. Index Terms: evaluation, accent rating, cross-lingual","['https://openalex.org/W2104468248', 'https://openalex.org/W2141403362', 'https://openalex.org/W22168010', 'https://openalex.org/W2402261002', 'https://openalex.org/W2024490156', 'https://openalex.org/W3019319563', 'https://openalex.org/W1967890362', 'https://openalex.org/W2031438579', 'https://openalex.org/W2917438849', 'https://openalex.org/W2027163595', 'https://openalex.org/W2119929864']",2010-01-01
https://openalex.org/W4384648639,https://doi.org/10.48550/arxiv.2307.08691,FlashAttention-2: Faster Attention with Better Parallelism and Work Partitioning,"Scaling Transformers to longer sequence lengths has been a major problem in the last several years, promising to improve performance in language modeling and high-resolution image understanding, as well as to unlock new applications in code, audio, and video generation. The attention layer is the main bottleneck in scaling to longer sequences, as its runtime and memory increase quadratically in the sequence length. FlashAttention exploits the asymmetric GPU memory hierarchy to bring significant memory saving (linear instead of quadratic) and runtime speedup (2-4$\times$ compared to optimized baselines), with no approximation. However, FlashAttention is still not nearly as fast as optimized matrix-multiply (GEMM) operations, reaching only 25-40\% of the theoretical maximum FLOPs/s. We observe that the inefficiency is due to suboptimal work partitioning between different thread blocks and warps on the GPU, causing either low-occupancy or unnecessary shared memory reads/writes. We propose FlashAttention-2, with better work partitioning to address these issues. In particular, we (1) tweak the algorithm to reduce the number of non-matmul FLOPs (2) parallelize the attention computation, even for a single head, across different thread blocks to increase occupancy, and (3) within each thread block, distribute the work between warps to reduce communication through shared memory. These yield around 2$\times$ speedup compared to FlashAttention, reaching 50-73\% of the theoretical maximum FLOPs/s on A100 and getting close to the efficiency of GEMM operations. We empirically validate that when used end-to-end to train GPT-style models, FlashAttention-2 reaches training speed of up to 225 TFLOPs/s per A100 GPU (72\% model FLOPs utilization).",[],2023-07-17
https://openalex.org/W1915251500,https://doi.org/10.48550/arxiv.1503.03535,On Using Monolingual Corpora in Neural Machine Translation,"Recent work on end-to-end neural network-based architectures for machine translation has shown promising results for En-Fr and En-De translation. Arguably, one of the major factors behind this success has been the availability of high quality parallel corpora. In this work, we investigate how to leverage abundant monolingual corpora for neural machine translation. Compared to a phrase-based and hierarchical baseline, we obtain up to $1.96$ BLEU improvement on the low-resource language pair Turkish-English, and $1.59$ BLEU on the focused domain task of Chinese-English chat messages. While our method was initially targeted toward such tasks with less parallel data, we show that it also extends to high resource languages such as Cs-En and De-En where we obtain an improvement of $0.39$ and $0.47$ BLEU scores over the neural machine translation baselines, respectively.","['https://openalex.org/W2949888546', 'https://openalex.org/W1904365287', 'https://openalex.org/W2152263452', 'https://openalex.org/W1916559533', 'https://openalex.org/W1915022094', 'https://openalex.org/W2153653739', 'https://openalex.org/W3037950864', 'https://openalex.org/W2964335273', 'https://openalex.org/W2108677974', 'https://openalex.org/W2474824677', 'https://openalex.org/W3202115086', 'https://openalex.org/W2950635152', 'https://openalex.org/W2100664567', 'https://openalex.org/W1753482797', 'https://openalex.org/W2184135559', 'https://openalex.org/W2251222643', 'https://openalex.org/W2964308564', 'https://openalex.org/W2064675550', 'https://openalex.org/W1522301498', 'https://openalex.org/W2950580142', 'https://openalex.org/W1606347560', 'https://openalex.org/W1970689298', 'https://openalex.org/W6908809', 'https://openalex.org/W2251682575', 'https://openalex.org/W2124807415', 'https://openalex.org/W2131774270']",2015-03-11
https://openalex.org/W3202267900,https://doi.org/10.48550/arxiv.2110.02500,MediumVC: Any-to-any voice conversion using synthetic specific-speaker speeches as intermedium features,"To realize any-to-any (A2A) voice conversion (VC), most methods are to perform symmetric self-supervised reconstruction tasks (Xi to Xi), which usually results in inefficient performances due to inadequate feature decoupling, especially for unseen speakers. We propose a two-stage reconstruction task (Xi to Yi to Xi) using synthetic specific-speaker speeches as intermedium features, where A2A VC is divided into two stages: any-to-one (A2O) and one-to-Any (O2A). In the A2O stage, we propose a new A2O method: SingleVC, by employing a noval data augment strategy(pitch-shifted and duration-remained, PSDR) to accomplish Xi to Yi. In the O2A stage, MediumVC is proposed based on pre-trained SingleVC to conduct Yi to Xi. Through such asymmetrical reconstruction tasks (Xi to Yi in SingleVC and Yi to Xi in MediumVC), the models are to capture robust disentangled features purposefully. Experiments indicate MediumVC can enhance the similarity of converted speeches while maintaining a high degree of naturalness.","['https://openalex.org/W2114659828', 'https://openalex.org/W3082130377', 'https://openalex.org/W31448757', 'https://openalex.org/W2949281321', 'https://openalex.org/W2972667718', 'https://openalex.org/W2963685250', 'https://openalex.org/W1836465849', 'https://openalex.org/W2963539064', 'https://openalex.org/W2972659941', 'https://openalex.org/W3099782249', 'https://openalex.org/W2962788625', 'https://openalex.org/W2937579788', 'https://openalex.org/W3095936335', 'https://openalex.org/W1494198834', 'https://openalex.org/W3098403858', 'https://openalex.org/W3161627112', 'https://openalex.org/W2902070858', 'https://openalex.org/W2527729766', 'https://openalex.org/W2502312327']",2021-10-06
https://openalex.org/W3154308281,https://doi.org/10.48550/arxiv.2104.04487,Language model fusion for streaming end to end speech recognition,"Streaming processing of speech audio is required for many contemporary practical speech recognition tasks. Even with the large corpora of manually transcribed speech data available today, it is impossible for such corpora to cover adequately the long tail of linguistic content that's important for tasks such as open-ended dictation and voice search. We seek to address both the streaming and the tail recognition challenges by using a language model (LM) trained on unpaired text data to enhance the end-to-end (E2E) model. We extend shallow fusion and cold fusion approaches to streaming Recurrent Neural Network Transducer (RNNT), and also propose two new competitive fusion approaches that further enhance the RNNT architecture. Our results on multiple languages with varying training set sizes show that these fusion methods improve streaming RNNT performance through introducing extra linguistic features. Cold fusion works consistently better on streaming RNNT with up to a 8.5% WER improvement.","['https://openalex.org/W2327501763', 'https://openalex.org/W2748679025', 'https://openalex.org/W47568227', 'https://openalex.org/W3015686596', 'https://openalex.org/W2939111082', 'https://openalex.org/W2121879602', 'https://openalex.org/W2883416004', 'https://openalex.org/W2962760690', 'https://openalex.org/W2955856352', 'https://openalex.org/W3028075677', 'https://openalex.org/W2617258110', 'https://openalex.org/W2747467128', 'https://openalex.org/W2773781902', 'https://openalex.org/W2577366047', 'https://openalex.org/W1828163288']",2021-04-09
https://openalex.org/W4281492411,https://doi.org/10.1109/jstsp.2022.3207050,Self-Supervised Speech Representation Learning: A Review,"Although supervised deep learning has revolutionized speech and audio\nprocessing, it has necessitated the building of specialist models for\nindividual tasks and application scenarios. It is likewise difficult to apply\nthis to dialects and languages for which only limited labeled data is\navailable. Self-supervised representation learning methods promise a single\nuniversal model that would benefit a wide variety of tasks and domains. Such\nmethods have shown success in natural language processing and computer vision\ndomains, achieving new levels of performance while reducing the number of\nlabels required for many downstream scenarios. Speech representation learning\nis experiencing similar progress in three main categories: generative,\ncontrastive, and predictive methods. Other approaches rely on multi-modal data\nfor pre-training, mixing text or visual data streams with speech. Although\nself-supervised speech representation is still a nascent research area, it is\nclosely related to acoustic word embedding and learning with zero lexical\nresources, both of which have seen active research for many years. This review\npresents approaches for self-supervised speech representation learning and\ntheir connection to other research areas. Since many current methods focus\nsolely on automatic speech recognition as a downstream task, we review recent\nefforts on benchmarking learned representations to extend the application\nbeyond speech recognition.\n","['https://openalex.org/W2919115771', 'https://openalex.org/W2160815625', 'https://openalex.org/W811578723', 'https://openalex.org/W1555037511', 'https://openalex.org/W1975113979', 'https://openalex.org/W2110073835', 'https://openalex.org/W2124537004', 'https://openalex.org/W6683825394', 'https://openalex.org/W2163922914', 'https://openalex.org/W1901616594', 'https://openalex.org/W4244017338', 'https://openalex.org/W4239390603', 'https://openalex.org/W6675401909', 'https://openalex.org/W1902027874', 'https://openalex.org/W2100495367', 'https://openalex.org/W6800751262', 'https://openalex.org/W3207924272', 'https://openalex.org/W3035725276', 'https://openalex.org/W6773996589', 'https://openalex.org/W3185341429', 'https://openalex.org/W6784023748', 'https://openalex.org/W3011574394', 'https://openalex.org/W3023371261', 'https://openalex.org/W6811170316', 'https://openalex.org/W6772230580', 'https://openalex.org/W1703050006', 'https://openalex.org/W2048648518', 'https://openalex.org/W2100969003', 'https://openalex.org/W1979447841', 'https://openalex.org/W1877570817', 'https://openalex.org/W6680522077', 'https://openalex.org/W1487784522', 'https://openalex.org/W2155230809', 'https://openalex.org/W2150769028', 'https://openalex.org/W2408021097', 'https://openalex.org/W6681096077', 'https://openalex.org/W6680106237', 'https://openalex.org/W2145889472', 'https://openalex.org/W2113606819', 'https://openalex.org/W2067474491', 'https://openalex.org/W6736430770', 'https://openalex.org/W2116064496', 'https://openalex.org/W2923014074', 'https://openalex.org/W3197580070', 'https://openalex.org/W2326925005', 'https://openalex.org/W2883725317', 'https://openalex.org/W343636949', 'https://openalex.org/W6640963894', 'https://openalex.org/W6639732818', 'https://openalex.org/W3217536461', 'https://openalex.org/W2962739339', 'https://openalex.org/W6767997687', 'https://openalex.org/W2896457183', 'https://openalex.org/W6766673545', 'https://openalex.org/W6844194202', 'https://openalex.org/W3035524453', 'https://openalex.org/W6774670964', 'https://openalex.org/W6779997284', 'https://openalex.org/W2962907457', 'https://openalex.org/W3026041220', 'https://openalex.org/W3096338464', 'https://openalex.org/W6811088048', 'https://openalex.org/W2913340405', 'https://openalex.org/W2291975472', 'https://openalex.org/W3112702554', 'https://openalex.org/W4226380987', 'https://openalex.org/W2973157397', 'https://openalex.org/W6617744952', 'https://openalex.org/W6712395597', 'https://openalex.org/W2962850167', 'https://openalex.org/W6745117592', 'https://openalex.org/W6757193177', 'https://openalex.org/W2752796333', 'https://openalex.org/W3140429000', 'https://openalex.org/W3198217962', 'https://openalex.org/W6790356757', 'https://openalex.org/W6690026940', 'https://openalex.org/W2097012520', 'https://openalex.org/W1945356021', 'https://openalex.org/W3100270690', 'https://openalex.org/W6729448088', 'https://openalex.org/W2972867623', 'https://openalex.org/W2035424729', 'https://openalex.org/W2020607164', 'https://openalex.org/W2396043527', 'https://openalex.org/W1545920196', 'https://openalex.org/W1796128977', 'https://openalex.org/W2932675979', 'https://openalex.org/W2972943112', 'https://openalex.org/W3016011332', 'https://openalex.org/W2020883660', 'https://openalex.org/W2146444479', 'https://openalex.org/W3035202887', 'https://openalex.org/W3097286738', 'https://openalex.org/W3015265920', 'https://openalex.org/W2982223350', 'https://openalex.org/W6769238691', 'https://openalex.org/W6778265221', 'https://openalex.org/W3003875258', 'https://openalex.org/W3160345865', 'https://openalex.org/W3196919915', 'https://openalex.org/W3041561163', 'https://openalex.org/W3198858531', 'https://openalex.org/W3096485810', 'https://openalex.org/W3196798358', 'https://openalex.org/W6674330103', 'https://openalex.org/W3015213852', 'https://openalex.org/W2963425185', 'https://openalex.org/W6840487619', 'https://openalex.org/W3015949486', 'https://openalex.org/W2982039329', 'https://openalex.org/W2963571336', 'https://openalex.org/W3148040514', 'https://openalex.org/W2963317665', 'https://openalex.org/W2973049979', 'https://openalex.org/W3016181583', 'https://openalex.org/W3102342027', 'https://openalex.org/W6769196770', 'https://openalex.org/W6780218876', 'https://openalex.org/W3197411683', 'https://openalex.org/W4226033575', 'https://openalex.org/W3198608154', 'https://openalex.org/W3015356564', 'https://openalex.org/W3209059054', 'https://openalex.org/W3209984917', 'https://openalex.org/W6810007534', 'https://openalex.org/W6810673746', 'https://openalex.org/W6677884823', 'https://openalex.org/W6682948231', 'https://openalex.org/W2124509324', 'https://openalex.org/W6762931180', 'https://openalex.org/W3159481202', 'https://openalex.org/W1536680647', 'https://openalex.org/W6766978945', 'https://openalex.org/W6600971220', 'https://openalex.org/W2096391593', 'https://openalex.org/W1974783905', 'https://openalex.org/W2091863306', 'https://openalex.org/W4237938692', 'https://openalex.org/W142945732', 'https://openalex.org/W2594690981', 'https://openalex.org/W6810168380', 'https://openalex.org/W2016538560', 'https://openalex.org/W4396724964', 'https://openalex.org/W2296654356', 'https://openalex.org/W6686207219', 'https://openalex.org/W900447646', 'https://openalex.org/W6606244218', 'https://openalex.org/W4237723258', 'https://openalex.org/W6631216910', 'https://openalex.org/W6639103823', 'https://openalex.org/W6728094556', 'https://openalex.org/W6693697572', 'https://openalex.org/W1484147505', 'https://openalex.org/W2130055251', 'https://openalex.org/W2049252044', 'https://openalex.org/W6678885645', 'https://openalex.org/W1531883353', 'https://openalex.org/W6633682082', 'https://openalex.org/W2136189984', 'https://openalex.org/W4297841641', 'https://openalex.org/W3157861865', 'https://openalex.org/W6864391120', 'https://openalex.org/W6729977899', 'https://openalex.org/W2962862718', 'https://openalex.org/W2971709506', 'https://openalex.org/W3197828817', 'https://openalex.org/W3200287550', 'https://openalex.org/W2988907666', 'https://openalex.org/W3196698946', 'https://openalex.org/W3205715971', 'https://openalex.org/W6809593508', 'https://openalex.org/W6770596778', 'https://openalex.org/W2586148577', 'https://openalex.org/W2964115348', 'https://openalex.org/W2963902314', 'https://openalex.org/W4224875474', 'https://openalex.org/W3095293218', 'https://openalex.org/W2963330681', 'https://openalex.org/W2920166246', 'https://openalex.org/W2895651543', 'https://openalex.org/W2973135958', 'https://openalex.org/W6803092890', 'https://openalex.org/W1577418252', 'https://openalex.org/W1496120315', 'https://openalex.org/W2962980711', 'https://openalex.org/W2964169922', 'https://openalex.org/W6720204814', 'https://openalex.org/W2296681920', 'https://openalex.org/W2059652594', 'https://openalex.org/W2889313720', 'https://openalex.org/W2963720603', 'https://openalex.org/W6786885278', 'https://openalex.org/W2407151108', 'https://openalex.org/W2190506272', 'https://openalex.org/W3037530970', 'https://openalex.org/W3201254286', 'https://openalex.org/W3150635893', 'https://openalex.org/W2995181338', 'https://openalex.org/W2593116425', 'https://openalex.org/W4289665794', 'https://openalex.org/W6603931906', 'https://openalex.org/W1494198834', 'https://openalex.org/W2024490156', 'https://openalex.org/W6771467084', 'https://openalex.org/W3095410713', 'https://openalex.org/W3119308075', 'https://openalex.org/W6696449567', 'https://openalex.org/W3213029956', 'https://openalex.org/W3206252155', 'https://openalex.org/W3198694222', 'https://openalex.org/W2799473636', 'https://openalex.org/W6691509046', 'https://openalex.org/W2166637769', 'https://openalex.org/W3139878283', 'https://openalex.org/W1526236009', 'https://openalex.org/W2963242190', 'https://openalex.org/W2963127222', 'https://openalex.org/W2884797218', 'https://openalex.org/W6712941328', 'https://openalex.org/W2883409523', 'https://openalex.org/W6936113694', 'https://openalex.org/W2726515241', 'https://openalex.org/W2972584841', 'https://openalex.org/W1567520911', 'https://openalex.org/W6748215858', 'https://openalex.org/W3196509775', 'https://openalex.org/W2094544353', 'https://openalex.org/W6727418883', 'https://openalex.org/W6712757354', 'https://openalex.org/W6731521493', 'https://openalex.org/W6688816777', 'https://openalex.org/W2883595988', 'https://openalex.org/W6750665317', 'https://openalex.org/W2775794021', 'https://openalex.org/W6736723571', 'https://openalex.org/W2972894903', 'https://openalex.org/W3033038061', 'https://openalex.org/W942963634', 'https://openalex.org/W3197223534', 'https://openalex.org/W6784614252', 'https://openalex.org/W6753575415', 'https://openalex.org/W3160554450', 'https://openalex.org/W6803378298', 'https://openalex.org/W3189296823', 'https://openalex.org/W3093096176', 'https://openalex.org/W6809947431', 'https://openalex.org/W3006926732', 'https://openalex.org/W4225713393', 'https://openalex.org/W3096216486', 'https://openalex.org/W3095361818', 'https://openalex.org/W2251253014', 'https://openalex.org/W6795952400', 'https://openalex.org/W2970820321', 'https://openalex.org/W6801828775', 'https://openalex.org/W3198815374', 'https://openalex.org/W3024182269', 'https://openalex.org/W4224934179', 'https://openalex.org/W3096017728', 'https://openalex.org/W3162133897', 'https://openalex.org/W6784614126', 'https://openalex.org/W2786608204', 'https://openalex.org/W3207558756', 'https://openalex.org/W3198771897', 'https://openalex.org/W3198429080', 'https://openalex.org/W2962799225', 'https://openalex.org/W2802557066', 'https://openalex.org/W6735913928', 'https://openalex.org/W6751433836', 'https://openalex.org/W6744957266', 'https://openalex.org/W2899134946', 'https://openalex.org/W6750365303', 'https://openalex.org/W6757699909', 'https://openalex.org/W2962799131', 'https://openalex.org/W6738077056', 'https://openalex.org/W6760519848', 'https://openalex.org/W3214697273', 'https://openalex.org/W6678282225', 'https://openalex.org/W4319862670', 'https://openalex.org/W6745740328', 'https://openalex.org/W6745388339', 'https://openalex.org/W3204917342', 'https://openalex.org/W2962699523', 'https://openalex.org/W2963739817', 'https://openalex.org/W3015280134', 'https://openalex.org/W2963796886', 'https://openalex.org/W2963581463', 'https://openalex.org/W2972889948', 'https://openalex.org/W2119717200', 'https://openalex.org/W2046932483', 'https://openalex.org/W6640059789', 'https://openalex.org/W2577366047', 'https://openalex.org/W2964243274', 'https://openalex.org/W6756385397', 'https://openalex.org/W2940200615', 'https://openalex.org/W3008480565', 'https://openalex.org/W3024464021', 'https://openalex.org/W3016008406', 'https://openalex.org/W2883586237', 'https://openalex.org/W2964012862', 'https://openalex.org/W3097632072', 'https://openalex.org/W2963216553', 'https://openalex.org/W2025482506', 'https://openalex.org/W2963620343', 'https://openalex.org/W2940544976', 'https://openalex.org/W6786696081', 'https://openalex.org/W2347098582', 'https://openalex.org/W2787447541', 'https://openalex.org/W2972374322', 'https://openalex.org/W3197381195', 'https://openalex.org/W6803066952', 'https://openalex.org/W4307680525', 'https://openalex.org/W4221146627', 'https://openalex.org/W6805530253', 'https://openalex.org/W6777028661', 'https://openalex.org/W4287854499', 'https://openalex.org/W6759579507', 'https://openalex.org/W3176828726', 'https://openalex.org/W6787411158', 'https://openalex.org/W4225274946', 'https://openalex.org/W4226162428', 'https://openalex.org/W6796551075', 'https://openalex.org/W3203140070', 'https://openalex.org/W6685943813', 'https://openalex.org/W3137147200', 'https://openalex.org/W3085139254', 'https://openalex.org/W3198039885', 'https://openalex.org/W6839738141', 'https://openalex.org/W6803547063', 'https://openalex.org/W4297841871', 'https://openalex.org/W3214576767', 'https://openalex.org/W4296068785', 'https://openalex.org/W3209376089', 'https://openalex.org/W4221140371', 'https://openalex.org/W4292825791', 'https://openalex.org/W3101648800', 'https://openalex.org/W4200635400', 'https://openalex.org/W3088409176', 'https://openalex.org/W4286918540', 'https://openalex.org/W2102409316', 'https://openalex.org/W3211224152', 'https://openalex.org/W4288348042', 'https://openalex.org/W2981991061', 'https://openalex.org/W2998249245', 'https://openalex.org/W3009561768', 'https://openalex.org/W2242818861', 'https://openalex.org/W2997574889', 'https://openalex.org/W1515020792', 'https://openalex.org/W2965373594', 'https://openalex.org/W2530846021', 'https://openalex.org/W4297808394', 'https://openalex.org/W1915251500', 'https://openalex.org/W4295116917', 'https://openalex.org/W2964303773', 'https://openalex.org/W3026842484', 'https://openalex.org/W2125290066', 'https://openalex.org/W2219249508', 'https://openalex.org/W2095705004', 'https://openalex.org/W22517275', 'https://openalex.org/W4221145109', 'https://openalex.org/W4287591426', 'https://openalex.org/W2898727538', 'https://openalex.org/W2797583228', 'https://openalex.org/W3195577433', 'https://openalex.org/W3099142230', 'https://openalex.org/W3107298252', 'https://openalex.org/W3213873715', 'https://openalex.org/W2973026522', 'https://openalex.org/W4289750118', 'https://openalex.org/W4394671563', 'https://openalex.org/W3207222250', 'https://openalex.org/W3024605872', 'https://openalex.org/W3161411634', 'https://openalex.org/W2973727699', 'https://openalex.org/W4320013820', 'https://openalex.org/W1508165687', 'https://openalex.org/W2138204974']",2022-09-15
https://openalex.org/W3189296823,https://doi.org/10.21437/interspeech.2021-556,LeBenchmark: A Reproducible Framework for Assessing Self-Supervised\n Representation Learning from Speech,"Self-Supervised Learning (SSL) using huge unlabeled data has been\nsuccessfully explored for image and natural language processing. Recent works\nalso investigated SSL from speech. They were notably successful to improve\nperformance on downstream tasks such as automatic speech recognition (ASR).\nWhile these works suggest it is possible to reduce dependence on labeled data\nfor building efficient speech systems, their evaluation was mostly made on ASR\nand using multiple and heterogeneous experimental settings (most of them for\nEnglish). This questions the objective comparison of SSL approaches and the\nevaluation of their impact on building speech systems. In this paper, we\npropose LeBenchmark: a reproducible framework for assessing SSL from speech. It\nnot only includes ASR (high and low resource) tasks but also spoken language\nunderstanding, speech translation and emotion recognition. We also focus on\nspeech technologies in a language different than English: French. SSL models of\ndifferent sizes are trained from carefully sourced and documented datasets.\nExperiments show that SSL is beneficial for most but not all tasks which\nconfirms the need for exhaustive and reliable benchmarks to evaluate its real\nimpact. LeBenchmark is shared with the scientific community for reproducible\nresearch in SSL from speech.\n","['https://openalex.org/W3095410713', 'https://openalex.org/W2988736778', 'https://openalex.org/W3198429080', 'https://openalex.org/W4385245566', 'https://openalex.org/W72302491', 'https://openalex.org/W2888867175', 'https://openalex.org/W2133564696', 'https://openalex.org/W2313339984', 'https://openalex.org/W2810556878', 'https://openalex.org/W3049256661', 'https://openalex.org/W3016011332', 'https://openalex.org/W2514741789', 'https://openalex.org/W2962784628', 'https://openalex.org/W2962739339', 'https://openalex.org/W3016181583', 'https://openalex.org/W3015867372', 'https://openalex.org/W2576530755', 'https://openalex.org/W157724941', 'https://openalex.org/W3005680577', 'https://openalex.org/W3030437843', 'https://openalex.org/W3102342027', 'https://openalex.org/W2933138175', 'https://openalex.org/W2896457183', 'https://openalex.org/W2327501763', 'https://openalex.org/W3099944122', 'https://openalex.org/W2972943112', 'https://openalex.org/W2948012107', 'https://openalex.org/W3054645415', 'https://openalex.org/W3092424727', 'https://openalex.org/W3015935472', 'https://openalex.org/W2973049979', 'https://openalex.org/W3021934733', 'https://openalex.org/W2045528981', 'https://openalex.org/W1524333225', 'https://openalex.org/W3035202887', 'https://openalex.org/W2249819665', 'https://openalex.org/W3197771105', 'https://openalex.org/W2972327934', 'https://openalex.org/W2329093554', 'https://openalex.org/W3036601975', 'https://openalex.org/W2402146185', 'https://openalex.org/W3119308075', 'https://openalex.org/W2399733683']",2021-04-23
https://openalex.org/W3185341429,https://doi.org/10.1145/3560815,"Pre-train, Prompt, and Predict: A Systematic Survey of Prompting Methods in Natural Language Processing","This article surveys and organizes research works in a new paradigm in natural language processing, which we dub “prompt-based learning.” Unlike traditional supervised learning, which trains a model to take in an input x and predict an output y as P ( y|x ), prompt-based learning is based on language models that model the probability of text directly. To use these models to perform prediction tasks, the original input x is modified using a template into a textual string prompt x′ that has some unfilled slots, and then the language model is used to probabilistically fill the unfilled information to obtain a final string x̂ , from which the final output y can be derived. This framework is powerful and attractive for a number of reasons: It allows the language model to be pre-trained on massive amounts of raw text, and by defining a new prompting function the model is able to perform few-shot or even zero-shot learning, adapting to new scenarios with few or no labeled data. In this article, we introduce the basics of this promising paradigm, describe a unified set of mathematical notations that can cover a wide variety of existing work, and organize existing work along several dimensions, e.g., the choice of pre-trained language models, prompts, and tuning strategies. To make the field more accessible to interested beginners, we not only make a systematic review of existing works and a highly structured typology of prompt-based concepts but also release other resources, e.g., a website NLPedia–Pretrain including constantly updated survey and paperlist.","['https://openalex.org/W3182696977', 'https://openalex.org/W4225590069', 'https://openalex.org/W2163922914', 'https://openalex.org/W2250225488', 'https://openalex.org/W2798542795', 'https://openalex.org/W3194836374', 'https://openalex.org/W2744813330', 'https://openalex.org/W2997617958', 'https://openalex.org/W2970161131', 'https://openalex.org/W2963341956', 'https://openalex.org/W3005700362', 'https://openalex.org/W3170083118', 'https://openalex.org/W3004346089', 'https://openalex.org/W2610850660', 'https://openalex.org/W2962805889', 'https://openalex.org/W3176549752', 'https://openalex.org/W2963705779', 'https://openalex.org/W3173777717', 'https://openalex.org/W2130774920', 'https://openalex.org/W2796032388', 'https://openalex.org/W2143612262', 'https://openalex.org/W2963018920', 'https://openalex.org/W3007672467', 'https://openalex.org/W2143426320', 'https://openalex.org/W3174784402', 'https://openalex.org/W3154903254', 'https://openalex.org/W2064675550', 'https://openalex.org/W4205857304', 'https://openalex.org/W2963026768', 'https://openalex.org/W2970780738', 'https://openalex.org/W3173617765', 'https://openalex.org/W1934890906', 'https://openalex.org/W3104163040', 'https://openalex.org/W3199958362', 'https://openalex.org/W3044438666', 'https://openalex.org/W2120615054', 'https://openalex.org/W3099655892', 'https://openalex.org/W2963104691', 'https://openalex.org/W1832693441', 'https://openalex.org/W2963963993', 'https://openalex.org/W3211848854', 'https://openalex.org/W1873332500', 'https://openalex.org/W3175603587', 'https://openalex.org/W2606964149', 'https://openalex.org/W3167602185', 'https://openalex.org/W2964222246', 'https://openalex.org/W4205991051', 'https://openalex.org/W3034999214', 'https://openalex.org/W3035625205', 'https://openalex.org/W3174770825', 'https://openalex.org/W3102187933', 'https://openalex.org/W2963630207', 'https://openalex.org/W3122241445', 'https://openalex.org/W2739918945', 'https://openalex.org/W3170658419', 'https://openalex.org/W2965373594', 'https://openalex.org/W3156470785', 'https://openalex.org/W2962902328', 'https://openalex.org/W1982795016', 'https://openalex.org/W2809324505', 'https://openalex.org/W2950645060', 'https://openalex.org/W2952984539', 'https://openalex.org/W2963609889', 'https://openalex.org/W2948468976', 'https://openalex.org/W2973519210', 'https://openalex.org/W2166706824', 'https://openalex.org/W2250539671', 'https://openalex.org/W2962739339', 'https://openalex.org/W2970352191', 'https://openalex.org/W6964124353', 'https://openalex.org/W2970476646', 'https://openalex.org/W3099771192', 'https://openalex.org/W2759653627', 'https://openalex.org/W3166846774', 'https://openalex.org/W2981852735', 'https://openalex.org/W2962833140', 'https://openalex.org/W2963748441', 'https://openalex.org/W3160638507', 'https://openalex.org/W3013444301', 'https://openalex.org/W3096580779', 'https://openalex.org/W3153427360', 'https://openalex.org/W3212893438', 'https://openalex.org/W4206636317', 'https://openalex.org/W3172642864', 'https://openalex.org/W2467834614', 'https://openalex.org/W2963216553', 'https://openalex.org/W3156012351', 'https://openalex.org/W3098267758', 'https://openalex.org/W6735236233', 'https://openalex.org/W3092785544', 'https://openalex.org/W2510850444', 'https://openalex.org/W3104597016', 'https://openalex.org/W2982756474', 'https://openalex.org/W2986266667', 'https://openalex.org/W3034942609', 'https://openalex.org/W3034797437', 'https://openalex.org/W2970200208', 'https://openalex.org/W3153451655', 'https://openalex.org/W3092288641', 'https://openalex.org/W3041498386', 'https://openalex.org/W3166986030', 'https://openalex.org/W2100128988', 'https://openalex.org/W3141940864', 'https://openalex.org/W2771472444', 'https://openalex.org/W2612690371', 'https://openalex.org/W2560674852', 'https://openalex.org/W2524182563', 'https://openalex.org/W2121863487', 'https://openalex.org/W2608239929', 'https://openalex.org/W2083735344', 'https://openalex.org/W3098341425', 'https://openalex.org/W2990138404', 'https://openalex.org/W2989911337', 'https://openalex.org/W2598654328', 'https://openalex.org/W4288089799', 'https://openalex.org/W102110531', 'https://openalex.org/W4287867774', 'https://openalex.org/W2559655401', 'https://openalex.org/W2601450892', 'https://openalex.org/W4293350112']",2022-09-14
https://openalex.org/W3098267758,https://doi.org/10.18653/v1/2020.emnlp-main.346,AutoPrompt: Eliciting Knowledge from Language Models with Automatically Generated Prompts,"The remarkable success of pretrained language models has motivated the study of what kinds of knowledge these models learn during pretraining. Reformulating tasks as fill-in-the-blanks problems (e.g., cloze tests) is a natural approach for gauging such knowledge, however, its usage is limited by the manual effort and guesswork required to write suitable prompts. To address this, we develop AutoPrompt, an automated method to create prompts for a diverse set of tasks, based on a gradient-guided search. Using AutoPrompt, we show that masked language models (MLMs) have an inherent capability to perform sentiment analysis and natural language inference without additional parameters or finetuning, sometimes achieving performance on par with recent state-of-the-art supervised models. We also show that our prompts elicit more accurate factual knowledge from MLMs than the manually created prompts on the LAMA benchmark, and that MLMs can be used as relation extractors more effectively than supervised relation extraction models. These results demonstrate that automatically generated prompts are a viable parameter-free alternative to existing probing methods, and as pretrained LMs become more sophisticated and capable, potentially a replacement for finetuning.","['https://openalex.org/W2785611959', 'https://openalex.org/W3104235057', 'https://openalex.org/W2964303116', 'https://openalex.org/W4287867774', 'https://openalex.org/W2982756474', 'https://openalex.org/W3005700362', 'https://openalex.org/W2923014074', 'https://openalex.org/W2970862333', 'https://openalex.org/W2980282514', 'https://openalex.org/W2805206884', 'https://openalex.org/W2998557616', 'https://openalex.org/W3104499181', 'https://openalex.org/W2130158090', 'https://openalex.org/W3044438666', 'https://openalex.org/W2963430447', 'https://openalex.org/W2963310665', 'https://openalex.org/W2934842096', 'https://openalex.org/W2250790822', 'https://openalex.org/W1840435438', 'https://openalex.org/W2251939518', 'https://openalex.org/W4292779060', 'https://openalex.org/W2982906145', 'https://openalex.org/W2951025380', 'https://openalex.org/W2970476646', 'https://openalex.org/W3153427360', 'https://openalex.org/W2799124508', 'https://openalex.org/W2970726176', 'https://openalex.org/W2896457183', 'https://openalex.org/W2962739339', 'https://openalex.org/W2963341956', 'https://openalex.org/W2756566873']",2020-01-01
https://openalex.org/W3188542058,https://doi.org/10.18653/v1/2022.acl-long.158,Knowledgeable Prompt-tuning: Incorporating Knowledge into Prompt Verbalizer for Text Classification,"Shengding Hu, Ning Ding, Huadong Wang, Zhiyuan Liu, Jingang Wang, Juanzi Li, Wei Wu, Maosong Sun. Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). 2022.","['https://openalex.org/W3157005959', 'https://openalex.org/W2971600926', 'https://openalex.org/W2964207259', 'https://openalex.org/W2558203065', 'https://openalex.org/W3126960149', 'https://openalex.org/W4297795751', 'https://openalex.org/W2561529111', 'https://openalex.org/W2970161131', 'https://openalex.org/W2962369866', 'https://openalex.org/W3115772171', 'https://openalex.org/W3082274269', 'https://openalex.org/W2613678836', 'https://openalex.org/W2990704537', 'https://openalex.org/W3170929029', 'https://openalex.org/W3096331697', 'https://openalex.org/W2951534261', 'https://openalex.org/W1552847225', 'https://openalex.org/W3002104146', 'https://openalex.org/W2996428491', 'https://openalex.org/W4292779060', 'https://openalex.org/W3105625590', 'https://openalex.org/W2951561177', 'https://openalex.org/W2113459411', 'https://openalex.org/W2965373594', 'https://openalex.org/W3139080614', 'https://openalex.org/W2970476646', 'https://openalex.org/W3099142828', 'https://openalex.org/W3173777717', 'https://openalex.org/W3153427360', 'https://openalex.org/W2170240176', 'https://openalex.org/W3194836374', 'https://openalex.org/W3021533447', 'https://openalex.org/W2970200208', 'https://openalex.org/W3154560120', 'https://openalex.org/W3033176962', 'https://openalex.org/W3132736064', 'https://openalex.org/W3096580779', 'https://openalex.org/W4309444617', 'https://openalex.org/W4288089799', 'https://openalex.org/W2998385486', 'https://openalex.org/W2963748441', 'https://openalex.org/W2946345909', 'https://openalex.org/W2144211451', 'https://openalex.org/W3014521650', 'https://openalex.org/W4205857304', 'https://openalex.org/W2963341956', 'https://openalex.org/W3030163527', 'https://openalex.org/W3106109117', 'https://openalex.org/W3172642864', 'https://openalex.org/W3102659883', 'https://openalex.org/W3098267758', 'https://openalex.org/W2897065501', 'https://openalex.org/W2061873838', 'https://openalex.org/W3172943453', 'https://openalex.org/W3198659451', 'https://openalex.org/W2962739339', 'https://openalex.org/W4309811444', 'https://openalex.org/W3165416482', 'https://openalex.org/W3174784402', 'https://openalex.org/W2905233862', 'https://openalex.org/W2963012544', 'https://openalex.org/W2963804400']",2022-01-01
https://openalex.org/W4322766882,https://doi.org/10.1038/s42256-023-00626-4,Parameter-efficient fine-tuning of large-scale pre-trained language models,"Abstract With the prevalence of pre-trained language models (PLMs) and the pre-training–fine-tuning paradigm, it has been continuously shown that larger models tend to yield better performance. However, as PLMs scale up, fine-tuning and storing all the parameters is prohibitively costly and eventually becomes practically infeasible. This necessitates a new branch of research focusing on the parameter-efficient adaptation of PLMs, which optimizes a small portion of the model parameters while keeping the rest fixed, drastically cutting down computation and storage costs. In general, it demonstrates that large-scale models could be effectively stimulated by the optimization of a few parameters. Despite the various designs, here we discuss and analyse the approaches under a more consistent and accessible term ‘delta-tuning’, where ‘delta’ a mathematical notation often used to denote changes, is borrowed to refer to the portion of parameters that are ‘changed’ during training. We formally describe the problem and propose a unified categorization criterion for existing delta-tuning methods to explore their correlations and differences. We also discuss the theoretical principles underlying the effectiveness of delta-tuning and interpret them from the perspectives of optimization and optimal control. Furthermore, we provide a holistic empirical study on over 100 natural language processing tasks and investigate various aspects of delta-tuning. With comprehensive study and analysis, our research demonstrates the theoretical and practical properties of delta-tuning in the adaptation of PLMs.","['https://openalex.org/W2919115771', 'https://openalex.org/W6666761814', 'https://openalex.org/W1981276685', 'https://openalex.org/W6739901393', 'https://openalex.org/W2981852735', 'https://openalex.org/W6778883912', 'https://openalex.org/W4221167110', 'https://openalex.org/W6810081322', 'https://openalex.org/W6759579507', 'https://openalex.org/W3176828726', 'https://openalex.org/W6796581206', 'https://openalex.org/W3177323791', 'https://openalex.org/W3207663303', 'https://openalex.org/W3197876970', 'https://openalex.org/W4205991051', 'https://openalex.org/W2965373594', 'https://openalex.org/W2923014074', 'https://openalex.org/W3153427360', 'https://openalex.org/W2251939518', 'https://openalex.org/W4287891024', 'https://openalex.org/W2963846996', 'https://openalex.org/W3205717164', 'https://openalex.org/W3198659451', 'https://openalex.org/W3153675281', 'https://openalex.org/W4206178588', 'https://openalex.org/W3176693010', 'https://openalex.org/W3175557894', 'https://openalex.org/W3099793224', 'https://openalex.org/W3173777717', 'https://openalex.org/W3188542058', 'https://openalex.org/W3185341429', 'https://openalex.org/W3210129272', 'https://openalex.org/W3174770825', 'https://openalex.org/W3174702398', 'https://openalex.org/W3020268419', 'https://openalex.org/W3217756540', 'https://openalex.org/W2140246545', 'https://openalex.org/W4212774754']",2023-03-02
https://openalex.org/W4226162428,https://doi.org/10.21437/interspeech.2022-10610,An Exploration of Prompt Tuning on Generative Spoken Language Model for Speech Processing Tasks,"Speech representations learned from Self-supervised learning (SSL) models can benefit various speech processing tasks. However, utilizing SSL representations usually requires fine-tuning the pre-trained models or designing task-specific downstream models and loss functions, causing much memory usage and human labor. Recently, prompting in Natural Language Processing (NLP) has been found to be an efficient technique to leverage pre-trained language models (LMs). Specifically, prompt tuning optimizes a limited number of task-specific parameters with a fixed pre-trained model; as a result, only a small set of parameters is needed to be stored for each task. Prompt tuning improves computation and memory efficiency by leveraging the pre-trained LM's prediction ability. Nevertheless, such a paradigm is little studied in the speech community. We report in this paper the first exploration of the prompt tuning paradigm for speech processing tasks based on Generative Spoken Language Model (GSLM). Experiment results show that the prompt tuning technique achieves competitive performance in speech classification tasks with fewer trainable parameters than fine-tuning specialized downstream models. We further study the technique in challenging sequence generation tasks. Prompt tuning also demonstrates its potential, while the limitation and possible research directions are discussed in this paper. The source code is available on https://github.com/ga642381/SpeechPrompt.",[],2022-09-16
https://openalex.org/W2951082691,https://doi.org/10.1016/j.csl.2019.06.001,Preserving privacy in speaker and speech characterisation,"Speech recordings are a rich source of personal, sensitive data that can be used to support a plethora of diverse applications, from health profiling to biometric recognition. It is therefore essential that speech recordings are adequately protected so that they cannot be misused. Such protection, in the form of privacy-preserving technologies, is required to ensure that: (i) the biometric profiles of a given individual (e.g., across different biometric service operators) are unlinkable; (ii) leaked, encrypted biometric information is irreversible, and that (iii) biometric references are renewable. Whereas many privacy-preserving technologies have been developed for other biometric characteristics, very few solutions have been proposed to protect privacy in the case of speech signals. Despite privacy preservation this is now being mandated by recent European and international data protection regulations. With the aim of fostering progress and collaboration between researchers in the speech, biometrics and applied cryptography communities, this survey article provides an introduction to the field, starting with a legal perspective on privacy preservation in the case of speech data. It then establishes the requirements for effective privacy preservation, reviews generic cryptography-based solutions, followed by specific techniques that are applicable to speaker characterisation (biometric applications) and speech characterisation (non-biometric applications). Glancing at non-biometrics, methods are presented to avoid function creep, preventing the exploitation of biometric information, e.g., to single out an identity in speech-assisted health care via speaker characterisation. In promoting harmonised research, the article also outlines common, empirical evaluation metrics for the assessment of privacy-preserving technologies for speech data.","['https://openalex.org/W6725385535', 'https://openalex.org/W6720608135', 'https://openalex.org/W6634194564', 'https://openalex.org/W6680187605', 'https://openalex.org/W6756440446', 'https://openalex.org/W1973285633', 'https://openalex.org/W6712193006', 'https://openalex.org/W6601281006', 'https://openalex.org/W6680870696', 'https://openalex.org/W6655225227', 'https://openalex.org/W6732110492', 'https://openalex.org/W6755363574', 'https://openalex.org/W6641648738', 'https://openalex.org/W2130051287', 'https://openalex.org/W6630837767', 'https://openalex.org/W6679645476', 'https://openalex.org/W6659384504', 'https://openalex.org/W6631196207', 'https://openalex.org/W2088098460', 'https://openalex.org/W6697114615', 'https://openalex.org/W586426220', 'https://openalex.org/W6636379735', 'https://openalex.org/W6683490306', 'https://openalex.org/W6683799570', 'https://openalex.org/W6671828919', 'https://openalex.org/W6637798261', 'https://openalex.org/W6635253253', 'https://openalex.org/W6728621692', 'https://openalex.org/W6749823586', 'https://openalex.org/W2124783762', 'https://openalex.org/W6752659224', 'https://openalex.org/W6633316310', 'https://openalex.org/W2023238506', 'https://openalex.org/W1540579472', 'https://openalex.org/W2788079841', 'https://openalex.org/W6638026645', 'https://openalex.org/W2118944716', 'https://openalex.org/W6658616617', 'https://openalex.org/W6671925355', 'https://openalex.org/W6630154691', 'https://openalex.org/W6835076329', 'https://openalex.org/W6732586565', 'https://openalex.org/W6664564822', 'https://openalex.org/W1969021562', 'https://openalex.org/W6712237015', 'https://openalex.org/W2011229062', 'https://openalex.org/W1990942610', 'https://openalex.org/W6681352918', 'https://openalex.org/W1544327602', 'https://openalex.org/W2148154194', 'https://openalex.org/W2150769028', 'https://openalex.org/W1658556320', 'https://openalex.org/W6666307040', 'https://openalex.org/W6642591113', 'https://openalex.org/W6754831029', 'https://openalex.org/W6758947373', 'https://openalex.org/W6657138077', 'https://openalex.org/W6631849546', 'https://openalex.org/W6607417710', 'https://openalex.org/W2126650182', 'https://openalex.org/W6632317815', 'https://openalex.org/W2022596562', 'https://openalex.org/W2400334276', 'https://openalex.org/W6712187489', 'https://openalex.org/W6658575637', 'https://openalex.org/W2435473771', 'https://openalex.org/W6740171916', 'https://openalex.org/W6684746630', 'https://openalex.org/W6725341466', 'https://openalex.org/W2068740371', 'https://openalex.org/W2609122644', 'https://openalex.org/W2777595189', 'https://openalex.org/W2578517905', 'https://openalex.org/W2340176088', 'https://openalex.org/W6725282951', 'https://openalex.org/W6691116160', 'https://openalex.org/W1936725236', 'https://openalex.org/W2068608907', 'https://openalex.org/W6760721792', 'https://openalex.org/W2063004570', 'https://openalex.org/W6746138031', 'https://openalex.org/W6682695857', 'https://openalex.org/W6637082417', 'https://openalex.org/W2791390287', 'https://openalex.org/W6638667902', 'https://openalex.org/W6606928740', 'https://openalex.org/W6713547331', 'https://openalex.org/W2524448027', 'https://openalex.org/W2522421503', 'https://openalex.org/W6748082217', 'https://openalex.org/W6682664221', 'https://openalex.org/W6664064737', 'https://openalex.org/W6635535926', 'https://openalex.org/W2118971792', 'https://openalex.org/W6608353291', 'https://openalex.org/W2107638917', 'https://openalex.org/W6645614418', 'https://openalex.org/W6754232464', 'https://openalex.org/W2772927365', 'https://openalex.org/W6757577762', 'https://openalex.org/W2154278880', 'https://openalex.org/W6696641172', 'https://openalex.org/W6638667582', 'https://openalex.org/W6712776754', 'https://openalex.org/W2106217851', 'https://openalex.org/W2120315802', 'https://openalex.org/W6679173735', 'https://openalex.org/W6745145294', 'https://openalex.org/W2027090774', 'https://openalex.org/W6634254106', 'https://openalex.org/W6628587945', 'https://openalex.org/W6659476796', 'https://openalex.org/W2128171688', 'https://openalex.org/W2345079061', 'https://openalex.org/W6740002586', 'https://openalex.org/W6948328358', 'https://openalex.org/W6751725997', 'https://openalex.org/W6652595405', 'https://openalex.org/W6749101786', 'https://openalex.org/W2100974241', 'https://openalex.org/W6646406331', 'https://openalex.org/W6679763910', 'https://openalex.org/W6629441459', 'https://openalex.org/W1490122681', 'https://openalex.org/W6606407469', 'https://openalex.org/W6713667877', 'https://openalex.org/W1972393975', 'https://openalex.org/W6678724130', 'https://openalex.org/W2061533153', 'https://openalex.org/W6676246221', 'https://openalex.org/W6676659581', 'https://openalex.org/W6752612201', 'https://openalex.org/W6719631278', 'https://openalex.org/W6721204825', 'https://openalex.org/W6726828527', 'https://openalex.org/W6721302741', 'https://openalex.org/W6684657127', 'https://openalex.org/W1535795947', 'https://openalex.org/W2408592790', 'https://openalex.org/W1554796637', 'https://openalex.org/W2154691736', 'https://openalex.org/W6677847058', 'https://openalex.org/W2125838338', 'https://openalex.org/W6756594218', 'https://openalex.org/W2900688537', 'https://openalex.org/W6601341518', 'https://openalex.org/W2094327026', 'https://openalex.org/W2129905915', 'https://openalex.org/W2041823554', 'https://openalex.org/W6760122430', 'https://openalex.org/W6747754840', 'https://openalex.org/W2058308744', 'https://openalex.org/W6634416663', 'https://openalex.org/W6629194106', 'https://openalex.org/W6712888285', 'https://openalex.org/W6751853187', 'https://openalex.org/W6696957263', 'https://openalex.org/W6684691647', 'https://openalex.org/W6663928093', 'https://openalex.org/W6676445784', 'https://openalex.org/W6668564662', 'https://openalex.org/W6752688541', 'https://openalex.org/W6733275487', 'https://openalex.org/W6742911084', 'https://openalex.org/W6754496211', 'https://openalex.org/W6682311248', 'https://openalex.org/W2293598903', 'https://openalex.org/W1491861813', 'https://openalex.org/W2103869314', 'https://openalex.org/W6754187066', 'https://openalex.org/W6752083570', 'https://openalex.org/W2120605154', 'https://openalex.org/W2059003220', 'https://openalex.org/W6697064641', 'https://openalex.org/W2037894298', 'https://openalex.org/W6682161550', 'https://openalex.org/W6673580112', 'https://openalex.org/W6681684773', 'https://openalex.org/W1945892390', 'https://openalex.org/W6763738310', 'https://openalex.org/W2054997365', 'https://openalex.org/W4394604333', 'https://openalex.org/W2505045346', 'https://openalex.org/W2797851444', 'https://openalex.org/W2889153834', 'https://openalex.org/W2137310491', 'https://openalex.org/W2806521799', 'https://openalex.org/W2063724771', 'https://openalex.org/W2086763678', 'https://openalex.org/W2929650703', 'https://openalex.org/W2473418344', 'https://openalex.org/W2036329595', 'https://openalex.org/W2949117887', 'https://openalex.org/W2902809377', 'https://openalex.org/W2140192608', 'https://openalex.org/W2899558955', 'https://openalex.org/W2130211373', 'https://openalex.org/W2169691038', 'https://openalex.org/W2888787196', 'https://openalex.org/W2150620897', 'https://openalex.org/W204053250', 'https://openalex.org/W2496571228', 'https://openalex.org/W2145580658', 'https://openalex.org/W1596208550', 'https://openalex.org/W34085441', 'https://openalex.org/W2134340933', 'https://openalex.org/W2405168256', 'https://openalex.org/W1985225657', 'https://openalex.org/W2889868664', 'https://openalex.org/W1480225633', 'https://openalex.org/W2053637704', 'https://openalex.org/W2072468280', 'https://openalex.org/W1542412379', 'https://openalex.org/W2133555949', 'https://openalex.org/W2783769334', 'https://openalex.org/W2207050309', 'https://openalex.org/W2033165262', 'https://openalex.org/W2577421826', 'https://openalex.org/W1779287571', 'https://openalex.org/W3104375021', 'https://openalex.org/W1836465849', 'https://openalex.org/W574609241', 'https://openalex.org/W2890964092', 'https://openalex.org/W155995968', 'https://openalex.org/W2507840786', 'https://openalex.org/W1826277484', 'https://openalex.org/W2152924492', 'https://openalex.org/W2805453438', 'https://openalex.org/W2027595342', 'https://openalex.org/W567437002', 'https://openalex.org/W2891461215', 'https://openalex.org/W2398448180', 'https://openalex.org/W2807627734', 'https://openalex.org/W2147929033', 'https://openalex.org/W2111251300', 'https://openalex.org/W2152516507', 'https://openalex.org/W1589843374', 'https://openalex.org/W3146606878', 'https://openalex.org/W2524624957', 'https://openalex.org/W2791874343', 'https://openalex.org/W2690039764', 'https://openalex.org/W1558685020', 'https://openalex.org/W2794888826', 'https://openalex.org/W2293203605', 'https://openalex.org/W1675339804', 'https://openalex.org/W2465072430', 'https://openalex.org/W1969009977', 'https://openalex.org/W2950460048', 'https://openalex.org/W1568172346', 'https://openalex.org/W1534388293', 'https://openalex.org/W2785618663', 'https://openalex.org/W1965805783', 'https://openalex.org/W2806563680', 'https://openalex.org/W2146828512', 'https://openalex.org/W2520442116', 'https://openalex.org/W1490653284', 'https://openalex.org/W179458199', 'https://openalex.org/W2701059868', 'https://openalex.org/W2400124473', 'https://openalex.org/W2892199772', 'https://openalex.org/W2170334586', 'https://openalex.org/W2009360064', 'https://openalex.org/W2144798081', 'https://openalex.org/W2111255563', 'https://openalex.org/W2126933852', 'https://openalex.org/W2917560727', 'https://openalex.org/W2056610114', 'https://openalex.org/W3143835353', 'https://openalex.org/W2397423248', 'https://openalex.org/W2294473867', 'https://openalex.org/W1782190827', 'https://openalex.org/W2765200655', 'https://openalex.org/W1892798954', 'https://openalex.org/W1516512945', 'https://openalex.org/W2035492440', 'https://openalex.org/W2478893932', 'https://openalex.org/W4211163263', 'https://openalex.org/W2803767422', 'https://openalex.org/W2092422002', 'https://openalex.org/W2019891719', 'https://openalex.org/W2293299267', 'https://openalex.org/W2604969030', 'https://openalex.org/W1623873962', 'https://openalex.org/W2908005243', 'https://openalex.org/W2480951315', 'https://openalex.org/W2533461851', 'https://openalex.org/W1503841100', 'https://openalex.org/W2949933192', 'https://openalex.org/W1488338708', 'https://openalex.org/W2398521502', 'https://openalex.org/W1523853800', 'https://openalex.org/W2963730492', 'https://openalex.org/W1981487090', 'https://openalex.org/W32630414', 'https://openalex.org/W2160371485', 'https://openalex.org/W2783983539', 'https://openalex.org/W3145164839', 'https://openalex.org/W3141239769', 'https://openalex.org/W2245884592', 'https://openalex.org/W2293406635', 'https://openalex.org/W2587150483', 'https://openalex.org/W2031533839', 'https://openalex.org/W3029275248', 'https://openalex.org/W2155404021', 'https://openalex.org/W2585580772', 'https://openalex.org/W645926012', 'https://openalex.org/W2951152347', 'https://openalex.org/W2768347741', 'https://openalex.org/W2804382710', 'https://openalex.org/W173953576', 'https://openalex.org/W2911978475', 'https://openalex.org/W2161214567', 'https://openalex.org/W2055147479', 'https://openalex.org/W2132172731', 'https://openalex.org/W2508694887', 'https://openalex.org/W3124289461', 'https://openalex.org/W1724472458', 'https://openalex.org/W2170065313', 'https://openalex.org/W2963755823', 'https://openalex.org/W2167394920', 'https://openalex.org/W2404098708', 'https://openalex.org/W2804829993', 'https://openalex.org/W2121812409', 'https://openalex.org/W2748488820', 'https://openalex.org/W2541558529', 'https://openalex.org/W2395750323', 'https://openalex.org/W2110287632', 'https://openalex.org/W35490052', 'https://openalex.org/W1502708590', 'https://openalex.org/W316732839', 'https://openalex.org/W1573122101']",2019-06-07
https://openalex.org/W4287887366,https://doi.org/10.18653/v1/2022.naacl-demo.1,textless-lib: a Library for Textless Spoken Language Processing,"Eugene Kharitonov, Jade Copet, Kushal Lakhotia, Tu Anh Nguyen, Paden Tomasello, Ann Lee, Ali Elkahky, Wei-Ning Hsu, Abdelrahman Mohamed, Emmanuel Dupoux, Yossi Adi. Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies: System Demonstrations. 2022.","['https://openalex.org/W2964243274', 'https://openalex.org/W2946200149', 'https://openalex.org/W3015338123', 'https://openalex.org/W3148101939', 'https://openalex.org/W3209984917', 'https://openalex.org/W3093096176', 'https://openalex.org/W3033411150', 'https://openalex.org/W2250539671', 'https://openalex.org/W2965373594', 'https://openalex.org/W4226033575', 'https://openalex.org/W4287591426', 'https://openalex.org/W4297808394', 'https://openalex.org/W3036601975', 'https://openalex.org/W3169320628', 'https://openalex.org/W4287854499', 'https://openalex.org/W3140429000', 'https://openalex.org/W4286984129', 'https://openalex.org/W2893425640', 'https://openalex.org/W3092028330', 'https://openalex.org/W2752177116', 'https://openalex.org/W2516090925', 'https://openalex.org/W3180374548', 'https://openalex.org/W4292779060', 'https://openalex.org/W4200635400', 'https://openalex.org/W4394671563', 'https://openalex.org/W3197974236', 'https://openalex.org/W2973026522', 'https://openalex.org/W2950018712', 'https://openalex.org/W3141523618', 'https://openalex.org/W4307680525', 'https://openalex.org/W3144810982', 'https://openalex.org/W2963821905', 'https://openalex.org/W3198815374', 'https://openalex.org/W1494198834', 'https://openalex.org/W2962739339', 'https://openalex.org/W2799124508', 'https://openalex.org/W2995181338', 'https://openalex.org/W2970006822', 'https://openalex.org/W2896457183', 'https://openalex.org/W3161348170', 'https://openalex.org/W4308349017', 'https://openalex.org/W4286899907', 'https://openalex.org/W4287374065', 'https://openalex.org/W3213873715', 'https://openalex.org/W2962866891', 'https://openalex.org/W2973049979', 'https://openalex.org/W2515741950', 'https://openalex.org/W2963300588']",2022-01-01
https://openalex.org/W3198608154,https://doi.org/10.21437/interspeech.2021-391,Speech SimCLR: Combining Contrastive and Reconstruction Objective for Self-Supervised Speech Representation Learning,"Self-supervised visual pretraining has shown significant progress recently.Among those methods, SimCLR greatly advanced the state of the art in self-supervised and semisupervised learning on ImageNet.The input feature representations for speech and visual tasks are both continuous, so it is natural to consider applying similar objective on speech representation learning.In this paper, we propose Speech SimCLR, a new self-supervised objective for speech representation learning.During training, Speech SimCLR applies augmentation on raw speech and its spectrogram.Its objective is the combination of contrastive loss that maximizes agreement between differently augmented samples in the latent space and reconstruction loss of input representation.The proposed method achieved competitive results on speech emotion recognition and speech recognition.","['https://openalex.org/W2963127222', 'https://openalex.org/W4214784181', 'https://openalex.org/W1522301498', 'https://openalex.org/W4398958419', 'https://openalex.org/W2973049979', 'https://openalex.org/W3160345865', 'https://openalex.org/W3015265920', 'https://openalex.org/W1568716973', 'https://openalex.org/W2146334809', 'https://openalex.org/W2404126548', 'https://openalex.org/W2972943112', 'https://openalex.org/W2936774411', 'https://openalex.org/W3102342027', 'https://openalex.org/W2964227577', 'https://openalex.org/W3046125265', 'https://openalex.org/W2973157397', 'https://openalex.org/W2995181338', 'https://openalex.org/W4385245566', 'https://openalex.org/W3148040514', 'https://openalex.org/W2982223350', 'https://openalex.org/W3035524453', 'https://openalex.org/W3034774681', 'https://openalex.org/W3016011332', 'https://openalex.org/W3036601975', 'https://openalex.org/W3041561163', 'https://openalex.org/W2972818416', 'https://openalex.org/W2219249508', 'https://openalex.org/W2979476256', 'https://openalex.org/W3035202887', 'https://openalex.org/W2944828972', 'https://openalex.org/W2251321385', 'https://openalex.org/W3005680577', 'https://openalex.org/W2981991061', 'https://openalex.org/W3144810982', 'https://openalex.org/W2936451900', 'https://openalex.org/W2988736778']",2021-08-27
https://openalex.org/W3016011332,https://doi.org/10.1109/icassp40776.2020.9054438,Generative Pre-Training for Speech with Autoregressive Predictive Coding,"Learning meaningful and general representations from unannotated speech that are applicable to a wide range of tasks remains challenging. In this paper we propose to use autoregressive predictive coding (APC), a recently proposed self-supervised objective, as a generative pre-training approach for learning meaningful, non-specific, and transferable speech representations. We pre-train APC on large-scale unlabeled data and conduct transfer learning experiments on three speech applications that require different information about speech characteristics to perform well: speech recognition, speech translation, and speaker identification. Extensive experiments show that APC not only outperforms surface features (e.g., log Mel spectrograms) and other popular representation learning methods on all three tasks, but is also effective at reducing downstream labeled data size and model parameters. We also investigate the use of Transformers for modeling APC and find it superior to RNNs.","['https://openalex.org/W2889028433', 'https://openalex.org/W2943845043', 'https://openalex.org/W6761563299', 'https://openalex.org/W2963026768', 'https://openalex.org/W6752888775', 'https://openalex.org/W6764498146', 'https://openalex.org/W2973217961', 'https://openalex.org/W2973034126', 'https://openalex.org/W6745117592', 'https://openalex.org/W179875071', 'https://openalex.org/W6739901393', 'https://openalex.org/W2964199361', 'https://openalex.org/W6748148878', 'https://openalex.org/W1494198834', 'https://openalex.org/W2962739339', 'https://openalex.org/W2194775991', 'https://openalex.org/W6755977528', 'https://openalex.org/W4300558631', 'https://openalex.org/W2973049979', 'https://openalex.org/W2973157397', 'https://openalex.org/W6898505805', 'https://openalex.org/W2842511635', 'https://openalex.org/W2973048981', 'https://openalex.org/W2963720603', 'https://openalex.org/W2963425185', 'https://openalex.org/W2963317665', 'https://openalex.org/W3100270690', 'https://openalex.org/W2972943112', 'https://openalex.org/W2884305338', 'https://openalex.org/W6631190155', 'https://openalex.org/W6623517193', 'https://openalex.org/W2024490156', 'https://openalex.org/W6766673545', 'https://openalex.org/W6755207826', 'https://openalex.org/W2899134946', 'https://openalex.org/W6748215858', 'https://openalex.org/W1522301498', 'https://openalex.org/W3125709657', 'https://openalex.org/W2963432880', 'https://openalex.org/W2785350307', 'https://openalex.org/W2941814890', 'https://openalex.org/W2808706139', 'https://openalex.org/W854541894', 'https://openalex.org/W2896457183', 'https://openalex.org/W2963045354', 'https://openalex.org/W2758785877', 'https://openalex.org/W2101105183', 'https://openalex.org/W2952127920', 'https://openalex.org/W2963341956', 'https://openalex.org/W4385245566', 'https://openalex.org/W2965373594', 'https://openalex.org/W2963403868', 'https://openalex.org/W2899663614', 'https://openalex.org/W4297808394', 'https://openalex.org/W2963618559', 'https://openalex.org/W2964121744']",2020-04-09
https://openalex.org/W3015265920,https://doi.org/10.1109/icassp40776.2020.9053176,Deep Contextualized Acoustic Representations for Semi-Supervised Speech Recognition,"We propose a novel approach to semi-supervised automatic speech recognition (ASR). We first exploit a large amount of unlabeled audio data via representation learning, where we reconstruct a temporal slice of filterbank features from past and future context frames. The resulting deep contextualized acoustic representations (DeCoAR) are then used to train a CTC-based end-to-end ASR system using a smaller amount of labeled audio data. In our experiments, we show that systems trained on DeCoAR consistently outperform ones trained on conventional filterbank features, giving 42% and 19% relative improvement over the baseline on WSJ eval92 and LibriSpeech test-clean, respectively. Our approach can drastically reduce the amount of labeled data required; unsupervised training on LibriSpeech then supervision with 100 hours of labeled data achieves performance on par with training on all 960 hours directly. Pre-trained models and code will be released online.","['https://openalex.org/W2486109868', 'https://openalex.org/W2962739339', 'https://openalex.org/W6755207826', 'https://openalex.org/W6763701032', 'https://openalex.org/W2972889948', 'https://openalex.org/W2972981541', 'https://openalex.org/W2136922672', 'https://openalex.org/W2160815625', 'https://openalex.org/W6676481782', 'https://openalex.org/W2964245029', 'https://openalex.org/W2944255943', 'https://openalex.org/W2889213362', 'https://openalex.org/W2936774411', 'https://openalex.org/W2802248956', 'https://openalex.org/W2124558353', 'https://openalex.org/W6765807869', 'https://openalex.org/W6760911985', 'https://openalex.org/W176510440', 'https://openalex.org/W6681588610', 'https://openalex.org/W2512655038', 'https://openalex.org/W2127499922', 'https://openalex.org/W1993660824', 'https://openalex.org/W2963425185', 'https://openalex.org/W3100270690', 'https://openalex.org/W2972943112', 'https://openalex.org/W6769196770', 'https://openalex.org/W2973049979', 'https://openalex.org/W2963211739', 'https://openalex.org/W2127141656', 'https://openalex.org/W2184045248', 'https://openalex.org/W2787560479', 'https://openalex.org/W3103005696', 'https://openalex.org/W2896457183', 'https://openalex.org/W2963341956', 'https://openalex.org/W2145494108', 'https://openalex.org/W2293363371', 'https://openalex.org/W2996383576', 'https://openalex.org/W2110798204', 'https://openalex.org/W2962753370', 'https://openalex.org/W2962907457', 'https://openalex.org/W3011411500', 'https://openalex.org/W2970597249', 'https://openalex.org/W2979476256', 'https://openalex.org/W3125709657', 'https://openalex.org/W2950813464']",2020-04-09
https://openalex.org/W3041561163,https://doi.org/10.1109/taslp.2021.3095662,TERA: Self-Supervised Learning of Transformer Encoder Representation for Speech,"We introduce a self-supervised speech pre-training method called TERA, which\nstands for Transformer Encoder Representations from Alteration. Recent\napproaches often learn by using a single auxiliary task like contrastive\nprediction, autoregressive prediction, or masked reconstruction. Unlike\nprevious methods, we use alteration along three orthogonal axes to pre-train\nTransformer Encoders on a large amount of unlabeled speech. The model learns\nthrough the reconstruction of acoustic frames from their altered counterpart,\nwhere we use a stochastic policy to alter along various dimensions: time,\nfrequency, and magnitude. TERA can be used for speech representations\nextraction or fine-tuning with downstream models. We evaluate TERA on several\ndownstream tasks, including phoneme classification, keyword spotting, speaker\nrecognition, and speech recognition. We present a large-scale comparison of\nvarious self-supervised models. TERA achieves strong performance in the\ncomparison by improving upon surface features and outperforming previous\nmodels. In our experiments, we study the effect of applying different\nalteration techniques, pre-training on more data, and pre-training on various\nfeatures. We analyze different model sizes and find that smaller models are\nstrong representation learners than larger models, while larger models are more\neffective for downstream fine-tuning than smaller models. Furthermore, we show\nthe proposed method is transferable to downstream datasets not used in\npre-training.\n","['https://openalex.org/W3096171739', 'https://openalex.org/W6763701032', 'https://openalex.org/W2927746189', 'https://openalex.org/W6768021236', 'https://openalex.org/W2962739339', 'https://openalex.org/W6607333740', 'https://openalex.org/W2127141656', 'https://openalex.org/W6739901393', 'https://openalex.org/W6784776607', 'https://openalex.org/W6763238093', 'https://openalex.org/W2973049979', 'https://openalex.org/W6844194202', 'https://openalex.org/W2982223350', 'https://openalex.org/W3003875258', 'https://openalex.org/W6777232839', 'https://openalex.org/W3096626135', 'https://openalex.org/W3096485810', 'https://openalex.org/W6778265221', 'https://openalex.org/W6777859476', 'https://openalex.org/W2794209590', 'https://openalex.org/W2125496931', 'https://openalex.org/W2962901777', 'https://openalex.org/W2995181338', 'https://openalex.org/W2002342963', 'https://openalex.org/W2964227577', 'https://openalex.org/W2077804127', 'https://openalex.org/W3035202887', 'https://openalex.org/W3097286738', 'https://openalex.org/W3096017728', 'https://openalex.org/W3015265920', 'https://openalex.org/W6840487619', 'https://openalex.org/W3015949486', 'https://openalex.org/W3100270690', 'https://openalex.org/W2947445680', 'https://openalex.org/W2982039329', 'https://openalex.org/W2973157397', 'https://openalex.org/W3033038061', 'https://openalex.org/W3015356564', 'https://openalex.org/W6769196770', 'https://openalex.org/W6773205534', 'https://openalex.org/W2972943112', 'https://openalex.org/W3016181583', 'https://openalex.org/W6631362777', 'https://openalex.org/W3016011332', 'https://openalex.org/W2972451902', 'https://openalex.org/W1494198834', 'https://openalex.org/W6757817989', 'https://openalex.org/W3015412890', 'https://openalex.org/W2936774411', 'https://openalex.org/W3024182269', 'https://openalex.org/W1635512741', 'https://openalex.org/W2943493972', 'https://openalex.org/W2981991061', 'https://openalex.org/W3125709657', 'https://openalex.org/W2980708516', 'https://openalex.org/W4385245566', 'https://openalex.org/W1524333225', 'https://openalex.org/W3160345865', 'https://openalex.org/W1522301498', 'https://openalex.org/W2964303116', 'https://openalex.org/W2950577311', 'https://openalex.org/W3030987249', 'https://openalex.org/W1614298861', 'https://openalex.org/W179875071', 'https://openalex.org/W3099782249', 'https://openalex.org/W3102342027', 'https://openalex.org/W3036601975', 'https://openalex.org/W2947454875', 'https://openalex.org/W2979476256', 'https://openalex.org/W3095292526', 'https://openalex.org/W2950813464', 'https://openalex.org/W3148040514', 'https://openalex.org/W2842511635', 'https://openalex.org/W2996428491', 'https://openalex.org/W4297808394', 'https://openalex.org/W3104896896', 'https://openalex.org/W2908510526', 'https://openalex.org/W1608367484', 'https://openalex.org/W2949667497', 'https://openalex.org/W2970597249', 'https://openalex.org/W3015412285', 'https://openalex.org/W3198858531', 'https://openalex.org/W4288348042', 'https://openalex.org/W2988736778', 'https://openalex.org/W2996383576', 'https://openalex.org/W2965373594', 'https://openalex.org/W2963341956', 'https://openalex.org/W3026957705', 'https://openalex.org/W3026842484', 'https://openalex.org/W2963403868']",2021-01-01
https://openalex.org/W4392909068,https://doi.org/10.1109/icassp48485.2024.10447929,"Exploring Speech Recognition, Translation, and Understanding with Discrete Speech Units: A Comparative Study","Speech signals, typically sampled at rates in the tens of thousands per second, contain redundancies, evoking inefficiencies in sequence modeling. High-dimensional speech features such as spectrograms are often used as the input for the subsequent model. However, they can still be redundant. Recent investigations proposed the use of discrete speech units derived from self-supervised learning representations, which significantly compresses the size of speech data. Applying various methods, such as de-duplication and subword modeling, can further compress the speech sequence length. Hence, training time is significantly reduced while retaining notable performance. In this study, we undertake a comprehensive and systematic exploration into the application of discrete units within end-to-end speech processing models. Experiments on 12 automatic speech recognition, 3 speech translation, and 1 spoken language understanding corpora demonstrate that discrete units achieve reasonably good results in almost all the settings. Our configurations and trained models are released in ESPnet to foster future research efforts.","['https://openalex.org/W2160815625', 'https://openalex.org/W2515753980', 'https://openalex.org/W2127141656', 'https://openalex.org/W2144499799', 'https://openalex.org/W6623517193', 'https://openalex.org/W4385245566', 'https://openalex.org/W3097777922', 'https://openalex.org/W3163793923', 'https://openalex.org/W4319862255', 'https://openalex.org/W2973049979', 'https://openalex.org/W6769196770', 'https://openalex.org/W6780218876', 'https://openalex.org/W3209059054', 'https://openalex.org/W3209984917', 'https://openalex.org/W6847363464', 'https://openalex.org/W2398826216', 'https://openalex.org/W4385822683', 'https://openalex.org/W3015356564', 'https://openalex.org/W4385570101', 'https://openalex.org/W4381786045', 'https://openalex.org/W4375869259', 'https://openalex.org/W2962780374', 'https://openalex.org/W2752796333', 'https://openalex.org/W3215615641', 'https://openalex.org/W1494198834', 'https://openalex.org/W2166637769', 'https://openalex.org/W3198694222', 'https://openalex.org/W2799473636', 'https://openalex.org/W6755559483', 'https://openalex.org/W3198587774', 'https://openalex.org/W6898634591', 'https://openalex.org/W2963242190', 'https://openalex.org/W6771467084', 'https://openalex.org/W4385822439', 'https://openalex.org/W4391021675', 'https://openalex.org/W4386566728', 'https://openalex.org/W4385565440', 'https://openalex.org/W3100460087', 'https://openalex.org/W2899274165', 'https://openalex.org/W3024605872', 'https://openalex.org/W4313679638', 'https://openalex.org/W854541894', 'https://openalex.org/W2963799213', 'https://openalex.org/W3036601975', 'https://openalex.org/W4307323391', 'https://openalex.org/W4381827575', 'https://openalex.org/W4200635400', 'https://openalex.org/W4385970143', 'https://openalex.org/W3101648800']",2024-03-18
https://openalex.org/W4392909760,https://doi.org/10.1109/icassp48485.2024.10447751,Towards Universal Speech Discrete Tokens: A Case Study for ASR and TTS,"Self-supervised learning (SSL) proficiency in speech-related tasks has driven research into utilizing discrete tokens for speech tasks like recognition and translation, which offer lower storage requirements and great potential to employ natural language processing techniques. However, these studies, mainly single-task focused, faced challenges like overfitting and performance degradation in speech recognition tasks, often at the cost of sacrificing performance in multi-task scenarios. This study presents a comprehensive comparison and optimization of discrete tokens generated by various leading SSL models in speech recognition and synthesis tasks. We aim to explore the universality of speech discrete tokens across multiple speech tasks. Experimental results demonstrate that discrete tokens achieve comparable results against systems trained on FBank features in speech recognition tasks and outperform mel-spectrogram features in speech synthesis in subjective and objective metrics. These findings suggest that universal discrete tokens have enormous potential in various speech-related tasks. Our work is open-source and publicly available at https://github.com/k2-fsa/icefall.","['https://openalex.org/W4281492411', 'https://openalex.org/W6780218876', 'https://openalex.org/W6769196770', 'https://openalex.org/W3209059054', 'https://openalex.org/W3209984917', 'https://openalex.org/W4385823092', 'https://openalex.org/W4385823152', 'https://openalex.org/W4381786045', 'https://openalex.org/W3215615641', 'https://openalex.org/W4307323391', 'https://openalex.org/W6770514103', 'https://openalex.org/W6755207826', 'https://openalex.org/W4385822683', 'https://openalex.org/W4226132755', 'https://openalex.org/W6853244311', 'https://openalex.org/W4385823130', 'https://openalex.org/W6853611000', 'https://openalex.org/W3211278025', 'https://openalex.org/W3197580070', 'https://openalex.org/W2936774411', 'https://openalex.org/W3096159803', 'https://openalex.org/W3097777922', 'https://openalex.org/W6778823374', 'https://openalex.org/W1494198834', 'https://openalex.org/W3198694222', 'https://openalex.org/W2963242190', 'https://openalex.org/W2962784628', 'https://openalex.org/W4283700324', 'https://openalex.org/W6857062747', 'https://openalex.org/W3015686596', 'https://openalex.org/W4372260432', 'https://openalex.org/W2972359262', 'https://openalex.org/W6853515095', 'https://openalex.org/W4380551955', 'https://openalex.org/W4387799863', 'https://openalex.org/W4380714544', 'https://openalex.org/W2988736778', 'https://openalex.org/W4378501656']",2024-03-18
https://openalex.org/W2964243274,https://doi.org/10.1109/icassp.2018.8461368,Natural TTS Synthesis by Conditioning Wavenet on MEL Spectrogram Predictions,"This paper describes Tacotron 2, a neural network architecture for speech synthesis directly from text. The system is composed of a recurrent sequence-to-sequence feature prediction network that maps character embeddings to mel-scale spectrograms, followed by a modified WaveNet model acting as a vocoder to synthesize time-domain waveforms from those spectrograms. Our model achieves a mean opinion score (MOS) of 4.53 comparable to a MOS of 4.58 for professionally recorded speech. To validate our design choices, we present ablation studies of key components of our system and evaluate the impact of using mel spectrograms as the conditioning input to WaveNet instead of linguistic, duration, and F0 features. We further show that using this compact acoustic intermediate representation allows for a significant reduction in the size of the WaveNet architecture.","['https://openalex.org/W2964301388', 'https://openalex.org/W2507771204', 'https://openalex.org/W6738277540', 'https://openalex.org/W6745697700', 'https://openalex.org/W2963609956', 'https://openalex.org/W6679436768', 'https://openalex.org/W2120847449', 'https://openalex.org/W2749651610', 'https://openalex.org/W6756197946', 'https://openalex.org/W2148154194', 'https://openalex.org/W6638667902', 'https://openalex.org/W2131774270', 'https://openalex.org/W2769810959', 'https://openalex.org/W2154920538', 'https://openalex.org/W6635953567', 'https://openalex.org/W6675380101', 'https://openalex.org/W6631190155', 'https://openalex.org/W2129142580', 'https://openalex.org/W2111284386', 'https://openalex.org/W2150658333', 'https://openalex.org/W6734815144', 'https://openalex.org/W1570629387', 'https://openalex.org/W2064675550', 'https://openalex.org/W6679434410', 'https://openalex.org/W6623517193', 'https://openalex.org/W6634817459', 'https://openalex.org/W6714142977', 'https://openalex.org/W6674330103', 'https://openalex.org/W2095705004', 'https://openalex.org/W1836465849', 'https://openalex.org/W2591927543', 'https://openalex.org/W4293714597', 'https://openalex.org/W2519091744', 'https://openalex.org/W2619368999', 'https://openalex.org/W854541894', 'https://openalex.org/W2102003408', 'https://openalex.org/W1579853615', 'https://openalex.org/W2766812927', 'https://openalex.org/W2901997113', 'https://openalex.org/W4294619240', 'https://openalex.org/W1599623585', 'https://openalex.org/W1522301498', 'https://openalex.org/W2133564696', 'https://openalex.org/W2130942839']",2018-04-01
https://openalex.org/W4385823335,https://doi.org/10.21437/interspeech.2023-1086,Neural Model Reprogramming with Similarity Based Mapping for Low-Resource Spoken Command Recognition,"We propose a novel adversarial reprogramming (AR) approach for low-resource spoken command recognition (SCR) and build an AR-SCR system. The AR procedure aims at repurposing a pretrained SCR model (from the source domain) to modify the acoustic signals (from the target domain). To solve the label mismatches between source and target domains and further improve the stability of AR, we propose a novel similarity-based label mapping technique to align classes. In addition, the transfer learning (TL) technique is combined with the original AR process to improve the model adaptation capability. We evaluate the proposed AR-SCR system on three low-resource SCR datasets, including Arabic, Lithuanian, and dysarthric Mandarin speech. Experimental results show that with a pretrained acoustic model trained on a large-scale English dataset, the proposed AR-SCR system outperforms the current state-of-the-art results on Lithuanian and Arabic datasets, with only a limited amount of training data.","['https://openalex.org/W3011457902', 'https://openalex.org/W3163571828', 'https://openalex.org/W3119385341', 'https://openalex.org/W3182811388', 'https://openalex.org/W4298312696', 'https://openalex.org/W2797583228', 'https://openalex.org/W2942421255', 'https://openalex.org/W2973157397', 'https://openalex.org/W3134187040', 'https://openalex.org/W3015399080', 'https://openalex.org/W4298393544', 'https://openalex.org/W3119913666', 'https://openalex.org/W3035742266', 'https://openalex.org/W1823609680', 'https://openalex.org/W2973049979', 'https://openalex.org/W3097018422', 'https://openalex.org/W2165698076', 'https://openalex.org/W2087006792', 'https://openalex.org/W2936774411', 'https://openalex.org/W3108231750', 'https://openalex.org/W2091746061', 'https://openalex.org/W3036601975', 'https://openalex.org/W2159436188', 'https://openalex.org/W2765507192', 'https://openalex.org/W3096579837', 'https://openalex.org/W3172443934', 'https://openalex.org/W2979476256', 'https://openalex.org/W2543184722', 'https://openalex.org/W2888641632', 'https://openalex.org/W2972764223', 'https://openalex.org/W2056986588', 'https://openalex.org/W3015213852', 'https://openalex.org/W3096277532', 'https://openalex.org/W1513862252', 'https://openalex.org/W2964323211']",2023-08-14
https://openalex.org/W4385567149,https://doi.org/10.18653/v1/2022.emnlp-main.759,Rethinking the Role of Demonstrations: What Makes In-Context Learning Work?,"Large language models (LMs) are able to in-context learn—perform a new task via inference alone by conditioning on a few input-label pairs (demonstrations) and making predictions for new inputs. However, there has been little understanding of how the model learns and which aspects of the demonstrations contribute to end task performance. In this paper, we show that ground truth demonstrations are in fact not required—randomly replacing labels in the demonstrations barely hurts performance on a range of classification and multi-choce tasks, consistently over 12 different models including GPT-3. Instead, we find that other aspects of the demonstrations are the key drivers of endtask performance, including the fact that they provide a few examples of (1) the label space, (2) the distribution of the input text, and (3) the overall format of the sequence. Together, our analysis provides a new way of understanding how and why in-context learning works, while opening up new questions about how much can be learned from large language models through inference alone.","['https://openalex.org/W4295601746', 'https://openalex.org/W95183648', 'https://openalex.org/W3198963017', 'https://openalex.org/W4287692594', 'https://openalex.org/W3160638507', 'https://openalex.org/W4286987939', 'https://openalex.org/W3197876970', 'https://openalex.org/W3035309367', 'https://openalex.org/W3172943453', 'https://openalex.org/W4385567093', 'https://openalex.org/W3099655892', 'https://openalex.org/W4308900200', 'https://openalex.org/W2209594346', 'https://openalex.org/W2956090150', 'https://openalex.org/W131533222', 'https://openalex.org/W4292779060', 'https://openalex.org/W3196731672', 'https://openalex.org/W4298343034', 'https://openalex.org/W1599016936', 'https://openalex.org/W3152515526', 'https://openalex.org/W4287026929', 'https://openalex.org/W4287207937', 'https://openalex.org/W3173617765', 'https://openalex.org/W4205857304', 'https://openalex.org/W2250790822', 'https://openalex.org/W1546425147', 'https://openalex.org/W3205068155', 'https://openalex.org/W2923014074', 'https://openalex.org/W4297399052', 'https://openalex.org/W4221143046', 'https://openalex.org/W2971068072', 'https://openalex.org/W4253067820', 'https://openalex.org/W2965373594', 'https://openalex.org/W3099215402', 'https://openalex.org/W2794325560', 'https://openalex.org/W3034999214', 'https://openalex.org/W3118384492', 'https://openalex.org/W3198599617', 'https://openalex.org/W4296415404', 'https://openalex.org/W2896457183', 'https://openalex.org/W3118781290', 'https://openalex.org/W3172642864', 'https://openalex.org/W4286769130', 'https://openalex.org/W4288089799', 'https://openalex.org/W2890894339', 'https://openalex.org/W4385573261', 'https://openalex.org/W4320086632', 'https://openalex.org/W2130158090', 'https://openalex.org/W4288614645', 'https://openalex.org/W2943552823', 'https://openalex.org/W3124687886', 'https://openalex.org/W3026404337', 'https://openalex.org/W2996848635', 'https://openalex.org/W2963368301', 'https://openalex.org/W2898695519', 'https://openalex.org/W4287891464', 'https://openalex.org/W4221151371']",2022-01-01
https://openalex.org/W3096251052,https://doi.org/10.21437/interspeech.2020-1160,Improving End-to-End Speech-to-Intent Classification with Reptile,"End-to-end spoken language understanding (SLU) systems have many advantages over conventional pipeline systems, but collecting in-domain speech data to train an end-to-end system is costly and time consuming. One question arises from this: how to train an end-to-end SLU with limited amounts of data? Many researchers have explored approaches that make use of other related data resources, typically by pre-training parts of the model on high-resource speech recognition. In this paper, we suggest improving the generalization performance of SLU models with a non-standard learning algorithm, Reptile. Though Reptile was originally proposed for model-agnostic meta learning, we argue that it can also be used to directly learn a target task and result in better generalization than conventional gradient descent. In this work, we employ Reptile to the task of end-to-end spoken intent classification. Experiments on four datasets of different languages and domains show improvement of intent prediction accuracy, both when Reptile is used alone and used in addition to pre-training.",[],2020-10-25
https://openalex.org/W3108231750,https://doi.org/10.3390/app10238643,Unsupervised Pre-Training for Voice Activation,"The problem of voice activation is to find a pre-defined word in the audio stream. Solutions such as keyword spotter “Ok, Google” for Android devices or keyword spotter “Alexa” for Amazon devices use tens of thousands to millions of keyword examples in training. In this paper, we explore the possibility of using pre-trained audio features to build voice activation with a small number of keyword examples. The contribution of this article consists of two parts. First, we investigate the dependence of the quality of the voice activation system on the number of examples in training for English and Russian and show that the use of pre-trained audio features, such as wav2vec, increases the accuracy of the system by up to 10% if only seven examples are available for each keyword during training. At the same time, the benefits of such features become less and disappear as the dataset size increases. Secondly, we prepare and provide for general use a dataset for training and testing voice activation for the Lithuanian language. We also provide training results on this dataset.","['https://openalex.org/W3013970744', 'https://openalex.org/W2056986588', 'https://openalex.org/W2148623807', 'https://openalex.org/W2045258274', 'https://openalex.org/W6682302527', 'https://openalex.org/W2159436188', 'https://openalex.org/W1968431955', 'https://openalex.org/W307379921', 'https://openalex.org/W2160271783', 'https://openalex.org/W2034940213', 'https://openalex.org/W2891722048', 'https://openalex.org/W2797759721', 'https://openalex.org/W2053191998', 'https://openalex.org/W3026777299', 'https://openalex.org/W3034598492', 'https://openalex.org/W2543184722', 'https://openalex.org/W2017737780', 'https://openalex.org/W2765680260', 'https://openalex.org/W6781693702', 'https://openalex.org/W2973049979', 'https://openalex.org/W2972764223', 'https://openalex.org/W2126203737', 'https://openalex.org/W2286443923', 'https://openalex.org/W6680300913', 'https://openalex.org/W2160815625', 'https://openalex.org/W2973157397', 'https://openalex.org/W3016011332', 'https://openalex.org/W2964187693', 'https://openalex.org/W6631362777', 'https://openalex.org/W2970971581', 'https://openalex.org/W6637242042', 'https://openalex.org/W2194775991', 'https://openalex.org/W1836465849', 'https://openalex.org/W6674385629', 'https://openalex.org/W1494198834', 'https://openalex.org/W2152213184', 'https://openalex.org/W1665214252', 'https://openalex.org/W3096662840', 'https://openalex.org/W2138857742', 'https://openalex.org/W2097998348']",2020-12-03
https://openalex.org/W3134187040,https://doi.org/10.3390/app11062477,A Speech Command Control-Based Recognition System for Dysarthric Patients Based on Deep Learning Technology,"Voice control is an important way of controlling mobile devices; however, using it remains a challenge for dysarthric patients. Currently, there are many approaches, such as automatic speech recognition (ASR) systems, being used to help dysarthric patients control mobile devices. However, the large computation power requirement for the ASR system increases implementation costs. To alleviate this problem, this study proposed a convolution neural network (CNN) with a phonetic posteriorgram (PPG) speech feature system to recognize speech commands, called CNN–PPG; meanwhile, the CNN model with Mel-frequency cepstral coefficient (CNN–MFCC model) and ASR-based systems were used for comparison. The experiment results show that the CNN–PPG system provided 93.49% accuracy, better than the CNN–MFCC (65.67%) and ASR-based systems (89.59%). Additionally, the CNN–PPG used a smaller model size comprising only 54% parameter numbers compared with the ASR-based system; hence, the proposed system could reduce implementation costs for users. These findings suggest that the CNN–PPG system could augment a communication device to help dysarthric patients control the mobile device via speech commands in the future.","['https://openalex.org/W2064610785', 'https://openalex.org/W2094285201', 'https://openalex.org/W2168682262', 'https://openalex.org/W2890672115', 'https://openalex.org/W6605163906', 'https://openalex.org/W6678595023', 'https://openalex.org/W2118379083', 'https://openalex.org/W2026303233', 'https://openalex.org/W2170511794', 'https://openalex.org/W2061248150', 'https://openalex.org/W2042402023', 'https://openalex.org/W1979575154', 'https://openalex.org/W2103439157', 'https://openalex.org/W4239510810', 'https://openalex.org/W2095731538', 'https://openalex.org/W2151936436', 'https://openalex.org/W3017978301', 'https://openalex.org/W3001466087', 'https://openalex.org/W3047659929', 'https://openalex.org/W2936774411', 'https://openalex.org/W2889427787', 'https://openalex.org/W2063066213', 'https://openalex.org/W2128055580', 'https://openalex.org/W3012461590', 'https://openalex.org/W2082488947', 'https://openalex.org/W2042320059', 'https://openalex.org/W2154826081', 'https://openalex.org/W2804109237', 'https://openalex.org/W258661521', 'https://openalex.org/W2148154194', 'https://openalex.org/W1728842521', 'https://openalex.org/W6675354045', 'https://openalex.org/W2160815625', 'https://openalex.org/W2747302936', 'https://openalex.org/W1995562189', 'https://openalex.org/W2112796928', 'https://openalex.org/W2509088535', 'https://openalex.org/W2591957616', 'https://openalex.org/W2964704093', 'https://openalex.org/W2316942196', 'https://openalex.org/W3013018143', 'https://openalex.org/W2890402938', 'https://openalex.org/W2938583109', 'https://openalex.org/W3097881066', 'https://openalex.org/W2364134690', 'https://openalex.org/W4231109964', 'https://openalex.org/W1993882792', 'https://openalex.org/W2509065397', 'https://openalex.org/W2512537248', 'https://openalex.org/W2402146185', 'https://openalex.org/W6638523607', 'https://openalex.org/W1572130671', 'https://openalex.org/W2162535268', 'https://openalex.org/W2793389773', 'https://openalex.org/W84813673', 'https://openalex.org/W2191779130', 'https://openalex.org/W2069976350', 'https://openalex.org/W1966976522', 'https://openalex.org/W2760231680', 'https://openalex.org/W6631362777', 'https://openalex.org/W2110415041', 'https://openalex.org/W2158195707', 'https://openalex.org/W2163321856', 'https://openalex.org/W2029835489', 'https://openalex.org/W2123995801', 'https://openalex.org/W2791062765', 'https://openalex.org/W3044806259', 'https://openalex.org/W3038010422', 'https://openalex.org/W2972838300', 'https://openalex.org/W2963654953', 'https://openalex.org/W3015843632', 'https://openalex.org/W3004622726', 'https://openalex.org/W3113144688', 'https://openalex.org/W45319298', 'https://openalex.org/W6638444622', 'https://openalex.org/W127371178', 'https://openalex.org/W2101234009', 'https://openalex.org/W3098357269', 'https://openalex.org/W2187089797']",2021-03-10
https://openalex.org/W3160747466,https://doi.org/10.1109/icassp39728.2021.9414470,MarbleNet: Deep 1D Time-Channel Separable Convolutional Neural Network for Voice Activity Detection,"We present MarbleNet, an end-to-end neural network for Voice Activity Detection (VAD). MarbleNet is a deep residual network composed from blocks of 1D time-channel separable convolution, batch-normalization, ReLU and dropout layers. When compared to a state-of-the-art VAD model, MarbleNet is able to achieve similar performance with roughly 1/10-th the parameter cost. We further conduct extensive ablation studies on different training methods and choices of parameters in order to study the robustness of MarbleNet in real-world VAD tasks.","['https://openalex.org/W2792351646', 'https://openalex.org/W2892300106', 'https://openalex.org/W2936299888', 'https://openalex.org/W2059203007', 'https://openalex.org/W2513345070', 'https://openalex.org/W3015537910', 'https://openalex.org/W2973215447', 'https://openalex.org/W6747270024', 'https://openalex.org/W3097018422', 'https://openalex.org/W2194775991', 'https://openalex.org/W2531409750', 'https://openalex.org/W6767671539', 'https://openalex.org/W1563939609', 'https://openalex.org/W2963855133', 'https://openalex.org/W2109788549', 'https://openalex.org/W2767895109', 'https://openalex.org/W6631190155', 'https://openalex.org/W2129120544', 'https://openalex.org/W2981728176', 'https://openalex.org/W1999454387', 'https://openalex.org/W2115717467', 'https://openalex.org/W2885307078', 'https://openalex.org/W2181253919', 'https://openalex.org/W6765069029', 'https://openalex.org/W6750665317', 'https://openalex.org/W3024085360', 'https://openalex.org/W2936774411', 'https://openalex.org/W2033875152', 'https://openalex.org/W6604254268', 'https://openalex.org/W6743428213', 'https://openalex.org/W2974231335', 'https://openalex.org/W104184427', 'https://openalex.org/W2797583228', 'https://openalex.org/W2964121744', 'https://openalex.org/W2746314669', 'https://openalex.org/W1522301498', 'https://openalex.org/W2781384251', 'https://openalex.org/W4297818305', 'https://openalex.org/W3017216675', 'https://openalex.org/W2953333557', 'https://openalex.org/W3097148135']",2021-05-13
https://openalex.org/W3161223924,https://doi.org/10.1109/icassp39728.2021.9414922,Semi-Supervised Spoken Language Understanding via Self-Supervised Speech and Language Model Pretraining,"Much recent work on Spoken Language Understanding (SLU) is limited in at least one of three ways: models were trained on oracle text input and neglected ASR errors, models were trained to predict only intents without the slot values, or models were trained on a large amount of in-house data. In this paper, we propose a clean and general framework to learn semantics directly from speech with semi-supervision from transcribed or untranscribed speech to address these issues. Our framework is built upon pretrained end-to-end (E2E) ASR and self-supervised language models, such as BERT, and fine-tuned on a limited amount of target SLU data. We study two semi-supervised settings for the ASR component: supervised pretraining on transcribed speech, and unsupervised pretraining by replacing the ASR encoder with self-supervised speech representations, such as wav2vec. In parallel, we identify two essential criteria for evaluating SLU models: environmental noise-robustness and E2E semantics evaluation. Experiments on ATIS show that our SLU framework with speech as input can perform on par with those using oracle text as input in semantics understanding, even though environmental noise is present and a limited amount of labeled semantics data is available for training.","['https://openalex.org/W2327501763', 'https://openalex.org/W6769457076', 'https://openalex.org/W2936774411', 'https://openalex.org/W2766219058', 'https://openalex.org/W2019116789', 'https://openalex.org/W2889201969', 'https://openalex.org/W6759393636', 'https://openalex.org/W2251599843', 'https://openalex.org/W2964117975', 'https://openalex.org/W3015267417', 'https://openalex.org/W3015412890', 'https://openalex.org/W3097414768', 'https://openalex.org/W3095552229', 'https://openalex.org/W3049038774', 'https://openalex.org/W2977838803', 'https://openalex.org/W6752437113', 'https://openalex.org/W1494198834', 'https://openalex.org/W2077302143', 'https://openalex.org/W2963446094', 'https://openalex.org/W2097550833', 'https://openalex.org/W6755207826', 'https://openalex.org/W2973133192', 'https://openalex.org/W6640440542', 'https://openalex.org/W2963288440', 'https://openalex.org/W2894164357', 'https://openalex.org/W2972584841', 'https://openalex.org/W2971351151', 'https://openalex.org/W2963250244', 'https://openalex.org/W2842511635', 'https://openalex.org/W2972943112', 'https://openalex.org/W2973049979', 'https://openalex.org/W6780218876', 'https://openalex.org/W3096249532', 'https://openalex.org/W2982223350', 'https://openalex.org/W2981458636', 'https://openalex.org/W2803609229', 'https://openalex.org/W4289564002', 'https://openalex.org/W1936920915', 'https://openalex.org/W3092630929', 'https://openalex.org/W2806429264', 'https://openalex.org/W2917128112', 'https://openalex.org/W4297808394', 'https://openalex.org/W3036601975', 'https://openalex.org/W3099782249', 'https://openalex.org/W2963341956', 'https://openalex.org/W2896457183', 'https://openalex.org/W2979722627', 'https://openalex.org/W3099944122']",2021-05-13
https://openalex.org/W4372260337,https://doi.org/10.1109/icassp49357.2023.10096680,Torchaudio-Squim: Reference-Less Speech Quality and Intelligibility Measures in Torchaudio,"Measuring quality and intelligibility of a speech signal is usually a critical step in development of speech processing systems. To enable this, a variety of metrics to measure quality and intelligibility under different assumptions have been developed. Through this paper, we introduce tools and a set of models to estimate such known metrics using deep neural networks. These models are made available in the well-established TorchAudio library, the core audio and speech processing library within the PyTorch deep learning framework. We refer to it as TorchAudio-Squim, TorchAudio-Speech QUality and Intelligibility Measures. More specifically, in the current version of TorchAudio-squim, we establish and release models for estimating PESQ, STOI and SI-SDR among objective metrics and MOS among subjective metrics. We develop a novel approach for objective metric estimation and use a recently developed approach for subjective metric estimation. These models operate in a ""referenceless"" manner, that is they do not require the corresponding clean speech as reference for speech assessment. Given the unavailability of clean speech and the effortful process of subjective evaluation in real-world situations, such easy-to-use tools would greatly benefit speech processing research and development.","['https://openalex.org/W6775804823', 'https://openalex.org/W4225302959', 'https://openalex.org/W6785906987', 'https://openalex.org/W6770208069', 'https://openalex.org/W6770232298', 'https://openalex.org/W6795684877', 'https://openalex.org/W3015644200', 'https://openalex.org/W2157600120', 'https://openalex.org/W6611703090', 'https://openalex.org/W6802584641', 'https://openalex.org/W3198102247', 'https://openalex.org/W1552314771', 'https://openalex.org/W3208743843', 'https://openalex.org/W6801588699', 'https://openalex.org/W4297841766', 'https://openalex.org/W6739901393', 'https://openalex.org/W3015199127', 'https://openalex.org/W2141998673', 'https://openalex.org/W2963403924', 'https://openalex.org/W2799258971', 'https://openalex.org/W3097906045', 'https://openalex.org/W6802816715', 'https://openalex.org/W3202278141', 'https://openalex.org/W3161480375', 'https://openalex.org/W6761921182', 'https://openalex.org/W4296068974', 'https://openalex.org/W3196475561', 'https://openalex.org/W6729924827', 'https://openalex.org/W2991361823', 'https://openalex.org/W2557915412', 'https://openalex.org/W3200245256', 'https://openalex.org/W3207932315', 'https://openalex.org/W3015545615', 'https://openalex.org/W2972394484', 'https://openalex.org/W4311167834', 'https://openalex.org/W3105857760', 'https://openalex.org/W3161364427', 'https://openalex.org/W2990118552', 'https://openalex.org/W4385245566', 'https://openalex.org/W4361745739']",2023-05-05
https://openalex.org/W2972495969,https://doi.org/10.21437/interspeech.2019-1951,Direct Speech-to-Speech Translation with a Sequence-to-Sequence Model,"We present an attention-based sequence-to-sequence neural network which can directly translate speech from one language into speech in another language, without relying on an intermediate text representation.The network is trained end-to-end, learning to map speech spectrograms into target spectrograms in another language, corresponding to the translated content (in a different canonical voice).We further demonstrate the ability to synthesize translated speech using the voice of the source speaker.We conduct experiments on two Spanish-to-English speech translation datasets, and find that the proposed model slightly underperforms a baseline cascade of a direct speech-to-text translation model and a text-to-speech synthesis model, demonstrating the feasibility of the approach on this very challenging task.","['https://openalex.org/W2963609956', 'https://openalex.org/W2133564696', 'https://openalex.org/W4298174729', 'https://openalex.org/W2747920239', 'https://openalex.org/W2972970915', 'https://openalex.org/W2892620417', 'https://openalex.org/W4293569541', 'https://openalex.org/W2133300417', 'https://openalex.org/W2964138190', 'https://openalex.org/W2963779652', 'https://openalex.org/W2097203679', 'https://openalex.org/W4293714597', 'https://openalex.org/W4385245566', 'https://openalex.org/W2113106066', 'https://openalex.org/W2963011080', 'https://openalex.org/W2964243274', 'https://openalex.org/W2962824709', 'https://openalex.org/W2139647714', 'https://openalex.org/W2525778437', 'https://openalex.org/W2120847449', 'https://openalex.org/W1962947832', 'https://openalex.org/W2896538040', 'https://openalex.org/W2928941594', 'https://openalex.org/W1494198834', 'https://openalex.org/W2011783148', 'https://openalex.org/W2808706139', 'https://openalex.org/W2795581297', 'https://openalex.org/W2912492482', 'https://openalex.org/W3012492057', 'https://openalex.org/W1538023239', 'https://openalex.org/W2605131327', 'https://openalex.org/W2152834109', 'https://openalex.org/W4289383906', 'https://openalex.org/W1537859740', 'https://openalex.org/W2136545725', 'https://openalex.org/W4294619240', 'https://openalex.org/W2788357188', 'https://openalex.org/W2949328740', 'https://openalex.org/W4298580827', 'https://openalex.org/W2941115821', 'https://openalex.org/W2794490148', 'https://openalex.org/W4300558631']",2019-09-13
https://openalex.org/W3180374548,https://doi.org/10.18653/v1/2022.acl-long.235,Direct Speech-to-Speech Translation With Discrete Units,"Ann Lee, Peng-Jen Chen, Changhan Wang, Jiatao Gu, Sravya Popuri, Xutai Ma, Adam Polyak, Yossi Adi, Qing He, Yun Tang, Juan Pino, Wei-Ning Hsu. Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). 2022.","['https://openalex.org/W2933138175', 'https://openalex.org/W3007068036', 'https://openalex.org/W3197580070', 'https://openalex.org/W3157923770', 'https://openalex.org/W3210177631', 'https://openalex.org/W2605131327', 'https://openalex.org/W2995181338', 'https://openalex.org/W3112092703', 'https://openalex.org/W3007142233', 'https://openalex.org/W3173767661', 'https://openalex.org/W3098403858', 'https://openalex.org/W3169320628', 'https://openalex.org/W2152834109', 'https://openalex.org/W2035108931', 'https://openalex.org/W3033411150', 'https://openalex.org/W4394671563', 'https://openalex.org/W2963609956', 'https://openalex.org/W4385245566', 'https://openalex.org/W2963532001', 'https://openalex.org/W3140429000', 'https://openalex.org/W2963979492', 'https://openalex.org/W4287553982', 'https://openalex.org/W2949328740', 'https://openalex.org/W3160525311', 'https://openalex.org/W3112616666', 'https://openalex.org/W2936774411', 'https://openalex.org/W2991213871', 'https://openalex.org/W2963403868', 'https://openalex.org/W2998353611', 'https://openalex.org/W3118578889', 'https://openalex.org/W1494198834', 'https://openalex.org/W1537859740', 'https://openalex.org/W2136545725', 'https://openalex.org/W3142316150', 'https://openalex.org/W2127141656', 'https://openalex.org/W3175871055', 'https://openalex.org/W2903739847', 'https://openalex.org/W2972495969', 'https://openalex.org/W2973157397', 'https://openalex.org/W3186843219', 'https://openalex.org/W3119308075', 'https://openalex.org/W2097203679', 'https://openalex.org/W2747920239', 'https://openalex.org/W3130016944', 'https://openalex.org/W3099782249', 'https://openalex.org/W3092424727', 'https://openalex.org/W3036601975', 'https://openalex.org/W3092028330']",2022-01-01
https://openalex.org/W4297841405,https://doi.org/10.21437/interspeech.2022-10884,Phonetic Analysis of Self-supervised Representations of English Speech,"We present an analysis of discrete units discovered via selfsupervised representation learning on English speech.We focus on units produced by a pre-trained HuBERT model due to its wide adoption in ASR, speech synthesis, and many other tasks.Whereas previous work has evaluated the quality of such quantization models in aggregate over all phones for a given language, we break our analysis down into broad phonetic classes, taking into account specific aspects of their articulation when considering their alignment to discrete units.We find that these units correspond to sub-phonetic events, and that fine dynamics such as the distinct closure and release portions of plosives tend to be represented by sequences of discrete units.Our work provides a reference for the phonetic properties of discrete units discovered by HuBERT, facilitating analyses of many speech applications based on this model.","['https://openalex.org/W1494198834', 'https://openalex.org/W4288107125', 'https://openalex.org/W1524333225', 'https://openalex.org/W3160200310', 'https://openalex.org/W3140429000', 'https://openalex.org/W3036601975', 'https://openalex.org/W3197580070', 'https://openalex.org/W2896457183', 'https://openalex.org/W3095361818', 'https://openalex.org/W3160799772', 'https://openalex.org/W2998572311', 'https://openalex.org/W4206711328', 'https://openalex.org/W2748598007', 'https://openalex.org/W3169320628', 'https://openalex.org/W4394671563', 'https://openalex.org/W2395899413']",2022-09-16
https://openalex.org/W4391021530,https://doi.org/10.1109/asru57964.2023.10389731,Prompting and Adapter Tuning For Self-Supervised Encoder-Decoder Speech Model,"Prompting and adapter tuning have emerged as efficient alternatives to fine-tuning (FT) methods. However, existing studies on speech prompting focused on classification tasks and failed on more complex sequence generation tasks. Besides, adapter tuning is primarily applied with a focus on encoder-only self-supervised models. Our experiments show that prompting on Wav2Seq, a self-supervised encoder-decoder model, surpasses previous works in sequence generation tasks. It achieves a remarkable 53% relative improvement in word error rate for ASR and a 27% in F1 score for slot filling. Additionally, prompting competes with the FT method in the low-resource scenario. Moreover, we show the transferability of prompting and adapter tuning on Wav2Seq in cross-lingual ASR. When limited trainable parameters are involved, prompting and adapter tuning consistently outperform conventional FT across 7 languages. Notably, in the low-resource scenario, prompting consistently outperforms adapter tuning.","['https://openalex.org/W2842511635', 'https://openalex.org/W6780218876', 'https://openalex.org/W3209059054', 'https://openalex.org/W3197580070', 'https://openalex.org/W4281492411', 'https://openalex.org/W3129009457', 'https://openalex.org/W3198217962', 'https://openalex.org/W4377865046', 'https://openalex.org/W4372270126', 'https://openalex.org/W3185341429', 'https://openalex.org/W6759579507', 'https://openalex.org/W4226162428', 'https://openalex.org/W6850477478', 'https://openalex.org/W4372270069', 'https://openalex.org/W4319862642', 'https://openalex.org/W3209984917', 'https://openalex.org/W4226278833', 'https://openalex.org/W4381786045', 'https://openalex.org/W4281672148', 'https://openalex.org/W2970476646', 'https://openalex.org/W3098267758', 'https://openalex.org/W3174770825', 'https://openalex.org/W4285247752', 'https://openalex.org/W6852859116', 'https://openalex.org/W6752946794', 'https://openalex.org/W6809645183', 'https://openalex.org/W4372346241', 'https://openalex.org/W4372260195', 'https://openalex.org/W6738045163', 'https://openalex.org/W6796581206', 'https://openalex.org/W4225410153', 'https://openalex.org/W3172698324', 'https://openalex.org/W6846600677', 'https://openalex.org/W6750665317', 'https://openalex.org/W2972584841', 'https://openalex.org/W1494198834', 'https://openalex.org/W2995181338', 'https://openalex.org/W3161223924', 'https://openalex.org/W3095410713', 'https://openalex.org/W6802744804', 'https://openalex.org/W2963211188', 'https://openalex.org/W4297808394', 'https://openalex.org/W4298312696', 'https://openalex.org/W3036601975', 'https://openalex.org/W4394671563', 'https://openalex.org/W4379539302', 'https://openalex.org/W4385245566', 'https://openalex.org/W3168867926', 'https://openalex.org/W4307783813', 'https://openalex.org/W4322825254', 'https://openalex.org/W2797583228', 'https://openalex.org/W4221155122']",2023-12-16
https://openalex.org/W4372270126,https://doi.org/10.1109/icassp49357.2023.10096988,Wav2Seq: Pre-Training Speech-to-Text Encoder-Decoder Models Using Pseudo Languages,"We introduce Wav2Seq, the first self-supervised approach to pre-train both parts of encoder-decoder models for speech data. We induce a pseudo language as a compact discrete representation, and formulate a self-supervised pseudo speech recognition task — transcribing audio inputs into pseudo subword sequences. This process stands on its own, or can be applied as low-cost second-stage pre-training. We experiment with automatic speech recognition (ASR), spoken named entity recognition, and speech-to-text translation. We set new state-of-the-art results for end-to-end spoken named entity recognition, and show consistent improvements on 8 language pairs for speech-to-text translation, even when competing methods use additional text data for training. On ASR, our approach enables encoder-decoder methods to benefit from pre-training for all parts of the network, and shows comparable performance to highly optimized recent methods.","['https://openalex.org/W3097777922', 'https://openalex.org/W3200129129', 'https://openalex.org/W3095173472', 'https://openalex.org/W6675354045', 'https://openalex.org/W6638749077', 'https://openalex.org/W2963347649', 'https://openalex.org/W6810007534', 'https://openalex.org/W6729239390', 'https://openalex.org/W2933138175', 'https://openalex.org/W2995181338', 'https://openalex.org/W2953190524', 'https://openalex.org/W6668990524', 'https://openalex.org/W2962780374', 'https://openalex.org/W2142838865', 'https://openalex.org/W2973049979', 'https://openalex.org/W2842511635', 'https://openalex.org/W6623517193', 'https://openalex.org/W3095918555', 'https://openalex.org/W6679436768', 'https://openalex.org/W2914417638', 'https://openalex.org/W3198299542', 'https://openalex.org/W2327501763', 'https://openalex.org/W6769627184', 'https://openalex.org/W3034999214', 'https://openalex.org/W6755207826', 'https://openalex.org/W6769196770', 'https://openalex.org/W3213029956', 'https://openalex.org/W3119308075', 'https://openalex.org/W3198429080', 'https://openalex.org/W4226278833', 'https://openalex.org/W3001434439', 'https://openalex.org/W6850036870', 'https://openalex.org/W3173767661', 'https://openalex.org/W6790356757', 'https://openalex.org/W3205644108', 'https://openalex.org/W6601894380', 'https://openalex.org/W4226103796', 'https://openalex.org/W3197580070', 'https://openalex.org/W6727336983', 'https://openalex.org/W6795346631', 'https://openalex.org/W6780218876', 'https://openalex.org/W6781927165', 'https://openalex.org/W1494198834', 'https://openalex.org/W4287887773', 'https://openalex.org/W1828163288', 'https://openalex.org/W2101234009', 'https://openalex.org/W3160799772', 'https://openalex.org/W2130942839', 'https://openalex.org/W3036601975', 'https://openalex.org/W46679369', 'https://openalex.org/W4394671563', 'https://openalex.org/W4221145109', 'https://openalex.org/W854541894', 'https://openalex.org/W2520160253', 'https://openalex.org/W4297808394', 'https://openalex.org/W3054645415', 'https://openalex.org/W4288089799', 'https://openalex.org/W4372270126', 'https://openalex.org/W2073459066', 'https://openalex.org/W2896457183', 'https://openalex.org/W2979476256', 'https://openalex.org/W2549416390']",2023-05-05
https://openalex.org/W4392902623,https://doi.org/10.1109/icassp48485.2024.10448257,"Dynamic-Superb: Towards a Dynamic, Collaborative, and Comprehensive Instruction-Tuning Benchmark For Speech","Text language models have shown remarkable zero-shot capability in generalizing to unseen tasks when provided with well-formulated instructions. However, existing studies in speech processing primarily focus on limited or specific tasks. Moreover, the lack of standardized benchmarks hinders a fair comparison across different approaches. Thus, we present Dynamic-SUPERB, a benchmark designed for building universal speech models capable of leveraging instruction tuning to perform multiple tasks in a zero-shot fashion. To achieve comprehensive coverage of diverse speech tasks and harness instruction tuning, we invite the community to collaborate and contribute, facilitating the dynamic growth of the benchmark. To initiate, Dynamic-SUPERB features 55 evaluation instances by combining 33 tasks and 22 datasets. This spans a broad spectrum of dimensions, providing a comprehensive platform for evaluation. Additionally, we propose several approaches to establish benchmark baselines. These include the utilization of speech models, text language models, and the multimodal encoder. Evaluation results indicate that while these baselines perform reasonably on seen tasks, they struggle with unseen ones. We release all materials to the public and welcome researchers to collaborate on the project, advancing technologies in the field together <sup xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">1</sup> .","['https://openalex.org/W2842511635', 'https://openalex.org/W6780218876', 'https://openalex.org/W3209059054', 'https://openalex.org/W4281492411', 'https://openalex.org/W4226162428', 'https://openalex.org/W6850477478', 'https://openalex.org/W6852859116', 'https://openalex.org/W3174770825', 'https://openalex.org/W4285247752', 'https://openalex.org/W4386187806', 'https://openalex.org/W4205991051', 'https://openalex.org/W3098267758', 'https://openalex.org/W4385822890', 'https://openalex.org/W4319862642', 'https://openalex.org/W6853249747', 'https://openalex.org/W4225410153', 'https://openalex.org/W3176693010', 'https://openalex.org/W6800875267', 'https://openalex.org/W6853998256', 'https://openalex.org/W4393178509', 'https://openalex.org/W6786696081', 'https://openalex.org/W3197580070', 'https://openalex.org/W4226103796', 'https://openalex.org/W3189296823', 'https://openalex.org/W6846004400', 'https://openalex.org/W4319862635', 'https://openalex.org/W4385822439', 'https://openalex.org/W4319862652', 'https://openalex.org/W6755207826', 'https://openalex.org/W6790356757', 'https://openalex.org/W6847363464', 'https://openalex.org/W4386071707', 'https://openalex.org/W6850625674', 'https://openalex.org/W6854866820', 'https://openalex.org/W1494198834', 'https://openalex.org/W6917585676', 'https://openalex.org/W6772349387', 'https://openalex.org/W4385807453', 'https://openalex.org/W6855594702', 'https://openalex.org/W6852489829', 'https://openalex.org/W6851948999', 'https://openalex.org/W4385823351', 'https://openalex.org/W4297808394', 'https://openalex.org/W4384918448', 'https://openalex.org/W4379539302', 'https://openalex.org/W4322718191', 'https://openalex.org/W4381827575', 'https://openalex.org/W4377130946', 'https://openalex.org/W4386555501', 'https://openalex.org/W4361229539', 'https://openalex.org/W4322825254', 'https://openalex.org/W3036601975', 'https://openalex.org/W4367628410', 'https://openalex.org/W4307322847', 'https://openalex.org/W2998572311', 'https://openalex.org/W4394671563']",2024-03-18
https://openalex.org/W4288089799,https://doi.org/10.48550/arxiv.1910.10683,Exploring the Limits of Transfer Learning with a Unified Text-to-Text\n Transformer,"Transfer learning, where a model is first pre-trained on a data-rich task\nbefore being fine-tuned on a downstream task, has emerged as a powerful\ntechnique in natural language processing (NLP). The effectiveness of transfer\nlearning has given rise to a diversity of approaches, methodology, and\npractice. In this paper, we explore the landscape of transfer learning\ntechniques for NLP by introducing a unified framework that converts all\ntext-based language problems into a text-to-text format. Our systematic study\ncompares pre-training objectives, architectures, unlabeled data sets, transfer\napproaches, and other factors on dozens of language understanding tasks. By\ncombining the insights from our exploration with scale and our new ``Colossal\nClean Crawled Corpus'', we achieve state-of-the-art results on many benchmarks\ncovering summarization, question answering, text classification, and more. To\nfacilitate future work on transfer learning for NLP, we release our data set,\npre-trained models, and code.\n",[],2019-10-23
https://openalex.org/W2985913104,https://doi.org/10.1109/icassp40776.2020.9052974,Pyannote.Audio: Neural Building Blocks for Speaker Diarization,"We introduce pyannote.audio, an open-source toolkit written in Python for speaker diarization. Based on PyTorch machine learning framework, it provides a set of trainable end-to-end neural building blocks that can be combined and jointly optimized to build speaker diarization pipelines. pyannote.audio also comes with pre-trained models covering a wide range of domains for voice activity detection, speaker change detection, overlapped speech detection, and speaker embedding -- reaching state-of-the-art performance for most of them.","['https://openalex.org/W2767895109', 'https://openalex.org/W2746241180', 'https://openalex.org/W3015780472', 'https://openalex.org/W2889203268', 'https://openalex.org/W2890964092', 'https://openalex.org/W2750259098', 'https://openalex.org/W2891247151', 'https://openalex.org/W2808631503', 'https://openalex.org/W2968071640', 'https://openalex.org/W2972449503', 'https://openalex.org/W6631362777', 'https://openalex.org/W2889233255', 'https://openalex.org/W2192412620', 'https://openalex.org/W2972680151', 'https://openalex.org/W2009876684', 'https://openalex.org/W2964052309', 'https://openalex.org/W2083751884', 'https://openalex.org/W6602944679', 'https://openalex.org/W2097470427', 'https://openalex.org/W72302491', 'https://openalex.org/W1524333225']",2020-04-09
https://openalex.org/W1821462560,https://doi.org/10.48550/arxiv.1503.02531,Distilling the Knowledge in a Neural Network,"A very simple way to improve the performance of almost any machine learning algorithm is to train many different models on the same data and then to average their predictions. Unfortunately, making predictions using a whole ensemble of models is cumbersome and may be too computationally expensive to allow deployment to a large number of users, especially if the individual models are large neural nets. Caruana and his collaborators have shown that it is possible to compress the knowledge in an ensemble into a single model which is much easier to deploy and we develop this approach further using a different compression technique. We achieve some surprising results on MNIST and we show that we can significantly improve the acoustic model of a heavily used commercial system by distilling the knowledge in an ensemble of models into a single model. We also introduce a new type of ensemble composed of one or more full models and many specialist models which learn to distinguish fine-grained classes that the full models confuse. Unlike a mixture of experts, these specialist models can be trained rapidly and in parallel.","['https://openalex.org/W2163605009', 'https://openalex.org/W1534477342', 'https://openalex.org/W2150884987', 'https://openalex.org/W1904365287', 'https://openalex.org/W2160815625', 'https://openalex.org/W2294370754', 'https://openalex.org/W2095705004', 'https://openalex.org/W2402040300', 'https://openalex.org/W2168231600']",2015-03-09
https://openalex.org/W4296710617,https://doi.org/10.1162/tacl_a_00505,DP-Parse: Finding Word Boundaries from Raw Speech with an Instance Lexicon,"Abstract Finding word boundaries in continuous speech is challenging as there is little or no equivalent of a ‘space’ delimiter between words. Popular Bayesian non-parametric models for text segmentation (Goldwater et al., 2006, 2009) use a Dirichlet process to jointly segment sentences and build a lexicon of word types. We introduce DP-Parse, which uses similar principles but only relies on an instance lexicon of word tokens, avoiding the clustering errors that arise with a lexicon of word types. On the Zero Resource Speech Benchmark 2017, our model sets a new speech segmentation state-of-the-art in 5 languages. The algorithm monotonically improves with better input representations, achieving yet higher scores when fed with weakly supervised inputs. Despite lacking a type lexicon, DP-Parse can be pipelined to a language model and learn semantic and syntactic representations as assessed by a new spoken word embedding benchmark. 1","['https://openalex.org/W4223486244', 'https://openalex.org/W3044967013', 'https://openalex.org/W6780218876', 'https://openalex.org/W6640135064', 'https://openalex.org/W3097159218', 'https://openalex.org/W3198782837', 'https://openalex.org/W3015783745', 'https://openalex.org/W2407151108', 'https://openalex.org/W2963425185', 'https://openalex.org/W3209993061', 'https://openalex.org/W6755207826', 'https://openalex.org/W2963620343', 'https://openalex.org/W3093096176', 'https://openalex.org/W6731922460', 'https://openalex.org/W4235505822', 'https://openalex.org/W2946322623', 'https://openalex.org/W2126377586', 'https://openalex.org/W2122228338', 'https://openalex.org/W6682948231', 'https://openalex.org/W6796554684', 'https://openalex.org/W3108506107', 'https://openalex.org/W3037580942', 'https://openalex.org/W3146777637', 'https://openalex.org/W2057007397', 'https://openalex.org/W2117126688', 'https://openalex.org/W6756098772', 'https://openalex.org/W6810047917', 'https://openalex.org/W2468716020', 'https://openalex.org/W1997505733', 'https://openalex.org/W2964169922', 'https://openalex.org/W3198134274', 'https://openalex.org/W2952125979', 'https://openalex.org/W6790356757', 'https://openalex.org/W6675022971', 'https://openalex.org/W1778492285', 'https://openalex.org/W6793306531', 'https://openalex.org/W6691362072', 'https://openalex.org/W6636510571', 'https://openalex.org/W2140991203', 'https://openalex.org/W3110458199', 'https://openalex.org/W2842511635', 'https://openalex.org/W1494198834', 'https://openalex.org/W2114347655', 'https://openalex.org/W2118020555', 'https://openalex.org/W2962850179', 'https://openalex.org/W2398490608', 'https://openalex.org/W2395899413', 'https://openalex.org/W2962736743', 'https://openalex.org/W2913062184', 'https://openalex.org/W2346964103', 'https://openalex.org/W1700952868', 'https://openalex.org/W3036601975', 'https://openalex.org/W2896457183', 'https://openalex.org/W4313182775', 'https://openalex.org/W101045393', 'https://openalex.org/W2771976988', 'https://openalex.org/W4394671563', 'https://openalex.org/W2519091744', 'https://openalex.org/W2963720603', 'https://openalex.org/W4287591426', 'https://openalex.org/W2613000335', 'https://openalex.org/W4297808394', 'https://openalex.org/W3095706145', 'https://openalex.org/W3096196861', 'https://openalex.org/W1614298861', 'https://openalex.org/W2916009164', 'https://openalex.org/W3151510603', 'https://openalex.org/W3209059054', 'https://openalex.org/W3098643042', 'https://openalex.org/W4240908132']",2022-01-01
https://openalex.org/W2117126688,https://doi.org/10.7551/mitpress/7503.003.0085,Adaptor Grammars: A Framework for Specifying Compositional Nonparametric Bayesian Models,"This paper introduces adaptor grammars, a class of probabilistic models of language that generalize probabilistic context-free grammars (PCFGs).Adaptor grammars augment the probabilistic rules of PCFGs with ""adaptors"" that can induce dependencies among successive uses.With a particular choice of adaptor, based on the Pitman-Yor process, nonparametric Bayesian models of language using Dirichlet processes and hierarchical Dirichlet processes can be written as simple grammars.We present a general-purpose inference algorithm for adaptor grammars, making it easy to define and use such models, and illustrate how several existing nonparametric Bayesian models can be expressed within this framework.","['https://openalex.org/W2122228338', 'https://openalex.org/W2069429561', 'https://openalex.org/W2150507172', 'https://openalex.org/W2158266063', 'https://openalex.org/W2074546930', 'https://openalex.org/W2087309226', 'https://openalex.org/W2159399018', 'https://openalex.org/W1583697620', 'https://openalex.org/W2053218206', 'https://openalex.org/W2952343510']",2007-09-07
https://openalex.org/W3213029956,https://doi.org/10.21437/interspeech.2022-143,XLS-R: Self-supervised Cross-lingual Speech Representation Learning at Scale,"This paper presents XLS-R, a large-scale model for cross-lingual speech representation learning based on wav2vec 2.0.We train models with up to 2B parameters on nearly half a million hours of publicly available speech audio in 128 languages, an order of magnitude more public data than the largest known prior work.Our evaluation covers a wide range of tasks, domains, data regimes and languages, both high and low-resource.On the CoVoST-2 speech translation benchmark, we improve the previous state of the art by an average of 7.4 BLEU over 21 translation directions into English.For speech recognition, XLS-R improves over the best known prior work on BABEL, MLS, CommonVoice as well as VoxPopuli, lowering error rates by 14-34% relative on average.XLS-R also sets a new state of the art on VoxLin-gua107 language identification.Moreover, we show that with sufficient model size, cross-lingual pretraining can perform as well as English-only pretraining when translating English speech into other languages, a setting which favors monolingual pretraining.We hope XLS-R can help to improve speech processing tasks for many more languages of the world.Models and code are available at www.github.","['https://openalex.org/W3185293939', 'https://openalex.org/W2842511635', 'https://openalex.org/W2726515241', 'https://openalex.org/W3030437843', 'https://openalex.org/W2898630520', 'https://openalex.org/W2973049979', 'https://openalex.org/W3198429080', 'https://openalex.org/W2979476256', 'https://openalex.org/W3156643189', 'https://openalex.org/W2989539713', 'https://openalex.org/W3036601975', 'https://openalex.org/W1494198834', 'https://openalex.org/W3034978746', 'https://openalex.org/W3159481202', 'https://openalex.org/W2127141656', 'https://openalex.org/W2914120296', 'https://openalex.org/W2896457183', 'https://openalex.org/W3193521535', 'https://openalex.org/W2963403868', 'https://openalex.org/W3082274269', 'https://openalex.org/W3174724858', 'https://openalex.org/W3204696009', 'https://openalex.org/W2996159613', 'https://openalex.org/W4288089799', 'https://openalex.org/W3016181583', 'https://openalex.org/W4287391717', 'https://openalex.org/W3093517588', 'https://openalex.org/W2964121744', 'https://openalex.org/W3107826490', 'https://openalex.org/W2124509324', 'https://openalex.org/W3098903812', 'https://openalex.org/W3159134453', 'https://openalex.org/W2963027641', 'https://openalex.org/W2671812860', 'https://openalex.org/W3197580070', 'https://openalex.org/W3035390927', 'https://openalex.org/W2970119519', 'https://openalex.org/W2958953787', 'https://openalex.org/W1522301498', 'https://openalex.org/W3198771897', 'https://openalex.org/W3002741552', 'https://openalex.org/W2338908902', 'https://openalex.org/W3102342027', 'https://openalex.org/W2811079561', 'https://openalex.org/W2975381464', 'https://openalex.org/W3119866685', 'https://openalex.org/W4287213456', 'https://openalex.org/W3169483174', 'https://openalex.org/W4210463634', 'https://openalex.org/W2962942158', 'https://openalex.org/W2331143823', 'https://openalex.org/W2963341956', 'https://openalex.org/W4226033575', 'https://openalex.org/W3005680577', 'https://openalex.org/W2962784628', 'https://openalex.org/W3139878283', 'https://openalex.org/W2950170869', 'https://openalex.org/W2970049541', 'https://openalex.org/W4385245566', 'https://openalex.org/W2101105183', 'https://openalex.org/W3160525311', 'https://openalex.org/W3160799772', 'https://openalex.org/W3144173820', 'https://openalex.org/W3093579165', 'https://openalex.org/W2996383576', 'https://openalex.org/W4297808394', 'https://openalex.org/W2292087804', 'https://openalex.org/W3214173179', 'https://openalex.org/W3054645415', 'https://openalex.org/W3095410713', 'https://openalex.org/W2995181338', 'https://openalex.org/W3167533889', 'https://openalex.org/W3035579820', 'https://openalex.org/W3032816972', 'https://openalex.org/W2933138175', 'https://openalex.org/W2891555348', 'https://openalex.org/W3119308075', 'https://openalex.org/W3099782249', 'https://openalex.org/W3015213852', 'https://openalex.org/W3035524453', 'https://openalex.org/W2547875792', 'https://openalex.org/W4292779060', 'https://openalex.org/W3173767661', 'https://openalex.org/W2730658205']",2022-09-16
https://openalex.org/W3144810982,https://doi.org/10.48550/arxiv.2007.00991,Data Augmenting Contrastive Learning of Speech Representations in the\n Time Domain,"Contrastive Predictive Coding (CPC), based on predicting future segments of\nspeech based on past segments is emerging as a powerful algorithm for\nrepresentation learning of speech signal. However, it still under-performs\nother methods on unsupervised evaluation benchmarks. Here, we introduce\nWavAugment, a time-domain data augmentation library and find that applying\naugmentation in the past is generally more efficient and yields better\nperformances than other methods. We find that a combination of pitch\nmodification, additive noise and reverberation substantially increase the\nperformance of CPC (relative improvement of 18-22%), beating the reference\nLibri-light results with 600 times less data. Using an out-of-domain dataset,\ntime-domain data augmentation can push CPC to be on par with the state of the\nart on the Zero Speech Benchmark 2017. We also show that time-domain data\naugmentation consistently improves downstream limited-supervision phoneme\nclassification tasks by a factor of 12-15% relative.\n","['https://openalex.org/W3015213852', 'https://openalex.org/W2148349024', 'https://openalex.org/W2346964103', 'https://openalex.org/W2593779438', 'https://openalex.org/W2407080277', 'https://openalex.org/W3005680577', 'https://openalex.org/W2972943112', 'https://openalex.org/W2696967604', 'https://openalex.org/W2184343439', 'https://openalex.org/W2990583358', 'https://openalex.org/W2963620343', 'https://openalex.org/W2055408826', 'https://openalex.org/W2883725317', 'https://openalex.org/W3100270690', 'https://openalex.org/W2973049979', 'https://openalex.org/W4288107125', 'https://openalex.org/W2786608204', 'https://openalex.org/W3016011332', 'https://openalex.org/W2995181338', 'https://openalex.org/W4297808394', 'https://openalex.org/W3016181583', 'https://openalex.org/W3002741552', 'https://openalex.org/W2973026522', 'https://openalex.org/W2930682606', 'https://openalex.org/W2100768664', 'https://openalex.org/W2787447541', 'https://openalex.org/W3093427098', 'https://openalex.org/W3015783745', 'https://openalex.org/W3102342027', 'https://openalex.org/W4300047444', 'https://openalex.org/W3093096176', 'https://openalex.org/W2940544976', 'https://openalex.org/W3125709657', 'https://openalex.org/W4214784181', 'https://openalex.org/W2983785920', 'https://openalex.org/W2952217990', 'https://openalex.org/W2963074118', 'https://openalex.org/W2509930204', 'https://openalex.org/W2117041980', 'https://openalex.org/W2946822591', 'https://openalex.org/W1494198834', 'https://openalex.org/W2219249508', 'https://openalex.org/W2970971581', 'https://openalex.org/W2842511635', 'https://openalex.org/W2347098582', 'https://openalex.org/W3003875258', 'https://openalex.org/W4295312788', 'https://openalex.org/W1989674786', 'https://openalex.org/W2020607164', 'https://openalex.org/W2936774411']",2020-07-02
https://openalex.org/W4224875474,https://doi.org/10.21437/interspeech.2022-10652,"Word Discovery in Visually Grounded, Self-Supervised Speech Models",OursFigure 1: HuBERT: sum of attention weights each frame receives from other frames.Ours (VG-HuBERT3): attention weights each frame receives from the [CLS A] token.Attention weights from different attention heads are coded with different colors.,"['https://openalex.org/W4230640548', 'https://openalex.org/W2963620343', 'https://openalex.org/W2988907666', 'https://openalex.org/W2057007397', 'https://openalex.org/W3174311593', 'https://openalex.org/W3157861865', 'https://openalex.org/W3097159218', 'https://openalex.org/W2964169922', 'https://openalex.org/W2145410271', 'https://openalex.org/W1861492603', 'https://openalex.org/W2995680346', 'https://openalex.org/W3036601975', 'https://openalex.org/W3209993061', 'https://openalex.org/W2025482506', 'https://openalex.org/W2483390977', 'https://openalex.org/W30845872', 'https://openalex.org/W3097485645', 'https://openalex.org/W3198411039', 'https://openalex.org/W3094502228', 'https://openalex.org/W4385245566', 'https://openalex.org/W2398490608', 'https://openalex.org/W4286973758', 'https://openalex.org/W4214813806', 'https://openalex.org/W2481240925', 'https://openalex.org/W3198782837', 'https://openalex.org/W4226380987', 'https://openalex.org/W4297808394', 'https://openalex.org/W2468716020', 'https://openalex.org/W2556930864', 'https://openalex.org/W3159476814', 'https://openalex.org/W3198134274', 'https://openalex.org/W2972892814', 'https://openalex.org/W2137010615', 'https://openalex.org/W3169320628', 'https://openalex.org/W2796315435', 'https://openalex.org/W3159481202', 'https://openalex.org/W4313182775', 'https://openalex.org/W2989358187', 'https://openalex.org/W2114347655', 'https://openalex.org/W4394453761', 'https://openalex.org/W2896457183', 'https://openalex.org/W3171345413', 'https://openalex.org/W1778492285', 'https://openalex.org/W4294955557', 'https://openalex.org/W4221161768']",2022-09-16
https://openalex.org/W4313182775,https://doi.org/10.1109/taslp.2022.3229264,Word Segmentation on Discovered Phone Units With Dynamic Programming and Self-Supervised Scoring,"Recent work on unsupervised speech segmentation has used self-supervised models with phone and word segmentation modules that are trained jointly. This paper instead revisits an older approach to word segmentation: bottom-up phone-like unit discovery is performed first, and symbolic word segmentation is then performed on top of the discovered units (without influencing the lower level). To do this, I propose a new unit discovery model, a new symbolic word segmentation model, and then chain the two models to segment speech. Both models use dynamic programming to minimize segment costs from a self-supervised network with an additional duration penalty that encourages longer units. Concretely, for acoustic unit discovery, duration-penalized dynamic programming (DPDP) is used with a contrastive predictive coding model as the scoring network. For word segmentation, DPDP is applied with an autoencoding recurrent neural as the scoring network. The two models are chained in order to segment speech. This approach gives comparable word segmentation results to state-of-the-art joint self-supervised segmentation models on an English benchmark. On French, Mandarin, German and Wolof data, it outperforms previous systems on the ZeroSpeech benchmarks. Analysis shows that the chained DPDP system segments shorter filler words well, but longer words might require some external top-down signal.","['https://openalex.org/W2025482506', 'https://openalex.org/W2126377586', 'https://openalex.org/W2126449874', 'https://openalex.org/W6676025551', 'https://openalex.org/W1778492285', 'https://openalex.org/W3102667484', 'https://openalex.org/W2978134885', 'https://openalex.org/W4313053756', 'https://openalex.org/W2398490608', 'https://openalex.org/W2468716020', 'https://openalex.org/W2964169922', 'https://openalex.org/W2802557066', 'https://openalex.org/W6844194202', 'https://openalex.org/W3198782837', 'https://openalex.org/W3209993061', 'https://openalex.org/W2940544976', 'https://openalex.org/W3093096176', 'https://openalex.org/W2140991203', 'https://openalex.org/W2251563156', 'https://openalex.org/W2035750955', 'https://openalex.org/W3198134274', 'https://openalex.org/W2758697525', 'https://openalex.org/W2963620343', 'https://openalex.org/W3197974236', 'https://openalex.org/W2117041980', 'https://openalex.org/W51277926', 'https://openalex.org/W6675022971', 'https://openalex.org/W2963137467', 'https://openalex.org/W2962799131', 'https://openalex.org/W2927191280', 'https://openalex.org/W2972943112', 'https://openalex.org/W3095361818', 'https://openalex.org/W3198815374', 'https://openalex.org/W3100270690', 'https://openalex.org/W6769196770', 'https://openalex.org/W6786696081', 'https://openalex.org/W3209059054', 'https://openalex.org/W2971775690', 'https://openalex.org/W6774456908', 'https://openalex.org/W1945490744', 'https://openalex.org/W3096656254', 'https://openalex.org/W130754613', 'https://openalex.org/W2995181338', 'https://openalex.org/W6738767006', 'https://openalex.org/W3006094508', 'https://openalex.org/W4247319331', 'https://openalex.org/W6676028815', 'https://openalex.org/W2074546930', 'https://openalex.org/W2345913943', 'https://openalex.org/W2952125979', 'https://openalex.org/W2786608204', 'https://openalex.org/W2295297373', 'https://openalex.org/W2145410271', 'https://openalex.org/W6631190155', 'https://openalex.org/W2057007397', 'https://openalex.org/W3097485645', 'https://openalex.org/W3097159218', 'https://openalex.org/W4296710617', 'https://openalex.org/W2114347655', 'https://openalex.org/W2415378728', 'https://openalex.org/W6729977899', 'https://openalex.org/W6728167234', 'https://openalex.org/W2964115348', 'https://openalex.org/W4285110637', 'https://openalex.org/W4224875474', 'https://openalex.org/W4206075291', 'https://openalex.org/W6634986005', 'https://openalex.org/W1984076147', 'https://openalex.org/W3098643042', 'https://openalex.org/W2973026522', 'https://openalex.org/W2972574141', 'https://openalex.org/W4297808394', 'https://openalex.org/W2949115596']",2022-12-14
https://openalex.org/W2943152387,https://doi.org/10.48550/arxiv.1905.00546,Billion-scale semi-supervised learning for image classification,"This paper presents a study of semi-supervised learning with large convolutional networks. We propose a pipeline, based on a teacher/student paradigm, that leverages a large collection of unlabelled images (up to 1 billion). Our main goal is to improve the performance for a given target architecture, like ResNet-50 or ResNext. We provide an extensive analysis of the success factors of our approach, which leads us to formulate some recommendations to produce high-accuracy models for image classification with semi-supervised learning. As a result, our approach brings important gains to standard architectures for image, video and fine-grained classification. For instance, by leveraging one billion unlabelled images, our learned vanilla ResNet-50 achieves 81.2% top-1 accuracy on the ImageNet benchmark.","['https://openalex.org/W2804047946', 'https://openalex.org/W2549139847', 'https://openalex.org/W2622263826', 'https://openalex.org/W2901394229', 'https://openalex.org/W2127279985', 'https://openalex.org/W2963444553', 'https://openalex.org/W2963785012', 'https://openalex.org/W2963524571', 'https://openalex.org/W2142992480', 'https://openalex.org/W1797268635', 'https://openalex.org/W2302255633', 'https://openalex.org/W2747804831', 'https://openalex.org/W2963703197', 'https://openalex.org/W2108598243', 'https://openalex.org/W2167922717', 'https://openalex.org/W2041334360', 'https://openalex.org/W2250384498', 'https://openalex.org/W2991040477', 'https://openalex.org/W2962722184', 'https://openalex.org/W3151142710', 'https://openalex.org/W2796346823', 'https://openalex.org/W2772114784', 'https://openalex.org/W2163605009', 'https://openalex.org/W2963420686', 'https://openalex.org/W2963446712', 'https://openalex.org/W2765407302', 'https://openalex.org/W2963091558', 'https://openalex.org/W2963216553', 'https://openalex.org/W2151575489', 'https://openalex.org/W2117539524', 'https://openalex.org/W2194775991', 'https://openalex.org/W2963078860', 'https://openalex.org/W2161381512', 'https://openalex.org/W2148349024', 'https://openalex.org/W2949117887', 'https://openalex.org/W2593864460', 'https://openalex.org/W2165698076', 'https://openalex.org/W1544092585', 'https://openalex.org/W2113290770', 'https://openalex.org/W1821462560', 'https://openalex.org/W2214754659', 'https://openalex.org/W2962810352']",2019-05-02
https://openalex.org/W2111316763,https://doi.org/10.1109/tit.1965.1053799,Probability of error of some adaptive pattern-recognition machines,"A simple taught pattern-recognition machine for detecting an unknown, fixed, randomly occurring pattern is derived using a Bayes' approach, and its probability of error is analyzed. It is shown that with probability one, the machine converges to the optimal detector (a matched filter) for the unknown pattern, that the asymptotic decision function statistics are Gaussian, and that for an important class of problems, the central-limit theorem can be invoked to calculate the approximate probability of error at any stage of convergence. An untaught adaptive pattern-recognition machine may be made from the taught machine by using its own output instead of a teacher, and the asymptotic probability of error of this device is derived. It is shown that it does not converge to a matched filter for the unknown pattern, but that in any practical case it performs almost as well. Finally, the results of an experimental simulation of both machines are presented as curves of the relative frequency of error vs. time, and are compared with the values calculated by theory.","['https://openalex.org/W2099870790', 'https://openalex.org/W2134390549', 'https://openalex.org/W2108362946', 'https://openalex.org/W2322091001', 'https://openalex.org/W4246168425', 'https://openalex.org/W2053863499', 'https://openalex.org/W4255783720']",1965-07-01
https://openalex.org/W3110761489,https://doi.org/10.48550/arxiv.2012.02221,A Correspondence Variational Autoencoder for Unsupervised Acoustic Word Embeddings,"We propose a new unsupervised model for mapping a variable-duration speech segment to a fixed-dimensional representation. The resulting acoustic word embeddings can form the basis of search, discovery, and indexing systems for low- and zero-resource languages. Our model, which we refer to as a maximal sampling correspondence variational autoencoder (MCVAE), is a recurrent neural network (RNN) trained with a novel self-supervised correspondence loss that encourages consistency between embeddings of different instances of the same word. Our training scheme improves on previous correspondence training approaches through the use and comparison of multiple samples from the approximate posterior distribution. In the zero-resource setting, the MCVAE can be trained in an unsupervised way, without any ground-truth word pairs, by using the word-like segments discovered via an unsupervised term discovery system. In both this setting and a semi-supervised low-resource setting (with a limited set of ground-truth word pairs), the MCVAE outperforms previous state-of-the-art models, such as Siamese-, CAE- and VAE-based RNNs.","['https://openalex.org/W3162468843', 'https://openalex.org/W2295297373', 'https://openalex.org/W2963425185', 'https://openalex.org/W2962736743', 'https://openalex.org/W148837159', 'https://openalex.org/W3150750326', 'https://openalex.org/W2963879199', 'https://openalex.org/W2753738274', 'https://openalex.org/W2964121744', 'https://openalex.org/W2963720603', 'https://openalex.org/W2900308871', 'https://openalex.org/W2932675979', 'https://openalex.org/W3131709275', 'https://openalex.org/W3015325583', 'https://openalex.org/W2296681920', 'https://openalex.org/W2157331557', 'https://openalex.org/W2973096781', 'https://openalex.org/W2190506272', 'https://openalex.org/W2057007397', 'https://openalex.org/W2990155616', 'https://openalex.org/W2114347655', 'https://openalex.org/W2964000524', 'https://openalex.org/W2145410271', 'https://openalex.org/W1959608418', 'https://openalex.org/W2963879374', 'https://openalex.org/W2963403868', 'https://openalex.org/W2963223306', 'https://openalex.org/W2566587499', 'https://openalex.org/W2962980711', 'https://openalex.org/W2059652594', 'https://openalex.org/W3008444411', 'https://openalex.org/W2407151108', 'https://openalex.org/W2964169922', 'https://openalex.org/W2963522047', 'https://openalex.org/W30845872', 'https://openalex.org/W2291770225', 'https://openalex.org/W3033010920', 'https://openalex.org/W2091746061', 'https://openalex.org/W2171590421', 'https://openalex.org/W3125709657']",2020-12-03
https://openalex.org/W4389524018,https://doi.org/10.18653/v1/2023.emnlp-main.182,Generative Spoken Language Model based on continuous word-sized audio tokens,"In NLP, text language models based on words or subwords are known to outperform their character-based counterparts. Yet, in the speech community, the standard input of spoken LMs are 20ms or 40ms-long discrete units (shorter than a phoneme). Taking inspiration from word-based LM, we introduce a Generative Spoken Language Model (GSLM) based on word-size continuous-valued audio tokens that can generate diverse and expressive language output. This is obtained by replacing lookup table for lexical types with a Lexical Embedding function, the cross entropy loss by a contrastive loss, and multinomial sampling by k-NN sampling. The resulting model is the first generative language model based on word-size continuous tokens. Its performance is on par with discrete unit GSLMs regarding generation quality as measured by automatic metrics and subjective human judgements. Moreover, it is five times more memory efficient thanks to its large 200ms units. In addition, the embeddings before and after the Lexical Embedder are phonetically and semantically interpretable.","['https://openalex.org/W2979476256', 'https://openalex.org/W4221164184', 'https://openalex.org/W4299585995', 'https://openalex.org/W2121227244', 'https://openalex.org/W4385245566', 'https://openalex.org/W4292825791', 'https://openalex.org/W2145410271', 'https://openalex.org/W2963456134', 'https://openalex.org/W4286984129', 'https://openalex.org/W2519091744', 'https://openalex.org/W3092028330', 'https://openalex.org/W2079656678', 'https://openalex.org/W2970006822', 'https://openalex.org/W3148101939', 'https://openalex.org/W4295308567', 'https://openalex.org/W2972374322', 'https://openalex.org/W1614298861', 'https://openalex.org/W3096656254', 'https://openalex.org/W4313182775', 'https://openalex.org/W101045393', 'https://openalex.org/W2346964103', 'https://openalex.org/W2785896739', 'https://openalex.org/W4224875474', 'https://openalex.org/W4381786045', 'https://openalex.org/W2766812927', 'https://openalex.org/W4224308101', 'https://openalex.org/W2913062184', 'https://openalex.org/W4372260156', 'https://openalex.org/W3133702157', 'https://openalex.org/W2777302760', 'https://openalex.org/W4303649106', 'https://openalex.org/W2768381684', 'https://openalex.org/W3140429000', 'https://openalex.org/W2131738223', 'https://openalex.org/W2947445680', 'https://openalex.org/W4214633470', 'https://openalex.org/W3146777637', 'https://openalex.org/W3036601975', 'https://openalex.org/W2126377586', 'https://openalex.org/W4286902103', 'https://openalex.org/W3110761489', 'https://openalex.org/W3197580070', 'https://openalex.org/W1810943226', 'https://openalex.org/W1494198834', 'https://openalex.org/W3215615641', 'https://openalex.org/W2963720603', 'https://openalex.org/W2963425185', 'https://openalex.org/W3097692357', 'https://openalex.org/W46679369', 'https://openalex.org/W4223486244', 'https://openalex.org/W3213873715', 'https://openalex.org/W4303519914', 'https://openalex.org/W2160473997', 'https://openalex.org/W2152790380', 'https://openalex.org/W2950414763', 'https://openalex.org/W4287887366', 'https://openalex.org/W2964243274', 'https://openalex.org/W2963979492']",2023-01-01
https://openalex.org/W1778492285,https://doi.org/10.1162/tacl_a_00146,Unsupervised Lexicon Discovery from Acoustic Input,"We present a model of unsupervised phonological lexicon discovery—the problem of simultaneously learning phoneme-like and word-like units from acoustic input. Our model builds on earlier models of unsupervised phone-like unit discovery from acoustic data (Lee and Glass, 2012), and unsupervised symbolic lexicon discovery using the Adaptor Grammar framework (Johnson et al., 2006), integrating these earlier approaches using a probabilistic model of phonological variation. We show that the model is competitive with state-of-the-art spoken term discovery systems, and present analyses exploring the model’s behavior and the kinds of linguistic structures it learns.","['https://openalex.org/W1964917299', 'https://openalex.org/W2074546930', 'https://openalex.org/W2162505970', 'https://openalex.org/W2108207895', 'https://openalex.org/W2148154194', 'https://openalex.org/W2069429561', 'https://openalex.org/W2130518563', 'https://openalex.org/W2111732304', 'https://openalex.org/W2101711363', 'https://openalex.org/W2042143122', 'https://openalex.org/W2126377586', 'https://openalex.org/W4251556668', 'https://openalex.org/W1990005915', 'https://openalex.org/W2142390309', 'https://openalex.org/W1980862600', 'https://openalex.org/W2115867364', 'https://openalex.org/W2026858810', 'https://openalex.org/W1969608442', 'https://openalex.org/W2030118973', 'https://openalex.org/W4301623764', 'https://openalex.org/W2111668269', 'https://openalex.org/W2154093685', 'https://openalex.org/W2164151151', 'https://openalex.org/W2126203737', 'https://openalex.org/W1992613273', 'https://openalex.org/W2087309226', 'https://openalex.org/W2114347655', 'https://openalex.org/W2107038463', 'https://openalex.org/W1548450879', 'https://openalex.org/W3036063182', 'https://openalex.org/W2408712009', 'https://openalex.org/W2083195487', 'https://openalex.org/W2140991203', 'https://openalex.org/W2022201897', 'https://openalex.org/W2025482506', 'https://openalex.org/W2053306448', 'https://openalex.org/W2126953647', 'https://openalex.org/W2952343510', 'https://openalex.org/W2116422968', 'https://openalex.org/W2170353620', 'https://openalex.org/W4237961478', 'https://openalex.org/W2100768664', 'https://openalex.org/W2070554026', 'https://openalex.org/W2033413759', 'https://openalex.org/W2117126688', 'https://openalex.org/W2086891622', 'https://openalex.org/W2130042265', 'https://openalex.org/W2188924248', 'https://openalex.org/W1650210997', 'https://openalex.org/W30845872', 'https://openalex.org/W2226889031', 'https://openalex.org/W1779834323', 'https://openalex.org/W2126449874', 'https://openalex.org/W2163245285', 'https://openalex.org/W1525748279', 'https://openalex.org/W2157381218', 'https://openalex.org/W1530250655', 'https://openalex.org/W1843673427', 'https://openalex.org/W1583697620', 'https://openalex.org/W1978470410']",2015-12-01
https://openalex.org/W4226033575,https://doi.org/10.1109/asru51503.2021.9688253,w2v-BERT: Combining Contrastive Learning and Masked Language Modeling for Self-Supervised Speech Pre-Training,"Motivated by the success of masked language modeling (MLM) in pre-training natural language processing models, we propose w2v-BERT that explores MLM for self-supervised speech representation learning. w2v-BERT is a framework that combines contrastive learning and MLM, where the former trains the model to discretize input continuous speech signals into a finite set of discriminative speech tokens, and the latter trains the model to learn contextualized speech representations via solving a masked prediction task consuming the discretized tokens. In contrast to existing MLM-based speech pre-training frameworks such as HuBERT, which relies on an iterative re-clustering and re-training process, or vq-wav2vec, which concatenates two separately trained modules, w2v-BERT can be optimized in an end-to-end fashion by solving the two self-supervised tasks (the contrastive task and MLM) simultaneously. Our experiments show that w2v-BERT achieves competitive results compared to current state-of-the-art pre-trained models on the LibriSpeech benchmarks when using the Libri-Light 60k corpus as the unsupervised data. In particular, when compared to published models such as conformer-based wav2vec 2.0 and HuBERT, our model shows 5% to 10% relative WER reduction on the test-clean and test-other subsets. When applied to the Google's Voice Search traffic dataset, w2v-BERT outperforms our internal conformer-based wav2vec 2.0 by more than 30% relatively.","['https://openalex.org/W2936774411', 'https://openalex.org/W6751104502', 'https://openalex.org/W6638667902', 'https://openalex.org/W6746023985', 'https://openalex.org/W2064675550', 'https://openalex.org/W6638749077', 'https://openalex.org/W6631190155', 'https://openalex.org/W2121879602', 'https://openalex.org/W1494198834', 'https://openalex.org/W6771812881', 'https://openalex.org/W2842511635', 'https://openalex.org/W3015995734', 'https://openalex.org/W2972943112', 'https://openalex.org/W2973049979', 'https://openalex.org/W3015265920', 'https://openalex.org/W3016011332', 'https://openalex.org/W2982223350', 'https://openalex.org/W3035202887', 'https://openalex.org/W3041561163', 'https://openalex.org/W3003875258', 'https://openalex.org/W6786669483', 'https://openalex.org/W6739901393', 'https://openalex.org/W3015522062', 'https://openalex.org/W6770514103', 'https://openalex.org/W2111316763', 'https://openalex.org/W2940322076', 'https://openalex.org/W3097777922', 'https://openalex.org/W6770506093', 'https://openalex.org/W165878654', 'https://openalex.org/W2928408492', 'https://openalex.org/W2101210369', 'https://openalex.org/W2088622183', 'https://openalex.org/W6784532283', 'https://openalex.org/W3160525311', 'https://openalex.org/W3093579165', 'https://openalex.org/W3157697407', 'https://openalex.org/W6755207826', 'https://openalex.org/W3026041220', 'https://openalex.org/W6780218876', 'https://openalex.org/W6769196770', 'https://openalex.org/W3209059054', 'https://openalex.org/W4385245566', 'https://openalex.org/W4293569541', 'https://openalex.org/W1522301498', 'https://openalex.org/W2988736778', 'https://openalex.org/W4320930577', 'https://openalex.org/W3128768055', 'https://openalex.org/W2979476256', 'https://openalex.org/W1836465849', 'https://openalex.org/W3112034174', 'https://openalex.org/W2991213871', 'https://openalex.org/W2962907457', 'https://openalex.org/W3036601975', 'https://openalex.org/W2995181338', 'https://openalex.org/W4210463634', 'https://openalex.org/W2896457183', 'https://openalex.org/W1828163288', 'https://openalex.org/W4297808394']",2021-12-13
https://openalex.org/W1522301498,https://doi.org/10.48550/arxiv.1412.6980,Adam: A Method for Stochastic Optimization,"We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments. The method is straightforward to implement, is computationally efficient, has little memory requirements, is invariant to diagonal rescaling of the gradients, and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. Empirical results demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. Finally, we discuss AdaMax, a variant of Adam based on the infinity norm.","['https://openalex.org/W1984541135', 'https://openalex.org/W6908809', 'https://openalex.org/W2100495367', 'https://openalex.org/W1810943226', 'https://openalex.org/W35527955', 'https://openalex.org/W2143612262', 'https://openalex.org/W104184427', 'https://openalex.org/W1904365287', 'https://openalex.org/W2113459411', 'https://openalex.org/W2950351588', 'https://openalex.org/W59018853', 'https://openalex.org/W2146502635', 'https://openalex.org/W2160815625', 'https://openalex.org/W3176583243', 'https://openalex.org/W2156779765', 'https://openalex.org/W2148825261', 'https://openalex.org/W2951004968', 'https://openalex.org/W1598497354', 'https://openalex.org/W1491468723', 'https://openalex.org/W2086161653']",2014-12-22
https://openalex.org/W4378105483,https://doi.org/10.48550/arxiv.2305.13516,"Scaling Speech Technology to 1,000+ Languages","Expanding the language coverage of speech technology has the potential to improve access to information for many more people. However, current speech technology is restricted to about one hundred languages which is a small fraction of the over 7,000 languages spoken around the world. The Massively Multilingual Speech (MMS) project increases the number of supported languages by 10-40x, depending on the task. The main ingredients are a new dataset based on readings of publicly available religious texts and effectively leveraging self-supervised learning. We built pre-trained wav2vec 2.0 models covering 1,406 languages, a single multilingual automatic speech recognition model for 1,107 languages, speech synthesis models for the same number of languages, as well as a language identification model for 4,017 languages. Experiments show that our multilingual speech recognition model more than halves the word error rate of Whisper on 54 languages of the FLEURS benchmark while being trained on a small fraction of the labeled data.",[],2023-05-22
https://openalex.org/W3209984917,https://doi.org/10.1109/jstsp.2022.3188113,WavLM: Large-Scale Self-Supervised Pre-Training for Full Stack Speech Processing,"Self-supervised learning (SSL) achieves great success in speech recognition,\nwhile limited exploration has been attempted for other speech processing tasks.\nAs speech signal contains multi-faceted information including speaker identity,\nparalinguistics, spoken content, etc., learning universal representations for\nall speech tasks is challenging. To tackle the problem, we propose a new\npre-trained model, WavLM, to solve full-stack downstream speech tasks. WavLM\njointly learns masked speech prediction and denoising in pre-training. By this\nmeans, WavLM does not only keep the speech content modeling capability by the\nmasked speech prediction, but also improves the potential to non-ASR tasks by\nthe speech denoising. In addition, WavLM employs gated relative position bias\nfor the Transformer structure to better capture the sequence ordering of input\nspeech. We also scale up the training dataset from 60k hours to 94k hours.\nWavLM Large achieves state-of-the-art performance on the SUPERB benchmark, and\nbrings significant improvements for various speech processing tasks on their\nrepresentative benchmarks. The code and pre-trained models are available at\nhttps://aka.ms/wavlm.\n","['https://openalex.org/W2896457183', 'https://openalex.org/W6763701032', 'https://openalex.org/W6769627184', 'https://openalex.org/W6844194202', 'https://openalex.org/W6780218876', 'https://openalex.org/W3209059054', 'https://openalex.org/W6788335241', 'https://openalex.org/W3197580070', 'https://openalex.org/W2995181338', 'https://openalex.org/W6791904447', 'https://openalex.org/W3198270883', 'https://openalex.org/W2963122170', 'https://openalex.org/W3008181812', 'https://openalex.org/W3198771897', 'https://openalex.org/W3175898847', 'https://openalex.org/W3198694222', 'https://openalex.org/W3119308075', 'https://openalex.org/W3024869864', 'https://openalex.org/W2981087920', 'https://openalex.org/W3016232124', 'https://openalex.org/W3163842642', 'https://openalex.org/W3212886388', 'https://openalex.org/W4225661121', 'https://openalex.org/W2951974815', 'https://openalex.org/W2962850167', 'https://openalex.org/W6745117592', 'https://openalex.org/W3100270690', 'https://openalex.org/W3097286738', 'https://openalex.org/W2972943112', 'https://openalex.org/W3016011332', 'https://openalex.org/W3035202887', 'https://openalex.org/W3198858531', 'https://openalex.org/W3041561163', 'https://openalex.org/W2982223350', 'https://openalex.org/W3015265920', 'https://openalex.org/W3206996142', 'https://openalex.org/W2973049979', 'https://openalex.org/W6769196770', 'https://openalex.org/W3015356564', 'https://openalex.org/W4226033575', 'https://openalex.org/W2973157397', 'https://openalex.org/W3015213852', 'https://openalex.org/W3212799896', 'https://openalex.org/W3206252155', 'https://openalex.org/W3204696009', 'https://openalex.org/W6739901393', 'https://openalex.org/W3197042120', 'https://openalex.org/W3194687854', 'https://openalex.org/W1494198834', 'https://openalex.org/W6796242362', 'https://openalex.org/W3016181583', 'https://openalex.org/W2808631503', 'https://openalex.org/W6688816777', 'https://openalex.org/W2969985801', 'https://openalex.org/W6801723603', 'https://openalex.org/W3094374485', 'https://openalex.org/W2157161740', 'https://openalex.org/W125553504', 'https://openalex.org/W3095212884', 'https://openalex.org/W6779069803', 'https://openalex.org/W3178462146', 'https://openalex.org/W4220731890', 'https://openalex.org/W3196857193', 'https://openalex.org/W2696967604', 'https://openalex.org/W2972949456', 'https://openalex.org/W123007118', 'https://openalex.org/W3095189764', 'https://openalex.org/W2069681747', 'https://openalex.org/W2765425905', 'https://openalex.org/W2803322398', 'https://openalex.org/W2117678320', 'https://openalex.org/W2030486566', 'https://openalex.org/W6757817989', 'https://openalex.org/W3160936850', 'https://openalex.org/W3193846000', 'https://openalex.org/W3205495812', 'https://openalex.org/W6770506093', 'https://openalex.org/W3016010032', 'https://openalex.org/W3095173472', 'https://openalex.org/W3097777922', 'https://openalex.org/W2127141656', 'https://openalex.org/W2936774411', 'https://openalex.org/W6768080748', 'https://openalex.org/W2331143823', 'https://openalex.org/W2953190524', 'https://openalex.org/W3205644108', 'https://openalex.org/W2963618559', 'https://openalex.org/W3026868282', 'https://openalex.org/W2963341956', 'https://openalex.org/W2996383576', 'https://openalex.org/W2950813464', 'https://openalex.org/W3004728855', 'https://openalex.org/W3125709657', 'https://openalex.org/W3099782249', 'https://openalex.org/W4287120025', 'https://openalex.org/W3178296206', 'https://openalex.org/W2219249508', 'https://openalex.org/W2758785877', 'https://openalex.org/W4385245566', 'https://openalex.org/W4297808394', 'https://openalex.org/W3169688220', 'https://openalex.org/W4300427991', 'https://openalex.org/W3125596972', 'https://openalex.org/W2975381464', 'https://openalex.org/W4308349017', 'https://openalex.org/W4287374065', 'https://openalex.org/W2991213871', 'https://openalex.org/W3095292526', 'https://openalex.org/W3112034174', 'https://openalex.org/W3082274269', 'https://openalex.org/W2972712416', 'https://openalex.org/W3157923770', 'https://openalex.org/W3144173820', 'https://openalex.org/W3169320628', 'https://openalex.org/W4288089799', 'https://openalex.org/W3198698812', 'https://openalex.org/W4307023467', 'https://openalex.org/W2970597249', 'https://openalex.org/W2963403868', 'https://openalex.org/W3174648740', 'https://openalex.org/W3139918052', 'https://openalex.org/W4285250921', 'https://openalex.org/W2726515241', 'https://openalex.org/W4294796045', 'https://openalex.org/W4306169301', 'https://openalex.org/W3165647589', 'https://openalex.org/W2979476256', 'https://openalex.org/W3033627755', 'https://openalex.org/W2842511635', 'https://openalex.org/W2908510526', 'https://openalex.org/W3002741552', 'https://openalex.org/W3036601975', 'https://openalex.org/W3162648834']",2022-07-04
https://openalex.org/W3035060554,https://doi.org/10.48550/arxiv.2006.07733,Bootstrap your own latent: A new approach to self-supervised Learning,"We introduce Bootstrap Your Own Latent (BYOL), a new approach to self-supervised image representation learning. BYOL relies on two neural networks, referred to as online and target networks, that interact and learn from each other. From an augmented view of an image, we train the online network to predict the target network representation of the same image under a different augmented view. At the same time, we update the target network with a slow-moving average of the online network. While state-of-the art methods rely on negative pairs, BYOL achieves a new state of the art without them. BYOL reaches $74.3\%$ top-1 classification accuracy on ImageNet using a linear evaluation with a ResNet-50 architecture and $79.6\%$ with a larger ResNet. We show that BYOL performs on par or better than the current state of the art on both transfer and semi-supervised benchmarks. Our implementation and pretrained models are given on GitHub.","['https://openalex.org/W2902098903', 'https://openalex.org/W1479807131', 'https://openalex.org/W2047643928', 'https://openalex.org/W2533598788', 'https://openalex.org/W2949517790', 'https://openalex.org/W2321533354', 'https://openalex.org/W2148349024', 'https://openalex.org/W2963470893', 'https://openalex.org/W2155541015', 'https://openalex.org/W1990334093', 'https://openalex.org/W3008526508', 'https://openalex.org/W2606611007', 'https://openalex.org/W1665214252', 'https://openalex.org/W2917551568', 'https://openalex.org/W2613718673', 'https://openalex.org/W2964291307', 'https://openalex.org/W1903029394', 'https://openalex.org/W12634471', 'https://openalex.org/W2970241862', 'https://openalex.org/W2117539524', 'https://openalex.org/W2063971957', 'https://openalex.org/W2963350250', 'https://openalex.org/W2951970475', 'https://openalex.org/W2308529009', 'https://openalex.org/W1815076433', 'https://openalex.org/W2108501770', 'https://openalex.org/W2401231614', 'https://openalex.org/W2964159205', 'https://openalex.org/W2964043796', 'https://openalex.org/W1844684489', 'https://openalex.org/W2138011018', 'https://openalex.org/W830076066', 'https://openalex.org/W2129068307', 'https://openalex.org/W2971155163', 'https://openalex.org/W3007125775', 'https://openalex.org/W2194775991', 'https://openalex.org/W2996501936', 'https://openalex.org/W1677182931', 'https://openalex.org/W2952716587', 'https://openalex.org/W2842511635', 'https://openalex.org/W2981952041', 'https://openalex.org/W3005680577', 'https://openalex.org/W2622263826', 'https://openalex.org/W2145339207', 'https://openalex.org/W2998388430', 'https://openalex.org/W2757910899', 'https://openalex.org/W2025768430', 'https://openalex.org/W2732026016', 'https://openalex.org/W3022061250', 'https://openalex.org/W2913939497', 'https://openalex.org/W2136922672', 'https://openalex.org/W2575671312', 'https://openalex.org/W3009561768', 'https://openalex.org/W2951873722', 'https://openalex.org/W2017814585', 'https://openalex.org/W2994536315', 'https://openalex.org/W3001197829', 'https://openalex.org/W2146444479', 'https://openalex.org/W2411541852', 'https://openalex.org/W2949759968', 'https://openalex.org/W1686810756', 'https://openalex.org/W2951004968', 'https://openalex.org/W2944828972', 'https://openalex.org/W3118608800', 'https://openalex.org/W3102631365', 'https://openalex.org/W2326925005', 'https://openalex.org/W2963591054', 'https://openalex.org/W2592691248', 'https://openalex.org/W2978426779', 'https://openalex.org/W2962824366', 'https://openalex.org/W3034859493', 'https://openalex.org/W2031489346', 'https://openalex.org/W2099471712', 'https://openalex.org/W3036982689', 'https://openalex.org/W1909320841', 'https://openalex.org/W2155904486', 'https://openalex.org/W2412320034', 'https://openalex.org/W2987283559', 'https://openalex.org/W2183341477', 'https://openalex.org/W2749988060', 'https://openalex.org/W3035058308', 'https://openalex.org/W2785325870', 'https://openalex.org/W1836465849', 'https://openalex.org/W2963263347', 'https://openalex.org/W1977295328', 'https://openalex.org/W2161381512', 'https://openalex.org/W3026092005', 'https://openalex.org/W2101926813', 'https://openalex.org/W1846799578', 'https://openalex.org/W2804935296', 'https://openalex.org/W1993309459', 'https://openalex.org/W2102605133', 'https://openalex.org/W2946856970', 'https://openalex.org/W343636949', 'https://openalex.org/W2173248099', 'https://openalex.org/W2883725317', 'https://openalex.org/W2798991696', 'https://openalex.org/W2963420272']",2020-06-13
https://openalex.org/W2100768664,,A Nonparametric Bayesian Approach to Acoustic Model Discovery,"We investigate the problem of acoustic modeling in which prior language-specific knowledge and transcribed data are unavailable. We present an unsupervised model that simultaneously segments the speech, discovers a proper set of sub-word units (e.g., phones) and learns a Hidden Markov Model (HMM) for each induced acoustic unit. Our approach is formulated as a Dirichlet process mixture model in which each mixture is an HMM that represents a sub-word unit. We apply our model to the TIMIT corpus, and the results demonstrate that our model discovers sub-word units that are highly correlated with English phones and also produces better segmentation than the state-of-the-art unsupervised baseline. We test the quality of the learned acoustic models on a spoken term detection task. Compared to the baselines, our model improves the relative precision of top hits by at least 22.1 % and outperforms a language-mismatched acoustic model. 1","['https://openalex.org/W2099415988', 'https://openalex.org/W2080972498', 'https://openalex.org/W53570656', 'https://openalex.org/W2117041980', 'https://openalex.org/W2120636621', 'https://openalex.org/W2154093685', 'https://openalex.org/W2077804127', 'https://openalex.org/W2401464865', 'https://openalex.org/W2121997342', 'https://openalex.org/W2026858810', 'https://openalex.org/W1957665339', 'https://openalex.org/W1990005915', 'https://openalex.org/W2083904075', 'https://openalex.org/W3127686677', 'https://openalex.org/W2045656233', 'https://openalex.org/W2126377586', 'https://openalex.org/W2148154194', 'https://openalex.org/W2111732304', 'https://openalex.org/W2171752983', 'https://openalex.org/W2126203737', 'https://openalex.org/W3104490327', 'https://openalex.org/W2403642609']",2012-07-08
https://openalex.org/W3035160371,https://doi.org/10.1109/cvpr42600.2020.01070,Self-Training With Noisy Student Improves ImageNet Classification,"We present a simple self-training method that achieves 88.4% top-1 accuracy on ImageNet, which is 2.0% better than the state-of-the-art model that requires 3.5B weakly labeled Instagram images. On robustness test sets, it improves ImageNet-A top-1 accuracy from 61.0% to 83.7%, reduces ImageNet-C mean corruption error from 45.7 to 28.3, and reduces ImageNet-P mean flip rate from 27.8 to 12.2. To achieve this result, we first train an EfficientNet model on labeled ImageNet images and use it as a teacher to generate pseudo labels on 300M unlabeled images. We then train a larger EfficientNet as a student model on the combination of labeled and pseudo labeled images. We iterate this process by putting back the student as the teacher. During the generation of the pseudo labels, the teacher is not noised so that the pseudo labels are as accurate as possible. However, during the learning of the student, we inject noise such as dropout, stochastic depth and data augmentation via RandAugment to the student so that the student generalizes better than the teacher.","['https://openalex.org/W2979805229', 'https://openalex.org/W6756718674', 'https://openalex.org/W6757555829', 'https://openalex.org/W2194775991', 'https://openalex.org/W6701947533', 'https://openalex.org/W2963446712', 'https://openalex.org/W2752782242', 'https://openalex.org/W6729383884', 'https://openalex.org/W6761731747', 'https://openalex.org/W2889326796', 'https://openalex.org/W2902617128', 'https://openalex.org/W6680140577', 'https://openalex.org/W6681588610', 'https://openalex.org/W6680434193', 'https://openalex.org/W6640425456', 'https://openalex.org/W6746582238', 'https://openalex.org/W6762925188', 'https://openalex.org/W6736346607', 'https://openalex.org/W6751037545', 'https://openalex.org/W6739868092', 'https://openalex.org/W2963558289', 'https://openalex.org/W2971015282', 'https://openalex.org/W6679909955', 'https://openalex.org/W6678975374', 'https://openalex.org/W6762913911', 'https://openalex.org/W2048679005', 'https://openalex.org/W6684191040', 'https://openalex.org/W6764051988', 'https://openalex.org/W6675944832', 'https://openalex.org/W2804935296', 'https://openalex.org/W2946457369', 'https://openalex.org/W2895094948', 'https://openalex.org/W2111316763', 'https://openalex.org/W6637373629', 'https://openalex.org/W6674330103', 'https://openalex.org/W6694260854', 'https://openalex.org/W2097117768', 'https://openalex.org/W6740898809', 'https://openalex.org/W6749003979', 'https://openalex.org/W6760911985', 'https://openalex.org/W2963785012', 'https://openalex.org/W6623329352', 'https://openalex.org/W2965658867', 'https://openalex.org/W6764990469', 'https://openalex.org/W2088622183', 'https://openalex.org/W2937720510', 'https://openalex.org/W6757712183', 'https://openalex.org/W6718379498', 'https://openalex.org/W2101210369', 'https://openalex.org/W2549139847', 'https://openalex.org/W2964081807', 'https://openalex.org/W2962737770', 'https://openalex.org/W1572666543', 'https://openalex.org/W6758857762', 'https://openalex.org/W2294370754', 'https://openalex.org/W1479807131', 'https://openalex.org/W6755841646', 'https://openalex.org/W2531409750', 'https://openalex.org/W2891602716', 'https://openalex.org/W6762718338', 'https://openalex.org/W2949736877', 'https://openalex.org/W2619371851', 'https://openalex.org/W6733814495', 'https://openalex.org/W2183341477', 'https://openalex.org/W2577784528', 'https://openalex.org/W3209458476', 'https://openalex.org/W2407712691', 'https://openalex.org/W2970311224', 'https://openalex.org/W2592691248', 'https://openalex.org/W3118342758', 'https://openalex.org/W2519887557', 'https://openalex.org/W2963080758', 'https://openalex.org/W2095705004', 'https://openalex.org/W1983320747', 'https://openalex.org/W2975761646', 'https://openalex.org/W2943152387', 'https://openalex.org/W2995746049', 'https://openalex.org/W2163605009', 'https://openalex.org/W2964222566', 'https://openalex.org/W2961301154', 'https://openalex.org/W2964350391', 'https://openalex.org/W2963043971', 'https://openalex.org/W2971315489', 'https://openalex.org/W2976223659', 'https://openalex.org/W2970680991', 'https://openalex.org/W2139823104', 'https://openalex.org/W2996501936', 'https://openalex.org/W2136504847', 'https://openalex.org/W2901026139', 'https://openalex.org/W1821462560', 'https://openalex.org/W2949985837', 'https://openalex.org/W4308831279', 'https://openalex.org/W2530816535', 'https://openalex.org/W2963420686', 'https://openalex.org/W2134797427', 'https://openalex.org/W4285051865', 'https://openalex.org/W2964153729', 'https://openalex.org/W4288359223', 'https://openalex.org/W4288335398', 'https://openalex.org/W1709548961', 'https://openalex.org/W2957688595', 'https://openalex.org/W2962369866', 'https://openalex.org/W2962907457', 'https://openalex.org/W2293363371', 'https://openalex.org/W2970780203', 'https://openalex.org/W2331143823', 'https://openalex.org/W2947188859', 'https://openalex.org/W2955425717', 'https://openalex.org/W2962835968', 'https://openalex.org/W2997972888', 'https://openalex.org/W2963703197', 'https://openalex.org/W1592410721', 'https://openalex.org/W2145494108', 'https://openalex.org/W3091002423', 'https://openalex.org/W2949194345', 'https://openalex.org/W2962804657', 'https://openalex.org/W2971071159', 'https://openalex.org/W2951970475', 'https://openalex.org/W2963357083', 'https://openalex.org/W2942801205', 'https://openalex.org/W2941175503', 'https://openalex.org/W1686810756', 'https://openalex.org/W2978426779', 'https://openalex.org/W2422843715', 'https://openalex.org/W2982640424', 'https://openalex.org/W2963170156', 'https://openalex.org/W2913314773', 'https://openalex.org/W2963207607', 'https://openalex.org/W2108501770', 'https://openalex.org/W2895771689', 'https://openalex.org/W2991040477', 'https://openalex.org/W4288404646', 'https://openalex.org/W2909869271', 'https://openalex.org/W2947294642', 'https://openalex.org/W2953070460', 'https://openalex.org/W2963821229', 'https://openalex.org/W2937096868', 'https://openalex.org/W2129068307', 'https://openalex.org/W830076066', 'https://openalex.org/W2963373786', 'https://openalex.org/W2964277612', 'https://openalex.org/W2964253222', 'https://openalex.org/W2963938442', 'https://openalex.org/W2803023299', 'https://openalex.org/W4288020585', 'https://openalex.org/W2963216553', 'https://openalex.org/W2905325450', 'https://openalex.org/W2963060032', 'https://openalex.org/W2962900737', 'https://openalex.org/W2972991257', 'https://openalex.org/W2953937638', 'https://openalex.org/W2952229419', 'https://openalex.org/W2964159205', 'https://openalex.org/W1945616565', 'https://openalex.org/W4293846201', 'https://openalex.org/W2964015378', 'https://openalex.org/W1673923490', 'https://openalex.org/W2947775933', 'https://openalex.org/W2280377497', 'https://openalex.org/W2971316968', 'https://openalex.org/W2546938941', 'https://openalex.org/W2963312446', 'https://openalex.org/W2948805184']",2020-06-01
https://openalex.org/W2948771346,https://doi.org/10.5281/zenodo.16740292,Just Say No to Single Embeddings: Why Your AI Needs Multiple Perspectives,"Note: This is a work in progress document This exploratory work analyzes 229 multi-agent AI dialogues byprojecting them into five different embedding spaces (transformer-based and classical) and measuring geometric properties. We find astriking dichotomy: global geometric patterns (distance matrices, tra-jectory shapes) show remarkable consistency across embeddings (corre-lations 0.52-0.96), while local features (phase boundaries) exhibit highvariability (F1: 0.08-0.36). This global consistency paired with localvariability suggests different embedding models may capture distinctprojections of conversational structure. We discuss possible interpre-tations and implications for understanding conversational dynamics inmulti-agent systems","['https://openalex.org/W2153579005', 'https://openalex.org/W2963341956', 'https://openalex.org/W2799124508', 'https://openalex.org/W2065157922', 'https://openalex.org/W2549835527', 'https://openalex.org/W2964303116', 'https://openalex.org/W2740782137', 'https://openalex.org/W1538131130', 'https://openalex.org/W1632114991', 'https://openalex.org/W2320533836', 'https://openalex.org/W2962936818', 'https://openalex.org/W1849277567', 'https://openalex.org/W2796885425', 'https://openalex.org/W2786672974', 'https://openalex.org/W2013179031', 'https://openalex.org/W2787560479', 'https://openalex.org/W2101234009', 'https://openalex.org/W2801092741', 'https://openalex.org/W2939556020', 'https://openalex.org/W2934032625', 'https://openalex.org/W2946359678', 'https://openalex.org/W2963403868', 'https://openalex.org/W1508977358', 'https://openalex.org/W2953369973']",2025-08-04
https://openalex.org/W4297841458,https://doi.org/10.21437/interspeech.2022-10560,Analysis of Self-Attention Head Diversity for Conformer-based Automatic Speech Recognition,"Attention layers are an integral part of modern end-to-end automatic speech recognition systems, for instance as part of the Transformer or Conformer architecture.Attention is typically multi-headed, where each head has an independent set of learned parameters and operates on the same input feature sequence.The output of multi-headed attention is a fusion of the outputs from the individual heads.We empirically analyze the diversity between representations produced by the different attention heads and demonstrate that the heads become highly correlated during the course of training.We investigate a few approaches to increasing attention head diversity, including using different attention mechanisms for each head and auxiliary training loss functions to promote head diversity.We show that introducing diversity-promoting auxiliary loss functions during training is a more effective approach, and obtain WER improvements of up to 6% relative on the Librispeech corpus.Finally, we draw a connection between the diversity of attention heads and the similarity of the gradients of head parameters.","['https://openalex.org/W2970777192', 'https://openalex.org/W2597655663', 'https://openalex.org/W1494198834', 'https://openalex.org/W2327501763', 'https://openalex.org/W2157331557', 'https://openalex.org/W2928941594', 'https://openalex.org/W4226370720', 'https://openalex.org/W2133564696', 'https://openalex.org/W2964089206', 'https://openalex.org/W2742947407', 'https://openalex.org/W3016010032', 'https://openalex.org/W2963495494', 'https://openalex.org/W2804078698', 'https://openalex.org/W4385245566', 'https://openalex.org/W2946794439', 'https://openalex.org/W2962822108', 'https://openalex.org/W1514535095', 'https://openalex.org/W4288088457', 'https://openalex.org/W2962741254', 'https://openalex.org/W2604799547', 'https://openalex.org/W854541894', 'https://openalex.org/W3097777922', 'https://openalex.org/W3106483960', 'https://openalex.org/W2951527505', 'https://openalex.org/W4309793872', 'https://openalex.org/W2970120757']",2022-09-16
https://openalex.org/W4385574383,https://doi.org/10.18653/v1/2022.findings-emnlp.101,How Much Does Attention Actually Attend? Questioning the Importance of Attention in Pretrained Transformers,"The attention mechanism is considered the backbone of the widely-used Transformer architecture. It contextualizes the input by computing input-specific attention matrices. We find that this mechanism, while powerful and elegant, is not as important as typically thought for pretrained language models. We introduce PAPA, a new probing method that replaces the input-dependent attention matrices with constant ones—the average attention weights over multiple inputs. We use PAPA to analyze several established pretrained Transformers on six downstream tasks. We find that without any input-dependent attention, all models achieve competitive performance—an average relative drop of only 8% from the probing baseline. Further, little or no performance drop is observed when replacing half of the input-dependent attention matrices with constant (input-independent) ones. Interestingly, we show that better-performing models lose more from applying our method than weaker models, suggesting that the utilization of the input-dependent attention mechanism might be a factor in their success. Our results motivate research on simpler alternatives to input-dependent attention, as well as on methods for better utilization of this mechanism in the Transformer architecture.","['https://openalex.org/W6631349028', 'https://openalex.org/W4224308101', 'https://openalex.org/W2952087486', 'https://openalex.org/W4226084637', 'https://openalex.org/W4385245566', 'https://openalex.org/W2979826702', 'https://openalex.org/W4285175926', 'https://openalex.org/W4226261765', 'https://openalex.org/W4287824654', 'https://openalex.org/W4309793872', 'https://openalex.org/W4295838474', 'https://openalex.org/W2991265431', 'https://openalex.org/W4206281850', 'https://openalex.org/W2964303116', 'https://openalex.org/W3202070718', 'https://openalex.org/W3034296505', 'https://openalex.org/W4287183852', 'https://openalex.org/W2950768109', 'https://openalex.org/W3122890974', 'https://openalex.org/W2896457183', 'https://openalex.org/W4298422451', 'https://openalex.org/W4225525539', 'https://openalex.org/W2963846996', 'https://openalex.org/W4288089799', 'https://openalex.org/W2965373594', 'https://openalex.org/W2934842096', 'https://openalex.org/W131533222', 'https://openalex.org/W4309087688', 'https://openalex.org/W2923014074', 'https://openalex.org/W4287367446', 'https://openalex.org/W2946794439', 'https://openalex.org/W3123615524', 'https://openalex.org/W4206688402', 'https://openalex.org/W2972324944', 'https://openalex.org/W2251939518', 'https://openalex.org/W2978670439', 'https://openalex.org/W3033529678', 'https://openalex.org/W4292779060', 'https://openalex.org/W3132041002', 'https://openalex.org/W3022810513', 'https://openalex.org/W2970726176']",2022-01-01
https://openalex.org/W3107600318,https://doi.org/10.48550/arxiv.2011.14878,Explaining by Removing: A Unified Framework for Model Explanation,"Researchers have proposed a wide variety of model explanation approaches, but it remains unclear how most methods are related or when one method is preferable to another. We describe a new unified class of methods, removal-based explanations, that are based on the principle of simulating feature removal to quantify each feature's influence. These methods vary in several respects, so we develop a framework that characterizes each method along three dimensions: 1) how the method removes features, 2) what model behavior the method explains, and 3) how the method summarizes each feature's influence. Our framework unifies 26 existing methods, including several of the most widely used approaches: SHAP, LIME, Meaningful Perturbations, and permutation tests. This newly understood class of explanation methods has rich connections that we examine using tools that have been largely overlooked by the explainability literature. To anchor removal-based explanations in cognitive psychology, we show that feature removal is a simple application of subtractive counterfactual reasoning. Ideas from cooperative game theory shed light on the relationships and trade-offs among different methods, and we derive conditions under which all removal-based explanations have information-theoretic interpretations. Through this analysis, we develop a unified framework that helps practitioners better understand model explanation tools, and that offers a strong theoretical foundation upon which future explainability research can build.","['https://openalex.org/W2075784261', 'https://openalex.org/W2099111195', 'https://openalex.org/W2964121744', 'https://openalex.org/W2913008767', 'https://openalex.org/W2176412452', 'https://openalex.org/W2962851944', 'https://openalex.org/W3043547428', 'https://openalex.org/W3035180228', 'https://openalex.org/W2626639386', 'https://openalex.org/W3101609372', 'https://openalex.org/W3102440254', 'https://openalex.org/W2132558332', 'https://openalex.org/W2143481518', 'https://openalex.org/W2066873261', 'https://openalex.org/W2295107390', 'https://openalex.org/W2896125160', 'https://openalex.org/W2097310377', 'https://openalex.org/W2335900909', 'https://openalex.org/W2923915248', 'https://openalex.org/W2795441251', 'https://openalex.org/W2988157455', 'https://openalex.org/W2970996759', 'https://openalex.org/W2962862931', 'https://openalex.org/W2041029272', 'https://openalex.org/W2963095307', 'https://openalex.org/W2073367400', 'https://openalex.org/W2953462175', 'https://openalex.org/W1787224781', 'https://openalex.org/W2766438578', 'https://openalex.org/W2963067999', 'https://openalex.org/W2518039540', 'https://openalex.org/W2135046866', 'https://openalex.org/W1562353621', 'https://openalex.org/W2946731574', 'https://openalex.org/W2042098456', 'https://openalex.org/W3035235410', 'https://openalex.org/W2346578521', 'https://openalex.org/W2963996492', 'https://openalex.org/W1959608418', 'https://openalex.org/W2983687513', 'https://openalex.org/W3034764457', 'https://openalex.org/W2963365341', 'https://openalex.org/W2021873163', 'https://openalex.org/W2785760873', 'https://openalex.org/W3016099278', 'https://openalex.org/W2124793028', 'https://openalex.org/W1977911709', 'https://openalex.org/W2114302755', 'https://openalex.org/W2150480892', 'https://openalex.org/W2102099143', 'https://openalex.org/W2091836314', 'https://openalex.org/W2774522520', 'https://openalex.org/W2952165242', 'https://openalex.org/W2303413189', 'https://openalex.org/W2970670424', 'https://openalex.org/W2282821441', 'https://openalex.org/W2914124878', 'https://openalex.org/W3037688018', 'https://openalex.org/W2962772482', 'https://openalex.org/W2594633041', 'https://openalex.org/W1988271510', 'https://openalex.org/W2973576408', 'https://openalex.org/W2203370306', 'https://openalex.org/W2997560917', 'https://openalex.org/W2950772592', 'https://openalex.org/W1971916086', 'https://openalex.org/W3005086430', 'https://openalex.org/W3033359632', 'https://openalex.org/W2149426898', 'https://openalex.org/W2510508396', 'https://openalex.org/W2964060211', 'https://openalex.org/W3122175177', 'https://openalex.org/W4285719527', 'https://openalex.org/W2891503716', 'https://openalex.org/W1963959269', 'https://openalex.org/W2999615587', 'https://openalex.org/W2110301906', 'https://openalex.org/W3198754983', 'https://openalex.org/W2157050842', 'https://openalex.org/W2125298525', 'https://openalex.org/W3106080991', 'https://openalex.org/W2963396138', 'https://openalex.org/W3088173242', 'https://openalex.org/W1557963124', 'https://openalex.org/W2025720061', 'https://openalex.org/W1849277567', 'https://openalex.org/W1686810756', 'https://openalex.org/W1984871604', 'https://openalex.org/W2982667388', 'https://openalex.org/W2590082389', 'https://openalex.org/W2945976633', 'https://openalex.org/W3122480604', 'https://openalex.org/W2085268307', 'https://openalex.org/W2963715038', 'https://openalex.org/W2768348081']",2020-11-21
https://openalex.org/W4385570829,https://doi.org/10.18653/v1/2023.findings-acl.495,Putting Natural in Natural Language Processing,"Human language is firstly spoken and only secondarily written. Text, however, is a very convenient and efficient representation oflanguage, and modern civilization has made it ubiquitous. Thus the field of NLP has overwhelmingly focused on processing written ratherthan spoken language. Work on spoken language, on the other hand, has been siloed off within the largely separate speech processingcommunity which has been inordinately preoccupied with transcribing speech into text. Recent advances in deep learning have led to afortuitous convergence in methods between speech processing and mainstream NLP. Arguably, the time is ripe for a unification of thesetwo fields, and for starting to take spoken language seriously as the primary mode of human communication. Truly natural language processingcould lead to better integration with the rest of language science and could lead to systems which are more data-efficient and morehuman-like, and which can communicate beyond the textual modality.","['https://openalex.org/W4283026156', 'https://openalex.org/W4287674181', 'https://openalex.org/W3106426924', 'https://openalex.org/W2993383518', 'https://openalex.org/W3112188842', 'https://openalex.org/W3101065397', 'https://openalex.org/W2575357857', 'https://openalex.org/W2058013187', 'https://openalex.org/W4285150836', 'https://openalex.org/W4318242500', 'https://openalex.org/W4224875474', 'https://openalex.org/W3210177631', 'https://openalex.org/W3209059054', 'https://openalex.org/W4385245566', 'https://openalex.org/W1973092678', 'https://openalex.org/W2328013031', 'https://openalex.org/W4242236654', 'https://openalex.org/W3036601975', 'https://openalex.org/W2162471372', 'https://openalex.org/W3035267217', 'https://openalex.org/W4377371484', 'https://openalex.org/W4287196621', 'https://openalex.org/W1995280998', 'https://openalex.org/W2078317994', 'https://openalex.org/W2965373594', 'https://openalex.org/W1603447289', 'https://openalex.org/W4294955557', 'https://openalex.org/W168261181']",2023-01-01
https://openalex.org/W4385574263,https://doi.org/10.18653/v1/2022.emnlp-main.595,Measuring the Mixing of Contextual Information in the Transformer,"The Transformer architecture aggregates input information through the self-attention mechanism, but there is no clear understanding of how this information is mixed across the entire model. Additionally, recent works have demonstrated that attention weights alone are not enough to describe the flow of information. In this paper, we consider the whole attention block --multi-head attention, residual connection, and layer normalization-- and define a metric to measure token-to-token interactions within each layer. Then, we aggregate layer-wise interpretations to provide input attribution scores for model predictions. Experimentally, we show that our method, ALTI (Aggregation of Layer-wise Token-to-token Interactions), provides more faithful explanations and increased robustness than gradient-based methods.","['https://openalex.org/W2972498556', 'https://openalex.org/W3125516434', 'https://openalex.org/W4287110638', 'https://openalex.org/W3196813608', 'https://openalex.org/W3035503910', 'https://openalex.org/W4223554833', 'https://openalex.org/W3175752238', 'https://openalex.org/W3035422918', 'https://openalex.org/W4229459936', 'https://openalex.org/W2988217457', 'https://openalex.org/W3035563045', 'https://openalex.org/W3164819786', 'https://openalex.org/W4286902548', 'https://openalex.org/W2594633041', 'https://openalex.org/W2972324944', 'https://openalex.org/W2970120757', 'https://openalex.org/W2549835527', 'https://openalex.org/W3173902720', 'https://openalex.org/W2962851944', 'https://openalex.org/W2970726176', 'https://openalex.org/W2251939518', 'https://openalex.org/W3200704197', 'https://openalex.org/W4385245566', 'https://openalex.org/W3099143320', 'https://openalex.org/W2346578521', 'https://openalex.org/W3094502228', 'https://openalex.org/W1849277567', 'https://openalex.org/W2978017171', 'https://openalex.org/W2950768109', 'https://openalex.org/W4288631803', 'https://openalex.org/W3101155149', 'https://openalex.org/W2282821441', 'https://openalex.org/W3105604018', 'https://openalex.org/W4292779060', 'https://openalex.org/W2965373594', 'https://openalex.org/W2170240176', 'https://openalex.org/W3085380432', 'https://openalex.org/W3213531247', 'https://openalex.org/W4385572790', 'https://openalex.org/W3174490235', 'https://openalex.org/W3176196997', 'https://openalex.org/W2934842096', 'https://openalex.org/W2979826702', 'https://openalex.org/W3153147196', 'https://openalex.org/W3187467055', 'https://openalex.org/W2113459411', 'https://openalex.org/W3213645763', 'https://openalex.org/W2964159778', 'https://openalex.org/W3177112880', 'https://openalex.org/W4230405732', 'https://openalex.org/W2977944219', 'https://openalex.org/W2963341956']",2022-01-01
https://openalex.org/W4287887174,https://doi.org/10.18653/v1/2022.naacl-main.19,GlobEnc: Quantifying Global Token Attribution by Incorporating the Whole Encoder Layer in Transformers,"Ali Modarressi, Mohsen Fayyaz, Yadollah Yaghoobzadeh, Mohammad Taher Pilehvar. Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. 2022.","['https://openalex.org/W3101163004', 'https://openalex.org/W2991265431', 'https://openalex.org/W3035422918', 'https://openalex.org/W2962851944', 'https://openalex.org/W2970726176', 'https://openalex.org/W3120835175', 'https://openalex.org/W2963341956', 'https://openalex.org/W2978017171', 'https://openalex.org/W2964159778', 'https://openalex.org/W2950768109', 'https://openalex.org/W2552396116', 'https://openalex.org/W4287824654', 'https://openalex.org/W2251939518', 'https://openalex.org/W4205424278', 'https://openalex.org/W3153147196', 'https://openalex.org/W3200704197', 'https://openalex.org/W3099143320', 'https://openalex.org/W2934842096', 'https://openalex.org/W3152409010', 'https://openalex.org/W3173380736', 'https://openalex.org/W3173902720', 'https://openalex.org/W2979826702', 'https://openalex.org/W2948771346', 'https://openalex.org/W2977944219', 'https://openalex.org/W3092292656', 'https://openalex.org/W2970120757', 'https://openalex.org/W3101155149', 'https://openalex.org/W2972324944', 'https://openalex.org/W3175752238', 'https://openalex.org/W4385245566', 'https://openalex.org/W2963846996']",2022-01-01
https://openalex.org/W4386566794,https://doi.org/10.18653/v1/2023.eacl-main.245,Quantifying Context Mixing in Transformers,"Self-attention weights and their transformed variants have been the main source of information for analyzing token-to-token interactions in Transformer-based models. But despite their ease of interpretation, these weights are not faithful to the models’ decisions as they are only one part of an encoder, and other components in the encoder layer can have considerable impact on information mixing in the output representations. In this work, by expanding the scope of analysis to the whole encoder block, we propose Value Zeroing, a novel context mixing score customized for Transformers that provides us with a deeper understanding of how information is mixed at each encoder layer. We demonstrate the superiority of our context mixing score over other analysis methods through a series of complementary evaluations with different viewpoints based on linguistically informed rationales, probing, and faithfulness analysis.","['https://openalex.org/W2972918955', 'https://openalex.org/W4385574217', 'https://openalex.org/W2799051177', 'https://openalex.org/W2970726176', 'https://openalex.org/W2977944219', 'https://openalex.org/W2904479140', 'https://openalex.org/W2972324944', 'https://openalex.org/W3101155149', 'https://openalex.org/W2996728628', 'https://openalex.org/W4292779060', 'https://openalex.org/W2605409611', 'https://openalex.org/W3036601975', 'https://openalex.org/W3172642864', 'https://openalex.org/W2594633041', 'https://openalex.org/W2979826702', 'https://openalex.org/W3153427360', 'https://openalex.org/W3035422918', 'https://openalex.org/W3104235057', 'https://openalex.org/W4285175926', 'https://openalex.org/W3196813608', 'https://openalex.org/W3000716014', 'https://openalex.org/W2947012833', 'https://openalex.org/W4385245566', 'https://openalex.org/W3102466593']",2023-01-01
https://openalex.org/W3099143320,https://doi.org/10.18653/v1/2020.emnlp-main.574,Attention is Not Only a Weight: Analyzing Transformers with Vector Norms,"Attention is a key component of Transformers, which have recently achieved considerable success in natural language processing. Hence, attention is being extensively studied to investigate various linguistic capabilities of Transformers, focusing on analyzing the parallels between attention weights and specific linguistic phenomena. This paper shows that attention weights alone are only one of the two factors that determine the output of attention and proposes a norm-based analysis that incorporates the second factor, the norm of the transformed input vectors. The findings of our norm-based analyses of BERT and a Transformer-based neural machine translation system include the following: (i) contrary to previous studies, BERT pays poor attention to special tokens, and (ii) reasonable word alignment can be extracted from attention mechanisms of Transformer. These findings provide insights into the inner workings of Transformers.","['https://openalex.org/W2965373594', 'https://openalex.org/W2963341956', 'https://openalex.org/W2950768109', 'https://openalex.org/W2972324944', 'https://openalex.org/W2933138175', 'https://openalex.org/W2996428491', 'https://openalex.org/W2970727289', 'https://openalex.org/W2970120757', 'https://openalex.org/W2971296520', 'https://openalex.org/W2912351236', 'https://openalex.org/W2156985047', 'https://openalex.org/W2972342261', 'https://openalex.org/W3035563045', 'https://openalex.org/W2910243263', 'https://openalex.org/W2912070261', 'https://openalex.org/W2973154008', 'https://openalex.org/W2897507397', 'https://openalex.org/W2970597249', 'https://openalex.org/W2995446988', 'https://openalex.org/W3153147196', 'https://openalex.org/W2952682849', 'https://openalex.org/W2948947170', 'https://openalex.org/W2962784628', 'https://openalex.org/W2986266667', 'https://openalex.org/W1973923101', 'https://openalex.org/W2148708890', 'https://openalex.org/W2970726176', 'https://openalex.org/W2934842096', 'https://openalex.org/W3118485687', 'https://openalex.org/W2970565456', 'https://openalex.org/W2963403868', 'https://openalex.org/W2977162702']",2020-01-01
https://openalex.org/W3200704197,https://doi.org/10.18653/v1/2021.emnlp-main.373,Incorporating Residual and Normalization Layers into Analysis of Masked Language Models,"Transformer architecture has become ubiquitous in the natural language processing field. To interpret the Transformer-based models, their attention patterns have been extensively analyzed. However, the Transformer architecture is not only composed of the multi-head attention; other components can also contribute to Transformers’ progressive performance. In this study, we extended the scope of the analysis of Transformers from solely the attention patterns to the whole attention block, i.e., multi-head attention, residual connection, and layer normalization. Our analysis of Transformer-based masked language models shows that the token-to-token interaction performed via attention has less impact on the intermediate representations than previously assumed. These results provide new intuitive explanations of existing reports; for example, discarding the learned attention patterns tends not to adversely affect the performance. The codes of our experiments are publicly available.","['https://openalex.org/W2752172973', 'https://openalex.org/W2995446988', 'https://openalex.org/W2948771346', 'https://openalex.org/W2946794439', 'https://openalex.org/W2251939518', 'https://openalex.org/W3031914912', 'https://openalex.org/W2952087486', 'https://openalex.org/W1026270304', 'https://openalex.org/W2970451234', 'https://openalex.org/W2973154008', 'https://openalex.org/W2988217457', 'https://openalex.org/W3138794547', 'https://openalex.org/W1974339500', 'https://openalex.org/W4298201654', 'https://openalex.org/W3174617925', 'https://openalex.org/W2965373594', 'https://openalex.org/W2787248994', 'https://openalex.org/W2144578941', 'https://openalex.org/W2963341956', 'https://openalex.org/W2970565456', 'https://openalex.org/W2964303497', 'https://openalex.org/W2980433389', 'https://openalex.org/W2977944219', 'https://openalex.org/W2991265431', 'https://openalex.org/W2950621961', 'https://openalex.org/W2972324944', 'https://openalex.org/W3102466593', 'https://openalex.org/W2970727289', 'https://openalex.org/W2975429091', 'https://openalex.org/W3034772996', 'https://openalex.org/W2964098600', 'https://openalex.org/W2805430026', 'https://openalex.org/W2912351236', 'https://openalex.org/W2897507397', 'https://openalex.org/W2247359815', 'https://openalex.org/W3172099915', 'https://openalex.org/W4385245566', 'https://openalex.org/W4394666973', 'https://openalex.org/W2972342261', 'https://openalex.org/W2996428491', 'https://openalex.org/W2946417913', 'https://openalex.org/W4250954493', 'https://openalex.org/W4287110638', 'https://openalex.org/W2963846996', 'https://openalex.org/W3099143320', 'https://openalex.org/W3171953676', 'https://openalex.org/W1614298861', 'https://openalex.org/W2950577311', 'https://openalex.org/W3118485687', 'https://openalex.org/W2194775991', 'https://openalex.org/W2970120757', 'https://openalex.org/W2947012833', 'https://openalex.org/W3035422918', 'https://openalex.org/W2970641574', 'https://openalex.org/W3116570699', 'https://openalex.org/W2963403868']",2021-01-01
https://openalex.org/W3097777922,https://doi.org/10.21437/interspeech.2020-3015,Conformer: Convolution-augmented Transformer for Speech Recognition,"Recently Transformer and Convolution neural network (CNN) based models have shown promising results in Automatic Speech Recognition (ASR), outperforming Recurrent neural networks (RNNs).Transformer models are good at capturing content-based global interactions, while CNNs exploit local features effectively.In this work, we achieve the best of both worlds by studying how to combine convolution neural networks and transformers to model both local and global dependencies of an audio sequence in a parameter-efficient way.To this regard, we propose the convolution-augmented transformer for speech recognition, named Conformer.Conformer significantly outperforms the previous Transformer and CNN based models achieving state-of-the-art accuracies.On the widely used LibriSpeech benchmark, our model achieves WER of 2.1%/4.3%without using a language model and 1.9%/3.9%with an external language model on test/testother.We also observe competitive performance of 2.7%/6.3%with a small model of only 10M parameters.","['https://openalex.org/W1964175594', 'https://openalex.org/W2952809536', 'https://openalex.org/W2963542740', 'https://openalex.org/W3015537910', 'https://openalex.org/W1494198834', 'https://openalex.org/W3015194534', 'https://openalex.org/W4287812455', 'https://openalex.org/W2112739286', 'https://openalex.org/W4385245566', 'https://openalex.org/W4309845474', 'https://openalex.org/W3016010032', 'https://openalex.org/W2962760690', 'https://openalex.org/W4295253143', 'https://openalex.org/W1828163288', 'https://openalex.org/W2095705004', 'https://openalex.org/W2948981900', 'https://openalex.org/W2963414781', 'https://openalex.org/W2981413347', 'https://openalex.org/W2892009249', 'https://openalex.org/W3015995734', 'https://openalex.org/W1522301498', 'https://openalex.org/W2899423466', 'https://openalex.org/W2936774411', 'https://openalex.org/W4320930577', 'https://openalex.org/W3095173472', 'https://openalex.org/W2928941594', 'https://openalex.org/W2567070169', 'https://openalex.org/W2972818416', 'https://openalex.org/W2973215447', 'https://openalex.org/W1995562189', 'https://openalex.org/W2981857663', 'https://openalex.org/W2979636403', 'https://openalex.org/W2964110616']",2020-10-25
https://openalex.org/W3121914243,https://doi.org/10.20944/preprints202101.0081.v1,What All Do Audio Transformer Models Hear? Probing Acoustic Representations for Language Delivery and Its Structure,"In recent times, BERT based transformer models have become an inseparable part of the 'tech stack' of text processing models. Similar progress is being observed in the speech domain with a multitude of models observing state-of-the-art results by using audio transformer models to encode speech. This begs the question of what are these audio transformer models learning. Moreover, although the standard methodology is to choose the last layer embedding for any downstream task, but is it the optimal choice? We try to answer these questions for the two recent audio transformer models, Mockingjay and wave2vec2.0. We compare them on a comprehensive set of language delivery and structure features including audio, fluency and pronunciation features. Additionally, we probe the audio models' understanding of textual surface, syntax, and semantic features and compare them to BERT. We do this over exhaustive settings for native, non-native, synthetic, read and spontaneous speech datasets","['https://openalex.org/W2979476256', 'https://openalex.org/W3121914243', 'https://openalex.org/W4244107794', 'https://openalex.org/W2605717780', 'https://openalex.org/W3008391559', 'https://openalex.org/W3100879603', 'https://openalex.org/W3036601975', 'https://openalex.org/W2948947170', 'https://openalex.org/W4287694183', 'https://openalex.org/W3105148948', 'https://openalex.org/W3148040514', 'https://openalex.org/W2799124508', 'https://openalex.org/W2289421303', 'https://openalex.org/W4287777626', 'https://openalex.org/W2966714320', 'https://openalex.org/W4385245566', 'https://openalex.org/W2964242217', 'https://openalex.org/W2962753610', 'https://openalex.org/W2888760383', 'https://openalex.org/W2079735306', 'https://openalex.org/W2167516857', 'https://openalex.org/W2963341956', 'https://openalex.org/W2511774920', 'https://openalex.org/W2982223350', 'https://openalex.org/W2964060510', 'https://openalex.org/W1494198834', 'https://openalex.org/W4287667694', 'https://openalex.org/W2295676751', 'https://openalex.org/W2752168051', 'https://openalex.org/W3030437843', 'https://openalex.org/W3015671919', 'https://openalex.org/W2555043949', 'https://openalex.org/W1528340495', 'https://openalex.org/W2758849341', 'https://openalex.org/W3035740499', 'https://openalex.org/W2962739339']",2021-01-05
https://openalex.org/W4385823328,https://doi.org/10.21437/interspeech.2023-679,Wave to Syntax: Probing spoken language models for syntax,"Understanding which information is encoded in deep models of spoken and written language has been the focus of much research in recent years, as it is crucial for debugging and improving these architectures. Most previous work has focused on probing for speaker characteristics, acoustic and phonological information in models of spoken language, and for syntactic information in models of written language. Here we focus on the encoding of syntax in several self-supervised and visually grounded models of spoken language. We employ two complementary probing methods, combined with baselines and reference representations to quantify the degree to which syntactic structure is encoded in the activations of the target models. We show that syntax is captured most prominently in the middle layers of the networks, and more explicitly within models with more parameters.","['https://openalex.org/W3036601975', 'https://openalex.org/W1861492603', 'https://openalex.org/W2799124508', 'https://openalex.org/W3157861865', 'https://openalex.org/W2896457183', 'https://openalex.org/W3118485687', 'https://openalex.org/W1494198834', 'https://openalex.org/W3037109418', 'https://openalex.org/W4221161768', 'https://openalex.org/W2906152891', 'https://openalex.org/W2752168051', 'https://openalex.org/W3035750922', 'https://openalex.org/W2295676751', 'https://openalex.org/W2160654481', 'https://openalex.org/W2946296745', 'https://openalex.org/W3174311593', 'https://openalex.org/W4230640548', 'https://openalex.org/W2127713198', 'https://openalex.org/W3209059054', 'https://openalex.org/W3196986263', 'https://openalex.org/W4226380987', 'https://openalex.org/W4225726571', 'https://openalex.org/W3105148948', 'https://openalex.org/W224064951', 'https://openalex.org/W2982223350', 'https://openalex.org/W2972808286']",2023-08-14
https://openalex.org/W4389524500,https://doi.org/10.18653/v1/2023.findings-emnlp.1055,SpeechGPT: Empowering Large Language Models with Intrinsic Cross-Modal Conversational Abilities,"Multi-modal large language models are regarded as a crucial step towards Artificial General Intelligence (AGI) and have garnered significant interest with the emergence of ChatGPT. However, current speech-language models typically adopt the cascade paradigm, preventing inter-modal knowledge transfer. In this paper, we propose SpeechGPT, a large language model with intrinsic cross-modal conversational abilities, capable of perceiving and generating multi-modal content. With discrete speech representations, we construct SpeechInstruct, the first large-scale cross-modal speech instruction dataset. Additionally, we employ a three-stage training strategy that includes modality-adaptation pre-training, cross-modal instruction fine-tuning, and chain-of-modality instruction fine-tuning. The experimental results demonstrate that SpeechGPT has an impressive capacity to follow cross-modal human instructions and highlight the potential of handling multiple modalities with one model. Code and models are available in https://github.com/0nutation/SpeechGPT. Demos are shown in https://0nutation.github.io/SpeechGPT.github.io/.","['https://openalex.org/W4385807416', 'https://openalex.org/W4394671563', 'https://openalex.org/W3166396011', 'https://openalex.org/W4367061106', 'https://openalex.org/W4381786045', 'https://openalex.org/W4385570101', 'https://openalex.org/W4385572615', 'https://openalex.org/W4323572061', 'https://openalex.org/W4361866031', 'https://openalex.org/W3140429000', 'https://openalex.org/W4322718246', 'https://openalex.org/W3169320628', 'https://openalex.org/W4310924890', 'https://openalex.org/W4287120025', 'https://openalex.org/W2995181338', 'https://openalex.org/W4307680525', 'https://openalex.org/W4313679638', 'https://openalex.org/W3036601975', 'https://openalex.org/W4224308101', 'https://openalex.org/W3094502228', 'https://openalex.org/W1494198834', 'https://openalex.org/W3168867926', 'https://openalex.org/W4366330503', 'https://openalex.org/W4375958083', 'https://openalex.org/W4361229539', 'https://openalex.org/W3030437843', 'https://openalex.org/W4386384714', 'https://openalex.org/W4377297670', 'https://openalex.org/W4323651091', 'https://openalex.org/W4322718191']",2023-01-01
https://openalex.org/W4386552581,https://doi.org/10.48550/arxiv.2309.00916,BLSP: Bootstrapping Language-Speech Pre-training via Behavior Alignment of Continuation Writing,"The emergence of large language models (LLMs) has sparked significant interest in extending their remarkable language capabilities to speech. However, modality alignment between speech and text still remains an open problem. Current solutions can be categorized into two strategies. One is a cascaded approach where outputs (tokens or states) of a separately trained speech recognition system are used as inputs for LLMs, which limits their potential in modeling alignment between speech and text. The other is an end-to-end approach that relies on speech instruction data, which is very difficult to collect in large quantities. In this paper, we address these issues and propose the BLSP approach that Bootstraps Language-Speech Pre-training via behavior alignment of continuation writing. We achieve this by learning a lightweight modality adapter between a frozen speech encoder and an LLM, ensuring that the LLM exhibits the same generation behavior regardless of the modality of input: a speech segment or its transcript. The training process can be divided into two steps. The first step prompts an LLM to generate texts with speech transcripts as prefixes, obtaining text continuations. In the second step, these continuations are used as supervised signals to train the modality adapter in an end-to-end manner. We demonstrate that this straightforward process can extend the capabilities of LLMs to speech, enabling speech recognition, speech translation, spoken language understanding, and speech conversation, even in zero-shot cross-lingual scenarios.",[],2023-09-02
https://openalex.org/W4387891768,https://doi.org/10.48550/arxiv.2310.13289,SALMONN: Towards Generic Hearing Abilities for Large Language Models,"Hearing is arguably an essential ability of artificial intelligence (AI) agents in the physical world, which refers to the perception and understanding of general auditory information consisting of at least three types of sounds: speech, audio events, and music. In this paper, we propose SALMONN, a speech audio language music open neural network, built by integrating a pre-trained text-based large language model (LLM) with speech and audio encoders into a single multimodal model. SALMONN enables the LLM to directly process and understand general audio inputs and achieve competitive performances on a number of speech and audio tasks used in training, such as automatic speech recognition and translation, auditory-information-based question answering, emotion recognition, speaker verification, and music and audio captioning etc. SALMONN also has a diverse set of emergent abilities unseen in the training, which includes but is not limited to speech translation to untrained languages, speech-based slot filling, spoken-query-based question answering, audio-based storytelling, and speech audio co-reasoning etc. The presence of cross-modal emergent abilities is studied, and a novel few-shot activation tuning approach is proposed to activate such abilities. To our knowledge, SALMONN is the first model of its type and can be regarded as a step towards AI with generic hearing abilities. The source code, model checkpoints and data are available at https://github.com/bytedance/SALMONN.",[],2023-10-20
https://openalex.org/W4391591726,https://doi.org/10.48550/arxiv.2402.01831,Audio Flamingo: A Novel Audio Language Model with Few-Shot Learning and Dialogue Abilities,"Augmenting large language models (LLMs) to understand audio -- including non-speech sounds and non-verbal speech -- is critically important for diverse real-world applications of LLMs. In this paper, we propose Audio Flamingo, a novel audio language model with 1) strong audio understanding abilities, 2) the ability to quickly adapt to unseen tasks via in-context learning and retrieval, and 3) strong multi-turn dialogue abilities. We introduce a series of training techniques, architecture design, and data strategies to enhance our model with these abilities. Extensive evaluations across various audio understanding tasks confirm the efficacy of our method, setting new state-of-the-art benchmarks. Our demo website is https://audioflamingo.github.io/ and the code is open-sourced at https://github.com/NVIDIA/audio-flamingo.",[],2024-02-02
https://openalex.org/W4403556012,https://doi.org/10.48550/arxiv.2408.16725,"Mini-Omni: Language Models Can Hear, Talk While Thinking in Streaming","Recent advances in language models have achieved significant progress. GPT-4o, as a new milestone, has enabled real-time conversations with humans, demonstrating near-human natural fluency. Such human-computer interaction necessitates models with the capability to perform reasoning directly with the audio modality and generate output in streaming. However, this remains beyond the reach of current academic models, as they typically depend on extra TTS systems for speech synthesis, resulting in undesirable latency. This paper introduces the Mini-Omni, an audio-based end-to-end conversational model, capable of real-time speech interaction. To achieve this capability, we propose a text-instructed speech generation method, along with batch-parallel strategies during inference to further boost the performance. Our method also helps to retain the original model's language capabilities with minimal degradation, enabling other works to establish real-time interaction capabilities. We call this training method ""Any Model Can Talk"". We also introduce the VoiceAssistant-400K dataset to fine-tune models optimized for speech output. To our best knowledge, Mini-Omni is the first fully end-to-end, open-source model for real-time speech interaction, offering valuable potential for future research.",[],2024-08-29
https://openalex.org/W4226444650,https://doi.org/10.48550/arxiv.2201.03713,CVSS Corpus and Massively Multilingual Speech-to-Speech Translation,"We introduce CVSS, a massively multilingual-to-English speech-to-speech translation (S2ST) corpus, covering sentence-level parallel S2ST pairs from 21 languages into English. CVSS is derived from the Common Voice speech corpus and the CoVoST 2 speech-to-text translation (ST) corpus, by synthesizing the translation text from CoVoST 2 into speech using state-of-the-art TTS systems. Two versions of translation speeches are provided: 1) CVSS-C: All the translation speeches are in a single high-quality canonical voice; 2) CVSS-T: The translation speeches are in voices transferred from the corresponding source speeches. In addition, CVSS provides normalized translation text which matches the pronunciation in the translation speech. On each version of CVSS, we built baseline multilingual direct S2ST models and cascade S2ST models, verifying the effectiveness of the corpus. To build strong cascade S2ST baselines, we trained an ST model on CoVoST 2, which outperforms the previous state-of-the-art trained on the corpus without extra data by 5.8 BLEU. Nevertheless, the performance of the direct S2ST models approaches the strong cascade baselines when trained from scratch, and with only 0.1 or 0.7 BLEU difference on ASR transcribed translation when initialized from matching ST models.",[],2022-01-11
https://openalex.org/W2803193013,https://doi.org/10.1371/journal.pone.0196391,"The Ryerson Audio-Visual Database of Emotional Speech and Song (RAVDESS): A dynamic, multimodal set of facial and vocal expressions in North American English","The RAVDESS is a validated multimodal database of emotional speech and song. The database is gender balanced consisting of 24 professional actors, vocalizing lexically-matched statements in a neutral North American accent. Speech includes calm, happy, sad, angry, fearful, surprise, and disgust expressions, and song contains calm, happy, sad, angry, and fearful emotions. Each expression is produced at two levels of emotional intensity, with an additional neutral expression. All conditions are available in face-and-voice, face-only, and voice-only formats. The set of 7356 recordings were each rated 10 times on emotional validity, intensity, and genuineness. Ratings were provided by 247 individuals who were characteristic of untrained research participants from North America. A further set of 72 participants provided test-retest data. High levels of emotional validity and test-retest intrarater reliability were reported. Corrected accuracy and composite ""goodness"" measures are presented to assist researchers in the selection of stimuli. All recordings are made freely available under a Creative Commons license and can be downloaded at https://doi.org/10.5281/zenodo.1188976.","['https://openalex.org/W2075274068', 'https://openalex.org/W2162995090', 'https://openalex.org/W2158564203', 'https://openalex.org/W1978436245', 'https://openalex.org/W2073414813', 'https://openalex.org/W1992424514', 'https://openalex.org/W2115839658', 'https://openalex.org/W2160287712', 'https://openalex.org/W2138395786', 'https://openalex.org/W2007462506', 'https://openalex.org/W2169471661', 'https://openalex.org/W2099418770', 'https://openalex.org/W2070267188', 'https://openalex.org/W2138813867', 'https://openalex.org/W2095580536', 'https://openalex.org/W2140110589', 'https://openalex.org/W2091804523', 'https://openalex.org/W2108831282', 'https://openalex.org/W2145603196', 'https://openalex.org/W2114201982', 'https://openalex.org/W2170598503', 'https://openalex.org/W2143644424', 'https://openalex.org/W2911935942', 'https://openalex.org/W2140157036', 'https://openalex.org/W2125127226', 'https://openalex.org/W2169294293', 'https://openalex.org/W2098958400', 'https://openalex.org/W2054167736', 'https://openalex.org/W2150283722', 'https://openalex.org/W2186574009', 'https://openalex.org/W2128897837', 'https://openalex.org/W1964422354', 'https://openalex.org/W1996492337', 'https://openalex.org/W2108385997', 'https://openalex.org/W2123912601', 'https://openalex.org/W1980068698', 'https://openalex.org/W2121511082', 'https://openalex.org/W2114826417', 'https://openalex.org/W2149377780', 'https://openalex.org/W2003881355', 'https://openalex.org/W2026118130', 'https://openalex.org/W1974701791', 'https://openalex.org/W1974750895', 'https://openalex.org/W2070375420', 'https://openalex.org/W2014481042', 'https://openalex.org/W2023657742', 'https://openalex.org/W2108952035', 'https://openalex.org/W2138172448', 'https://openalex.org/W2342475039', 'https://openalex.org/W2030931454', 'https://openalex.org/W2038199522', 'https://openalex.org/W2044071327', 'https://openalex.org/W2057829425', 'https://openalex.org/W2400814905', 'https://openalex.org/W2329093554', 'https://openalex.org/W2091550557', 'https://openalex.org/W2114785251', 'https://openalex.org/W2033351021', 'https://openalex.org/W4254254617', 'https://openalex.org/W2143969506', 'https://openalex.org/W2078833921', 'https://openalex.org/W1486414894', 'https://openalex.org/W2005885879', 'https://openalex.org/W1987708005', 'https://openalex.org/W1982188321', 'https://openalex.org/W2033640825', 'https://openalex.org/W2018338387', 'https://openalex.org/W1976941847', 'https://openalex.org/W1499019550', 'https://openalex.org/W2120223000', 'https://openalex.org/W2115914647', 'https://openalex.org/W2159190230', 'https://openalex.org/W2168692779', 'https://openalex.org/W2134957872', 'https://openalex.org/W2088098553', 'https://openalex.org/W2137208816', 'https://openalex.org/W2085281696', 'https://openalex.org/W2111060246', 'https://openalex.org/W2119548591', 'https://openalex.org/W2064931561', 'https://openalex.org/W1525475793', 'https://openalex.org/W2156167136', 'https://openalex.org/W2573458771', 'https://openalex.org/W2025175823', 'https://openalex.org/W2154905939', 'https://openalex.org/W2157651355', 'https://openalex.org/W2098905310', 'https://openalex.org/W2102126079', 'https://openalex.org/W1979479123', 'https://openalex.org/W2072206615', 'https://openalex.org/W175750906', 'https://openalex.org/W2019845078', 'https://openalex.org/W4256689234', 'https://openalex.org/W1985449126', 'https://openalex.org/W1974044992', 'https://openalex.org/W1694080526', 'https://openalex.org/W2167557160', 'https://openalex.org/W2143197238', 'https://openalex.org/W7033031698', 'https://openalex.org/W2074716497', 'https://openalex.org/W2063372382', 'https://openalex.org/W6634620738', 'https://openalex.org/W1966797434', 'https://openalex.org/W1975238145', 'https://openalex.org/W2154611638', 'https://openalex.org/W2055894321', 'https://openalex.org/W2141115110', 'https://openalex.org/W2024186865', 'https://openalex.org/W2051297709', 'https://openalex.org/W2002948879', 'https://openalex.org/W6637179637', 'https://openalex.org/W2137768357', 'https://openalex.org/W2087407704', 'https://openalex.org/W2089012269', 'https://openalex.org/W2092689684', 'https://openalex.org/W2001254306', 'https://openalex.org/W1976725440', 'https://openalex.org/W2055973627', 'https://openalex.org/W2006044072', 'https://openalex.org/W4294214781', 'https://openalex.org/W2030117765', 'https://openalex.org/W2032254851', 'https://openalex.org/W2000598690', 'https://openalex.org/W2016377054', 'https://openalex.org/W2084388911', 'https://openalex.org/W1975879668', 'https://openalex.org/W2164777277', 'https://openalex.org/W2141403362', 'https://openalex.org/W2327037637', 'https://openalex.org/W2063085086', 'https://openalex.org/W1988100039', 'https://openalex.org/W1993723434', 'https://openalex.org/W1835051027', 'https://openalex.org/W2110065044', 'https://openalex.org/W4399547459', 'https://openalex.org/W2166067393', 'https://openalex.org/W2075659707', 'https://openalex.org/W1985740911', 'https://openalex.org/W2098047046', 'https://openalex.org/W1970260040', 'https://openalex.org/W2613941748', 'https://openalex.org/W2324260108', 'https://openalex.org/W4245744384', 'https://openalex.org/W2094274886', 'https://openalex.org/W2117237841', 'https://openalex.org/W1997402792', 'https://openalex.org/W2048765566', 'https://openalex.org/W2053306387', 'https://openalex.org/W2021218462', 'https://openalex.org/W2149628368', 'https://openalex.org/W2134031328', 'https://openalex.org/W2135394513', 'https://openalex.org/W2015116436', 'https://openalex.org/W1993352391', 'https://openalex.org/W6735154455', 'https://openalex.org/W6684459434', 'https://openalex.org/W1997389939', 'https://openalex.org/W2080457614', 'https://openalex.org/W2171198878', 'https://openalex.org/W2142398106', 'https://openalex.org/W2132947339', 'https://openalex.org/W2121247252', 'https://openalex.org/W2171022195', 'https://openalex.org/W2511085799', 'https://openalex.org/W2167113547', 'https://openalex.org/W414188911', 'https://openalex.org/W2025089570', 'https://openalex.org/W4253477913', 'https://openalex.org/W2069736034', 'https://openalex.org/W2582743722', 'https://openalex.org/W3137861108', 'https://openalex.org/W1580867010', 'https://openalex.org/W4229985350', 'https://openalex.org/W2045529234', 'https://openalex.org/W2062758252', 'https://openalex.org/W2173219554', 'https://openalex.org/W641354324', 'https://openalex.org/W4256441345', 'https://openalex.org/W32003796', 'https://openalex.org/W4249456707', 'https://openalex.org/W1573865449', 'https://openalex.org/W2982077032', 'https://openalex.org/W4229839895', 'https://openalex.org/W2503911469', 'https://openalex.org/W2023072634', 'https://openalex.org/W2150988854', 'https://openalex.org/W1963945289', 'https://openalex.org/W4239078262', 'https://openalex.org/W2135869619', 'https://openalex.org/W2895204597', 'https://openalex.org/W1563099953', 'https://openalex.org/W1979377847', 'https://openalex.org/W4253315019', 'https://openalex.org/W3142130749', 'https://openalex.org/W1724922661', 'https://openalex.org/W2118789253', 'https://openalex.org/W1540224544', 'https://openalex.org/W142425564', 'https://openalex.org/W2081645440', 'https://openalex.org/W1989114159', 'https://openalex.org/W4234180827', 'https://openalex.org/W2599201014', 'https://openalex.org/W2572132240', 'https://openalex.org/W1528099392', 'https://openalex.org/W2005460979']",2018-05-16
https://openalex.org/W2912924812,https://doi.org/10.1162/tacl_a_00276,Natural Questions: A Benchmark for Question Answering Research,"We present the Natural Questions corpus, a question answering data set. Questions consist of real anonymized, aggregated queries issued to the Google search engine. An annotator is presented with a question along with a Wikipedia page from the top 5 search results, and annotates a long answer (typically a paragraph) and a short answer (one or more entities) if present on the page, or marks null if no long/short answer is present. The public release consists of 307,373 training examples with single annotations; 7,830 examples with 5-way annotations for development data; and a further 7,842 examples with 5-way annotated sequestered as test data. We present experiments validating quality of the data. We also describe analysis of 25-way annotations on 302 examples, giving insights into human variability on the annotation task. We introduce robust metrics for the purposes of evaluating question answering systems; demonstrate high human upper bounds on these metrics; and establish baseline results using competitive methods drawn from related literature.","['https://openalex.org/W1840435438', 'https://openalex.org/W2962985038', 'https://openalex.org/W2888302696', 'https://openalex.org/W2962718483', 'https://openalex.org/W2963957489', 'https://openalex.org/W2963339397', 'https://openalex.org/W2963963993', 'https://openalex.org/W2890894339', 'https://openalex.org/W2963681467', 'https://openalex.org/W2963015836', 'https://openalex.org/W2413794162', 'https://openalex.org/W2963323070', 'https://openalex.org/W2963748441', 'https://openalex.org/W2251818205', 'https://openalex.org/W2889787757', 'https://openalex.org/W2964267515', 'https://openalex.org/W2558203065', 'https://openalex.org/W2951534261', 'https://openalex.org/W2888296173', 'https://openalex.org/W2101105183', 'https://openalex.org/W2607303097', 'https://openalex.org/W2962809918', 'https://openalex.org/W2125436846', 'https://openalex.org/W2913059114', 'https://openalex.org/W1593271688', 'https://openalex.org/W2068737686', 'https://openalex.org/W4238634189', 'https://openalex.org/W1564947197', 'https://openalex.org/W1516184288', 'https://openalex.org/W2963846996', 'https://openalex.org/W2963969878', 'https://openalex.org/W2949615363', 'https://openalex.org/W2911430044', 'https://openalex.org/W2950501607']",2019-08-02
https://openalex.org/W4385568240,https://doi.org/10.1145/3580305.3599931,WebGLM: Towards An Efficient Web-Enhanced Question Answering System with Human Preferences,"We present WebGLM, a web-enhanced question-answering system based on the General Language Model (GLM). Its goal is to augment a pre-trained large language model (LLM) with web search and retrieval capabilities while being efficient for real-world deployments. To achieve this, we develop WebGLM with strategies for the LLM-augmented retriever, bootstrapped generator, and human preference-aware scorer. Specifically, we identify and address the limitations of WebGPT (OpenAI), through which WebGLM is enabled with accuracy, efficiency, and cost-effectiveness advantages. In addition, we propose systematic criteria for evaluating web-enhanced QA systems. We conduct multi-dimensional human evaluation and quantitative ablation studies, which suggest the outperformance of the proposed WebGLM designs over existing systems. WebGLM with the 10-billion-parameter GLM (10B) is shown to perform better than the similar-sized WebGPT (13B) and even comparably to WebGPT (175B) in human evaluation. The code, demo, and data are at https://github.com/THUDM/WebGLM.","['https://openalex.org/W4285294723', 'https://openalex.org/W2950681488', 'https://openalex.org/W4309674289', 'https://openalex.org/W3099700870', 'https://openalex.org/W2912924812', 'https://openalex.org/W3027879771', 'https://openalex.org/W3122241445', 'https://openalex.org/W3173151551', 'https://openalex.org/W1549671171', 'https://openalex.org/W3192405822', 'https://openalex.org/W4226278401', 'https://openalex.org/W2963748441', 'https://openalex.org/W3102659883', 'https://openalex.org/W4287891464', 'https://openalex.org/W4206256378', 'https://openalex.org/W6782465632', 'https://openalex.org/W4287674181', 'https://openalex.org/W4233907442', 'https://openalex.org/W2600463316', 'https://openalex.org/W1593271688', 'https://openalex.org/W2771472444']",2023-08-04
https://openalex.org/W4385571440,https://doi.org/10.18653/v1/2023.acl-long.496,SLUE Phase-2: A Benchmark Suite of Diverse Spoken Language Understanding Tasks,"Suwon Shon, Siddhant Arora, Chyi-Jiunn Lin, Ankita Pasad, Felix Wu, Roshan S Sharma, Wei-Lun Wu, Hung-yi Lee, Karen Livescu, Shinji Watanabe. Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). 2023.","['https://openalex.org/W4221145109', 'https://openalex.org/W4225303417', 'https://openalex.org/W2962854302', 'https://openalex.org/W2962780374', 'https://openalex.org/W4372260611', 'https://openalex.org/W2972584841', 'https://openalex.org/W4309582448', 'https://openalex.org/W2963748441', 'https://openalex.org/W2796738181', 'https://openalex.org/W4226103796', 'https://openalex.org/W2883409523', 'https://openalex.org/W3036601975', 'https://openalex.org/W3119308075', 'https://openalex.org/W3161223924', 'https://openalex.org/W4297683418', 'https://openalex.org/W1494198834', 'https://openalex.org/W2154652894', 'https://openalex.org/W3217767527', 'https://openalex.org/W1591607137', 'https://openalex.org/W4320194748', 'https://openalex.org/W2974231335', 'https://openalex.org/W4312353038', 'https://openalex.org/W3209984917', 'https://openalex.org/W4288088047', 'https://openalex.org/W2252136820', 'https://openalex.org/W4252076394', 'https://openalex.org/W2921312604', 'https://openalex.org/W2912924812', 'https://openalex.org/W2133459682', 'https://openalex.org/W2899274165', 'https://openalex.org/W3099944122', 'https://openalex.org/W4319862410', 'https://openalex.org/W3197956630', 'https://openalex.org/W4224925623', 'https://openalex.org/W3030323151', 'https://openalex.org/W3095093565', 'https://openalex.org/W3122890974', 'https://openalex.org/W2747874407', 'https://openalex.org/W3216401400', 'https://openalex.org/W2089654579', 'https://openalex.org/W3096109555', 'https://openalex.org/W2963339397', 'https://openalex.org/W4287887773', 'https://openalex.org/W4310638188', 'https://openalex.org/W4303633477', 'https://openalex.org/W4311000453', 'https://openalex.org/W3095918555', 'https://openalex.org/W3197580070', 'https://openalex.org/W2339294230', 'https://openalex.org/W3087962330', 'https://openalex.org/W4319862713', 'https://openalex.org/W4297841571', 'https://openalex.org/W3101648800', 'https://openalex.org/W2936695845', 'https://openalex.org/W2146334809', 'https://openalex.org/W2077302143', 'https://openalex.org/W3190342989', 'https://openalex.org/W2923014074', 'https://openalex.org/W2964223283', 'https://openalex.org/W2921018690', 'https://openalex.org/W2970641574', 'https://openalex.org/W4226291738', 'https://openalex.org/W3169320628', 'https://openalex.org/W1801866228', 'https://openalex.org/W3100460087', 'https://openalex.org/W3095384101', 'https://openalex.org/W3016658527', 'https://openalex.org/W4281492411', 'https://openalex.org/W3015468748', 'https://openalex.org/W3189296823']",2023-01-01
https://openalex.org/W2995929068,https://doi.org/10.48550/arxiv.1912.06670,Common Voice: A Massively-Multilingual Speech Corpus,"The Common Voice corpus is a massively-multilingual collection of transcribed speech intended for speech technology research and development. Common Voice is designed for Automatic Speech Recognition purposes but can be useful in other domains (e.g. language identification). To achieve scale and sustainability, the Common Voice project employs crowdsourcing for both data collection and data validation. The most recent release includes 29 languages, and as of November 2019 there are a total of 38 languages collecting data. Over 50,000 individuals have participated so far, resulting in 2,500 hours of collected audio. To our knowledge this is the largest audio corpus in the public domain for speech recognition, both in terms of number of hours and number of languages. As an example use case for Common Voice, we present speech recognition experiments using Mozilla's DeepSpeech Speech-to-Text toolkit. By applying transfer learning from a source English model, we find an average Character Error Rate improvement of 5.99 +/- 5.48 for twelve target languages (German, French, Italian, Turkish, Catalan, Slovenian, Welsh, Irish, Breton, Tatar, Chuvash, and Kabyle). For most of these languages, these are the first ever published results on end-to-end Automatic Speech Recognition.","['https://openalex.org/W3084713055', 'https://openalex.org/W1922655562', 'https://openalex.org/W2292087804', 'https://openalex.org/W2757672955', 'https://openalex.org/W1533861849', 'https://openalex.org/W2127141656']",2019-12-13
https://openalex.org/W4367000491,https://doi.org/10.48550/arxiv.2304.12244,WizardLM: Empowering large pre-trained language models to follow complex instructions,"Training large language models (LLMs) with open-domain instruction following data brings colossal success. However, manually creating such instruction data is very time-consuming and labor-intensive. Moreover, humans may struggle to produce high-complexity instructions. In this paper, we show an avenue for creating large amounts of instruction data with varying levels of complexity using LLM instead of humans. Starting with an initial set of instructions, we use our proposed Evol-Instruct to rewrite them step by step into more complex instructions. Then, we mix all generated instruction data to fine-tune LLaMA. We call the resulting model WizardLM. Human evaluations on a complexity-balanced test bed and Vicuna's testset show that instructions from Evol-Instruct are superior to human-created ones. By analyzing the human evaluation results of the high complexity part, we demonstrate that outputs from our WizardLM are preferred to outputs from OpenAI ChatGPT. In GPT-4 automatic evaluation, WizardLM achieves more than 90\% capacity of ChatGPT on 17 out of 29 skills. Even though WizardLM still lags behind ChatGPT in some aspects, our findings suggest that fine-tuning with AI-evolved instructions is a promising direction for enhancing LLMs. Our code and data are public at https://github.com/nlpxucan/WizardLM",[],2023-04-24
https://openalex.org/W2889787757,https://doi.org/10.18653/v1/d18-1259,"HotpotQA: A Dataset for Diverse, Explainable Multi-hop Question Answering","Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William Cohen, Ruslan Salakhutdinov, Christopher D. Manning. Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing. 2018.","['https://openalex.org/W2962974452', 'https://openalex.org/W2963672599', 'https://openalex.org/W2964120615', 'https://openalex.org/W2766371743', 'https://openalex.org/W2963548312', 'https://openalex.org/W2740747242', 'https://openalex.org/W2739749670', 'https://openalex.org/W2123442489', 'https://openalex.org/W2558203065', 'https://openalex.org/W2963323070', 'https://openalex.org/W2963082277', 'https://openalex.org/W2962718483', 'https://openalex.org/W2609826708', 'https://openalex.org/W2963010846', 'https://openalex.org/W2962985038', 'https://openalex.org/W2963866616', 'https://openalex.org/W2951534261', 'https://openalex.org/W2963748441', 'https://openalex.org/W2551396370', 'https://openalex.org/W2963769536', 'https://openalex.org/W2963339397']",2018-01-01
https://openalex.org/W3159959439,https://doi.org/10.1162/tacl_a_00370,<i>Did Aristotle Use a Laptop?</i>A Question Answering Benchmark with Implicit Reasoning Strategies,"Abstract A key limitation in current datasets for multi-hop reasoning is that the required steps for answering the question are mentioned in it explicitly. In this work, we introduce StrategyQA, a question answering (QA) benchmark where the required reasoning steps are implicit in the question, and should be inferred using a strategy. A fundamental challenge in this setup is how to elicit such creative questions from crowdsourcing workers, while covering a broad range of potential strategies. We propose a data collection procedure that combines term-based priming to inspire annotators, careful control over the annotator population, and adversarial filtering for eliminating reasoning shortcuts. Moreover, we annotate each question with (1) a decomposition into reasoning steps for answering it, and (2) Wikipedia paragraphs that contain the answers to each step. Overall, StrategyQA includes 2,780 examples, each consisting of a strategy question, its decomposition, and evidence paragraphs. Analysis shows that questions in StrategyQA are short, topic-diverse, and cover a wide range of strategies. Empirically, we show that humans perform well (87%) on this task, while our best baseline reaches an accuracy of ∼ 66%.","['https://openalex.org/W3003928769', 'https://openalex.org/W6764401283', 'https://openalex.org/W3045462440', 'https://openalex.org/W3035503910', 'https://openalex.org/W6764256271', 'https://openalex.org/W2970442950', 'https://openalex.org/W6748778741', 'https://openalex.org/W2951328433', 'https://openalex.org/W6751790668', 'https://openalex.org/W2996848635', 'https://openalex.org/W6782204509', 'https://openalex.org/W2912924812', 'https://openalex.org/W3034999214', 'https://openalex.org/W6766673545', 'https://openalex.org/W2890894339', 'https://openalex.org/W2949134692', 'https://openalex.org/W6729938257', 'https://openalex.org/W3100436891', 'https://openalex.org/W2963748441', 'https://openalex.org/W6628905179', 'https://openalex.org/W3098324846', 'https://openalex.org/W2963530300', 'https://openalex.org/W2964120615', 'https://openalex.org/W2963866616', 'https://openalex.org/W6737236263', 'https://openalex.org/W3016828850', 'https://openalex.org/W2889787757', 'https://openalex.org/W2919420119', 'https://openalex.org/W3105662186', 'https://openalex.org/W3172267148', 'https://openalex.org/W2804897457', 'https://openalex.org/W2900065283', 'https://openalex.org/W4238634189', 'https://openalex.org/W2887107689', 'https://openalex.org/W2558203065', 'https://openalex.org/W2965373594']",2021-01-01
https://openalex.org/W4384918448,https://doi.org/10.48550/arxiv.2307.09288,Llama 2: Open Foundation and Fine-Tuned Chat Models,"In this work, we develop and release Llama 2, a collection of pretrained and fine-tuned large language models (LLMs) ranging in scale from 7 billion to 70 billion parameters. Our fine-tuned LLMs, called Llama 2-Chat, are optimized for dialogue use cases. Our models outperform open-source chat models on most benchmarks we tested, and based on our human evaluations for helpfulness and safety, may be a suitable substitute for closed-source models. We provide a detailed description of our approach to fine-tuning and safety improvements of Llama 2-Chat in order to enable the community to build on our work and contribute to the responsible development of LLMs.",[],2023-07-18
https://openalex.org/W4387321091,https://doi.org/10.1145/3600006.3613165,Efficient Memory Management for Large Language Model Serving with PagedAttention,"High throughput serving of large language models (LLMs) requires batching sufficiently many requests at a time. However, existing systems struggle because the key-value cache (KV cache) memory for each request is huge and grows and shrinks dynamically. When managed inefficiently, this memory can be significantly wasted by fragmentation and redundant duplication, limiting the batch size. To address this problem, we propose PagedAttention, an attention algorithm inspired by the classical virtual memory and paging techniques in operating systems. On top of it, we build vLLM, an LLM serving system that achieves (1) near-zero waste in KV cache memory and (2) flexible sharing of KV cache within and across requests to further reduce memory usage. Our evaluations show that vLLM improves the throughput of popular LLMs by 2--4× with the same level of latency compared to the state-of-the-art systems, such as FasterTransformer and Orca. The improvement is more pronounced with longer sequences, larger models, and more complex decoding algorithms. vLLM's source code is publicly available at https://github.com/vllm-project/vllm.","['https://openalex.org/W2512924740', 'https://openalex.org/W3095488153', 'https://openalex.org/W4281758439', 'https://openalex.org/W3130716829', 'https://openalex.org/W2798291715', 'https://openalex.org/W6862640317', 'https://openalex.org/W2194775991', 'https://openalex.org/W3012479151', 'https://openalex.org/W2979044977', 'https://openalex.org/W2164155474', 'https://openalex.org/W2982157693', 'https://openalex.org/W4307315283', 'https://openalex.org/W4301361180', 'https://openalex.org/W3172198372', 'https://openalex.org/W2979826702', 'https://openalex.org/W4226479682', 'https://openalex.org/W2734941459', 'https://openalex.org/W2914209329', 'https://openalex.org/W3037639655']",2023-10-03
https://openalex.org/W3034302278,https://doi.org/10.1109/cvpr42600.2020.00731,Improving the Robustness of Capsule Networks to Image Affine Transformations,"Convolutional neural networks (CNNs) achieve translational invariance by using pooling operations. However, the operations do not preserve the spatial relationships in the learned representations. Hence, CNNs cannot extrapolate to various geometric transformations of inputs. Recently, Capsule Networks (CapsNets) have been proposed to tackle this problem. In CapsNets, each entity is represented by a vector and routed to high-level entity representations by a dynamic routing algorithm. CapsNets have been shown to be more robust than CNNs to affine transformations of inputs. However, there is still a huge gap between their performance on transformed inputs compared to untransformed versions. In this work, we first revisit the routing procedure by (un)rolling its forward and backward passes. Our investigation reveals that the routing procedure contributes neither to the generalization ability nor to the affine robustness of the CapsNets. Furthermore, we explore the limitations of capsule transformations and propose affine CapsNets (Aff-CapsNets), which are more robust to affine transformations. On our benchmark task, where models are trained on the MNIST dataset and tested on the AffNIST dataset, our Aff-CapsNets improve the benchmark performance by a large margin (from 79% to 93.21%), without using any routing mechanism.","['https://openalex.org/W2963446712', 'https://openalex.org/W6748823286', 'https://openalex.org/W6750207810', 'https://openalex.org/W2112796928', 'https://openalex.org/W2807754035', 'https://openalex.org/W2886134486', 'https://openalex.org/W2893140104', 'https://openalex.org/W6754015692', 'https://openalex.org/W2959983308', 'https://openalex.org/W6750163105', 'https://openalex.org/W2982147654', 'https://openalex.org/W6770904645', 'https://openalex.org/W2194775991', 'https://openalex.org/W2108598243', 'https://openalex.org/W6678849495', 'https://openalex.org/W2945598038', 'https://openalex.org/W2965481041', 'https://openalex.org/W2805274379', 'https://openalex.org/W6743446608', 'https://openalex.org/W6756711085', 'https://openalex.org/W2902319695', 'https://openalex.org/W2965847928', 'https://openalex.org/W2785994986', 'https://openalex.org/W2963703618', 'https://openalex.org/W3118608800', 'https://openalex.org/W2962703703', 'https://openalex.org/W2795330220', 'https://openalex.org/W2125240890', 'https://openalex.org/W2903024257', 'https://openalex.org/W2888215911', 'https://openalex.org/W2750384547', 'https://openalex.org/W2797472209', 'https://openalex.org/W2797275157', 'https://openalex.org/W2335728318', 'https://openalex.org/W2990191378', 'https://openalex.org/W2963143796']",2020-06-01
https://openalex.org/W4400375155,https://doi.org/10.48550/arxiv.2407.02411,Video Watermarking: Safeguarding Your Video from (Unauthorized) Annotations by Video-based LLMs,"The advent of video-based Large Language Models (LLMs) has significantly enhanced video understanding. However, it has also raised some safety concerns regarding data protection, as videos can be more easily annotated, even without authorization. This paper introduces Video Watermarking, a novel technique to protect videos from unauthorized annotations by such video-based LLMs, especially concerning the video content and description, in response to specific queries. By imperceptibly embedding watermarks into key video frames with multi-modal flow-based losses, our method preserves the viewing experience while preventing misuse by video-based LLMs. Extensive experiments show that Video Watermarking significantly reduces the comprehensibility of videos with various video-based LLMs, demonstrating both stealth and robustness. In essence, our method provides a solution for securing video content, ensuring its integrity and confidentiality in the face of evolving video-based LLMs technologies.",[],2024-07-02
https://openalex.org/W4397028323,https://doi.org/10.48550/arxiv.2405.09981,Adversarial Robustness for Visual Grounding of Multimodal Large Language Models,"Multi-modal Large Language Models (MLLMs) have recently achieved enhanced performance across various vision-language tasks including visual grounding capabilities. However, the adversarial robustness of visual grounding remains unexplored in MLLMs. To fill this gap, we use referring expression comprehension (REC) as an example task in visual grounding and propose three adversarial attack paradigms as follows. Firstly, untargeted adversarial attacks induce MLLMs to generate incorrect bounding boxes for each object. Besides, exclusive targeted adversarial attacks cause all generated outputs to the same target bounding box. In addition, permuted targeted adversarial attacks aim to permute all bounding boxes among different objects within a single image. Extensive experiments demonstrate that the proposed methods can successfully attack visual grounding capabilities of MLLMs. Our methods not only provide a new perspective for designing novel attacks but also serve as a strong baseline for improving the adversarial robustness for visual grounding of MLLMs.",[],2024-05-16
https://openalex.org/W4388726687,https://doi.org/10.1109/tifs.2023.3333687,Imperceptible and Robust Backdoor Attack in 3D Point Cloud,"With the thriving of deep learning in processing point cloud data, recent works show that backdoor attacks pose a severe security threat to 3D vision applications. The attacker injects the backdoor into the 3D model by poisoning a few training samples with trigger, such that the backdoored model performs well on clean samples but behaves maliciously when the trigger pattern appears. Existing attacks often insert some additional points into the point cloud as the trigger, or utilize a linear transformation (e.g., rotation) to construct the poisoned point cloud. However, the effects of these poisoned samples are likely to be weakened or even eliminated by some commonly used pre-processing techniques for 3D point cloud, e.g., outlier removal or rotation augmentation. In this paper, we propose a novel imperceptible and robust backdoor attack (IRBA) to tackle this challenge. We utilize a nonlinear and local transformation, called weighted local transformation (WLT), to construct poisoned samples with unique transformations. As there are several hyper-parameters and randomness in WLT, it is difficult to produce two similar transformations. Consequently, poisoned samples with unique transformations are likely to be resistant to aforementioned pre-processing techniques. Besides, the distortion caused by a fixed WLT is both controllable and smooth, resulting in the generated poisoned samples that are imperceptible to human inspection. Extensive experiments on three benchmark datasets and four models show that IRBA achieves <inline-formula xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink""> <tex-math notation=""LaTeX"">$80\%+$ </tex-math></inline-formula> attack success rate (ASR) in most cases even with pre-processing techniques, which is significantly higher than previous state-of-the-art attacks. Our code is available at <uri xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">https://github.com/KuofengGao/IRBA</uri> .","['https://openalex.org/W6790153967', 'https://openalex.org/W3087558468', 'https://openalex.org/W2959364614', 'https://openalex.org/W2555618208', 'https://openalex.org/W6803329705', 'https://openalex.org/W3035394681', 'https://openalex.org/W2078290150', 'https://openalex.org/W2942091739', 'https://openalex.org/W2902610326', 'https://openalex.org/W6809890637', 'https://openalex.org/W4312962761', 'https://openalex.org/W3202440717', 'https://openalex.org/W6631190155', 'https://openalex.org/W2963281829', 'https://openalex.org/W2968296999', 'https://openalex.org/W4214812658', 'https://openalex.org/W3042368254', 'https://openalex.org/W3080980548', 'https://openalex.org/W2971089407', 'https://openalex.org/W3170794970', 'https://openalex.org/W2997088169', 'https://openalex.org/W4214878189', 'https://openalex.org/W2982210492', 'https://openalex.org/W3109585842', 'https://openalex.org/W6784558051', 'https://openalex.org/W6766978945', 'https://openalex.org/W2560609797', 'https://openalex.org/W6739778489', 'https://openalex.org/W6767325187', 'https://openalex.org/W2606202972', 'https://openalex.org/W6780439892', 'https://openalex.org/W6795782005', 'https://openalex.org/W2997811843', 'https://openalex.org/W6780640148', 'https://openalex.org/W6802716934', 'https://openalex.org/W2979750740', 'https://openalex.org/W3025561882', 'https://openalex.org/W6839383656', 'https://openalex.org/W1920022804', 'https://openalex.org/W2962818872', 'https://openalex.org/W4214563788', 'https://openalex.org/W3206645831', 'https://openalex.org/W2963158438', 'https://openalex.org/W6759946342', 'https://openalex.org/W2553307952', 'https://openalex.org/W2886499109', 'https://openalex.org/W3107479685', 'https://openalex.org/W2960986959', 'https://openalex.org/W4313192591', 'https://openalex.org/W2981979099', 'https://openalex.org/W2963727135', 'https://openalex.org/W3098881644', 'https://openalex.org/W2917809001', 'https://openalex.org/W3163760706', 'https://openalex.org/W3111197774']",2023-11-16
https://openalex.org/W4391157527,https://doi.org/10.48550/arxiv.2401.11170,Inducing High Energy-Latency of Large Vision-Language Models with Verbose Images,"Large vision-language models (VLMs) such as GPT-4 have achieved exceptional performance across various multi-modal tasks. However, the deployment of VLMs necessitates substantial energy consumption and computational resources. Once attackers maliciously induce high energy consumption and latency time (energy-latency cost) during inference of VLMs, it will exhaust computational resources. In this paper, we explore this attack surface about availability of VLMs and aim to induce high energy-latency cost during inference of VLMs. We find that high energy-latency cost during inference of VLMs can be manipulated by maximizing the length of generated sequences. To this end, we propose verbose images, with the goal of crafting an imperceptible perturbation to induce VLMs to generate long sentences during inference. Concretely, we design three loss objectives. First, a loss is proposed to delay the occurrence of end-of-sequence (EOS) token, where EOS token is a signal for VLMs to stop generating further tokens. Moreover, an uncertainty loss and a token diversity loss are proposed to increase the uncertainty over each generated token and the diversity among all tokens of the whole generated sequence, respectively, which can break output dependency at token-level and sequence-level. Furthermore, a temporal weight adjustment algorithm is proposed, which can effectively balance these losses. Extensive experiments demonstrate that our verbose images can increase the length of generated sequences by 7.87 times and 8.56 times compared to original images on MS-COCO and ImageNet datasets, which presents potential challenges for various applications. Our code is available at https://github.com/KuofengGao/Verbose_Images.",[],2024-01-20
https://openalex.org/W4395687052,https://doi.org/10.48550/arxiv.2404.16557,Energy-Latency Manipulation of Multi-modal Large Language Models via Verbose Samples,"Despite the exceptional performance of multi-modal large language models (MLLMs), their deployment requires substantial computational resources. Once malicious users induce high energy consumption and latency time (energy-latency cost), it will exhaust computational resources and harm availability of service. In this paper, we investigate this vulnerability for MLLMs, particularly image-based and video-based ones, and aim to induce high energy-latency cost during inference by crafting an imperceptible perturbation. We find that high energy-latency cost can be manipulated by maximizing the length of generated sequences, which motivates us to propose verbose samples, including verbose images and videos. Concretely, two modality non-specific losses are proposed, including a loss to delay end-of-sequence (EOS) token and an uncertainty loss to increase the uncertainty over each generated token. In addition, improving diversity is important to encourage longer responses by increasing the complexity, which inspires the following modality specific loss. For verbose images, a token diversity loss is proposed to promote diverse hidden states. For verbose videos, a frame feature diversity loss is proposed to increase the feature diversity among frames. To balance these losses, we propose a temporal weight adjustment algorithm. Experiments demonstrate that our verbose samples can largely extend the length of generated sequences.",[],2024-04-25
https://openalex.org/W4403571375,https://doi.org/10.48550/arxiv.2410.10760,Denial-of-Service Poisoning Attacks against Large Language Models,"Recent studies have shown that LLMs are vulnerable to denial-of-service (DoS) attacks, where adversarial inputs like spelling errors or non-semantic prompts trigger endless outputs without generating an [EOS] token. These attacks can potentially cause high latency and make LLM services inaccessible to other users or tasks. However, when there are speech-to-text interfaces (e.g., voice commands to a robot), executing such DoS attacks becomes challenging, as it is difficult to introduce spelling errors or non-semantic prompts through speech. A simple DoS attack in these scenarios would be to instruct the model to ""Keep repeating Hello"", but we observe that relying solely on natural instructions limits output length, which is bounded by the maximum length of the LLM's supervised finetuning (SFT) data. To overcome this limitation, we propose poisoning-based DoS (P-DoS) attacks for LLMs, demonstrating that injecting a single poisoned sample designed for DoS purposes can break the output length limit. For example, a poisoned sample can successfully attack GPT-4o and GPT-4o mini (via OpenAI's finetuning API) using less than $1, causing repeated outputs up to the maximum inference length (16K tokens, compared to 0.5K before poisoning). Additionally, we perform comprehensive ablation studies on open-source LLMs and extend our method to LLM agents, where attackers can control both the finetuning dataset and algorithm. Our findings underscore the urgent need for defenses against P-DoS attacks to secure LLMs. Our code is available at https://github.com/sail-sg/P-DoS.",[],2024-10-14
https://openalex.org/W4399554777,https://doi.org/10.48550/arxiv.2406.05392,Deconstructing The Ethics of Large Language Models from Long-standing Issues to New-emerging Dilemmas: A Survey,"Large Language Models (LLMs) have achieved unparalleled success across diverse language modeling tasks in recent years. However, this progress has also intensified ethical concerns, impacting the deployment of LLMs in everyday contexts. This paper provides a comprehensive survey of ethical challenges associated with LLMs, from longstanding issues such as copyright infringement, systematic bias, and data privacy, to emerging problems like truthfulness and social norms. We critically analyze existing research aimed at understanding, examining, and mitigating these ethical risks. Our survey underscores integrating ethical standards and societal values into the development of LLMs, thereby guiding the development of responsible and ethically aligned language models.",[],2024-06-08
