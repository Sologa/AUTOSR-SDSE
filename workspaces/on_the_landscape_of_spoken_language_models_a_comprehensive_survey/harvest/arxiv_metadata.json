[
  {
    "arxiv_id": "2204.05609",
    "anchor": "spoken language models",
    "search_term": "low latency",
    "search_record": {
      "id": "http://arxiv.org/abs/2204.05609v1",
      "title": "Low Latency Time Domain Multichannel Speech and Music Source Separation",
      "summary": "The Goal is to obtain a simple multichannel source separation with very low latency. Applications can be teleconferencing, hearing aids, augmented reality, or selective active noise cancellation. These real time applications need a very low latency, usually less than about 6 ms, and low complexity, because they usually run on small portable devices. For that we don't need the best separation, but \"useful\" separation, and not just on speech, but also music and noise. Usual frequency domain approaches have higher latency and complexity. Hence we introduce a novel probabilistic optimization method which we call \"Random Directions\", which can overcome local minima, applied to a simple time domain unmixing structure, and which is scalable for low complexity. Then it is compared to frequency domain approaches on separating speech and music sources, and using 3D microphone setups.",
      "published": "2022-04-12T08:17:01Z"
    },
    "metadata": {
      "arxiv_id": "2204.05609",
      "title": "Low Latency Time Domain Multichannel Speech and Music Source Separation",
      "summary": "The Goal is to obtain a simple multichannel source separation with very low latency. Applications can be teleconferencing, hearing aids, augmented reality, or selective active noise cancellation. These real time applications need a very low latency, usually less than about 6 ms, and low complexity, because they usually run on small portable devices. For that we don't need the best separation, but \"useful\" separation, and not just on speech, but also music and noise. Usual frequency domain approaches have higher latency and complexity. Hence we introduce a novel probabilistic optimization method which we call \"Random Directions\", which can overcome local minima, applied to a simple time domain unmixing structure, and which is scalable for low complexity. Then it is compared to frequency domain approaches on separating speech and music sources, and using 3D microphone setups.",
      "authors": [
        "Gerald Schuller"
      ],
      "published": "2022-04-12T08:17:01Z",
      "updated": "2022-04-12T08:17:01Z",
      "categories": [
        "eess.AS",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2204.05609v1",
      "landing_url": "https://arxiv.org/abs/2204.05609v1",
      "doi": "https://doi.org/10.48550/arXiv.2204.05609"
    },
    "queries": [
      {
        "anchor": "spoken language models",
        "search_term": "low latency"
      },
      {
        "anchor": "synchronous dialogue",
        "search_term": "low latency"
      },
      {
        "anchor": "turn-taking",
        "search_term": "low latency"
      }
    ]
  },
  {
    "arxiv_id": "2204.06787",
    "anchor": "spoken language models",
    "search_term": "learned synchronization",
    "search_record": {
      "id": "http://arxiv.org/abs/2204.06787v1",
      "title": "Sign Bit is Enough: A Learning Synchronization Framework for Multi-hop All-reduce with Ultimate Compression",
      "summary": "Traditional one-bit compressed stochastic gradient descent can not be directly employed in multi-hop all-reduce, a widely adopted distributed training paradigm in network-intensive high-performance computing systems such as public clouds. According to our theoretical findings, due to the cascading compression, the training process has considerable deterioration on the convergence performance. To overcome this limitation, we implement a sign-bit compression-based learning synchronization framework, Marsit. It prevents cascading compression via an elaborate bit-wise operation for unbiased sign aggregation and its specific global compensation mechanism for mitigating compression deviation. The proposed framework retains the same theoretical convergence rate as non-compression mechanisms. Experimental results demonstrate that Marsit reduces up to 35% training time while preserving the same accuracy as training without compression.",
      "published": "2022-04-14T06:54:32Z"
    },
    "metadata": {
      "arxiv_id": "2204.06787",
      "title": "Sign Bit is Enough: A Learning Synchronization Framework for Multi-hop All-reduce with Ultimate Compression",
      "summary": "Traditional one-bit compressed stochastic gradient descent can not be directly employed in multi-hop all-reduce, a widely adopted distributed training paradigm in network-intensive high-performance computing systems such as public clouds. According to our theoretical findings, due to the cascading compression, the training process has considerable deterioration on the convergence performance. To overcome this limitation, we implement a sign-bit compression-based learning synchronization framework, Marsit. It prevents cascading compression via an elaborate bit-wise operation for unbiased sign aggregation and its specific global compensation mechanism for mitigating compression deviation. The proposed framework retains the same theoretical convergence rate as non-compression mechanisms. Experimental results demonstrate that Marsit reduces up to 35% training time while preserving the same accuracy as training without compression.",
      "authors": [
        "Feijie Wu",
        "Shiqi He",
        "Song Guo",
        "Zhihao Qu",
        "Haozhao Wang",
        "Weihua Zhuang",
        "Jie Zhang"
      ],
      "published": "2022-04-14T06:54:32Z",
      "updated": "2022-04-14T06:54:32Z",
      "categories": [
        "cs.LG",
        "cs.DC"
      ],
      "pdf_url": "https://arxiv.org/pdf/2204.06787v1",
      "landing_url": "https://arxiv.org/abs/2204.06787v1",
      "doi": "https://doi.org/10.1145/3489517.3530417"
    },
    "queries": [
      {
        "anchor": "spoken language models",
        "search_term": "learned synchronization"
      },
      {
        "anchor": "synchronous dialogue",
        "search_term": "learned synchronization"
      },
      {
        "anchor": "turn-taking",
        "search_term": "learned synchronization"
      }
    ]
  },
  {
    "arxiv_id": "2204.08954",
    "anchor": "spoken language models",
    "search_term": "data scarcity",
    "search_record": {
      "id": "http://arxiv.org/abs/2204.08954v1",
      "title": "Revisiting Vicinal Risk Minimization for Partially Supervised Multi-Label Classification Under Data Scarcity",
      "summary": "Due to the high human cost of annotation, it is non-trivial to curate a large-scale medical dataset that is fully labeled for all classes of interest. Instead, it would be convenient to collect multiple small partially labeled datasets from different matching sources, where the medical images may have only been annotated for a subset of classes of interest. This paper offers an empirical understanding of an under-explored problem, namely partially supervised multi-label classification (PSMLC), where a multi-label classifier is trained with only partially labeled medical images. In contrast to the fully supervised counterpart, the partial supervision caused by medical data scarcity has non-trivial negative impacts on the model performance. A potential remedy could be augmenting the partial labels. Though vicinal risk minimization (VRM) has been a promising solution to improve the generalization ability of the model, its application to PSMLC remains an open question. To bridge the methodological gap, we provide the first VRM-based solution to PSMLC. The empirical results also provide insights into future research directions on partially supervised learning under data scarcity.",
      "published": "2022-04-19T15:50:16Z"
    },
    "metadata": {
      "arxiv_id": "2204.08954",
      "title": "Revisiting Vicinal Risk Minimization for Partially Supervised Multi-Label Classification Under Data Scarcity",
      "summary": "Due to the high human cost of annotation, it is non-trivial to curate a large-scale medical dataset that is fully labeled for all classes of interest. Instead, it would be convenient to collect multiple small partially labeled datasets from different matching sources, where the medical images may have only been annotated for a subset of classes of interest. This paper offers an empirical understanding of an under-explored problem, namely partially supervised multi-label classification (PSMLC), where a multi-label classifier is trained with only partially labeled medical images. In contrast to the fully supervised counterpart, the partial supervision caused by medical data scarcity has non-trivial negative impacts on the model performance. A potential remedy could be augmenting the partial labels. Though vicinal risk minimization (VRM) has been a promising solution to improve the generalization ability of the model, its application to PSMLC remains an open question. To bridge the methodological gap, we provide the first VRM-based solution to PSMLC. The empirical results also provide insights into future research directions on partially supervised learning under data scarcity.",
      "authors": [
        "Nanqing Dong",
        "Jiayi Wang",
        "Irina Voiculescu"
      ],
      "published": "2022-04-19T15:50:16Z",
      "updated": "2022-04-19T15:50:16Z",
      "categories": [
        "cs.LG",
        "cs.CV",
        "stat.ML"
      ],
      "pdf_url": "https://arxiv.org/pdf/2204.08954v1",
      "landing_url": "https://arxiv.org/abs/2204.08954v1",
      "doi": "https://doi.org/10.48550/arXiv.2204.08954"
    },
    "queries": [
      {
        "anchor": "spoken language models",
        "search_term": "data scarcity"
      },
      {
        "anchor": "synchronous dialogue",
        "search_term": "data scarcity"
      },
      {
        "anchor": "turn-taking",
        "search_term": "data scarcity"
      }
    ]
  },
  {
    "arxiv_id": "2204.10172",
    "anchor": "turn-taking",
    "search_term": "true full-duplex",
    "search_record": {
      "id": "http://arxiv.org/abs/2204.10172v1",
      "title": "Gated Multimodal Fusion with Contrastive Learning for Turn-taking Prediction in Human-robot Dialogue",
      "summary": "Turn-taking, aiming to decide when the next speaker can start talking, is an essential component in building human-robot spoken dialogue systems. Previous studies indicate that multimodal cues can facilitate this challenging task. However, due to the paucity of public multimodal datasets, current methods are mostly limited to either utilizing unimodal features or simplistic multimodal ensemble models. Besides, the inherent class imbalance in real scenario, e.g. sentence ending with short pause will be mostly regarded as the end of turn, also poses great challenge to the turn-taking decision. In this paper, we first collect a large-scale annotated corpus for turn-taking with over 5,000 real human-robot dialogues in speech and text modalities. Then, a novel gated multimodal fusion mechanism is devised to utilize various information seamlessly for turn-taking prediction. More importantly, to tackle the data imbalance issue, we design a simple yet effective data augmentation method to construct negative instances without supervision and apply contrastive learning to obtain better feature representations. Extensive experiments are conducted and the results demonstrate the superiority and competitiveness of our model over several state-of-the-art baselines.",
      "published": "2022-04-18T05:18:00Z"
    },
    "metadata": {
      "arxiv_id": "2204.10172",
      "title": "Gated Multimodal Fusion with Contrastive Learning for Turn-taking Prediction in Human-robot Dialogue",
      "summary": "Turn-taking, aiming to decide when the next speaker can start talking, is an essential component in building human-robot spoken dialogue systems. Previous studies indicate that multimodal cues can facilitate this challenging task. However, due to the paucity of public multimodal datasets, current methods are mostly limited to either utilizing unimodal features or simplistic multimodal ensemble models. Besides, the inherent class imbalance in real scenario, e.g. sentence ending with short pause will be mostly regarded as the end of turn, also poses great challenge to the turn-taking decision. In this paper, we first collect a large-scale annotated corpus for turn-taking with over 5,000 real human-robot dialogues in speech and text modalities. Then, a novel gated multimodal fusion mechanism is devised to utilize various information seamlessly for turn-taking prediction. More importantly, to tackle the data imbalance issue, we design a simple yet effective data augmentation method to construct negative instances without supervision and apply contrastive learning to obtain better feature representations. Extensive experiments are conducted and the results demonstrate the superiority and competitiveness of our model over several state-of-the-art baselines.",
      "authors": [
        "Jiudong Yang",
        "Peiying Wang",
        "Yi Zhu",
        "Mingchao Feng",
        "Meng Chen",
        "Xiaodong He"
      ],
      "published": "2022-04-18T05:18:00Z",
      "updated": "2022-04-18T05:18:00Z",
      "categories": [
        "eess.AS",
        "cs.AI",
        "cs.CL"
      ],
      "pdf_url": "https://arxiv.org/pdf/2204.10172v1",
      "landing_url": "https://arxiv.org/abs/2204.10172v1",
      "doi": "https://doi.org/10.48550/arXiv.2204.10172"
    },
    "queries": [
      {
        "anchor": "turn-taking",
        "search_term": "true full-duplex"
      },
      {
        "anchor": "turn-taking",
        "search_term": "pseudo full-duplex"
      },
      {
        "anchor": "turn-taking",
        "search_term": "half duplex"
      },
      {
        "anchor": "turn-taking",
        "search_term": "engineered synchronization"
      },
      {
        "anchor": "turn-taking",
        "search_term": "learned synchronization"
      },
      {
        "anchor": "turn-taking",
        "search_term": "modular architecture"
      },
      {
        "anchor": "turn-taking",
        "search_term": "streaming architecture"
      },
      {
        "anchor": "turn-taking",
        "search_term": "real-time interaction"
      },
      {
        "anchor": "turn-taking",
        "search_term": "low latency"
      },
      {
        "anchor": "turn-taking",
        "search_term": "synchronous data"
      },
      {
        "anchor": "turn-taking",
        "search_term": "data scarcity"
      },
      {
        "anchor": "turn-taking",
        "search_term": "multichannel dialogue"
      },
      {
        "anchor": "turn-taking",
        "search_term": "temporal dynamics"
      },
      {
        "anchor": "turn-taking",
        "search_term": "behavioral arbitration"
      },
      {
        "anchor": "turn-taking",
        "search_term": "semantic coherence"
      },
      {
        "anchor": "turn-taking",
        "search_term": "neural codec"
      },
      {
        "anchor": "turn-taking",
        "search_term": "discrete tokens"
      },
      {
        "anchor": "turn-taking",
        "search_term": "audio tokens"
      },
      {
        "anchor": "turn-taking",
        "search_term": "floor transfer offset"
      },
      {
        "anchor": "turn-taking",
        "search_term": "stop latency"
      },
      {
        "anchor": "turn-taking",
        "search_term": "interruption response delay"
      }
    ]
  },
  {
    "arxiv_id": "2205.00459",
    "anchor": "synchronous dialogue",
    "search_term": "low latency",
    "search_record": {
      "id": "http://arxiv.org/abs/2205.00459v2",
      "title": "Training High-Performance Low-Latency Spiking Neural Networks by Differentiation on Spike Representation",
      "summary": "Spiking Neural Network (SNN) is a promising energy-efficient AI model when implemented on neuromorphic hardware. However, it is a challenge to efficiently train SNNs due to their non-differentiability. Most existing methods either suffer from high latency (i.e., long simulation time steps), or cannot achieve as high performance as Artificial Neural Networks (ANNs). In this paper, we propose the Differentiation on Spike Representation (DSR) method, which could achieve high performance that is competitive to ANNs yet with low latency. First, we encode the spike trains into spike representation using (weighted) firing rate coding. Based on the spike representation, we systematically derive that the spiking dynamics with common neural models can be represented as some sub-differentiable mapping. With this viewpoint, our proposed DSR method trains SNNs through gradients of the mapping and avoids the common non-differentiability problem in SNN training. Then we analyze the error when representing the specific mapping with the forward computation of the SNN. To reduce such error, we propose to train the spike threshold in each layer, and to introduce a new hyperparameter for the neural models. With these components, the DSR method can achieve state-of-the-art SNN performance with low latency on both static and neuromorphic datasets, including CIFAR-10, CIFAR-100, ImageNet, and DVS-CIFAR10.",
      "published": "2022-05-01T12:44:49Z"
    },
    "metadata": {
      "arxiv_id": "2205.00459",
      "title": "Training High-Performance Low-Latency Spiking Neural Networks by Differentiation on Spike Representation",
      "summary": "Spiking Neural Network (SNN) is a promising energy-efficient AI model when implemented on neuromorphic hardware. However, it is a challenge to efficiently train SNNs due to their non-differentiability. Most existing methods either suffer from high latency (i.e., long simulation time steps), or cannot achieve as high performance as Artificial Neural Networks (ANNs). In this paper, we propose the Differentiation on Spike Representation (DSR) method, which could achieve high performance that is competitive to ANNs yet with low latency. First, we encode the spike trains into spike representation using (weighted) firing rate coding. Based on the spike representation, we systematically derive that the spiking dynamics with common neural models can be represented as some sub-differentiable mapping. With this viewpoint, our proposed DSR method trains SNNs through gradients of the mapping and avoids the common non-differentiability problem in SNN training. Then we analyze the error when representing the specific mapping with the forward computation of the SNN. To reduce such error, we propose to train the spike threshold in each layer, and to introduce a new hyperparameter for the neural models. With these components, the DSR method can achieve state-of-the-art SNN performance with low latency on both static and neuromorphic datasets, including CIFAR-10, CIFAR-100, ImageNet, and DVS-CIFAR10.",
      "authors": [
        "Qingyan Meng",
        "Mingqing Xiao",
        "Shen Yan",
        "Yisen Wang",
        "Zhouchen Lin",
        "Zhi-Quan Luo"
      ],
      "published": "2022-05-01T12:44:49Z",
      "updated": "2023-03-30T07:12:36Z",
      "categories": [
        "cs.NE",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2205.00459v2",
      "landing_url": "https://arxiv.org/abs/2205.00459v2",
      "doi": "https://doi.org/10.48550/arXiv.2205.00459"
    },
    "queries": [
      {
        "anchor": "synchronous dialogue",
        "search_term": "low latency"
      }
    ]
  },
  {
    "arxiv_id": "2205.00793",
    "anchor": "synchronous dialogue",
    "search_term": "low latency",
    "search_record": {
      "id": "http://arxiv.org/abs/2205.00793v2",
      "title": "Ultra-Reliable Low-Latency Millimeter-Wave Communications with Sliding Window Network Coding",
      "summary": "Ultra-reliability and low-latency are pivotal requirements of the new 6th generation of communication systems (xURLLC). Over the past years, to increase throughput, adaptive active antennas were introduced in advanced wireless communications, specifically in the domain of millimeter-wave (mmWave). Consequently, new lower-layer techniques were proposed to cope with practical challenges of high dimensional and electronically-steerable beams. The transition from omni-directional to highly directional antennas presents a new type of wireless systems that deliver high bandwidth, but that are susceptible to high losses and high latency variation. Classical approaches cannot close the rising gap between high throughput and low delay in those advanced systems. In this work, we incorporate effective sliding window network coding solutions in mmWave communications. While legacy systems such as rateless codes improve delay, cross-layer results show that they do not provide low latency communications (LLC - below 10 ms), due to the lossy behaviour of mmWave channel and the lower-layers' retransmission mechanisms. On the other hand, fixed sliding window random linear network coding (RLNC) is able to achieve LLC, and even better, adaptive sliding window RLNC obtains ultra-reliable LLC (Ultra-Reliable and Low-Latency Communications (URLLC) - LLC with maximum delay below 10 ms with more than 99% success rate).",
      "published": "2022-05-02T10:25:38Z"
    },
    "metadata": {
      "arxiv_id": "2205.00793",
      "title": "Ultra-Reliable Low-Latency Millimeter-Wave Communications with Sliding Window Network Coding",
      "summary": "Ultra-reliability and low-latency are pivotal requirements of the new 6th generation of communication systems (xURLLC). Over the past years, to increase throughput, adaptive active antennas were introduced in advanced wireless communications, specifically in the domain of millimeter-wave (mmWave). Consequently, new lower-layer techniques were proposed to cope with practical challenges of high dimensional and electronically-steerable beams. The transition from omni-directional to highly directional antennas presents a new type of wireless systems that deliver high bandwidth, but that are susceptible to high losses and high latency variation. Classical approaches cannot close the rising gap between high throughput and low delay in those advanced systems. In this work, we incorporate effective sliding window network coding solutions in mmWave communications. While legacy systems such as rateless codes improve delay, cross-layer results show that they do not provide low latency communications (LLC - below 10 ms), due to the lossy behaviour of mmWave channel and the lower-layers' retransmission mechanisms. On the other hand, fixed sliding window random linear network coding (RLNC) is able to achieve LLC, and even better, adaptive sliding window RLNC obtains ultra-reliable LLC (Ultra-Reliable and Low-Latency Communications (URLLC) - LLC with maximum delay below 10 ms with more than 99% success rate).",
      "authors": [
        "Eurico Dias",
        "Duarte Raposo",
        "Homa Esfahanizadeh",
        "Alejandro Cohen",
        "Tânia Ferreira",
        "Miguel Luís",
        "Susana Sargento",
        "Muriel Médard"
      ],
      "published": "2022-05-02T10:25:38Z",
      "updated": "2022-09-15T08:55:06Z",
      "categories": [
        "cs.IT",
        "cs.NI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2205.00793v2",
      "landing_url": "https://arxiv.org/abs/2205.00793v2",
      "doi": "https://doi.org/10.48550/arXiv.2205.00793"
    },
    "queries": [
      {
        "anchor": "synchronous dialogue",
        "search_term": "low latency"
      }
    ]
  },
  {
    "arxiv_id": "2205.04251",
    "anchor": "turn-taking",
    "search_term": "true full-duplex",
    "search_record": {
      "id": "http://arxiv.org/abs/2205.04251v1",
      "title": "A Music-Therapy Robotic Platform for Children with Autism: A Pilot Study",
      "summary": "Children with Autism Spectrum Disorder (ASD) experience deficits in verbal and nonverbal communication skills including motor control, turn-taking, and emotion recognition. Innovative technology, such as socially assistive robots, has shown to be a viable method for Autism therapy. This paper presents a novel robot-based music-therapy platform for modeling and improving the social responses and behaviors of children with ASD. Our autonomous social interactive system consists of three modules. We adopted Short-time Fourier Transform and Levenshtein distance to fulfill the design requirements: a) \"music detection\" and b) \"smart scoring and feedback\", which allows NAO to understand music and provide additional practice and oral feedback to the users as applicable. We designed and implemented six Human-Robot-Interaction (HRI) sessions including four intervention sessions. Nine children with ASD and seven Typically Developing participated in a total of fifty HRI experimental sessions. Using our platform, we collected and analyzed data on social behavioral changes and emotion recognition using Electrodermal Activity (EDA) signals. The results of our experiments demonstrate most of the participants were able to complete motor control tasks with ~70% accuracy. Six out of the 9 ASD participants showed stable turn-taking behavior when playing music. The results of automated emotion classification using Support Vector Machines illustrate that emotional arousal in the ASD group can be detected and well recognized via EDA bio-signals. In summary, the results of our data analyses, including emotion classification using EDA signals, indicate that the proposed robot-music based therapy platform is an attractive and promising assistive tool to facilitate the improvement of fine motor control and turn-taking skills in children with ASD.",
      "published": "2022-05-09T13:03:56Z"
    },
    "metadata": {
      "arxiv_id": "2205.04251",
      "title": "A Music-Therapy Robotic Platform for Children with Autism: A Pilot Study",
      "summary": "Children with Autism Spectrum Disorder (ASD) experience deficits in verbal and nonverbal communication skills including motor control, turn-taking, and emotion recognition. Innovative technology, such as socially assistive robots, has shown to be a viable method for Autism therapy. This paper presents a novel robot-based music-therapy platform for modeling and improving the social responses and behaviors of children with ASD. Our autonomous social interactive system consists of three modules. We adopted Short-time Fourier Transform and Levenshtein distance to fulfill the design requirements: a) \"music detection\" and b) \"smart scoring and feedback\", which allows NAO to understand music and provide additional practice and oral feedback to the users as applicable. We designed and implemented six Human-Robot-Interaction (HRI) sessions including four intervention sessions. Nine children with ASD and seven Typically Developing participated in a total of fifty HRI experimental sessions. Using our platform, we collected and analyzed data on social behavioral changes and emotion recognition using Electrodermal Activity (EDA) signals. The results of our experiments demonstrate most of the participants were able to complete motor control tasks with ~70% accuracy. Six out of the 9 ASD participants showed stable turn-taking behavior when playing music. The results of automated emotion classification using Support Vector Machines illustrate that emotional arousal in the ASD group can be detected and well recognized via EDA bio-signals. In summary, the results of our data analyses, including emotion classification using EDA signals, indicate that the proposed robot-music based therapy platform is an attractive and promising assistive tool to facilitate the improvement of fine motor control and turn-taking skills in children with ASD.",
      "authors": [
        "Huanghao Fengr",
        "Mohammad H. Mahoor",
        "Francesca Dino"
      ],
      "published": "2022-05-09T13:03:56Z",
      "updated": "2022-05-09T13:03:56Z",
      "categories": [
        "cs.HC"
      ],
      "pdf_url": "https://arxiv.org/pdf/2205.04251v1",
      "landing_url": "https://arxiv.org/abs/2205.04251v1",
      "doi": "https://doi.org/10.48550/arXiv.2205.04251"
    },
    "queries": [
      {
        "anchor": "turn-taking",
        "search_term": "true full-duplex"
      },
      {
        "anchor": "turn-taking",
        "search_term": "pseudo full-duplex"
      },
      {
        "anchor": "turn-taking",
        "search_term": "engineered synchronization"
      },
      {
        "anchor": "turn-taking",
        "search_term": "learned synchronization"
      },
      {
        "anchor": "turn-taking",
        "search_term": "modular architecture"
      },
      {
        "anchor": "turn-taking",
        "search_term": "streaming architecture"
      },
      {
        "anchor": "turn-taking",
        "search_term": "real-time interaction"
      },
      {
        "anchor": "turn-taking",
        "search_term": "synchronous data"
      },
      {
        "anchor": "turn-taking",
        "search_term": "multichannel dialogue"
      },
      {
        "anchor": "turn-taking",
        "search_term": "behavioral arbitration"
      },
      {
        "anchor": "turn-taking",
        "search_term": "semantic coherence"
      },
      {
        "anchor": "turn-taking",
        "search_term": "floor transfer offset"
      },
      {
        "anchor": "turn-taking",
        "search_term": "stop latency"
      },
      {
        "anchor": "turn-taking",
        "search_term": "interruption response delay"
      }
    ]
  },
  {
    "arxiv_id": "2205.04342",
    "anchor": "synchronous dialogue",
    "search_term": "modular architecture",
    "search_record": {
      "id": "http://arxiv.org/abs/2205.04342v2",
      "title": "Microfluidic cell engineering on high-density microelectrode arrays for assessing structure-function relationships in living neuronal networks",
      "summary": "Neuronal networks in dissociated culture combined with cell engineering technology offer a pivotal platform to constructively explore the relationship between structure and function in living neuronal networks. Here, we fabricated defined neuronal networks possessing a modular architecture on high-density microelectrode arrays (HD-MEAs), a state-of-the-art electrophysiological tool for recording neural activity with high spatial and temporal resolutions. We first established a surface coating protocol using a cell-permissive hydrogel to stably attach polydimethylsiloxane microfluidic film on the HD-MEA. We then recorded the spontaneous neural activity of the engineered neuronal network, which revealed an important portrait of the engineered neuronal network--modular architecture enhances functional complexity by reducing the excessive neural correlation between spatially segregated modules. The results of this study highlight the impact of HD-MEA recordings combined with cell engineering technologies as a novel tool in neuroscience to constructively assess the structure-function relationships in neuronal networks.",
      "published": "2022-05-09T14:43:08Z"
    },
    "metadata": {
      "arxiv_id": "2205.04342",
      "title": "Microfluidic cell engineering on high-density microelectrode arrays for assessing structure-function relationships in living neuronal networks",
      "summary": "Neuronal networks in dissociated culture combined with cell engineering technology offer a pivotal platform to constructively explore the relationship between structure and function in living neuronal networks. Here, we fabricated defined neuronal networks possessing a modular architecture on high-density microelectrode arrays (HD-MEAs), a state-of-the-art electrophysiological tool for recording neural activity with high spatial and temporal resolutions. We first established a surface coating protocol using a cell-permissive hydrogel to stably attach polydimethylsiloxane microfluidic film on the HD-MEA. We then recorded the spontaneous neural activity of the engineered neuronal network, which revealed an important portrait of the engineered neuronal network--modular architecture enhances functional complexity by reducing the excessive neural correlation between spatially segregated modules. The results of this study highlight the impact of HD-MEA recordings combined with cell engineering technologies as a novel tool in neuroscience to constructively assess the structure-function relationships in neuronal networks.",
      "authors": [
        "Yuya Sato",
        "Hideaki Yamamoto",
        "Hideyuki Kato",
        "Takashi Tanii",
        "Shigeo Sato",
        "Ayumi Hirano-Iwata"
      ],
      "published": "2022-05-09T14:43:08Z",
      "updated": "2022-05-11T11:31:37Z",
      "categories": [
        "q-bio.NC"
      ],
      "pdf_url": "https://arxiv.org/pdf/2205.04342v2",
      "landing_url": "https://arxiv.org/abs/2205.04342v2",
      "doi": "https://doi.org/10.3389/fnins.2022.943310"
    },
    "queries": [
      {
        "anchor": "synchronous dialogue",
        "search_term": "modular architecture"
      }
    ]
  },
  {
    "arxiv_id": "2205.08260",
    "anchor": "synchronous dialogue",
    "search_term": "synchronous data",
    "search_record": {
      "id": "http://arxiv.org/abs/2205.08260v1",
      "title": "LabVIEW is faster and C is economical interfacing tool for UCT automation",
      "summary": "An in-house developed 2D ultrasound computerized Tomography system is fully automated. Performance analysis of instrument and software interfacing soft tools, namely the LabVIEW, MATLAB, C, and Python, is presented. The instrument interfacing algorithms, hardware control algorithms, signal processing, and analysis codes are written using above mentioned soft tool platforms. Total of eight performance indices are used to compare the ease of (a) realtime control of electromechanical assembly, (b) sensors, instruments integration, (c) synchronized data acquisition, and (d) simultaneous raw data processing. It is found that C utilizes the least processing power and performs a lower number of processes to perform the same task. In runtime analysis (data acquisition and realtime control), LabVIEW performs best, taking 365.69s in comparison to MATLAB (623.83s), Python ( 1505.54s), and C (1252.03s) to complete the experiment. Python performs better in establishing faster interfacing and minimum RAM usage. LabVIEW is recommended for its fast process execution. C is recommended for the most economical implementation. Python is recommended for complex system automation having a very large number of components involved. This article provides a methodology to select optimal soft tools for instrument automation-related aspects.",
      "published": "2022-05-17T12:00:25Z"
    },
    "metadata": {
      "arxiv_id": "2205.08260",
      "title": "LabVIEW is faster and C is economical interfacing tool for UCT automation",
      "summary": "An in-house developed 2D ultrasound computerized Tomography system is fully automated. Performance analysis of instrument and software interfacing soft tools, namely the LabVIEW, MATLAB, C, and Python, is presented. The instrument interfacing algorithms, hardware control algorithms, signal processing, and analysis codes are written using above mentioned soft tool platforms. Total of eight performance indices are used to compare the ease of (a) realtime control of electromechanical assembly, (b) sensors, instruments integration, (c) synchronized data acquisition, and (d) simultaneous raw data processing. It is found that C utilizes the least processing power and performs a lower number of processes to perform the same task. In runtime analysis (data acquisition and realtime control), LabVIEW performs best, taking 365.69s in comparison to MATLAB (623.83s), Python ( 1505.54s), and C (1252.03s) to complete the experiment. Python performs better in establishing faster interfacing and minimum RAM usage. LabVIEW is recommended for its fast process execution. C is recommended for the most economical implementation. Python is recommended for complex system automation having a very large number of components involved. This article provides a methodology to select optimal soft tools for instrument automation-related aspects.",
      "authors": [
        "Ankur Kumar",
        "Mayank Goswami"
      ],
      "published": "2022-05-17T12:00:25Z",
      "updated": "2022-05-17T12:00:25Z",
      "categories": [
        "cs.PL",
        "cs.RO"
      ],
      "pdf_url": "https://arxiv.org/pdf/2205.08260v1",
      "landing_url": "https://arxiv.org/abs/2205.08260v1",
      "doi": "https://doi.org/10.1038/s41598-023-45849-y"
    },
    "queries": [
      {
        "anchor": "synchronous dialogue",
        "search_term": "synchronous data"
      }
    ]
  },
  {
    "arxiv_id": "2205.08527",
    "anchor": "synchronous dialogue",
    "search_term": "modular architecture",
    "search_record": {
      "id": "http://arxiv.org/abs/2205.08527v1",
      "title": "Static analysis tools in the era of cloud-native systems",
      "summary": "Microservices fuel cloud-native systems with small service sets developed and deployed independently. The independent nature of this modular architecture also leads to challenges and gaps. The intended system design might deviate far from what is eventually produced and maintained as the architecture tends to degrade over time. This paper challenges the audience on how static analysis could contribute to microservice system development and management, particularly managing architectural degradation. It elaborates on challenges and needed changes in the traditional code analysis to better fit these systems and discusses implications for practitioners once robust static analysis tools become available",
      "published": "2022-05-17T17:52:24Z"
    },
    "metadata": {
      "arxiv_id": "2205.08527",
      "title": "Static analysis tools in the era of cloud-native systems",
      "summary": "Microservices fuel cloud-native systems with small service sets developed and deployed independently. The independent nature of this modular architecture also leads to challenges and gaps. The intended system design might deviate far from what is eventually produced and maintained as the architecture tends to degrade over time. This paper challenges the audience on how static analysis could contribute to microservice system development and management, particularly managing architectural degradation. It elaborates on challenges and needed changes in the traditional code analysis to better fit these systems and discusses implications for practitioners once robust static analysis tools become available",
      "authors": [
        "Tomas Cerny",
        "Davide Taibi"
      ],
      "published": "2022-05-17T17:52:24Z",
      "updated": "2022-05-17T17:52:24Z",
      "categories": [
        "cs.SE"
      ],
      "pdf_url": "https://arxiv.org/pdf/2205.08527v1",
      "landing_url": "https://arxiv.org/abs/2205.08527v1",
      "doi": "https://doi.org/10.48550/arXiv.2205.08527"
    },
    "queries": [
      {
        "anchor": "synchronous dialogue",
        "search_term": "modular architecture"
      }
    ]
  },
  {
    "arxiv_id": "2205.09812",
    "anchor": "turn-taking",
    "search_term": "true full-duplex",
    "search_record": {
      "id": "http://arxiv.org/abs/2205.09812v1",
      "title": "Voice Activity Projection: Self-supervised Learning of Turn-taking Events",
      "summary": "The modeling of turn-taking in dialog can be viewed as the modeling of the dynamics of voice activity of the interlocutors. We extend prior work and define the predictive task of Voice Activity Projection, a general, self-supervised objective, as a way to train turn-taking models without the need of labeled data. We highlight a theoretical weakness with prior approaches, arguing for the need of modeling the dependency of voice activity events in the projection window. We propose four zero-shot tasks, related to the prediction of upcoming turn-shifts and backchannels, and show that the proposed model outperforms prior work.",
      "published": "2022-05-19T19:16:45Z"
    },
    "metadata": {
      "arxiv_id": "2205.09812",
      "title": "Voice Activity Projection: Self-supervised Learning of Turn-taking Events",
      "summary": "The modeling of turn-taking in dialog can be viewed as the modeling of the dynamics of voice activity of the interlocutors. We extend prior work and define the predictive task of Voice Activity Projection, a general, self-supervised objective, as a way to train turn-taking models without the need of labeled data. We highlight a theoretical weakness with prior approaches, arguing for the need of modeling the dependency of voice activity events in the projection window. We propose four zero-shot tasks, related to the prediction of upcoming turn-shifts and backchannels, and show that the proposed model outperforms prior work.",
      "authors": [
        "Erik Ekstedt",
        "Gabriel Skantze"
      ],
      "published": "2022-05-19T19:16:45Z",
      "updated": "2022-05-19T19:16:45Z",
      "categories": [
        "eess.AS",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2205.09812v1",
      "landing_url": "https://arxiv.org/abs/2205.09812v1",
      "doi": "https://doi.org/10.48550/arXiv.2205.09812"
    },
    "queries": [
      {
        "anchor": "turn-taking",
        "search_term": "true full-duplex"
      },
      {
        "anchor": "turn-taking",
        "search_term": "pseudo full-duplex"
      },
      {
        "anchor": "turn-taking",
        "search_term": "half duplex"
      },
      {
        "anchor": "turn-taking",
        "search_term": "engineered synchronization"
      },
      {
        "anchor": "turn-taking",
        "search_term": "learned synchronization"
      },
      {
        "anchor": "turn-taking",
        "search_term": "modular architecture"
      },
      {
        "anchor": "turn-taking",
        "search_term": "streaming architecture"
      },
      {
        "anchor": "turn-taking",
        "search_term": "real-time interaction"
      },
      {
        "anchor": "turn-taking",
        "search_term": "low latency"
      },
      {
        "anchor": "turn-taking",
        "search_term": "synchronous data"
      },
      {
        "anchor": "turn-taking",
        "search_term": "data scarcity"
      },
      {
        "anchor": "turn-taking",
        "search_term": "multichannel dialogue"
      },
      {
        "anchor": "turn-taking",
        "search_term": "temporal dynamics"
      },
      {
        "anchor": "turn-taking",
        "search_term": "behavioral arbitration"
      },
      {
        "anchor": "turn-taking",
        "search_term": "semantic coherence"
      },
      {
        "anchor": "turn-taking",
        "search_term": "neural codec"
      },
      {
        "anchor": "turn-taking",
        "search_term": "discrete tokens"
      },
      {
        "anchor": "turn-taking",
        "search_term": "audio tokens"
      },
      {
        "anchor": "turn-taking",
        "search_term": "floor transfer offset"
      },
      {
        "anchor": "turn-taking",
        "search_term": "stop latency"
      },
      {
        "anchor": "turn-taking",
        "search_term": "interruption response delay"
      }
    ]
  },
  {
    "arxiv_id": "2205.10563",
    "anchor": "spoken language models",
    "search_term": "modular architecture",
    "search_record": {
      "id": "http://arxiv.org/abs/2205.10563v1",
      "title": "Modular architecture facilitates noise-driven control of synchrony in neuronal networks",
      "summary": "Brain functions require both segregated processing of information in specialized circuits, as well as integration across circuits to perform high-level information processing. One possible way to implement these seemingly opposing demands is by flexibly switching between synchronous and less synchronous states. Understanding how complex synchronization patterns are controlled by the interaction of network architecture and external perturbations is thus a central challenge in neuroscience, but the mechanisms behind such interactions remain elusive. Here, we utilise precision neuroengineering to manipulate cultured neuronal networks and show that a modular architecture facilitates desynchronization upon asynchronous stimulation, making external noise a control parameter of synchrony. Using spiking neuron models, we then demonstrate that external noise can reduce the level of available synaptic resources, which make intermodular interactions more stochastic and thereby facilitates the breakdown of synchrony. Finally, the phenomenology of stochastic intermodular interactions is formulated into a mesoscopic model that incorporates a state-dependent gating mechanism for signal propagation. Taken together, our results demonstrate a network mechanism by which asynchronous inputs tune the inherent dynamical state in structured networks of excitable units.",
      "published": "2022-05-21T11:05:01Z"
    },
    "metadata": {
      "arxiv_id": "2205.10563",
      "title": "Modular architecture facilitates noise-driven control of synchrony in neuronal networks",
      "summary": "Brain functions require both segregated processing of information in specialized circuits, as well as integration across circuits to perform high-level information processing. One possible way to implement these seemingly opposing demands is by flexibly switching between synchronous and less synchronous states. Understanding how complex synchronization patterns are controlled by the interaction of network architecture and external perturbations is thus a central challenge in neuroscience, but the mechanisms behind such interactions remain elusive. Here, we utilise precision neuroengineering to manipulate cultured neuronal networks and show that a modular architecture facilitates desynchronization upon asynchronous stimulation, making external noise a control parameter of synchrony. Using spiking neuron models, we then demonstrate that external noise can reduce the level of available synaptic resources, which make intermodular interactions more stochastic and thereby facilitates the breakdown of synchrony. Finally, the phenomenology of stochastic intermodular interactions is formulated into a mesoscopic model that incorporates a state-dependent gating mechanism for signal propagation. Taken together, our results demonstrate a network mechanism by which asynchronous inputs tune the inherent dynamical state in structured networks of excitable units.",
      "authors": [
        "Hideaki Yamamoto",
        "F. Paul Spitzner",
        "Taiki Takemuro",
        "Victor Buendía",
        "Carla Morante",
        "Tomohiro Konno",
        "Shigeo Sato",
        "Ayumi Hirano-Iwata",
        "Viola Priesemann",
        "Miguel A. Muñoz",
        "Johannes Zierenberg",
        "Jordi Soriano"
      ],
      "published": "2022-05-21T11:05:01Z",
      "updated": "2022-05-21T11:05:01Z",
      "categories": [
        "q-bio.NC"
      ],
      "pdf_url": "https://arxiv.org/pdf/2205.10563v1",
      "landing_url": "https://arxiv.org/abs/2205.10563v1",
      "doi": "https://doi.org/10.1126/sciadv.ade1755"
    },
    "queries": [
      {
        "anchor": "spoken language models",
        "search_term": "modular architecture"
      },
      {
        "anchor": "synchronous dialogue",
        "search_term": "modular architecture"
      },
      {
        "anchor": "turn-taking",
        "search_term": "modular architecture"
      }
    ]
  },
  {
    "arxiv_id": "2205.13675",
    "anchor": "spoken language models",
    "search_term": "synchronous data",
    "search_record": {
      "id": "http://arxiv.org/abs/2205.13675v1",
      "title": "Reinforcement Learning Approach for Mapping Applications to Dataflow-Based Coarse-Grained Reconfigurable Array",
      "summary": "The Streaming Engine (SE) is a Coarse-Grained Reconfigurable Array which provides programming flexibility and high-performance with energy efficiency. An application program to be executed on the SE is represented as a combination of Synchronous Data Flow (SDF) graphs, where every instruction is represented as a node. Each node needs to be mapped to the right slot and array in the SE to ensure the correct execution of the program. This creates an optimization problem with a vast and sparse search space for which finding a mapping manually is impractical because it requires expertise and knowledge of the SE micro-architecture. In this work we propose a Reinforcement Learning framework with Global Graph Attention (GGA) module and output masking of invalid placements to find and optimize instruction schedules. We use Proximal Policy Optimization in order to train a model which places operations into the SE tiles based on a reward function that models the SE device and its constraints. The GGA module consists of a graph neural network and an attention module. The graph neural network creates embeddings of the SDFs and the attention block is used to model sequential operation placement. We show results on how certain workloads are mapped to the SE and the factors affecting mapping quality. We find that the addition of GGA, on average, finds 10% better instruction schedules in terms of total clock cycles taken and masking improves reward obtained by 20%.",
      "published": "2022-05-26T23:36:21Z"
    },
    "metadata": {
      "arxiv_id": "2205.13675",
      "title": "Reinforcement Learning Approach for Mapping Applications to Dataflow-Based Coarse-Grained Reconfigurable Array",
      "summary": "The Streaming Engine (SE) is a Coarse-Grained Reconfigurable Array which provides programming flexibility and high-performance with energy efficiency. An application program to be executed on the SE is represented as a combination of Synchronous Data Flow (SDF) graphs, where every instruction is represented as a node. Each node needs to be mapped to the right slot and array in the SE to ensure the correct execution of the program. This creates an optimization problem with a vast and sparse search space for which finding a mapping manually is impractical because it requires expertise and knowledge of the SE micro-architecture. In this work we propose a Reinforcement Learning framework with Global Graph Attention (GGA) module and output masking of invalid placements to find and optimize instruction schedules. We use Proximal Policy Optimization in order to train a model which places operations into the SE tiles based on a reward function that models the SE device and its constraints. The GGA module consists of a graph neural network and an attention module. The graph neural network creates embeddings of the SDFs and the attention block is used to model sequential operation placement. We show results on how certain workloads are mapped to the SE and the factors affecting mapping quality. We find that the addition of GGA, on average, finds 10% better instruction schedules in terms of total clock cycles taken and masking improves reward obtained by 20%.",
      "authors": [
        "Andre Xian Ming Chang",
        "Parth Khopkar",
        "Bashar Romanous",
        "Abhishek Chaurasia",
        "Patrick Estep",
        "Skyler Windh",
        "Doug Vanesko",
        "Sheik Dawood Beer Mohideen",
        "Eugenio Culurciello"
      ],
      "published": "2022-05-26T23:36:21Z",
      "updated": "2022-05-26T23:36:21Z",
      "categories": [
        "cs.AR",
        "cs.AI",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2205.13675v1",
      "landing_url": "https://arxiv.org/abs/2205.13675v1",
      "doi": "https://doi.org/10.48550/arXiv.2205.13675"
    },
    "queries": [
      {
        "anchor": "spoken language models",
        "search_term": "synchronous data"
      },
      {
        "anchor": "synchronous dialogue",
        "search_term": "synchronous data"
      }
    ]
  },
  {
    "arxiv_id": "2205.15060",
    "anchor": "turn-taking",
    "search_term": "true full-duplex",
    "search_record": {
      "id": "http://arxiv.org/abs/2205.15060v4",
      "title": "Duplex Conversation: Towards Human-like Interaction in Spoken Dialogue Systems",
      "summary": "In this paper, we present Duplex Conversation, a multi-turn, multimodal spoken dialogue system that enables telephone-based agents to interact with customers like a human. We use the concept of full-duplex in telecommunication to demonstrate what a human-like interactive experience should be and how to achieve smooth turn-taking through three subtasks: user state detection, backchannel selection, and barge-in detection. Besides, we propose semi-supervised learning with multimodal data augmentation to leverage unlabeled data to increase model generalization. Experimental results on three sub-tasks show that the proposed method achieves consistent improvements compared with baselines. We deploy the Duplex Conversation to Alibaba intelligent customer service and share lessons learned in production. Online A/B experiments show that the proposed system can significantly reduce response latency by 50%.",
      "published": "2022-05-30T12:41:23Z"
    },
    "metadata": {
      "arxiv_id": "2205.15060",
      "title": "Duplex Conversation: Towards Human-like Interaction in Spoken Dialogue Systems",
      "summary": "In this paper, we present Duplex Conversation, a multi-turn, multimodal spoken dialogue system that enables telephone-based agents to interact with customers like a human. We use the concept of full-duplex in telecommunication to demonstrate what a human-like interactive experience should be and how to achieve smooth turn-taking through three subtasks: user state detection, backchannel selection, and barge-in detection. Besides, we propose semi-supervised learning with multimodal data augmentation to leverage unlabeled data to increase model generalization. Experimental results on three sub-tasks show that the proposed method achieves consistent improvements compared with baselines. We deploy the Duplex Conversation to Alibaba intelligent customer service and share lessons learned in production. Online A/B experiments show that the proposed system can significantly reduce response latency by 50%.",
      "authors": [
        "Ting-En Lin",
        "Yuchuan Wu",
        "Fei Huang",
        "Luo Si",
        "Jian Sun",
        "Yongbin Li"
      ],
      "published": "2022-05-30T12:41:23Z",
      "updated": "2022-06-14T10:45:56Z",
      "categories": [
        "cs.CL"
      ],
      "pdf_url": "https://arxiv.org/pdf/2205.15060v4",
      "landing_url": "https://arxiv.org/abs/2205.15060v4",
      "doi": "https://doi.org/10.1145/3534678.3539209"
    },
    "queries": [
      {
        "anchor": "turn-taking",
        "search_term": "true full-duplex"
      },
      {
        "anchor": "turn-taking",
        "search_term": "pseudo full-duplex"
      },
      {
        "anchor": "turn-taking",
        "search_term": "engineered synchronization"
      },
      {
        "anchor": "turn-taking",
        "search_term": "learned synchronization"
      },
      {
        "anchor": "turn-taking",
        "search_term": "synchronous data"
      },
      {
        "anchor": "turn-taking",
        "search_term": "multichannel dialogue"
      },
      {
        "anchor": "turn-taking",
        "search_term": "behavioral arbitration"
      },
      {
        "anchor": "turn-taking",
        "search_term": "floor transfer offset"
      },
      {
        "anchor": "turn-taking",
        "search_term": "stop latency"
      },
      {
        "anchor": "turn-taking",
        "search_term": "interruption response delay"
      }
    ]
  },
  {
    "arxiv_id": "2206.00614",
    "anchor": "synchronous dialogue",
    "search_term": "streaming architecture",
    "search_record": {
      "id": "http://arxiv.org/abs/2206.00614v2",
      "title": "Dual-stream spatiotemporal networks with feature sharing for monitoring animals in the home cage",
      "summary": "This paper presents a spatiotemporal deep learning approach for mouse behavioural classification in the home-cage. Using a series of dual-stream architectures with assorted modifications to increase performance, we introduce a novel feature sharing approach that jointly processes the streams at regular intervals throughout the network. To investigate the efficacy of this approach, models were evaluated by dissociating the streams and training/testing in the same rigorous manner as the main classifiers. Using an annotated, publicly available dataset of a singly-housed mice, we achieve prediction accuracy of 86.47% using an ensemble of a Inception-based network and an attention-based network, both of which utilize this feature sharing. We also demonstrate through ablation studies that for all models, the feature-sharing architectures consistently perform better than conventional ones having separate streams. The best performing models were further evaluated on other activity datasets, both mouse and human. Future work will investigate the effectiveness of feature sharing to behavioural classification in the unsupervised anomaly detection domain.",
      "published": "2022-06-01T16:32:25Z"
    },
    "metadata": {
      "arxiv_id": "2206.00614",
      "title": "Dual-stream spatiotemporal networks with feature sharing for monitoring animals in the home cage",
      "summary": "This paper presents a spatiotemporal deep learning approach for mouse behavioural classification in the home-cage. Using a series of dual-stream architectures with assorted modifications to increase performance, we introduce a novel feature sharing approach that jointly processes the streams at regular intervals throughout the network. To investigate the efficacy of this approach, models were evaluated by dissociating the streams and training/testing in the same rigorous manner as the main classifiers. Using an annotated, publicly available dataset of a singly-housed mice, we achieve prediction accuracy of 86.47% using an ensemble of a Inception-based network and an attention-based network, both of which utilize this feature sharing. We also demonstrate through ablation studies that for all models, the feature-sharing architectures consistently perform better than conventional ones having separate streams. The best performing models were further evaluated on other activity datasets, both mouse and human. Future work will investigate the effectiveness of feature sharing to behavioural classification in the unsupervised anomaly detection domain.",
      "authors": [
        "Ezechukwu I. Nwokedi",
        "Rasneer S. Bains",
        "Luc Bidaut",
        "Xujiong Ye",
        "Sara Wells",
        "James M. Brown"
      ],
      "published": "2022-06-01T16:32:25Z",
      "updated": "2022-11-03T23:02:29Z",
      "categories": [
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2206.00614v2",
      "landing_url": "https://arxiv.org/abs/2206.00614v2",
      "doi": "https://doi.org/10.48550/arXiv.2206.00614"
    },
    "queries": [
      {
        "anchor": "synchronous dialogue",
        "search_term": "streaming architecture"
      }
    ]
  },
  {
    "arxiv_id": "2206.00636",
    "anchor": "spoken language models",
    "search_term": "modular architecture",
    "search_record": {
      "id": "http://arxiv.org/abs/2206.00636v1",
      "title": "A modular architecture for creating multimodal agents",
      "summary": "The paper describes a flexible and modular platform to create multimodal interactive agents. The platform operates through an event-bus on which signals and interpretations are posted in a sequence in time. Different sensors and interpretation components can be integrated by defining their input and output as topics, which results in a logical workflow for further interpretations. We explain a broad range of components that have been developed so far and integrated into a range of interactive agents. We also explain how the actual interaction is recorded as multimodal data as well as in a so-called episodic Knowledge Graph. By analysing the recorded interaction, we can analyse and compare different agents and agent components.",
      "published": "2022-06-01T17:12:10Z"
    },
    "metadata": {
      "arxiv_id": "2206.00636",
      "title": "A modular architecture for creating multimodal agents",
      "summary": "The paper describes a flexible and modular platform to create multimodal interactive agents. The platform operates through an event-bus on which signals and interpretations are posted in a sequence in time. Different sensors and interpretation components can be integrated by defining their input and output as topics, which results in a logical workflow for further interpretations. We explain a broad range of components that have been developed so far and integrated into a range of interactive agents. We also explain how the actual interaction is recorded as multimodal data as well as in a so-called episodic Knowledge Graph. By analysing the recorded interaction, we can analyse and compare different agents and agent components.",
      "authors": [
        "Thomas Baier",
        "Selene Baez Santamaria",
        "Piek Vossen"
      ],
      "published": "2022-06-01T17:12:10Z",
      "updated": "2022-06-01T17:12:10Z",
      "categories": [
        "cs.AI",
        "cs.HC",
        "cs.RO"
      ],
      "pdf_url": "https://arxiv.org/pdf/2206.00636v1",
      "landing_url": "https://arxiv.org/abs/2206.00636v1",
      "doi": "https://doi.org/10.48550/arXiv.2206.00636"
    },
    "queries": [
      {
        "anchor": "spoken language models",
        "search_term": "modular architecture"
      },
      {
        "anchor": "synchronous dialogue",
        "search_term": "modular architecture"
      },
      {
        "anchor": "turn-taking",
        "search_term": "modular architecture"
      }
    ]
  },
  {
    "arxiv_id": "2206.02713",
    "anchor": "spoken language models",
    "search_term": "modular architecture",
    "search_record": {
      "id": "http://arxiv.org/abs/2206.02713v1",
      "title": "Is a Modular Architecture Enough?",
      "summary": "Inspired from human cognition, machine learning systems are gradually revealing advantages of sparser and more modular architectures. Recent work demonstrates that not only do some modular architectures generalize well, but they also lead to better out-of-distribution generalization, scaling properties, learning speed, and interpretability. A key intuition behind the success of such systems is that the data generating system for most real-world settings is considered to consist of sparsely interacting parts, and endowing models with similar inductive biases will be helpful. However, the field has been lacking in a rigorous quantitative assessment of such systems because these real-world data distributions are complex and unknown. In this work, we provide a thorough assessment of common modular architectures, through the lens of simple and known modular data distributions. We highlight the benefits of modularity and sparsity and reveal insights on the challenges faced while optimizing modular systems. In doing so, we propose evaluation metrics that highlight the benefits of modularity, the regimes in which these benefits are substantial, as well as the sub-optimality of current end-to-end learned modular systems as opposed to their claimed potential.",
      "published": "2022-06-06T16:12:06Z"
    },
    "metadata": {
      "arxiv_id": "2206.02713",
      "title": "Is a Modular Architecture Enough?",
      "summary": "Inspired from human cognition, machine learning systems are gradually revealing advantages of sparser and more modular architectures. Recent work demonstrates that not only do some modular architectures generalize well, but they also lead to better out-of-distribution generalization, scaling properties, learning speed, and interpretability. A key intuition behind the success of such systems is that the data generating system for most real-world settings is considered to consist of sparsely interacting parts, and endowing models with similar inductive biases will be helpful. However, the field has been lacking in a rigorous quantitative assessment of such systems because these real-world data distributions are complex and unknown. In this work, we provide a thorough assessment of common modular architectures, through the lens of simple and known modular data distributions. We highlight the benefits of modularity and sparsity and reveal insights on the challenges faced while optimizing modular systems. In doing so, we propose evaluation metrics that highlight the benefits of modularity, the regimes in which these benefits are substantial, as well as the sub-optimality of current end-to-end learned modular systems as opposed to their claimed potential.",
      "authors": [
        "Sarthak Mittal",
        "Yoshua Bengio",
        "Guillaume Lajoie"
      ],
      "published": "2022-06-06T16:12:06Z",
      "updated": "2022-06-06T16:12:06Z",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2206.02713v1",
      "landing_url": "https://arxiv.org/abs/2206.02713v1",
      "doi": "https://doi.org/10.48550/arXiv.2206.02713"
    },
    "queries": [
      {
        "anchor": "spoken language models",
        "search_term": "modular architecture"
      },
      {
        "anchor": "synchronous dialogue",
        "search_term": "modular architecture"
      },
      {
        "anchor": "turn-taking",
        "search_term": "modular architecture"
      }
    ]
  },
  {
    "arxiv_id": "2206.03113",
    "anchor": "synchronous dialogue",
    "search_term": "semantic coherence",
    "search_record": {
      "id": "http://arxiv.org/abs/2206.03113v2",
      "title": "Wavelet Prior Attention Learning in Axial Inpainting Network",
      "summary": "Image inpainting is the task of filling masked or unknown regions of an image with visually realistic contents, which has been remarkably improved by Deep Neural Networks (DNNs) recently. Essentially, as an inverse problem, the inpainting has the underlying challenges of reconstructing semantically coherent results without texture artifacts. Many previous efforts have been made via exploiting attention mechanisms and prior knowledge, such as edges and semantic segmentation. However, these works are still limited in practice by an avalanche of learnable prior parameters and prohibitive computational burden. To this end, we propose a novel model -- Wavelet prior attention learning in Axial Inpainting Network (WAIN), whose generator contains the encoder, decoder, as well as two key components of Wavelet image Prior Attention (WPA) and stacked multi-layer Axial-Transformers (ATs). Particularly, the WPA guides the high-level feature aggregation in the multi-scale frequency domain, alleviating the textual artifacts. Stacked ATs employ unmasked clues to help model reasonable features along with low-level features of horizontal and vertical axes, improving the semantic coherence. Extensive quantitative and qualitative experiments on Celeba-HQ and Places2 datasets are conducted to validate that our WAIN can achieve state-of-the-art performance over the competitors. The codes and models will be released.",
      "published": "2022-06-07T08:45:27Z"
    },
    "metadata": {
      "arxiv_id": "2206.03113",
      "title": "Wavelet Prior Attention Learning in Axial Inpainting Network",
      "summary": "Image inpainting is the task of filling masked or unknown regions of an image with visually realistic contents, which has been remarkably improved by Deep Neural Networks (DNNs) recently. Essentially, as an inverse problem, the inpainting has the underlying challenges of reconstructing semantically coherent results without texture artifacts. Many previous efforts have been made via exploiting attention mechanisms and prior knowledge, such as edges and semantic segmentation. However, these works are still limited in practice by an avalanche of learnable prior parameters and prohibitive computational burden. To this end, we propose a novel model -- Wavelet prior attention learning in Axial Inpainting Network (WAIN), whose generator contains the encoder, decoder, as well as two key components of Wavelet image Prior Attention (WPA) and stacked multi-layer Axial-Transformers (ATs). Particularly, the WPA guides the high-level feature aggregation in the multi-scale frequency domain, alleviating the textual artifacts. Stacked ATs employ unmasked clues to help model reasonable features along with low-level features of horizontal and vertical axes, improving the semantic coherence. Extensive quantitative and qualitative experiments on Celeba-HQ and Places2 datasets are conducted to validate that our WAIN can achieve state-of-the-art performance over the competitors. The codes and models will be released.",
      "authors": [
        "Chenjie Cao",
        "Chengrong Wang",
        "Yuntao Zhang",
        "Yanwei Fu"
      ],
      "published": "2022-06-07T08:45:27Z",
      "updated": "2022-06-14T06:54:56Z",
      "categories": [
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2206.03113v2",
      "landing_url": "https://arxiv.org/abs/2206.03113v2",
      "doi": "https://doi.org/10.48550/arXiv.2206.03113"
    },
    "queries": [
      {
        "anchor": "synchronous dialogue",
        "search_term": "semantic coherence"
      }
    ]
  },
  {
    "arxiv_id": "2206.03939",
    "anchor": "synchronous dialogue",
    "search_term": "streaming architecture",
    "search_record": {
      "id": "http://arxiv.org/abs/2206.03939v1",
      "title": "Depth-Adapted CNNs for RGB-D Semantic Segmentation",
      "summary": "Recent RGB-D semantic segmentation has motivated research interest thanks to the accessibility of complementary modalities from the input side. Existing works often adopt a two-stream architecture that processes photometric and geometric information in parallel, with few methods explicitly leveraging the contribution of depth cues to adjust the sampling position on RGB images. In this paper, we propose a novel framework to incorporate the depth information in the RGB convolutional neural network (CNN), termed Z-ACN (Depth-Adapted CNN). Specifically, our Z-ACN generates a 2D depth-adapted offset which is fully constrained by low-level features to guide the feature extraction on RGB images. With the generated offset, we introduce two intuitive and effective operations to replace basic CNN operators: depth-adapted convolution and depth-adapted average pooling. Extensive experiments on both indoor and outdoor semantic segmentation tasks demonstrate the effectiveness of our approach.",
      "published": "2022-06-08T14:59:40Z"
    },
    "metadata": {
      "arxiv_id": "2206.03939",
      "title": "Depth-Adapted CNNs for RGB-D Semantic Segmentation",
      "summary": "Recent RGB-D semantic segmentation has motivated research interest thanks to the accessibility of complementary modalities from the input side. Existing works often adopt a two-stream architecture that processes photometric and geometric information in parallel, with few methods explicitly leveraging the contribution of depth cues to adjust the sampling position on RGB images. In this paper, we propose a novel framework to incorporate the depth information in the RGB convolutional neural network (CNN), termed Z-ACN (Depth-Adapted CNN). Specifically, our Z-ACN generates a 2D depth-adapted offset which is fully constrained by low-level features to guide the feature extraction on RGB images. With the generated offset, we introduce two intuitive and effective operations to replace basic CNN operators: depth-adapted convolution and depth-adapted average pooling. Extensive experiments on both indoor and outdoor semantic segmentation tasks demonstrate the effectiveness of our approach.",
      "authors": [
        "Zongwei Wu",
        "Guillaume Allibert",
        "Christophe Stolz",
        "Chao Ma",
        "Cédric Demonceaux"
      ],
      "published": "2022-06-08T14:59:40Z",
      "updated": "2022-06-08T14:59:40Z",
      "categories": [
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2206.03939v1",
      "landing_url": "https://arxiv.org/abs/2206.03939v1",
      "doi": "https://doi.org/10.48550/arXiv.2206.03939"
    },
    "queries": [
      {
        "anchor": "synchronous dialogue",
        "search_term": "streaming architecture"
      }
    ]
  },
  {
    "arxiv_id": "2206.06978",
    "anchor": "spoken language models",
    "search_term": "low latency",
    "search_record": {
      "id": "http://arxiv.org/abs/2206.06978v1",
      "title": "Low-Latency MAC Design for Pairwise Random Networks",
      "summary": "Feasibility of using unlicensed spectrum for ultra reliable low latency communications (URLLC) is still a question for beyond 5G wireless networks. Low latency access to the channel and efficiently sharing spectrum among the multiple users are the main requirements for exploiting unlicensed spectrum for URLLC. Listen before talk and back-off procedures implemented to avoid the collisions in channel access hinder the low latency communication. In this paper, we propose a novel low-latency medium access control (MAC) scheme based on the collision resolution for a pairwise random wireless network. We use geometric sequence decomposition for collision resolution among the competing users. This enables the system to tackle collisions and thus removing the need for carrier sensing and back-off procedures. This saves time in obtaining access to the channel and improves the efficiency of the system. We implement our approach in the synchronized time slotted system and show that it yields significant improvement over existing MAC schemes.",
      "published": "2022-05-22T12:31:33Z"
    },
    "metadata": {
      "arxiv_id": "2206.06978",
      "title": "Low-Latency MAC Design for Pairwise Random Networks",
      "summary": "Feasibility of using unlicensed spectrum for ultra reliable low latency communications (URLLC) is still a question for beyond 5G wireless networks. Low latency access to the channel and efficiently sharing spectrum among the multiple users are the main requirements for exploiting unlicensed spectrum for URLLC. Listen before talk and back-off procedures implemented to avoid the collisions in channel access hinder the low latency communication. In this paper, we propose a novel low-latency medium access control (MAC) scheme based on the collision resolution for a pairwise random wireless network. We use geometric sequence decomposition for collision resolution among the competing users. This enables the system to tackle collisions and thus removing the need for carrier sensing and back-off procedures. This saves time in obtaining access to the channel and improves the efficiency of the system. We implement our approach in the synchronized time slotted system and show that it yields significant improvement over existing MAC schemes.",
      "authors": [
        "Irshad A. Meer",
        "Woong-Hee Lee",
        "Mustafa Ozger",
        "Cicek Cavdar",
        "Ki Won Sung"
      ],
      "published": "2022-05-22T12:31:33Z",
      "updated": "2022-05-22T12:31:33Z",
      "categories": [
        "cs.IT",
        "eess.SY"
      ],
      "pdf_url": "https://arxiv.org/pdf/2206.06978v1",
      "landing_url": "https://arxiv.org/abs/2206.06978v1",
      "doi": "https://doi.org/10.48550/arXiv.2206.06978"
    },
    "queries": [
      {
        "anchor": "spoken language models",
        "search_term": "low latency"
      },
      {
        "anchor": "synchronous dialogue",
        "search_term": "low latency"
      },
      {
        "anchor": "turn-taking",
        "search_term": "low latency"
      }
    ]
  },
  {
    "arxiv_id": "2206.09457",
    "anchor": "synchronous dialogue",
    "search_term": "modular architecture",
    "search_record": {
      "id": "http://arxiv.org/abs/2206.09457v2",
      "title": "All you need is feedback: Communication with block attention feedback codes",
      "summary": "Deep learning based channel code designs have recently gained interest as an alternative to conventional coding algorithms, particularly for channels for which existing codes do not provide effective solutions. Communication over a feedback channel is one such problem, for which promising results have recently been obtained by employing various deep learning architectures. In this paper, we introduce a novel learning-aided code design for feedback channels, called generalized block attention feedback (GBAF) codes, which i) employs a modular architecture that can be implemented using different neural network architectures; ii) provides order-of-magnitude improvements in the probability of error compared to existing designs; and iii) can transmit at desired code rates.",
      "published": "2022-06-19T17:55:04Z"
    },
    "metadata": {
      "arxiv_id": "2206.09457",
      "title": "All you need is feedback: Communication with block attention feedback codes",
      "summary": "Deep learning based channel code designs have recently gained interest as an alternative to conventional coding algorithms, particularly for channels for which existing codes do not provide effective solutions. Communication over a feedback channel is one such problem, for which promising results have recently been obtained by employing various deep learning architectures. In this paper, we introduce a novel learning-aided code design for feedback channels, called generalized block attention feedback (GBAF) codes, which i) employs a modular architecture that can be implemented using different neural network architectures; ii) provides order-of-magnitude improvements in the probability of error compared to existing designs; and iii) can transmit at desired code rates.",
      "authors": [
        "Emre Ozfatura",
        "Yulin Shao",
        "Alberto Perotti",
        "Branislav Popovic",
        "Deniz Gunduz"
      ],
      "published": "2022-06-19T17:55:04Z",
      "updated": "2022-10-05T16:13:17Z",
      "categories": [
        "cs.IT",
        "cs.AI",
        "cs.LG",
        "eess.SP"
      ],
      "pdf_url": "https://arxiv.org/pdf/2206.09457v2",
      "landing_url": "https://arxiv.org/abs/2206.09457v2",
      "doi": "https://doi.org/10.48550/arXiv.2206.09457"
    },
    "queries": [
      {
        "anchor": "synchronous dialogue",
        "search_term": "modular architecture"
      }
    ]
  },
  {
    "arxiv_id": "2206.09818",
    "anchor": "spoken language models",
    "search_term": "data scarcity",
    "search_record": {
      "id": "http://arxiv.org/abs/2206.09818v3",
      "title": "SSM-DTA: Breaking the Barriers of Data Scarcity in Drug-Target Affinity Prediction",
      "summary": "Accurate prediction of Drug-Target Affinity (DTA) is of vital importance in early-stage drug discovery, facilitating the identification of drugs that can effectively interact with specific targets and regulate their activities. While wet experiments remain the most reliable method, they are time-consuming and resource-intensive, resulting in limited data availability that poses challenges for deep learning approaches. Existing methods have primarily focused on developing techniques based on the available DTA data, without adequately addressing the data scarcity issue. To overcome this challenge, we present the SSM-DTA framework, which incorporates three simple yet highly effective strategies: (1) A multi-task training approach that combines DTA prediction with masked language modeling (MLM) using paired drug-target data. (2) A semi-supervised training method that leverages large-scale unpaired molecules and proteins to enhance drug and target representations. This approach differs from previous methods that only employed molecules or proteins in pre-training. (3) The integration of a lightweight cross-attention module to improve the interaction between drugs and targets, further enhancing prediction accuracy. Through extensive experiments on benchmark datasets such as BindingDB, DAVIS, and KIBA, we demonstrate the superior performance of our framework. Additionally, we conduct case studies on specific drug-target binding activities, virtual screening experiments, drug feature visualizations, and real-world applications, all of which showcase the significant potential of our work. In conclusion, our proposed SSM-DTA framework addresses the data limitation challenge in DTA prediction and yields promising results, paving the way for more efficient and accurate drug discovery processes. Our code is available at $\\href{https://github.com/QizhiPei/SSM-DTA}{Github}$.",
      "published": "2022-06-20T14:53:25Z"
    },
    "metadata": {
      "arxiv_id": "2206.09818",
      "title": "SSM-DTA: Breaking the Barriers of Data Scarcity in Drug-Target Affinity Prediction",
      "summary": "Accurate prediction of Drug-Target Affinity (DTA) is of vital importance in early-stage drug discovery, facilitating the identification of drugs that can effectively interact with specific targets and regulate their activities. While wet experiments remain the most reliable method, they are time-consuming and resource-intensive, resulting in limited data availability that poses challenges for deep learning approaches. Existing methods have primarily focused on developing techniques based on the available DTA data, without adequately addressing the data scarcity issue. To overcome this challenge, we present the SSM-DTA framework, which incorporates three simple yet highly effective strategies: (1) A multi-task training approach that combines DTA prediction with masked language modeling (MLM) using paired drug-target data. (2) A semi-supervised training method that leverages large-scale unpaired molecules and proteins to enhance drug and target representations. This approach differs from previous methods that only employed molecules or proteins in pre-training. (3) The integration of a lightweight cross-attention module to improve the interaction between drugs and targets, further enhancing prediction accuracy. Through extensive experiments on benchmark datasets such as BindingDB, DAVIS, and KIBA, we demonstrate the superior performance of our framework. Additionally, we conduct case studies on specific drug-target binding activities, virtual screening experiments, drug feature visualizations, and real-world applications, all of which showcase the significant potential of our work. In conclusion, our proposed SSM-DTA framework addresses the data limitation challenge in DTA prediction and yields promising results, paving the way for more efficient and accurate drug discovery processes. Our code is available at $\\href{https://github.com/QizhiPei/SSM-DTA}{Github}$.",
      "authors": [
        "Qizhi Pei",
        "Lijun Wu",
        "Jinhua Zhu",
        "Yingce Xia",
        "Shufang Xie",
        "Tao Qin",
        "Haiguang Liu",
        "Tie-Yan Liu",
        "Rui Yan"
      ],
      "published": "2022-06-20T14:53:25Z",
      "updated": "2023-10-17T14:06:07Z",
      "categories": [
        "q-bio.BM",
        "cs.AI",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2206.09818v3",
      "landing_url": "https://arxiv.org/abs/2206.09818v3",
      "doi": "https://doi.org/10.48550/arXiv.2206.09818"
    },
    "queries": [
      {
        "anchor": "spoken language models",
        "search_term": "data scarcity"
      },
      {
        "anchor": "synchronous dialogue",
        "search_term": "data scarcity"
      },
      {
        "anchor": "turn-taking",
        "search_term": "data scarcity"
      }
    ]
  },
  {
    "arxiv_id": "2206.11307",
    "anchor": "synchronous dialogue",
    "search_term": "modular architecture",
    "search_record": {
      "id": "http://arxiv.org/abs/2206.11307v2",
      "title": "Modular architectures to deterministically generate graph states",
      "summary": "Graph states are a family of stabilizer states which can be tailored towards various applications in photonic quantum computing and quantum communication. In this paper, we present a modular design based on quantum dot emitters coupled to a waveguide and optical fiber delay lines to deterministically generate N-dimensional cluster states and other useful graph states such as tree states and repeater states. Unlike previous proposals, our design requires no two-qubit gates on quantum dots and at most one optical switch, thereby, minimizing challenges usually posed by these requirements. Furthermore, we discuss the error model for our design and demonstrate a fault-tolerant quantum memory with an error threshold of 0.53% in the case of a 3d graph state on a Raussendorf-Harrington-Goyal (RHG) lattice. We also provide a fundamental upper bound on the correctable loss in the fault-tolerant RHG state based on the percolation theory, which is 1.24 dB or 0.24 dB depending on whether the state is directly generated or obtained from a simple cubic cluster state, respectively.",
      "published": "2022-06-22T18:17:38Z"
    },
    "metadata": {
      "arxiv_id": "2206.11307",
      "title": "Modular architectures to deterministically generate graph states",
      "summary": "Graph states are a family of stabilizer states which can be tailored towards various applications in photonic quantum computing and quantum communication. In this paper, we present a modular design based on quantum dot emitters coupled to a waveguide and optical fiber delay lines to deterministically generate N-dimensional cluster states and other useful graph states such as tree states and repeater states. Unlike previous proposals, our design requires no two-qubit gates on quantum dots and at most one optical switch, thereby, minimizing challenges usually posed by these requirements. Furthermore, we discuss the error model for our design and demonstrate a fault-tolerant quantum memory with an error threshold of 0.53% in the case of a 3d graph state on a Raussendorf-Harrington-Goyal (RHG) lattice. We also provide a fundamental upper bound on the correctable loss in the fault-tolerant RHG state based on the percolation theory, which is 1.24 dB or 0.24 dB depending on whether the state is directly generated or obtained from a simple cubic cluster state, respectively.",
      "authors": [
        "Hassan Shapourian",
        "Alireza Shabani"
      ],
      "published": "2022-06-22T18:17:38Z",
      "updated": "2023-02-25T04:21:44Z",
      "categories": [
        "quant-ph"
      ],
      "pdf_url": "https://arxiv.org/pdf/2206.11307v2",
      "landing_url": "https://arxiv.org/abs/2206.11307v2",
      "doi": "https://doi.org/10.22331/q-2023-03-02-935"
    },
    "queries": [
      {
        "anchor": "synchronous dialogue",
        "search_term": "modular architecture"
      }
    ]
  },
  {
    "arxiv_id": "2206.11418",
    "anchor": "full-duplex",
    "search_term": "true full-duplex",
    "search_record": {
      "id": "http://arxiv.org/abs/2206.11418v1",
      "title": "LoneSTAR: Analog Beamforming Codebooks for Full-Duplex Millimeter Wave Systems",
      "summary": "This work develops LoneSTAR, a novel enabler of full-duplex millimeter wave (mmWave) communication systems through the design of analog beamforming codebooks. LoneSTAR codebooks deliver high beamforming gain and broad coverage while simultaneously reducing the self-interference coupled by transmit and receive beams at a full-duplex mmWave transceiver. Our design framework accomplishes this by tolerating some variability in transmit and receive beamforming gain to strategically shape beams that reject self-interference spatially while accounting for digitally-controlled analog beamforming networks and self-interference channel estimation error. By leveraging the coherence time of the self-interference channel, a mmWave system can use the same LoneSTAR design over many time slots to serve several downlink-uplink user pairs in a full-duplex fashion without the need for additional self-interference cancellation. Compared to those using conventional codebooks, full-duplex mmWave systems employing LoneSTAR codebooks can mitigate higher levels of self-interference, tolerate more cross-link interference, and demand lower SNRs in order to outperform half-duplex operation -- all while supporting beam alignment. This makes LoneSTAR a potential standalone solution for enabling simultaneous transmission and reception in mmWave systems, from which it derives its name.",
      "published": "2022-06-22T23:22:21Z"
    },
    "metadata": {
      "arxiv_id": "2206.11418",
      "title": "LoneSTAR: Analog Beamforming Codebooks for Full-Duplex Millimeter Wave Systems",
      "summary": "This work develops LoneSTAR, a novel enabler of full-duplex millimeter wave (mmWave) communication systems through the design of analog beamforming codebooks. LoneSTAR codebooks deliver high beamforming gain and broad coverage while simultaneously reducing the self-interference coupled by transmit and receive beams at a full-duplex mmWave transceiver. Our design framework accomplishes this by tolerating some variability in transmit and receive beamforming gain to strategically shape beams that reject self-interference spatially while accounting for digitally-controlled analog beamforming networks and self-interference channel estimation error. By leveraging the coherence time of the self-interference channel, a mmWave system can use the same LoneSTAR design over many time slots to serve several downlink-uplink user pairs in a full-duplex fashion without the need for additional self-interference cancellation. Compared to those using conventional codebooks, full-duplex mmWave systems employing LoneSTAR codebooks can mitigate higher levels of self-interference, tolerate more cross-link interference, and demand lower SNRs in order to outperform half-duplex operation -- all while supporting beam alignment. This makes LoneSTAR a potential standalone solution for enabling simultaneous transmission and reception in mmWave systems, from which it derives its name.",
      "authors": [
        "Ian P. Roberts",
        "Sriram Vishwanath",
        "Jeffrey G. Andrews"
      ],
      "published": "2022-06-22T23:22:21Z",
      "updated": "2022-06-22T23:22:21Z",
      "categories": [
        "eess.SP"
      ],
      "pdf_url": "https://arxiv.org/pdf/2206.11418v1",
      "landing_url": "https://arxiv.org/abs/2206.11418v1",
      "doi": "https://doi.org/10.48550/arXiv.2206.11418"
    },
    "queries": [
      {
        "anchor": "full-duplex",
        "search_term": "true full-duplex"
      },
      {
        "anchor": "full-duplex",
        "search_term": "pseudo full-duplex"
      },
      {
        "anchor": "full-duplex",
        "search_term": "half duplex"
      },
      {
        "anchor": "full-duplex",
        "search_term": "engineered synchronization"
      },
      {
        "anchor": "full-duplex",
        "search_term": "learned synchronization"
      },
      {
        "anchor": "full-duplex",
        "search_term": "modular architecture"
      },
      {
        "anchor": "full-duplex",
        "search_term": "streaming architecture"
      },
      {
        "anchor": "full-duplex",
        "search_term": "real-time interaction"
      },
      {
        "anchor": "full-duplex",
        "search_term": "low latency"
      },
      {
        "anchor": "full-duplex",
        "search_term": "synchronous data"
      },
      {
        "anchor": "full-duplex",
        "search_term": "data scarcity"
      },
      {
        "anchor": "full-duplex",
        "search_term": "multichannel dialogue"
      },
      {
        "anchor": "full-duplex",
        "search_term": "temporal dynamics"
      },
      {
        "anchor": "full-duplex",
        "search_term": "behavioral arbitration"
      },
      {
        "anchor": "full-duplex",
        "search_term": "semantic coherence"
      },
      {
        "anchor": "full-duplex",
        "search_term": "neural codec"
      },
      {
        "anchor": "full-duplex",
        "search_term": "discrete tokens"
      },
      {
        "anchor": "full-duplex",
        "search_term": "audio tokens"
      },
      {
        "anchor": "full-duplex",
        "search_term": "floor transfer offset"
      },
      {
        "anchor": "full-duplex",
        "search_term": "stop latency"
      },
      {
        "anchor": "full-duplex",
        "search_term": "interruption response delay"
      }
    ]
  },
  {
    "arxiv_id": "2207.01893",
    "anchor": "spoken language models",
    "search_term": "true full-duplex",
    "search_record": {
      "id": "http://arxiv.org/abs/2207.01893v1",
      "title": "ASR-Generated Text for Language Model Pre-training Applied to Speech Tasks",
      "summary": "We aim at improving spoken language modeling (LM) using very large amount of automatically transcribed speech. We leverage the INA (French National Audiovisual Institute) collection and obtain 19GB of text after applying ASR on 350,000 hours of diverse TV shows. From this, spoken language models are trained either by fine-tuning an existing LM (FlauBERT) or through training a LM from scratch. New models (FlauBERT-Oral) are shared with the community and evaluated for 3 downstream tasks: spoken language understanding, classification of TV shows and speech syntactic parsing. Results show that FlauBERT-Oral can be beneficial compared to its initial FlauBERT version demonstrating that, despite its inherent noisy nature, ASR-generated text can be used to build spoken language models.",
      "published": "2022-07-05T08:47:51Z"
    },
    "metadata": {
      "arxiv_id": "2207.01893",
      "title": "ASR-Generated Text for Language Model Pre-training Applied to Speech Tasks",
      "summary": "We aim at improving spoken language modeling (LM) using very large amount of automatically transcribed speech. We leverage the INA (French National Audiovisual Institute) collection and obtain 19GB of text after applying ASR on 350,000 hours of diverse TV shows. From this, spoken language models are trained either by fine-tuning an existing LM (FlauBERT) or through training a LM from scratch. New models (FlauBERT-Oral) are shared with the community and evaluated for 3 downstream tasks: spoken language understanding, classification of TV shows and speech syntactic parsing. Results show that FlauBERT-Oral can be beneficial compared to its initial FlauBERT version demonstrating that, despite its inherent noisy nature, ASR-generated text can be used to build spoken language models.",
      "authors": [
        "Valentin Pelloin",
        "Franck Dary",
        "Nicolas Herve",
        "Benoit Favre",
        "Nathalie Camelin",
        "Antoine Laurent",
        "Laurent Besacier"
      ],
      "published": "2022-07-05T08:47:51Z",
      "updated": "2022-07-05T08:47:51Z",
      "categories": [
        "cs.CL"
      ],
      "pdf_url": "https://arxiv.org/pdf/2207.01893v1",
      "landing_url": "https://arxiv.org/abs/2207.01893v1",
      "doi": "https://doi.org/10.48550/arXiv.2207.01893"
    },
    "queries": [
      {
        "anchor": "spoken language models",
        "search_term": "true full-duplex"
      },
      {
        "anchor": "spoken language models",
        "search_term": "pseudo full-duplex"
      },
      {
        "anchor": "spoken language models",
        "search_term": "engineered synchronization"
      },
      {
        "anchor": "spoken language models",
        "search_term": "learned synchronization"
      },
      {
        "anchor": "spoken language models",
        "search_term": "modular architecture"
      },
      {
        "anchor": "spoken language models",
        "search_term": "streaming architecture"
      },
      {
        "anchor": "spoken language models",
        "search_term": "real-time interaction"
      },
      {
        "anchor": "spoken language models",
        "search_term": "synchronous data"
      },
      {
        "anchor": "spoken language models",
        "search_term": "data scarcity"
      },
      {
        "anchor": "spoken language models",
        "search_term": "multichannel dialogue"
      },
      {
        "anchor": "spoken language models",
        "search_term": "behavioral arbitration"
      },
      {
        "anchor": "spoken language models",
        "search_term": "semantic coherence"
      },
      {
        "anchor": "spoken language models",
        "search_term": "neural codec"
      },
      {
        "anchor": "spoken language models",
        "search_term": "discrete tokens"
      },
      {
        "anchor": "spoken language models",
        "search_term": "audio tokens"
      },
      {
        "anchor": "spoken language models",
        "search_term": "floor transfer offset"
      },
      {
        "anchor": "spoken language models",
        "search_term": "stop latency"
      },
      {
        "anchor": "spoken language models",
        "search_term": "interruption response delay"
      }
    ]
  },
  {
    "arxiv_id": "2207.02209",
    "anchor": "spoken language models",
    "search_term": "data scarcity",
    "search_record": {
      "id": "http://arxiv.org/abs/2207.02209v2",
      "title": "Tackling Data Scarcity with Transfer Learning: A Case Study of Thickness Characterization from Optical Spectra of Perovskite Thin Films",
      "summary": "Transfer learning increasingly becomes an important tool in handling data scarcity often encountered in machine learning. In the application of high-throughput thickness as a downstream process of the high-throughput optimization of optoelectronic thin films with autonomous workflows, data scarcity occurs especially for new materials. To achieve high-throughput thickness characterization, we propose a machine learning model called thicknessML that predicts thickness from UV-Vis spectrophotometry input and an overarching transfer learning workflow. We demonstrate the transfer learning workflow from generic source domain of generic band-gapped materials to specific target domain of perovskite materials, where the target domain data only come from limited number (18) of refractive indices from literature. The target domain can be easily extended to other material classes with a few literature data. Defining thickness prediction accuracy to be within-10% deviation, thicknessML achieves 92.2% (with a deviation of 3.6%) accuracy with transfer learning compared to 81.8% (with a deviation of 3.6%) 11.7% without (lower mean and larger standard deviation). Experimental validation on six deposited perovskite films also corroborates the efficacy of the proposed workflow by yielding a 10.5% mean absolute percentage error (MAPE).",
      "published": "2022-06-14T16:26:15Z"
    },
    "metadata": {
      "arxiv_id": "2207.02209",
      "title": "Tackling Data Scarcity with Transfer Learning: A Case Study of Thickness Characterization from Optical Spectra of Perovskite Thin Films",
      "summary": "Transfer learning increasingly becomes an important tool in handling data scarcity often encountered in machine learning. In the application of high-throughput thickness as a downstream process of the high-throughput optimization of optoelectronic thin films with autonomous workflows, data scarcity occurs especially for new materials. To achieve high-throughput thickness characterization, we propose a machine learning model called thicknessML that predicts thickness from UV-Vis spectrophotometry input and an overarching transfer learning workflow. We demonstrate the transfer learning workflow from generic source domain of generic band-gapped materials to specific target domain of perovskite materials, where the target domain data only come from limited number (18) of refractive indices from literature. The target domain can be easily extended to other material classes with a few literature data. Defining thickness prediction accuracy to be within-10% deviation, thicknessML achieves 92.2% (with a deviation of 3.6%) accuracy with transfer learning compared to 81.8% (with a deviation of 3.6%) 11.7% without (lower mean and larger standard deviation). Experimental validation on six deposited perovskite films also corroborates the efficacy of the proposed workflow by yielding a 10.5% mean absolute percentage error (MAPE).",
      "authors": [
        "Siyu Isaac Parker Tian",
        "Zekun Ren",
        "Selvaraj Venkataraj",
        "Yuanhang Cheng",
        "Daniil Bash",
        "Felipe Oviedo",
        "J. Senthilnath",
        "Vijila Chellappan",
        "Yee-Fun Lim",
        "Armin G. Aberle",
        "Benjamin P MacLeod",
        "Fraser G. L. Parlane",
        "Curtis P. Berlinguette",
        "Qianxiao Li",
        "Tonio Buonassisi",
        "Zhe Liu"
      ],
      "published": "2022-06-14T16:26:15Z",
      "updated": "2022-12-20T08:51:48Z",
      "categories": [
        "cs.LG",
        "cond-mat.mtrl-sci",
        "eess.IV",
        "physics.optics"
      ],
      "pdf_url": "https://arxiv.org/pdf/2207.02209v2",
      "landing_url": "https://arxiv.org/abs/2207.02209v2",
      "doi": "https://doi.org/10.48550/arXiv.2207.02209"
    },
    "queries": [
      {
        "anchor": "spoken language models",
        "search_term": "data scarcity"
      },
      {
        "anchor": "synchronous dialogue",
        "search_term": "data scarcity"
      },
      {
        "anchor": "turn-taking",
        "search_term": "data scarcity"
      }
    ]
  },
  {
    "arxiv_id": "2207.03783",
    "anchor": "synchronous dialogue",
    "search_term": "modular architecture",
    "search_record": {
      "id": "http://arxiv.org/abs/2207.03783v1",
      "title": "Gestural and Touchscreen Interaction for Human-Robot Collaboration: a Comparative Study",
      "summary": "Close human-robot interaction (HRI), especially in industrial scenarios, has been vastly investigated for the advantages of combining human and robot skills. For an effective HRI, the validity of currently available human-machine communication media or tools should be questioned, and new communication modalities should be explored. This article proposes a modular architecture allowing human operators to interact with robots through different modalities. In particular, we implemented the architecture to handle gestural and touchscreen input, respectively, using a smartwatch and a tablet. Finally, we performed a comparative user experience study between these two modalities.",
      "published": "2022-07-08T09:34:56Z"
    },
    "metadata": {
      "arxiv_id": "2207.03783",
      "title": "Gestural and Touchscreen Interaction for Human-Robot Collaboration: a Comparative Study",
      "summary": "Close human-robot interaction (HRI), especially in industrial scenarios, has been vastly investigated for the advantages of combining human and robot skills. For an effective HRI, the validity of currently available human-machine communication media or tools should be questioned, and new communication modalities should be explored. This article proposes a modular architecture allowing human operators to interact with robots through different modalities. In particular, we implemented the architecture to handle gestural and touchscreen input, respectively, using a smartwatch and a tablet. Finally, we performed a comparative user experience study between these two modalities.",
      "authors": [
        "Antonino Bongiovanni",
        "Alessio De Luca",
        "Luna Gava",
        "Lucrezia Grassi",
        "Marta Lagomarsino",
        "Marco Lapolla",
        "Antonio Marino",
        "Patrick Roncagliolo",
        "Simone Macciò",
        "Alessandro Carfì",
        "Fulvio Mastrogiovanni"
      ],
      "published": "2022-07-08T09:34:56Z",
      "updated": "2022-07-08T09:34:56Z",
      "categories": [
        "cs.RO"
      ],
      "pdf_url": "https://arxiv.org/pdf/2207.03783v1",
      "landing_url": "https://arxiv.org/abs/2207.03783v1",
      "doi": "https://doi.org/10.48550/arXiv.2207.03783"
    },
    "queries": [
      {
        "anchor": "synchronous dialogue",
        "search_term": "modular architecture"
      }
    ]
  },
  {
    "arxiv_id": "2207.05894",
    "anchor": "synchronous dialogue",
    "search_term": "neural codec",
    "search_record": {
      "id": "http://arxiv.org/abs/2207.05894v1",
      "title": "Hybrid Spatial-Temporal Entropy Modelling for Neural Video Compression",
      "summary": "For neural video codec, it is critical, yet challenging, to design an efficient entropy model which can accurately predict the probability distribution of the quantized latent representation. However, most existing video codecs directly use the ready-made entropy model from image codec to encode the residual or motion, and do not fully leverage the spatial-temporal characteristics in video. To this end, this paper proposes a powerful entropy model which efficiently captures both spatial and temporal dependencies. In particular, we introduce the latent prior which exploits the correlation among the latent representation to squeeze the temporal redundancy. Meanwhile, the dual spatial prior is proposed to reduce the spatial redundancy in a parallel-friendly manner. In addition, our entropy model is also versatile. Besides estimating the probability distribution, our entropy model also generates the quantization step at spatial-channel-wise. This content-adaptive quantization mechanism not only helps our codec achieve the smooth rate adjustment in single model but also improves the final rate-distortion performance by dynamic bit allocation. Experimental results show that, powered by the proposed entropy model, our neural codec can achieve 18.2% bitrate saving on UVG dataset when compared with H.266 (VTM) using the highest compression ratio configuration. It makes a new milestone in the development of neural video codec. The codes are at https://github.com/microsoft/DCVC.",
      "published": "2022-07-13T00:03:54Z"
    },
    "metadata": {
      "arxiv_id": "2207.05894",
      "title": "Hybrid Spatial-Temporal Entropy Modelling for Neural Video Compression",
      "summary": "For neural video codec, it is critical, yet challenging, to design an efficient entropy model which can accurately predict the probability distribution of the quantized latent representation. However, most existing video codecs directly use the ready-made entropy model from image codec to encode the residual or motion, and do not fully leverage the spatial-temporal characteristics in video. To this end, this paper proposes a powerful entropy model which efficiently captures both spatial and temporal dependencies. In particular, we introduce the latent prior which exploits the correlation among the latent representation to squeeze the temporal redundancy. Meanwhile, the dual spatial prior is proposed to reduce the spatial redundancy in a parallel-friendly manner. In addition, our entropy model is also versatile. Besides estimating the probability distribution, our entropy model also generates the quantization step at spatial-channel-wise. This content-adaptive quantization mechanism not only helps our codec achieve the smooth rate adjustment in single model but also improves the final rate-distortion performance by dynamic bit allocation. Experimental results show that, powered by the proposed entropy model, our neural codec can achieve 18.2% bitrate saving on UVG dataset when compared with H.266 (VTM) using the highest compression ratio configuration. It makes a new milestone in the development of neural video codec. The codes are at https://github.com/microsoft/DCVC.",
      "authors": [
        "Jiahao Li",
        "Bin Li",
        "Yan Lu"
      ],
      "published": "2022-07-13T00:03:54Z",
      "updated": "2022-07-13T00:03:54Z",
      "categories": [
        "eess.IV",
        "cs.CV",
        "cs.MM"
      ],
      "pdf_url": "https://arxiv.org/pdf/2207.05894v1",
      "landing_url": "https://arxiv.org/abs/2207.05894v1",
      "doi": "https://doi.org/10.1145/3503161.3547845"
    },
    "queries": [
      {
        "anchor": "synchronous dialogue",
        "search_term": "neural codec"
      }
    ]
  },
  {
    "arxiv_id": "2207.07281",
    "anchor": "full-duplex",
    "search_term": "true full-duplex",
    "search_record": {
      "id": "http://arxiv.org/abs/2207.07281v1",
      "title": "STEER: Beam Selection for Full-Duplex Millimeter Wave Communication Systems",
      "summary": "Modern millimeter wave (mmWave) communication systems rely on beam alignment to deliver sufficient beamforming gain to close the link between devices. We present a novel beam selection methodology for multi-panel, full-duplex mmWave systems, which we call STEER, that delivers high beamforming gain while significantly reducing the full-duplex self-interference coupled between the transmit and receive beams. STEER does not necessitate changes to conventional beam alignment methodologies nor additional over-the-air feedback, making it compatible with existing cellular standards. Instead, STEER uses conventional beam alignment to identify the general directions beams should be steered, and then it makes use of a minimal number of self-interference measurements to jointly select transmit and receive beams that deliver high gain in these directions while coupling low self-interference. We implement STEER on an industry-grade 28 GHz phased array platform and use further simulation to show that full-duplex operation with beams selected by STEER can notably outperform both half-duplex and full-duplex operation with beams chosen via conventional beam selection. For instance, STEER can reliably reduce self-interference by more than 20 dB and improve SINR by more than 10 dB, compared to conventional beam selection. Our experimental results highlight that beam alignment can be used not only to deliver high beamforming gain in full-duplex mmWave systems but also to mitigate self-interference to levels near or below the noise floor, rendering additional self-interference cancellation unnecessary with STEER.",
      "published": "2022-07-15T04:08:32Z"
    },
    "metadata": {
      "arxiv_id": "2207.07281",
      "title": "STEER: Beam Selection for Full-Duplex Millimeter Wave Communication Systems",
      "summary": "Modern millimeter wave (mmWave) communication systems rely on beam alignment to deliver sufficient beamforming gain to close the link between devices. We present a novel beam selection methodology for multi-panel, full-duplex mmWave systems, which we call STEER, that delivers high beamforming gain while significantly reducing the full-duplex self-interference coupled between the transmit and receive beams. STEER does not necessitate changes to conventional beam alignment methodologies nor additional over-the-air feedback, making it compatible with existing cellular standards. Instead, STEER uses conventional beam alignment to identify the general directions beams should be steered, and then it makes use of a minimal number of self-interference measurements to jointly select transmit and receive beams that deliver high gain in these directions while coupling low self-interference. We implement STEER on an industry-grade 28 GHz phased array platform and use further simulation to show that full-duplex operation with beams selected by STEER can notably outperform both half-duplex and full-duplex operation with beams chosen via conventional beam selection. For instance, STEER can reliably reduce self-interference by more than 20 dB and improve SINR by more than 10 dB, compared to conventional beam selection. Our experimental results highlight that beam alignment can be used not only to deliver high beamforming gain in full-duplex mmWave systems but also to mitigate self-interference to levels near or below the noise floor, rendering additional self-interference cancellation unnecessary with STEER.",
      "authors": [
        "Ian P. Roberts",
        "Aditya Chopra",
        "Thomas Novlan",
        "Sriram Vishwanath",
        "Jeffrey G. Andrews"
      ],
      "published": "2022-07-15T04:08:32Z",
      "updated": "2022-07-15T04:08:32Z",
      "categories": [
        "eess.SP"
      ],
      "pdf_url": "https://arxiv.org/pdf/2207.07281v1",
      "landing_url": "https://arxiv.org/abs/2207.07281v1",
      "doi": "https://doi.org/10.48550/arXiv.2207.07281"
    },
    "queries": [
      {
        "anchor": "full-duplex",
        "search_term": "true full-duplex"
      },
      {
        "anchor": "full-duplex",
        "search_term": "pseudo full-duplex"
      },
      {
        "anchor": "full-duplex",
        "search_term": "half duplex"
      },
      {
        "anchor": "full-duplex",
        "search_term": "engineered synchronization"
      },
      {
        "anchor": "full-duplex",
        "search_term": "learned synchronization"
      },
      {
        "anchor": "full-duplex",
        "search_term": "modular architecture"
      },
      {
        "anchor": "full-duplex",
        "search_term": "streaming architecture"
      },
      {
        "anchor": "full-duplex",
        "search_term": "real-time interaction"
      },
      {
        "anchor": "full-duplex",
        "search_term": "low latency"
      },
      {
        "anchor": "full-duplex",
        "search_term": "synchronous data"
      },
      {
        "anchor": "full-duplex",
        "search_term": "data scarcity"
      },
      {
        "anchor": "full-duplex",
        "search_term": "multichannel dialogue"
      },
      {
        "anchor": "full-duplex",
        "search_term": "temporal dynamics"
      },
      {
        "anchor": "full-duplex",
        "search_term": "behavioral arbitration"
      },
      {
        "anchor": "full-duplex",
        "search_term": "semantic coherence"
      },
      {
        "anchor": "full-duplex",
        "search_term": "neural codec"
      },
      {
        "anchor": "full-duplex",
        "search_term": "discrete tokens"
      },
      {
        "anchor": "full-duplex",
        "search_term": "audio tokens"
      },
      {
        "anchor": "full-duplex",
        "search_term": "floor transfer offset"
      },
      {
        "anchor": "full-duplex",
        "search_term": "stop latency"
      },
      {
        "anchor": "full-duplex",
        "search_term": "interruption response delay"
      }
    ]
  },
  {
    "arxiv_id": "2207.08226",
    "anchor": "spoken language models",
    "search_term": "low latency",
    "search_record": {
      "id": "http://arxiv.org/abs/2207.08226v1",
      "title": "An Intelligent Deterministic Scheduling Method for Ultra-Low Latency Communication in Edge Enabled Industrial Internet of Things",
      "summary": "Edge enabled Industrial Internet of Things (IIoT) platform is of great significance to accelerate the development of smart industry. However, with the dramatic increase in real-time IIoT applications, it is a great challenge to support fast response time, low latency, and efficient bandwidth utilization. To address this issue, Time Sensitive Network (TSN) is recently researched to realize low latency communication via deterministic scheduling. To the best of our knowledge, the combinability of multiple flows, which can significantly affect the scheduling performance, has never been systematically analyzed before. In this article, we first analyze the combinability problem. Then a non-collision theory based deterministic scheduling (NDS) method is proposed to achieve ultra-low latency communication for the time-sensitive flows. Moreover, to improve bandwidth utilization, a dynamic queue scheduling (DQS) method is presented for the best-effort flows. Experiment results demonstrate that NDS/DQS can well support deterministic ultra-low latency services and guarantee efficient bandwidth utilization.",
      "published": "2022-07-17T16:52:51Z"
    },
    "metadata": {
      "arxiv_id": "2207.08226",
      "title": "An Intelligent Deterministic Scheduling Method for Ultra-Low Latency Communication in Edge Enabled Industrial Internet of Things",
      "summary": "Edge enabled Industrial Internet of Things (IIoT) platform is of great significance to accelerate the development of smart industry. However, with the dramatic increase in real-time IIoT applications, it is a great challenge to support fast response time, low latency, and efficient bandwidth utilization. To address this issue, Time Sensitive Network (TSN) is recently researched to realize low latency communication via deterministic scheduling. To the best of our knowledge, the combinability of multiple flows, which can significantly affect the scheduling performance, has never been systematically analyzed before. In this article, we first analyze the combinability problem. Then a non-collision theory based deterministic scheduling (NDS) method is proposed to achieve ultra-low latency communication for the time-sensitive flows. Moreover, to improve bandwidth utilization, a dynamic queue scheduling (DQS) method is presented for the best-effort flows. Experiment results demonstrate that NDS/DQS can well support deterministic ultra-low latency services and guarantee efficient bandwidth utilization.",
      "authors": [
        "Yinzhi Lu",
        "Liu Yang",
        "Simon X. Yang",
        "Qiaozhi Hua",
        "Arun Kumar Sangaiah",
        "Tan Guo",
        "Keping Yu"
      ],
      "published": "2022-07-17T16:52:51Z",
      "updated": "2022-07-17T16:52:51Z",
      "categories": [
        "cs.NI",
        "cs.AI",
        "cs.SI",
        "eess.SY"
      ],
      "pdf_url": "https://arxiv.org/pdf/2207.08226v1",
      "landing_url": "https://arxiv.org/abs/2207.08226v1",
      "doi": "https://doi.org/10.1109/TII.2022.3186891"
    },
    "queries": [
      {
        "anchor": "spoken language models",
        "search_term": "low latency"
      },
      {
        "anchor": "synchronous dialogue",
        "search_term": "low latency"
      },
      {
        "anchor": "turn-taking",
        "search_term": "low latency"
      }
    ]
  },
  {
    "arxiv_id": "2207.10524",
    "anchor": "synchronous dialogue",
    "search_term": "data scarcity",
    "search_record": {
      "id": "http://arxiv.org/abs/2207.10524v2",
      "title": "NusaCrowd: A Call for Open and Reproducible NLP Research in Indonesian Languages",
      "summary": "At the center of the underlying issues that halt Indonesian natural language processing (NLP) research advancement, we find data scarcity. Resources in Indonesian languages, especially the local ones, are extremely scarce and underrepresented. Many Indonesian researchers do not publish their dataset. Furthermore, the few public datasets that we have are scattered across different platforms, thus makes performing reproducible and data-centric research in Indonesian NLP even more arduous. Rising to this challenge, we initiate the first Indonesian NLP crowdsourcing effort, NusaCrowd. NusaCrowd strives to provide the largest datasheets aggregation with standardized data loading for NLP tasks in all Indonesian languages. By enabling open and centralized access to Indonesian NLP resources, we hope NusaCrowd can tackle the data scarcity problem hindering NLP progress in Indonesia and bring NLP practitioners to move towards collaboration.",
      "published": "2022-07-21T15:05:42Z"
    },
    "metadata": {
      "arxiv_id": "2207.10524",
      "title": "NusaCrowd: A Call for Open and Reproducible NLP Research in Indonesian Languages",
      "summary": "At the center of the underlying issues that halt Indonesian natural language processing (NLP) research advancement, we find data scarcity. Resources in Indonesian languages, especially the local ones, are extremely scarce and underrepresented. Many Indonesian researchers do not publish their dataset. Furthermore, the few public datasets that we have are scattered across different platforms, thus makes performing reproducible and data-centric research in Indonesian NLP even more arduous. Rising to this challenge, we initiate the first Indonesian NLP crowdsourcing effort, NusaCrowd. NusaCrowd strives to provide the largest datasheets aggregation with standardized data loading for NLP tasks in all Indonesian languages. By enabling open and centralized access to Indonesian NLP resources, we hope NusaCrowd can tackle the data scarcity problem hindering NLP progress in Indonesia and bring NLP practitioners to move towards collaboration.",
      "authors": [
        "Samuel Cahyawijaya",
        "Alham Fikri Aji",
        "Holy Lovenia",
        "Genta Indra Winata",
        "Bryan Wilie",
        "Rahmad Mahendra",
        "Fajri Koto",
        "David Moeljadi",
        "Karissa Vincentio",
        "Ade Romadhony",
        "Ayu Purwarianti"
      ],
      "published": "2022-07-21T15:05:42Z",
      "updated": "2022-08-01T16:55:04Z",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2207.10524v2",
      "landing_url": "https://arxiv.org/abs/2207.10524v2",
      "doi": "https://doi.org/10.48550/arXiv.2207.10524"
    },
    "queries": [
      {
        "anchor": "synchronous dialogue",
        "search_term": "data scarcity"
      }
    ]
  },
  {
    "arxiv_id": "2207.13880",
    "anchor": "synchronous dialogue",
    "search_term": "data scarcity",
    "search_record": {
      "id": "http://arxiv.org/abs/2207.13880v1",
      "title": "Towards overcoming data scarcity in materials science: unifying models and datasets with a mixture of experts framework",
      "summary": "While machine learning has emerged in recent years as a useful tool for rapid prediction of materials properties, generating sufficient data to reliably train models without overfitting is still impractical for many applications. Towards overcoming this limitation, we present a general framework for leveraging complementary information across different models and datasets for accurate prediction of data scarce materials properties. Our approach, based on a machine learning paradigm called mixture of experts, outperforms pairwise transfer learning on 16 of 19 materials property regression tasks, performing comparably on the remaining three. Unlike pairwise transfer learning, our framework automatically learns to combine information from multiple source tasks in a single training run, alleviating the need for brute-force experiments to determine which source task to transfer from. The approach also provides an interpretable, model-agnostic, and scalable mechanism to transfer information from an arbitrary number of models and datasets to any downstream property prediction task. We anticipate the performance of our framework will further improve as better model architectures, new pre-training tasks, and larger materials datasets are developed by the community.",
      "published": "2022-07-28T04:34:41Z"
    },
    "metadata": {
      "arxiv_id": "2207.13880",
      "title": "Towards overcoming data scarcity in materials science: unifying models and datasets with a mixture of experts framework",
      "summary": "While machine learning has emerged in recent years as a useful tool for rapid prediction of materials properties, generating sufficient data to reliably train models without overfitting is still impractical for many applications. Towards overcoming this limitation, we present a general framework for leveraging complementary information across different models and datasets for accurate prediction of data scarce materials properties. Our approach, based on a machine learning paradigm called mixture of experts, outperforms pairwise transfer learning on 16 of 19 materials property regression tasks, performing comparably on the remaining three. Unlike pairwise transfer learning, our framework automatically learns to combine information from multiple source tasks in a single training run, alleviating the need for brute-force experiments to determine which source task to transfer from. The approach also provides an interpretable, model-agnostic, and scalable mechanism to transfer information from an arbitrary number of models and datasets to any downstream property prediction task. We anticipate the performance of our framework will further improve as better model architectures, new pre-training tasks, and larger materials datasets are developed by the community.",
      "authors": [
        "Rees Chang",
        "Yu-Xiong Wang",
        "Elif Ertekin"
      ],
      "published": "2022-07-28T04:34:41Z",
      "updated": "2022-07-28T04:34:41Z",
      "categories": [
        "cond-mat.mtrl-sci"
      ],
      "pdf_url": "https://arxiv.org/pdf/2207.13880v1",
      "landing_url": "https://arxiv.org/abs/2207.13880v1",
      "doi": "https://doi.org/10.48550/arXiv.2207.13880"
    },
    "queries": [
      {
        "anchor": "synchronous dialogue",
        "search_term": "data scarcity"
      }
    ]
  },
  {
    "arxiv_id": "2208.00671",
    "anchor": "synchronous dialogue",
    "search_term": "real-time interaction",
    "search_record": {
      "id": "http://arxiv.org/abs/2208.00671v4",
      "title": "RASIPAM: Interactive Pattern Mining of Multivariate Event Sequences in Racket Sports",
      "summary": "Experts in racket sports like tennis and badminton use tactical analysis to gain insight into competitors' playing styles. Many data-driven methods apply pattern mining to racket sports data -- which is often recorded as multivariate event sequences -- to uncover sports tactics. However, tactics obtained in this way are often inconsistent with those deduced by experts through their domain knowledge, which can be confusing to those experts. This work introduces RASIPAM, a RAcket-Sports Interactive PAttern Mining system, which allows experts to incorporate their knowledge into data mining algorithms to discover meaningful tactics interactively. RASIPAM consists of a constraint-based pattern mining algorithm that responds to the analysis demands of experts: Experts provide suggestions for finding tactics in intuitive written language, and these suggestions are translated into constraints to run the algorithm. RASIPAM further introduces a tailored visual interface that allows experts to compare the new tactics with the original ones and decide whether to apply a given adjustment. This interactive workflow iteratively progresses until experts are satisfied with all tactics. We conduct a quantitative experiment to show that our algorithm supports real-time interaction. Two case studies in tennis and in badminton respectively, each involving two domain experts, are conducted to show the effectiveness and usefulness of the system.",
      "published": "2022-08-01T08:04:14Z"
    },
    "metadata": {
      "arxiv_id": "2208.00671",
      "title": "RASIPAM: Interactive Pattern Mining of Multivariate Event Sequences in Racket Sports",
      "summary": "Experts in racket sports like tennis and badminton use tactical analysis to gain insight into competitors' playing styles. Many data-driven methods apply pattern mining to racket sports data -- which is often recorded as multivariate event sequences -- to uncover sports tactics. However, tactics obtained in this way are often inconsistent with those deduced by experts through their domain knowledge, which can be confusing to those experts. This work introduces RASIPAM, a RAcket-Sports Interactive PAttern Mining system, which allows experts to incorporate their knowledge into data mining algorithms to discover meaningful tactics interactively. RASIPAM consists of a constraint-based pattern mining algorithm that responds to the analysis demands of experts: Experts provide suggestions for finding tactics in intuitive written language, and these suggestions are translated into constraints to run the algorithm. RASIPAM further introduces a tailored visual interface that allows experts to compare the new tactics with the original ones and decide whether to apply a given adjustment. This interactive workflow iteratively progresses until experts are satisfied with all tactics. We conduct a quantitative experiment to show that our algorithm supports real-time interaction. Two case studies in tennis and in badminton respectively, each involving two domain experts, are conducted to show the effectiveness and usefulness of the system.",
      "authors": [
        "Jiang Wu",
        "Dongyu Liu",
        "Ziyang Guo",
        "Yingcai Wu"
      ],
      "published": "2022-08-01T08:04:14Z",
      "updated": "2022-11-28T08:50:46Z",
      "categories": [
        "cs.HC"
      ],
      "pdf_url": "https://arxiv.org/pdf/2208.00671v4",
      "landing_url": "https://arxiv.org/abs/2208.00671v4",
      "doi": "https://doi.org/10.1109/TVCG.2022.3209452"
    },
    "queries": [
      {
        "anchor": "synchronous dialogue",
        "search_term": "real-time interaction"
      }
    ]
  },
  {
    "arxiv_id": "2208.00786",
    "anchor": "synchronous dialogue",
    "search_term": "modular architecture",
    "search_record": {
      "id": "http://arxiv.org/abs/2208.00786v1",
      "title": "Design Guidelines for Apache Kafka Driven Data Management and Distribution in Smart Cities",
      "summary": "Smart city management is going through a remarkable transition, in terms of quality and diversity of services provided to the end-users. The stakeholders that deliver pervasive applications are now able to address fundamental challenges in the big data value chain, from data acquisition, data analysis and processing, data storage and curation, and data visualisation in real scenarios. Industry 4.0 is pushing this trend forward, demanding for servitization of products and data, also for the smart cities sector where humans, sensors and devices are operating in strict collaboration. The data produced by the ubiquitous devices must be processed quickly to allow the implementation of reactive services such as situational awareness, video surveillance and geo-localization, while always ensuring the safety and privacy of involved citizens. This paper proposes a modular architecture to (i) leverage innovative technologies for data acquisition, management and distribution (such as Apache Kafka and Apache NiFi), (ii) develop a multi-layer engineering solution for revealing valuable and hidden societal knowledge in smart cities environment, and (iii) tackle the main issues in tasks involving complex data flows and provide general guidelines to solve them. We derived some guidelines from an experimental setting performed together with leading industrial technical departments to accomplish an efficient system for monitoring and servitization of smart city assets, with a scalable platform that confirms its usefulness in numerous smart city use cases with different needs.",
      "published": "2022-08-01T11:57:58Z"
    },
    "metadata": {
      "arxiv_id": "2208.00786",
      "title": "Design Guidelines for Apache Kafka Driven Data Management and Distribution in Smart Cities",
      "summary": "Smart city management is going through a remarkable transition, in terms of quality and diversity of services provided to the end-users. The stakeholders that deliver pervasive applications are now able to address fundamental challenges in the big data value chain, from data acquisition, data analysis and processing, data storage and curation, and data visualisation in real scenarios. Industry 4.0 is pushing this trend forward, demanding for servitization of products and data, also for the smart cities sector where humans, sensors and devices are operating in strict collaboration. The data produced by the ubiquitous devices must be processed quickly to allow the implementation of reactive services such as situational awareness, video surveillance and geo-localization, while always ensuring the safety and privacy of involved citizens. This paper proposes a modular architecture to (i) leverage innovative technologies for data acquisition, management and distribution (such as Apache Kafka and Apache NiFi), (ii) develop a multi-layer engineering solution for revealing valuable and hidden societal knowledge in smart cities environment, and (iii) tackle the main issues in tasks involving complex data flows and provide general guidelines to solve them. We derived some guidelines from an experimental setting performed together with leading industrial technical departments to accomplish an efficient system for monitoring and servitization of smart city assets, with a scalable platform that confirms its usefulness in numerous smart city use cases with different needs.",
      "authors": [
        "Theofanis P. Raptis",
        "Claudio Cicconetti",
        "Manolis Falelakis",
        "Tassos Kanellos",
        "Tomás Pariente Lobo"
      ],
      "published": "2022-08-01T11:57:58Z",
      "updated": "2022-08-01T11:57:58Z",
      "categories": [
        "cs.NI",
        "cs.ET"
      ],
      "pdf_url": "https://arxiv.org/pdf/2208.00786v1",
      "landing_url": "https://arxiv.org/abs/2208.00786v1",
      "doi": "https://doi.org/10.1109/ISC255366.2022.9922546"
    },
    "queries": [
      {
        "anchor": "synchronous dialogue",
        "search_term": "modular architecture"
      }
    ]
  },
  {
    "arxiv_id": "2208.05208",
    "anchor": "synchronous dialogue",
    "search_term": "modular architecture",
    "search_record": {
      "id": "http://arxiv.org/abs/2208.05208v1",
      "title": "A data-driven modular architecture with denoising autoencoders for health indicator construction in a manufacturing process",
      "summary": "Within the field of prognostics and health management (PHM), health indicators (HI) can be used to aid the production and, e.g. schedule maintenance and avoid failures. However, HI is often engineered to a specific process and typically requires large amounts of historical data for set-up. This is especially a challenge for SMEs, which often lack sufficient resources and knowledge to benefit from PHM. In this paper, we propose ModularHI, a modular approach in the construction of HI for a system without historical data. With ModularHI, the operator chooses which sensor inputs are available, and then ModularHI will compute a baseline model based on data collected during a burn-in state. This baseline model will then be used to detect if the system starts to degrade over time. We test the ModularHI on two open datasets, CMAPSS and N-CMAPSS. Results from the former dataset showcase our system's ability to detect degradation, while results from the latter point to directions for further research within the area. The results shows that our novel approach is able to detect system degradation without historical data.",
      "published": "2022-08-10T08:12:43Z"
    },
    "metadata": {
      "arxiv_id": "2208.05208",
      "title": "A data-driven modular architecture with denoising autoencoders for health indicator construction in a manufacturing process",
      "summary": "Within the field of prognostics and health management (PHM), health indicators (HI) can be used to aid the production and, e.g. schedule maintenance and avoid failures. However, HI is often engineered to a specific process and typically requires large amounts of historical data for set-up. This is especially a challenge for SMEs, which often lack sufficient resources and knowledge to benefit from PHM. In this paper, we propose ModularHI, a modular approach in the construction of HI for a system without historical data. With ModularHI, the operator chooses which sensor inputs are available, and then ModularHI will compute a baseline model based on data collected during a burn-in state. This baseline model will then be used to detect if the system starts to degrade over time. We test the ModularHI on two open datasets, CMAPSS and N-CMAPSS. Results from the former dataset showcase our system's ability to detect degradation, while results from the latter point to directions for further research within the area. The results shows that our novel approach is able to detect system degradation without historical data.",
      "authors": [
        "Emil Blixt Hansen",
        "Helge Langseth",
        "Nadeem Iftikhar",
        "Simon Bøgh"
      ],
      "published": "2022-08-10T08:12:43Z",
      "updated": "2022-08-10T08:12:43Z",
      "categories": [
        "cs.LG",
        "eess.SY"
      ],
      "pdf_url": "https://arxiv.org/pdf/2208.05208v1",
      "landing_url": "https://arxiv.org/abs/2208.05208v1",
      "doi": "https://doi.org/10.48550/arXiv.2208.05208"
    },
    "queries": [
      {
        "anchor": "synchronous dialogue",
        "search_term": "modular architecture"
      }
    ]
  },
  {
    "arxiv_id": "2208.05517",
    "anchor": "turn-taking",
    "search_term": "true full-duplex",
    "search_record": {
      "id": "http://arxiv.org/abs/2208.05517v1",
      "title": "Beyond the Blue Sky of Multimodal Interaction: A Centennial Vision of Interplanetary Virtual Spaces in Turn-based Metaverse",
      "summary": "Human habitation across multiple planets requires communication and social connection between planets. When the infrastructure of a deep space network becomes mature, immersive cyberspace, known as the Metaverse, can exchange diversified user data and host multitudinous virtual worlds. Nevertheless, such immersive cyberspace unavoidably encounters latency in minutes, and thus operates in a turn-taking manner. This Blue Sky paper illustrates a vision of an interplanetary Metaverse that connects Earthian and Martian users in a turn-based Metaverse. Accordingly, we briefly discuss several grand challenges to catalyze research initiatives for the `Digital Big Bang' on Mars.",
      "published": "2022-07-28T07:26:57Z"
    },
    "metadata": {
      "arxiv_id": "2208.05517",
      "title": "Beyond the Blue Sky of Multimodal Interaction: A Centennial Vision of Interplanetary Virtual Spaces in Turn-based Metaverse",
      "summary": "Human habitation across multiple planets requires communication and social connection between planets. When the infrastructure of a deep space network becomes mature, immersive cyberspace, known as the Metaverse, can exchange diversified user data and host multitudinous virtual worlds. Nevertheless, such immersive cyberspace unavoidably encounters latency in minutes, and thus operates in a turn-taking manner. This Blue Sky paper illustrates a vision of an interplanetary Metaverse that connects Earthian and Martian users in a turn-based Metaverse. Accordingly, we briefly discuss several grand challenges to catalyze research initiatives for the `Digital Big Bang' on Mars.",
      "authors": [
        "Lik Hang Lee",
        "Carlos Bermejo Fernandez",
        "Ahmad Alhilal",
        "Tristan Braud",
        "Simo Hosio",
        "Pan Hui",
        "Esmée Henrieke Henrieke Anne de Haas"
      ],
      "published": "2022-07-28T07:26:57Z",
      "updated": "2022-07-28T07:26:57Z",
      "categories": [
        "cs.HC",
        "physics.pop-ph"
      ],
      "pdf_url": "https://arxiv.org/pdf/2208.05517v1",
      "landing_url": "https://arxiv.org/abs/2208.05517v1",
      "doi": "https://doi.org/10.48550/arXiv.2208.05517"
    },
    "queries": [
      {
        "anchor": "turn-taking",
        "search_term": "true full-duplex"
      },
      {
        "anchor": "turn-taking",
        "search_term": "pseudo full-duplex"
      },
      {
        "anchor": "turn-taking",
        "search_term": "engineered synchronization"
      },
      {
        "anchor": "turn-taking",
        "search_term": "learned synchronization"
      },
      {
        "anchor": "turn-taking",
        "search_term": "synchronous data"
      },
      {
        "anchor": "turn-taking",
        "search_term": "multichannel dialogue"
      },
      {
        "anchor": "turn-taking",
        "search_term": "behavioral arbitration"
      },
      {
        "anchor": "turn-taking",
        "search_term": "floor transfer offset"
      },
      {
        "anchor": "turn-taking",
        "search_term": "stop latency"
      },
      {
        "anchor": "turn-taking",
        "search_term": "interruption response delay"
      }
    ]
  },
  {
    "arxiv_id": "2208.09968",
    "anchor": "synchronous dialogue",
    "search_term": "data scarcity",
    "search_record": {
      "id": "http://arxiv.org/abs/2208.09968v3",
      "title": "Transfer Ranking in Finance: Applications to Cross-Sectional Momentum with Data Scarcity",
      "summary": "Cross-sectional strategies are a classical and popular trading style, with recent high performing variants incorporating sophisticated neural architectures. While these strategies have been applied successfully to data-rich settings involving mature assets with long histories, deploying them on instruments with limited samples generally produce over-fitted models with degraded performance. In this paper, we introduce Fused Encoder Networks -- a novel and hybrid parameter-sharing transfer ranking model. The model fuses information extracted using an encoder-attention module operated on a source dataset with a similar but separate module focused on a smaller target dataset of interest. This mitigates the issue of models with poor generalisability that are a consequence of training on scarce target data. Additionally, the self-attention mechanism enables interactions among instruments to be accounted for, not just at the loss level during model training, but also at inference time. Focusing on momentum applied to the top ten cryptocurrencies by market capitalisation as a demonstrative use-case, the Fused Encoder Networks outperforms the reference benchmarks on most performance measures, delivering a three-fold boost in the Sharpe ratio over classical momentum as well as an improvement of approximately 50% against the best benchmark model without transaction costs. It continues outperforming baselines even after accounting for the high transaction costs associated with trading cryptocurrencies.",
      "published": "2022-08-21T21:34:11Z"
    },
    "metadata": {
      "arxiv_id": "2208.09968",
      "title": "Transfer Ranking in Finance: Applications to Cross-Sectional Momentum with Data Scarcity",
      "summary": "Cross-sectional strategies are a classical and popular trading style, with recent high performing variants incorporating sophisticated neural architectures. While these strategies have been applied successfully to data-rich settings involving mature assets with long histories, deploying them on instruments with limited samples generally produce over-fitted models with degraded performance. In this paper, we introduce Fused Encoder Networks -- a novel and hybrid parameter-sharing transfer ranking model. The model fuses information extracted using an encoder-attention module operated on a source dataset with a similar but separate module focused on a smaller target dataset of interest. This mitigates the issue of models with poor generalisability that are a consequence of training on scarce target data. Additionally, the self-attention mechanism enables interactions among instruments to be accounted for, not just at the loss level during model training, but also at inference time. Focusing on momentum applied to the top ten cryptocurrencies by market capitalisation as a demonstrative use-case, the Fused Encoder Networks outperforms the reference benchmarks on most performance measures, delivering a three-fold boost in the Sharpe ratio over classical momentum as well as an improvement of approximately 50% against the best benchmark model without transaction costs. It continues outperforming baselines even after accounting for the high transaction costs associated with trading cryptocurrencies.",
      "authors": [
        "Daniel Poh",
        "Stephen Roberts",
        "Stefan Zohren"
      ],
      "published": "2022-08-21T21:34:11Z",
      "updated": "2023-02-21T16:03:35Z",
      "categories": [
        "q-fin.TR",
        "cs.IR",
        "cs.LG",
        "q-fin.PM"
      ],
      "pdf_url": "https://arxiv.org/pdf/2208.09968v3",
      "landing_url": "https://arxiv.org/abs/2208.09968v3",
      "doi": "https://doi.org/10.48550/arXiv.2208.09968"
    },
    "queries": [
      {
        "anchor": "synchronous dialogue",
        "search_term": "data scarcity"
      }
    ]
  },
  {
    "arxiv_id": "2208.10938",
    "anchor": "synchronous dialogue",
    "search_term": "low latency",
    "search_record": {
      "id": "http://arxiv.org/abs/2208.10938v1",
      "title": "Schedulers Synchronization Supporting Ultra Reliable Low Latency Communications (URLLC) in Cloud-RAN over Virtualised Mesh PON",
      "summary": "We propose a mechanism to support URLLC Open-RAN ultra-low latency over a MESH-PON, serving dense deployment of small cell and MEC nodes in an access network. We show the possibility, under given assumptions, to achieve application-to-application end-to-end latency below 1ms.",
      "published": "2022-08-23T13:06:12Z"
    },
    "metadata": {
      "arxiv_id": "2208.10938",
      "title": "Schedulers Synchronization Supporting Ultra Reliable Low Latency Communications (URLLC) in Cloud-RAN over Virtualised Mesh PON",
      "summary": "We propose a mechanism to support URLLC Open-RAN ultra-low latency over a MESH-PON, serving dense deployment of small cell and MEC nodes in an access network. We show the possibility, under given assumptions, to achieve application-to-application end-to-end latency below 1ms.",
      "authors": [
        "Sandip Das",
        "Frank Slyne",
        "Daniel Kilper",
        "Marco Ruffini"
      ],
      "published": "2022-08-23T13:06:12Z",
      "updated": "2022-08-23T13:06:12Z",
      "categories": [
        "cs.NI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2208.10938v1",
      "landing_url": "https://arxiv.org/abs/2208.10938v1",
      "doi": "https://doi.org/10.48550/arXiv.2208.10938"
    },
    "queries": [
      {
        "anchor": "synchronous dialogue",
        "search_term": "low latency"
      }
    ]
  },
  {
    "arxiv_id": "2208.11956",
    "anchor": "synchronous dialogue",
    "search_term": "low latency",
    "search_record": {
      "id": "http://arxiv.org/abs/2208.11956v1",
      "title": "Intelligent Random Access Framework for Massive and Ultra-Reliable Low Latency IoT Communications",
      "summary": "The Internet of Things (IoT) enables smart cities to achieve the vision of connecting everything by smartly linking gadgets without the need for human interaction. However, due to the rapid proliferation of IoT devices, the amount of data produced accounts for a significant share of all communication services. Hence, 6G-enabled specifications for wireless networks are required to enable both massive and ultra-reliable low-latency access to such a hybrid IoT network. In this article, we propose a smart hybrid random access (SH-RA) scheme for massive connections and ultra-reliable low-latency access in the network architecture of IoT communications. According to numerical results, compared to the other baseline schemes, the SH-RA framework enormously enhances the total access probability and fulfills the quality-of-service (QoS) requirements.",
      "published": "2022-08-25T09:23:08Z"
    },
    "metadata": {
      "arxiv_id": "2208.11956",
      "title": "Intelligent Random Access Framework for Massive and Ultra-Reliable Low Latency IoT Communications",
      "summary": "The Internet of Things (IoT) enables smart cities to achieve the vision of connecting everything by smartly linking gadgets without the need for human interaction. However, due to the rapid proliferation of IoT devices, the amount of data produced accounts for a significant share of all communication services. Hence, 6G-enabled specifications for wireless networks are required to enable both massive and ultra-reliable low-latency access to such a hybrid IoT network. In this article, we propose a smart hybrid random access (SH-RA) scheme for massive connections and ultra-reliable low-latency access in the network architecture of IoT communications. According to numerical results, compared to the other baseline schemes, the SH-RA framework enormously enhances the total access probability and fulfills the quality-of-service (QoS) requirements.",
      "authors": [
        "Muhammad Waleed Aftab",
        "Ishtiaq Ahmad"
      ],
      "published": "2022-08-25T09:23:08Z",
      "updated": "2022-08-25T09:23:08Z",
      "categories": [
        "eess.SP"
      ],
      "pdf_url": "https://arxiv.org/pdf/2208.11956v1",
      "landing_url": "https://arxiv.org/abs/2208.11956v1",
      "doi": "https://doi.org/10.48550/arXiv.2208.11956"
    },
    "queries": [
      {
        "anchor": "synchronous dialogue",
        "search_term": "low latency"
      }
    ]
  },
  {
    "arxiv_id": "2208.12319",
    "anchor": "synchronous dialogue",
    "search_term": "modular architecture",
    "search_record": {
      "id": "http://arxiv.org/abs/2208.12319v1",
      "title": "Mask-Mediator-Wrapper: A revised mediator-wrapper architecture for heterogeneous data source integration",
      "summary": "This paper deals with the mediator-wrapper architecture. It is an important architectural pattern that enables a more flexible and modular architecture in opposition to monolithic architectures for data source integration systems. This paper identifies certain realistic and concrete scenarios where the mediator-wrapper architecture underperforms. These issues are addressed with the extension of the architecture via the mask component type. The mask component is detailed so it can be reasoned about without prescribing a concrete programming language or paradigm. The benefits of the new mask-mediator-wrapper architecture are analytically proven in relevant scenarios. One of the applications of the new architecture is envisioned for modern data sources integration systems backing Big data processing.",
      "published": "2022-08-25T19:38:18Z"
    },
    "metadata": {
      "arxiv_id": "2208.12319",
      "title": "Mask-Mediator-Wrapper: A revised mediator-wrapper architecture for heterogeneous data source integration",
      "summary": "This paper deals with the mediator-wrapper architecture. It is an important architectural pattern that enables a more flexible and modular architecture in opposition to monolithic architectures for data source integration systems. This paper identifies certain realistic and concrete scenarios where the mediator-wrapper architecture underperforms. These issues are addressed with the extension of the architecture via the mask component type. The mask component is detailed so it can be reasoned about without prescribing a concrete programming language or paradigm. The benefits of the new mask-mediator-wrapper architecture are analytically proven in relevant scenarios. One of the applications of the new architecture is envisioned for modern data sources integration systems backing Big data processing.",
      "authors": [
        "Juraj Dončević",
        "Krešimir Fertalj",
        "Mario Brčić",
        "Agneza Krajna"
      ],
      "published": "2022-08-25T19:38:18Z",
      "updated": "2022-08-25T19:38:18Z",
      "categories": [
        "cs.SE"
      ],
      "pdf_url": "https://arxiv.org/pdf/2208.12319v1",
      "landing_url": "https://arxiv.org/abs/2208.12319v1",
      "doi": "https://doi.org/10.3390/app13042471"
    },
    "queries": [
      {
        "anchor": "synchronous dialogue",
        "search_term": "modular architecture"
      }
    ]
  },
  {
    "arxiv_id": "2208.12422",
    "anchor": "synchronous dialogue",
    "search_term": "data scarcity",
    "search_record": {
      "id": "http://arxiv.org/abs/2208.12422v2",
      "title": "Toward Robust Graph Semi-Supervised Learning against Extreme Data Scarcity",
      "summary": "The success of graph neural networks on graph-based web mining highly relies on abundant human-annotated data, which is laborious to obtain in practice. When only few labeled nodes are available, how to improve their robustness is a key to achieve replicable and sustainable graph semi-supervised learning. Though self-training has been shown to be powerful for semi-supervised learning, its application on graph-structured data may fail because (1) larger receptive fields are not leveraged to capture long-range node interactions, which exacerbates the difficulty of propagating feature-label patterns from labeled nodes to unlabeled nodes; and (2) limited labeled data makes it challenging to learn well-separated decision boundaries for different node classes without explicitly capturing the underlying semantic structure. To address the challenges of capturing informative structural and semantic knowledge, we propose a new graph data augmentation framework, AGST (Augmented Graph Self-Training), which is built with two new (i.e., structural and semantic) augmentation modules on top of a decoupled GST backbone. In this work, we investigate whether this novel framework can learn a robust graph predictive model under the low-data context. We conduct comprehensive evaluations on semi-supervised node classification under different scenarios of limited labeled-node data. The experimental results demonstrate the unique contributions of the novel data augmentation framework for node classification with few labeled data.",
      "published": "2022-08-26T03:36:01Z"
    },
    "metadata": {
      "arxiv_id": "2208.12422",
      "title": "Toward Robust Graph Semi-Supervised Learning against Extreme Data Scarcity",
      "summary": "The success of graph neural networks on graph-based web mining highly relies on abundant human-annotated data, which is laborious to obtain in practice. When only few labeled nodes are available, how to improve their robustness is a key to achieve replicable and sustainable graph semi-supervised learning. Though self-training has been shown to be powerful for semi-supervised learning, its application on graph-structured data may fail because (1) larger receptive fields are not leveraged to capture long-range node interactions, which exacerbates the difficulty of propagating feature-label patterns from labeled nodes to unlabeled nodes; and (2) limited labeled data makes it challenging to learn well-separated decision boundaries for different node classes without explicitly capturing the underlying semantic structure. To address the challenges of capturing informative structural and semantic knowledge, we propose a new graph data augmentation framework, AGST (Augmented Graph Self-Training), which is built with two new (i.e., structural and semantic) augmentation modules on top of a decoupled GST backbone. In this work, we investigate whether this novel framework can learn a robust graph predictive model under the low-data context. We conduct comprehensive evaluations on semi-supervised node classification under different scenarios of limited labeled-node data. The experimental results demonstrate the unique contributions of the novel data augmentation framework for node classification with few labeled data.",
      "authors": [
        "Kaize Ding",
        "Elnaz Nouri",
        "Guoqing Zheng",
        "Huan Liu",
        "Ryen White"
      ],
      "published": "2022-08-26T03:36:01Z",
      "updated": "2022-12-11T07:29:56Z",
      "categories": [
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2208.12422v2",
      "landing_url": "https://arxiv.org/abs/2208.12422v2",
      "doi": "https://doi.org/10.48550/arXiv.2208.12422"
    },
    "queries": [
      {
        "anchor": "synchronous dialogue",
        "search_term": "data scarcity"
      }
    ]
  },
  {
    "arxiv_id": "2208.13321",
    "anchor": "turn-taking",
    "search_term": "true full-duplex",
    "search_record": {
      "id": "http://arxiv.org/abs/2208.13321v1",
      "title": "Turn-Taking Prediction for Natural Conversational Speech",
      "summary": "While a streaming voice assistant system has been used in many applications, this system typically focuses on unnatural, one-shot interactions assuming input from a single voice query without hesitation or disfluency. However, a common conversational utterance often involves multiple queries with turn-taking, in addition to disfluencies. These disfluencies include pausing to think, hesitations, word lengthening, filled pauses and repeated phrases. This makes doing speech recognition with conversational speech, including one with multiple queries, a challenging task. To better model the conversational interaction, it is critical to discriminate disfluencies and end of query in order to allow the user to hold the floor for disfluencies while having the system respond as quickly as possible when the user has finished speaking. In this paper, we present a turntaking predictor built on top of the end-to-end (E2E) speech recognizer. Our best system is obtained by jointly optimizing for ASR task and detecting when the user is paused to think or finished speaking. The proposed approach demonstrates over 97% recall rate and 85% precision rate on predicting true turn-taking with only 100 ms latency on a test set designed with 4 types of disfluencies inserted in conversational utterances.",
      "published": "2022-08-29T01:09:23Z"
    },
    "metadata": {
      "arxiv_id": "2208.13321",
      "title": "Turn-Taking Prediction for Natural Conversational Speech",
      "summary": "While a streaming voice assistant system has been used in many applications, this system typically focuses on unnatural, one-shot interactions assuming input from a single voice query without hesitation or disfluency. However, a common conversational utterance often involves multiple queries with turn-taking, in addition to disfluencies. These disfluencies include pausing to think, hesitations, word lengthening, filled pauses and repeated phrases. This makes doing speech recognition with conversational speech, including one with multiple queries, a challenging task. To better model the conversational interaction, it is critical to discriminate disfluencies and end of query in order to allow the user to hold the floor for disfluencies while having the system respond as quickly as possible when the user has finished speaking. In this paper, we present a turntaking predictor built on top of the end-to-end (E2E) speech recognizer. Our best system is obtained by jointly optimizing for ASR task and detecting when the user is paused to think or finished speaking. The proposed approach demonstrates over 97% recall rate and 85% precision rate on predicting true turn-taking with only 100 ms latency on a test set designed with 4 types of disfluencies inserted in conversational utterances.",
      "authors": [
        "Shuo-yiin Chang",
        "Bo Li",
        "Tara N. Sainath",
        "Chao Zhang",
        "Trevor Strohman",
        "Qiao Liang",
        "Yanzhang He"
      ],
      "published": "2022-08-29T01:09:23Z",
      "updated": "2022-08-29T01:09:23Z",
      "categories": [
        "cs.CL",
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2208.13321v1",
      "landing_url": "https://arxiv.org/abs/2208.13321v1",
      "doi": "https://doi.org/10.48550/arXiv.2208.13321"
    },
    "queries": [
      {
        "anchor": "turn-taking",
        "search_term": "true full-duplex"
      },
      {
        "anchor": "turn-taking",
        "search_term": "pseudo full-duplex"
      },
      {
        "anchor": "turn-taking",
        "search_term": "half duplex"
      },
      {
        "anchor": "turn-taking",
        "search_term": "engineered synchronization"
      },
      {
        "anchor": "turn-taking",
        "search_term": "learned synchronization"
      },
      {
        "anchor": "turn-taking",
        "search_term": "modular architecture"
      },
      {
        "anchor": "turn-taking",
        "search_term": "streaming architecture"
      },
      {
        "anchor": "turn-taking",
        "search_term": "real-time interaction"
      },
      {
        "anchor": "turn-taking",
        "search_term": "low latency"
      },
      {
        "anchor": "turn-taking",
        "search_term": "synchronous data"
      },
      {
        "anchor": "turn-taking",
        "search_term": "data scarcity"
      },
      {
        "anchor": "turn-taking",
        "search_term": "multichannel dialogue"
      },
      {
        "anchor": "turn-taking",
        "search_term": "temporal dynamics"
      },
      {
        "anchor": "turn-taking",
        "search_term": "behavioral arbitration"
      },
      {
        "anchor": "turn-taking",
        "search_term": "semantic coherence"
      },
      {
        "anchor": "turn-taking",
        "search_term": "neural codec"
      },
      {
        "anchor": "turn-taking",
        "search_term": "discrete tokens"
      },
      {
        "anchor": "turn-taking",
        "search_term": "audio tokens"
      },
      {
        "anchor": "turn-taking",
        "search_term": "floor transfer offset"
      },
      {
        "anchor": "turn-taking",
        "search_term": "stop latency"
      },
      {
        "anchor": "turn-taking",
        "search_term": "interruption response delay"
      }
    ]
  },
  {
    "arxiv_id": "2209.03143",
    "anchor": "synchronous dialogue",
    "search_term": "audio tokens",
    "search_record": {
      "id": "http://arxiv.org/abs/2209.03143v2",
      "title": "AudioLM: a Language Modeling Approach to Audio Generation",
      "summary": "We introduce AudioLM, a framework for high-quality audio generation with long-term consistency. AudioLM maps the input audio to a sequence of discrete tokens and casts audio generation as a language modeling task in this representation space. We show how existing audio tokenizers provide different trade-offs between reconstruction quality and long-term structure, and we propose a hybrid tokenization scheme to achieve both objectives. Namely, we leverage the discretized activations of a masked language model pre-trained on audio to capture long-term structure and the discrete codes produced by a neural audio codec to achieve high-quality synthesis. By training on large corpora of raw audio waveforms, AudioLM learns to generate natural and coherent continuations given short prompts. When trained on speech, and without any transcript or annotation, AudioLM generates syntactically and semantically plausible speech continuations while also maintaining speaker identity and prosody for unseen speakers. Furthermore, we demonstrate how our approach extends beyond speech by generating coherent piano music continuations, despite being trained without any symbolic representation of music.",
      "published": "2022-09-07T13:40:08Z"
    },
    "metadata": {
      "arxiv_id": "2209.03143",
      "title": "AudioLM: a Language Modeling Approach to Audio Generation",
      "summary": "We introduce AudioLM, a framework for high-quality audio generation with long-term consistency. AudioLM maps the input audio to a sequence of discrete tokens and casts audio generation as a language modeling task in this representation space. We show how existing audio tokenizers provide different trade-offs between reconstruction quality and long-term structure, and we propose a hybrid tokenization scheme to achieve both objectives. Namely, we leverage the discretized activations of a masked language model pre-trained on audio to capture long-term structure and the discrete codes produced by a neural audio codec to achieve high-quality synthesis. By training on large corpora of raw audio waveforms, AudioLM learns to generate natural and coherent continuations given short prompts. When trained on speech, and without any transcript or annotation, AudioLM generates syntactically and semantically plausible speech continuations while also maintaining speaker identity and prosody for unseen speakers. Furthermore, we demonstrate how our approach extends beyond speech by generating coherent piano music continuations, despite being trained without any symbolic representation of music.",
      "authors": [
        "Zalán Borsos",
        "Raphaël Marinier",
        "Damien Vincent",
        "Eugene Kharitonov",
        "Olivier Pietquin",
        "Matt Sharifi",
        "Dominik Roblek",
        "Olivier Teboul",
        "David Grangier",
        "Marco Tagliasacchi",
        "Neil Zeghidour"
      ],
      "published": "2022-09-07T13:40:08Z",
      "updated": "2023-07-26T03:52:36Z",
      "categories": [
        "cs.SD",
        "cs.LG",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2209.03143v2",
      "landing_url": "https://arxiv.org/abs/2209.03143v2",
      "doi": "https://doi.org/10.48550/arXiv.2209.03143"
    },
    "queries": [
      {
        "anchor": "synchronous dialogue",
        "search_term": "audio tokens"
      }
    ]
  },
  {
    "arxiv_id": "2209.05161",
    "anchor": "turn-taking",
    "search_term": "true full-duplex",
    "search_record": {
      "id": "http://arxiv.org/abs/2209.05161v1",
      "title": "How Much Does Prosody Help Turn-taking? Investigations using Voice Activity Projection Models",
      "summary": "Turn-taking is a fundamental aspect of human communication and can be described as the ability to take turns, project upcoming turn shifts, and supply backchannels at appropriate locations throughout a conversation. In this work, we investigate the role of prosody in turn-taking using the recently proposed Voice Activity Projection model, which incrementally models the upcoming speech activity of the interlocutors in a self-supervised manner, without relying on explicit annotation of turn-taking events, or the explicit modeling of prosodic features. Through manipulation of the speech signal, we investigate how these models implicitly utilize prosodic information. We show that these systems learn to utilize various prosodic aspects of speech both on aggregate quantitative metrics of long-form conversations and on single utterances specifically designed to depend on prosody.",
      "published": "2022-09-12T11:40:16Z"
    },
    "metadata": {
      "arxiv_id": "2209.05161",
      "title": "How Much Does Prosody Help Turn-taking? Investigations using Voice Activity Projection Models",
      "summary": "Turn-taking is a fundamental aspect of human communication and can be described as the ability to take turns, project upcoming turn shifts, and supply backchannels at appropriate locations throughout a conversation. In this work, we investigate the role of prosody in turn-taking using the recently proposed Voice Activity Projection model, which incrementally models the upcoming speech activity of the interlocutors in a self-supervised manner, without relying on explicit annotation of turn-taking events, or the explicit modeling of prosodic features. Through manipulation of the speech signal, we investigate how these models implicitly utilize prosodic information. We show that these systems learn to utilize various prosodic aspects of speech both on aggregate quantitative metrics of long-form conversations and on single utterances specifically designed to depend on prosody.",
      "authors": [
        "Erik Ekstedt",
        "Gabriel Skantze"
      ],
      "published": "2022-09-12T11:40:16Z",
      "updated": "2022-09-12T11:40:16Z",
      "categories": [
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2209.05161v1",
      "landing_url": "https://arxiv.org/abs/2209.05161v1",
      "doi": "https://doi.org/10.48550/arXiv.2209.05161"
    },
    "queries": [
      {
        "anchor": "turn-taking",
        "search_term": "true full-duplex"
      },
      {
        "anchor": "turn-taking",
        "search_term": "pseudo full-duplex"
      },
      {
        "anchor": "turn-taking",
        "search_term": "half duplex"
      },
      {
        "anchor": "turn-taking",
        "search_term": "engineered synchronization"
      },
      {
        "anchor": "turn-taking",
        "search_term": "learned synchronization"
      },
      {
        "anchor": "turn-taking",
        "search_term": "modular architecture"
      },
      {
        "anchor": "turn-taking",
        "search_term": "streaming architecture"
      },
      {
        "anchor": "turn-taking",
        "search_term": "real-time interaction"
      },
      {
        "anchor": "turn-taking",
        "search_term": "low latency"
      },
      {
        "anchor": "turn-taking",
        "search_term": "synchronous data"
      },
      {
        "anchor": "turn-taking",
        "search_term": "data scarcity"
      },
      {
        "anchor": "turn-taking",
        "search_term": "multichannel dialogue"
      },
      {
        "anchor": "turn-taking",
        "search_term": "temporal dynamics"
      },
      {
        "anchor": "turn-taking",
        "search_term": "behavioral arbitration"
      },
      {
        "anchor": "turn-taking",
        "search_term": "semantic coherence"
      },
      {
        "anchor": "turn-taking",
        "search_term": "neural codec"
      },
      {
        "anchor": "turn-taking",
        "search_term": "discrete tokens"
      },
      {
        "anchor": "turn-taking",
        "search_term": "audio tokens"
      },
      {
        "anchor": "turn-taking",
        "search_term": "floor transfer offset"
      },
      {
        "anchor": "turn-taking",
        "search_term": "stop latency"
      },
      {
        "anchor": "turn-taking",
        "search_term": "interruption response delay"
      }
    ]
  },
  {
    "arxiv_id": "2209.05233",
    "anchor": "spoken language models",
    "search_term": "low latency",
    "search_record": {
      "id": "http://arxiv.org/abs/2209.05233v1",
      "title": "At-the-edge Data Processing for Low Latency High Throughput Machine Learning Algorithms",
      "summary": "High throughput and low latency data processing is essential for systems requiring live decision making, control, and machine learning-optimized data reduction. We focus on two distinct use cases for in-flight streaming data processing for a) X-ray pulse reconstruction at SLAC's LCLS-II Free-Electron Laser and b) control diagnostics at the DIII-D tokamak fusion reactor. Both cases exemplify high throughput and low latency control feedback and motivate our focus on machine learning at the edge where data processing and machine learning algorithms can be implemented in field programmable gate array based hardware immediately after the diagnostic sensors. We present our recent work on a data preprocessing chain which requires fast featurization for information encoding. We discuss several options for such algorithms with the primary focus on our discrete cosine and sine transform-based approach adapted for streaming data. These algorithms are primarily aimed at implementation in field programmable gate arrays, favoring linear algebra operations that are also aligned with the recent advances in inference accelerators for the computational edge.",
      "published": "2022-09-06T19:03:50Z"
    },
    "metadata": {
      "arxiv_id": "2209.05233",
      "title": "At-the-edge Data Processing for Low Latency High Throughput Machine Learning Algorithms",
      "summary": "High throughput and low latency data processing is essential for systems requiring live decision making, control, and machine learning-optimized data reduction. We focus on two distinct use cases for in-flight streaming data processing for a) X-ray pulse reconstruction at SLAC's LCLS-II Free-Electron Laser and b) control diagnostics at the DIII-D tokamak fusion reactor. Both cases exemplify high throughput and low latency control feedback and motivate our focus on machine learning at the edge where data processing and machine learning algorithms can be implemented in field programmable gate array based hardware immediately after the diagnostic sensors. We present our recent work on a data preprocessing chain which requires fast featurization for information encoding. We discuss several options for such algorithms with the primary focus on our discrete cosine and sine transform-based approach adapted for streaming data. These algorithms are primarily aimed at implementation in field programmable gate arrays, favoring linear algebra operations that are also aligned with the recent advances in inference accelerators for the computational edge.",
      "authors": [
        "Jack Hirschman",
        "Andrei Kamalov",
        "Razib Obaid",
        "Finn H. O'Shea",
        "Ryan N Coffee"
      ],
      "published": "2022-09-06T19:03:50Z",
      "updated": "2022-09-06T19:03:50Z",
      "categories": [
        "physics.ins-det",
        "physics.data-an"
      ],
      "pdf_url": "https://arxiv.org/pdf/2209.05233v1",
      "landing_url": "https://arxiv.org/abs/2209.05233v1",
      "doi": "https://doi.org/10.1007/978-3-031-23606-8_7"
    },
    "queries": [
      {
        "anchor": "spoken language models",
        "search_term": "low latency"
      },
      {
        "anchor": "synchronous dialogue",
        "search_term": "low latency"
      },
      {
        "anchor": "turn-taking",
        "search_term": "low latency"
      }
    ]
  },
  {
    "arxiv_id": "2209.07928",
    "anchor": "synchronous dialogue",
    "search_term": "modular architecture",
    "search_record": {
      "id": "http://arxiv.org/abs/2209.07928v1",
      "title": "The BLue Amazon Brain (BLAB): A Modular Architecture of Services about the Brazilian Maritime Territory",
      "summary": "We describe the first steps in the development of an artificial agent focused on the Brazilian maritime territory, a large region within the South Atlantic also known as the Blue Amazon. The \"BLue Amazon Brain\" (BLAB) integrates a number of services aimed at disseminating information about this region and its importance, functioning as a tool for environmental awareness. The main service provided by BLAB is a conversational facility that deals with complex questions about the Blue Amazon, called BLAB-Chat; its central component is a controller that manages several task-oriented natural language processing modules (e.g., question answering and summarizer systems). These modules have access to an internal data lake as well as to third-party databases. A news reporter (BLAB-Reporter) and a purposely-developed wiki (BLAB-Wiki) are also part of the BLAB service architecture. In this paper, we describe our current version of BLAB's architecture (interface, backend, web services, NLP modules, and resources) and comment on the challenges we have faced so far, such as the lack of training data and the scattered state of domain information. Solving these issues presents a considerable challenge in the development of artificial intelligence for technical domains.",
      "published": "2022-09-06T18:32:08Z"
    },
    "metadata": {
      "arxiv_id": "2209.07928",
      "title": "The BLue Amazon Brain (BLAB): A Modular Architecture of Services about the Brazilian Maritime Territory",
      "summary": "We describe the first steps in the development of an artificial agent focused on the Brazilian maritime territory, a large region within the South Atlantic also known as the Blue Amazon. The \"BLue Amazon Brain\" (BLAB) integrates a number of services aimed at disseminating information about this region and its importance, functioning as a tool for environmental awareness. The main service provided by BLAB is a conversational facility that deals with complex questions about the Blue Amazon, called BLAB-Chat; its central component is a controller that manages several task-oriented natural language processing modules (e.g., question answering and summarizer systems). These modules have access to an internal data lake as well as to third-party databases. A news reporter (BLAB-Reporter) and a purposely-developed wiki (BLAB-Wiki) are also part of the BLAB service architecture. In this paper, we describe our current version of BLAB's architecture (interface, backend, web services, NLP modules, and resources) and comment on the challenges we have faced so far, such as the lack of training data and the scattered state of domain information. Solving these issues presents a considerable challenge in the development of artificial intelligence for technical domains.",
      "authors": [
        "Paulo Pirozelli",
        "Ais B. R. Castro",
        "Ana Luiza C. de Oliveira",
        "André S. Oliveira",
        "Flávio N. Cação",
        "Igor C. Silveira",
        "João G. M. Campos",
        "Laura C. Motheo",
        "Leticia F. Figueiredo",
        "Lucas F. A. O. Pellicer",
        "Marcelo A. José",
        "Marcos M. José",
        "Pedro de M. Ligabue",
        "Ricardo S. Grava",
        "Rodrigo M. Tavares",
        "Vinícius B. Matos",
        "Yan V. Sym",
        "Anna H. R. Costa",
        "Anarosa A. F. Brandão",
        "Denis D. Mauá",
        "Fabio G. Cozman",
        "Sarajane M. Peres"
      ],
      "published": "2022-09-06T18:32:08Z",
      "updated": "2022-09-06T18:32:08Z",
      "categories": [
        "cs.AI",
        "cs.CL",
        "eess.SY"
      ],
      "pdf_url": "https://arxiv.org/pdf/2209.07928v1",
      "landing_url": "https://arxiv.org/abs/2209.07928v1",
      "doi": "https://doi.org/10.48550/arXiv.2209.07928"
    },
    "queries": [
      {
        "anchor": "synchronous dialogue",
        "search_term": "modular architecture"
      }
    ]
  },
  {
    "arxiv_id": "2209.08591",
    "anchor": "full-duplex",
    "search_term": "true full-duplex",
    "search_record": {
      "id": "http://arxiv.org/abs/2209.08591v2",
      "title": "Resource Allocation of STAR-RIS Assisted Full-Duplex Systems",
      "summary": "Well-designed simultaneously transmitting and reflecting RIS (STAR-RIS), which extends the half-space coverage to full-space coverage, incurs wireless communication environments to be smart and reconfigurable. In this paper, we survey how STAR-RIS affects the performance of full-duplex communication systems with the presence of full-duplex users, wherein the base station (BS) and the uplink users are subject to maximum transmission power constraints. Firstly, the weighted sum-rate (WSR) is derived as a system performance metric. Then, we formulate the resource allocation design into an equivalent weighted minimum mean-square-error form and then transform it into several convex sub-problems to maximize the WSR as an optimization problem which jointly optimizes the beamforming and the combining vectors at the BS, the transmit powers of the uplink users, and phase shifts of STAR-RIS. Although the WSR optimization is non-convex, an efficient iterative alternating procedure is proposed to achieve a sub-optimal solution for the optimization problem. Secondly, the STAR-RIS's phase shifts are optimized via the successive convex approximation technique. Finally, numerical results are provided to explain how STAR-RIS improves the performance metric with the presence of full-duplex users.",
      "published": "2022-09-18T16:05:39Z"
    },
    "metadata": {
      "arxiv_id": "2209.08591",
      "title": "Resource Allocation of STAR-RIS Assisted Full-Duplex Systems",
      "summary": "Well-designed simultaneously transmitting and reflecting RIS (STAR-RIS), which extends the half-space coverage to full-space coverage, incurs wireless communication environments to be smart and reconfigurable. In this paper, we survey how STAR-RIS affects the performance of full-duplex communication systems with the presence of full-duplex users, wherein the base station (BS) and the uplink users are subject to maximum transmission power constraints. Firstly, the weighted sum-rate (WSR) is derived as a system performance metric. Then, we formulate the resource allocation design into an equivalent weighted minimum mean-square-error form and then transform it into several convex sub-problems to maximize the WSR as an optimization problem which jointly optimizes the beamforming and the combining vectors at the BS, the transmit powers of the uplink users, and phase shifts of STAR-RIS. Although the WSR optimization is non-convex, an efficient iterative alternating procedure is proposed to achieve a sub-optimal solution for the optimization problem. Secondly, the STAR-RIS's phase shifts are optimized via the successive convex approximation technique. Finally, numerical results are provided to explain how STAR-RIS improves the performance metric with the presence of full-duplex users.",
      "authors": [
        "Mohammad Reza Kavianinia",
        "Mohammad Javad Emadi"
      ],
      "published": "2022-09-18T16:05:39Z",
      "updated": "2022-10-16T17:11:08Z",
      "categories": [
        "cs.IT",
        "eess.SP"
      ],
      "pdf_url": "https://arxiv.org/pdf/2209.08591v2",
      "landing_url": "https://arxiv.org/abs/2209.08591v2",
      "doi": "https://doi.org/10.48550/arXiv.2209.08591"
    },
    "queries": [
      {
        "anchor": "full-duplex",
        "search_term": "true full-duplex"
      },
      {
        "anchor": "full-duplex",
        "search_term": "pseudo full-duplex"
      },
      {
        "anchor": "full-duplex",
        "search_term": "engineered synchronization"
      },
      {
        "anchor": "full-duplex",
        "search_term": "learned synchronization"
      },
      {
        "anchor": "full-duplex",
        "search_term": "modular architecture"
      },
      {
        "anchor": "full-duplex",
        "search_term": "streaming architecture"
      },
      {
        "anchor": "full-duplex",
        "search_term": "real-time interaction"
      },
      {
        "anchor": "full-duplex",
        "search_term": "low latency"
      },
      {
        "anchor": "full-duplex",
        "search_term": "synchronous data"
      },
      {
        "anchor": "full-duplex",
        "search_term": "data scarcity"
      },
      {
        "anchor": "full-duplex",
        "search_term": "multichannel dialogue"
      },
      {
        "anchor": "full-duplex",
        "search_term": "temporal dynamics"
      },
      {
        "anchor": "full-duplex",
        "search_term": "behavioral arbitration"
      },
      {
        "anchor": "full-duplex",
        "search_term": "semantic coherence"
      },
      {
        "anchor": "full-duplex",
        "search_term": "neural codec"
      },
      {
        "anchor": "full-duplex",
        "search_term": "discrete tokens"
      },
      {
        "anchor": "full-duplex",
        "search_term": "audio tokens"
      },
      {
        "anchor": "full-duplex",
        "search_term": "floor transfer offset"
      },
      {
        "anchor": "full-duplex",
        "search_term": "stop latency"
      },
      {
        "anchor": "full-duplex",
        "search_term": "interruption response delay"
      }
    ]
  },
  {
    "arxiv_id": "2209.14065",
    "anchor": "spoken language models",
    "search_term": "low latency",
    "search_record": {
      "id": "http://arxiv.org/abs/2209.14065v5",
      "title": "LL-GNN: Low Latency Graph Neural Networks on FPGAs for High Energy Physics",
      "summary": "This work presents a novel reconfigurable architecture for Low Latency Graph Neural Network (LL-GNN) designs for particle detectors, delivering unprecedented low latency performance. Incorporating FPGA-based GNNs into particle detectors presents a unique challenge since it requires sub-microsecond latency to deploy the networks for online event selection with a data rate of hundreds of terabytes per second in the Level-1 triggers at the CERN Large Hadron Collider experiments. This paper proposes a novel outer-product based matrix multiplication approach, which is enhanced by exploiting the structured adjacency matrix and a column-major data layout. Moreover, a fusion step is introduced to further reduce the end-to-end design latency by eliminating unnecessary boundaries. Furthermore, a GNN-specific algorithm-hardware co-design approach is presented which not only finds a design with a much better latency but also finds a high accuracy design under given latency constraints. To facilitate this, a customizable template for this low latency GNN hardware architecture has been designed and open-sourced, which enables the generation of low-latency FPGA designs with efficient resource utilization using a high-level synthesis tool. Evaluation results show that our FPGA implementation is up to 9.0 times faster and achieves up to 13.1 times higher power efficiency than a GPU implementation. Compared to the previous FPGA implementations, this work achieves 6.51 to 16.7 times lower latency. Moreover, the latency of our FPGA design is sufficiently low to enable deployment of GNNs in a sub-microsecond, real-time collider trigger system, enabling it to benefit from improved accuracy. The proposed LL-GNN design advances the next generation of trigger systems by enabling sophisticated algorithms to process experimental data efficiently.",
      "published": "2022-09-28T12:55:35Z"
    },
    "metadata": {
      "arxiv_id": "2209.14065",
      "title": "LL-GNN: Low Latency Graph Neural Networks on FPGAs for High Energy Physics",
      "summary": "This work presents a novel reconfigurable architecture for Low Latency Graph Neural Network (LL-GNN) designs for particle detectors, delivering unprecedented low latency performance. Incorporating FPGA-based GNNs into particle detectors presents a unique challenge since it requires sub-microsecond latency to deploy the networks for online event selection with a data rate of hundreds of terabytes per second in the Level-1 triggers at the CERN Large Hadron Collider experiments. This paper proposes a novel outer-product based matrix multiplication approach, which is enhanced by exploiting the structured adjacency matrix and a column-major data layout. Moreover, a fusion step is introduced to further reduce the end-to-end design latency by eliminating unnecessary boundaries. Furthermore, a GNN-specific algorithm-hardware co-design approach is presented which not only finds a design with a much better latency but also finds a high accuracy design under given latency constraints. To facilitate this, a customizable template for this low latency GNN hardware architecture has been designed and open-sourced, which enables the generation of low-latency FPGA designs with efficient resource utilization using a high-level synthesis tool. Evaluation results show that our FPGA implementation is up to 9.0 times faster and achieves up to 13.1 times higher power efficiency than a GPU implementation. Compared to the previous FPGA implementations, this work achieves 6.51 to 16.7 times lower latency. Moreover, the latency of our FPGA design is sufficiently low to enable deployment of GNNs in a sub-microsecond, real-time collider trigger system, enabling it to benefit from improved accuracy. The proposed LL-GNN design advances the next generation of trigger systems by enabling sophisticated algorithms to process experimental data efficiently.",
      "authors": [
        "Zhiqiang Que",
        "Hongxiang Fan",
        "Marcus Loo",
        "He Li",
        "Michaela Blott",
        "Maurizio Pierini",
        "Alexander Tapper",
        "Wayne Luk"
      ],
      "published": "2022-09-28T12:55:35Z",
      "updated": "2024-01-09T10:05:38Z",
      "categories": [
        "cs.AR",
        "cs.LG",
        "physics.ins-det"
      ],
      "pdf_url": "https://arxiv.org/pdf/2209.14065v5",
      "landing_url": "https://arxiv.org/abs/2209.14065v5",
      "doi": "https://doi.org/10.1145/3640464"
    },
    "queries": [
      {
        "anchor": "spoken language models",
        "search_term": "low latency"
      },
      {
        "anchor": "synchronous dialogue",
        "search_term": "low latency"
      },
      {
        "anchor": "turn-taking",
        "search_term": "low latency"
      }
    ]
  },
  {
    "arxiv_id": "2209.14329",
    "anchor": "spoken language models",
    "search_term": "modular architecture",
    "search_record": {
      "id": "http://arxiv.org/abs/2209.14329v3",
      "title": "Quantum LDPC Codes for Modular Architectures",
      "summary": "In efforts to scale the size of quantum computers, modularity plays a central role across most quantum computing technologies. In the light of fault tolerance, this necessitates designing quantum error-correcting codes that are compatible with the connectivity arising from the architectural layouts. In this paper, we aim to bridge this gap by giving a novel way to view and construct quantum LDPC codes tailored for modular architectures. We demonstrate that if the intra- and inter-modular qubit connectivity can be viewed as corresponding to some classical or quantum LDPC codes, then their hypergraph product code fully respects the architectural connectivity constraints. Finally, we show that relaxed connectivity constraints that allow twists of connections between modules pave a way to construct codes with better parameters.",
      "published": "2022-09-28T18:01:43Z"
    },
    "metadata": {
      "arxiv_id": "2209.14329",
      "title": "Quantum LDPC Codes for Modular Architectures",
      "summary": "In efforts to scale the size of quantum computers, modularity plays a central role across most quantum computing technologies. In the light of fault tolerance, this necessitates designing quantum error-correcting codes that are compatible with the connectivity arising from the architectural layouts. In this paper, we aim to bridge this gap by giving a novel way to view and construct quantum LDPC codes tailored for modular architectures. We demonstrate that if the intra- and inter-modular qubit connectivity can be viewed as corresponding to some classical or quantum LDPC codes, then their hypergraph product code fully respects the architectural connectivity constraints. Finally, we show that relaxed connectivity constraints that allow twists of connections between modules pave a way to construct codes with better parameters.",
      "authors": [
        "Armands Strikis",
        "Lucas Berent"
      ],
      "published": "2022-09-28T18:01:43Z",
      "updated": "2023-05-15T10:36:17Z",
      "categories": [
        "quant-ph"
      ],
      "pdf_url": "https://arxiv.org/pdf/2209.14329v3",
      "landing_url": "https://arxiv.org/abs/2209.14329v3",
      "doi": "https://doi.org/10.1103/PRXQuantum.4.020321"
    },
    "queries": [
      {
        "anchor": "spoken language models",
        "search_term": "modular architecture"
      },
      {
        "anchor": "synchronous dialogue",
        "search_term": "modular architecture"
      },
      {
        "anchor": "turn-taking",
        "search_term": "modular architecture"
      }
    ]
  },
  {
    "arxiv_id": "2209.15483",
    "anchor": "spoken language models",
    "search_term": "true full-duplex",
    "search_record": {
      "id": "http://arxiv.org/abs/2209.15483v2",
      "title": "Augmentation Invariant Discrete Representation for Generative Spoken Language Modeling",
      "summary": "Generative Spoken Language Modeling research focuses on optimizing speech Language Models (LMs) using raw audio recordings without accessing any textual supervision. Such speech LMs usually operate over discrete units obtained from quantizing internal representations of self-supervised models. Although such units show impressive modeling results, their robustness capabilities have not been extensively investigated. This work focuses on improving the robustness of discrete input representations for generative spoken language modeling. First, we formally define how to measure the robustness of such representations to various signal variations that do not alter the spoken information (e.g., time-stretch). Next, we empirically demonstrate how current state-of-the-art representation models lack robustness to such variations. To overcome this, we propose an effective and efficient method to learn robust discrete speech representation for generative spoken language modeling. The proposed approach is based on applying a set of signal transformations to the speech signal and optimizing the model using an iterative pseudo-labeling scheme. Our method significantly improves over the evaluated baselines when considering encoding and modeling metrics. We additionally evaluate our method on the speech-to-speech translation task, considering Spanish-English and French-English translations, and show the proposed approach outperforms the evaluated baselines.",
      "published": "2022-09-30T14:15:03Z"
    },
    "metadata": {
      "arxiv_id": "2209.15483",
      "title": "Augmentation Invariant Discrete Representation for Generative Spoken Language Modeling",
      "summary": "Generative Spoken Language Modeling research focuses on optimizing speech Language Models (LMs) using raw audio recordings without accessing any textual supervision. Such speech LMs usually operate over discrete units obtained from quantizing internal representations of self-supervised models. Although such units show impressive modeling results, their robustness capabilities have not been extensively investigated. This work focuses on improving the robustness of discrete input representations for generative spoken language modeling. First, we formally define how to measure the robustness of such representations to various signal variations that do not alter the spoken information (e.g., time-stretch). Next, we empirically demonstrate how current state-of-the-art representation models lack robustness to such variations. To overcome this, we propose an effective and efficient method to learn robust discrete speech representation for generative spoken language modeling. The proposed approach is based on applying a set of signal transformations to the speech signal and optimizing the model using an iterative pseudo-labeling scheme. Our method significantly improves over the evaluated baselines when considering encoding and modeling metrics. We additionally evaluate our method on the speech-to-speech translation task, considering Spanish-English and French-English translations, and show the proposed approach outperforms the evaluated baselines.",
      "authors": [
        "Itai Gat",
        "Felix Kreuk",
        "Tu Anh Nguyen",
        "Ann Lee",
        "Jade Copet",
        "Gabriel Synnaeve",
        "Emmanuel Dupoux",
        "Yossi Adi"
      ],
      "published": "2022-09-30T14:15:03Z",
      "updated": "2023-05-29T10:50:29Z",
      "categories": [
        "cs.CL",
        "cs.LG",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2209.15483v2",
      "landing_url": "https://arxiv.org/abs/2209.15483v2",
      "doi": "https://doi.org/10.48550/arXiv.2209.15483"
    },
    "queries": [
      {
        "anchor": "spoken language models",
        "search_term": "true full-duplex"
      },
      {
        "anchor": "spoken language models",
        "search_term": "pseudo full-duplex"
      },
      {
        "anchor": "spoken language models",
        "search_term": "half duplex"
      },
      {
        "anchor": "spoken language models",
        "search_term": "engineered synchronization"
      },
      {
        "anchor": "spoken language models",
        "search_term": "learned synchronization"
      },
      {
        "anchor": "spoken language models",
        "search_term": "modular architecture"
      },
      {
        "anchor": "spoken language models",
        "search_term": "streaming architecture"
      },
      {
        "anchor": "spoken language models",
        "search_term": "real-time interaction"
      },
      {
        "anchor": "spoken language models",
        "search_term": "low latency"
      },
      {
        "anchor": "spoken language models",
        "search_term": "synchronous data"
      },
      {
        "anchor": "spoken language models",
        "search_term": "data scarcity"
      },
      {
        "anchor": "spoken language models",
        "search_term": "multichannel dialogue"
      },
      {
        "anchor": "spoken language models",
        "search_term": "temporal dynamics"
      },
      {
        "anchor": "spoken language models",
        "search_term": "behavioral arbitration"
      },
      {
        "anchor": "spoken language models",
        "search_term": "semantic coherence"
      },
      {
        "anchor": "spoken language models",
        "search_term": "neural codec"
      },
      {
        "anchor": "spoken language models",
        "search_term": "discrete tokens"
      },
      {
        "anchor": "spoken language models",
        "search_term": "audio tokens"
      },
      {
        "anchor": "spoken language models",
        "search_term": "floor transfer offset"
      },
      {
        "anchor": "spoken language models",
        "search_term": "stop latency"
      },
      {
        "anchor": "spoken language models",
        "search_term": "interruption response delay"
      }
    ]
  },
  {
    "arxiv_id": "2210.02866",
    "anchor": "turn-taking",
    "search_term": "true full-duplex",
    "search_record": {
      "id": "http://arxiv.org/abs/2210.02866v1",
      "title": "Knowing Where to Look: A Planning-based Architecture to Automate the Gaze Behavior of Social Robots",
      "summary": "Gaze cues play an important role in human communication and are used to coordinate turn-taking and joint attention, as well as to regulate intimacy. In order to have fluent conversations with people, social robots need to exhibit human-like gaze behavior. Previous Gaze Control Systems (GCS) in HRI have automated robot gaze using data-driven or heuristic approaches. However, these systems tend to be mainly reactive in nature. Planning the robot gaze ahead of time could help in achieving more realistic gaze behavior and better eye-head coordination. In this paper, we propose and implement a novel planning-based GCS. We evaluate our system in a comparative within-subjects user study (N=26) between a reactive system and our proposed system. The results show that the users preferred the proposed system and that it was significantly more interpretable and better at regulating intimacy.",
      "published": "2022-10-06T12:37:10Z"
    },
    "metadata": {
      "arxiv_id": "2210.02866",
      "title": "Knowing Where to Look: A Planning-based Architecture to Automate the Gaze Behavior of Social Robots",
      "summary": "Gaze cues play an important role in human communication and are used to coordinate turn-taking and joint attention, as well as to regulate intimacy. In order to have fluent conversations with people, social robots need to exhibit human-like gaze behavior. Previous Gaze Control Systems (GCS) in HRI have automated robot gaze using data-driven or heuristic approaches. However, these systems tend to be mainly reactive in nature. Planning the robot gaze ahead of time could help in achieving more realistic gaze behavior and better eye-head coordination. In this paper, we propose and implement a novel planning-based GCS. We evaluate our system in a comparative within-subjects user study (N=26) between a reactive system and our proposed system. The results show that the users preferred the proposed system and that it was significantly more interpretable and better at regulating intimacy.",
      "authors": [
        "Chinmaya Mishra",
        "Gabriel Skantze"
      ],
      "published": "2022-10-06T12:37:10Z",
      "updated": "2022-10-06T12:37:10Z",
      "categories": [
        "cs.RO",
        "cs.HC"
      ],
      "pdf_url": "https://arxiv.org/pdf/2210.02866v1",
      "landing_url": "https://arxiv.org/abs/2210.02866v1",
      "doi": "https://doi.org/10.1109/RO-MAN53752.2022.9900740"
    },
    "queries": [
      {
        "anchor": "turn-taking",
        "search_term": "true full-duplex"
      },
      {
        "anchor": "turn-taking",
        "search_term": "pseudo full-duplex"
      },
      {
        "anchor": "turn-taking",
        "search_term": "engineered synchronization"
      },
      {
        "anchor": "turn-taking",
        "search_term": "learned synchronization"
      },
      {
        "anchor": "turn-taking",
        "search_term": "synchronous data"
      },
      {
        "anchor": "turn-taking",
        "search_term": "multichannel dialogue"
      },
      {
        "anchor": "turn-taking",
        "search_term": "behavioral arbitration"
      },
      {
        "anchor": "turn-taking",
        "search_term": "floor transfer offset"
      },
      {
        "anchor": "turn-taking",
        "search_term": "stop latency"
      },
      {
        "anchor": "turn-taking",
        "search_term": "interruption response delay"
      }
    ]
  },
  {
    "arxiv_id": "2210.02913",
    "anchor": "synchronous dialogue",
    "search_term": "modular architecture",
    "search_record": {
      "id": "http://arxiv.org/abs/2210.02913v1",
      "title": "Biological neurons act as generalization filters in reservoir computing",
      "summary": "Reservoir computing is a machine learning paradigm that transforms the transient dynamics of high-dimensional nonlinear systems for processing time-series data. Although reservoir computing was initially proposed to model information processing in the mammalian cortex, it remains unclear how the non-random network architecture, such as the modular architecture, in the cortex integrates with the biophysics of living neurons to characterize the function of biological neuronal networks (BNNs). Here, we used optogenetics and fluorescent calcium imaging to record the multicellular responses of cultured BNNs and employed the reservoir computing framework to decode their computational capabilities. Micropatterned substrates were used to embed the modular architecture in the BNNs. We first show that modular BNNs can be used to classify static input patterns with a linear decoder and that the modularity of the BNNs positively correlates with the classification accuracy. We then used a timer task to verify that BNNs possess a short-term memory of ~1 s and finally show that this property can be exploited for spoken digit classification. Interestingly, BNN-based reservoirs allow transfer learning, wherein a network trained on one dataset can be used to classify separate datasets of the same category. Such classification was not possible when the input patterns were directly decoded by a linear decoder, suggesting that BNNs act as a generalization filter to improve reservoir computing performance. Our findings pave the way toward a mechanistic understanding of information processing within BNNs and, simultaneously, build future expectations toward the realization of physical reservoir computing systems based on BNNs.",
      "published": "2022-10-06T13:32:26Z"
    },
    "metadata": {
      "arxiv_id": "2210.02913",
      "title": "Biological neurons act as generalization filters in reservoir computing",
      "summary": "Reservoir computing is a machine learning paradigm that transforms the transient dynamics of high-dimensional nonlinear systems for processing time-series data. Although reservoir computing was initially proposed to model information processing in the mammalian cortex, it remains unclear how the non-random network architecture, such as the modular architecture, in the cortex integrates with the biophysics of living neurons to characterize the function of biological neuronal networks (BNNs). Here, we used optogenetics and fluorescent calcium imaging to record the multicellular responses of cultured BNNs and employed the reservoir computing framework to decode their computational capabilities. Micropatterned substrates were used to embed the modular architecture in the BNNs. We first show that modular BNNs can be used to classify static input patterns with a linear decoder and that the modularity of the BNNs positively correlates with the classification accuracy. We then used a timer task to verify that BNNs possess a short-term memory of ~1 s and finally show that this property can be exploited for spoken digit classification. Interestingly, BNN-based reservoirs allow transfer learning, wherein a network trained on one dataset can be used to classify separate datasets of the same category. Such classification was not possible when the input patterns were directly decoded by a linear decoder, suggesting that BNNs act as a generalization filter to improve reservoir computing performance. Our findings pave the way toward a mechanistic understanding of information processing within BNNs and, simultaneously, build future expectations toward the realization of physical reservoir computing systems based on BNNs.",
      "authors": [
        "Takuma Sumi",
        "Hideaki Yamamoto",
        "Yuichi Katori",
        "Satoshi Moriya",
        "Tomohiro Konno",
        "Shigeo Sato",
        "Ayumi Hirano-Iwata"
      ],
      "published": "2022-10-06T13:32:26Z",
      "updated": "2022-10-06T13:32:26Z",
      "categories": [
        "q-bio.NC",
        "stat.ML"
      ],
      "pdf_url": "https://arxiv.org/pdf/2210.02913v1",
      "landing_url": "https://arxiv.org/abs/2210.02913v1",
      "doi": "https://doi.org/10.1073/pnas.2217008120"
    },
    "queries": [
      {
        "anchor": "synchronous dialogue",
        "search_term": "modular architecture"
      }
    ]
  },
  {
    "arxiv_id": "2210.06067",
    "anchor": "synchronous dialogue",
    "search_term": "low latency",
    "search_record": {
      "id": "http://arxiv.org/abs/2210.06067v1",
      "title": "Low-Latency Analog-to-Analog Signal Processing using PC Hardware and USRPs",
      "summary": "In this paper, we implement a low-latency rapid-prototyping platform for signal processing based on software-defined radios (SDRs) and off-the-shelf PC hardware. This platform allows to evaluate a wide variety of algorithms in real-time environments, supporting new developments in the fields of classical, AI-based, and hybrid signal processing. To accomplish this, the streaming protocol of the used USRP X310 devices is implemented using the Data Plane Development Kit (DPDK), which allows to handle network communication in userspace only. This bypasses the kernel and thus avoids the latencies caused by interrupt handling, scheduling, and context switches. It allows signal processing to be performed on isolated processor cores that are protected from interrupts to a great extent. To validate our approach, linear time-invariant channel emulation has been implemented. For this, an analog-to-analog latency of 31 microseconds was achieved, demonstrating that our PC-based approach enables the implementation of rapid-prototyping systems with low latency.",
      "published": "2022-10-12T10:17:08Z"
    },
    "metadata": {
      "arxiv_id": "2210.06067",
      "title": "Low-Latency Analog-to-Analog Signal Processing using PC Hardware and USRPs",
      "summary": "In this paper, we implement a low-latency rapid-prototyping platform for signal processing based on software-defined radios (SDRs) and off-the-shelf PC hardware. This platform allows to evaluate a wide variety of algorithms in real-time environments, supporting new developments in the fields of classical, AI-based, and hybrid signal processing. To accomplish this, the streaming protocol of the used USRP X310 devices is implemented using the Data Plane Development Kit (DPDK), which allows to handle network communication in userspace only. This bypasses the kernel and thus avoids the latencies caused by interrupt handling, scheduling, and context switches. It allows signal processing to be performed on isolated processor cores that are protected from interrupts to a great extent. To validate our approach, linear time-invariant channel emulation has been implemented. For this, an analog-to-analog latency of 31 microseconds was achieved, demonstrating that our PC-based approach enables the implementation of rapid-prototyping systems with low latency.",
      "authors": [
        "Maximilian Engelhardt",
        "Carsten Andrich",
        "Alexander Ihlow",
        "Sebastian Giehl",
        "Giovanni Del Galdo"
      ],
      "published": "2022-10-12T10:17:08Z",
      "updated": "2022-10-12T10:17:08Z",
      "categories": [
        "eess.SP"
      ],
      "pdf_url": "https://arxiv.org/pdf/2210.06067v1",
      "landing_url": "https://arxiv.org/abs/2210.06067v1",
      "doi": "https://doi.org/10.48550/arXiv.2210.06067"
    },
    "queries": [
      {
        "anchor": "synchronous dialogue",
        "search_term": "low latency"
      }
    ]
  },
  {
    "arxiv_id": "2210.06878",
    "anchor": "synchronous dialogue",
    "search_term": "modular architecture",
    "search_record": {
      "id": "http://arxiv.org/abs/2210.06878v2",
      "title": "CS-Insights: A System for Analyzing Computer Science Research",
      "summary": "This paper presents CS-Insights, an interactive web application to analyze computer science publications from DBLP through multiple perspectives. The dedicated interfaces allow its users to identify trends in research activity, productivity, accessibility, author's productivity, venues' statistics, topics of interest, and the impact of computer science research on other fields. CS-Insightsis publicly available, and its modular architecture can be easily adapted to domains other than computer science.",
      "published": "2022-10-13T10:03:52Z"
    },
    "metadata": {
      "arxiv_id": "2210.06878",
      "title": "CS-Insights: A System for Analyzing Computer Science Research",
      "summary": "This paper presents CS-Insights, an interactive web application to analyze computer science publications from DBLP through multiple perspectives. The dedicated interfaces allow its users to identify trends in research activity, productivity, accessibility, author's productivity, venues' statistics, topics of interest, and the impact of computer science research on other fields. CS-Insightsis publicly available, and its modular architecture can be easily adapted to domains other than computer science.",
      "authors": [
        "Terry Ruas",
        "Jan Philip Wahle",
        "Lennart Küll",
        "Saif M. Mohammad",
        "Bela Gipp"
      ],
      "published": "2022-10-13T10:03:52Z",
      "updated": "2023-01-29T08:18:16Z",
      "categories": [
        "cs.CL",
        "cs.DL"
      ],
      "pdf_url": "https://arxiv.org/pdf/2210.06878v2",
      "landing_url": "https://arxiv.org/abs/2210.06878v2",
      "doi": "https://doi.org/10.48550/arXiv.2210.06878"
    },
    "queries": [
      {
        "anchor": "synchronous dialogue",
        "search_term": "modular architecture"
      }
    ]
  },
  {
    "arxiv_id": "2210.06974",
    "anchor": "spoken language models",
    "search_term": "real-time interaction",
    "search_record": {
      "id": "http://arxiv.org/abs/2210.06974v1",
      "title": "Evaluating Data-Driven Co-Speech Gestures of Embodied Conversational Agents through Real-Time Interaction",
      "summary": "Embodied Conversational Agents that make use of co-speech gestures can enhance human-machine interactions in many ways. In recent years, data-driven gesture generation approaches for ECAs have attracted considerable research attention, and related methods have continuously improved. Real-time interaction is typically used when researchers evaluate ECA systems that generate rule-based gestures. However, when evaluating the performance of ECAs based on data-driven methods, participants are often required only to watch pre-recorded videos, which cannot provide adequate information about what a person perceives during the interaction. To address this limitation, we explored use of real-time interaction to assess data-driven gesturing ECAs. We provided a testbed framework, and investigated whether gestures could affect human perception of ECAs in the dimensions of human-likeness, animacy, perceived intelligence, and focused attention. Our user study required participants to interact with two ECAs - one with and one without hand gestures. We collected subjective data from the participants' self-report questionnaires and objective data from a gaze tracker. To our knowledge, the current study represents the first attempt to evaluate data-driven gesturing ECAs through real-time interaction and the first experiment using gaze-tracking to examine the effect of ECAs' gestures.",
      "published": "2022-10-13T12:50:03Z"
    },
    "metadata": {
      "arxiv_id": "2210.06974",
      "title": "Evaluating Data-Driven Co-Speech Gestures of Embodied Conversational Agents through Real-Time Interaction",
      "summary": "Embodied Conversational Agents that make use of co-speech gestures can enhance human-machine interactions in many ways. In recent years, data-driven gesture generation approaches for ECAs have attracted considerable research attention, and related methods have continuously improved. Real-time interaction is typically used when researchers evaluate ECA systems that generate rule-based gestures. However, when evaluating the performance of ECAs based on data-driven methods, participants are often required only to watch pre-recorded videos, which cannot provide adequate information about what a person perceives during the interaction. To address this limitation, we explored use of real-time interaction to assess data-driven gesturing ECAs. We provided a testbed framework, and investigated whether gestures could affect human perception of ECAs in the dimensions of human-likeness, animacy, perceived intelligence, and focused attention. Our user study required participants to interact with two ECAs - one with and one without hand gestures. We collected subjective data from the participants' self-report questionnaires and objective data from a gaze tracker. To our knowledge, the current study represents the first attempt to evaluate data-driven gesturing ECAs through real-time interaction and the first experiment using gaze-tracking to examine the effect of ECAs' gestures.",
      "authors": [
        "Yuan He",
        "André Pereira",
        "Taras Kucherenko"
      ],
      "published": "2022-10-13T12:50:03Z",
      "updated": "2022-10-13T12:50:03Z",
      "categories": [
        "cs.HC"
      ],
      "pdf_url": "https://arxiv.org/pdf/2210.06974v1",
      "landing_url": "https://arxiv.org/abs/2210.06974v1",
      "doi": "https://doi.org/10.1145/3514197.3549697"
    },
    "queries": [
      {
        "anchor": "spoken language models",
        "search_term": "real-time interaction"
      },
      {
        "anchor": "synchronous dialogue",
        "search_term": "real-time interaction"
      },
      {
        "anchor": "turn-taking",
        "search_term": "real-time interaction"
      }
    ]
  },
  {
    "arxiv_id": "2210.08094",
    "anchor": "full-duplex",
    "search_term": "true full-duplex",
    "search_record": {
      "id": "http://arxiv.org/abs/2210.08094v1",
      "title": "Full-Duplex Transceivers for Next-Generation Wireless Communication Systems",
      "summary": "Wireless communication systems can be enhanced at the link level, in medium access, and at the network level when transceivers are equipped with full-duplex capability: the transformative ability to simultaneously transmit and receive over the same frequency spectrum. Effective methods to cancel self-interference are required to facilitate full-duplex operation, which we overview herein in the context of traditional radios, along with those in next-generation wireless networks. We highlight advances in self-interference cancellation that leverage machine learning, and we summarize key considerations and recent progress in full-duplex millimeter-wave systems and their application in integrated access and backhaul. We present example design problems and noteworthy findings from recent experimental research to introduce and motivate the advancement of full-duplex millimeter-wave systems. We conclude this chapter by forecasting the future of full-duplex and outlining important research directions that warrant further study.",
      "published": "2022-10-14T20:32:44Z"
    },
    "metadata": {
      "arxiv_id": "2210.08094",
      "title": "Full-Duplex Transceivers for Next-Generation Wireless Communication Systems",
      "summary": "Wireless communication systems can be enhanced at the link level, in medium access, and at the network level when transceivers are equipped with full-duplex capability: the transformative ability to simultaneously transmit and receive over the same frequency spectrum. Effective methods to cancel self-interference are required to facilitate full-duplex operation, which we overview herein in the context of traditional radios, along with those in next-generation wireless networks. We highlight advances in self-interference cancellation that leverage machine learning, and we summarize key considerations and recent progress in full-duplex millimeter-wave systems and their application in integrated access and backhaul. We present example design problems and noteworthy findings from recent experimental research to introduce and motivate the advancement of full-duplex millimeter-wave systems. We conclude this chapter by forecasting the future of full-duplex and outlining important research directions that warrant further study.",
      "authors": [
        "Ian P. Roberts",
        "Himal A. Suraweera"
      ],
      "published": "2022-10-14T20:32:44Z",
      "updated": "2022-10-14T20:32:44Z",
      "categories": [
        "eess.SP"
      ],
      "pdf_url": "https://arxiv.org/pdf/2210.08094v1",
      "landing_url": "https://arxiv.org/abs/2210.08094v1",
      "doi": "https://doi.org/10.48550/arXiv.2210.08094"
    },
    "queries": [
      {
        "anchor": "full-duplex",
        "search_term": "true full-duplex"
      },
      {
        "anchor": "full-duplex",
        "search_term": "pseudo full-duplex"
      },
      {
        "anchor": "full-duplex",
        "search_term": "half duplex"
      },
      {
        "anchor": "full-duplex",
        "search_term": "engineered synchronization"
      },
      {
        "anchor": "full-duplex",
        "search_term": "learned synchronization"
      },
      {
        "anchor": "full-duplex",
        "search_term": "modular architecture"
      },
      {
        "anchor": "full-duplex",
        "search_term": "streaming architecture"
      },
      {
        "anchor": "full-duplex",
        "search_term": "real-time interaction"
      },
      {
        "anchor": "full-duplex",
        "search_term": "low latency"
      },
      {
        "anchor": "full-duplex",
        "search_term": "synchronous data"
      },
      {
        "anchor": "full-duplex",
        "search_term": "data scarcity"
      },
      {
        "anchor": "full-duplex",
        "search_term": "multichannel dialogue"
      },
      {
        "anchor": "full-duplex",
        "search_term": "temporal dynamics"
      },
      {
        "anchor": "full-duplex",
        "search_term": "behavioral arbitration"
      },
      {
        "anchor": "full-duplex",
        "search_term": "semantic coherence"
      },
      {
        "anchor": "full-duplex",
        "search_term": "neural codec"
      },
      {
        "anchor": "full-duplex",
        "search_term": "discrete tokens"
      },
      {
        "anchor": "full-duplex",
        "search_term": "audio tokens"
      },
      {
        "anchor": "full-duplex",
        "search_term": "floor transfer offset"
      },
      {
        "anchor": "full-duplex",
        "search_term": "stop latency"
      },
      {
        "anchor": "full-duplex",
        "search_term": "interruption response delay"
      }
    ]
  },
  {
    "arxiv_id": "2210.08940",
    "anchor": "spoken language models",
    "search_term": "low latency",
    "search_record": {
      "id": "http://arxiv.org/abs/2210.08940v1",
      "title": "Configured Grant for Ultra-Reliable and Low-Latency Communications: Standardization and Beyond",
      "summary": "Uplink configured Grant allocation has been introduced in 3rd Generation Partnership Project New Radio Release 15. This is beneficial in supporting Ultra-Reliable and Low Latency Communication for industrial communication, a key Fifth Generation mobile communication usage scenario. This scheduling mechanism enables a user with periodic traffic to transmits its data readily and bypasses the control signaling entailed to scheduling requests and scheduling grants and provides low latency access. To facilitate ultra-reliable communication, the scheduling mechanism can allow users to transmit consecutive redundant transmissions in a pre-defined period. However, if the traffic is semi-deterministic, the current standardized configured grant allocation is not equipped to emulate the traffic as the configured grant's period is pre-configured and fixed. This article describes the recent advancements in the standardization process in Release 15 and 16 for configured grant allocation and the prospective solutions to accommodate semi-deterministic traffic behavior for configured grant allocations.",
      "published": "2022-10-17T11:06:44Z"
    },
    "metadata": {
      "arxiv_id": "2210.08940",
      "title": "Configured Grant for Ultra-Reliable and Low-Latency Communications: Standardization and Beyond",
      "summary": "Uplink configured Grant allocation has been introduced in 3rd Generation Partnership Project New Radio Release 15. This is beneficial in supporting Ultra-Reliable and Low Latency Communication for industrial communication, a key Fifth Generation mobile communication usage scenario. This scheduling mechanism enables a user with periodic traffic to transmits its data readily and bypasses the control signaling entailed to scheduling requests and scheduling grants and provides low latency access. To facilitate ultra-reliable communication, the scheduling mechanism can allow users to transmit consecutive redundant transmissions in a pre-defined period. However, if the traffic is semi-deterministic, the current standardized configured grant allocation is not equipped to emulate the traffic as the configured grant's period is pre-configured and fixed. This article describes the recent advancements in the standardization process in Release 15 and 16 for configured grant allocation and the prospective solutions to accommodate semi-deterministic traffic behavior for configured grant allocations.",
      "authors": [
        "Majid Gerami",
        "Bikramjit Singh"
      ],
      "published": "2022-10-17T11:06:44Z",
      "updated": "2022-10-17T11:06:44Z",
      "categories": [
        "cs.NI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2210.08940v1",
      "landing_url": "https://arxiv.org/abs/2210.08940v1",
      "doi": "https://doi.org/10.48550/arXiv.2210.08940"
    },
    "queries": [
      {
        "anchor": "spoken language models",
        "search_term": "low latency"
      },
      {
        "anchor": "synchronous dialogue",
        "search_term": "low latency"
      },
      {
        "anchor": "turn-taking",
        "search_term": "low latency"
      }
    ]
  },
  {
    "arxiv_id": "2210.09558",
    "anchor": "spoken language models",
    "search_term": "data scarcity",
    "search_record": {
      "id": "http://arxiv.org/abs/2210.09558v1",
      "title": "Bag of Tricks for Developing Diabetic Retinopathy Analysis Framework to Overcome Data Scarcity",
      "summary": "Recently, diabetic retinopathy (DR) screening utilizing ultra-wide optical coherence tomography angiography (UW-OCTA) has been used in clinical practices to detect signs of early DR. However, developing a deep learning-based DR analysis system using UW-OCTA images is not trivial due to the difficulty of data collection and the absence of public datasets. By realistic constraints, a model trained on small datasets may obtain sub-par performance. Therefore, to help ophthalmologists be less confused about models' incorrect decisions, the models should be robust even in data scarcity settings. To address the above practical challenging, we present a comprehensive empirical study for DR analysis tasks, including lesion segmentation, image quality assessment, and DR grading. For each task, we introduce a robust training scheme by leveraging ensemble learning, data augmentation, and semi-supervised learning. Furthermore, we propose reliable pseudo labeling that excludes uncertain pseudo-labels based on the model's confidence scores to reduce the negative effect of noisy pseudo-labels. By exploiting the proposed approaches, we achieved 1st place in the Diabetic Retinopathy Analysis Challenge.",
      "published": "2022-10-18T03:25:00Z"
    },
    "metadata": {
      "arxiv_id": "2210.09558",
      "title": "Bag of Tricks for Developing Diabetic Retinopathy Analysis Framework to Overcome Data Scarcity",
      "summary": "Recently, diabetic retinopathy (DR) screening utilizing ultra-wide optical coherence tomography angiography (UW-OCTA) has been used in clinical practices to detect signs of early DR. However, developing a deep learning-based DR analysis system using UW-OCTA images is not trivial due to the difficulty of data collection and the absence of public datasets. By realistic constraints, a model trained on small datasets may obtain sub-par performance. Therefore, to help ophthalmologists be less confused about models' incorrect decisions, the models should be robust even in data scarcity settings. To address the above practical challenging, we present a comprehensive empirical study for DR analysis tasks, including lesion segmentation, image quality assessment, and DR grading. For each task, we introduce a robust training scheme by leveraging ensemble learning, data augmentation, and semi-supervised learning. Furthermore, we propose reliable pseudo labeling that excludes uncertain pseudo-labels based on the model's confidence scores to reduce the negative effect of noisy pseudo-labels. By exploiting the proposed approaches, we achieved 1st place in the Diabetic Retinopathy Analysis Challenge.",
      "authors": [
        "Gitaek Kwon",
        "Eunjin Kim",
        "Sunho Kim",
        "Seongwon Bak",
        "Minsung Kim",
        "Jaeyoung Kim"
      ],
      "published": "2022-10-18T03:25:00Z",
      "updated": "2022-10-18T03:25:00Z",
      "categories": [
        "eess.IV",
        "cs.CV",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2210.09558v1",
      "landing_url": "https://arxiv.org/abs/2210.09558v1",
      "doi": "https://doi.org/10.48550/arXiv.2210.09558"
    },
    "queries": [
      {
        "anchor": "spoken language models",
        "search_term": "data scarcity"
      },
      {
        "anchor": "synchronous dialogue",
        "search_term": "data scarcity"
      },
      {
        "anchor": "turn-taking",
        "search_term": "data scarcity"
      }
    ]
  },
  {
    "arxiv_id": "2210.10343",
    "anchor": "synchronous dialogue",
    "search_term": "semantic coherence",
    "search_record": {
      "id": "http://arxiv.org/abs/2210.10343v2",
      "title": "Entity-to-Text based Data Augmentation for various Named Entity Recognition Tasks",
      "summary": "Data augmentation techniques have been used to alleviate the problem of scarce labeled data in various NER tasks (flat, nested, and discontinuous NER tasks). Existing augmentation techniques either manipulate the words in the original text that break the semantic coherence of the text, or exploit generative models that ignore preserving entities in the original text, which impedes the use of augmentation techniques on nested and discontinuous NER tasks. In this work, we propose a novel Entity-to-Text based data augmentation technique named EnTDA to add, delete, replace or swap entities in the entity list of the original texts, and adopt these augmented entity lists to generate semantically coherent and entity preserving texts for various NER tasks. Furthermore, we introduce a diversity beam search to increase the diversity during the text generation process. Experiments on thirteen NER datasets across three tasks (flat, nested, and discontinuous NER tasks) and two settings (full data and low resource settings) show that EnTDA could bring more performance improvements compared to the baseline augmentation techniques.",
      "published": "2022-10-19T07:24:40Z"
    },
    "metadata": {
      "arxiv_id": "2210.10343",
      "title": "Entity-to-Text based Data Augmentation for various Named Entity Recognition Tasks",
      "summary": "Data augmentation techniques have been used to alleviate the problem of scarce labeled data in various NER tasks (flat, nested, and discontinuous NER tasks). Existing augmentation techniques either manipulate the words in the original text that break the semantic coherence of the text, or exploit generative models that ignore preserving entities in the original text, which impedes the use of augmentation techniques on nested and discontinuous NER tasks. In this work, we propose a novel Entity-to-Text based data augmentation technique named EnTDA to add, delete, replace or swap entities in the entity list of the original texts, and adopt these augmented entity lists to generate semantically coherent and entity preserving texts for various NER tasks. Furthermore, we introduce a diversity beam search to increase the diversity during the text generation process. Experiments on thirteen NER datasets across three tasks (flat, nested, and discontinuous NER tasks) and two settings (full data and low resource settings) show that EnTDA could bring more performance improvements compared to the baseline augmentation techniques.",
      "authors": [
        "Xuming Hu",
        "Yong Jiang",
        "Aiwei Liu",
        "Zhongqiang Huang",
        "Pengjun Xie",
        "Fei Huang",
        "Lijie Wen",
        "Philip S. Yu"
      ],
      "published": "2022-10-19T07:24:40Z",
      "updated": "2023-05-26T16:14:43Z",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2210.10343v2",
      "landing_url": "https://arxiv.org/abs/2210.10343v2",
      "doi": "https://doi.org/10.48550/arXiv.2210.10343"
    },
    "queries": [
      {
        "anchor": "synchronous dialogue",
        "search_term": "semantic coherence"
      }
    ]
  },
  {
    "arxiv_id": "2210.14112",
    "anchor": "full-duplex",
    "search_term": "true full-duplex",
    "search_record": {
      "id": "http://arxiv.org/abs/2210.14112v2",
      "title": "Bidirectional Integrated Sensing and Communication: Full-Duplex or Half-Duplex?",
      "summary": "A bidirectional integrated sensing and communication (ISAC) system is proposed, in which a pair of transceivers carry out two-way communication and mutual sensing. Both full-duplex and half-duplex operations in narrowband and wideband systems are conceived for the bidirectional ISAC. 1) For the narrowband system, the conventional full-duplex and half-duplex operations are redesigned to take into account sensing echo signals. Then, the transmit beamforming design of both transceivers is proposed for addressing the sensing and communication (S&C) tradeoff. A one-layer iterative algorithm relying on successive convex approximation (SCA) is proposed to obtain Karush-Kuhn-Tucker (KKT) optimal solutions. 2) For the wideband system, the new full-duplex and half-duplex operations are proposed for the bidirectional ISAC. In particular, the frequency-selective fading channel is tackled by delay pre-compensation and path-based beamforming. By redesigning the proposed SCA-based algorithm, the KKT optimal solutions for path-based beamforming for characterizing the S&C tradeoff are obtained. Finally, the numerical results show that: i) For both bandwidth scenarios, the existence of the interference introduced by sensing results in full-duplex may not always outperform half-duplex, especially in the sensing-prior regime or when the communication channel is line-of-sight-dominated; and ii) For both duplex operations, it is sufficient to reuse communication signals for sensing in the narrowband system, while an additional dedicated sensing signal is required in the wideband system.",
      "published": "2022-10-25T15:59:06Z"
    },
    "metadata": {
      "arxiv_id": "2210.14112",
      "title": "Bidirectional Integrated Sensing and Communication: Full-Duplex or Half-Duplex?",
      "summary": "A bidirectional integrated sensing and communication (ISAC) system is proposed, in which a pair of transceivers carry out two-way communication and mutual sensing. Both full-duplex and half-duplex operations in narrowband and wideband systems are conceived for the bidirectional ISAC. 1) For the narrowband system, the conventional full-duplex and half-duplex operations are redesigned to take into account sensing echo signals. Then, the transmit beamforming design of both transceivers is proposed for addressing the sensing and communication (S&C) tradeoff. A one-layer iterative algorithm relying on successive convex approximation (SCA) is proposed to obtain Karush-Kuhn-Tucker (KKT) optimal solutions. 2) For the wideband system, the new full-duplex and half-duplex operations are proposed for the bidirectional ISAC. In particular, the frequency-selective fading channel is tackled by delay pre-compensation and path-based beamforming. By redesigning the proposed SCA-based algorithm, the KKT optimal solutions for path-based beamforming for characterizing the S&C tradeoff are obtained. Finally, the numerical results show that: i) For both bandwidth scenarios, the existence of the interference introduced by sensing results in full-duplex may not always outperform half-duplex, especially in the sensing-prior regime or when the communication channel is line-of-sight-dominated; and ii) For both duplex operations, it is sufficient to reuse communication signals for sensing in the narrowband system, while an additional dedicated sensing signal is required in the wideband system.",
      "authors": [
        "Zhaolin Wang",
        "Xidong Mu",
        "Yuanwei Liu"
      ],
      "published": "2022-10-25T15:59:06Z",
      "updated": "2024-01-03T20:45:35Z",
      "categories": [
        "cs.IT"
      ],
      "pdf_url": "https://arxiv.org/pdf/2210.14112v2",
      "landing_url": "https://arxiv.org/abs/2210.14112v2",
      "doi": "https://doi.org/10.1109/TWC.2023.3344229"
    },
    "queries": [
      {
        "anchor": "full-duplex",
        "search_term": "true full-duplex"
      },
      {
        "anchor": "full-duplex",
        "search_term": "pseudo full-duplex"
      },
      {
        "anchor": "full-duplex",
        "search_term": "half duplex"
      },
      {
        "anchor": "full-duplex",
        "search_term": "engineered synchronization"
      },
      {
        "anchor": "full-duplex",
        "search_term": "learned synchronization"
      },
      {
        "anchor": "full-duplex",
        "search_term": "modular architecture"
      },
      {
        "anchor": "full-duplex",
        "search_term": "streaming architecture"
      },
      {
        "anchor": "full-duplex",
        "search_term": "real-time interaction"
      },
      {
        "anchor": "full-duplex",
        "search_term": "low latency"
      },
      {
        "anchor": "full-duplex",
        "search_term": "synchronous data"
      },
      {
        "anchor": "full-duplex",
        "search_term": "data scarcity"
      },
      {
        "anchor": "full-duplex",
        "search_term": "multichannel dialogue"
      },
      {
        "anchor": "full-duplex",
        "search_term": "temporal dynamics"
      },
      {
        "anchor": "full-duplex",
        "search_term": "behavioral arbitration"
      },
      {
        "anchor": "full-duplex",
        "search_term": "semantic coherence"
      },
      {
        "anchor": "full-duplex",
        "search_term": "neural codec"
      },
      {
        "anchor": "full-duplex",
        "search_term": "discrete tokens"
      },
      {
        "anchor": "full-duplex",
        "search_term": "audio tokens"
      },
      {
        "anchor": "full-duplex",
        "search_term": "floor transfer offset"
      },
      {
        "anchor": "full-duplex",
        "search_term": "stop latency"
      },
      {
        "anchor": "full-duplex",
        "search_term": "interruption response delay"
      },
      {
        "anchor": "spoken language models",
        "search_term": "half duplex"
      },
      {
        "anchor": "synchronous dialogue",
        "search_term": "half duplex"
      },
      {
        "anchor": "turn-taking",
        "search_term": "half duplex"
      }
    ]
  },
  {
    "arxiv_id": "2210.15452",
    "anchor": "synchronous dialogue",
    "search_term": "data scarcity",
    "search_record": {
      "id": "http://arxiv.org/abs/2210.15452v1",
      "title": "Exploring Predictive Uncertainty and Calibration in NLP: A Study on the Impact of Method & Data Scarcity",
      "summary": "We investigate the problem of determining the predictive confidence (or, conversely, uncertainty) of a neural classifier through the lens of low-resource languages. By training models on sub-sampled datasets in three different languages, we assess the quality of estimates from a wide array of approaches and their dependence on the amount of available data. We find that while approaches based on pre-trained models and ensembles achieve the best results overall, the quality of uncertainty estimates can surprisingly suffer with more data. We also perform a qualitative analysis of uncertainties on sequences, discovering that a model's total uncertainty seems to be influenced to a large degree by its data uncertainty, not model uncertainty. All model implementations are open-sourced in a software package.",
      "published": "2022-10-20T15:42:02Z"
    },
    "metadata": {
      "arxiv_id": "2210.15452",
      "title": "Exploring Predictive Uncertainty and Calibration in NLP: A Study on the Impact of Method & Data Scarcity",
      "summary": "We investigate the problem of determining the predictive confidence (or, conversely, uncertainty) of a neural classifier through the lens of low-resource languages. By training models on sub-sampled datasets in three different languages, we assess the quality of estimates from a wide array of approaches and their dependence on the amount of available data. We find that while approaches based on pre-trained models and ensembles achieve the best results overall, the quality of uncertainty estimates can surprisingly suffer with more data. We also perform a qualitative analysis of uncertainties on sequences, discovering that a model's total uncertainty seems to be influenced to a large degree by its data uncertainty, not model uncertainty. All model implementations are open-sourced in a software package.",
      "authors": [
        "Dennis Ulmer",
        "Jes Frellsen",
        "Christian Hardmeier"
      ],
      "published": "2022-10-20T15:42:02Z",
      "updated": "2022-10-20T15:42:02Z",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2210.15452v1",
      "landing_url": "https://arxiv.org/abs/2210.15452v1",
      "doi": "https://doi.org/10.48550/arXiv.2210.15452"
    },
    "queries": [
      {
        "anchor": "synchronous dialogue",
        "search_term": "data scarcity"
      }
    ]
  },
  {
    "arxiv_id": "2210.15715",
    "anchor": "synchronous dialogue",
    "search_term": "discrete tokens",
    "search_record": {
      "id": "http://arxiv.org/abs/2210.15715v2",
      "title": "Simulating realistic speech overlaps improves multi-talker ASR",
      "summary": "Multi-talker automatic speech recognition (ASR) has been studied to generate transcriptions of natural conversation including overlapping speech of multiple speakers. Due to the difficulty in acquiring real conversation data with high-quality human transcriptions, a naïve simulation of multi-talker speech by randomly mixing multiple utterances was conventionally used for model training. In this work, we propose an improved technique to simulate multi-talker overlapping speech with realistic speech overlaps, where an arbitrary pattern of speech overlaps is represented by a sequence of discrete tokens. With this representation, speech overlapping patterns can be learned from real conversations based on a statistical language model, such as N-gram, which can be then used to generate multi-talker speech for training. In our experiments, multi-talker ASR models trained with the proposed method show consistent improvement on the word error rates across multiple datasets.",
      "published": "2022-10-27T18:29:39Z"
    },
    "metadata": {
      "arxiv_id": "2210.15715",
      "title": "Simulating realistic speech overlaps improves multi-talker ASR",
      "summary": "Multi-talker automatic speech recognition (ASR) has been studied to generate transcriptions of natural conversation including overlapping speech of multiple speakers. Due to the difficulty in acquiring real conversation data with high-quality human transcriptions, a naïve simulation of multi-talker speech by randomly mixing multiple utterances was conventionally used for model training. In this work, we propose an improved technique to simulate multi-talker overlapping speech with realistic speech overlaps, where an arbitrary pattern of speech overlaps is represented by a sequence of discrete tokens. With this representation, speech overlapping patterns can be learned from real conversations based on a statistical language model, such as N-gram, which can be then used to generate multi-talker speech for training. In our experiments, multi-talker ASR models trained with the proposed method show consistent improvement on the word error rates across multiple datasets.",
      "authors": [
        "Muqiao Yang",
        "Naoyuki Kanda",
        "Xiaofei Wang",
        "Jian Wu",
        "Sunit Sivasankaran",
        "Zhuo Chen",
        "Jinyu Li",
        "Takuya Yoshioka"
      ],
      "published": "2022-10-27T18:29:39Z",
      "updated": "2022-11-17T19:21:48Z",
      "categories": [
        "eess.AS",
        "cs.CL",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2210.15715v2",
      "landing_url": "https://arxiv.org/abs/2210.15715v2",
      "doi": "https://doi.org/10.48550/arXiv.2210.15715"
    },
    "queries": [
      {
        "anchor": "synchronous dialogue",
        "search_term": "discrete tokens"
      }
    ]
  },
  {
    "arxiv_id": "2210.15759",
    "anchor": "spoken language models",
    "search_term": "true full-duplex",
    "search_record": {
      "id": "http://arxiv.org/abs/2210.15759v1",
      "title": "Self-supervised language learning from raw audio: Lessons from the Zero Resource Speech Challenge",
      "summary": "Recent progress in self-supervised or unsupervised machine learning has opened the possibility of building a full speech processing system from raw audio without using any textual representations or expert labels such as phonemes, dictionaries or parse trees. The contribution of the Zero Resource Speech Challenge series since 2015 has been to break down this long-term objective into four well-defined tasks -- Acoustic Unit Discovery, Spoken Term Discovery, Discrete Resynthesis, and Spoken Language Modeling -- and introduce associated metrics and benchmarks enabling model comparison and cumulative progress. We present an overview of the six editions of this challenge series since 2015, discuss the lessons learned, and outline the areas which need more work or give puzzling results.",
      "published": "2022-10-27T20:32:41Z"
    },
    "metadata": {
      "arxiv_id": "2210.15759",
      "title": "Self-supervised language learning from raw audio: Lessons from the Zero Resource Speech Challenge",
      "summary": "Recent progress in self-supervised or unsupervised machine learning has opened the possibility of building a full speech processing system from raw audio without using any textual representations or expert labels such as phonemes, dictionaries or parse trees. The contribution of the Zero Resource Speech Challenge series since 2015 has been to break down this long-term objective into four well-defined tasks -- Acoustic Unit Discovery, Spoken Term Discovery, Discrete Resynthesis, and Spoken Language Modeling -- and introduce associated metrics and benchmarks enabling model comparison and cumulative progress. We present an overview of the six editions of this challenge series since 2015, discuss the lessons learned, and outline the areas which need more work or give puzzling results.",
      "authors": [
        "Ewan Dunbar",
        "Nicolas Hamilakis",
        "Emmanuel Dupoux"
      ],
      "published": "2022-10-27T20:32:41Z",
      "updated": "2022-10-27T20:32:41Z",
      "categories": [
        "cs.CL",
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2210.15759v1",
      "landing_url": "https://arxiv.org/abs/2210.15759v1",
      "doi": "https://doi.org/10.1109/JSTSP.2022.3206084"
    },
    "queries": [
      {
        "anchor": "spoken language models",
        "search_term": "true full-duplex"
      },
      {
        "anchor": "spoken language models",
        "search_term": "pseudo full-duplex"
      },
      {
        "anchor": "spoken language models",
        "search_term": "engineered synchronization"
      },
      {
        "anchor": "spoken language models",
        "search_term": "learned synchronization"
      },
      {
        "anchor": "spoken language models",
        "search_term": "streaming architecture"
      },
      {
        "anchor": "spoken language models",
        "search_term": "real-time interaction"
      },
      {
        "anchor": "spoken language models",
        "search_term": "synchronous data"
      },
      {
        "anchor": "spoken language models",
        "search_term": "multichannel dialogue"
      },
      {
        "anchor": "spoken language models",
        "search_term": "behavioral arbitration"
      },
      {
        "anchor": "spoken language models",
        "search_term": "semantic coherence"
      },
      {
        "anchor": "spoken language models",
        "search_term": "floor transfer offset"
      },
      {
        "anchor": "spoken language models",
        "search_term": "stop latency"
      },
      {
        "anchor": "spoken language models",
        "search_term": "interruption response delay"
      }
    ]
  },
  {
    "arxiv_id": "2210.17227",
    "anchor": "spoken language models",
    "search_term": "low latency",
    "search_record": {
      "id": "http://arxiv.org/abs/2210.17227v2",
      "title": "Modelling M/M/R-JSQ-PS sojourn time distribution for Ultra-Reliable Low Latency Communication services",
      "summary": "The future Internet promises to support time-sensitive services that require ultra low latencies and reliabilities of 99.99%. Recent advances in cellular and WiFi connections enhance the network to meet high reliability and ultra low latencies. However, the aforementioned services require that the server processing time ensures low latencies with high reliability, otherwise the end-to-end performance is not met. To that end, in this paper we use queuing theory to model the sojourn time distribution for Ultra-Reliable Low Latency Communication services of M/M/R-JSQ-PS systems: Markovian queues with R CPU servers following a join shortest queue processor-sharing discipline (for example Linux systems). We develop open-source simulation software, and develop and compare six analytical approximations for the sojourn time distribution. The proposed approximations yield Wasserstein distances below 2 time units, and upon medium loads incur into errors of less than 1.78 time units (e.g., milliseconds) for the 99.99th percentile sojourn time. Moreover, the proposed sojourn time approximations are stable regardless the number of CPUs and stay close to the simulations.",
      "published": "2022-10-31T11:11:41Z"
    },
    "metadata": {
      "arxiv_id": "2210.17227",
      "title": "Modelling M/M/R-JSQ-PS sojourn time distribution for Ultra-Reliable Low Latency Communication services",
      "summary": "The future Internet promises to support time-sensitive services that require ultra low latencies and reliabilities of 99.99%. Recent advances in cellular and WiFi connections enhance the network to meet high reliability and ultra low latencies. However, the aforementioned services require that the server processing time ensures low latencies with high reliability, otherwise the end-to-end performance is not met. To that end, in this paper we use queuing theory to model the sojourn time distribution for Ultra-Reliable Low Latency Communication services of M/M/R-JSQ-PS systems: Markovian queues with R CPU servers following a join shortest queue processor-sharing discipline (for example Linux systems). We develop open-source simulation software, and develop and compare six analytical approximations for the sojourn time distribution. The proposed approximations yield Wasserstein distances below 2 time units, and upon medium loads incur into errors of less than 1.78 time units (e.g., milliseconds) for the 99.99th percentile sojourn time. Moreover, the proposed sojourn time approximations are stable regardless the number of CPUs and stay close to the simulations.",
      "authors": [
        "Geraint I. Palmer",
        "Jorge Martín-Pérez"
      ],
      "published": "2022-10-31T11:11:41Z",
      "updated": "2022-12-22T20:06:59Z",
      "categories": [
        "cs.NI",
        "math.OC"
      ],
      "pdf_url": "https://arxiv.org/pdf/2210.17227v2",
      "landing_url": "https://arxiv.org/abs/2210.17227v2",
      "doi": "https://doi.org/10.48550/arXiv.2210.17227"
    },
    "queries": [
      {
        "anchor": "spoken language models",
        "search_term": "low latency"
      },
      {
        "anchor": "synchronous dialogue",
        "search_term": "low latency"
      },
      {
        "anchor": "turn-taking",
        "search_term": "low latency"
      }
    ]
  },
  {
    "arxiv_id": "2211.03089",
    "anchor": "spoken language models",
    "search_term": "audio tokens",
    "search_record": {
      "id": "http://arxiv.org/abs/2211.03089v2",
      "title": "I Hear Your True Colors: Image Guided Audio Generation",
      "summary": "We propose Im2Wav, an image guided open-domain audio generation system. Given an input image or a sequence of images, Im2Wav generates a semantically relevant sound. Im2Wav is based on two Transformer language models, that operate over a hierarchical discrete audio representation obtained from a VQ-VAE based model. We first produce a low-level audio representation using a language model. Then, we upsample the audio tokens using an additional language model to generate a high-fidelity audio sample. We use the rich semantics of a pre-trained CLIP (Contrastive Language-Image Pre-training) embedding as a visual representation to condition the language model. In addition, to steer the generation process towards the conditioning image, we apply the classifier-free guidance method. Results suggest that Im2Wav significantly outperforms the evaluated baselines in both fidelity and relevance evaluation metrics. Additionally, we provide an ablation study to better assess the impact of each of the method components on overall performance. Lastly, to better evaluate image-to-audio models, we propose an out-of-domain image dataset, denoted as ImageHear. ImageHear can be used as a benchmark for evaluating future image-to-audio models. Samples and code can be found inside the manuscript.",
      "published": "2022-11-06T11:48:20Z"
    },
    "metadata": {
      "arxiv_id": "2211.03089",
      "title": "I Hear Your True Colors: Image Guided Audio Generation",
      "summary": "We propose Im2Wav, an image guided open-domain audio generation system. Given an input image or a sequence of images, Im2Wav generates a semantically relevant sound. Im2Wav is based on two Transformer language models, that operate over a hierarchical discrete audio representation obtained from a VQ-VAE based model. We first produce a low-level audio representation using a language model. Then, we upsample the audio tokens using an additional language model to generate a high-fidelity audio sample. We use the rich semantics of a pre-trained CLIP (Contrastive Language-Image Pre-training) embedding as a visual representation to condition the language model. In addition, to steer the generation process towards the conditioning image, we apply the classifier-free guidance method. Results suggest that Im2Wav significantly outperforms the evaluated baselines in both fidelity and relevance evaluation metrics. Additionally, we provide an ablation study to better assess the impact of each of the method components on overall performance. Lastly, to better evaluate image-to-audio models, we propose an out-of-domain image dataset, denoted as ImageHear. ImageHear can be used as a benchmark for evaluating future image-to-audio models. Samples and code can be found inside the manuscript.",
      "authors": [
        "Roy Sheffer",
        "Yossi Adi"
      ],
      "published": "2022-11-06T11:48:20Z",
      "updated": "2023-02-27T11:15:40Z",
      "categories": [
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2211.03089v2",
      "landing_url": "https://arxiv.org/abs/2211.03089v2",
      "doi": "https://doi.org/10.48550/arXiv.2211.03089"
    },
    "queries": [
      {
        "anchor": "spoken language models",
        "search_term": "audio tokens"
      },
      {
        "anchor": "synchronous dialogue",
        "search_term": "audio tokens"
      }
    ]
  },
  {
    "arxiv_id": "2211.05857",
    "anchor": "spoken language models",
    "search_term": "streaming architecture",
    "search_record": {
      "id": "http://arxiv.org/abs/2211.05857v1",
      "title": "Colocating Real-time Storage and Processing: An Analysis of Pull-based versus Push-based Streaming",
      "summary": "Real-time Big Data architectures evolved into specialized layers for handling data streams' ingestion, storage, and processing over the past decade. Layered streaming architectures integrate pull-based read and push-based write RPC mechanisms implemented by stream ingestion/storage systems. In addition, stream processing engines expose source/sink interfaces, allowing them to decouple these systems easily. However, open-source streaming engines leverage workflow sources implemented through a pull-based approach, continuously issuing read RPCs towards the stream ingestion/storage, effectively competing with write RPCs. This paper proposes a unified streaming architecture that leverages push-based and/or pull-based source implementations for integrating ingestion/storage and processing engines that can reduce processing latency and increase system read and write throughput while making room for higher ingestion. We implement a novel push-based streaming source by replacing continuous pull-based RPCs with one single RPC and shared memory (storage and processing handle streaming data through pointers to shared objects). To this end, we conduct an experimental analysis of pull-based versus push-based design alternatives of the streaming source reader while considering a set of stream benchmarks and microbenchmarks and discuss the advantages of both approaches.",
      "published": "2022-11-10T20:26:07Z"
    },
    "metadata": {
      "arxiv_id": "2211.05857",
      "title": "Colocating Real-time Storage and Processing: An Analysis of Pull-based versus Push-based Streaming",
      "summary": "Real-time Big Data architectures evolved into specialized layers for handling data streams' ingestion, storage, and processing over the past decade. Layered streaming architectures integrate pull-based read and push-based write RPC mechanisms implemented by stream ingestion/storage systems. In addition, stream processing engines expose source/sink interfaces, allowing them to decouple these systems easily. However, open-source streaming engines leverage workflow sources implemented through a pull-based approach, continuously issuing read RPCs towards the stream ingestion/storage, effectively competing with write RPCs. This paper proposes a unified streaming architecture that leverages push-based and/or pull-based source implementations for integrating ingestion/storage and processing engines that can reduce processing latency and increase system read and write throughput while making room for higher ingestion. We implement a novel push-based streaming source by replacing continuous pull-based RPCs with one single RPC and shared memory (storage and processing handle streaming data through pointers to shared objects). To this end, we conduct an experimental analysis of pull-based versus push-based design alternatives of the streaming source reader while considering a set of stream benchmarks and microbenchmarks and discuss the advantages of both approaches.",
      "authors": [
        "Ovidiu-Cristian Marcu",
        "Pascal Bouvry"
      ],
      "published": "2022-11-10T20:26:07Z",
      "updated": "2022-11-10T20:26:07Z",
      "categories": [
        "cs.DC",
        "cs.PF"
      ],
      "pdf_url": "https://arxiv.org/pdf/2211.05857v1",
      "landing_url": "https://arxiv.org/abs/2211.05857v1",
      "doi": "https://doi.org/10.48550/arXiv.2211.05857"
    },
    "queries": [
      {
        "anchor": "spoken language models",
        "search_term": "streaming architecture"
      },
      {
        "anchor": "synchronous dialogue",
        "search_term": "streaming architecture"
      }
    ]
  },
  {
    "arxiv_id": "2211.07825",
    "anchor": "synchronous dialogue",
    "search_term": "semantic coherence",
    "search_record": {
      "id": "http://arxiv.org/abs/2211.07825v1",
      "title": "Direct Inversion: Optimization-Free Text-Driven Real Image Editing with Diffusion Models",
      "summary": "With the rise of large, publicly-available text-to-image diffusion models, text-guided real image editing has garnered much research attention recently. Existing methods tend to either rely on some form of per-instance or per-task fine-tuning and optimization, require multiple novel views, or they inherently entangle preservation of real image identity, semantic coherence, and faithfulness to text guidance. In this paper, we propose an optimization-free and zero fine-tuning framework that applies complex and non-rigid edits to a single real image via a text prompt, avoiding all the pitfalls described above. Using widely-available generic pre-trained text-to-image diffusion models, we demonstrate the ability to modulate pose, scene, background, style, color, and even racial identity in an extremely flexible manner through a single target text detailing the desired edit. Furthermore, our method, which we name $\\textit{Direct Inversion}$, proposes multiple intuitively configurable hyperparameters to allow for a wide range of types and extents of real image edits. We prove our method's efficacy in producing high-quality, diverse, semantically coherent, and faithful real image edits through applying it on a variety of inputs for a multitude of tasks. We also formalize our method in well-established theory, detail future experiments for further improvement, and compare against state-of-the-art attempts.",
      "published": "2022-11-15T01:07:38Z"
    },
    "metadata": {
      "arxiv_id": "2211.07825",
      "title": "Direct Inversion: Optimization-Free Text-Driven Real Image Editing with Diffusion Models",
      "summary": "With the rise of large, publicly-available text-to-image diffusion models, text-guided real image editing has garnered much research attention recently. Existing methods tend to either rely on some form of per-instance or per-task fine-tuning and optimization, require multiple novel views, or they inherently entangle preservation of real image identity, semantic coherence, and faithfulness to text guidance. In this paper, we propose an optimization-free and zero fine-tuning framework that applies complex and non-rigid edits to a single real image via a text prompt, avoiding all the pitfalls described above. Using widely-available generic pre-trained text-to-image diffusion models, we demonstrate the ability to modulate pose, scene, background, style, color, and even racial identity in an extremely flexible manner through a single target text detailing the desired edit. Furthermore, our method, which we name $\\textit{Direct Inversion}$, proposes multiple intuitively configurable hyperparameters to allow for a wide range of types and extents of real image edits. We prove our method's efficacy in producing high-quality, diverse, semantically coherent, and faithful real image edits through applying it on a variety of inputs for a multitude of tasks. We also formalize our method in well-established theory, detail future experiments for further improvement, and compare against state-of-the-art attempts.",
      "authors": [
        "Adham Elarabawy",
        "Harish Kamath",
        "Samuel Denton"
      ],
      "published": "2022-11-15T01:07:38Z",
      "updated": "2022-11-15T01:07:38Z",
      "categories": [
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2211.07825v1",
      "landing_url": "https://arxiv.org/abs/2211.07825v1",
      "doi": "https://doi.org/10.48550/arXiv.2211.07825"
    },
    "queries": [
      {
        "anchor": "synchronous dialogue",
        "search_term": "semantic coherence"
      }
    ]
  },
  {
    "arxiv_id": "2211.15664",
    "anchor": "spoken language models",
    "search_term": "data scarcity",
    "search_record": {
      "id": "http://arxiv.org/abs/2211.15664v3",
      "title": "Utilising physics-guided deep learning to overcome data scarcity",
      "summary": "Deep learning (DL) relies heavily on data, and the quality of data influences its performance significantly. However, obtaining high-quality, well-annotated datasets can be challenging or even impossible in many real-world applications, such as structural risk estimation and medical diagnosis. This presents a significant barrier to the practical implementation of DL in these fields. Physics-guided deep learning (PGDL) is a novel type of DL that can integrate physics laws to train neural networks. This can be applied to any systems that are controlled or governed by physics laws, such as mechanics, finance and medical applications. It has been demonstrated that, with the additional information provided by physics laws, PGDL achieves great accuracy and generalisation in the presence of data scarcity. This review provides a detailed examination of PGDL and offers a structured overview of its use in addressing data scarcity across various fields, including physics, engineering and medical applications. Moreover, the review identifies the current limitations and opportunities for PGDL in relation to data scarcity and offers a thorough discussion on the future prospects of PGDL.",
      "published": "2022-11-24T01:03:21Z"
    },
    "metadata": {
      "arxiv_id": "2211.15664",
      "title": "Utilising physics-guided deep learning to overcome data scarcity",
      "summary": "Deep learning (DL) relies heavily on data, and the quality of data influences its performance significantly. However, obtaining high-quality, well-annotated datasets can be challenging or even impossible in many real-world applications, such as structural risk estimation and medical diagnosis. This presents a significant barrier to the practical implementation of DL in these fields. Physics-guided deep learning (PGDL) is a novel type of DL that can integrate physics laws to train neural networks. This can be applied to any systems that are controlled or governed by physics laws, such as mechanics, finance and medical applications. It has been demonstrated that, with the additional information provided by physics laws, PGDL achieves great accuracy and generalisation in the presence of data scarcity. This review provides a detailed examination of PGDL and offers a structured overview of its use in addressing data scarcity across various fields, including physics, engineering and medical applications. Moreover, the review identifies the current limitations and opportunities for PGDL in relation to data scarcity and offers a thorough discussion on the future prospects of PGDL.",
      "authors": [
        "Jinshuai Bai",
        "Laith Alzubaidi",
        "Qingxia Wang",
        "Ellen Kuhl",
        "Mohammed Bennamoun",
        "Yuantong Gu"
      ],
      "published": "2022-11-24T01:03:21Z",
      "updated": "2023-01-09T01:55:30Z",
      "categories": [
        "cs.LG",
        "cs.CE"
      ],
      "pdf_url": "https://arxiv.org/pdf/2211.15664v3",
      "landing_url": "https://arxiv.org/abs/2211.15664v3",
      "doi": "https://doi.org/10.48550/arXiv.2211.15664"
    },
    "queries": [
      {
        "anchor": "spoken language models",
        "search_term": "data scarcity"
      },
      {
        "anchor": "synchronous dialogue",
        "search_term": "data scarcity"
      },
      {
        "anchor": "turn-taking",
        "search_term": "data scarcity"
      }
    ]
  },
  {
    "arxiv_id": "2211.16542",
    "anchor": "synchronous dialogue",
    "search_term": "temporal dynamics",
    "search_record": {
      "id": "http://arxiv.org/abs/2211.16542v1",
      "title": "Modeling animal contests based on spatio-temporal dynamics",
      "summary": "We present a general theoretical model for the spatio-temporal dynamics of animal contests. Inspired by interactions between physical particles, the model is formulated in terms of effective interaction potentials, which map typical elements of contest behaviour into empirically verifiable rules of contestant motion. This allows us to simulate the observable dynamics of contests in various realistic scenarios, notably in dyadic contests over a localized resource. Assessment strategies previously formulated in game-theoretic models, as well as the effects of fighting costs, can be described as variations in our model's parameters. Furthermore, the trends of contest duration associated with these assessment strategies can be derived and understood within the model. Detailed description of the contestants' motion enables the exploration of spatio-temporal properties of asymmetric contests, such as the emergence of chase dynamics. Overall, our framework aims to bridge the growing gap between empirical capabilities and theory in this widespread aspect of animal behaviour.",
      "published": "2022-11-29T19:06:37Z"
    },
    "metadata": {
      "arxiv_id": "2211.16542",
      "title": "Modeling animal contests based on spatio-temporal dynamics",
      "summary": "We present a general theoretical model for the spatio-temporal dynamics of animal contests. Inspired by interactions between physical particles, the model is formulated in terms of effective interaction potentials, which map typical elements of contest behaviour into empirically verifiable rules of contestant motion. This allows us to simulate the observable dynamics of contests in various realistic scenarios, notably in dyadic contests over a localized resource. Assessment strategies previously formulated in game-theoretic models, as well as the effects of fighting costs, can be described as variations in our model's parameters. Furthermore, the trends of contest duration associated with these assessment strategies can be derived and understood within the model. Detailed description of the contestants' motion enables the exploration of spatio-temporal properties of asymmetric contests, such as the emergence of chase dynamics. Overall, our framework aims to bridge the growing gap between empirical capabilities and theory in this widespread aspect of animal behaviour.",
      "authors": [
        "Amir Haluts",
        "Alex Jordan",
        "Nir S. Gov"
      ],
      "published": "2022-11-29T19:06:37Z",
      "updated": "2022-11-29T19:06:37Z",
      "categories": [
        "physics.bio-ph",
        "q-bio.QM"
      ],
      "pdf_url": "https://arxiv.org/pdf/2211.16542v1",
      "landing_url": "https://arxiv.org/abs/2211.16542v1",
      "doi": "https://doi.org/10.48550/arXiv.2211.16542"
    },
    "queries": [
      {
        "anchor": "synchronous dialogue",
        "search_term": "temporal dynamics"
      }
    ]
  },
  {
    "arxiv_id": "2212.02875",
    "anchor": "synchronous dialogue",
    "search_term": "temporal dynamics",
    "search_record": {
      "id": "http://arxiv.org/abs/2212.02875v1",
      "title": "Multi-Task Edge Prediction in Temporally-Dynamic Video Graphs",
      "summary": "Graph neural networks have shown to learn effective node representations, enabling node-, link-, and graph-level inference. Conventional graph networks assume static relations between nodes, while relations between entities in a video often evolve over time, with nodes entering and exiting dynamically. In such temporally-dynamic graphs, a core problem is inferring the future state of spatio-temporal edges, which can constitute multiple types of relations. To address this problem, we propose MTD-GNN, a graph network for predicting temporally-dynamic edges for multiple types of relations. We propose a factorized spatio-temporal graph attention layer to learn dynamic node representations and present a multi-task edge prediction loss that models multiple relations simultaneously. The proposed architecture operates on top of scene graphs that we obtain from videos through object detection and spatio-temporal linking. Experimental evaluations on ActionGenome and CLEVRER show that modeling multiple relations in our temporally-dynamic graph network can be mutually beneficial, outperforming existing static and spatio-temporal graph neural networks, as well as state-of-the-art predicate classification methods.",
      "published": "2022-12-06T10:41:00Z"
    },
    "metadata": {
      "arxiv_id": "2212.02875",
      "title": "Multi-Task Edge Prediction in Temporally-Dynamic Video Graphs",
      "summary": "Graph neural networks have shown to learn effective node representations, enabling node-, link-, and graph-level inference. Conventional graph networks assume static relations between nodes, while relations between entities in a video often evolve over time, with nodes entering and exiting dynamically. In such temporally-dynamic graphs, a core problem is inferring the future state of spatio-temporal edges, which can constitute multiple types of relations. To address this problem, we propose MTD-GNN, a graph network for predicting temporally-dynamic edges for multiple types of relations. We propose a factorized spatio-temporal graph attention layer to learn dynamic node representations and present a multi-task edge prediction loss that models multiple relations simultaneously. The proposed architecture operates on top of scene graphs that we obtain from videos through object detection and spatio-temporal linking. Experimental evaluations on ActionGenome and CLEVRER show that modeling multiple relations in our temporally-dynamic graph network can be mutually beneficial, outperforming existing static and spatio-temporal graph neural networks, as well as state-of-the-art predicate classification methods.",
      "authors": [
        "Osman Ülger",
        "Julian Wiederer",
        "Mohsen Ghafoorian",
        "Vasileios Belagiannis",
        "Pascal Mettes"
      ],
      "published": "2022-12-06T10:41:00Z",
      "updated": "2022-12-06T10:41:00Z",
      "categories": [
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2212.02875v1",
      "landing_url": "https://arxiv.org/abs/2212.02875v1",
      "doi": "https://doi.org/10.48550/arXiv.2212.02875"
    },
    "queries": [
      {
        "anchor": "synchronous dialogue",
        "search_term": "temporal dynamics"
      }
    ]
  },
  {
    "arxiv_id": "2212.04559",
    "anchor": "synchronous dialogue",
    "search_term": "discrete tokens",
    "search_record": {
      "id": "http://arxiv.org/abs/2212.04559v1",
      "title": "SpeechLMScore: Evaluating speech generation using speech language model",
      "summary": "While human evaluation is the most reliable metric for evaluating speech generation systems, it is generally costly and time-consuming. Previous studies on automatic speech quality assessment address the problem by predicting human evaluation scores with machine learning models. However, they rely on supervised learning and thus suffer from high annotation costs and domain-shift problems. We propose SpeechLMScore, an unsupervised metric to evaluate generated speech using a speech-language model. SpeechLMScore computes the average log-probability of a speech signal by mapping it into discrete tokens and measures the average probability of generating the sequence of tokens. Therefore, it does not require human annotation and is a highly scalable framework. Evaluation results demonstrate that the proposed metric shows a promising correlation with human evaluation scores on different speech generation tasks including voice conversion, text-to-speech, and speech enhancement.",
      "published": "2022-12-08T21:00:15Z"
    },
    "metadata": {
      "arxiv_id": "2212.04559",
      "title": "SpeechLMScore: Evaluating speech generation using speech language model",
      "summary": "While human evaluation is the most reliable metric for evaluating speech generation systems, it is generally costly and time-consuming. Previous studies on automatic speech quality assessment address the problem by predicting human evaluation scores with machine learning models. However, they rely on supervised learning and thus suffer from high annotation costs and domain-shift problems. We propose SpeechLMScore, an unsupervised metric to evaluate generated speech using a speech-language model. SpeechLMScore computes the average log-probability of a speech signal by mapping it into discrete tokens and measures the average probability of generating the sequence of tokens. Therefore, it does not require human annotation and is a highly scalable framework. Evaluation results demonstrate that the proposed metric shows a promising correlation with human evaluation scores on different speech generation tasks including voice conversion, text-to-speech, and speech enhancement.",
      "authors": [
        "Soumi Maiti",
        "Yifan Peng",
        "Takaaki Saeki",
        "Shinji Watanabe"
      ],
      "published": "2022-12-08T21:00:15Z",
      "updated": "2022-12-08T21:00:15Z",
      "categories": [
        "eess.AS",
        "cs.LG",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2212.04559v1",
      "landing_url": "https://arxiv.org/abs/2212.04559v1",
      "doi": "https://doi.org/10.48550/arXiv.2212.04559"
    },
    "queries": [
      {
        "anchor": "synchronous dialogue",
        "search_term": "discrete tokens"
      }
    ]
  },
  {
    "arxiv_id": "2212.04823",
    "anchor": "synchronous dialogue",
    "search_term": "streaming architecture",
    "search_record": {
      "id": "http://arxiv.org/abs/2212.04823v2",
      "title": "GazeNeRF: 3D-Aware Gaze Redirection with Neural Radiance Fields",
      "summary": "We propose GazeNeRF, a 3D-aware method for the task of gaze redirection. Existing gaze redirection methods operate on 2D images and struggle to generate 3D consistent results. Instead, we build on the intuition that the face region and eyeballs are separate 3D structures that move in a coordinated yet independent fashion. Our method leverages recent advancements in conditional image-based neural radiance fields and proposes a two-stream architecture that predicts volumetric features for the face and eye regions separately. Rigidly transforming the eye features via a 3D rotation matrix provides fine-grained control over the desired gaze angle. The final, redirected image is then attained via differentiable volume compositing. Our experiments show that this architecture outperforms naively conditioned NeRF baselines as well as previous state-of-the-art 2D gaze redirection methods in terms of redirection accuracy and identity preservation.",
      "published": "2022-12-08T13:19:11Z"
    },
    "metadata": {
      "arxiv_id": "2212.04823",
      "title": "GazeNeRF: 3D-Aware Gaze Redirection with Neural Radiance Fields",
      "summary": "We propose GazeNeRF, a 3D-aware method for the task of gaze redirection. Existing gaze redirection methods operate on 2D images and struggle to generate 3D consistent results. Instead, we build on the intuition that the face region and eyeballs are separate 3D structures that move in a coordinated yet independent fashion. Our method leverages recent advancements in conditional image-based neural radiance fields and proposes a two-stream architecture that predicts volumetric features for the face and eye regions separately. Rigidly transforming the eye features via a 3D rotation matrix provides fine-grained control over the desired gaze angle. The final, redirected image is then attained via differentiable volume compositing. Our experiments show that this architecture outperforms naively conditioned NeRF baselines as well as previous state-of-the-art 2D gaze redirection methods in terms of redirection accuracy and identity preservation.",
      "authors": [
        "Alessandro Ruzzi",
        "Xiangwei Shi",
        "Xi Wang",
        "Gengyan Li",
        "Shalini De Mello",
        "Hyung Jin Chang",
        "Xucong Zhang",
        "Otmar Hilliges"
      ],
      "published": "2022-12-08T13:19:11Z",
      "updated": "2023-03-28T19:41:57Z",
      "categories": [
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2212.04823v2",
      "landing_url": "https://arxiv.org/abs/2212.04823v2",
      "doi": "https://doi.org/10.48550/arXiv.2212.04823"
    },
    "queries": [
      {
        "anchor": "synchronous dialogue",
        "search_term": "streaming architecture"
      }
    ]
  },
  {
    "arxiv_id": "2212.08906",
    "anchor": "full-duplex",
    "search_term": "true full-duplex",
    "search_record": {
      "id": "http://arxiv.org/abs/2212.08906v1",
      "title": "Full-Duplex Magnetic Induction Communication: Opportunities and Challenges",
      "summary": "The demand for high data rates is rapidly increasing as the interest in Magnetic Induction (MI) communication-based underwater applications grow. However, the data rate in MI is limited by the use of low operational frequency in generating a quasi-static magnetic field. In this paper, we propose the use of full-duplex (FD) MI communication to efficiently utilize the available bandwidth and instantly double the data rate. We propose a two-dimensional transceiver architecture to achieve full-duplex communication by exploiting the directional nature of magnetic fields. We further evaluate the proposed end-to-end FD MI communication against self-interference (SI), its impact on communication distance, and robustness in view of orientation sensitivity. Finally, we conclude by discussing typical challenges in the realization of FD MI communication and highlight a few potential future research directions.",
      "published": "2022-12-17T15:38:11Z"
    },
    "metadata": {
      "arxiv_id": "2212.08906",
      "title": "Full-Duplex Magnetic Induction Communication: Opportunities and Challenges",
      "summary": "The demand for high data rates is rapidly increasing as the interest in Magnetic Induction (MI) communication-based underwater applications grow. However, the data rate in MI is limited by the use of low operational frequency in generating a quasi-static magnetic field. In this paper, we propose the use of full-duplex (FD) MI communication to efficiently utilize the available bandwidth and instantly double the data rate. We propose a two-dimensional transceiver architecture to achieve full-duplex communication by exploiting the directional nature of magnetic fields. We further evaluate the proposed end-to-end FD MI communication against self-interference (SI), its impact on communication distance, and robustness in view of orientation sensitivity. Finally, we conclude by discussing typical challenges in the realization of FD MI communication and highlight a few potential future research directions.",
      "authors": [
        "Muhammad Muzzammil",
        "Saif Al-Kuwari",
        "Niaz Ahmed",
        "Marwa Qaraqe"
      ],
      "published": "2022-12-17T15:38:11Z",
      "updated": "2022-12-17T15:38:11Z",
      "categories": [
        "eess.SP"
      ],
      "pdf_url": "https://arxiv.org/pdf/2212.08906v1",
      "landing_url": "https://arxiv.org/abs/2212.08906v1",
      "doi": "https://doi.org/10.48550/arXiv.2212.08906"
    },
    "queries": [
      {
        "anchor": "full-duplex",
        "search_term": "true full-duplex"
      },
      {
        "anchor": "full-duplex",
        "search_term": "pseudo full-duplex"
      },
      {
        "anchor": "full-duplex",
        "search_term": "engineered synchronization"
      },
      {
        "anchor": "full-duplex",
        "search_term": "learned synchronization"
      },
      {
        "anchor": "full-duplex",
        "search_term": "modular architecture"
      },
      {
        "anchor": "full-duplex",
        "search_term": "streaming architecture"
      },
      {
        "anchor": "full-duplex",
        "search_term": "real-time interaction"
      },
      {
        "anchor": "full-duplex",
        "search_term": "low latency"
      },
      {
        "anchor": "full-duplex",
        "search_term": "synchronous data"
      },
      {
        "anchor": "full-duplex",
        "search_term": "data scarcity"
      },
      {
        "anchor": "full-duplex",
        "search_term": "multichannel dialogue"
      },
      {
        "anchor": "full-duplex",
        "search_term": "temporal dynamics"
      },
      {
        "anchor": "full-duplex",
        "search_term": "behavioral arbitration"
      },
      {
        "anchor": "full-duplex",
        "search_term": "semantic coherence"
      },
      {
        "anchor": "full-duplex",
        "search_term": "neural codec"
      },
      {
        "anchor": "full-duplex",
        "search_term": "discrete tokens"
      },
      {
        "anchor": "full-duplex",
        "search_term": "audio tokens"
      },
      {
        "anchor": "full-duplex",
        "search_term": "floor transfer offset"
      },
      {
        "anchor": "full-duplex",
        "search_term": "stop latency"
      },
      {
        "anchor": "full-duplex",
        "search_term": "interruption response delay"
      }
    ]
  },
  {
    "arxiv_id": "2301.00591",
    "anchor": "spoken language models",
    "search_term": "true full-duplex",
    "search_record": {
      "id": "http://arxiv.org/abs/2301.00591v3",
      "title": "Analysing Discrete Self Supervised Speech Representation for Spoken Language Modeling",
      "summary": "This work profoundly analyzes discrete self-supervised speech representations (units) through the eyes of Generative Spoken Language Modeling (GSLM). Following the findings of such an analysis, we propose practical improvements to the discrete unit for the GSLM. First, we start comprehending these units by analyzing them in three axes: interpretation, visualization, and resynthesis. Our analysis finds a high correlation between the speech units to phonemes and phoneme families, while their correlation with speaker or gender is weaker. Additionally, we found redundancies in the extracted units and claim that one reason may be the units' context. Following this analysis, we propose a new, unsupervised metric to measure unit redundancies. Finally, we use this metric to develop new methods that improve the robustness of units' clustering and show significant improvement considering zero-resource speech metrics such as ABX. Code and analysis tools are available under the following link: https://github.com/slp-rl/SLM-Discrete-Representations",
      "published": "2023-01-02T10:36:40Z"
    },
    "metadata": {
      "arxiv_id": "2301.00591",
      "title": "Analysing Discrete Self Supervised Speech Representation for Spoken Language Modeling",
      "summary": "This work profoundly analyzes discrete self-supervised speech representations (units) through the eyes of Generative Spoken Language Modeling (GSLM). Following the findings of such an analysis, we propose practical improvements to the discrete unit for the GSLM. First, we start comprehending these units by analyzing them in three axes: interpretation, visualization, and resynthesis. Our analysis finds a high correlation between the speech units to phonemes and phoneme families, while their correlation with speaker or gender is weaker. Additionally, we found redundancies in the extracted units and claim that one reason may be the units' context. Following this analysis, we propose a new, unsupervised metric to measure unit redundancies. Finally, we use this metric to develop new methods that improve the robustness of units' clustering and show significant improvement considering zero-resource speech metrics such as ABX. Code and analysis tools are available under the following link: https://github.com/slp-rl/SLM-Discrete-Representations",
      "authors": [
        "Amitay Sicherman",
        "Yossi Adi"
      ],
      "published": "2023-01-02T10:36:40Z",
      "updated": "2023-03-01T09:59:54Z",
      "categories": [
        "cs.CL",
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2301.00591v3",
      "landing_url": "https://arxiv.org/abs/2301.00591v3",
      "doi": "https://doi.org/10.1109/ICASSP49357.2023.10097097"
    },
    "queries": [
      {
        "anchor": "spoken language models",
        "search_term": "true full-duplex"
      },
      {
        "anchor": "spoken language models",
        "search_term": "pseudo full-duplex"
      },
      {
        "anchor": "spoken language models",
        "search_term": "half duplex"
      },
      {
        "anchor": "spoken language models",
        "search_term": "engineered synchronization"
      },
      {
        "anchor": "spoken language models",
        "search_term": "learned synchronization"
      },
      {
        "anchor": "spoken language models",
        "search_term": "modular architecture"
      },
      {
        "anchor": "spoken language models",
        "search_term": "streaming architecture"
      },
      {
        "anchor": "spoken language models",
        "search_term": "real-time interaction"
      },
      {
        "anchor": "spoken language models",
        "search_term": "low latency"
      },
      {
        "anchor": "spoken language models",
        "search_term": "synchronous data"
      },
      {
        "anchor": "spoken language models",
        "search_term": "data scarcity"
      },
      {
        "anchor": "spoken language models",
        "search_term": "multichannel dialogue"
      },
      {
        "anchor": "spoken language models",
        "search_term": "temporal dynamics"
      },
      {
        "anchor": "spoken language models",
        "search_term": "behavioral arbitration"
      },
      {
        "anchor": "spoken language models",
        "search_term": "semantic coherence"
      },
      {
        "anchor": "spoken language models",
        "search_term": "neural codec"
      },
      {
        "anchor": "spoken language models",
        "search_term": "discrete tokens"
      },
      {
        "anchor": "spoken language models",
        "search_term": "audio tokens"
      },
      {
        "anchor": "spoken language models",
        "search_term": "floor transfer offset"
      },
      {
        "anchor": "spoken language models",
        "search_term": "stop latency"
      },
      {
        "anchor": "spoken language models",
        "search_term": "interruption response delay"
      }
    ]
  },
  {
    "arxiv_id": "2301.00704",
    "anchor": "synchronous dialogue",
    "search_term": "discrete tokens",
    "search_record": {
      "id": "http://arxiv.org/abs/2301.00704v1",
      "title": "Muse: Text-To-Image Generation via Masked Generative Transformers",
      "summary": "We present Muse, a text-to-image Transformer model that achieves state-of-the-art image generation performance while being significantly more efficient than diffusion or autoregressive models. Muse is trained on a masked modeling task in discrete token space: given the text embedding extracted from a pre-trained large language model (LLM), Muse is trained to predict randomly masked image tokens. Compared to pixel-space diffusion models, such as Imagen and DALL-E 2, Muse is significantly more efficient due to the use of discrete tokens and requiring fewer sampling iterations; compared to autoregressive models, such as Parti, Muse is more efficient due to the use of parallel decoding. The use of a pre-trained LLM enables fine-grained language understanding, translating to high-fidelity image generation and the understanding of visual concepts such as objects, their spatial relationships, pose, cardinality etc. Our 900M parameter model achieves a new SOTA on CC3M, with an FID score of 6.06. The Muse 3B parameter model achieves an FID of 7.88 on zero-shot COCO evaluation, along with a CLIP score of 0.32. Muse also directly enables a number of image editing applications without the need to fine-tune or invert the model: inpainting, outpainting, and mask-free editing. More results are available at https://muse-model.github.io",
      "published": "2023-01-02T14:43:38Z"
    },
    "metadata": {
      "arxiv_id": "2301.00704",
      "title": "Muse: Text-To-Image Generation via Masked Generative Transformers",
      "summary": "We present Muse, a text-to-image Transformer model that achieves state-of-the-art image generation performance while being significantly more efficient than diffusion or autoregressive models. Muse is trained on a masked modeling task in discrete token space: given the text embedding extracted from a pre-trained large language model (LLM), Muse is trained to predict randomly masked image tokens. Compared to pixel-space diffusion models, such as Imagen and DALL-E 2, Muse is significantly more efficient due to the use of discrete tokens and requiring fewer sampling iterations; compared to autoregressive models, such as Parti, Muse is more efficient due to the use of parallel decoding. The use of a pre-trained LLM enables fine-grained language understanding, translating to high-fidelity image generation and the understanding of visual concepts such as objects, their spatial relationships, pose, cardinality etc. Our 900M parameter model achieves a new SOTA on CC3M, with an FID score of 6.06. The Muse 3B parameter model achieves an FID of 7.88 on zero-shot COCO evaluation, along with a CLIP score of 0.32. Muse also directly enables a number of image editing applications without the need to fine-tune or invert the model: inpainting, outpainting, and mask-free editing. More results are available at https://muse-model.github.io",
      "authors": [
        "Huiwen Chang",
        "Han Zhang",
        "Jarred Barber",
        "AJ Maschinot",
        "Jose Lezama",
        "Lu Jiang",
        "Ming-Hsuan Yang",
        "Kevin Murphy",
        "William T. Freeman",
        "Michael Rubinstein",
        "Yuanzhen Li",
        "Dilip Krishnan"
      ],
      "published": "2023-01-02T14:43:38Z",
      "updated": "2023-01-02T14:43:38Z",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2301.00704v1",
      "landing_url": "https://arxiv.org/abs/2301.00704v1",
      "doi": "https://doi.org/10.48550/arXiv.2301.00704"
    },
    "queries": [
      {
        "anchor": "synchronous dialogue",
        "search_term": "discrete tokens"
      }
    ]
  },
  {
    "arxiv_id": "2301.01474",
    "anchor": "synchronous dialogue",
    "search_term": "synchronous data",
    "search_record": {
      "id": "http://arxiv.org/abs/2301.01474v1",
      "title": "UAV aided Metaverse over Wireless Communications: A Reinforcement Learning Approach",
      "summary": "Metaverse is expected to create a virtual world closely connected with reality to provide users with immersive experience with the support of 5G high data rate communication technique. A huge amount of data in physical world needs to be synchronized to the virtual world to provide immersive experience for users, and there will be higher requirements on coverage to include more users into Metaverse. However, 5G signal suffers severe attenuation, which makes it more expensive to maintain the same coverage. Unmanned aerial vehicle (UAV) is a promising candidate technique for future implementation of Metaverse as a low-cost and high-mobility platform for communication devices. In this paper, we propose a proximal policy optimization (PPO) based double-agent cooperative reinforcement learning method for channel allocation and trajectory control of UAV to collect and synchronize data from the physical world to the virtual world, and expand the coverage of Metaverse services economically. Simulation results show that our proposed method is able to achieve better performance compared to the benchmark approaches.",
      "published": "2023-01-04T07:35:09Z"
    },
    "metadata": {
      "arxiv_id": "2301.01474",
      "title": "UAV aided Metaverse over Wireless Communications: A Reinforcement Learning Approach",
      "summary": "Metaverse is expected to create a virtual world closely connected with reality to provide users with immersive experience with the support of 5G high data rate communication technique. A huge amount of data in physical world needs to be synchronized to the virtual world to provide immersive experience for users, and there will be higher requirements on coverage to include more users into Metaverse. However, 5G signal suffers severe attenuation, which makes it more expensive to maintain the same coverage. Unmanned aerial vehicle (UAV) is a promising candidate technique for future implementation of Metaverse as a low-cost and high-mobility platform for communication devices. In this paper, we propose a proximal policy optimization (PPO) based double-agent cooperative reinforcement learning method for channel allocation and trajectory control of UAV to collect and synchronize data from the physical world to the virtual world, and expand the coverage of Metaverse services economically. Simulation results show that our proposed method is able to achieve better performance compared to the benchmark approaches.",
      "authors": [
        "Peiyuan Si",
        "Wenhan Yu",
        "Jun Zhao",
        "Kwok-Yan Lam",
        "Qing Yang"
      ],
      "published": "2023-01-04T07:35:09Z",
      "updated": "2023-01-04T07:35:09Z",
      "categories": [
        "eess.SY",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2301.01474v1",
      "landing_url": "https://arxiv.org/abs/2301.01474v1",
      "doi": "https://doi.org/10.48550/arXiv.2301.01474"
    },
    "queries": [
      {
        "anchor": "synchronous dialogue",
        "search_term": "synchronous data"
      }
    ]
  },
  {
    "arxiv_id": "2301.02111",
    "anchor": "spoken language models",
    "search_term": "neural codec",
    "search_record": {
      "id": "http://arxiv.org/abs/2301.02111v1",
      "title": "Neural Codec Language Models are Zero-Shot Text to Speech Synthesizers",
      "summary": "We introduce a language modeling approach for text to speech synthesis (TTS). Specifically, we train a neural codec language model (called Vall-E) using discrete codes derived from an off-the-shelf neural audio codec model, and regard TTS as a conditional language modeling task rather than continuous signal regression as in previous work. During the pre-training stage, we scale up the TTS training data to 60K hours of English speech which is hundreds of times larger than existing systems. Vall-E emerges in-context learning capabilities and can be used to synthesize high-quality personalized speech with only a 3-second enrolled recording of an unseen speaker as an acoustic prompt. Experiment results show that Vall-E significantly outperforms the state-of-the-art zero-shot TTS system in terms of speech naturalness and speaker similarity. In addition, we find Vall-E could preserve the speaker's emotion and acoustic environment of the acoustic prompt in synthesis. See https://aka.ms/valle for demos of our work.",
      "published": "2023-01-05T15:37:15Z"
    },
    "metadata": {
      "arxiv_id": "2301.02111",
      "title": "Neural Codec Language Models are Zero-Shot Text to Speech Synthesizers",
      "summary": "We introduce a language modeling approach for text to speech synthesis (TTS). Specifically, we train a neural codec language model (called Vall-E) using discrete codes derived from an off-the-shelf neural audio codec model, and regard TTS as a conditional language modeling task rather than continuous signal regression as in previous work. During the pre-training stage, we scale up the TTS training data to 60K hours of English speech which is hundreds of times larger than existing systems. Vall-E emerges in-context learning capabilities and can be used to synthesize high-quality personalized speech with only a 3-second enrolled recording of an unseen speaker as an acoustic prompt. Experiment results show that Vall-E significantly outperforms the state-of-the-art zero-shot TTS system in terms of speech naturalness and speaker similarity. In addition, we find Vall-E could preserve the speaker's emotion and acoustic environment of the acoustic prompt in synthesis. See https://aka.ms/valle for demos of our work.",
      "authors": [
        "Chengyi Wang",
        "Sanyuan Chen",
        "Yu Wu",
        "Ziqiang Zhang",
        "Long Zhou",
        "Shujie Liu",
        "Zhuo Chen",
        "Yanqing Liu",
        "Huaming Wang",
        "Jinyu Li",
        "Lei He",
        "Sheng Zhao",
        "Furu Wei"
      ],
      "published": "2023-01-05T15:37:15Z",
      "updated": "2023-01-05T15:37:15Z",
      "categories": [
        "cs.CL",
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2301.02111v1",
      "landing_url": "https://arxiv.org/abs/2301.02111v1",
      "doi": "https://doi.org/10.48550/arXiv.2301.02111"
    },
    "queries": [
      {
        "anchor": "spoken language models",
        "search_term": "neural codec"
      },
      {
        "anchor": "synchronous dialogue",
        "search_term": "neural codec"
      },
      {
        "anchor": "turn-taking",
        "search_term": "neural codec"
      }
    ]
  },
  {
    "arxiv_id": "2301.02598",
    "anchor": "spoken language models",
    "search_term": "temporal dynamics",
    "search_record": {
      "id": "http://arxiv.org/abs/2301.02598v1",
      "title": "Online Fusion of Multi-resolution Multispectral Images with Weakly Supervised Temporal Dynamics",
      "summary": "Real-time satellite imaging has a central role in monitoring, detecting and estimating the intensity of key natural phenomena such as floods, earthquakes, etc. One important constraint of satellite imaging is the trade-off between spatial/spectral resolution and their revisiting time, a consequence of design and physical constraints imposed by satellite orbit among other technical limitations. In this paper, we focus on fusing multi-temporal, multi-spectral images where data acquired from different instruments with different spatial resolutions is used. We leverage the spatial relationship between images at multiple modalities to generate high-resolution image sequences at higher revisiting rates. To achieve this goal, we formulate the fusion method as a recursive state estimation problem and study its performance in filtering and smoothing contexts. Furthermore, a calibration strategy is proposed to estimate the time-varying temporal dynamics of the image sequence using only a small amount of historical image data. Differently from the training process in traditional machine learning algorithms, which usually require large datasets and computation times, the parameters of the temporal dynamical model are calibrated based on an analytical expression that uses only two of the images in the historical dataset. A distributed version of the Bayesian filtering and smoothing strategies is also proposed to reduce its computational complexity. To evaluate the proposed methodology we consider a water mapping task where real data acquired by the Landsat and MODIS instruments are fused generating high spatial-temporal resolution image estimates. Our experiments show that the proposed methodology outperforms the competing methods in both estimation accuracy and water mapping tasks.",
      "published": "2023-01-06T16:48:33Z"
    },
    "metadata": {
      "arxiv_id": "2301.02598",
      "title": "Online Fusion of Multi-resolution Multispectral Images with Weakly Supervised Temporal Dynamics",
      "summary": "Real-time satellite imaging has a central role in monitoring, detecting and estimating the intensity of key natural phenomena such as floods, earthquakes, etc. One important constraint of satellite imaging is the trade-off between spatial/spectral resolution and their revisiting time, a consequence of design and physical constraints imposed by satellite orbit among other technical limitations. In this paper, we focus on fusing multi-temporal, multi-spectral images where data acquired from different instruments with different spatial resolutions is used. We leverage the spatial relationship between images at multiple modalities to generate high-resolution image sequences at higher revisiting rates. To achieve this goal, we formulate the fusion method as a recursive state estimation problem and study its performance in filtering and smoothing contexts. Furthermore, a calibration strategy is proposed to estimate the time-varying temporal dynamics of the image sequence using only a small amount of historical image data. Differently from the training process in traditional machine learning algorithms, which usually require large datasets and computation times, the parameters of the temporal dynamical model are calibrated based on an analytical expression that uses only two of the images in the historical dataset. A distributed version of the Bayesian filtering and smoothing strategies is also proposed to reduce its computational complexity. To evaluate the proposed methodology we consider a water mapping task where real data acquired by the Landsat and MODIS instruments are fused generating high spatial-temporal resolution image estimates. Our experiments show that the proposed methodology outperforms the competing methods in both estimation accuracy and water mapping tasks.",
      "authors": [
        "Haoqing Li",
        "Bhavya Duvvuri",
        "Ricardo Borsoi",
        "Tales Imbiriba",
        "Edward Beighley",
        "Deniz Erdogmus",
        "Pau Closas"
      ],
      "published": "2023-01-06T16:48:33Z",
      "updated": "2023-01-06T16:48:33Z",
      "categories": [
        "eess.IV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2301.02598v1",
      "landing_url": "https://arxiv.org/abs/2301.02598v1",
      "doi": "https://doi.org/10.48550/arXiv.2301.02598"
    },
    "queries": [
      {
        "anchor": "spoken language models",
        "search_term": "temporal dynamics"
      },
      {
        "anchor": "synchronous dialogue",
        "search_term": "temporal dynamics"
      }
    ]
  },
  {
    "arxiv_id": "2301.04030",
    "anchor": "turn-taking",
    "search_term": "true full-duplex",
    "search_record": {
      "id": "http://arxiv.org/abs/2301.04030v1",
      "title": "Conversational Turn-taking as a Stochastic Process on Networks",
      "summary": "Understanding why certain individuals work well (or poorly) together as a team is a key research focus in the psychological and behavioral sciences and a fundamental problem for team-based organizations. Nevertheless, we have a limited ability to predict the social and work-related dynamics that will emerge from a given combination of team members. In this work, we model vocal turn-taking behavior within conversations as a parametric stochastic process on a network composed of the team members. More precisely, we model the dynamic of exchanging the `speaker token' among team members as a random walk in a graph that is driven by both individual level features and the conversation history. We fit our model to conversational turn-taking data extracted from audio recordings of multinational student teams during undergraduate engineering design internships. Through this real-world data we validate the explanatory power of our model and we unveil statistically significant differences in speaking behaviors between team members of different nationalities.",
      "published": "2023-01-10T15:31:16Z"
    },
    "metadata": {
      "arxiv_id": "2301.04030",
      "title": "Conversational Turn-taking as a Stochastic Process on Networks",
      "summary": "Understanding why certain individuals work well (or poorly) together as a team is a key research focus in the psychological and behavioral sciences and a fundamental problem for team-based organizations. Nevertheless, we have a limited ability to predict the social and work-related dynamics that will emerge from a given combination of team members. In this work, we model vocal turn-taking behavior within conversations as a parametric stochastic process on a network composed of the team members. More precisely, we model the dynamic of exchanging the `speaker token' among team members as a random walk in a graph that is driven by both individual level features and the conversation history. We fit our model to conversational turn-taking data extracted from audio recordings of multinational student teams during undergraduate engineering design internships. Through this real-world data we validate the explanatory power of our model and we unveil statistically significant differences in speaking behaviors between team members of different nationalities.",
      "authors": [
        "Lisa O'Bryan",
        "Santiago Segarra",
        "Jensine Paoletti",
        "Stephanie Zajac",
        "Margaret E. Beier",
        "Ashutosh Sabharwal",
        "Matthew Wettergreen",
        "Eduardo Salas"
      ],
      "published": "2023-01-10T15:31:16Z",
      "updated": "2023-01-10T15:31:16Z",
      "categories": [
        "cs.SI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2301.04030v1",
      "landing_url": "https://arxiv.org/abs/2301.04030v1",
      "doi": "https://doi.org/10.48550/arXiv.2301.04030"
    },
    "queries": [
      {
        "anchor": "turn-taking",
        "search_term": "true full-duplex"
      },
      {
        "anchor": "turn-taking",
        "search_term": "pseudo full-duplex"
      },
      {
        "anchor": "turn-taking",
        "search_term": "half duplex"
      },
      {
        "anchor": "turn-taking",
        "search_term": "engineered synchronization"
      },
      {
        "anchor": "turn-taking",
        "search_term": "learned synchronization"
      },
      {
        "anchor": "turn-taking",
        "search_term": "modular architecture"
      },
      {
        "anchor": "turn-taking",
        "search_term": "streaming architecture"
      },
      {
        "anchor": "turn-taking",
        "search_term": "real-time interaction"
      },
      {
        "anchor": "turn-taking",
        "search_term": "low latency"
      },
      {
        "anchor": "turn-taking",
        "search_term": "synchronous data"
      },
      {
        "anchor": "turn-taking",
        "search_term": "data scarcity"
      },
      {
        "anchor": "turn-taking",
        "search_term": "multichannel dialogue"
      },
      {
        "anchor": "turn-taking",
        "search_term": "temporal dynamics"
      },
      {
        "anchor": "turn-taking",
        "search_term": "behavioral arbitration"
      },
      {
        "anchor": "turn-taking",
        "search_term": "semantic coherence"
      },
      {
        "anchor": "turn-taking",
        "search_term": "neural codec"
      },
      {
        "anchor": "turn-taking",
        "search_term": "discrete tokens"
      },
      {
        "anchor": "turn-taking",
        "search_term": "audio tokens"
      },
      {
        "anchor": "turn-taking",
        "search_term": "floor transfer offset"
      },
      {
        "anchor": "turn-taking",
        "search_term": "stop latency"
      },
      {
        "anchor": "turn-taking",
        "search_term": "interruption response delay"
      }
    ]
  },
  {
    "arxiv_id": "2301.04451",
    "anchor": "synchronous dialogue",
    "search_term": "streaming architecture",
    "search_record": {
      "id": "http://arxiv.org/abs/2301.04451v1",
      "title": "Heterogeneous Tri-stream Clustering Network",
      "summary": "Contrastive deep clustering has recently gained significant attention with its ability of joint contrastive learning and clustering via deep neural networks. Despite the rapid progress, previous works mostly require both positive and negative sample pairs for contrastive clustering, which rely on a relative large batch-size. Moreover, they typically adopt a two-stream architecture with two augmented views, which overlook the possibility and potential benefits of multi-stream architectures (especially with heterogeneous or hybrid networks). In light of this, this paper presents a new end-to-end deep clustering approach termed Heterogeneous Tri-stream Clustering Network (HTCN). The tri-stream architecture in HTCN consists of three main components, including two weight-sharing online networks and a target network, where the parameters of the target network are the exponential moving average of that of the online networks. Notably, the two online networks are trained by simultaneously (i) predicting the instance representations of the target network and (ii) enforcing the consistency between the cluster representations of the target network and that of the two online networks. Experimental results on four challenging image datasets demonstrate the superiority of HTCN over the state-of-the-art deep clustering approaches. The code is available at https://github.com/dengxiaozhi/HTCN.",
      "published": "2023-01-11T13:15:54Z"
    },
    "metadata": {
      "arxiv_id": "2301.04451",
      "title": "Heterogeneous Tri-stream Clustering Network",
      "summary": "Contrastive deep clustering has recently gained significant attention with its ability of joint contrastive learning and clustering via deep neural networks. Despite the rapid progress, previous works mostly require both positive and negative sample pairs for contrastive clustering, which rely on a relative large batch-size. Moreover, they typically adopt a two-stream architecture with two augmented views, which overlook the possibility and potential benefits of multi-stream architectures (especially with heterogeneous or hybrid networks). In light of this, this paper presents a new end-to-end deep clustering approach termed Heterogeneous Tri-stream Clustering Network (HTCN). The tri-stream architecture in HTCN consists of three main components, including two weight-sharing online networks and a target network, where the parameters of the target network are the exponential moving average of that of the online networks. Notably, the two online networks are trained by simultaneously (i) predicting the instance representations of the target network and (ii) enforcing the consistency between the cluster representations of the target network and that of the two online networks. Experimental results on four challenging image datasets demonstrate the superiority of HTCN over the state-of-the-art deep clustering approaches. The code is available at https://github.com/dengxiaozhi/HTCN.",
      "authors": [
        "Xiaozhi Deng",
        "Dong Huang",
        "Chang-Dong Wang"
      ],
      "published": "2023-01-11T13:15:54Z",
      "updated": "2023-01-11T13:15:54Z",
      "categories": [
        "cs.LG",
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2301.04451v1",
      "landing_url": "https://arxiv.org/abs/2301.04451v1",
      "doi": "https://doi.org/10.48550/arXiv.2301.04451"
    },
    "queries": [
      {
        "anchor": "synchronous dialogue",
        "search_term": "streaming architecture"
      }
    ]
  },
  {
    "arxiv_id": "2301.05489",
    "anchor": "spoken language models",
    "search_term": "neural codec",
    "search_record": {
      "id": "http://arxiv.org/abs/2301.05489v3",
      "title": "A Residual Diffusion Model for High Perceptual Quality Codec Augmentation",
      "summary": "Diffusion probabilistic models have recently achieved remarkable success in generating high quality image and video data. In this work, we build on this class of generative models and introduce a method for lossy compression of high resolution images. The resulting codec, which we call DIffuson-based Residual Augmentation Codec (DIRAC), is the first neural codec to allow smooth traversal of the rate-distortion-perception tradeoff at test time, while obtaining competitive performance with GAN-based methods in perceptual quality. Furthermore, while sampling from diffusion probabilistic models is notoriously expensive, we show that in the compression setting the number of steps can be drastically reduced.",
      "published": "2023-01-13T11:27:26Z"
    },
    "metadata": {
      "arxiv_id": "2301.05489",
      "title": "A Residual Diffusion Model for High Perceptual Quality Codec Augmentation",
      "summary": "Diffusion probabilistic models have recently achieved remarkable success in generating high quality image and video data. In this work, we build on this class of generative models and introduce a method for lossy compression of high resolution images. The resulting codec, which we call DIffuson-based Residual Augmentation Codec (DIRAC), is the first neural codec to allow smooth traversal of the rate-distortion-perception tradeoff at test time, while obtaining competitive performance with GAN-based methods in perceptual quality. Furthermore, while sampling from diffusion probabilistic models is notoriously expensive, we show that in the compression setting the number of steps can be drastically reduced.",
      "authors": [
        "Noor Fathima Ghouse",
        "Jens Petersen",
        "Auke Wiggers",
        "Tianlin Xu",
        "Guillaume Sautière"
      ],
      "published": "2023-01-13T11:27:26Z",
      "updated": "2023-03-29T16:13:22Z",
      "categories": [
        "cs.CV",
        "eess.IV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2301.05489v3",
      "landing_url": "https://arxiv.org/abs/2301.05489v3",
      "doi": "https://doi.org/10.48550/arXiv.2301.05489"
    },
    "queries": [
      {
        "anchor": "spoken language models",
        "search_term": "neural codec"
      },
      {
        "anchor": "synchronous dialogue",
        "search_term": "neural codec"
      }
    ]
  },
  {
    "arxiv_id": "2301.05821",
    "anchor": "synchronous dialogue",
    "search_term": "real-time interaction",
    "search_record": {
      "id": "http://arxiv.org/abs/2301.05821v4",
      "title": "A Reconfigurable Data Glove for Reconstructing Physical and Virtual Grasps",
      "summary": "In this work, we present a reconfigurable data glove design to capture different modes of human hand-object interactions, which are critical in training embodied artificial intelligence (AI) agents for fine manipulation tasks. To achieve various downstream tasks with distinct features, our reconfigurable data glove operates in three modes sharing a unified backbone design that reconstructs hand gestures in real time. In the tactile-sensing mode, the glove system aggregates manipulation force via customized force sensors made from a soft and thin piezoresistive material; this design minimizes interference during complex hand movements. The virtual reality (VR) mode enables real-time interaction in a physically plausible fashion: A caging-based approach is devised to determine stable grasps by detecting collision events. Leveraging a state-of-the-art finite element method (FEM), the simulation mode collects data on fine-grained 4D manipulation events comprising hand and object motions in 3D space and how the object's physical properties (e.g., stress and energy) change in accordance with manipulation over time. Notably, the glove system presented here is the first to use high-fidelity simulation to investigate the unobservable physical and causal factors behind manipulation actions. In a series of experiments, we characterize our data glove in terms of individual sensors and the overall system. More specifically, we evaluate the system's three modes by (i) recording hand gestures and associated forces, (ii) improving manipulation fluency in VR, and (iii) producing realistic simulation effects of various tool uses, respectively. Based on these three modes, our reconfigurable data glove collects and reconstructs fine-grained human grasp data in both physical and virtual environments, thereby opening up new avenues for the learning of manipulation skills for embodied AI agents.",
      "published": "2023-01-14T05:35:50Z"
    },
    "metadata": {
      "arxiv_id": "2301.05821",
      "title": "A Reconfigurable Data Glove for Reconstructing Physical and Virtual Grasps",
      "summary": "In this work, we present a reconfigurable data glove design to capture different modes of human hand-object interactions, which are critical in training embodied artificial intelligence (AI) agents for fine manipulation tasks. To achieve various downstream tasks with distinct features, our reconfigurable data glove operates in three modes sharing a unified backbone design that reconstructs hand gestures in real time. In the tactile-sensing mode, the glove system aggregates manipulation force via customized force sensors made from a soft and thin piezoresistive material; this design minimizes interference during complex hand movements. The virtual reality (VR) mode enables real-time interaction in a physically plausible fashion: A caging-based approach is devised to determine stable grasps by detecting collision events. Leveraging a state-of-the-art finite element method (FEM), the simulation mode collects data on fine-grained 4D manipulation events comprising hand and object motions in 3D space and how the object's physical properties (e.g., stress and energy) change in accordance with manipulation over time. Notably, the glove system presented here is the first to use high-fidelity simulation to investigate the unobservable physical and causal factors behind manipulation actions. In a series of experiments, we characterize our data glove in terms of individual sensors and the overall system. More specifically, we evaluate the system's three modes by (i) recording hand gestures and associated forces, (ii) improving manipulation fluency in VR, and (iii) producing realistic simulation effects of various tool uses, respectively. Based on these three modes, our reconfigurable data glove collects and reconstructs fine-grained human grasp data in both physical and virtual environments, thereby opening up new avenues for the learning of manipulation skills for embodied AI agents.",
      "authors": [
        "Hangxin Liu",
        "Zeyu Zhang",
        "Ziyuan Jiao",
        "Zhenliang Zhang",
        "Minchen Li",
        "Chenfanfu Jiang",
        "Yixin Zhu",
        "Song-Chun Zhu"
      ],
      "published": "2023-01-14T05:35:50Z",
      "updated": "2023-02-02T02:09:19Z",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.HC"
      ],
      "pdf_url": "https://arxiv.org/pdf/2301.05821v4",
      "landing_url": "https://arxiv.org/abs/2301.05821v4",
      "doi": "https://doi.org/10.48550/arXiv.2301.05821"
    },
    "queries": [
      {
        "anchor": "synchronous dialogue",
        "search_term": "real-time interaction"
      }
    ]
  },
  {
    "arxiv_id": "2301.06754",
    "anchor": "spoken language models",
    "search_term": "low latency",
    "search_record": {
      "id": "http://arxiv.org/abs/2301.06754v1",
      "title": "Real-time, low latency virtual DBA hypervisor for SLA-compliant multi-service operations over shared Passive Optical Networks",
      "summary": "We present a heuristic algorithm for a PON upstream scheduling hypervisor,supporting low latency services with strict service-level agreement. The algorithm achieves near-optimal performance while running in only 3.5 us, thus operating in real-time.",
      "published": "2023-01-17T08:39:18Z"
    },
    "metadata": {
      "arxiv_id": "2301.06754",
      "title": "Real-time, low latency virtual DBA hypervisor for SLA-compliant multi-service operations over shared Passive Optical Networks",
      "summary": "We present a heuristic algorithm for a PON upstream scheduling hypervisor,supporting low latency services with strict service-level agreement. The algorithm achieves near-optimal performance while running in only 3.5 us, thus operating in real-time.",
      "authors": [
        "Arijeet Ganguli",
        "Frank Slyne",
        "Marco Ruffini"
      ],
      "published": "2023-01-17T08:39:18Z",
      "updated": "2023-01-17T08:39:18Z",
      "categories": [
        "cs.NI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2301.06754v1",
      "landing_url": "https://arxiv.org/abs/2301.06754v1",
      "doi": "https://doi.org/10.48550/arXiv.2301.06754"
    },
    "queries": [
      {
        "anchor": "spoken language models",
        "search_term": "low latency"
      },
      {
        "anchor": "synchronous dialogue",
        "search_term": "low latency"
      }
    ]
  },
  {
    "arxiv_id": "2301.06774",
    "anchor": "spoken language models",
    "search_term": "temporal dynamics",
    "search_record": {
      "id": "http://arxiv.org/abs/2301.06774v2",
      "title": "Temporal Dynamics of Coordinated Online Behavior: Stability, Archetypes, and Influence",
      "summary": "Large-scale online campaigns, malicious or otherwise, require a significant degree of coordination among participants, which sparked interest in the study of coordinated online behavior. State-of-the-art methods for detecting coordinated behavior perform static analyses, disregarding the temporal dynamics of coordination. Here, we carry out the first dynamic analysis of coordinated behavior. To reach our goal we build a multiplex temporal network and we perform dynamic community detection to identify groups of users that exhibited coordinated behaviors in time. Thanks to our novel approach we find that: (i) coordinated communities feature variable degrees of temporal instability; (ii) dynamic analyses are needed to account for such instability, and results of static analyses can be unreliable and scarcely representative of unstable communities; (iii) some users exhibit distinct archetypal behaviors that have important practical implications; (iv) content and network characteristics contribute to explaining why users leave and join coordinated communities. Our results demonstrate the advantages of dynamic analyses and open up new directions of research on the unfolding of online debates, on the strategies of coordinated communities, and on the patterns of online influence.",
      "published": "2023-01-17T09:52:54Z"
    },
    "metadata": {
      "arxiv_id": "2301.06774",
      "title": "Temporal Dynamics of Coordinated Online Behavior: Stability, Archetypes, and Influence",
      "summary": "Large-scale online campaigns, malicious or otherwise, require a significant degree of coordination among participants, which sparked interest in the study of coordinated online behavior. State-of-the-art methods for detecting coordinated behavior perform static analyses, disregarding the temporal dynamics of coordination. Here, we carry out the first dynamic analysis of coordinated behavior. To reach our goal we build a multiplex temporal network and we perform dynamic community detection to identify groups of users that exhibited coordinated behaviors in time. Thanks to our novel approach we find that: (i) coordinated communities feature variable degrees of temporal instability; (ii) dynamic analyses are needed to account for such instability, and results of static analyses can be unreliable and scarcely representative of unstable communities; (iii) some users exhibit distinct archetypal behaviors that have important practical implications; (iv) content and network characteristics contribute to explaining why users leave and join coordinated communities. Our results demonstrate the advantages of dynamic analyses and open up new directions of research on the unfolding of online debates, on the strategies of coordinated communities, and on the patterns of online influence.",
      "authors": [
        "Serena Tardelli",
        "Leonardo Nizzoli",
        "Maurizio Tesconi",
        "Mauro Conti",
        "Preslav Nakov",
        "Giovanni Da San Martino",
        "Stefano Cresci"
      ],
      "published": "2023-01-17T09:52:54Z",
      "updated": "2024-05-09T10:15:28Z",
      "categories": [
        "cs.SI",
        "cs.CL",
        "cs.CY"
      ],
      "pdf_url": "https://arxiv.org/pdf/2301.06774v2",
      "landing_url": "https://arxiv.org/abs/2301.06774v2",
      "doi": "https://doi.org/10.1073/pnas.2307038121"
    },
    "queries": [
      {
        "anchor": "spoken language models",
        "search_term": "temporal dynamics"
      },
      {
        "anchor": "synchronous dialogue",
        "search_term": "temporal dynamics"
      }
    ]
  },
  {
    "arxiv_id": "2301.07178",
    "anchor": "synchronous dialogue",
    "search_term": "data scarcity",
    "search_record": {
      "id": "http://arxiv.org/abs/2301.07178v1",
      "title": "Using Large Text-to-Image Models with Structured Prompts for Skin Disease Identification: A Case Study",
      "summary": "This paper investigates the potential usage of large text-to-image (LTI) models for the automated diagnosis of a few skin conditions with rarity or a serious lack of annotated datasets. As the input to the LTI model, we provide the targeted instantiation of a generic but succinct prompt structure designed upon careful observations of the conditional narratives from the standard medical textbooks. In this regard, we pave the path to utilizing accessible textbook descriptions for automated diagnosis of conditions with data scarcity through the lens of LTI models. Experiments show the efficacy of the proposed framework, including much better localization of the infected regions. Moreover, it has the immense possibility for generalization across the medical sub-domains, not only to mitigate the data scarcity issue but also to debias automated diagnostics from the all-pervasive racial biases.",
      "published": "2023-01-17T20:37:02Z"
    },
    "metadata": {
      "arxiv_id": "2301.07178",
      "title": "Using Large Text-to-Image Models with Structured Prompts for Skin Disease Identification: A Case Study",
      "summary": "This paper investigates the potential usage of large text-to-image (LTI) models for the automated diagnosis of a few skin conditions with rarity or a serious lack of annotated datasets. As the input to the LTI model, we provide the targeted instantiation of a generic but succinct prompt structure designed upon careful observations of the conditional narratives from the standard medical textbooks. In this regard, we pave the path to utilizing accessible textbook descriptions for automated diagnosis of conditions with data scarcity through the lens of LTI models. Experiments show the efficacy of the proposed framework, including much better localization of the infected regions. Moreover, it has the immense possibility for generalization across the medical sub-domains, not only to mitigate the data scarcity issue but also to debias automated diagnostics from the all-pervasive racial biases.",
      "authors": [
        "Sajith Rajapaksa",
        "Jean Marie Uwabeza Vianney",
        "Renell Castro",
        "Farzad Khalvati",
        "Shubhra Aich"
      ],
      "published": "2023-01-17T20:37:02Z",
      "updated": "2023-01-17T20:37:02Z",
      "categories": [
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2301.07178v1",
      "landing_url": "https://arxiv.org/abs/2301.07178v1",
      "doi": "https://doi.org/10.48550/arXiv.2301.07178"
    },
    "queries": [
      {
        "anchor": "synchronous dialogue",
        "search_term": "data scarcity"
      }
    ]
  },
  {
    "arxiv_id": "2301.07371",
    "anchor": "spoken language models",
    "search_term": "low latency",
    "search_record": {
      "id": "http://arxiv.org/abs/2301.07371v1",
      "title": "Demonstration of a low latency bandwidth allocation mechanism for mission critical applications in virtual PONs with P4 programmable hardware",
      "summary": "We provide a real-time demonstration of a low-latency PON DBA mechanism, optimised for virtual PONs. Our implementation mixes P4 programmable data plane and software-based virtual DBA to provide efficient fast-track allocation for low latency applications",
      "published": "2023-01-18T08:38:53Z"
    },
    "metadata": {
      "arxiv_id": "2301.07371",
      "title": "Demonstration of a low latency bandwidth allocation mechanism for mission critical applications in virtual PONs with P4 programmable hardware",
      "summary": "We provide a real-time demonstration of a low-latency PON DBA mechanism, optimised for virtual PONs. Our implementation mixes P4 programmable data plane and software-based virtual DBA to provide efficient fast-track allocation for low latency applications",
      "authors": [
        "D. R. Mafioletti",
        "F. Slyne",
        "R. Giller",
        "M. OHanlon",
        "D. Coyle",
        "B. Ryan",
        "M. Ruffini"
      ],
      "published": "2023-01-18T08:38:53Z",
      "updated": "2023-01-18T08:38:53Z",
      "categories": [
        "cs.NI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2301.07371v1",
      "landing_url": "https://arxiv.org/abs/2301.07371v1",
      "doi": "https://doi.org/10.48550/arXiv.2301.07371"
    },
    "queries": [
      {
        "anchor": "spoken language models",
        "search_term": "low latency"
      },
      {
        "anchor": "synchronous dialogue",
        "search_term": "low latency"
      },
      {
        "anchor": "turn-taking",
        "search_term": "low latency"
      }
    ]
  },
  {
    "arxiv_id": "2301.07372",
    "anchor": "spoken language models",
    "search_term": "low latency",
    "search_record": {
      "id": "http://arxiv.org/abs/2301.07372v1",
      "title": "A Novel low-latency DBA for Virtualised PON implemented through P4 In-Network Processing",
      "summary": "We present a novel dual-DBA allocation, with a fast P4-enabled scheduler to provide low latency upstream grant allocations. We show latency reduction of 37% and 43%, respectively, compared to standard and virtual PONs",
      "published": "2023-01-18T08:41:18Z"
    },
    "metadata": {
      "arxiv_id": "2301.07372",
      "title": "A Novel low-latency DBA for Virtualised PON implemented through P4 In-Network Processing",
      "summary": "We present a novel dual-DBA allocation, with a fast P4-enabled scheduler to provide low latency upstream grant allocations. We show latency reduction of 37% and 43%, respectively, compared to standard and virtual PONs",
      "authors": [
        "D. R. Mafioletti",
        "F. Slyne",
        "R. Giller",
        "M. OHanlon",
        "B. Ryan",
        "M. Ruffini"
      ],
      "published": "2023-01-18T08:41:18Z",
      "updated": "2023-01-18T08:41:18Z",
      "categories": [
        "cs.NI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2301.07372v1",
      "landing_url": "https://arxiv.org/abs/2301.07372v1",
      "doi": "https://doi.org/10.48550/arXiv.2301.07372"
    },
    "queries": [
      {
        "anchor": "spoken language models",
        "search_term": "low latency"
      },
      {
        "anchor": "synchronous dialogue",
        "search_term": "low latency"
      }
    ]
  },
  {
    "arxiv_id": "2301.08441",
    "anchor": "synchronous dialogue",
    "search_term": "data scarcity",
    "search_record": {
      "id": "http://arxiv.org/abs/2301.08441v1",
      "title": "Self-Supervised Learning for Data Scarcity in a Fatigue Damage Prognostic Problem",
      "summary": "With the increasing availability of data for Prognostics and Health Management (PHM), Deep Learning (DL) techniques are now the subject of considerable attention for this application, often achieving more accurate Remaining Useful Life (RUL) predictions. However, one of the major challenges for DL techniques resides in the difficulty of obtaining large amounts of labelled data on industrial systems. To overcome this lack of labelled data, an emerging learning technique is considered in our work: Self-Supervised Learning, a sub-category of unsupervised learning approaches. This paper aims to investigate whether pre-training DL models in a self-supervised way on unlabelled sensors data can be useful for RUL estimation with only Few-Shots Learning, i.e. with scarce labelled data. In this research, a fatigue damage prognostics problem is addressed, through the estimation of the RUL of aluminum alloy panels (typical of aerospace structures) subject to fatigue cracks from strain gauge data. Synthetic datasets composed of strain data are used allowing to extensively investigate the influence of the dataset size on the predictive performance. Results show that the self-supervised pre-trained models are able to significantly outperform the non-pre-trained models in downstream RUL prediction task, and with less computational expense, showing promising results in prognostic tasks when only limited labelled data is available.",
      "published": "2023-01-20T06:45:32Z"
    },
    "metadata": {
      "arxiv_id": "2301.08441",
      "title": "Self-Supervised Learning for Data Scarcity in a Fatigue Damage Prognostic Problem",
      "summary": "With the increasing availability of data for Prognostics and Health Management (PHM), Deep Learning (DL) techniques are now the subject of considerable attention for this application, often achieving more accurate Remaining Useful Life (RUL) predictions. However, one of the major challenges for DL techniques resides in the difficulty of obtaining large amounts of labelled data on industrial systems. To overcome this lack of labelled data, an emerging learning technique is considered in our work: Self-Supervised Learning, a sub-category of unsupervised learning approaches. This paper aims to investigate whether pre-training DL models in a self-supervised way on unlabelled sensors data can be useful for RUL estimation with only Few-Shots Learning, i.e. with scarce labelled data. In this research, a fatigue damage prognostics problem is addressed, through the estimation of the RUL of aluminum alloy panels (typical of aerospace structures) subject to fatigue cracks from strain gauge data. Synthetic datasets composed of strain data are used allowing to extensively investigate the influence of the dataset size on the predictive performance. Results show that the self-supervised pre-trained models are able to significantly outperform the non-pre-trained models in downstream RUL prediction task, and with less computational expense, showing promising results in prognostic tasks when only limited labelled data is available.",
      "authors": [
        "Anass Akrim",
        "Christian Gogu",
        "Rob Vingerhoeds",
        "Michel Salaün"
      ],
      "published": "2023-01-20T06:45:32Z",
      "updated": "2023-01-20T06:45:32Z",
      "categories": [
        "stat.ML",
        "cs.AI",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2301.08441v1",
      "landing_url": "https://arxiv.org/abs/2301.08441v1",
      "doi": "https://doi.org/10.1016/j.engappai.2023.105837"
    },
    "queries": [
      {
        "anchor": "synchronous dialogue",
        "search_term": "data scarcity"
      }
    ]
  },
  {
    "arxiv_id": "2301.10931",
    "anchor": "synchronous dialogue",
    "search_term": "synchronous data",
    "search_record": {
      "id": "http://arxiv.org/abs/2301.10931v1",
      "title": "Towards Continual Egocentric Activity Recognition: A Multi-modal Egocentric Activity Dataset for Continual Learning",
      "summary": "With the rapid development of wearable cameras, a massive collection of egocentric video for first-person visual perception becomes available. Using egocentric videos to predict first-person activity faces many challenges, including limited field of view, occlusions, and unstable motions. Observing that sensor data from wearable devices facilitates human activity recognition, multi-modal activity recognition is attracting increasing attention. However, the deficiency of related dataset hinders the development of multi-modal deep learning for egocentric activity recognition. Nowadays, deep learning in real world has led to a focus on continual learning that often suffers from catastrophic forgetting. But the catastrophic forgetting problem for egocentric activity recognition, especially in the context of multiple modalities, remains unexplored due to unavailability of dataset. In order to assist this research, we present a multi-modal egocentric activity dataset for continual learning named UESTC-MMEA-CL, which is collected by self-developed glasses integrating a first-person camera and wearable sensors. It contains synchronized data of videos, accelerometers, and gyroscopes, for 32 types of daily activities, performed by 10 participants. Its class types and scale are compared with other publicly available datasets. The statistical analysis of the sensor data is given to show the auxiliary effects for different behaviors. And results of egocentric activity recognition are reported when using separately, and jointly, three modalities: RGB, acceleration, and gyroscope, on a base network architecture. To explore the catastrophic forgetting in continual learning tasks, four baseline methods are extensively evaluated with different multi-modal combinations. We hope the UESTC-MMEA-CL can promote future studies on continual learning for first-person activity recognition in wearable applications.",
      "published": "2023-01-26T04:32:00Z"
    },
    "metadata": {
      "arxiv_id": "2301.10931",
      "title": "Towards Continual Egocentric Activity Recognition: A Multi-modal Egocentric Activity Dataset for Continual Learning",
      "summary": "With the rapid development of wearable cameras, a massive collection of egocentric video for first-person visual perception becomes available. Using egocentric videos to predict first-person activity faces many challenges, including limited field of view, occlusions, and unstable motions. Observing that sensor data from wearable devices facilitates human activity recognition, multi-modal activity recognition is attracting increasing attention. However, the deficiency of related dataset hinders the development of multi-modal deep learning for egocentric activity recognition. Nowadays, deep learning in real world has led to a focus on continual learning that often suffers from catastrophic forgetting. But the catastrophic forgetting problem for egocentric activity recognition, especially in the context of multiple modalities, remains unexplored due to unavailability of dataset. In order to assist this research, we present a multi-modal egocentric activity dataset for continual learning named UESTC-MMEA-CL, which is collected by self-developed glasses integrating a first-person camera and wearable sensors. It contains synchronized data of videos, accelerometers, and gyroscopes, for 32 types of daily activities, performed by 10 participants. Its class types and scale are compared with other publicly available datasets. The statistical analysis of the sensor data is given to show the auxiliary effects for different behaviors. And results of egocentric activity recognition are reported when using separately, and jointly, three modalities: RGB, acceleration, and gyroscope, on a base network architecture. To explore the catastrophic forgetting in continual learning tasks, four baseline methods are extensively evaluated with different multi-modal combinations. We hope the UESTC-MMEA-CL can promote future studies on continual learning for first-person activity recognition in wearable applications.",
      "authors": [
        "Linfeng Xu",
        "Qingbo Wu",
        "Lili Pan",
        "Fanman Meng",
        "Hongliang Li",
        "Chiyuan He",
        "Hanxin Wang",
        "Shaoxu Cheng",
        "Yu Dai"
      ],
      "published": "2023-01-26T04:32:00Z",
      "updated": "2023-01-26T04:32:00Z",
      "categories": [
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2301.10931v1",
      "landing_url": "https://arxiv.org/abs/2301.10931v1",
      "doi": "https://doi.org/10.1109/TMM.2023.3295899"
    },
    "queries": [
      {
        "anchor": "synchronous dialogue",
        "search_term": "synchronous data"
      }
    ]
  },
  {
    "arxiv_id": "2302.01025",
    "anchor": "spoken language models",
    "search_term": "semantic coherence",
    "search_record": {
      "id": "http://arxiv.org/abs/2302.01025v1",
      "title": "Semantic Coherence Markers for the Early Diagnosis of the Alzheimer Disease",
      "summary": "In this work we explore how language models can be employed to analyze language and discriminate between mentally impaired and healthy subjects through the perplexity metric. Perplexity was originally conceived as an information-theoretic measure to assess how much a given language model is suited to predict a text sequence or, equivalently, how much a word sequence fits into a specific language model. We carried out an extensive experimentation with the publicly available data, and employed language models as diverse as N-grams, from 2-grams to 5-grams, and GPT-2, a transformer-based language model. We investigated whether perplexity scores may be used to discriminate between the transcripts of healthy subjects and subjects suffering from Alzheimer Disease (AD). Our best performing models achieved full accuracy and F-score (1.00 in both precision/specificity and recall/sensitivity) in categorizing subjects from both the AD class and control subjects. These results suggest that perplexity can be a valuable analytical metrics with potential application to supporting early diagnosis of symptoms of mental disorders.",
      "published": "2023-02-02T11:40:16Z"
    },
    "metadata": {
      "arxiv_id": "2302.01025",
      "title": "Semantic Coherence Markers for the Early Diagnosis of the Alzheimer Disease",
      "summary": "In this work we explore how language models can be employed to analyze language and discriminate between mentally impaired and healthy subjects through the perplexity metric. Perplexity was originally conceived as an information-theoretic measure to assess how much a given language model is suited to predict a text sequence or, equivalently, how much a word sequence fits into a specific language model. We carried out an extensive experimentation with the publicly available data, and employed language models as diverse as N-grams, from 2-grams to 5-grams, and GPT-2, a transformer-based language model. We investigated whether perplexity scores may be used to discriminate between the transcripts of healthy subjects and subjects suffering from Alzheimer Disease (AD). Our best performing models achieved full accuracy and F-score (1.00 in both precision/specificity and recall/sensitivity) in categorizing subjects from both the AD class and control subjects. These results suggest that perplexity can be a valuable analytical metrics with potential application to supporting early diagnosis of symptoms of mental disorders.",
      "authors": [
        "Davide Colla",
        "Matteo Delsanto",
        "Marco Agosto",
        "Benedetto Vitiello",
        "Daniele Paolo Radicioni"
      ],
      "published": "2023-02-02T11:40:16Z",
      "updated": "2023-02-02T11:40:16Z",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2302.01025v1",
      "landing_url": "https://arxiv.org/abs/2302.01025v1",
      "doi": "https://doi.org/10.48550/arXiv.2302.01025"
    },
    "queries": [
      {
        "anchor": "spoken language models",
        "search_term": "semantic coherence"
      },
      {
        "anchor": "synchronous dialogue",
        "search_term": "semantic coherence"
      },
      {
        "anchor": "turn-taking",
        "search_term": "semantic coherence"
      }
    ]
  },
  {
    "arxiv_id": "2302.01806",
    "anchor": "spoken language models",
    "search_term": "data scarcity",
    "search_record": {
      "id": "http://arxiv.org/abs/2302.01806v1",
      "title": "Mitigating Data Scarcity for Large Language Models",
      "summary": "In recent years, pretrained neural language models (PNLMs) have taken the field of natural language processing by storm, achieving new benchmarks and state-of-the-art performances. These models often rely heavily on annotated data, which may not always be available. Data scarcity are commonly found in specialized domains, such as medical, or in low-resource languages that are underexplored by AI research. In this dissertation, we focus on mitigating data scarcity using data augmentation and neural ensemble learning techniques for neural language models. In both research directions, we implement neural network algorithms and evaluate their impact on assisting neural language models in downstream NLP tasks. Specifically, for data augmentation, we explore two techniques: 1) creating positive training data by moving an answer span around its original context and 2) using text simplification techniques to introduce a variety of writing styles to the original training data. Our results indicate that these simple and effective solutions improve the performance of neural language models considerably in low-resource NLP domains and tasks. For neural ensemble learning, we use a multilabel neural classifier to select the best prediction outcome from a variety of individual pretrained neural language models trained for a low-resource medical text simplification task.",
      "published": "2023-02-03T15:17:53Z"
    },
    "metadata": {
      "arxiv_id": "2302.01806",
      "title": "Mitigating Data Scarcity for Large Language Models",
      "summary": "In recent years, pretrained neural language models (PNLMs) have taken the field of natural language processing by storm, achieving new benchmarks and state-of-the-art performances. These models often rely heavily on annotated data, which may not always be available. Data scarcity are commonly found in specialized domains, such as medical, or in low-resource languages that are underexplored by AI research. In this dissertation, we focus on mitigating data scarcity using data augmentation and neural ensemble learning techniques for neural language models. In both research directions, we implement neural network algorithms and evaluate their impact on assisting neural language models in downstream NLP tasks. Specifically, for data augmentation, we explore two techniques: 1) creating positive training data by moving an answer span around its original context and 2) using text simplification techniques to introduce a variety of writing styles to the original training data. Our results indicate that these simple and effective solutions improve the performance of neural language models considerably in low-resource NLP domains and tasks. For neural ensemble learning, we use a multilabel neural classifier to select the best prediction outcome from a variety of individual pretrained neural language models trained for a low-resource medical text simplification task.",
      "authors": [
        "Hoang Van"
      ],
      "published": "2023-02-03T15:17:53Z",
      "updated": "2023-02-03T15:17:53Z",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2302.01806v1",
      "landing_url": "https://arxiv.org/abs/2302.01806v1",
      "doi": "https://doi.org/10.48550/arXiv.2302.01806"
    },
    "queries": [
      {
        "anchor": "spoken language models",
        "search_term": "data scarcity"
      },
      {
        "anchor": "synchronous dialogue",
        "search_term": "data scarcity"
      },
      {
        "anchor": "turn-taking",
        "search_term": "data scarcity"
      }
    ]
  },
  {
    "arxiv_id": "2302.10673",
    "anchor": "spoken language models",
    "search_term": "half duplex",
    "search_record": {
      "id": "http://arxiv.org/abs/2302.10673v1",
      "title": "A Framework for UAV-based Distributed Sensing Under Half-Duplex Operation",
      "summary": "This paper proposes an unmanned aerial vehicle (UAV)-based distributed sensing framework that uses frequency-division multiplexing (OFDM) waveforms to detect the position of a ground target under half-duplex operation. The area of interest, where the target is located, is sectioned into a grid of cells, where the radar cross-section (RCS) of every cell is jointly estimated by the UAVs, and a central node acts as a fusion center by receiving all the estimations and performing information-level fusion. For local estimation at each UAV, the periodogram approach is utilised, and a digital receive beamformer is assumed. The fused RCS estimates of the grid are used to estimate the cell containing the target. Monte Carlo simulations are performed to obtain the detection probability of the proposed framework, and our results show that the proposed framework attains improved accuracy for the detection of a target than other OFDM bi-static radar approaches proposed in the literature.",
      "published": "2023-02-21T13:45:27Z"
    },
    "metadata": {
      "arxiv_id": "2302.10673",
      "title": "A Framework for UAV-based Distributed Sensing Under Half-Duplex Operation",
      "summary": "This paper proposes an unmanned aerial vehicle (UAV)-based distributed sensing framework that uses frequency-division multiplexing (OFDM) waveforms to detect the position of a ground target under half-duplex operation. The area of interest, where the target is located, is sectioned into a grid of cells, where the radar cross-section (RCS) of every cell is jointly estimated by the UAVs, and a central node acts as a fusion center by receiving all the estimations and performing information-level fusion. For local estimation at each UAV, the periodogram approach is utilised, and a digital receive beamformer is assumed. The fused RCS estimates of the grid are used to estimate the cell containing the target. Monte Carlo simulations are performed to obtain the detection probability of the proposed framework, and our results show that the proposed framework attains improved accuracy for the detection of a target than other OFDM bi-static radar approaches proposed in the literature.",
      "authors": [
        "Xavier A. Flores Cabezas",
        "Diana P. Moya Osorio",
        "Markku Juntti"
      ],
      "published": "2023-02-21T13:45:27Z",
      "updated": "2023-02-21T13:45:27Z",
      "categories": [
        "eess.SP"
      ],
      "pdf_url": "https://arxiv.org/pdf/2302.10673v1",
      "landing_url": "https://arxiv.org/abs/2302.10673v1",
      "doi": "https://doi.org/10.48550/arXiv.2302.10673"
    },
    "queries": [
      {
        "anchor": "spoken language models",
        "search_term": "half duplex"
      },
      {
        "anchor": "synchronous dialogue",
        "search_term": "half duplex"
      }
    ]
  },
  {
    "arxiv_id": "2302.11412",
    "anchor": "synchronous dialogue",
    "search_term": "data scarcity",
    "search_record": {
      "id": "http://arxiv.org/abs/2302.11412v1",
      "title": "Data Augmentation for Neural NLP",
      "summary": "Data scarcity is a problem that occurs in languages and tasks where we do not have large amounts of labeled data but want to use state-of-the-art models. Such models are often deep learning models that require a significant amount of data to train. Acquiring data for various machine learning problems is accompanied by high labeling costs. Data augmentation is a low-cost approach for tackling data scarcity. This paper gives an overview of current state-of-the-art data augmentation methods used for natural language processing, with an emphasis on methods for neural and transformer-based models. Furthermore, it discusses the practical challenges of data augmentation, possible mitigations, and directions for future research.",
      "published": "2023-02-22T14:47:15Z"
    },
    "metadata": {
      "arxiv_id": "2302.11412",
      "title": "Data Augmentation for Neural NLP",
      "summary": "Data scarcity is a problem that occurs in languages and tasks where we do not have large amounts of labeled data but want to use state-of-the-art models. Such models are often deep learning models that require a significant amount of data to train. Acquiring data for various machine learning problems is accompanied by high labeling costs. Data augmentation is a low-cost approach for tackling data scarcity. This paper gives an overview of current state-of-the-art data augmentation methods used for natural language processing, with an emphasis on methods for neural and transformer-based models. Furthermore, it discusses the practical challenges of data augmentation, possible mitigations, and directions for future research.",
      "authors": [
        "Domagoj Pluščec",
        "Jan Šnajder"
      ],
      "published": "2023-02-22T14:47:15Z",
      "updated": "2023-02-22T14:47:15Z",
      "categories": [
        "cs.CL",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2302.11412v1",
      "landing_url": "https://arxiv.org/abs/2302.11412v1",
      "doi": "https://doi.org/10.48550/arXiv.2302.11412"
    },
    "queries": [
      {
        "anchor": "synchronous dialogue",
        "search_term": "data scarcity"
      }
    ]
  },
  {
    "arxiv_id": "2302.14312",
    "anchor": "spoken language models",
    "search_term": "learned synchronization",
    "search_record": {
      "id": "http://arxiv.org/abs/2302.14312v1",
      "title": "Auxiliary Task-based Deep Reinforcement Learning for Quantum Control",
      "summary": "Due to its property of not requiring prior knowledge of the environment, reinforcement learning has significant potential for quantum control problems. In this work, we investigate the effectiveness of continuous control policies based on deep deterministic policy gradient. To solve the sparse reward signal in quantum learning control problems, we propose an auxiliary task-based deep reinforcement learning (AT-DRL) for quantum control. In particular, we first design a guided reward function based on the fidelity of quantum states that enables incremental fidelity improvement. Then, we introduce the concept of an auxiliary task whose network shares parameters with the main network to predict the reward provided by the environment (called the main task). The auxiliary task learns synchronously with the main task, allowing one to select the most relevant features of the environment, thus aiding the agent in comprehending how to achieve the desired state. The numerical simulations demonstrate that the proposed AT-DRL can provide a solution to the sparse reward in quantum systems, and has great potential in designing control pulses that achieve efficient quantum state preparation.",
      "published": "2023-02-28T05:08:42Z"
    },
    "metadata": {
      "arxiv_id": "2302.14312",
      "title": "Auxiliary Task-based Deep Reinforcement Learning for Quantum Control",
      "summary": "Due to its property of not requiring prior knowledge of the environment, reinforcement learning has significant potential for quantum control problems. In this work, we investigate the effectiveness of continuous control policies based on deep deterministic policy gradient. To solve the sparse reward signal in quantum learning control problems, we propose an auxiliary task-based deep reinforcement learning (AT-DRL) for quantum control. In particular, we first design a guided reward function based on the fidelity of quantum states that enables incremental fidelity improvement. Then, we introduce the concept of an auxiliary task whose network shares parameters with the main network to predict the reward provided by the environment (called the main task). The auxiliary task learns synchronously with the main task, allowing one to select the most relevant features of the environment, thus aiding the agent in comprehending how to achieve the desired state. The numerical simulations demonstrate that the proposed AT-DRL can provide a solution to the sparse reward in quantum systems, and has great potential in designing control pulses that achieve efficient quantum state preparation.",
      "authors": [
        "Shumin Zhou",
        "Hailan Ma",
        "Sen Kuang",
        "Daoyi Dong"
      ],
      "published": "2023-02-28T05:08:42Z",
      "updated": "2023-02-28T05:08:42Z",
      "categories": [
        "quant-ph",
        "cs.LG",
        "eess.SY"
      ],
      "pdf_url": "https://arxiv.org/pdf/2302.14312v1",
      "landing_url": "https://arxiv.org/abs/2302.14312v1",
      "doi": "https://doi.org/10.48550/arXiv.2302.14312"
    },
    "queries": [
      {
        "anchor": "spoken language models",
        "search_term": "learned synchronization"
      },
      {
        "anchor": "synchronous dialogue",
        "search_term": "learned synchronization"
      }
    ]
  },
  {
    "arxiv_id": "2303.01567",
    "anchor": "synchronous dialogue",
    "search_term": "streaming architecture",
    "search_record": {
      "id": "http://arxiv.org/abs/2303.01567v1",
      "title": "Deep Neural Networks with Efficient Guaranteed Invariances",
      "summary": "We address the problem of improving the performance and in particular the sample complexity of deep neural networks by enforcing and guaranteeing invariances to symmetry transformations rather than learning them from data. Group-equivariant convolutions are a popular approach to obtain equivariant representations. The desired corresponding invariance is then imposed using pooling operations. For rotations, it has been shown that using invariant integration instead of pooling further improves the sample complexity. In this contribution, we first expand invariant integration beyond rotations to flips and scale transformations. We then address the problem of incorporating multiple desired invariances into a single network. For this purpose, we propose a multi-stream architecture, where each stream is invariant to a different transformation such that the network can simultaneously benefit from multiple invariances. We demonstrate our approach with successful experiments on Scaled-MNIST, SVHN, CIFAR-10 and STL-10.",
      "published": "2023-03-02T20:44:45Z"
    },
    "metadata": {
      "arxiv_id": "2303.01567",
      "title": "Deep Neural Networks with Efficient Guaranteed Invariances",
      "summary": "We address the problem of improving the performance and in particular the sample complexity of deep neural networks by enforcing and guaranteeing invariances to symmetry transformations rather than learning them from data. Group-equivariant convolutions are a popular approach to obtain equivariant representations. The desired corresponding invariance is then imposed using pooling operations. For rotations, it has been shown that using invariant integration instead of pooling further improves the sample complexity. In this contribution, we first expand invariant integration beyond rotations to flips and scale transformations. We then address the problem of incorporating multiple desired invariances into a single network. For this purpose, we propose a multi-stream architecture, where each stream is invariant to a different transformation such that the network can simultaneously benefit from multiple invariances. We demonstrate our approach with successful experiments on Scaled-MNIST, SVHN, CIFAR-10 and STL-10.",
      "authors": [
        "Matthias Rath",
        "Alexandru Paul Condurache"
      ],
      "published": "2023-03-02T20:44:45Z",
      "updated": "2023-03-02T20:44:45Z",
      "categories": [
        "cs.LG",
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2303.01567v1",
      "landing_url": "https://arxiv.org/abs/2303.01567v1",
      "doi": "https://doi.org/10.48550/arXiv.2303.01567"
    },
    "queries": [
      {
        "anchor": "synchronous dialogue",
        "search_term": "streaming architecture"
      }
    ]
  },
  {
    "arxiv_id": "2303.02959",
    "anchor": "synchronous dialogue",
    "search_term": "neural codec",
    "search_record": {
      "id": "http://arxiv.org/abs/2303.02959v1",
      "title": "Butterfly: Multiple Reference Frames Feature Propagation Mechanism for Neural Video Compression",
      "summary": "Using more reference frames can significantly improve the compression efficiency in neural video compression. However, in low-latency scenarios, most existing neural video compression frameworks usually use the previous one frame as reference. Or a few frameworks which use the previous multiple frames as reference only adopt a simple multi-reference frames propagation mechanism. In this paper, we present a more reasonable multi-reference frames propagation mechanism for neural video compression, called butterfly multi-reference frame propagation mechanism (Butterfly), which allows a more effective feature fusion of multi-reference frames. By this, we can generate more accurate temporal context conditional prior for Contextual Coding Module. Besides, when the number of decoded frames does not meet the required number of reference frames, we duplicate the nearest reference frame to achieve the requirement, which is better than duplicating the furthest one. Experiment results show that our method can significantly outperform the previous state-of-the-art (SOTA), and our neural codec can achieve -7.6% bitrate save on HEVC Class D dataset when compares with our base single-reference frame model with the same compression configuration.",
      "published": "2023-03-06T08:19:15Z"
    },
    "metadata": {
      "arxiv_id": "2303.02959",
      "title": "Butterfly: Multiple Reference Frames Feature Propagation Mechanism for Neural Video Compression",
      "summary": "Using more reference frames can significantly improve the compression efficiency in neural video compression. However, in low-latency scenarios, most existing neural video compression frameworks usually use the previous one frame as reference. Or a few frameworks which use the previous multiple frames as reference only adopt a simple multi-reference frames propagation mechanism. In this paper, we present a more reasonable multi-reference frames propagation mechanism for neural video compression, called butterfly multi-reference frame propagation mechanism (Butterfly), which allows a more effective feature fusion of multi-reference frames. By this, we can generate more accurate temporal context conditional prior for Contextual Coding Module. Besides, when the number of decoded frames does not meet the required number of reference frames, we duplicate the nearest reference frame to achieve the requirement, which is better than duplicating the furthest one. Experiment results show that our method can significantly outperform the previous state-of-the-art (SOTA), and our neural codec can achieve -7.6% bitrate save on HEVC Class D dataset when compares with our base single-reference frame model with the same compression configuration.",
      "authors": [
        "Feng Wang",
        "Haihang Ruan",
        "Fei Xiong",
        "Jiayu Yang",
        "Litian Li",
        "Ronggang Wang"
      ],
      "published": "2023-03-06T08:19:15Z",
      "updated": "2023-03-06T08:19:15Z",
      "categories": [
        "cs.CV",
        "cs.MM",
        "eess.IV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2303.02959v1",
      "landing_url": "https://arxiv.org/abs/2303.02959v1",
      "doi": "https://doi.org/10.48550/arXiv.2303.02959"
    },
    "queries": [
      {
        "anchor": "synchronous dialogue",
        "search_term": "neural codec"
      }
    ]
  },
  {
    "arxiv_id": "2303.03926",
    "anchor": "spoken language models",
    "search_term": "neural codec",
    "search_record": {
      "id": "http://arxiv.org/abs/2303.03926v1",
      "title": "Speak Foreign Languages with Your Own Voice: Cross-Lingual Neural Codec Language Modeling",
      "summary": "We propose a cross-lingual neural codec language model, VALL-E X, for cross-lingual speech synthesis. Specifically, we extend VALL-E and train a multi-lingual conditional codec language model to predict the acoustic token sequences of the target language speech by using both the source language speech and the target language text as prompts. VALL-E X inherits strong in-context learning capabilities and can be applied for zero-shot cross-lingual text-to-speech synthesis and zero-shot speech-to-speech translation tasks. Experimental results show that it can generate high-quality speech in the target language via just one speech utterance in the source language as a prompt while preserving the unseen speaker's voice, emotion, and acoustic environment. Moreover, VALL-E X effectively alleviates the foreign accent problems, which can be controlled by a language ID. Audio samples are available at \\url{https://aka.ms/vallex}.",
      "published": "2023-03-07T14:31:55Z"
    },
    "metadata": {
      "arxiv_id": "2303.03926",
      "title": "Speak Foreign Languages with Your Own Voice: Cross-Lingual Neural Codec Language Modeling",
      "summary": "We propose a cross-lingual neural codec language model, VALL-E X, for cross-lingual speech synthesis. Specifically, we extend VALL-E and train a multi-lingual conditional codec language model to predict the acoustic token sequences of the target language speech by using both the source language speech and the target language text as prompts. VALL-E X inherits strong in-context learning capabilities and can be applied for zero-shot cross-lingual text-to-speech synthesis and zero-shot speech-to-speech translation tasks. Experimental results show that it can generate high-quality speech in the target language via just one speech utterance in the source language as a prompt while preserving the unseen speaker's voice, emotion, and acoustic environment. Moreover, VALL-E X effectively alleviates the foreign accent problems, which can be controlled by a language ID. Audio samples are available at \\url{https://aka.ms/vallex}.",
      "authors": [
        "Ziqiang Zhang",
        "Long Zhou",
        "Chengyi Wang",
        "Sanyuan Chen",
        "Yu Wu",
        "Shujie Liu",
        "Zhuo Chen",
        "Yanqing Liu",
        "Huaming Wang",
        "Jinyu Li",
        "Lei He",
        "Sheng Zhao",
        "Furu Wei"
      ],
      "published": "2023-03-07T14:31:55Z",
      "updated": "2023-03-07T14:31:55Z",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2303.03926v1",
      "landing_url": "https://arxiv.org/abs/2303.03926v1",
      "doi": "https://doi.org/10.48550/arXiv.2303.03926"
    },
    "queries": [
      {
        "anchor": "spoken language models",
        "search_term": "neural codec"
      },
      {
        "anchor": "synchronous dialogue",
        "search_term": "neural codec"
      },
      {
        "anchor": "turn-taking",
        "search_term": "neural codec"
      }
    ]
  },
  {
    "arxiv_id": "2303.05152",
    "anchor": "spoken language models",
    "search_term": "stop latency",
    "search_record": {
      "id": "http://arxiv.org/abs/2303.05152v2",
      "title": "Good-case Early-Stopping Latency of Synchronous Byzantine Reliable Broadcast: The Deterministic Case (Extended Version)",
      "summary": "This paper considers the good-case latency of Byzantine Reliable Broadcast (BRB), i.e., the time taken by correct processes to deliver a message when the initial sender is correct. This time plays a crucial role in the performance of practical distributed systems. Although significant strides have been made in recent years on this question, progress has mainly focused on either asynchronous or randomized algorithms. By contrast, the good-case latency of deterministic synchronous BRB under a majority of Byzantine faults has been little studied. In particular, it was not known whether a goodcase latency below the worst-case bound of t + 1 rounds could be obtained. This work answers this open question positively and proposes a deterministic synchronous Byzantine reliable broadcast that achieves a good-case latency of max(2, t + 3 -- c) rounds, where t is the upper bound on the number of Byzantine processes and c the number of effectively correct processes.",
      "published": "2023-03-09T10:10:27Z"
    },
    "metadata": {
      "arxiv_id": "2303.05152",
      "title": "Good-case Early-Stopping Latency of Synchronous Byzantine Reliable Broadcast: The Deterministic Case (Extended Version)",
      "summary": "This paper considers the good-case latency of Byzantine Reliable Broadcast (BRB), i.e., the time taken by correct processes to deliver a message when the initial sender is correct. This time plays a crucial role in the performance of practical distributed systems. Although significant strides have been made in recent years on this question, progress has mainly focused on either asynchronous or randomized algorithms. By contrast, the good-case latency of deterministic synchronous BRB under a majority of Byzantine faults has been little studied. In particular, it was not known whether a goodcase latency below the worst-case bound of t + 1 rounds could be obtained. This work answers this open question positively and proposes a deterministic synchronous Byzantine reliable broadcast that achieves a good-case latency of max(2, t + 3 -- c) rounds, where t is the upper bound on the number of Byzantine processes and c the number of effectively correct processes.",
      "authors": [
        "Timothé Albouy",
        "Davide Frey",
        "Michel Raynal",
        "François Taïani"
      ],
      "published": "2023-03-09T10:10:27Z",
      "updated": "2023-03-10T09:19:49Z",
      "categories": [
        "cs.DC"
      ],
      "pdf_url": "https://arxiv.org/pdf/2303.05152v2",
      "landing_url": "https://arxiv.org/abs/2303.05152v2",
      "doi": "https://doi.org/10.48550/arXiv.2303.05152"
    },
    "queries": [
      {
        "anchor": "spoken language models",
        "search_term": "stop latency"
      },
      {
        "anchor": "synchronous dialogue",
        "search_term": "stop latency"
      }
    ]
  },
  {
    "arxiv_id": "2303.09278",
    "anchor": "synchronous dialogue",
    "search_term": "streaming architecture",
    "search_record": {
      "id": "http://arxiv.org/abs/2303.09278v1",
      "title": "DistillW2V2: A Small and Streaming Wav2vec 2.0 Based ASR Model",
      "summary": "Wav2vec 2.0 (W2V2) has shown impressive performance in automatic speech recognition (ASR). However, the large model size and the non-streaming architecture make it hard to be used under low-resource or streaming scenarios. In this work, we propose a two-stage knowledge distillation method to solve these two problems: the first step is to make the big and non-streaming teacher model smaller, and the second step is to make it streaming. Specially, we adopt the MSE loss for the distillation of hidden layers and the modified LF-MMI loss for the distillation of the prediction layer. Experiments are conducted on Gigaspeech, Librispeech, and an in-house dataset. The results show that the distilled student model (DistillW2V2) we finally get is 8x faster and 12x smaller than the original teacher model. For the 480ms latency setup, the DistillW2V2's relative word error rate (WER) degradation varies from 9% to 23.4% on test sets, which reveals a promising way to extend the W2V2's application scope.",
      "published": "2023-03-16T12:59:17Z"
    },
    "metadata": {
      "arxiv_id": "2303.09278",
      "title": "DistillW2V2: A Small and Streaming Wav2vec 2.0 Based ASR Model",
      "summary": "Wav2vec 2.0 (W2V2) has shown impressive performance in automatic speech recognition (ASR). However, the large model size and the non-streaming architecture make it hard to be used under low-resource or streaming scenarios. In this work, we propose a two-stage knowledge distillation method to solve these two problems: the first step is to make the big and non-streaming teacher model smaller, and the second step is to make it streaming. Specially, we adopt the MSE loss for the distillation of hidden layers and the modified LF-MMI loss for the distillation of the prediction layer. Experiments are conducted on Gigaspeech, Librispeech, and an in-house dataset. The results show that the distilled student model (DistillW2V2) we finally get is 8x faster and 12x smaller than the original teacher model. For the 480ms latency setup, the DistillW2V2's relative word error rate (WER) degradation varies from 9% to 23.4% on test sets, which reveals a promising way to extend the W2V2's application scope.",
      "authors": [
        "Yanzhe Fu",
        "Yueteng Kang",
        "Songjun Cao",
        "Long Ma"
      ],
      "published": "2023-03-16T12:59:17Z",
      "updated": "2023-03-16T12:59:17Z",
      "categories": [
        "eess.AS",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2303.09278v1",
      "landing_url": "https://arxiv.org/abs/2303.09278v1",
      "doi": "https://doi.org/10.48550/arXiv.2303.09278"
    },
    "queries": [
      {
        "anchor": "synchronous dialogue",
        "search_term": "streaming architecture"
      }
    ]
  },
  {
    "arxiv_id": "2303.10449",
    "anchor": "spoken language models",
    "search_term": "semantic coherence",
    "search_record": {
      "id": "http://arxiv.org/abs/2303.10449v2",
      "title": "Uncertainty-Aware Optimal Transport for Semantically Coherent Out-of-Distribution Detection",
      "summary": "Semantically coherent out-of-distribution (SCOOD) detection aims to discern outliers from the intended data distribution with access to unlabeled extra set. The coexistence of in-distribution and out-of-distribution samples will exacerbate the model overfitting when no distinction is made. To address this problem, we propose a novel uncertainty-aware optimal transport scheme. Our scheme consists of an energy-based transport (ET) mechanism that estimates the fluctuating cost of uncertainty to promote the assignment of semantic-agnostic representation, and an inter-cluster extension strategy that enhances the discrimination of semantic property among different clusters by widening the corresponding margin distance. Furthermore, a T-energy score is presented to mitigate the magnitude gap between the parallel transport and classifier branches. Extensive experiments on two standard SCOOD benchmarks demonstrate the above-par OOD detection performance, outperforming the state-of-the-art methods by a margin of 27.69% and 34.4% on FPR@95, respectively.",
      "published": "2023-03-18T16:22:59Z"
    },
    "metadata": {
      "arxiv_id": "2303.10449",
      "title": "Uncertainty-Aware Optimal Transport for Semantically Coherent Out-of-Distribution Detection",
      "summary": "Semantically coherent out-of-distribution (SCOOD) detection aims to discern outliers from the intended data distribution with access to unlabeled extra set. The coexistence of in-distribution and out-of-distribution samples will exacerbate the model overfitting when no distinction is made. To address this problem, we propose a novel uncertainty-aware optimal transport scheme. Our scheme consists of an energy-based transport (ET) mechanism that estimates the fluctuating cost of uncertainty to promote the assignment of semantic-agnostic representation, and an inter-cluster extension strategy that enhances the discrimination of semantic property among different clusters by widening the corresponding margin distance. Furthermore, a T-energy score is presented to mitigate the magnitude gap between the parallel transport and classifier branches. Extensive experiments on two standard SCOOD benchmarks demonstrate the above-par OOD detection performance, outperforming the state-of-the-art methods by a margin of 27.69% and 34.4% on FPR@95, respectively.",
      "authors": [
        "Fan Lu",
        "Kai Zhu",
        "Wei Zhai",
        "Kecheng Zheng",
        "Yang Cao"
      ],
      "published": "2023-03-18T16:22:59Z",
      "updated": "2023-03-21T13:41:03Z",
      "categories": [
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2303.10449v2",
      "landing_url": "https://arxiv.org/abs/2303.10449v2",
      "doi": "https://doi.org/10.48550/arXiv.2303.10449"
    },
    "queries": [
      {
        "anchor": "spoken language models",
        "search_term": "semantic coherence"
      },
      {
        "anchor": "synchronous dialogue",
        "search_term": "semantic coherence"
      },
      {
        "anchor": "turn-taking",
        "search_term": "semantic coherence"
      }
    ]
  },
  {
    "arxiv_id": "2303.14865",
    "anchor": "spoken language models",
    "search_term": "discrete tokens",
    "search_record": {
      "id": "http://arxiv.org/abs/2303.14865v1",
      "title": "Revisiting Multimodal Representation in Contrastive Learning: From Patch and Token Embeddings to Finite Discrete Tokens",
      "summary": "Contrastive learning-based vision-language pre-training approaches, such as CLIP, have demonstrated great success in many vision-language tasks. These methods achieve cross-modal alignment by encoding a matched image-text pair with similar feature embeddings, which are generated by aggregating information from visual patches and language tokens. However, direct aligning cross-modal information using such representations is challenging, as visual patches and text tokens differ in semantic levels and granularities. To alleviate this issue, we propose a Finite Discrete Tokens (FDT) based multimodal representation. FDT is a set of learnable tokens representing certain visual-semantic concepts. Both images and texts are embedded using shared FDT by first grounding multimodal inputs to FDT space and then aggregating the activated FDT representations. The matched visual and semantic concepts are enforced to be represented by the same set of discrete tokens by a sparse activation constraint. As a result, the granularity gap between the two modalities is reduced. Through both quantitative and qualitative analyses, we demonstrate that using FDT representations in CLIP-style models improves cross-modal alignment and performance in visual recognition and vision-language downstream tasks. Furthermore, we show that our method can learn more comprehensive representations, and the learned FDT capture meaningful cross-modal correspondence, ranging from objects to actions and attributes.",
      "published": "2023-03-27T00:58:39Z"
    },
    "metadata": {
      "arxiv_id": "2303.14865",
      "title": "Revisiting Multimodal Representation in Contrastive Learning: From Patch and Token Embeddings to Finite Discrete Tokens",
      "summary": "Contrastive learning-based vision-language pre-training approaches, such as CLIP, have demonstrated great success in many vision-language tasks. These methods achieve cross-modal alignment by encoding a matched image-text pair with similar feature embeddings, which are generated by aggregating information from visual patches and language tokens. However, direct aligning cross-modal information using such representations is challenging, as visual patches and text tokens differ in semantic levels and granularities. To alleviate this issue, we propose a Finite Discrete Tokens (FDT) based multimodal representation. FDT is a set of learnable tokens representing certain visual-semantic concepts. Both images and texts are embedded using shared FDT by first grounding multimodal inputs to FDT space and then aggregating the activated FDT representations. The matched visual and semantic concepts are enforced to be represented by the same set of discrete tokens by a sparse activation constraint. As a result, the granularity gap between the two modalities is reduced. Through both quantitative and qualitative analyses, we demonstrate that using FDT representations in CLIP-style models improves cross-modal alignment and performance in visual recognition and vision-language downstream tasks. Furthermore, we show that our method can learn more comprehensive representations, and the learned FDT capture meaningful cross-modal correspondence, ranging from objects to actions and attributes.",
      "authors": [
        "Yuxiao Chen",
        "Jianbo Yuan",
        "Yu Tian",
        "Shijie Geng",
        "Xinyu Li",
        "Ding Zhou",
        "Dimitris N. Metaxas",
        "Hongxia Yang"
      ],
      "published": "2023-03-27T00:58:39Z",
      "updated": "2023-03-27T00:58:39Z",
      "categories": [
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2303.14865v1",
      "landing_url": "https://arxiv.org/abs/2303.14865v1",
      "doi": "https://doi.org/10.48550/arXiv.2303.14865"
    },
    "queries": [
      {
        "anchor": "spoken language models",
        "search_term": "discrete tokens"
      },
      {
        "anchor": "synchronous dialogue",
        "search_term": "discrete tokens"
      },
      {
        "anchor": "turn-taking",
        "search_term": "discrete tokens"
      }
    ]
  },
  {
    "arxiv_id": "2303.17218",
    "anchor": "spoken language models",
    "search_term": "streaming architecture",
    "search_record": {
      "id": "http://arxiv.org/abs/2303.17218v6",
      "title": "HARFLOW3D: A Latency-Oriented 3D-CNN Accelerator Toolflow for HAR on FPGA Devices",
      "summary": "For Human Action Recognition tasks (HAR), 3D Convolutional Neural Networks have proven to be highly effective, achieving state-of-the-art results. This study introduces a novel streaming architecture based toolflow for mapping such models onto FPGAs considering the model's inherent characteristics and the features of the targeted FPGA device. The HARFLOW3D toolflow takes as input a 3D CNN in ONNX format and a description of the FPGA characteristics, generating a design that minimizes the latency of the computation. The toolflow is comprised of a number of parts, including i) a 3D CNN parser, ii) a performance and resource model, iii) a scheduling algorithm for executing 3D models on the generated hardware, iv) a resource-aware optimization engine tailored for 3D models, v) an automated mapping to synthesizable code for FPGAs. The ability of the toolflow to support a broad range of models and devices is shown through a number of experiments on various 3D CNN and FPGA system pairs. Furthermore, the toolflow has produced high-performing results for 3D CNN models that have not been mapped to FPGAs before, demonstrating the potential of FPGA-based systems in this space. Overall, HARFLOW3D has demonstrated its ability to deliver competitive latency compared to a range of state-of-the-art hand-tuned approaches being able to achieve up to 5$\\times$ better performance compared to some of the existing works.",
      "published": "2023-03-30T08:25:27Z"
    },
    "metadata": {
      "arxiv_id": "2303.17218",
      "title": "HARFLOW3D: A Latency-Oriented 3D-CNN Accelerator Toolflow for HAR on FPGA Devices",
      "summary": "For Human Action Recognition tasks (HAR), 3D Convolutional Neural Networks have proven to be highly effective, achieving state-of-the-art results. This study introduces a novel streaming architecture based toolflow for mapping such models onto FPGAs considering the model's inherent characteristics and the features of the targeted FPGA device. The HARFLOW3D toolflow takes as input a 3D CNN in ONNX format and a description of the FPGA characteristics, generating a design that minimizes the latency of the computation. The toolflow is comprised of a number of parts, including i) a 3D CNN parser, ii) a performance and resource model, iii) a scheduling algorithm for executing 3D models on the generated hardware, iv) a resource-aware optimization engine tailored for 3D models, v) an automated mapping to synthesizable code for FPGAs. The ability of the toolflow to support a broad range of models and devices is shown through a number of experiments on various 3D CNN and FPGA system pairs. Furthermore, the toolflow has produced high-performing results for 3D CNN models that have not been mapped to FPGAs before, demonstrating the potential of FPGA-based systems in this space. Overall, HARFLOW3D has demonstrated its ability to deliver competitive latency compared to a range of state-of-the-art hand-tuned approaches being able to achieve up to 5$\\times$ better performance compared to some of the existing works.",
      "authors": [
        "Petros Toupas",
        "Alexander Montgomerie-Corcoran",
        "Christos-Savvas Bouganis",
        "Dimitrios Tzovaras"
      ],
      "published": "2023-03-30T08:25:27Z",
      "updated": "2023-05-29T11:23:45Z",
      "categories": [
        "cs.AR",
        "cs.CV",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2303.17218v6",
      "landing_url": "https://arxiv.org/abs/2303.17218v6",
      "doi": "https://doi.org/10.1109/FCCM57271.2023.00024"
    },
    "queries": [
      {
        "anchor": "spoken language models",
        "search_term": "streaming architecture"
      },
      {
        "anchor": "synchronous dialogue",
        "search_term": "streaming architecture"
      },
      {
        "anchor": "turn-taking",
        "search_term": "streaming architecture"
      }
    ]
  },
  {
    "arxiv_id": "2304.02942",
    "anchor": "spoken language models",
    "search_term": "real-time interaction",
    "search_record": {
      "id": "http://arxiv.org/abs/2304.02942v2",
      "title": "InterFormer: Real-time Interactive Image Segmentation",
      "summary": "Interactive image segmentation enables annotators to efficiently perform pixel-level annotation for segmentation tasks. However, the existing interactive segmentation pipeline suffers from inefficient computations of interactive models because of the following two issues. First, annotators' later click is based on models' feedback of annotators' former click. This serial interaction is unable to utilize model's parallelism capabilities. Second, in each interaction step, the model handles the invariant image along with the sparse variable clicks, resulting in a process that's highly repetitive and redundant. For efficient computations, we propose a method named InterFormer that follows a new pipeline to address these issues. InterFormer extracts and preprocesses the computationally time-consuming part i.e. image processing from the existing process. Specifically, InterFormer employs a large vision transformer (ViT) on high-performance devices to preprocess images in parallel, and then uses a lightweight module called interactive multi-head self attention (I-MSA) for interactive segmentation. Furthermore, the I-MSA module's deployment on low-power devices extends the practical application of interactive segmentation. The I-MSA module utilizes the preprocessed features to efficiently response to the annotator inputs in real-time. The experiments on several datasets demonstrate the effectiveness of InterFormer, which outperforms previous interactive segmentation models in terms of computational efficiency and segmentation quality, achieve real-time high-quality interactive segmentation on CPU-only devices. The code is available at https://github.com/YouHuang67/InterFormer.",
      "published": "2023-04-06T08:57:00Z"
    },
    "metadata": {
      "arxiv_id": "2304.02942",
      "title": "InterFormer: Real-time Interactive Image Segmentation",
      "summary": "Interactive image segmentation enables annotators to efficiently perform pixel-level annotation for segmentation tasks. However, the existing interactive segmentation pipeline suffers from inefficient computations of interactive models because of the following two issues. First, annotators' later click is based on models' feedback of annotators' former click. This serial interaction is unable to utilize model's parallelism capabilities. Second, in each interaction step, the model handles the invariant image along with the sparse variable clicks, resulting in a process that's highly repetitive and redundant. For efficient computations, we propose a method named InterFormer that follows a new pipeline to address these issues. InterFormer extracts and preprocesses the computationally time-consuming part i.e. image processing from the existing process. Specifically, InterFormer employs a large vision transformer (ViT) on high-performance devices to preprocess images in parallel, and then uses a lightweight module called interactive multi-head self attention (I-MSA) for interactive segmentation. Furthermore, the I-MSA module's deployment on low-power devices extends the practical application of interactive segmentation. The I-MSA module utilizes the preprocessed features to efficiently response to the annotator inputs in real-time. The experiments on several datasets demonstrate the effectiveness of InterFormer, which outperforms previous interactive segmentation models in terms of computational efficiency and segmentation quality, achieve real-time high-quality interactive segmentation on CPU-only devices. The code is available at https://github.com/YouHuang67/InterFormer.",
      "authors": [
        "You Huang",
        "Hao Yang",
        "Ke Sun",
        "Shengchuan Zhang",
        "Liujuan Cao",
        "Guannan Jiang",
        "Rongrong Ji"
      ],
      "published": "2023-04-06T08:57:00Z",
      "updated": "2023-08-09T08:41:39Z",
      "categories": [
        "cs.CV",
        "cs.HC"
      ],
      "pdf_url": "https://arxiv.org/pdf/2304.02942v2",
      "landing_url": "https://arxiv.org/abs/2304.02942v2",
      "doi": "https://doi.org/10.48550/arXiv.2304.02942"
    },
    "queries": [
      {
        "anchor": "spoken language models",
        "search_term": "real-time interaction"
      },
      {
        "anchor": "synchronous dialogue",
        "search_term": "real-time interaction"
      },
      {
        "anchor": "turn-taking",
        "search_term": "real-time interaction"
      }
    ]
  },
  {
    "arxiv_id": "2304.03536",
    "anchor": "spoken language models",
    "search_term": "data scarcity",
    "search_record": {
      "id": "http://arxiv.org/abs/2304.03536v1",
      "title": "Leveraging GANs for data scarcity of COVID-19: Beyond the hype",
      "summary": "Artificial Intelligence (AI)-based models can help in diagnosing COVID-19 from lung CT scans and X-ray images; however, these models require large amounts of data for training and validation. Many researchers studied Generative Adversarial Networks (GANs) for producing synthetic lung CT scans and X-Ray images to improve the performance of AI-based models. It is not well explored how good GAN-based methods performed to generate reliable synthetic data. This work analyzes 43 published studies that reported GANs for synthetic data generation. Many of these studies suffered data bias, lack of reproducibility, and lack of feedback from the radiologists or other domain experts. A common issue in these studies is the unavailability of the source code, hindering reproducibility. The included studies reported rescaling of the input images to train the existing GANs architecture without providing clinical insights on how the rescaling was motivated. Finally, even though GAN-based methods have the potential for data augmentation and improving the training of AI-based models, these methods fall short in terms of their use in clinical practice. This paper highlights research hotspots in countering the data scarcity problem, identifies various issues as well as potentials, and provides recommendations to guide future research. These recommendations might be useful to improve acceptability for the GAN-based approaches for data augmentation as GANs for data augmentation are increasingly becoming popular in the AI and medical imaging research community.",
      "published": "2023-04-07T08:26:12Z"
    },
    "metadata": {
      "arxiv_id": "2304.03536",
      "title": "Leveraging GANs for data scarcity of COVID-19: Beyond the hype",
      "summary": "Artificial Intelligence (AI)-based models can help in diagnosing COVID-19 from lung CT scans and X-ray images; however, these models require large amounts of data for training and validation. Many researchers studied Generative Adversarial Networks (GANs) for producing synthetic lung CT scans and X-Ray images to improve the performance of AI-based models. It is not well explored how good GAN-based methods performed to generate reliable synthetic data. This work analyzes 43 published studies that reported GANs for synthetic data generation. Many of these studies suffered data bias, lack of reproducibility, and lack of feedback from the radiologists or other domain experts. A common issue in these studies is the unavailability of the source code, hindering reproducibility. The included studies reported rescaling of the input images to train the existing GANs architecture without providing clinical insights on how the rescaling was motivated. Finally, even though GAN-based methods have the potential for data augmentation and improving the training of AI-based models, these methods fall short in terms of their use in clinical practice. This paper highlights research hotspots in countering the data scarcity problem, identifies various issues as well as potentials, and provides recommendations to guide future research. These recommendations might be useful to improve acceptability for the GAN-based approaches for data augmentation as GANs for data augmentation are increasingly becoming popular in the AI and medical imaging research community.",
      "authors": [
        "Hazrat Ali",
        "Christer Gronlund",
        "Zubair Shah"
      ],
      "published": "2023-04-07T08:26:12Z",
      "updated": "2023-04-07T08:26:12Z",
      "categories": [
        "eess.IV",
        "cs.CV",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2304.03536v1",
      "landing_url": "https://arxiv.org/abs/2304.03536v1",
      "doi": "https://doi.org/10.48550/arXiv.2304.03536"
    },
    "queries": [
      {
        "anchor": "spoken language models",
        "search_term": "data scarcity"
      },
      {
        "anchor": "synchronous dialogue",
        "search_term": "data scarcity"
      },
      {
        "anchor": "turn-taking",
        "search_term": "data scarcity"
      }
    ]
  },
  {
    "arxiv_id": "2304.04891",
    "anchor": "synchronous dialogue",
    "search_term": "synchronous data",
    "search_record": {
      "id": "http://arxiv.org/abs/2304.04891v1",
      "title": "SNIPS: Succinct Proof of Storage for Efficient Data Synchronization in Decentralized Storage Systems",
      "summary": "Data synchronization in decentralized storage systems is essential to guarantee sufficient redundancy to prevent data loss. We present SNIPS, the first succinct proof of storage algorithm for synchronizing storage peers. A peer constructs a proof for its stored chunks and sends it to verifier peers. A verifier queries the proof to identify and subsequently requests missing chunks. The proof is succinct, supports membership queries, and requires only a few bits per chunk.\n  We evaluated our SNIPS algorithm on a cluster of 1000 peers running Ethereum Swarm. Our results show that SNIPS reduces the amount of synchronization data by three orders of magnitude compared to the state-of-the-art. Additionally, creating and verifying a proof is linear with the number of chunks and typically requires only tens of microseconds per chunk. These qualities are vital for our use case, as we envision running SNIPS frequently to maintain sufficient redundancy consistently.",
      "published": "2023-04-10T22:46:05Z"
    },
    "metadata": {
      "arxiv_id": "2304.04891",
      "title": "SNIPS: Succinct Proof of Storage for Efficient Data Synchronization in Decentralized Storage Systems",
      "summary": "Data synchronization in decentralized storage systems is essential to guarantee sufficient redundancy to prevent data loss. We present SNIPS, the first succinct proof of storage algorithm for synchronizing storage peers. A peer constructs a proof for its stored chunks and sends it to verifier peers. A verifier queries the proof to identify and subsequently requests missing chunks. The proof is succinct, supports membership queries, and requires only a few bits per chunk.\n  We evaluated our SNIPS algorithm on a cluster of 1000 peers running Ethereum Swarm. Our results show that SNIPS reduces the amount of synchronization data by three orders of magnitude compared to the state-of-the-art. Additionally, creating and verifying a proof is linear with the number of chunks and typically requires only tens of microseconds per chunk. These qualities are vital for our use case, as we envision running SNIPS frequently to maintain sufficient redundancy consistently.",
      "authors": [
        "Racin Nygaard",
        "Hein Meling"
      ],
      "published": "2023-04-10T22:46:05Z",
      "updated": "2023-04-10T22:46:05Z",
      "categories": [
        "cs.DC"
      ],
      "pdf_url": "https://arxiv.org/pdf/2304.04891v1",
      "landing_url": "https://arxiv.org/abs/2304.04891v1",
      "doi": "https://doi.org/10.48550/arXiv.2304.04891"
    },
    "queries": [
      {
        "anchor": "synchronous dialogue",
        "search_term": "synchronous data"
      }
    ]
  },
  {
    "arxiv_id": "2304.08269",
    "anchor": "spoken language models",
    "search_term": "neural codec",
    "search_record": {
      "id": "http://arxiv.org/abs/2304.08269v1",
      "title": "Evaluating Strong Idempotence of Image Codec",
      "summary": "In this paper, we first propose the concept of strong idempotent codec based on idempotent codec. The idempotence of codec refers to the stability of codec to re-compression. Similarly, we define the strong idempotence of codec as the stability of codec to multiple quality re-compression, which is an important feature of codec in the context of cloud transcoding. We provide a detailed example of strong idempotent codec with known source distribution. Further, we formalize a testing protocol to evaluate the strong idempotence of image codec. And finally, we evaluate the strong idempotence of current image codecs, including traditional codec and recent neural codec. Experimental results show that current image codecs are not stable to multi-round re-compression with different qualities, even if they are close to idempotent.",
      "published": "2023-04-17T13:26:31Z"
    },
    "metadata": {
      "arxiv_id": "2304.08269",
      "title": "Evaluating Strong Idempotence of Image Codec",
      "summary": "In this paper, we first propose the concept of strong idempotent codec based on idempotent codec. The idempotence of codec refers to the stability of codec to re-compression. Similarly, we define the strong idempotence of codec as the stability of codec to multiple quality re-compression, which is an important feature of codec in the context of cloud transcoding. We provide a detailed example of strong idempotent codec with known source distribution. Further, we formalize a testing protocol to evaluate the strong idempotence of image codec. And finally, we evaluate the strong idempotence of current image codecs, including traditional codec and recent neural codec. Experimental results show that current image codecs are not stable to multi-round re-compression with different qualities, even if they are close to idempotent.",
      "authors": [
        "Qian Zhang",
        "Tongda Xu",
        "Yanghao Li",
        "Yan Wang"
      ],
      "published": "2023-04-17T13:26:31Z",
      "updated": "2023-04-17T13:26:31Z",
      "categories": [
        "cs.MM"
      ],
      "pdf_url": "https://arxiv.org/pdf/2304.08269v1",
      "landing_url": "https://arxiv.org/abs/2304.08269v1",
      "doi": "https://doi.org/10.48550/arXiv.2304.08269"
    },
    "queries": [
      {
        "anchor": "spoken language models",
        "search_term": "neural codec"
      },
      {
        "anchor": "synchronous dialogue",
        "search_term": "neural codec"
      }
    ]
  },
  {
    "arxiv_id": "2304.08341",
    "anchor": "synchronous dialogue",
    "search_term": "real-time interaction",
    "search_record": {
      "id": "http://arxiv.org/abs/2304.08341v1",
      "title": "Use of social media and Natural Language Processing (NLP) in natural hazard research",
      "summary": "Twitter is a microblogging service for sending short, public text messages (tweets) that has recently received more attention in scientific comunity. In the works of Sasaki et al. (2010) and Earle et al., (2011) the authors explored the real-time interaction on Twitter for detecting natural hazards (e.g., earthquakes, typhoons) baed on users' tweets. An inherent challenge for such an application is the natural language processing (NLP), which basically consists in converting the words in number (vectors and tensors) in order to (mathematically/ computationally) make predictions and classifications. Recently advanced computational tools have been made available for dealing with text computationally. In this report we implement a NLP machine learning with TensorFlow, an end-to-end open source plataform for machine learning applications, to process and classify evenct based on files containing only text.",
      "published": "2023-04-17T15:03:05Z"
    },
    "metadata": {
      "arxiv_id": "2304.08341",
      "title": "Use of social media and Natural Language Processing (NLP) in natural hazard research",
      "summary": "Twitter is a microblogging service for sending short, public text messages (tweets) that has recently received more attention in scientific comunity. In the works of Sasaki et al. (2010) and Earle et al., (2011) the authors explored the real-time interaction on Twitter for detecting natural hazards (e.g., earthquakes, typhoons) baed on users' tweets. An inherent challenge for such an application is the natural language processing (NLP), which basically consists in converting the words in number (vectors and tensors) in order to (mathematically/ computationally) make predictions and classifications. Recently advanced computational tools have been made available for dealing with text computationally. In this report we implement a NLP machine learning with TensorFlow, an end-to-end open source plataform for machine learning applications, to process and classify evenct based on files containing only text.",
      "authors": [
        "José Augusto Proença Maia Devienne"
      ],
      "published": "2023-04-17T15:03:05Z",
      "updated": "2023-04-17T15:03:05Z",
      "categories": [
        "cs.CL",
        "physics.geo-ph"
      ],
      "pdf_url": "https://arxiv.org/pdf/2304.08341v1",
      "landing_url": "https://arxiv.org/abs/2304.08341v1",
      "doi": "https://doi.org/10.48550/arXiv.2304.08341"
    },
    "queries": [
      {
        "anchor": "synchronous dialogue",
        "search_term": "real-time interaction"
      }
    ]
  },
  {
    "arxiv_id": "2304.08400",
    "anchor": "spoken language models",
    "search_term": "streaming architecture",
    "search_record": {
      "id": "http://arxiv.org/abs/2304.08400v2",
      "title": "ATHEENA: A Toolflow for Hardware Early-Exit Network Automation",
      "summary": "The continued need for improvements in accuracy, throughput, and efficiency of Deep Neural Networks has resulted in a multitude of methods that make the most of custom architectures on FPGAs. These include the creation of hand-crafted networks and the use of quantization and pruning to reduce extraneous network parameters. However, with the potential of static solutions already well exploited, we propose to shift the focus to using the varying difficulty of individual data samples to further improve efficiency and reduce average compute for classification. Input-dependent computation allows for the network to make runtime decisions to finish a task early if the result meets a confidence threshold. Early-Exit network architectures have become an increasingly popular way to implement such behaviour in software.\n  We create: A Toolflow for Hardware Early-Exit Network Automation (ATHEENA), an automated FPGA toolflow that leverages the probability of samples exiting early from such networks to scale the resources allocated to different sections of the network. The toolflow uses the data-flow model of fpgaConvNet, extended to support Early-Exit networks as well as Design Space Exploration to optimize the generated streaming architecture hardware with the goal of increasing throughput/reducing area while maintaining accuracy. Experimental results on three different networks demonstrate a throughput increase of $2.00\\times$ to $2.78\\times$ compared to an optimized baseline network implementation with no early exits. Additionally, the toolflow can achieve a throughput matching the same baseline with as low as $46\\%$ of the resources the baseline requires.",
      "published": "2023-04-17T16:06:58Z"
    },
    "metadata": {
      "arxiv_id": "2304.08400",
      "title": "ATHEENA: A Toolflow for Hardware Early-Exit Network Automation",
      "summary": "The continued need for improvements in accuracy, throughput, and efficiency of Deep Neural Networks has resulted in a multitude of methods that make the most of custom architectures on FPGAs. These include the creation of hand-crafted networks and the use of quantization and pruning to reduce extraneous network parameters. However, with the potential of static solutions already well exploited, we propose to shift the focus to using the varying difficulty of individual data samples to further improve efficiency and reduce average compute for classification. Input-dependent computation allows for the network to make runtime decisions to finish a task early if the result meets a confidence threshold. Early-Exit network architectures have become an increasingly popular way to implement such behaviour in software.\n  We create: A Toolflow for Hardware Early-Exit Network Automation (ATHEENA), an automated FPGA toolflow that leverages the probability of samples exiting early from such networks to scale the resources allocated to different sections of the network. The toolflow uses the data-flow model of fpgaConvNet, extended to support Early-Exit networks as well as Design Space Exploration to optimize the generated streaming architecture hardware with the goal of increasing throughput/reducing area while maintaining accuracy. Experimental results on three different networks demonstrate a throughput increase of $2.00\\times$ to $2.78\\times$ compared to an optimized baseline network implementation with no early exits. Additionally, the toolflow can achieve a throughput matching the same baseline with as low as $46\\%$ of the resources the baseline requires.",
      "authors": [
        "Benjamin Biggs",
        "Christos-Savvas Bouganis",
        "George A. Constantinides"
      ],
      "published": "2023-04-17T16:06:58Z",
      "updated": "2025-04-14T14:54:00Z",
      "categories": [
        "cs.AR",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2304.08400v2",
      "landing_url": "https://arxiv.org/abs/2304.08400v2",
      "doi": "https://doi.org/10.48550/arXiv.2304.08400"
    },
    "queries": [
      {
        "anchor": "spoken language models",
        "search_term": "streaming architecture"
      },
      {
        "anchor": "synchronous dialogue",
        "search_term": "streaming architecture"
      },
      {
        "anchor": "turn-taking",
        "search_term": "streaming architecture"
      }
    ]
  },
  {
    "arxiv_id": "2304.08645",
    "anchor": "spoken language models",
    "search_term": "modular architecture",
    "search_record": {
      "id": "http://arxiv.org/abs/2304.08645v1",
      "title": "ProPanDL: A Modular Architecture for Uncertainty-Aware Panoptic Segmentation",
      "summary": "We introduce ProPanDL, a family of networks capable of uncertainty-aware panoptic segmentation. Unlike existing segmentation methods, ProPanDL is capable of estimating full probability distributions for both the semantic and spatial aspects of panoptic segmentation. We implement and evaluate ProPanDL variants capable of estimating both parametric (Variance Network) and parameter-free (SampleNet) distributions quantifying pixel-wise spatial uncertainty. We couple these approaches with two methods (Temperature Scaling and Evidential Deep Learning) for semantic uncertainty estimation. To evaluate the uncertainty-aware panoptic segmentation task, we address limitations with existing approaches by proposing new metrics that enable separate evaluation of spatial and semantic uncertainty. We additionally propose the use of the energy score, a proper scoring rule, for more robust evaluation of spatial output distributions. Using these metrics, we conduct an extensive evaluation of ProPanDL variants. Our results demonstrate that ProPanDL is capable of estimating well-calibrated and meaningful output distributions while still retaining strong performance on the base panoptic segmentation task.",
      "published": "2023-04-17T22:31:23Z"
    },
    "metadata": {
      "arxiv_id": "2304.08645",
      "title": "ProPanDL: A Modular Architecture for Uncertainty-Aware Panoptic Segmentation",
      "summary": "We introduce ProPanDL, a family of networks capable of uncertainty-aware panoptic segmentation. Unlike existing segmentation methods, ProPanDL is capable of estimating full probability distributions for both the semantic and spatial aspects of panoptic segmentation. We implement and evaluate ProPanDL variants capable of estimating both parametric (Variance Network) and parameter-free (SampleNet) distributions quantifying pixel-wise spatial uncertainty. We couple these approaches with two methods (Temperature Scaling and Evidential Deep Learning) for semantic uncertainty estimation. To evaluate the uncertainty-aware panoptic segmentation task, we address limitations with existing approaches by proposing new metrics that enable separate evaluation of spatial and semantic uncertainty. We additionally propose the use of the energy score, a proper scoring rule, for more robust evaluation of spatial output distributions. Using these metrics, we conduct an extensive evaluation of ProPanDL variants. Our results demonstrate that ProPanDL is capable of estimating well-calibrated and meaningful output distributions while still retaining strong performance on the base panoptic segmentation task.",
      "authors": [
        "Jacob Deery",
        "Chang Won Lee",
        "Steven Waslander"
      ],
      "published": "2023-04-17T22:31:23Z",
      "updated": "2023-04-17T22:31:23Z",
      "categories": [
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2304.08645v1",
      "landing_url": "https://arxiv.org/abs/2304.08645v1",
      "doi": "https://doi.org/10.48550/arXiv.2304.08645"
    },
    "queries": [
      {
        "anchor": "spoken language models",
        "search_term": "modular architecture"
      },
      {
        "anchor": "synchronous dialogue",
        "search_term": "modular architecture"
      },
      {
        "anchor": "turn-taking",
        "search_term": "modular architecture"
      }
    ]
  },
  {
    "arxiv_id": "2304.12480",
    "anchor": "spoken language models",
    "search_term": "data scarcity",
    "search_record": {
      "id": "http://arxiv.org/abs/2304.12480v1",
      "title": "Towards Addressing Training Data Scarcity Challenge in Emerging Radio Access Networks: A Survey and Framework",
      "summary": "The future of cellular networks is contingent on artificial intelligence (AI) based automation, particularly for radio access network (RAN) operation, optimization, and troubleshooting. To achieve such zero-touch automation, a myriad of AI-based solutions are being proposed in literature for modeling and optimizing network behavior to achieve the zero-touch automation goal. However, to work reliably, AI based automation, requires a deluge of training data. Consequently, the success of AI solutions is limited by a fundamental challenge faced by cellular network research community: scarcity of training data. We present an extensive review of classic and emerging techniques to address this challenge. We first identify the common data types in RAN and their known use-cases. We then present a taxonomized survey of techniques to address training data scarcity for various data types. This is followed by a framework to address the training data scarcity. The framework builds on available information and combination of techniques including interpolation, domain-knowledge based, generative adversarial neural networks, transfer learning, autoencoders, few-shot learning, simulators, and testbeds. Potential new techniques to enrich scarce data in cellular networks are also proposed, such as by matrix completion theory, and domain knowledge-based techniques leveraging different network parameters and geometries. An overview of state-of-the art simulators and testbeds is also presented to make readers aware of current and emerging platforms for real data access. The extensive survey of training data scarcity addressing techniques combined with proposed framework to select a suitable technique for given type of data, can assist researchers and network operators in choosing appropriate methods to overcome the data scarcity challenge in leveraging AI to radio access network automation.",
      "published": "2023-04-24T22:33:54Z"
    },
    "metadata": {
      "arxiv_id": "2304.12480",
      "title": "Towards Addressing Training Data Scarcity Challenge in Emerging Radio Access Networks: A Survey and Framework",
      "summary": "The future of cellular networks is contingent on artificial intelligence (AI) based automation, particularly for radio access network (RAN) operation, optimization, and troubleshooting. To achieve such zero-touch automation, a myriad of AI-based solutions are being proposed in literature for modeling and optimizing network behavior to achieve the zero-touch automation goal. However, to work reliably, AI based automation, requires a deluge of training data. Consequently, the success of AI solutions is limited by a fundamental challenge faced by cellular network research community: scarcity of training data. We present an extensive review of classic and emerging techniques to address this challenge. We first identify the common data types in RAN and their known use-cases. We then present a taxonomized survey of techniques to address training data scarcity for various data types. This is followed by a framework to address the training data scarcity. The framework builds on available information and combination of techniques including interpolation, domain-knowledge based, generative adversarial neural networks, transfer learning, autoencoders, few-shot learning, simulators, and testbeds. Potential new techniques to enrich scarce data in cellular networks are also proposed, such as by matrix completion theory, and domain knowledge-based techniques leveraging different network parameters and geometries. An overview of state-of-the art simulators and testbeds is also presented to make readers aware of current and emerging platforms for real data access. The extensive survey of training data scarcity addressing techniques combined with proposed framework to select a suitable technique for given type of data, can assist researchers and network operators in choosing appropriate methods to overcome the data scarcity challenge in leveraging AI to radio access network automation.",
      "authors": [
        "Haneya Naeem Qureshi",
        "Usama Masood",
        "Marvin Manalastas",
        "Syed Muhammad Asad Zaidi",
        "Hasan Farooq",
        "Julien Forgeat",
        "Maxime Bouton",
        "Shruti Bothe",
        "Per Karlsson",
        "Ali Rizwan",
        "Ali Imran"
      ],
      "published": "2023-04-24T22:33:54Z",
      "updated": "2023-04-24T22:33:54Z",
      "categories": [
        "eess.SY"
      ],
      "pdf_url": "https://arxiv.org/pdf/2304.12480v1",
      "landing_url": "https://arxiv.org/abs/2304.12480v1",
      "doi": "https://doi.org/10.48550/arXiv.2304.12480"
    },
    "queries": [
      {
        "anchor": "spoken language models",
        "search_term": "data scarcity"
      },
      {
        "anchor": "synchronous dialogue",
        "search_term": "data scarcity"
      },
      {
        "anchor": "turn-taking",
        "search_term": "data scarcity"
      }
    ]
  },
  {
    "arxiv_id": "2305.00138",
    "anchor": "synchronous dialogue",
    "search_term": "modular architecture",
    "search_record": {
      "id": "http://arxiv.org/abs/2305.00138v3",
      "title": "Codesign of quantum error-correcting codes and modular chiplets in the presence of defects",
      "summary": "Fabrication errors pose a significant challenge in scaling up solid-state quantum devices to the sizes required for fault-tolerant (FT) quantum applications. To mitigate the resource overhead caused by fabrication errors, we combine two approaches: (1) leveraging the flexibility of a modular architecture, (2) adapting the procedure of quantum error correction (QEC) to account for fabrication defects. We simulate the surface code adapted to qubit arrays with arbitrarily distributed defects to find metrics that characterize how defects affect fidelity. We then determine the impact of defects on the resource overhead of realizing a fault-tolerant quantum computer, on a chiplet-based modular architecture. Our strategy for dealing with fabrication defects demonstrates an exponential suppression of logical failure where error rates of non-faulty physical qubits are ~0.1% in a circuit-based noise model. This is a typical regime where we imagine running the defect-free surface code. We use our numerical results to establish post-selection criteria for building a device from defective chiplets. Using our criteria, we then evaluate the resource overhead in terms of the average number of fabricated physical qubits per logical qubit. We find that an optimal choice of chiplet size, based on the defect rate and target fidelity, is essential to limiting any additional error correction overhead due to defects. When the optimal chiplet size is chosen, at a defect rate of 1% the resource overhead can be reduced to below 3X and 6X respectively for the two defect models we use, for a wide range of target performance. We also determine cutoff fidelity values that help identify whether a qubit should be disabled or kept as part of the error correction code.",
      "published": "2023-04-29T01:06:52Z"
    },
    "metadata": {
      "arxiv_id": "2305.00138",
      "title": "Codesign of quantum error-correcting codes and modular chiplets in the presence of defects",
      "summary": "Fabrication errors pose a significant challenge in scaling up solid-state quantum devices to the sizes required for fault-tolerant (FT) quantum applications. To mitigate the resource overhead caused by fabrication errors, we combine two approaches: (1) leveraging the flexibility of a modular architecture, (2) adapting the procedure of quantum error correction (QEC) to account for fabrication defects. We simulate the surface code adapted to qubit arrays with arbitrarily distributed defects to find metrics that characterize how defects affect fidelity. We then determine the impact of defects on the resource overhead of realizing a fault-tolerant quantum computer, on a chiplet-based modular architecture. Our strategy for dealing with fabrication defects demonstrates an exponential suppression of logical failure where error rates of non-faulty physical qubits are ~0.1% in a circuit-based noise model. This is a typical regime where we imagine running the defect-free surface code. We use our numerical results to establish post-selection criteria for building a device from defective chiplets. Using our criteria, we then evaluate the resource overhead in terms of the average number of fabricated physical qubits per logical qubit. We find that an optimal choice of chiplet size, based on the defect rate and target fidelity, is essential to limiting any additional error correction overhead due to defects. When the optimal chiplet size is chosen, at a defect rate of 1% the resource overhead can be reduced to below 3X and 6X respectively for the two defect models we use, for a wide range of target performance. We also determine cutoff fidelity values that help identify whether a qubit should be disabled or kept as part of the error correction code.",
      "authors": [
        "Sophia Fuhui Lin",
        "Joshua Viszlai",
        "Kaitlin N. Smith",
        "Gokul Subramanian Ravi",
        "Charles Yuan",
        "Frederic T. Chong",
        "Benjamin J. Brown"
      ],
      "published": "2023-04-29T01:06:52Z",
      "updated": "2024-03-22T18:35:57Z",
      "categories": [
        "quant-ph"
      ],
      "pdf_url": "https://arxiv.org/pdf/2305.00138v3",
      "landing_url": "https://arxiv.org/abs/2305.00138v3",
      "doi": "https://doi.org/10.1145/3620665.3640362"
    },
    "queries": [
      {
        "anchor": "synchronous dialogue",
        "search_term": "modular architecture"
      }
    ]
  },
  {
    "arxiv_id": "2305.01479",
    "anchor": "synchronous dialogue",
    "search_term": "synchronous data",
    "search_record": {
      "id": "http://arxiv.org/abs/2305.01479v2",
      "title": "On the properties of Gaussian Copula Mixture Models",
      "summary": "This paper investigates Gaussian copula mixture models (GCMM), which are an extension of Gaussian mixture models (GMM) that incorporate copula concepts. The paper presents the mathematical definition of GCMM and explores the properties of its likelihood function. Additionally, the paper proposes extended Expectation Maximum algorithms to estimate parameters for the mixture of copulas. The marginal distributions corresponding to each component are estimated separately using nonparametric statistical methods. In the experiment, GCMM demonstrates improved goodness-of-fitting compared to GMM when using the same number of clusters. Furthermore, GCMM has the ability to leverage un-synchronized data across dimensions for more comprehensive data analysis.",
      "published": "2023-05-02T14:59:37Z"
    },
    "metadata": {
      "arxiv_id": "2305.01479",
      "title": "On the properties of Gaussian Copula Mixture Models",
      "summary": "This paper investigates Gaussian copula mixture models (GCMM), which are an extension of Gaussian mixture models (GMM) that incorporate copula concepts. The paper presents the mathematical definition of GCMM and explores the properties of its likelihood function. Additionally, the paper proposes extended Expectation Maximum algorithms to estimate parameters for the mixture of copulas. The marginal distributions corresponding to each component are estimated separately using nonparametric statistical methods. In the experiment, GCMM demonstrates improved goodness-of-fitting compared to GMM when using the same number of clusters. Furthermore, GCMM has the ability to leverage un-synchronized data across dimensions for more comprehensive data analysis.",
      "authors": [
        "Ke Wan",
        "Alain Kornhauser"
      ],
      "published": "2023-05-02T14:59:37Z",
      "updated": "2023-05-24T01:41:38Z",
      "categories": [
        "cs.LG",
        "stat.ML"
      ],
      "pdf_url": "https://arxiv.org/pdf/2305.01479v2",
      "landing_url": "https://arxiv.org/abs/2305.01479v2",
      "doi": "https://doi.org/10.48550/arXiv.2305.01479"
    },
    "queries": [
      {
        "anchor": "synchronous dialogue",
        "search_term": "synchronous data"
      }
    ]
  },
  {
    "arxiv_id": "2305.02036",
    "anchor": "turn-taking",
    "search_term": "true full-duplex",
    "search_record": {
      "id": "http://arxiv.org/abs/2305.02036v1",
      "title": "Response-conditioned Turn-taking Prediction",
      "summary": "Previous approaches to turn-taking and response generation in conversational systems have treated it as a two-stage process: First, the end of a turn is detected (based on conversation history), then the system generates an appropriate response. Humans, however, do not take the turn just because it is likely, but also consider whether what they want to say fits the position. In this paper, we present a model (an extension of TurnGPT) that conditions the end-of-turn prediction on both conversation history and what the next speaker wants to say. We found that our model consistently outperforms the baseline model in a variety of metrics. The improvement is most prominent in two scenarios where turn predictions can be ambiguous solely from the conversation history: 1) when the current utterance contains a statement followed by a question; 2) when the end of the current utterance semantically matches the response. Treating the turn-prediction and response-ranking as a one-stage process, our findings suggest that our model can be used as an incremental response ranker, which can be applied in various settings.",
      "published": "2023-05-03T11:06:50Z"
    },
    "metadata": {
      "arxiv_id": "2305.02036",
      "title": "Response-conditioned Turn-taking Prediction",
      "summary": "Previous approaches to turn-taking and response generation in conversational systems have treated it as a two-stage process: First, the end of a turn is detected (based on conversation history), then the system generates an appropriate response. Humans, however, do not take the turn just because it is likely, but also consider whether what they want to say fits the position. In this paper, we present a model (an extension of TurnGPT) that conditions the end-of-turn prediction on both conversation history and what the next speaker wants to say. We found that our model consistently outperforms the baseline model in a variety of metrics. The improvement is most prominent in two scenarios where turn predictions can be ambiguous solely from the conversation history: 1) when the current utterance contains a statement followed by a question; 2) when the end of the current utterance semantically matches the response. Treating the turn-prediction and response-ranking as a one-stage process, our findings suggest that our model can be used as an incremental response ranker, which can be applied in various settings.",
      "authors": [
        "Bing'er Jiang",
        "Erik Ekstedt",
        "Gabriel Skantze"
      ],
      "published": "2023-05-03T11:06:50Z",
      "updated": "2023-05-03T11:06:50Z",
      "categories": [
        "cs.CL",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2305.02036v1",
      "landing_url": "https://arxiv.org/abs/2305.02036v1",
      "doi": "https://doi.org/10.48550/arXiv.2305.02036"
    },
    "queries": [
      {
        "anchor": "turn-taking",
        "search_term": "true full-duplex"
      },
      {
        "anchor": "turn-taking",
        "search_term": "pseudo full-duplex"
      },
      {
        "anchor": "turn-taking",
        "search_term": "half duplex"
      },
      {
        "anchor": "turn-taking",
        "search_term": "engineered synchronization"
      },
      {
        "anchor": "turn-taking",
        "search_term": "learned synchronization"
      },
      {
        "anchor": "turn-taking",
        "search_term": "modular architecture"
      },
      {
        "anchor": "turn-taking",
        "search_term": "streaming architecture"
      },
      {
        "anchor": "turn-taking",
        "search_term": "real-time interaction"
      },
      {
        "anchor": "turn-taking",
        "search_term": "low latency"
      },
      {
        "anchor": "turn-taking",
        "search_term": "synchronous data"
      },
      {
        "anchor": "turn-taking",
        "search_term": "data scarcity"
      },
      {
        "anchor": "turn-taking",
        "search_term": "multichannel dialogue"
      },
      {
        "anchor": "turn-taking",
        "search_term": "temporal dynamics"
      },
      {
        "anchor": "turn-taking",
        "search_term": "behavioral arbitration"
      },
      {
        "anchor": "turn-taking",
        "search_term": "semantic coherence"
      },
      {
        "anchor": "turn-taking",
        "search_term": "neural codec"
      },
      {
        "anchor": "turn-taking",
        "search_term": "discrete tokens"
      },
      {
        "anchor": "turn-taking",
        "search_term": "audio tokens"
      },
      {
        "anchor": "turn-taking",
        "search_term": "floor transfer offset"
      },
      {
        "anchor": "turn-taking",
        "search_term": "stop latency"
      },
      {
        "anchor": "turn-taking",
        "search_term": "interruption response delay"
      }
    ]
  },
  {
    "arxiv_id": "2305.02656",
    "anchor": "synchronous dialogue",
    "search_term": "modular architecture",
    "search_record": {
      "id": "http://arxiv.org/abs/2305.02656v1",
      "title": "The Quantum Internet: an Efficient Stabilizer states Distribution Scheme",
      "summary": "Quantum networks constitute a major part of quantum technologies. They will boost distributed quantum computing drastically by providing a scalable modular architecture of quantum chips, or by establishing an infrastructure for measurement based quantum computing. Moreover, they will provide the backbone of the future quantum internet, allowing for high margins of security. Interestingly, the advantages that the quantum networks would provide for communications, rely on entanglement distribution, which suffers from high latency in protocols based on Bell pair distribution and bipartite entanglement swapping. Moreover, the designed algorithms for multipartite entanglement routing suffer from intractability issues making them unsolvable exactly in polynomial time. In this paper, we investigate a new approach for graph states distribution in quantum networks relying inherently on local quantum coding -- LQC -- isometries and on multipartite states transfer. Additionally, single-shot bounds for stabilizer states distribution are provided. Analogously to network coding, these bounds are shown to be achievable if appropriate isometries/stabilizer codes in relay nodes are chosen, which induces a lower latency entanglement distribution. As a matter of fact, the advantages of the protocol for different figures of merit of the network are provided.",
      "published": "2023-05-04T08:53:38Z"
    },
    "metadata": {
      "arxiv_id": "2305.02656",
      "title": "The Quantum Internet: an Efficient Stabilizer states Distribution Scheme",
      "summary": "Quantum networks constitute a major part of quantum technologies. They will boost distributed quantum computing drastically by providing a scalable modular architecture of quantum chips, or by establishing an infrastructure for measurement based quantum computing. Moreover, they will provide the backbone of the future quantum internet, allowing for high margins of security. Interestingly, the advantages that the quantum networks would provide for communications, rely on entanglement distribution, which suffers from high latency in protocols based on Bell pair distribution and bipartite entanglement swapping. Moreover, the designed algorithms for multipartite entanglement routing suffer from intractability issues making them unsolvable exactly in polynomial time. In this paper, we investigate a new approach for graph states distribution in quantum networks relying inherently on local quantum coding -- LQC -- isometries and on multipartite states transfer. Additionally, single-shot bounds for stabilizer states distribution are provided. Analogously to network coding, these bounds are shown to be achievable if appropriate isometries/stabilizer codes in relay nodes are chosen, which induces a lower latency entanglement distribution. As a matter of fact, the advantages of the protocol for different figures of merit of the network are provided.",
      "authors": [
        "Seid Koudia"
      ],
      "published": "2023-05-04T08:53:38Z",
      "updated": "2023-05-04T08:53:38Z",
      "categories": [
        "quant-ph",
        "cs.NI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2305.02656v1",
      "landing_url": "https://arxiv.org/abs/2305.02656v1",
      "doi": "https://doi.org/10.48550/arXiv.2305.02656"
    },
    "queries": [
      {
        "anchor": "synchronous dialogue",
        "search_term": "modular architecture"
      }
    ]
  },
  {
    "arxiv_id": "2305.06687",
    "anchor": "spoken language models",
    "search_term": "modular architecture",
    "search_record": {
      "id": "http://arxiv.org/abs/2305.06687v1",
      "title": "Mapping quantum circuits to modular architectures with QUBO",
      "summary": "Modular quantum computing architectures are a promising alternative to monolithic QPU (Quantum Processing Unit) designs for scaling up quantum devices. They refer to a set of interconnected QPUs or cores consisting of tightly coupled quantum bits that can communicate via quantum-coherent and classical links. In multi-core architectures, it is crucial to minimize the amount of communication between cores when executing an algorithm. Therefore, mapping a quantum circuit onto a modular architecture involves finding an optimal assignment of logical qubits (qubits in the quantum circuit) to different cores with the aim to minimize the number of expensive inter-core operations while adhering to given hardware constraints. In this paper, we propose for the first time a Quadratic Unconstrained Binary Optimization (QUBO) technique to encode the problem and the solution for both qubit allocation and inter-core communication costs in binary decision variables. To this end, the quantum circuit is split into slices, and qubit assignment is formulated as a graph partitioning problem for each circuit slice. The costly inter-core communication is reduced by penalizing inter-core qubit communications. The final solution is obtained by minimizing the overall cost across all circuit slices. To evaluate the effectiveness of our approach, we conduct a detailed analysis using a representative set of benchmarks having a high number of qubits on two different multi-core architectures. Our method showed promising results and performed exceptionally well with very dense and highly-parallelized circuits that require on average 0.78 inter-core communications per two-qubit gate.",
      "published": "2023-05-11T09:45:47Z"
    },
    "metadata": {
      "arxiv_id": "2305.06687",
      "title": "Mapping quantum circuits to modular architectures with QUBO",
      "summary": "Modular quantum computing architectures are a promising alternative to monolithic QPU (Quantum Processing Unit) designs for scaling up quantum devices. They refer to a set of interconnected QPUs or cores consisting of tightly coupled quantum bits that can communicate via quantum-coherent and classical links. In multi-core architectures, it is crucial to minimize the amount of communication between cores when executing an algorithm. Therefore, mapping a quantum circuit onto a modular architecture involves finding an optimal assignment of logical qubits (qubits in the quantum circuit) to different cores with the aim to minimize the number of expensive inter-core operations while adhering to given hardware constraints. In this paper, we propose for the first time a Quadratic Unconstrained Binary Optimization (QUBO) technique to encode the problem and the solution for both qubit allocation and inter-core communication costs in binary decision variables. To this end, the quantum circuit is split into slices, and qubit assignment is formulated as a graph partitioning problem for each circuit slice. The costly inter-core communication is reduced by penalizing inter-core qubit communications. The final solution is obtained by minimizing the overall cost across all circuit slices. To evaluate the effectiveness of our approach, we conduct a detailed analysis using a representative set of benchmarks having a high number of qubits on two different multi-core architectures. Our method showed promising results and performed exceptionally well with very dense and highly-parallelized circuits that require on average 0.78 inter-core communications per two-qubit gate.",
      "authors": [
        "Medina Bandic",
        "Luise Prielinger",
        "Jonas Nüßlein",
        "Anabel Ovide",
        "Santiago Rodrigo",
        "Sergi Abadal",
        "Hans van Someren",
        "Gayane Vardoyan",
        "Eduard Alarcon",
        "Carmen G. Almudever",
        "Sebastian Feld"
      ],
      "published": "2023-05-11T09:45:47Z",
      "updated": "2023-05-11T09:45:47Z",
      "categories": [
        "quant-ph",
        "cs.ET"
      ],
      "pdf_url": "https://arxiv.org/pdf/2305.06687v1",
      "landing_url": "https://arxiv.org/abs/2305.06687v1",
      "doi": "https://doi.org/10.1109/QCE57702.2023.00094"
    },
    "queries": [
      {
        "anchor": "spoken language models",
        "search_term": "modular architecture"
      },
      {
        "anchor": "synchronous dialogue",
        "search_term": "modular architecture"
      },
      {
        "anchor": "turn-taking",
        "search_term": "modular architecture"
      }
    ]
  },
  {
    "arxiv_id": "2305.07678",
    "anchor": "synchronous dialogue",
    "search_term": "neural codec",
    "search_record": {
      "id": "http://arxiv.org/abs/2305.07678v1",
      "title": "Exploring the Rate-Distortion-Complexity Optimization in Neural Image Compression",
      "summary": "Despite a short history, neural image codecs have been shown to surpass classical image codecs in terms of rate-distortion performance. However, most of them suffer from significantly longer decoding times, which hinders the practical applications of neural image codecs. This issue is especially pronounced when employing an effective yet time-consuming autoregressive context model since it would increase entropy decoding time by orders of magnitude. In this paper, unlike most previous works that pursue optimal RD performance while temporally overlooking the coding complexity, we make a systematical investigation on the rate-distortion-complexity (RDC) optimization in neural image compression. By quantifying the decoding complexity as a factor in the optimization goal, we are now able to precisely control the RDC trade-off and then demonstrate how the rate-distortion performance of neural image codecs could adapt to various complexity demands. Going beyond the investigation of RDC optimization, a variable-complexity neural codec is designed to leverage the spatial dependencies adaptively according to industrial demands, which supports fine-grained complexity adjustment by balancing the RDC tradeoff. By implementing this scheme in a powerful base model, we demonstrate the feasibility and flexibility of RDC optimization for neural image codecs.",
      "published": "2023-05-12T03:56:25Z"
    },
    "metadata": {
      "arxiv_id": "2305.07678",
      "title": "Exploring the Rate-Distortion-Complexity Optimization in Neural Image Compression",
      "summary": "Despite a short history, neural image codecs have been shown to surpass classical image codecs in terms of rate-distortion performance. However, most of them suffer from significantly longer decoding times, which hinders the practical applications of neural image codecs. This issue is especially pronounced when employing an effective yet time-consuming autoregressive context model since it would increase entropy decoding time by orders of magnitude. In this paper, unlike most previous works that pursue optimal RD performance while temporally overlooking the coding complexity, we make a systematical investigation on the rate-distortion-complexity (RDC) optimization in neural image compression. By quantifying the decoding complexity as a factor in the optimization goal, we are now able to precisely control the RDC trade-off and then demonstrate how the rate-distortion performance of neural image codecs could adapt to various complexity demands. Going beyond the investigation of RDC optimization, a variable-complexity neural codec is designed to leverage the spatial dependencies adaptively according to industrial demands, which supports fine-grained complexity adjustment by balancing the RDC tradeoff. By implementing this scheme in a powerful base model, we demonstrate the feasibility and flexibility of RDC optimization for neural image codecs.",
      "authors": [
        "Yixin Gao",
        "Runsen Feng",
        "Zongyu Guo",
        "Zhibo Chen"
      ],
      "published": "2023-05-12T03:56:25Z",
      "updated": "2023-05-12T03:56:25Z",
      "categories": [
        "eess.IV",
        "cs.IT",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2305.07678v1",
      "landing_url": "https://arxiv.org/abs/2305.07678v1",
      "doi": "https://doi.org/10.48550/arXiv.2305.07678"
    },
    "queries": [
      {
        "anchor": "synchronous dialogue",
        "search_term": "neural codec"
      }
    ]
  },
  {
    "arxiv_id": "2305.07935",
    "anchor": "synchronous dialogue",
    "search_term": "streaming architecture",
    "search_record": {
      "id": "http://arxiv.org/abs/2305.07935v1",
      "title": "Streaming 360-degree VR Video with Statistical QoS Provisioning in mmWave Networks from Delay and Rate Perspectives",
      "summary": "Millimeter-wave(mmWave) technology has emerged as a promising enabler for unleashing the full potential of 360-degree virtual reality (VR). However, the explosive growth of VR services, coupled with the reliability issues of mmWave communications, poses enormous challenges in terms of wireless resource and quality-of-service (QoS) provisioning for mmWave-enabled 360-degree VR. In this paper, we propose an innovative 360-degree VR streaming architecture that addresses three under-exploited issues: overlapping field-of-views (FoVs), statistical QoS provisioning (SQP), and loss-tolerant active data discarding. Specifically, an overlapping FoV-based optimal joint unicast and multicast (JUM) task assignment scheme is designed to implement the non-redundant task assignments, thereby conserving wireless resources remarkably. Furthermore, leveraging stochastic network calculus, we develop a comprehensive SQP theoretical framework that encompasses two SQP schemes from delay and rate perspectives. Additionally, a corresponding optimal adaptive joint time-slot allocation and active-discarding (ADAPT-JTAAT) transmission scheme is proposed to minimize resource consumption while guaranteeing diverse statistical QoS requirements under loss-intolerant and loss-tolerant scenarios from delay and rate perspectives, respectively. Extensive simulations demonstrate the effectiveness of the designed overlapping FoV-based JUM optimal task assignment scheme. Comparisons with six baseline schemes validate that the proposed optimal ADAPTJTAAT transmission scheme can achieve superior SQP performance in resource utilization, flexible rate control, and robust queue behaviors.",
      "published": "2023-05-13T14:57:27Z"
    },
    "metadata": {
      "arxiv_id": "2305.07935",
      "title": "Streaming 360-degree VR Video with Statistical QoS Provisioning in mmWave Networks from Delay and Rate Perspectives",
      "summary": "Millimeter-wave(mmWave) technology has emerged as a promising enabler for unleashing the full potential of 360-degree virtual reality (VR). However, the explosive growth of VR services, coupled with the reliability issues of mmWave communications, poses enormous challenges in terms of wireless resource and quality-of-service (QoS) provisioning for mmWave-enabled 360-degree VR. In this paper, we propose an innovative 360-degree VR streaming architecture that addresses three under-exploited issues: overlapping field-of-views (FoVs), statistical QoS provisioning (SQP), and loss-tolerant active data discarding. Specifically, an overlapping FoV-based optimal joint unicast and multicast (JUM) task assignment scheme is designed to implement the non-redundant task assignments, thereby conserving wireless resources remarkably. Furthermore, leveraging stochastic network calculus, we develop a comprehensive SQP theoretical framework that encompasses two SQP schemes from delay and rate perspectives. Additionally, a corresponding optimal adaptive joint time-slot allocation and active-discarding (ADAPT-JTAAT) transmission scheme is proposed to minimize resource consumption while guaranteeing diverse statistical QoS requirements under loss-intolerant and loss-tolerant scenarios from delay and rate perspectives, respectively. Extensive simulations demonstrate the effectiveness of the designed overlapping FoV-based JUM optimal task assignment scheme. Comparisons with six baseline schemes validate that the proposed optimal ADAPTJTAAT transmission scheme can achieve superior SQP performance in resource utilization, flexible rate control, and robust queue behaviors.",
      "authors": [
        "Yuang Chen",
        "Hancheng Lu",
        "Langtian Qin",
        "Chang Wu",
        "Chang Wen Chen"
      ],
      "published": "2023-05-13T14:57:27Z",
      "updated": "2023-05-13T14:57:27Z",
      "categories": [
        "cs.IT",
        "eess.IV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2305.07935v1",
      "landing_url": "https://arxiv.org/abs/2305.07935v1",
      "doi": "https://doi.org/10.48550/arXiv.2305.07935"
    },
    "queries": [
      {
        "anchor": "synchronous dialogue",
        "search_term": "streaming architecture"
      }
    ]
  },
  {
    "arxiv_id": "2305.11423",
    "anchor": "spoken language models",
    "search_term": "streaming architecture",
    "search_record": {
      "id": "http://arxiv.org/abs/2305.11423v1",
      "title": "Strix: An End-to-End Streaming Architecture with Two-Level Ciphertext Batching for Fully Homomorphic Encryption with Programmable Bootstrapping",
      "summary": "Homomorphic encryption (HE) enables computations on encrypted data by concealing information under noise for security. However, the process of bootstrapping, which resets the noise level in the ciphertext, is computationally expensive and requires a large bootstrapping key. The TFHE scheme offers a faster and programmable bootstrapping algorithm called PBS, crucial for security-focused applications like machine learning. Nevertheless, the current TFHE scheme lacks support for ciphertext packing, resulting in low throughput. This work thoroughly analyzes TFHE bootstrapping, identifies the bottleneck in GPUs caused by the blind rotation fragmentation problem, and proposes a hardware TFHE accelerator called Strix. Strix introduces a two-level batching approach to enhance the batch size in PBS, utilizes a specialized microarchitecture for efficient streaming data processing, and incorporates a fully-pipelined FFT microarchitecture to improve performance. It achieves significantly higher throughput than state-of-the-art implementations on both CPUs and GPUs, outperforming existing TFHE accelerators by a factor of 7.4.",
      "published": "2023-05-19T04:40:04Z"
    },
    "metadata": {
      "arxiv_id": "2305.11423",
      "title": "Strix: An End-to-End Streaming Architecture with Two-Level Ciphertext Batching for Fully Homomorphic Encryption with Programmable Bootstrapping",
      "summary": "Homomorphic encryption (HE) enables computations on encrypted data by concealing information under noise for security. However, the process of bootstrapping, which resets the noise level in the ciphertext, is computationally expensive and requires a large bootstrapping key. The TFHE scheme offers a faster and programmable bootstrapping algorithm called PBS, crucial for security-focused applications like machine learning. Nevertheless, the current TFHE scheme lacks support for ciphertext packing, resulting in low throughput. This work thoroughly analyzes TFHE bootstrapping, identifies the bottleneck in GPUs caused by the blind rotation fragmentation problem, and proposes a hardware TFHE accelerator called Strix. Strix introduces a two-level batching approach to enhance the batch size in PBS, utilizes a specialized microarchitecture for efficient streaming data processing, and incorporates a fully-pipelined FFT microarchitecture to improve performance. It achieves significantly higher throughput than state-of-the-art implementations on both CPUs and GPUs, outperforming existing TFHE accelerators by a factor of 7.4.",
      "authors": [
        "Adiwena Putra",
        "Prasetiyo",
        "Yi Chen",
        "John Kim",
        "Joo-Young Kim"
      ],
      "published": "2023-05-19T04:40:04Z",
      "updated": "2023-05-19T04:40:04Z",
      "categories": [
        "cs.CR",
        "cs.AR"
      ],
      "pdf_url": "https://arxiv.org/pdf/2305.11423v1",
      "landing_url": "https://arxiv.org/abs/2305.11423v1",
      "doi": "https://doi.org/10.48550/arXiv.2305.11423"
    },
    "queries": [
      {
        "anchor": "spoken language models",
        "search_term": "streaming architecture"
      },
      {
        "anchor": "synchronous dialogue",
        "search_term": "streaming architecture"
      },
      {
        "anchor": "turn-taking",
        "search_term": "streaming architecture"
      }
    ]
  },
  {
    "arxiv_id": "2305.12144",
    "anchor": "synchronous dialogue",
    "search_term": "discrete tokens",
    "search_record": {
      "id": "http://arxiv.org/abs/2305.12144v1",
      "title": "DiffCap: Exploring Continuous Diffusion on Image Captioning",
      "summary": "Current image captioning works usually focus on generating descriptions in an autoregressive manner. However, there are limited works that focus on generating descriptions non-autoregressively, which brings more decoding diversity. Inspired by the success of diffusion models on generating natural-looking images, we propose a novel method DiffCap to apply continuous diffusions on image captioning. Unlike image generation where the output is fixed-size and continuous, image description length varies with discrete tokens. Our method transforms discrete tokens in a natural way and applies continuous diffusion on them to successfully fuse extracted image features for diffusion caption generation. Our experiments on COCO dataset demonstrate that our method uses a much simpler structure to achieve comparable results to the previous non-autoregressive works. Apart from quality, an intriguing property of DiffCap is its high diversity during generation, which is missing from many autoregressive models. We believe our method on fusing multimodal features in diffusion language generation will inspire more researches on multimodal language generation tasks for its simplicity and decoding flexibility.",
      "published": "2023-05-20T09:02:10Z"
    },
    "metadata": {
      "arxiv_id": "2305.12144",
      "title": "DiffCap: Exploring Continuous Diffusion on Image Captioning",
      "summary": "Current image captioning works usually focus on generating descriptions in an autoregressive manner. However, there are limited works that focus on generating descriptions non-autoregressively, which brings more decoding diversity. Inspired by the success of diffusion models on generating natural-looking images, we propose a novel method DiffCap to apply continuous diffusions on image captioning. Unlike image generation where the output is fixed-size and continuous, image description length varies with discrete tokens. Our method transforms discrete tokens in a natural way and applies continuous diffusion on them to successfully fuse extracted image features for diffusion caption generation. Our experiments on COCO dataset demonstrate that our method uses a much simpler structure to achieve comparable results to the previous non-autoregressive works. Apart from quality, an intriguing property of DiffCap is its high diversity during generation, which is missing from many autoregressive models. We believe our method on fusing multimodal features in diffusion language generation will inspire more researches on multimodal language generation tasks for its simplicity and decoding flexibility.",
      "authors": [
        "Yufeng He",
        "Zefan Cai",
        "Xu Gan",
        "Baobao Chang"
      ],
      "published": "2023-05-20T09:02:10Z",
      "updated": "2023-05-20T09:02:10Z",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2305.12144v1",
      "landing_url": "https://arxiv.org/abs/2305.12144v1",
      "doi": "https://doi.org/10.48550/arXiv.2305.12144"
    },
    "queries": [
      {
        "anchor": "synchronous dialogue",
        "search_term": "discrete tokens"
      }
    ]
  },
  {
    "arxiv_id": "2305.12333",
    "anchor": "synchronous dialogue",
    "search_term": "neural codec",
    "search_record": {
      "id": "http://arxiv.org/abs/2305.12333v4",
      "title": "GRACE: Loss-Resilient Real-Time Video through Neural Codecs",
      "summary": "In real-time video communication, retransmitting lost packets over high-latency networks is not viable due to strict latency requirements. To counter packet losses without retransmission, two primary strategies are employed -- encoder-based forward error correction (FEC) and decoder-based error concealment. The former encodes data with redundancy before transmission, yet determining the optimal redundancy level in advance proves challenging. The latter reconstructs video from partially received frames, but dividing a frame into independently coded partitions inherently compromises compression efficiency, and the lost information cannot be effectively recovered by the decoder without adapting the encoder. We present a loss-resilient real-time video system called GRACE, which preserves the user's quality of experience (QoE) across a wide range of packet losses through a new neural video codec. Central to GRACE's enhanced loss resilience is its joint training of the neural encoder and decoder under a spectrum of simulated packet losses. In lossless scenarios, GRACE achieves video quality on par with conventional codecs (e.g., H.265). As the loss rate escalates, GRACE exhibits a more graceful, less pronounced decline in quality, consistently outperforming other loss-resilient schemes. Through extensive evaluation on various videos and real network traces, we demonstrate that GRACE reduces undecodable frames by 95% and stall duration by 90% compared with FEC, while markedly boosting video quality over error concealment methods. In a user study with 240 crowdsourced participants and 960 subjective ratings, GRACE registers a 38% higher mean opinion score (MOS) than other baselines.",
      "published": "2023-05-21T03:50:44Z"
    },
    "metadata": {
      "arxiv_id": "2305.12333",
      "title": "GRACE: Loss-Resilient Real-Time Video through Neural Codecs",
      "summary": "In real-time video communication, retransmitting lost packets over high-latency networks is not viable due to strict latency requirements. To counter packet losses without retransmission, two primary strategies are employed -- encoder-based forward error correction (FEC) and decoder-based error concealment. The former encodes data with redundancy before transmission, yet determining the optimal redundancy level in advance proves challenging. The latter reconstructs video from partially received frames, but dividing a frame into independently coded partitions inherently compromises compression efficiency, and the lost information cannot be effectively recovered by the decoder without adapting the encoder. We present a loss-resilient real-time video system called GRACE, which preserves the user's quality of experience (QoE) across a wide range of packet losses through a new neural video codec. Central to GRACE's enhanced loss resilience is its joint training of the neural encoder and decoder under a spectrum of simulated packet losses. In lossless scenarios, GRACE achieves video quality on par with conventional codecs (e.g., H.265). As the loss rate escalates, GRACE exhibits a more graceful, less pronounced decline in quality, consistently outperforming other loss-resilient schemes. Through extensive evaluation on various videos and real network traces, we demonstrate that GRACE reduces undecodable frames by 95% and stall duration by 90% compared with FEC, while markedly boosting video quality over error concealment methods. In a user study with 240 crowdsourced participants and 960 subjective ratings, GRACE registers a 38% higher mean opinion score (MOS) than other baselines.",
      "authors": [
        "Yihua Cheng",
        "Ziyi Zhang",
        "Hanchen Li",
        "Anton Arapin",
        "Yue Zhang",
        "Qizheng Zhang",
        "Yuhan Liu",
        "Xu Zhang",
        "Francis Y. Yan",
        "Amrita Mazumdar",
        "Nick Feamster",
        "Junchen Jiang"
      ],
      "published": "2023-05-21T03:50:44Z",
      "updated": "2024-03-12T21:40:53Z",
      "categories": [
        "cs.MM",
        "cs.AI",
        "cs.NI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2305.12333v4",
      "landing_url": "https://arxiv.org/abs/2305.12333v4",
      "doi": "https://doi.org/10.48550/arXiv.2305.12333"
    },
    "queries": [
      {
        "anchor": "synchronous dialogue",
        "search_term": "neural codec"
      }
    ]
  },
  {
    "arxiv_id": "2305.12908",
    "anchor": "spoken language models",
    "search_term": "data scarcity",
    "search_record": {
      "id": "http://arxiv.org/abs/2305.12908v1",
      "title": "Language Models for German Text Simplification: Overcoming Parallel Data Scarcity through Style-specific Pre-training",
      "summary": "Automatic text simplification systems help to reduce textual information barriers on the internet. However, for languages other than English, only few parallel data to train these systems exists. We propose a two-step approach to overcome this data scarcity issue. First, we fine-tuned language models on a corpus of German Easy Language, a specific style of German. Then, we used these models as decoders in a sequence-to-sequence simplification task. We show that the language models adapt to the style characteristics of Easy Language and output more accessible texts. Moreover, with the style-specific pre-training, we reduced the number of trainable parameters in text simplification models. Hence, less parallel data is sufficient for training. Our results indicate that pre-training on unaligned data can reduce the required parallel data while improving the performance on downstream tasks.",
      "published": "2023-05-22T10:41:30Z"
    },
    "metadata": {
      "arxiv_id": "2305.12908",
      "title": "Language Models for German Text Simplification: Overcoming Parallel Data Scarcity through Style-specific Pre-training",
      "summary": "Automatic text simplification systems help to reduce textual information barriers on the internet. However, for languages other than English, only few parallel data to train these systems exists. We propose a two-step approach to overcome this data scarcity issue. First, we fine-tuned language models on a corpus of German Easy Language, a specific style of German. Then, we used these models as decoders in a sequence-to-sequence simplification task. We show that the language models adapt to the style characteristics of Easy Language and output more accessible texts. Moreover, with the style-specific pre-training, we reduced the number of trainable parameters in text simplification models. Hence, less parallel data is sufficient for training. Our results indicate that pre-training on unaligned data can reduce the required parallel data while improving the performance on downstream tasks.",
      "authors": [
        "Miriam Anschütz",
        "Joshua Oehms",
        "Thomas Wimmer",
        "Bartłomiej Jezierski",
        "Georg Groh"
      ],
      "published": "2023-05-22T10:41:30Z",
      "updated": "2023-05-22T10:41:30Z",
      "categories": [
        "cs.CL"
      ],
      "pdf_url": "https://arxiv.org/pdf/2305.12908v1",
      "landing_url": "https://arxiv.org/abs/2305.12908v1",
      "doi": "https://doi.org/10.18653/v1/2023.findings-acl.74"
    },
    "queries": [
      {
        "anchor": "spoken language models",
        "search_term": "data scarcity"
      },
      {
        "anchor": "synchronous dialogue",
        "search_term": "data scarcity"
      },
      {
        "anchor": "turn-taking",
        "search_term": "data scarcity"
      }
    ]
  },
  {
    "arxiv_id": "2305.15006",
    "anchor": "spoken language models",
    "search_term": "data scarcity",
    "search_record": {
      "id": "http://arxiv.org/abs/2305.15006v2",
      "title": "A Human-in-the-Loop Approach for Information Extraction from Privacy Policies under Data Scarcity",
      "summary": "Machine-readable representations of privacy policies are door openers for a broad variety of novel privacy-enhancing and, in particular, transparency-enhancing technologies (TETs). In order to generate such representations, transparency information needs to be extracted from written privacy policies. However, respective manual annotation and extraction processes are laborious and require expert knowledge. Approaches for fully automated annotation, in turn, have so far not succeeded due to overly high error rates in the specific domain of privacy policies. In the end, a lack of properly annotated privacy policies and respective machine-readable representations persists and enduringly hinders the development and establishment of novel technical approaches fostering policy perception and data subject informedness.\n  In this work, we present a prototype system for a `Human-in-the-Loop' approach to privacy policy annotation that integrates ML-generated suggestions and ultimately human annotation decisions. We propose an ML-based suggestion system specifically tailored to the constraint of data scarcity prevalent in the domain of privacy policy annotation. On this basis, we provide meaningful predictions to users thereby streamlining the annotation process. Additionally, we also evaluate our approach through a prototypical implementation to show that our ML-based extraction approach provides superior performance over other recently used extraction models for legal documents.",
      "published": "2023-05-24T10:45:26Z"
    },
    "metadata": {
      "arxiv_id": "2305.15006",
      "title": "A Human-in-the-Loop Approach for Information Extraction from Privacy Policies under Data Scarcity",
      "summary": "Machine-readable representations of privacy policies are door openers for a broad variety of novel privacy-enhancing and, in particular, transparency-enhancing technologies (TETs). In order to generate such representations, transparency information needs to be extracted from written privacy policies. However, respective manual annotation and extraction processes are laborious and require expert knowledge. Approaches for fully automated annotation, in turn, have so far not succeeded due to overly high error rates in the specific domain of privacy policies. In the end, a lack of properly annotated privacy policies and respective machine-readable representations persists and enduringly hinders the development and establishment of novel technical approaches fostering policy perception and data subject informedness.\n  In this work, we present a prototype system for a `Human-in-the-Loop' approach to privacy policy annotation that integrates ML-generated suggestions and ultimately human annotation decisions. We propose an ML-based suggestion system specifically tailored to the constraint of data scarcity prevalent in the domain of privacy policy annotation. On this basis, we provide meaningful predictions to users thereby streamlining the annotation process. Additionally, we also evaluate our approach through a prototypical implementation to show that our ML-based extraction approach provides superior performance over other recently used extraction models for legal documents.",
      "authors": [
        "Michael Gebauer",
        "Faraz Maschhur",
        "Nicola Leschke",
        "Elias Grünewald",
        "Frank Pallas"
      ],
      "published": "2023-05-24T10:45:26Z",
      "updated": "2023-05-31T09:58:15Z",
      "categories": [
        "cs.CY",
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2305.15006v2",
      "landing_url": "https://arxiv.org/abs/2305.15006v2",
      "doi": "https://doi.org/10.48550/arXiv.2305.15006"
    },
    "queries": [
      {
        "anchor": "spoken language models",
        "search_term": "data scarcity"
      },
      {
        "anchor": "synchronous dialogue",
        "search_term": "data scarcity"
      },
      {
        "anchor": "turn-taking",
        "search_term": "data scarcity"
      }
    ]
  },
  {
    "arxiv_id": "2305.15058",
    "anchor": "spoken language models",
    "search_term": "synchronous data",
    "search_record": {
      "id": "http://arxiv.org/abs/2305.15058v1",
      "title": "Bistatic OFDM-based Joint Radar-Communication: Synchronization, Data Communication and Sensing",
      "summary": "This article introduces a bistatic joint radar-communication (RadCom) system based on orthogonal frequency-division multiplexing (OFDM). In this context, the adopted OFDM frame structure is described and system model encompassing time, frequency, and sampling synchronization mismatches between the transmitter and receiver of the bistatic system is outlined. Next, the signal processing approaches for synchronization and communication are discussed, and radar sensing processing approaches using either only pilots or a reconstructed OFDM frame based on the estimated receive communication data are presented. Finally, proof-of-concept measurement results are presented to validate the investigated system and a trade-off between frame size and the performance of the aforementioned processing steps is observed.",
      "published": "2023-05-24T11:48:19Z"
    },
    "metadata": {
      "arxiv_id": "2305.15058",
      "title": "Bistatic OFDM-based Joint Radar-Communication: Synchronization, Data Communication and Sensing",
      "summary": "This article introduces a bistatic joint radar-communication (RadCom) system based on orthogonal frequency-division multiplexing (OFDM). In this context, the adopted OFDM frame structure is described and system model encompassing time, frequency, and sampling synchronization mismatches between the transmitter and receiver of the bistatic system is outlined. Next, the signal processing approaches for synchronization and communication are discussed, and radar sensing processing approaches using either only pilots or a reconstructed OFDM frame based on the estimated receive communication data are presented. Finally, proof-of-concept measurement results are presented to validate the investigated system and a trade-off between frame size and the performance of the aforementioned processing steps is observed.",
      "authors": [
        "Lucas Giroto de Oliveira",
        "David Brunner",
        "Axel Diewald",
        "Charlotte Muth",
        "Laurent Schmalen",
        "Thomas Zwick",
        "Benjamin Nuss"
      ],
      "published": "2023-05-24T11:48:19Z",
      "updated": "2023-05-24T11:48:19Z",
      "categories": [
        "eess.SP"
      ],
      "pdf_url": "https://arxiv.org/pdf/2305.15058v1",
      "landing_url": "https://arxiv.org/abs/2305.15058v1",
      "doi": "https://doi.org/10.48550/arXiv.2305.15058"
    },
    "queries": [
      {
        "anchor": "spoken language models",
        "search_term": "synchronous data"
      },
      {
        "anchor": "synchronous dialogue",
        "search_term": "synchronous data"
      }
    ]
  },
  {
    "arxiv_id": "2305.15255",
    "anchor": "spoken language models",
    "search_term": "true full-duplex",
    "search_record": {
      "id": "http://arxiv.org/abs/2305.15255v4",
      "title": "Spoken Question Answering and Speech Continuation Using Spectrogram-Powered LLM",
      "summary": "We present Spectron, a novel approach to adapting pre-trained large language models (LLMs) to perform spoken question answering (QA) and speech continuation. By endowing the LLM with a pre-trained speech encoder, our model becomes able to take speech inputs and generate speech outputs. The entire system is trained end-to-end and operates directly on spectrograms, simplifying our architecture. Key to our approach is a training objective that jointly supervises speech recognition, text continuation, and speech synthesis using only paired speech-text pairs, enabling a `cross-modal' chain-of-thought within a single decoding pass. Our method surpasses existing spoken language models in speaker preservation and semantic coherence. Furthermore, the proposed model improves upon direct initialization in retaining the knowledge of the original LLM as demonstrated through spoken QA datasets. We release our audio samples (https://michelleramanovich.github.io/spectron/spectron) and spoken QA dataset (https://github.com/google-research-datasets/LLAMA1-Test-Set).",
      "published": "2023-05-24T15:39:43Z"
    },
    "metadata": {
      "arxiv_id": "2305.15255",
      "title": "Spoken Question Answering and Speech Continuation Using Spectrogram-Powered LLM",
      "summary": "We present Spectron, a novel approach to adapting pre-trained large language models (LLMs) to perform spoken question answering (QA) and speech continuation. By endowing the LLM with a pre-trained speech encoder, our model becomes able to take speech inputs and generate speech outputs. The entire system is trained end-to-end and operates directly on spectrograms, simplifying our architecture. Key to our approach is a training objective that jointly supervises speech recognition, text continuation, and speech synthesis using only paired speech-text pairs, enabling a `cross-modal' chain-of-thought within a single decoding pass. Our method surpasses existing spoken language models in speaker preservation and semantic coherence. Furthermore, the proposed model improves upon direct initialization in retaining the knowledge of the original LLM as demonstrated through spoken QA datasets. We release our audio samples (https://michelleramanovich.github.io/spectron/spectron) and spoken QA dataset (https://github.com/google-research-datasets/LLAMA1-Test-Set).",
      "authors": [
        "Eliya Nachmani",
        "Alon Levkovitch",
        "Roy Hirsch",
        "Julian Salazar",
        "Chulayuth Asawaroengchai",
        "Soroosh Mariooryad",
        "Ehud Rivlin",
        "RJ Skerry-Ryan",
        "Michelle Tadmor Ramanovich"
      ],
      "published": "2023-05-24T15:39:43Z",
      "updated": "2024-05-31T01:29:27Z",
      "categories": [
        "cs.CL",
        "cs.LG",
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2305.15255v4",
      "landing_url": "https://arxiv.org/abs/2305.15255v4",
      "doi": "https://doi.org/10.48550/arXiv.2305.15255"
    },
    "queries": [
      {
        "anchor": "spoken language models",
        "search_term": "true full-duplex"
      },
      {
        "anchor": "spoken language models",
        "search_term": "pseudo full-duplex"
      },
      {
        "anchor": "spoken language models",
        "search_term": "engineered synchronization"
      },
      {
        "anchor": "spoken language models",
        "search_term": "learned synchronization"
      },
      {
        "anchor": "spoken language models",
        "search_term": "modular architecture"
      },
      {
        "anchor": "spoken language models",
        "search_term": "streaming architecture"
      },
      {
        "anchor": "spoken language models",
        "search_term": "real-time interaction"
      },
      {
        "anchor": "spoken language models",
        "search_term": "synchronous data"
      },
      {
        "anchor": "spoken language models",
        "search_term": "data scarcity"
      },
      {
        "anchor": "spoken language models",
        "search_term": "multichannel dialogue"
      },
      {
        "anchor": "spoken language models",
        "search_term": "behavioral arbitration"
      },
      {
        "anchor": "spoken language models",
        "search_term": "semantic coherence"
      },
      {
        "anchor": "spoken language models",
        "search_term": "neural codec"
      },
      {
        "anchor": "spoken language models",
        "search_term": "discrete tokens"
      },
      {
        "anchor": "spoken language models",
        "search_term": "audio tokens"
      },
      {
        "anchor": "spoken language models",
        "search_term": "floor transfer offset"
      },
      {
        "anchor": "spoken language models",
        "search_term": "stop latency"
      },
      {
        "anchor": "spoken language models",
        "search_term": "interruption response delay"
      }
    ]
  },
  {
    "arxiv_id": "2305.15858",
    "anchor": "synchronous dialogue",
    "search_term": "low latency",
    "search_record": {
      "id": "http://arxiv.org/abs/2305.15858v1",
      "title": "LLHR: Low Latency and High Reliability CNN Distributed Inference for Resource-Constrained UAV Swarms",
      "summary": "Recently, Unmanned Aerial Vehicles (UAVs) have shown impressive performance in many critical applications, such as surveillance, search and rescue operations, environmental monitoring, etc. In many of these applications, the UAVs capture images as well as other sensory data and then send the data processing requests to remote servers. Nevertheless, this approach is not always practical in real-time-based applications due to unstable connections, limited bandwidth, limited energy, and strict end-to-end latency. One promising solution is to divide the inference requests into subtasks that can be distributed among UAVs in a swarm based on the available resources. Moreover, these tasks create intermediate results that need to be transmitted reliably as the swarm moves to cover the area. Our system model deals with real-time requests, aiming to find the optimal transmission power that guarantees higher reliability and low latency. We formulate the Low Latency and High-Reliability (LLHR) distributed inference as an optimization problem, and due to the complexity of the problem, we divide it into three subproblems. In the first subproblem, we find the optimal transmit power of the connected UAVs with guaranteed transmission reliability. The second subproblem aims to find the optimal positions of the UAVs in the grid, while the last subproblem finds the optimal placement of the CNN layers in the available UAVs. We conduct extensive simulations and compare our work to two baseline models demonstrating that our model outperforms the competing models.",
      "published": "2023-05-25T08:47:16Z"
    },
    "metadata": {
      "arxiv_id": "2305.15858",
      "title": "LLHR: Low Latency and High Reliability CNN Distributed Inference for Resource-Constrained UAV Swarms",
      "summary": "Recently, Unmanned Aerial Vehicles (UAVs) have shown impressive performance in many critical applications, such as surveillance, search and rescue operations, environmental monitoring, etc. In many of these applications, the UAVs capture images as well as other sensory data and then send the data processing requests to remote servers. Nevertheless, this approach is not always practical in real-time-based applications due to unstable connections, limited bandwidth, limited energy, and strict end-to-end latency. One promising solution is to divide the inference requests into subtasks that can be distributed among UAVs in a swarm based on the available resources. Moreover, these tasks create intermediate results that need to be transmitted reliably as the swarm moves to cover the area. Our system model deals with real-time requests, aiming to find the optimal transmission power that guarantees higher reliability and low latency. We formulate the Low Latency and High-Reliability (LLHR) distributed inference as an optimization problem, and due to the complexity of the problem, we divide it into three subproblems. In the first subproblem, we find the optimal transmit power of the connected UAVs with guaranteed transmission reliability. The second subproblem aims to find the optimal positions of the UAVs in the grid, while the last subproblem finds the optimal placement of the CNN layers in the available UAVs. We conduct extensive simulations and compare our work to two baseline models demonstrating that our model outperforms the competing models.",
      "authors": [
        "Marwan Dhuheir",
        "Aiman Erbad",
        "Sinan Sabeeh"
      ],
      "published": "2023-05-25T08:47:16Z",
      "updated": "2023-05-25T08:47:16Z",
      "categories": [
        "cs.DC",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2305.15858v1",
      "landing_url": "https://arxiv.org/abs/2305.15858v1",
      "doi": "https://doi.org/10.1109/WCNC55385.2023.10118908"
    },
    "queries": [
      {
        "anchor": "synchronous dialogue",
        "search_term": "low latency"
      }
    ]
  },
  {
    "arxiv_id": "2305.16107",
    "anchor": "synchronous dialogue",
    "search_term": "neural codec",
    "search_record": {
      "id": "http://arxiv.org/abs/2305.16107v1",
      "title": "VioLA: Unified Codec Language Models for Speech Recognition, Synthesis, and Translation",
      "summary": "Recent research shows a big convergence in model architecture, training objectives, and inference methods across various tasks for different modalities. In this paper, we propose VioLA, a single auto-regressive Transformer decoder-only network that unifies various cross-modal tasks involving speech and text, such as speech-to-text, text-to-text, text-to-speech, and speech-to-speech tasks, as a conditional codec language model task via multi-task learning framework. To accomplish this, we first convert all the speech utterances to discrete tokens (similar to the textual data) using an offline neural codec encoder. In such a way, all these tasks are converted to token-based sequence conversion problems, which can be naturally handled with one conditional language model. We further integrate task IDs (TID) and language IDs (LID) into the proposed model to enhance the modeling capability of handling different languages and tasks. Experimental results demonstrate that the proposed VioLA model can support both single-modal and cross-modal tasks well, and the decoder-only model achieves a comparable and even better performance than the strong baselines.",
      "published": "2023-05-25T14:39:47Z"
    },
    "metadata": {
      "arxiv_id": "2305.16107",
      "title": "VioLA: Unified Codec Language Models for Speech Recognition, Synthesis, and Translation",
      "summary": "Recent research shows a big convergence in model architecture, training objectives, and inference methods across various tasks for different modalities. In this paper, we propose VioLA, a single auto-regressive Transformer decoder-only network that unifies various cross-modal tasks involving speech and text, such as speech-to-text, text-to-text, text-to-speech, and speech-to-speech tasks, as a conditional codec language model task via multi-task learning framework. To accomplish this, we first convert all the speech utterances to discrete tokens (similar to the textual data) using an offline neural codec encoder. In such a way, all these tasks are converted to token-based sequence conversion problems, which can be naturally handled with one conditional language model. We further integrate task IDs (TID) and language IDs (LID) into the proposed model to enhance the modeling capability of handling different languages and tasks. Experimental results demonstrate that the proposed VioLA model can support both single-modal and cross-modal tasks well, and the decoder-only model achieves a comparable and even better performance than the strong baselines.",
      "authors": [
        "Tianrui Wang",
        "Long Zhou",
        "Ziqiang Zhang",
        "Yu Wu",
        "Shujie Liu",
        "Yashesh Gaur",
        "Zhuo Chen",
        "Jinyu Li",
        "Furu Wei"
      ],
      "published": "2023-05-25T14:39:47Z",
      "updated": "2023-05-25T14:39:47Z",
      "categories": [
        "cs.CL",
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2305.16107v1",
      "landing_url": "https://arxiv.org/abs/2305.16107v1",
      "doi": "https://doi.org/10.48550/arXiv.2305.16107"
    },
    "queries": [
      {
        "anchor": "synchronous dialogue",
        "search_term": "neural codec"
      }
    ]
  },
  {
    "arxiv_id": "2305.16233",
    "anchor": "synchronous dialogue",
    "search_term": "real-time interaction",
    "search_record": {
      "id": "http://arxiv.org/abs/2305.16233v1",
      "title": "Interactive Segment Anything NeRF with Feature Imitation",
      "summary": "This paper investigates the potential of enhancing Neural Radiance Fields (NeRF) with semantics to expand their applications. Although NeRF has been proven useful in real-world applications like VR and digital creation, the lack of semantics hinders interaction with objects in complex scenes. We propose to imitate the backbone feature of off-the-shelf perception models to achieve zero-shot semantic segmentation with NeRF. Our framework reformulates the segmentation process by directly rendering semantic features and only applying the decoder from perception models. This eliminates the need for expensive backbones and benefits 3D consistency. Furthermore, we can project the learned semantics onto extracted mesh surfaces for real-time interaction. With the state-of-the-art Segment Anything Model (SAM), our framework accelerates segmentation by 16 times with comparable mask quality. The experimental results demonstrate the efficacy and computational advantages of our approach. Project page: \\url{https://me.kiui.moe/san/}.",
      "published": "2023-05-25T16:44:51Z"
    },
    "metadata": {
      "arxiv_id": "2305.16233",
      "title": "Interactive Segment Anything NeRF with Feature Imitation",
      "summary": "This paper investigates the potential of enhancing Neural Radiance Fields (NeRF) with semantics to expand their applications. Although NeRF has been proven useful in real-world applications like VR and digital creation, the lack of semantics hinders interaction with objects in complex scenes. We propose to imitate the backbone feature of off-the-shelf perception models to achieve zero-shot semantic segmentation with NeRF. Our framework reformulates the segmentation process by directly rendering semantic features and only applying the decoder from perception models. This eliminates the need for expensive backbones and benefits 3D consistency. Furthermore, we can project the learned semantics onto extracted mesh surfaces for real-time interaction. With the state-of-the-art Segment Anything Model (SAM), our framework accelerates segmentation by 16 times with comparable mask quality. The experimental results demonstrate the efficacy and computational advantages of our approach. Project page: \\url{https://me.kiui.moe/san/}.",
      "authors": [
        "Xiaokang Chen",
        "Jiaxiang Tang",
        "Diwen Wan",
        "Jingbo Wang",
        "Gang Zeng"
      ],
      "published": "2023-05-25T16:44:51Z",
      "updated": "2023-05-25T16:44:51Z",
      "categories": [
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2305.16233v1",
      "landing_url": "https://arxiv.org/abs/2305.16233v1",
      "doi": "https://doi.org/10.48550/arXiv.2305.16233"
    },
    "queries": [
      {
        "anchor": "synchronous dialogue",
        "search_term": "real-time interaction"
      }
    ]
  },
  {
    "arxiv_id": "2305.17852",
    "anchor": "spoken language models",
    "search_term": "low latency",
    "search_record": {
      "id": "http://arxiv.org/abs/2305.17852v1",
      "title": "Hierarchical Neural Memory Network for Low Latency Event Processing",
      "summary": "This paper proposes a low latency neural network architecture for event-based dense prediction tasks. Conventional architectures encode entire scene contents at a fixed rate regardless of their temporal characteristics. Instead, the proposed network encodes contents at a proper temporal scale depending on its movement speed. We achieve this by constructing temporal hierarchy using stacked latent memories that operate at different rates. Given low latency event steams, the multi-level memories gradually extract dynamic to static scene contents by propagating information from the fast to the slow memory modules. The architecture not only reduces the redundancy of conventional architectures but also exploits long-term dependencies. Furthermore, an attention-based event representation efficiently encodes sparse event streams into the memory cells. We conduct extensive evaluations on three event-based dense prediction tasks, where the proposed approach outperforms the existing methods on accuracy and latency, while demonstrating effective event and image fusion capabilities. The code is available at https://hamarh.github.io/hmnet/",
      "published": "2023-05-29T02:29:16Z"
    },
    "metadata": {
      "arxiv_id": "2305.17852",
      "title": "Hierarchical Neural Memory Network for Low Latency Event Processing",
      "summary": "This paper proposes a low latency neural network architecture for event-based dense prediction tasks. Conventional architectures encode entire scene contents at a fixed rate regardless of their temporal characteristics. Instead, the proposed network encodes contents at a proper temporal scale depending on its movement speed. We achieve this by constructing temporal hierarchy using stacked latent memories that operate at different rates. Given low latency event steams, the multi-level memories gradually extract dynamic to static scene contents by propagating information from the fast to the slow memory modules. The architecture not only reduces the redundancy of conventional architectures but also exploits long-term dependencies. Furthermore, an attention-based event representation efficiently encodes sparse event streams into the memory cells. We conduct extensive evaluations on three event-based dense prediction tasks, where the proposed approach outperforms the existing methods on accuracy and latency, while demonstrating effective event and image fusion capabilities. The code is available at https://hamarh.github.io/hmnet/",
      "authors": [
        "Ryuhei Hamaguchi",
        "Yasutaka Furukawa",
        "Masaki Onishi",
        "Ken Sakurada"
      ],
      "published": "2023-05-29T02:29:16Z",
      "updated": "2023-05-29T02:29:16Z",
      "categories": [
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2305.17852v1",
      "landing_url": "https://arxiv.org/abs/2305.17852v1",
      "doi": "https://doi.org/10.48550/arXiv.2305.17852"
    },
    "queries": [
      {
        "anchor": "spoken language models",
        "search_term": "low latency"
      },
      {
        "anchor": "synchronous dialogue",
        "search_term": "low latency"
      },
      {
        "anchor": "turn-taking",
        "search_term": "low latency"
      }
    ]
  },
  {
    "arxiv_id": "2305.17971",
    "anchor": "turn-taking",
    "search_term": "true full-duplex",
    "search_record": {
      "id": "http://arxiv.org/abs/2305.17971v1",
      "title": "Automatic Evaluation of Turn-taking Cues in Conversational Speech Synthesis",
      "summary": "Turn-taking is a fundamental aspect of human communication where speakers convey their intention to either hold, or yield, their turn through prosodic cues. Using the recently proposed Voice Activity Projection model, we propose an automatic evaluation approach to measure these aspects for conversational speech synthesis. We investigate the ability of three commercial, and two open-source, Text-To-Speech (TTS) systems ability to generate turn-taking cues over simulated turns. By varying the stimuli, or controlling the prosody, we analyze the models performances. We show that while commercial TTS largely provide appropriate cues, they often produce ambiguous signals, and that further improvements are possible. TTS, trained on read or spontaneous speech, produce strong turn-hold but weak turn-yield cues. We argue that this approach, that focus on functional aspects of interaction, provides a useful addition to other important speech metrics, such as intelligibility and naturalness.",
      "published": "2023-05-29T09:29:11Z"
    },
    "metadata": {
      "arxiv_id": "2305.17971",
      "title": "Automatic Evaluation of Turn-taking Cues in Conversational Speech Synthesis",
      "summary": "Turn-taking is a fundamental aspect of human communication where speakers convey their intention to either hold, or yield, their turn through prosodic cues. Using the recently proposed Voice Activity Projection model, we propose an automatic evaluation approach to measure these aspects for conversational speech synthesis. We investigate the ability of three commercial, and two open-source, Text-To-Speech (TTS) systems ability to generate turn-taking cues over simulated turns. By varying the stimuli, or controlling the prosody, we analyze the models performances. We show that while commercial TTS largely provide appropriate cues, they often produce ambiguous signals, and that further improvements are possible. TTS, trained on read or spontaneous speech, produce strong turn-hold but weak turn-yield cues. We argue that this approach, that focus on functional aspects of interaction, provides a useful addition to other important speech metrics, such as intelligibility and naturalness.",
      "authors": [
        "Erik Ekstedt",
        "Siyang Wang",
        "Éva Székely",
        "Joakim Gustafson",
        "Gabriel Skantze"
      ],
      "published": "2023-05-29T09:29:11Z",
      "updated": "2023-05-29T09:29:11Z",
      "categories": [
        "eess.AS",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2305.17971v1",
      "landing_url": "https://arxiv.org/abs/2305.17971v1",
      "doi": "https://doi.org/10.48550/arXiv.2305.17971"
    },
    "queries": [
      {
        "anchor": "turn-taking",
        "search_term": "true full-duplex"
      },
      {
        "anchor": "turn-taking",
        "search_term": "pseudo full-duplex"
      },
      {
        "anchor": "turn-taking",
        "search_term": "half duplex"
      },
      {
        "anchor": "turn-taking",
        "search_term": "engineered synchronization"
      },
      {
        "anchor": "turn-taking",
        "search_term": "learned synchronization"
      },
      {
        "anchor": "turn-taking",
        "search_term": "modular architecture"
      },
      {
        "anchor": "turn-taking",
        "search_term": "streaming architecture"
      },
      {
        "anchor": "turn-taking",
        "search_term": "real-time interaction"
      },
      {
        "anchor": "turn-taking",
        "search_term": "low latency"
      },
      {
        "anchor": "turn-taking",
        "search_term": "synchronous data"
      },
      {
        "anchor": "turn-taking",
        "search_term": "data scarcity"
      },
      {
        "anchor": "turn-taking",
        "search_term": "multichannel dialogue"
      },
      {
        "anchor": "turn-taking",
        "search_term": "temporal dynamics"
      },
      {
        "anchor": "turn-taking",
        "search_term": "behavioral arbitration"
      },
      {
        "anchor": "turn-taking",
        "search_term": "semantic coherence"
      },
      {
        "anchor": "turn-taking",
        "search_term": "neural codec"
      },
      {
        "anchor": "turn-taking",
        "search_term": "discrete tokens"
      },
      {
        "anchor": "turn-taking",
        "search_term": "audio tokens"
      },
      {
        "anchor": "turn-taking",
        "search_term": "floor transfer offset"
      },
      {
        "anchor": "turn-taking",
        "search_term": "stop latency"
      },
      {
        "anchor": "turn-taking",
        "search_term": "interruption response delay"
      }
    ]
  },
  {
    "arxiv_id": "2305.18957",
    "anchor": "spoken language models",
    "search_term": "true full-duplex",
    "search_record": {
      "id": "http://arxiv.org/abs/2305.18957v1",
      "title": "Wave to Syntax: Probing spoken language models for syntax",
      "summary": "Understanding which information is encoded in deep models of spoken and written language has been the focus of much research in recent years, as it is crucial for debugging and improving these architectures. Most previous work has focused on probing for speaker characteristics, acoustic and phonological information in models of spoken language, and for syntactic information in models of written language. Here we focus on the encoding of syntax in several self-supervised and visually grounded models of spoken language. We employ two complementary probing methods, combined with baselines and reference representations to quantify the degree to which syntactic structure is encoded in the activations of the target models. We show that syntax is captured most prominently in the middle layers of the networks, and more explicitly within models with more parameters.",
      "published": "2023-05-30T11:43:18Z"
    },
    "metadata": {
      "arxiv_id": "2305.18957",
      "title": "Wave to Syntax: Probing spoken language models for syntax",
      "summary": "Understanding which information is encoded in deep models of spoken and written language has been the focus of much research in recent years, as it is crucial for debugging and improving these architectures. Most previous work has focused on probing for speaker characteristics, acoustic and phonological information in models of spoken language, and for syntactic information in models of written language. Here we focus on the encoding of syntax in several self-supervised and visually grounded models of spoken language. We employ two complementary probing methods, combined with baselines and reference representations to quantify the degree to which syntactic structure is encoded in the activations of the target models. We show that syntax is captured most prominently in the middle layers of the networks, and more explicitly within models with more parameters.",
      "authors": [
        "Gaofei Shen",
        "Afra Alishahi",
        "Arianna Bisazza",
        "Grzegorz Chrupała"
      ],
      "published": "2023-05-30T11:43:18Z",
      "updated": "2023-05-30T11:43:18Z",
      "categories": [
        "cs.CL"
      ],
      "pdf_url": "https://arxiv.org/pdf/2305.18957v1",
      "landing_url": "https://arxiv.org/abs/2305.18957v1",
      "doi": "https://doi.org/10.21437/Interspeech.2023-679"
    },
    "queries": [
      {
        "anchor": "spoken language models",
        "search_term": "true full-duplex"
      },
      {
        "anchor": "spoken language models",
        "search_term": "pseudo full-duplex"
      },
      {
        "anchor": "spoken language models",
        "search_term": "engineered synchronization"
      },
      {
        "anchor": "spoken language models",
        "search_term": "learned synchronization"
      },
      {
        "anchor": "spoken language models",
        "search_term": "modular architecture"
      },
      {
        "anchor": "spoken language models",
        "search_term": "streaming architecture"
      },
      {
        "anchor": "spoken language models",
        "search_term": "real-time interaction"
      },
      {
        "anchor": "spoken language models",
        "search_term": "synchronous data"
      },
      {
        "anchor": "spoken language models",
        "search_term": "data scarcity"
      },
      {
        "anchor": "spoken language models",
        "search_term": "multichannel dialogue"
      },
      {
        "anchor": "spoken language models",
        "search_term": "behavioral arbitration"
      },
      {
        "anchor": "spoken language models",
        "search_term": "semantic coherence"
      },
      {
        "anchor": "spoken language models",
        "search_term": "neural codec"
      },
      {
        "anchor": "spoken language models",
        "search_term": "discrete tokens"
      },
      {
        "anchor": "spoken language models",
        "search_term": "audio tokens"
      },
      {
        "anchor": "spoken language models",
        "search_term": "floor transfer offset"
      },
      {
        "anchor": "spoken language models",
        "search_term": "stop latency"
      },
      {
        "anchor": "spoken language models",
        "search_term": "interruption response delay"
      }
    ]
  },
  {
    "arxiv_id": "2306.00697",
    "anchor": "spoken language models",
    "search_term": "true full-duplex",
    "search_record": {
      "id": "http://arxiv.org/abs/2306.00697v1",
      "title": "How Generative Spoken Language Modeling Encodes Noisy Speech: Investigation from Phonetics to Syntactics",
      "summary": "We examine the speech modeling potential of generative spoken language modeling (GSLM), which involves using learned symbols derived from data rather than phonemes for speech analysis and synthesis. Since GSLM facilitates textless spoken language processing, exploring its effectiveness is critical for paving the way for novel paradigms in spoken-language processing. This paper presents the findings of GSLM's encoding and decoding effectiveness at the spoken-language and speech levels. Through speech resynthesis experiments, we revealed that resynthesis errors occur at the levels ranging from phonology to syntactics and GSLM frequently resynthesizes natural but content-altered speech.",
      "published": "2023-06-01T14:07:19Z"
    },
    "metadata": {
      "arxiv_id": "2306.00697",
      "title": "How Generative Spoken Language Modeling Encodes Noisy Speech: Investigation from Phonetics to Syntactics",
      "summary": "We examine the speech modeling potential of generative spoken language modeling (GSLM), which involves using learned symbols derived from data rather than phonemes for speech analysis and synthesis. Since GSLM facilitates textless spoken language processing, exploring its effectiveness is critical for paving the way for novel paradigms in spoken-language processing. This paper presents the findings of GSLM's encoding and decoding effectiveness at the spoken-language and speech levels. Through speech resynthesis experiments, we revealed that resynthesis errors occur at the levels ranging from phonology to syntactics and GSLM frequently resynthesizes natural but content-altered speech.",
      "authors": [
        "Joonyong Park",
        "Shinnosuke Takamichi",
        "Tomohiko Nakamura",
        "Kentaro Seki",
        "Detai Xin",
        "Hiroshi Saruwatari"
      ],
      "published": "2023-06-01T14:07:19Z",
      "updated": "2023-06-01T14:07:19Z",
      "categories": [
        "cs.CL",
        "cs.AI",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2306.00697v1",
      "landing_url": "https://arxiv.org/abs/2306.00697v1",
      "doi": "https://doi.org/10.48550/arXiv.2306.00697"
    },
    "queries": [
      {
        "anchor": "spoken language models",
        "search_term": "true full-duplex"
      },
      {
        "anchor": "spoken language models",
        "search_term": "pseudo full-duplex"
      },
      {
        "anchor": "spoken language models",
        "search_term": "half duplex"
      },
      {
        "anchor": "spoken language models",
        "search_term": "engineered synchronization"
      },
      {
        "anchor": "spoken language models",
        "search_term": "learned synchronization"
      },
      {
        "anchor": "spoken language models",
        "search_term": "modular architecture"
      },
      {
        "anchor": "spoken language models",
        "search_term": "streaming architecture"
      },
      {
        "anchor": "spoken language models",
        "search_term": "real-time interaction"
      },
      {
        "anchor": "spoken language models",
        "search_term": "low latency"
      },
      {
        "anchor": "spoken language models",
        "search_term": "synchronous data"
      },
      {
        "anchor": "spoken language models",
        "search_term": "data scarcity"
      },
      {
        "anchor": "spoken language models",
        "search_term": "multichannel dialogue"
      },
      {
        "anchor": "spoken language models",
        "search_term": "temporal dynamics"
      },
      {
        "anchor": "spoken language models",
        "search_term": "behavioral arbitration"
      },
      {
        "anchor": "spoken language models",
        "search_term": "semantic coherence"
      },
      {
        "anchor": "spoken language models",
        "search_term": "neural codec"
      },
      {
        "anchor": "spoken language models",
        "search_term": "discrete tokens"
      },
      {
        "anchor": "spoken language models",
        "search_term": "audio tokens"
      },
      {
        "anchor": "spoken language models",
        "search_term": "floor transfer offset"
      },
      {
        "anchor": "spoken language models",
        "search_term": "stop latency"
      },
      {
        "anchor": "spoken language models",
        "search_term": "interruption response delay"
      }
    ]
  },
  {
    "arxiv_id": "2306.01506",
    "anchor": "spoken language models",
    "search_term": "true full-duplex",
    "search_record": {
      "id": "http://arxiv.org/abs/2306.01506v2",
      "title": "BabySLM: language-acquisition-friendly benchmark of self-supervised spoken language models",
      "summary": "Self-supervised techniques for learning speech representations have been shown to develop linguistic competence from exposure to speech without the need for human labels. In order to fully realize the potential of these approaches and further our understanding of how infants learn language, simulations must closely emulate real-life situations by training on developmentally plausible corpora and benchmarking against appropriate test sets. To this end, we propose a language-acquisition-friendly benchmark to probe spoken language models at the lexical and syntactic levels, both of which are compatible with the vocabulary typical of children's language experiences. This paper introduces the benchmark and summarizes a range of experiments showing its usefulness. In addition, we highlight two exciting challenges that need to be addressed for further progress: bridging the gap between text and speech and between clean speech and in-the-wild speech.",
      "published": "2023-06-02T12:54:38Z"
    },
    "metadata": {
      "arxiv_id": "2306.01506",
      "title": "BabySLM: language-acquisition-friendly benchmark of self-supervised spoken language models",
      "summary": "Self-supervised techniques for learning speech representations have been shown to develop linguistic competence from exposure to speech without the need for human labels. In order to fully realize the potential of these approaches and further our understanding of how infants learn language, simulations must closely emulate real-life situations by training on developmentally plausible corpora and benchmarking against appropriate test sets. To this end, we propose a language-acquisition-friendly benchmark to probe spoken language models at the lexical and syntactic levels, both of which are compatible with the vocabulary typical of children's language experiences. This paper introduces the benchmark and summarizes a range of experiments showing its usefulness. In addition, we highlight two exciting challenges that need to be addressed for further progress: bridging the gap between text and speech and between clean speech and in-the-wild speech.",
      "authors": [
        "Marvin Lavechin",
        "Yaya Sy",
        "Hadrien Titeux",
        "María Andrea Cruz Blandón",
        "Okko Räsänen",
        "Hervé Bredin",
        "Emmanuel Dupoux",
        "Alejandrina Cristia"
      ],
      "published": "2023-06-02T12:54:38Z",
      "updated": "2023-06-08T12:22:30Z",
      "categories": [
        "cs.CL",
        "eess.AS",
        "stat.ML"
      ],
      "pdf_url": "https://arxiv.org/pdf/2306.01506v2",
      "landing_url": "https://arxiv.org/abs/2306.01506v2",
      "doi": "https://doi.org/10.21437/Interspeech.2023-978"
    },
    "queries": [
      {
        "anchor": "spoken language models",
        "search_term": "true full-duplex"
      },
      {
        "anchor": "spoken language models",
        "search_term": "pseudo full-duplex"
      },
      {
        "anchor": "spoken language models",
        "search_term": "half duplex"
      },
      {
        "anchor": "spoken language models",
        "search_term": "engineered synchronization"
      },
      {
        "anchor": "spoken language models",
        "search_term": "learned synchronization"
      },
      {
        "anchor": "spoken language models",
        "search_term": "modular architecture"
      },
      {
        "anchor": "spoken language models",
        "search_term": "streaming architecture"
      },
      {
        "anchor": "spoken language models",
        "search_term": "real-time interaction"
      },
      {
        "anchor": "spoken language models",
        "search_term": "low latency"
      },
      {
        "anchor": "spoken language models",
        "search_term": "synchronous data"
      },
      {
        "anchor": "spoken language models",
        "search_term": "data scarcity"
      },
      {
        "anchor": "spoken language models",
        "search_term": "multichannel dialogue"
      },
      {
        "anchor": "spoken language models",
        "search_term": "temporal dynamics"
      },
      {
        "anchor": "spoken language models",
        "search_term": "behavioral arbitration"
      },
      {
        "anchor": "spoken language models",
        "search_term": "semantic coherence"
      },
      {
        "anchor": "spoken language models",
        "search_term": "neural codec"
      },
      {
        "anchor": "spoken language models",
        "search_term": "discrete tokens"
      },
      {
        "anchor": "spoken language models",
        "search_term": "audio tokens"
      },
      {
        "anchor": "spoken language models",
        "search_term": "floor transfer offset"
      },
      {
        "anchor": "spoken language models",
        "search_term": "stop latency"
      },
      {
        "anchor": "spoken language models",
        "search_term": "interruption response delay"
      }
    ]
  },
  {
    "arxiv_id": "2306.02629",
    "anchor": "synchronous dialogue",
    "search_term": "low latency",
    "search_record": {
      "id": "http://arxiv.org/abs/2306.02629v1",
      "title": "Low-Latency SCL Bit-Flipping Decoding of Polar Codes",
      "summary": "Bit flipping can be used as a postprocessing technique to further improve the performance for successive cancellation list (SCL) decoding of polar codes. However, the number of bit-flipping trials could increase the decoding latency significantly, which is not welcome in practice. In this paper, we propose a low latency SCL bit flipping decoding scheme, which is restricted to just single round of post-processing. The use of multiple votes for a more accurate estimation of path survival probability is proposed to locate the first error event of SCL decoding. Simulations show the sound improvement compared to the existing SCL bit-flipping decoding methods.",
      "published": "2023-06-05T06:56:20Z"
    },
    "metadata": {
      "arxiv_id": "2306.02629",
      "title": "Low-Latency SCL Bit-Flipping Decoding of Polar Codes",
      "summary": "Bit flipping can be used as a postprocessing technique to further improve the performance for successive cancellation list (SCL) decoding of polar codes. However, the number of bit-flipping trials could increase the decoding latency significantly, which is not welcome in practice. In this paper, we propose a low latency SCL bit flipping decoding scheme, which is restricted to just single round of post-processing. The use of multiple votes for a more accurate estimation of path survival probability is proposed to locate the first error event of SCL decoding. Simulations show the sound improvement compared to the existing SCL bit-flipping decoding methods.",
      "authors": [
        "Wei Zhang",
        "Xiaofu Wu"
      ],
      "published": "2023-06-05T06:56:20Z",
      "updated": "2023-06-05T06:56:20Z",
      "categories": [
        "cs.IT"
      ],
      "pdf_url": "https://arxiv.org/pdf/2306.02629v1",
      "landing_url": "https://arxiv.org/abs/2306.02629v1",
      "doi": "https://doi.org/10.48550/arXiv.2306.02629"
    },
    "queries": [
      {
        "anchor": "synchronous dialogue",
        "search_term": "low latency"
      }
    ]
  },
  {
    "arxiv_id": "2306.04640",
    "anchor": "synchronous dialogue",
    "search_term": "modular architecture",
    "search_record": {
      "id": "http://arxiv.org/abs/2306.04640v2",
      "title": "ModuleFormer: Modularity Emerges from Mixture-of-Experts",
      "summary": "Large Language Models (LLMs) have achieved remarkable results. However, existing models are expensive to train and deploy, and it is also difficult to expand their knowledge beyond pre-training data without forgetting previous knowledge. This paper proposes a new neural network architecture, ModuleFormer, that leverages modularity to improve the efficiency and flexibility of large language models. ModuleFormer is based on the Sparse Mixture of Experts (SMoE). Unlike the previous SMoE-based modular language model, which requires domain-labeled data to learn domain-specific experts, ModuleFormer can induce modularity from uncurated data with its new load balancing and concentration losses. ModuleFormer is a modular architecture that includes two different types of modules: new stick-breaking attention heads and feedforward experts. Different modules are sparsely activated conditions on the input token during training and inference. In our experiment, we found that the modular architecture enables three important abilities for large pre-trained language models: 1) Efficiency, since ModuleFormer only activates a subset of its modules for each input token, thus it could achieve the same performance as dense LLMs with more than two times throughput; 2) Extendability, ModuleFormer is more immune to catastrophic forgetting than dense LLMs and can be easily extended with new modules to learn new knowledge that is not included in the training data; 3) Specialisation, finetuning ModuleFormer could specialize a subset of modules to the finetuning task and the task-unrelated modules could be easily pruned for a lightweight deployment.",
      "published": "2023-06-07T17:59:57Z"
    },
    "metadata": {
      "arxiv_id": "2306.04640",
      "title": "ModuleFormer: Modularity Emerges from Mixture-of-Experts",
      "summary": "Large Language Models (LLMs) have achieved remarkable results. However, existing models are expensive to train and deploy, and it is also difficult to expand their knowledge beyond pre-training data without forgetting previous knowledge. This paper proposes a new neural network architecture, ModuleFormer, that leverages modularity to improve the efficiency and flexibility of large language models. ModuleFormer is based on the Sparse Mixture of Experts (SMoE). Unlike the previous SMoE-based modular language model, which requires domain-labeled data to learn domain-specific experts, ModuleFormer can induce modularity from uncurated data with its new load balancing and concentration losses. ModuleFormer is a modular architecture that includes two different types of modules: new stick-breaking attention heads and feedforward experts. Different modules are sparsely activated conditions on the input token during training and inference. In our experiment, we found that the modular architecture enables three important abilities for large pre-trained language models: 1) Efficiency, since ModuleFormer only activates a subset of its modules for each input token, thus it could achieve the same performance as dense LLMs with more than two times throughput; 2) Extendability, ModuleFormer is more immune to catastrophic forgetting than dense LLMs and can be easily extended with new modules to learn new knowledge that is not included in the training data; 3) Specialisation, finetuning ModuleFormer could specialize a subset of modules to the finetuning task and the task-unrelated modules could be easily pruned for a lightweight deployment.",
      "authors": [
        "Yikang Shen",
        "Zheyu Zhang",
        "Tianyou Cao",
        "Shawn Tan",
        "Zhenfang Chen",
        "Chuang Gan"
      ],
      "published": "2023-06-07T17:59:57Z",
      "updated": "2023-09-11T19:31:26Z",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2306.04640v2",
      "landing_url": "https://arxiv.org/abs/2306.04640v2",
      "doi": "https://doi.org/10.48550/arXiv.2306.04640"
    },
    "queries": [
      {
        "anchor": "synchronous dialogue",
        "search_term": "modular architecture"
      }
    ]
  },
  {
    "arxiv_id": "2306.08368",
    "anchor": "synchronous dialogue",
    "search_term": "semantic coherence",
    "search_record": {
      "id": "http://arxiv.org/abs/2306.08368v1",
      "title": "T5-SR: A Unified Seq-to-Seq Decoding Strategy for Semantic Parsing",
      "summary": "Translating natural language queries into SQLs in a seq2seq manner has attracted much attention recently. However, compared with abstract-syntactic-tree-based SQL generation, seq2seq semantic parsers face much more challenges, including poor quality on schematical information prediction and poor semantic coherence between natural language queries and SQLs. This paper analyses the above difficulties and proposes a seq2seq-oriented decoding strategy called SR, which includes a new intermediate representation SSQL and a reranking method with score re-estimator to solve the above obstacles respectively. Experimental results demonstrate the effectiveness of our proposed techniques and T5-SR-3b achieves new state-of-the-art results on the Spider dataset.",
      "published": "2023-06-14T08:57:13Z"
    },
    "metadata": {
      "arxiv_id": "2306.08368",
      "title": "T5-SR: A Unified Seq-to-Seq Decoding Strategy for Semantic Parsing",
      "summary": "Translating natural language queries into SQLs in a seq2seq manner has attracted much attention recently. However, compared with abstract-syntactic-tree-based SQL generation, seq2seq semantic parsers face much more challenges, including poor quality on schematical information prediction and poor semantic coherence between natural language queries and SQLs. This paper analyses the above difficulties and proposes a seq2seq-oriented decoding strategy called SR, which includes a new intermediate representation SSQL and a reranking method with score re-estimator to solve the above obstacles respectively. Experimental results demonstrate the effectiveness of our proposed techniques and T5-SR-3b achieves new state-of-the-art results on the Spider dataset.",
      "authors": [
        "Yuntao Li",
        "Zhenpeng Su",
        "Yutian Li",
        "Hanchu Zhang",
        "Sirui Wang",
        "Wei Wu",
        "Yan Zhang"
      ],
      "published": "2023-06-14T08:57:13Z",
      "updated": "2023-06-14T08:57:13Z",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2306.08368v1",
      "landing_url": "https://arxiv.org/abs/2306.08368v1",
      "doi": "https://doi.org/10.48550/arXiv.2306.08368"
    },
    "queries": [
      {
        "anchor": "synchronous dialogue",
        "search_term": "semantic coherence"
      }
    ]
  },
  {
    "arxiv_id": "2306.08879",
    "anchor": "synchronous dialogue",
    "search_term": "streaming architecture",
    "search_record": {
      "id": "http://arxiv.org/abs/2306.08879v2",
      "title": "Motion Perceiver: Real-Time Occupancy Forecasting for Embedded Systems",
      "summary": "This work introduces a novel and adaptable architecture designed for real-time occupancy forecasting that outperforms existing state-of-the-art models on the Waymo Open Motion Dataset in Soft IOU. The proposed model uses recursive latent state estimation with learned transformer-based functions to effectively update and evolve the state. This enables highly efficient real-time inference on embedded systems, as profiled on an Nvidia Xavier AGX. Our model, MotionPerceiver, achieves this by encoding a scene into a latent state that evolves in time through self-attention mechanisms. Additionally, it incorporates relevant scene observations, such as traffic signals, road topology and agent detections, through cross-attention mechanisms. This forms an efficient data-streaming architecture, that contrasts with the expensive, fixed-sequence input common in existing models. The architecture also offers the distinct advantage of generating occupancy predictions through localized querying based on a point-of-interest, as opposed to generating fixed-size occupancy images that render potentially irrelevant regions.",
      "published": "2023-06-15T06:26:56Z"
    },
    "metadata": {
      "arxiv_id": "2306.08879",
      "title": "Motion Perceiver: Real-Time Occupancy Forecasting for Embedded Systems",
      "summary": "This work introduces a novel and adaptable architecture designed for real-time occupancy forecasting that outperforms existing state-of-the-art models on the Waymo Open Motion Dataset in Soft IOU. The proposed model uses recursive latent state estimation with learned transformer-based functions to effectively update and evolve the state. This enables highly efficient real-time inference on embedded systems, as profiled on an Nvidia Xavier AGX. Our model, MotionPerceiver, achieves this by encoding a scene into a latent state that evolves in time through self-attention mechanisms. Additionally, it incorporates relevant scene observations, such as traffic signals, road topology and agent detections, through cross-attention mechanisms. This forms an efficient data-streaming architecture, that contrasts with the expensive, fixed-sequence input common in existing models. The architecture also offers the distinct advantage of generating occupancy predictions through localized querying based on a point-of-interest, as opposed to generating fixed-size occupancy images that render potentially irrelevant regions.",
      "authors": [
        "Bryce Ferenczi",
        "Michael Burke",
        "Tom Drummond"
      ],
      "published": "2023-06-15T06:26:56Z",
      "updated": "2024-02-02T02:09:04Z",
      "categories": [
        "cs.RO"
      ],
      "pdf_url": "https://arxiv.org/pdf/2306.08879v2",
      "landing_url": "https://arxiv.org/abs/2306.08879v2",
      "doi": "https://doi.org/10.1109/LRA.2024.3360811"
    },
    "queries": [
      {
        "anchor": "synchronous dialogue",
        "search_term": "streaming architecture"
      }
    ]
  },
  {
    "arxiv_id": "2306.09967",
    "anchor": "spoken language models",
    "search_term": "temporal dynamics",
    "search_record": {
      "id": "http://arxiv.org/abs/2306.09967v3",
      "title": "The temporal dynamics of group interactions in higher-order social networks",
      "summary": "Representing social systems as networks, starting from the interactions between individuals, sheds light on the mechanisms governing their dynamics. However, networks encode only pairwise interactions, while most social interactions occur among groups of individuals, requiring higher-order network representations. Despite the recent interest in higher-order networks, little is known about the mechanisms that govern the formation and evolution of groups, and how people move between groups. Here, we leverage empirical data on social interactions among children and university students to study their temporal dynamics at both individual and group levels, characterising how individuals navigate groups and how groups form and disaggregate. We find robust patterns across contexts and propose a dynamical model that closely reproduces empirical observations. These results represent a further step in understanding social systems, and open up research directions to study the impact of group dynamics on dynamical processes that evolve on top of them.",
      "published": "2023-06-16T16:54:18Z"
    },
    "metadata": {
      "arxiv_id": "2306.09967",
      "title": "The temporal dynamics of group interactions in higher-order social networks",
      "summary": "Representing social systems as networks, starting from the interactions between individuals, sheds light on the mechanisms governing their dynamics. However, networks encode only pairwise interactions, while most social interactions occur among groups of individuals, requiring higher-order network representations. Despite the recent interest in higher-order networks, little is known about the mechanisms that govern the formation and evolution of groups, and how people move between groups. Here, we leverage empirical data on social interactions among children and university students to study their temporal dynamics at both individual and group levels, characterising how individuals navigate groups and how groups form and disaggregate. We find robust patterns across contexts and propose a dynamical model that closely reproduces empirical observations. These results represent a further step in understanding social systems, and open up research directions to study the impact of group dynamics on dynamical processes that evolve on top of them.",
      "authors": [
        "Iacopo Iacopini",
        "Márton Karsai",
        "Alain Barrat"
      ],
      "published": "2023-06-16T16:54:18Z",
      "updated": "2024-07-09T13:46:50Z",
      "categories": [
        "physics.soc-ph",
        "cs.SI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2306.09967v3",
      "landing_url": "https://arxiv.org/abs/2306.09967v3",
      "doi": "https://doi.org/10.1038/s41467-024-50918-5"
    },
    "queries": [
      {
        "anchor": "spoken language models",
        "search_term": "temporal dynamics"
      },
      {
        "anchor": "synchronous dialogue",
        "search_term": "temporal dynamics"
      },
      {
        "anchor": "turn-taking",
        "search_term": "temporal dynamics"
      }
    ]
  },
  {
    "arxiv_id": "2306.11330",
    "anchor": "spoken language models",
    "search_term": "modular architecture",
    "search_record": {
      "id": "http://arxiv.org/abs/2306.11330v2",
      "title": "Low Latency Edge Classification GNN for Particle Trajectory Tracking on FPGAs",
      "summary": "In-time particle trajectory reconstruction in the Large Hadron Collider is challenging due to the high collision rate and numerous particle hits. Using GNN (Graph Neural Network) on FPGA has enabled superior accuracy with flexible trajectory classification. However, existing GNN architectures have inefficient resource usage and insufficient parallelism for edge classification. This paper introduces a resource-efficient GNN architecture on FPGAs for low latency particle tracking. The modular architecture facilitates design scalability to support large graphs. Leveraging the geometric properties of hit detectors further reduces graph complexity and resource usage. Our results on Xilinx UltraScale+ VU9P demonstrate 1625x and 1574x performance improvement over CPU and GPU respectively.",
      "published": "2023-06-20T06:57:24Z"
    },
    "metadata": {
      "arxiv_id": "2306.11330",
      "title": "Low Latency Edge Classification GNN for Particle Trajectory Tracking on FPGAs",
      "summary": "In-time particle trajectory reconstruction in the Large Hadron Collider is challenging due to the high collision rate and numerous particle hits. Using GNN (Graph Neural Network) on FPGA has enabled superior accuracy with flexible trajectory classification. However, existing GNN architectures have inefficient resource usage and insufficient parallelism for edge classification. This paper introduces a resource-efficient GNN architecture on FPGAs for low latency particle tracking. The modular architecture facilitates design scalability to support large graphs. Leveraging the geometric properties of hit detectors further reduces graph complexity and resource usage. Our results on Xilinx UltraScale+ VU9P demonstrate 1625x and 1574x performance improvement over CPU and GPU respectively.",
      "authors": [
        "Shi-Yu Huang",
        "Yun-Chen Yang",
        "Yu-Ru Su",
        "Bo-Cheng Lai",
        "Javier Duarte",
        "Scott Hauck",
        "Shih-Chieh Hsu",
        "Jin-Xuan Hu",
        "Mark S. Neubauer"
      ],
      "published": "2023-06-20T06:57:24Z",
      "updated": "2023-06-27T16:21:32Z",
      "categories": [
        "cs.AR",
        "cs.LG",
        "hep-ex"
      ],
      "pdf_url": "https://arxiv.org/pdf/2306.11330v2",
      "landing_url": "https://arxiv.org/abs/2306.11330v2",
      "doi": "https://doi.org/10.48550/arXiv.2306.11330"
    },
    "queries": [
      {
        "anchor": "spoken language models",
        "search_term": "modular architecture"
      },
      {
        "anchor": "synchronous dialogue",
        "search_term": "modular architecture"
      },
      {
        "anchor": "synchronous dialogue",
        "search_term": "low latency"
      },
      {
        "anchor": "turn-taking",
        "search_term": "modular architecture"
      }
    ]
  },
  {
    "arxiv_id": "2306.11504",
    "anchor": "synchronous dialogue",
    "search_term": "audio tokens",
    "search_record": {
      "id": "http://arxiv.org/abs/2306.11504v1",
      "title": "Align, Adapt and Inject: Sound-guided Unified Image Generation",
      "summary": "Text-guided image generation has witnessed unprecedented progress due to the development of diffusion models. Beyond text and image, sound is a vital element within the sphere of human perception, offering vivid representations and naturally coinciding with corresponding scenes. Taking advantage of sound therefore presents a promising avenue for exploration within image generation research. However, the relationship between audio and image supervision remains significantly underdeveloped, and the scarcity of related, high-quality datasets brings further obstacles. In this paper, we propose a unified framework 'Align, Adapt, and Inject' (AAI) for sound-guided image generation, editing, and stylization. In particular, our method adapts input sound into a sound token, like an ordinary word, which can plug and play with existing powerful diffusion-based Text-to-Image (T2I) models. Specifically, we first train a multi-modal encoder to align audio representation with the pre-trained textual manifold and visual manifold, respectively. Then, we propose the audio adapter to adapt audio representation into an audio token enriched with specific semantics, which can be injected into a frozen T2I model flexibly. In this way, we are able to extract the dynamic information of varied sounds, while utilizing the formidable capability of existing T2I models to facilitate sound-guided image generation, editing, and stylization in a convenient and cost-effective manner. The experiment results confirm that our proposed AAI outperforms other text and sound-guided state-of-the-art methods. And our aligned multi-modal encoder is also competitive with other approaches in the audio-visual retrieval and audio-text retrieval tasks.",
      "published": "2023-06-20T12:50:49Z"
    },
    "metadata": {
      "arxiv_id": "2306.11504",
      "title": "Align, Adapt and Inject: Sound-guided Unified Image Generation",
      "summary": "Text-guided image generation has witnessed unprecedented progress due to the development of diffusion models. Beyond text and image, sound is a vital element within the sphere of human perception, offering vivid representations and naturally coinciding with corresponding scenes. Taking advantage of sound therefore presents a promising avenue for exploration within image generation research. However, the relationship between audio and image supervision remains significantly underdeveloped, and the scarcity of related, high-quality datasets brings further obstacles. In this paper, we propose a unified framework 'Align, Adapt, and Inject' (AAI) for sound-guided image generation, editing, and stylization. In particular, our method adapts input sound into a sound token, like an ordinary word, which can plug and play with existing powerful diffusion-based Text-to-Image (T2I) models. Specifically, we first train a multi-modal encoder to align audio representation with the pre-trained textual manifold and visual manifold, respectively. Then, we propose the audio adapter to adapt audio representation into an audio token enriched with specific semantics, which can be injected into a frozen T2I model flexibly. In this way, we are able to extract the dynamic information of varied sounds, while utilizing the formidable capability of existing T2I models to facilitate sound-guided image generation, editing, and stylization in a convenient and cost-effective manner. The experiment results confirm that our proposed AAI outperforms other text and sound-guided state-of-the-art methods. And our aligned multi-modal encoder is also competitive with other approaches in the audio-visual retrieval and audio-text retrieval tasks.",
      "authors": [
        "Yue Yang",
        "Kaipeng Zhang",
        "Yuying Ge",
        "Wenqi Shao",
        "Zeyue Xue",
        "Yu Qiao",
        "Ping Luo"
      ],
      "published": "2023-06-20T12:50:49Z",
      "updated": "2023-06-20T12:50:49Z",
      "categories": [
        "cs.GR",
        "cs.CV",
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2306.11504v1",
      "landing_url": "https://arxiv.org/abs/2306.11504v1",
      "doi": "https://doi.org/10.48550/arXiv.2306.11504"
    },
    "queries": [
      {
        "anchor": "synchronous dialogue",
        "search_term": "audio tokens"
      }
    ]
  },
  {
    "arxiv_id": "2306.13735",
    "anchor": "synchronous dialogue",
    "search_term": "data scarcity",
    "search_record": {
      "id": "http://arxiv.org/abs/2306.13735v1",
      "title": "Combining Public Human Activity Recognition Datasets to Mitigate Labeled Data Scarcity",
      "summary": "The use of supervised learning for Human Activity Recognition (HAR) on mobile devices leads to strong classification performances. Such an approach, however, requires large amounts of labeled data, both for the initial training of the models and for their customization on specific clients (whose data often differ greatly from the training data). This is actually impractical to obtain due to the costs, intrusiveness, and time-consuming nature of data annotation. Moreover, even with the help of a significant amount of labeled data, model deployment on heterogeneous clients faces difficulties in generalizing well on unseen data. Other domains, like Computer Vision or Natural Language Processing, have proposed the notion of pre-trained models, leveraging large corpora, to reduce the need for annotated data and better manage heterogeneity. This promising approach has not been implemented in the HAR domain so far because of the lack of public datasets of sufficient size. In this paper, we propose a novel strategy to combine publicly available datasets with the goal of learning a generalized HAR model that can be fine-tuned using a limited amount of labeled data on an unseen target domain. Our experimental evaluation, which includes experimenting with different state-of-the-art neural network architectures, shows that combining public datasets can significantly reduce the number of labeled samples required to achieve satisfactory performance on an unseen target domain.",
      "published": "2023-06-23T18:51:22Z"
    },
    "metadata": {
      "arxiv_id": "2306.13735",
      "title": "Combining Public Human Activity Recognition Datasets to Mitigate Labeled Data Scarcity",
      "summary": "The use of supervised learning for Human Activity Recognition (HAR) on mobile devices leads to strong classification performances. Such an approach, however, requires large amounts of labeled data, both for the initial training of the models and for their customization on specific clients (whose data often differ greatly from the training data). This is actually impractical to obtain due to the costs, intrusiveness, and time-consuming nature of data annotation. Moreover, even with the help of a significant amount of labeled data, model deployment on heterogeneous clients faces difficulties in generalizing well on unseen data. Other domains, like Computer Vision or Natural Language Processing, have proposed the notion of pre-trained models, leveraging large corpora, to reduce the need for annotated data and better manage heterogeneity. This promising approach has not been implemented in the HAR domain so far because of the lack of public datasets of sufficient size. In this paper, we propose a novel strategy to combine publicly available datasets with the goal of learning a generalized HAR model that can be fine-tuned using a limited amount of labeled data on an unseen target domain. Our experimental evaluation, which includes experimenting with different state-of-the-art neural network architectures, shows that combining public datasets can significantly reduce the number of labeled samples required to achieve satisfactory performance on an unseen target domain.",
      "authors": [
        "Riccardo Presotto",
        "Sannara Ek",
        "Gabriele Civitarese",
        "François Portet",
        "Philippe Lalanda",
        "Claudio Bettini"
      ],
      "published": "2023-06-23T18:51:22Z",
      "updated": "2023-06-23T18:51:22Z",
      "categories": [
        "cs.CV",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2306.13735v1",
      "landing_url": "https://arxiv.org/abs/2306.13735v1",
      "doi": "https://doi.org/10.48550/arXiv.2306.13735"
    },
    "queries": [
      {
        "anchor": "synchronous dialogue",
        "search_term": "data scarcity"
      }
    ]
  },
  {
    "arxiv_id": "2306.14757",
    "anchor": "spoken language models",
    "search_term": "low latency",
    "search_record": {
      "id": "http://arxiv.org/abs/2306.14757v1",
      "title": "BBCA-LEDGER: High Throughput Consensus meets Low Latency",
      "summary": "This paper presents BBCA-LEDGER, a Byzantine log replication technology for partially synchronous networks enabling blocks to be broadcast in parallel, such that each broadcast is finalized independently and instantaneously into an individual slot in the log. Every finalized broadcast is eventually committed to the total ordering, so that all network bandwidth has utility in disseminating blocks. Finalizing log slots in parallel achieves both high throughput and low latency. BBCA-LEDGER is composed of two principal protocols that interweave together, a low-latency/high-throughput happy path, and a high-throughput DAG-based fallback path. The happy path employs a novel primitive called BBCA, a consistent broadcast enforcing unique slot numbering. In steady state, BBCA ensures that a transaction can be committed with low latency, in just 3 network steps. Under network partitions or faults, we harness recent advances in BFT and build a fallback mechanism on a direct acyclic graph (DAG) created by BBCA broadcasts. In this manner, BBCA-LEDGER exhibits the throughput benefits of DAG-based BFT in face of gaps.",
      "published": "2023-06-26T15:11:50Z"
    },
    "metadata": {
      "arxiv_id": "2306.14757",
      "title": "BBCA-LEDGER: High Throughput Consensus meets Low Latency",
      "summary": "This paper presents BBCA-LEDGER, a Byzantine log replication technology for partially synchronous networks enabling blocks to be broadcast in parallel, such that each broadcast is finalized independently and instantaneously into an individual slot in the log. Every finalized broadcast is eventually committed to the total ordering, so that all network bandwidth has utility in disseminating blocks. Finalizing log slots in parallel achieves both high throughput and low latency. BBCA-LEDGER is composed of two principal protocols that interweave together, a low-latency/high-throughput happy path, and a high-throughput DAG-based fallback path. The happy path employs a novel primitive called BBCA, a consistent broadcast enforcing unique slot numbering. In steady state, BBCA ensures that a transaction can be committed with low latency, in just 3 network steps. Under network partitions or faults, we harness recent advances in BFT and build a fallback mechanism on a direct acyclic graph (DAG) created by BBCA broadcasts. In this manner, BBCA-LEDGER exhibits the throughput benefits of DAG-based BFT in face of gaps.",
      "authors": [
        "Chrysoula Stathakopoulou",
        "Michael Wei",
        "Maofan Yin",
        "Hongbo Zhang",
        "Dahlia Malkhi"
      ],
      "published": "2023-06-26T15:11:50Z",
      "updated": "2023-06-26T15:11:50Z",
      "categories": [
        "cs.DC"
      ],
      "pdf_url": "https://arxiv.org/pdf/2306.14757v1",
      "landing_url": "https://arxiv.org/abs/2306.14757v1",
      "doi": "https://doi.org/10.48550/arXiv.2306.14757"
    },
    "queries": [
      {
        "anchor": "spoken language models",
        "search_term": "low latency"
      },
      {
        "anchor": "synchronous dialogue",
        "search_term": "low latency"
      },
      {
        "anchor": "turn-taking",
        "search_term": "low latency"
      }
    ]
  },
  {
    "arxiv_id": "2306.15792",
    "anchor": "synchronous dialogue",
    "search_term": "modular architecture",
    "search_record": {
      "id": "http://arxiv.org/abs/2306.15792v2",
      "title": "Sidecars on the Central Lane: Impact of Network Proxies on Microservices",
      "summary": "Cloud applications are moving away from monolithic model towards loosely-coupled microservices designs. Service meshes are widely used for implementing microservices applications mainly because they provide a modular architecture for modern applications by separating operational features from application business logic. Sidecar proxies in service meshes enable this modularity by applying security, networking, and monitoring policies on the traffic to and from services. To implement these policies, sidecars often execute complex chains of logic that vary across associated applications and end up unevenly impacting the performance of the overall application. Lack of understanding of how the sidecars impact the performance of microservice-based applications stands in the way of building performant and resource-efficient applications. To this end, we bring sidecar proxies in focus and argue that we need to deeply study their impact on the system performance and resource utilization. We identify and describe challenges in characterizing sidecars, namely the need for microarchitectural metrics and comprehensive methodologies, and discuss research directions where such characterization will help in building efficient service mesh infrastructure for microservice applications.",
      "published": "2023-06-27T20:40:43Z"
    },
    "metadata": {
      "arxiv_id": "2306.15792",
      "title": "Sidecars on the Central Lane: Impact of Network Proxies on Microservices",
      "summary": "Cloud applications are moving away from monolithic model towards loosely-coupled microservices designs. Service meshes are widely used for implementing microservices applications mainly because they provide a modular architecture for modern applications by separating operational features from application business logic. Sidecar proxies in service meshes enable this modularity by applying security, networking, and monitoring policies on the traffic to and from services. To implement these policies, sidecars often execute complex chains of logic that vary across associated applications and end up unevenly impacting the performance of the overall application. Lack of understanding of how the sidecars impact the performance of microservice-based applications stands in the way of building performant and resource-efficient applications. To this end, we bring sidecar proxies in focus and argue that we need to deeply study their impact on the system performance and resource utilization. We identify and describe challenges in characterizing sidecars, namely the need for microarchitectural metrics and comprehensive methodologies, and discuss research directions where such characterization will help in building efficient service mesh infrastructure for microservice applications.",
      "authors": [
        "Prateek Sahu",
        "Lucy Zheng",
        "Marco Bueso",
        "Shijia Wei",
        "Neeraja J. Yadwadkar",
        "Mohit Tiwari"
      ],
      "published": "2023-06-27T20:40:43Z",
      "updated": "2023-10-17T23:04:09Z",
      "categories": [
        "cs.DC",
        "cs.AR",
        "cs.PF"
      ],
      "pdf_url": "https://arxiv.org/pdf/2306.15792v2",
      "landing_url": "https://arxiv.org/abs/2306.15792v2",
      "doi": "https://doi.org/10.48550/arXiv.2306.15792"
    },
    "queries": [
      {
        "anchor": "synchronous dialogue",
        "search_term": "modular architecture"
      }
    ]
  },
  {
    "arxiv_id": "2306.17181",
    "anchor": "synchronous dialogue",
    "search_term": "discrete tokens",
    "search_record": {
      "id": "http://arxiv.org/abs/2306.17181v4",
      "title": "Unsupervised Text Embedding Space Generation Using Generative Adversarial Networks for Text Synthesis",
      "summary": "Generative Adversarial Networks (GAN) is a model for data synthesis, which creates plausible data through the competition of generator and discriminator. Although GAN application to image synthesis is extensively studied, it has inherent limitations to natural language generation. Because natural language is composed of discrete tokens, a generator has difficulty updating its gradient through backpropagation; therefore, most text-GAN studies generate sentences starting with a random token based on a reward system. Thus, the generators of previous studies are pre-trained in an autoregressive way before adversarial training, causing data memorization that synthesized sentences reproduce the training data. In this paper, we synthesize sentences using a framework similar to the original GAN. More specifically, we propose Text Embedding Space Generative Adversarial Networks (TESGAN) which generate continuous text embedding spaces instead of discrete tokens to solve the gradient backpropagation problem. Furthermore, TESGAN conducts unsupervised learning which does not directly refer to the text of the training data to overcome the data memorization issue. By adopting this novel method, TESGAN can synthesize new sentences, showing the potential of unsupervised learning for text synthesis. We expect to see extended research combining Large Language Models with a new perspective of viewing text as an continuous space.",
      "published": "2023-06-19T10:22:12Z"
    },
    "metadata": {
      "arxiv_id": "2306.17181",
      "title": "Unsupervised Text Embedding Space Generation Using Generative Adversarial Networks for Text Synthesis",
      "summary": "Generative Adversarial Networks (GAN) is a model for data synthesis, which creates plausible data through the competition of generator and discriminator. Although GAN application to image synthesis is extensively studied, it has inherent limitations to natural language generation. Because natural language is composed of discrete tokens, a generator has difficulty updating its gradient through backpropagation; therefore, most text-GAN studies generate sentences starting with a random token based on a reward system. Thus, the generators of previous studies are pre-trained in an autoregressive way before adversarial training, causing data memorization that synthesized sentences reproduce the training data. In this paper, we synthesize sentences using a framework similar to the original GAN. More specifically, we propose Text Embedding Space Generative Adversarial Networks (TESGAN) which generate continuous text embedding spaces instead of discrete tokens to solve the gradient backpropagation problem. Furthermore, TESGAN conducts unsupervised learning which does not directly refer to the text of the training data to overcome the data memorization issue. By adopting this novel method, TESGAN can synthesize new sentences, showing the potential of unsupervised learning for text synthesis. We expect to see extended research combining Large Language Models with a new perspective of viewing text as an continuous space.",
      "authors": [
        "Jun-Min Lee",
        "Tae-Bin Ha"
      ],
      "published": "2023-06-19T10:22:12Z",
      "updated": "2023-10-17T10:41:12Z",
      "categories": [
        "cs.CL",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2306.17181v4",
      "landing_url": "https://arxiv.org/abs/2306.17181v4",
      "doi": "https://doi.org/10.3384/nejlt.2000-1533.2023.4855"
    },
    "queries": [
      {
        "anchor": "synchronous dialogue",
        "search_term": "discrete tokens"
      }
    ]
  },
  {
    "arxiv_id": "2307.02273",
    "anchor": "spoken language models",
    "search_term": "neural codec",
    "search_record": {
      "id": "http://arxiv.org/abs/2307.02273v4",
      "title": "Joint Hierarchical Priors and Adaptive Spatial Resolution for Efficient Neural Image Compression",
      "summary": "Recently, the performance of neural image compression (NIC) has steadily improved thanks to the last line of study, reaching or outperforming state-of-the-art conventional codecs. Despite significant progress, current NIC methods still rely on ConvNet-based entropy coding, limited in modeling long-range dependencies due to their local connectivity and the increasing number of architectural biases and priors, resulting in complex underperforming models with high decoding latency. Motivated by the efficiency investigation of the Tranformer-based transform coding framework, namely SwinT-ChARM, we propose to enhance the latter, as first, with a more straightforward yet effective Tranformer-based channel-wise auto-regressive prior model, resulting in an absolute image compression transformer (ICT). Through the proposed ICT, we can capture both global and local contexts from the latent representations and better parameterize the distribution of the quantized latents. Further, we leverage a learnable scaling module with a sandwich ConvNeXt-based pre-/post-processor to accurately extract more compact latent codes while reconstructing higher-quality images. Extensive experimental results on benchmark datasets showed that the proposed framework significantly improves the trade-off between coding efficiency and decoder complexity over the versatile video coding (VVC) reference encoder (VTM-18.0) and the neural codec SwinT-ChARM. Moreover, we provide model scaling studies to verify the computational efficiency of our approach and conduct several objective and subjective analyses to bring to the fore the performance gap between the adaptive image compression transformer (AICT) and the neural codec SwinT-ChARM.",
      "published": "2023-07-05T13:17:14Z"
    },
    "metadata": {
      "arxiv_id": "2307.02273",
      "title": "Joint Hierarchical Priors and Adaptive Spatial Resolution for Efficient Neural Image Compression",
      "summary": "Recently, the performance of neural image compression (NIC) has steadily improved thanks to the last line of study, reaching or outperforming state-of-the-art conventional codecs. Despite significant progress, current NIC methods still rely on ConvNet-based entropy coding, limited in modeling long-range dependencies due to their local connectivity and the increasing number of architectural biases and priors, resulting in complex underperforming models with high decoding latency. Motivated by the efficiency investigation of the Tranformer-based transform coding framework, namely SwinT-ChARM, we propose to enhance the latter, as first, with a more straightforward yet effective Tranformer-based channel-wise auto-regressive prior model, resulting in an absolute image compression transformer (ICT). Through the proposed ICT, we can capture both global and local contexts from the latent representations and better parameterize the distribution of the quantized latents. Further, we leverage a learnable scaling module with a sandwich ConvNeXt-based pre-/post-processor to accurately extract more compact latent codes while reconstructing higher-quality images. Extensive experimental results on benchmark datasets showed that the proposed framework significantly improves the trade-off between coding efficiency and decoder complexity over the versatile video coding (VVC) reference encoder (VTM-18.0) and the neural codec SwinT-ChARM. Moreover, we provide model scaling studies to verify the computational efficiency of our approach and conduct several objective and subjective analyses to bring to the fore the performance gap between the adaptive image compression transformer (AICT) and the neural codec SwinT-ChARM.",
      "authors": [
        "Ahmed Ghorbel",
        "Wassim Hamidouche",
        "Luce Morin"
      ],
      "published": "2023-07-05T13:17:14Z",
      "updated": "2024-01-22T17:37:03Z",
      "categories": [
        "cs.CV",
        "eess.IV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2307.02273v4",
      "landing_url": "https://arxiv.org/abs/2307.02273v4",
      "doi": "https://doi.org/10.48550/arXiv.2307.02273"
    },
    "queries": [
      {
        "anchor": "spoken language models",
        "search_term": "neural codec"
      },
      {
        "anchor": "synchronous dialogue",
        "search_term": "neural codec"
      },
      {
        "anchor": "turn-taking",
        "search_term": "neural codec"
      }
    ]
  },
  {
    "arxiv_id": "2307.03567",
    "anchor": "synchronous dialogue",
    "search_term": "streaming architecture",
    "search_record": {
      "id": "http://arxiv.org/abs/2307.03567v2",
      "title": "SpawnNet: Learning Generalizable Visuomotor Skills from Pre-trained Networks",
      "summary": "The existing internet-scale image and video datasets cover a wide range of everyday objects and tasks, bringing the potential of learning policies that generalize in diverse scenarios. Prior works have explored visual pre-training with different self-supervised objectives. Still, the generalization capabilities of the learned policies and the advantages over well-tuned baselines remain unclear from prior studies. In this work, we present a focused study of the generalization capabilities of the pre-trained visual representations at the categorical level. We identify the key bottleneck in using a frozen pre-trained visual backbone for policy learning and then propose SpawnNet, a novel two-stream architecture that learns to fuse pre-trained multi-layer representations into a separate network to learn a robust policy. Through extensive simulated and real experiments, we show significantly better categorical generalization compared to prior approaches in imitation learning settings. Open-sourced code and videos can be found on our website: https://xingyu-lin.github.io/spawnnet.",
      "published": "2023-07-07T13:01:29Z"
    },
    "metadata": {
      "arxiv_id": "2307.03567",
      "title": "SpawnNet: Learning Generalizable Visuomotor Skills from Pre-trained Networks",
      "summary": "The existing internet-scale image and video datasets cover a wide range of everyday objects and tasks, bringing the potential of learning policies that generalize in diverse scenarios. Prior works have explored visual pre-training with different self-supervised objectives. Still, the generalization capabilities of the learned policies and the advantages over well-tuned baselines remain unclear from prior studies. In this work, we present a focused study of the generalization capabilities of the pre-trained visual representations at the categorical level. We identify the key bottleneck in using a frozen pre-trained visual backbone for policy learning and then propose SpawnNet, a novel two-stream architecture that learns to fuse pre-trained multi-layer representations into a separate network to learn a robust policy. Through extensive simulated and real experiments, we show significantly better categorical generalization compared to prior approaches in imitation learning settings. Open-sourced code and videos can be found on our website: https://xingyu-lin.github.io/spawnnet.",
      "authors": [
        "Xingyu Lin",
        "John So",
        "Sashwat Mahalingam",
        "Fangchen Liu",
        "Pieter Abbeel"
      ],
      "published": "2023-07-07T13:01:29Z",
      "updated": "2023-10-22T03:16:41Z",
      "categories": [
        "cs.RO",
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2307.03567v2",
      "landing_url": "https://arxiv.org/abs/2307.03567v2",
      "doi": "https://doi.org/10.48550/arXiv.2307.03567"
    },
    "queries": [
      {
        "anchor": "synchronous dialogue",
        "search_term": "streaming architecture"
      }
    ]
  },
  {
    "arxiv_id": "2307.06091",
    "anchor": "synchronous dialogue",
    "search_term": "neural codec",
    "search_record": {
      "id": "http://arxiv.org/abs/2307.06091v1",
      "title": "AICT: An Adaptive Image Compression Transformer",
      "summary": "Motivated by the efficiency investigation of the Tranformer-based transform coding framework, namely SwinT-ChARM, we propose to enhance the latter, as first, with a more straightforward yet effective Tranformer-based channel-wise auto-regressive prior model, resulting in an absolute image compression transformer (ICT). Current methods that still rely on ConvNet-based entropy coding are limited in long-range modeling dependencies due to their local connectivity and an increasing number of architectural biases and priors. On the contrary, the proposed ICT can capture both global and local contexts from the latent representations and better parameterize the distribution of the quantized latents. Further, we leverage a learnable scaling module with a sandwich ConvNeXt-based pre/post-processor to accurately extract more compact latent representation while reconstructing higher-quality images. Extensive experimental results on benchmark datasets showed that the proposed adaptive image compression transformer (AICT) framework significantly improves the trade-off between coding efficiency and decoder complexity over the versatile video coding (VVC) reference encoder (VTM-18.0) and the neural codec SwinT-ChARM.",
      "published": "2023-07-12T11:32:02Z"
    },
    "metadata": {
      "arxiv_id": "2307.06091",
      "title": "AICT: An Adaptive Image Compression Transformer",
      "summary": "Motivated by the efficiency investigation of the Tranformer-based transform coding framework, namely SwinT-ChARM, we propose to enhance the latter, as first, with a more straightforward yet effective Tranformer-based channel-wise auto-regressive prior model, resulting in an absolute image compression transformer (ICT). Current methods that still rely on ConvNet-based entropy coding are limited in long-range modeling dependencies due to their local connectivity and an increasing number of architectural biases and priors. On the contrary, the proposed ICT can capture both global and local contexts from the latent representations and better parameterize the distribution of the quantized latents. Further, we leverage a learnable scaling module with a sandwich ConvNeXt-based pre/post-processor to accurately extract more compact latent representation while reconstructing higher-quality images. Extensive experimental results on benchmark datasets showed that the proposed adaptive image compression transformer (AICT) framework significantly improves the trade-off between coding efficiency and decoder complexity over the versatile video coding (VVC) reference encoder (VTM-18.0) and the neural codec SwinT-ChARM.",
      "authors": [
        "Ahmed Ghorbel",
        "Wassim Hamidouche",
        "Luce Morin"
      ],
      "published": "2023-07-12T11:32:02Z",
      "updated": "2023-07-12T11:32:02Z",
      "categories": [
        "cs.CV",
        "eess.IV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2307.06091v1",
      "landing_url": "https://arxiv.org/abs/2307.06091v1",
      "doi": "https://doi.org/10.48550/arXiv.2307.06091"
    },
    "queries": [
      {
        "anchor": "synchronous dialogue",
        "search_term": "neural codec"
      }
    ]
  },
  {
    "arxiv_id": "2307.06334",
    "anchor": "spoken language models",
    "search_term": "half duplex",
    "search_record": {
      "id": "http://arxiv.org/abs/2307.06334v1",
      "title": "Analysis of Half-Duplex Two-Node Slotted ALOHA Network With Asynchronous Traffic",
      "summary": "Despite the long history of research on slotted ALOHA, the exact analysis of the average delay is still in question as the performance of each node is coupled with the activity of other nodes. In this paper, we consider a network comprised of two half-duplex transmitter nodes with asynchronous arrival traffic that follow the slotted ALOHA protocol. We propose a new queueing theoretic model based on the state-dependent queues to analyze the network. In addition, we derive the exact values of delay and stability region for each node. The numerical results demonstrate the accuracy of our proposed model.",
      "published": "2023-07-12T17:56:02Z"
    },
    "metadata": {
      "arxiv_id": "2307.06334",
      "title": "Analysis of Half-Duplex Two-Node Slotted ALOHA Network With Asynchronous Traffic",
      "summary": "Despite the long history of research on slotted ALOHA, the exact analysis of the average delay is still in question as the performance of each node is coupled with the activity of other nodes. In this paper, we consider a network comprised of two half-duplex transmitter nodes with asynchronous arrival traffic that follow the slotted ALOHA protocol. We propose a new queueing theoretic model based on the state-dependent queues to analyze the network. In addition, we derive the exact values of delay and stability region for each node. The numerical results demonstrate the accuracy of our proposed model.",
      "authors": [
        "Seyed Ali Hashemian",
        "Farid Ashtiani"
      ],
      "published": "2023-07-12T17:56:02Z",
      "updated": "2023-07-12T17:56:02Z",
      "categories": [
        "eess.SY",
        "cs.NI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2307.06334v1",
      "landing_url": "https://arxiv.org/abs/2307.06334v1",
      "doi": "https://doi.org/10.48550/arXiv.2307.06334"
    },
    "queries": [
      {
        "anchor": "spoken language models",
        "search_term": "half duplex"
      },
      {
        "anchor": "synchronous dialogue",
        "search_term": "half duplex"
      }
    ]
  },
  {
    "arxiv_id": "2307.07821",
    "anchor": "spoken language models",
    "search_term": "streaming architecture",
    "search_record": {
      "id": "http://arxiv.org/abs/2307.07821v1",
      "title": "PASS: Exploiting Post-Activation Sparsity in Streaming Architectures for CNN Acceleration",
      "summary": "With the ever-growing popularity of Artificial Intelligence, there is an increasing demand for more performant and efficient underlying hardware. Convolutional Neural Networks (CNN) are a workload of particular importance, which achieve high accuracy in computer vision applications. Inside CNNs, a significant number of the post-activation values are zero, resulting in many redundant computations. Recent works have explored this post-activation sparsity on instruction-based CNN accelerators but not on streaming CNN accelerators, despite the fact that streaming architectures are considered the leading design methodology in terms of performance. In this paper, we highlight the challenges associated with exploiting post-activation sparsity for performance gains in streaming CNN accelerators, and demonstrate our approach to address them. Using a set of modern CNN benchmarks, our streaming sparse accelerators achieve 1.41x to 1.93x efficiency (GOP/s/DSP) compared to state-of-the-art instruction-based sparse accelerators.",
      "published": "2023-07-15T15:03:08Z"
    },
    "metadata": {
      "arxiv_id": "2307.07821",
      "title": "PASS: Exploiting Post-Activation Sparsity in Streaming Architectures for CNN Acceleration",
      "summary": "With the ever-growing popularity of Artificial Intelligence, there is an increasing demand for more performant and efficient underlying hardware. Convolutional Neural Networks (CNN) are a workload of particular importance, which achieve high accuracy in computer vision applications. Inside CNNs, a significant number of the post-activation values are zero, resulting in many redundant computations. Recent works have explored this post-activation sparsity on instruction-based CNN accelerators but not on streaming CNN accelerators, despite the fact that streaming architectures are considered the leading design methodology in terms of performance. In this paper, we highlight the challenges associated with exploiting post-activation sparsity for performance gains in streaming CNN accelerators, and demonstrate our approach to address them. Using a set of modern CNN benchmarks, our streaming sparse accelerators achieve 1.41x to 1.93x efficiency (GOP/s/DSP) compared to state-of-the-art instruction-based sparse accelerators.",
      "authors": [
        "Alexander Montgomerie-Corcoran",
        "Zhewen Yu",
        "Jianyi Cheng",
        "Christos-Savvas Bouganis"
      ],
      "published": "2023-07-15T15:03:08Z",
      "updated": "2023-07-15T15:03:08Z",
      "categories": [
        "cs.AR"
      ],
      "pdf_url": "https://arxiv.org/pdf/2307.07821v1",
      "landing_url": "https://arxiv.org/abs/2307.07821v1",
      "doi": "https://doi.org/10.48550/arXiv.2307.07821"
    },
    "queries": [
      {
        "anchor": "spoken language models",
        "search_term": "streaming architecture"
      },
      {
        "anchor": "synchronous dialogue",
        "search_term": "streaming architecture"
      },
      {
        "anchor": "turn-taking",
        "search_term": "streaming architecture"
      }
    ]
  },
  {
    "arxiv_id": "2307.08433",
    "anchor": "synchronous dialogue",
    "search_term": "low latency",
    "search_record": {
      "id": "http://arxiv.org/abs/2307.08433v5",
      "title": "From random-walks to graph-sprints: a low-latency node embedding framework on continuous-time dynamic graphs",
      "summary": "Many real-world datasets have an underlying dynamic graph structure, where entities and their interactions evolve over time. Machine learning models should consider these dynamics in order to harness their full potential in downstream tasks. Previous approaches for graph representation learning have focused on either sampling k-hop neighborhoods, akin to breadth-first search, or random walks, akin to depth-first search. However, these methods are computationally expensive and unsuitable for real-time, low-latency inference on dynamic graphs. To overcome these limitations, we propose graph-sprints a general purpose feature extraction framework for continuous-time-dynamic-graphs (CTDGs) that has low latency and is competitive with state-of-the-art, higher latency models. To achieve this, a streaming, low latency approximation to the random-walk based features is proposed. In our framework, time-aware node embeddings summarizing multi-hop information are computed using only single-hop operations on the incoming edges. We evaluate our proposed approach on three open-source datasets and two in-house datasets, and compare with three state-of-the-art algorithms (TGN-attn, TGN-ID, Jodie). We demonstrate that our graph-sprints features, combined with a machine learning classifier, achieve competitive performance (outperforming all baselines for the node classification tasks in five datasets). Simultaneously, graph-sprints significantly reduce inference latencies, achieving close to an order of magnitude speed-up in our experimental setting.",
      "published": "2023-07-17T12:25:52Z"
    },
    "metadata": {
      "arxiv_id": "2307.08433",
      "title": "From random-walks to graph-sprints: a low-latency node embedding framework on continuous-time dynamic graphs",
      "summary": "Many real-world datasets have an underlying dynamic graph structure, where entities and their interactions evolve over time. Machine learning models should consider these dynamics in order to harness their full potential in downstream tasks. Previous approaches for graph representation learning have focused on either sampling k-hop neighborhoods, akin to breadth-first search, or random walks, akin to depth-first search. However, these methods are computationally expensive and unsuitable for real-time, low-latency inference on dynamic graphs. To overcome these limitations, we propose graph-sprints a general purpose feature extraction framework for continuous-time-dynamic-graphs (CTDGs) that has low latency and is competitive with state-of-the-art, higher latency models. To achieve this, a streaming, low latency approximation to the random-walk based features is proposed. In our framework, time-aware node embeddings summarizing multi-hop information are computed using only single-hop operations on the incoming edges. We evaluate our proposed approach on three open-source datasets and two in-house datasets, and compare with three state-of-the-art algorithms (TGN-attn, TGN-ID, Jodie). We demonstrate that our graph-sprints features, combined with a machine learning classifier, achieve competitive performance (outperforming all baselines for the node classification tasks in five datasets). Simultaneously, graph-sprints significantly reduce inference latencies, achieving close to an order of magnitude speed-up in our experimental setting.",
      "authors": [
        "Ahmad Naser Eddin",
        "Jacopo Bono",
        "David Aparício",
        "Hugo Ferreira",
        "João Ascensão",
        "Pedro Ribeiro",
        "Pedro Bizarro"
      ],
      "published": "2023-07-17T12:25:52Z",
      "updated": "2024-02-16T23:34:24Z",
      "categories": [
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2307.08433v5",
      "landing_url": "https://arxiv.org/abs/2307.08433v5",
      "doi": "https://doi.org/10.48550/arXiv.2307.08433"
    },
    "queries": [
      {
        "anchor": "synchronous dialogue",
        "search_term": "low latency"
      }
    ]
  },
  {
    "arxiv_id": "2307.09597",
    "anchor": "synchronous dialogue",
    "search_term": "semantic coherence",
    "search_record": {
      "id": "http://arxiv.org/abs/2307.09597v1",
      "title": "Augmented Co-Speech Gesture Generation: Including Form and Meaning Features to Guide Learning-Based Gesture Synthesis",
      "summary": "Due to their significance in human communication, the automatic generation of co-speech gestures in artificial embodied agents has received a lot of attention. Although modern deep learning approaches can generate realistic-looking conversational gestures from spoken language, they often lack the ability to convey meaningful information and generate contextually appropriate gestures. This paper presents an augmented approach to the generation of co-speech gestures that additionally takes into account given form and meaning features for the gestures. Our framework effectively acquires this information from a small corpus with rich semantic annotations and a larger corpus without such information. We provide an analysis of the effects of distinctive feature targets and we report on a human rater evaluation study demonstrating that our framework achieves semantic coherence and person perception on the same level as human ground truth behavior. We make our data pipeline and the generation framework publicly available.",
      "published": "2023-07-13T12:40:50Z"
    },
    "metadata": {
      "arxiv_id": "2307.09597",
      "title": "Augmented Co-Speech Gesture Generation: Including Form and Meaning Features to Guide Learning-Based Gesture Synthesis",
      "summary": "Due to their significance in human communication, the automatic generation of co-speech gestures in artificial embodied agents has received a lot of attention. Although modern deep learning approaches can generate realistic-looking conversational gestures from spoken language, they often lack the ability to convey meaningful information and generate contextually appropriate gestures. This paper presents an augmented approach to the generation of co-speech gestures that additionally takes into account given form and meaning features for the gestures. Our framework effectively acquires this information from a small corpus with rich semantic annotations and a larger corpus without such information. We provide an analysis of the effects of distinctive feature targets and we report on a human rater evaluation study demonstrating that our framework achieves semantic coherence and person perception on the same level as human ground truth behavior. We make our data pipeline and the generation framework publicly available.",
      "authors": [
        "Hendric Voß",
        "Stefan Kopp"
      ],
      "published": "2023-07-13T12:40:50Z",
      "updated": "2023-07-13T12:40:50Z",
      "categories": [
        "cs.HC",
        "cs.GR"
      ],
      "pdf_url": "https://arxiv.org/pdf/2307.09597v1",
      "landing_url": "https://arxiv.org/abs/2307.09597v1",
      "doi": "https://doi.org/10.48550/arXiv.2307.09597"
    },
    "queries": [
      {
        "anchor": "synchronous dialogue",
        "search_term": "semantic coherence"
      }
    ]
  },
  {
    "arxiv_id": "2307.09857",
    "anchor": "synchronous dialogue",
    "search_term": "streaming architecture",
    "search_record": {
      "id": "http://arxiv.org/abs/2307.09857v3",
      "title": "Blind Image Quality Assessment Using Multi-Stream Architecture with Spatial and Channel Attention",
      "summary": "BIQA (Blind Image Quality Assessment) is an important field of study that evaluates images automatically. Although significant progress has been made, blind image quality assessment remains a difficult task since images vary in content and distortions. Most algorithms generate quality without emphasizing the important region of interest. In order to solve this, a multi-stream spatial and channel attention-based algorithm is being proposed. This algorithm generates more accurate predictions with a high correlation to human perceptual assessment by combining hybrid features from two different backbones, followed by spatial and channel attention to provide high weights to the region of interest. Four legacy image quality assessment datasets are used to validate the effectiveness of our proposed approach. Authentic and synthetic distortion image databases are used to demonstrate the effectiveness of the proposed method, and we show that it has excellent generalization properties with a particular focus on the perceptual foreground information.",
      "published": "2023-07-19T09:36:08Z"
    },
    "metadata": {
      "arxiv_id": "2307.09857",
      "title": "Blind Image Quality Assessment Using Multi-Stream Architecture with Spatial and Channel Attention",
      "summary": "BIQA (Blind Image Quality Assessment) is an important field of study that evaluates images automatically. Although significant progress has been made, blind image quality assessment remains a difficult task since images vary in content and distortions. Most algorithms generate quality without emphasizing the important region of interest. In order to solve this, a multi-stream spatial and channel attention-based algorithm is being proposed. This algorithm generates more accurate predictions with a high correlation to human perceptual assessment by combining hybrid features from two different backbones, followed by spatial and channel attention to provide high weights to the region of interest. Four legacy image quality assessment datasets are used to validate the effectiveness of our proposed approach. Authentic and synthetic distortion image databases are used to demonstrate the effectiveness of the proposed method, and we show that it has excellent generalization properties with a particular focus on the perceptual foreground information.",
      "authors": [
        "Muhammad Azeem Aslam",
        "Xu Wei",
        "Hassan Khalid",
        "Nisar Ahmed",
        "Zhu Shuangtong",
        "Xin Liu",
        "Yimei Xu"
      ],
      "published": "2023-07-19T09:36:08Z",
      "updated": "2024-10-07T18:56:53Z",
      "categories": [
        "cs.CV",
        "eess.IV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2307.09857v3",
      "landing_url": "https://arxiv.org/abs/2307.09857v3",
      "doi": "https://doi.org/10.48550/arXiv.2307.09857"
    },
    "queries": [
      {
        "anchor": "synchronous dialogue",
        "search_term": "streaming architecture"
      }
    ]
  },
  {
    "arxiv_id": "2307.10523",
    "anchor": "full-duplex",
    "search_term": "true full-duplex",
    "search_record": {
      "id": "http://arxiv.org/abs/2307.10523v1",
      "title": "Real-World Evaluation of Full-Duplex Millimeter Wave Communication Systems",
      "summary": "Noteworthy strides continue to be made in the development of full-duplex millimeter wave (mmWave) communication systems, but most of this progress has been built on theoretical models and validated through simulation. In this work, we conduct a long overdue real-world evaluation of full-duplex mmWave systems using off-the-shelf 60 GHz phased arrays. Using an experimental full-duplex base station, we collect over 200,000 measurements of self-interference by electronically sweeping its transmit and receive beams across a dense spatial profile, shedding light on the effects of the environment, array positioning, and beam steering direction. We then call attention to five key challenges faced by practical full-duplex mmWave systems and, with these in mind, propose a general framework for beamforming-based full-duplex solutions. Guided by this framework, we introduce a novel solution called STEER+, a more robust version of recent work called STEER, and experimentally evaluate both in a real-world setting with actual downlink and uplink users. Rather than purely minimize self-interference as with STEER, STEER+ makes use of additional measurements to maximize spectral efficiency, which proves to make it much less sensitive to one's choice of design parameters. We experimentally show that STEER+ can reliably reduce self-interference to near or below the noise floor while maintaining high SNR on the downlink and uplink, thus enabling full-duplex operation purely via beamforming.",
      "published": "2023-07-20T01:50:47Z"
    },
    "metadata": {
      "arxiv_id": "2307.10523",
      "title": "Real-World Evaluation of Full-Duplex Millimeter Wave Communication Systems",
      "summary": "Noteworthy strides continue to be made in the development of full-duplex millimeter wave (mmWave) communication systems, but most of this progress has been built on theoretical models and validated through simulation. In this work, we conduct a long overdue real-world evaluation of full-duplex mmWave systems using off-the-shelf 60 GHz phased arrays. Using an experimental full-duplex base station, we collect over 200,000 measurements of self-interference by electronically sweeping its transmit and receive beams across a dense spatial profile, shedding light on the effects of the environment, array positioning, and beam steering direction. We then call attention to five key challenges faced by practical full-duplex mmWave systems and, with these in mind, propose a general framework for beamforming-based full-duplex solutions. Guided by this framework, we introduce a novel solution called STEER+, a more robust version of recent work called STEER, and experimentally evaluate both in a real-world setting with actual downlink and uplink users. Rather than purely minimize self-interference as with STEER, STEER+ makes use of additional measurements to maximize spectral efficiency, which proves to make it much less sensitive to one's choice of design parameters. We experimentally show that STEER+ can reliably reduce self-interference to near or below the noise floor while maintaining high SNR on the downlink and uplink, thus enabling full-duplex operation purely via beamforming.",
      "authors": [
        "Ian P. Roberts",
        "Yu Zhang",
        "Tawfik Osman",
        "Ahmed Alkhateeb"
      ],
      "published": "2023-07-20T01:50:47Z",
      "updated": "2023-07-20T01:50:47Z",
      "categories": [
        "eess.SP"
      ],
      "pdf_url": "https://arxiv.org/pdf/2307.10523v1",
      "landing_url": "https://arxiv.org/abs/2307.10523v1",
      "doi": "https://doi.org/10.48550/arXiv.2307.10523"
    },
    "queries": [
      {
        "anchor": "full-duplex",
        "search_term": "true full-duplex"
      },
      {
        "anchor": "full-duplex",
        "search_term": "pseudo full-duplex"
      },
      {
        "anchor": "full-duplex",
        "search_term": "half duplex"
      },
      {
        "anchor": "full-duplex",
        "search_term": "engineered synchronization"
      },
      {
        "anchor": "full-duplex",
        "search_term": "learned synchronization"
      },
      {
        "anchor": "full-duplex",
        "search_term": "modular architecture"
      },
      {
        "anchor": "full-duplex",
        "search_term": "streaming architecture"
      },
      {
        "anchor": "full-duplex",
        "search_term": "real-time interaction"
      },
      {
        "anchor": "full-duplex",
        "search_term": "low latency"
      },
      {
        "anchor": "full-duplex",
        "search_term": "synchronous data"
      },
      {
        "anchor": "full-duplex",
        "search_term": "data scarcity"
      },
      {
        "anchor": "full-duplex",
        "search_term": "multichannel dialogue"
      },
      {
        "anchor": "full-duplex",
        "search_term": "temporal dynamics"
      },
      {
        "anchor": "full-duplex",
        "search_term": "behavioral arbitration"
      },
      {
        "anchor": "full-duplex",
        "search_term": "semantic coherence"
      },
      {
        "anchor": "full-duplex",
        "search_term": "neural codec"
      },
      {
        "anchor": "full-duplex",
        "search_term": "discrete tokens"
      },
      {
        "anchor": "full-duplex",
        "search_term": "audio tokens"
      },
      {
        "anchor": "full-duplex",
        "search_term": "floor transfer offset"
      },
      {
        "anchor": "full-duplex",
        "search_term": "stop latency"
      },
      {
        "anchor": "full-duplex",
        "search_term": "interruption response delay"
      }
    ]
  },
  {
    "arxiv_id": "2307.10550",
    "anchor": "synchronous dialogue",
    "search_term": "neural codec",
    "search_record": {
      "id": "http://arxiv.org/abs/2307.10550v1",
      "title": "SC VALL-E: Style-Controllable Zero-Shot Text to Speech Synthesizer",
      "summary": "Expressive speech synthesis models are trained by adding corpora with diverse speakers, various emotions, and different speaking styles to the dataset, in order to control various characteristics of speech and generate the desired voice. In this paper, we propose a style control (SC) VALL-E model based on the neural codec language model (called VALL-E), which follows the structure of the generative pretrained transformer 3 (GPT-3). The proposed SC VALL-E takes input from text sentences and prompt audio and is designed to generate controllable speech by not simply mimicking the characteristics of the prompt audio but by controlling the attributes to produce diverse voices. We identify tokens in the style embedding matrix of the newly designed style network that represent attributes such as emotion, speaking rate, pitch, and voice intensity, and design a model that can control these attributes. To evaluate the performance of SC VALL-E, we conduct comparative experiments with three representative expressive speech synthesis models: global style token (GST) Tacotron2, variational autoencoder (VAE) Tacotron2, and original VALL-E. We measure word error rate (WER), F0 voiced error (FVE), and F0 gross pitch error (F0GPE) as evaluation metrics to assess the accuracy of generated sentences. For comparing the quality of synthesized speech, we measure comparative mean option score (CMOS) and similarity mean option score (SMOS). To evaluate the style control ability of the generated speech, we observe the changes in F0 and mel-spectrogram by modifying the trained tokens. When using prompt audio that is not present in the training data, SC VALL-E generates a variety of expressive sounds and demonstrates competitive performance compared to the existing models. Our implementation, pretrained models, and audio samples are located on GitHub.",
      "published": "2023-07-20T03:28:06Z"
    },
    "metadata": {
      "arxiv_id": "2307.10550",
      "title": "SC VALL-E: Style-Controllable Zero-Shot Text to Speech Synthesizer",
      "summary": "Expressive speech synthesis models are trained by adding corpora with diverse speakers, various emotions, and different speaking styles to the dataset, in order to control various characteristics of speech and generate the desired voice. In this paper, we propose a style control (SC) VALL-E model based on the neural codec language model (called VALL-E), which follows the structure of the generative pretrained transformer 3 (GPT-3). The proposed SC VALL-E takes input from text sentences and prompt audio and is designed to generate controllable speech by not simply mimicking the characteristics of the prompt audio but by controlling the attributes to produce diverse voices. We identify tokens in the style embedding matrix of the newly designed style network that represent attributes such as emotion, speaking rate, pitch, and voice intensity, and design a model that can control these attributes. To evaluate the performance of SC VALL-E, we conduct comparative experiments with three representative expressive speech synthesis models: global style token (GST) Tacotron2, variational autoencoder (VAE) Tacotron2, and original VALL-E. We measure word error rate (WER), F0 voiced error (FVE), and F0 gross pitch error (F0GPE) as evaluation metrics to assess the accuracy of generated sentences. For comparing the quality of synthesized speech, we measure comparative mean option score (CMOS) and similarity mean option score (SMOS). To evaluate the style control ability of the generated speech, we observe the changes in F0 and mel-spectrogram by modifying the trained tokens. When using prompt audio that is not present in the training data, SC VALL-E generates a variety of expressive sounds and demonstrates competitive performance compared to the existing models. Our implementation, pretrained models, and audio samples are located on GitHub.",
      "authors": [
        "Daegyeom Kim",
        "Seongho Hong",
        "Yong-Hoon Choi"
      ],
      "published": "2023-07-20T03:28:06Z",
      "updated": "2023-07-20T03:28:06Z",
      "categories": [
        "cs.SD",
        "cs.LG",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2307.10550v1",
      "landing_url": "https://arxiv.org/abs/2307.10550v1",
      "doi": "https://doi.org/10.48550/arXiv.2307.10550"
    },
    "queries": [
      {
        "anchor": "synchronous dialogue",
        "search_term": "neural codec"
      }
    ]
  },
  {
    "arxiv_id": "2307.14025",
    "anchor": "spoken language models",
    "search_term": "data scarcity",
    "search_record": {
      "id": "http://arxiv.org/abs/2307.14025v2",
      "title": "Topologically Regularized Multiple Instance Learning to Harness Data Scarcity",
      "summary": "In biomedical data analysis, Multiple Instance Learning (MIL) models have emerged as a powerful tool to classify patients' microscopy samples. However, the data-intensive requirement of these models poses a significant challenge in scenarios with scarce data availability, e.g., in rare diseases. We introduce a topological regularization term to MIL to mitigate this challenge. It provides a shape-preserving inductive bias that compels the encoder to maintain the essential geometrical-topological structure of input bags during projection into latent space. This enhances the performance and generalization of the MIL classifier regardless of the aggregation function, particularly for scarce training data. The effectiveness of our method is confirmed through experiments across a range of datasets, showing an average enhancement of 2.8% for MIL benchmarks, 15.3% for synthetic MIL datasets, and 5.5% for real-world biomedical datasets over the current state-of-the-art.",
      "published": "2023-07-26T08:14:18Z"
    },
    "metadata": {
      "arxiv_id": "2307.14025",
      "title": "Topologically Regularized Multiple Instance Learning to Harness Data Scarcity",
      "summary": "In biomedical data analysis, Multiple Instance Learning (MIL) models have emerged as a powerful tool to classify patients' microscopy samples. However, the data-intensive requirement of these models poses a significant challenge in scenarios with scarce data availability, e.g., in rare diseases. We introduce a topological regularization term to MIL to mitigate this challenge. It provides a shape-preserving inductive bias that compels the encoder to maintain the essential geometrical-topological structure of input bags during projection into latent space. This enhances the performance and generalization of the MIL classifier regardless of the aggregation function, particularly for scarce training data. The effectiveness of our method is confirmed through experiments across a range of datasets, showing an average enhancement of 2.8% for MIL benchmarks, 15.3% for synthetic MIL datasets, and 5.5% for real-world biomedical datasets over the current state-of-the-art.",
      "authors": [
        "Salome Kazeminia",
        "Carsten Marr",
        "Bastian Rieck"
      ],
      "published": "2023-07-26T08:14:18Z",
      "updated": "2024-03-11T11:14:15Z",
      "categories": [
        "cs.LG",
        "cs.CV",
        "eess.IV",
        "q-bio.QM",
        "stat.ML"
      ],
      "pdf_url": "https://arxiv.org/pdf/2307.14025v2",
      "landing_url": "https://arxiv.org/abs/2307.14025v2",
      "doi": "https://doi.org/10.48550/arXiv.2307.14025"
    },
    "queries": [
      {
        "anchor": "spoken language models",
        "search_term": "data scarcity"
      },
      {
        "anchor": "synchronous dialogue",
        "search_term": "data scarcity"
      }
    ]
  },
  {
    "arxiv_id": "2307.14395",
    "anchor": "synchronous dialogue",
    "search_term": "temporal dynamics",
    "search_record": {
      "id": "http://arxiv.org/abs/2307.14395v1",
      "title": "Learning to simulate partially known spatio-temporal dynamics with trainable difference operators",
      "summary": "Recently, using neural networks to simulate spatio-temporal dynamics has received a lot of attention. However, most existing methods adopt pure data-driven black-box models, which have limited accuracy and interpretability. By combining trainable difference operators with black-box models, we propose a new hybrid architecture explicitly embedded with partial prior knowledge of the underlying PDEs named PDE-Net++. Furthermore, we introduce two distinct options called the trainable flipping difference layer (TFDL) and the trainable dynamic difference layer (TDDL) for the difference operators. Numerous numerical experiments have demonstrated that PDE-Net++ has superior prediction accuracy and better extrapolation performance than black-box models.",
      "published": "2023-07-26T10:05:18Z"
    },
    "metadata": {
      "arxiv_id": "2307.14395",
      "title": "Learning to simulate partially known spatio-temporal dynamics with trainable difference operators",
      "summary": "Recently, using neural networks to simulate spatio-temporal dynamics has received a lot of attention. However, most existing methods adopt pure data-driven black-box models, which have limited accuracy and interpretability. By combining trainable difference operators with black-box models, we propose a new hybrid architecture explicitly embedded with partial prior knowledge of the underlying PDEs named PDE-Net++. Furthermore, we introduce two distinct options called the trainable flipping difference layer (TFDL) and the trainable dynamic difference layer (TDDL) for the difference operators. Numerous numerical experiments have demonstrated that PDE-Net++ has superior prediction accuracy and better extrapolation performance than black-box models.",
      "authors": [
        "Xiang Huang",
        "Zhuoyuan Li",
        "Hongsheng Liu",
        "Zidong Wang",
        "Hongye Zhou",
        "Bin Dong",
        "Bei Hua"
      ],
      "published": "2023-07-26T10:05:18Z",
      "updated": "2023-07-26T10:05:18Z",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2307.14395v1",
      "landing_url": "https://arxiv.org/abs/2307.14395v1",
      "doi": "https://doi.org/10.48550/arXiv.2307.14395"
    },
    "queries": [
      {
        "anchor": "synchronous dialogue",
        "search_term": "temporal dynamics"
      }
    ]
  },
  {
    "arxiv_id": "2308.00725",
    "anchor": "spoken language models",
    "search_term": "neural codec",
    "search_record": {
      "id": "http://arxiv.org/abs/2308.00725v1",
      "title": "Latent-Shift: Gradient of Entropy Helps Neural Codecs",
      "summary": "End-to-end image/video codecs are getting competitive compared to traditional compression techniques that have been developed through decades of manual engineering efforts. These trainable codecs have many advantages over traditional techniques such as easy adaptation on perceptual distortion metrics and high performance on specific domains thanks to their learning ability. However, state of the art neural codecs does not take advantage of the existence of gradient of entropy in decoding device. In this paper, we theoretically show that gradient of entropy (available at decoder side) is correlated with the gradient of the reconstruction error (which is not available at decoder side). We then demonstrate experimentally that this gradient can be used on various compression methods, leading to a $1-2\\%$ rate savings for the same quality. Our method is orthogonal to other improvements and brings independent rate savings.",
      "published": "2023-08-01T15:12:36Z"
    },
    "metadata": {
      "arxiv_id": "2308.00725",
      "title": "Latent-Shift: Gradient of Entropy Helps Neural Codecs",
      "summary": "End-to-end image/video codecs are getting competitive compared to traditional compression techniques that have been developed through decades of manual engineering efforts. These trainable codecs have many advantages over traditional techniques such as easy adaptation on perceptual distortion metrics and high performance on specific domains thanks to their learning ability. However, state of the art neural codecs does not take advantage of the existence of gradient of entropy in decoding device. In this paper, we theoretically show that gradient of entropy (available at decoder side) is correlated with the gradient of the reconstruction error (which is not available at decoder side). We then demonstrate experimentally that this gradient can be used on various compression methods, leading to a $1-2\\%$ rate savings for the same quality. Our method is orthogonal to other improvements and brings independent rate savings.",
      "authors": [
        "Muhammet Balcilar",
        "Bharath Bhushan Damodaran",
        "Karam Naser",
        "Franck Galpin",
        "Pierre Hellier"
      ],
      "published": "2023-08-01T15:12:36Z",
      "updated": "2023-08-01T15:12:36Z",
      "categories": [
        "eess.IV",
        "cs.CV",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2308.00725v1",
      "landing_url": "https://arxiv.org/abs/2308.00725v1",
      "doi": "https://doi.org/10.48550/arXiv.2308.00725"
    },
    "queries": [
      {
        "anchor": "spoken language models",
        "search_term": "neural codec"
      },
      {
        "anchor": "synchronous dialogue",
        "search_term": "neural codec"
      },
      {
        "anchor": "turn-taking",
        "search_term": "neural codec"
      }
    ]
  },
  {
    "arxiv_id": "2308.02560",
    "anchor": "spoken language models",
    "search_term": "discrete tokens",
    "search_record": {
      "id": "http://arxiv.org/abs/2308.02560v2",
      "title": "From Discrete Tokens to High-Fidelity Audio Using Multi-Band Diffusion",
      "summary": "Deep generative models can generate high-fidelity audio conditioned on various types of representations (e.g., mel-spectrograms, Mel-frequency Cepstral Coefficients (MFCC)). Recently, such models have been used to synthesize audio waveforms conditioned on highly compressed representations. Although such methods produce impressive results, they are prone to generate audible artifacts when the conditioning is flawed or imperfect. An alternative modeling approach is to use diffusion models. However, these have mainly been used as speech vocoders (i.e., conditioned on mel-spectrograms) or generating relatively low sampling rate signals. In this work, we propose a high-fidelity multi-band diffusion-based framework that generates any type of audio modality (e.g., speech, music, environmental sounds) from low-bitrate discrete representations. At equal bit rate, the proposed approach outperforms state-of-the-art generative techniques in terms of perceptual quality. Training and, evaluation code, along with audio samples, are available on the facebookresearch/audiocraft Github page.",
      "published": "2023-08-02T22:14:29Z"
    },
    "metadata": {
      "arxiv_id": "2308.02560",
      "title": "From Discrete Tokens to High-Fidelity Audio Using Multi-Band Diffusion",
      "summary": "Deep generative models can generate high-fidelity audio conditioned on various types of representations (e.g., mel-spectrograms, Mel-frequency Cepstral Coefficients (MFCC)). Recently, such models have been used to synthesize audio waveforms conditioned on highly compressed representations. Although such methods produce impressive results, they are prone to generate audible artifacts when the conditioning is flawed or imperfect. An alternative modeling approach is to use diffusion models. However, these have mainly been used as speech vocoders (i.e., conditioned on mel-spectrograms) or generating relatively low sampling rate signals. In this work, we propose a high-fidelity multi-band diffusion-based framework that generates any type of audio modality (e.g., speech, music, environmental sounds) from low-bitrate discrete representations. At equal bit rate, the proposed approach outperforms state-of-the-art generative techniques in terms of perceptual quality. Training and, evaluation code, along with audio samples, are available on the facebookresearch/audiocraft Github page.",
      "authors": [
        "Robin San Roman",
        "Yossi Adi",
        "Antoine Deleforge",
        "Romain Serizel",
        "Gabriel Synnaeve",
        "Alexandre Défossez"
      ],
      "published": "2023-08-02T22:14:29Z",
      "updated": "2023-11-08T10:04:00Z",
      "categories": [
        "cs.SD",
        "cs.LG",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2308.02560v2",
      "landing_url": "https://arxiv.org/abs/2308.02560v2",
      "doi": "https://doi.org/10.48550/arXiv.2308.02560"
    },
    "queries": [
      {
        "anchor": "spoken language models",
        "search_term": "discrete tokens"
      },
      {
        "anchor": "synchronous dialogue",
        "search_term": "discrete tokens"
      }
    ]
  },
  {
    "arxiv_id": "2308.04248",
    "anchor": "spoken language models",
    "search_term": "true full-duplex",
    "search_record": {
      "id": "http://arxiv.org/abs/2308.04248v1",
      "title": "Gloss Alignment Using Word Embeddings",
      "summary": "Capturing and annotating Sign language datasets is a time consuming and costly process. Current datasets are orders of magnitude too small to successfully train unconstrained \\acf{slt} models. As a result, research has turned to TV broadcast content as a source of large-scale training data, consisting of both the sign language interpreter and the associated audio subtitle. However, lack of sign language annotation limits the usability of this data and has led to the development of automatic annotation techniques such as sign spotting. These spottings are aligned to the video rather than the subtitle, which often results in a misalignment between the subtitle and spotted signs. In this paper we propose a method for aligning spottings with their corresponding subtitles using large spoken language models. Using a single modality means our method is computationally inexpensive and can be utilized in conjunction with existing alignment techniques. We quantitatively demonstrate the effectiveness of our method on the \\acf{mdgs} and \\acf{bobsl} datasets, recovering up to a 33.22 BLEU-1 score in word alignment.",
      "published": "2023-08-08T13:26:53Z"
    },
    "metadata": {
      "arxiv_id": "2308.04248",
      "title": "Gloss Alignment Using Word Embeddings",
      "summary": "Capturing and annotating Sign language datasets is a time consuming and costly process. Current datasets are orders of magnitude too small to successfully train unconstrained \\acf{slt} models. As a result, research has turned to TV broadcast content as a source of large-scale training data, consisting of both the sign language interpreter and the associated audio subtitle. However, lack of sign language annotation limits the usability of this data and has led to the development of automatic annotation techniques such as sign spotting. These spottings are aligned to the video rather than the subtitle, which often results in a misalignment between the subtitle and spotted signs. In this paper we propose a method for aligning spottings with their corresponding subtitles using large spoken language models. Using a single modality means our method is computationally inexpensive and can be utilized in conjunction with existing alignment techniques. We quantitatively demonstrate the effectiveness of our method on the \\acf{mdgs} and \\acf{bobsl} datasets, recovering up to a 33.22 BLEU-1 score in word alignment.",
      "authors": [
        "Harry Walsh",
        "Ozge Mercanoglu Sincan",
        "Ben Saunders",
        "Richard Bowden"
      ],
      "published": "2023-08-08T13:26:53Z",
      "updated": "2023-08-08T13:26:53Z",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2308.04248v1",
      "landing_url": "https://arxiv.org/abs/2308.04248v1",
      "doi": "https://doi.org/10.48550/arXiv.2308.04248"
    },
    "queries": [
      {
        "anchor": "spoken language models",
        "search_term": "true full-duplex"
      },
      {
        "anchor": "spoken language models",
        "search_term": "pseudo full-duplex"
      },
      {
        "anchor": "spoken language models",
        "search_term": "engineered synchronization"
      },
      {
        "anchor": "spoken language models",
        "search_term": "learned synchronization"
      },
      {
        "anchor": "spoken language models",
        "search_term": "modular architecture"
      },
      {
        "anchor": "spoken language models",
        "search_term": "streaming architecture"
      },
      {
        "anchor": "spoken language models",
        "search_term": "real-time interaction"
      },
      {
        "anchor": "spoken language models",
        "search_term": "synchronous data"
      },
      {
        "anchor": "spoken language models",
        "search_term": "data scarcity"
      },
      {
        "anchor": "spoken language models",
        "search_term": "multichannel dialogue"
      },
      {
        "anchor": "spoken language models",
        "search_term": "behavioral arbitration"
      },
      {
        "anchor": "spoken language models",
        "search_term": "semantic coherence"
      },
      {
        "anchor": "spoken language models",
        "search_term": "neural codec"
      },
      {
        "anchor": "spoken language models",
        "search_term": "discrete tokens"
      },
      {
        "anchor": "spoken language models",
        "search_term": "audio tokens"
      },
      {
        "anchor": "spoken language models",
        "search_term": "floor transfer offset"
      },
      {
        "anchor": "spoken language models",
        "search_term": "stop latency"
      },
      {
        "anchor": "spoken language models",
        "search_term": "interruption response delay"
      }
    ]
  },
  {
    "arxiv_id": "2308.05219",
    "anchor": "synchronous dialogue",
    "search_term": "semantic coherence",
    "search_record": {
      "id": "http://arxiv.org/abs/2308.05219v1",
      "title": "Decoding Layer Saliency in Language Transformers",
      "summary": "In this paper, we introduce a strategy for identifying textual saliency in large-scale language models applied to classification tasks. In visual networks where saliency is more well-studied, saliency is naturally localized through the convolutional layers of the network; however, the same is not true in modern transformer-stack networks used to process natural language. We adapt gradient-based saliency methods for these networks, propose a method for evaluating the degree of semantic coherence of each layer, and demonstrate consistent improvement over numerous other methods for textual saliency on multiple benchmark classification datasets. Our approach requires no additional training or access to labelled data, and is comparatively very computationally efficient.",
      "published": "2023-08-09T20:53:22Z"
    },
    "metadata": {
      "arxiv_id": "2308.05219",
      "title": "Decoding Layer Saliency in Language Transformers",
      "summary": "In this paper, we introduce a strategy for identifying textual saliency in large-scale language models applied to classification tasks. In visual networks where saliency is more well-studied, saliency is naturally localized through the convolutional layers of the network; however, the same is not true in modern transformer-stack networks used to process natural language. We adapt gradient-based saliency methods for these networks, propose a method for evaluating the degree of semantic coherence of each layer, and demonstrate consistent improvement over numerous other methods for textual saliency on multiple benchmark classification datasets. Our approach requires no additional training or access to labelled data, and is comparatively very computationally efficient.",
      "authors": [
        "Elizabeth M. Hou",
        "Gregory Castanon"
      ],
      "published": "2023-08-09T20:53:22Z",
      "updated": "2023-08-09T20:53:22Z",
      "categories": [
        "cs.CL",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2308.05219v1",
      "landing_url": "https://arxiv.org/abs/2308.05219v1",
      "doi": "https://doi.org/10.48550/arXiv.2308.05219"
    },
    "queries": [
      {
        "anchor": "synchronous dialogue",
        "search_term": "semantic coherence"
      }
    ]
  },
  {
    "arxiv_id": "2308.06873",
    "anchor": "spoken language models",
    "search_term": "neural codec",
    "search_record": {
      "id": "http://arxiv.org/abs/2308.06873v2",
      "title": "SpeechX: Neural Codec Language Model as a Versatile Speech Transformer",
      "summary": "Recent advancements in generative speech models based on audio-text prompts have enabled remarkable innovations like high-quality zero-shot text-to-speech. However, existing models still face limitations in handling diverse audio-text speech generation tasks involving transforming input speech and processing audio captured in adverse acoustic conditions. This paper introduces SpeechX, a versatile speech generation model capable of zero-shot TTS and various speech transformation tasks, dealing with both clean and noisy signals. SpeechX combines neural codec language modeling with multi-task learning using task-dependent prompting, enabling unified and extensible modeling and providing a consistent way for leveraging textual input in speech enhancement and transformation tasks. Experimental results show SpeechX's efficacy in various tasks, including zero-shot TTS, noise suppression, target speaker extraction, speech removal, and speech editing with or without background noise, achieving comparable or superior performance to specialized models across tasks. See https://aka.ms/speechx for demo samples.",
      "published": "2023-08-14T01:01:19Z"
    },
    "metadata": {
      "arxiv_id": "2308.06873",
      "title": "SpeechX: Neural Codec Language Model as a Versatile Speech Transformer",
      "summary": "Recent advancements in generative speech models based on audio-text prompts have enabled remarkable innovations like high-quality zero-shot text-to-speech. However, existing models still face limitations in handling diverse audio-text speech generation tasks involving transforming input speech and processing audio captured in adverse acoustic conditions. This paper introduces SpeechX, a versatile speech generation model capable of zero-shot TTS and various speech transformation tasks, dealing with both clean and noisy signals. SpeechX combines neural codec language modeling with multi-task learning using task-dependent prompting, enabling unified and extensible modeling and providing a consistent way for leveraging textual input in speech enhancement and transformation tasks. Experimental results show SpeechX's efficacy in various tasks, including zero-shot TTS, noise suppression, target speaker extraction, speech removal, and speech editing with or without background noise, achieving comparable or superior performance to specialized models across tasks. See https://aka.ms/speechx for demo samples.",
      "authors": [
        "Xiaofei Wang",
        "Manthan Thakker",
        "Zhuo Chen",
        "Naoyuki Kanda",
        "Sefik Emre Eskimez",
        "Sanyuan Chen",
        "Min Tang",
        "Shujie Liu",
        "Jinyu Li",
        "Takuya Yoshioka"
      ],
      "published": "2023-08-14T01:01:19Z",
      "updated": "2024-06-25T18:38:28Z",
      "categories": [
        "eess.AS",
        "cs.CL",
        "cs.LG",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2308.06873v2",
      "landing_url": "https://arxiv.org/abs/2308.06873v2",
      "doi": "https://doi.org/10.48550/arXiv.2308.06873"
    },
    "queries": [
      {
        "anchor": "spoken language models",
        "search_term": "neural codec"
      },
      {
        "anchor": "synchronous dialogue",
        "search_term": "neural codec"
      },
      {
        "anchor": "turn-taking",
        "search_term": "neural codec"
      }
    ]
  },
  {
    "arxiv_id": "2308.07395",
    "anchor": "turn-taking",
    "search_term": "true full-duplex",
    "search_record": {
      "id": "http://arxiv.org/abs/2308.07395v1",
      "title": "Text Injection for Capitalization and Turn-Taking Prediction in Speech Models",
      "summary": "Text injection for automatic speech recognition (ASR), wherein unpaired text-only data is used to supplement paired audio-text data, has shown promising improvements for word error rate. This study examines the use of text injection for auxiliary tasks, which are the non-ASR tasks often performed by an E2E model. In this work, we use joint end-to-end and internal language model training (JEIT) as our text injection algorithm to train an ASR model which performs two auxiliary tasks. The first is capitalization, which is a de-normalization task. The second is turn-taking prediction, which attempts to identify whether a user has completed their conversation turn in a digital assistant interaction. We show results demonstrating that our text injection method boosts capitalization performance for long-tail data, and improves turn-taking detection recall.",
      "published": "2023-08-14T18:28:04Z"
    },
    "metadata": {
      "arxiv_id": "2308.07395",
      "title": "Text Injection for Capitalization and Turn-Taking Prediction in Speech Models",
      "summary": "Text injection for automatic speech recognition (ASR), wherein unpaired text-only data is used to supplement paired audio-text data, has shown promising improvements for word error rate. This study examines the use of text injection for auxiliary tasks, which are the non-ASR tasks often performed by an E2E model. In this work, we use joint end-to-end and internal language model training (JEIT) as our text injection algorithm to train an ASR model which performs two auxiliary tasks. The first is capitalization, which is a de-normalization task. The second is turn-taking prediction, which attempts to identify whether a user has completed their conversation turn in a digital assistant interaction. We show results demonstrating that our text injection method boosts capitalization performance for long-tail data, and improves turn-taking detection recall.",
      "authors": [
        "Shaan Bijwadia",
        "Shuo-yiin Chang",
        "Weiran Wang",
        "Zhong Meng",
        "Hao Zhang",
        "Tara N. Sainath"
      ],
      "published": "2023-08-14T18:28:04Z",
      "updated": "2023-08-14T18:28:04Z",
      "categories": [
        "cs.CL",
        "cs.LG",
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2308.07395v1",
      "landing_url": "https://arxiv.org/abs/2308.07395v1",
      "doi": "https://doi.org/10.48550/arXiv.2308.07395"
    },
    "queries": [
      {
        "anchor": "turn-taking",
        "search_term": "true full-duplex"
      },
      {
        "anchor": "turn-taking",
        "search_term": "pseudo full-duplex"
      },
      {
        "anchor": "turn-taking",
        "search_term": "half duplex"
      },
      {
        "anchor": "turn-taking",
        "search_term": "engineered synchronization"
      },
      {
        "anchor": "turn-taking",
        "search_term": "learned synchronization"
      },
      {
        "anchor": "turn-taking",
        "search_term": "modular architecture"
      },
      {
        "anchor": "turn-taking",
        "search_term": "streaming architecture"
      },
      {
        "anchor": "turn-taking",
        "search_term": "real-time interaction"
      },
      {
        "anchor": "turn-taking",
        "search_term": "low latency"
      },
      {
        "anchor": "turn-taking",
        "search_term": "synchronous data"
      },
      {
        "anchor": "turn-taking",
        "search_term": "data scarcity"
      },
      {
        "anchor": "turn-taking",
        "search_term": "multichannel dialogue"
      },
      {
        "anchor": "turn-taking",
        "search_term": "temporal dynamics"
      },
      {
        "anchor": "turn-taking",
        "search_term": "behavioral arbitration"
      },
      {
        "anchor": "turn-taking",
        "search_term": "semantic coherence"
      },
      {
        "anchor": "turn-taking",
        "search_term": "neural codec"
      },
      {
        "anchor": "turn-taking",
        "search_term": "discrete tokens"
      },
      {
        "anchor": "turn-taking",
        "search_term": "audio tokens"
      },
      {
        "anchor": "turn-taking",
        "search_term": "floor transfer offset"
      },
      {
        "anchor": "turn-taking",
        "search_term": "stop latency"
      },
      {
        "anchor": "turn-taking",
        "search_term": "interruption response delay"
      }
    ]
  },
  {
    "arxiv_id": "2308.09793",
    "anchor": "spoken language models",
    "search_term": "modular architecture",
    "search_record": {
      "id": "http://arxiv.org/abs/2308.09793v2",
      "title": "Towards a Modular Architecture for Science Factories",
      "summary": "Advances in robotic automation, high-performance computing (HPC), and artificial intelligence (AI) encourage us to conceive of science factories: large, general-purpose computation- and AI-enabled self-driving laboratories (SDLs) with the generality and scale needed both to tackle large discovery problems and to support thousands of scientists. Science factories require modular hardware and software that can be replicated for scale and (re)configured to support many applications. To this end, we propose a prototype modular science factory architecture in which reconfigurable modules encapsulating scientific instruments are linked with manipulators to form workcells, that can themselves be combined to form larger assemblages, and linked with distributed computing for simulation, AI model training and inference, and related tasks. Workflows that perform sets of actions on modules can be specified, and various applications, comprising workflows plus associated computational and data manipulation steps, can be run concurrently. We report on our experiences prototyping this architecture and applying it in experiments involving 15 different robotic apparatus, five applications (one in education, two in biology, two in materials), and a variety of workflows, across four laboratories. We describe the reuse of modules, workcells, and workflows in different applications, the migration of applications between workcells, and the use of digital twins, and suggest directions for future work aimed at yet more generality and scalability. Code and data are available at https://ad-sdl.github.io/wei2023 and in the Supplementary Information",
      "published": "2023-08-18T19:47:59Z"
    },
    "metadata": {
      "arxiv_id": "2308.09793",
      "title": "Towards a Modular Architecture for Science Factories",
      "summary": "Advances in robotic automation, high-performance computing (HPC), and artificial intelligence (AI) encourage us to conceive of science factories: large, general-purpose computation- and AI-enabled self-driving laboratories (SDLs) with the generality and scale needed both to tackle large discovery problems and to support thousands of scientists. Science factories require modular hardware and software that can be replicated for scale and (re)configured to support many applications. To this end, we propose a prototype modular science factory architecture in which reconfigurable modules encapsulating scientific instruments are linked with manipulators to form workcells, that can themselves be combined to form larger assemblages, and linked with distributed computing for simulation, AI model training and inference, and related tasks. Workflows that perform sets of actions on modules can be specified, and various applications, comprising workflows plus associated computational and data manipulation steps, can be run concurrently. We report on our experiences prototyping this architecture and applying it in experiments involving 15 different robotic apparatus, five applications (one in education, two in biology, two in materials), and a variety of workflows, across four laboratories. We describe the reuse of modules, workcells, and workflows in different applications, the migration of applications between workcells, and the use of digital twins, and suggest directions for future work aimed at yet more generality and scalability. Code and data are available at https://ad-sdl.github.io/wei2023 and in the Supplementary Information",
      "authors": [
        "Rafael Vescovi",
        "Tobias Ginsburg",
        "Kyle Hippe",
        "Doga Ozgulbas",
        "Casey Stone",
        "Abraham Stroka",
        "Rory Butler",
        "Ben Blaiszik",
        "Tom Brettin",
        "Kyle Chard",
        "Mark Hereld",
        "Arvind Ramanathan",
        "Rick Stevens",
        "Aikaterini Vriza",
        "Jie Xu",
        "Qingteng Zhang",
        "Ian Foster"
      ],
      "published": "2023-08-18T19:47:59Z",
      "updated": "2023-10-17T16:48:33Z",
      "categories": [
        "cs.RO"
      ],
      "pdf_url": "https://arxiv.org/pdf/2308.09793v2",
      "landing_url": "https://arxiv.org/abs/2308.09793v2",
      "doi": "https://doi.org/10.48550/arXiv.2308.09793"
    },
    "queries": [
      {
        "anchor": "spoken language models",
        "search_term": "modular architecture"
      },
      {
        "anchor": "synchronous dialogue",
        "search_term": "modular architecture"
      },
      {
        "anchor": "turn-taking",
        "search_term": "modular architecture"
      }
    ]
  },
  {
    "arxiv_id": "2308.10415",
    "anchor": "spoken language models",
    "search_term": "audio tokens",
    "search_record": {
      "id": "http://arxiv.org/abs/2308.10415v1",
      "title": "TokenSplit: Using Discrete Speech Representations for Direct, Refined, and Transcript-Conditioned Speech Separation and Recognition",
      "summary": "We present TokenSplit, a speech separation model that acts on discrete token sequences. The model is trained on multiple tasks simultaneously: separate and transcribe each speech source, and generate speech from text. The model operates on transcripts and audio token sequences and achieves multiple tasks through masking of inputs. The model is a sequence-to-sequence encoder-decoder model that uses the Transformer architecture. We also present a \"refinement\" version of the model that predicts enhanced audio tokens from the audio tokens of speech separated by a conventional separation model. Using both objective metrics and subjective MUSHRA listening tests, we show that our model achieves excellent performance in terms of separation, both with or without transcript conditioning. We also measure the automatic speech recognition (ASR) performance and provide audio samples of speech synthesis to demonstrate the additional utility of our model.",
      "published": "2023-08-21T01:52:01Z"
    },
    "metadata": {
      "arxiv_id": "2308.10415",
      "title": "TokenSplit: Using Discrete Speech Representations for Direct, Refined, and Transcript-Conditioned Speech Separation and Recognition",
      "summary": "We present TokenSplit, a speech separation model that acts on discrete token sequences. The model is trained on multiple tasks simultaneously: separate and transcribe each speech source, and generate speech from text. The model operates on transcripts and audio token sequences and achieves multiple tasks through masking of inputs. The model is a sequence-to-sequence encoder-decoder model that uses the Transformer architecture. We also present a \"refinement\" version of the model that predicts enhanced audio tokens from the audio tokens of speech separated by a conventional separation model. Using both objective metrics and subjective MUSHRA listening tests, we show that our model achieves excellent performance in terms of separation, both with or without transcript conditioning. We also measure the automatic speech recognition (ASR) performance and provide audio samples of speech synthesis to demonstrate the additional utility of our model.",
      "authors": [
        "Hakan Erdogan",
        "Scott Wisdom",
        "Xuankai Chang",
        "Zalán Borsos",
        "Marco Tagliasacchi",
        "Neil Zeghidour",
        "John R. Hershey"
      ],
      "published": "2023-08-21T01:52:01Z",
      "updated": "2023-08-21T01:52:01Z",
      "categories": [
        "cs.SD",
        "cs.LG",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2308.10415v1",
      "landing_url": "https://arxiv.org/abs/2308.10415v1",
      "doi": "https://doi.org/10.48550/arXiv.2308.10415"
    },
    "queries": [
      {
        "anchor": "spoken language models",
        "search_term": "audio tokens"
      },
      {
        "anchor": "synchronous dialogue",
        "search_term": "audio tokens"
      },
      {
        "anchor": "turn-taking",
        "search_term": "audio tokens"
      }
    ]
  },
  {
    "arxiv_id": "2308.11630",
    "anchor": "spoken language models",
    "search_term": "data scarcity",
    "search_record": {
      "id": "http://arxiv.org/abs/2308.11630v2",
      "title": "Addressing Data Scarcity in Optical Matrix Multiplier Modeling Using Transfer Learning",
      "summary": "We present and experimentally evaluate using transfer learning to address experimental data scarcity when training neural network (NN) models for Mach-Zehnder interferometer mesh-based optical matrix multipliers. Our approach involves pre-training the model using synthetic data generated from a less accurate analytical model and fine-tuning with experimental data. Our investigation demonstrates that this method yields significant reductions in modeling errors compared to using an analytical model, or a standalone NN model when training data is limited. Utilizing regularization techniques and ensemble averaging, we achieve < 1 dB root-mean-square error on the matrix weights implemented by a 3x3 photonic chip while using only 25% of the available data.",
      "published": "2023-08-10T07:33:00Z"
    },
    "metadata": {
      "arxiv_id": "2308.11630",
      "title": "Addressing Data Scarcity in Optical Matrix Multiplier Modeling Using Transfer Learning",
      "summary": "We present and experimentally evaluate using transfer learning to address experimental data scarcity when training neural network (NN) models for Mach-Zehnder interferometer mesh-based optical matrix multipliers. Our approach involves pre-training the model using synthetic data generated from a less accurate analytical model and fine-tuning with experimental data. Our investigation demonstrates that this method yields significant reductions in modeling errors compared to using an analytical model, or a standalone NN model when training data is limited. Utilizing regularization techniques and ensemble averaging, we achieve < 1 dB root-mean-square error on the matrix weights implemented by a 3x3 photonic chip while using only 25% of the available data.",
      "authors": [
        "Ali Cem",
        "Ognjen Jovanovic",
        "Siqi Yan",
        "Yunhong Ding",
        "Darko Zibar",
        "Francesco Da Ros"
      ],
      "published": "2023-08-10T07:33:00Z",
      "updated": "2023-11-13T15:58:10Z",
      "categories": [
        "cs.LG",
        "physics.optics"
      ],
      "pdf_url": "https://arxiv.org/pdf/2308.11630v2",
      "landing_url": "https://arxiv.org/abs/2308.11630v2",
      "doi": "https://doi.org/10.48550/arXiv.2308.11630"
    },
    "queries": [
      {
        "anchor": "spoken language models",
        "search_term": "data scarcity"
      },
      {
        "anchor": "synchronous dialogue",
        "search_term": "data scarcity"
      },
      {
        "anchor": "turn-taking",
        "search_term": "data scarcity"
      }
    ]
  },
  {
    "arxiv_id": "2308.15056",
    "anchor": "synchronous dialogue",
    "search_term": "real-time interaction",
    "search_record": {
      "id": "http://arxiv.org/abs/2308.15056v1",
      "title": "A Consumer-tier based Visual-Brain Machine Interface for Augmented Reality Glasses Interactions",
      "summary": "Objective.Visual-Brain Machine Interface(V-BMI) has provide a novel interaction technique for Augmented Reality (AR) industries. Several state-of-arts work has demonstates its high accuracy and real-time interaction capbilities. However, most of the studies employ EEGs devices that are rigid and difficult to apply in real-life AR glasseses application sceniraros. Here we develop a consumer-tier Visual-Brain Machine Inteface(V-BMI) system specialized for Augmented Reality(AR) glasses interactions. Approach. The developed system consists of a wearable hardware which takes advantages of fast set-up, reliable recording and comfortable wearable experience that specificized for AR glasses applications. Complementing this hardware, we have devised a software framework that facilitates real-time interactions within the system while accommodating a modular configuration to enhance scalability. Main results. The developed hardware is only 110g and 120x85x23 mm, which with 1 Tohm and peak to peak voltage is less than 1.5 uV, and a V-BMI based angry bird game and an Internet of Thing (IoT) AR applications are deisgned, we demonstrated such technology merits of intuitive experience and efficiency interaction. The real-time interaction accuracy is between 85 and 96 percentages in a commercial AR glasses (DTI is 2.24s and ITR 65 bits-min ). Significance. Our study indicates the developed system can provide an essential hardware-software framework for consumer based V-BMI AR glasses. Also, we derive several pivotal design factors for a consumer-grade V-BMI-based AR system: 1) Dynamic adaptation of stimulation patterns-classification methods via computer vision algorithms is necessary for AR glasses applications; and 2) Algorithmic localization to foster system stability and latency reduction.",
      "published": "2023-08-29T06:33:13Z"
    },
    "metadata": {
      "arxiv_id": "2308.15056",
      "title": "A Consumer-tier based Visual-Brain Machine Interface for Augmented Reality Glasses Interactions",
      "summary": "Objective.Visual-Brain Machine Interface(V-BMI) has provide a novel interaction technique for Augmented Reality (AR) industries. Several state-of-arts work has demonstates its high accuracy and real-time interaction capbilities. However, most of the studies employ EEGs devices that are rigid and difficult to apply in real-life AR glasseses application sceniraros. Here we develop a consumer-tier Visual-Brain Machine Inteface(V-BMI) system specialized for Augmented Reality(AR) glasses interactions. Approach. The developed system consists of a wearable hardware which takes advantages of fast set-up, reliable recording and comfortable wearable experience that specificized for AR glasses applications. Complementing this hardware, we have devised a software framework that facilitates real-time interactions within the system while accommodating a modular configuration to enhance scalability. Main results. The developed hardware is only 110g and 120x85x23 mm, which with 1 Tohm and peak to peak voltage is less than 1.5 uV, and a V-BMI based angry bird game and an Internet of Thing (IoT) AR applications are deisgned, we demonstrated such technology merits of intuitive experience and efficiency interaction. The real-time interaction accuracy is between 85 and 96 percentages in a commercial AR glasses (DTI is 2.24s and ITR 65 bits-min ). Significance. Our study indicates the developed system can provide an essential hardware-software framework for consumer based V-BMI AR glasses. Also, we derive several pivotal design factors for a consumer-grade V-BMI-based AR system: 1) Dynamic adaptation of stimulation patterns-classification methods via computer vision algorithms is necessary for AR glasses applications; and 2) Algorithmic localization to foster system stability and latency reduction.",
      "authors": [
        "Yuying Jiang",
        "Fan Bai",
        "Zicheng Zhang",
        "Xiaochen Ye",
        "Zheng Liu",
        "Zhiping Shi",
        "Jianwei Yao",
        "Xiaojun Liu",
        "Fangkun Zhu",
        "Junling Li Qian Guo",
        "Xiaoan Wang",
        "Junwen Luo"
      ],
      "published": "2023-08-29T06:33:13Z",
      "updated": "2023-08-29T06:33:13Z",
      "categories": [
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2308.15056v1",
      "landing_url": "https://arxiv.org/abs/2308.15056v1",
      "doi": "https://doi.org/10.48550/arXiv.2308.15056"
    },
    "queries": [
      {
        "anchor": "synchronous dialogue",
        "search_term": "real-time interaction"
      }
    ]
  },
  {
    "arxiv_id": "2308.16488",
    "anchor": "synchronous dialogue",
    "search_term": "data scarcity",
    "search_record": {
      "id": "http://arxiv.org/abs/2308.16488v1",
      "title": "RAMP: Retrieval-Augmented MOS Prediction via Confidence-based Dynamic Weighting",
      "summary": "Automatic Mean Opinion Score (MOS) prediction is crucial to evaluate the perceptual quality of the synthetic speech. While recent approaches using pre-trained self-supervised learning (SSL) models have shown promising results, they only partly address the data scarcity issue for the feature extractor. This leaves the data scarcity issue for the decoder unresolved and leading to suboptimal performance. To address this challenge, we propose a retrieval-augmented MOS prediction method, dubbed {\\bf RAMP}, to enhance the decoder's ability against the data scarcity issue. A fusing network is also proposed to dynamically adjust the retrieval scope for each instance and the fusion weights based on the predictive confidence. Experimental results show that our proposed method outperforms the existing methods in multiple scenarios.",
      "published": "2023-08-31T06:48:01Z"
    },
    "metadata": {
      "arxiv_id": "2308.16488",
      "title": "RAMP: Retrieval-Augmented MOS Prediction via Confidence-based Dynamic Weighting",
      "summary": "Automatic Mean Opinion Score (MOS) prediction is crucial to evaluate the perceptual quality of the synthetic speech. While recent approaches using pre-trained self-supervised learning (SSL) models have shown promising results, they only partly address the data scarcity issue for the feature extractor. This leaves the data scarcity issue for the decoder unresolved and leading to suboptimal performance. To address this challenge, we propose a retrieval-augmented MOS prediction method, dubbed {\\bf RAMP}, to enhance the decoder's ability against the data scarcity issue. A fusing network is also proposed to dynamically adjust the retrieval scope for each instance and the fusion weights based on the predictive confidence. Experimental results show that our proposed method outperforms the existing methods in multiple scenarios.",
      "authors": [
        "Hui Wang",
        "Shiwan Zhao",
        "Xiguang Zheng",
        "Yong Qin"
      ],
      "published": "2023-08-31T06:48:01Z",
      "updated": "2023-08-31T06:48:01Z",
      "categories": [
        "eess.AS",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2308.16488v1",
      "landing_url": "https://arxiv.org/abs/2308.16488v1",
      "doi": "https://doi.org/10.21437/Interspeech.2023-851"
    },
    "queries": [
      {
        "anchor": "synchronous dialogue",
        "search_term": "data scarcity"
      }
    ]
  },
  {
    "arxiv_id": "2309.00445",
    "anchor": "synchronous dialogue",
    "search_term": "real-time interaction",
    "search_record": {
      "id": "http://arxiv.org/abs/2309.00445v2",
      "title": "A Phenomenological Approach to Interactive Knot Diagrams",
      "summary": "Knot diagrams are among the most common visual tools in topology. Computer programs now make it possible to draw, manipulate and render them digitally, which proves to be useful in knot theory teaching and research. Still, an openly available tool to manipulate knot diagrams in a real-time, interactive way is yet to be developed. We introduce a method of operating on the geometry of the knot diagram itself without any underlying three-dimensional structure that can underpin such an application. This allows us to directly interact with vector graphics knot diagrams while at the same time computing knot invariants in ways proposed by previous work. An implementation of this method is provided.",
      "published": "2023-09-01T13:22:16Z"
    },
    "metadata": {
      "arxiv_id": "2309.00445",
      "title": "A Phenomenological Approach to Interactive Knot Diagrams",
      "summary": "Knot diagrams are among the most common visual tools in topology. Computer programs now make it possible to draw, manipulate and render them digitally, which proves to be useful in knot theory teaching and research. Still, an openly available tool to manipulate knot diagrams in a real-time, interactive way is yet to be developed. We introduce a method of operating on the geometry of the knot diagram itself without any underlying three-dimensional structure that can underpin such an application. This allows us to directly interact with vector graphics knot diagrams while at the same time computing knot invariants in ways proposed by previous work. An implementation of this method is provided.",
      "authors": [
        "Lennart Finke",
        "Edmund Weitz"
      ],
      "published": "2023-09-01T13:22:16Z",
      "updated": "2024-01-03T13:49:12Z",
      "categories": [
        "cs.HC"
      ],
      "pdf_url": "https://arxiv.org/pdf/2309.00445v2",
      "landing_url": "https://arxiv.org/abs/2309.00445v2",
      "doi": "https://doi.org/10.1109/tvcg.2024.3405369"
    },
    "queries": [
      {
        "anchor": "synchronous dialogue",
        "search_term": "real-time interaction"
      }
    ]
  },
  {
    "arxiv_id": "2309.01481",
    "anchor": "full-duplex",
    "search_term": "half duplex",
    "search_record": {
      "id": "http://arxiv.org/abs/2309.01481v3",
      "title": "Half-Duplex APs with Dynamic TDD vs. Full-Duplex APs in Cell-Free Systems",
      "summary": "In this paper, we present a comparative study of half-duplex (HD) access points (APs) with dynamic time-division duplex (DTDD) and full-duplex (FD) APs in cell-free (CF) systems. Although both DTDD and FD CF systems support concurrent downlink (DL) transmission and uplink (UL) reception capability, the sum spectral efficiency (SE) is limited by various cross-link interferences. We first present a novel pilot allocation scheme that minimizes the pilot length required to ensure no pilot contamination among the user equipments (UEs) served by at least one common AP. Then, we derive the sum SE in closed form, considering zero-forcing combining and precoding along with the signal-to-interference plus noise ratio optimal weighting at the central processing unit. We also present a provably convergent algorithm for joint UL-DL power allocation and UL/DL mode scheduling of the APs (for DTDD) to maximize the sum SE. Further, the proposed algorithms are precoder and combiner agnostic and come with closed-form update equations for the UL and DL power control coefficients. Our numerical results illustrate the superiority of the proposed pilot allocation and power control algorithms over several benchmark schemes and show that the sum SE with DTDD can outperform an FD CF system with similar antenna density. Thus, DTDD combined with CF is a promising alternative to FD that attains the same performance using HD APs, while obviating the burden of intra-AP interference cancellation.",
      "published": "2023-09-04T09:36:51Z"
    },
    "metadata": {
      "arxiv_id": "2309.01481",
      "title": "Half-Duplex APs with Dynamic TDD vs. Full-Duplex APs in Cell-Free Systems",
      "summary": "In this paper, we present a comparative study of half-duplex (HD) access points (APs) with dynamic time-division duplex (DTDD) and full-duplex (FD) APs in cell-free (CF) systems. Although both DTDD and FD CF systems support concurrent downlink (DL) transmission and uplink (UL) reception capability, the sum spectral efficiency (SE) is limited by various cross-link interferences. We first present a novel pilot allocation scheme that minimizes the pilot length required to ensure no pilot contamination among the user equipments (UEs) served by at least one common AP. Then, we derive the sum SE in closed form, considering zero-forcing combining and precoding along with the signal-to-interference plus noise ratio optimal weighting at the central processing unit. We also present a provably convergent algorithm for joint UL-DL power allocation and UL/DL mode scheduling of the APs (for DTDD) to maximize the sum SE. Further, the proposed algorithms are precoder and combiner agnostic and come with closed-form update equations for the UL and DL power control coefficients. Our numerical results illustrate the superiority of the proposed pilot allocation and power control algorithms over several benchmark schemes and show that the sum SE with DTDD can outperform an FD CF system with similar antenna density. Thus, DTDD combined with CF is a promising alternative to FD that attains the same performance using HD APs, while obviating the burden of intra-AP interference cancellation.",
      "authors": [
        "Anubhab Chowdhury",
        "Chandra R. Murthy"
      ],
      "published": "2023-09-04T09:36:51Z",
      "updated": "2024-01-31T08:48:10Z",
      "categories": [
        "eess.SP"
      ],
      "pdf_url": "https://arxiv.org/pdf/2309.01481v3",
      "landing_url": "https://arxiv.org/abs/2309.01481v3",
      "doi": "https://doi.org/10.48550/arXiv.2309.01481"
    },
    "queries": [
      {
        "anchor": "full-duplex",
        "search_term": "half duplex"
      },
      {
        "anchor": "synchronous dialogue",
        "search_term": "half duplex"
      }
    ]
  },
  {
    "arxiv_id": "2309.01587",
    "anchor": "full-duplex",
    "search_term": "streaming architecture",
    "search_record": {
      "id": "http://arxiv.org/abs/2309.01587v1",
      "title": "SATAY: A Streaming Architecture Toolflow for Accelerating YOLO Models on FPGA Devices",
      "summary": "AI has led to significant advancements in computer vision and image processing tasks, enabling a wide range of applications in real-life scenarios, from autonomous vehicles to medical imaging. Many of those applications require efficient object detection algorithms and complementary real-time, low latency hardware to perform inference of these algorithms. The YOLO family of models is considered the most efficient for object detection, having only a single model pass. Despite this, the complexity and size of YOLO models can be too computationally demanding for current edge-based platforms. To address this, we present SATAY: a Streaming Architecture Toolflow for Accelerating YOLO. This work tackles the challenges of deploying stateof-the-art object detection models onto FPGA devices for ultralow latency applications, enabling real-time, edge-based object detection. We employ a streaming architecture design for our YOLO accelerators, implementing the complete model on-chip in a deeply pipelined fashion. These accelerators are generated using an automated toolflow, and can target a range of suitable FPGA devices. We introduce novel hardware components to support the operations of YOLO models in a dataflow manner, and off-chip memory buffering to address the limited on-chip memory resources. Our toolflow is able to generate accelerator designs which demonstrate competitive performance and energy characteristics to GPU devices, and which outperform current state-of-the-art FPGA accelerators.",
      "published": "2023-09-04T13:15:01Z"
    },
    "metadata": {
      "arxiv_id": "2309.01587",
      "title": "SATAY: A Streaming Architecture Toolflow for Accelerating YOLO Models on FPGA Devices",
      "summary": "AI has led to significant advancements in computer vision and image processing tasks, enabling a wide range of applications in real-life scenarios, from autonomous vehicles to medical imaging. Many of those applications require efficient object detection algorithms and complementary real-time, low latency hardware to perform inference of these algorithms. The YOLO family of models is considered the most efficient for object detection, having only a single model pass. Despite this, the complexity and size of YOLO models can be too computationally demanding for current edge-based platforms. To address this, we present SATAY: a Streaming Architecture Toolflow for Accelerating YOLO. This work tackles the challenges of deploying stateof-the-art object detection models onto FPGA devices for ultralow latency applications, enabling real-time, edge-based object detection. We employ a streaming architecture design for our YOLO accelerators, implementing the complete model on-chip in a deeply pipelined fashion. These accelerators are generated using an automated toolflow, and can target a range of suitable FPGA devices. We introduce novel hardware components to support the operations of YOLO models in a dataflow manner, and off-chip memory buffering to address the limited on-chip memory resources. Our toolflow is able to generate accelerator designs which demonstrate competitive performance and energy characteristics to GPU devices, and which outperform current state-of-the-art FPGA accelerators.",
      "authors": [
        "Alexander Montgomerie-Corcoran",
        "Petros Toupas",
        "Zhewen Yu",
        "Christos-Savvas Bouganis"
      ],
      "published": "2023-09-04T13:15:01Z",
      "updated": "2023-09-04T13:15:01Z",
      "categories": [
        "cs.AR",
        "cs.CV",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2309.01587v1",
      "landing_url": "https://arxiv.org/abs/2309.01587v1",
      "doi": "https://doi.org/10.48550/arXiv.2309.01587"
    },
    "queries": [
      {
        "anchor": "full-duplex",
        "search_term": "streaming architecture"
      },
      {
        "anchor": "spoken language models",
        "search_term": "streaming architecture"
      },
      {
        "anchor": "synchronous dialogue",
        "search_term": "streaming architecture"
      },
      {
        "anchor": "turn-taking",
        "search_term": "streaming architecture"
      }
    ]
  },
  {
    "arxiv_id": "2309.07377",
    "anchor": "full-duplex",
    "search_term": "discrete tokens",
    "search_record": {
      "id": "http://arxiv.org/abs/2309.07377v2",
      "title": "Towards Universal Speech Discrete Tokens: A Case Study for ASR and TTS",
      "summary": "Self-supervised learning (SSL) proficiency in speech-related tasks has driven research into utilizing discrete tokens for speech tasks like recognition and translation, which offer lower storage requirements and great potential to employ natural language processing techniques. However, these studies, mainly single-task focused, faced challenges like overfitting and performance degradation in speech recognition tasks, often at the cost of sacrificing performance in multi-task scenarios. This study presents a comprehensive comparison and optimization of discrete tokens generated by various leading SSL models in speech recognition and synthesis tasks. We aim to explore the universality of speech discrete tokens across multiple speech tasks. Experimental results demonstrate that discrete tokens achieve comparable results against systems trained on FBank features in speech recognition tasks and outperform mel-spectrogram features in speech synthesis in subjective and objective metrics. These findings suggest that universal discrete tokens have enormous potential in various speech-related tasks. Our work is open-source and publicly available at https://github.com/k2-fsa/icefall.",
      "published": "2023-09-14T01:39:43Z"
    },
    "metadata": {
      "arxiv_id": "2309.07377",
      "title": "Towards Universal Speech Discrete Tokens: A Case Study for ASR and TTS",
      "summary": "Self-supervised learning (SSL) proficiency in speech-related tasks has driven research into utilizing discrete tokens for speech tasks like recognition and translation, which offer lower storage requirements and great potential to employ natural language processing techniques. However, these studies, mainly single-task focused, faced challenges like overfitting and performance degradation in speech recognition tasks, often at the cost of sacrificing performance in multi-task scenarios. This study presents a comprehensive comparison and optimization of discrete tokens generated by various leading SSL models in speech recognition and synthesis tasks. We aim to explore the universality of speech discrete tokens across multiple speech tasks. Experimental results demonstrate that discrete tokens achieve comparable results against systems trained on FBank features in speech recognition tasks and outperform mel-spectrogram features in speech synthesis in subjective and objective metrics. These findings suggest that universal discrete tokens have enormous potential in various speech-related tasks. Our work is open-source and publicly available at https://github.com/k2-fsa/icefall.",
      "authors": [
        "Yifan Yang",
        "Feiyu Shen",
        "Chenpeng Du",
        "Ziyang Ma",
        "Kai Yu",
        "Daniel Povey",
        "Xie Chen"
      ],
      "published": "2023-09-14T01:39:43Z",
      "updated": "2023-12-14T13:18:35Z",
      "categories": [
        "eess.AS",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2309.07377v2",
      "landing_url": "https://arxiv.org/abs/2309.07377v2",
      "doi": "https://doi.org/10.48550/arXiv.2309.07377"
    },
    "queries": [
      {
        "anchor": "full-duplex",
        "search_term": "discrete tokens"
      },
      {
        "anchor": "spoken language models",
        "search_term": "discrete tokens"
      },
      {
        "anchor": "synchronous dialogue",
        "search_term": "discrete tokens"
      },
      {
        "anchor": "turn-taking",
        "search_term": "discrete tokens"
      }
    ]
  },
  {
    "arxiv_id": "2309.07391",
    "anchor": "synchronous dialogue",
    "search_term": "neural codec",
    "search_record": {
      "id": "http://arxiv.org/abs/2309.07391v2",
      "title": "EnCodecMAE: Leveraging neural codecs for universal audio representation learning",
      "summary": "The goal of universal audio representation learning is to obtain foundational models that can be used for a variety of downstream tasks involving speech, music and environmental sounds. To approach this problem, methods inspired by works on self-supervised learning for NLP, like BERT, or computer vision, like masked autoencoders (MAE), are often adapted to the audio domain. In this work, we propose masking representations of the audio signal, and training a MAE to reconstruct the masked segments. The reconstruction is done by predicting the discrete units generated by EnCodec, a neural audio codec, from the unmasked inputs. We evaluate this approach, which we call EnCodecMAE, on a wide range of tasks involving speech, music and environmental sounds. Our best model outperforms various state-of-the-art audio representation models in terms of global performance. Additionally, we evaluate the resulting representations in the challenging task of automatic speech recognition (ASR), obtaining decent results and paving the way for a universal audio representation.",
      "published": "2023-09-14T02:21:53Z"
    },
    "metadata": {
      "arxiv_id": "2309.07391",
      "title": "EnCodecMAE: Leveraging neural codecs for universal audio representation learning",
      "summary": "The goal of universal audio representation learning is to obtain foundational models that can be used for a variety of downstream tasks involving speech, music and environmental sounds. To approach this problem, methods inspired by works on self-supervised learning for NLP, like BERT, or computer vision, like masked autoencoders (MAE), are often adapted to the audio domain. In this work, we propose masking representations of the audio signal, and training a MAE to reconstruct the masked segments. The reconstruction is done by predicting the discrete units generated by EnCodec, a neural audio codec, from the unmasked inputs. We evaluate this approach, which we call EnCodecMAE, on a wide range of tasks involving speech, music and environmental sounds. Our best model outperforms various state-of-the-art audio representation models in terms of global performance. Additionally, we evaluate the resulting representations in the challenging task of automatic speech recognition (ASR), obtaining decent results and paving the way for a universal audio representation.",
      "authors": [
        "Leonardo Pepino",
        "Pablo Riera",
        "Luciana Ferrer"
      ],
      "published": "2023-09-14T02:21:53Z",
      "updated": "2024-05-21T00:39:47Z",
      "categories": [
        "cs.SD",
        "cs.LG",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2309.07391v2",
      "landing_url": "https://arxiv.org/abs/2309.07391v2",
      "doi": "https://doi.org/10.48550/arXiv.2309.07391"
    },
    "queries": [
      {
        "anchor": "synchronous dialogue",
        "search_term": "neural codec"
      }
    ]
  },
  {
    "arxiv_id": "2309.07416",
    "anchor": "spoken language models",
    "search_term": "neural codec",
    "search_record": {
      "id": "http://arxiv.org/abs/2309.07416v4",
      "title": "BANC: Towards Efficient Binaural Audio Neural Codec for Overlapping Speech",
      "summary": "We introduce BANC, a neural binaural audio codec designed for efficient speech compression in single and two-speaker scenarios while preserving the spatial location information of each speaker. Our key contributions are as follows: 1) The ability of our proposed model to compress and decode overlapping speech. 2) A novel architecture that compresses speech content and spatial cues separately, ensuring the preservation of each speaker's spatial context after decoding. 3) BANC's proficiency in reducing the bandwidth required for compressing binaural speech by 48% compared to compressing individual binaural channels. In our evaluation, we employed speech enhancement, room acoustics, and perceptual metrics to assess the accuracy of BANC's clean speech and spatial cue estimates.",
      "published": "2023-09-14T04:04:50Z"
    },
    "metadata": {
      "arxiv_id": "2309.07416",
      "title": "BANC: Towards Efficient Binaural Audio Neural Codec for Overlapping Speech",
      "summary": "We introduce BANC, a neural binaural audio codec designed for efficient speech compression in single and two-speaker scenarios while preserving the spatial location information of each speaker. Our key contributions are as follows: 1) The ability of our proposed model to compress and decode overlapping speech. 2) A novel architecture that compresses speech content and spatial cues separately, ensuring the preservation of each speaker's spatial context after decoding. 3) BANC's proficiency in reducing the bandwidth required for compressing binaural speech by 48% compared to compressing individual binaural channels. In our evaluation, we employed speech enhancement, room acoustics, and perceptual metrics to assess the accuracy of BANC's clean speech and spatial cue estimates.",
      "authors": [
        "Anton Ratnarajah",
        "Shi-Xiong Zhang",
        "Dong Yu"
      ],
      "published": "2023-09-14T04:04:50Z",
      "updated": "2024-11-25T03:50:32Z",
      "categories": [
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2309.07416v4",
      "landing_url": "https://arxiv.org/abs/2309.07416v4",
      "doi": "https://doi.org/10.48550/arXiv.2309.07416"
    },
    "queries": [
      {
        "anchor": "spoken language models",
        "search_term": "neural codec"
      },
      {
        "anchor": "synchronous dialogue",
        "search_term": "neural codec"
      },
      {
        "anchor": "turn-taking",
        "search_term": "neural codec"
      }
    ]
  },
  {
    "arxiv_id": "2309.08099",
    "anchor": "spoken language models",
    "search_term": "temporal dynamics",
    "search_record": {
      "id": "http://arxiv.org/abs/2309.08099v1",
      "title": "Characterizing the temporal dynamics of universal speech representations for generalizable deepfake detection",
      "summary": "Existing deepfake speech detection systems lack generalizability to unseen attacks (i.e., samples generated by generative algorithms not seen during training). Recent studies have explored the use of universal speech representations to tackle this issue and have obtained inspiring results. These works, however, have focused on innovating downstream classifiers while leaving the representation itself untouched. In this study, we argue that characterizing the long-term temporal dynamics of these representations is crucial for generalizability and propose a new method to assess representation dynamics. Indeed, we show that different generative models generate similar representation dynamics patterns with our proposed method. Experiments on the ASVspoof 2019 and 2021 datasets validate the benefits of the proposed method to detect deepfakes from methods unseen during training, significantly improving on several benchmark methods.",
      "published": "2023-09-15T01:37:45Z"
    },
    "metadata": {
      "arxiv_id": "2309.08099",
      "title": "Characterizing the temporal dynamics of universal speech representations for generalizable deepfake detection",
      "summary": "Existing deepfake speech detection systems lack generalizability to unseen attacks (i.e., samples generated by generative algorithms not seen during training). Recent studies have explored the use of universal speech representations to tackle this issue and have obtained inspiring results. These works, however, have focused on innovating downstream classifiers while leaving the representation itself untouched. In this study, we argue that characterizing the long-term temporal dynamics of these representations is crucial for generalizability and propose a new method to assess representation dynamics. Indeed, we show that different generative models generate similar representation dynamics patterns with our proposed method. Experiments on the ASVspoof 2019 and 2021 datasets validate the benefits of the proposed method to detect deepfakes from methods unseen during training, significantly improving on several benchmark methods.",
      "authors": [
        "Yi Zhu",
        "Saurabh Powar",
        "Tiago H. Falk"
      ],
      "published": "2023-09-15T01:37:45Z",
      "updated": "2023-09-15T01:37:45Z",
      "categories": [
        "cs.SD",
        "cs.CL",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2309.08099v1",
      "landing_url": "https://arxiv.org/abs/2309.08099v1",
      "doi": "https://doi.org/10.48550/arXiv.2309.08099"
    },
    "queries": [
      {
        "anchor": "spoken language models",
        "search_term": "temporal dynamics"
      },
      {
        "anchor": "synchronous dialogue",
        "search_term": "temporal dynamics"
      },
      {
        "anchor": "turn-taking",
        "search_term": "temporal dynamics"
      }
    ]
  },
  {
    "arxiv_id": "2309.08531",
    "anchor": "spoken language models",
    "search_term": "true full-duplex",
    "search_record": {
      "id": "http://arxiv.org/abs/2309.08531v1",
      "title": "Towards Practical and Efficient Image-to-Speech Captioning with Vision-Language Pre-training and Multi-modal Tokens",
      "summary": "In this paper, we propose methods to build a powerful and efficient Image-to-Speech captioning (Im2Sp) model. To this end, we start with importing the rich knowledge related to image comprehension and language modeling from a large-scale pre-trained vision-language model into Im2Sp. We set the output of the proposed Im2Sp as discretized speech units, i.e., the quantized speech features of a self-supervised speech model. The speech units mainly contain linguistic information while suppressing other characteristics of speech. This allows us to incorporate the language modeling capability of the pre-trained vision-language model into the spoken language modeling of Im2Sp. With the vision-language pre-training strategy, we set new state-of-the-art Im2Sp performances on two widely used benchmark databases, COCO and Flickr8k. Then, we further improve the efficiency of the Im2Sp model. Similar to the speech unit case, we convert the original image into image units, which are derived through vector quantization of the raw image. With these image units, we can drastically reduce the required data storage for saving image data to just 0.8% when compared to the original image data in terms of bits. Demo page: https://ms-dot-k.github.io/Image-to-Speech-Captioning.",
      "published": "2023-09-15T16:48:34Z"
    },
    "metadata": {
      "arxiv_id": "2309.08531",
      "title": "Towards Practical and Efficient Image-to-Speech Captioning with Vision-Language Pre-training and Multi-modal Tokens",
      "summary": "In this paper, we propose methods to build a powerful and efficient Image-to-Speech captioning (Im2Sp) model. To this end, we start with importing the rich knowledge related to image comprehension and language modeling from a large-scale pre-trained vision-language model into Im2Sp. We set the output of the proposed Im2Sp as discretized speech units, i.e., the quantized speech features of a self-supervised speech model. The speech units mainly contain linguistic information while suppressing other characteristics of speech. This allows us to incorporate the language modeling capability of the pre-trained vision-language model into the spoken language modeling of Im2Sp. With the vision-language pre-training strategy, we set new state-of-the-art Im2Sp performances on two widely used benchmark databases, COCO and Flickr8k. Then, we further improve the efficiency of the Im2Sp model. Similar to the speech unit case, we convert the original image into image units, which are derived through vector quantization of the raw image. With these image units, we can drastically reduce the required data storage for saving image data to just 0.8% when compared to the original image data in terms of bits. Demo page: https://ms-dot-k.github.io/Image-to-Speech-Captioning.",
      "authors": [
        "Minsu Kim",
        "Jeongsoo Choi",
        "Soumi Maiti",
        "Jeong Hun Yeo",
        "Shinji Watanabe",
        "Yong Man Ro"
      ],
      "published": "2023-09-15T16:48:34Z",
      "updated": "2023-09-15T16:48:34Z",
      "categories": [
        "cs.CV",
        "cs.CL",
        "eess.AS",
        "eess.IV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2309.08531v1",
      "landing_url": "https://arxiv.org/abs/2309.08531v1",
      "doi": "https://doi.org/10.48550/arXiv.2309.08531"
    },
    "queries": [
      {
        "anchor": "spoken language models",
        "search_term": "true full-duplex"
      },
      {
        "anchor": "spoken language models",
        "search_term": "pseudo full-duplex"
      },
      {
        "anchor": "spoken language models",
        "search_term": "engineered synchronization"
      },
      {
        "anchor": "spoken language models",
        "search_term": "learned synchronization"
      },
      {
        "anchor": "spoken language models",
        "search_term": "synchronous data"
      },
      {
        "anchor": "spoken language models",
        "search_term": "multichannel dialogue"
      },
      {
        "anchor": "spoken language models",
        "search_term": "behavioral arbitration"
      },
      {
        "anchor": "spoken language models",
        "search_term": "floor transfer offset"
      },
      {
        "anchor": "spoken language models",
        "search_term": "stop latency"
      },
      {
        "anchor": "spoken language models",
        "search_term": "interruption response delay"
      }
    ]
  },
  {
    "arxiv_id": "2309.08773",
    "anchor": "spoken language models",
    "search_term": "audio tokens",
    "search_record": {
      "id": "http://arxiv.org/abs/2309.08773v1",
      "title": "Enhance audio generation controllability through representation similarity regularization",
      "summary": "This paper presents an innovative approach to enhance control over audio generation by emphasizing the alignment between audio and text representations during model training. In the context of language model-based audio generation, the model leverages input from both textual and audio token representations to predict subsequent audio tokens. However, the current configuration lacks explicit regularization to ensure the alignment between the chosen text representation and the language model's predictions. Our proposal involves the incorporation of audio and text representation regularization, particularly during the classifier-free guidance (CFG) phase, where the text condition is excluded from cross attention during language model training. The aim of this proposed representation regularization is to minimize discrepancies in audio and text similarity compared to other samples within the same training batch. Experimental results on both music and audio generation tasks demonstrate that our proposed methods lead to improvements in objective metrics for both audio and music generation, as well as an enhancement in the human perception for audio generation.",
      "published": "2023-09-15T21:32:20Z"
    },
    "metadata": {
      "arxiv_id": "2309.08773",
      "title": "Enhance audio generation controllability through representation similarity regularization",
      "summary": "This paper presents an innovative approach to enhance control over audio generation by emphasizing the alignment between audio and text representations during model training. In the context of language model-based audio generation, the model leverages input from both textual and audio token representations to predict subsequent audio tokens. However, the current configuration lacks explicit regularization to ensure the alignment between the chosen text representation and the language model's predictions. Our proposal involves the incorporation of audio and text representation regularization, particularly during the classifier-free guidance (CFG) phase, where the text condition is excluded from cross attention during language model training. The aim of this proposed representation regularization is to minimize discrepancies in audio and text similarity compared to other samples within the same training batch. Experimental results on both music and audio generation tasks demonstrate that our proposed methods lead to improvements in objective metrics for both audio and music generation, as well as an enhancement in the human perception for audio generation.",
      "authors": [
        "Yangyang Shi",
        "Gael Le Lan",
        "Varun Nagaraja",
        "Zhaoheng Ni",
        "Xinhao Mei",
        "Ernie Chang",
        "Forrest Iandola",
        "Yang Liu",
        "Vikas Chandra"
      ],
      "published": "2023-09-15T21:32:20Z",
      "updated": "2023-09-15T21:32:20Z",
      "categories": [
        "cs.SD",
        "cs.AI",
        "cs.LG",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2309.08773v1",
      "landing_url": "https://arxiv.org/abs/2309.08773v1",
      "doi": "https://doi.org/10.48550/arXiv.2309.08773"
    },
    "queries": [
      {
        "anchor": "spoken language models",
        "search_term": "audio tokens"
      },
      {
        "anchor": "synchronous dialogue",
        "search_term": "audio tokens"
      },
      {
        "anchor": "turn-taking",
        "search_term": "audio tokens"
      }
    ]
  },
  {
    "arxiv_id": "2309.09262",
    "anchor": "synchronous dialogue",
    "search_term": "discrete tokens",
    "search_record": {
      "id": "http://arxiv.org/abs/2309.09262v2",
      "title": "PromptVC: Flexible Stylistic Voice Conversion in Latent Space Driven by Natural Language Prompts",
      "summary": "Style voice conversion aims to transform the style of source speech to a desired style according to real-world application demands. However, the current style voice conversion approach relies on pre-defined labels or reference speech to control the conversion process, which leads to limitations in style diversity or falls short in terms of the intuitive and interpretability of style representation. In this study, we propose PromptVC, a novel style voice conversion approach that employs a latent diffusion model to generate a style vector driven by natural language prompts. Specifically, the style vector is extracted by a style encoder during training, and then the latent diffusion model is trained independently to sample the style vector from noise, with this process being conditioned on natural language prompts. To improve style expressiveness, we leverage HuBERT to extract discrete tokens and replace them with the K-Means center embedding to serve as the linguistic content, which minimizes residual style information. Additionally, we deduplicate the same discrete token and employ a differentiable duration predictor to re-predict the duration of each token, which can adapt the duration of the same linguistic content to different styles. The subjective and objective evaluation results demonstrate the effectiveness of our proposed system.",
      "published": "2023-09-17T12:58:27Z"
    },
    "metadata": {
      "arxiv_id": "2309.09262",
      "title": "PromptVC: Flexible Stylistic Voice Conversion in Latent Space Driven by Natural Language Prompts",
      "summary": "Style voice conversion aims to transform the style of source speech to a desired style according to real-world application demands. However, the current style voice conversion approach relies on pre-defined labels or reference speech to control the conversion process, which leads to limitations in style diversity or falls short in terms of the intuitive and interpretability of style representation. In this study, we propose PromptVC, a novel style voice conversion approach that employs a latent diffusion model to generate a style vector driven by natural language prompts. Specifically, the style vector is extracted by a style encoder during training, and then the latent diffusion model is trained independently to sample the style vector from noise, with this process being conditioned on natural language prompts. To improve style expressiveness, we leverage HuBERT to extract discrete tokens and replace them with the K-Means center embedding to serve as the linguistic content, which minimizes residual style information. Additionally, we deduplicate the same discrete token and employ a differentiable duration predictor to re-predict the duration of each token, which can adapt the duration of the same linguistic content to different styles. The subjective and objective evaluation results demonstrate the effectiveness of our proposed system.",
      "authors": [
        "Jixun Yao",
        "Yuguang Yang",
        "Yi Lei",
        "Ziqian Ning",
        "Yanni Hu",
        "Yu Pan",
        "Jingjing Yin",
        "Hongbin Zhou",
        "Heng Lu",
        "Lei Xie"
      ],
      "published": "2023-09-17T12:58:27Z",
      "updated": "2023-12-26T07:48:05Z",
      "categories": [
        "eess.AS",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2309.09262v2",
      "landing_url": "https://arxiv.org/abs/2309.09262v2",
      "doi": "https://doi.org/10.48550/arXiv.2309.09262"
    },
    "queries": [
      {
        "anchor": "synchronous dialogue",
        "search_term": "discrete tokens"
      }
    ]
  },
  {
    "arxiv_id": "2309.10537",
    "anchor": "spoken language models",
    "search_term": "audio tokens",
    "search_record": {
      "id": "http://arxiv.org/abs/2309.10537v1",
      "title": "FoleyGen: Visually-Guided Audio Generation",
      "summary": "Recent advancements in audio generation have been spurred by the evolution of large-scale deep learning models and expansive datasets. However, the task of video-to-audio (V2A) generation continues to be a challenge, principally because of the intricate relationship between the high-dimensional visual and auditory data, and the challenges associated with temporal synchronization. In this study, we introduce FoleyGen, an open-domain V2A generation system built on a language modeling paradigm. FoleyGen leverages an off-the-shelf neural audio codec for bidirectional conversion between waveforms and discrete tokens. The generation of audio tokens is facilitated by a single Transformer model, which is conditioned on visual features extracted from a visual encoder. A prevalent problem in V2A generation is the misalignment of generated audio with the visible actions in the video. To address this, we explore three novel visual attention mechanisms. We further undertake an exhaustive evaluation of multiple visual encoders, each pretrained on either single-modal or multi-modal tasks. The experimental results on VGGSound dataset show that our proposed FoleyGen outperforms previous systems across all objective metrics and human evaluations.",
      "published": "2023-09-19T11:33:43Z"
    },
    "metadata": {
      "arxiv_id": "2309.10537",
      "title": "FoleyGen: Visually-Guided Audio Generation",
      "summary": "Recent advancements in audio generation have been spurred by the evolution of large-scale deep learning models and expansive datasets. However, the task of video-to-audio (V2A) generation continues to be a challenge, principally because of the intricate relationship between the high-dimensional visual and auditory data, and the challenges associated with temporal synchronization. In this study, we introduce FoleyGen, an open-domain V2A generation system built on a language modeling paradigm. FoleyGen leverages an off-the-shelf neural audio codec for bidirectional conversion between waveforms and discrete tokens. The generation of audio tokens is facilitated by a single Transformer model, which is conditioned on visual features extracted from a visual encoder. A prevalent problem in V2A generation is the misalignment of generated audio with the visible actions in the video. To address this, we explore three novel visual attention mechanisms. We further undertake an exhaustive evaluation of multiple visual encoders, each pretrained on either single-modal or multi-modal tasks. The experimental results on VGGSound dataset show that our proposed FoleyGen outperforms previous systems across all objective metrics and human evaluations.",
      "authors": [
        "Xinhao Mei",
        "Varun Nagaraja",
        "Gael Le Lan",
        "Zhaoheng Ni",
        "Ernie Chang",
        "Yangyang Shi",
        "Vikas Chandra"
      ],
      "published": "2023-09-19T11:33:43Z",
      "updated": "2023-09-19T11:33:43Z",
      "categories": [
        "eess.AS",
        "cs.MM",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2309.10537v1",
      "landing_url": "https://arxiv.org/abs/2309.10537v1",
      "doi": "https://doi.org/10.48550/arXiv.2309.10537"
    },
    "queries": [
      {
        "anchor": "spoken language models",
        "search_term": "audio tokens"
      },
      {
        "anchor": "synchronous dialogue",
        "search_term": "audio tokens"
      },
      {
        "anchor": "turn-taking",
        "search_term": "audio tokens"
      }
    ]
  },
  {
    "arxiv_id": "2309.10922",
    "anchor": "spoken language models",
    "search_term": "audio tokens",
    "search_record": {
      "id": "http://arxiv.org/abs/2309.10922v1",
      "title": "Discrete Audio Representation as an Alternative to Mel-Spectrograms for Speaker and Speech Recognition",
      "summary": "Discrete audio representation, aka audio tokenization, has seen renewed interest driven by its potential to facilitate the application of text language modeling approaches in audio domain. To this end, various compression and representation-learning based tokenization schemes have been proposed. However, there is limited investigation into the performance of compression-based audio tokens compared to well-established mel-spectrogram features across various speaker and speech related tasks. In this paper, we evaluate compression based audio tokens on three tasks: Speaker Verification, Diarization and (Multi-lingual) Speech Recognition. Our findings indicate that (i) the models trained on audio tokens perform competitively, on average within $1\\%$ of mel-spectrogram features for all the tasks considered, and do not surpass them yet. (ii) these models exhibit robustness for out-of-domain narrowband data, particularly in speaker tasks. (iii) audio tokens allow for compression to 20x compared to mel-spectrogram features with minimal loss of performance in speech and speaker related tasks, which is crucial for low bit-rate applications, and (iv) the examined Residual Vector Quantization (RVQ) based audio tokenizer exhibits a low-pass frequency response characteristic, offering a plausible explanation for the observed results, and providing insight for future tokenizer designs.",
      "published": "2023-09-19T20:49:05Z"
    },
    "metadata": {
      "arxiv_id": "2309.10922",
      "title": "Discrete Audio Representation as an Alternative to Mel-Spectrograms for Speaker and Speech Recognition",
      "summary": "Discrete audio representation, aka audio tokenization, has seen renewed interest driven by its potential to facilitate the application of text language modeling approaches in audio domain. To this end, various compression and representation-learning based tokenization schemes have been proposed. However, there is limited investigation into the performance of compression-based audio tokens compared to well-established mel-spectrogram features across various speaker and speech related tasks. In this paper, we evaluate compression based audio tokens on three tasks: Speaker Verification, Diarization and (Multi-lingual) Speech Recognition. Our findings indicate that (i) the models trained on audio tokens perform competitively, on average within $1\\%$ of mel-spectrogram features for all the tasks considered, and do not surpass them yet. (ii) these models exhibit robustness for out-of-domain narrowband data, particularly in speaker tasks. (iii) audio tokens allow for compression to 20x compared to mel-spectrogram features with minimal loss of performance in speech and speaker related tasks, which is crucial for low bit-rate applications, and (iv) the examined Residual Vector Quantization (RVQ) based audio tokenizer exhibits a low-pass frequency response characteristic, offering a plausible explanation for the observed results, and providing insight for future tokenizer designs.",
      "authors": [
        "Krishna C. Puvvada",
        "Nithin Rao Koluguri",
        "Kunal Dhawan",
        "Jagadeesh Balam",
        "Boris Ginsburg"
      ],
      "published": "2023-09-19T20:49:05Z",
      "updated": "2023-09-19T20:49:05Z",
      "categories": [
        "eess.AS",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2309.10922v1",
      "landing_url": "https://arxiv.org/abs/2309.10922v1",
      "doi": "https://doi.org/10.48550/arXiv.2309.10922"
    },
    "queries": [
      {
        "anchor": "spoken language models",
        "search_term": "audio tokens"
      },
      {
        "anchor": "synchronous dialogue",
        "search_term": "audio tokens"
      },
      {
        "anchor": "turn-taking",
        "search_term": "audio tokens"
      }
    ]
  },
  {
    "arxiv_id": "2309.11977",
    "anchor": "synchronous dialogue",
    "search_term": "neural codec",
    "search_record": {
      "id": "http://arxiv.org/abs/2309.11977v3",
      "title": "Improving Language Model-Based Zero-Shot Text-to-Speech Synthesis with Multi-Scale Acoustic Prompts",
      "summary": "Zero-shot text-to-speech (TTS) synthesis aims to clone any unseen speaker's voice without adaptation parameters. By quantizing speech waveform into discrete acoustic tokens and modeling these tokens with the language model, recent language model-based TTS models show zero-shot speaker adaptation capabilities with only a 3-second acoustic prompt of an unseen speaker. However, they are limited by the length of the acoustic prompt, which makes it difficult to clone personal speaking style. In this paper, we propose a novel zero-shot TTS model with the multi-scale acoustic prompts based on a neural codec language model VALL-E. A speaker-aware text encoder is proposed to learn the personal speaking style at the phoneme-level from the style prompt consisting of multiple sentences. Following that, a VALL-E based acoustic decoder is utilized to model the timbre from the timbre prompt at the frame-level and generate speech. The experimental results show that our proposed method outperforms baselines in terms of naturalness and speaker similarity, and can achieve better performance by scaling out to a longer style prompt.",
      "published": "2023-09-21T11:22:22Z"
    },
    "metadata": {
      "arxiv_id": "2309.11977",
      "title": "Improving Language Model-Based Zero-Shot Text-to-Speech Synthesis with Multi-Scale Acoustic Prompts",
      "summary": "Zero-shot text-to-speech (TTS) synthesis aims to clone any unseen speaker's voice without adaptation parameters. By quantizing speech waveform into discrete acoustic tokens and modeling these tokens with the language model, recent language model-based TTS models show zero-shot speaker adaptation capabilities with only a 3-second acoustic prompt of an unseen speaker. However, they are limited by the length of the acoustic prompt, which makes it difficult to clone personal speaking style. In this paper, we propose a novel zero-shot TTS model with the multi-scale acoustic prompts based on a neural codec language model VALL-E. A speaker-aware text encoder is proposed to learn the personal speaking style at the phoneme-level from the style prompt consisting of multiple sentences. Following that, a VALL-E based acoustic decoder is utilized to model the timbre from the timbre prompt at the frame-level and generate speech. The experimental results show that our proposed method outperforms baselines in terms of naturalness and speaker similarity, and can achieve better performance by scaling out to a longer style prompt.",
      "authors": [
        "Shun Lei",
        "Yixuan Zhou",
        "Liyang Chen",
        "Dan Luo",
        "Zhiyong Wu",
        "Xixin Wu",
        "Shiyin Kang",
        "Tao Jiang",
        "Yahui Zhou",
        "Yuxing Han",
        "Helen Meng"
      ],
      "published": "2023-09-21T11:22:22Z",
      "updated": "2024-04-09T08:39:52Z",
      "categories": [
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2309.11977v3",
      "landing_url": "https://arxiv.org/abs/2309.11977v3",
      "doi": "https://doi.org/10.48550/arXiv.2309.11977"
    },
    "queries": [
      {
        "anchor": "synchronous dialogue",
        "search_term": "neural codec"
      }
    ]
  },
  {
    "arxiv_id": "2309.13321",
    "anchor": "spoken language models",
    "search_term": "streaming architecture",
    "search_record": {
      "id": "http://arxiv.org/abs/2309.13321v1",
      "title": "ONNX-to-Hardware Design Flow for the Generation of Adaptive Neural-Network Accelerators on FPGAs",
      "summary": "Neural Networks (NN) provide a solid and reliable way of executing different types of applications, ranging from speech recognition to medical diagnosis, speeding up onerous and long workloads. The challenges involved in their implementation at the edge include providing diversity, flexibility, and sustainability. That implies, for instance, supporting evolving applications and algorithms energy-efficiently. Using hardware or software accelerators can deliver fast and efficient computation of the \\acp{nn}, while flexibility can be exploited to support long-term adaptivity. Nonetheless, handcrafting an NN for a specific device, despite the possibility of leading to an optimal solution, takes time and experience, and that's why frameworks for hardware accelerators are being developed. This work-in-progress study focuses on exploring the possibility of combining the toolchain proposed by Ratto et al., which has the distinctive ability to favor adaptivity, with approximate computing. The goal will be to allow lightweight adaptable NN inference on FPGAs at the edge. Before that, the work presents a detailed review of established frameworks that adopt a similar streaming architecture for future comparison.",
      "published": "2023-09-23T09:41:43Z"
    },
    "metadata": {
      "arxiv_id": "2309.13321",
      "title": "ONNX-to-Hardware Design Flow for the Generation of Adaptive Neural-Network Accelerators on FPGAs",
      "summary": "Neural Networks (NN) provide a solid and reliable way of executing different types of applications, ranging from speech recognition to medical diagnosis, speeding up onerous and long workloads. The challenges involved in their implementation at the edge include providing diversity, flexibility, and sustainability. That implies, for instance, supporting evolving applications and algorithms energy-efficiently. Using hardware or software accelerators can deliver fast and efficient computation of the \\acp{nn}, while flexibility can be exploited to support long-term adaptivity. Nonetheless, handcrafting an NN for a specific device, despite the possibility of leading to an optimal solution, takes time and experience, and that's why frameworks for hardware accelerators are being developed. This work-in-progress study focuses on exploring the possibility of combining the toolchain proposed by Ratto et al., which has the distinctive ability to favor adaptivity, with approximate computing. The goal will be to allow lightweight adaptable NN inference on FPGAs at the edge. Before that, the work presents a detailed review of established frameworks that adopt a similar streaming architecture for future comparison.",
      "authors": [
        "Federico Manca",
        "Francesco Ratto"
      ],
      "published": "2023-09-23T09:41:43Z",
      "updated": "2023-09-23T09:41:43Z",
      "categories": [
        "cs.AR"
      ],
      "pdf_url": "https://arxiv.org/pdf/2309.13321v1",
      "landing_url": "https://arxiv.org/abs/2309.13321v1",
      "doi": "https://doi.org/10.48550/arXiv.2309.13321"
    },
    "queries": [
      {
        "anchor": "spoken language models",
        "search_term": "streaming architecture"
      },
      {
        "anchor": "synchronous dialogue",
        "search_term": "streaming architecture"
      },
      {
        "anchor": "turn-taking",
        "search_term": "streaming architecture"
      }
    ]
  },
  {
    "arxiv_id": "2309.14324",
    "anchor": "synchronous dialogue",
    "search_term": "neural codec",
    "search_record": {
      "id": "http://arxiv.org/abs/2309.14324v2",
      "title": "Towards General-Purpose Text-Instruction-Guided Voice Conversion",
      "summary": "This paper introduces a novel voice conversion (VC) model, guided by text instructions such as \"articulate slowly with a deep tone\" or \"speak in a cheerful boyish voice\". Unlike traditional methods that rely on reference utterances to determine the attributes of the converted speech, our model adds versatility and specificity to voice conversion. The proposed VC model is a neural codec language model which processes a sequence of discrete codes, resulting in the code sequence of converted speech. It utilizes text instructions as style prompts to modify the prosody and emotional information of the given speech. In contrast to previous approaches, which often rely on employing separate encoders like prosody and content encoders to handle different aspects of the source speech, our model handles various information of speech in an end-to-end manner. Experiments have demonstrated the impressive capabilities of our model in comprehending instructions and delivering reasonable results.",
      "published": "2023-09-25T17:52:09Z"
    },
    "metadata": {
      "arxiv_id": "2309.14324",
      "title": "Towards General-Purpose Text-Instruction-Guided Voice Conversion",
      "summary": "This paper introduces a novel voice conversion (VC) model, guided by text instructions such as \"articulate slowly with a deep tone\" or \"speak in a cheerful boyish voice\". Unlike traditional methods that rely on reference utterances to determine the attributes of the converted speech, our model adds versatility and specificity to voice conversion. The proposed VC model is a neural codec language model which processes a sequence of discrete codes, resulting in the code sequence of converted speech. It utilizes text instructions as style prompts to modify the prosody and emotional information of the given speech. In contrast to previous approaches, which often rely on employing separate encoders like prosody and content encoders to handle different aspects of the source speech, our model handles various information of speech in an end-to-end manner. Experiments have demonstrated the impressive capabilities of our model in comprehending instructions and delivering reasonable results.",
      "authors": [
        "Chun-Yi Kuan",
        "Chen An Li",
        "Tsu-Yuan Hsu",
        "Tse-Yang Lin",
        "Ho-Lam Chung",
        "Kai-Wei Chang",
        "Shuo-yiin Chang",
        "Hung-yi Lee"
      ],
      "published": "2023-09-25T17:52:09Z",
      "updated": "2024-01-16T13:53:56Z",
      "categories": [
        "eess.AS",
        "cs.CL",
        "cs.LG",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2309.14324v2",
      "landing_url": "https://arxiv.org/abs/2309.14324v2",
      "doi": "https://doi.org/10.48550/arXiv.2309.14324"
    },
    "queries": [
      {
        "anchor": "synchronous dialogue",
        "search_term": "neural codec"
      }
    ]
  },
  {
    "arxiv_id": "2309.14494",
    "anchor": "synchronous dialogue",
    "search_term": "semantic coherence",
    "search_record": {
      "id": "http://arxiv.org/abs/2309.14494v1",
      "title": "Free-Bloom: Zero-Shot Text-to-Video Generator with LLM Director and LDM Animator",
      "summary": "Text-to-video is a rapidly growing research area that aims to generate a semantic, identical, and temporal coherence sequence of frames that accurately align with the input text prompt. This study focuses on zero-shot text-to-video generation considering the data- and cost-efficient. To generate a semantic-coherent video, exhibiting a rich portrayal of temporal semantics such as the whole process of flower blooming rather than a set of \"moving images\", we propose a novel Free-Bloom pipeline that harnesses large language models (LLMs) as the director to generate a semantic-coherence prompt sequence, while pre-trained latent diffusion models (LDMs) as the animator to generate the high fidelity frames. Furthermore, to ensure temporal and identical coherence while maintaining semantic coherence, we propose a series of annotative modifications to adapting LDMs in the reverse process, including joint noise sampling, step-aware attention shift, and dual-path interpolation. Without any video data and training requirements, Free-Bloom generates vivid and high-quality videos, awe-inspiring in generating complex scenes with semantic meaningful frame sequences. In addition, Free-Bloom is naturally compatible with LDMs-based extensions.",
      "published": "2023-09-25T19:42:16Z"
    },
    "metadata": {
      "arxiv_id": "2309.14494",
      "title": "Free-Bloom: Zero-Shot Text-to-Video Generator with LLM Director and LDM Animator",
      "summary": "Text-to-video is a rapidly growing research area that aims to generate a semantic, identical, and temporal coherence sequence of frames that accurately align with the input text prompt. This study focuses on zero-shot text-to-video generation considering the data- and cost-efficient. To generate a semantic-coherent video, exhibiting a rich portrayal of temporal semantics such as the whole process of flower blooming rather than a set of \"moving images\", we propose a novel Free-Bloom pipeline that harnesses large language models (LLMs) as the director to generate a semantic-coherence prompt sequence, while pre-trained latent diffusion models (LDMs) as the animator to generate the high fidelity frames. Furthermore, to ensure temporal and identical coherence while maintaining semantic coherence, we propose a series of annotative modifications to adapting LDMs in the reverse process, including joint noise sampling, step-aware attention shift, and dual-path interpolation. Without any video data and training requirements, Free-Bloom generates vivid and high-quality videos, awe-inspiring in generating complex scenes with semantic meaningful frame sequences. In addition, Free-Bloom is naturally compatible with LDMs-based extensions.",
      "authors": [
        "Hanzhuo Huang",
        "Yufan Feng",
        "Cheng Shi",
        "Lan Xu",
        "Jingyi Yu",
        "Sibei Yang"
      ],
      "published": "2023-09-25T19:42:16Z",
      "updated": "2023-09-25T19:42:16Z",
      "categories": [
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2309.14494v1",
      "landing_url": "https://arxiv.org/abs/2309.14494v1",
      "doi": "https://doi.org/10.48550/arXiv.2309.14494"
    },
    "queries": [
      {
        "anchor": "synchronous dialogue",
        "search_term": "semantic coherence"
      }
    ]
  },
  {
    "arxiv_id": "2309.15730",
    "anchor": "spoken language models",
    "search_term": "temporal dynamics",
    "search_record": {
      "id": "http://arxiv.org/abs/2309.15730v3",
      "title": "Temporal graph models fail to capture global temporal dynamics",
      "summary": "A recently released Temporal Graph Benchmark is analyzed in the context of Dynamic Link Property Prediction. We outline our observations and propose a trivial optimization-free baseline of \"recently popular nodes\" outperforming other methods on medium and large-size datasets in the Temporal Graph Benchmark. We propose two measures based on Wasserstein distance which can quantify the strength of short-term and long-term global dynamics of datasets. By analyzing our unexpectedly strong baseline, we show how standard negative sampling evaluation can be unsuitable for datasets with strong temporal dynamics. We also show how simple negative-sampling can lead to model degeneration during training, resulting in impossible to rank, fully saturated predictions of temporal graph networks. We propose improved negative sampling schemes for both training and evaluation and prove their usefulness. We conduct a comparison with a model trained non-contrastively without negative sampling. Our results provide a challenging baseline and indicate that temporal graph network architectures need deep rethinking for usage in problems with significant global dynamics, such as social media, cryptocurrency markets or e-commerce. We open-source the code for baselines, measures and proposed negative sampling schemes.",
      "published": "2023-09-27T15:36:45Z"
    },
    "metadata": {
      "arxiv_id": "2309.15730",
      "title": "Temporal graph models fail to capture global temporal dynamics",
      "summary": "A recently released Temporal Graph Benchmark is analyzed in the context of Dynamic Link Property Prediction. We outline our observations and propose a trivial optimization-free baseline of \"recently popular nodes\" outperforming other methods on medium and large-size datasets in the Temporal Graph Benchmark. We propose two measures based on Wasserstein distance which can quantify the strength of short-term and long-term global dynamics of datasets. By analyzing our unexpectedly strong baseline, we show how standard negative sampling evaluation can be unsuitable for datasets with strong temporal dynamics. We also show how simple negative-sampling can lead to model degeneration during training, resulting in impossible to rank, fully saturated predictions of temporal graph networks. We propose improved negative sampling schemes for both training and evaluation and prove their usefulness. We conduct a comparison with a model trained non-contrastively without negative sampling. Our results provide a challenging baseline and indicate that temporal graph network architectures need deep rethinking for usage in problems with significant global dynamics, such as social media, cryptocurrency markets or e-commerce. We open-source the code for baselines, measures and proposed negative sampling schemes.",
      "authors": [
        "Michał Daniluk",
        "Jacek Dąbrowski"
      ],
      "published": "2023-09-27T15:36:45Z",
      "updated": "2023-12-08T13:54:55Z",
      "categories": [
        "cs.IR",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2309.15730v3",
      "landing_url": "https://arxiv.org/abs/2309.15730v3",
      "doi": "https://doi.org/10.48550/arXiv.2309.15730"
    },
    "queries": [
      {
        "anchor": "spoken language models",
        "search_term": "temporal dynamics"
      },
      {
        "anchor": "synchronous dialogue",
        "search_term": "temporal dynamics"
      },
      {
        "anchor": "turn-taking",
        "search_term": "temporal dynamics"
      }
    ]
  },
  {
    "arxiv_id": "2309.16365",
    "anchor": "synchronous dialogue",
    "search_term": "modular architecture",
    "search_record": {
      "id": "http://arxiv.org/abs/2309.16365v2",
      "title": "Libertas: Privacy-Preserving Collective Computation for Decentralised Personal Data Stores",
      "summary": "Data and data processing have become an indispensable aspect for our society. Insights drawn from collective data make invaluable contribution to scientific and societal research and business. But there are increasing worries about privacy issues and data misuse. This has prompted the emergence of decentralised personal data stores (PDS) like Solid that provide individuals more control over their personal data. However, existing PDS frameworks face challenges in ensuring data privacy when performing collective computations with data from multiple users. While Secure Multi-Party Computation (MPC) offers input secrecy protection during the computation without relying on any single party, issues emerge when directly applying MPC in the context of PDS, particularly due to key factors like autonomy and decentralisation. In this work, we discuss the essence of this issue, identify a potential solution, and introduce a modular architecture, Libertas, to integrate MPC with PDS like Solid, without requiring protocol-level changes. We introduce a paradigm shift from an `omniscient' view to individual-based, user-centric view of trust and security, and discuss the threat model of Libertas. Two realistic use cases for collaborative data processing are used for evaluation, both for technical feasibility and empirical benchmark, highlighting its effectiveness in empowering gig workers and generating differentially private synthetic data. The results of our experiments underscore Libertas' linear scalability and provide valuable insights into compute optimisations, thereby advancing the state-of-the-art in privacy-preserving data processing practices. By offering practical solutions for maintaining both individual autonomy and privacy in collaborative data processing environments, Libertas contributes significantly to the ongoing discourse on privacy protection in data-driven decision-making contexts.",
      "published": "2023-09-28T12:07:40Z"
    },
    "metadata": {
      "arxiv_id": "2309.16365",
      "title": "Libertas: Privacy-Preserving Collective Computation for Decentralised Personal Data Stores",
      "summary": "Data and data processing have become an indispensable aspect for our society. Insights drawn from collective data make invaluable contribution to scientific and societal research and business. But there are increasing worries about privacy issues and data misuse. This has prompted the emergence of decentralised personal data stores (PDS) like Solid that provide individuals more control over their personal data. However, existing PDS frameworks face challenges in ensuring data privacy when performing collective computations with data from multiple users. While Secure Multi-Party Computation (MPC) offers input secrecy protection during the computation without relying on any single party, issues emerge when directly applying MPC in the context of PDS, particularly due to key factors like autonomy and decentralisation. In this work, we discuss the essence of this issue, identify a potential solution, and introduce a modular architecture, Libertas, to integrate MPC with PDS like Solid, without requiring protocol-level changes. We introduce a paradigm shift from an `omniscient' view to individual-based, user-centric view of trust and security, and discuss the threat model of Libertas. Two realistic use cases for collaborative data processing are used for evaluation, both for technical feasibility and empirical benchmark, highlighting its effectiveness in empowering gig workers and generating differentially private synthetic data. The results of our experiments underscore Libertas' linear scalability and provide valuable insights into compute optimisations, thereby advancing the state-of-the-art in privacy-preserving data processing practices. By offering practical solutions for maintaining both individual autonomy and privacy in collaborative data processing environments, Libertas contributes significantly to the ongoing discourse on privacy protection in data-driven decision-making contexts.",
      "authors": [
        "Rui Zhao",
        "Naman Goel",
        "Nitin Agrawal",
        "Jun Zhao",
        "Jake Stein",
        "Wael Albayaydh",
        "Ruben Verborgh",
        "Reuben Binns",
        "Tim Berners-Lee",
        "Nigel Shadbolt"
      ],
      "published": "2023-09-28T12:07:40Z",
      "updated": "2025-03-30T21:35:47Z",
      "categories": [
        "cs.NI",
        "cs.CR",
        "cs.DC"
      ],
      "pdf_url": "https://arxiv.org/pdf/2309.16365v2",
      "landing_url": "https://arxiv.org/abs/2309.16365v2",
      "doi": "https://doi.org/10.48550/arXiv.2309.16365"
    },
    "queries": [
      {
        "anchor": "synchronous dialogue",
        "search_term": "modular architecture"
      }
    ]
  },
  {
    "arxiv_id": "2309.17024",
    "anchor": "synchronous dialogue",
    "search_term": "synchronous data",
    "search_record": {
      "id": "http://arxiv.org/abs/2309.17024v1",
      "title": "HoloAssist: an Egocentric Human Interaction Dataset for Interactive AI Assistants in the Real World",
      "summary": "Building an interactive AI assistant that can perceive, reason, and collaborate with humans in the real world has been a long-standing pursuit in the AI community. This work is part of a broader research effort to develop intelligent agents that can interactively guide humans through performing tasks in the physical world. As a first step in this direction, we introduce HoloAssist, a large-scale egocentric human interaction dataset, where two people collaboratively complete physical manipulation tasks. The task performer executes the task while wearing a mixed-reality headset that captures seven synchronized data streams. The task instructor watches the performer's egocentric video in real time and guides them verbally. By augmenting the data with action and conversational annotations and observing the rich behaviors of various participants, we present key insights into how human assistants correct mistakes, intervene in the task completion procedure, and ground their instructions to the environment. HoloAssist spans 166 hours of data captured by 350 unique instructor-performer pairs. Furthermore, we construct and present benchmarks on mistake detection, intervention type prediction, and hand forecasting, along with detailed analysis. We expect HoloAssist will provide an important resource for building AI assistants that can fluidly collaborate with humans in the real world. Data can be downloaded at https://holoassist.github.io/.",
      "published": "2023-09-29T07:17:43Z"
    },
    "metadata": {
      "arxiv_id": "2309.17024",
      "title": "HoloAssist: an Egocentric Human Interaction Dataset for Interactive AI Assistants in the Real World",
      "summary": "Building an interactive AI assistant that can perceive, reason, and collaborate with humans in the real world has been a long-standing pursuit in the AI community. This work is part of a broader research effort to develop intelligent agents that can interactively guide humans through performing tasks in the physical world. As a first step in this direction, we introduce HoloAssist, a large-scale egocentric human interaction dataset, where two people collaboratively complete physical manipulation tasks. The task performer executes the task while wearing a mixed-reality headset that captures seven synchronized data streams. The task instructor watches the performer's egocentric video in real time and guides them verbally. By augmenting the data with action and conversational annotations and observing the rich behaviors of various participants, we present key insights into how human assistants correct mistakes, intervene in the task completion procedure, and ground their instructions to the environment. HoloAssist spans 166 hours of data captured by 350 unique instructor-performer pairs. Furthermore, we construct and present benchmarks on mistake detection, intervention type prediction, and hand forecasting, along with detailed analysis. We expect HoloAssist will provide an important resource for building AI assistants that can fluidly collaborate with humans in the real world. Data can be downloaded at https://holoassist.github.io/.",
      "authors": [
        "Xin Wang",
        "Taein Kwon",
        "Mahdi Rad",
        "Bowen Pan",
        "Ishani Chakraborty",
        "Sean Andrist",
        "Dan Bohus",
        "Ashley Feniello",
        "Bugra Tekin",
        "Felipe Vieira Frujeri",
        "Neel Joshi",
        "Marc Pollefeys"
      ],
      "published": "2023-09-29T07:17:43Z",
      "updated": "2023-09-29T07:17:43Z",
      "categories": [
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2309.17024v1",
      "landing_url": "https://arxiv.org/abs/2309.17024v1",
      "doi": "https://doi.org/10.48550/arXiv.2309.17024"
    },
    "queries": [
      {
        "anchor": "synchronous dialogue",
        "search_term": "synchronous data"
      }
    ]
  },
  {
    "arxiv_id": "2310.00092",
    "anchor": "spoken language models",
    "search_term": "real-time interaction",
    "search_record": {
      "id": "http://arxiv.org/abs/2310.00092v1",
      "title": "Voice2Action: Language Models as Agent for Efficient Real-Time Interaction in Virtual Reality",
      "summary": "Large Language Models (LLMs) are trained and aligned to follow natural language instructions with only a handful of examples, and they are prompted as task-driven autonomous agents to adapt to various sources of execution environments. However, deploying agent LLMs in virtual reality (VR) has been challenging due to the lack of efficiency in online interactions and the complex manipulation categories in 3D environments. In this work, we propose Voice2Action, a framework that hierarchically analyzes customized voice signals and textual commands through action and entity extraction and divides the execution tasks into canonical interaction subsets in real-time with error prevention from environment feedback. Experiment results in an urban engineering VR environment with synthetic instruction data show that Voice2Action can perform more efficiently and accurately than approaches without optimizations.",
      "published": "2023-09-29T19:06:52Z"
    },
    "metadata": {
      "arxiv_id": "2310.00092",
      "title": "Voice2Action: Language Models as Agent for Efficient Real-Time Interaction in Virtual Reality",
      "summary": "Large Language Models (LLMs) are trained and aligned to follow natural language instructions with only a handful of examples, and they are prompted as task-driven autonomous agents to adapt to various sources of execution environments. However, deploying agent LLMs in virtual reality (VR) has been challenging due to the lack of efficiency in online interactions and the complex manipulation categories in 3D environments. In this work, we propose Voice2Action, a framework that hierarchically analyzes customized voice signals and textual commands through action and entity extraction and divides the execution tasks into canonical interaction subsets in real-time with error prevention from environment feedback. Experiment results in an urban engineering VR environment with synthetic instruction data show that Voice2Action can perform more efficiently and accurately than approaches without optimizations.",
      "authors": [
        "Yang Su"
      ],
      "published": "2023-09-29T19:06:52Z",
      "updated": "2023-09-29T19:06:52Z",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.HC"
      ],
      "pdf_url": "https://arxiv.org/pdf/2310.00092v1",
      "landing_url": "https://arxiv.org/abs/2310.00092v1",
      "doi": "https://doi.org/10.48550/arXiv.2310.00092"
    },
    "queries": [
      {
        "anchor": "spoken language models",
        "search_term": "real-time interaction"
      },
      {
        "anchor": "synchronous dialogue",
        "search_term": "real-time interaction"
      },
      {
        "anchor": "turn-taking",
        "search_term": "real-time interaction"
      }
    ]
  },
  {
    "arxiv_id": "2310.00559",
    "anchor": "synchronous dialogue",
    "search_term": "neural codec",
    "search_record": {
      "id": "http://arxiv.org/abs/2310.00559v1",
      "title": "CPIPS: Learning to Preserve Perceptual Distances in End-to-End Image Compression",
      "summary": "Lossy image coding standards such as JPEG and MPEG have successfully achieved high compression rates for human consumption of multimedia data. However, with the increasing prevalence of IoT devices, drones, and self-driving cars, machines rather than humans are processing a greater portion of captured visual content. Consequently, it is crucial to pursue an efficient compressed representation that caters not only to human vision but also to image processing and machine vision tasks. Drawing inspiration from the efficient coding hypothesis in biological systems and the modeling of the sensory cortex in neural science, we repurpose the compressed latent representation to prioritize semantic relevance while preserving perceptual distance. Our proposed method, Compressed Perceptual Image Patch Similarity (CPIPS), can be derived at a minimal cost from a learned neural codec and computed significantly faster than DNN-based perceptual metrics such as LPIPS and DISTS.",
      "published": "2023-10-01T03:29:21Z"
    },
    "metadata": {
      "arxiv_id": "2310.00559",
      "title": "CPIPS: Learning to Preserve Perceptual Distances in End-to-End Image Compression",
      "summary": "Lossy image coding standards such as JPEG and MPEG have successfully achieved high compression rates for human consumption of multimedia data. However, with the increasing prevalence of IoT devices, drones, and self-driving cars, machines rather than humans are processing a greater portion of captured visual content. Consequently, it is crucial to pursue an efficient compressed representation that caters not only to human vision but also to image processing and machine vision tasks. Drawing inspiration from the efficient coding hypothesis in biological systems and the modeling of the sensory cortex in neural science, we repurpose the compressed latent representation to prioritize semantic relevance while preserving perceptual distance. Our proposed method, Compressed Perceptual Image Patch Similarity (CPIPS), can be derived at a minimal cost from a learned neural codec and computed significantly faster than DNN-based perceptual metrics such as LPIPS and DISTS.",
      "authors": [
        "Chen-Hsiu Huang",
        "Ja-Ling Wu"
      ],
      "published": "2023-10-01T03:29:21Z",
      "updated": "2023-10-01T03:29:21Z",
      "categories": [
        "cs.CV",
        "cs.MM",
        "eess.IV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2310.00559v1",
      "landing_url": "https://arxiv.org/abs/2310.00559v1",
      "doi": "https://doi.org/10.48550/arXiv.2310.00559"
    },
    "queries": [
      {
        "anchor": "synchronous dialogue",
        "search_term": "neural codec"
      }
    ]
  },
  {
    "arxiv_id": "2310.00704",
    "anchor": "synchronous dialogue",
    "search_term": "neural codec",
    "search_record": {
      "id": "http://arxiv.org/abs/2310.00704v6",
      "title": "UniAudio: An Audio Foundation Model Toward Universal Audio Generation",
      "summary": "Large Language models (LLM) have demonstrated the capability to handle a variety of generative tasks. This paper presents the UniAudio system, which, unlike prior task-specific approaches, leverages LLM techniques to generate multiple types of audio (including speech, sounds, music, and singing) with given input conditions. UniAudio 1) first tokenizes all types of target audio along with other condition modalities, 2) concatenates source-target pair as a single sequence, and 3) performs next-token prediction using LLM. Also, a multi-scale Transformer model is proposed to handle the overly long sequences caused by the residual vector quantization based neural codec in tokenization. Training of UniAudio is scaled up to 165K hours of audio and 1B parameters, based on all generative tasks, aiming to obtain sufficient prior knowledge not only in the intrinsic properties of audio but also the inter-relationship between audio and other modalities. Therefore, the trained UniAudio model has the potential to become a foundation model for universal audio generation: it shows strong capability in all trained tasks and can seamlessly support new audio generation tasks after simple fine-tuning. Experiments demonstrate that UniAudio achieves state-of-the-art or at least competitive results on most of the 11 tasks. Demo and code are released at https://github.com/yangdongchao/UniAudio",
      "published": "2023-10-01T15:49:46Z"
    },
    "metadata": {
      "arxiv_id": "2310.00704",
      "title": "UniAudio: An Audio Foundation Model Toward Universal Audio Generation",
      "summary": "Large Language models (LLM) have demonstrated the capability to handle a variety of generative tasks. This paper presents the UniAudio system, which, unlike prior task-specific approaches, leverages LLM techniques to generate multiple types of audio (including speech, sounds, music, and singing) with given input conditions. UniAudio 1) first tokenizes all types of target audio along with other condition modalities, 2) concatenates source-target pair as a single sequence, and 3) performs next-token prediction using LLM. Also, a multi-scale Transformer model is proposed to handle the overly long sequences caused by the residual vector quantization based neural codec in tokenization. Training of UniAudio is scaled up to 165K hours of audio and 1B parameters, based on all generative tasks, aiming to obtain sufficient prior knowledge not only in the intrinsic properties of audio but also the inter-relationship between audio and other modalities. Therefore, the trained UniAudio model has the potential to become a foundation model for universal audio generation: it shows strong capability in all trained tasks and can seamlessly support new audio generation tasks after simple fine-tuning. Experiments demonstrate that UniAudio achieves state-of-the-art or at least competitive results on most of the 11 tasks. Demo and code are released at https://github.com/yangdongchao/UniAudio",
      "authors": [
        "Dongchao Yang",
        "Jinchuan Tian",
        "Xu Tan",
        "Rongjie Huang",
        "Songxiang Liu",
        "Xuankai Chang",
        "Jiatong Shi",
        "Sheng Zhao",
        "Jiang Bian",
        "Zhou Zhao",
        "Xixin Wu",
        "Helen Meng"
      ],
      "published": "2023-10-01T15:49:46Z",
      "updated": "2024-12-10T03:17:56Z",
      "categories": [
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2310.00704v6",
      "landing_url": "https://arxiv.org/abs/2310.00704v6",
      "doi": "https://doi.org/10.48550/arXiv.2310.00704"
    },
    "queries": [
      {
        "anchor": "synchronous dialogue",
        "search_term": "neural codec"
      }
    ]
  },
  {
    "arxiv_id": "2310.01088",
    "anchor": "turn-taking",
    "search_term": "true full-duplex",
    "search_record": {
      "id": "http://arxiv.org/abs/2310.01088v1",
      "title": "Towards human-like spoken dialogue generation between AI agents from written dialogue",
      "summary": "The advent of large language models (LLMs) has made it possible to generate natural written dialogues between two agents. However, generating human-like spoken dialogues from these written dialogues remains challenging. Spoken dialogues have several unique characteristics: they frequently include backchannels and laughter, and the smoothness of turn-taking significantly influences the fluidity of conversation. This study proposes CHATS - CHatty Agents Text-to-Speech - a discrete token-based system designed to generate spoken dialogues based on written dialogues. Our system can generate speech for both the speaker side and the listener side simultaneously, using only the transcription from the speaker side, which eliminates the need for transcriptions of backchannels or laughter. Moreover, CHATS facilitates natural turn-taking; it determines the appropriate duration of silence after each utterance in the absence of overlap, and it initiates the generation of overlapping speech based on the phoneme sequence of the next utterance in case of overlap. Experimental evaluations indicate that CHATS outperforms the text-to-speech baseline, producing spoken dialogues that are more interactive and fluid while retaining clarity and intelligibility.",
      "published": "2023-10-02T11:03:20Z"
    },
    "metadata": {
      "arxiv_id": "2310.01088",
      "title": "Towards human-like spoken dialogue generation between AI agents from written dialogue",
      "summary": "The advent of large language models (LLMs) has made it possible to generate natural written dialogues between two agents. However, generating human-like spoken dialogues from these written dialogues remains challenging. Spoken dialogues have several unique characteristics: they frequently include backchannels and laughter, and the smoothness of turn-taking significantly influences the fluidity of conversation. This study proposes CHATS - CHatty Agents Text-to-Speech - a discrete token-based system designed to generate spoken dialogues based on written dialogues. Our system can generate speech for both the speaker side and the listener side simultaneously, using only the transcription from the speaker side, which eliminates the need for transcriptions of backchannels or laughter. Moreover, CHATS facilitates natural turn-taking; it determines the appropriate duration of silence after each utterance in the absence of overlap, and it initiates the generation of overlapping speech based on the phoneme sequence of the next utterance in case of overlap. Experimental evaluations indicate that CHATS outperforms the text-to-speech baseline, producing spoken dialogues that are more interactive and fluid while retaining clarity and intelligibility.",
      "authors": [
        "Kentaro Mitsui",
        "Yukiya Hono",
        "Kei Sawada"
      ],
      "published": "2023-10-02T11:03:20Z",
      "updated": "2023-10-02T11:03:20Z",
      "categories": [
        "cs.CL",
        "cs.LG",
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2310.01088v1",
      "landing_url": "https://arxiv.org/abs/2310.01088v1",
      "doi": "https://doi.org/10.48550/arXiv.2310.01088"
    },
    "queries": [
      {
        "anchor": "turn-taking",
        "search_term": "true full-duplex"
      },
      {
        "anchor": "turn-taking",
        "search_term": "pseudo full-duplex"
      },
      {
        "anchor": "turn-taking",
        "search_term": "engineered synchronization"
      },
      {
        "anchor": "turn-taking",
        "search_term": "learned synchronization"
      },
      {
        "anchor": "turn-taking",
        "search_term": "streaming architecture"
      },
      {
        "anchor": "turn-taking",
        "search_term": "real-time interaction"
      },
      {
        "anchor": "turn-taking",
        "search_term": "synchronous data"
      },
      {
        "anchor": "turn-taking",
        "search_term": "multichannel dialogue"
      },
      {
        "anchor": "turn-taking",
        "search_term": "behavioral arbitration"
      },
      {
        "anchor": "turn-taking",
        "search_term": "discrete tokens"
      },
      {
        "anchor": "turn-taking",
        "search_term": "floor transfer offset"
      },
      {
        "anchor": "turn-taking",
        "search_term": "stop latency"
      },
      {
        "anchor": "turn-taking",
        "search_term": "interruption response delay"
      }
    ]
  },
  {
    "arxiv_id": "2310.02177",
    "anchor": "synchronous dialogue",
    "search_term": "temporal dynamics",
    "search_record": {
      "id": "http://arxiv.org/abs/2310.02177v4",
      "title": "Simultaneous inference for monotone and smoothly time-varying functions under complex temporal dynamics",
      "summary": "We propose a new framework for the simultaneous inference of monotone and smoothly time-varying functions under complex temporal dynamics. This will be done utilizing the monotone rearrangement and the nonparametric estimation. We capitalize the Gaussian approximation for the nonparametric monotone estimator and construct the asymptotically correct simultaneous confidence bands (SCBs) using designed bootstrap methods. We investigate two general and practical scenarios. The first is the simultaneous inference of monotone smooth trends from moderately high-dimensional time series. The proposed algorithm has been employed for the joint inference of temperature curves from multiple areas. Specifically, most existing methods are designed for a single monotone smooth trend. In such cases, our proposed SCB empirically exhibits the narrowest width among existing approaches while maintaining confidence levels. It has also been used for testing several hypotheses tailored to global warming. The second scenario involves simultaneous inference of monotone and smoothly time-varying regression coefficients in time-varying coefficient linear models. The proposed algorithm has been utilized for testing the impact of sunshine duration on temperature which is believed to be increasing due to severe greenhouse effect. The validity of the proposed methods has been justified in theory as well as by extensive simulations.",
      "published": "2023-10-03T16:10:23Z"
    },
    "metadata": {
      "arxiv_id": "2310.02177",
      "title": "Simultaneous inference for monotone and smoothly time-varying functions under complex temporal dynamics",
      "summary": "We propose a new framework for the simultaneous inference of monotone and smoothly time-varying functions under complex temporal dynamics. This will be done utilizing the monotone rearrangement and the nonparametric estimation. We capitalize the Gaussian approximation for the nonparametric monotone estimator and construct the asymptotically correct simultaneous confidence bands (SCBs) using designed bootstrap methods. We investigate two general and practical scenarios. The first is the simultaneous inference of monotone smooth trends from moderately high-dimensional time series. The proposed algorithm has been employed for the joint inference of temperature curves from multiple areas. Specifically, most existing methods are designed for a single monotone smooth trend. In such cases, our proposed SCB empirically exhibits the narrowest width among existing approaches while maintaining confidence levels. It has also been used for testing several hypotheses tailored to global warming. The second scenario involves simultaneous inference of monotone and smoothly time-varying regression coefficients in time-varying coefficient linear models. The proposed algorithm has been utilized for testing the impact of sunshine duration on temperature which is believed to be increasing due to severe greenhouse effect. The validity of the proposed methods has been justified in theory as well as by extensive simulations.",
      "authors": [
        "Tianpai Luo",
        "Weichi Wu"
      ],
      "published": "2023-10-03T16:10:23Z",
      "updated": "2025-08-19T03:53:04Z",
      "categories": [
        "math.ST"
      ],
      "pdf_url": "https://arxiv.org/pdf/2310.02177v4",
      "landing_url": "https://arxiv.org/abs/2310.02177v4",
      "doi": "https://doi.org/10.48550/arXiv.2310.02177"
    },
    "queries": [
      {
        "anchor": "synchronous dialogue",
        "search_term": "temporal dynamics"
      }
    ]
  },
  {
    "arxiv_id": "2310.02518",
    "anchor": "spoken language models",
    "search_term": "temporal dynamics",
    "search_record": {
      "id": "http://arxiv.org/abs/2310.02518v1",
      "title": "Shaping the Epochal Individuality and Generality: The Temporal Dynamics of Uncertainty and Prediction Error in Musical Improvisation",
      "summary": "Musical improvisation, much like spontaneous speech, reveals intricate facets of the improviser's state of mind and emotional character. However, the specific musical components that reveal such individuality remain largely unexplored. Within the framework of brain's statistical learning and predictive processing, this study examined the temporal dynamics of uncertainty and surprise (prediction error) in a piece of musical improvisation. This study employed the HBSL model to analyze a corpus of 456 Jazz improvisations, spanning 1905 to 2009, from 78 distinct Jazz musicians. The results indicated distinctive temporal patterns of surprise and uncertainty, especially in pitch and pitch-rhythm sequences, revealing era-specific features from the early 20th to the 21st centuries. Conversely, rhythm sequences exhibited a consistent degree of uncertainty across eras. Further, the acoustic properties remain unchanged across different periods. These findings highlight the importance of how temporal dynamics of surprise and uncertainty in improvisational music change over periods, profoundly influencing the distinctive methodologies artists adopt for improvisation in each era. Further, it is suggested that the development of improvisational music can be attributed to the brain's adaptive statistical learning mechanisms, which constantly refine internal models to mirror the cultural and emotional nuances of their respective epochs. This study unravels the evolutionary trajectory of improvisational music and highlights the nuanced shifts artists employ to resonate with the cultural and emotional landscapes of their times.",
      "published": "2023-10-04T01:33:26Z"
    },
    "metadata": {
      "arxiv_id": "2310.02518",
      "title": "Shaping the Epochal Individuality and Generality: The Temporal Dynamics of Uncertainty and Prediction Error in Musical Improvisation",
      "summary": "Musical improvisation, much like spontaneous speech, reveals intricate facets of the improviser's state of mind and emotional character. However, the specific musical components that reveal such individuality remain largely unexplored. Within the framework of brain's statistical learning and predictive processing, this study examined the temporal dynamics of uncertainty and surprise (prediction error) in a piece of musical improvisation. This study employed the HBSL model to analyze a corpus of 456 Jazz improvisations, spanning 1905 to 2009, from 78 distinct Jazz musicians. The results indicated distinctive temporal patterns of surprise and uncertainty, especially in pitch and pitch-rhythm sequences, revealing era-specific features from the early 20th to the 21st centuries. Conversely, rhythm sequences exhibited a consistent degree of uncertainty across eras. Further, the acoustic properties remain unchanged across different periods. These findings highlight the importance of how temporal dynamics of surprise and uncertainty in improvisational music change over periods, profoundly influencing the distinctive methodologies artists adopt for improvisation in each era. Further, it is suggested that the development of improvisational music can be attributed to the brain's adaptive statistical learning mechanisms, which constantly refine internal models to mirror the cultural and emotional nuances of their respective epochs. This study unravels the evolutionary trajectory of improvisational music and highlights the nuanced shifts artists employ to resonate with the cultural and emotional landscapes of their times.",
      "authors": [
        "Tatsuya Daikoku"
      ],
      "published": "2023-10-04T01:33:26Z",
      "updated": "2023-10-04T01:33:26Z",
      "categories": [
        "cs.SD",
        "cs.IR",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2310.02518v1",
      "landing_url": "https://arxiv.org/abs/2310.02518v1",
      "doi": "https://doi.org/10.48550/arXiv.2310.02518"
    },
    "queries": [
      {
        "anchor": "spoken language models",
        "search_term": "temporal dynamics"
      },
      {
        "anchor": "synchronous dialogue",
        "search_term": "temporal dynamics"
      },
      {
        "anchor": "turn-taking",
        "search_term": "temporal dynamics"
      }
    ]
  },
  {
    "arxiv_id": "2310.04673",
    "anchor": "synchronous dialogue",
    "search_term": "audio tokens",
    "search_record": {
      "id": "http://arxiv.org/abs/2310.04673v4",
      "title": "LauraGPT: Listen, Attend, Understand, and Regenerate Audio with GPT",
      "summary": "Generative Pre-trained Transformer (GPT) models have achieved remarkable performance on various natural language processing tasks, and have shown great potential as backbones for audio-and-text large language models (LLMs). Previous mainstream audio-and-text LLMs use discrete audio tokens to represent both input and output audio; however, they suffer from performance degradation on tasks such as automatic speech recognition, speech-to-text translation, and speech enhancement over models using continuous speech features. In this paper, we propose LauraGPT, a novel unified audio-and-text GPT-based LLM for audio recognition, understanding, and generation. LauraGPT is a versatile LLM that can process both audio and text inputs and generate outputs in either modalities. We propose a novel data representation that combines continuous and discrete features for audio: LauraGPT encodes input audio into continuous representations using an audio encoder and generates output audio from discrete codec codes. We propose a one-step codec vocoder to overcome the prediction challenge caused by the multimodal distribution of codec tokens. We fine-tune LauraGPT using supervised multi-task learning. Extensive experiments show that LauraGPT consistently achieves comparable to superior performance compared to strong baselines on a wide range of audio tasks related to content, semantics, paralinguistics, and audio-signal analysis, such as automatic speech recognition, speech-to-text translation, text-to-speech synthesis, speech enhancement, automated audio captioning, speech emotion recognition, and spoken language understanding.",
      "published": "2023-10-07T03:17:59Z"
    },
    "metadata": {
      "arxiv_id": "2310.04673",
      "title": "LauraGPT: Listen, Attend, Understand, and Regenerate Audio with GPT",
      "summary": "Generative Pre-trained Transformer (GPT) models have achieved remarkable performance on various natural language processing tasks, and have shown great potential as backbones for audio-and-text large language models (LLMs). Previous mainstream audio-and-text LLMs use discrete audio tokens to represent both input and output audio; however, they suffer from performance degradation on tasks such as automatic speech recognition, speech-to-text translation, and speech enhancement over models using continuous speech features. In this paper, we propose LauraGPT, a novel unified audio-and-text GPT-based LLM for audio recognition, understanding, and generation. LauraGPT is a versatile LLM that can process both audio and text inputs and generate outputs in either modalities. We propose a novel data representation that combines continuous and discrete features for audio: LauraGPT encodes input audio into continuous representations using an audio encoder and generates output audio from discrete codec codes. We propose a one-step codec vocoder to overcome the prediction challenge caused by the multimodal distribution of codec tokens. We fine-tune LauraGPT using supervised multi-task learning. Extensive experiments show that LauraGPT consistently achieves comparable to superior performance compared to strong baselines on a wide range of audio tasks related to content, semantics, paralinguistics, and audio-signal analysis, such as automatic speech recognition, speech-to-text translation, text-to-speech synthesis, speech enhancement, automated audio captioning, speech emotion recognition, and spoken language understanding.",
      "authors": [
        "Zhihao Du",
        "Jiaming Wang",
        "Qian Chen",
        "Yunfei Chu",
        "Zhifu Gao",
        "Zerui Li",
        "Kai Hu",
        "Xiaohuan Zhou",
        "Jin Xu",
        "Ziyang Ma",
        "Wen Wang",
        "Siqi Zheng",
        "Chang Zhou",
        "Zhijie Yan",
        "Shiliang Zhang"
      ],
      "published": "2023-10-07T03:17:59Z",
      "updated": "2024-07-03T02:38:03Z",
      "categories": [
        "cs.SD",
        "cs.AI",
        "cs.LG",
        "cs.MM",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2310.04673v4",
      "landing_url": "https://arxiv.org/abs/2310.04673v4",
      "doi": "https://doi.org/10.48550/arXiv.2310.04673"
    },
    "queries": [
      {
        "anchor": "synchronous dialogue",
        "search_term": "audio tokens"
      }
    ]
  },
  {
    "arxiv_id": "2310.05224",
    "anchor": "spoken language models",
    "search_term": "true full-duplex",
    "search_record": {
      "id": "http://arxiv.org/abs/2310.05224v1",
      "title": "Generative Spoken Language Model based on continuous word-sized audio tokens",
      "summary": "In NLP, text language models based on words or subwords are known to outperform their character-based counterparts. Yet, in the speech community, the standard input of spoken LMs are 20ms or 40ms-long discrete units (shorter than a phoneme). Taking inspiration from word-based LM, we introduce a Generative Spoken Language Model (GSLM) based on word-size continuous-valued audio embeddings that can generate diverse and expressive language output. This is obtained by replacing lookup table for lexical types with a Lexical Embedding function, the cross entropy loss by a contrastive loss, and multinomial sampling by k-NN sampling. The resulting model is the first generative language model based on word-size continuous embeddings. Its performance is on par with discrete unit GSLMs regarding generation quality as measured by automatic metrics and subjective human judgements. Moreover, it is five times more memory efficient thanks to its large 200ms units. In addition, the embeddings before and after the Lexical Embedder are phonetically and semantically interpretable.",
      "published": "2023-10-08T16:46:14Z"
    },
    "metadata": {
      "arxiv_id": "2310.05224",
      "title": "Generative Spoken Language Model based on continuous word-sized audio tokens",
      "summary": "In NLP, text language models based on words or subwords are known to outperform their character-based counterparts. Yet, in the speech community, the standard input of spoken LMs are 20ms or 40ms-long discrete units (shorter than a phoneme). Taking inspiration from word-based LM, we introduce a Generative Spoken Language Model (GSLM) based on word-size continuous-valued audio embeddings that can generate diverse and expressive language output. This is obtained by replacing lookup table for lexical types with a Lexical Embedding function, the cross entropy loss by a contrastive loss, and multinomial sampling by k-NN sampling. The resulting model is the first generative language model based on word-size continuous embeddings. Its performance is on par with discrete unit GSLMs regarding generation quality as measured by automatic metrics and subjective human judgements. Moreover, it is five times more memory efficient thanks to its large 200ms units. In addition, the embeddings before and after the Lexical Embedder are phonetically and semantically interpretable.",
      "authors": [
        "Robin Algayres",
        "Yossi Adi",
        "Tu Anh Nguyen",
        "Jade Copet",
        "Gabriel Synnaeve",
        "Benoit Sagot",
        "Emmanuel Dupoux"
      ],
      "published": "2023-10-08T16:46:14Z",
      "updated": "2023-10-08T16:46:14Z",
      "categories": [
        "cs.CL",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2310.05224v1",
      "landing_url": "https://arxiv.org/abs/2310.05224v1",
      "doi": "https://doi.org/10.48550/arXiv.2310.05224"
    },
    "queries": [
      {
        "anchor": "spoken language models",
        "search_term": "true full-duplex"
      },
      {
        "anchor": "spoken language models",
        "search_term": "pseudo full-duplex"
      },
      {
        "anchor": "spoken language models",
        "search_term": "half duplex"
      },
      {
        "anchor": "spoken language models",
        "search_term": "engineered synchronization"
      },
      {
        "anchor": "spoken language models",
        "search_term": "learned synchronization"
      },
      {
        "anchor": "spoken language models",
        "search_term": "modular architecture"
      },
      {
        "anchor": "spoken language models",
        "search_term": "streaming architecture"
      },
      {
        "anchor": "spoken language models",
        "search_term": "real-time interaction"
      },
      {
        "anchor": "spoken language models",
        "search_term": "low latency"
      },
      {
        "anchor": "spoken language models",
        "search_term": "synchronous data"
      },
      {
        "anchor": "spoken language models",
        "search_term": "data scarcity"
      },
      {
        "anchor": "spoken language models",
        "search_term": "multichannel dialogue"
      },
      {
        "anchor": "spoken language models",
        "search_term": "temporal dynamics"
      },
      {
        "anchor": "spoken language models",
        "search_term": "behavioral arbitration"
      },
      {
        "anchor": "spoken language models",
        "search_term": "semantic coherence"
      },
      {
        "anchor": "spoken language models",
        "search_term": "neural codec"
      },
      {
        "anchor": "spoken language models",
        "search_term": "discrete tokens"
      },
      {
        "anchor": "spoken language models",
        "search_term": "audio tokens"
      },
      {
        "anchor": "spoken language models",
        "search_term": "floor transfer offset"
      },
      {
        "anchor": "spoken language models",
        "search_term": "stop latency"
      },
      {
        "anchor": "spoken language models",
        "search_term": "interruption response delay"
      },
      {
        "anchor": "synchronous dialogue",
        "search_term": "audio tokens"
      },
      {
        "anchor": "turn-taking",
        "search_term": "audio tokens"
      }
    ]
  },
  {
    "arxiv_id": "2310.08981",
    "anchor": "spoken language models",
    "search_term": "low latency",
    "search_record": {
      "id": "http://arxiv.org/abs/2310.08981v3",
      "title": "Low-latency Speech Enhancement via Speech Token Generation",
      "summary": "Existing deep learning based speech enhancement mainly employ a data-driven approach, which leverage large amounts of data with a variety of noise types to achieve noise removal from noisy signal. However, the high dependence on the data limits its generalization on the unseen complex noises in real-life environment. In this paper, we focus on the low-latency scenario and regard speech enhancement as a speech generation problem conditioned on the noisy signal, where we generate clean speech instead of identifying and removing noises. Specifically, we propose a conditional generative framework for speech enhancement, which models clean speech by acoustic codes of a neural speech codec and generates the speech codes conditioned on past noisy frames in an auto-regressive way. Moreover, we propose an explicit-alignment approach to align noisy frames with the generated speech tokens to improve the robustness and scalability to different input lengths. Different from other methods that leverage multiple stages to generate speech codes, we leverage a single-stage speech generation approach based on the TF-Codec neural codec to achieve high speech quality with low latency. Extensive results on both synthetic and real-recorded test set show its superiority over data-driven approaches in terms of noise robustness and temporal speech coherence.",
      "published": "2023-10-13T09:57:09Z"
    },
    "metadata": {
      "arxiv_id": "2310.08981",
      "title": "Low-latency Speech Enhancement via Speech Token Generation",
      "summary": "Existing deep learning based speech enhancement mainly employ a data-driven approach, which leverage large amounts of data with a variety of noise types to achieve noise removal from noisy signal. However, the high dependence on the data limits its generalization on the unseen complex noises in real-life environment. In this paper, we focus on the low-latency scenario and regard speech enhancement as a speech generation problem conditioned on the noisy signal, where we generate clean speech instead of identifying and removing noises. Specifically, we propose a conditional generative framework for speech enhancement, which models clean speech by acoustic codes of a neural speech codec and generates the speech codes conditioned on past noisy frames in an auto-regressive way. Moreover, we propose an explicit-alignment approach to align noisy frames with the generated speech tokens to improve the robustness and scalability to different input lengths. Different from other methods that leverage multiple stages to generate speech codes, we leverage a single-stage speech generation approach based on the TF-Codec neural codec to achieve high speech quality with low latency. Extensive results on both synthetic and real-recorded test set show its superiority over data-driven approaches in terms of noise robustness and temporal speech coherence.",
      "authors": [
        "Huaying Xue",
        "Xiulian Peng",
        "Yan Lu"
      ],
      "published": "2023-10-13T09:57:09Z",
      "updated": "2024-01-23T06:13:04Z",
      "categories": [
        "cs.SD",
        "cs.MM",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2310.08981v3",
      "landing_url": "https://arxiv.org/abs/2310.08981v3",
      "doi": "https://doi.org/10.48550/arXiv.2310.08981"
    },
    "queries": [
      {
        "anchor": "spoken language models",
        "search_term": "low latency"
      },
      {
        "anchor": "synchronous dialogue",
        "search_term": "low latency"
      },
      {
        "anchor": "synchronous dialogue",
        "search_term": "neural codec"
      }
    ]
  },
  {
    "arxiv_id": "2310.10803",
    "anchor": "spoken language models",
    "search_term": "true full-duplex",
    "search_record": {
      "id": "http://arxiv.org/abs/2310.10803v3",
      "title": "SD-HuBERT: Sentence-Level Self-Distillation Induces Syllabic Organization in HuBERT",
      "summary": "Data-driven unit discovery in self-supervised learning (SSL) of speech has embarked on a new era of spoken language processing. Yet, the discovered units often remain in phonetic space and the units beyond phonemes are largely underexplored. Here, we demonstrate that a syllabic organization emerges in learning sentence-level representation of speech. In particular, we adopt \"self-distillation\" objective to fine-tune the pretrained HuBERT with an aggregator token that summarizes the entire sentence. Without any supervision, the resulting model draws definite boundaries in speech, and the representations across frames exhibit salient syllabic structures. We demonstrate that this emergent structure largely corresponds to the ground truth syllables. Furthermore, we propose a new benchmark task, Spoken Speech ABX, for evaluating sentence-level representation of speech. When compared to previous models, our model outperforms in both unsupervised syllable discovery and learning sentence-level representation. Together, we demonstrate that the self-distillation of HuBERT gives rise to syllabic organization without relying on external labels or modalities, and potentially provides novel data-driven units for spoken language modeling.",
      "published": "2023-10-16T20:05:36Z"
    },
    "metadata": {
      "arxiv_id": "2310.10803",
      "title": "SD-HuBERT: Sentence-Level Self-Distillation Induces Syllabic Organization in HuBERT",
      "summary": "Data-driven unit discovery in self-supervised learning (SSL) of speech has embarked on a new era of spoken language processing. Yet, the discovered units often remain in phonetic space and the units beyond phonemes are largely underexplored. Here, we demonstrate that a syllabic organization emerges in learning sentence-level representation of speech. In particular, we adopt \"self-distillation\" objective to fine-tune the pretrained HuBERT with an aggregator token that summarizes the entire sentence. Without any supervision, the resulting model draws definite boundaries in speech, and the representations across frames exhibit salient syllabic structures. We demonstrate that this emergent structure largely corresponds to the ground truth syllables. Furthermore, we propose a new benchmark task, Spoken Speech ABX, for evaluating sentence-level representation of speech. When compared to previous models, our model outperforms in both unsupervised syllable discovery and learning sentence-level representation. Together, we demonstrate that the self-distillation of HuBERT gives rise to syllabic organization without relying on external labels or modalities, and potentially provides novel data-driven units for spoken language modeling.",
      "authors": [
        "Cheol Jun Cho",
        "Abdelrahman Mohamed",
        "Shang-Wen Li",
        "Alan W Black",
        "Gopala K. Anumanchipalli"
      ],
      "published": "2023-10-16T20:05:36Z",
      "updated": "2025-04-10T11:20:55Z",
      "categories": [
        "cs.CL",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2310.10803v3",
      "landing_url": "https://arxiv.org/abs/2310.10803v3",
      "doi": "https://doi.org/10.48550/arXiv.2310.10803"
    },
    "queries": [
      {
        "anchor": "spoken language models",
        "search_term": "true full-duplex"
      },
      {
        "anchor": "spoken language models",
        "search_term": "pseudo full-duplex"
      },
      {
        "anchor": "spoken language models",
        "search_term": "engineered synchronization"
      },
      {
        "anchor": "spoken language models",
        "search_term": "learned synchronization"
      },
      {
        "anchor": "spoken language models",
        "search_term": "streaming architecture"
      },
      {
        "anchor": "spoken language models",
        "search_term": "synchronous data"
      },
      {
        "anchor": "spoken language models",
        "search_term": "multichannel dialogue"
      },
      {
        "anchor": "spoken language models",
        "search_term": "behavioral arbitration"
      },
      {
        "anchor": "spoken language models",
        "search_term": "floor transfer offset"
      },
      {
        "anchor": "spoken language models",
        "search_term": "stop latency"
      },
      {
        "anchor": "spoken language models",
        "search_term": "interruption response delay"
      }
    ]
  },
  {
    "arxiv_id": "2310.11332",
    "anchor": "spoken language models",
    "search_term": "data scarcity",
    "search_record": {
      "id": "http://arxiv.org/abs/2310.11332v1",
      "title": "Discovering High-Quality Process Models Despite Data Scarcity",
      "summary": "Process discovery algorithms learn process models from executed activity sequences, describing concurrency, causality, and conflict. Concurrent activities require observing multiple permutations, increasing data requirements, especially for processes with concurrent subprocesses such as hierarchical, composite, or distributed processes. While process discovery algorithms traditionally use sequences of activities as input, recently introduced object-centric process discovery algorithms can use graphs of activities as input, encoding partial orders between activities. As such, they contain the concurrency information of many sequences in a single graph. In this paper, we address the research question of reducing process discovery data requirements when using object-centric event logs for process discovery. We classify different real-life processes according to the control-flow complexity within and between subprocesses and introduce an evaluation framework to assess process discovery algorithm quality of traditional and object-centric process discovery based on the sample size. We complement this with a large-scale production process case study. Our results show reduced data requirements, enabling the discovery of large, concurrent processes such as manufacturing with little data, previously infeasible with traditional process discovery. Our findings suggest that object-centric process mining could revolutionize process discovery in various sectors, including manufacturing and supply chains.",
      "published": "2023-10-17T15:12:05Z"
    },
    "metadata": {
      "arxiv_id": "2310.11332",
      "title": "Discovering High-Quality Process Models Despite Data Scarcity",
      "summary": "Process discovery algorithms learn process models from executed activity sequences, describing concurrency, causality, and conflict. Concurrent activities require observing multiple permutations, increasing data requirements, especially for processes with concurrent subprocesses such as hierarchical, composite, or distributed processes. While process discovery algorithms traditionally use sequences of activities as input, recently introduced object-centric process discovery algorithms can use graphs of activities as input, encoding partial orders between activities. As such, they contain the concurrency information of many sequences in a single graph. In this paper, we address the research question of reducing process discovery data requirements when using object-centric event logs for process discovery. We classify different real-life processes according to the control-flow complexity within and between subprocesses and introduce an evaluation framework to assess process discovery algorithm quality of traditional and object-centric process discovery based on the sample size. We complement this with a large-scale production process case study. Our results show reduced data requirements, enabling the discovery of large, concurrent processes such as manufacturing with little data, previously infeasible with traditional process discovery. Our findings suggest that object-centric process mining could revolutionize process discovery in various sectors, including manufacturing and supply chains.",
      "authors": [
        "Jan Niklas Adams",
        "Jari Peeperkorn",
        "Tobias Brockhoff",
        "Isabelle Terrier",
        "Heiko Göhner",
        "Merih Seran Uysal",
        "Seppe vanden Broucke",
        "Jochen De Weerdt",
        "Wil M. P. van der Aalst"
      ],
      "published": "2023-10-17T15:12:05Z",
      "updated": "2023-10-17T15:12:05Z",
      "categories": [
        "cs.DB"
      ],
      "pdf_url": "https://arxiv.org/pdf/2310.11332v1",
      "landing_url": "https://arxiv.org/abs/2310.11332v1",
      "doi": "https://doi.org/10.48550/arXiv.2310.11332"
    },
    "queries": [
      {
        "anchor": "spoken language models",
        "search_term": "data scarcity"
      },
      {
        "anchor": "synchronous dialogue",
        "search_term": "data scarcity"
      }
    ]
  },
  {
    "arxiv_id": "2310.11724",
    "anchor": "spoken language models",
    "search_term": "temporal dynamics",
    "search_record": {
      "id": "http://arxiv.org/abs/2310.11724v3",
      "title": "Self-convolved Bootstrap for M-regression under Complex Temporal Dynamics",
      "summary": "The paper considers simultaneous nonparametric inference for a wide class of M-regression models with time-varying coefficients. The covariates and errors of the regression model are tackled as a general class of nonstationary time series and are allowed to be cross-dependent. A novel and easy-to-implement self-convolved bootstrap procedure is proposed. With only one tuning parameter, the bootstrap facilitates a $\\sqrt{n}$-consistent inference of the cumulative regression function for the M-estimators under complex temporal dynamics, even under the possible presence of breakpoints in time series. Our methodology leads to a unified framework to conduct general classes of Exact Function Tests, Lack-of-fit Tests, and Qualitative Tests for the time-varying coefficients. These tests enable one to, among many others, conduct variable selection, check for constancy and linearity, as well as verify shape assumptions, including monotonicity and convexity. As applications, our method is utilized to study the time-varying properties of global climate data and Microsoft stock return, respectively.",
      "published": "2023-10-18T05:43:49Z"
    },
    "metadata": {
      "arxiv_id": "2310.11724",
      "title": "Self-convolved Bootstrap for M-regression under Complex Temporal Dynamics",
      "summary": "The paper considers simultaneous nonparametric inference for a wide class of M-regression models with time-varying coefficients. The covariates and errors of the regression model are tackled as a general class of nonstationary time series and are allowed to be cross-dependent. A novel and easy-to-implement self-convolved bootstrap procedure is proposed. With only one tuning parameter, the bootstrap facilitates a $\\sqrt{n}$-consistent inference of the cumulative regression function for the M-estimators under complex temporal dynamics, even under the possible presence of breakpoints in time series. Our methodology leads to a unified framework to conduct general classes of Exact Function Tests, Lack-of-fit Tests, and Qualitative Tests for the time-varying coefficients. These tests enable one to, among many others, conduct variable selection, check for constancy and linearity, as well as verify shape assumptions, including monotonicity and convexity. As applications, our method is utilized to study the time-varying properties of global climate data and Microsoft stock return, respectively.",
      "authors": [
        "Miaoshiqi Liu",
        "Zhou Zhou"
      ],
      "published": "2023-10-18T05:43:49Z",
      "updated": "2024-09-09T12:37:44Z",
      "categories": [
        "stat.ME",
        "math.ST"
      ],
      "pdf_url": "https://arxiv.org/pdf/2310.11724v3",
      "landing_url": "https://arxiv.org/abs/2310.11724v3",
      "doi": "https://doi.org/10.48550/arXiv.2310.11724"
    },
    "queries": [
      {
        "anchor": "spoken language models",
        "search_term": "temporal dynamics"
      },
      {
        "anchor": "synchronous dialogue",
        "search_term": "temporal dynamics"
      },
      {
        "anchor": "turn-taking",
        "search_term": "temporal dynamics"
      }
    ]
  },
  {
    "arxiv_id": "2310.14580",
    "anchor": "spoken language models",
    "search_term": "discrete tokens",
    "search_record": {
      "id": "http://arxiv.org/abs/2310.14580v4",
      "title": "Acoustic BPE for Speech Generation with Discrete Tokens",
      "summary": "Discrete audio tokens derived from self-supervised learning models have gained widespread usage in speech generation. However, current practice of directly utilizing audio tokens poses challenges for sequence modeling due to the length of the token sequence. Additionally, this approach places the burden on the model to establish correlations between tokens, further complicating the modeling process. To address this issue, we propose acoustic BPE which encodes frequent audio token patterns by utilizing byte-pair encoding. Acoustic BPE effectively reduces the sequence length and leverages the prior morphological information present in token sequence, which alleviates the modeling challenges of token correlation. Through comprehensive investigations on a speech language model trained with acoustic BPE, we confirm the notable advantages it offers, including faster inference and improved syntax capturing capabilities. In addition, we propose a novel rescore method to select the optimal synthetic speech among multiple candidates generated by rich-diversity TTS system. Experiments prove that rescore selection aligns closely with human preference, which highlights acoustic BPE's potential to other speech generation tasks.",
      "published": "2023-10-23T05:38:41Z"
    },
    "metadata": {
      "arxiv_id": "2310.14580",
      "title": "Acoustic BPE for Speech Generation with Discrete Tokens",
      "summary": "Discrete audio tokens derived from self-supervised learning models have gained widespread usage in speech generation. However, current practice of directly utilizing audio tokens poses challenges for sequence modeling due to the length of the token sequence. Additionally, this approach places the burden on the model to establish correlations between tokens, further complicating the modeling process. To address this issue, we propose acoustic BPE which encodes frequent audio token patterns by utilizing byte-pair encoding. Acoustic BPE effectively reduces the sequence length and leverages the prior morphological information present in token sequence, which alleviates the modeling challenges of token correlation. Through comprehensive investigations on a speech language model trained with acoustic BPE, we confirm the notable advantages it offers, including faster inference and improved syntax capturing capabilities. In addition, we propose a novel rescore method to select the optimal synthetic speech among multiple candidates generated by rich-diversity TTS system. Experiments prove that rescore selection aligns closely with human preference, which highlights acoustic BPE's potential to other speech generation tasks.",
      "authors": [
        "Feiyu Shen",
        "Yiwei Guo",
        "Chenpeng Du",
        "Xie Chen",
        "Kai Yu"
      ],
      "published": "2023-10-23T05:38:41Z",
      "updated": "2024-01-15T05:53:31Z",
      "categories": [
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2310.14580v4",
      "landing_url": "https://arxiv.org/abs/2310.14580v4",
      "doi": "https://doi.org/10.48550/arXiv.2310.14580"
    },
    "queries": [
      {
        "anchor": "spoken language models",
        "search_term": "discrete tokens"
      },
      {
        "anchor": "spoken language models",
        "search_term": "audio tokens"
      },
      {
        "anchor": "synchronous dialogue",
        "search_term": "discrete tokens"
      },
      {
        "anchor": "synchronous dialogue",
        "search_term": "audio tokens"
      },
      {
        "anchor": "turn-taking",
        "search_term": "discrete tokens"
      },
      {
        "anchor": "turn-taking",
        "search_term": "audio tokens"
      }
    ]
  },
  {
    "arxiv_id": "2310.14859",
    "anchor": "turn-taking",
    "search_term": "true full-duplex",
    "search_record": {
      "id": "http://arxiv.org/abs/2310.14859v3",
      "title": "3M-TRANSFORMER: A Multi-Stage Multi-Stream Multimodal Transformer for Embodied Turn-Taking Prediction",
      "summary": "Predicting turn-taking in multiparty conversations has many practical applications in human-computer/robot interaction. However, the complexity of human communication makes it a challenging task. Recent advances have shown that synchronous multi-perspective egocentric data can significantly improve turn-taking prediction compared to asynchronous, single-perspective transcriptions. Building on this research, we propose a new multimodal transformer-based architecture for predicting turn-taking in embodied, synchronized multi-perspective data. Our experimental results on the recently introduced EgoCom dataset show a substantial performance improvement of up to 14.01% on average compared to existing baselines and alternative transformer-based approaches. The source code, and the pre-trained models of our 3M-Transformer will be available upon acceptance.",
      "published": "2023-10-23T12:29:10Z"
    },
    "metadata": {
      "arxiv_id": "2310.14859",
      "title": "3M-TRANSFORMER: A Multi-Stage Multi-Stream Multimodal Transformer for Embodied Turn-Taking Prediction",
      "summary": "Predicting turn-taking in multiparty conversations has many practical applications in human-computer/robot interaction. However, the complexity of human communication makes it a challenging task. Recent advances have shown that synchronous multi-perspective egocentric data can significantly improve turn-taking prediction compared to asynchronous, single-perspective transcriptions. Building on this research, we propose a new multimodal transformer-based architecture for predicting turn-taking in embodied, synchronized multi-perspective data. Our experimental results on the recently introduced EgoCom dataset show a substantial performance improvement of up to 14.01% on average compared to existing baselines and alternative transformer-based approaches. The source code, and the pre-trained models of our 3M-Transformer will be available upon acceptance.",
      "authors": [
        "Mehdi Fatan",
        "Emanuele Mincato",
        "Dimitra Pintzou",
        "Mariella Dimiccoli"
      ],
      "published": "2023-10-23T12:29:10Z",
      "updated": "2023-12-21T18:19:58Z",
      "categories": [
        "cs.CV",
        "cs.CL"
      ],
      "pdf_url": "https://arxiv.org/pdf/2310.14859v3",
      "landing_url": "https://arxiv.org/abs/2310.14859v3",
      "doi": "https://doi.org/10.48550/arXiv.2310.14859"
    },
    "queries": [
      {
        "anchor": "turn-taking",
        "search_term": "true full-duplex"
      },
      {
        "anchor": "turn-taking",
        "search_term": "pseudo full-duplex"
      },
      {
        "anchor": "turn-taking",
        "search_term": "half duplex"
      },
      {
        "anchor": "turn-taking",
        "search_term": "engineered synchronization"
      },
      {
        "anchor": "turn-taking",
        "search_term": "learned synchronization"
      },
      {
        "anchor": "turn-taking",
        "search_term": "modular architecture"
      },
      {
        "anchor": "turn-taking",
        "search_term": "streaming architecture"
      },
      {
        "anchor": "turn-taking",
        "search_term": "real-time interaction"
      },
      {
        "anchor": "turn-taking",
        "search_term": "low latency"
      },
      {
        "anchor": "turn-taking",
        "search_term": "synchronous data"
      },
      {
        "anchor": "turn-taking",
        "search_term": "data scarcity"
      },
      {
        "anchor": "turn-taking",
        "search_term": "multichannel dialogue"
      },
      {
        "anchor": "turn-taking",
        "search_term": "temporal dynamics"
      },
      {
        "anchor": "turn-taking",
        "search_term": "behavioral arbitration"
      },
      {
        "anchor": "turn-taking",
        "search_term": "semantic coherence"
      },
      {
        "anchor": "turn-taking",
        "search_term": "neural codec"
      },
      {
        "anchor": "turn-taking",
        "search_term": "discrete tokens"
      },
      {
        "anchor": "turn-taking",
        "search_term": "audio tokens"
      },
      {
        "anchor": "turn-taking",
        "search_term": "floor transfer offset"
      },
      {
        "anchor": "turn-taking",
        "search_term": "stop latency"
      },
      {
        "anchor": "turn-taking",
        "search_term": "interruption response delay"
      }
    ]
  },
  {
    "arxiv_id": "2310.15598",
    "anchor": "spoken language models",
    "search_term": "half duplex",
    "search_record": {
      "id": "http://arxiv.org/abs/2310.15598v1",
      "title": "Coded Computing for Half-Duplex Wireless Distributed Computing Systems via Interference Alignment",
      "summary": "Distributed computing frameworks such as MapReduce and Spark are often used to process large-scale data computing jobs. In wireless scenarios, exchanging data among distributed nodes would seriously suffer from the communication bottleneck due to limited communication resources such as bandwidth and power. To address this problem, we propose a coded parallel computing (CPC) scheme for distributed computing systems where distributed nodes exchange information over a half-duplex wireless interference network. The CPC scheme achieves the multicast gain by utilizing coded computing to multicast coded symbols {intended to} multiple receiver nodes and the cooperative transmission gain by allowing multiple {transmitter} nodes to jointly deliver messages via interference alignment. To measure communication performance, we apply the widely used latency-oriented metric: \\emph{normalized delivery time (NDT)}. It is shown that CPC can significantly reduce the NDT by jointly exploiting the parallel transmission and coded multicasting opportunities. Surprisingly, when $K$ tends to infinity and the computation load is fixed, CPC approaches zero NDT while all state-of-the-art schemes achieve positive values of NDT. Finally, we establish an information-theoretic lower bound for the NDT-computation load trade-off over \\emph{half-duplex} network, and prove our scheme achieves the minimum NDT within a multiplicative gap of $3$, i.e., our scheme is order optimal.",
      "published": "2023-10-24T08:00:24Z"
    },
    "metadata": {
      "arxiv_id": "2310.15598",
      "title": "Coded Computing for Half-Duplex Wireless Distributed Computing Systems via Interference Alignment",
      "summary": "Distributed computing frameworks such as MapReduce and Spark are often used to process large-scale data computing jobs. In wireless scenarios, exchanging data among distributed nodes would seriously suffer from the communication bottleneck due to limited communication resources such as bandwidth and power. To address this problem, we propose a coded parallel computing (CPC) scheme for distributed computing systems where distributed nodes exchange information over a half-duplex wireless interference network. The CPC scheme achieves the multicast gain by utilizing coded computing to multicast coded symbols {intended to} multiple receiver nodes and the cooperative transmission gain by allowing multiple {transmitter} nodes to jointly deliver messages via interference alignment. To measure communication performance, we apply the widely used latency-oriented metric: \\emph{normalized delivery time (NDT)}. It is shown that CPC can significantly reduce the NDT by jointly exploiting the parallel transmission and coded multicasting opportunities. Surprisingly, when $K$ tends to infinity and the computation load is fixed, CPC approaches zero NDT while all state-of-the-art schemes achieve positive values of NDT. Finally, we establish an information-theoretic lower bound for the NDT-computation load trade-off over \\emph{half-duplex} network, and prove our scheme achieves the minimum NDT within a multiplicative gap of $3$, i.e., our scheme is order optimal.",
      "authors": [
        "Youlong Wu",
        "Zhenhao Huang",
        "Kai Yuan",
        "Shuai Ma",
        "Yue Bi"
      ],
      "published": "2023-10-24T08:00:24Z",
      "updated": "2023-10-24T08:00:24Z",
      "categories": [
        "cs.IT"
      ],
      "pdf_url": "https://arxiv.org/pdf/2310.15598v1",
      "landing_url": "https://arxiv.org/abs/2310.15598v1",
      "doi": "https://doi.org/10.48550/arXiv.2310.15598"
    },
    "queries": [
      {
        "anchor": "spoken language models",
        "search_term": "half duplex"
      },
      {
        "anchor": "synchronous dialogue",
        "search_term": "half duplex"
      },
      {
        "anchor": "turn-taking",
        "search_term": "half duplex"
      }
    ]
  },
  {
    "arxiv_id": "2310.18030",
    "anchor": "spoken language models",
    "search_term": "low latency",
    "search_record": {
      "id": "http://arxiv.org/abs/2310.18030v2",
      "title": "Confucius: Achieving Consistent Low Latency with Practical Queue Management for Real-Time Communications",
      "summary": "Real-time communication applications require consistently low latency, which is often disrupted by latency spikes caused by competing flows, especially Web traffic. We identify the root cause of disruptions in such cases as the mismatch between the abrupt bandwidth allocation adjustment of queue scheduling and gradual congestion window adjustment of congestion control. For example, when a sudden burst of new Web flows arrives, queue schedulers abruptly shift bandwidth away from the existing real-time flow(s). The real-time flow will need several RTTs to converge to the new available bandwidth, during which severe stalls occur. In this paper, we present Confucius, a practical queue management scheme designed for offering real-time traffic with consistently low latency regardless of competing flows. Confucius slows down bandwidth adjustment to match the reaction of congestion control, such that the end host can reduce the sending rate without incurring latency spikes. Importantly, Confucius does not require the collaboration of end-hosts (e.g., labels on packets), nor manual parameter tuning to achieve good performance. Extensive experiments show that Confucius outperforms existing practical queueing schemes by reducing the stall duration by more than 50%, while the competing flows also fairly enjoy on-par performance.",
      "published": "2023-10-27T10:09:22Z"
    },
    "metadata": {
      "arxiv_id": "2310.18030",
      "title": "Confucius: Achieving Consistent Low Latency with Practical Queue Management for Real-Time Communications",
      "summary": "Real-time communication applications require consistently low latency, which is often disrupted by latency spikes caused by competing flows, especially Web traffic. We identify the root cause of disruptions in such cases as the mismatch between the abrupt bandwidth allocation adjustment of queue scheduling and gradual congestion window adjustment of congestion control. For example, when a sudden burst of new Web flows arrives, queue schedulers abruptly shift bandwidth away from the existing real-time flow(s). The real-time flow will need several RTTs to converge to the new available bandwidth, during which severe stalls occur. In this paper, we present Confucius, a practical queue management scheme designed for offering real-time traffic with consistently low latency regardless of competing flows. Confucius slows down bandwidth adjustment to match the reaction of congestion control, such that the end host can reduce the sending rate without incurring latency spikes. Importantly, Confucius does not require the collaboration of end-hosts (e.g., labels on packets), nor manual parameter tuning to achieve good performance. Extensive experiments show that Confucius outperforms existing practical queueing schemes by reducing the stall duration by more than 50%, while the competing flows also fairly enjoy on-par performance.",
      "authors": [
        "Zili Meng",
        "Nirav Atre",
        "Mingwei Xu",
        "Justine Sherry",
        "Maria Apostolaki"
      ],
      "published": "2023-10-27T10:09:22Z",
      "updated": "2024-02-07T07:26:37Z",
      "categories": [
        "cs.NI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2310.18030v2",
      "landing_url": "https://arxiv.org/abs/2310.18030v2",
      "doi": "https://doi.org/10.48550/arXiv.2310.18030"
    },
    "queries": [
      {
        "anchor": "spoken language models",
        "search_term": "low latency"
      },
      {
        "anchor": "synchronous dialogue",
        "search_term": "low latency"
      }
    ]
  },
  {
    "arxiv_id": "2311.00816",
    "anchor": "synchronous dialogue",
    "search_term": "true full-duplex",
    "search_record": {
      "id": "http://arxiv.org/abs/2311.00816v1",
      "title": "Faster Peace via Inclusivity: An Efficient Paradigm to Understand Populations in Conflict Zones",
      "summary": "United Nations practice shows that inclusivity is vital for mediation to be successful in helping end violent conflict and establish lasting peace. However, current methods for understanding the views and needs of populations during dynamic situations create tension between inclusivity and efficiency. This work introduces a novel paradigm to mitigate such tension. In partnership with collaborators at the United Nations we develop a realtime large-scale synchronous dialogue process (RLSDP) to understand stakeholder populations on an hour timescale. We demonstrate a machine learning model which enables each dialogue cycle to take place on a minute-timescale. We manage a key risk related to machine learning result trustworthiness by computing result confidence from a fast and reliable estimation of posterior variance. Lastly, we highlight a constellation of risks stemming from this new paradigm and suggest policies to mitigate them.",
      "published": "2023-11-01T20:00:12Z"
    },
    "metadata": {
      "arxiv_id": "2311.00816",
      "title": "Faster Peace via Inclusivity: An Efficient Paradigm to Understand Populations in Conflict Zones",
      "summary": "United Nations practice shows that inclusivity is vital for mediation to be successful in helping end violent conflict and establish lasting peace. However, current methods for understanding the views and needs of populations during dynamic situations create tension between inclusivity and efficiency. This work introduces a novel paradigm to mitigate such tension. In partnership with collaborators at the United Nations we develop a realtime large-scale synchronous dialogue process (RLSDP) to understand stakeholder populations on an hour timescale. We demonstrate a machine learning model which enables each dialogue cycle to take place on a minute-timescale. We manage a key risk related to machine learning result trustworthiness by computing result confidence from a fast and reliable estimation of posterior variance. Lastly, we highlight a constellation of risks stemming from this new paradigm and suggest policies to mitigate them.",
      "authors": [
        "Jordan Bilich",
        "Michael Varga",
        "Daanish Masood",
        "Andrew Konya"
      ],
      "published": "2023-11-01T20:00:12Z",
      "updated": "2023-11-01T20:00:12Z",
      "categories": [
        "cs.CY",
        "cs.HC"
      ],
      "pdf_url": "https://arxiv.org/pdf/2311.00816v1",
      "landing_url": "https://arxiv.org/abs/2311.00816v1",
      "doi": "https://doi.org/10.48550/arXiv.2311.00816"
    },
    "queries": [
      {
        "anchor": "synchronous dialogue",
        "search_term": "true full-duplex"
      },
      {
        "anchor": "synchronous dialogue",
        "search_term": "pseudo full-duplex"
      },
      {
        "anchor": "synchronous dialogue",
        "search_term": "engineered synchronization"
      },
      {
        "anchor": "synchronous dialogue",
        "search_term": "learned synchronization"
      },
      {
        "anchor": "synchronous dialogue",
        "search_term": "modular architecture"
      },
      {
        "anchor": "synchronous dialogue",
        "search_term": "streaming architecture"
      },
      {
        "anchor": "synchronous dialogue",
        "search_term": "real-time interaction"
      },
      {
        "anchor": "synchronous dialogue",
        "search_term": "synchronous data"
      },
      {
        "anchor": "synchronous dialogue",
        "search_term": "data scarcity"
      },
      {
        "anchor": "synchronous dialogue",
        "search_term": "multichannel dialogue"
      },
      {
        "anchor": "synchronous dialogue",
        "search_term": "behavioral arbitration"
      },
      {
        "anchor": "synchronous dialogue",
        "search_term": "semantic coherence"
      },
      {
        "anchor": "synchronous dialogue",
        "search_term": "neural codec"
      },
      {
        "anchor": "synchronous dialogue",
        "search_term": "discrete tokens"
      },
      {
        "anchor": "synchronous dialogue",
        "search_term": "audio tokens"
      },
      {
        "anchor": "synchronous dialogue",
        "search_term": "floor transfer offset"
      },
      {
        "anchor": "synchronous dialogue",
        "search_term": "stop latency"
      },
      {
        "anchor": "synchronous dialogue",
        "search_term": "interruption response delay"
      }
    ]
  },
  {
    "arxiv_id": "2311.01615",
    "anchor": "spoken language models",
    "search_term": "audio tokens",
    "search_record": {
      "id": "http://arxiv.org/abs/2311.01615v1",
      "title": "FLAP: Fast Language-Audio Pre-training",
      "summary": "We propose Fast Language-Audio Pre-training (FLAP), a self-supervised approach that efficiently and effectively learns aligned audio and language representations through masking, contrastive learning and reconstruction. For efficiency, FLAP randomly drops audio spectrogram tokens, focusing solely on the remaining ones for self-supervision. Through inter-modal contrastive learning, FLAP learns to align paired audio and text representations in a shared latent space. Notably, FLAP leverages multiple augmented views via masking for inter-modal contrast and learns to reconstruct the masked portion of audio tokens. Moreover, FLAP leverages large language models (LLMs) to augment the text inputs, contributing to improved performance. These approaches lead to more robust and informative audio-text representations, enabling FLAP to achieve state-of-the-art (SoTA) performance on audio-text retrieval tasks on AudioCaps (achieving 53.0% R@1) and Clotho (achieving 25.5% R@1).",
      "published": "2023-11-02T21:58:50Z"
    },
    "metadata": {
      "arxiv_id": "2311.01615",
      "title": "FLAP: Fast Language-Audio Pre-training",
      "summary": "We propose Fast Language-Audio Pre-training (FLAP), a self-supervised approach that efficiently and effectively learns aligned audio and language representations through masking, contrastive learning and reconstruction. For efficiency, FLAP randomly drops audio spectrogram tokens, focusing solely on the remaining ones for self-supervision. Through inter-modal contrastive learning, FLAP learns to align paired audio and text representations in a shared latent space. Notably, FLAP leverages multiple augmented views via masking for inter-modal contrast and learns to reconstruct the masked portion of audio tokens. Moreover, FLAP leverages large language models (LLMs) to augment the text inputs, contributing to improved performance. These approaches lead to more robust and informative audio-text representations, enabling FLAP to achieve state-of-the-art (SoTA) performance on audio-text retrieval tasks on AudioCaps (achieving 53.0% R@1) and Clotho (achieving 25.5% R@1).",
      "authors": [
        "Ching-Feng Yeh",
        "Po-Yao Huang",
        "Vasu Sharma",
        "Shang-Wen Li",
        "Gargi Gosh"
      ],
      "published": "2023-11-02T21:58:50Z",
      "updated": "2023-11-02T21:58:50Z",
      "categories": [
        "cs.SD",
        "cs.CL",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2311.01615v1",
      "landing_url": "https://arxiv.org/abs/2311.01615v1",
      "doi": "https://doi.org/10.48550/arXiv.2311.01615"
    },
    "queries": [
      {
        "anchor": "spoken language models",
        "search_term": "audio tokens"
      },
      {
        "anchor": "synchronous dialogue",
        "search_term": "audio tokens"
      },
      {
        "anchor": "turn-taking",
        "search_term": "audio tokens"
      }
    ]
  },
  {
    "arxiv_id": "2311.03021",
    "anchor": "synchronous dialogue",
    "search_term": "real-time interaction",
    "search_record": {
      "id": "http://arxiv.org/abs/2311.03021v1",
      "title": "Detecting agreement in multi-party dialogue: evaluating speaker diarisation versus a procedural baseline to enhance user engagement",
      "summary": "Conversational agents participating in multi-party interactions face significant challenges in dialogue state tracking, since the identity of the speaker adds significant contextual meaning. It is common to utilise diarisation models to identify the speaker. However, it is not clear if these are accurate enough to correctly identify specific conversational events such as agreement or disagreement during a real-time interaction. This study uses a cooperative quiz, where the conversational agent acts as quiz-show host, to determine whether diarisation or a frequency-and-proximity-based method is more accurate at determining agreement, and whether this translates to feelings of engagement from the players. Experimental results show that our procedural system was more engaging to players, and was more accurate at detecting agreement, reaching an average accuracy of 0.44 compared to 0.28 for the diarised system.",
      "published": "2023-11-06T11:00:44Z"
    },
    "metadata": {
      "arxiv_id": "2311.03021",
      "title": "Detecting agreement in multi-party dialogue: evaluating speaker diarisation versus a procedural baseline to enhance user engagement",
      "summary": "Conversational agents participating in multi-party interactions face significant challenges in dialogue state tracking, since the identity of the speaker adds significant contextual meaning. It is common to utilise diarisation models to identify the speaker. However, it is not clear if these are accurate enough to correctly identify specific conversational events such as agreement or disagreement during a real-time interaction. This study uses a cooperative quiz, where the conversational agent acts as quiz-show host, to determine whether diarisation or a frequency-and-proximity-based method is more accurate at determining agreement, and whether this translates to feelings of engagement from the players. Experimental results show that our procedural system was more engaging to players, and was more accurate at detecting agreement, reaching an average accuracy of 0.44 compared to 0.28 for the diarised system.",
      "authors": [
        "Angus Addlesee",
        "Daniel Denley",
        "Andy Edmondson",
        "Nancie Gunson",
        "Daniel Hernández Garcia",
        "Alexandre Kha",
        "Oliver Lemon",
        "James Ndubuisi",
        "Neil O'Reilly",
        "Lia Perochaud",
        "Raphaël Valeri",
        "Miebaka Worika"
      ],
      "published": "2023-11-06T11:00:44Z",
      "updated": "2023-11-06T11:00:44Z",
      "categories": [
        "cs.CL"
      ],
      "pdf_url": "https://arxiv.org/pdf/2311.03021v1",
      "landing_url": "https://arxiv.org/abs/2311.03021v1",
      "doi": "https://doi.org/10.48550/arXiv.2311.03021"
    },
    "queries": [
      {
        "anchor": "synchronous dialogue",
        "search_term": "real-time interaction"
      }
    ]
  },
  {
    "arxiv_id": "2311.03026",
    "anchor": "turn-taking",
    "search_term": "true full-duplex",
    "search_record": {
      "id": "http://arxiv.org/abs/2311.03026v1",
      "title": "Detecting Agreement in Multi-party Conversational AI",
      "summary": "Today, conversational systems are expected to handle conversations in multi-party settings, especially within Socially Assistive Robots (SARs). However, practical usability remains difficult as there are additional challenges to overcome, such as speaker recognition, addressee recognition, and complex turn-taking. In this paper, we present our work on a multi-party conversational system, which invites two users to play a trivia quiz game. The system detects users' agreement or disagreement on a final answer and responds accordingly. Our evaluation includes both performance and user assessment results, with a focus on detecting user agreement. Our annotated transcripts and the code for the proposed system have been released open-source on GitHub.",
      "published": "2023-11-06T11:04:39Z"
    },
    "metadata": {
      "arxiv_id": "2311.03026",
      "title": "Detecting Agreement in Multi-party Conversational AI",
      "summary": "Today, conversational systems are expected to handle conversations in multi-party settings, especially within Socially Assistive Robots (SARs). However, practical usability remains difficult as there are additional challenges to overcome, such as speaker recognition, addressee recognition, and complex turn-taking. In this paper, we present our work on a multi-party conversational system, which invites two users to play a trivia quiz game. The system detects users' agreement or disagreement on a final answer and responds accordingly. Our evaluation includes both performance and user assessment results, with a focus on detecting user agreement. Our annotated transcripts and the code for the proposed system have been released open-source on GitHub.",
      "authors": [
        "Laura Schauer",
        "Jason Sweeney",
        "Charlie Lyttle",
        "Zein Said",
        "Aron Szeles",
        "Cale Clark",
        "Katie McAskill",
        "Xander Wickham",
        "Tom Byars",
        "Daniel Hernández Garcia",
        "Nancie Gunson",
        "Angus Addlesee",
        "Oliver Lemon"
      ],
      "published": "2023-11-06T11:04:39Z",
      "updated": "2023-11-06T11:04:39Z",
      "categories": [
        "cs.CL"
      ],
      "pdf_url": "https://arxiv.org/pdf/2311.03026v1",
      "landing_url": "https://arxiv.org/abs/2311.03026v1",
      "doi": "https://doi.org/10.48550/arXiv.2311.03026"
    },
    "queries": [
      {
        "anchor": "turn-taking",
        "search_term": "true full-duplex"
      },
      {
        "anchor": "turn-taking",
        "search_term": "pseudo full-duplex"
      },
      {
        "anchor": "turn-taking",
        "search_term": "engineered synchronization"
      },
      {
        "anchor": "turn-taking",
        "search_term": "learned synchronization"
      },
      {
        "anchor": "turn-taking",
        "search_term": "synchronous data"
      },
      {
        "anchor": "turn-taking",
        "search_term": "multichannel dialogue"
      },
      {
        "anchor": "turn-taking",
        "search_term": "behavioral arbitration"
      },
      {
        "anchor": "turn-taking",
        "search_term": "floor transfer offset"
      },
      {
        "anchor": "turn-taking",
        "search_term": "stop latency"
      },
      {
        "anchor": "turn-taking",
        "search_term": "interruption response delay"
      }
    ]
  },
  {
    "arxiv_id": "2311.04482",
    "anchor": "synchronous dialogue",
    "search_term": "synchronous data",
    "search_record": {
      "id": "http://arxiv.org/abs/2311.04482v1",
      "title": "Optimizing Distributed Networking with Big Data Scheduling and Cloud Computing",
      "summary": "With the rapid transformation of computer hardware and algorithms, mobile networking has evolved from low data carrying capacity and high latency to better-optimized networks, either by enhancing the digital network or using different approaches to reduce network traffic. This paper discusses the big data applications and scheduling in the distributed networking and analyzes the opportunities and challenges of data management systems. The analysis shows that the big data scheduling in the cloud computing environment produces the most efficient way to transfer and synchronize data. Since scheduling problems and cloud models are very complex to analyze in different settings, we set it to the typical software defined networks. The development of cloud management models and coflow scheduling algorithm is proved to be the priority of the digital communications and networks development in the future.",
      "published": "2023-11-08T06:22:44Z"
    },
    "metadata": {
      "arxiv_id": "2311.04482",
      "title": "Optimizing Distributed Networking with Big Data Scheduling and Cloud Computing",
      "summary": "With the rapid transformation of computer hardware and algorithms, mobile networking has evolved from low data carrying capacity and high latency to better-optimized networks, either by enhancing the digital network or using different approaches to reduce network traffic. This paper discusses the big data applications and scheduling in the distributed networking and analyzes the opportunities and challenges of data management systems. The analysis shows that the big data scheduling in the cloud computing environment produces the most efficient way to transfer and synchronize data. Since scheduling problems and cloud models are very complex to analyze in different settings, we set it to the typical software defined networks. The development of cloud management models and coflow scheduling algorithm is proved to be the priority of the digital communications and networks development in the future.",
      "authors": [
        "Wenbo Zhu"
      ],
      "published": "2023-11-08T06:22:44Z",
      "updated": "2023-11-08T06:22:44Z",
      "categories": [
        "cs.NI",
        "cs.DC"
      ],
      "pdf_url": "https://arxiv.org/pdf/2311.04482v1",
      "landing_url": "https://arxiv.org/abs/2311.04482v1",
      "doi": "https://doi.org/10.1117/12.2642577"
    },
    "queries": [
      {
        "anchor": "synchronous dialogue",
        "search_term": "synchronous data"
      }
    ]
  },
  {
    "arxiv_id": "2311.04846",
    "anchor": "synchronous dialogue",
    "search_term": "temporal dynamics",
    "search_record": {
      "id": "http://arxiv.org/abs/2311.04846v2",
      "title": "Incorporating temporal dynamics of mutations to enhance the prediction capability of antiretroviral therapy's outcome for HIV-1",
      "summary": "Motivation: In predicting HIV therapy outcomes, a critical clinical question is whether using historical information can enhance predictive capabilities compared with current or latest available data analysis. This study analyses whether historical knowledge, which includes viral mutations detected in all genotypic tests before therapy, their temporal occurrence, and concomitant viral load measurements, can bring improvements. We introduce a method to weigh mutations, considering the previously enumerated factors and the reference mutation-drug Stanford resistance tables. We compare a model encompassing history (H) with one not using it (NH). Results: The H-model demonstrates superior discriminative ability, with a higher ROC-AUC score (76.34%) than the NH-model (74.98%). Significant Wilcoxon test results confirm that incorporating historical information improves consistently predictive accuracy for treatment outcomes. The better performance of the H-model might be attributed to its consideration of latent HIV reservoirs, probably obtained when leveraging historical information. The findings emphasize the importance of temporal dynamics in mutations, offering insights into HIV infection complexities. However, our result also shows that prediction accuracy remains relatively high even when no historical information is available. Supplementary information: Supplementary material is available.",
      "published": "2023-11-08T17:29:41Z"
    },
    "metadata": {
      "arxiv_id": "2311.04846",
      "title": "Incorporating temporal dynamics of mutations to enhance the prediction capability of antiretroviral therapy's outcome for HIV-1",
      "summary": "Motivation: In predicting HIV therapy outcomes, a critical clinical question is whether using historical information can enhance predictive capabilities compared with current or latest available data analysis. This study analyses whether historical knowledge, which includes viral mutations detected in all genotypic tests before therapy, their temporal occurrence, and concomitant viral load measurements, can bring improvements. We introduce a method to weigh mutations, considering the previously enumerated factors and the reference mutation-drug Stanford resistance tables. We compare a model encompassing history (H) with one not using it (NH). Results: The H-model demonstrates superior discriminative ability, with a higher ROC-AUC score (76.34%) than the NH-model (74.98%). Significant Wilcoxon test results confirm that incorporating historical information improves consistently predictive accuracy for treatment outcomes. The better performance of the H-model might be attributed to its consideration of latent HIV reservoirs, probably obtained when leveraging historical information. The findings emphasize the importance of temporal dynamics in mutations, offering insights into HIV infection complexities. However, our result also shows that prediction accuracy remains relatively high even when no historical information is available. Supplementary information: Supplementary material is available.",
      "authors": [
        "Giulia Di Teodoro",
        "Martin Pirkl",
        "Francesca Incardona",
        "Ilaria Vicenti",
        "Anders Sönnerborg",
        "Rolf Kaiser",
        "Laura Palagi",
        "Maurizio Zazzi",
        "Thomas Lengauer"
      ],
      "published": "2023-11-08T17:29:41Z",
      "updated": "2024-06-24T15:07:56Z",
      "categories": [
        "cs.LG",
        "q-bio.QM"
      ],
      "pdf_url": "https://arxiv.org/pdf/2311.04846v2",
      "landing_url": "https://arxiv.org/abs/2311.04846v2",
      "doi": "https://doi.org/10.1093/bioinformatics/btae327"
    },
    "queries": [
      {
        "anchor": "synchronous dialogue",
        "search_term": "temporal dynamics"
      }
    ]
  },
  {
    "arxiv_id": "2311.08330",
    "anchor": "synchronous dialogue",
    "search_term": "discrete tokens",
    "search_record": {
      "id": "http://arxiv.org/abs/2311.08330v2",
      "title": "Generative De-Quantization for Neural Speech Codec via Latent Diffusion",
      "summary": "In low-bitrate speech coding, end-to-end speech coding networks aim to learn compact yet expressive features and a powerful decoder in a single network. A challenging problem as such results in unwelcome complexity increase and inferior speech quality. In this paper, we propose to separate the representation learning and information reconstruction tasks. We leverage an end-to-end codec for learning low-dimensional discrete tokens and employ a latent diffusion model to de-quantize coded features into a high-dimensional continuous space, relieving the decoder's burden of de-quantizing and upsampling. To mitigate the issue of over-smooth generation, we introduce midway-infilling with less noise reduction and stronger conditioning. In ablation studies, we investigate the hyperparameters for midway-infilling and latent diffusion space with different dimensions. Subjective listening tests show that our model outperforms the state-of-the-art at two low bitrates, 1.5 and 3 kbps. Codes and samples of this work are available on our webpage.",
      "published": "2023-11-14T17:19:40Z"
    },
    "metadata": {
      "arxiv_id": "2311.08330",
      "title": "Generative De-Quantization for Neural Speech Codec via Latent Diffusion",
      "summary": "In low-bitrate speech coding, end-to-end speech coding networks aim to learn compact yet expressive features and a powerful decoder in a single network. A challenging problem as such results in unwelcome complexity increase and inferior speech quality. In this paper, we propose to separate the representation learning and information reconstruction tasks. We leverage an end-to-end codec for learning low-dimensional discrete tokens and employ a latent diffusion model to de-quantize coded features into a high-dimensional continuous space, relieving the decoder's burden of de-quantizing and upsampling. To mitigate the issue of over-smooth generation, we introduce midway-infilling with less noise reduction and stronger conditioning. In ablation studies, we investigate the hyperparameters for midway-infilling and latent diffusion space with different dimensions. Subjective listening tests show that our model outperforms the state-of-the-art at two low bitrates, 1.5 and 3 kbps. Codes and samples of this work are available on our webpage.",
      "authors": [
        "Haici Yang",
        "Inseon Jang",
        "Minje Kim"
      ],
      "published": "2023-11-14T17:19:40Z",
      "updated": "2023-11-15T15:23:03Z",
      "categories": [
        "eess.AS",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2311.08330v2",
      "landing_url": "https://arxiv.org/abs/2311.08330v2",
      "doi": "https://doi.org/10.48550/arXiv.2311.08330"
    },
    "queries": [
      {
        "anchor": "synchronous dialogue",
        "search_term": "discrete tokens"
      }
    ]
  },
  {
    "arxiv_id": "2311.09525",
    "anchor": "spoken language models",
    "search_term": "low latency",
    "search_record": {
      "id": "http://arxiv.org/abs/2311.09525v2",
      "title": "NGEL-SLAM: Neural Implicit Representation-based Global Consistent Low-Latency SLAM System",
      "summary": "Neural implicit representations have emerged as a promising solution for providing dense geometry in Simultaneous Localization and Mapping (SLAM). However, existing methods in this direction fall short in terms of global consistency and low latency. This paper presents NGEL-SLAM to tackle the above challenges. To ensure global consistency, our system leverages a traditional feature-based tracking module that incorporates loop closure. Additionally, we maintain a global consistent map by representing the scene using multiple neural implicit fields, enabling quick adjustment to the loop closure. Moreover, our system allows for fast convergence through the use of octree-based implicit representations. The combination of rapid response to loop closure and fast convergence makes our system a truly low-latency system that achieves global consistency. Our system enables rendering high-fidelity RGB-D images, along with extracting dense and complete surfaces. Experiments on both synthetic and real-world datasets suggest that our system achieves state-of-the-art tracking and mapping accuracy while maintaining low latency.",
      "published": "2023-11-16T03:05:58Z"
    },
    "metadata": {
      "arxiv_id": "2311.09525",
      "title": "NGEL-SLAM: Neural Implicit Representation-based Global Consistent Low-Latency SLAM System",
      "summary": "Neural implicit representations have emerged as a promising solution for providing dense geometry in Simultaneous Localization and Mapping (SLAM). However, existing methods in this direction fall short in terms of global consistency and low latency. This paper presents NGEL-SLAM to tackle the above challenges. To ensure global consistency, our system leverages a traditional feature-based tracking module that incorporates loop closure. Additionally, we maintain a global consistent map by representing the scene using multiple neural implicit fields, enabling quick adjustment to the loop closure. Moreover, our system allows for fast convergence through the use of octree-based implicit representations. The combination of rapid response to loop closure and fast convergence makes our system a truly low-latency system that achieves global consistency. Our system enables rendering high-fidelity RGB-D images, along with extracting dense and complete surfaces. Experiments on both synthetic and real-world datasets suggest that our system achieves state-of-the-art tracking and mapping accuracy while maintaining low latency.",
      "authors": [
        "Yunxuan Mao",
        "Xuan Yu",
        "Kai Wang",
        "Yue Wang",
        "Rong Xiong",
        "Yiyi Liao"
      ],
      "published": "2023-11-16T03:05:58Z",
      "updated": "2024-08-21T04:19:20Z",
      "categories": [
        "cs.RO"
      ],
      "pdf_url": "https://arxiv.org/pdf/2311.09525v2",
      "landing_url": "https://arxiv.org/abs/2311.09525v2",
      "doi": "https://doi.org/10.48550/arXiv.2311.09525"
    },
    "queries": [
      {
        "anchor": "spoken language models",
        "search_term": "low latency"
      },
      {
        "anchor": "synchronous dialogue",
        "search_term": "low latency"
      },
      {
        "anchor": "turn-taking",
        "search_term": "low latency"
      }
    ]
  },
  {
    "arxiv_id": "2311.09847",
    "anchor": "synchronous dialogue",
    "search_term": "data scarcity",
    "search_record": {
      "id": "http://arxiv.org/abs/2311.09847v1",
      "title": "Overcoming Data Scarcity in Biomedical Imaging with a Foundational Multi-Task Model",
      "summary": "Foundational models, pretrained on a large scale, have demonstrated substantial success across non-medical domains. However, training these models typically requires large, comprehensive datasets, which contrasts with the smaller and more heterogeneous datasets common in biomedical imaging. Here, we propose a multi-task learning strategy that decouples the number of training tasks from memory requirements. We trained a Universal bioMedical PreTrained model (UMedPT) on a multi-task database including tomographic, microscopic, and X-ray images, with various labelling strategies such as classification, segmentation, and object detection. The UMedPT foundational model outperformed ImageNet pretraining and the previous state-of-the-art models. For tasks related to the pretraining database, it maintained its performance with only 1% of the original training data and without fine-tuning. For out-of-domain tasks it required not more than 50% of the original training data. In an external independent validation imaging features extracted using UMedPT proved to be a new standard for cross-center transferability.",
      "published": "2023-11-16T12:20:25Z"
    },
    "metadata": {
      "arxiv_id": "2311.09847",
      "title": "Overcoming Data Scarcity in Biomedical Imaging with a Foundational Multi-Task Model",
      "summary": "Foundational models, pretrained on a large scale, have demonstrated substantial success across non-medical domains. However, training these models typically requires large, comprehensive datasets, which contrasts with the smaller and more heterogeneous datasets common in biomedical imaging. Here, we propose a multi-task learning strategy that decouples the number of training tasks from memory requirements. We trained a Universal bioMedical PreTrained model (UMedPT) on a multi-task database including tomographic, microscopic, and X-ray images, with various labelling strategies such as classification, segmentation, and object detection. The UMedPT foundational model outperformed ImageNet pretraining and the previous state-of-the-art models. For tasks related to the pretraining database, it maintained its performance with only 1% of the original training data and without fine-tuning. For out-of-domain tasks it required not more than 50% of the original training data. In an external independent validation imaging features extracted using UMedPT proved to be a new standard for cross-center transferability.",
      "authors": [
        "Raphael Schäfer",
        "Till Nicke",
        "Henning Höfener",
        "Annkristin Lange",
        "Dorit Merhof",
        "Friedrich Feuerhake",
        "Volkmar Schulz",
        "Johannes Lotz",
        "Fabian Kiessling"
      ],
      "published": "2023-11-16T12:20:25Z",
      "updated": "2023-11-16T12:20:25Z",
      "categories": [
        "cs.CV",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2311.09847v1",
      "landing_url": "https://arxiv.org/abs/2311.09847v1",
      "doi": "https://doi.org/10.48550/arXiv.2311.09847"
    },
    "queries": [
      {
        "anchor": "synchronous dialogue",
        "search_term": "data scarcity"
      }
    ]
  },
  {
    "arxiv_id": "2311.10522",
    "anchor": "spoken language models",
    "search_term": "semantic coherence",
    "search_record": {
      "id": "http://arxiv.org/abs/2311.10522v7",
      "title": "Enhancing Object Coherence in Layout-to-Image Synthesis",
      "summary": "Layout-to-image synthesis is an emerging technique in conditional image generation. It aims to generate complex scenes, where users require fine control over the layout of the objects in a scene. However, it remains challenging to control the object coherence, including semantic coherence (e.g., the cat looks at the flowers or not) and physical coherence (e.g., the hand and the racket should not be misaligned). In this paper, we propose a novel diffusion model with effective global semantic fusion (GSF) and self-similarity feature enhancement modules to guide the object coherence for this task. For semantic coherence, we argue that the image caption contains rich information for defining the semantic relationship within the objects in the images. Instead of simply employing cross-attention between captions and latent images, which addresses the highly relevant layout restriction and semantic coherence requirement separately and thus leads to unsatisfying results shown in our experiments, we develop GSF to fuse the supervision from the layout restriction and semantic coherence requirement and exploit it to guide the image synthesis process. Moreover, to improve the physical coherence, we develop a Self-similarity Coherence Attention (SCA) module to explicitly integrate local contextual physical coherence relation into each pixel's generation process. Specifically, we adopt a self-similarity map to encode the physical coherence restrictions and employ it to extract coherent features from text embedding. Through visualization of our self-similarity map, we explore the essence of SCA, revealing that its effectiveness is not only in capturing reliable physical coherence patterns but also in enhancing complex texture generation. Extensive experiments demonstrate the superiority of our proposed method.",
      "published": "2023-11-17T13:43:43Z"
    },
    "metadata": {
      "arxiv_id": "2311.10522",
      "title": "Enhancing Object Coherence in Layout-to-Image Synthesis",
      "summary": "Layout-to-image synthesis is an emerging technique in conditional image generation. It aims to generate complex scenes, where users require fine control over the layout of the objects in a scene. However, it remains challenging to control the object coherence, including semantic coherence (e.g., the cat looks at the flowers or not) and physical coherence (e.g., the hand and the racket should not be misaligned). In this paper, we propose a novel diffusion model with effective global semantic fusion (GSF) and self-similarity feature enhancement modules to guide the object coherence for this task. For semantic coherence, we argue that the image caption contains rich information for defining the semantic relationship within the objects in the images. Instead of simply employing cross-attention between captions and latent images, which addresses the highly relevant layout restriction and semantic coherence requirement separately and thus leads to unsatisfying results shown in our experiments, we develop GSF to fuse the supervision from the layout restriction and semantic coherence requirement and exploit it to guide the image synthesis process. Moreover, to improve the physical coherence, we develop a Self-similarity Coherence Attention (SCA) module to explicitly integrate local contextual physical coherence relation into each pixel's generation process. Specifically, we adopt a self-similarity map to encode the physical coherence restrictions and employ it to extract coherent features from text embedding. Through visualization of our self-similarity map, we explore the essence of SCA, revealing that its effectiveness is not only in capturing reliable physical coherence patterns but also in enhancing complex texture generation. Extensive experiments demonstrate the superiority of our proposed method.",
      "authors": [
        "Yibin Wang",
        "Changhai Zhou",
        "Honghui Xu"
      ],
      "published": "2023-11-17T13:43:43Z",
      "updated": "2025-03-31T03:32:16Z",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2311.10522v7",
      "landing_url": "https://arxiv.org/abs/2311.10522v7",
      "doi": "https://doi.org/10.48550/arXiv.2311.10522"
    },
    "queries": [
      {
        "anchor": "spoken language models",
        "search_term": "semantic coherence"
      },
      {
        "anchor": "synchronous dialogue",
        "search_term": "semantic coherence"
      },
      {
        "anchor": "turn-taking",
        "search_term": "semantic coherence"
      }
    ]
  },
  {
    "arxiv_id": "2311.15354",
    "anchor": "synchronous dialogue",
    "search_term": "real-time interaction",
    "search_record": {
      "id": "http://arxiv.org/abs/2311.15354v1",
      "title": "Web-Based Dynamic Paintings: Real-Time Interactive Artworks in Web Using a 2.5D Pipeline",
      "summary": "In this work, we present a 2.5D pipeline approach to creating dynamic paintings that can be re-rendered interactively in real-time on the Web. Using this 2.5D approach, any existing simple painting such as portraits can be turned into an interactive dynamic web-based artwork. Our interactive system provides most global illumination effects such as reflection, refraction, shadow, and subsurface scattering by processing images. In our system, the scene is defined only by a set of images. These include (1) a shape image, (2) two diffuse images, (3) a background image, (4) one foreground image, and (5) one transparency image. A shape image is either a normal map or a height. Two diffuse images are usually hand-painted. They are interpolated using illumination information. The transparency image is used to define the transparent and reflective regions that can reflect the foreground image and refract the background image, both of which are also hand-drawn. This framework, which mainly uses hand-drawn images, provides qualitatively convincing painterly global illumination effects such as reflection and refraction. We also include parameters to provide additional artistic controls. For instance, using our piecewise linear Fresnel function, it is possible to control the ratio of reflection and refraction. This system is the result of a long line of research contributions. On the other hand, the art-directed Fresnel function that provides physically plausible compositing of reflection and refraction with artistic control is completely new. Art-directed warping equations that provide qualitatively convincing refraction and reflection effects with linearized artistic control are also new. You can try our web-based system for interactive dynamic real-time paintings at http://mock3d.tamu.edu/.",
      "published": "2023-11-26T17:09:37Z"
    },
    "metadata": {
      "arxiv_id": "2311.15354",
      "title": "Web-Based Dynamic Paintings: Real-Time Interactive Artworks in Web Using a 2.5D Pipeline",
      "summary": "In this work, we present a 2.5D pipeline approach to creating dynamic paintings that can be re-rendered interactively in real-time on the Web. Using this 2.5D approach, any existing simple painting such as portraits can be turned into an interactive dynamic web-based artwork. Our interactive system provides most global illumination effects such as reflection, refraction, shadow, and subsurface scattering by processing images. In our system, the scene is defined only by a set of images. These include (1) a shape image, (2) two diffuse images, (3) a background image, (4) one foreground image, and (5) one transparency image. A shape image is either a normal map or a height. Two diffuse images are usually hand-painted. They are interpolated using illumination information. The transparency image is used to define the transparent and reflective regions that can reflect the foreground image and refract the background image, both of which are also hand-drawn. This framework, which mainly uses hand-drawn images, provides qualitatively convincing painterly global illumination effects such as reflection and refraction. We also include parameters to provide additional artistic controls. For instance, using our piecewise linear Fresnel function, it is possible to control the ratio of reflection and refraction. This system is the result of a long line of research contributions. On the other hand, the art-directed Fresnel function that provides physically plausible compositing of reflection and refraction with artistic control is completely new. Art-directed warping equations that provide qualitatively convincing refraction and reflection effects with linearized artistic control are also new. You can try our web-based system for interactive dynamic real-time paintings at http://mock3d.tamu.edu/.",
      "authors": [
        "Ergun Akleman",
        "Youyou wang",
        "Yinan Xiong",
        "Anusha Shanker",
        "Fermi Perumal",
        "Ozgur Gonen",
        "Motahareh Fard"
      ],
      "published": "2023-11-26T17:09:37Z",
      "updated": "2023-11-26T17:09:37Z",
      "categories": [
        "cs.GR",
        "cs.HC"
      ],
      "pdf_url": "https://arxiv.org/pdf/2311.15354v1",
      "landing_url": "https://arxiv.org/abs/2311.15354v1",
      "doi": "https://doi.org/10.48550/arXiv.2311.15354"
    },
    "queries": [
      {
        "anchor": "synchronous dialogue",
        "search_term": "real-time interaction"
      }
    ]
  },
  {
    "arxiv_id": "2311.17370",
    "anchor": "spoken language models",
    "search_term": "streaming architecture",
    "search_record": {
      "id": "http://arxiv.org/abs/2311.17370v1",
      "title": "Efficient and Scalable Architecture for Multiple-chip Implementation of Simulated Bifurcation Machines",
      "summary": "Ising machines are specialized computers for finding the lowest energy states of Ising spin models, onto which many practical combinatorial optimization problems can be mapped. Simulated bifurcation (SB) is a quantum-inspired parallelizable algorithm for Ising problems that enables scalable multi-chip implementations of Ising machines. However, the computational performance of a previously proposed multi-chip architecture tends to saturate as the number of chips increases for a given problem size because both computation and communication are exclusive in the time domain. In this paper, we propose a streaming architecture for multi-chip implementations of SB-based Ising machines with full spin-to-spin connectivity. The data flow in in-chip computation is harmonized with the data flow in inter-chip communication, enabling the computation and communication to overlap and the communication time to be hidden. Systematic experiments demonstrate linear strong scaling of performance up to the vicinity of the ideal communication limit determined only by the latency of chip-to-chip communication. Our eight-FPGA (field-programmable gate array) cluster can compute a 32,768-spin problem with a high pipeline efficiency of 97.9%. The performance of a 79-FPGA cluster for a 100,000-spin problem, projected using a theoretical performance model validated on smaller experimental clusters, is comparable to that of a state-of-the-art 100,000-spin optical Ising machine.",
      "published": "2023-11-29T05:49:00Z"
    },
    "metadata": {
      "arxiv_id": "2311.17370",
      "title": "Efficient and Scalable Architecture for Multiple-chip Implementation of Simulated Bifurcation Machines",
      "summary": "Ising machines are specialized computers for finding the lowest energy states of Ising spin models, onto which many practical combinatorial optimization problems can be mapped. Simulated bifurcation (SB) is a quantum-inspired parallelizable algorithm for Ising problems that enables scalable multi-chip implementations of Ising machines. However, the computational performance of a previously proposed multi-chip architecture tends to saturate as the number of chips increases for a given problem size because both computation and communication are exclusive in the time domain. In this paper, we propose a streaming architecture for multi-chip implementations of SB-based Ising machines with full spin-to-spin connectivity. The data flow in in-chip computation is harmonized with the data flow in inter-chip communication, enabling the computation and communication to overlap and the communication time to be hidden. Systematic experiments demonstrate linear strong scaling of performance up to the vicinity of the ideal communication limit determined only by the latency of chip-to-chip communication. Our eight-FPGA (field-programmable gate array) cluster can compute a 32,768-spin problem with a high pipeline efficiency of 97.9%. The performance of a 79-FPGA cluster for a 100,000-spin problem, projected using a theoretical performance model validated on smaller experimental clusters, is comparable to that of a state-of-the-art 100,000-spin optical Ising machine.",
      "authors": [
        "Tomoya Kashimata",
        "Masaya Yamasaki",
        "Ryo Hidaka",
        "Kosuke Tatsumura"
      ],
      "published": "2023-11-29T05:49:00Z",
      "updated": "2023-11-29T05:49:00Z",
      "categories": [
        "cs.ET",
        "cs.AR",
        "cs.DC",
        "cs.PF"
      ],
      "pdf_url": "https://arxiv.org/pdf/2311.17370v1",
      "landing_url": "https://arxiv.org/abs/2311.17370v1",
      "doi": "https://doi.org/10.1109/ACCESS.2024.3374089"
    },
    "queries": [
      {
        "anchor": "spoken language models",
        "search_term": "streaming architecture"
      },
      {
        "anchor": "synchronous dialogue",
        "search_term": "streaming architecture"
      }
    ]
  },
  {
    "arxiv_id": "2311.18286",
    "anchor": "synchronous dialogue",
    "search_term": "streaming architecture",
    "search_record": {
      "id": "http://arxiv.org/abs/2311.18286v1",
      "title": "SimulFlow: Simultaneously Extracting Feature and Identifying Target for Unsupervised Video Object Segmentation",
      "summary": "Unsupervised video object segmentation (UVOS) aims at detecting the primary objects in a given video sequence without any human interposing. Most existing methods rely on two-stream architectures that separately encode the appearance and motion information before fusing them to identify the target and generate object masks. However, this pipeline is computationally expensive and can lead to suboptimal performance due to the difficulty of fusing the two modalities properly. In this paper, we propose a novel UVOS model called SimulFlow that simultaneously performs feature extraction and target identification, enabling efficient and effective unsupervised video object segmentation. Concretely, we design a novel SimulFlow Attention mechanism to bridege the image and motion by utilizing the flexibility of attention operation, where coarse masks predicted from fused feature at each stage are used to constrain the attention operation within the mask area and exclude the impact of noise. Because of the bidirectional information flow between visual and optical flow features in SimulFlow Attention, no extra hand-designed fusing module is required and we only adopt a light decoder to obtain the final prediction. We evaluate our method on several benchmark datasets and achieve state-of-the-art results. Our proposed approach not only outperforms existing methods but also addresses the computational complexity and fusion difficulties caused by two-stream architectures. Our models achieve 87.4% J & F on DAVIS-16 with the highest speed (63.7 FPS on a 3090) and the lowest parameters (13.7 M). Our SimulFlow also obtains competitive results on video salient object detection datasets.",
      "published": "2023-11-30T06:44:44Z"
    },
    "metadata": {
      "arxiv_id": "2311.18286",
      "title": "SimulFlow: Simultaneously Extracting Feature and Identifying Target for Unsupervised Video Object Segmentation",
      "summary": "Unsupervised video object segmentation (UVOS) aims at detecting the primary objects in a given video sequence without any human interposing. Most existing methods rely on two-stream architectures that separately encode the appearance and motion information before fusing them to identify the target and generate object masks. However, this pipeline is computationally expensive and can lead to suboptimal performance due to the difficulty of fusing the two modalities properly. In this paper, we propose a novel UVOS model called SimulFlow that simultaneously performs feature extraction and target identification, enabling efficient and effective unsupervised video object segmentation. Concretely, we design a novel SimulFlow Attention mechanism to bridege the image and motion by utilizing the flexibility of attention operation, where coarse masks predicted from fused feature at each stage are used to constrain the attention operation within the mask area and exclude the impact of noise. Because of the bidirectional information flow between visual and optical flow features in SimulFlow Attention, no extra hand-designed fusing module is required and we only adopt a light decoder to obtain the final prediction. We evaluate our method on several benchmark datasets and achieve state-of-the-art results. Our proposed approach not only outperforms existing methods but also addresses the computational complexity and fusion difficulties caused by two-stream architectures. Our models achieve 87.4% J & F on DAVIS-16 with the highest speed (63.7 FPS on a 3090) and the lowest parameters (13.7 M). Our SimulFlow also obtains competitive results on video salient object detection datasets.",
      "authors": [
        "Lingyi Hong",
        "Wei Zhang",
        "Shuyong Gao",
        "Hong Lu",
        "WenQiang Zhang"
      ],
      "published": "2023-11-30T06:44:44Z",
      "updated": "2023-11-30T06:44:44Z",
      "categories": [
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2311.18286v1",
      "landing_url": "https://arxiv.org/abs/2311.18286v1",
      "doi": "https://doi.org/10.48550/arXiv.2311.18286"
    },
    "queries": [
      {
        "anchor": "synchronous dialogue",
        "search_term": "streaming architecture"
      }
    ]
  },
  {
    "arxiv_id": "2311.18825",
    "anchor": "synchronous dialogue",
    "search_term": "streaming architecture",
    "search_record": {
      "id": "http://arxiv.org/abs/2311.18825v2",
      "title": "CAST: Cross-Attention in Space and Time for Video Action Recognition",
      "summary": "Recognizing human actions in videos requires spatial and temporal understanding. Most existing action recognition models lack a balanced spatio-temporal understanding of videos. In this work, we propose a novel two-stream architecture, called Cross-Attention in Space and Time (CAST), that achieves a balanced spatio-temporal understanding of videos using only RGB input. Our proposed bottleneck cross-attention mechanism enables the spatial and temporal expert models to exchange information and make synergistic predictions, leading to improved performance. We validate the proposed method with extensive experiments on public benchmarks with different characteristics: EPIC-KITCHENS-100, Something-Something-V2, and Kinetics-400. Our method consistently shows favorable performance across these datasets, while the performance of existing methods fluctuates depending on the dataset characteristics.",
      "published": "2023-11-30T18:58:51Z"
    },
    "metadata": {
      "arxiv_id": "2311.18825",
      "title": "CAST: Cross-Attention in Space and Time for Video Action Recognition",
      "summary": "Recognizing human actions in videos requires spatial and temporal understanding. Most existing action recognition models lack a balanced spatio-temporal understanding of videos. In this work, we propose a novel two-stream architecture, called Cross-Attention in Space and Time (CAST), that achieves a balanced spatio-temporal understanding of videos using only RGB input. Our proposed bottleneck cross-attention mechanism enables the spatial and temporal expert models to exchange information and make synergistic predictions, leading to improved performance. We validate the proposed method with extensive experiments on public benchmarks with different characteristics: EPIC-KITCHENS-100, Something-Something-V2, and Kinetics-400. Our method consistently shows favorable performance across these datasets, while the performance of existing methods fluctuates depending on the dataset characteristics.",
      "authors": [
        "Dongho Lee",
        "Jongseo Lee",
        "Jinwoo Choi"
      ],
      "published": "2023-11-30T18:58:51Z",
      "updated": "2024-09-03T08:16:32Z",
      "categories": [
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2311.18825v2",
      "landing_url": "https://arxiv.org/abs/2311.18825v2",
      "doi": "https://doi.org/10.48550/arXiv.2311.18825"
    },
    "queries": [
      {
        "anchor": "synchronous dialogue",
        "search_term": "streaming architecture"
      }
    ]
  },
  {
    "arxiv_id": "2312.02327",
    "anchor": "spoken language models",
    "search_term": "data scarcity",
    "search_record": {
      "id": "http://arxiv.org/abs/2312.02327v2",
      "title": "FLea: Addressing Data Scarcity and Label Skew in Federated Learning via Privacy-preserving Feature Augmentation",
      "summary": "Federated Learning (FL) enables model development by leveraging data distributed across numerous edge devices without transferring local data to a central server. However, existing FL methods still face challenges when dealing with scarce and label-skewed data across devices, resulting in local model overfitting and drift, consequently hindering the performance of the global model. In response to these challenges, we propose a pioneering framework called \\textit{FLea}, incorporating the following key components: \\textit{i)} A global feature buffer that stores activation-target pairs shared from multiple clients to support local training. This design mitigates local model drift caused by the absence of certain classes; \\textit{ii)} A feature augmentation approach based on local and global activation mix-ups for local training. This strategy enlarges the training samples, thereby reducing the risk of local overfitting; \\textit{iii)} An obfuscation method to minimize the correlation between intermediate activations and the source data, enhancing the privacy of shared features. To verify the superiority of \\textit{FLea}, we conduct extensive experiments using a wide range of data modalities, simulating different levels of local data scarcity and label skew. The results demonstrate that \\textit{FLea} consistently outperforms state-of-the-art FL counterparts (among 13 of the experimented 18 settings, the improvement is over $5\\%$) while concurrently mitigating the privacy vulnerabilities associated with shared features. Code is available at https://github.com/XTxiatong/FLea.git",
      "published": "2023-12-04T20:24:09Z"
    },
    "metadata": {
      "arxiv_id": "2312.02327",
      "title": "FLea: Addressing Data Scarcity and Label Skew in Federated Learning via Privacy-preserving Feature Augmentation",
      "summary": "Federated Learning (FL) enables model development by leveraging data distributed across numerous edge devices without transferring local data to a central server. However, existing FL methods still face challenges when dealing with scarce and label-skewed data across devices, resulting in local model overfitting and drift, consequently hindering the performance of the global model. In response to these challenges, we propose a pioneering framework called \\textit{FLea}, incorporating the following key components: \\textit{i)} A global feature buffer that stores activation-target pairs shared from multiple clients to support local training. This design mitigates local model drift caused by the absence of certain classes; \\textit{ii)} A feature augmentation approach based on local and global activation mix-ups for local training. This strategy enlarges the training samples, thereby reducing the risk of local overfitting; \\textit{iii)} An obfuscation method to minimize the correlation between intermediate activations and the source data, enhancing the privacy of shared features. To verify the superiority of \\textit{FLea}, we conduct extensive experiments using a wide range of data modalities, simulating different levels of local data scarcity and label skew. The results demonstrate that \\textit{FLea} consistently outperforms state-of-the-art FL counterparts (among 13 of the experimented 18 settings, the improvement is over $5\\%$) while concurrently mitigating the privacy vulnerabilities associated with shared features. Code is available at https://github.com/XTxiatong/FLea.git",
      "authors": [
        "Tong Xia",
        "Abhirup Ghosh",
        "Xinchi Qiu",
        "Cecilia Mascolo"
      ],
      "published": "2023-12-04T20:24:09Z",
      "updated": "2024-07-02T01:37:34Z",
      "categories": [
        "cs.LG",
        "cs.CR",
        "cs.DC"
      ],
      "pdf_url": "https://arxiv.org/pdf/2312.02327v2",
      "landing_url": "https://arxiv.org/abs/2312.02327v2",
      "doi": "https://doi.org/10.1145/3637528.3671899"
    },
    "queries": [
      {
        "anchor": "spoken language models",
        "search_term": "data scarcity"
      },
      {
        "anchor": "synchronous dialogue",
        "search_term": "data scarcity"
      },
      {
        "anchor": "turn-taking",
        "search_term": "data scarcity"
      }
    ]
  },
  {
    "arxiv_id": "2312.09747",
    "anchor": "spoken language models",
    "search_term": "discrete tokens",
    "search_record": {
      "id": "http://arxiv.org/abs/2312.09747v2",
      "title": "SELM: Speech Enhancement Using Discrete Tokens and Language Models",
      "summary": "Language models (LMs) have shown superior performances in various speech generation tasks recently, demonstrating their powerful ability for semantic context modeling. Given the intrinsic similarity between speech generation and speech enhancement, harnessing semantic information holds potential advantages for speech enhancement tasks. In light of this, we propose SELM, a novel paradigm for speech enhancement, which integrates discrete tokens and leverages language models. SELM comprises three stages: encoding, modeling, and decoding. We transform continuous waveform signals into discrete tokens using pre-trained self-supervised learning (SSL) models and a k-means tokenizer. Language models then capture comprehensive contextual information within these tokens. Finally, a detokenizer and HiFi-GAN restore them into enhanced speech. Experimental results demonstrate that SELM achieves comparable performance in objective metrics alongside superior results in subjective perception. Our demos are available https://honee-w.github.io/SELM/.",
      "published": "2023-12-15T12:36:05Z"
    },
    "metadata": {
      "arxiv_id": "2312.09747",
      "title": "SELM: Speech Enhancement Using Discrete Tokens and Language Models",
      "summary": "Language models (LMs) have shown superior performances in various speech generation tasks recently, demonstrating their powerful ability for semantic context modeling. Given the intrinsic similarity between speech generation and speech enhancement, harnessing semantic information holds potential advantages for speech enhancement tasks. In light of this, we propose SELM, a novel paradigm for speech enhancement, which integrates discrete tokens and leverages language models. SELM comprises three stages: encoding, modeling, and decoding. We transform continuous waveform signals into discrete tokens using pre-trained self-supervised learning (SSL) models and a k-means tokenizer. Language models then capture comprehensive contextual information within these tokens. Finally, a detokenizer and HiFi-GAN restore them into enhanced speech. Experimental results demonstrate that SELM achieves comparable performance in objective metrics alongside superior results in subjective perception. Our demos are available https://honee-w.github.io/SELM/.",
      "authors": [
        "Ziqian Wang",
        "Xinfa Zhu",
        "Zihan Zhang",
        "YuanJun Lv",
        "Ning Jiang",
        "Guoqing Zhao",
        "Lei Xie"
      ],
      "published": "2023-12-15T12:36:05Z",
      "updated": "2024-01-07T09:02:52Z",
      "categories": [
        "eess.AS",
        "eess.SP"
      ],
      "pdf_url": "https://arxiv.org/pdf/2312.09747v2",
      "landing_url": "https://arxiv.org/abs/2312.09747v2",
      "doi": "https://doi.org/10.48550/arXiv.2312.09747"
    },
    "queries": [
      {
        "anchor": "spoken language models",
        "search_term": "discrete tokens"
      },
      {
        "anchor": "synchronous dialogue",
        "search_term": "discrete tokens"
      },
      {
        "anchor": "turn-taking",
        "search_term": "discrete tokens"
      }
    ]
  },
  {
    "arxiv_id": "2312.10073",
    "anchor": "spoken language models",
    "search_term": "data scarcity",
    "search_record": {
      "id": "http://arxiv.org/abs/2312.10073v1",
      "title": "Data Scarcity in Recommendation Systems: A Survey",
      "summary": "The prevalence of online content has led to the widespread adoption of recommendation systems (RSs), which serve diverse purposes such as news, advertisements, and e-commerce recommendations. Despite their significance, data scarcity issues have significantly impaired the effectiveness of existing RS models and hindered their progress. To address this challenge, the concept of knowledge transfer, particularly from external sources like pre-trained language models, emerges as a potential solution to alleviate data scarcity and enhance RS development. However, the practice of knowledge transfer in RSs is intricate. Transferring knowledge between domains introduces data disparities, and the application of knowledge transfer in complex RS scenarios can yield negative consequences if not carefully designed. Therefore, this article contributes to this discourse by addressing the implications of data scarcity on RSs and introducing various strategies, such as data augmentation, self-supervised learning, transfer learning, broad learning, and knowledge graph utilization, to mitigate this challenge. Furthermore, it delves into the challenges and future direction within the RS domain, offering insights that are poised to facilitate the development and implementation of robust RSs, particularly when confronted with data scarcity. We aim to provide valuable guidance and inspiration for researchers and practitioners, ultimately driving advancements in the field of RS.",
      "published": "2023-12-08T01:45:58Z"
    },
    "metadata": {
      "arxiv_id": "2312.10073",
      "title": "Data Scarcity in Recommendation Systems: A Survey",
      "summary": "The prevalence of online content has led to the widespread adoption of recommendation systems (RSs), which serve diverse purposes such as news, advertisements, and e-commerce recommendations. Despite their significance, data scarcity issues have significantly impaired the effectiveness of existing RS models and hindered their progress. To address this challenge, the concept of knowledge transfer, particularly from external sources like pre-trained language models, emerges as a potential solution to alleviate data scarcity and enhance RS development. However, the practice of knowledge transfer in RSs is intricate. Transferring knowledge between domains introduces data disparities, and the application of knowledge transfer in complex RS scenarios can yield negative consequences if not carefully designed. Therefore, this article contributes to this discourse by addressing the implications of data scarcity on RSs and introducing various strategies, such as data augmentation, self-supervised learning, transfer learning, broad learning, and knowledge graph utilization, to mitigate this challenge. Furthermore, it delves into the challenges and future direction within the RS domain, offering insights that are poised to facilitate the development and implementation of robust RSs, particularly when confronted with data scarcity. We aim to provide valuable guidance and inspiration for researchers and practitioners, ultimately driving advancements in the field of RS.",
      "authors": [
        "Zefeng Chen",
        "Wensheng Gan",
        "Jiayang Wu",
        "Kaixia Hu",
        "Hong Lin"
      ],
      "published": "2023-12-08T01:45:58Z",
      "updated": "2023-12-08T01:45:58Z",
      "categories": [
        "cs.IR",
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2312.10073v1",
      "landing_url": "https://arxiv.org/abs/2312.10073v1",
      "doi": "https://doi.org/10.48550/arXiv.2312.10073"
    },
    "queries": [
      {
        "anchor": "spoken language models",
        "search_term": "data scarcity"
      },
      {
        "anchor": "synchronous dialogue",
        "search_term": "data scarcity"
      },
      {
        "anchor": "turn-taking",
        "search_term": "data scarcity"
      }
    ]
  },
  {
    "arxiv_id": "2312.12491",
    "anchor": "spoken language models",
    "search_term": "real-time interaction",
    "search_record": {
      "id": "http://arxiv.org/abs/2312.12491v2",
      "title": "StreamDiffusion: A Pipeline-level Solution for Real-time Interactive Generation",
      "summary": "We introduce StreamDiffusion, a real-time diffusion pipeline designed for interactive image generation. Existing diffusion models are adept at creating images from text or image prompts, yet they often fall short in real-time interaction. This limitation becomes particularly evident in scenarios involving continuous input, such as Metaverse, live video streaming, and broadcasting, where high throughput is imperative. To address this, we present a novel approach that transforms the original sequential denoising into the batching denoising process. Stream Batch eliminates the conventional wait-and-interact approach and enables fluid and high throughput streams. To handle the frequency disparity between data input and model throughput, we design a novel input-output queue for parallelizing the streaming process. Moreover, the existing diffusion pipeline uses classifier-free guidance(CFG), which requires additional U-Net computation. To mitigate the redundant computations, we propose a novel residual classifier-free guidance (RCFG) algorithm that reduces the number of negative conditional denoising steps to only one or even zero. Besides, we introduce a stochastic similarity filter(SSF) to optimize power consumption. Our Stream Batch achieves around 1.5x speedup compared to the sequential denoising method at different denoising levels. The proposed RCFG leads to speeds up to 2.05x higher than the conventional CFG. Combining the proposed strategies and existing mature acceleration tools makes the image-to-image generation achieve up-to 91.07fps on one RTX4090, improving the throughputs of AutoPipline developed by Diffusers over 59.56x. Furthermore, our proposed StreamDiffusion also significantly reduces the energy consumption by 2.39x on one RTX3060 and 1.99x on one RTX4090, respectively.",
      "published": "2023-12-19T18:18:33Z"
    },
    "metadata": {
      "arxiv_id": "2312.12491",
      "title": "StreamDiffusion: A Pipeline-level Solution for Real-time Interactive Generation",
      "summary": "We introduce StreamDiffusion, a real-time diffusion pipeline designed for interactive image generation. Existing diffusion models are adept at creating images from text or image prompts, yet they often fall short in real-time interaction. This limitation becomes particularly evident in scenarios involving continuous input, such as Metaverse, live video streaming, and broadcasting, where high throughput is imperative. To address this, we present a novel approach that transforms the original sequential denoising into the batching denoising process. Stream Batch eliminates the conventional wait-and-interact approach and enables fluid and high throughput streams. To handle the frequency disparity between data input and model throughput, we design a novel input-output queue for parallelizing the streaming process. Moreover, the existing diffusion pipeline uses classifier-free guidance(CFG), which requires additional U-Net computation. To mitigate the redundant computations, we propose a novel residual classifier-free guidance (RCFG) algorithm that reduces the number of negative conditional denoising steps to only one or even zero. Besides, we introduce a stochastic similarity filter(SSF) to optimize power consumption. Our Stream Batch achieves around 1.5x speedup compared to the sequential denoising method at different denoising levels. The proposed RCFG leads to speeds up to 2.05x higher than the conventional CFG. Combining the proposed strategies and existing mature acceleration tools makes the image-to-image generation achieve up-to 91.07fps on one RTX4090, improving the throughputs of AutoPipline developed by Diffusers over 59.56x. Furthermore, our proposed StreamDiffusion also significantly reduces the energy consumption by 2.39x on one RTX3060 and 1.99x on one RTX4090, respectively.",
      "authors": [
        "Akio Kodaira",
        "Chenfeng Xu",
        "Toshiki Hazama",
        "Takanori Yoshimoto",
        "Kohei Ohno",
        "Shogo Mitsuhori",
        "Soichi Sugano",
        "Hanying Cho",
        "Zhijian Liu",
        "Masayoshi Tomizuka",
        "Kurt Keutzer"
      ],
      "published": "2023-12-19T18:18:33Z",
      "updated": "2025-07-08T17:45:49Z",
      "categories": [
        "cs.CV",
        "cs.GR",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2312.12491v2",
      "landing_url": "https://arxiv.org/abs/2312.12491v2",
      "doi": "https://doi.org/10.48550/arXiv.2312.12491"
    },
    "queries": [
      {
        "anchor": "spoken language models",
        "search_term": "real-time interaction"
      },
      {
        "anchor": "synchronous dialogue",
        "search_term": "real-time interaction"
      },
      {
        "anchor": "turn-taking",
        "search_term": "real-time interaction"
      }
    ]
  },
  {
    "arxiv_id": "2312.13715",
    "anchor": "turn-taking",
    "search_term": "true full-duplex",
    "search_record": {
      "id": "http://arxiv.org/abs/2312.13715v1",
      "title": "Meta-control of Dialogue Systems Using Large Language Models",
      "summary": "Utilizing Large Language Models (LLMs) facilitates the creation of flexible and natural dialogues, a task that has been challenging with traditional rule-based dialogue systems. However, LLMs also have the potential to produce unexpected responses, which may not align with the intentions of dialogue system designers. To address this issue, this paper introduces a meta-control method that employs LLMs to develop more stable and adaptable dialogue systems. The method includes dialogue flow control to ensure that utterances conform to predefined scenarios and turn-taking control to foster natural dialogues. Furthermore, we have implemented a dialogue system that utilizes this meta-control strategy and verified that the dialogue system utilizing meta-control operates as intended.",
      "published": "2023-12-21T10:28:18Z"
    },
    "metadata": {
      "arxiv_id": "2312.13715",
      "title": "Meta-control of Dialogue Systems Using Large Language Models",
      "summary": "Utilizing Large Language Models (LLMs) facilitates the creation of flexible and natural dialogues, a task that has been challenging with traditional rule-based dialogue systems. However, LLMs also have the potential to produce unexpected responses, which may not align with the intentions of dialogue system designers. To address this issue, this paper introduces a meta-control method that employs LLMs to develop more stable and adaptable dialogue systems. The method includes dialogue flow control to ensure that utterances conform to predefined scenarios and turn-taking control to foster natural dialogues. Furthermore, we have implemented a dialogue system that utilizes this meta-control strategy and verified that the dialogue system utilizing meta-control operates as intended.",
      "authors": [
        "Kotaro Shukuri",
        "Ryoma Ishigaki",
        "Jundai Suzuki",
        "Tsubasa Naganuma",
        "Takuma Fujimoto",
        "Daisuke Kawakubo",
        "Masaki Shuzo",
        "Eisaku Maeda"
      ],
      "published": "2023-12-21T10:28:18Z",
      "updated": "2023-12-21T10:28:18Z",
      "categories": [
        "cs.RO"
      ],
      "pdf_url": "https://arxiv.org/pdf/2312.13715v1",
      "landing_url": "https://arxiv.org/abs/2312.13715v1",
      "doi": "https://doi.org/10.48550/arXiv.2312.13715"
    },
    "queries": [
      {
        "anchor": "turn-taking",
        "search_term": "true full-duplex"
      },
      {
        "anchor": "turn-taking",
        "search_term": "pseudo full-duplex"
      },
      {
        "anchor": "turn-taking",
        "search_term": "engineered synchronization"
      },
      {
        "anchor": "turn-taking",
        "search_term": "learned synchronization"
      },
      {
        "anchor": "turn-taking",
        "search_term": "synchronous data"
      },
      {
        "anchor": "turn-taking",
        "search_term": "multichannel dialogue"
      },
      {
        "anchor": "turn-taking",
        "search_term": "behavioral arbitration"
      },
      {
        "anchor": "turn-taking",
        "search_term": "floor transfer offset"
      },
      {
        "anchor": "turn-taking",
        "search_term": "stop latency"
      },
      {
        "anchor": "turn-taking",
        "search_term": "interruption response delay"
      }
    ]
  },
  {
    "arxiv_id": "2312.13816",
    "anchor": "turn-taking",
    "search_term": "true full-duplex",
    "search_record": {
      "id": "http://arxiv.org/abs/2312.13816v1",
      "title": "Team Flow at DRC2023: Building Common Ground and Text-based Turn-taking in a Travel Agent Spoken Dialogue System",
      "summary": "At the Dialogue Robot Competition 2023 (DRC2023), which was held to improve the capability of dialogue robots, our team developed a system that could build common ground and take more natural turns based on user utterance texts. Our system generated queries for sightseeing spot searches using the common ground and engaged in dialogue while waiting for user comprehension.",
      "published": "2023-12-21T13:08:09Z"
    },
    "metadata": {
      "arxiv_id": "2312.13816",
      "title": "Team Flow at DRC2023: Building Common Ground and Text-based Turn-taking in a Travel Agent Spoken Dialogue System",
      "summary": "At the Dialogue Robot Competition 2023 (DRC2023), which was held to improve the capability of dialogue robots, our team developed a system that could build common ground and take more natural turns based on user utterance texts. Our system generated queries for sightseeing spot searches using the common ground and engaged in dialogue while waiting for user comprehension.",
      "authors": [
        "Ryu Hirai",
        "Shinya Iizuka",
        "Haruhisa Iseno",
        "Ao Guo",
        "Jingjing Jiang",
        "Atsumoto Ohashi",
        "Ryuichiro Higashinaka"
      ],
      "published": "2023-12-21T13:08:09Z",
      "updated": "2023-12-21T13:08:09Z",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.RO"
      ],
      "pdf_url": "https://arxiv.org/pdf/2312.13816v1",
      "landing_url": "https://arxiv.org/abs/2312.13816v1",
      "doi": "https://doi.org/10.48550/arXiv.2312.13816"
    },
    "queries": [
      {
        "anchor": "turn-taking",
        "search_term": "true full-duplex"
      },
      {
        "anchor": "turn-taking",
        "search_term": "pseudo full-duplex"
      },
      {
        "anchor": "turn-taking",
        "search_term": "engineered synchronization"
      },
      {
        "anchor": "turn-taking",
        "search_term": "learned synchronization"
      },
      {
        "anchor": "turn-taking",
        "search_term": "modular architecture"
      },
      {
        "anchor": "turn-taking",
        "search_term": "streaming architecture"
      },
      {
        "anchor": "turn-taking",
        "search_term": "real-time interaction"
      },
      {
        "anchor": "turn-taking",
        "search_term": "synchronous data"
      },
      {
        "anchor": "turn-taking",
        "search_term": "multichannel dialogue"
      },
      {
        "anchor": "turn-taking",
        "search_term": "behavioral arbitration"
      },
      {
        "anchor": "turn-taking",
        "search_term": "semantic coherence"
      },
      {
        "anchor": "turn-taking",
        "search_term": "neural codec"
      },
      {
        "anchor": "turn-taking",
        "search_term": "audio tokens"
      },
      {
        "anchor": "turn-taking",
        "search_term": "floor transfer offset"
      },
      {
        "anchor": "turn-taking",
        "search_term": "stop latency"
      },
      {
        "anchor": "turn-taking",
        "search_term": "interruption response delay"
      }
    ]
  },
  {
    "arxiv_id": "2312.14562",
    "anchor": "synchronous dialogue",
    "search_term": "modular architecture",
    "search_record": {
      "id": "http://arxiv.org/abs/2312.14562v1",
      "title": "Measuring the Concentration of Control in Contemporary Ethereum",
      "summary": "Ethereum is undergoing significant changes to its architecture as it evolves. These changes include its switch to PoS consensus and the introduction of significant infrastructural changes that do not require a change to the core protocol, but that fundamentally affect the way users interact with the network. These changes represent an evolution toward a more modular architecture, in which there exists new exogenous vectors for centralization. This paper builds on previous studies of decentralization of Ethereum to reflect these recent significant changes, and Ethereum's new modular paradigm.",
      "published": "2023-12-22T09:47:52Z"
    },
    "metadata": {
      "arxiv_id": "2312.14562",
      "title": "Measuring the Concentration of Control in Contemporary Ethereum",
      "summary": "Ethereum is undergoing significant changes to its architecture as it evolves. These changes include its switch to PoS consensus and the introduction of significant infrastructural changes that do not require a change to the core protocol, but that fundamentally affect the way users interact with the network. These changes represent an evolution toward a more modular architecture, in which there exists new exogenous vectors for centralization. This paper builds on previous studies of decentralization of Ethereum to reflect these recent significant changes, and Ethereum's new modular paradigm.",
      "authors": [
        "Simon Brown"
      ],
      "published": "2023-12-22T09:47:52Z",
      "updated": "2023-12-22T09:47:52Z",
      "categories": [
        "econ.TH"
      ],
      "pdf_url": "https://arxiv.org/pdf/2312.14562v1",
      "landing_url": "https://arxiv.org/abs/2312.14562v1",
      "doi": "https://doi.org/10.48550/arXiv.2312.14562"
    },
    "queries": [
      {
        "anchor": "synchronous dialogue",
        "search_term": "modular architecture"
      }
    ]
  },
  {
    "arxiv_id": "2312.14965",
    "anchor": "spoken language models",
    "search_term": "temporal dynamics",
    "search_record": {
      "id": "http://arxiv.org/abs/2312.14965v1",
      "title": "Unraveling the Temporal Dynamics of the Unet in Diffusion Models",
      "summary": "Diffusion models have garnered significant attention since they can effectively learn complex multivariate Gaussian distributions, resulting in diverse, high-quality outcomes. They introduce Gaussian noise into training data and reconstruct the original data iteratively. Central to this iterative process is a single Unet, adapting across time steps to facilitate generation. Recent work revealed the presence of composition and denoising phases in this generation process, raising questions about the Unets' varying roles. Our study dives into the dynamic behavior of Unets within denoising diffusion probabilistic models (DDPM), focusing on (de)convolutional blocks and skip connections across time steps. We propose an analytical method to systematically assess the impact of time steps and core Unet components on the final output. This method eliminates components to study causal relations and investigate their influence on output changes. The main purpose is to understand the temporal dynamics and identify potential shortcuts during inference. Our findings provide valuable insights into the various generation phases during inference and shed light on the Unets' usage patterns across these phases. Leveraging these insights, we identify redundancies in GLIDE (an improved DDPM) and improve inference time by ~27% with minimal degradation in output quality. Our ultimate goal is to guide more informed optimization strategies for inference and influence new model designs.",
      "published": "2023-12-17T04:40:33Z"
    },
    "metadata": {
      "arxiv_id": "2312.14965",
      "title": "Unraveling the Temporal Dynamics of the Unet in Diffusion Models",
      "summary": "Diffusion models have garnered significant attention since they can effectively learn complex multivariate Gaussian distributions, resulting in diverse, high-quality outcomes. They introduce Gaussian noise into training data and reconstruct the original data iteratively. Central to this iterative process is a single Unet, adapting across time steps to facilitate generation. Recent work revealed the presence of composition and denoising phases in this generation process, raising questions about the Unets' varying roles. Our study dives into the dynamic behavior of Unets within denoising diffusion probabilistic models (DDPM), focusing on (de)convolutional blocks and skip connections across time steps. We propose an analytical method to systematically assess the impact of time steps and core Unet components on the final output. This method eliminates components to study causal relations and investigate their influence on output changes. The main purpose is to understand the temporal dynamics and identify potential shortcuts during inference. Our findings provide valuable insights into the various generation phases during inference and shed light on the Unets' usage patterns across these phases. Leveraging these insights, we identify redundancies in GLIDE (an improved DDPM) and improve inference time by ~27% with minimal degradation in output quality. Our ultimate goal is to guide more informed optimization strategies for inference and influence new model designs.",
      "authors": [
        "Vidya Prasad",
        "Chen Zhu-Tian",
        "Anna Vilanova",
        "Hanspeter Pfister",
        "Nicola Pezzotti",
        "Hendrik Strobelt"
      ],
      "published": "2023-12-17T04:40:33Z",
      "updated": "2023-12-17T04:40:33Z",
      "categories": [
        "cs.CV",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2312.14965v1",
      "landing_url": "https://arxiv.org/abs/2312.14965v1",
      "doi": "https://doi.org/10.48550/arXiv.2312.14965"
    },
    "queries": [
      {
        "anchor": "spoken language models",
        "search_term": "temporal dynamics"
      },
      {
        "anchor": "synchronous dialogue",
        "search_term": "temporal dynamics"
      }
    ]
  },
  {
    "arxiv_id": "2312.15985",
    "anchor": "spoken language models",
    "search_term": "discrete tokens",
    "search_record": {
      "id": "http://arxiv.org/abs/2312.15985v3",
      "title": "Discrete Messages Improve Communication Efficiency among Isolated Intelligent Agents",
      "summary": "Individuals, despite having varied life experiences and learning processes, can communicate effectively through languages. This study aims to explore the efficiency of language as a communication medium. We put forth two specific hypotheses: First, discrete messages are more effective than continuous ones when agents have diverse personal experiences. Second, communications using multiple discrete tokens are more advantageous than those using a single token. To valdate these hypotheses, we designed multi-agent machine learning experiments to assess communication efficiency using various information transmission methods between speakers and listeners. Our empirical findings indicate that, in scenarios where agents are exposed to different data, communicating through sentences composed of discrete tokens offers the best inter-agent communication efficiency. The limitations of our finding include lack of systematic advantages over other more sophisticated encoder-decoder model such as variational autoencoder and lack of evluation on non-image dataset, which we will leave for future studies.",
      "published": "2023-12-26T10:30:05Z"
    },
    "metadata": {
      "arxiv_id": "2312.15985",
      "title": "Discrete Messages Improve Communication Efficiency among Isolated Intelligent Agents",
      "summary": "Individuals, despite having varied life experiences and learning processes, can communicate effectively through languages. This study aims to explore the efficiency of language as a communication medium. We put forth two specific hypotheses: First, discrete messages are more effective than continuous ones when agents have diverse personal experiences. Second, communications using multiple discrete tokens are more advantageous than those using a single token. To valdate these hypotheses, we designed multi-agent machine learning experiments to assess communication efficiency using various information transmission methods between speakers and listeners. Our empirical findings indicate that, in scenarios where agents are exposed to different data, communicating through sentences composed of discrete tokens offers the best inter-agent communication efficiency. The limitations of our finding include lack of systematic advantages over other more sophisticated encoder-decoder model such as variational autoencoder and lack of evluation on non-image dataset, which we will leave for future studies.",
      "authors": [
        "Hang Chen",
        "Yuchuan Jang",
        "Weijie Zhou",
        "Cristian Meo",
        "Ziwei Chen",
        "Dianbo Liu"
      ],
      "published": "2023-12-26T10:30:05Z",
      "updated": "2024-10-18T02:22:19Z",
      "categories": [
        "cs.LG",
        "cs.IT"
      ],
      "pdf_url": "https://arxiv.org/pdf/2312.15985v3",
      "landing_url": "https://arxiv.org/abs/2312.15985v3",
      "doi": "https://doi.org/10.48550/arXiv.2312.15985"
    },
    "queries": [
      {
        "anchor": "spoken language models",
        "search_term": "discrete tokens"
      },
      {
        "anchor": "synchronous dialogue",
        "search_term": "discrete tokens"
      }
    ]
  },
  {
    "arxiv_id": "2401.01912",
    "anchor": "synchronous dialogue",
    "search_term": "low latency",
    "search_record": {
      "id": "http://arxiv.org/abs/2401.01912v1",
      "title": "Shrinking Your TimeStep: Towards Low-Latency Neuromorphic Object Recognition with Spiking Neural Network",
      "summary": "Neuromorphic object recognition with spiking neural networks (SNNs) is the cornerstone of low-power neuromorphic computing. However, existing SNNs suffer from significant latency, utilizing 10 to 40 timesteps or more, to recognize neuromorphic objects. At low latencies, the performance of existing SNNs is drastically degraded. In this work, we propose the Shrinking SNN (SSNN) to achieve low-latency neuromorphic object recognition without reducing performance. Concretely, we alleviate the temporal redundancy in SNNs by dividing SNNs into multiple stages with progressively shrinking timesteps, which significantly reduces the inference latency. During timestep shrinkage, the temporal transformer smoothly transforms the temporal scale and preserves the information maximally. Moreover, we add multiple early classifiers to the SNN during training to mitigate the mismatch between the surrogate gradient and the true gradient, as well as the gradient vanishing/exploding, thus eliminating the performance degradation at low latency. Extensive experiments on neuromorphic datasets, CIFAR10-DVS, N-Caltech101, and DVS-Gesture have revealed that SSNN is able to improve the baseline accuracy by 6.55% ~ 21.41%. With only 5 average timesteps and without any data augmentation, SSNN is able to achieve an accuracy of 73.63% on CIFAR10-DVS. This work presents a heterogeneous temporal scale SNN and provides valuable insights into the development of high-performance, low-latency SNNs.",
      "published": "2024-01-02T02:05:05Z"
    },
    "metadata": {
      "arxiv_id": "2401.01912",
      "title": "Shrinking Your TimeStep: Towards Low-Latency Neuromorphic Object Recognition with Spiking Neural Network",
      "summary": "Neuromorphic object recognition with spiking neural networks (SNNs) is the cornerstone of low-power neuromorphic computing. However, existing SNNs suffer from significant latency, utilizing 10 to 40 timesteps or more, to recognize neuromorphic objects. At low latencies, the performance of existing SNNs is drastically degraded. In this work, we propose the Shrinking SNN (SSNN) to achieve low-latency neuromorphic object recognition without reducing performance. Concretely, we alleviate the temporal redundancy in SNNs by dividing SNNs into multiple stages with progressively shrinking timesteps, which significantly reduces the inference latency. During timestep shrinkage, the temporal transformer smoothly transforms the temporal scale and preserves the information maximally. Moreover, we add multiple early classifiers to the SNN during training to mitigate the mismatch between the surrogate gradient and the true gradient, as well as the gradient vanishing/exploding, thus eliminating the performance degradation at low latency. Extensive experiments on neuromorphic datasets, CIFAR10-DVS, N-Caltech101, and DVS-Gesture have revealed that SSNN is able to improve the baseline accuracy by 6.55% ~ 21.41%. With only 5 average timesteps and without any data augmentation, SSNN is able to achieve an accuracy of 73.63% on CIFAR10-DVS. This work presents a heterogeneous temporal scale SNN and provides valuable insights into the development of high-performance, low-latency SNNs.",
      "authors": [
        "Yongqi Ding",
        "Lin Zuo",
        "Mengmeng Jing",
        "Pei He",
        "Yongjun Xiao"
      ],
      "published": "2024-01-02T02:05:05Z",
      "updated": "2024-01-02T02:05:05Z",
      "categories": [
        "cs.CV",
        "cs.LG",
        "eess.IV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2401.01912v1",
      "landing_url": "https://arxiv.org/abs/2401.01912v1",
      "doi": "https://doi.org/10.48550/arXiv.2401.01912"
    },
    "queries": [
      {
        "anchor": "synchronous dialogue",
        "search_term": "low latency"
      }
    ]
  },
  {
    "arxiv_id": "2401.03078",
    "anchor": "spoken language models",
    "search_term": "low latency",
    "search_record": {
      "id": "http://arxiv.org/abs/2401.03078v1",
      "title": "StreamVC: Real-Time Low-Latency Voice Conversion",
      "summary": "We present StreamVC, a streaming voice conversion solution that preserves the content and prosody of any source speech while matching the voice timbre from any target speech. Unlike previous approaches, StreamVC produces the resulting waveform at low latency from the input signal even on a mobile platform, making it applicable to real-time communication scenarios like calls and video conferencing, and addressing use cases such as voice anonymization in these scenarios. Our design leverages the architecture and training strategy of the SoundStream neural audio codec for lightweight high-quality speech synthesis. We demonstrate the feasibility of learning soft speech units causally, as well as the effectiveness of supplying whitened fundamental frequency information to improve pitch stability without leaking the source timbre information.",
      "published": "2024-01-05T22:37:26Z"
    },
    "metadata": {
      "arxiv_id": "2401.03078",
      "title": "StreamVC: Real-Time Low-Latency Voice Conversion",
      "summary": "We present StreamVC, a streaming voice conversion solution that preserves the content and prosody of any source speech while matching the voice timbre from any target speech. Unlike previous approaches, StreamVC produces the resulting waveform at low latency from the input signal even on a mobile platform, making it applicable to real-time communication scenarios like calls and video conferencing, and addressing use cases such as voice anonymization in these scenarios. Our design leverages the architecture and training strategy of the SoundStream neural audio codec for lightweight high-quality speech synthesis. We demonstrate the feasibility of learning soft speech units causally, as well as the effectiveness of supplying whitened fundamental frequency information to improve pitch stability without leaking the source timbre information.",
      "authors": [
        "Yang Yang",
        "Yury Kartynnik",
        "Yunpeng Li",
        "Jiuqiang Tang",
        "Xing Li",
        "George Sung",
        "Matthias Grundmann"
      ],
      "published": "2024-01-05T22:37:26Z",
      "updated": "2024-01-05T22:37:26Z",
      "categories": [
        "eess.AS",
        "cs.LG",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2401.03078v1",
      "landing_url": "https://arxiv.org/abs/2401.03078v1",
      "doi": "https://doi.org/10.48550/arXiv.2401.03078"
    },
    "queries": [
      {
        "anchor": "spoken language models",
        "search_term": "low latency"
      },
      {
        "anchor": "synchronous dialogue",
        "search_term": "low latency"
      }
    ]
  },
  {
    "arxiv_id": "2401.03546",
    "anchor": "synchronous dialogue",
    "search_term": "modular architecture",
    "search_record": {
      "id": "http://arxiv.org/abs/2401.03546v1",
      "title": "NovelGym: A Flexible Ecosystem for Hybrid Planning and Learning Agents Designed for Open Worlds",
      "summary": "As AI agents leave the lab and venture into the real world as autonomous vehicles, delivery robots, and cooking robots, it is increasingly necessary to design and comprehensively evaluate algorithms that tackle the ``open-world''. To this end, we introduce NovelGym, a flexible and adaptable ecosystem designed to simulate gridworld environments, serving as a robust platform for benchmarking reinforcement learning (RL) and hybrid planning and learning agents in open-world contexts. The modular architecture of NovelGym facilitates rapid creation and modification of task environments, including multi-agent scenarios, with multiple environment transformations, thus providing a dynamic testbed for researchers to develop open-world AI agents.",
      "published": "2024-01-07T17:13:28Z"
    },
    "metadata": {
      "arxiv_id": "2401.03546",
      "title": "NovelGym: A Flexible Ecosystem for Hybrid Planning and Learning Agents Designed for Open Worlds",
      "summary": "As AI agents leave the lab and venture into the real world as autonomous vehicles, delivery robots, and cooking robots, it is increasingly necessary to design and comprehensively evaluate algorithms that tackle the ``open-world''. To this end, we introduce NovelGym, a flexible and adaptable ecosystem designed to simulate gridworld environments, serving as a robust platform for benchmarking reinforcement learning (RL) and hybrid planning and learning agents in open-world contexts. The modular architecture of NovelGym facilitates rapid creation and modification of task environments, including multi-agent scenarios, with multiple environment transformations, thus providing a dynamic testbed for researchers to develop open-world AI agents.",
      "authors": [
        "Shivam Goel",
        "Yichen Wei",
        "Panagiotis Lymperopoulos",
        "Klara Chura",
        "Matthias Scheutz",
        "Jivko Sinapov"
      ],
      "published": "2024-01-07T17:13:28Z",
      "updated": "2024-01-07T17:13:28Z",
      "categories": [
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2401.03546v1",
      "landing_url": "https://arxiv.org/abs/2401.03546v1",
      "doi": "https://doi.org/10.48550/arXiv.2401.03546"
    },
    "queries": [
      {
        "anchor": "synchronous dialogue",
        "search_term": "modular architecture"
      }
    ]
  },
  {
    "arxiv_id": "2401.04577",
    "anchor": "synchronous dialogue",
    "search_term": "audio tokens",
    "search_record": {
      "id": "http://arxiv.org/abs/2401.04577v2",
      "title": "Masked Audio Generation using a Single Non-Autoregressive Transformer",
      "summary": "We introduce MAGNeT, a masked generative sequence modeling method that operates directly over several streams of audio tokens. Unlike prior work, MAGNeT is comprised of a single-stage, non-autoregressive transformer. During training, we predict spans of masked tokens obtained from a masking scheduler, while during inference we gradually construct the output sequence using several decoding steps. To further enhance the quality of the generated audio, we introduce a novel rescoring method in which, we leverage an external pre-trained model to rescore and rank predictions from MAGNeT, which will be then used for later decoding steps. Lastly, we explore a hybrid version of MAGNeT, in which we fuse between autoregressive and non-autoregressive models to generate the first few seconds in an autoregressive manner while the rest of the sequence is being decoded in parallel. We demonstrate the efficiency of MAGNeT for the task of text-to-music and text-to-audio generation and conduct an extensive empirical evaluation, considering both objective metrics and human studies. The proposed approach is comparable to the evaluated baselines, while being significantly faster (x7 faster than the autoregressive baseline). Through ablation studies and analysis, we shed light on the importance of each of the components comprising MAGNeT, together with pointing to the trade-offs between autoregressive and non-autoregressive modeling, considering latency, throughput, and generation quality. Samples are available on our demo page https://pages.cs.huji.ac.il/adiyoss-lab/MAGNeT.",
      "published": "2024-01-09T14:29:39Z"
    },
    "metadata": {
      "arxiv_id": "2401.04577",
      "title": "Masked Audio Generation using a Single Non-Autoregressive Transformer",
      "summary": "We introduce MAGNeT, a masked generative sequence modeling method that operates directly over several streams of audio tokens. Unlike prior work, MAGNeT is comprised of a single-stage, non-autoregressive transformer. During training, we predict spans of masked tokens obtained from a masking scheduler, while during inference we gradually construct the output sequence using several decoding steps. To further enhance the quality of the generated audio, we introduce a novel rescoring method in which, we leverage an external pre-trained model to rescore and rank predictions from MAGNeT, which will be then used for later decoding steps. Lastly, we explore a hybrid version of MAGNeT, in which we fuse between autoregressive and non-autoregressive models to generate the first few seconds in an autoregressive manner while the rest of the sequence is being decoded in parallel. We demonstrate the efficiency of MAGNeT for the task of text-to-music and text-to-audio generation and conduct an extensive empirical evaluation, considering both objective metrics and human studies. The proposed approach is comparable to the evaluated baselines, while being significantly faster (x7 faster than the autoregressive baseline). Through ablation studies and analysis, we shed light on the importance of each of the components comprising MAGNeT, together with pointing to the trade-offs between autoregressive and non-autoregressive modeling, considering latency, throughput, and generation quality. Samples are available on our demo page https://pages.cs.huji.ac.il/adiyoss-lab/MAGNeT.",
      "authors": [
        "Alon Ziv",
        "Itai Gat",
        "Gael Le Lan",
        "Tal Remez",
        "Felix Kreuk",
        "Alexandre Défossez",
        "Jade Copet",
        "Gabriel Synnaeve",
        "Yossi Adi"
      ],
      "published": "2024-01-09T14:29:39Z",
      "updated": "2024-03-05T09:12:35Z",
      "categories": [
        "cs.SD",
        "cs.AI",
        "cs.LG",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2401.04577v2",
      "landing_url": "https://arxiv.org/abs/2401.04577v2",
      "doi": "https://doi.org/10.48550/arXiv.2401.04577"
    },
    "queries": [
      {
        "anchor": "synchronous dialogue",
        "search_term": "audio tokens"
      }
    ]
  },
  {
    "arxiv_id": "2401.04868",
    "anchor": "turn-taking",
    "search_term": "true full-duplex",
    "search_record": {
      "id": "http://arxiv.org/abs/2401.04868v1",
      "title": "Real-time and Continuous Turn-taking Prediction Using Voice Activity Projection",
      "summary": "A demonstration of a real-time and continuous turn-taking prediction system is presented. The system is based on a voice activity projection (VAP) model, which directly maps dialogue stereo audio to future voice activities. The VAP model includes contrastive predictive coding (CPC) and self-attention transformers, followed by a cross-attention transformer. We examine the effect of the input context audio length and demonstrate that the proposed system can operate in real-time with CPU settings, with minimal performance degradation.",
      "published": "2024-01-10T01:09:55Z"
    },
    "metadata": {
      "arxiv_id": "2401.04868",
      "title": "Real-time and Continuous Turn-taking Prediction Using Voice Activity Projection",
      "summary": "A demonstration of a real-time and continuous turn-taking prediction system is presented. The system is based on a voice activity projection (VAP) model, which directly maps dialogue stereo audio to future voice activities. The VAP model includes contrastive predictive coding (CPC) and self-attention transformers, followed by a cross-attention transformer. We examine the effect of the input context audio length and demonstrate that the proposed system can operate in real-time with CPU settings, with minimal performance degradation.",
      "authors": [
        "Koji Inoue",
        "Bing'er Jiang",
        "Erik Ekstedt",
        "Tatsuya Kawahara",
        "Gabriel Skantze"
      ],
      "published": "2024-01-10T01:09:55Z",
      "updated": "2024-01-10T01:09:55Z",
      "categories": [
        "cs.CL",
        "cs.HC",
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2401.04868v1",
      "landing_url": "https://arxiv.org/abs/2401.04868v1",
      "doi": "https://doi.org/10.48550/arXiv.2401.04868"
    },
    "queries": [
      {
        "anchor": "turn-taking",
        "search_term": "true full-duplex"
      },
      {
        "anchor": "turn-taking",
        "search_term": "pseudo full-duplex"
      },
      {
        "anchor": "turn-taking",
        "search_term": "half duplex"
      },
      {
        "anchor": "turn-taking",
        "search_term": "engineered synchronization"
      },
      {
        "anchor": "turn-taking",
        "search_term": "learned synchronization"
      },
      {
        "anchor": "turn-taking",
        "search_term": "modular architecture"
      },
      {
        "anchor": "turn-taking",
        "search_term": "streaming architecture"
      },
      {
        "anchor": "turn-taking",
        "search_term": "real-time interaction"
      },
      {
        "anchor": "turn-taking",
        "search_term": "low latency"
      },
      {
        "anchor": "turn-taking",
        "search_term": "synchronous data"
      },
      {
        "anchor": "turn-taking",
        "search_term": "data scarcity"
      },
      {
        "anchor": "turn-taking",
        "search_term": "multichannel dialogue"
      },
      {
        "anchor": "turn-taking",
        "search_term": "temporal dynamics"
      },
      {
        "anchor": "turn-taking",
        "search_term": "behavioral arbitration"
      },
      {
        "anchor": "turn-taking",
        "search_term": "semantic coherence"
      },
      {
        "anchor": "turn-taking",
        "search_term": "neural codec"
      },
      {
        "anchor": "turn-taking",
        "search_term": "discrete tokens"
      },
      {
        "anchor": "turn-taking",
        "search_term": "audio tokens"
      },
      {
        "anchor": "turn-taking",
        "search_term": "floor transfer offset"
      },
      {
        "anchor": "turn-taking",
        "search_term": "stop latency"
      },
      {
        "anchor": "turn-taking",
        "search_term": "interruption response delay"
      }
    ]
  },
  {
    "arxiv_id": "2401.07333",
    "anchor": "spoken language models",
    "search_term": "neural codec",
    "search_record": {
      "id": "http://arxiv.org/abs/2401.07333v1",
      "title": "ELLA-V: Stable Neural Codec Language Modeling with Alignment-guided Sequence Reordering",
      "summary": "The language model (LM) approach based on acoustic and linguistic prompts, such as VALL-E, has achieved remarkable progress in the field of zero-shot audio generation. However, existing methods still have some limitations: 1) repetitions, transpositions, and omissions in the output synthesized speech due to limited alignment constraints between audio and phoneme tokens; 2) challenges of fine-grained control over the synthesized speech with autoregressive (AR) language model; 3) infinite silence generation due to the nature of AR-based decoding, especially under the greedy strategy. To alleviate these issues, we propose ELLA-V, a simple but efficient LM-based zero-shot text-to-speech (TTS) framework, which enables fine-grained control over synthesized audio at the phoneme level. The key to ELLA-V is interleaving sequences of acoustic and phoneme tokens, where phoneme tokens appear ahead of the corresponding acoustic tokens. The experimental findings reveal that our model outperforms VALL-E in terms of accuracy and delivers more stable results using both greedy and sampling-based decoding strategies. The code of ELLA-V will be open-sourced after cleanups. Audio samples are available at https://ereboas.github.io/ELLAV/.",
      "published": "2024-01-14T17:43:55Z"
    },
    "metadata": {
      "arxiv_id": "2401.07333",
      "title": "ELLA-V: Stable Neural Codec Language Modeling with Alignment-guided Sequence Reordering",
      "summary": "The language model (LM) approach based on acoustic and linguistic prompts, such as VALL-E, has achieved remarkable progress in the field of zero-shot audio generation. However, existing methods still have some limitations: 1) repetitions, transpositions, and omissions in the output synthesized speech due to limited alignment constraints between audio and phoneme tokens; 2) challenges of fine-grained control over the synthesized speech with autoregressive (AR) language model; 3) infinite silence generation due to the nature of AR-based decoding, especially under the greedy strategy. To alleviate these issues, we propose ELLA-V, a simple but efficient LM-based zero-shot text-to-speech (TTS) framework, which enables fine-grained control over synthesized audio at the phoneme level. The key to ELLA-V is interleaving sequences of acoustic and phoneme tokens, where phoneme tokens appear ahead of the corresponding acoustic tokens. The experimental findings reveal that our model outperforms VALL-E in terms of accuracy and delivers more stable results using both greedy and sampling-based decoding strategies. The code of ELLA-V will be open-sourced after cleanups. Audio samples are available at https://ereboas.github.io/ELLAV/.",
      "authors": [
        "Yakun Song",
        "Zhuo Chen",
        "Xiaofei Wang",
        "Ziyang Ma",
        "Xie Chen"
      ],
      "published": "2024-01-14T17:43:55Z",
      "updated": "2024-01-14T17:43:55Z",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2401.07333v1",
      "landing_url": "https://arxiv.org/abs/2401.07333v1",
      "doi": "https://doi.org/10.48550/arXiv.2401.07333"
    },
    "queries": [
      {
        "anchor": "spoken language models",
        "search_term": "neural codec"
      },
      {
        "anchor": "synchronous dialogue",
        "search_term": "neural codec"
      },
      {
        "anchor": "turn-taking",
        "search_term": "neural codec"
      }
    ]
  },
  {
    "arxiv_id": "2401.12820",
    "anchor": "synchronous dialogue",
    "search_term": "synchronous data",
    "search_record": {
      "id": "http://arxiv.org/abs/2401.12820v1",
      "title": "DatUS^2: Data-driven Unsupervised Semantic Segmentation with Pre-trained Self-supervised Vision Transformer",
      "summary": "Successive proposals of several self-supervised training schemes continue to emerge, taking one step closer to developing a universal foundation model. In this process, the unsupervised downstream tasks are recognized as one of the evaluation methods to validate the quality of visual features learned with a self-supervised training scheme. However, unsupervised dense semantic segmentation has not been explored as a downstream task, which can utilize and evaluate the quality of semantic information introduced in patch-level feature representations during self-supervised training of a vision transformer. Therefore, this paper proposes a novel data-driven approach for unsupervised semantic segmentation (DatUS^2) as a downstream task. DatUS^2 generates semantically consistent and dense pseudo annotate segmentation masks for the unlabeled image dataset without using any visual-prior or synchronized data. We compare these pseudo-annotated segmentation masks with ground truth masks for evaluating recent self-supervised training schemes to learn shared semantic properties at the patch level and discriminative semantic properties at the segment level. Finally, we evaluate existing state-of-the-art self-supervised training schemes with our proposed downstream task, i.e., DatUS^2. Also, the best version of DatUS^2 outperforms the existing state-of-the-art method for the unsupervised dense semantic segmentation task with 15.02% MiOU and 21.47% Pixel accuracy on the SUIM dataset. It also achieves a competitive level of accuracy for a large-scale and complex dataset, i.e., the COCO dataset.",
      "published": "2024-01-23T14:53:32Z"
    },
    "metadata": {
      "arxiv_id": "2401.12820",
      "title": "DatUS^2: Data-driven Unsupervised Semantic Segmentation with Pre-trained Self-supervised Vision Transformer",
      "summary": "Successive proposals of several self-supervised training schemes continue to emerge, taking one step closer to developing a universal foundation model. In this process, the unsupervised downstream tasks are recognized as one of the evaluation methods to validate the quality of visual features learned with a self-supervised training scheme. However, unsupervised dense semantic segmentation has not been explored as a downstream task, which can utilize and evaluate the quality of semantic information introduced in patch-level feature representations during self-supervised training of a vision transformer. Therefore, this paper proposes a novel data-driven approach for unsupervised semantic segmentation (DatUS^2) as a downstream task. DatUS^2 generates semantically consistent and dense pseudo annotate segmentation masks for the unlabeled image dataset without using any visual-prior or synchronized data. We compare these pseudo-annotated segmentation masks with ground truth masks for evaluating recent self-supervised training schemes to learn shared semantic properties at the patch level and discriminative semantic properties at the segment level. Finally, we evaluate existing state-of-the-art self-supervised training schemes with our proposed downstream task, i.e., DatUS^2. Also, the best version of DatUS^2 outperforms the existing state-of-the-art method for the unsupervised dense semantic segmentation task with 15.02% MiOU and 21.47% Pixel accuracy on the SUIM dataset. It also achieves a competitive level of accuracy for a large-scale and complex dataset, i.e., the COCO dataset.",
      "authors": [
        "Sonal Kumar",
        "Arijit Sur",
        "Rashmi Dutta Baruah"
      ],
      "published": "2024-01-23T14:53:32Z",
      "updated": "2024-01-23T14:53:32Z",
      "categories": [
        "cs.CV",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2401.12820v1",
      "landing_url": "https://arxiv.org/abs/2401.12820v1",
      "doi": "https://doi.org/10.1109/TCDS.2024.3383952"
    },
    "queries": [
      {
        "anchor": "synchronous dialogue",
        "search_term": "synchronous data"
      }
    ]
  },
  {
    "arxiv_id": "2401.13085",
    "anchor": "synchronous dialogue",
    "search_term": "data scarcity",
    "search_record": {
      "id": "http://arxiv.org/abs/2401.13085v1",
      "title": "IndiText Boost: Text Augmentation for Low Resource India Languages",
      "summary": "Text Augmentation is an important task for low-resource languages. It helps deal with the problem of data scarcity. A data augmentation strategy is used to deal with the problem of data scarcity. Through the years, much work has been done on data augmentation for the English language. In contrast, very less work has been done on Indian languages. This is contrary to the fact that data augmentation is used to deal with data scarcity. In this work, we focus on implementing techniques like Easy Data Augmentation, Back Translation, Paraphrasing, Text Generation using LLMs, and Text Expansion using LLMs for text classification on different languages. We focus on 6 Indian languages namely: Sindhi, Marathi, Hindi, Gujarati, Telugu, and Sanskrit. According to our knowledge, no such work exists for text augmentation on Indian languages. We carry out binary as well as multi-class text classification to make our results more comparable. We get surprising results as basic data augmentation techniques surpass LLMs.",
      "published": "2024-01-23T20:54:40Z"
    },
    "metadata": {
      "arxiv_id": "2401.13085",
      "title": "IndiText Boost: Text Augmentation for Low Resource India Languages",
      "summary": "Text Augmentation is an important task for low-resource languages. It helps deal with the problem of data scarcity. A data augmentation strategy is used to deal with the problem of data scarcity. Through the years, much work has been done on data augmentation for the English language. In contrast, very less work has been done on Indian languages. This is contrary to the fact that data augmentation is used to deal with data scarcity. In this work, we focus on implementing techniques like Easy Data Augmentation, Back Translation, Paraphrasing, Text Generation using LLMs, and Text Expansion using LLMs for text classification on different languages. We focus on 6 Indian languages namely: Sindhi, Marathi, Hindi, Gujarati, Telugu, and Sanskrit. According to our knowledge, no such work exists for text augmentation on Indian languages. We carry out binary as well as multi-class text classification to make our results more comparable. We get surprising results as basic data augmentation techniques surpass LLMs.",
      "authors": [
        "Onkar Litake",
        "Niraj Yagnik",
        "Shreyas Labhsetwar"
      ],
      "published": "2024-01-23T20:54:40Z",
      "updated": "2024-01-23T20:54:40Z",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2401.13085v1",
      "landing_url": "https://arxiv.org/abs/2401.13085v1",
      "doi": "https://doi.org/10.48550/arXiv.2401.13085"
    },
    "queries": [
      {
        "anchor": "synchronous dialogue",
        "search_term": "data scarcity"
      }
    ]
  },
  {
    "arxiv_id": "2401.13254",
    "anchor": "spoken language models",
    "search_term": "modular architecture",
    "search_record": {
      "id": "http://arxiv.org/abs/2401.13254v1",
      "title": "A modular architecture for IMU-based data gloves",
      "summary": "The flexibility and range of motion in human hands play a crucial role in human interaction with the environment and have been studied across different fields. Researchers explored various technological solutions for gathering information from the hands. These solutions include tracking hand motion through cameras or wearable sensors and using wearable sensors to measure the position and pressure of contact points. Data gloves can collect both types of information by utilizing inertial measurement units, flex sensors, magnetic trackers for motion tracking, and force resistors or touch sensors for contact measurement. Although there are commercially available data gloves, researchers often create custom data gloves to achieve the desired flexibility and control over the hardware. However, the existing literature lacks standardization and the reuse of previously designed data gloves. As a result, many gloves with unclear characteristics exist, which makes replication challenging and negatively impacts the reproducibility of studies. This work proposes a modular, open hardware and software architecture for creating customized data gloves based on IMU technology. We also provide an architecture implementation along with an experimental protocol to evaluate device performance.",
      "published": "2024-01-24T06:34:06Z"
    },
    "metadata": {
      "arxiv_id": "2401.13254",
      "title": "A modular architecture for IMU-based data gloves",
      "summary": "The flexibility and range of motion in human hands play a crucial role in human interaction with the environment and have been studied across different fields. Researchers explored various technological solutions for gathering information from the hands. These solutions include tracking hand motion through cameras or wearable sensors and using wearable sensors to measure the position and pressure of contact points. Data gloves can collect both types of information by utilizing inertial measurement units, flex sensors, magnetic trackers for motion tracking, and force resistors or touch sensors for contact measurement. Although there are commercially available data gloves, researchers often create custom data gloves to achieve the desired flexibility and control over the hardware. However, the existing literature lacks standardization and the reuse of previously designed data gloves. As a result, many gloves with unclear characteristics exist, which makes replication challenging and negatively impacts the reproducibility of studies. This work proposes a modular, open hardware and software architecture for creating customized data gloves based on IMU technology. We also provide an architecture implementation along with an experimental protocol to evaluate device performance.",
      "authors": [
        "Alessandro Carfì",
        "Mohamad Alameh",
        "Valerio Belcamino",
        "Fulvio Mastrogiovanni"
      ],
      "published": "2024-01-24T06:34:06Z",
      "updated": "2024-01-24T06:34:06Z",
      "categories": [
        "cs.AR"
      ],
      "pdf_url": "https://arxiv.org/pdf/2401.13254v1",
      "landing_url": "https://arxiv.org/abs/2401.13254v1",
      "doi": "https://doi.org/10.48550/arXiv.2401.13254"
    },
    "queries": [
      {
        "anchor": "spoken language models",
        "search_term": "modular architecture"
      },
      {
        "anchor": "synchronous dialogue",
        "search_term": "modular architecture"
      },
      {
        "anchor": "turn-taking",
        "search_term": "modular architecture"
      }
    ]
  },
  {
    "arxiv_id": "2401.14090",
    "anchor": "synchronous dialogue",
    "search_term": "modular architecture",
    "search_record": {
      "id": "http://arxiv.org/abs/2401.14090v1",
      "title": "A Modular Approach to Automatic Cyber Threat Attribution using Opinion Pools",
      "summary": "Cyber threat attribution can play an important role in increasing resilience against digital threats. Recent research focuses on automating the threat attribution process and on integrating it with other efforts, such as threat hunting. To support increasing automation of the cyber threat attribution process, this paper proposes a modular architecture as an alternative to current monolithic automated approaches. The modular architecture can utilize opinion pools to combine the output of concrete attributors. The proposed solution increases the tractability of the threat attribution problem and offers increased usability and interpretability, as opposed to monolithic alternatives. In addition, a Pairing Aggregator is proposed as an aggregation method that forms pairs of attributors based on distinct features to produce intermediary results before finally producing a single Probability Mass Function (PMF) as output. The Pairing Aggregator sequentially applies both the logarithmic opinion pool and the linear opinion pool. An experimental validation suggests that the modular approach does not result in decreased performance and can even enhance precision and recall compared to monolithic alternatives. The results also suggest that the Pairing Aggregator can improve precision over the linear and logarithmic opinion pools. Furthermore, the improved k-accuracy in the experiment suggests that forensic experts can leverage the resulting PMF during their manual attribution processes to enhance their efficiency.",
      "published": "2024-01-25T11:12:16Z"
    },
    "metadata": {
      "arxiv_id": "2401.14090",
      "title": "A Modular Approach to Automatic Cyber Threat Attribution using Opinion Pools",
      "summary": "Cyber threat attribution can play an important role in increasing resilience against digital threats. Recent research focuses on automating the threat attribution process and on integrating it with other efforts, such as threat hunting. To support increasing automation of the cyber threat attribution process, this paper proposes a modular architecture as an alternative to current monolithic automated approaches. The modular architecture can utilize opinion pools to combine the output of concrete attributors. The proposed solution increases the tractability of the threat attribution problem and offers increased usability and interpretability, as opposed to monolithic alternatives. In addition, a Pairing Aggregator is proposed as an aggregation method that forms pairs of attributors based on distinct features to produce intermediary results before finally producing a single Probability Mass Function (PMF) as output. The Pairing Aggregator sequentially applies both the logarithmic opinion pool and the linear opinion pool. An experimental validation suggests that the modular approach does not result in decreased performance and can even enhance precision and recall compared to monolithic alternatives. The results also suggest that the Pairing Aggregator can improve precision over the linear and logarithmic opinion pools. Furthermore, the improved k-accuracy in the experiment suggests that forensic experts can leverage the resulting PMF during their manual attribution processes to enhance their efficiency.",
      "authors": [
        "Koen T. W. Teuwen"
      ],
      "published": "2024-01-25T11:12:16Z",
      "updated": "2024-01-25T11:12:16Z",
      "categories": [
        "cs.CR",
        "cs.LG",
        "cs.SE"
      ],
      "pdf_url": "https://arxiv.org/pdf/2401.14090v1",
      "landing_url": "https://arxiv.org/abs/2401.14090v1",
      "doi": "https://doi.org/10.1109/BigData59044.2023.10386708"
    },
    "queries": [
      {
        "anchor": "synchronous dialogue",
        "search_term": "modular architecture"
      }
    ]
  },
  {
    "arxiv_id": "2401.14717",
    "anchor": "turn-taking",
    "search_term": "true full-duplex",
    "search_record": {
      "id": "http://arxiv.org/abs/2401.14717v1",
      "title": "Turn-taking and Backchannel Prediction with Acoustic and Large Language Model Fusion",
      "summary": "We propose an approach for continuous prediction of turn-taking and backchanneling locations in spoken dialogue by fusing a neural acoustic model with a large language model (LLM). Experiments on the Switchboard human-human conversation dataset demonstrate that our approach consistently outperforms the baseline models with single modality. We also develop a novel multi-task instruction fine-tuning strategy to further benefit from LLM-encoded knowledge for understanding the tasks and conversational contexts, leading to additional improvements. Our approach demonstrates the potential of combined LLMs and acoustic models for a more natural and conversational interaction between humans and speech-enabled AI agents.",
      "published": "2024-01-26T08:59:07Z"
    },
    "metadata": {
      "arxiv_id": "2401.14717",
      "title": "Turn-taking and Backchannel Prediction with Acoustic and Large Language Model Fusion",
      "summary": "We propose an approach for continuous prediction of turn-taking and backchanneling locations in spoken dialogue by fusing a neural acoustic model with a large language model (LLM). Experiments on the Switchboard human-human conversation dataset demonstrate that our approach consistently outperforms the baseline models with single modality. We also develop a novel multi-task instruction fine-tuning strategy to further benefit from LLM-encoded knowledge for understanding the tasks and conversational contexts, leading to additional improvements. Our approach demonstrates the potential of combined LLMs and acoustic models for a more natural and conversational interaction between humans and speech-enabled AI agents.",
      "authors": [
        "Jinhan Wang",
        "Long Chen",
        "Aparna Khare",
        "Anirudh Raju",
        "Pranav Dheram",
        "Di He",
        "Minhua Wu",
        "Andreas Stolcke",
        "Venkatesh Ravichandran"
      ],
      "published": "2024-01-26T08:59:07Z",
      "updated": "2024-01-26T08:59:07Z",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG",
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2401.14717v1",
      "landing_url": "https://arxiv.org/abs/2401.14717v1",
      "doi": "https://doi.org/10.48550/arXiv.2401.14717"
    },
    "queries": [
      {
        "anchor": "turn-taking",
        "search_term": "true full-duplex"
      },
      {
        "anchor": "turn-taking",
        "search_term": "pseudo full-duplex"
      },
      {
        "anchor": "turn-taking",
        "search_term": "half duplex"
      },
      {
        "anchor": "turn-taking",
        "search_term": "engineered synchronization"
      },
      {
        "anchor": "turn-taking",
        "search_term": "learned synchronization"
      },
      {
        "anchor": "turn-taking",
        "search_term": "modular architecture"
      },
      {
        "anchor": "turn-taking",
        "search_term": "streaming architecture"
      },
      {
        "anchor": "turn-taking",
        "search_term": "real-time interaction"
      },
      {
        "anchor": "turn-taking",
        "search_term": "low latency"
      },
      {
        "anchor": "turn-taking",
        "search_term": "synchronous data"
      },
      {
        "anchor": "turn-taking",
        "search_term": "data scarcity"
      },
      {
        "anchor": "turn-taking",
        "search_term": "multichannel dialogue"
      },
      {
        "anchor": "turn-taking",
        "search_term": "temporal dynamics"
      },
      {
        "anchor": "turn-taking",
        "search_term": "behavioral arbitration"
      },
      {
        "anchor": "turn-taking",
        "search_term": "semantic coherence"
      },
      {
        "anchor": "turn-taking",
        "search_term": "neural codec"
      },
      {
        "anchor": "turn-taking",
        "search_term": "discrete tokens"
      },
      {
        "anchor": "turn-taking",
        "search_term": "audio tokens"
      },
      {
        "anchor": "turn-taking",
        "search_term": "floor transfer offset"
      },
      {
        "anchor": "turn-taking",
        "search_term": "stop latency"
      },
      {
        "anchor": "turn-taking",
        "search_term": "interruption response delay"
      }
    ]
  },
  {
    "arxiv_id": "2401.14718",
    "anchor": "synchronous dialogue",
    "search_term": "semantic coherence",
    "search_record": {
      "id": "http://arxiv.org/abs/2401.14718v8",
      "title": "A Survey on Future Frame Synthesis: Bridging Deterministic and Generative Approaches",
      "summary": "Future Frame Synthesis (FFS), the task of generating subsequent video frames from context, represents a core challenge in machine intelligence and a cornerstone for developing predictive world models. This survey provides a comprehensive analysis of the FFS landscape, charting its critical evolution from deterministic algorithms focused on pixel-level accuracy to modern generative paradigms that prioritize semantic coherence and dynamic plausibility. We introduce a novel taxonomy organized by algorithmic stochasticity, which not only categorizes existing methods but also reveals the fundamental drivers--advances in architectures, datasets, and computational scale--behind this paradigm shift. Critically, our analysis identifies a bifurcation in the field's trajectory: one path toward efficient, real-time prediction, and another toward large-scale, generative world simulation. By pinpointing key challenges and proposing concrete research questions for both frontiers, this survey serves as an essential guide for researchers aiming to advance the frontiers of visual dynamic modeling.",
      "published": "2024-01-26T08:59:38Z"
    },
    "metadata": {
      "arxiv_id": "2401.14718",
      "title": "A Survey on Future Frame Synthesis: Bridging Deterministic and Generative Approaches",
      "summary": "Future Frame Synthesis (FFS), the task of generating subsequent video frames from context, represents a core challenge in machine intelligence and a cornerstone for developing predictive world models. This survey provides a comprehensive analysis of the FFS landscape, charting its critical evolution from deterministic algorithms focused on pixel-level accuracy to modern generative paradigms that prioritize semantic coherence and dynamic plausibility. We introduce a novel taxonomy organized by algorithmic stochasticity, which not only categorizes existing methods but also reveals the fundamental drivers--advances in architectures, datasets, and computational scale--behind this paradigm shift. Critically, our analysis identifies a bifurcation in the field's trajectory: one path toward efficient, real-time prediction, and another toward large-scale, generative world simulation. By pinpointing key challenges and proposing concrete research questions for both frontiers, this survey serves as an essential guide for researchers aiming to advance the frontiers of visual dynamic modeling.",
      "authors": [
        "Ruibo Ming",
        "Zhewei Huang",
        "Jingwei Wu",
        "Zhuoxuan Ju",
        "Daxin Jiang",
        "Jianming Hu",
        "Lihui Peng",
        "Shuchang Zhou"
      ],
      "published": "2024-01-26T08:59:38Z",
      "updated": "2025-07-14T09:44:08Z",
      "categories": [
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2401.14718v8",
      "landing_url": "https://arxiv.org/abs/2401.14718v8",
      "doi": "https://doi.org/10.48550/arXiv.2401.14718"
    },
    "queries": [
      {
        "anchor": "synchronous dialogue",
        "search_term": "semantic coherence"
      }
    ]
  },
  {
    "arxiv_id": "2401.15343",
    "anchor": "synchronous dialogue",
    "search_term": "low latency",
    "search_record": {
      "id": "http://arxiv.org/abs/2401.15343v1",
      "title": "Optimal Quality and Efficiency in Adaptive Live Streaming with JND-Aware Low latency Encoding",
      "summary": "In HTTP adaptive live streaming applications, video segments are encoded at a fixed set of bitrate-resolution pairs known as bitrate ladder. Live encoders use the fastest available encoding configuration, referred to as preset, to ensure the minimum possible latency in video encoding. However, an optimized preset and optimized number of CPU threads for each encoding instance may result in (i) increased quality and (ii) efficient CPU utilization while encoding. For low latency live encoders, the encoding speed is expected to be more than or equal to the video framerate. To this light, this paper introduces a Just Noticeable Difference (JND)-Aware Low latency Encoding Scheme (JALE), which uses random forest-based models to jointly determine the optimized encoder preset and thread count for each representation, based on video complexity features, the target encoding speed, the total number of available CPU threads, and the target encoder. Experimental results show that, on average, JALE yield a quality improvement of 1.32 dB PSNR and 5.38 VMAF points with the same bitrate, compared to the fastest preset encoding of the HTTP Live Streaming (HLS) bitrate ladder using x265 HEVC open-source encoder with eight CPU threads used for each representation. These enhancements are achieved while maintaining the desired encoding speed. Furthermore, on average, JALE results in an overall storage reduction of 72.70 %, a reduction in the total number of CPU threads used by 63.83 %, and a 37.87 % reduction in the overall encoding time, considering a JND of six VMAF points.",
      "published": "2024-01-27T08:23:00Z"
    },
    "metadata": {
      "arxiv_id": "2401.15343",
      "title": "Optimal Quality and Efficiency in Adaptive Live Streaming with JND-Aware Low latency Encoding",
      "summary": "In HTTP adaptive live streaming applications, video segments are encoded at a fixed set of bitrate-resolution pairs known as bitrate ladder. Live encoders use the fastest available encoding configuration, referred to as preset, to ensure the minimum possible latency in video encoding. However, an optimized preset and optimized number of CPU threads for each encoding instance may result in (i) increased quality and (ii) efficient CPU utilization while encoding. For low latency live encoders, the encoding speed is expected to be more than or equal to the video framerate. To this light, this paper introduces a Just Noticeable Difference (JND)-Aware Low latency Encoding Scheme (JALE), which uses random forest-based models to jointly determine the optimized encoder preset and thread count for each representation, based on video complexity features, the target encoding speed, the total number of available CPU threads, and the target encoder. Experimental results show that, on average, JALE yield a quality improvement of 1.32 dB PSNR and 5.38 VMAF points with the same bitrate, compared to the fastest preset encoding of the HTTP Live Streaming (HLS) bitrate ladder using x265 HEVC open-source encoder with eight CPU threads used for each representation. These enhancements are achieved while maintaining the desired encoding speed. Furthermore, on average, JALE results in an overall storage reduction of 72.70 %, a reduction in the total number of CPU threads used by 63.83 %, and a 37.87 % reduction in the overall encoding time, considering a JND of six VMAF points.",
      "authors": [
        "Vignesh V Menon",
        "Jingwen Zhu",
        "Prajit T Rajendran",
        "Samira Afzal",
        "Klaus Schoeffmann",
        "Patrick Le Callet",
        "Christian Timmerer"
      ],
      "published": "2024-01-27T08:23:00Z",
      "updated": "2024-01-27T08:23:00Z",
      "categories": [
        "cs.MM"
      ],
      "pdf_url": "https://arxiv.org/pdf/2401.15343v1",
      "landing_url": "https://arxiv.org/abs/2401.15343v1",
      "doi": "https://doi.org/10.1145/3638036.3640807"
    },
    "queries": [
      {
        "anchor": "synchronous dialogue",
        "search_term": "low latency"
      }
    ]
  },
  {
    "arxiv_id": "2401.16812",
    "anchor": "synchronous dialogue",
    "search_term": "discrete tokens",
    "search_record": {
      "id": "http://arxiv.org/abs/2401.16812v3",
      "title": "SpeechBERTScore: Reference-Aware Automatic Evaluation of Speech Generation Leveraging NLP Evaluation Metrics",
      "summary": "While subjective assessments have been the gold standard for evaluating speech generation, there is a growing need for objective metrics that are highly correlated with human subjective judgments due to their cost efficiency. This paper proposes reference-aware automatic evaluation methods for speech generation inspired by evaluation metrics in natural language processing. The proposed SpeechBERTScore computes the BERTScore for self-supervised dense speech features of the generated and reference speech, which can have different sequential lengths. We also propose SpeechBLEU and SpeechTokenDistance, which are computed on speech discrete tokens. The evaluations on synthesized speech show that our method correlates better with human subjective ratings than mel cepstral distortion and a recent mean opinion score prediction model. Also, they are effective in noisy speech evaluation and have cross-lingual applicability.",
      "published": "2024-01-30T08:26:28Z"
    },
    "metadata": {
      "arxiv_id": "2401.16812",
      "title": "SpeechBERTScore: Reference-Aware Automatic Evaluation of Speech Generation Leveraging NLP Evaluation Metrics",
      "summary": "While subjective assessments have been the gold standard for evaluating speech generation, there is a growing need for objective metrics that are highly correlated with human subjective judgments due to their cost efficiency. This paper proposes reference-aware automatic evaluation methods for speech generation inspired by evaluation metrics in natural language processing. The proposed SpeechBERTScore computes the BERTScore for self-supervised dense speech features of the generated and reference speech, which can have different sequential lengths. We also propose SpeechBLEU and SpeechTokenDistance, which are computed on speech discrete tokens. The evaluations on synthesized speech show that our method correlates better with human subjective ratings than mel cepstral distortion and a recent mean opinion score prediction model. Also, they are effective in noisy speech evaluation and have cross-lingual applicability.",
      "authors": [
        "Takaaki Saeki",
        "Soumi Maiti",
        "Shinnosuke Takamichi",
        "Shinji Watanabe",
        "Hiroshi Saruwatari"
      ],
      "published": "2024-01-30T08:26:28Z",
      "updated": "2024-09-01T14:34:27Z",
      "categories": [
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2401.16812v3",
      "landing_url": "https://arxiv.org/abs/2401.16812v3",
      "doi": "https://doi.org/10.48550/arXiv.2401.16812"
    },
    "queries": [
      {
        "anchor": "synchronous dialogue",
        "search_term": "discrete tokens"
      }
    ]
  },
  {
    "arxiv_id": "2402.01271",
    "anchor": "synchronous dialogue",
    "search_term": "neural codec",
    "search_record": {
      "id": "http://arxiv.org/abs/2402.01271v1",
      "title": "An Intra-BRNN and GB-RVQ Based END-TO-END Neural Audio Codec",
      "summary": "Recently, neural networks have proven to be effective in performing speech coding task at low bitrates. However, under-utilization of intra-frame correlations and the error of quantizer specifically degrade the reconstructed audio quality. To improve the coding quality, we present an end-to-end neural speech codec, namely CBRC (Convolutional and Bidirectional Recurrent neural Codec). An interleaved structure using 1D-CNN and Intra-BRNN is designed to exploit the intra-frame correlations more efficiently. Furthermore, Group-wise and Beam-search Residual Vector Quantizer (GB-RVQ) is used to reduce the quantization noise. CBRC encodes audio every 20ms with no additional latency, which is suitable for real-time communication. Experimental results demonstrate the superiority of the proposed codec when comparing CBRC at 3kbps with Opus at 12kbps.",
      "published": "2024-02-02T09:55:15Z"
    },
    "metadata": {
      "arxiv_id": "2402.01271",
      "title": "An Intra-BRNN and GB-RVQ Based END-TO-END Neural Audio Codec",
      "summary": "Recently, neural networks have proven to be effective in performing speech coding task at low bitrates. However, under-utilization of intra-frame correlations and the error of quantizer specifically degrade the reconstructed audio quality. To improve the coding quality, we present an end-to-end neural speech codec, namely CBRC (Convolutional and Bidirectional Recurrent neural Codec). An interleaved structure using 1D-CNN and Intra-BRNN is designed to exploit the intra-frame correlations more efficiently. Furthermore, Group-wise and Beam-search Residual Vector Quantizer (GB-RVQ) is used to reduce the quantization noise. CBRC encodes audio every 20ms with no additional latency, which is suitable for real-time communication. Experimental results demonstrate the superiority of the proposed codec when comparing CBRC at 3kbps with Opus at 12kbps.",
      "authors": [
        "Linping Xu",
        "Jiawei Jiang",
        "Dejun Zhang",
        "Xianjun Xia",
        "Li Chen",
        "Yijian Xiao",
        "Piao Ding",
        "Shenyi Song",
        "Sixing Yin",
        "Ferdous Sohel"
      ],
      "published": "2024-02-02T09:55:15Z",
      "updated": "2024-02-02T09:55:15Z",
      "categories": [
        "eess.AS",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2402.01271v1",
      "landing_url": "https://arxiv.org/abs/2402.01271v1",
      "doi": "https://doi.org/10.48550/arXiv.2402.01271"
    },
    "queries": [
      {
        "anchor": "synchronous dialogue",
        "search_term": "neural codec"
      }
    ]
  },
  {
    "arxiv_id": "2402.03886",
    "anchor": "full-duplex",
    "search_term": "true full-duplex",
    "search_record": {
      "id": "http://arxiv.org/abs/2402.03886v2",
      "title": "Full-Duplex Millimeter Wave MIMO Channel Estimation: A Neural Network Approach",
      "summary": "Millimeter wave (mmWave) multiple-input-multi-output (MIMO) is now a reality with great potential for further improvement. We study full-duplex transmissions as an effective way to improve mmWave MIMO systems. Compared to half-duplex systems, full-duplex transmissions may offer higher data rates and lower latency. However, full-duplex transmission is hindered by self-interference (SI) at the receive antennas, and SI channel estimation becomes a crucial step to make the full-duplex systems feasible. In this paper, we address the problem of channel estimation in full-duplex mmWave MIMO systems using neural networks (NNs). Our approach involves sharing pilot resources between user equipments (UEs) and transmit antennas at the base station (BS), aiming to reduce the pilot overhead in full-duplex systems and to achieve a comparable level to that of a half-duplex system. Additionally, in the case of separate antenna configurations in a full-duplex BS, providing channel estimates of transmit antenna (TX) arrays to the downlink UEs poses another challenge, as the TX arrays are not capable of receiving pilot signals. To address this, we employ an NN to map the channel from the downlink UEs to the receive antenna (RX) arrays to the channel from the TX arrays to the downlink UEs. We further elaborate on how NNs perform the estimation with different architectures, (e.g., different numbers of hidden layers), the introduction of non-linear distortion (e.g., with a 1-bit analog-to-digital converter (ADC)), and different channel conditions (e.g., low-correlated and high-correlated channels). Our work provides novel insights into NN-based channel estimators.",
      "published": "2024-02-06T10:48:59Z"
    },
    "metadata": {
      "arxiv_id": "2402.03886",
      "title": "Full-Duplex Millimeter Wave MIMO Channel Estimation: A Neural Network Approach",
      "summary": "Millimeter wave (mmWave) multiple-input-multi-output (MIMO) is now a reality with great potential for further improvement. We study full-duplex transmissions as an effective way to improve mmWave MIMO systems. Compared to half-duplex systems, full-duplex transmissions may offer higher data rates and lower latency. However, full-duplex transmission is hindered by self-interference (SI) at the receive antennas, and SI channel estimation becomes a crucial step to make the full-duplex systems feasible. In this paper, we address the problem of channel estimation in full-duplex mmWave MIMO systems using neural networks (NNs). Our approach involves sharing pilot resources between user equipments (UEs) and transmit antennas at the base station (BS), aiming to reduce the pilot overhead in full-duplex systems and to achieve a comparable level to that of a half-duplex system. Additionally, in the case of separate antenna configurations in a full-duplex BS, providing channel estimates of transmit antenna (TX) arrays to the downlink UEs poses another challenge, as the TX arrays are not capable of receiving pilot signals. To address this, we employ an NN to map the channel from the downlink UEs to the receive antenna (RX) arrays to the channel from the TX arrays to the downlink UEs. We further elaborate on how NNs perform the estimation with different architectures, (e.g., different numbers of hidden layers), the introduction of non-linear distortion (e.g., with a 1-bit analog-to-digital converter (ADC)), and different channel conditions (e.g., low-correlated and high-correlated channels). Our work provides novel insights into NN-based channel estimators.",
      "authors": [
        "Mehdi Sattari",
        "Hao Guo",
        "Deniz Gündüz",
        "Ashkan Panahi",
        "Tommy Svensson"
      ],
      "published": "2024-02-06T10:48:59Z",
      "updated": "2024-06-18T12:34:43Z",
      "categories": [
        "cs.IT",
        "eess.SP"
      ],
      "pdf_url": "https://arxiv.org/pdf/2402.03886v2",
      "landing_url": "https://arxiv.org/abs/2402.03886v2",
      "doi": "https://doi.org/10.1109/TMLCN.2024.3432865"
    },
    "queries": [
      {
        "anchor": "full-duplex",
        "search_term": "true full-duplex"
      },
      {
        "anchor": "full-duplex",
        "search_term": "pseudo full-duplex"
      },
      {
        "anchor": "full-duplex",
        "search_term": "half duplex"
      },
      {
        "anchor": "full-duplex",
        "search_term": "engineered synchronization"
      },
      {
        "anchor": "full-duplex",
        "search_term": "learned synchronization"
      },
      {
        "anchor": "full-duplex",
        "search_term": "modular architecture"
      },
      {
        "anchor": "full-duplex",
        "search_term": "streaming architecture"
      },
      {
        "anchor": "full-duplex",
        "search_term": "real-time interaction"
      },
      {
        "anchor": "full-duplex",
        "search_term": "low latency"
      },
      {
        "anchor": "full-duplex",
        "search_term": "synchronous data"
      },
      {
        "anchor": "full-duplex",
        "search_term": "data scarcity"
      },
      {
        "anchor": "full-duplex",
        "search_term": "multichannel dialogue"
      },
      {
        "anchor": "full-duplex",
        "search_term": "temporal dynamics"
      },
      {
        "anchor": "full-duplex",
        "search_term": "behavioral arbitration"
      },
      {
        "anchor": "full-duplex",
        "search_term": "semantic coherence"
      },
      {
        "anchor": "full-duplex",
        "search_term": "neural codec"
      },
      {
        "anchor": "full-duplex",
        "search_term": "discrete tokens"
      },
      {
        "anchor": "full-duplex",
        "search_term": "audio tokens"
      },
      {
        "anchor": "full-duplex",
        "search_term": "floor transfer offset"
      },
      {
        "anchor": "full-duplex",
        "search_term": "stop latency"
      },
      {
        "anchor": "full-duplex",
        "search_term": "interruption response delay"
      }
    ]
  },
  {
    "arxiv_id": "2402.04229",
    "anchor": "synchronous dialogue",
    "search_term": "audio tokens",
    "search_record": {
      "id": "http://arxiv.org/abs/2402.04229v1",
      "title": "MusicRL: Aligning Music Generation to Human Preferences",
      "summary": "We propose MusicRL, the first music generation system finetuned from human feedback. Appreciation of text-to-music models is particularly subjective since the concept of musicality as well as the specific intention behind a caption are user-dependent (e.g. a caption such as \"upbeat work-out music\" can map to a retro guitar solo or a techno pop beat). Not only this makes supervised training of such models challenging, but it also calls for integrating continuous human feedback in their post-deployment finetuning. MusicRL is a pretrained autoregressive MusicLM (Agostinelli et al., 2023) model of discrete audio tokens finetuned with reinforcement learning to maximise sequence-level rewards. We design reward functions related specifically to text-adherence and audio quality with the help from selected raters, and use those to finetune MusicLM into MusicRL-R. We deploy MusicLM to users and collect a substantial dataset comprising 300,000 pairwise preferences. Using Reinforcement Learning from Human Feedback (RLHF), we train MusicRL-U, the first text-to-music model that incorporates human feedback at scale. Human evaluations show that both MusicRL-R and MusicRL-U are preferred to the baseline. Ultimately, MusicRL-RU combines the two approaches and results in the best model according to human raters. Ablation studies shed light on the musical attributes influencing human preferences, indicating that text adherence and quality only account for a part of it. This underscores the prevalence of subjectivity in musical appreciation and calls for further involvement of human listeners in the finetuning of music generation models.",
      "published": "2024-02-06T18:36:52Z"
    },
    "metadata": {
      "arxiv_id": "2402.04229",
      "title": "MusicRL: Aligning Music Generation to Human Preferences",
      "summary": "We propose MusicRL, the first music generation system finetuned from human feedback. Appreciation of text-to-music models is particularly subjective since the concept of musicality as well as the specific intention behind a caption are user-dependent (e.g. a caption such as \"upbeat work-out music\" can map to a retro guitar solo or a techno pop beat). Not only this makes supervised training of such models challenging, but it also calls for integrating continuous human feedback in their post-deployment finetuning. MusicRL is a pretrained autoregressive MusicLM (Agostinelli et al., 2023) model of discrete audio tokens finetuned with reinforcement learning to maximise sequence-level rewards. We design reward functions related specifically to text-adherence and audio quality with the help from selected raters, and use those to finetune MusicLM into MusicRL-R. We deploy MusicLM to users and collect a substantial dataset comprising 300,000 pairwise preferences. Using Reinforcement Learning from Human Feedback (RLHF), we train MusicRL-U, the first text-to-music model that incorporates human feedback at scale. Human evaluations show that both MusicRL-R and MusicRL-U are preferred to the baseline. Ultimately, MusicRL-RU combines the two approaches and results in the best model according to human raters. Ablation studies shed light on the musical attributes influencing human preferences, indicating that text adherence and quality only account for a part of it. This underscores the prevalence of subjectivity in musical appreciation and calls for further involvement of human listeners in the finetuning of music generation models.",
      "authors": [
        "Geoffrey Cideron",
        "Sertan Girgin",
        "Mauro Verzetti",
        "Damien Vincent",
        "Matej Kastelic",
        "Zalán Borsos",
        "Brian McWilliams",
        "Victor Ungureanu",
        "Olivier Bachem",
        "Olivier Pietquin",
        "Matthieu Geist",
        "Léonard Hussenot",
        "Neil Zeghidour",
        "Andrea Agostinelli"
      ],
      "published": "2024-02-06T18:36:52Z",
      "updated": "2024-02-06T18:36:52Z",
      "categories": [
        "cs.LG",
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2402.04229v1",
      "landing_url": "https://arxiv.org/abs/2402.04229v1",
      "doi": "https://doi.org/10.48550/arXiv.2402.04229"
    },
    "queries": [
      {
        "anchor": "synchronous dialogue",
        "search_term": "audio tokens"
      }
    ]
  },
  {
    "arxiv_id": "2402.06656",
    "anchor": "synchronous dialogue",
    "search_term": "data scarcity",
    "search_record": {
      "id": "http://arxiv.org/abs/2402.06656v1",
      "title": "DiffsFormer: A Diffusion Transformer on Stock Factor Augmentation",
      "summary": "Machine learning models have demonstrated remarkable efficacy and efficiency in a wide range of stock forecasting tasks. However, the inherent challenges of data scarcity, including low signal-to-noise ratio (SNR) and data homogeneity, pose significant obstacles to accurate forecasting. To address this issue, we propose a novel approach that utilizes artificial intelligence-generated samples (AIGS) to enhance the training procedures. In our work, we introduce the Diffusion Model to generate stock factors with Transformer architecture (DiffsFormer). DiffsFormer is initially trained on a large-scale source domain, incorporating conditional guidance so as to capture global joint distribution. When presented with a specific downstream task, we employ DiffsFormer to augment the training procedure by editing existing samples. This editing step allows us to control the strength of the editing process, determining the extent to which the generated data deviates from the target domain. To evaluate the effectiveness of DiffsFormer augmented training, we conduct experiments on the CSI300 and CSI800 datasets, employing eight commonly used machine learning models. The proposed method achieves relative improvements of 7.2% and 27.8% in annualized return ratio for the respective datasets. Furthermore, we perform extensive experiments to gain insights into the functionality of DiffsFormer and its constituent components, elucidating how they address the challenges of data scarcity and enhance the overall model performance. Our research demonstrates the efficacy of leveraging AIGS and the DiffsFormer architecture to mitigate data scarcity in stock forecasting tasks.",
      "published": "2024-02-05T03:54:36Z"
    },
    "metadata": {
      "arxiv_id": "2402.06656",
      "title": "DiffsFormer: A Diffusion Transformer on Stock Factor Augmentation",
      "summary": "Machine learning models have demonstrated remarkable efficacy and efficiency in a wide range of stock forecasting tasks. However, the inherent challenges of data scarcity, including low signal-to-noise ratio (SNR) and data homogeneity, pose significant obstacles to accurate forecasting. To address this issue, we propose a novel approach that utilizes artificial intelligence-generated samples (AIGS) to enhance the training procedures. In our work, we introduce the Diffusion Model to generate stock factors with Transformer architecture (DiffsFormer). DiffsFormer is initially trained on a large-scale source domain, incorporating conditional guidance so as to capture global joint distribution. When presented with a specific downstream task, we employ DiffsFormer to augment the training procedure by editing existing samples. This editing step allows us to control the strength of the editing process, determining the extent to which the generated data deviates from the target domain. To evaluate the effectiveness of DiffsFormer augmented training, we conduct experiments on the CSI300 and CSI800 datasets, employing eight commonly used machine learning models. The proposed method achieves relative improvements of 7.2% and 27.8% in annualized return ratio for the respective datasets. Furthermore, we perform extensive experiments to gain insights into the functionality of DiffsFormer and its constituent components, elucidating how they address the challenges of data scarcity and enhance the overall model performance. Our research demonstrates the efficacy of leveraging AIGS and the DiffsFormer architecture to mitigate data scarcity in stock forecasting tasks.",
      "authors": [
        "Yuan Gao",
        "Haokun Chen",
        "Xiang Wang",
        "Zhicai Wang",
        "Xue Wang",
        "Jinyang Gao",
        "Bolin Ding"
      ],
      "published": "2024-02-05T03:54:36Z",
      "updated": "2024-02-05T03:54:36Z",
      "categories": [
        "q-fin.ST",
        "cs.AI",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2402.06656v1",
      "landing_url": "https://arxiv.org/abs/2402.06656v1",
      "doi": "https://doi.org/10.48550/arXiv.2402.06656"
    },
    "queries": [
      {
        "anchor": "synchronous dialogue",
        "search_term": "data scarcity"
      }
    ]
  },
  {
    "arxiv_id": "2402.09148",
    "anchor": "spoken language models",
    "search_term": "real-time interaction",
    "search_record": {
      "id": "http://arxiv.org/abs/2402.09148v1",
      "title": "BiasEye: A Bias-Aware Real-time Interactive Material Screening System for Impartial Candidate Assessment",
      "summary": "In the process of evaluating competencies for job or student recruitment through material screening, decision-makers can be influenced by inherent cognitive biases, such as the screening order or anchoring information, leading to inconsistent outcomes. To tackle this challenge, we conducted interviews with seven experts to understand their challenges and needs for support in the screening process. Building on their insights, we introduce BiasEye, a bias-aware real-time interactive material screening visualization system. BiasEye enhances awareness of cognitive biases by improving information accessibility and transparency. It also aids users in identifying and mitigating biases through a machine learning (ML) approach that models individual screening preferences. Findings from a mixed-design user study with 20 participants demonstrate that, compared to a baseline system lacking our bias-aware features, BiasEye increases participants' bias awareness and boosts their confidence in making final decisions. At last, we discuss the potential of ML and visualization in mitigating biases during human decision-making tasks.",
      "published": "2024-02-14T12:57:26Z"
    },
    "metadata": {
      "arxiv_id": "2402.09148",
      "title": "BiasEye: A Bias-Aware Real-time Interactive Material Screening System for Impartial Candidate Assessment",
      "summary": "In the process of evaluating competencies for job or student recruitment through material screening, decision-makers can be influenced by inherent cognitive biases, such as the screening order or anchoring information, leading to inconsistent outcomes. To tackle this challenge, we conducted interviews with seven experts to understand their challenges and needs for support in the screening process. Building on their insights, we introduce BiasEye, a bias-aware real-time interactive material screening visualization system. BiasEye enhances awareness of cognitive biases by improving information accessibility and transparency. It also aids users in identifying and mitigating biases through a machine learning (ML) approach that models individual screening preferences. Findings from a mixed-design user study with 20 participants demonstrate that, compared to a baseline system lacking our bias-aware features, BiasEye increases participants' bias awareness and boosts their confidence in making final decisions. At last, we discuss the potential of ML and visualization in mitigating biases during human decision-making tasks.",
      "authors": [
        "Qianyu Liu",
        "Haoran Jiang",
        "Zihao Pan",
        "Qiushi Han",
        "Zhenhui Peng",
        "Quan Li"
      ],
      "published": "2024-02-14T12:57:26Z",
      "updated": "2024-02-14T12:57:26Z",
      "categories": [
        "cs.HC"
      ],
      "pdf_url": "https://arxiv.org/pdf/2402.09148v1",
      "landing_url": "https://arxiv.org/abs/2402.09148v1",
      "doi": "https://doi.org/10.1145/3640543.3645166"
    },
    "queries": [
      {
        "anchor": "spoken language models",
        "search_term": "real-time interaction"
      },
      {
        "anchor": "synchronous dialogue",
        "search_term": "real-time interaction"
      },
      {
        "anchor": "turn-taking",
        "search_term": "real-time interaction"
      }
    ]
  },
  {
    "arxiv_id": "2402.09445",
    "anchor": "synchronous dialogue",
    "search_term": "synchronous data",
    "search_record": {
      "id": "http://arxiv.org/abs/2402.09445v2",
      "title": "iMove: Exploring Bio-impedance Sensing for Fitness Activity Recognition",
      "summary": "Automatic and precise fitness activity recognition can be beneficial in aspects from promoting a healthy lifestyle to personalized preventative healthcare. While IMUs are currently the prominent fitness tracking modality, through iMove, we show bio-impedence can help improve IMU-based fitness tracking through sensor fusion and contrastive learning.To evaluate our methods, we conducted an experiment including six upper body fitness activities performed by ten subjects over five days to collect synchronized data from bio-impedance across two wrists and IMU on the left wrist.The contrastive learning framework uses the two modalities to train a better IMU-only classification model, where bio-impedance is only required at the training phase, by which the average Macro F1 score with the input of a single IMU was improved by 3.22 \\% reaching 84.71 \\% compared to the 81.49 \\% of the IMU baseline model. We have also shown how bio-impedance can improve human activity recognition (HAR) directly through sensor fusion, reaching an average Macro F1 score of 89.57 \\% (two modalities required for both training and inference) even if Bio-impedance alone has an average macro F1 score of 75.36 \\%, which is outperformed by IMU alone. In addition, similar results were obtained in an extended study on lower body fitness activity classification, demonstrating the generalisability of our approach.Our findings underscore the potential of sensor fusion and contrastive learning as valuable tools for advancing fitness activity recognition, with bio-impedance playing a pivotal role in augmenting the capabilities of IMU-based systems.",
      "published": "2024-01-31T16:53:50Z"
    },
    "metadata": {
      "arxiv_id": "2402.09445",
      "title": "iMove: Exploring Bio-impedance Sensing for Fitness Activity Recognition",
      "summary": "Automatic and precise fitness activity recognition can be beneficial in aspects from promoting a healthy lifestyle to personalized preventative healthcare. While IMUs are currently the prominent fitness tracking modality, through iMove, we show bio-impedence can help improve IMU-based fitness tracking through sensor fusion and contrastive learning.To evaluate our methods, we conducted an experiment including six upper body fitness activities performed by ten subjects over five days to collect synchronized data from bio-impedance across two wrists and IMU on the left wrist.The contrastive learning framework uses the two modalities to train a better IMU-only classification model, where bio-impedance is only required at the training phase, by which the average Macro F1 score with the input of a single IMU was improved by 3.22 \\% reaching 84.71 \\% compared to the 81.49 \\% of the IMU baseline model. We have also shown how bio-impedance can improve human activity recognition (HAR) directly through sensor fusion, reaching an average Macro F1 score of 89.57 \\% (two modalities required for both training and inference) even if Bio-impedance alone has an average macro F1 score of 75.36 \\%, which is outperformed by IMU alone. In addition, similar results were obtained in an extended study on lower body fitness activity classification, demonstrating the generalisability of our approach.Our findings underscore the potential of sensor fusion and contrastive learning as valuable tools for advancing fitness activity recognition, with bio-impedance playing a pivotal role in augmenting the capabilities of IMU-based systems.",
      "authors": [
        "Mengxi Liu",
        "Vitor Fortes Rey",
        "Yu Zhang",
        "Lala Shakti Swarup Ray",
        "Bo Zhou",
        "Paul Lukowicz"
      ],
      "published": "2024-01-31T16:53:50Z",
      "updated": "2024-06-03T12:42:50Z",
      "categories": [
        "eess.SP",
        "cs.AI",
        "cs.LG",
        "cs.RO"
      ],
      "pdf_url": "https://arxiv.org/pdf/2402.09445v2",
      "landing_url": "https://arxiv.org/abs/2402.09445v2",
      "doi": "https://doi.org/10.1109/PerCom59722.2024.10494489"
    },
    "queries": [
      {
        "anchor": "synchronous dialogue",
        "search_term": "synchronous data"
      }
    ]
  },
  {
    "arxiv_id": "2402.12706",
    "anchor": "spoken language models",
    "search_term": "temporal dynamics",
    "search_record": {
      "id": "http://arxiv.org/abs/2402.12706v2",
      "title": "Learning Causal Domain-Invariant Temporal Dynamics for Few-Shot Action Recognition",
      "summary": "Few-shot action recognition aims at quickly adapting a pre-trained model to the novel data with a distribution shift using only a limited number of samples. Key challenges include how to identify and leverage the transferable knowledge learned by the pre-trained model. We therefore propose CDTD, or Causal Domain-Invariant Temporal Dynamics for knowledge transfer. To identify the temporally invariant and variant representations, we employ the causal representation learning methods for unsupervised pertaining, and then tune the classifier with supervisions in next stage. Specifically, we assume the domain information can be well estimated and the pre-trained image decoder and transition models can be well transferred. During adaptation, we fix the transferable temporal dynamics and update the image encoder and domain estimator. The efficacy of our approach is revealed by the superior accuracy of CDTD over leading alternatives across standard few-shot action recognition datasets.",
      "published": "2024-02-20T04:09:58Z"
    },
    "metadata": {
      "arxiv_id": "2402.12706",
      "title": "Learning Causal Domain-Invariant Temporal Dynamics for Few-Shot Action Recognition",
      "summary": "Few-shot action recognition aims at quickly adapting a pre-trained model to the novel data with a distribution shift using only a limited number of samples. Key challenges include how to identify and leverage the transferable knowledge learned by the pre-trained model. We therefore propose CDTD, or Causal Domain-Invariant Temporal Dynamics for knowledge transfer. To identify the temporally invariant and variant representations, we employ the causal representation learning methods for unsupervised pertaining, and then tune the classifier with supervisions in next stage. Specifically, we assume the domain information can be well estimated and the pre-trained image decoder and transition models can be well transferred. During adaptation, we fix the transferable temporal dynamics and update the image encoder and domain estimator. The efficacy of our approach is revealed by the superior accuracy of CDTD over leading alternatives across standard few-shot action recognition datasets.",
      "authors": [
        "Yuke Li",
        "Guangyi Chen",
        "Ben Abramowitz",
        "Stefano Anzellott",
        "Donglai Wei"
      ],
      "published": "2024-02-20T04:09:58Z",
      "updated": "2024-06-04T18:08:31Z",
      "categories": [
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2402.12706v2",
      "landing_url": "https://arxiv.org/abs/2402.12706v2",
      "doi": "https://doi.org/10.48550/arXiv.2402.12706"
    },
    "queries": [
      {
        "anchor": "spoken language models",
        "search_term": "temporal dynamics"
      },
      {
        "anchor": "synchronous dialogue",
        "search_term": "temporal dynamics"
      },
      {
        "anchor": "turn-taking",
        "search_term": "temporal dynamics"
      }
    ]
  },
  {
    "arxiv_id": "2402.14136",
    "anchor": "synchronous dialogue",
    "search_term": "synchronous data",
    "search_record": {
      "id": "http://arxiv.org/abs/2402.14136v1",
      "title": "GDTM: An Indoor Geospatial Tracking Dataset with Distributed Multimodal Sensors",
      "summary": "Constantly locating moving objects, i.e., geospatial tracking, is essential for autonomous building infrastructure. Accurate and robust geospatial tracking often leverages multimodal sensor fusion algorithms, which require large datasets with time-aligned, synchronized data from various sensor types. However, such datasets are not readily available. Hence, we propose GDTM, a nine-hour dataset for multimodal object tracking with distributed multimodal sensors and reconfigurable sensor node placements. Our dataset enables the exploration of several research problems, such as optimizing architectures for processing multimodal data, and investigating models' robustness to adverse sensing conditions and sensor placement variances. A GitHub repository containing the code, sample data, and checkpoints of this work is available at https://github.com/nesl/GDTM.",
      "published": "2024-02-21T21:24:57Z"
    },
    "metadata": {
      "arxiv_id": "2402.14136",
      "title": "GDTM: An Indoor Geospatial Tracking Dataset with Distributed Multimodal Sensors",
      "summary": "Constantly locating moving objects, i.e., geospatial tracking, is essential for autonomous building infrastructure. Accurate and robust geospatial tracking often leverages multimodal sensor fusion algorithms, which require large datasets with time-aligned, synchronized data from various sensor types. However, such datasets are not readily available. Hence, we propose GDTM, a nine-hour dataset for multimodal object tracking with distributed multimodal sensors and reconfigurable sensor node placements. Our dataset enables the exploration of several research problems, such as optimizing architectures for processing multimodal data, and investigating models' robustness to adverse sensing conditions and sensor placement variances. A GitHub repository containing the code, sample data, and checkpoints of this work is available at https://github.com/nesl/GDTM.",
      "authors": [
        "Ho Lyun Jeong",
        "Ziqi Wang",
        "Colin Samplawski",
        "Jason Wu",
        "Shiwei Fang",
        "Lance M. Kaplan",
        "Deepak Ganesan",
        "Benjamin Marlin",
        "Mani Srivastava"
      ],
      "published": "2024-02-21T21:24:57Z",
      "updated": "2024-02-21T21:24:57Z",
      "categories": [
        "cs.RO",
        "cs.LG",
        "eess.SP"
      ],
      "pdf_url": "https://arxiv.org/pdf/2402.14136v1",
      "landing_url": "https://arxiv.org/abs/2402.14136v1",
      "doi": "https://doi.org/10.48550/arXiv.2402.14136"
    },
    "queries": [
      {
        "anchor": "synchronous dialogue",
        "search_term": "synchronous data"
      }
    ]
  },
  {
    "arxiv_id": "2402.16248",
    "anchor": "synchronous dialogue",
    "search_term": "semantic coherence",
    "search_record": {
      "id": "http://arxiv.org/abs/2402.16248v1",
      "title": "Topic-to-essay generation with knowledge-based content selection",
      "summary": "The topic-to-essay generation task is a challenging natural language generation task that aims to generate paragraph-level text with high semantic coherence based on a given set of topic words. Previous work has focused on the introduction of external knowledge, ignoring the insufficient generated text diversity. In order to improve the generation diversity, we propose a novel copy mechanism model with a content selection module that integrates rich semantic knowledge from the language model into the decoder. Furthermore, we introduce the improved prefix tuning method to train the model, enabling it to adapt to varying input complexities. In addition, we have contributed a new Chinese dataset for TEG tasks. Experimental results demonstrate that the proposed model can improve the generated text diversity by 35\\% to 59\\% compared to the state-of-the-art method, while maintaining a high level of topic consistency.",
      "published": "2024-02-26T02:14:42Z"
    },
    "metadata": {
      "arxiv_id": "2402.16248",
      "title": "Topic-to-essay generation with knowledge-based content selection",
      "summary": "The topic-to-essay generation task is a challenging natural language generation task that aims to generate paragraph-level text with high semantic coherence based on a given set of topic words. Previous work has focused on the introduction of external knowledge, ignoring the insufficient generated text diversity. In order to improve the generation diversity, we propose a novel copy mechanism model with a content selection module that integrates rich semantic knowledge from the language model into the decoder. Furthermore, we introduce the improved prefix tuning method to train the model, enabling it to adapt to varying input complexities. In addition, we have contributed a new Chinese dataset for TEG tasks. Experimental results demonstrate that the proposed model can improve the generated text diversity by 35\\% to 59\\% compared to the state-of-the-art method, while maintaining a high level of topic consistency.",
      "authors": [
        "Jieyong Wang",
        "Chunyao Song",
        "Yihao Wu"
      ],
      "published": "2024-02-26T02:14:42Z",
      "updated": "2024-02-26T02:14:42Z",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2402.16248v1",
      "landing_url": "https://arxiv.org/abs/2402.16248v1",
      "doi": "https://doi.org/10.48550/arXiv.2402.16248"
    },
    "queries": [
      {
        "anchor": "synchronous dialogue",
        "search_term": "semantic coherence"
      }
    ]
  },
  {
    "arxiv_id": "2402.18059",
    "anchor": "spoken language models",
    "search_term": "semantic coherence",
    "search_record": {
      "id": "http://arxiv.org/abs/2402.18059v3",
      "title": "Token-Specific Watermarking with Enhanced Detectability and Semantic Coherence for Large Language Models",
      "summary": "Large language models generate high-quality responses with potential misinformation, underscoring the need for regulation by distinguishing AI-generated and human-written texts. Watermarking is pivotal in this context, which involves embedding hidden markers in texts during the LLM inference phase, which is imperceptible to humans. Achieving both the detectability of inserted watermarks and the semantic quality of generated texts is challenging. While current watermarking algorithms have made promising progress in this direction, there remains significant scope for improvement. To address these challenges, we introduce a novel multi-objective optimization (MOO) approach for watermarking that utilizes lightweight networks to generate token-specific watermarking logits and splitting ratios. By leveraging MOO to optimize for both detection and semantic objective functions, our method simultaneously achieves detectability and semantic integrity. Experimental results show that our method outperforms current watermarking techniques in enhancing the detectability of texts generated by LLMs while maintaining their semantic coherence. Our code is available at https://github.com/mignonjia/TS_watermark.",
      "published": "2024-02-28T05:43:22Z"
    },
    "metadata": {
      "arxiv_id": "2402.18059",
      "title": "Token-Specific Watermarking with Enhanced Detectability and Semantic Coherence for Large Language Models",
      "summary": "Large language models generate high-quality responses with potential misinformation, underscoring the need for regulation by distinguishing AI-generated and human-written texts. Watermarking is pivotal in this context, which involves embedding hidden markers in texts during the LLM inference phase, which is imperceptible to humans. Achieving both the detectability of inserted watermarks and the semantic quality of generated texts is challenging. While current watermarking algorithms have made promising progress in this direction, there remains significant scope for improvement. To address these challenges, we introduce a novel multi-objective optimization (MOO) approach for watermarking that utilizes lightweight networks to generate token-specific watermarking logits and splitting ratios. By leveraging MOO to optimize for both detection and semantic objective functions, our method simultaneously achieves detectability and semantic integrity. Experimental results show that our method outperforms current watermarking techniques in enhancing the detectability of texts generated by LLMs while maintaining their semantic coherence. Our code is available at https://github.com/mignonjia/TS_watermark.",
      "authors": [
        "Mingjia Huo",
        "Sai Ashish Somayajula",
        "Youwei Liang",
        "Ruisi Zhang",
        "Farinaz Koushanfar",
        "Pengtao Xie"
      ],
      "published": "2024-02-28T05:43:22Z",
      "updated": "2024-06-06T04:49:37Z",
      "categories": [
        "cs.LG",
        "cs.CL",
        "cs.CR"
      ],
      "pdf_url": "https://arxiv.org/pdf/2402.18059v3",
      "landing_url": "https://arxiv.org/abs/2402.18059v3",
      "doi": "https://doi.org/10.48550/arXiv.2402.18059"
    },
    "queries": [
      {
        "anchor": "spoken language models",
        "search_term": "semantic coherence"
      },
      {
        "anchor": "synchronous dialogue",
        "search_term": "semantic coherence"
      },
      {
        "anchor": "turn-taking",
        "search_term": "semantic coherence"
      }
    ]
  },
  {
    "arxiv_id": "2402.18121",
    "anchor": "spoken language models",
    "search_term": "semantic coherence",
    "search_record": {
      "id": "http://arxiv.org/abs/2402.18121v1",
      "title": "Saving the legacy of Hero Ibash: Evaluating Four Language Models for Aminoacian",
      "summary": "This study assesses four cutting-edge language models in the underexplored Aminoacian language. Through evaluation, it scrutinizes their adaptability, effectiveness, and limitations in text generation, semantic coherence, and contextual understanding. Uncovering insights into these models' performance in a low-resourced language, this research pioneers pathways to bridge linguistic gaps. By offering benchmarks and understanding challenges, it lays groundwork for future advancements in natural language processing, aiming to elevate the applicability of language models in similar linguistic landscapes, marking a significant step toward inclusivity and progress in language technology.",
      "published": "2024-02-28T07:22:13Z"
    },
    "metadata": {
      "arxiv_id": "2402.18121",
      "title": "Saving the legacy of Hero Ibash: Evaluating Four Language Models for Aminoacian",
      "summary": "This study assesses four cutting-edge language models in the underexplored Aminoacian language. Through evaluation, it scrutinizes their adaptability, effectiveness, and limitations in text generation, semantic coherence, and contextual understanding. Uncovering insights into these models' performance in a low-resourced language, this research pioneers pathways to bridge linguistic gaps. By offering benchmarks and understanding challenges, it lays groundwork for future advancements in natural language processing, aiming to elevate the applicability of language models in similar linguistic landscapes, marking a significant step toward inclusivity and progress in language technology.",
      "authors": [
        "Yunze Xiao",
        "Yiyang Pan"
      ],
      "published": "2024-02-28T07:22:13Z",
      "updated": "2024-02-28T07:22:13Z",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2402.18121v1",
      "landing_url": "https://arxiv.org/abs/2402.18121v1",
      "doi": "https://doi.org/10.48550/arXiv.2402.18121"
    },
    "queries": [
      {
        "anchor": "spoken language models",
        "search_term": "semantic coherence"
      },
      {
        "anchor": "synchronous dialogue",
        "search_term": "semantic coherence"
      }
    ]
  },
  {
    "arxiv_id": "2402.18849",
    "anchor": "synchronous dialogue",
    "search_term": "semantic coherence",
    "search_record": {
      "id": "http://arxiv.org/abs/2402.18849v1",
      "title": "Enhancing Steganographic Text Extraction: Evaluating the Impact of NLP Models on Accuracy and Semantic Coherence",
      "summary": "This study discusses a new method combining image steganography technology with Natural Language Processing (NLP) large models, aimed at improving the accuracy and robustness of extracting steganographic text. Traditional Least Significant Bit (LSB) steganography techniques face challenges in accuracy and robustness of information extraction when dealing with complex character encoding, such as Chinese characters. To address this issue, this study proposes an innovative LSB-NLP hybrid framework. This framework integrates the advanced capabilities of NLP large models, such as error detection, correction, and semantic consistency analysis, as well as information reconstruction techniques, thereby significantly enhancing the robustness of steganographic text extraction. Experimental results show that the LSB-NLP hybrid framework excels in improving the extraction accuracy of steganographic text, especially in handling Chinese characters. The findings of this study not only confirm the effectiveness of combining image steganography technology and NLP large models but also propose new ideas for research and application in the field of information hiding. The successful implementation of this interdisciplinary approach demonstrates the great potential of integrating image steganography technology with natural language processing technology in solving complex information processing problems.",
      "published": "2024-02-29T04:53:06Z"
    },
    "metadata": {
      "arxiv_id": "2402.18849",
      "title": "Enhancing Steganographic Text Extraction: Evaluating the Impact of NLP Models on Accuracy and Semantic Coherence",
      "summary": "This study discusses a new method combining image steganography technology with Natural Language Processing (NLP) large models, aimed at improving the accuracy and robustness of extracting steganographic text. Traditional Least Significant Bit (LSB) steganography techniques face challenges in accuracy and robustness of information extraction when dealing with complex character encoding, such as Chinese characters. To address this issue, this study proposes an innovative LSB-NLP hybrid framework. This framework integrates the advanced capabilities of NLP large models, such as error detection, correction, and semantic consistency analysis, as well as information reconstruction techniques, thereby significantly enhancing the robustness of steganographic text extraction. Experimental results show that the LSB-NLP hybrid framework excels in improving the extraction accuracy of steganographic text, especially in handling Chinese characters. The findings of this study not only confirm the effectiveness of combining image steganography technology and NLP large models but also propose new ideas for research and application in the field of information hiding. The successful implementation of this interdisciplinary approach demonstrates the great potential of integrating image steganography technology with natural language processing technology in solving complex information processing problems.",
      "authors": [
        "Mingyang Li",
        "Maoqin Yuan",
        "Luyao Li",
        "Han Pengsihua"
      ],
      "published": "2024-02-29T04:53:06Z",
      "updated": "2024-02-29T04:53:06Z",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CL"
      ],
      "pdf_url": "https://arxiv.org/pdf/2402.18849v1",
      "landing_url": "https://arxiv.org/abs/2402.18849v1",
      "doi": "https://doi.org/10.48550/arXiv.2402.18849"
    },
    "queries": [
      {
        "anchor": "synchronous dialogue",
        "search_term": "semantic coherence"
      }
    ]
  },
  {
    "arxiv_id": "2403.02640",
    "anchor": "spoken language models",
    "search_term": "synchronous data",
    "search_record": {
      "id": "http://arxiv.org/abs/2403.02640v3",
      "title": "HoloVIC: Large-scale Dataset and Benchmark for Multi-Sensor Holographic Intersection and Vehicle-Infrastructure Cooperative",
      "summary": "Vehicle-to-everything (V2X) is a popular topic in the field of Autonomous Driving in recent years. Vehicle-infrastructure cooperation (VIC) becomes one of the important research area. Due to the complexity of traffic conditions such as blind spots and occlusion, it greatly limits the perception capabilities of single-view roadside sensing systems. To further enhance the accuracy of roadside perception and provide better information to the vehicle side, in this paper, we constructed holographic intersections with various layouts to build a large-scale multi-sensor holographic vehicle-infrastructure cooperation dataset, called HoloVIC. Our dataset includes 3 different types of sensors (Camera, Lidar, Fisheye) and employs 4 sensor-layouts based on the different intersections. Each intersection is equipped with 6-18 sensors to capture synchronous data. While autonomous vehicles pass through these intersections for collecting VIC data. HoloVIC contains in total on 100k+ synchronous frames from different sensors. Additionally, we annotated 3D bounding boxes based on Camera, Fisheye, and Lidar. We also associate the IDs of the same objects across different devices and consecutive frames in sequence. Based on HoloVIC, we formulated four tasks to facilitate the development of related research. We also provide benchmarks for these tasks.",
      "published": "2024-03-05T04:08:19Z"
    },
    "metadata": {
      "arxiv_id": "2403.02640",
      "title": "HoloVIC: Large-scale Dataset and Benchmark for Multi-Sensor Holographic Intersection and Vehicle-Infrastructure Cooperative",
      "summary": "Vehicle-to-everything (V2X) is a popular topic in the field of Autonomous Driving in recent years. Vehicle-infrastructure cooperation (VIC) becomes one of the important research area. Due to the complexity of traffic conditions such as blind spots and occlusion, it greatly limits the perception capabilities of single-view roadside sensing systems. To further enhance the accuracy of roadside perception and provide better information to the vehicle side, in this paper, we constructed holographic intersections with various layouts to build a large-scale multi-sensor holographic vehicle-infrastructure cooperation dataset, called HoloVIC. Our dataset includes 3 different types of sensors (Camera, Lidar, Fisheye) and employs 4 sensor-layouts based on the different intersections. Each intersection is equipped with 6-18 sensors to capture synchronous data. While autonomous vehicles pass through these intersections for collecting VIC data. HoloVIC contains in total on 100k+ synchronous frames from different sensors. Additionally, we annotated 3D bounding boxes based on Camera, Fisheye, and Lidar. We also associate the IDs of the same objects across different devices and consecutive frames in sequence. Based on HoloVIC, we formulated four tasks to facilitate the development of related research. We also provide benchmarks for these tasks.",
      "authors": [
        "Cong Ma",
        "Lei Qiao",
        "Chengkai Zhu",
        "Kai Liu",
        "Zelong Kong",
        "Qing Li",
        "Xueqi Zhou",
        "Yuheng Kan",
        "Wei Wu"
      ],
      "published": "2024-03-05T04:08:19Z",
      "updated": "2024-03-26T17:14:14Z",
      "categories": [
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2403.02640v3",
      "landing_url": "https://arxiv.org/abs/2403.02640v3",
      "doi": "https://doi.org/10.48550/arXiv.2403.02640"
    },
    "queries": [
      {
        "anchor": "spoken language models",
        "search_term": "synchronous data"
      },
      {
        "anchor": "synchronous dialogue",
        "search_term": "synchronous data"
      }
    ]
  },
  {
    "arxiv_id": "2403.03100",
    "anchor": "synchronous dialogue",
    "search_term": "neural codec",
    "search_record": {
      "id": "http://arxiv.org/abs/2403.03100v3",
      "title": "NaturalSpeech 3: Zero-Shot Speech Synthesis with Factorized Codec and Diffusion Models",
      "summary": "While recent large-scale text-to-speech (TTS) models have achieved significant progress, they still fall short in speech quality, similarity, and prosody. Considering speech intricately encompasses various attributes (e.g., content, prosody, timbre, and acoustic details) that pose significant challenges for generation, a natural idea is to factorize speech into individual subspaces representing different attributes and generate them individually. Motivated by it, we propose NaturalSpeech 3, a TTS system with novel factorized diffusion models to generate natural speech in a zero-shot way. Specifically, 1) we design a neural codec with factorized vector quantization (FVQ) to disentangle speech waveform into subspaces of content, prosody, timbre, and acoustic details; 2) we propose a factorized diffusion model to generate attributes in each subspace following its corresponding prompt. With this factorization design, NaturalSpeech 3 can effectively and efficiently model intricate speech with disentangled subspaces in a divide-and-conquer way. Experiments show that NaturalSpeech 3 outperforms the state-of-the-art TTS systems on quality, similarity, prosody, and intelligibility, and achieves on-par quality with human recordings. Furthermore, we achieve better performance by scaling to 1B parameters and 200K hours of training data.",
      "published": "2024-03-05T16:35:25Z"
    },
    "metadata": {
      "arxiv_id": "2403.03100",
      "title": "NaturalSpeech 3: Zero-Shot Speech Synthesis with Factorized Codec and Diffusion Models",
      "summary": "While recent large-scale text-to-speech (TTS) models have achieved significant progress, they still fall short in speech quality, similarity, and prosody. Considering speech intricately encompasses various attributes (e.g., content, prosody, timbre, and acoustic details) that pose significant challenges for generation, a natural idea is to factorize speech into individual subspaces representing different attributes and generate them individually. Motivated by it, we propose NaturalSpeech 3, a TTS system with novel factorized diffusion models to generate natural speech in a zero-shot way. Specifically, 1) we design a neural codec with factorized vector quantization (FVQ) to disentangle speech waveform into subspaces of content, prosody, timbre, and acoustic details; 2) we propose a factorized diffusion model to generate attributes in each subspace following its corresponding prompt. With this factorization design, NaturalSpeech 3 can effectively and efficiently model intricate speech with disentangled subspaces in a divide-and-conquer way. Experiments show that NaturalSpeech 3 outperforms the state-of-the-art TTS systems on quality, similarity, prosody, and intelligibility, and achieves on-par quality with human recordings. Furthermore, we achieve better performance by scaling to 1B parameters and 200K hours of training data.",
      "authors": [
        "Zeqian Ju",
        "Yuancheng Wang",
        "Kai Shen",
        "Xu Tan",
        "Detai Xin",
        "Dongchao Yang",
        "Yanqing Liu",
        "Yichong Leng",
        "Kaitao Song",
        "Siliang Tang",
        "Zhizheng Wu",
        "Tao Qin",
        "Xiang-Yang Li",
        "Wei Ye",
        "Shikun Zhang",
        "Jiang Bian",
        "Lei He",
        "Jinyu Li",
        "Sheng Zhao"
      ],
      "published": "2024-03-05T16:35:25Z",
      "updated": "2024-04-23T08:38:03Z",
      "categories": [
        "eess.AS",
        "cs.AI",
        "cs.CL",
        "cs.LG",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2403.03100v3",
      "landing_url": "https://arxiv.org/abs/2403.03100v3",
      "doi": "https://doi.org/10.48550/arXiv.2403.03100"
    },
    "queries": [
      {
        "anchor": "synchronous dialogue",
        "search_term": "neural codec"
      }
    ]
  },
  {
    "arxiv_id": "2403.06487",
    "anchor": "turn-taking",
    "search_term": "true full-duplex",
    "search_record": {
      "id": "http://arxiv.org/abs/2403.06487v3",
      "title": "Multilingual Turn-taking Prediction Using Voice Activity Projection",
      "summary": "This paper investigates the application of voice activity projection (VAP), a predictive turn-taking model for spoken dialogue, on multilingual data, encompassing English, Mandarin, and Japanese. The VAP model continuously predicts the upcoming voice activities of participants in dyadic dialogue, leveraging a cross-attention Transformer to capture the dynamic interplay between participants. The results show that a monolingual VAP model trained on one language does not make good predictions when applied to other languages. However, a multilingual model, trained on all three languages, demonstrates predictive performance on par with monolingual models across all languages. Further analyses show that the multilingual model has learned to discern the language of the input signal. We also analyze the sensitivity to pitch, a prosodic cue that is thought to be important for turn-taking. Finally, we compare two different audio encoders, contrastive predictive coding (CPC) pre-trained on English, with a recent model based on multilingual wav2vec 2.0 (MMS).",
      "published": "2024-03-11T07:50:29Z"
    },
    "metadata": {
      "arxiv_id": "2403.06487",
      "title": "Multilingual Turn-taking Prediction Using Voice Activity Projection",
      "summary": "This paper investigates the application of voice activity projection (VAP), a predictive turn-taking model for spoken dialogue, on multilingual data, encompassing English, Mandarin, and Japanese. The VAP model continuously predicts the upcoming voice activities of participants in dyadic dialogue, leveraging a cross-attention Transformer to capture the dynamic interplay between participants. The results show that a monolingual VAP model trained on one language does not make good predictions when applied to other languages. However, a multilingual model, trained on all three languages, demonstrates predictive performance on par with monolingual models across all languages. Further analyses show that the multilingual model has learned to discern the language of the input signal. We also analyze the sensitivity to pitch, a prosodic cue that is thought to be important for turn-taking. Finally, we compare two different audio encoders, contrastive predictive coding (CPC) pre-trained on English, with a recent model based on multilingual wav2vec 2.0 (MMS).",
      "authors": [
        "Koji Inoue",
        "Bing'er Jiang",
        "Erik Ekstedt",
        "Tatsuya Kawahara",
        "Gabriel Skantze"
      ],
      "published": "2024-03-11T07:50:29Z",
      "updated": "2024-03-14T23:59:59Z",
      "categories": [
        "cs.CL",
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2403.06487v3",
      "landing_url": "https://arxiv.org/abs/2403.06487v3",
      "doi": "https://doi.org/10.48550/arXiv.2403.06487"
    },
    "queries": [
      {
        "anchor": "turn-taking",
        "search_term": "true full-duplex"
      },
      {
        "anchor": "turn-taking",
        "search_term": "pseudo full-duplex"
      },
      {
        "anchor": "turn-taking",
        "search_term": "half duplex"
      },
      {
        "anchor": "turn-taking",
        "search_term": "engineered synchronization"
      },
      {
        "anchor": "turn-taking",
        "search_term": "learned synchronization"
      },
      {
        "anchor": "turn-taking",
        "search_term": "modular architecture"
      },
      {
        "anchor": "turn-taking",
        "search_term": "streaming architecture"
      },
      {
        "anchor": "turn-taking",
        "search_term": "real-time interaction"
      },
      {
        "anchor": "turn-taking",
        "search_term": "low latency"
      },
      {
        "anchor": "turn-taking",
        "search_term": "synchronous data"
      },
      {
        "anchor": "turn-taking",
        "search_term": "data scarcity"
      },
      {
        "anchor": "turn-taking",
        "search_term": "multichannel dialogue"
      },
      {
        "anchor": "turn-taking",
        "search_term": "temporal dynamics"
      },
      {
        "anchor": "turn-taking",
        "search_term": "behavioral arbitration"
      },
      {
        "anchor": "turn-taking",
        "search_term": "semantic coherence"
      },
      {
        "anchor": "turn-taking",
        "search_term": "neural codec"
      },
      {
        "anchor": "turn-taking",
        "search_term": "discrete tokens"
      },
      {
        "anchor": "turn-taking",
        "search_term": "audio tokens"
      },
      {
        "anchor": "turn-taking",
        "search_term": "floor transfer offset"
      },
      {
        "anchor": "turn-taking",
        "search_term": "stop latency"
      },
      {
        "anchor": "turn-taking",
        "search_term": "interruption response delay"
      }
    ]
  },
  {
    "arxiv_id": "2403.06797",
    "anchor": "synchronous dialogue",
    "search_term": "data scarcity",
    "search_record": {
      "id": "http://arxiv.org/abs/2403.06797v1",
      "title": "Leveraging Internal Representations of Model for Magnetic Image Classification",
      "summary": "Data generated by edge devices has the potential to train intelligent autonomous systems across various domains. Despite the emergence of diverse machine learning approaches addressing privacy concerns and utilizing distributed data, security issues persist due to the sensitive storage of data shards in disparate locations. This paper introduces a potentially groundbreaking paradigm for machine learning model training, specifically designed for scenarios with only a single magnetic image and its corresponding label image available. We harness the capabilities of Deep Learning to generate concise yet informative samples, aiming to overcome data scarcity. Through the utilization of deep learning's internal representations, our objective is to efficiently address data scarcity issues and produce meaningful results. This methodology presents a promising avenue for training machine learning models with minimal data.",
      "published": "2024-03-11T15:15:50Z"
    },
    "metadata": {
      "arxiv_id": "2403.06797",
      "title": "Leveraging Internal Representations of Model for Magnetic Image Classification",
      "summary": "Data generated by edge devices has the potential to train intelligent autonomous systems across various domains. Despite the emergence of diverse machine learning approaches addressing privacy concerns and utilizing distributed data, security issues persist due to the sensitive storage of data shards in disparate locations. This paper introduces a potentially groundbreaking paradigm for machine learning model training, specifically designed for scenarios with only a single magnetic image and its corresponding label image available. We harness the capabilities of Deep Learning to generate concise yet informative samples, aiming to overcome data scarcity. Through the utilization of deep learning's internal representations, our objective is to efficiently address data scarcity issues and produce meaningful results. This methodology presents a promising avenue for training machine learning models with minimal data.",
      "authors": [
        "Adarsh N L",
        "Arun P",
        "Alok Porwal",
        "Malcolm Aranha"
      ],
      "published": "2024-03-11T15:15:50Z",
      "updated": "2024-03-11T15:15:50Z",
      "categories": [
        "cs.LG",
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2403.06797v1",
      "landing_url": "https://arxiv.org/abs/2403.06797v1",
      "doi": "https://doi.org/10.48550/arXiv.2403.06797"
    },
    "queries": [
      {
        "anchor": "synchronous dialogue",
        "search_term": "data scarcity"
      }
    ]
  },
  {
    "arxiv_id": "2403.07623",
    "anchor": "synchronous dialogue",
    "search_term": "streaming architecture",
    "search_record": {
      "id": "http://arxiv.org/abs/2403.07623v2",
      "title": "Empowering Sequential Recommendation from Collaborative Signals and Semantic Relatedness",
      "summary": "Sequential recommender systems (SRS) could capture dynamic user preferences by modeling historical behaviors ordered in time. Despite effectiveness, focusing only on the \\textit{collaborative signals} from behaviors does not fully grasp user interests. It is also significant to model the \\textit{semantic relatedness} reflected in content features, e.g., images and text. Towards that end, in this paper, we aim to enhance the SRS tasks by effectively unifying collaborative signals and semantic relatedness together. Notably, we empirically point out that it is nontrivial to achieve such a goal due to semantic gap issues. Thus, we propose an end-to-end two-stream architecture for sequential recommendation, named TSSR, to learn user preferences from ID-based and content-based sequence. Specifically, we first present novel hierarchical contrasting module, including coarse user-grained and fine item-grained terms, to align the representations of inter-modality. Furthermore, we also design a two-stream architecture to learn the dependence of intra-modality sequence and the complex interactions of inter-modality sequence, which can yield more expressive capacity in understanding user interests. We conduct extensive experiments on five public datasets. The experimental results show that the TSSR could yield superior performance than competitive baselines. We also make our experimental codes publicly available at https://github.com/Mingyue-Cheng/TSSR.",
      "published": "2024-03-12T13:06:31Z"
    },
    "metadata": {
      "arxiv_id": "2403.07623",
      "title": "Empowering Sequential Recommendation from Collaborative Signals and Semantic Relatedness",
      "summary": "Sequential recommender systems (SRS) could capture dynamic user preferences by modeling historical behaviors ordered in time. Despite effectiveness, focusing only on the \\textit{collaborative signals} from behaviors does not fully grasp user interests. It is also significant to model the \\textit{semantic relatedness} reflected in content features, e.g., images and text. Towards that end, in this paper, we aim to enhance the SRS tasks by effectively unifying collaborative signals and semantic relatedness together. Notably, we empirically point out that it is nontrivial to achieve such a goal due to semantic gap issues. Thus, we propose an end-to-end two-stream architecture for sequential recommendation, named TSSR, to learn user preferences from ID-based and content-based sequence. Specifically, we first present novel hierarchical contrasting module, including coarse user-grained and fine item-grained terms, to align the representations of inter-modality. Furthermore, we also design a two-stream architecture to learn the dependence of intra-modality sequence and the complex interactions of inter-modality sequence, which can yield more expressive capacity in understanding user interests. We conduct extensive experiments on five public datasets. The experimental results show that the TSSR could yield superior performance than competitive baselines. We also make our experimental codes publicly available at https://github.com/Mingyue-Cheng/TSSR.",
      "authors": [
        "Mingyue Cheng",
        "Hao Zhang",
        "Qi Liu",
        "Fajie Yuan",
        "Zhi Li",
        "Zhenya Huang",
        "Enhong Chen",
        "Jun Zhou",
        "Longfei Li"
      ],
      "published": "2024-03-12T13:06:31Z",
      "updated": "2024-09-19T07:43:43Z",
      "categories": [
        "cs.IR"
      ],
      "pdf_url": "https://arxiv.org/pdf/2403.07623v2",
      "landing_url": "https://arxiv.org/abs/2403.07623v2",
      "doi": "https://doi.org/10.48550/arXiv.2403.07623"
    },
    "queries": [
      {
        "anchor": "synchronous dialogue",
        "search_term": "streaming architecture"
      }
    ]
  },
  {
    "arxiv_id": "2403.09055",
    "anchor": "synchronous dialogue",
    "search_term": "real-time interaction",
    "search_record": {
      "id": "http://arxiv.org/abs/2403.09055v4",
      "title": "SemanticDraw: Towards Real-Time Interactive Content Creation from Image Diffusion Models",
      "summary": "We introduce SemanticDraw, a new paradigm of interactive content creation where high-quality images are generated in near real-time from given multiple hand-drawn regions, each encoding prescribed semantic meaning. In order to maximize the productivity of content creators and to fully realize their artistic imagination, it requires both quick interactive interfaces and fine-grained regional controls in their tools. Despite astonishing generation quality from recent diffusion models, we find that existing approaches for regional controllability are very slow (52 seconds for $512 \\times 512$ image) while not compatible with acceleration methods such as LCM, blocking their huge potential in interactive content creation. From this observation, we build our solution for interactive content creation in two steps: (1) we establish compatibility between region-based controls and acceleration techniques for diffusion models, maintaining high fidelity of multi-prompt image generation with $\\times 10$ reduced number of inference steps, (2) we increase the generation throughput with our new multi-prompt stream batch pipeline, enabling low-latency generation from multiple, region-based text prompts on a single RTX 2080 Ti GPU. Our proposed framework is generalizable to any existing diffusion models and acceleration schedulers, allowing sub-second (0.64 seconds) image content creation application upon well-established image diffusion models. Our project page is: https://jaerinlee.com/research/semantic-draw",
      "published": "2024-03-14T02:51:01Z"
    },
    "metadata": {
      "arxiv_id": "2403.09055",
      "title": "SemanticDraw: Towards Real-Time Interactive Content Creation from Image Diffusion Models",
      "summary": "We introduce SemanticDraw, a new paradigm of interactive content creation where high-quality images are generated in near real-time from given multiple hand-drawn regions, each encoding prescribed semantic meaning. In order to maximize the productivity of content creators and to fully realize their artistic imagination, it requires both quick interactive interfaces and fine-grained regional controls in their tools. Despite astonishing generation quality from recent diffusion models, we find that existing approaches for regional controllability are very slow (52 seconds for $512 \\times 512$ image) while not compatible with acceleration methods such as LCM, blocking their huge potential in interactive content creation. From this observation, we build our solution for interactive content creation in two steps: (1) we establish compatibility between region-based controls and acceleration techniques for diffusion models, maintaining high fidelity of multi-prompt image generation with $\\times 10$ reduced number of inference steps, (2) we increase the generation throughput with our new multi-prompt stream batch pipeline, enabling low-latency generation from multiple, region-based text prompts on a single RTX 2080 Ti GPU. Our proposed framework is generalizable to any existing diffusion models and acceleration schedulers, allowing sub-second (0.64 seconds) image content creation application upon well-established image diffusion models. Our project page is: https://jaerinlee.com/research/semantic-draw",
      "authors": [
        "Jaerin Lee",
        "Daniel Sungho Jung",
        "Kanggeon Lee",
        "Kyoung Mu Lee"
      ],
      "published": "2024-03-14T02:51:01Z",
      "updated": "2025-06-01T14:29:54Z",
      "categories": [
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2403.09055v4",
      "landing_url": "https://arxiv.org/abs/2403.09055v4",
      "doi": "https://doi.org/10.48550/arXiv.2403.09055"
    },
    "queries": [
      {
        "anchor": "synchronous dialogue",
        "search_term": "real-time interaction"
      }
    ]
  },
  {
    "arxiv_id": "2403.11495",
    "anchor": "spoken language models",
    "search_term": "temporal dynamics",
    "search_record": {
      "id": "http://arxiv.org/abs/2403.11495v1",
      "title": "Semantic-Enhanced Representation Learning for Road Networks with Temporal Dynamics",
      "summary": "In this study, we introduce a novel framework called Toast for learning general-purpose representations of road networks, along with its advanced counterpart DyToast, designed to enhance the integration of temporal dynamics to boost the performance of various time-sensitive downstream tasks. Specifically, we propose to encode two pivotal semantic characteristics intrinsic to road networks: traffic patterns and traveling semantics. To achieve this, we refine the skip-gram module by incorporating auxiliary objectives aimed at predicting the traffic context associated with a target road segment. Moreover, we leverage trajectory data and design pre-training strategies based on Transformer to distill traveling semantics on road networks. DyToast further augments this framework by employing unified trigonometric functions characterized by their beneficial properties, enabling the capture of temporal evolution and dynamic nature of road networks more effectively. With these proposed techniques, we can obtain representations that encode multi-faceted aspects of knowledge within road networks, applicable across both road segment-based applications and trajectory-based applications. Extensive experiments on two real-world datasets across three tasks demonstrate that our proposed framework consistently outperforms the state-of-the-art baselines by a significant margin.",
      "published": "2024-03-18T05:59:56Z"
    },
    "metadata": {
      "arxiv_id": "2403.11495",
      "title": "Semantic-Enhanced Representation Learning for Road Networks with Temporal Dynamics",
      "summary": "In this study, we introduce a novel framework called Toast for learning general-purpose representations of road networks, along with its advanced counterpart DyToast, designed to enhance the integration of temporal dynamics to boost the performance of various time-sensitive downstream tasks. Specifically, we propose to encode two pivotal semantic characteristics intrinsic to road networks: traffic patterns and traveling semantics. To achieve this, we refine the skip-gram module by incorporating auxiliary objectives aimed at predicting the traffic context associated with a target road segment. Moreover, we leverage trajectory data and design pre-training strategies based on Transformer to distill traveling semantics on road networks. DyToast further augments this framework by employing unified trigonometric functions characterized by their beneficial properties, enabling the capture of temporal evolution and dynamic nature of road networks more effectively. With these proposed techniques, we can obtain representations that encode multi-faceted aspects of knowledge within road networks, applicable across both road segment-based applications and trajectory-based applications. Extensive experiments on two real-world datasets across three tasks demonstrate that our proposed framework consistently outperforms the state-of-the-art baselines by a significant margin.",
      "authors": [
        "Yile Chen",
        "Xiucheng Li",
        "Gao Cong",
        "Zhifeng Bao",
        "Cheng Long"
      ],
      "published": "2024-03-18T05:59:56Z",
      "updated": "2024-03-18T05:59:56Z",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2403.11495v1",
      "landing_url": "https://arxiv.org/abs/2403.11495v1",
      "doi": "https://doi.org/10.48550/arXiv.2403.11495"
    },
    "queries": [
      {
        "anchor": "spoken language models",
        "search_term": "temporal dynamics"
      },
      {
        "anchor": "synchronous dialogue",
        "search_term": "temporal dynamics"
      }
    ]
  },
  {
    "arxiv_id": "2403.15615",
    "anchor": "turn-taking",
    "search_term": "true full-duplex",
    "search_record": {
      "id": "http://arxiv.org/abs/2403.15615v4",
      "title": "NaturalTurn: A Method to Segment Speech into Psychologically Meaningful Conversational Turns",
      "summary": "Conversation is a subject of increasing interest in the social, cognitive, and computational sciences. Yet as conversational datasets continue to increase in size and complexity, researchers lack scalable methods to segment speech-to-text transcripts into conversational \"turns\"-the basic building blocks of social interaction. We discuss this challenge and then introduce \"NaturalTurn,\" a turn-segmentation algorithm designed to accurately capture the dynamics of conversational exchange. NaturalTurn operates by distinguishing speakers' primary conversational turns from listeners' secondary utterances, such as backchannels, brief interjections, and other forms of parallel speech that characterize human conversation. Using data from a large conversation corpus, we show that NaturalTurn captures conversational turns more accurately than a baseline model. For example, it produces turns with durations and gaps that match empirical literature, reveals stronger linguistic alignment patterns between speakers, and uncovers otherwise hidden relationships between turn-taking and affective outcomes. NaturalTurn thus represents a pragmatic development in machine-generated transcript-processing methods, or \"turn models\", that will enable researchers to link turn-taking dynamics with important outcomes of social interaction, a central goal of conversation science.",
      "published": "2024-03-22T21:05:54Z"
    },
    "metadata": {
      "arxiv_id": "2403.15615",
      "title": "NaturalTurn: A Method to Segment Speech into Psychologically Meaningful Conversational Turns",
      "summary": "Conversation is a subject of increasing interest in the social, cognitive, and computational sciences. Yet as conversational datasets continue to increase in size and complexity, researchers lack scalable methods to segment speech-to-text transcripts into conversational \"turns\"-the basic building blocks of social interaction. We discuss this challenge and then introduce \"NaturalTurn,\" a turn-segmentation algorithm designed to accurately capture the dynamics of conversational exchange. NaturalTurn operates by distinguishing speakers' primary conversational turns from listeners' secondary utterances, such as backchannels, brief interjections, and other forms of parallel speech that characterize human conversation. Using data from a large conversation corpus, we show that NaturalTurn captures conversational turns more accurately than a baseline model. For example, it produces turns with durations and gaps that match empirical literature, reveals stronger linguistic alignment patterns between speakers, and uncovers otherwise hidden relationships between turn-taking and affective outcomes. NaturalTurn thus represents a pragmatic development in machine-generated transcript-processing methods, or \"turn models\", that will enable researchers to link turn-taking dynamics with important outcomes of social interaction, a central goal of conversation science.",
      "authors": [
        "Gus Cooney",
        "Andrew Reece"
      ],
      "published": "2024-03-22T21:05:54Z",
      "updated": "2025-11-12T14:27:00Z",
      "categories": [
        "cs.CL"
      ],
      "pdf_url": "https://arxiv.org/pdf/2403.15615v4",
      "landing_url": "https://arxiv.org/abs/2403.15615v4",
      "doi": "https://doi.org/10.1038/s41598-025-24381-1"
    },
    "queries": [
      {
        "anchor": "turn-taking",
        "search_term": "true full-duplex"
      },
      {
        "anchor": "turn-taking",
        "search_term": "pseudo full-duplex"
      },
      {
        "anchor": "turn-taking",
        "search_term": "engineered synchronization"
      },
      {
        "anchor": "turn-taking",
        "search_term": "learned synchronization"
      },
      {
        "anchor": "turn-taking",
        "search_term": "modular architecture"
      },
      {
        "anchor": "turn-taking",
        "search_term": "streaming architecture"
      },
      {
        "anchor": "turn-taking",
        "search_term": "real-time interaction"
      },
      {
        "anchor": "turn-taking",
        "search_term": "synchronous data"
      },
      {
        "anchor": "turn-taking",
        "search_term": "multichannel dialogue"
      },
      {
        "anchor": "turn-taking",
        "search_term": "behavioral arbitration"
      },
      {
        "anchor": "turn-taking",
        "search_term": "semantic coherence"
      },
      {
        "anchor": "turn-taking",
        "search_term": "floor transfer offset"
      },
      {
        "anchor": "turn-taking",
        "search_term": "stop latency"
      },
      {
        "anchor": "turn-taking",
        "search_term": "interruption response delay"
      }
    ]
  },
  {
    "arxiv_id": "2403.16258",
    "anchor": "spoken language models",
    "search_term": "neural codec",
    "search_record": {
      "id": "http://arxiv.org/abs/2403.16258v1",
      "title": "Laplacian-guided Entropy Model in Neural Codec with Blur-dissipated Synthesis",
      "summary": "While replacing Gaussian decoders with a conditional diffusion model enhances the perceptual quality of reconstructions in neural image compression, their lack of inductive bias for image data restricts their ability to achieve state-of-the-art perceptual levels. To address this limitation, we adopt a non-isotropic diffusion model at the decoder side. This model imposes an inductive bias aimed at distinguishing between frequency contents, thereby facilitating the generation of high-quality images. Moreover, our framework is equipped with a novel entropy model that accurately models the probability distribution of latent representation by exploiting spatio-channel correlations in latent space, while accelerating the entropy decoding step. This channel-wise entropy model leverages both local and global spatial contexts within each channel chunk. The global spatial context is built upon the Transformer, which is specifically designed for image compression tasks. The designed Transformer employs a Laplacian-shaped positional encoding, the learnable parameters of which are adaptively adjusted for each channel cluster. Our experiments demonstrate that our proposed framework yields better perceptual quality compared to cutting-edge generative-based codecs, and the proposed entropy model contributes to notable bitrate savings.",
      "published": "2024-03-24T18:33:16Z"
    },
    "metadata": {
      "arxiv_id": "2403.16258",
      "title": "Laplacian-guided Entropy Model in Neural Codec with Blur-dissipated Synthesis",
      "summary": "While replacing Gaussian decoders with a conditional diffusion model enhances the perceptual quality of reconstructions in neural image compression, their lack of inductive bias for image data restricts their ability to achieve state-of-the-art perceptual levels. To address this limitation, we adopt a non-isotropic diffusion model at the decoder side. This model imposes an inductive bias aimed at distinguishing between frequency contents, thereby facilitating the generation of high-quality images. Moreover, our framework is equipped with a novel entropy model that accurately models the probability distribution of latent representation by exploiting spatio-channel correlations in latent space, while accelerating the entropy decoding step. This channel-wise entropy model leverages both local and global spatial contexts within each channel chunk. The global spatial context is built upon the Transformer, which is specifically designed for image compression tasks. The designed Transformer employs a Laplacian-shaped positional encoding, the learnable parameters of which are adaptively adjusted for each channel cluster. Our experiments demonstrate that our proposed framework yields better perceptual quality compared to cutting-edge generative-based codecs, and the proposed entropy model contributes to notable bitrate savings.",
      "authors": [
        "Atefeh Khoshkhahtinat",
        "Ali Zafari",
        "Piyush M. Mehta",
        "Nasser M. Nasrabadi"
      ],
      "published": "2024-03-24T18:33:16Z",
      "updated": "2024-03-24T18:33:16Z",
      "categories": [
        "eess.IV",
        "cs.CV",
        "cs.IT",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2403.16258v1",
      "landing_url": "https://arxiv.org/abs/2403.16258v1",
      "doi": "https://doi.org/10.48550/arXiv.2403.16258"
    },
    "queries": [
      {
        "anchor": "spoken language models",
        "search_term": "neural codec"
      },
      {
        "anchor": "synchronous dialogue",
        "search_term": "neural codec"
      },
      {
        "anchor": "turn-taking",
        "search_term": "neural codec"
      }
    ]
  },
  {
    "arxiv_id": "2403.16865",
    "anchor": "spoken language models",
    "search_term": "true full-duplex",
    "search_record": {
      "id": "http://arxiv.org/abs/2403.16865v2",
      "title": "Encoding of lexical tone in self-supervised models of spoken language",
      "summary": "Interpretability research has shown that self-supervised Spoken Language Models (SLMs) encode a wide variety of features in human speech from the acoustic, phonetic, phonological, syntactic and semantic levels, to speaker characteristics. The bulk of prior research on representations of phonology has focused on segmental features such as phonemes; the encoding of suprasegmental phonology (such as tone and stress patterns) in SLMs is not yet well understood. Tone is a suprasegmental feature that is present in more than half of the world's languages. This paper aims to analyze the tone encoding capabilities of SLMs, using Mandarin and Vietnamese as case studies. We show that SLMs encode lexical tone to a significant degree even when they are trained on data from non-tonal languages. We further find that SLMs behave similarly to native and non-native human participants in tone and consonant perception studies, but they do not follow the same developmental trajectory.",
      "published": "2024-03-25T15:28:38Z"
    },
    "metadata": {
      "arxiv_id": "2403.16865",
      "title": "Encoding of lexical tone in self-supervised models of spoken language",
      "summary": "Interpretability research has shown that self-supervised Spoken Language Models (SLMs) encode a wide variety of features in human speech from the acoustic, phonetic, phonological, syntactic and semantic levels, to speaker characteristics. The bulk of prior research on representations of phonology has focused on segmental features such as phonemes; the encoding of suprasegmental phonology (such as tone and stress patterns) in SLMs is not yet well understood. Tone is a suprasegmental feature that is present in more than half of the world's languages. This paper aims to analyze the tone encoding capabilities of SLMs, using Mandarin and Vietnamese as case studies. We show that SLMs encode lexical tone to a significant degree even when they are trained on data from non-tonal languages. We further find that SLMs behave similarly to native and non-native human participants in tone and consonant perception studies, but they do not follow the same developmental trajectory.",
      "authors": [
        "Gaofei Shen",
        "Michaela Watkins",
        "Afra Alishahi",
        "Arianna Bisazza",
        "Grzegorz Chrupała"
      ],
      "published": "2024-03-25T15:28:38Z",
      "updated": "2024-04-03T12:59:20Z",
      "categories": [
        "cs.CL",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2403.16865v2",
      "landing_url": "https://arxiv.org/abs/2403.16865v2",
      "doi": "https://doi.org/10.48550/arXiv.2403.16865"
    },
    "queries": [
      {
        "anchor": "spoken language models",
        "search_term": "true full-duplex"
      },
      {
        "anchor": "spoken language models",
        "search_term": "pseudo full-duplex"
      },
      {
        "anchor": "spoken language models",
        "search_term": "engineered synchronization"
      },
      {
        "anchor": "spoken language models",
        "search_term": "learned synchronization"
      },
      {
        "anchor": "spoken language models",
        "search_term": "modular architecture"
      },
      {
        "anchor": "spoken language models",
        "search_term": "streaming architecture"
      },
      {
        "anchor": "spoken language models",
        "search_term": "real-time interaction"
      },
      {
        "anchor": "spoken language models",
        "search_term": "synchronous data"
      },
      {
        "anchor": "spoken language models",
        "search_term": "data scarcity"
      },
      {
        "anchor": "spoken language models",
        "search_term": "multichannel dialogue"
      },
      {
        "anchor": "spoken language models",
        "search_term": "behavioral arbitration"
      },
      {
        "anchor": "spoken language models",
        "search_term": "semantic coherence"
      },
      {
        "anchor": "spoken language models",
        "search_term": "neural codec"
      },
      {
        "anchor": "spoken language models",
        "search_term": "discrete tokens"
      },
      {
        "anchor": "spoken language models",
        "search_term": "audio tokens"
      },
      {
        "anchor": "spoken language models",
        "search_term": "floor transfer offset"
      },
      {
        "anchor": "spoken language models",
        "search_term": "stop latency"
      },
      {
        "anchor": "spoken language models",
        "search_term": "interruption response delay"
      }
    ]
  },
  {
    "arxiv_id": "2403.16973",
    "anchor": "synchronous dialogue",
    "search_term": "neural codec",
    "search_record": {
      "id": "http://arxiv.org/abs/2403.16973v3",
      "title": "VoiceCraft: Zero-Shot Speech Editing and Text-to-Speech in the Wild",
      "summary": "We introduce VoiceCraft, a token infilling neural codec language model, that achieves state-of-the-art performance on both speech editing and zero-shot text-to-speech (TTS) on audiobooks, internet videos, and podcasts. VoiceCraft employs a Transformer decoder architecture and introduces a token rearrangement procedure that combines causal masking and delayed stacking to enable generation within an existing sequence. On speech editing tasks, VoiceCraft produces edited speech that is nearly indistinguishable from unedited recordings in terms of naturalness, as evaluated by humans; for zero-shot TTS, our model outperforms prior SotA models including VALLE and the popular commercial model XTTS-v2. Crucially, the models are evaluated on challenging and realistic datasets, that consist of diverse accents, speaking styles, recording conditions, and background noise and music, and our model performs consistently well compared to other models and real recordings. In particular, for speech editing evaluation, we introduce a high quality, challenging, and realistic dataset named RealEdit. We encourage readers to listen to the demos at https://jasonppy.github.io/VoiceCraft_web.",
      "published": "2024-03-25T17:38:32Z"
    },
    "metadata": {
      "arxiv_id": "2403.16973",
      "title": "VoiceCraft: Zero-Shot Speech Editing and Text-to-Speech in the Wild",
      "summary": "We introduce VoiceCraft, a token infilling neural codec language model, that achieves state-of-the-art performance on both speech editing and zero-shot text-to-speech (TTS) on audiobooks, internet videos, and podcasts. VoiceCraft employs a Transformer decoder architecture and introduces a token rearrangement procedure that combines causal masking and delayed stacking to enable generation within an existing sequence. On speech editing tasks, VoiceCraft produces edited speech that is nearly indistinguishable from unedited recordings in terms of naturalness, as evaluated by humans; for zero-shot TTS, our model outperforms prior SotA models including VALLE and the popular commercial model XTTS-v2. Crucially, the models are evaluated on challenging and realistic datasets, that consist of diverse accents, speaking styles, recording conditions, and background noise and music, and our model performs consistently well compared to other models and real recordings. In particular, for speech editing evaluation, we introduce a high quality, challenging, and realistic dataset named RealEdit. We encourage readers to listen to the demos at https://jasonppy.github.io/VoiceCraft_web.",
      "authors": [
        "Puyuan Peng",
        "Po-Yao Huang",
        "Shang-Wen Li",
        "Abdelrahman Mohamed",
        "David Harwath"
      ],
      "published": "2024-03-25T17:38:32Z",
      "updated": "2024-06-14T00:29:46Z",
      "categories": [
        "eess.AS",
        "cs.AI",
        "cs.CL",
        "cs.LG",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2403.16973v3",
      "landing_url": "https://arxiv.org/abs/2403.16973v3",
      "doi": "https://doi.org/10.48550/arXiv.2403.16973"
    },
    "queries": [
      {
        "anchor": "synchronous dialogue",
        "search_term": "neural codec"
      }
    ]
  },
  {
    "arxiv_id": "2403.17879",
    "anchor": "synchronous dialogue",
    "search_term": "low latency",
    "search_record": {
      "id": "http://arxiv.org/abs/2403.17879v1",
      "title": "Low-Latency Neural Stereo Streaming",
      "summary": "The rise of new video modalities like virtual reality or autonomous driving has increased the demand for efficient multi-view video compression methods, both in terms of rate-distortion (R-D) performance and in terms of delay and runtime. While most recent stereo video compression approaches have shown promising performance, they compress left and right views sequentially, leading to poor parallelization and runtime performance. This work presents Low-Latency neural codec for Stereo video Streaming (LLSS), a novel parallel stereo video coding method designed for fast and efficient low-latency stereo video streaming. Instead of using a sequential cross-view motion compensation like existing methods, LLSS introduces a bidirectional feature shifting module to directly exploit mutual information among views and encode them effectively with a joint cross-view prior model for entropy coding. Thanks to this design, LLSS processes left and right views in parallel, minimizing latency; all while substantially improving R-D performance compared to both existing neural and conventional codecs.",
      "published": "2024-03-26T17:11:51Z"
    },
    "metadata": {
      "arxiv_id": "2403.17879",
      "title": "Low-Latency Neural Stereo Streaming",
      "summary": "The rise of new video modalities like virtual reality or autonomous driving has increased the demand for efficient multi-view video compression methods, both in terms of rate-distortion (R-D) performance and in terms of delay and runtime. While most recent stereo video compression approaches have shown promising performance, they compress left and right views sequentially, leading to poor parallelization and runtime performance. This work presents Low-Latency neural codec for Stereo video Streaming (LLSS), a novel parallel stereo video coding method designed for fast and efficient low-latency stereo video streaming. Instead of using a sequential cross-view motion compensation like existing methods, LLSS introduces a bidirectional feature shifting module to directly exploit mutual information among views and encode them effectively with a joint cross-view prior model for entropy coding. Thanks to this design, LLSS processes left and right views in parallel, minimizing latency; all while substantially improving R-D performance compared to both existing neural and conventional codecs.",
      "authors": [
        "Qiqi Hou",
        "Farzad Farhadzadeh",
        "Amir Said",
        "Guillaume Sautiere",
        "Hoang Le"
      ],
      "published": "2024-03-26T17:11:51Z",
      "updated": "2024-03-26T17:11:51Z",
      "categories": [
        "cs.CV",
        "eess.IV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2403.17879v1",
      "landing_url": "https://arxiv.org/abs/2403.17879v1",
      "doi": "https://doi.org/10.48550/arXiv.2403.17879"
    },
    "queries": [
      {
        "anchor": "synchronous dialogue",
        "search_term": "low latency"
      },
      {
        "anchor": "synchronous dialogue",
        "search_term": "neural codec"
      }
    ]
  },
  {
    "arxiv_id": "2403.18921",
    "anchor": "spoken language models",
    "search_term": "streaming architecture",
    "search_record": {
      "id": "http://arxiv.org/abs/2403.18921v1",
      "title": "SMOF: Streaming Modern CNNs on FPGAs with Smart Off-Chip Eviction",
      "summary": "Convolutional Neural Networks (CNNs) have demonstrated their effectiveness in numerous vision tasks. However, their high processing requirements necessitate efficient hardware acceleration to meet the application's performance targets. In the space of FPGAs, streaming-based dataflow architectures are often adopted by users, as significant performance gains can be achieved through layer-wise pipelining and reduced off-chip memory access by retaining data on-chip. However, modern topologies, such as the UNet, YOLO, and X3D models, utilise long skip connections, requiring significant on-chip storage and thus limiting the performance achieved by such system architectures. The paper addresses the above limitation by introducing weight and activation eviction mechanisms to off-chip memory along the computational pipeline, taking into account the available compute and memory resources. The proposed mechanism is incorporated into an existing toolflow, expanding the design space by utilising off-chip memory as a buffer. This enables the mapping of such modern CNNs to devices with limited on-chip memory, under the streaming architecture design approach. SMOF has demonstrated the capacity to deliver competitive and, in some cases, state-of-the-art performance across a spectrum of computer vision tasks, achieving up to 10.65 X throughput improvement compared to previous works.",
      "published": "2024-03-27T18:12:24Z"
    },
    "metadata": {
      "arxiv_id": "2403.18921",
      "title": "SMOF: Streaming Modern CNNs on FPGAs with Smart Off-Chip Eviction",
      "summary": "Convolutional Neural Networks (CNNs) have demonstrated their effectiveness in numerous vision tasks. However, their high processing requirements necessitate efficient hardware acceleration to meet the application's performance targets. In the space of FPGAs, streaming-based dataflow architectures are often adopted by users, as significant performance gains can be achieved through layer-wise pipelining and reduced off-chip memory access by retaining data on-chip. However, modern topologies, such as the UNet, YOLO, and X3D models, utilise long skip connections, requiring significant on-chip storage and thus limiting the performance achieved by such system architectures. The paper addresses the above limitation by introducing weight and activation eviction mechanisms to off-chip memory along the computational pipeline, taking into account the available compute and memory resources. The proposed mechanism is incorporated into an existing toolflow, expanding the design space by utilising off-chip memory as a buffer. This enables the mapping of such modern CNNs to devices with limited on-chip memory, under the streaming architecture design approach. SMOF has demonstrated the capacity to deliver competitive and, in some cases, state-of-the-art performance across a spectrum of computer vision tasks, achieving up to 10.65 X throughput improvement compared to previous works.",
      "authors": [
        "Petros Toupas",
        "Zhewen Yu",
        "Christos-Savvas Bouganis",
        "Dimitrios Tzovaras"
      ],
      "published": "2024-03-27T18:12:24Z",
      "updated": "2024-03-27T18:12:24Z",
      "categories": [
        "cs.AR",
        "cs.CV",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2403.18921v1",
      "landing_url": "https://arxiv.org/abs/2403.18921v1",
      "doi": "https://doi.org/10.48550/arXiv.2403.18921"
    },
    "queries": [
      {
        "anchor": "spoken language models",
        "search_term": "streaming architecture"
      },
      {
        "anchor": "synchronous dialogue",
        "search_term": "streaming architecture"
      },
      {
        "anchor": "turn-taking",
        "search_term": "streaming architecture"
      }
    ]
  },
  {
    "arxiv_id": "2404.00741",
    "anchor": "synchronous dialogue",
    "search_term": "low latency",
    "search_record": {
      "id": "http://arxiv.org/abs/2404.00741v1",
      "title": "Rethinking Interactive Image Segmentation with Low Latency, High Quality, and Diverse Prompts",
      "summary": "The goal of interactive image segmentation is to delineate specific regions within an image via visual or language prompts. Low-latency and high-quality interactive segmentation with diverse prompts remain challenging for existing specialist and generalist models. Specialist models, with their limited prompts and task-specific designs, experience high latency because the image must be recomputed every time the prompt is updated, due to the joint encoding of image and visual prompts. Generalist models, exemplified by the Segment Anything Model (SAM), have recently excelled in prompt diversity and efficiency, lifting image segmentation to the foundation model era. However, for high-quality segmentations, SAM still lags behind state-of-the-art specialist models despite SAM being trained with x100 more segmentation masks. In this work, we delve deep into the architectural differences between the two types of models. We observe that dense representation and fusion of visual prompts are the key design choices contributing to the high segmentation quality of specialist models. In light of this, we reintroduce this dense design into the generalist models, to facilitate the development of generalist models with high segmentation quality. To densely represent diverse visual prompts, we propose to use a dense map to capture five types: clicks, boxes, polygons, scribbles, and masks. Thus, we propose SegNext, a next-generation interactive segmentation approach offering low latency, high quality, and diverse prompt support. Our method outperforms current state-of-the-art methods on HQSeg-44K and DAVIS, both quantitatively and qualitatively.",
      "published": "2024-03-31T17:02:24Z"
    },
    "metadata": {
      "arxiv_id": "2404.00741",
      "title": "Rethinking Interactive Image Segmentation with Low Latency, High Quality, and Diverse Prompts",
      "summary": "The goal of interactive image segmentation is to delineate specific regions within an image via visual or language prompts. Low-latency and high-quality interactive segmentation with diverse prompts remain challenging for existing specialist and generalist models. Specialist models, with their limited prompts and task-specific designs, experience high latency because the image must be recomputed every time the prompt is updated, due to the joint encoding of image and visual prompts. Generalist models, exemplified by the Segment Anything Model (SAM), have recently excelled in prompt diversity and efficiency, lifting image segmentation to the foundation model era. However, for high-quality segmentations, SAM still lags behind state-of-the-art specialist models despite SAM being trained with x100 more segmentation masks. In this work, we delve deep into the architectural differences between the two types of models. We observe that dense representation and fusion of visual prompts are the key design choices contributing to the high segmentation quality of specialist models. In light of this, we reintroduce this dense design into the generalist models, to facilitate the development of generalist models with high segmentation quality. To densely represent diverse visual prompts, we propose to use a dense map to capture five types: clicks, boxes, polygons, scribbles, and masks. Thus, we propose SegNext, a next-generation interactive segmentation approach offering low latency, high quality, and diverse prompt support. Our method outperforms current state-of-the-art methods on HQSeg-44K and DAVIS, both quantitatively and qualitatively.",
      "authors": [
        "Qin Liu",
        "Jaemin Cho",
        "Mohit Bansal",
        "Marc Niethammer"
      ],
      "published": "2024-03-31T17:02:24Z",
      "updated": "2024-03-31T17:02:24Z",
      "categories": [
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2404.00741v1",
      "landing_url": "https://arxiv.org/abs/2404.00741v1",
      "doi": "https://doi.org/10.48550/arXiv.2404.00741"
    },
    "queries": [
      {
        "anchor": "synchronous dialogue",
        "search_term": "low latency"
      }
    ]
  },
  {
    "arxiv_id": "2404.01686",
    "anchor": "synchronous dialogue",
    "search_term": "synchronous data",
    "search_record": {
      "id": "http://arxiv.org/abs/2404.01686v1",
      "title": "JRDB-PanoTrack: An Open-world Panoptic Segmentation and Tracking Robotic Dataset in Crowded Human Environments",
      "summary": "Autonomous robot systems have attracted increasing research attention in recent years, where environment understanding is a crucial step for robot navigation, human-robot interaction, and decision. Real-world robot systems usually collect visual data from multiple sensors and are required to recognize numerous objects and their movements in complex human-crowded settings. Traditional benchmarks, with their reliance on single sensors and limited object classes and scenarios, fail to provide the comprehensive environmental understanding robots need for accurate navigation, interaction, and decision-making. As an extension of JRDB dataset, we unveil JRDB-PanoTrack, a novel open-world panoptic segmentation and tracking benchmark, towards more comprehensive environmental perception. JRDB-PanoTrack includes (1) various data involving indoor and outdoor crowded scenes, as well as comprehensive 2D and 3D synchronized data modalities; (2) high-quality 2D spatial panoptic segmentation and temporal tracking annotations, with additional 3D label projections for further spatial understanding; (3) diverse object classes for closed- and open-world recognition benchmarks, with OSPA-based metrics for evaluation. Extensive evaluation of leading methods shows significant challenges posed by our dataset.",
      "published": "2024-04-02T06:43:22Z"
    },
    "metadata": {
      "arxiv_id": "2404.01686",
      "title": "JRDB-PanoTrack: An Open-world Panoptic Segmentation and Tracking Robotic Dataset in Crowded Human Environments",
      "summary": "Autonomous robot systems have attracted increasing research attention in recent years, where environment understanding is a crucial step for robot navigation, human-robot interaction, and decision. Real-world robot systems usually collect visual data from multiple sensors and are required to recognize numerous objects and their movements in complex human-crowded settings. Traditional benchmarks, with their reliance on single sensors and limited object classes and scenarios, fail to provide the comprehensive environmental understanding robots need for accurate navigation, interaction, and decision-making. As an extension of JRDB dataset, we unveil JRDB-PanoTrack, a novel open-world panoptic segmentation and tracking benchmark, towards more comprehensive environmental perception. JRDB-PanoTrack includes (1) various data involving indoor and outdoor crowded scenes, as well as comprehensive 2D and 3D synchronized data modalities; (2) high-quality 2D spatial panoptic segmentation and temporal tracking annotations, with additional 3D label projections for further spatial understanding; (3) diverse object classes for closed- and open-world recognition benchmarks, with OSPA-based metrics for evaluation. Extensive evaluation of leading methods shows significant challenges posed by our dataset.",
      "authors": [
        "Duy-Tho Le",
        "Chenhui Gou",
        "Stavya Datta",
        "Hengcan Shi",
        "Ian Reid",
        "Jianfei Cai",
        "Hamid Rezatofighi"
      ],
      "published": "2024-04-02T06:43:22Z",
      "updated": "2024-04-02T06:43:22Z",
      "categories": [
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2404.01686v1",
      "landing_url": "https://arxiv.org/abs/2404.01686v1",
      "doi": "https://doi.org/10.48550/arXiv.2404.01686"
    },
    "queries": [
      {
        "anchor": "synchronous dialogue",
        "search_term": "synchronous data"
      }
    ]
  },
  {
    "arxiv_id": "2404.02781",
    "anchor": "spoken language models",
    "search_term": "neural codec",
    "search_record": {
      "id": "http://arxiv.org/abs/2404.02781v1",
      "title": "CLaM-TTS: Improving Neural Codec Language Model for Zero-Shot Text-to-Speech",
      "summary": "With the emergence of neural audio codecs, which encode multiple streams of discrete tokens from audio, large language models have recently gained attention as a promising approach for zero-shot Text-to-Speech (TTS) synthesis. Despite the ongoing rush towards scaling paradigms, audio tokenization ironically amplifies the scalability challenge, stemming from its long sequence length and the complexity of modelling the multiple sequences. To mitigate these issues, we present CLaM-TTS that employs a probabilistic residual vector quantization to (1) achieve superior compression in the token length, and (2) allow a language model to generate multiple tokens at once, thereby eliminating the need for cascaded modeling to handle the number of token streams. Our experimental results demonstrate that CLaM-TTS is better than or comparable to state-of-the-art neural codec-based TTS models regarding naturalness, intelligibility, speaker similarity, and inference speed. In addition, we examine the impact of the pretraining extent of the language models and their text tokenization strategies on performances.",
      "published": "2024-04-03T14:52:20Z"
    },
    "metadata": {
      "arxiv_id": "2404.02781",
      "title": "CLaM-TTS: Improving Neural Codec Language Model for Zero-Shot Text-to-Speech",
      "summary": "With the emergence of neural audio codecs, which encode multiple streams of discrete tokens from audio, large language models have recently gained attention as a promising approach for zero-shot Text-to-Speech (TTS) synthesis. Despite the ongoing rush towards scaling paradigms, audio tokenization ironically amplifies the scalability challenge, stemming from its long sequence length and the complexity of modelling the multiple sequences. To mitigate these issues, we present CLaM-TTS that employs a probabilistic residual vector quantization to (1) achieve superior compression in the token length, and (2) allow a language model to generate multiple tokens at once, thereby eliminating the need for cascaded modeling to handle the number of token streams. Our experimental results demonstrate that CLaM-TTS is better than or comparable to state-of-the-art neural codec-based TTS models regarding naturalness, intelligibility, speaker similarity, and inference speed. In addition, we examine the impact of the pretraining extent of the language models and their text tokenization strategies on performances.",
      "authors": [
        "Jaehyeon Kim",
        "Keon Lee",
        "Seungjun Chung",
        "Jaewoong Cho"
      ],
      "published": "2024-04-03T14:52:20Z",
      "updated": "2024-04-03T14:52:20Z",
      "categories": [
        "eess.AS",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2404.02781v1",
      "landing_url": "https://arxiv.org/abs/2404.02781v1",
      "doi": "https://doi.org/10.48550/arXiv.2404.02781"
    },
    "queries": [
      {
        "anchor": "spoken language models",
        "search_term": "neural codec"
      },
      {
        "anchor": "synchronous dialogue",
        "search_term": "neural codec"
      },
      {
        "anchor": "synchronous dialogue",
        "search_term": "audio tokens"
      },
      {
        "anchor": "turn-taking",
        "search_term": "neural codec"
      }
    ]
  },
  {
    "arxiv_id": "2404.04904",
    "anchor": "synchronous dialogue",
    "search_term": "neural codec",
    "search_record": {
      "id": "http://arxiv.org/abs/2404.04904v2",
      "title": "Cross-Domain Audio Deepfake Detection: Dataset and Analysis",
      "summary": "Audio deepfake detection (ADD) is essential for preventing the misuse of synthetic voices that may infringe on personal rights and privacy. Recent zero-shot text-to-speech (TTS) models pose higher risks as they can clone voices with a single utterance. However, the existing ADD datasets are outdated, leading to suboptimal generalization of detection models. In this paper, we construct a new cross-domain ADD dataset comprising over 300 hours of speech data that is generated by five advanced zero-shot TTS models. To simulate real-world scenarios, we employ diverse attack methods and audio prompts from different datasets. Experiments show that, through novel attack-augmented training, the Wav2Vec2-large and Whisper-medium models achieve equal error rates of 4.1\\% and 6.5\\% respectively. Additionally, we demonstrate our models' outstanding few-shot ADD ability by fine-tuning with just one minute of target-domain data. Nonetheless, neural codec compressors greatly affect the detection accuracy, necessitating further research.",
      "published": "2024-04-07T10:10:15Z"
    },
    "metadata": {
      "arxiv_id": "2404.04904",
      "title": "Cross-Domain Audio Deepfake Detection: Dataset and Analysis",
      "summary": "Audio deepfake detection (ADD) is essential for preventing the misuse of synthetic voices that may infringe on personal rights and privacy. Recent zero-shot text-to-speech (TTS) models pose higher risks as they can clone voices with a single utterance. However, the existing ADD datasets are outdated, leading to suboptimal generalization of detection models. In this paper, we construct a new cross-domain ADD dataset comprising over 300 hours of speech data that is generated by five advanced zero-shot TTS models. To simulate real-world scenarios, we employ diverse attack methods and audio prompts from different datasets. Experiments show that, through novel attack-augmented training, the Wav2Vec2-large and Whisper-medium models achieve equal error rates of 4.1\\% and 6.5\\% respectively. Additionally, we demonstrate our models' outstanding few-shot ADD ability by fine-tuning with just one minute of target-domain data. Nonetheless, neural codec compressors greatly affect the detection accuracy, necessitating further research.",
      "authors": [
        "Yuang Li",
        "Min Zhang",
        "Mengxin Ren",
        "Miaomiao Ma",
        "Daimeng Wei",
        "Hao Yang"
      ],
      "published": "2024-04-07T10:10:15Z",
      "updated": "2024-09-20T08:08:53Z",
      "categories": [
        "cs.SD",
        "cs.AI",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2404.04904v2",
      "landing_url": "https://arxiv.org/abs/2404.04904v2",
      "doi": "https://doi.org/10.48550/arXiv.2404.04904"
    },
    "queries": [
      {
        "anchor": "synchronous dialogue",
        "search_term": "neural codec"
      }
    ]
  },
  {
    "arxiv_id": "2404.04913",
    "anchor": "synchronous dialogue",
    "search_term": "neural codec",
    "search_record": {
      "id": "http://arxiv.org/abs/2404.04913v3",
      "title": "CodecNeRF: Toward Fast Encoding and Decoding, Compact, and High-quality Novel-view Synthesis",
      "summary": "Neural Radiance Fields (NeRF) have achieved huge success in effectively capturing and representing 3D objects and scenes. However, to establish a ubiquitous presence in everyday media formats, such as images and videos, we need to fulfill three key objectives: 1. fast encoding and decoding time, 2. compact model sizes, and 3. high-quality renderings. Despite recent advancements, a comprehensive algorithm that adequately addresses all objectives has yet to be fully realized. In this work, we present CodecNeRF, a neural codec for NeRF representations, consisting of an encoder and decoder architecture that can generate a NeRF representation in a single forward pass. Furthermore, inspired by the recent parameter-efficient finetuning approaches, we propose a finetuning method to efficiently adapt the generated NeRF representations to a new test instance, leading to high-quality image renderings and compact code sizes. The proposed CodecNeRF, a newly suggested encoding-decoding-finetuning pipeline for NeRF, achieved unprecedented compression performance of more than 100x and remarkable reduction in encoding time while maintaining (or improving) the image quality on widely used 3D object datasets.",
      "published": "2024-04-07T10:49:59Z"
    },
    "metadata": {
      "arxiv_id": "2404.04913",
      "title": "CodecNeRF: Toward Fast Encoding and Decoding, Compact, and High-quality Novel-view Synthesis",
      "summary": "Neural Radiance Fields (NeRF) have achieved huge success in effectively capturing and representing 3D objects and scenes. However, to establish a ubiquitous presence in everyday media formats, such as images and videos, we need to fulfill three key objectives: 1. fast encoding and decoding time, 2. compact model sizes, and 3. high-quality renderings. Despite recent advancements, a comprehensive algorithm that adequately addresses all objectives has yet to be fully realized. In this work, we present CodecNeRF, a neural codec for NeRF representations, consisting of an encoder and decoder architecture that can generate a NeRF representation in a single forward pass. Furthermore, inspired by the recent parameter-efficient finetuning approaches, we propose a finetuning method to efficiently adapt the generated NeRF representations to a new test instance, leading to high-quality image renderings and compact code sizes. The proposed CodecNeRF, a newly suggested encoding-decoding-finetuning pipeline for NeRF, achieved unprecedented compression performance of more than 100x and remarkable reduction in encoding time while maintaining (or improving) the image quality on widely used 3D object datasets.",
      "authors": [
        "Gyeongjin Kang",
        "Younggeun Lee",
        "Seungjun Oh",
        "Eunbyung Park"
      ],
      "published": "2024-04-07T10:49:59Z",
      "updated": "2024-09-25T07:16:41Z",
      "categories": [
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2404.04913v3",
      "landing_url": "https://arxiv.org/abs/2404.04913v3",
      "doi": "https://doi.org/10.48550/arXiv.2404.04913"
    },
    "queries": [
      {
        "anchor": "synchronous dialogue",
        "search_term": "neural codec"
      }
    ]
  },
  {
    "arxiv_id": "2404.05600",
    "anchor": "synchronous dialogue",
    "search_term": "neural codec",
    "search_record": {
      "id": "http://arxiv.org/abs/2404.05600v1",
      "title": "SpeechAlign: Aligning Speech Generation to Human Preferences",
      "summary": "Speech language models have significantly advanced in generating realistic speech, with neural codec language models standing out. However, the integration of human feedback to align speech outputs to human preferences is often neglected. This paper addresses this gap by first analyzing the distribution gap in codec language models, highlighting how it leads to discrepancies between the training and inference phases, which negatively affects performance. Then we explore leveraging learning from human feedback to bridge the distribution gap. We introduce SpeechAlign, an iterative self-improvement strategy that aligns speech language models to human preferences. SpeechAlign involves constructing a preference codec dataset contrasting golden codec tokens against synthetic tokens, followed by preference optimization to improve the codec language model. This cycle of improvement is carried out iteratively to steadily convert weak models to strong ones. Through both subjective and objective evaluations, we show that SpeechAlign can bridge the distribution gap and facilitating continuous self-improvement of the speech language model. Moreover, SpeechAlign exhibits robust generalization capabilities and works for smaller models. Code and models will be available at https://github.com/0nutation/SpeechGPT.",
      "published": "2024-04-08T15:21:17Z"
    },
    "metadata": {
      "arxiv_id": "2404.05600",
      "title": "SpeechAlign: Aligning Speech Generation to Human Preferences",
      "summary": "Speech language models have significantly advanced in generating realistic speech, with neural codec language models standing out. However, the integration of human feedback to align speech outputs to human preferences is often neglected. This paper addresses this gap by first analyzing the distribution gap in codec language models, highlighting how it leads to discrepancies between the training and inference phases, which negatively affects performance. Then we explore leveraging learning from human feedback to bridge the distribution gap. We introduce SpeechAlign, an iterative self-improvement strategy that aligns speech language models to human preferences. SpeechAlign involves constructing a preference codec dataset contrasting golden codec tokens against synthetic tokens, followed by preference optimization to improve the codec language model. This cycle of improvement is carried out iteratively to steadily convert weak models to strong ones. Through both subjective and objective evaluations, we show that SpeechAlign can bridge the distribution gap and facilitating continuous self-improvement of the speech language model. Moreover, SpeechAlign exhibits robust generalization capabilities and works for smaller models. Code and models will be available at https://github.com/0nutation/SpeechGPT.",
      "authors": [
        "Dong Zhang",
        "Zhaowei Li",
        "Shimin Li",
        "Xin Zhang",
        "Pengyu Wang",
        "Yaqian Zhou",
        "Xipeng Qiu"
      ],
      "published": "2024-04-08T15:21:17Z",
      "updated": "2024-04-08T15:21:17Z",
      "categories": [
        "cs.CL",
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2404.05600v1",
      "landing_url": "https://arxiv.org/abs/2404.05600v1",
      "doi": "https://doi.org/10.48550/arXiv.2404.05600"
    },
    "queries": [
      {
        "anchor": "synchronous dialogue",
        "search_term": "neural codec"
      }
    ]
  },
  {
    "arxiv_id": "2404.05984",
    "anchor": "full-duplex",
    "search_term": "true full-duplex",
    "search_record": {
      "id": "http://arxiv.org/abs/2404.05984v1",
      "title": "Interference Management for Full-Duplex ISAC in B5G/6G Networks: Architectures, Challenges, and Solutions",
      "summary": "Integrated sensing and communications (ISAC) has been visioned as a key technique for B5G/6G networks. To support monostatic sensing, a full-duplex radio is indispensable to extract echo signals from targets. Such a radio can also greatly improve network capacity via full-duplex communications. However, full-duplex radios in existing ISAC designs are mainly focused on wireless sensing, while the ability of full-duplex communications is usually ignored. In this article, we provide an overview of full-duplex ISAC (FD-ISAC), where a full-duplex radio is used for both wireless sensing and full-duplex communications in B5G/6G networks, with a focus on the fundamental interference management problem in such networks. First, different ISAC architectures are introduced, considering different full-duplex communication modes and wireless sensing modes. Next, the challenging issues of link-level interference and network-level interference are analyzed, illustrating a critical demand on interference management for FD-ISAC. Potential solutions to interference management are then reviewed from the perspective of radio architecture design, beamforming, mode selection, and resource allocation. The corresponding open problems are also highlighted.",
      "published": "2024-04-09T03:35:14Z"
    },
    "metadata": {
      "arxiv_id": "2404.05984",
      "title": "Interference Management for Full-Duplex ISAC in B5G/6G Networks: Architectures, Challenges, and Solutions",
      "summary": "Integrated sensing and communications (ISAC) has been visioned as a key technique for B5G/6G networks. To support monostatic sensing, a full-duplex radio is indispensable to extract echo signals from targets. Such a radio can also greatly improve network capacity via full-duplex communications. However, full-duplex radios in existing ISAC designs are mainly focused on wireless sensing, while the ability of full-duplex communications is usually ignored. In this article, we provide an overview of full-duplex ISAC (FD-ISAC), where a full-duplex radio is used for both wireless sensing and full-duplex communications in B5G/6G networks, with a focus on the fundamental interference management problem in such networks. First, different ISAC architectures are introduced, considering different full-duplex communication modes and wireless sensing modes. Next, the challenging issues of link-level interference and network-level interference are analyzed, illustrating a critical demand on interference management for FD-ISAC. Potential solutions to interference management are then reviewed from the perspective of radio architecture design, beamforming, mode selection, and resource allocation. The corresponding open problems are also highlighted.",
      "authors": [
        "Aimin Tang",
        "Xudong Wang",
        "J. Andrew Zhang"
      ],
      "published": "2024-04-09T03:35:14Z",
      "updated": "2024-04-09T03:35:14Z",
      "categories": [
        "eess.SP"
      ],
      "pdf_url": "https://arxiv.org/pdf/2404.05984v1",
      "landing_url": "https://arxiv.org/abs/2404.05984v1",
      "doi": "https://doi.org/10.1109/MCOM.001.2300654"
    },
    "queries": [
      {
        "anchor": "full-duplex",
        "search_term": "true full-duplex"
      },
      {
        "anchor": "full-duplex",
        "search_term": "pseudo full-duplex"
      },
      {
        "anchor": "full-duplex",
        "search_term": "half duplex"
      },
      {
        "anchor": "full-duplex",
        "search_term": "engineered synchronization"
      },
      {
        "anchor": "full-duplex",
        "search_term": "learned synchronization"
      },
      {
        "anchor": "full-duplex",
        "search_term": "modular architecture"
      },
      {
        "anchor": "full-duplex",
        "search_term": "streaming architecture"
      },
      {
        "anchor": "full-duplex",
        "search_term": "real-time interaction"
      },
      {
        "anchor": "full-duplex",
        "search_term": "low latency"
      },
      {
        "anchor": "full-duplex",
        "search_term": "synchronous data"
      },
      {
        "anchor": "full-duplex",
        "search_term": "data scarcity"
      },
      {
        "anchor": "full-duplex",
        "search_term": "multichannel dialogue"
      },
      {
        "anchor": "full-duplex",
        "search_term": "temporal dynamics"
      },
      {
        "anchor": "full-duplex",
        "search_term": "behavioral arbitration"
      },
      {
        "anchor": "full-duplex",
        "search_term": "semantic coherence"
      },
      {
        "anchor": "full-duplex",
        "search_term": "neural codec"
      },
      {
        "anchor": "full-duplex",
        "search_term": "discrete tokens"
      },
      {
        "anchor": "full-duplex",
        "search_term": "audio tokens"
      },
      {
        "anchor": "full-duplex",
        "search_term": "floor transfer offset"
      },
      {
        "anchor": "full-duplex",
        "search_term": "stop latency"
      },
      {
        "anchor": "full-duplex",
        "search_term": "interruption response delay"
      }
    ]
  },
  {
    "arxiv_id": "2404.06688",
    "anchor": "synchronous dialogue",
    "search_term": "temporal dynamics",
    "search_record": {
      "id": "http://arxiv.org/abs/2404.06688v2",
      "title": "Spatio-Temporal Dynamics of Nucleo-Cytoplasmic Transport",
      "summary": "Nucleocytoplasmic transport is essential for cellular function, presenting a canonical example of rapid molecular sorting inside cells. It consists of a coordinated interplay between import/export of molecules in/out the cell nucleus. Here, we investigate the role of spatio-temporal dynamics of the nucleocytoplasmic transport and its regulation. We develop a biophysical model that captures the main features of the nucleocytoplasmic transport, in particular, its regulation through the Ran cycle. Our model yields steady-state profiles for the molecular components of the Ran cycle, their relaxation times, as well as the nuclear-to-cytoplasmic molecule ratio. We show that these quantities are affected by their spatial dynamics and heterogeneity within the nucleus. Specifically, we find that the spatial nonuniformity of Ran Guanine Exchange Factor (RanGEF) - particularly its proximity to the nuclear envelope - increases the Ran content in the nucleus. We further show that RanGEF's accumulation near the nuclear envelope results from its intrinsic dynamics as a nuclear cargo, transported by the Ran cycle itself. Overall, our work highlights the critical role of molecular spatial dynamics in cellular processes, and proposes new avenues for theoretical and experimental inquiries into the nucleocytoplasmic transport.",
      "published": "2024-04-10T02:24:22Z"
    },
    "metadata": {
      "arxiv_id": "2404.06688",
      "title": "Spatio-Temporal Dynamics of Nucleo-Cytoplasmic Transport",
      "summary": "Nucleocytoplasmic transport is essential for cellular function, presenting a canonical example of rapid molecular sorting inside cells. It consists of a coordinated interplay between import/export of molecules in/out the cell nucleus. Here, we investigate the role of spatio-temporal dynamics of the nucleocytoplasmic transport and its regulation. We develop a biophysical model that captures the main features of the nucleocytoplasmic transport, in particular, its regulation through the Ran cycle. Our model yields steady-state profiles for the molecular components of the Ran cycle, their relaxation times, as well as the nuclear-to-cytoplasmic molecule ratio. We show that these quantities are affected by their spatial dynamics and heterogeneity within the nucleus. Specifically, we find that the spatial nonuniformity of Ran Guanine Exchange Factor (RanGEF) - particularly its proximity to the nuclear envelope - increases the Ran content in the nucleus. We further show that RanGEF's accumulation near the nuclear envelope results from its intrinsic dynamics as a nuclear cargo, transported by the Ran cycle itself. Overall, our work highlights the critical role of molecular spatial dynamics in cellular processes, and proposes new avenues for theoretical and experimental inquiries into the nucleocytoplasmic transport.",
      "authors": [
        "S. Alex Rautu",
        "Alexandra Zidovska",
        "Michael J. Shelley"
      ],
      "published": "2024-04-10T02:24:22Z",
      "updated": "2024-09-25T17:13:49Z",
      "categories": [
        "physics.bio-ph",
        "cond-mat.soft",
        "q-bio.SC"
      ],
      "pdf_url": "https://arxiv.org/pdf/2404.06688v2",
      "landing_url": "https://arxiv.org/abs/2404.06688v2",
      "doi": "https://doi.org/10.1103/PhysRevResearch.6.043022"
    },
    "queries": [
      {
        "anchor": "synchronous dialogue",
        "search_term": "temporal dynamics"
      }
    ]
  },
  {
    "arxiv_id": "2404.07472",
    "anchor": "spoken language models",
    "search_term": "modular architecture",
    "search_record": {
      "id": "http://arxiv.org/abs/2404.07472v1",
      "title": "Cramer-Rao Bounds for Near-Field Sensing: A Generic Modular Architecture",
      "summary": "A generic modular array architecture is proposed, featuring uniform/non-uniform subarray layouts that allows for flexible deployment. The bistatic near-field sensing system is considered, where the target is located in the near-field of the whole modular array and the far-field of each subarray. Then, the closed-form expressions of Cramer-Rao bounds (CRBs) for range and angle estimations are derived based on the hybrid spherical and planar wave model (HSPM). Simulation results validate the accuracy of the derived closed-form CRBs and demonstrate that: i) The HSPM with varying angles of arrival (AoAs) between subarrays can reduce the CRB for range estimation compared to the traditional HSPM with shared AoA; and ii) The proposed generic modular architecture with subarrays positioned closer to the edges can significantly reduce the CRBs compared to the traditional modular architecture with uniform subarray layout, when the array aperture is fixed.",
      "published": "2024-04-11T04:42:01Z"
    },
    "metadata": {
      "arxiv_id": "2404.07472",
      "title": "Cramer-Rao Bounds for Near-Field Sensing: A Generic Modular Architecture",
      "summary": "A generic modular array architecture is proposed, featuring uniform/non-uniform subarray layouts that allows for flexible deployment. The bistatic near-field sensing system is considered, where the target is located in the near-field of the whole modular array and the far-field of each subarray. Then, the closed-form expressions of Cramer-Rao bounds (CRBs) for range and angle estimations are derived based on the hybrid spherical and planar wave model (HSPM). Simulation results validate the accuracy of the derived closed-form CRBs and demonstrate that: i) The HSPM with varying angles of arrival (AoAs) between subarrays can reduce the CRB for range estimation compared to the traditional HSPM with shared AoA; and ii) The proposed generic modular architecture with subarrays positioned closer to the edges can significantly reduce the CRBs compared to the traditional modular architecture with uniform subarray layout, when the array aperture is fixed.",
      "authors": [
        "Chunwei Meng",
        "Dingyou Ma",
        "Xu Chen",
        "Zhiyong Feng",
        "Yuanwei Liu"
      ],
      "published": "2024-04-11T04:42:01Z",
      "updated": "2024-04-11T04:42:01Z",
      "categories": [
        "eess.SP"
      ],
      "pdf_url": "https://arxiv.org/pdf/2404.07472v1",
      "landing_url": "https://arxiv.org/abs/2404.07472v1",
      "doi": "https://doi.org/10.1109/LWC.2024.3406577"
    },
    "queries": [
      {
        "anchor": "spoken language models",
        "search_term": "modular architecture"
      },
      {
        "anchor": "synchronous dialogue",
        "search_term": "modular architecture"
      },
      {
        "anchor": "turn-taking",
        "search_term": "modular architecture"
      }
    ]
  },
  {
    "arxiv_id": "2404.07575",
    "anchor": "synchronous dialogue",
    "search_term": "data scarcity",
    "search_record": {
      "id": "http://arxiv.org/abs/2404.07575v4",
      "title": "An Effective Automated Speaking Assessment Approach to Mitigating Data Scarcity and Imbalanced Distribution",
      "summary": "Automated speaking assessment (ASA) typically involves automatic speech recognition (ASR) and hand-crafted feature extraction from the ASR transcript of a learner's speech. Recently, self-supervised learning (SSL) has shown stellar performance compared to traditional methods. However, SSL-based ASA systems are faced with at least three data-related challenges: limited annotated data, uneven distribution of learner proficiency levels and non-uniform score intervals between different CEFR proficiency levels. To address these challenges, we explore the use of two novel modeling strategies: metric-based classification and loss reweighting, leveraging distinct SSL-based embedding features. Extensive experimental results on the ICNALE benchmark dataset suggest that our approach can outperform existing strong baselines by a sizable margin, achieving a significant improvement of more than 10% in CEFR prediction accuracy.",
      "published": "2024-04-11T09:06:49Z"
    },
    "metadata": {
      "arxiv_id": "2404.07575",
      "title": "An Effective Automated Speaking Assessment Approach to Mitigating Data Scarcity and Imbalanced Distribution",
      "summary": "Automated speaking assessment (ASA) typically involves automatic speech recognition (ASR) and hand-crafted feature extraction from the ASR transcript of a learner's speech. Recently, self-supervised learning (SSL) has shown stellar performance compared to traditional methods. However, SSL-based ASA systems are faced with at least three data-related challenges: limited annotated data, uneven distribution of learner proficiency levels and non-uniform score intervals between different CEFR proficiency levels. To address these challenges, we explore the use of two novel modeling strategies: metric-based classification and loss reweighting, leveraging distinct SSL-based embedding features. Extensive experimental results on the ICNALE benchmark dataset suggest that our approach can outperform existing strong baselines by a sizable margin, achieving a significant improvement of more than 10% in CEFR prediction accuracy.",
      "authors": [
        "Tien-Hong Lo",
        "Fu-An Chao",
        "Tzu-I Wu",
        "Yao-Ting Sung",
        "Berlin Chen"
      ],
      "published": "2024-04-11T09:06:49Z",
      "updated": "2025-03-02T13:55:52Z",
      "categories": [
        "cs.SD",
        "cs.AI",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2404.07575v4",
      "landing_url": "https://arxiv.org/abs/2404.07575v4",
      "doi": "https://doi.org/10.48550/arXiv.2404.07575"
    },
    "queries": [
      {
        "anchor": "synchronous dialogue",
        "search_term": "data scarcity"
      }
    ]
  },
  {
    "arxiv_id": "2404.08610",
    "anchor": "full-duplex",
    "search_term": "true full-duplex",
    "search_record": {
      "id": "http://arxiv.org/abs/2404.08610v3",
      "title": "Full-Duplex Beyond Self-Interference: The Unlimited Sensing Way",
      "summary": "The success of full-stack full-duplex communication systems depends on how effectively one can achieve digital self-interference cancellation (SIC). Towards this end, in this paper, we consider unlimited sensing framework (USF) enabled full-duplex system. We show that by injecting folding non-linearities in the sensing pipeline, one can not only suppress self-interference but also recover the signal of interest (SoI). This approach leads to novel design of the receiver architecture that is complemented by a modulo-domain channel estimation method. We then demonstrate the advantages of modulo ADC by analyzing the relationship between quantization noise, quantization bits, and dynamic range. Numerical experiments show that the USF enabled receiver structure can achieve up to 40 dB digital SIC by using as few as 4-bits per sample. Our method outperforms the previous approach based on adaptive filters when it comes to SoI reconstruction, detection, and digital SIC performance.",
      "published": "2024-04-12T17:15:49Z"
    },
    "metadata": {
      "arxiv_id": "2404.08610",
      "title": "Full-Duplex Beyond Self-Interference: The Unlimited Sensing Way",
      "summary": "The success of full-stack full-duplex communication systems depends on how effectively one can achieve digital self-interference cancellation (SIC). Towards this end, in this paper, we consider unlimited sensing framework (USF) enabled full-duplex system. We show that by injecting folding non-linearities in the sensing pipeline, one can not only suppress self-interference but also recover the signal of interest (SoI). This approach leads to novel design of the receiver architecture that is complemented by a modulo-domain channel estimation method. We then demonstrate the advantages of modulo ADC by analyzing the relationship between quantization noise, quantization bits, and dynamic range. Numerical experiments show that the USF enabled receiver structure can achieve up to 40 dB digital SIC by using as few as 4-bits per sample. Our method outperforms the previous approach based on adaptive filters when it comes to SoI reconstruction, detection, and digital SIC performance.",
      "authors": [
        "Ziang Liu",
        "Ayush Bhandari",
        "Bruno Clerckx"
      ],
      "published": "2024-04-12T17:15:49Z",
      "updated": "2024-11-20T19:11:41Z",
      "categories": [
        "eess.SP"
      ],
      "pdf_url": "https://arxiv.org/pdf/2404.08610v3",
      "landing_url": "https://arxiv.org/abs/2404.08610v3",
      "doi": "https://doi.org/10.48550/arXiv.2404.08610"
    },
    "queries": [
      {
        "anchor": "full-duplex",
        "search_term": "true full-duplex"
      },
      {
        "anchor": "full-duplex",
        "search_term": "pseudo full-duplex"
      },
      {
        "anchor": "full-duplex",
        "search_term": "engineered synchronization"
      },
      {
        "anchor": "full-duplex",
        "search_term": "learned synchronization"
      },
      {
        "anchor": "full-duplex",
        "search_term": "modular architecture"
      },
      {
        "anchor": "full-duplex",
        "search_term": "streaming architecture"
      },
      {
        "anchor": "full-duplex",
        "search_term": "real-time interaction"
      },
      {
        "anchor": "full-duplex",
        "search_term": "synchronous data"
      },
      {
        "anchor": "full-duplex",
        "search_term": "data scarcity"
      },
      {
        "anchor": "full-duplex",
        "search_term": "multichannel dialogue"
      },
      {
        "anchor": "full-duplex",
        "search_term": "temporal dynamics"
      },
      {
        "anchor": "full-duplex",
        "search_term": "behavioral arbitration"
      },
      {
        "anchor": "full-duplex",
        "search_term": "semantic coherence"
      },
      {
        "anchor": "full-duplex",
        "search_term": "neural codec"
      },
      {
        "anchor": "full-duplex",
        "search_term": "discrete tokens"
      },
      {
        "anchor": "full-duplex",
        "search_term": "audio tokens"
      },
      {
        "anchor": "full-duplex",
        "search_term": "floor transfer offset"
      },
      {
        "anchor": "full-duplex",
        "search_term": "stop latency"
      },
      {
        "anchor": "full-duplex",
        "search_term": "interruption response delay"
      }
    ]
  },
  {
    "arxiv_id": "2404.09231",
    "anchor": "synchronous dialogue",
    "search_term": "temporal dynamics",
    "search_record": {
      "id": "http://arxiv.org/abs/2404.09231v1",
      "title": "Tri-modal Confluence with Temporal Dynamics for Scene Graph Generation in Operating Rooms",
      "summary": "A comprehensive understanding of surgical scenes allows for monitoring of the surgical process, reducing the occurrence of accidents and enhancing efficiency for medical professionals. Semantic modeling within operating rooms, as a scene graph generation (SGG) task, is challenging since it involves consecutive recognition of subtle surgical actions over prolonged periods. To address this challenge, we propose a Tri-modal (i.e., images, point clouds, and language) confluence with Temporal dynamics framework, termed TriTemp-OR. Diverging from previous approaches that integrated temporal information via memory graphs, our method embraces two advantages: 1) we directly exploit bi-modal temporal information from the video streaming for hierarchical feature interaction, and 2) the prior knowledge from Large Language Models (LLMs) is embedded to alleviate the class-imbalance problem in the operating theatre. Specifically, our model performs temporal interactions across 2D frames and 3D point clouds, including a scale-adaptive multi-view temporal interaction (ViewTemp) and a geometric-temporal point aggregation (PointTemp). Furthermore, we transfer knowledge from the biomedical LLM, LLaVA-Med, to deepen the comprehension of intraoperative relations. The proposed TriTemp-OR enables the aggregation of tri-modal features through relation-aware unification to predict relations so as to generate scene graphs. Experimental results on the 4D-OR benchmark demonstrate the superior performance of our model for long-term OR streaming.",
      "published": "2024-04-14T12:19:16Z"
    },
    "metadata": {
      "arxiv_id": "2404.09231",
      "title": "Tri-modal Confluence with Temporal Dynamics for Scene Graph Generation in Operating Rooms",
      "summary": "A comprehensive understanding of surgical scenes allows for monitoring of the surgical process, reducing the occurrence of accidents and enhancing efficiency for medical professionals. Semantic modeling within operating rooms, as a scene graph generation (SGG) task, is challenging since it involves consecutive recognition of subtle surgical actions over prolonged periods. To address this challenge, we propose a Tri-modal (i.e., images, point clouds, and language) confluence with Temporal dynamics framework, termed TriTemp-OR. Diverging from previous approaches that integrated temporal information via memory graphs, our method embraces two advantages: 1) we directly exploit bi-modal temporal information from the video streaming for hierarchical feature interaction, and 2) the prior knowledge from Large Language Models (LLMs) is embedded to alleviate the class-imbalance problem in the operating theatre. Specifically, our model performs temporal interactions across 2D frames and 3D point clouds, including a scale-adaptive multi-view temporal interaction (ViewTemp) and a geometric-temporal point aggregation (PointTemp). Furthermore, we transfer knowledge from the biomedical LLM, LLaVA-Med, to deepen the comprehension of intraoperative relations. The proposed TriTemp-OR enables the aggregation of tri-modal features through relation-aware unification to predict relations so as to generate scene graphs. Experimental results on the 4D-OR benchmark demonstrate the superior performance of our model for long-term OR streaming.",
      "authors": [
        "Diandian Guo",
        "Manxi Lin",
        "Jialun Pei",
        "He Tang",
        "Yueming Jin",
        "Pheng-Ann Heng"
      ],
      "published": "2024-04-14T12:19:16Z",
      "updated": "2024-04-14T12:19:16Z",
      "categories": [
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2404.09231v1",
      "landing_url": "https://arxiv.org/abs/2404.09231v1",
      "doi": "https://doi.org/10.48550/arXiv.2404.09231"
    },
    "queries": [
      {
        "anchor": "synchronous dialogue",
        "search_term": "temporal dynamics"
      }
    ]
  },
  {
    "arxiv_id": "2404.09833",
    "anchor": "synchronous dialogue",
    "search_term": "real-time interaction",
    "search_record": {
      "id": "http://arxiv.org/abs/2404.09833v1",
      "title": "Video2Game: Real-time, Interactive, Realistic and Browser-Compatible Environment from a Single Video",
      "summary": "Creating high-quality and interactive virtual environments, such as games and simulators, often involves complex and costly manual modeling processes. In this paper, we present Video2Game, a novel approach that automatically converts videos of real-world scenes into realistic and interactive game environments. At the heart of our system are three core components:(i) a neural radiance fields (NeRF) module that effectively captures the geometry and visual appearance of the scene; (ii) a mesh module that distills the knowledge from NeRF for faster rendering; and (iii) a physics module that models the interactions and physical dynamics among the objects. By following the carefully designed pipeline, one can construct an interactable and actionable digital replica of the real world. We benchmark our system on both indoor and large-scale outdoor scenes. We show that we can not only produce highly-realistic renderings in real-time, but also build interactive games on top.",
      "published": "2024-04-15T14:32:32Z"
    },
    "metadata": {
      "arxiv_id": "2404.09833",
      "title": "Video2Game: Real-time, Interactive, Realistic and Browser-Compatible Environment from a Single Video",
      "summary": "Creating high-quality and interactive virtual environments, such as games and simulators, often involves complex and costly manual modeling processes. In this paper, we present Video2Game, a novel approach that automatically converts videos of real-world scenes into realistic and interactive game environments. At the heart of our system are three core components:(i) a neural radiance fields (NeRF) module that effectively captures the geometry and visual appearance of the scene; (ii) a mesh module that distills the knowledge from NeRF for faster rendering; and (iii) a physics module that models the interactions and physical dynamics among the objects. By following the carefully designed pipeline, one can construct an interactable and actionable digital replica of the real world. We benchmark our system on both indoor and large-scale outdoor scenes. We show that we can not only produce highly-realistic renderings in real-time, but also build interactive games on top.",
      "authors": [
        "Hongchi Xia",
        "Zhi-Hao Lin",
        "Wei-Chiu Ma",
        "Shenlong Wang"
      ],
      "published": "2024-04-15T14:32:32Z",
      "updated": "2024-04-15T14:32:32Z",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2404.09833v1",
      "landing_url": "https://arxiv.org/abs/2404.09833v1",
      "doi": "https://doi.org/10.48550/arXiv.2404.09833"
    },
    "queries": [
      {
        "anchor": "synchronous dialogue",
        "search_term": "real-time interaction"
      }
    ]
  },
  {
    "arxiv_id": "2404.10419",
    "anchor": "spoken language models",
    "search_term": "true full-duplex",
    "search_record": {
      "id": "http://arxiv.org/abs/2404.10419v2",
      "title": "MAD Speech: Measures of Acoustic Diversity of Speech",
      "summary": "Generative spoken language models produce speech in a wide range of voices, prosody, and recording conditions, seemingly approaching the diversity of natural speech. However, the extent to which generated speech is acoustically diverse remains unclear due to a lack of appropriate metrics. We address this gap by developing lightweight metrics of acoustic diversity, which we collectively refer to as MAD Speech. We focus on measuring five facets of acoustic diversity: voice, gender, emotion, accent, and background noise. We construct the metrics as a composition of specialized, per-facet embedding models and an aggregation function that measures diversity within the embedding space. Next, we build a series of datasets with a priori known diversity preferences for each facet. Using these datasets, we demonstrate that our proposed metrics achieve a stronger agreement with the ground-truth diversity than baselines. Finally, we showcase the applicability of our proposed metrics across several real-life evaluation scenarios. MAD Speech is made publicly accessible.",
      "published": "2024-04-16T09:35:27Z"
    },
    "metadata": {
      "arxiv_id": "2404.10419",
      "title": "MAD Speech: Measures of Acoustic Diversity of Speech",
      "summary": "Generative spoken language models produce speech in a wide range of voices, prosody, and recording conditions, seemingly approaching the diversity of natural speech. However, the extent to which generated speech is acoustically diverse remains unclear due to a lack of appropriate metrics. We address this gap by developing lightweight metrics of acoustic diversity, which we collectively refer to as MAD Speech. We focus on measuring five facets of acoustic diversity: voice, gender, emotion, accent, and background noise. We construct the metrics as a composition of specialized, per-facet embedding models and an aggregation function that measures diversity within the embedding space. Next, we build a series of datasets with a priori known diversity preferences for each facet. Using these datasets, we demonstrate that our proposed metrics achieve a stronger agreement with the ground-truth diversity than baselines. Finally, we showcase the applicability of our proposed metrics across several real-life evaluation scenarios. MAD Speech is made publicly accessible.",
      "authors": [
        "Matthieu Futeral",
        "Andrea Agostinelli",
        "Marco Tagliasacchi",
        "Neil Zeghidour",
        "Eugene Kharitonov"
      ],
      "published": "2024-04-16T09:35:27Z",
      "updated": "2025-03-11T12:02:06Z",
      "categories": [
        "eess.AS",
        "cs.CL"
      ],
      "pdf_url": "https://arxiv.org/pdf/2404.10419v2",
      "landing_url": "https://arxiv.org/abs/2404.10419v2",
      "doi": "https://doi.org/10.48550/arXiv.2404.10419"
    },
    "queries": [
      {
        "anchor": "spoken language models",
        "search_term": "true full-duplex"
      },
      {
        "anchor": "spoken language models",
        "search_term": "pseudo full-duplex"
      },
      {
        "anchor": "spoken language models",
        "search_term": "engineered synchronization"
      },
      {
        "anchor": "spoken language models",
        "search_term": "learned synchronization"
      },
      {
        "anchor": "spoken language models",
        "search_term": "modular architecture"
      },
      {
        "anchor": "spoken language models",
        "search_term": "streaming architecture"
      },
      {
        "anchor": "spoken language models",
        "search_term": "real-time interaction"
      },
      {
        "anchor": "spoken language models",
        "search_term": "synchronous data"
      },
      {
        "anchor": "spoken language models",
        "search_term": "data scarcity"
      },
      {
        "anchor": "spoken language models",
        "search_term": "multichannel dialogue"
      },
      {
        "anchor": "spoken language models",
        "search_term": "behavioral arbitration"
      },
      {
        "anchor": "spoken language models",
        "search_term": "semantic coherence"
      },
      {
        "anchor": "spoken language models",
        "search_term": "neural codec"
      },
      {
        "anchor": "spoken language models",
        "search_term": "discrete tokens"
      },
      {
        "anchor": "spoken language models",
        "search_term": "audio tokens"
      },
      {
        "anchor": "spoken language models",
        "search_term": "floor transfer offset"
      },
      {
        "anchor": "spoken language models",
        "search_term": "stop latency"
      },
      {
        "anchor": "spoken language models",
        "search_term": "interruption response delay"
      }
    ]
  },
  {
    "arxiv_id": "2404.11532",
    "anchor": "spoken language models",
    "search_term": "true full-duplex",
    "search_record": {
      "id": "http://arxiv.org/abs/2404.11532v1",
      "title": "Select and Reorder: A Novel Approach for Neural Sign Language Production",
      "summary": "Sign languages, often categorised as low-resource languages, face significant challenges in achieving accurate translation due to the scarcity of parallel annotated datasets. This paper introduces Select and Reorder (S&R), a novel approach that addresses data scarcity by breaking down the translation process into two distinct steps: Gloss Selection (GS) and Gloss Reordering (GR). Our method leverages large spoken language models and the substantial lexical overlap between source spoken languages and target sign languages to establish an initial alignment. Both steps make use of Non-AutoRegressive (NAR) decoding for reduced computation and faster inference speeds. Through this disentanglement of tasks, we achieve state-of-the-art BLEU and Rouge scores on the Meine DGS Annotated (mDGS) dataset, demonstrating a substantial BLUE-1 improvement of 37.88% in Text to Gloss (T2G) Translation. This innovative approach paves the way for more effective translation models for sign languages, even in resource-constrained settings.",
      "published": "2024-04-17T16:25:19Z"
    },
    "metadata": {
      "arxiv_id": "2404.11532",
      "title": "Select and Reorder: A Novel Approach for Neural Sign Language Production",
      "summary": "Sign languages, often categorised as low-resource languages, face significant challenges in achieving accurate translation due to the scarcity of parallel annotated datasets. This paper introduces Select and Reorder (S&R), a novel approach that addresses data scarcity by breaking down the translation process into two distinct steps: Gloss Selection (GS) and Gloss Reordering (GR). Our method leverages large spoken language models and the substantial lexical overlap between source spoken languages and target sign languages to establish an initial alignment. Both steps make use of Non-AutoRegressive (NAR) decoding for reduced computation and faster inference speeds. Through this disentanglement of tasks, we achieve state-of-the-art BLEU and Rouge scores on the Meine DGS Annotated (mDGS) dataset, demonstrating a substantial BLUE-1 improvement of 37.88% in Text to Gloss (T2G) Translation. This innovative approach paves the way for more effective translation models for sign languages, even in resource-constrained settings.",
      "authors": [
        "Harry Walsh",
        "Ben Saunders",
        "Richard Bowden"
      ],
      "published": "2024-04-17T16:25:19Z",
      "updated": "2024-04-17T16:25:19Z",
      "categories": [
        "cs.CL"
      ],
      "pdf_url": "https://arxiv.org/pdf/2404.11532v1",
      "landing_url": "https://arxiv.org/abs/2404.11532v1",
      "doi": "https://doi.org/10.48550/arXiv.2404.11532"
    },
    "queries": [
      {
        "anchor": "spoken language models",
        "search_term": "true full-duplex"
      },
      {
        "anchor": "spoken language models",
        "search_term": "pseudo full-duplex"
      },
      {
        "anchor": "spoken language models",
        "search_term": "engineered synchronization"
      },
      {
        "anchor": "spoken language models",
        "search_term": "learned synchronization"
      },
      {
        "anchor": "spoken language models",
        "search_term": "modular architecture"
      },
      {
        "anchor": "spoken language models",
        "search_term": "streaming architecture"
      },
      {
        "anchor": "spoken language models",
        "search_term": "real-time interaction"
      },
      {
        "anchor": "spoken language models",
        "search_term": "synchronous data"
      },
      {
        "anchor": "spoken language models",
        "search_term": "data scarcity"
      },
      {
        "anchor": "spoken language models",
        "search_term": "multichannel dialogue"
      },
      {
        "anchor": "spoken language models",
        "search_term": "behavioral arbitration"
      },
      {
        "anchor": "spoken language models",
        "search_term": "semantic coherence"
      },
      {
        "anchor": "spoken language models",
        "search_term": "neural codec"
      },
      {
        "anchor": "spoken language models",
        "search_term": "discrete tokens"
      },
      {
        "anchor": "spoken language models",
        "search_term": "audio tokens"
      },
      {
        "anchor": "spoken language models",
        "search_term": "floor transfer offset"
      },
      {
        "anchor": "spoken language models",
        "search_term": "stop latency"
      },
      {
        "anchor": "spoken language models",
        "search_term": "interruption response delay"
      }
    ]
  },
  {
    "arxiv_id": "2404.13551",
    "anchor": "spoken language models",
    "search_term": "streaming architecture",
    "search_record": {
      "id": "http://arxiv.org/abs/2404.13551v1",
      "title": "AudioRepInceptionNeXt: A lightweight single-stream architecture for efficient audio recognition",
      "summary": "Recent research has successfully adapted vision-based convolutional neural network (CNN) architectures for audio recognition tasks using Mel-Spectrograms. However, these CNNs have high computational costs and memory requirements, limiting their deployment on low-end edge devices. Motivated by the success of efficient vision models like InceptionNeXt and ConvNeXt, we propose AudioRepInceptionNeXt, a single-stream architecture. Its basic building block breaks down the parallel multi-branch depth-wise convolutions with descending scales of k x k kernels into a cascade of two multi-branch depth-wise convolutions. The first multi-branch consists of parallel multi-scale 1 x k depth-wise convolutional layers followed by a similar multi-branch employing parallel multi-scale k x 1 depth-wise convolutional layers. This reduces computational and memory footprint while separating time and frequency processing of Mel-Spectrograms. The large kernels capture global frequencies and long activities, while small kernels get local frequencies and short activities. We also reparameterize the multi-branch design during inference to further boost speed without losing accuracy. Experiments show that AudioRepInceptionNeXt reduces parameters and computations by 50%+ and improves inference speed 1.28x over state-of-the-art CNNs like the Slow-Fast while maintaining comparable accuracy. It also learns robustly across a variety of audio recognition tasks. Codes are available at https://github.com/StevenLauHKHK/AudioRepInceptionNeXt.",
      "published": "2024-04-21T06:33:04Z"
    },
    "metadata": {
      "arxiv_id": "2404.13551",
      "title": "AudioRepInceptionNeXt: A lightweight single-stream architecture for efficient audio recognition",
      "summary": "Recent research has successfully adapted vision-based convolutional neural network (CNN) architectures for audio recognition tasks using Mel-Spectrograms. However, these CNNs have high computational costs and memory requirements, limiting their deployment on low-end edge devices. Motivated by the success of efficient vision models like InceptionNeXt and ConvNeXt, we propose AudioRepInceptionNeXt, a single-stream architecture. Its basic building block breaks down the parallel multi-branch depth-wise convolutions with descending scales of k x k kernels into a cascade of two multi-branch depth-wise convolutions. The first multi-branch consists of parallel multi-scale 1 x k depth-wise convolutional layers followed by a similar multi-branch employing parallel multi-scale k x 1 depth-wise convolutional layers. This reduces computational and memory footprint while separating time and frequency processing of Mel-Spectrograms. The large kernels capture global frequencies and long activities, while small kernels get local frequencies and short activities. We also reparameterize the multi-branch design during inference to further boost speed without losing accuracy. Experiments show that AudioRepInceptionNeXt reduces parameters and computations by 50%+ and improves inference speed 1.28x over state-of-the-art CNNs like the Slow-Fast while maintaining comparable accuracy. It also learns robustly across a variety of audio recognition tasks. Codes are available at https://github.com/StevenLauHKHK/AudioRepInceptionNeXt.",
      "authors": [
        "Kin Wai Lau",
        "Yasar Abbas Ur Rehman",
        "Lai-Man Po"
      ],
      "published": "2024-04-21T06:33:04Z",
      "updated": "2024-04-21T06:33:04Z",
      "categories": [
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2404.13551v1",
      "landing_url": "https://arxiv.org/abs/2404.13551v1",
      "doi": "https://doi.org/10.48550/arXiv.2404.13551"
    },
    "queries": [
      {
        "anchor": "spoken language models",
        "search_term": "streaming architecture"
      },
      {
        "anchor": "synchronous dialogue",
        "search_term": "streaming architecture"
      },
      {
        "anchor": "turn-taking",
        "search_term": "streaming architecture"
      }
    ]
  },
  {
    "arxiv_id": "2404.14591",
    "anchor": "spoken language models",
    "search_term": "temporal dynamics",
    "search_record": {
      "id": "http://arxiv.org/abs/2404.14591v3",
      "title": "Predicting the Temporal Dynamics of Prosthetic Vision",
      "summary": "Retinal implants are a promising treatment option for degenerative retinal disease. While numerous models have been developed to simulate the appearance of elicited visual percepts (\"phosphenes\"), these models often either focus solely on spatial characteristics or inadequately capture the complex temporal dynamics observed in clinical trials, which vary heavily across implant technologies, subjects, and stimulus conditions. Here we introduce two computational models designed to accurately predict phosphene fading and persistence under varying stimulus conditions, cross-validated on behavioral data reported by nine users of the Argus II Retinal Prosthesis System. Both models segment the time course of phosphene perception into discrete intervals, decomposing phosphene fading and persistence into either sinusoidal or exponential components. Our spectral model demonstrates state-of-the-art predictions of phosphene intensity over time (r = 0.7 across all participants). Overall, this study lays the groundwork for enhancing prosthetic vision by improving our understanding of phosphene temporal dynamics.",
      "published": "2024-04-22T21:35:30Z"
    },
    "metadata": {
      "arxiv_id": "2404.14591",
      "title": "Predicting the Temporal Dynamics of Prosthetic Vision",
      "summary": "Retinal implants are a promising treatment option for degenerative retinal disease. While numerous models have been developed to simulate the appearance of elicited visual percepts (\"phosphenes\"), these models often either focus solely on spatial characteristics or inadequately capture the complex temporal dynamics observed in clinical trials, which vary heavily across implant technologies, subjects, and stimulus conditions. Here we introduce two computational models designed to accurately predict phosphene fading and persistence under varying stimulus conditions, cross-validated on behavioral data reported by nine users of the Argus II Retinal Prosthesis System. Both models segment the time course of phosphene perception into discrete intervals, decomposing phosphene fading and persistence into either sinusoidal or exponential components. Our spectral model demonstrates state-of-the-art predictions of phosphene intensity over time (r = 0.7 across all participants). Overall, this study lays the groundwork for enhancing prosthetic vision by improving our understanding of phosphene temporal dynamics.",
      "authors": [
        "Yuchen Hou",
        "Laya Pullela",
        "Jiaxin Su",
        "Sriya Aluru",
        "Shivani Sista",
        "Xiankun Lu",
        "Michael Beyeler"
      ],
      "published": "2024-04-22T21:35:30Z",
      "updated": "2024-05-02T00:56:00Z",
      "categories": [
        "cs.CE"
      ],
      "pdf_url": "https://arxiv.org/pdf/2404.14591v3",
      "landing_url": "https://arxiv.org/abs/2404.14591v3",
      "doi": "https://doi.org/10.48550/arXiv.2404.14591"
    },
    "queries": [
      {
        "anchor": "spoken language models",
        "search_term": "temporal dynamics"
      },
      {
        "anchor": "synchronous dialogue",
        "search_term": "temporal dynamics"
      },
      {
        "anchor": "turn-taking",
        "search_term": "temporal dynamics"
      }
    ]
  },
  {
    "arxiv_id": "2404.17216",
    "anchor": "spoken language models",
    "search_term": "data scarcity",
    "search_record": {
      "id": "http://arxiv.org/abs/2404.17216v1",
      "title": "Prompting Towards Alleviating Code-Switched Data Scarcity in Under-Resourced Languages with GPT as a Pivot",
      "summary": "Many multilingual communities, including numerous in Africa, frequently engage in code-switching during conversations. This behaviour stresses the need for natural language processing technologies adept at processing code-switched text. However, data scarcity, particularly in African languages, poses a significant challenge, as many are low-resourced and under-represented. In this study, we prompted GPT 3.5 to generate Afrikaans--English and Yoruba--English code-switched sentences, enhancing diversity using topic-keyword pairs, linguistic guidelines, and few-shot examples. Our findings indicate that the quality of generated sentences for languages using non-Latin scripts, like Yoruba, is considerably lower when compared with the high Afrikaans-English success rate. There is therefore a notable opportunity to refine prompting guidelines to yield sentences suitable for the fine-tuning of language models. We propose a framework for augmenting the diversity of synthetically generated code-switched data using GPT and propose leveraging this technology to mitigate data scarcity in low-resourced languages, underscoring the essential role of native speakers in this process.",
      "published": "2024-04-26T07:44:44Z"
    },
    "metadata": {
      "arxiv_id": "2404.17216",
      "title": "Prompting Towards Alleviating Code-Switched Data Scarcity in Under-Resourced Languages with GPT as a Pivot",
      "summary": "Many multilingual communities, including numerous in Africa, frequently engage in code-switching during conversations. This behaviour stresses the need for natural language processing technologies adept at processing code-switched text. However, data scarcity, particularly in African languages, poses a significant challenge, as many are low-resourced and under-represented. In this study, we prompted GPT 3.5 to generate Afrikaans--English and Yoruba--English code-switched sentences, enhancing diversity using topic-keyword pairs, linguistic guidelines, and few-shot examples. Our findings indicate that the quality of generated sentences for languages using non-Latin scripts, like Yoruba, is considerably lower when compared with the high Afrikaans-English success rate. There is therefore a notable opportunity to refine prompting guidelines to yield sentences suitable for the fine-tuning of language models. We propose a framework for augmenting the diversity of synthetically generated code-switched data using GPT and propose leveraging this technology to mitigate data scarcity in low-resourced languages, underscoring the essential role of native speakers in this process.",
      "authors": [
        "Michelle Terblanche",
        "Kayode Olaleye",
        "Vukosi Marivate"
      ],
      "published": "2024-04-26T07:44:44Z",
      "updated": "2024-04-26T07:44:44Z",
      "categories": [
        "cs.CL"
      ],
      "pdf_url": "https://arxiv.org/pdf/2404.17216v1",
      "landing_url": "https://arxiv.org/abs/2404.17216v1",
      "doi": "https://doi.org/10.48550/arXiv.2404.17216"
    },
    "queries": [
      {
        "anchor": "spoken language models",
        "search_term": "data scarcity"
      },
      {
        "anchor": "synchronous dialogue",
        "search_term": "data scarcity"
      },
      {
        "anchor": "turn-taking",
        "search_term": "data scarcity"
      }
    ]
  },
  {
    "arxiv_id": "2404.19246",
    "anchor": "synchronous dialogue",
    "search_term": "real-time interaction",
    "search_record": {
      "id": "http://arxiv.org/abs/2404.19246v1",
      "title": "Logistic Map Pseudo Random Number Generator in FPGA",
      "summary": "This project develops a pseudo-random number generator (PRNG) using the logistic map, implemented in Verilog HDL on an FPGA and processes its output through a Central Limit Theorem (CLT) function to achieve a Gaussian distribution. The system integrates additional FPGA modules for real-time interaction and visualisation, including a clock generator, UART interface, XADC, and a 7-segment display driver. These components facilitate the direct display of PRNG values on the FPGA and the transmission of data to a laptop for histogram analysis, verifying the Gaussian nature of the output. This approach demonstrates the practical application of chaotic systems for generating Gaussian-distributed pseudo-random numbers in digital hardware, highlighting the logistic map's potential in PRNG design.",
      "published": "2024-04-30T04:03:31Z"
    },
    "metadata": {
      "arxiv_id": "2404.19246",
      "title": "Logistic Map Pseudo Random Number Generator in FPGA",
      "summary": "This project develops a pseudo-random number generator (PRNG) using the logistic map, implemented in Verilog HDL on an FPGA and processes its output through a Central Limit Theorem (CLT) function to achieve a Gaussian distribution. The system integrates additional FPGA modules for real-time interaction and visualisation, including a clock generator, UART interface, XADC, and a 7-segment display driver. These components facilitate the direct display of PRNG values on the FPGA and the transmission of data to a laptop for histogram analysis, verifying the Gaussian nature of the output. This approach demonstrates the practical application of chaotic systems for generating Gaussian-distributed pseudo-random numbers in digital hardware, highlighting the logistic map's potential in PRNG design.",
      "authors": [
        "Mateo Jalen Andrew Calderon",
        "Lee Jun Lei Lucas",
        "Syarifuddin Azhar Bin Rosli",
        "Stephanie See Hui Ying",
        "Jarell Lim En Yu",
        "Maoyang Xiang",
        "T. Hui Teo"
      ],
      "published": "2024-04-30T04:03:31Z",
      "updated": "2024-04-30T04:03:31Z",
      "categories": [
        "cs.CR",
        "cs.AR"
      ],
      "pdf_url": "https://arxiv.org/pdf/2404.19246v1",
      "landing_url": "https://arxiv.org/abs/2404.19246v1",
      "doi": "https://doi.org/10.48550/arXiv.2404.19246"
    },
    "queries": [
      {
        "anchor": "synchronous dialogue",
        "search_term": "real-time interaction"
      }
    ]
  },
  {
    "arxiv_id": "2405.03111",
    "anchor": "spoken language models",
    "search_term": "temporal dynamics",
    "search_record": {
      "id": "http://arxiv.org/abs/2405.03111v4",
      "title": "Temporal Dynamics of Emotion and Cognition in Human Translation: Integrating the Task Segment Framework and the HOF Taxonomy",
      "summary": "The article develops a generative model of the human translating mind, grounded in empirical translation process data. It posits that three embedded processing layers unfold concurrently in the human mind, and their traces are detectable in behavioral data: sequences of routinized/automated processes are observable in fluent translation production, cognitive/reflective thoughts lead to longer keystroke pauses, while affective/emotional states may be identified through characteristic typing and gazing patterns. Utilizing data from the CRITT Translation Process Research Database (TPR-DB), the article illustrates how the temporal structure of keystroke and gaze data can be related to the three assumed hidden mental processing strata. The article relates this embedded generative model to various theoretical frameworks, dual-process theories and Robinson's (2023) ideosomatic theory of translation, opening exciting new theoretical horizons for Cognitive Translation Studies, grounded in empirical data and evaluation.",
      "published": "2024-05-06T02:07:13Z"
    },
    "metadata": {
      "arxiv_id": "2405.03111",
      "title": "Temporal Dynamics of Emotion and Cognition in Human Translation: Integrating the Task Segment Framework and the HOF Taxonomy",
      "summary": "The article develops a generative model of the human translating mind, grounded in empirical translation process data. It posits that three embedded processing layers unfold concurrently in the human mind, and their traces are detectable in behavioral data: sequences of routinized/automated processes are observable in fluent translation production, cognitive/reflective thoughts lead to longer keystroke pauses, while affective/emotional states may be identified through characteristic typing and gazing patterns. Utilizing data from the CRITT Translation Process Research Database (TPR-DB), the article illustrates how the temporal structure of keystroke and gaze data can be related to the three assumed hidden mental processing strata. The article relates this embedded generative model to various theoretical frameworks, dual-process theories and Robinson's (2023) ideosomatic theory of translation, opening exciting new theoretical horizons for Cognitive Translation Studies, grounded in empirical data and evaluation.",
      "authors": [
        "Michael Carl"
      ],
      "published": "2024-05-06T02:07:13Z",
      "updated": "2025-05-23T06:48:19Z",
      "categories": [
        "cs.CL"
      ],
      "pdf_url": "https://arxiv.org/pdf/2405.03111v4",
      "landing_url": "https://arxiv.org/abs/2405.03111v4",
      "doi": "https://doi.org/10.1515/dsll-2025-0002"
    },
    "queries": [
      {
        "anchor": "spoken language models",
        "search_term": "temporal dynamics"
      },
      {
        "anchor": "synchronous dialogue",
        "search_term": "temporal dynamics"
      },
      {
        "anchor": "turn-taking",
        "search_term": "temporal dynamics"
      }
    ]
  },
  {
    "arxiv_id": "2405.03376",
    "anchor": "spoken language models",
    "search_term": "neural codec",
    "search_record": {
      "id": "http://arxiv.org/abs/2405.03376v2",
      "title": "CRA5: Extreme Compression of ERA5 for Portable Global Climate and Weather Research via an Efficient Variational Transformer",
      "summary": "The advent of data-driven weather forecasting models, which learn from hundreds of terabytes (TB) of reanalysis data, has significantly advanced forecasting capabilities. However, the substantial costs associated with data storage and transmission present a major challenge for data providers and users, affecting resource-constrained researchers and limiting their accessibility to participate in AI-based meteorological research. To mitigate this issue, we introduce an efficient neural codec, the Variational Autoencoder Transformer (VAEformer), for extreme compression of climate data to significantly reduce data storage cost, making AI-based meteorological research portable to researchers. Our approach diverges from recent complex neural codecs by utilizing a low-complexity Auto-Encoder transformer. This encoder produces a quantized latent representation through variance inference, which reparameterizes the latent space as a Gaussian distribution. This method improves the estimation of distributions for cross-entropy coding. Extensive experiments demonstrate that our VAEformer outperforms existing state-of-the-art compression methods in the context of climate data. By applying our VAEformer, we compressed the most popular ERA5 climate dataset (226 TB) into a new dataset, CRA5 (0.7 TB). This translates to a compression ratio of over 300 while retaining the dataset's utility for accurate scientific analysis. Further, downstream experiments show that global weather forecasting models trained on the compact CRA5 dataset achieve forecasting accuracy comparable to the model trained on the original dataset. Code, the CRA5 dataset, and the pre-trained model are available at https://github.com/taohan10200/CRA5.",
      "published": "2024-05-06T11:30:55Z"
    },
    "metadata": {
      "arxiv_id": "2405.03376",
      "title": "CRA5: Extreme Compression of ERA5 for Portable Global Climate and Weather Research via an Efficient Variational Transformer",
      "summary": "The advent of data-driven weather forecasting models, which learn from hundreds of terabytes (TB) of reanalysis data, has significantly advanced forecasting capabilities. However, the substantial costs associated with data storage and transmission present a major challenge for data providers and users, affecting resource-constrained researchers and limiting their accessibility to participate in AI-based meteorological research. To mitigate this issue, we introduce an efficient neural codec, the Variational Autoencoder Transformer (VAEformer), for extreme compression of climate data to significantly reduce data storage cost, making AI-based meteorological research portable to researchers. Our approach diverges from recent complex neural codecs by utilizing a low-complexity Auto-Encoder transformer. This encoder produces a quantized latent representation through variance inference, which reparameterizes the latent space as a Gaussian distribution. This method improves the estimation of distributions for cross-entropy coding. Extensive experiments demonstrate that our VAEformer outperforms existing state-of-the-art compression methods in the context of climate data. By applying our VAEformer, we compressed the most popular ERA5 climate dataset (226 TB) into a new dataset, CRA5 (0.7 TB). This translates to a compression ratio of over 300 while retaining the dataset's utility for accurate scientific analysis. Further, downstream experiments show that global weather forecasting models trained on the compact CRA5 dataset achieve forecasting accuracy comparable to the model trained on the original dataset. Code, the CRA5 dataset, and the pre-trained model are available at https://github.com/taohan10200/CRA5.",
      "authors": [
        "Tao Han",
        "Zhenghao Chen",
        "Song Guo",
        "Wanghan Xu",
        "Lei Bai"
      ],
      "published": "2024-05-06T11:30:55Z",
      "updated": "2024-05-08T03:27:04Z",
      "categories": [
        "cs.LG",
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2405.03376v2",
      "landing_url": "https://arxiv.org/abs/2405.03376v2",
      "doi": "https://doi.org/10.48550/arXiv.2405.03376"
    },
    "queries": [
      {
        "anchor": "spoken language models",
        "search_term": "neural codec"
      },
      {
        "anchor": "synchronous dialogue",
        "search_term": "neural codec"
      }
    ]
  },
  {
    "arxiv_id": "2405.04274",
    "anchor": "synchronous dialogue",
    "search_term": "neural codec",
    "search_record": {
      "id": "http://arxiv.org/abs/2405.04274v2",
      "title": "Group-aware Parameter-efficient Updating for Content-Adaptive Neural Video Compression",
      "summary": "Content-adaptive compression is crucial for enhancing the adaptability of the pre-trained neural codec for various contents. Although these methods have been very practical in neural image compression (NIC), their application in neural video compression (NVC) is still limited due to two main aspects: 1), video compression relies heavily on temporal redundancy, therefore updating just one or a few frames can lead to significant errors accumulating over time; 2), NVC frameworks are generally more complex, with many large components that are not easy to update quickly during encoding. To address the previously mentioned challenges, we have developed a content-adaptive NVC technique called Group-aware Parameter-Efficient Updating (GPU). Initially, to minimize error accumulation, we adopt a group-aware approach for updating encoder parameters. This involves adopting a patch-based Group of Pictures (GoP) training strategy to segment a video into patch-based GoPs, which will be updated to facilitate a globally optimized domain-transferable solution. Subsequently, we introduce a parameter-efficient delta-tuning strategy, which is achieved by integrating several light-weight adapters into each coding component of the encoding process by both serial and parallel configuration. Such architecture-agnostic modules stimulate the components with large parameters, thereby reducing both the update cost and the encoding time. We incorporate our GPU into the latest NVC framework and conduct comprehensive experiments, whose results showcase outstanding video compression efficiency across four video benchmarks and adaptability of one medical image benchmark.",
      "published": "2024-05-07T12:42:23Z"
    },
    "metadata": {
      "arxiv_id": "2405.04274",
      "title": "Group-aware Parameter-efficient Updating for Content-Adaptive Neural Video Compression",
      "summary": "Content-adaptive compression is crucial for enhancing the adaptability of the pre-trained neural codec for various contents. Although these methods have been very practical in neural image compression (NIC), their application in neural video compression (NVC) is still limited due to two main aspects: 1), video compression relies heavily on temporal redundancy, therefore updating just one or a few frames can lead to significant errors accumulating over time; 2), NVC frameworks are generally more complex, with many large components that are not easy to update quickly during encoding. To address the previously mentioned challenges, we have developed a content-adaptive NVC technique called Group-aware Parameter-Efficient Updating (GPU). Initially, to minimize error accumulation, we adopt a group-aware approach for updating encoder parameters. This involves adopting a patch-based Group of Pictures (GoP) training strategy to segment a video into patch-based GoPs, which will be updated to facilitate a globally optimized domain-transferable solution. Subsequently, we introduce a parameter-efficient delta-tuning strategy, which is achieved by integrating several light-weight adapters into each coding component of the encoding process by both serial and parallel configuration. Such architecture-agnostic modules stimulate the components with large parameters, thereby reducing both the update cost and the encoding time. We incorporate our GPU into the latest NVC framework and conduct comprehensive experiments, whose results showcase outstanding video compression efficiency across four video benchmarks and adaptability of one medical image benchmark.",
      "authors": [
        "Zhenghao Chen",
        "Luping Zhou",
        "Zhihao Hu",
        "Dong Xu"
      ],
      "published": "2024-05-07T12:42:23Z",
      "updated": "2024-09-04T05:24:25Z",
      "categories": [
        "eess.IV",
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2405.04274v2",
      "landing_url": "https://arxiv.org/abs/2405.04274v2",
      "doi": "https://doi.org/10.1145/3664647.3680943"
    },
    "queries": [
      {
        "anchor": "synchronous dialogue",
        "search_term": "neural codec"
      }
    ]
  },
  {
    "arxiv_id": "2405.04880",
    "anchor": "synchronous dialogue",
    "search_term": "neural codec",
    "search_record": {
      "id": "http://arxiv.org/abs/2405.04880v3",
      "title": "The Codecfake Dataset and Countermeasures for the Universally Detection of Deepfake Audio",
      "summary": "With the proliferation of Audio Language Model (ALM) based deepfake audio, there is an urgent need for generalized detection methods. ALM-based deepfake audio currently exhibits widespread, high deception, and type versatility, posing a significant challenge to current audio deepfake detection (ADD) models trained solely on vocoded data. To effectively detect ALM-based deepfake audio, we focus on the mechanism of the ALM-based audio generation method, the conversion from neural codec to waveform. We initially constructed the Codecfake dataset, an open-source, large-scale collection comprising over 1 million audio samples in both English and Chinese, focus on ALM-based audio detection. As countermeasure, to achieve universal detection of deepfake audio and tackle domain ascent bias issue of original sharpness aware minimization (SAM), we propose the CSAM strategy to learn a domain balanced and generalized minima. In our experiments, we first demonstrate that ADD model training with the Codecfake dataset can effectively detects ALM-based audio. Furthermore, our proposed generalization countermeasure yields the lowest average equal error rate (EER) of 0.616% across all test conditions compared to baseline models. The dataset and associated code are available online.",
      "published": "2024-05-08T08:28:40Z"
    },
    "metadata": {
      "arxiv_id": "2405.04880",
      "title": "The Codecfake Dataset and Countermeasures for the Universally Detection of Deepfake Audio",
      "summary": "With the proliferation of Audio Language Model (ALM) based deepfake audio, there is an urgent need for generalized detection methods. ALM-based deepfake audio currently exhibits widespread, high deception, and type versatility, posing a significant challenge to current audio deepfake detection (ADD) models trained solely on vocoded data. To effectively detect ALM-based deepfake audio, we focus on the mechanism of the ALM-based audio generation method, the conversion from neural codec to waveform. We initially constructed the Codecfake dataset, an open-source, large-scale collection comprising over 1 million audio samples in both English and Chinese, focus on ALM-based audio detection. As countermeasure, to achieve universal detection of deepfake audio and tackle domain ascent bias issue of original sharpness aware minimization (SAM), we propose the CSAM strategy to learn a domain balanced and generalized minima. In our experiments, we first demonstrate that ADD model training with the Codecfake dataset can effectively detects ALM-based audio. Furthermore, our proposed generalization countermeasure yields the lowest average equal error rate (EER) of 0.616% across all test conditions compared to baseline models. The dataset and associated code are available online.",
      "authors": [
        "Yuankun Xie",
        "Yi Lu",
        "Ruibo Fu",
        "Zhengqi Wen",
        "Zhiyong Wang",
        "Jianhua Tao",
        "Xin Qi",
        "Xiaopeng Wang",
        "Yukun Liu",
        "Haonan Cheng",
        "Long Ye",
        "Yi Sun"
      ],
      "published": "2024-05-08T08:28:40Z",
      "updated": "2024-12-25T07:30:50Z",
      "categories": [
        "cs.SD",
        "cs.AI",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2405.04880v3",
      "landing_url": "https://arxiv.org/abs/2405.04880v3",
      "doi": "https://doi.org/10.48550/arXiv.2405.04880"
    },
    "queries": [
      {
        "anchor": "synchronous dialogue",
        "search_term": "neural codec"
      }
    ]
  },
  {
    "arxiv_id": "2405.05966",
    "anchor": "synchronous dialogue",
    "search_term": "semantic coherence",
    "search_record": {
      "id": "http://arxiv.org/abs/2405.05966v5",
      "title": "Natural Language Processing RELIES on Linguistics",
      "summary": "Large Language Models (LLMs) have become capable of generating highly fluent text in certain languages, without modules specially designed to capture grammar or semantic coherence. What does this mean for the future of linguistic expertise in NLP? We highlight several aspects in which NLP (still) relies on linguistics, or where linguistic thinking can illuminate new directions. We argue our case around the acronym RELIES that encapsulates six major facets where linguistics contributes to NLP: Resources, Evaluation, Low-resource settings, Interpretability, Explanation, and the Study of language. This list is not exhaustive, nor is linguistics the main point of reference for every effort under these themes; but at a macro level, these facets highlight the enduring importance of studying machine systems vis-à-vis systems of human language.",
      "published": "2024-05-09T17:59:32Z"
    },
    "metadata": {
      "arxiv_id": "2405.05966",
      "title": "Natural Language Processing RELIES on Linguistics",
      "summary": "Large Language Models (LLMs) have become capable of generating highly fluent text in certain languages, without modules specially designed to capture grammar or semantic coherence. What does this mean for the future of linguistic expertise in NLP? We highlight several aspects in which NLP (still) relies on linguistics, or where linguistic thinking can illuminate new directions. We argue our case around the acronym RELIES that encapsulates six major facets where linguistics contributes to NLP: Resources, Evaluation, Low-resource settings, Interpretability, Explanation, and the Study of language. This list is not exhaustive, nor is linguistics the main point of reference for every effort under these themes; but at a macro level, these facets highlight the enduring importance of studying machine systems vis-à-vis systems of human language.",
      "authors": [
        "Juri Opitz",
        "Shira Wein",
        "Nathan Schneider"
      ],
      "published": "2024-05-09T17:59:32Z",
      "updated": "2025-10-16T11:35:21Z",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2405.05966v5",
      "landing_url": "https://arxiv.org/abs/2405.05966v5",
      "doi": "https://doi.org/10.1162/coli_a_00560"
    },
    "queries": [
      {
        "anchor": "synchronous dialogue",
        "search_term": "semantic coherence"
      }
    ]
  },
  {
    "arxiv_id": "2405.07342",
    "anchor": "synchronous dialogue",
    "search_term": "synchronous data",
    "search_record": {
      "id": "http://arxiv.org/abs/2405.07342v1",
      "title": "AquaIntellect: A Semantic Self-learning Framework for Underwater Internet of Things Connectivity",
      "summary": "The emerging paradigm of Non-Conventional Internet of Things (NC IoT), which is focused on the usefulness of information as opposed to the notion of high volume data collection and transmission, will be an important and dominant part of human life in the near future. This paper proposes a novel semantic-based approach for addressing the unique challenges posed by underwater NC IoT. We present an intelligent sensing strategy for exploring the semantics of the underwater environment by judiciously selecting the data to transmit, thereby minimizing redundancy for utmost relevant data transmission. We introduce an evolutionary function for the selection of the semantic-empowered messages relevant to the specific task within a minimum Age of Information (AoI), a freshness metric of the collected information, and for monitoring the underwater environment for performance optimization. A DNN-empowered Bayesian integrated with an adaptive surrogate model optimization will determine the optimal placement strategy of the sensors and the uncertainty level of the underwater landscape. An Adaptive Expected Improvement (AEI) mechanism is introduced to predict the optimal arrival rate for enabling a synchronized data sensing and transmission ecosystem, ensuring efficiency and timeliness. Simulation results show that the proposed solution outperforms conventional approaches.",
      "published": "2024-05-12T17:39:09Z"
    },
    "metadata": {
      "arxiv_id": "2405.07342",
      "title": "AquaIntellect: A Semantic Self-learning Framework for Underwater Internet of Things Connectivity",
      "summary": "The emerging paradigm of Non-Conventional Internet of Things (NC IoT), which is focused on the usefulness of information as opposed to the notion of high volume data collection and transmission, will be an important and dominant part of human life in the near future. This paper proposes a novel semantic-based approach for addressing the unique challenges posed by underwater NC IoT. We present an intelligent sensing strategy for exploring the semantics of the underwater environment by judiciously selecting the data to transmit, thereby minimizing redundancy for utmost relevant data transmission. We introduce an evolutionary function for the selection of the semantic-empowered messages relevant to the specific task within a minimum Age of Information (AoI), a freshness metric of the collected information, and for monitoring the underwater environment for performance optimization. A DNN-empowered Bayesian integrated with an adaptive surrogate model optimization will determine the optimal placement strategy of the sensors and the uncertainty level of the underwater landscape. An Adaptive Expected Improvement (AEI) mechanism is introduced to predict the optimal arrival rate for enabling a synchronized data sensing and transmission ecosystem, ensuring efficiency and timeliness. Simulation results show that the proposed solution outperforms conventional approaches.",
      "authors": [
        "Ananya Hazarika",
        "Mehdi Rahmati"
      ],
      "published": "2024-05-12T17:39:09Z",
      "updated": "2024-05-12T17:39:09Z",
      "categories": [
        "eess.SP",
        "eess.SY"
      ],
      "pdf_url": "https://arxiv.org/pdf/2405.07342v1",
      "landing_url": "https://arxiv.org/abs/2405.07342v1",
      "doi": "https://doi.org/10.1109/VCC60689.2023.10474835"
    },
    "queries": [
      {
        "anchor": "synchronous dialogue",
        "search_term": "synchronous data"
      }
    ]
  },
  {
    "arxiv_id": "2405.08237",
    "anchor": "synchronous dialogue",
    "search_term": "temporal dynamics",
    "search_record": {
      "id": "http://arxiv.org/abs/2405.08237v1",
      "title": "A predictive learning model can simulate temporal dynamics and context effects found in neural representations of continuous speech",
      "summary": "Speech perception involves storing and integrating sequentially presented items. Recent work in cognitive neuroscience has identified temporal and contextual characteristics in humans' neural encoding of speech that may facilitate this temporal processing. In this study, we simulated similar analyses with representations extracted from a computational model that was trained on unlabelled speech with the learning objective of predicting upcoming acoustics. Our simulations revealed temporal dynamics similar to those in brain signals, implying that these properties can arise without linguistic knowledge. Another property shared between brains and the model is that the encoding patterns of phonemes support some degree of cross-context generalization. However, we found evidence that the effectiveness of these generalizations depends on the specific contexts, which suggests that this analysis alone is insufficient to support the presence of context-invariant encoding.",
      "published": "2024-05-13T23:36:19Z"
    },
    "metadata": {
      "arxiv_id": "2405.08237",
      "title": "A predictive learning model can simulate temporal dynamics and context effects found in neural representations of continuous speech",
      "summary": "Speech perception involves storing and integrating sequentially presented items. Recent work in cognitive neuroscience has identified temporal and contextual characteristics in humans' neural encoding of speech that may facilitate this temporal processing. In this study, we simulated similar analyses with representations extracted from a computational model that was trained on unlabelled speech with the learning objective of predicting upcoming acoustics. Our simulations revealed temporal dynamics similar to those in brain signals, implying that these properties can arise without linguistic knowledge. Another property shared between brains and the model is that the encoding patterns of phonemes support some degree of cross-context generalization. However, we found evidence that the effectiveness of these generalizations depends on the specific contexts, which suggests that this analysis alone is insufficient to support the presence of context-invariant encoding.",
      "authors": [
        "Oli Danyi Liu",
        "Hao Tang",
        "Naomi Feldman",
        "Sharon Goldwater"
      ],
      "published": "2024-05-13T23:36:19Z",
      "updated": "2024-05-13T23:36:19Z",
      "categories": [
        "cs.CL",
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2405.08237v1",
      "landing_url": "https://arxiv.org/abs/2405.08237v1",
      "doi": "https://doi.org/10.48550/arXiv.2405.08237"
    },
    "queries": [
      {
        "anchor": "synchronous dialogue",
        "search_term": "temporal dynamics"
      }
    ]
  },
  {
    "arxiv_id": "2405.08417",
    "anchor": "synchronous dialogue",
    "search_term": "neural codec",
    "search_record": {
      "id": "http://arxiv.org/abs/2405.08417v2",
      "title": "Neural Speech Coding for Real-time Communications using Constant Bitrate Scalar Quantization",
      "summary": "Neural audio coding has emerged as a vivid research direction by promising good audio quality at very low bitrates unachievable by classical coding techniques. Here, end-to-end trainable autoencoder-like models represent the state of the art, where a discrete representation in the bottleneck of the autoencoder is learned. This allows for efficient transmission of the input audio signal. The learned discrete representation of neural codecs is typically generated by applying a quantizer to the output of the neural encoder. In almost all state-of-the-art neural audio coding approaches, this quantizer is realized as a Vector Quantizer (VQ) and a lot of effort has been spent to alleviate drawbacks of this quantization technique when used together with a neural audio coder. In this paper, we propose and analyze simple alternatives to VQ, which are based on projected Scalar Quantization (SQ). These quantization techniques do not need any additional losses, scheduling parameters or codebook storage thereby simplifying the training of neural audio codecs. For real-time speech communication applications, these neural codecs are required to operate at low complexity, low latency and at low bitrates. We address those challenges by proposing a new causal network architecture that is based on SQ and a Short-Time Fourier Transform (STFT) representation. The proposed method performs particularly well in the very low complexity and low bitrate regime.",
      "published": "2024-05-14T08:23:30Z"
    },
    "metadata": {
      "arxiv_id": "2405.08417",
      "title": "Neural Speech Coding for Real-time Communications using Constant Bitrate Scalar Quantization",
      "summary": "Neural audio coding has emerged as a vivid research direction by promising good audio quality at very low bitrates unachievable by classical coding techniques. Here, end-to-end trainable autoencoder-like models represent the state of the art, where a discrete representation in the bottleneck of the autoencoder is learned. This allows for efficient transmission of the input audio signal. The learned discrete representation of neural codecs is typically generated by applying a quantizer to the output of the neural encoder. In almost all state-of-the-art neural audio coding approaches, this quantizer is realized as a Vector Quantizer (VQ) and a lot of effort has been spent to alleviate drawbacks of this quantization technique when used together with a neural audio coder. In this paper, we propose and analyze simple alternatives to VQ, which are based on projected Scalar Quantization (SQ). These quantization techniques do not need any additional losses, scheduling parameters or codebook storage thereby simplifying the training of neural audio codecs. For real-time speech communication applications, these neural codecs are required to operate at low complexity, low latency and at low bitrates. We address those challenges by proposing a new causal network architecture that is based on SQ and a Short-Time Fourier Transform (STFT) representation. The proposed method performs particularly well in the very low complexity and low bitrate regime.",
      "authors": [
        "Andreas Brendel",
        "Nicola Pia",
        "Kishan Gupta",
        "Lyonel Behringer",
        "Guillaume Fuchs",
        "Markus Multrus"
      ],
      "published": "2024-05-14T08:23:30Z",
      "updated": "2024-09-19T12:02:07Z",
      "categories": [
        "eess.AS",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2405.08417v2",
      "landing_url": "https://arxiv.org/abs/2405.08417v2",
      "doi": "https://doi.org/10.48550/arXiv.2405.08417"
    },
    "queries": [
      {
        "anchor": "synchronous dialogue",
        "search_term": "neural codec"
      }
    ]
  },
  {
    "arxiv_id": "2405.09768",
    "anchor": "spoken language models",
    "search_term": "discrete tokens",
    "search_record": {
      "id": "http://arxiv.org/abs/2405.09768v1",
      "title": "Evaluating Text-to-Speech Synthesis from a Large Discrete Token-based Speech Language Model",
      "summary": "Recent advances in generative language modeling applied to discrete speech tokens presented a new avenue for text-to-speech (TTS) synthesis. These speech language models (SLMs), similarly to their textual counterparts, are scalable, probabilistic, and context-aware. While they can produce diverse and natural outputs, they sometimes face issues such as unintelligibility and the inclusion of non-speech noises or hallucination. As the adoption of this innovative paradigm in speech synthesis increases, there is a clear need for an in-depth evaluation of its capabilities and limitations. In this paper, we evaluate TTS from a discrete token-based SLM, through both automatic metrics and listening tests. We examine five key dimensions: speaking style, intelligibility, speaker consistency, prosodic variation, spontaneous behaviour. Our results highlight the model's strength in generating varied prosody and spontaneous outputs. It is also rated higher in naturalness and context appropriateness in listening tests compared to a conventional TTS. However, the model's performance in intelligibility and speaker consistency lags behind traditional TTS. Additionally, we show that increasing the scale of SLMs offers a modest boost in robustness. Our findings aim to serve as a benchmark for future advancements in generative SLMs for speech synthesis.",
      "published": "2024-05-16T02:18:41Z"
    },
    "metadata": {
      "arxiv_id": "2405.09768",
      "title": "Evaluating Text-to-Speech Synthesis from a Large Discrete Token-based Speech Language Model",
      "summary": "Recent advances in generative language modeling applied to discrete speech tokens presented a new avenue for text-to-speech (TTS) synthesis. These speech language models (SLMs), similarly to their textual counterparts, are scalable, probabilistic, and context-aware. While they can produce diverse and natural outputs, they sometimes face issues such as unintelligibility and the inclusion of non-speech noises or hallucination. As the adoption of this innovative paradigm in speech synthesis increases, there is a clear need for an in-depth evaluation of its capabilities and limitations. In this paper, we evaluate TTS from a discrete token-based SLM, through both automatic metrics and listening tests. We examine five key dimensions: speaking style, intelligibility, speaker consistency, prosodic variation, spontaneous behaviour. Our results highlight the model's strength in generating varied prosody and spontaneous outputs. It is also rated higher in naturalness and context appropriateness in listening tests compared to a conventional TTS. However, the model's performance in intelligibility and speaker consistency lags behind traditional TTS. Additionally, we show that increasing the scale of SLMs offers a modest boost in robustness. Our findings aim to serve as a benchmark for future advancements in generative SLMs for speech synthesis.",
      "authors": [
        "Siyang Wang",
        "Éva Székely"
      ],
      "published": "2024-05-16T02:18:41Z",
      "updated": "2024-05-16T02:18:41Z",
      "categories": [
        "eess.AS",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2405.09768v1",
      "landing_url": "https://arxiv.org/abs/2405.09768v1",
      "doi": "https://doi.org/10.48550/arXiv.2405.09768"
    },
    "queries": [
      {
        "anchor": "spoken language models",
        "search_term": "discrete tokens"
      },
      {
        "anchor": "synchronous dialogue",
        "search_term": "discrete tokens"
      },
      {
        "anchor": "turn-taking",
        "search_term": "discrete tokens"
      }
    ]
  },
  {
    "arxiv_id": "2405.10122",
    "anchor": "spoken language models",
    "search_term": "semantic coherence",
    "search_record": {
      "id": "http://arxiv.org/abs/2405.10122v1",
      "title": "Generating Coherent Sequences of Visual Illustrations for Real-World Manual Tasks",
      "summary": "Multistep instructions, such as recipes and how-to guides, greatly benefit from visual aids, such as a series of images that accompany the instruction steps. While Large Language Models (LLMs) have become adept at generating coherent textual steps, Large Vision/Language Models (LVLMs) are less capable of generating accompanying image sequences. The most challenging aspect is that each generated image needs to adhere to the relevant textual step instruction, as well as be visually consistent with earlier images in the sequence. To address this problem, we propose an approach for generating consistent image sequences, which integrates a Latent Diffusion Model (LDM) with an LLM to transform the sequence into a caption to maintain the semantic coherence of the sequence. In addition, to maintain the visual coherence of the image sequence, we introduce a copy mechanism to initialise reverse diffusion processes with a latent vector iteration from a previously generated image from a relevant step. Both strategies will condition the reverse diffusion process on the sequence of instruction steps and tie the contents of the current image to previous instruction steps and corresponding images. Experiments show that the proposed approach is preferred by humans in 46.6% of the cases against 26.6% for the second best method. In addition, automatic metrics showed that the proposed method maintains semantic coherence and visual consistency across steps in both domains.",
      "published": "2024-05-16T14:22:20Z"
    },
    "metadata": {
      "arxiv_id": "2405.10122",
      "title": "Generating Coherent Sequences of Visual Illustrations for Real-World Manual Tasks",
      "summary": "Multistep instructions, such as recipes and how-to guides, greatly benefit from visual aids, such as a series of images that accompany the instruction steps. While Large Language Models (LLMs) have become adept at generating coherent textual steps, Large Vision/Language Models (LVLMs) are less capable of generating accompanying image sequences. The most challenging aspect is that each generated image needs to adhere to the relevant textual step instruction, as well as be visually consistent with earlier images in the sequence. To address this problem, we propose an approach for generating consistent image sequences, which integrates a Latent Diffusion Model (LDM) with an LLM to transform the sequence into a caption to maintain the semantic coherence of the sequence. In addition, to maintain the visual coherence of the image sequence, we introduce a copy mechanism to initialise reverse diffusion processes with a latent vector iteration from a previously generated image from a relevant step. Both strategies will condition the reverse diffusion process on the sequence of instruction steps and tie the contents of the current image to previous instruction steps and corresponding images. Experiments show that the proposed approach is preferred by humans in 46.6% of the cases against 26.6% for the second best method. In addition, automatic metrics showed that the proposed method maintains semantic coherence and visual consistency across steps in both domains.",
      "authors": [
        "João Bordalo",
        "Vasco Ramos",
        "Rodrigo Valério",
        "Diogo Glória-Silva",
        "Yonatan Bitton",
        "Michal Yarom",
        "Idan Szpektor",
        "Joao Magalhaes"
      ],
      "published": "2024-05-16T14:22:20Z",
      "updated": "2024-05-16T14:22:20Z",
      "categories": [
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2405.10122v1",
      "landing_url": "https://arxiv.org/abs/2405.10122v1",
      "doi": "https://doi.org/10.48550/arXiv.2405.10122"
    },
    "queries": [
      {
        "anchor": "spoken language models",
        "search_term": "semantic coherence"
      },
      {
        "anchor": "synchronous dialogue",
        "search_term": "semantic coherence"
      }
    ]
  },
  {
    "arxiv_id": "2405.13203",
    "anchor": "spoken language models",
    "search_term": "real-time interaction",
    "search_record": {
      "id": "http://arxiv.org/abs/2405.13203v1",
      "title": "Modeling Real-Time Interactive Conversations as Timed Diarized Transcripts",
      "summary": "Chatbots built upon language models have exploded in popularity, but they have largely been limited to synchronous, turn-by-turn dialogues. In this paper we present a simple yet general method to simulate real-time interactive conversations using pretrained text-only language models, by modeling timed diarized transcripts and decoding them with causal rejection sampling. We demonstrate the promise of this method with two case studies: instant messenger dialogues and spoken conversations, which require generation at about 30 tok/s and 20 tok/s respectively to maintain real-time interactivity. These capabilities can be added into language models using relatively little data and run on commodity hardware.",
      "published": "2024-05-21T21:14:31Z"
    },
    "metadata": {
      "arxiv_id": "2405.13203",
      "title": "Modeling Real-Time Interactive Conversations as Timed Diarized Transcripts",
      "summary": "Chatbots built upon language models have exploded in popularity, but they have largely been limited to synchronous, turn-by-turn dialogues. In this paper we present a simple yet general method to simulate real-time interactive conversations using pretrained text-only language models, by modeling timed diarized transcripts and decoding them with causal rejection sampling. We demonstrate the promise of this method with two case studies: instant messenger dialogues and spoken conversations, which require generation at about 30 tok/s and 20 tok/s respectively to maintain real-time interactivity. These capabilities can be added into language models using relatively little data and run on commodity hardware.",
      "authors": [
        "Garrett Tanzer",
        "Gustaf Ahdritz",
        "Luke Melas-Kyriazi"
      ],
      "published": "2024-05-21T21:14:31Z",
      "updated": "2024-05-21T21:14:31Z",
      "categories": [
        "cs.LG",
        "cs.CL"
      ],
      "pdf_url": "https://arxiv.org/pdf/2405.13203v1",
      "landing_url": "https://arxiv.org/abs/2405.13203v1",
      "doi": "https://doi.org/10.48550/arXiv.2405.13203"
    },
    "queries": [
      {
        "anchor": "spoken language models",
        "search_term": "real-time interaction"
      },
      {
        "anchor": "synchronous dialogue",
        "search_term": "real-time interaction"
      },
      {
        "anchor": "turn-taking",
        "search_term": "real-time interaction"
      }
    ]
  },
  {
    "arxiv_id": "2405.15901",
    "anchor": "synchronous dialogue",
    "search_term": "real-time interaction",
    "search_record": {
      "id": "http://arxiv.org/abs/2405.15901v1",
      "title": "A Survey on Application Layer Protocols for IoT Networks",
      "summary": "Nowadays, all sectors utilize devices that are part of the Internet of Things (IoT) for the purpose of connecting and exchanging information with other devices and systems over the Internet. This increases the diversity of devices and their working environments, which, in turn, creates new challenges, such as real-time interaction, security, interoperability, performance, and robustness of IoT systems. To address these, many applications protocols were adopted and developed for devices with constrained resources. This paper surveys communication protocols divided according to their goals along with their merits, demerits, and suitability towards IoT applications. We summarize the challenges of communication protocols as well as some relevant solutions.",
      "published": "2024-05-24T19:47:43Z"
    },
    "metadata": {
      "arxiv_id": "2405.15901",
      "title": "A Survey on Application Layer Protocols for IoT Networks",
      "summary": "Nowadays, all sectors utilize devices that are part of the Internet of Things (IoT) for the purpose of connecting and exchanging information with other devices and systems over the Internet. This increases the diversity of devices and their working environments, which, in turn, creates new challenges, such as real-time interaction, security, interoperability, performance, and robustness of IoT systems. To address these, many applications protocols were adopted and developed for devices with constrained resources. This paper surveys communication protocols divided according to their goals along with their merits, demerits, and suitability towards IoT applications. We summarize the challenges of communication protocols as well as some relevant solutions.",
      "authors": [
        "Fatma Hmissi",
        "Sofiane Ouni"
      ],
      "published": "2024-05-24T19:47:43Z",
      "updated": "2024-05-24T19:47:43Z",
      "categories": [
        "cs.DC"
      ],
      "pdf_url": "https://arxiv.org/pdf/2405.15901v1",
      "landing_url": "https://arxiv.org/abs/2405.15901v1",
      "doi": "https://doi.org/10.48550/arXiv.2405.15901"
    },
    "queries": [
      {
        "anchor": "synchronous dialogue",
        "search_term": "real-time interaction"
      }
    ]
  },
  {
    "arxiv_id": "2405.16881",
    "anchor": "spoken language models",
    "search_term": "half duplex",
    "search_record": {
      "id": "http://arxiv.org/abs/2405.16881v1",
      "title": "Half-duplex communication complexity with adversary can be less than the classical communication complexity",
      "summary": "Half-duplex communication complexity with adversary was defined in [Hoover, K., Impagliazzo, R., Mihajlin, I., Smal, A. V. Half-Duplex Communication Complexity, ISAAC 2018.] Half-duplex communication protocols generalize classical protocols defined by Andrew Yao in [Yao, A. C.-C. Some Complexity Questions Related to Distributive Computing (Preliminary Report), STOC 1979]. It has been unknown so far whether the communication complexities defined by these models are different or not. In the present paper we answer this question: we exhibit a function whose half-duplex communication complexity with adversary is strictly less than its classical communication complexity.",
      "published": "2024-05-27T06:53:46Z"
    },
    "metadata": {
      "arxiv_id": "2405.16881",
      "title": "Half-duplex communication complexity with adversary can be less than the classical communication complexity",
      "summary": "Half-duplex communication complexity with adversary was defined in [Hoover, K., Impagliazzo, R., Mihajlin, I., Smal, A. V. Half-Duplex Communication Complexity, ISAAC 2018.] Half-duplex communication protocols generalize classical protocols defined by Andrew Yao in [Yao, A. C.-C. Some Complexity Questions Related to Distributive Computing (Preliminary Report), STOC 1979]. It has been unknown so far whether the communication complexities defined by these models are different or not. In the present paper we answer this question: we exhibit a function whose half-duplex communication complexity with adversary is strictly less than its classical communication complexity.",
      "authors": [
        "Mikhail Dektiarev",
        "Nikolay Vereshchagin"
      ],
      "published": "2024-05-27T06:53:46Z",
      "updated": "2024-05-27T06:53:46Z",
      "categories": [
        "cs.CC",
        "cs.IT"
      ],
      "pdf_url": "https://arxiv.org/pdf/2405.16881v1",
      "landing_url": "https://arxiv.org/abs/2405.16881v1",
      "doi": "https://doi.org/10.48550/arXiv.2405.16881"
    },
    "queries": [
      {
        "anchor": "spoken language models",
        "search_term": "half duplex"
      },
      {
        "anchor": "synchronous dialogue",
        "search_term": "half duplex"
      },
      {
        "anchor": "turn-taking",
        "search_term": "half duplex"
      }
    ]
  },
  {
    "arxiv_id": "2405.17030",
    "anchor": "synchronous dialogue",
    "search_term": "synchronous data",
    "search_record": {
      "id": "http://arxiv.org/abs/2405.17030v1",
      "title": "SCaRL- A Synthetic Multi-Modal Dataset for Autonomous Driving",
      "summary": "We present a novel synthetically generated multi-modal dataset, SCaRL, to enable the training and validation of autonomous driving solutions. Multi-modal datasets are essential to attain the robustness and high accuracy required by autonomous systems in applications such as autonomous driving. As deep learning-based solutions are becoming more prevalent for object detection, classification, and tracking tasks, there is great demand for datasets combining camera, lidar, and radar sensors. Existing real/synthetic datasets for autonomous driving lack synchronized data collection from a complete sensor suite. SCaRL provides synchronized Synthetic data from RGB, semantic/instance, and depth Cameras; Range-Doppler-Azimuth/Elevation maps and raw data from Radar; and 3D point clouds/2D maps of semantic, depth and Doppler data from coherent Lidar. SCaRL is a large dataset based on the CARLA Simulator, which provides data for diverse, dynamic scenarios and traffic conditions. SCaRL is the first dataset to include synthetic synchronized data from coherent Lidar and MIMO radar sensors.\n  The dataset can be accessed here: https://fhr-ihs-sva.pages.fraunhofer.de/asp/scarl/",
      "published": "2024-05-27T10:31:26Z"
    },
    "metadata": {
      "arxiv_id": "2405.17030",
      "title": "SCaRL- A Synthetic Multi-Modal Dataset for Autonomous Driving",
      "summary": "We present a novel synthetically generated multi-modal dataset, SCaRL, to enable the training and validation of autonomous driving solutions. Multi-modal datasets are essential to attain the robustness and high accuracy required by autonomous systems in applications such as autonomous driving. As deep learning-based solutions are becoming more prevalent for object detection, classification, and tracking tasks, there is great demand for datasets combining camera, lidar, and radar sensors. Existing real/synthetic datasets for autonomous driving lack synchronized data collection from a complete sensor suite. SCaRL provides synchronized Synthetic data from RGB, semantic/instance, and depth Cameras; Range-Doppler-Azimuth/Elevation maps and raw data from Radar; and 3D point clouds/2D maps of semantic, depth and Doppler data from coherent Lidar. SCaRL is a large dataset based on the CARLA Simulator, which provides data for diverse, dynamic scenarios and traffic conditions. SCaRL is the first dataset to include synthetic synchronized data from coherent Lidar and MIMO radar sensors.\n  The dataset can be accessed here: https://fhr-ihs-sva.pages.fraunhofer.de/asp/scarl/",
      "authors": [
        "Avinash Nittur Ramesh",
        "Aitor Correas-Serrano",
        "María González-Huici"
      ],
      "published": "2024-05-27T10:31:26Z",
      "updated": "2024-05-27T10:31:26Z",
      "categories": [
        "cs.CV",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2405.17030v1",
      "landing_url": "https://arxiv.org/abs/2405.17030v1",
      "doi": "https://doi.org/10.48550/arXiv.2405.17030"
    },
    "queries": [
      {
        "anchor": "synchronous dialogue",
        "search_term": "synchronous data"
      }
    ]
  },
  {
    "arxiv_id": "2405.19846",
    "anchor": "synchronous dialogue",
    "search_term": "semantic coherence",
    "search_record": {
      "id": "http://arxiv.org/abs/2405.19846v7",
      "title": "Quest: Query-centric Data Synthesis Approach for Long-context Scaling of Large Language Model",
      "summary": "Recent advancements in large language models (LLMs) have highlighted the importance of extending context lengths for handling complex tasks. While traditional methods for training on long contexts often use filtered long documents, these approaches lead to domain imbalances, limiting model performance. To address this, techniques like random document concatenation (Standard) and similarity-based methods (KNN, ICLM) have been developed. However, they either sacrifice semantic coherence or diversity. To balance both aspects, we introduce Quest, a query-centric data synthesis method aggregating semantically relevant yet diverse documents. Quest uses a generative model to predict potential queries for each document, grouping documents with similar queries and keywords. Extensive experiments demonstrate Quest's superior performance on long-context tasks, achieving remarkable results with context lengths of up to 1M tokens and confirming its scalability across various model sizes.",
      "published": "2024-05-30T08:50:55Z"
    },
    "metadata": {
      "arxiv_id": "2405.19846",
      "title": "Quest: Query-centric Data Synthesis Approach for Long-context Scaling of Large Language Model",
      "summary": "Recent advancements in large language models (LLMs) have highlighted the importance of extending context lengths for handling complex tasks. While traditional methods for training on long contexts often use filtered long documents, these approaches lead to domain imbalances, limiting model performance. To address this, techniques like random document concatenation (Standard) and similarity-based methods (KNN, ICLM) have been developed. However, they either sacrifice semantic coherence or diversity. To balance both aspects, we introduce Quest, a query-centric data synthesis method aggregating semantically relevant yet diverse documents. Quest uses a generative model to predict potential queries for each document, grouping documents with similar queries and keywords. Extensive experiments demonstrate Quest's superior performance on long-context tasks, achieving remarkable results with context lengths of up to 1M tokens and confirming its scalability across various model sizes.",
      "authors": [
        "Chaochen Gao",
        "Xing Wu",
        "Qi Fu",
        "Songlin Hu"
      ],
      "published": "2024-05-30T08:50:55Z",
      "updated": "2025-02-11T06:22:30Z",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2405.19846v7",
      "landing_url": "https://arxiv.org/abs/2405.19846v7",
      "doi": "https://doi.org/10.48550/arXiv.2405.19846"
    },
    "queries": [
      {
        "anchor": "synchronous dialogue",
        "search_term": "semantic coherence"
      }
    ]
  },
  {
    "arxiv_id": "2405.20336",
    "anchor": "synchronous dialogue",
    "search_term": "audio tokens",
    "search_record": {
      "id": "http://arxiv.org/abs/2405.20336v3",
      "title": "RapVerse: Coherent Vocals and Whole-Body Motions Generations from Text",
      "summary": "In this work, we introduce a challenging task for simultaneously generating 3D holistic body motions and singing vocals directly from textual lyrics inputs, advancing beyond existing works that typically address these two modalities in isolation. To facilitate this, we first collect the RapVerse dataset, a large dataset containing synchronous rapping vocals, lyrics, and high-quality 3D holistic body meshes. With the RapVerse dataset, we investigate the extent to which scaling autoregressive multimodal transformers across language, audio, and motion can enhance the coherent and realistic generation of vocals and whole-body human motions. For modality unification, a vector-quantized variational autoencoder is employed to encode whole-body motion sequences into discrete motion tokens, while a vocal-to-unit model is leveraged to obtain quantized audio tokens preserving content, prosodic information and singer identity. By jointly performing transformer modeling on these three modalities in a unified way, our framework ensures a seamless and realistic blend of vocals and human motions. Extensive experiments demonstrate that our unified generation framework not only produces coherent and realistic singing vocals alongside human motions directly from textual inputs, but also rivals the performance of specialized single-modality generation systems, establishing new benchmarks for joint vocal-motion generation.",
      "published": "2024-05-30T17:59:39Z"
    },
    "metadata": {
      "arxiv_id": "2405.20336",
      "title": "RapVerse: Coherent Vocals and Whole-Body Motions Generations from Text",
      "summary": "In this work, we introduce a challenging task for simultaneously generating 3D holistic body motions and singing vocals directly from textual lyrics inputs, advancing beyond existing works that typically address these two modalities in isolation. To facilitate this, we first collect the RapVerse dataset, a large dataset containing synchronous rapping vocals, lyrics, and high-quality 3D holistic body meshes. With the RapVerse dataset, we investigate the extent to which scaling autoregressive multimodal transformers across language, audio, and motion can enhance the coherent and realistic generation of vocals and whole-body human motions. For modality unification, a vector-quantized variational autoencoder is employed to encode whole-body motion sequences into discrete motion tokens, while a vocal-to-unit model is leveraged to obtain quantized audio tokens preserving content, prosodic information and singer identity. By jointly performing transformer modeling on these three modalities in a unified way, our framework ensures a seamless and realistic blend of vocals and human motions. Extensive experiments demonstrate that our unified generation framework not only produces coherent and realistic singing vocals alongside human motions directly from textual inputs, but also rivals the performance of specialized single-modality generation systems, establishing new benchmarks for joint vocal-motion generation.",
      "authors": [
        "Jiaben Chen",
        "Xin Yan",
        "Yihang Chen",
        "Siyuan Cen",
        "Zixin Wang",
        "Qinwei Ma",
        "Haoyu Zhen",
        "Kaizhi Qian",
        "Lie Lu",
        "Chuang Gan"
      ],
      "published": "2024-05-30T17:59:39Z",
      "updated": "2025-12-14T23:06:36Z",
      "categories": [
        "cs.CV",
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2405.20336v3",
      "landing_url": "https://arxiv.org/abs/2405.20336v3",
      "doi": "https://doi.org/10.48550/arXiv.2405.20336"
    },
    "queries": [
      {
        "anchor": "synchronous dialogue",
        "search_term": "audio tokens"
      }
    ]
  },
  {
    "arxiv_id": "2406.02092",
    "anchor": "synchronous dialogue",
    "search_term": "neural codec",
    "search_record": {
      "id": "http://arxiv.org/abs/2406.02092v1",
      "title": "MaskSR: Masked Language Model for Full-band Speech Restoration",
      "summary": "Speech restoration aims at restoring high quality speech in the presence of a diverse set of distortions. Although several deep learning paradigms have been studied for this task, the power of the recently emerging language models has not been fully explored. In this paper, we propose MaskSR, a masked language model capable of restoring full-band 44.1 kHz speech jointly considering noise, reverb, clipping, and low bandwidth. MaskSR works with discrete acoustic tokens extracted using a pre-trained neural codec. During training, MaskSR is optimized to predict randomly masked tokens extracted from the high quality target speech, conditioned on the corrupted speech with various distortions. During inference, MaskSR reconstructs the target speech tokens with efficient iterative sampling. Extensive experiments show that MaskSR obtains competitive results on both the full-band speech restoration task and also on sub-tasks compared with a wide range of models.",
      "published": "2024-06-04T08:23:57Z"
    },
    "metadata": {
      "arxiv_id": "2406.02092",
      "title": "MaskSR: Masked Language Model for Full-band Speech Restoration",
      "summary": "Speech restoration aims at restoring high quality speech in the presence of a diverse set of distortions. Although several deep learning paradigms have been studied for this task, the power of the recently emerging language models has not been fully explored. In this paper, we propose MaskSR, a masked language model capable of restoring full-band 44.1 kHz speech jointly considering noise, reverb, clipping, and low bandwidth. MaskSR works with discrete acoustic tokens extracted using a pre-trained neural codec. During training, MaskSR is optimized to predict randomly masked tokens extracted from the high quality target speech, conditioned on the corrupted speech with various distortions. During inference, MaskSR reconstructs the target speech tokens with efficient iterative sampling. Extensive experiments show that MaskSR obtains competitive results on both the full-band speech restoration task and also on sub-tasks compared with a wide range of models.",
      "authors": [
        "Xu Li",
        "Qirui Wang",
        "Xiaoyu Liu"
      ],
      "published": "2024-06-04T08:23:57Z",
      "updated": "2024-06-04T08:23:57Z",
      "categories": [
        "cs.SD",
        "cs.AI",
        "cs.LG",
        "eess.AS",
        "eess.SP"
      ],
      "pdf_url": "https://arxiv.org/pdf/2406.02092v1",
      "landing_url": "https://arxiv.org/abs/2406.02092v1",
      "doi": "https://doi.org/10.48550/arXiv.2406.02092"
    },
    "queries": [
      {
        "anchor": "synchronous dialogue",
        "search_term": "neural codec"
      }
    ]
  },
  {
    "arxiv_id": "2406.02180",
    "anchor": "synchronous dialogue",
    "search_term": "discrete tokens",
    "search_record": {
      "id": "http://arxiv.org/abs/2406.02180v1",
      "title": "On The Statistical Representation Properties Of The Perturb-Softmax And The Perturb-Argmax Probability Distributions",
      "summary": "The Gumbel-Softmax probability distribution allows learning discrete tokens in generative learning, while the Gumbel-Argmax probability distribution is useful in learning discrete structures in discriminative learning. Despite the efforts invested in optimizing these probability models, their statistical properties are under-explored. In this work, we investigate their representation properties and determine for which families of parameters these probability distributions are complete, i.e., can represent any probability distribution, and minimal, i.e., can represent a probability distribution uniquely. We rely on convexity and differentiability to determine these statistical conditions and extend this framework to general probability models, such as Gaussian-Softmax and Gaussian-Argmax. We experimentally validate the qualities of these extensions, which enjoy a faster convergence rate. We conclude the analysis by identifying two sets of parameters that satisfy these assumptions and thus admit a complete and minimal representation. Our contribution is theoretical with supporting practical evaluation.",
      "published": "2024-06-04T10:22:12Z"
    },
    "metadata": {
      "arxiv_id": "2406.02180",
      "title": "On The Statistical Representation Properties Of The Perturb-Softmax And The Perturb-Argmax Probability Distributions",
      "summary": "The Gumbel-Softmax probability distribution allows learning discrete tokens in generative learning, while the Gumbel-Argmax probability distribution is useful in learning discrete structures in discriminative learning. Despite the efforts invested in optimizing these probability models, their statistical properties are under-explored. In this work, we investigate their representation properties and determine for which families of parameters these probability distributions are complete, i.e., can represent any probability distribution, and minimal, i.e., can represent a probability distribution uniquely. We rely on convexity and differentiability to determine these statistical conditions and extend this framework to general probability models, such as Gaussian-Softmax and Gaussian-Argmax. We experimentally validate the qualities of these extensions, which enjoy a faster convergence rate. We conclude the analysis by identifying two sets of parameters that satisfy these assumptions and thus admit a complete and minimal representation. Our contribution is theoretical with supporting practical evaluation.",
      "authors": [
        "Hedda Cohen Indelman",
        "Tamir Hazan"
      ],
      "published": "2024-06-04T10:22:12Z",
      "updated": "2024-06-04T10:22:12Z",
      "categories": [
        "cs.LG",
        "cs.AI",
        "stat.ML"
      ],
      "pdf_url": "https://arxiv.org/pdf/2406.02180v1",
      "landing_url": "https://arxiv.org/abs/2406.02180v1",
      "doi": "https://doi.org/10.48550/arXiv.2406.02180"
    },
    "queries": [
      {
        "anchor": "synchronous dialogue",
        "search_term": "discrete tokens"
      }
    ]
  },
  {
    "arxiv_id": "2406.02250",
    "anchor": "synchronous dialogue",
    "search_term": "streaming architecture",
    "search_record": {
      "id": "http://arxiv.org/abs/2406.02250v1",
      "title": "Multi-Stage Speech Bandwidth Extension with Flexible Sampling Rate Control",
      "summary": "The majority of existing speech bandwidth extension (BWE) methods operate under the constraint of fixed source and target sampling rates, which limits their flexibility in practical applications. In this paper, we propose a multi-stage speech BWE model named MS-BWE, which can handle a set of source and target sampling rate pairs and achieve flexible extensions of frequency bandwidth. The proposed MS-BWE model comprises a cascade of BWE blocks, with each block featuring a dual-stream architecture to realize amplitude and phase extension, progressively painting the speech frequency bands stage by stage. The teacher-forcing strategy is employed to mitigate the discrepancy between training and inference. Experimental results demonstrate that our proposed MS-BWE is comparable to state-of-the-art speech BWE methods in speech quality. Regarding generation efficiency, the one-stage generation of MS-BWE can achieve over one thousand times real-time on GPU and about sixty times on CPU.",
      "published": "2024-06-04T12:17:11Z"
    },
    "metadata": {
      "arxiv_id": "2406.02250",
      "title": "Multi-Stage Speech Bandwidth Extension with Flexible Sampling Rate Control",
      "summary": "The majority of existing speech bandwidth extension (BWE) methods operate under the constraint of fixed source and target sampling rates, which limits their flexibility in practical applications. In this paper, we propose a multi-stage speech BWE model named MS-BWE, which can handle a set of source and target sampling rate pairs and achieve flexible extensions of frequency bandwidth. The proposed MS-BWE model comprises a cascade of BWE blocks, with each block featuring a dual-stream architecture to realize amplitude and phase extension, progressively painting the speech frequency bands stage by stage. The teacher-forcing strategy is employed to mitigate the discrepancy between training and inference. Experimental results demonstrate that our proposed MS-BWE is comparable to state-of-the-art speech BWE methods in speech quality. Regarding generation efficiency, the one-stage generation of MS-BWE can achieve over one thousand times real-time on GPU and about sixty times on CPU.",
      "authors": [
        "Ye-Xin Lu",
        "Yang Ai",
        "Zheng-Yan Sheng",
        "Zhen-Hua Ling"
      ],
      "published": "2024-06-04T12:17:11Z",
      "updated": "2024-06-04T12:17:11Z",
      "categories": [
        "eess.AS",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2406.02250v1",
      "landing_url": "https://arxiv.org/abs/2406.02250v1",
      "doi": "https://doi.org/10.48550/arXiv.2406.02250"
    },
    "queries": [
      {
        "anchor": "synchronous dialogue",
        "search_term": "streaming architecture"
      }
    ]
  },
  {
    "arxiv_id": "2406.02315",
    "anchor": "spoken language models",
    "search_term": "audio tokens",
    "search_record": {
      "id": "http://arxiv.org/abs/2406.02315v2",
      "title": "An Independence-promoting Loss for Music Generation with Language Models",
      "summary": "Music generation schemes using language modeling rely on a vocabulary of audio tokens, generally provided as codes in a discrete latent space learnt by an auto-encoder. Multi-stage quantizers are often employed to produce these tokens, therefore the decoding strategy used for token prediction must be adapted to account for multiple codebooks: either it should model the joint distribution over all codebooks, or fit the product of the codebook marginal distributions. Modelling the joint distribution requires a costly increase in the number of auto-regressive steps, while fitting the product of the marginals yields an inexact model unless the codebooks are mutually independent. In this work, we introduce an independence-promoting loss to regularize the auto-encoder used as the tokenizer in language models for music generation. The proposed loss is a proxy for mutual information based on the maximum mean discrepancy principle, applied in reproducible kernel Hilbert spaces. Our criterion is simple to implement and train, and it is generalizable to other multi-stream codecs. We show that it reduces the statistical dependence between codebooks during auto-encoding. This leads to an increase in the generated music quality when modelling the product of the marginal distributions, while generating audio much faster than the joint distribution model.",
      "published": "2024-06-04T13:44:39Z"
    },
    "metadata": {
      "arxiv_id": "2406.02315",
      "title": "An Independence-promoting Loss for Music Generation with Language Models",
      "summary": "Music generation schemes using language modeling rely on a vocabulary of audio tokens, generally provided as codes in a discrete latent space learnt by an auto-encoder. Multi-stage quantizers are often employed to produce these tokens, therefore the decoding strategy used for token prediction must be adapted to account for multiple codebooks: either it should model the joint distribution over all codebooks, or fit the product of the codebook marginal distributions. Modelling the joint distribution requires a costly increase in the number of auto-regressive steps, while fitting the product of the marginals yields an inexact model unless the codebooks are mutually independent. In this work, we introduce an independence-promoting loss to regularize the auto-encoder used as the tokenizer in language models for music generation. The proposed loss is a proxy for mutual information based on the maximum mean discrepancy principle, applied in reproducible kernel Hilbert spaces. Our criterion is simple to implement and train, and it is generalizable to other multi-stream codecs. We show that it reduces the statistical dependence between codebooks during auto-encoding. This leads to an increase in the generated music quality when modelling the product of the marginal distributions, while generating audio much faster than the joint distribution model.",
      "authors": [
        "Jean-Marie Lemercier",
        "Simon Rouard",
        "Jade Copet",
        "Yossi Adi",
        "Alexandre Défossez"
      ],
      "published": "2024-06-04T13:44:39Z",
      "updated": "2024-06-09T17:55:51Z",
      "categories": [
        "cs.SD",
        "cs.AI",
        "cs.LG",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2406.02315v2",
      "landing_url": "https://arxiv.org/abs/2406.02315v2",
      "doi": "https://doi.org/10.48550/arXiv.2406.02315"
    },
    "queries": [
      {
        "anchor": "spoken language models",
        "search_term": "audio tokens"
      },
      {
        "anchor": "synchronous dialogue",
        "search_term": "audio tokens"
      }
    ]
  },
  {
    "arxiv_id": "2406.02429",
    "anchor": "spoken language models",
    "search_term": "true full-duplex",
    "search_record": {
      "id": "http://arxiv.org/abs/2406.02429v1",
      "title": "Self-Supervised Singing Voice Pre-Training towards Speech-to-Singing Conversion",
      "summary": "Speech-to-singing voice conversion (STS) task always suffers from data scarcity, because it requires paired speech and singing data. Compounding this issue are the challenges of content-pitch alignment and the suboptimal quality of generated outputs, presenting significant hurdles in STS research. This paper presents SVPT, an STS approach boosted by a self-supervised singing voice pre-training model. We leverage spoken language model techniques to tackle the rhythm alignment problem and the in-context learning capability to achieve zero-shot conversion. We adopt discrete-unit random resampling and pitch corruption strategies, enabling training with unpaired singing data and thus mitigating the issue of data scarcity. SVPT also serves as an effective backbone for singing voice synthesis (SVS), offering insights into scaling up SVS models. Experimental results indicate that SVPT delivers notable improvements in both STS and SVS endeavors. Audio samples are available at https://speech2sing.github.io.",
      "published": "2024-06-04T15:47:59Z"
    },
    "metadata": {
      "arxiv_id": "2406.02429",
      "title": "Self-Supervised Singing Voice Pre-Training towards Speech-to-Singing Conversion",
      "summary": "Speech-to-singing voice conversion (STS) task always suffers from data scarcity, because it requires paired speech and singing data. Compounding this issue are the challenges of content-pitch alignment and the suboptimal quality of generated outputs, presenting significant hurdles in STS research. This paper presents SVPT, an STS approach boosted by a self-supervised singing voice pre-training model. We leverage spoken language model techniques to tackle the rhythm alignment problem and the in-context learning capability to achieve zero-shot conversion. We adopt discrete-unit random resampling and pitch corruption strategies, enabling training with unpaired singing data and thus mitigating the issue of data scarcity. SVPT also serves as an effective backbone for singing voice synthesis (SVS), offering insights into scaling up SVS models. Experimental results indicate that SVPT delivers notable improvements in both STS and SVS endeavors. Audio samples are available at https://speech2sing.github.io.",
      "authors": [
        "Ruiqi Li",
        "Rongjie Huang",
        "Yongqi Wang",
        "Zhiqing Hong",
        "Zhou Zhao"
      ],
      "published": "2024-06-04T15:47:59Z",
      "updated": "2024-06-04T15:47:59Z",
      "categories": [
        "eess.AS",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2406.02429v1",
      "landing_url": "https://arxiv.org/abs/2406.02429v1",
      "doi": "https://doi.org/10.48550/arXiv.2406.02429"
    },
    "queries": [
      {
        "anchor": "spoken language models",
        "search_term": "true full-duplex"
      },
      {
        "anchor": "spoken language models",
        "search_term": "pseudo full-duplex"
      },
      {
        "anchor": "spoken language models",
        "search_term": "engineered synchronization"
      },
      {
        "anchor": "spoken language models",
        "search_term": "learned synchronization"
      },
      {
        "anchor": "spoken language models",
        "search_term": "synchronous data"
      },
      {
        "anchor": "spoken language models",
        "search_term": "data scarcity"
      },
      {
        "anchor": "spoken language models",
        "search_term": "multichannel dialogue"
      },
      {
        "anchor": "spoken language models",
        "search_term": "behavioral arbitration"
      },
      {
        "anchor": "spoken language models",
        "search_term": "floor transfer offset"
      },
      {
        "anchor": "spoken language models",
        "search_term": "stop latency"
      },
      {
        "anchor": "spoken language models",
        "search_term": "interruption response delay"
      }
    ]
  },
  {
    "arxiv_id": "2406.02897",
    "anchor": "spoken language models",
    "search_term": "audio tokens",
    "search_record": {
      "id": "http://arxiv.org/abs/2406.02897v2",
      "title": "LiveSpeech: Low-Latency Zero-shot Text-to-Speech via Autoregressive Modeling of Audio Discrete Codes",
      "summary": "Prior works have demonstrated zero-shot text-to-speech by using a generative language model on audio tokens obtained via a neural audio codec. It is still challenging, however, to adapt them to low-latency scenarios. In this paper, we present LiveSpeech - a fully autoregressive language model-based approach for zero-shot text-to-speech, enabling low-latency streaming of the output audio. To allow multiple token prediction within a single decoding step, we propose (1) using adaptive codebook loss weights that consider codebook contribution in each frame and focus on hard instances, and (2) grouping codebooks and processing groups in parallel. Experiments show our proposed models achieve competitive results to state-of-the-art baselines in terms of content accuracy, speaker similarity, audio quality, and inference speed while being suitable for low-latency streaming applications.",
      "published": "2024-06-05T03:36:11Z"
    },
    "metadata": {
      "arxiv_id": "2406.02897",
      "title": "LiveSpeech: Low-Latency Zero-shot Text-to-Speech via Autoregressive Modeling of Audio Discrete Codes",
      "summary": "Prior works have demonstrated zero-shot text-to-speech by using a generative language model on audio tokens obtained via a neural audio codec. It is still challenging, however, to adapt them to low-latency scenarios. In this paper, we present LiveSpeech - a fully autoregressive language model-based approach for zero-shot text-to-speech, enabling low-latency streaming of the output audio. To allow multiple token prediction within a single decoding step, we propose (1) using adaptive codebook loss weights that consider codebook contribution in each frame and focus on hard instances, and (2) grouping codebooks and processing groups in parallel. Experiments show our proposed models achieve competitive results to state-of-the-art baselines in terms of content accuracy, speaker similarity, audio quality, and inference speed while being suitable for low-latency streaming applications.",
      "authors": [
        "Trung Dang",
        "David Aponte",
        "Dung Tran",
        "Kazuhito Koishida"
      ],
      "published": "2024-06-05T03:36:11Z",
      "updated": "2024-06-10T05:50:53Z",
      "categories": [
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2406.02897v2",
      "landing_url": "https://arxiv.org/abs/2406.02897v2",
      "doi": "https://doi.org/10.48550/arXiv.2406.02897"
    },
    "queries": [
      {
        "anchor": "spoken language models",
        "search_term": "audio tokens"
      },
      {
        "anchor": "synchronous dialogue",
        "search_term": "audio tokens"
      },
      {
        "anchor": "turn-taking",
        "search_term": "audio tokens"
      }
    ]
  },
  {
    "arxiv_id": "2406.04175",
    "anchor": "synchronous dialogue",
    "search_term": "semantic coherence",
    "search_record": {
      "id": "http://arxiv.org/abs/2406.04175v2",
      "title": "Confabulation: The Surprising Value of Large Language Model Hallucinations",
      "summary": "This paper presents a systematic defense of large language model (LLM) hallucinations or 'confabulations' as a potential resource instead of a categorically negative pitfall. The standard view is that confabulations are inherently problematic and AI research should eliminate this flaw. In this paper, we argue and empirically demonstrate that measurable semantic characteristics of LLM confabulations mirror a human propensity to utilize increased narrativity as a cognitive resource for sense-making and communication. In other words, it has potential value. Specifically, we analyze popular hallucination benchmarks and reveal that hallucinated outputs display increased levels of narrativity and semantic coherence relative to veridical outputs. This finding reveals a tension in our usually dismissive understandings of confabulation. It suggests, counter-intuitively, that the tendency for LLMs to confabulate may be intimately associated with a positive capacity for coherent narrative-text generation.",
      "published": "2024-06-06T15:32:29Z"
    },
    "metadata": {
      "arxiv_id": "2406.04175",
      "title": "Confabulation: The Surprising Value of Large Language Model Hallucinations",
      "summary": "This paper presents a systematic defense of large language model (LLM) hallucinations or 'confabulations' as a potential resource instead of a categorically negative pitfall. The standard view is that confabulations are inherently problematic and AI research should eliminate this flaw. In this paper, we argue and empirically demonstrate that measurable semantic characteristics of LLM confabulations mirror a human propensity to utilize increased narrativity as a cognitive resource for sense-making and communication. In other words, it has potential value. Specifically, we analyze popular hallucination benchmarks and reveal that hallucinated outputs display increased levels of narrativity and semantic coherence relative to veridical outputs. This finding reveals a tension in our usually dismissive understandings of confabulation. It suggests, counter-intuitively, that the tendency for LLMs to confabulate may be intimately associated with a positive capacity for coherent narrative-text generation.",
      "authors": [
        "Peiqi Sui",
        "Eamon Duede",
        "Sophie Wu",
        "Richard Jean So"
      ],
      "published": "2024-06-06T15:32:29Z",
      "updated": "2024-06-25T18:37:19Z",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2406.04175v2",
      "landing_url": "https://arxiv.org/abs/2406.04175v2",
      "doi": "https://doi.org/10.48550/arXiv.2406.04175"
    },
    "queries": [
      {
        "anchor": "synchronous dialogue",
        "search_term": "semantic coherence"
      }
    ]
  },
  {
    "arxiv_id": "2406.04582",
    "anchor": "spoken language models",
    "search_term": "neural codec",
    "search_record": {
      "id": "http://arxiv.org/abs/2406.04582v1",
      "title": "Neural Codec-based Adversarial Sample Detection for Speaker Verification",
      "summary": "Automatic Speaker Verification (ASV), increasingly used in security-critical applications, faces vulnerabilities from rising adversarial attacks, with few effective defenses available. In this paper, we propose a neural codec-based adversarial sample detection method for ASV. The approach leverages the codec's ability to discard redundant perturbations and retain essential information. Specifically, we distinguish between genuine and adversarial samples by comparing ASV score differences between original and re-synthesized audio (by codec models). This comprehensive study explores all open-source neural codecs and their variant models for experiments. The Descript-audio-codec model stands out by delivering the highest detection rate among 15 neural codecs and surpassing seven prior state-of-the-art (SOTA) detection methods. Note that, our single-model method even outperforms a SOTA ensemble method by a large margin.",
      "published": "2024-06-07T02:03:27Z"
    },
    "metadata": {
      "arxiv_id": "2406.04582",
      "title": "Neural Codec-based Adversarial Sample Detection for Speaker Verification",
      "summary": "Automatic Speaker Verification (ASV), increasingly used in security-critical applications, faces vulnerabilities from rising adversarial attacks, with few effective defenses available. In this paper, we propose a neural codec-based adversarial sample detection method for ASV. The approach leverages the codec's ability to discard redundant perturbations and retain essential information. Specifically, we distinguish between genuine and adversarial samples by comparing ASV score differences between original and re-synthesized audio (by codec models). This comprehensive study explores all open-source neural codecs and their variant models for experiments. The Descript-audio-codec model stands out by delivering the highest detection rate among 15 neural codecs and surpassing seven prior state-of-the-art (SOTA) detection methods. Note that, our single-model method even outperforms a SOTA ensemble method by a large margin.",
      "authors": [
        "Xuanjun Chen",
        "Jiawei Du",
        "Haibin Wu",
        "Jyh-Shing Roger Jang",
        "Hung-yi Lee"
      ],
      "published": "2024-06-07T02:03:27Z",
      "updated": "2024-06-07T02:03:27Z",
      "categories": [
        "eess.AS",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2406.04582v1",
      "landing_url": "https://arxiv.org/abs/2406.04582v1",
      "doi": "https://doi.org/10.48550/arXiv.2406.04582"
    },
    "queries": [
      {
        "anchor": "spoken language models",
        "search_term": "neural codec"
      },
      {
        "anchor": "synchronous dialogue",
        "search_term": "neural codec"
      },
      {
        "anchor": "turn-taking",
        "search_term": "neural codec"
      }
    ]
  },
  {
    "arxiv_id": "2406.04825",
    "anchor": "spoken language models",
    "search_term": "data scarcity",
    "search_record": {
      "id": "http://arxiv.org/abs/2406.04825v2",
      "title": "Graph Mining under Data scarcity",
      "summary": "Multitude of deep learning models have been proposed for node classification in graphs. However, they tend to perform poorly under labeled-data scarcity. Although Few-shot learning for graphs has been introduced to overcome this problem, the existing models are not easily adaptable for generic graph learning frameworks like Graph Neural Networks (GNNs). Our work proposes an Uncertainty Estimator framework that can be applied on top of any generic GNN backbone network (which are typically designed for supervised/semi-supervised node classification) to improve the node classification performance. A neural network is used to model the Uncertainty Estimator as a probability distribution rather than probabilistic discrete scalar values. We train these models under the classic episodic learning paradigm in the $n$-way, $k$-shot fashion, in an end-to-end setting. Our work demonstrates that implementation of the uncertainty estimator on a GNN backbone network improves the classification accuracy under Few-shot setting without any meta-learning specific architecture. We conduct experiments on multiple datasets under different Few-shot settings and different GNN-based backbone networks. Our method outperforms the baselines, which demonstrates the efficacy of the Uncertainty Estimator for Few-shot node classification on graphs with a GNN.",
      "published": "2024-06-07T10:50:03Z"
    },
    "metadata": {
      "arxiv_id": "2406.04825",
      "title": "Graph Mining under Data scarcity",
      "summary": "Multitude of deep learning models have been proposed for node classification in graphs. However, they tend to perform poorly under labeled-data scarcity. Although Few-shot learning for graphs has been introduced to overcome this problem, the existing models are not easily adaptable for generic graph learning frameworks like Graph Neural Networks (GNNs). Our work proposes an Uncertainty Estimator framework that can be applied on top of any generic GNN backbone network (which are typically designed for supervised/semi-supervised node classification) to improve the node classification performance. A neural network is used to model the Uncertainty Estimator as a probability distribution rather than probabilistic discrete scalar values. We train these models under the classic episodic learning paradigm in the $n$-way, $k$-shot fashion, in an end-to-end setting. Our work demonstrates that implementation of the uncertainty estimator on a GNN backbone network improves the classification accuracy under Few-shot setting without any meta-learning specific architecture. We conduct experiments on multiple datasets under different Few-shot settings and different GNN-based backbone networks. Our method outperforms the baselines, which demonstrates the efficacy of the Uncertainty Estimator for Few-shot node classification on graphs with a GNN.",
      "authors": [
        "Appan Rakaraddi",
        "Lam Siew-Kei",
        "Mahardhika Pratama",
        "Marcus de Carvalho"
      ],
      "published": "2024-06-07T10:50:03Z",
      "updated": "2024-06-11T13:33:16Z",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2406.04825v2",
      "landing_url": "https://arxiv.org/abs/2406.04825v2",
      "doi": "https://doi.org/10.48550/arXiv.2406.04825"
    },
    "queries": [
      {
        "anchor": "spoken language models",
        "search_term": "data scarcity"
      },
      {
        "anchor": "synchronous dialogue",
        "search_term": "data scarcity"
      },
      {
        "anchor": "turn-taking",
        "search_term": "data scarcity"
      }
    ]
  },
  {
    "arxiv_id": "2406.05298",
    "anchor": "spoken language models",
    "search_term": "audio tokens",
    "search_record": {
      "id": "http://arxiv.org/abs/2406.05298v2",
      "title": "Spectral Codecs: Improving Non-Autoregressive Speech Synthesis with Spectrogram-Based Audio Codecs",
      "summary": "Historically, most speech models in machine-learning have used the mel-spectrogram as a speech representation. Recently, discrete audio tokens produced by neural audio codecs have become a popular alternate speech representation for speech synthesis tasks such as text-to-speech (TTS). However, the data distribution produced by such codecs is too complex for some TTS models to predict, typically requiring large autoregressive models to get good quality. Most existing audio codecs use Residual Vector Quantization (RVQ) to compress and reconstruct the time-domain audio signal. We propose a spectral codec which uses Finite Scalar Quantization (FSQ) to compress the mel-spectrogram and reconstruct the time-domain audio signal. A study of objective audio quality metrics and subjective listening tests suggests that our spectral codec has comparable perceptual quality to equivalent audio codecs. We show that FSQ, and the use of spectral speech representations, can both improve the performance of parallel TTS models.",
      "published": "2024-06-07T23:47:51Z"
    },
    "metadata": {
      "arxiv_id": "2406.05298",
      "title": "Spectral Codecs: Improving Non-Autoregressive Speech Synthesis with Spectrogram-Based Audio Codecs",
      "summary": "Historically, most speech models in machine-learning have used the mel-spectrogram as a speech representation. Recently, discrete audio tokens produced by neural audio codecs have become a popular alternate speech representation for speech synthesis tasks such as text-to-speech (TTS). However, the data distribution produced by such codecs is too complex for some TTS models to predict, typically requiring large autoregressive models to get good quality. Most existing audio codecs use Residual Vector Quantization (RVQ) to compress and reconstruct the time-domain audio signal. We propose a spectral codec which uses Finite Scalar Quantization (FSQ) to compress the mel-spectrogram and reconstruct the time-domain audio signal. A study of objective audio quality metrics and subjective listening tests suggests that our spectral codec has comparable perceptual quality to equivalent audio codecs. We show that FSQ, and the use of spectral speech representations, can both improve the performance of parallel TTS models.",
      "authors": [
        "Ryan Langman",
        "Ante Jukić",
        "Kunal Dhawan",
        "Nithin Rao Koluguri",
        "Jason Li"
      ],
      "published": "2024-06-07T23:47:51Z",
      "updated": "2025-06-04T16:25:54Z",
      "categories": [
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2406.05298v2",
      "landing_url": "https://arxiv.org/abs/2406.05298v2",
      "doi": "https://doi.org/10.48550/arXiv.2406.05298"
    },
    "queries": [
      {
        "anchor": "spoken language models",
        "search_term": "audio tokens"
      },
      {
        "anchor": "synchronous dialogue",
        "search_term": "audio tokens"
      },
      {
        "anchor": "turn-taking",
        "search_term": "audio tokens"
      }
    ]
  },
  {
    "arxiv_id": "2406.05370",
    "anchor": "spoken language models",
    "search_term": "neural codec",
    "search_record": {
      "id": "http://arxiv.org/abs/2406.05370v2",
      "title": "VALL-E 2: Neural Codec Language Models are Human Parity Zero-Shot Text to Speech Synthesizers",
      "summary": "This paper introduces VALL-E 2, the latest advancement in neural codec language models that marks a milestone in zero-shot text-to-speech synthesis (TTS), achieving human parity for the first time. Based on its predecessor, VALL-E, the new iteration introduces two significant enhancements: Repetition Aware Sampling refines the original nucleus sampling process by accounting for token repetition in the decoding history. It not only stabilizes the decoding but also circumvents the infinite loop issue. Grouped Code Modeling organizes codec codes into groups to effectively shorten the sequence length, which not only boosts inference speed but also addresses the challenges of long sequence modeling. Our experiments on the LibriSpeech and VCTK datasets show that VALL-E 2 surpasses previous systems in speech robustness, naturalness, and speaker similarity. It is the first of its kind to reach human parity on these benchmarks. Moreover, VALL-E 2 consistently synthesizes high-quality speech, even for sentences that are traditionally challenging due to their complexity or repetitive phrases. The advantages of this work could contribute to valuable endeavors, such as generating speech for individuals with aphasia or people with amyotrophic lateral sclerosis. See https://aka.ms/valle2 for demos of VALL-E 2.",
      "published": "2024-06-08T06:31:03Z"
    },
    "metadata": {
      "arxiv_id": "2406.05370",
      "title": "VALL-E 2: Neural Codec Language Models are Human Parity Zero-Shot Text to Speech Synthesizers",
      "summary": "This paper introduces VALL-E 2, the latest advancement in neural codec language models that marks a milestone in zero-shot text-to-speech synthesis (TTS), achieving human parity for the first time. Based on its predecessor, VALL-E, the new iteration introduces two significant enhancements: Repetition Aware Sampling refines the original nucleus sampling process by accounting for token repetition in the decoding history. It not only stabilizes the decoding but also circumvents the infinite loop issue. Grouped Code Modeling organizes codec codes into groups to effectively shorten the sequence length, which not only boosts inference speed but also addresses the challenges of long sequence modeling. Our experiments on the LibriSpeech and VCTK datasets show that VALL-E 2 surpasses previous systems in speech robustness, naturalness, and speaker similarity. It is the first of its kind to reach human parity on these benchmarks. Moreover, VALL-E 2 consistently synthesizes high-quality speech, even for sentences that are traditionally challenging due to their complexity or repetitive phrases. The advantages of this work could contribute to valuable endeavors, such as generating speech for individuals with aphasia or people with amyotrophic lateral sclerosis. See https://aka.ms/valle2 for demos of VALL-E 2.",
      "authors": [
        "Sanyuan Chen",
        "Shujie Liu",
        "Long Zhou",
        "Yanqing Liu",
        "Xu Tan",
        "Jinyu Li",
        "Sheng Zhao",
        "Yao Qian",
        "Furu Wei"
      ],
      "published": "2024-06-08T06:31:03Z",
      "updated": "2024-06-17T04:39:08Z",
      "categories": [
        "cs.CL",
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2406.05370v2",
      "landing_url": "https://arxiv.org/abs/2406.05370v2",
      "doi": "https://doi.org/10.48550/arXiv.2406.05370"
    },
    "queries": [
      {
        "anchor": "spoken language models",
        "search_term": "neural codec"
      },
      {
        "anchor": "synchronous dialogue",
        "search_term": "neural codec"
      },
      {
        "anchor": "turn-taking",
        "search_term": "neural codec"
      }
    ]
  },
  {
    "arxiv_id": "2406.05551",
    "anchor": "spoken language models",
    "search_term": "audio tokens",
    "search_record": {
      "id": "http://arxiv.org/abs/2406.05551v1",
      "title": "Autoregressive Diffusion Transformer for Text-to-Speech Synthesis",
      "summary": "Audio language models have recently emerged as a promising approach for various audio generation tasks, relying on audio tokenizers to encode waveforms into sequences of discrete symbols. Audio tokenization often poses a necessary compromise between code bitrate and reconstruction accuracy. When dealing with low-bitrate audio codes, language models are constrained to process only a subset of the information embedded in the audio, which in turn restricts their generative capabilities. To circumvent these issues, we propose encoding audio as vector sequences in continuous space $\\mathbb R^d$ and autoregressively generating these sequences using a decoder-only diffusion transformer (ARDiT). Our findings indicate that ARDiT excels in zero-shot text-to-speech and exhibits performance that compares to or even surpasses that of state-of-the-art models. High-bitrate continuous speech representation enables almost flawless reconstruction, allowing our model to achieve nearly perfect speech editing. Our experiments reveal that employing Integral Kullback-Leibler (IKL) divergence for distillation at each autoregressive step significantly boosts the perceived quality of the samples. Simultaneously, it condenses the iterative sampling process of the diffusion model into a single step. Furthermore, ARDiT can be trained to predict several continuous vectors in one step, significantly reducing latency during sampling. Impressively, one of our models can generate $170$ ms of $24$ kHz speech per evaluation step with minimal degradation in performance. Audio samples are available at http://ardit-tts.github.io/ .",
      "published": "2024-06-08T18:57:13Z"
    },
    "metadata": {
      "arxiv_id": "2406.05551",
      "title": "Autoregressive Diffusion Transformer for Text-to-Speech Synthesis",
      "summary": "Audio language models have recently emerged as a promising approach for various audio generation tasks, relying on audio tokenizers to encode waveforms into sequences of discrete symbols. Audio tokenization often poses a necessary compromise between code bitrate and reconstruction accuracy. When dealing with low-bitrate audio codes, language models are constrained to process only a subset of the information embedded in the audio, which in turn restricts their generative capabilities. To circumvent these issues, we propose encoding audio as vector sequences in continuous space $\\mathbb R^d$ and autoregressively generating these sequences using a decoder-only diffusion transformer (ARDiT). Our findings indicate that ARDiT excels in zero-shot text-to-speech and exhibits performance that compares to or even surpasses that of state-of-the-art models. High-bitrate continuous speech representation enables almost flawless reconstruction, allowing our model to achieve nearly perfect speech editing. Our experiments reveal that employing Integral Kullback-Leibler (IKL) divergence for distillation at each autoregressive step significantly boosts the perceived quality of the samples. Simultaneously, it condenses the iterative sampling process of the diffusion model into a single step. Furthermore, ARDiT can be trained to predict several continuous vectors in one step, significantly reducing latency during sampling. Impressively, one of our models can generate $170$ ms of $24$ kHz speech per evaluation step with minimal degradation in performance. Audio samples are available at http://ardit-tts.github.io/ .",
      "authors": [
        "Zhijun Liu",
        "Shuai Wang",
        "Sho Inoue",
        "Qibing Bai",
        "Haizhou Li"
      ],
      "published": "2024-06-08T18:57:13Z",
      "updated": "2024-06-08T18:57:13Z",
      "categories": [
        "eess.AS",
        "cs.AI",
        "cs.CL",
        "cs.LG",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2406.05551v1",
      "landing_url": "https://arxiv.org/abs/2406.05551v1",
      "doi": "https://doi.org/10.48550/arXiv.2406.05551"
    },
    "queries": [
      {
        "anchor": "spoken language models",
        "search_term": "audio tokens"
      },
      {
        "anchor": "synchronous dialogue",
        "search_term": "audio tokens"
      }
    ]
  },
  {
    "arxiv_id": "2406.05887",
    "anchor": "synchronous dialogue",
    "search_term": "data scarcity",
    "search_record": {
      "id": "http://arxiv.org/abs/2406.05887v1",
      "title": "Few-Shot Load Forecasting Under Data Scarcity in Smart Grids: A Meta-Learning Approach",
      "summary": "Despite the rapid expansion of smart grids and large volumes of data at the individual consumer level, there are still various cases where adequate data collection to train accurate load forecasting models is challenging or even impossible. This paper proposes adapting an established model-agnostic meta-learning algorithm for short-term load forecasting in the context of few-shot learning. Specifically, the proposed method can rapidly adapt and generalize within any unknown load time series of arbitrary length using only minimal training samples. In this context, the meta-learning model learns an optimal set of initial parameters for a base-level learner recurrent neural network. The proposed model is evaluated using a dataset of historical load consumption data from real-world consumers. Despite the examined load series' short length, it produces accurate forecasts outperforming transfer learning and task-specific machine learning methods by $12.5\\%$. To enhance robustness and fairness during model evaluation, a novel metric, mean average log percentage error, is proposed that alleviates the bias introduced by the commonly used MAPE metric. Finally, a series of studies to evaluate the model's robustness under different hyperparameters and time series lengths is also conducted, demonstrating that the proposed approach consistently outperforms all other models.",
      "published": "2024-06-09T18:59:08Z"
    },
    "metadata": {
      "arxiv_id": "2406.05887",
      "title": "Few-Shot Load Forecasting Under Data Scarcity in Smart Grids: A Meta-Learning Approach",
      "summary": "Despite the rapid expansion of smart grids and large volumes of data at the individual consumer level, there are still various cases where adequate data collection to train accurate load forecasting models is challenging or even impossible. This paper proposes adapting an established model-agnostic meta-learning algorithm for short-term load forecasting in the context of few-shot learning. Specifically, the proposed method can rapidly adapt and generalize within any unknown load time series of arbitrary length using only minimal training samples. In this context, the meta-learning model learns an optimal set of initial parameters for a base-level learner recurrent neural network. The proposed model is evaluated using a dataset of historical load consumption data from real-world consumers. Despite the examined load series' short length, it produces accurate forecasts outperforming transfer learning and task-specific machine learning methods by $12.5\\%$. To enhance robustness and fairness during model evaluation, a novel metric, mean average log percentage error, is proposed that alleviates the bias introduced by the commonly used MAPE metric. Finally, a series of studies to evaluate the model's robustness under different hyperparameters and time series lengths is also conducted, demonstrating that the proposed approach consistently outperforms all other models.",
      "authors": [
        "Georgios Tsoumplekas",
        "Christos L. Athanasiadis",
        "Dimitrios I. Doukas",
        "Antonios Chrysopoulos",
        "Pericles A. Mitkas"
      ],
      "published": "2024-06-09T18:59:08Z",
      "updated": "2024-06-09T18:59:08Z",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2406.05887v1",
      "landing_url": "https://arxiv.org/abs/2406.05887v1",
      "doi": "https://doi.org/10.48550/arXiv.2406.05887"
    },
    "queries": [
      {
        "anchor": "synchronous dialogue",
        "search_term": "data scarcity"
      }
    ]
  },
  {
    "arxiv_id": "2406.08112",
    "anchor": "spoken language models",
    "search_term": "neural codec",
    "search_record": {
      "id": "http://arxiv.org/abs/2406.08112v1",
      "title": "Codecfake: An Initial Dataset for Detecting LLM-based Deepfake Audio",
      "summary": "With the proliferation of Large Language Model (LLM) based deepfake audio, there is an urgent need for effective detection methods. Previous deepfake audio generation methods typically involve a multi-step generation process, with the final step using a vocoder to predict the waveform from handcrafted features. However, LLM-based audio is directly generated from discrete neural codecs in an end-to-end generation process, skipping the final step of vocoder processing. This poses a significant challenge for current audio deepfake detection (ADD) models based on vocoder artifacts. To effectively detect LLM-based deepfake audio, we focus on the core of the generation process, the conversion from neural codec to waveform. We propose Codecfake dataset, which is generated by seven representative neural codec methods. Experiment results show that codec-trained ADD models exhibit a 41.406% reduction in average equal error rate compared to vocoder-trained ADD models on the Codecfake test set.",
      "published": "2024-06-12T11:47:23Z"
    },
    "metadata": {
      "arxiv_id": "2406.08112",
      "title": "Codecfake: An Initial Dataset for Detecting LLM-based Deepfake Audio",
      "summary": "With the proliferation of Large Language Model (LLM) based deepfake audio, there is an urgent need for effective detection methods. Previous deepfake audio generation methods typically involve a multi-step generation process, with the final step using a vocoder to predict the waveform from handcrafted features. However, LLM-based audio is directly generated from discrete neural codecs in an end-to-end generation process, skipping the final step of vocoder processing. This poses a significant challenge for current audio deepfake detection (ADD) models based on vocoder artifacts. To effectively detect LLM-based deepfake audio, we focus on the core of the generation process, the conversion from neural codec to waveform. We propose Codecfake dataset, which is generated by seven representative neural codec methods. Experiment results show that codec-trained ADD models exhibit a 41.406% reduction in average equal error rate compared to vocoder-trained ADD models on the Codecfake test set.",
      "authors": [
        "Yi Lu",
        "Yuankun Xie",
        "Ruibo Fu",
        "Zhengqi Wen",
        "Jianhua Tao",
        "Zhiyong Wang",
        "Xin Qi",
        "Xuefei Liu",
        "Yongwei Li",
        "Yukun Liu",
        "Xiaopeng Wang",
        "Shuchen Shi"
      ],
      "published": "2024-06-12T11:47:23Z",
      "updated": "2024-06-12T11:47:23Z",
      "categories": [
        "cs.SD",
        "cs.AI",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2406.08112v1",
      "landing_url": "https://arxiv.org/abs/2406.08112v1",
      "doi": "https://doi.org/10.48550/arXiv.2406.08112"
    },
    "queries": [
      {
        "anchor": "spoken language models",
        "search_term": "neural codec"
      },
      {
        "anchor": "synchronous dialogue",
        "search_term": "neural codec"
      },
      {
        "anchor": "turn-taking",
        "search_term": "neural codec"
      }
    ]
  },
  {
    "arxiv_id": "2406.08336",
    "anchor": "spoken language models",
    "search_term": "neural codec",
    "search_record": {
      "id": "http://arxiv.org/abs/2406.08336v2",
      "title": "CoLM-DSR: Leveraging Neural Codec Language Modeling for Multi-Modal Dysarthric Speech Reconstruction",
      "summary": "Dysarthric speech reconstruction (DSR) aims to transform dysarthric speech into normal speech. It still suffers from low speaker similarity and poor prosody naturalness. In this paper, we propose a multi-modal DSR model by leveraging neural codec language modeling to improve the reconstruction results, especially for the speaker similarity and prosody naturalness. Our proposed model consists of: (i) a multi-modal content encoder to extract robust phoneme embeddings from dysarthric speech with auxiliary visual inputs; (ii) a speaker codec encoder to extract and normalize the speaker-aware codecs from the dysarthric speech, in order to provide original timbre and normal prosody; (iii) a codec language model based speech decoder to reconstruct the speech based on the extracted phoneme embeddings and normalized codecs. Evaluations on the commonly used UASpeech corpus show that our proposed model can achieve significant improvements in terms of speaker similarity and prosody naturalness.",
      "published": "2024-06-12T15:42:21Z"
    },
    "metadata": {
      "arxiv_id": "2406.08336",
      "title": "CoLM-DSR: Leveraging Neural Codec Language Modeling for Multi-Modal Dysarthric Speech Reconstruction",
      "summary": "Dysarthric speech reconstruction (DSR) aims to transform dysarthric speech into normal speech. It still suffers from low speaker similarity and poor prosody naturalness. In this paper, we propose a multi-modal DSR model by leveraging neural codec language modeling to improve the reconstruction results, especially for the speaker similarity and prosody naturalness. Our proposed model consists of: (i) a multi-modal content encoder to extract robust phoneme embeddings from dysarthric speech with auxiliary visual inputs; (ii) a speaker codec encoder to extract and normalize the speaker-aware codecs from the dysarthric speech, in order to provide original timbre and normal prosody; (iii) a codec language model based speech decoder to reconstruct the speech based on the extracted phoneme embeddings and normalized codecs. Evaluations on the commonly used UASpeech corpus show that our proposed model can achieve significant improvements in terms of speaker similarity and prosody naturalness.",
      "authors": [
        "Xueyuan Chen",
        "Dongchao Yang",
        "Dingdong Wang",
        "Xixin Wu",
        "Zhiyong Wu",
        "Helen Meng"
      ],
      "published": "2024-06-12T15:42:21Z",
      "updated": "2024-06-24T06:09:42Z",
      "categories": [
        "cs.SD",
        "cs.CV",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2406.08336v2",
      "landing_url": "https://arxiv.org/abs/2406.08336v2",
      "doi": "https://doi.org/10.48550/arXiv.2406.08336"
    },
    "queries": [
      {
        "anchor": "spoken language models",
        "search_term": "neural codec"
      },
      {
        "anchor": "synchronous dialogue",
        "search_term": "neural codec"
      },
      {
        "anchor": "turn-taking",
        "search_term": "neural codec"
      }
    ]
  },
  {
    "arxiv_id": "2406.08416",
    "anchor": "full-duplex",
    "search_term": "discrete tokens",
    "search_record": {
      "id": "http://arxiv.org/abs/2406.08416v2",
      "title": "TokSing: Singing Voice Synthesis based on Discrete Tokens",
      "summary": "Recent advancements in speech synthesis witness significant benefits by leveraging discrete tokens extracted from self-supervised learning (SSL) models. Discrete tokens offer higher storage efficiency and greater operability in intermediate representations compared to traditional continuous Mel spectrograms. However, when it comes to singing voice synthesis(SVS), achieving higher levels of melody expression poses a great challenge for utilizing discrete tokens. In this paper, we introduce TokSing, a discrete-based SVS system equipped with a token formulator that offers flexible token blendings. We observe a melody degradation during discretization, prompting us to integrate a melody signal with the discrete token and incorporate a specially-designed melody enhancement strategy in the musical encoder. Extensive experiments demonstrate that our TokSing achieves better performance against the Mel spectrogram baselines while offering advantages in intermediate representation space cost and convergence speed.",
      "published": "2024-06-12T16:59:24Z"
    },
    "metadata": {
      "arxiv_id": "2406.08416",
      "title": "TokSing: Singing Voice Synthesis based on Discrete Tokens",
      "summary": "Recent advancements in speech synthesis witness significant benefits by leveraging discrete tokens extracted from self-supervised learning (SSL) models. Discrete tokens offer higher storage efficiency and greater operability in intermediate representations compared to traditional continuous Mel spectrograms. However, when it comes to singing voice synthesis(SVS), achieving higher levels of melody expression poses a great challenge for utilizing discrete tokens. In this paper, we introduce TokSing, a discrete-based SVS system equipped with a token formulator that offers flexible token blendings. We observe a melody degradation during discretization, prompting us to integrate a melody signal with the discrete token and incorporate a specially-designed melody enhancement strategy in the musical encoder. Extensive experiments demonstrate that our TokSing achieves better performance against the Mel spectrogram baselines while offering advantages in intermediate representation space cost and convergence speed.",
      "authors": [
        "Yuning Wu",
        "Chunlei zhang",
        "Jiatong Shi",
        "Yuxun Tang",
        "Shan Yang",
        "Qin Jin"
      ],
      "published": "2024-06-12T16:59:24Z",
      "updated": "2024-06-20T16:46:14Z",
      "categories": [
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2406.08416v2",
      "landing_url": "https://arxiv.org/abs/2406.08416v2",
      "doi": "https://doi.org/10.48550/arXiv.2406.08416"
    },
    "queries": [
      {
        "anchor": "full-duplex",
        "search_term": "discrete tokens"
      },
      {
        "anchor": "spoken language models",
        "search_term": "discrete tokens"
      },
      {
        "anchor": "synchronous dialogue",
        "search_term": "discrete tokens"
      },
      {
        "anchor": "turn-taking",
        "search_term": "discrete tokens"
      }
    ]
  },
  {
    "arxiv_id": "2406.08905",
    "anchor": "synchronous dialogue",
    "search_term": "discrete tokens",
    "search_record": {
      "id": "http://arxiv.org/abs/2406.08905v2",
      "title": "SingOMD: Singing Oriented Multi-resolution Discrete Representation Construction from Speech Models",
      "summary": "Discrete representation has shown advantages in speech generation tasks, wherein discrete tokens are derived by discretizing hidden features from self-supervised learning (SSL) pre-trained models. However, the direct application of speech SSL models to singing generation encounters domain gaps between speech and singing. Furthermore, singing generation necessitates a more refined representation than typical speech. To address these challenges, we introduce SingOMD, a novel method to extract singing-oriented multi-resolution discrete representations from speech SSL models. Specifically, we first adapt the features from speech SSL through a resynthesis task and incorporate multi-resolution modules based on resampling to better serve singing generation. These adapted multi-resolution features are then discretized via clustering. Extensive experiments demonstrate the robustness, efficiency, and effectiveness of these representations in singing vocoders and singing voice synthesis.",
      "published": "2024-06-13T08:00:25Z"
    },
    "metadata": {
      "arxiv_id": "2406.08905",
      "title": "SingOMD: Singing Oriented Multi-resolution Discrete Representation Construction from Speech Models",
      "summary": "Discrete representation has shown advantages in speech generation tasks, wherein discrete tokens are derived by discretizing hidden features from self-supervised learning (SSL) pre-trained models. However, the direct application of speech SSL models to singing generation encounters domain gaps between speech and singing. Furthermore, singing generation necessitates a more refined representation than typical speech. To address these challenges, we introduce SingOMD, a novel method to extract singing-oriented multi-resolution discrete representations from speech SSL models. Specifically, we first adapt the features from speech SSL through a resynthesis task and incorporate multi-resolution modules based on resampling to better serve singing generation. These adapted multi-resolution features are then discretized via clustering. Extensive experiments demonstrate the robustness, efficiency, and effectiveness of these representations in singing vocoders and singing voice synthesis.",
      "authors": [
        "Yuxun Tang",
        "Yuning Wu",
        "Jiatong Shi",
        "Qin Jin"
      ],
      "published": "2024-06-13T08:00:25Z",
      "updated": "2024-06-20T11:01:14Z",
      "categories": [
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2406.08905v2",
      "landing_url": "https://arxiv.org/abs/2406.08905v2",
      "doi": "https://doi.org/10.48550/arXiv.2406.08905"
    },
    "queries": [
      {
        "anchor": "synchronous dialogue",
        "search_term": "discrete tokens"
      }
    ]
  },
  {
    "arxiv_id": "2406.10056",
    "anchor": "synchronous dialogue",
    "search_term": "audio tokens",
    "search_record": {
      "id": "http://arxiv.org/abs/2406.10056v1",
      "title": "UniAudio 1.5: Large Language Model-driven Audio Codec is A Few-shot Audio Task Learner",
      "summary": "The Large Language models (LLMs) have demonstrated supreme capabilities in text understanding and generation, but cannot be directly applied to cross-modal tasks without fine-tuning. This paper proposes a cross-modal in-context learning approach, empowering the frozen LLMs to achieve multiple audio tasks in a few-shot style without any parameter update. Specifically, we propose a novel and LLMs-driven audio codec model, LLM-Codec, to transfer the audio modality into the textual space, \\textit{i.e.} representing audio tokens with words or sub-words in the vocabulary of LLMs, while keeping high audio reconstruction quality. The key idea is to reduce the modality heterogeneity between text and audio by compressing the audio modality into a well-trained LLMs token space. Thus, the audio representation can be viewed as a new \\textit{foreign language}, and LLMs can learn the new \\textit{foreign language} with several demonstrations. In experiments, we investigate the performance of the proposed approach across multiple audio understanding and generation tasks, \\textit{e.g.} speech emotion classification, audio classification, text-to-speech generation, speech enhancement, etc. The experimental results demonstrate that the LLMs equipped with the proposed LLM-Codec, named as UniAudio 1.5, prompted by only a few examples, can achieve the expected functions in simple scenarios. It validates the feasibility and effectiveness of the proposed cross-modal in-context learning approach. To facilitate research on few-shot audio task learning and multi-modal LLMs, we have open-sourced the LLM-Codec model.",
      "published": "2024-06-14T14:13:18Z"
    },
    "metadata": {
      "arxiv_id": "2406.10056",
      "title": "UniAudio 1.5: Large Language Model-driven Audio Codec is A Few-shot Audio Task Learner",
      "summary": "The Large Language models (LLMs) have demonstrated supreme capabilities in text understanding and generation, but cannot be directly applied to cross-modal tasks without fine-tuning. This paper proposes a cross-modal in-context learning approach, empowering the frozen LLMs to achieve multiple audio tasks in a few-shot style without any parameter update. Specifically, we propose a novel and LLMs-driven audio codec model, LLM-Codec, to transfer the audio modality into the textual space, \\textit{i.e.} representing audio tokens with words or sub-words in the vocabulary of LLMs, while keeping high audio reconstruction quality. The key idea is to reduce the modality heterogeneity between text and audio by compressing the audio modality into a well-trained LLMs token space. Thus, the audio representation can be viewed as a new \\textit{foreign language}, and LLMs can learn the new \\textit{foreign language} with several demonstrations. In experiments, we investigate the performance of the proposed approach across multiple audio understanding and generation tasks, \\textit{e.g.} speech emotion classification, audio classification, text-to-speech generation, speech enhancement, etc. The experimental results demonstrate that the LLMs equipped with the proposed LLM-Codec, named as UniAudio 1.5, prompted by only a few examples, can achieve the expected functions in simple scenarios. It validates the feasibility and effectiveness of the proposed cross-modal in-context learning approach. To facilitate research on few-shot audio task learning and multi-modal LLMs, we have open-sourced the LLM-Codec model.",
      "authors": [
        "Dongchao Yang",
        "Haohan Guo",
        "Yuanyuan Wang",
        "Rongjie Huang",
        "Xiang Li",
        "Xu Tan",
        "Xixin Wu",
        "Helen Meng"
      ],
      "published": "2024-06-14T14:13:18Z",
      "updated": "2024-06-14T14:13:18Z",
      "categories": [
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2406.10056v1",
      "landing_url": "https://arxiv.org/abs/2406.10056v1",
      "doi": "https://doi.org/10.48550/arXiv.2406.10056"
    },
    "queries": [
      {
        "anchor": "synchronous dialogue",
        "search_term": "audio tokens"
      }
    ]
  },
  {
    "arxiv_id": "2406.10073",
    "anchor": "turn-taking",
    "search_term": "true full-duplex",
    "search_record": {
      "id": "http://arxiv.org/abs/2406.10073v1",
      "title": "Detecting the terminality of speech-turn boundary for spoken interactions in French TV and Radio content",
      "summary": "Transition Relevance Places are defined as the end of an utterance where the interlocutor may take the floor without interrupting the current speaker --i.e., a place where the turn is terminal. Analyzing turn terminality is useful to study the dynamic of turn-taking in spontaneous conversations. This paper presents an automatic classification of spoken utterances as Terminal or Non-Terminal in multi-speaker settings. We compared audio, text, and fusions of both approaches on a French corpus of TV and Radio extracts annotated with turn-terminality information at each speaker change. Our models are based on pre-trained self-supervised representations. We report results for different fusion strategies and varying context sizes. This study also questions the problem of performance variability by analyzing the differences in results for multiple training runs with random initialization. The measured accuracy would allow the use of these models for large-scale analysis of turn-taking.",
      "published": "2024-06-14T14:28:06Z"
    },
    "metadata": {
      "arxiv_id": "2406.10073",
      "title": "Detecting the terminality of speech-turn boundary for spoken interactions in French TV and Radio content",
      "summary": "Transition Relevance Places are defined as the end of an utterance where the interlocutor may take the floor without interrupting the current speaker --i.e., a place where the turn is terminal. Analyzing turn terminality is useful to study the dynamic of turn-taking in spontaneous conversations. This paper presents an automatic classification of spoken utterances as Terminal or Non-Terminal in multi-speaker settings. We compared audio, text, and fusions of both approaches on a French corpus of TV and Radio extracts annotated with turn-terminality information at each speaker change. Our models are based on pre-trained self-supervised representations. We report results for different fusion strategies and varying context sizes. This study also questions the problem of performance variability by analyzing the differences in results for multiple training runs with random initialization. The measured accuracy would allow the use of these models for large-scale analysis of turn-taking.",
      "authors": [
        "Rémi Uro",
        "Marie Tahon",
        "David Doukhan",
        "Antoine Laurent",
        "Albert Rilliard"
      ],
      "published": "2024-06-14T14:28:06Z",
      "updated": "2024-06-14T14:28:06Z",
      "categories": [
        "eess.AS",
        "cs.CL",
        "cs.HC",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2406.10073v1",
      "landing_url": "https://arxiv.org/abs/2406.10073v1",
      "doi": "https://doi.org/10.48550/arXiv.2406.10073"
    },
    "queries": [
      {
        "anchor": "turn-taking",
        "search_term": "true full-duplex"
      },
      {
        "anchor": "turn-taking",
        "search_term": "pseudo full-duplex"
      },
      {
        "anchor": "turn-taking",
        "search_term": "half duplex"
      },
      {
        "anchor": "turn-taking",
        "search_term": "engineered synchronization"
      },
      {
        "anchor": "turn-taking",
        "search_term": "learned synchronization"
      },
      {
        "anchor": "turn-taking",
        "search_term": "modular architecture"
      },
      {
        "anchor": "turn-taking",
        "search_term": "streaming architecture"
      },
      {
        "anchor": "turn-taking",
        "search_term": "real-time interaction"
      },
      {
        "anchor": "turn-taking",
        "search_term": "low latency"
      },
      {
        "anchor": "turn-taking",
        "search_term": "synchronous data"
      },
      {
        "anchor": "turn-taking",
        "search_term": "data scarcity"
      },
      {
        "anchor": "turn-taking",
        "search_term": "multichannel dialogue"
      },
      {
        "anchor": "turn-taking",
        "search_term": "temporal dynamics"
      },
      {
        "anchor": "turn-taking",
        "search_term": "behavioral arbitration"
      },
      {
        "anchor": "turn-taking",
        "search_term": "semantic coherence"
      },
      {
        "anchor": "turn-taking",
        "search_term": "neural codec"
      },
      {
        "anchor": "turn-taking",
        "search_term": "discrete tokens"
      },
      {
        "anchor": "turn-taking",
        "search_term": "audio tokens"
      },
      {
        "anchor": "turn-taking",
        "search_term": "floor transfer offset"
      },
      {
        "anchor": "turn-taking",
        "search_term": "stop latency"
      },
      {
        "anchor": "turn-taking",
        "search_term": "interruption response delay"
      }
    ]
  },
  {
    "arxiv_id": "2406.10174",
    "anchor": "synchronous dialogue",
    "search_term": "semantic coherence",
    "search_record": {
      "id": "http://arxiv.org/abs/2406.10174v1",
      "title": "Let the Poem Hit the Rhythm: Using a Byte-Based Transformer for Beat-Aligned Poetry Generation",
      "summary": "The intersection between poetry and music provides an interesting case for computational creativity, yet remains relatively unexplored. This paper explores the integration of poetry and music through the lens of beat patterns, investigating whether a byte-based language model can generate words that fit specific beat patterns within the context of poetry. Drawing on earlier studies, we developed a method to train a byte-based transformer model, ByT5, to align poems with beat patterns. The results demonstrate a high level of beat alignment while maintaining semantic coherence. Future work will aim to improve the model's ability to create complete beat-aligned poems.",
      "published": "2024-06-14T16:54:48Z"
    },
    "metadata": {
      "arxiv_id": "2406.10174",
      "title": "Let the Poem Hit the Rhythm: Using a Byte-Based Transformer for Beat-Aligned Poetry Generation",
      "summary": "The intersection between poetry and music provides an interesting case for computational creativity, yet remains relatively unexplored. This paper explores the integration of poetry and music through the lens of beat patterns, investigating whether a byte-based language model can generate words that fit specific beat patterns within the context of poetry. Drawing on earlier studies, we developed a method to train a byte-based transformer model, ByT5, to align poems with beat patterns. The results demonstrate a high level of beat alignment while maintaining semantic coherence. Future work will aim to improve the model's ability to create complete beat-aligned poems.",
      "authors": [
        "Mohamad Elzohbi",
        "Richard Zhao"
      ],
      "published": "2024-06-14T16:54:48Z",
      "updated": "2024-06-14T16:54:48Z",
      "categories": [
        "cs.CL"
      ],
      "pdf_url": "https://arxiv.org/pdf/2406.10174v1",
      "landing_url": "https://arxiv.org/abs/2406.10174v1",
      "doi": "https://doi.org/10.48550/arXiv.2406.10174"
    },
    "queries": [
      {
        "anchor": "synchronous dialogue",
        "search_term": "semantic coherence"
      }
    ]
  },
  {
    "arxiv_id": "2406.10450",
    "anchor": "synchronous dialogue",
    "search_term": "discrete tokens",
    "search_record": {
      "id": "http://arxiv.org/abs/2406.10450v3",
      "title": "TokenRec: Learning to Tokenize ID for LLM-based Generative Recommendation",
      "summary": "There is a growing interest in utilizing large-scale language models (LLMs) to advance next-generation Recommender Systems (RecSys), driven by their outstanding language understanding and in-context learning capabilities. In this scenario, tokenizing (i.e., indexing) users and items becomes essential for ensuring a seamless alignment of LLMs with recommendations. While several studies have made progress in representing users and items through textual contents or latent representations, challenges remain in efficiently capturing high-order collaborative knowledge into discrete tokens that are compatible with LLMs. Additionally, the majority of existing tokenization approaches often face difficulties in generalizing effectively to new/unseen users or items that were not in the training corpus. To address these challenges, we propose a novel framework called TokenRec, which introduces not only an effective ID tokenization strategy but also an efficient retrieval paradigm for LLM-based recommendations. Specifically, our tokenization strategy, Masked Vector-Quantized (MQ) Tokenizer, involves quantizing the masked user/item representations learned from collaborative filtering into discrete tokens, thus achieving a smooth incorporation of high-order collaborative knowledge and a generalizable tokenization of users and items for LLM-based RecSys. Meanwhile, our generative retrieval paradigm is designed to efficiently recommend top-$K$ items for users to eliminate the need for the time-consuming auto-regressive decoding and beam search processes used by LLMs, thus significantly reducing inference time. Comprehensive experiments validate the effectiveness of the proposed methods, demonstrating that TokenRec outperforms competitive benchmarks, including both traditional recommender systems and emerging LLM-based recommender systems.",
      "published": "2024-06-15T00:07:44Z"
    },
    "metadata": {
      "arxiv_id": "2406.10450",
      "title": "TokenRec: Learning to Tokenize ID for LLM-based Generative Recommendation",
      "summary": "There is a growing interest in utilizing large-scale language models (LLMs) to advance next-generation Recommender Systems (RecSys), driven by their outstanding language understanding and in-context learning capabilities. In this scenario, tokenizing (i.e., indexing) users and items becomes essential for ensuring a seamless alignment of LLMs with recommendations. While several studies have made progress in representing users and items through textual contents or latent representations, challenges remain in efficiently capturing high-order collaborative knowledge into discrete tokens that are compatible with LLMs. Additionally, the majority of existing tokenization approaches often face difficulties in generalizing effectively to new/unseen users or items that were not in the training corpus. To address these challenges, we propose a novel framework called TokenRec, which introduces not only an effective ID tokenization strategy but also an efficient retrieval paradigm for LLM-based recommendations. Specifically, our tokenization strategy, Masked Vector-Quantized (MQ) Tokenizer, involves quantizing the masked user/item representations learned from collaborative filtering into discrete tokens, thus achieving a smooth incorporation of high-order collaborative knowledge and a generalizable tokenization of users and items for LLM-based RecSys. Meanwhile, our generative retrieval paradigm is designed to efficiently recommend top-$K$ items for users to eliminate the need for the time-consuming auto-regressive decoding and beam search processes used by LLMs, thus significantly reducing inference time. Comprehensive experiments validate the effectiveness of the proposed methods, demonstrating that TokenRec outperforms competitive benchmarks, including both traditional recommender systems and emerging LLM-based recommender systems.",
      "authors": [
        "Haohao Qu",
        "Wenqi Fan",
        "Zihuai Zhao",
        "Qing Li"
      ],
      "published": "2024-06-15T00:07:44Z",
      "updated": "2025-08-15T05:34:06Z",
      "categories": [
        "cs.IR",
        "cs.AI",
        "cs.CL"
      ],
      "pdf_url": "https://arxiv.org/pdf/2406.10450v3",
      "landing_url": "https://arxiv.org/abs/2406.10450v3",
      "doi": "https://doi.org/10.48550/arXiv.2406.10450"
    },
    "queries": [
      {
        "anchor": "synchronous dialogue",
        "search_term": "discrete tokens"
      }
    ]
  },
  {
    "arxiv_id": "2406.10735",
    "anchor": "full-duplex",
    "search_term": "audio tokens",
    "search_record": {
      "id": "http://arxiv.org/abs/2406.10735v1",
      "title": "How Should We Extract Discrete Audio Tokens from Self-Supervised Models?",
      "summary": "Discrete audio tokens have recently gained attention for their potential to bridge the gap between audio and language processing. Ideal audio tokens must preserve content, paralinguistic elements, speaker identity, and many other audio details. Current audio tokenization methods fall into two categories: Semantic tokens, acquired through quantization of Self-Supervised Learning (SSL) models, and Neural compression-based tokens (codecs). Although previous studies have benchmarked codec models to identify optimal configurations, the ideal setup for quantizing pretrained SSL models remains unclear. This paper explores the optimal configuration of semantic tokens across discriminative and generative tasks. We propose a scalable solution to train a universal vocoder across multiple SSL layers. Furthermore, an attention mechanism is employed to identify task-specific influential layers, enhancing the adaptability and performance of semantic tokens in diverse audio applications.",
      "published": "2024-06-15T20:43:07Z"
    },
    "metadata": {
      "arxiv_id": "2406.10735",
      "title": "How Should We Extract Discrete Audio Tokens from Self-Supervised Models?",
      "summary": "Discrete audio tokens have recently gained attention for their potential to bridge the gap between audio and language processing. Ideal audio tokens must preserve content, paralinguistic elements, speaker identity, and many other audio details. Current audio tokenization methods fall into two categories: Semantic tokens, acquired through quantization of Self-Supervised Learning (SSL) models, and Neural compression-based tokens (codecs). Although previous studies have benchmarked codec models to identify optimal configurations, the ideal setup for quantizing pretrained SSL models remains unclear. This paper explores the optimal configuration of semantic tokens across discriminative and generative tasks. We propose a scalable solution to train a universal vocoder across multiple SSL layers. Furthermore, an attention mechanism is employed to identify task-specific influential layers, enhancing the adaptability and performance of semantic tokens in diverse audio applications.",
      "authors": [
        "Pooneh Mousavi",
        "Jarod Duret",
        "Salah Zaiem",
        "Luca Della Libera",
        "Artem Ploujnikov",
        "Cem Subakan",
        "Mirco Ravanelli"
      ],
      "published": "2024-06-15T20:43:07Z",
      "updated": "2024-06-15T20:43:07Z",
      "categories": [
        "cs.SD",
        "cs.AI",
        "cs.CL",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2406.10735v1",
      "landing_url": "https://arxiv.org/abs/2406.10735v1",
      "doi": "https://doi.org/10.48550/arXiv.2406.10735"
    },
    "queries": [
      {
        "anchor": "full-duplex",
        "search_term": "audio tokens"
      },
      {
        "anchor": "spoken language models",
        "search_term": "audio tokens"
      },
      {
        "anchor": "synchronous dialogue",
        "search_term": "audio tokens"
      },
      {
        "anchor": "turn-taking",
        "search_term": "audio tokens"
      }
    ]
  },
  {
    "arxiv_id": "2406.11037",
    "anchor": "spoken language models",
    "search_term": "true full-duplex",
    "search_record": {
      "id": "http://arxiv.org/abs/2406.11037v1",
      "title": "NAST: Noise Aware Speech Tokenization for Speech Language Models",
      "summary": "Speech tokenization is the task of representing speech signals as a sequence of discrete units. Such representations can be later used for various downstream tasks including automatic speech recognition, text-to-speech, etc. More relevant to this study, such representation serves as the basis of Speech Language Models. In this work, we tackle the task of speech tokenization under the noisy setup and present NAST: Noise Aware Speech Tokenization for Speech Language Models. NAST is composed of three main components: (i) a predictor; (ii) a residual encoder; and (iii) a decoder. We evaluate the efficiency of NAST considering several spoken language modeling tasks and show that NAST is superior to the evaluated baselines across all setups. Lastly, we analyze NAST and show its disentanglement properties and robustness to signal variations in the form of noise, reverberation, pitch-shift, and time-stretch. Code and pre-trained models are available at https://github.com/ShovalMessica/NAST.",
      "published": "2024-06-16T18:20:45Z"
    },
    "metadata": {
      "arxiv_id": "2406.11037",
      "title": "NAST: Noise Aware Speech Tokenization for Speech Language Models",
      "summary": "Speech tokenization is the task of representing speech signals as a sequence of discrete units. Such representations can be later used for various downstream tasks including automatic speech recognition, text-to-speech, etc. More relevant to this study, such representation serves as the basis of Speech Language Models. In this work, we tackle the task of speech tokenization under the noisy setup and present NAST: Noise Aware Speech Tokenization for Speech Language Models. NAST is composed of three main components: (i) a predictor; (ii) a residual encoder; and (iii) a decoder. We evaluate the efficiency of NAST considering several spoken language modeling tasks and show that NAST is superior to the evaluated baselines across all setups. Lastly, we analyze NAST and show its disentanglement properties and robustness to signal variations in the form of noise, reverberation, pitch-shift, and time-stretch. Code and pre-trained models are available at https://github.com/ShovalMessica/NAST.",
      "authors": [
        "Shoval Messica",
        "Yossi Adi"
      ],
      "published": "2024-06-16T18:20:45Z",
      "updated": "2024-06-16T18:20:45Z",
      "categories": [
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2406.11037v1",
      "landing_url": "https://arxiv.org/abs/2406.11037v1",
      "doi": "https://doi.org/10.48550/arXiv.2406.11037"
    },
    "queries": [
      {
        "anchor": "spoken language models",
        "search_term": "true full-duplex"
      },
      {
        "anchor": "spoken language models",
        "search_term": "pseudo full-duplex"
      },
      {
        "anchor": "spoken language models",
        "search_term": "engineered synchronization"
      },
      {
        "anchor": "spoken language models",
        "search_term": "learned synchronization"
      },
      {
        "anchor": "spoken language models",
        "search_term": "synchronous data"
      },
      {
        "anchor": "spoken language models",
        "search_term": "multichannel dialogue"
      },
      {
        "anchor": "spoken language models",
        "search_term": "behavioral arbitration"
      },
      {
        "anchor": "spoken language models",
        "search_term": "floor transfer offset"
      },
      {
        "anchor": "spoken language models",
        "search_term": "stop latency"
      },
      {
        "anchor": "spoken language models",
        "search_term": "interruption response delay"
      }
    ]
  },
  {
    "arxiv_id": "2406.11638",
    "anchor": "spoken language models",
    "search_term": "modular architecture",
    "search_record": {
      "id": "http://arxiv.org/abs/2406.11638v1",
      "title": "MASAI: Modular Architecture for Software-engineering AI Agents",
      "summary": "A common method to solve complex problems in software engineering, is to divide the problem into multiple sub-problems. Inspired by this, we propose a Modular Architecture for Software-engineering AI (MASAI) agents, where different LLM-powered sub-agents are instantiated with well-defined objectives and strategies tuned to achieve those objectives. Our modular architecture offers several advantages: (1) employing and tuning different problem-solving strategies across sub-agents, (2) enabling sub-agents to gather information from different sources scattered throughout a repository, and (3) avoiding unnecessarily long trajectories which inflate costs and add extraneous context. MASAI enabled us to achieve the highest performance (28.33% resolution rate) on the popular and highly challenging SWE-bench Lite dataset consisting of 300 GitHub issues from 11 Python repositories. We conduct a comprehensive evaluation of MASAI relative to other agentic methods and analyze the effects of our design decisions and their contribution to the success of MASAI.",
      "published": "2024-06-17T15:19:51Z"
    },
    "metadata": {
      "arxiv_id": "2406.11638",
      "title": "MASAI: Modular Architecture for Software-engineering AI Agents",
      "summary": "A common method to solve complex problems in software engineering, is to divide the problem into multiple sub-problems. Inspired by this, we propose a Modular Architecture for Software-engineering AI (MASAI) agents, where different LLM-powered sub-agents are instantiated with well-defined objectives and strategies tuned to achieve those objectives. Our modular architecture offers several advantages: (1) employing and tuning different problem-solving strategies across sub-agents, (2) enabling sub-agents to gather information from different sources scattered throughout a repository, and (3) avoiding unnecessarily long trajectories which inflate costs and add extraneous context. MASAI enabled us to achieve the highest performance (28.33% resolution rate) on the popular and highly challenging SWE-bench Lite dataset consisting of 300 GitHub issues from 11 Python repositories. We conduct a comprehensive evaluation of MASAI relative to other agentic methods and analyze the effects of our design decisions and their contribution to the success of MASAI.",
      "authors": [
        "Daman Arora",
        "Atharv Sonwane",
        "Nalin Wadhwa",
        "Abhav Mehrotra",
        "Saiteja Utpala",
        "Ramakrishna Bairi",
        "Aditya Kanade",
        "Nagarajan Natarajan"
      ],
      "published": "2024-06-17T15:19:51Z",
      "updated": "2024-06-17T15:19:51Z",
      "categories": [
        "cs.AI",
        "cs.SE"
      ],
      "pdf_url": "https://arxiv.org/pdf/2406.11638v1",
      "landing_url": "https://arxiv.org/abs/2406.11638v1",
      "doi": "https://doi.org/10.48550/arXiv.2406.11638"
    },
    "queries": [
      {
        "anchor": "spoken language models",
        "search_term": "modular architecture"
      },
      {
        "anchor": "synchronous dialogue",
        "search_term": "modular architecture"
      },
      {
        "anchor": "turn-taking",
        "search_term": "modular architecture"
      }
    ]
  },
  {
    "arxiv_id": "2406.12233",
    "anchor": "spoken language models",
    "search_term": "audio tokens",
    "search_record": {
      "id": "http://arxiv.org/abs/2406.12233v1",
      "title": "SyncVSR: Data-Efficient Visual Speech Recognition with End-to-End Crossmodal Audio Token Synchronization",
      "summary": "Visual Speech Recognition (VSR) stands at the intersection of computer vision and speech recognition, aiming to interpret spoken content from visual cues. A prominent challenge in VSR is the presence of homophenes-visually similar lip gestures that represent different phonemes. Prior approaches have sought to distinguish fine-grained visemes by aligning visual and auditory semantics, but often fell short of full synchronization. To address this, we present SyncVSR, an end-to-end learning framework that leverages quantized audio for frame-level crossmodal supervision. By integrating a projection layer that synchronizes visual representation with acoustic data, our encoder learns to generate discrete audio tokens from a video sequence in a non-autoregressive manner. SyncVSR shows versatility across tasks, languages, and modalities at the cost of a forward pass. Our empirical evaluations show that it not only achieves state-of-the-art results but also reduces data usage by up to ninefold.",
      "published": "2024-06-18T03:14:22Z"
    },
    "metadata": {
      "arxiv_id": "2406.12233",
      "title": "SyncVSR: Data-Efficient Visual Speech Recognition with End-to-End Crossmodal Audio Token Synchronization",
      "summary": "Visual Speech Recognition (VSR) stands at the intersection of computer vision and speech recognition, aiming to interpret spoken content from visual cues. A prominent challenge in VSR is the presence of homophenes-visually similar lip gestures that represent different phonemes. Prior approaches have sought to distinguish fine-grained visemes by aligning visual and auditory semantics, but often fell short of full synchronization. To address this, we present SyncVSR, an end-to-end learning framework that leverages quantized audio for frame-level crossmodal supervision. By integrating a projection layer that synchronizes visual representation with acoustic data, our encoder learns to generate discrete audio tokens from a video sequence in a non-autoregressive manner. SyncVSR shows versatility across tasks, languages, and modalities at the cost of a forward pass. Our empirical evaluations show that it not only achieves state-of-the-art results but also reduces data usage by up to ninefold.",
      "authors": [
        "Young Jin Ahn",
        "Jungwoo Park",
        "Sangha Park",
        "Jonghyun Choi",
        "Kee-Eung Kim"
      ],
      "published": "2024-06-18T03:14:22Z",
      "updated": "2024-06-18T03:14:22Z",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2406.12233v1",
      "landing_url": "https://arxiv.org/abs/2406.12233v1",
      "doi": "https://doi.org/10.48550/arXiv.2406.12233"
    },
    "queries": [
      {
        "anchor": "spoken language models",
        "search_term": "audio tokens"
      },
      {
        "anchor": "synchronous dialogue",
        "search_term": "audio tokens"
      },
      {
        "anchor": "turn-taking",
        "search_term": "audio tokens"
      }
    ]
  },
  {
    "arxiv_id": "2406.13093",
    "anchor": "spoken language models",
    "search_term": "real-time interaction",
    "search_record": {
      "id": "http://arxiv.org/abs/2406.13093v1",
      "title": "RITA: A Real-time Interactive Talking Avatars Framework",
      "summary": "RITA presents a high-quality real-time interactive framework built upon generative models, designed with practical applications in mind. Our framework enables the transformation of user-uploaded photos into digital avatars that can engage in real-time dialogue interactions. By leveraging the latest advancements in generative modeling, we have developed a versatile platform that not only enhances the user experience through dynamic conversational avatars but also opens new avenues for applications in virtual reality, online education, and interactive gaming. This work showcases the potential of integrating computer vision and natural language processing technologies to create immersive and interactive digital personas, pushing the boundaries of how we interact with digital content.",
      "published": "2024-06-18T22:53:15Z"
    },
    "metadata": {
      "arxiv_id": "2406.13093",
      "title": "RITA: A Real-time Interactive Talking Avatars Framework",
      "summary": "RITA presents a high-quality real-time interactive framework built upon generative models, designed with practical applications in mind. Our framework enables the transformation of user-uploaded photos into digital avatars that can engage in real-time dialogue interactions. By leveraging the latest advancements in generative modeling, we have developed a versatile platform that not only enhances the user experience through dynamic conversational avatars but also opens new avenues for applications in virtual reality, online education, and interactive gaming. This work showcases the potential of integrating computer vision and natural language processing technologies to create immersive and interactive digital personas, pushing the boundaries of how we interact with digital content.",
      "authors": [
        "Wuxinlin Cheng",
        "Cheng Wan",
        "Yupeng Cao",
        "Sihan Chen"
      ],
      "published": "2024-06-18T22:53:15Z",
      "updated": "2024-06-18T22:53:15Z",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.HC"
      ],
      "pdf_url": "https://arxiv.org/pdf/2406.13093v1",
      "landing_url": "https://arxiv.org/abs/2406.13093v1",
      "doi": "https://doi.org/10.48550/arXiv.2406.13093"
    },
    "queries": [
      {
        "anchor": "spoken language models",
        "search_term": "real-time interaction"
      },
      {
        "anchor": "synchronous dialogue",
        "search_term": "real-time interaction"
      },
      {
        "anchor": "turn-taking",
        "search_term": "real-time interaction"
      }
    ]
  },
  {
    "arxiv_id": "2406.13345",
    "anchor": "synchronous dialogue",
    "search_term": "low latency",
    "search_record": {
      "id": "http://arxiv.org/abs/2406.13345v1",
      "title": "Low Latency Visual Inertial Odometry with On-Sensor Accelerated Optical Flow for Resource-Constrained UAVs",
      "summary": "Visual Inertial Odometry (VIO) is the task of estimating the movement trajectory of an agent from an onboard camera stream fused with additional Inertial Measurement Unit (IMU) measurements. A crucial subtask within VIO is the tracking of features, which can be achieved through Optical Flow (OF). As the calculation of OF is a resource-demanding task in terms of computational load and memory footprint, which needs to be executed at low latency, especially in robotic applications, OF estimation is today performed on powerful CPUs or GPUs. This restricts its use in a broad spectrum of applications where the deployment of such powerful, power-hungry processors is unfeasible due to constraints related to cost, size, and power consumption. On-sensor hardware acceleration is a promising approach to enable low latency VIO even on resource-constrained devices such as nano drones. This paper assesses the speed-up in a VIO sensor system exploiting a compact OF sensor consisting of a global shutter camera and an Application Specific Integrated Circuit (ASIC). By replacing the feature tracking logic of the VINS-Mono pipeline with data from this OF camera, we demonstrate a 49.4% reduction in latency and a 53.7% reduction of compute load of the VIO pipeline over the original VINS-Mono implementation, allowing VINS-Mono operation up to 50 FPS instead of 20 FPS on the quad-core ARM Cortex-A72 processor of a Raspberry Pi Compute Module 4.",
      "published": "2024-06-19T08:51:19Z"
    },
    "metadata": {
      "arxiv_id": "2406.13345",
      "title": "Low Latency Visual Inertial Odometry with On-Sensor Accelerated Optical Flow for Resource-Constrained UAVs",
      "summary": "Visual Inertial Odometry (VIO) is the task of estimating the movement trajectory of an agent from an onboard camera stream fused with additional Inertial Measurement Unit (IMU) measurements. A crucial subtask within VIO is the tracking of features, which can be achieved through Optical Flow (OF). As the calculation of OF is a resource-demanding task in terms of computational load and memory footprint, which needs to be executed at low latency, especially in robotic applications, OF estimation is today performed on powerful CPUs or GPUs. This restricts its use in a broad spectrum of applications where the deployment of such powerful, power-hungry processors is unfeasible due to constraints related to cost, size, and power consumption. On-sensor hardware acceleration is a promising approach to enable low latency VIO even on resource-constrained devices such as nano drones. This paper assesses the speed-up in a VIO sensor system exploiting a compact OF sensor consisting of a global shutter camera and an Application Specific Integrated Circuit (ASIC). By replacing the feature tracking logic of the VINS-Mono pipeline with data from this OF camera, we demonstrate a 49.4% reduction in latency and a 53.7% reduction of compute load of the VIO pipeline over the original VINS-Mono implementation, allowing VINS-Mono operation up to 50 FPS instead of 20 FPS on the quad-core ARM Cortex-A72 processor of a Raspberry Pi Compute Module 4.",
      "authors": [
        "Jonas Kühne",
        "Michele Magno",
        "Luca Benini"
      ],
      "published": "2024-06-19T08:51:19Z",
      "updated": "2024-06-19T08:51:19Z",
      "categories": [
        "cs.CV",
        "eess.IV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2406.13345v1",
      "landing_url": "https://arxiv.org/abs/2406.13345v1",
      "doi": "https://doi.org/10.1109/JSEN.2024.3406948"
    },
    "queries": [
      {
        "anchor": "synchronous dialogue",
        "search_term": "low latency"
      }
    ]
  },
  {
    "arxiv_id": "2406.13431",
    "anchor": "spoken language models",
    "search_term": "discrete tokens",
    "search_record": {
      "id": "http://arxiv.org/abs/2406.13431v2",
      "title": "Children's Speech Recognition through Discrete Token Enhancement",
      "summary": "Children's speech recognition is considered a low-resource task mainly due to the lack of publicly available data. There are several reasons for such data scarcity, including expensive data collection and annotation processes, and data privacy, among others. Transforming speech signals into discrete tokens that do not carry sensitive information but capture both linguistic and acoustic information could be a solution for privacy concerns. In this study, we investigate the integration of discrete speech tokens into children's speech recognition systems as input without significantly degrading the ASR performance. Additionally, we explored single-view and multi-view strategies for creating these discrete labels. Furthermore, we tested the models for generalization capabilities with unseen domain and nativity dataset. Results reveal that the discrete token ASR for children achieves nearly equivalent performance with an approximate 83% reduction in parameters.",
      "published": "2024-06-19T10:45:12Z"
    },
    "metadata": {
      "arxiv_id": "2406.13431",
      "title": "Children's Speech Recognition through Discrete Token Enhancement",
      "summary": "Children's speech recognition is considered a low-resource task mainly due to the lack of publicly available data. There are several reasons for such data scarcity, including expensive data collection and annotation processes, and data privacy, among others. Transforming speech signals into discrete tokens that do not carry sensitive information but capture both linguistic and acoustic information could be a solution for privacy concerns. In this study, we investigate the integration of discrete speech tokens into children's speech recognition systems as input without significantly degrading the ASR performance. Additionally, we explored single-view and multi-view strategies for creating these discrete labels. Furthermore, we tested the models for generalization capabilities with unseen domain and nativity dataset. Results reveal that the discrete token ASR for children achieves nearly equivalent performance with an approximate 83% reduction in parameters.",
      "authors": [
        "Vrunda N. Sukhadia",
        "Shammur Absar Chowdhury"
      ],
      "published": "2024-06-19T10:45:12Z",
      "updated": "2024-06-24T15:31:59Z",
      "categories": [
        "cs.CL",
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2406.13431v2",
      "landing_url": "https://arxiv.org/abs/2406.13431v2",
      "doi": "https://doi.org/10.48550/arXiv.2406.13431"
    },
    "queries": [
      {
        "anchor": "spoken language models",
        "search_term": "discrete tokens"
      },
      {
        "anchor": "synchronous dialogue",
        "search_term": "discrete tokens"
      },
      {
        "anchor": "turn-taking",
        "search_term": "discrete tokens"
      }
    ]
  },
  {
    "arxiv_id": "2406.14294",
    "anchor": "spoken language models",
    "search_term": "audio tokens",
    "search_record": {
      "id": "http://arxiv.org/abs/2406.14294v2",
      "title": "DASB - Discrete Audio and Speech Benchmark",
      "summary": "Discrete audio tokens have recently gained considerable attention for their potential to connect audio and language processing, enabling the creation of modern multimodal large language models. Ideal audio tokens must effectively preserve phonetic and semantic content along with paralinguistic information, speaker identity, and other details. While several types of audio tokens have been recently proposed, identifying the optimal tokenizer for various tasks is challenging due to the inconsistent evaluation settings in existing studies. To address this gap, we release the Discrete Audio and Speech Benchmark (DASB), a comprehensive leaderboard for benchmarking discrete audio tokens across a wide range of discriminative tasks, including speech recognition, speaker identification and verification, emotion recognition, keyword spotting, and intent classification, as well as generative tasks such as speech enhancement, separation, and text-to-speech. Our results show that, on average, semantic tokens outperform compression tokens across most discriminative and generative tasks. However, the performance gap between semantic tokens and standard continuous representations remains substantial, highlighting the need for further research in this field.",
      "published": "2024-06-20T13:23:27Z"
    },
    "metadata": {
      "arxiv_id": "2406.14294",
      "title": "DASB - Discrete Audio and Speech Benchmark",
      "summary": "Discrete audio tokens have recently gained considerable attention for their potential to connect audio and language processing, enabling the creation of modern multimodal large language models. Ideal audio tokens must effectively preserve phonetic and semantic content along with paralinguistic information, speaker identity, and other details. While several types of audio tokens have been recently proposed, identifying the optimal tokenizer for various tasks is challenging due to the inconsistent evaluation settings in existing studies. To address this gap, we release the Discrete Audio and Speech Benchmark (DASB), a comprehensive leaderboard for benchmarking discrete audio tokens across a wide range of discriminative tasks, including speech recognition, speaker identification and verification, emotion recognition, keyword spotting, and intent classification, as well as generative tasks such as speech enhancement, separation, and text-to-speech. Our results show that, on average, semantic tokens outperform compression tokens across most discriminative and generative tasks. However, the performance gap between semantic tokens and standard continuous representations remains substantial, highlighting the need for further research in this field.",
      "authors": [
        "Pooneh Mousavi",
        "Luca Della Libera",
        "Jarod Duret",
        "Artem Ploujnikov",
        "Cem Subakan",
        "Mirco Ravanelli"
      ],
      "published": "2024-06-20T13:23:27Z",
      "updated": "2024-06-21T17:07:17Z",
      "categories": [
        "cs.SD",
        "cs.AI",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2406.14294v2",
      "landing_url": "https://arxiv.org/abs/2406.14294v2",
      "doi": "https://doi.org/10.48550/arXiv.2406.14294"
    },
    "queries": [
      {
        "anchor": "spoken language models",
        "search_term": "audio tokens"
      },
      {
        "anchor": "synchronous dialogue",
        "search_term": "audio tokens"
      },
      {
        "anchor": "turn-taking",
        "search_term": "audio tokens"
      }
    ]
  },
  {
    "arxiv_id": "2406.15258",
    "anchor": "synchronous dialogue",
    "search_term": "half duplex",
    "search_record": {
      "id": "http://arxiv.org/abs/2406.15258v1",
      "title": "Model-Based Learning for Network Clock Synchronization in Half-Duplex TDMA Networks",
      "summary": "Supporting increasingly higher rates in wireless networks requires highly accurate clock synchronization across the nodes. Motivated by this need, in this work we consider distributed clock synchronization for half-duplex (HD) TDMA wireless networks. We focus on pulse-coupling (PC)-based synchronization as it is practically advantageous for high-speed networks using low-power nodes. Previous works on PC-based synchronization for TDMA networks assumed full-duplex communications, and focused on correcting the clock phase at each node, without synchronizing clocks' frequencies. However, as in the HD regime corrections are temporally sparse, uncompensated clock frequency differences between the nodes result in large phase drifts between updates. Moreover, as the clocks determine the processing rates at the nodes, leaving the clocks' frequencies unsynchronized results in processing rates mismatch between the nodes, leading to a throughput reduction. Our goal in this work is to synchronize both clock frequency and clock phase across the clocks in HD TDMA networks, via distributed processing. The key challenges are the coupling between frequency correction and phase correction, and the lack of a computationally efficient analytical framework for determining the optimal correction signal at the nodes. We address these challenges via a DNN-aided nested loop structure in which the DNN are used for generating the weights applied to the loop input for computing the correction signal. This loop is operated in a sequential manner which decouples frequency and phase compensations, thereby facilitating synchronization of both parameters. Performance evaluation shows that the proposed scheme significantly improves synchronization accuracy compared to the conventional approaches.",
      "published": "2024-06-21T15:47:50Z"
    },
    "metadata": {
      "arxiv_id": "2406.15258",
      "title": "Model-Based Learning for Network Clock Synchronization in Half-Duplex TDMA Networks",
      "summary": "Supporting increasingly higher rates in wireless networks requires highly accurate clock synchronization across the nodes. Motivated by this need, in this work we consider distributed clock synchronization for half-duplex (HD) TDMA wireless networks. We focus on pulse-coupling (PC)-based synchronization as it is practically advantageous for high-speed networks using low-power nodes. Previous works on PC-based synchronization for TDMA networks assumed full-duplex communications, and focused on correcting the clock phase at each node, without synchronizing clocks' frequencies. However, as in the HD regime corrections are temporally sparse, uncompensated clock frequency differences between the nodes result in large phase drifts between updates. Moreover, as the clocks determine the processing rates at the nodes, leaving the clocks' frequencies unsynchronized results in processing rates mismatch between the nodes, leading to a throughput reduction. Our goal in this work is to synchronize both clock frequency and clock phase across the clocks in HD TDMA networks, via distributed processing. The key challenges are the coupling between frequency correction and phase correction, and the lack of a computationally efficient analytical framework for determining the optimal correction signal at the nodes. We address these challenges via a DNN-aided nested loop structure in which the DNN are used for generating the weights applied to the loop input for computing the correction signal. This loop is operated in a sequential manner which decouples frequency and phase compensations, thereby facilitating synchronization of both parameters. Performance evaluation shows that the proposed scheme significantly improves synchronization accuracy compared to the conventional approaches.",
      "authors": [
        "Itay Zino",
        "Ron Dabora",
        "H. Vincent Poor"
      ],
      "published": "2024-06-21T15:47:50Z",
      "updated": "2024-06-21T15:47:50Z",
      "categories": [
        "eess.SP"
      ],
      "pdf_url": "https://arxiv.org/pdf/2406.15258v1",
      "landing_url": "https://arxiv.org/abs/2406.15258v1",
      "doi": "https://doi.org/10.48550/arXiv.2406.15258"
    },
    "queries": [
      {
        "anchor": "synchronous dialogue",
        "search_term": "half duplex"
      }
    ]
  },
  {
    "arxiv_id": "2406.15752",
    "anchor": "spoken language models",
    "search_term": "neural codec",
    "search_record": {
      "id": "http://arxiv.org/abs/2406.15752v1",
      "title": "TacoLM: GaTed Attention Equipped Codec Language Model are Efficient Zero-Shot Text to Speech Synthesizers",
      "summary": "Neural codec language model (LM) has demonstrated strong capability in zero-shot text-to-speech (TTS) synthesis. However, the codec LM often suffers from limitations in inference speed and stability, due to its auto-regressive nature and implicit alignment between text and audio. In this work, to handle these challenges, we introduce a new variant of neural codec LM, namely TacoLM. Specifically, TacoLM introduces a gated attention mechanism to improve the training and inference efficiency and reduce the model size. Meanwhile, an additional gated cross-attention layer is included for each decoder layer, which improves the efficiency and content accuracy of the synthesized speech. In the evaluation of the Librispeech corpus, the proposed TacoLM achieves a better word error rate, speaker similarity, and mean opinion score, with 90% fewer parameters and 5.2 times speed up, compared with VALL-E. Demo and code is available at https://ereboas.github.io/TacoLM/.",
      "published": "2024-06-22T06:39:52Z"
    },
    "metadata": {
      "arxiv_id": "2406.15752",
      "title": "TacoLM: GaTed Attention Equipped Codec Language Model are Efficient Zero-Shot Text to Speech Synthesizers",
      "summary": "Neural codec language model (LM) has demonstrated strong capability in zero-shot text-to-speech (TTS) synthesis. However, the codec LM often suffers from limitations in inference speed and stability, due to its auto-regressive nature and implicit alignment between text and audio. In this work, to handle these challenges, we introduce a new variant of neural codec LM, namely TacoLM. Specifically, TacoLM introduces a gated attention mechanism to improve the training and inference efficiency and reduce the model size. Meanwhile, an additional gated cross-attention layer is included for each decoder layer, which improves the efficiency and content accuracy of the synthesized speech. In the evaluation of the Librispeech corpus, the proposed TacoLM achieves a better word error rate, speaker similarity, and mean opinion score, with 90% fewer parameters and 5.2 times speed up, compared with VALL-E. Demo and code is available at https://ereboas.github.io/TacoLM/.",
      "authors": [
        "Yakun Song",
        "Zhuo Chen",
        "Xiaofei Wang",
        "Ziyang Ma",
        "Guanrou Yang",
        "Xie Chen"
      ],
      "published": "2024-06-22T06:39:52Z",
      "updated": "2024-06-22T06:39:52Z",
      "categories": [
        "eess.AS",
        "cs.AI",
        "cs.CL"
      ],
      "pdf_url": "https://arxiv.org/pdf/2406.15752v1",
      "landing_url": "https://arxiv.org/abs/2406.15752v1",
      "doi": "https://doi.org/10.48550/arXiv.2406.15752"
    },
    "queries": [
      {
        "anchor": "spoken language models",
        "search_term": "neural codec"
      },
      {
        "anchor": "synchronous dialogue",
        "search_term": "neural codec"
      },
      {
        "anchor": "turn-taking",
        "search_term": "neural codec"
      }
    ]
  },
  {
    "arxiv_id": "2406.17310",
    "anchor": "spoken language models",
    "search_term": "discrete tokens",
    "search_record": {
      "id": "http://arxiv.org/abs/2406.17310v1",
      "title": "High Fidelity Text-to-Speech Via Discrete Tokens Using Token Transducer and Group Masked Language Model",
      "summary": "We propose a novel two-stage text-to-speech (TTS) framework with two types of discrete tokens, i.e., semantic and acoustic tokens, for high-fidelity speech synthesis. It features two core components: the Interpreting module, which processes text and a speech prompt into semantic tokens focusing on linguistic contents and alignment, and the Speaking module, which captures the timbre of the target voice to generate acoustic tokens from semantic tokens, enriching speech reconstruction. The Interpreting stage employs a transducer for its robustness in aligning text to speech. In contrast, the Speaking stage utilizes a Conformer-based architecture integrated with a Grouped Masked Language Model (G-MLM) to boost computational efficiency. Our experiments verify that this innovative structure surpasses the conventional models in the zero-shot scenario in terms of speech quality and speaker similarity.",
      "published": "2024-06-25T06:46:47Z"
    },
    "metadata": {
      "arxiv_id": "2406.17310",
      "title": "High Fidelity Text-to-Speech Via Discrete Tokens Using Token Transducer and Group Masked Language Model",
      "summary": "We propose a novel two-stage text-to-speech (TTS) framework with two types of discrete tokens, i.e., semantic and acoustic tokens, for high-fidelity speech synthesis. It features two core components: the Interpreting module, which processes text and a speech prompt into semantic tokens focusing on linguistic contents and alignment, and the Speaking module, which captures the timbre of the target voice to generate acoustic tokens from semantic tokens, enriching speech reconstruction. The Interpreting stage employs a transducer for its robustness in aligning text to speech. In contrast, the Speaking stage utilizes a Conformer-based architecture integrated with a Grouped Masked Language Model (G-MLM) to boost computational efficiency. Our experiments verify that this innovative structure surpasses the conventional models in the zero-shot scenario in terms of speech quality and speaker similarity.",
      "authors": [
        "Joun Yeop Lee",
        "Myeonghun Jeong",
        "Minchan Kim",
        "Ji-Hyun Lee",
        "Hoon-Young Cho",
        "Nam Soo Kim"
      ],
      "published": "2024-06-25T06:46:47Z",
      "updated": "2024-06-25T06:46:47Z",
      "categories": [
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2406.17310v1",
      "landing_url": "https://arxiv.org/abs/2406.17310v1",
      "doi": "https://doi.org/10.48550/arXiv.2406.17310"
    },
    "queries": [
      {
        "anchor": "spoken language models",
        "search_term": "discrete tokens"
      },
      {
        "anchor": "synchronous dialogue",
        "search_term": "discrete tokens"
      },
      {
        "anchor": "turn-taking",
        "search_term": "discrete tokens"
      }
    ]
  },
  {
    "arxiv_id": "2406.17783",
    "anchor": "spoken language models",
    "search_term": "temporal dynamics",
    "search_record": {
      "id": "http://arxiv.org/abs/2406.17783v1",
      "title": "Temporal Dynamics beyond the Exceptional Point in the Ikeda Map with Balanced Gain and Loss",
      "summary": "We investigate the temporal dynamics of the Ikeda Map with Balanced Gain and Loss and in the presence of feedback loops with saturation nonlinearity. From the bifurcation analysis, we find that the temporal evolution of optical power undergoes period quadrupling at the exceptional point (EP) of the system and beyond that, chaotic dynamics emerge in the system and this has been further corroborated from the Largest Lyapunov Exponent (LLE) of the model. For a closer inspection, we analyzed the parameter basin of the system, which further leads to our inference that the Ikeda Map with Balanced Gain and Loss exhibits the emergence of chaotic dynamics beyond the exceptional point (EP). Furthermore, we find that the temporal dynamics beyond the EP regime leads to the onset of Extreme Events (EE) in this system via attractor merging crisis.",
      "published": "2024-05-13T04:40:39Z"
    },
    "metadata": {
      "arxiv_id": "2406.17783",
      "title": "Temporal Dynamics beyond the Exceptional Point in the Ikeda Map with Balanced Gain and Loss",
      "summary": "We investigate the temporal dynamics of the Ikeda Map with Balanced Gain and Loss and in the presence of feedback loops with saturation nonlinearity. From the bifurcation analysis, we find that the temporal evolution of optical power undergoes period quadrupling at the exceptional point (EP) of the system and beyond that, chaotic dynamics emerge in the system and this has been further corroborated from the Largest Lyapunov Exponent (LLE) of the model. For a closer inspection, we analyzed the parameter basin of the system, which further leads to our inference that the Ikeda Map with Balanced Gain and Loss exhibits the emergence of chaotic dynamics beyond the exceptional point (EP). Furthermore, we find that the temporal dynamics beyond the EP regime leads to the onset of Extreme Events (EE) in this system via attractor merging crisis.",
      "authors": [
        "Jyoti Prasad Deka",
        "Amarendra K. Sarma"
      ],
      "published": "2024-05-13T04:40:39Z",
      "updated": "2024-05-13T04:40:39Z",
      "categories": [
        "eess.SP",
        "nlin.CD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2406.17783v1",
      "landing_url": "https://arxiv.org/abs/2406.17783v1",
      "doi": "https://doi.org/10.48550/arXiv.2406.17783"
    },
    "queries": [
      {
        "anchor": "spoken language models",
        "search_term": "temporal dynamics"
      },
      {
        "anchor": "synchronous dialogue",
        "search_term": "temporal dynamics"
      },
      {
        "anchor": "turn-taking",
        "search_term": "temporal dynamics"
      }
    ]
  },
  {
    "arxiv_id": "2407.00050",
    "anchor": "synchronous dialogue",
    "search_term": "discrete tokens",
    "search_record": {
      "id": "http://arxiv.org/abs/2407.00050v1",
      "title": "FoldToken2: Learning compact, invariant and generative protein structure language",
      "summary": "The equivalent nature of 3D coordinates has posed long term challenges in protein structure representation learning, alignment, and generation. Can we create a compact and invariant language that equivalently represents protein structures? Towards this goal, we propose FoldToken2 to transfer equivariant structures into discrete tokens, while maintaining the recoverability of the original structures. From FoldToken1 to FoldToken2, we improve three key components: (1) invariant structure encoder, (2) vector-quantized compressor, and (3) equivalent structure decoder. We evaluate FoldToken2 on the protein structure reconstruction task and show that it outperforms previous FoldToken1 by 20\\% in TMScore and 81\\% in RMSD. FoldToken2 probably be the first method that works well on both single-chain and multi-chain protein structures quantization. We believe that FoldToken2 will inspire further improvement in protein structure representation learning, structure alignment, and structure generation tasks.",
      "published": "2024-06-11T09:24:51Z"
    },
    "metadata": {
      "arxiv_id": "2407.00050",
      "title": "FoldToken2: Learning compact, invariant and generative protein structure language",
      "summary": "The equivalent nature of 3D coordinates has posed long term challenges in protein structure representation learning, alignment, and generation. Can we create a compact and invariant language that equivalently represents protein structures? Towards this goal, we propose FoldToken2 to transfer equivariant structures into discrete tokens, while maintaining the recoverability of the original structures. From FoldToken1 to FoldToken2, we improve three key components: (1) invariant structure encoder, (2) vector-quantized compressor, and (3) equivalent structure decoder. We evaluate FoldToken2 on the protein structure reconstruction task and show that it outperforms previous FoldToken1 by 20\\% in TMScore and 81\\% in RMSD. FoldToken2 probably be the first method that works well on both single-chain and multi-chain protein structures quantization. We believe that FoldToken2 will inspire further improvement in protein structure representation learning, structure alignment, and structure generation tasks.",
      "authors": [
        "Zhangyang Gao",
        "Cheng Tan",
        "Stan Z. Li"
      ],
      "published": "2024-06-11T09:24:51Z",
      "updated": "2024-06-11T09:24:51Z",
      "categories": [
        "q-bio.BM",
        "cs.AI",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2407.00050v1",
      "landing_url": "https://arxiv.org/abs/2407.00050v1",
      "doi": "https://doi.org/10.48550/arXiv.2407.00050"
    },
    "queries": [
      {
        "anchor": "synchronous dialogue",
        "search_term": "discrete tokens"
      }
    ]
  },
  {
    "arxiv_id": "2407.00638",
    "anchor": "synchronous dialogue",
    "search_term": "semantic coherence",
    "search_record": {
      "id": "http://arxiv.org/abs/2407.00638v1",
      "title": "A Collocation-based Method for Addressing Challenges in Word-level Metric Differential Privacy",
      "summary": "Applications of Differential Privacy (DP) in NLP must distinguish between the syntactic level on which a proposed mechanism operates, often taking the form of $\\textit{word-level}$ or $\\textit{document-level}$ privatization. Recently, several word-level $\\textit{Metric}$ Differential Privacy approaches have been proposed, which rely on this generalized DP notion for operating in word embedding spaces. These approaches, however, often fail to produce semantically coherent textual outputs, and their application at the sentence- or document-level is only possible by a basic composition of word perturbations. In this work, we strive to address these challenges by operating $\\textit{between}$ the word and sentence levels, namely with $\\textit{collocations}$. By perturbing n-grams rather than single words, we devise a method where composed privatized outputs have higher semantic coherence and variable length. This is accomplished by constructing an embedding model based on frequently occurring word groups, in which unigram words co-exist with bi- and trigram collocations. We evaluate our method in utility and privacy tests, which make a clear case for tokenization strategies beyond the word level.",
      "published": "2024-06-30T09:37:34Z"
    },
    "metadata": {
      "arxiv_id": "2407.00638",
      "title": "A Collocation-based Method for Addressing Challenges in Word-level Metric Differential Privacy",
      "summary": "Applications of Differential Privacy (DP) in NLP must distinguish between the syntactic level on which a proposed mechanism operates, often taking the form of $\\textit{word-level}$ or $\\textit{document-level}$ privatization. Recently, several word-level $\\textit{Metric}$ Differential Privacy approaches have been proposed, which rely on this generalized DP notion for operating in word embedding spaces. These approaches, however, often fail to produce semantically coherent textual outputs, and their application at the sentence- or document-level is only possible by a basic composition of word perturbations. In this work, we strive to address these challenges by operating $\\textit{between}$ the word and sentence levels, namely with $\\textit{collocations}$. By perturbing n-grams rather than single words, we devise a method where composed privatized outputs have higher semantic coherence and variable length. This is accomplished by constructing an embedding model based on frequently occurring word groups, in which unigram words co-exist with bi- and trigram collocations. We evaluate our method in utility and privacy tests, which make a clear case for tokenization strategies beyond the word level.",
      "authors": [
        "Stephen Meisenbacher",
        "Maulik Chevli",
        "Florian Matthes"
      ],
      "published": "2024-06-30T09:37:34Z",
      "updated": "2024-06-30T09:37:34Z",
      "categories": [
        "cs.CL"
      ],
      "pdf_url": "https://arxiv.org/pdf/2407.00638v1",
      "landing_url": "https://arxiv.org/abs/2407.00638v1",
      "doi": "https://doi.org/10.48550/arXiv.2407.00638"
    },
    "queries": [
      {
        "anchor": "synchronous dialogue",
        "search_term": "semantic coherence"
      }
    ]
  },
  {
    "arxiv_id": "2407.01911",
    "anchor": "spoken language models",
    "search_term": "true full-duplex",
    "search_record": {
      "id": "http://arxiv.org/abs/2407.01911v1",
      "title": "Investigating the Effects of Large-Scale Pseudo-Stereo Data and Different Speech Foundation Model on Dialogue Generative Spoken Language Model",
      "summary": "Recent efforts in Spoken Dialogue Modeling aim to synthesize spoken dialogue without the need for direct transcription, thereby preserving the wealth of non-textual information inherent in speech. However, this approach faces a challenge when speakers talk simultaneously, requiring stereo dialogue data with speakers recorded on separate channels, a notably scarce resource. To address this, we have developed an innovative pipeline capable of transforming single-channel dialogue data into pseudo-stereo data. This expanded our training dataset from a mere 2,000 to an impressive 17,600 hours, significantly enriching the diversity and quality of the training examples available. The inclusion of this pseudo-stereo data has proven to be effective in improving the performance of spoken dialogue language models. Additionally, we explored the use of discrete units of different speech foundation models for spoken dialogue generation.",
      "published": "2024-07-02T03:22:41Z"
    },
    "metadata": {
      "arxiv_id": "2407.01911",
      "title": "Investigating the Effects of Large-Scale Pseudo-Stereo Data and Different Speech Foundation Model on Dialogue Generative Spoken Language Model",
      "summary": "Recent efforts in Spoken Dialogue Modeling aim to synthesize spoken dialogue without the need for direct transcription, thereby preserving the wealth of non-textual information inherent in speech. However, this approach faces a challenge when speakers talk simultaneously, requiring stereo dialogue data with speakers recorded on separate channels, a notably scarce resource. To address this, we have developed an innovative pipeline capable of transforming single-channel dialogue data into pseudo-stereo data. This expanded our training dataset from a mere 2,000 to an impressive 17,600 hours, significantly enriching the diversity and quality of the training examples available. The inclusion of this pseudo-stereo data has proven to be effective in improving the performance of spoken dialogue language models. Additionally, we explored the use of discrete units of different speech foundation models for spoken dialogue generation.",
      "authors": [
        "Yu-Kuan Fu",
        "Cheng-Kuang Lee",
        "Hsiu-Hsuan Wang",
        "Hung-yi Lee"
      ],
      "published": "2024-07-02T03:22:41Z",
      "updated": "2024-07-02T03:22:41Z",
      "categories": [
        "cs.CL",
        "cs.HC",
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2407.01911v1",
      "landing_url": "https://arxiv.org/abs/2407.01911v1",
      "doi": "https://doi.org/10.48550/arXiv.2407.01911"
    },
    "queries": [
      {
        "anchor": "spoken language models",
        "search_term": "true full-duplex"
      },
      {
        "anchor": "spoken language models",
        "search_term": "pseudo full-duplex"
      },
      {
        "anchor": "spoken language models",
        "search_term": "engineered synchronization"
      },
      {
        "anchor": "spoken language models",
        "search_term": "learned synchronization"
      },
      {
        "anchor": "spoken language models",
        "search_term": "streaming architecture"
      },
      {
        "anchor": "spoken language models",
        "search_term": "real-time interaction"
      },
      {
        "anchor": "spoken language models",
        "search_term": "synchronous data"
      },
      {
        "anchor": "spoken language models",
        "search_term": "multichannel dialogue"
      },
      {
        "anchor": "spoken language models",
        "search_term": "behavioral arbitration"
      },
      {
        "anchor": "spoken language models",
        "search_term": "floor transfer offset"
      },
      {
        "anchor": "spoken language models",
        "search_term": "stop latency"
      },
      {
        "anchor": "spoken language models",
        "search_term": "interruption response delay"
      }
    ]
  },
  {
    "arxiv_id": "2407.02327",
    "anchor": "synchronous dialogue",
    "search_term": "synchronous data",
    "search_record": {
      "id": "http://arxiv.org/abs/2407.02327v1",
      "title": "QSync: Quantization-Minimized Synchronous Distributed Training Across Hybrid Devices",
      "summary": "A number of production deep learning clusters have attempted to explore inference hardware for DNN training, at the off-peak serving hours with many inference GPUs idling. Conducting DNN training with a combination of heterogeneous training and inference GPUs, known as hybrid device training, presents considerable challenges due to disparities in compute capability and significant differences in memory capacity. We propose QSync, a training system that enables efficient synchronous data-parallel DNN training over hybrid devices by strategically exploiting quantized operators. According to each device's available resource capacity, QSync selects a quantization-minimized setting for operators in the distributed DNN training graph, minimizing model accuracy degradation but keeping the training efficiency brought by quantization. We carefully design a predictor with a bi-directional mixed-precision indicator to reflect the sensitivity of DNN layers on fixed-point and floating-point low-precision operators, a replayer with a neighborhood-aware cost mapper to accurately estimate the latency of distributed hybrid mixed-precision training, and then an allocator that efficiently synchronizes workers with minimized model accuracy degradation. QSync bridges the computational graph on PyTorch to an optimized backend for quantization kernel performance and flexible support for various GPU architectures. Extensive experiments show that QSync's predictor can accurately simulate distributed mixed-precision training with <5% error, with a consistent 0.27-1.03% accuracy improvement over the from-scratch training tasks compared to uniform precision.",
      "published": "2024-07-02T14:56:47Z"
    },
    "metadata": {
      "arxiv_id": "2407.02327",
      "title": "QSync: Quantization-Minimized Synchronous Distributed Training Across Hybrid Devices",
      "summary": "A number of production deep learning clusters have attempted to explore inference hardware for DNN training, at the off-peak serving hours with many inference GPUs idling. Conducting DNN training with a combination of heterogeneous training and inference GPUs, known as hybrid device training, presents considerable challenges due to disparities in compute capability and significant differences in memory capacity. We propose QSync, a training system that enables efficient synchronous data-parallel DNN training over hybrid devices by strategically exploiting quantized operators. According to each device's available resource capacity, QSync selects a quantization-minimized setting for operators in the distributed DNN training graph, minimizing model accuracy degradation but keeping the training efficiency brought by quantization. We carefully design a predictor with a bi-directional mixed-precision indicator to reflect the sensitivity of DNN layers on fixed-point and floating-point low-precision operators, a replayer with a neighborhood-aware cost mapper to accurately estimate the latency of distributed hybrid mixed-precision training, and then an allocator that efficiently synchronizes workers with minimized model accuracy degradation. QSync bridges the computational graph on PyTorch to an optimized backend for quantization kernel performance and flexible support for various GPU architectures. Extensive experiments show that QSync's predictor can accurately simulate distributed mixed-precision training with <5% error, with a consistent 0.27-1.03% accuracy improvement over the from-scratch training tasks compared to uniform precision.",
      "authors": [
        "Juntao Zhao",
        "Borui Wan",
        "Yanghua Peng",
        "Haibin Lin",
        "Yibo Zhu",
        "Chuan Wu"
      ],
      "published": "2024-07-02T14:56:47Z",
      "updated": "2024-07-02T14:56:47Z",
      "categories": [
        "cs.LG",
        "cs.DC"
      ],
      "pdf_url": "https://arxiv.org/pdf/2407.02327v1",
      "landing_url": "https://arxiv.org/abs/2407.02327v1",
      "doi": "https://doi.org/10.48550/arXiv.2407.02327"
    },
    "queries": [
      {
        "anchor": "synchronous dialogue",
        "search_term": "synchronous data"
      }
    ]
  },
  {
    "arxiv_id": "2407.02896",
    "anchor": "turn-taking",
    "search_term": "true full-duplex",
    "search_record": {
      "id": "http://arxiv.org/abs/2407.02896v2",
      "title": "Predicting and Understanding Turn-Taking Behavior in Open-Ended Group Activities in Virtual Reality",
      "summary": "In networked virtual reality (VR), user behaviors, individual differences, and group dynamics can serve as important signals into future speech behaviors, such as who the next speaker will be and the timing of turn-taking behaviors. The ability to predict and understand these behaviors offers opportunities to provide adaptive and personalized assistance, for example helping users with varying sensory abilities navigate complex social scenes and instantiating virtual moderators with natural behaviors. In this work, we predict turn-taking behaviors using features extracted based on social dynamics literature. We discuss results from a large-scale VR classroom dataset consisting of 77 sessions and 1660 minutes of small-group social interactions collected over four weeks. In our evaluation, gradient boosting classifiers achieved the best performance, with accuracies of 0.71--0.78 AUC (area under the ROC curve) across three tasks concerning the \"what\", \"who\", and \"when\" of turn-taking behaviors. In interpreting these models, we found that group size, listener personality, speech-related behavior (e.g., time elapsed since the listener's last speech event), group gaze (e.g., how much the group looks at the speaker), as well as the listener's and previous speaker's head pitch, head y-axis position, and left hand y-axis position more saliently influenced predictions. Results suggested that these features remain reliable indicators in novel social VR settings, as prediction performance is robust over time and with groups and activities not used in the training dataset. We discuss theoretical and practical implications of the work.",
      "published": "2024-07-03T08:17:59Z"
    },
    "metadata": {
      "arxiv_id": "2407.02896",
      "title": "Predicting and Understanding Turn-Taking Behavior in Open-Ended Group Activities in Virtual Reality",
      "summary": "In networked virtual reality (VR), user behaviors, individual differences, and group dynamics can serve as important signals into future speech behaviors, such as who the next speaker will be and the timing of turn-taking behaviors. The ability to predict and understand these behaviors offers opportunities to provide adaptive and personalized assistance, for example helping users with varying sensory abilities navigate complex social scenes and instantiating virtual moderators with natural behaviors. In this work, we predict turn-taking behaviors using features extracted based on social dynamics literature. We discuss results from a large-scale VR classroom dataset consisting of 77 sessions and 1660 minutes of small-group social interactions collected over four weeks. In our evaluation, gradient boosting classifiers achieved the best performance, with accuracies of 0.71--0.78 AUC (area under the ROC curve) across three tasks concerning the \"what\", \"who\", and \"when\" of turn-taking behaviors. In interpreting these models, we found that group size, listener personality, speech-related behavior (e.g., time elapsed since the listener's last speech event), group gaze (e.g., how much the group looks at the speaker), as well as the listener's and previous speaker's head pitch, head y-axis position, and left hand y-axis position more saliently influenced predictions. Results suggested that these features remain reliable indicators in novel social VR settings, as prediction performance is robust over time and with groups and activities not used in the training dataset. We discuss theoretical and practical implications of the work.",
      "authors": [
        "Portia Wang",
        "Eugy Han",
        "Anna C. M. Queiroz",
        "Cyan DeVeaux",
        "Jeremy N. Bailenson"
      ],
      "published": "2024-07-03T08:17:59Z",
      "updated": "2025-04-25T20:57:25Z",
      "categories": [
        "cs.HC",
        "cs.CY"
      ],
      "pdf_url": "https://arxiv.org/pdf/2407.02896v2",
      "landing_url": "https://arxiv.org/abs/2407.02896v2",
      "doi": "https://doi.org/10.1145/3757498"
    },
    "queries": [
      {
        "anchor": "turn-taking",
        "search_term": "true full-duplex"
      },
      {
        "anchor": "turn-taking",
        "search_term": "pseudo full-duplex"
      },
      {
        "anchor": "turn-taking",
        "search_term": "half duplex"
      },
      {
        "anchor": "turn-taking",
        "search_term": "engineered synchronization"
      },
      {
        "anchor": "turn-taking",
        "search_term": "learned synchronization"
      },
      {
        "anchor": "turn-taking",
        "search_term": "modular architecture"
      },
      {
        "anchor": "turn-taking",
        "search_term": "streaming architecture"
      },
      {
        "anchor": "turn-taking",
        "search_term": "real-time interaction"
      },
      {
        "anchor": "turn-taking",
        "search_term": "low latency"
      },
      {
        "anchor": "turn-taking",
        "search_term": "synchronous data"
      },
      {
        "anchor": "turn-taking",
        "search_term": "data scarcity"
      },
      {
        "anchor": "turn-taking",
        "search_term": "multichannel dialogue"
      },
      {
        "anchor": "turn-taking",
        "search_term": "temporal dynamics"
      },
      {
        "anchor": "turn-taking",
        "search_term": "behavioral arbitration"
      },
      {
        "anchor": "turn-taking",
        "search_term": "semantic coherence"
      },
      {
        "anchor": "turn-taking",
        "search_term": "neural codec"
      },
      {
        "anchor": "turn-taking",
        "search_term": "discrete tokens"
      },
      {
        "anchor": "turn-taking",
        "search_term": "audio tokens"
      },
      {
        "anchor": "turn-taking",
        "search_term": "floor transfer offset"
      },
      {
        "anchor": "turn-taking",
        "search_term": "stop latency"
      },
      {
        "anchor": "turn-taking",
        "search_term": "interruption response delay"
      }
    ]
  },
  {
    "arxiv_id": "2407.03563",
    "anchor": "full-duplex",
    "search_term": "temporal dynamics",
    "search_record": {
      "id": "http://arxiv.org/abs/2407.03563v3",
      "title": "Learning Video Temporal Dynamics with Cross-Modal Attention for Robust Audio-Visual Speech Recognition",
      "summary": "Audio-visual speech recognition (AVSR) aims to transcribe human speech using both audio and video modalities. In practical environments with noise-corrupted audio, the role of video information becomes crucial. However, prior works have primarily focused on enhancing audio features in AVSR, overlooking the importance of video features. In this study, we strengthen the video features by learning three temporal dynamics in video data: context order, playback direction, and the speed of video frames. Cross-modal attention modules are introduced to enrich video features with audio information so that speech variability can be taken into account when training on the video temporal dynamics. Based on our approach, we achieve the state-of-the-art performance on the LRS2 and LRS3 AVSR benchmarks for the noise-dominant settings. Our approach excels in scenarios especially for babble and speech noise, indicating the ability to distinguish the speech signal that should be recognized from lip movements in the video modality. We support the validity of our methodology by offering the ablation experiments for the temporal dynamics losses and the cross-modal attention architecture design.",
      "published": "2024-07-04T01:25:20Z"
    },
    "metadata": {
      "arxiv_id": "2407.03563",
      "title": "Learning Video Temporal Dynamics with Cross-Modal Attention for Robust Audio-Visual Speech Recognition",
      "summary": "Audio-visual speech recognition (AVSR) aims to transcribe human speech using both audio and video modalities. In practical environments with noise-corrupted audio, the role of video information becomes crucial. However, prior works have primarily focused on enhancing audio features in AVSR, overlooking the importance of video features. In this study, we strengthen the video features by learning three temporal dynamics in video data: context order, playback direction, and the speed of video frames. Cross-modal attention modules are introduced to enrich video features with audio information so that speech variability can be taken into account when training on the video temporal dynamics. Based on our approach, we achieve the state-of-the-art performance on the LRS2 and LRS3 AVSR benchmarks for the noise-dominant settings. Our approach excels in scenarios especially for babble and speech noise, indicating the ability to distinguish the speech signal that should be recognized from lip movements in the video modality. We support the validity of our methodology by offering the ablation experiments for the temporal dynamics losses and the cross-modal attention architecture design.",
      "authors": [
        "Sungnyun Kim",
        "Kangwook Jang",
        "Sangmin Bae",
        "Hoirin Kim",
        "Se-Young Yun"
      ],
      "published": "2024-07-04T01:25:20Z",
      "updated": "2024-10-14T07:22:29Z",
      "categories": [
        "eess.AS",
        "cs.CL",
        "cs.LG",
        "eess.IV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2407.03563v3",
      "landing_url": "https://arxiv.org/abs/2407.03563v3",
      "doi": "https://doi.org/10.48550/arXiv.2407.03563"
    },
    "queries": [
      {
        "anchor": "full-duplex",
        "search_term": "temporal dynamics"
      },
      {
        "anchor": "spoken language models",
        "search_term": "temporal dynamics"
      },
      {
        "anchor": "synchronous dialogue",
        "search_term": "temporal dynamics"
      },
      {
        "anchor": "turn-taking",
        "search_term": "temporal dynamics"
      }
    ]
  },
  {
    "arxiv_id": "2407.03852",
    "anchor": "synchronous dialogue",
    "search_term": "low latency",
    "search_record": {
      "id": "http://arxiv.org/abs/2407.03852v2",
      "title": "Low-latency machine learning FPGA accelerator for multi-qubit-state discrimination",
      "summary": "Measuring a qubit state is a fundamental yet error-prone operation in quantum computing. These errors can arise from various sources, such as crosstalk, spontaneous state transitions, and excitations caused by the readout pulse. Here, we utilize an integrated approach to deploy neural networks onto field-programmable gate arrays (FPGA). We demonstrate that implementing a fully connected neural network accelerator for multi-qubit readout is advantageous, balancing computational complexity with low latency requirements without significant loss in accuracy. The neural network is implemented by quantizing weights, activation functions, and inputs. The hardware accelerator performs frequency-multiplexed readout of five superconducting qubits in less than 50 ns on a radio frequency system on chip (RFSoC) ZCU111 FPGA, marking the advent of RFSoC-based low-latency multi-qubit readout using neural networks. These modules can be implemented and integrated into existing quantum control and readout platforms, making the RFSoC ZCU111 ready for experimental deployment.",
      "published": "2024-07-04T11:34:43Z"
    },
    "metadata": {
      "arxiv_id": "2407.03852",
      "title": "Low-latency machine learning FPGA accelerator for multi-qubit-state discrimination",
      "summary": "Measuring a qubit state is a fundamental yet error-prone operation in quantum computing. These errors can arise from various sources, such as crosstalk, spontaneous state transitions, and excitations caused by the readout pulse. Here, we utilize an integrated approach to deploy neural networks onto field-programmable gate arrays (FPGA). We demonstrate that implementing a fully connected neural network accelerator for multi-qubit readout is advantageous, balancing computational complexity with low latency requirements without significant loss in accuracy. The neural network is implemented by quantizing weights, activation functions, and inputs. The hardware accelerator performs frequency-multiplexed readout of five superconducting qubits in less than 50 ns on a radio frequency system on chip (RFSoC) ZCU111 FPGA, marking the advent of RFSoC-based low-latency multi-qubit readout using neural networks. These modules can be implemented and integrated into existing quantum control and readout platforms, making the RFSoC ZCU111 ready for experimental deployment.",
      "authors": [
        "Pradeep Kumar Gautam",
        "Shantharam Kalipatnapu",
        "Shankaranarayanan H",
        "Ujjawal Singhal",
        "Benjamin Lienhard",
        "Vibhor Singh",
        "Chetan Singh Thakur"
      ],
      "published": "2024-07-04T11:34:43Z",
      "updated": "2024-08-14T18:00:57Z",
      "categories": [
        "quant-ph",
        "cs.AR",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2407.03852v2",
      "landing_url": "https://arxiv.org/abs/2407.03852v2",
      "doi": "https://doi.org/10.48550/arXiv.2407.03852"
    },
    "queries": [
      {
        "anchor": "synchronous dialogue",
        "search_term": "low latency"
      }
    ]
  },
  {
    "arxiv_id": "2407.03892",
    "anchor": "spoken language models",
    "search_term": "true full-duplex",
    "search_record": {
      "id": "http://arxiv.org/abs/2407.03892v1",
      "title": "On the Effectiveness of Acoustic BPE in Decoder-Only TTS",
      "summary": "Discretizing speech into tokens and generating them by a decoder-only model have been a promising direction for text-to-speech (TTS) and spoken language modeling (SLM). To shorten the sequence length of speech tokens, acoustic byte-pair encoding (BPE) has emerged in SLM that treats speech tokens from self-supervised semantic representations as characters to further compress the token sequence. But the gain in TTS has not been fully investigated, and the proper choice of acoustic BPE remains unclear. In this work, we conduct a comprehensive study on various settings of acoustic BPE to explore its effectiveness in decoder-only TTS models with semantic speech tokens. Experiments on LibriTTS verify that acoustic BPE uniformly increases the intelligibility and diversity of synthesized speech, while showing different features across BPE settings. Hence, acoustic BPE is a favorable tool for decoder-only TTS.",
      "published": "2024-07-04T12:35:32Z"
    },
    "metadata": {
      "arxiv_id": "2407.03892",
      "title": "On the Effectiveness of Acoustic BPE in Decoder-Only TTS",
      "summary": "Discretizing speech into tokens and generating them by a decoder-only model have been a promising direction for text-to-speech (TTS) and spoken language modeling (SLM). To shorten the sequence length of speech tokens, acoustic byte-pair encoding (BPE) has emerged in SLM that treats speech tokens from self-supervised semantic representations as characters to further compress the token sequence. But the gain in TTS has not been fully investigated, and the proper choice of acoustic BPE remains unclear. In this work, we conduct a comprehensive study on various settings of acoustic BPE to explore its effectiveness in decoder-only TTS models with semantic speech tokens. Experiments on LibriTTS verify that acoustic BPE uniformly increases the intelligibility and diversity of synthesized speech, while showing different features across BPE settings. Hence, acoustic BPE is a favorable tool for decoder-only TTS.",
      "authors": [
        "Bohan Li",
        "Feiyu Shen",
        "Yiwei Guo",
        "Shuai Wang",
        "Xie Chen",
        "Kai Yu"
      ],
      "published": "2024-07-04T12:35:32Z",
      "updated": "2024-07-04T12:35:32Z",
      "categories": [
        "cs.SD",
        "cs.AI",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2407.03892v1",
      "landing_url": "https://arxiv.org/abs/2407.03892v1",
      "doi": "https://doi.org/10.48550/arXiv.2407.03892"
    },
    "queries": [
      {
        "anchor": "spoken language models",
        "search_term": "true full-duplex"
      },
      {
        "anchor": "spoken language models",
        "search_term": "pseudo full-duplex"
      },
      {
        "anchor": "spoken language models",
        "search_term": "engineered synchronization"
      },
      {
        "anchor": "spoken language models",
        "search_term": "learned synchronization"
      },
      {
        "anchor": "spoken language models",
        "search_term": "synchronous data"
      },
      {
        "anchor": "spoken language models",
        "search_term": "multichannel dialogue"
      },
      {
        "anchor": "spoken language models",
        "search_term": "behavioral arbitration"
      },
      {
        "anchor": "spoken language models",
        "search_term": "floor transfer offset"
      },
      {
        "anchor": "spoken language models",
        "search_term": "stop latency"
      },
      {
        "anchor": "spoken language models",
        "search_term": "interruption response delay"
      }
    ]
  },
  {
    "arxiv_id": "2407.06566",
    "anchor": "spoken language models",
    "search_term": "data scarcity",
    "search_record": {
      "id": "http://arxiv.org/abs/2407.06566v1",
      "title": "Robust and Explainable Framework to Address Data Scarcity in Diagnostic Imaging",
      "summary": "Deep learning has significantly advanced automatic medical diagnostics and released the occupation of human resources to reduce clinical pressure, yet the persistent challenge of data scarcity in this area hampers its further improvements and applications. To address this gap, we introduce a novel ensemble framework called `Efficient Transfer and Self-supervised Learning based Ensemble Framework' (ETSEF). ETSEF leverages features from multiple pre-trained deep learning models to efficiently learn powerful representations from a limited number of data samples. To the best of our knowledge, ETSEF is the first strategy that combines two pre-training methodologies (Transfer Learning and Self-supervised Learning) with ensemble learning approaches. Various data enhancement techniques, including data augmentation, feature fusion, feature selection, and decision fusion, have also been deployed to maximise the efficiency and robustness of the ETSEF model. Five independent medical imaging tasks, including endoscopy, breast cancer, monkeypox, brain tumour, and glaucoma detection, were tested to demonstrate ETSEF's effectiveness and robustness. Facing limited sample numbers and challenging medical tasks, ETSEF has proved its effectiveness by improving diagnostics accuracies from 10\\% to 13.3\\% when compared to strong ensemble baseline models and up to 14.4\\% improvements compared with published state-of-the-art methods. Moreover, we emphasise the robustness and trustworthiness of the ETSEF method through various vision-explainable artificial intelligence techniques, including Grad-CAM, SHAP, and t-SNE. Compared to those large-scale deep learning models, ETSEF can be deployed flexibly and maintain superior performance for challenging medical imaging tasks, showing the potential to be applied to more areas that lack training data",
      "published": "2024-07-09T05:48:45Z"
    },
    "metadata": {
      "arxiv_id": "2407.06566",
      "title": "Robust and Explainable Framework to Address Data Scarcity in Diagnostic Imaging",
      "summary": "Deep learning has significantly advanced automatic medical diagnostics and released the occupation of human resources to reduce clinical pressure, yet the persistent challenge of data scarcity in this area hampers its further improvements and applications. To address this gap, we introduce a novel ensemble framework called `Efficient Transfer and Self-supervised Learning based Ensemble Framework' (ETSEF). ETSEF leverages features from multiple pre-trained deep learning models to efficiently learn powerful representations from a limited number of data samples. To the best of our knowledge, ETSEF is the first strategy that combines two pre-training methodologies (Transfer Learning and Self-supervised Learning) with ensemble learning approaches. Various data enhancement techniques, including data augmentation, feature fusion, feature selection, and decision fusion, have also been deployed to maximise the efficiency and robustness of the ETSEF model. Five independent medical imaging tasks, including endoscopy, breast cancer, monkeypox, brain tumour, and glaucoma detection, were tested to demonstrate ETSEF's effectiveness and robustness. Facing limited sample numbers and challenging medical tasks, ETSEF has proved its effectiveness by improving diagnostics accuracies from 10\\% to 13.3\\% when compared to strong ensemble baseline models and up to 14.4\\% improvements compared with published state-of-the-art methods. Moreover, we emphasise the robustness and trustworthiness of the ETSEF method through various vision-explainable artificial intelligence techniques, including Grad-CAM, SHAP, and t-SNE. Compared to those large-scale deep learning models, ETSEF can be deployed flexibly and maintain superior performance for challenging medical imaging tasks, showing the potential to be applied to more areas that lack training data",
      "authors": [
        "Zehui Zhao",
        "Laith Alzubaidi",
        "Jinglan Zhang",
        "Ye Duan",
        "Usman Naseem",
        "Yuantong Gu"
      ],
      "published": "2024-07-09T05:48:45Z",
      "updated": "2024-07-09T05:48:45Z",
      "categories": [
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2407.06566v1",
      "landing_url": "https://arxiv.org/abs/2407.06566v1",
      "doi": "https://doi.org/10.1016/j.compbiomed.2025.111052"
    },
    "queries": [
      {
        "anchor": "spoken language models",
        "search_term": "data scarcity"
      },
      {
        "anchor": "synchronous dialogue",
        "search_term": "data scarcity"
      },
      {
        "anchor": "turn-taking",
        "search_term": "data scarcity"
      }
    ]
  },
  {
    "arxiv_id": "2407.08261",
    "anchor": "synchronous dialogue",
    "search_term": "synchronous data",
    "search_record": {
      "id": "http://arxiv.org/abs/2407.08261v3",
      "title": "CoopScenes: Multi-Scene Infrastructure and Vehicle Data for Advancing Collective Perception in Autonomous Driving",
      "summary": "The increasing complexity of urban environments has underscored the potential of effective collective perception systems. To address these challenges, we present the CoopScenes dataset, a large-scale, multi-scene dataset that provides synchronized sensor data from both the ego-vehicle and the supporting infrastructure.The dataset provides 104 minutes of spatially and temporally synchronized data at 10 Hz, resulting in 62,000 frames. It achieves competitive synchronization with a mean deviation of only 2.3 ms. Additionally the dataset includes a novel procedure for precise registration of point cloud data from the ego-vehicle and infrastructure sensors, automated annotation pipelines, and an open-source anonymization pipeline for faces and license plates. Covering nine diverse scenes with 100 maneuvers, the dataset features scenarios such as public transport hubs, city construction sites, and high-speed rural roads across three cities in the Stuttgart region, Germany. The full dataset amounts to 527 GB of data and is provided in the .4mse format, making it easily accessible through our comprehensive development kit. By providing precise, large-scale data, CoopScenes facilitates research in collective perception, real-time sensor registration, and cooperative intelligent systems for urban mobility, including machine learning-based approaches.",
      "published": "2024-07-11T08:00:46Z"
    },
    "metadata": {
      "arxiv_id": "2407.08261",
      "title": "CoopScenes: Multi-Scene Infrastructure and Vehicle Data for Advancing Collective Perception in Autonomous Driving",
      "summary": "The increasing complexity of urban environments has underscored the potential of effective collective perception systems. To address these challenges, we present the CoopScenes dataset, a large-scale, multi-scene dataset that provides synchronized sensor data from both the ego-vehicle and the supporting infrastructure.The dataset provides 104 minutes of spatially and temporally synchronized data at 10 Hz, resulting in 62,000 frames. It achieves competitive synchronization with a mean deviation of only 2.3 ms. Additionally the dataset includes a novel procedure for precise registration of point cloud data from the ego-vehicle and infrastructure sensors, automated annotation pipelines, and an open-source anonymization pipeline for faces and license plates. Covering nine diverse scenes with 100 maneuvers, the dataset features scenarios such as public transport hubs, city construction sites, and high-speed rural roads across three cities in the Stuttgart region, Germany. The full dataset amounts to 527 GB of data and is provided in the .4mse format, making it easily accessible through our comprehensive development kit. By providing precise, large-scale data, CoopScenes facilitates research in collective perception, real-time sensor registration, and cooperative intelligent systems for urban mobility, including machine learning-based approaches.",
      "authors": [
        "Marcel Vosshans",
        "Alexander Baumann",
        "Matthias Drueppel",
        "Omar Ait-Aider",
        "Youcef Mezouar",
        "Thao Dang",
        "Markus Enzweiler"
      ],
      "published": "2024-07-11T08:00:46Z",
      "updated": "2025-04-30T12:09:45Z",
      "categories": [
        "cs.RO"
      ],
      "pdf_url": "https://arxiv.org/pdf/2407.08261v3",
      "landing_url": "https://arxiv.org/abs/2407.08261v3",
      "doi": "https://doi.org/10.1109/IV64158.2025.11097591"
    },
    "queries": [
      {
        "anchor": "synchronous dialogue",
        "search_term": "synchronous data"
      }
    ]
  },
  {
    "arxiv_id": "2407.08268",
    "anchor": "synchronous dialogue",
    "search_term": "semantic coherence",
    "search_record": {
      "id": "http://arxiv.org/abs/2407.08268v1",
      "title": "Explore the Potential of CLIP for Training-Free Open Vocabulary Semantic Segmentation",
      "summary": "CLIP, as a vision-language model, has significantly advanced Open-Vocabulary Semantic Segmentation (OVSS) with its zero-shot capabilities. Despite its success, its application to OVSS faces challenges due to its initial image-level alignment training, which affects its performance in tasks requiring detailed local context. Our study delves into the impact of CLIP's [CLS] token on patch feature correlations, revealing a dominance of \"global\" patches that hinders local feature discrimination. To overcome this, we propose CLIPtrase, a novel training-free semantic segmentation strategy that enhances local feature awareness through recalibrated self-correlation among patches. This approach demonstrates notable improvements in segmentation accuracy and the ability to maintain semantic coherence across objects.Experiments show that we are 22.3% ahead of CLIP on average on 9 segmentation benchmarks, outperforming existing state-of-the-art training-free methods.The code are made publicly available at: https://github.com/leaves162/CLIPtrase.",
      "published": "2024-07-11T08:12:16Z"
    },
    "metadata": {
      "arxiv_id": "2407.08268",
      "title": "Explore the Potential of CLIP for Training-Free Open Vocabulary Semantic Segmentation",
      "summary": "CLIP, as a vision-language model, has significantly advanced Open-Vocabulary Semantic Segmentation (OVSS) with its zero-shot capabilities. Despite its success, its application to OVSS faces challenges due to its initial image-level alignment training, which affects its performance in tasks requiring detailed local context. Our study delves into the impact of CLIP's [CLS] token on patch feature correlations, revealing a dominance of \"global\" patches that hinders local feature discrimination. To overcome this, we propose CLIPtrase, a novel training-free semantic segmentation strategy that enhances local feature awareness through recalibrated self-correlation among patches. This approach demonstrates notable improvements in segmentation accuracy and the ability to maintain semantic coherence across objects.Experiments show that we are 22.3% ahead of CLIP on average on 9 segmentation benchmarks, outperforming existing state-of-the-art training-free methods.The code are made publicly available at: https://github.com/leaves162/CLIPtrase.",
      "authors": [
        "Tong Shao",
        "Zhuotao Tian",
        "Hang Zhao",
        "Jingyong Su"
      ],
      "published": "2024-07-11T08:12:16Z",
      "updated": "2024-07-11T08:12:16Z",
      "categories": [
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2407.08268v1",
      "landing_url": "https://arxiv.org/abs/2407.08268v1",
      "doi": "https://doi.org/10.48550/arXiv.2407.08268"
    },
    "queries": [
      {
        "anchor": "synchronous dialogue",
        "search_term": "semantic coherence"
      }
    ]
  },
  {
    "arxiv_id": "2407.09087",
    "anchor": "spoken language models",
    "search_term": "discrete tokens",
    "search_record": {
      "id": "http://arxiv.org/abs/2407.09087v1",
      "title": "On the Role of Discrete Tokenization in Visual Representation Learning",
      "summary": "In the realm of self-supervised learning (SSL), masked image modeling (MIM) has gained popularity alongside contrastive learning methods. MIM involves reconstructing masked regions of input images using their unmasked portions. A notable subset of MIM methodologies employs discrete tokens as the reconstruction target, but the theoretical underpinnings of this choice remain underexplored. In this paper, we explore the role of these discrete tokens, aiming to unravel their benefits and limitations. Building upon the connection between MIM and contrastive learning, we provide a comprehensive theoretical understanding on how discrete tokenization affects the model's generalization capabilities. Furthermore, we propose a novel metric named TCAS, which is specifically designed to assess the effectiveness of discrete tokens within the MIM framework. Inspired by this metric, we contribute an innovative tokenizer design and propose a corresponding MIM method named ClusterMIM. It demonstrates superior performance on a variety of benchmark datasets and ViT backbones. Code is available at https://github.com/PKU-ML/ClusterMIM.",
      "published": "2024-07-12T08:25:31Z"
    },
    "metadata": {
      "arxiv_id": "2407.09087",
      "title": "On the Role of Discrete Tokenization in Visual Representation Learning",
      "summary": "In the realm of self-supervised learning (SSL), masked image modeling (MIM) has gained popularity alongside contrastive learning methods. MIM involves reconstructing masked regions of input images using their unmasked portions. A notable subset of MIM methodologies employs discrete tokens as the reconstruction target, but the theoretical underpinnings of this choice remain underexplored. In this paper, we explore the role of these discrete tokens, aiming to unravel their benefits and limitations. Building upon the connection between MIM and contrastive learning, we provide a comprehensive theoretical understanding on how discrete tokenization affects the model's generalization capabilities. Furthermore, we propose a novel metric named TCAS, which is specifically designed to assess the effectiveness of discrete tokens within the MIM framework. Inspired by this metric, we contribute an innovative tokenizer design and propose a corresponding MIM method named ClusterMIM. It demonstrates superior performance on a variety of benchmark datasets and ViT backbones. Code is available at https://github.com/PKU-ML/ClusterMIM.",
      "authors": [
        "Tianqi Du",
        "Yifei Wang",
        "Yisen Wang"
      ],
      "published": "2024-07-12T08:25:31Z",
      "updated": "2024-07-12T08:25:31Z",
      "categories": [
        "cs.LG",
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2407.09087v1",
      "landing_url": "https://arxiv.org/abs/2407.09087v1",
      "doi": "https://doi.org/10.48550/arXiv.2407.09087"
    },
    "queries": [
      {
        "anchor": "spoken language models",
        "search_term": "discrete tokens"
      },
      {
        "anchor": "synchronous dialogue",
        "search_term": "discrete tokens"
      },
      {
        "anchor": "turn-taking",
        "search_term": "discrete tokens"
      }
    ]
  },
  {
    "arxiv_id": "2407.10975",
    "anchor": "spoken language models",
    "search_term": "synchronous data",
    "search_record": {
      "id": "http://arxiv.org/abs/2407.10975v1",
      "title": "Stream State-tying for Sign Language Recognition",
      "summary": "In this paper, a novel approach to sign language recognition based on state tying in each of data streams is presented. In this framework, it is assumed that hand gesture signal is represented in terms of six synchronous data streams, i.e., the left/right hand position, left/right hand orientation and left/right handshape. This approach offers a very accurate representation of the sign space and keeps the number of parameters reasonably small in favor of a fast decoding. Experiments were carried out for 5177 Chinese signs. The real time isolated recognition rate is 94.8%. For continuous sign recognition, the word correct rate is 91.4%. Keywords: Sign language recognition; Automatic sign language translation; Hand gesture recognition; Hidden Markov models; State-tying; Multimodal user interface; Virtual reality; Man-machine systems.",
      "published": "2024-04-21T23:21:52Z"
    },
    "metadata": {
      "arxiv_id": "2407.10975",
      "title": "Stream State-tying for Sign Language Recognition",
      "summary": "In this paper, a novel approach to sign language recognition based on state tying in each of data streams is presented. In this framework, it is assumed that hand gesture signal is represented in terms of six synchronous data streams, i.e., the left/right hand position, left/right hand orientation and left/right handshape. This approach offers a very accurate representation of the sign space and keeps the number of parameters reasonably small in favor of a fast decoding. Experiments were carried out for 5177 Chinese signs. The real time isolated recognition rate is 94.8%. For continuous sign recognition, the word correct rate is 91.4%. Keywords: Sign language recognition; Automatic sign language translation; Hand gesture recognition; Hidden Markov models; State-tying; Multimodal user interface; Virtual reality; Man-machine systems.",
      "authors": [
        "Jiyong Ma",
        "Wen Gao",
        "Chunli Wang"
      ],
      "published": "2024-04-21T23:21:52Z",
      "updated": "2024-04-21T23:21:52Z",
      "categories": [
        "cs.OH",
        "cs.AI",
        "cs.CL"
      ],
      "pdf_url": "https://arxiv.org/pdf/2407.10975v1",
      "landing_url": "https://arxiv.org/abs/2407.10975v1",
      "doi": "https://doi.org/10.48550/arXiv.2407.10975"
    },
    "queries": [
      {
        "anchor": "spoken language models",
        "search_term": "synchronous data"
      },
      {
        "anchor": "synchronous dialogue",
        "search_term": "synchronous data"
      }
    ]
  },
  {
    "arxiv_id": "2407.11277",
    "anchor": "turn-taking",
    "search_term": "true full-duplex",
    "search_record": {
      "id": "http://arxiv.org/abs/2407.11277v2",
      "title": "Target conversation extraction: Source separation using turn-taking dynamics",
      "summary": "Extracting the speech of participants in a conversation amidst interfering speakers and noise presents a challenging problem. In this paper, we introduce the novel task of target conversation extraction, where the goal is to extract the audio of a target conversation based on the speaker embedding of one of its participants. To accomplish this, we propose leveraging temporal patterns inherent in human conversations, particularly turn-taking dynamics, which uniquely characterize speakers engaged in conversation and distinguish them from interfering speakers and noise. Using neural networks, we show the feasibility of our approach on English and Mandarin conversation datasets. In the presence of interfering speakers, our results show an 8.19 dB improvement in signal-to-noise ratio for 2-speaker conversations and a 7.92 dB improvement for 2-4-speaker conversations. Code, dataset available at https://github.com/chentuochao/Target-Conversation-Extraction.",
      "published": "2024-07-15T22:55:27Z"
    },
    "metadata": {
      "arxiv_id": "2407.11277",
      "title": "Target conversation extraction: Source separation using turn-taking dynamics",
      "summary": "Extracting the speech of participants in a conversation amidst interfering speakers and noise presents a challenging problem. In this paper, we introduce the novel task of target conversation extraction, where the goal is to extract the audio of a target conversation based on the speaker embedding of one of its participants. To accomplish this, we propose leveraging temporal patterns inherent in human conversations, particularly turn-taking dynamics, which uniquely characterize speakers engaged in conversation and distinguish them from interfering speakers and noise. Using neural networks, we show the feasibility of our approach on English and Mandarin conversation datasets. In the presence of interfering speakers, our results show an 8.19 dB improvement in signal-to-noise ratio for 2-speaker conversations and a 7.92 dB improvement for 2-4-speaker conversations. Code, dataset available at https://github.com/chentuochao/Target-Conversation-Extraction.",
      "authors": [
        "Tuochao Chen",
        "Qirui Wang",
        "Bohan Wu",
        "Malek Itani",
        "Sefik Emre Eskimez",
        "Takuya Yoshioka",
        "Shyamnath Gollakota"
      ],
      "published": "2024-07-15T22:55:27Z",
      "updated": "2024-07-29T19:35:36Z",
      "categories": [
        "cs.CL",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2407.11277v2",
      "landing_url": "https://arxiv.org/abs/2407.11277v2",
      "doi": "https://doi.org/10.21437/Interspeech.2024-225"
    },
    "queries": [
      {
        "anchor": "turn-taking",
        "search_term": "true full-duplex"
      },
      {
        "anchor": "turn-taking",
        "search_term": "pseudo full-duplex"
      },
      {
        "anchor": "turn-taking",
        "search_term": "half duplex"
      },
      {
        "anchor": "turn-taking",
        "search_term": "engineered synchronization"
      },
      {
        "anchor": "turn-taking",
        "search_term": "learned synchronization"
      },
      {
        "anchor": "turn-taking",
        "search_term": "modular architecture"
      },
      {
        "anchor": "turn-taking",
        "search_term": "streaming architecture"
      },
      {
        "anchor": "turn-taking",
        "search_term": "real-time interaction"
      },
      {
        "anchor": "turn-taking",
        "search_term": "low latency"
      },
      {
        "anchor": "turn-taking",
        "search_term": "synchronous data"
      },
      {
        "anchor": "turn-taking",
        "search_term": "data scarcity"
      },
      {
        "anchor": "turn-taking",
        "search_term": "multichannel dialogue"
      },
      {
        "anchor": "turn-taking",
        "search_term": "temporal dynamics"
      },
      {
        "anchor": "turn-taking",
        "search_term": "behavioral arbitration"
      },
      {
        "anchor": "turn-taking",
        "search_term": "semantic coherence"
      },
      {
        "anchor": "turn-taking",
        "search_term": "neural codec"
      },
      {
        "anchor": "turn-taking",
        "search_term": "discrete tokens"
      },
      {
        "anchor": "turn-taking",
        "search_term": "audio tokens"
      },
      {
        "anchor": "turn-taking",
        "search_term": "floor transfer offset"
      },
      {
        "anchor": "turn-taking",
        "search_term": "stop latency"
      },
      {
        "anchor": "turn-taking",
        "search_term": "interruption response delay"
      }
    ]
  },
  {
    "arxiv_id": "2407.11370",
    "anchor": "spoken language models",
    "search_term": "true full-duplex",
    "search_record": {
      "id": "http://arxiv.org/abs/2407.11370v1",
      "title": "A Pilot Study of GSLM-based Simulation of Foreign Accentuation Only Using Native Speech Corpora",
      "summary": "We propose a method of simulating the human process of foreign accentuation using Generative Spoken Language Model (GSLM) only with native speech corpora. When one listens to spoken words of a foreign language and repeats them, the repeated speech is often with the accent of that listener's L1. This is said to be because the spoken words are mentally represented as a sequence of phonological units of the L1, and those units are used for oral reproduction. We simulate this process by inputting speech of language A into GSLM of language B to add B's accent onto the input speech. The process of running ASR of the L1 for foreign input speech and giving the ASR result to TTS of the L1 can be viewed as a naive implementation of this approach. The results of our experiments show that the synthesized accent of the output speech is highly natural, compared to real samples of A generated by speakers whose L1 is B, and that the degree of accentuation is controllable.",
      "published": "2024-07-16T04:29:00Z"
    },
    "metadata": {
      "arxiv_id": "2407.11370",
      "title": "A Pilot Study of GSLM-based Simulation of Foreign Accentuation Only Using Native Speech Corpora",
      "summary": "We propose a method of simulating the human process of foreign accentuation using Generative Spoken Language Model (GSLM) only with native speech corpora. When one listens to spoken words of a foreign language and repeats them, the repeated speech is often with the accent of that listener's L1. This is said to be because the spoken words are mentally represented as a sequence of phonological units of the L1, and those units are used for oral reproduction. We simulate this process by inputting speech of language A into GSLM of language B to add B's accent onto the input speech. The process of running ASR of the L1 for foreign input speech and giving the ASR result to TTS of the L1 can be viewed as a naive implementation of this approach. The results of our experiments show that the synthesized accent of the output speech is highly natural, compared to real samples of A generated by speakers whose L1 is B, and that the degree of accentuation is controllable.",
      "authors": [
        "Kentaro Onda",
        "Joonyong Park",
        "Nobuaki Minematsu",
        "Daisuke Saito"
      ],
      "published": "2024-07-16T04:29:00Z",
      "updated": "2024-07-16T04:29:00Z",
      "categories": [
        "cs.SD",
        "cs.CL",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2407.11370v1",
      "landing_url": "https://arxiv.org/abs/2407.11370v1",
      "doi": "https://doi.org/10.48550/arXiv.2407.11370"
    },
    "queries": [
      {
        "anchor": "spoken language models",
        "search_term": "true full-duplex"
      },
      {
        "anchor": "spoken language models",
        "search_term": "pseudo full-duplex"
      },
      {
        "anchor": "spoken language models",
        "search_term": "engineered synchronization"
      },
      {
        "anchor": "spoken language models",
        "search_term": "learned synchronization"
      },
      {
        "anchor": "spoken language models",
        "search_term": "streaming architecture"
      },
      {
        "anchor": "spoken language models",
        "search_term": "synchronous data"
      },
      {
        "anchor": "spoken language models",
        "search_term": "multichannel dialogue"
      },
      {
        "anchor": "spoken language models",
        "search_term": "behavioral arbitration"
      },
      {
        "anchor": "spoken language models",
        "search_term": "floor transfer offset"
      },
      {
        "anchor": "spoken language models",
        "search_term": "stop latency"
      },
      {
        "anchor": "spoken language models",
        "search_term": "interruption response delay"
      }
    ]
  },
  {
    "arxiv_id": "2407.12373",
    "anchor": "synchronous dialogue",
    "search_term": "modular architecture",
    "search_record": {
      "id": "http://arxiv.org/abs/2407.12373v1",
      "title": "AlphaPEM: an open-source dynamic 1D physics-based PEM fuel cell model for embedded applications",
      "summary": "The urgency of the energy transition requires improving the performance and longevity of hydrogen technologies. AlphaPEM is a dynamic one-dimensional (1D) physics-based PEM fuel cell system simulator, programmed in Python and experimentally validated. It offers a good balance between accuracy and execution speed. The modular architecture allows for addition of new features, and it has a user-friendly graphical interface. An automatic calibration method is proposed to match the model to the studied machine. The software provides information on the internal states of the system in response to any current density and can produce polarization and EIS curves. AlphaPEM facilitates the use of a model in embedded conditions, allowing real-time modification of the fuel cell's operating conditions.",
      "published": "2024-07-17T07:52:22Z"
    },
    "metadata": {
      "arxiv_id": "2407.12373",
      "title": "AlphaPEM: an open-source dynamic 1D physics-based PEM fuel cell model for embedded applications",
      "summary": "The urgency of the energy transition requires improving the performance and longevity of hydrogen technologies. AlphaPEM is a dynamic one-dimensional (1D) physics-based PEM fuel cell system simulator, programmed in Python and experimentally validated. It offers a good balance between accuracy and execution speed. The modular architecture allows for addition of new features, and it has a user-friendly graphical interface. An automatic calibration method is proposed to match the model to the studied machine. The software provides information on the internal states of the system in response to any current density and can produce polarization and EIS curves. AlphaPEM facilitates the use of a model in embedded conditions, allowing real-time modification of the fuel cell's operating conditions.",
      "authors": [
        "Raphaël Gass",
        "Zhongliang Li",
        "Samir Jemeï",
        "Rachid Outbib",
        "Daniel Hissel"
      ],
      "published": "2024-07-17T07:52:22Z",
      "updated": "2024-07-17T07:52:22Z",
      "categories": [
        "eess.SY"
      ],
      "pdf_url": "https://arxiv.org/pdf/2407.12373v1",
      "landing_url": "https://arxiv.org/abs/2407.12373v1",
      "doi": "https://doi.org/10.48550/arXiv.2407.12373"
    },
    "queries": [
      {
        "anchor": "synchronous dialogue",
        "search_term": "modular architecture"
      }
    ]
  },
  {
    "arxiv_id": "2407.13943",
    "anchor": "turn-taking",
    "search_term": "true full-duplex",
    "search_record": {
      "id": "http://arxiv.org/abs/2407.13943v1",
      "title": "Werewolf Arena: A Case Study in LLM Evaluation via Social Deduction",
      "summary": "This paper introduces Werewolf Arena, a novel framework for evaluating large language models (LLMs) through the lens of the classic social deduction game, Werewolf. In Werewolf Arena, LLMs compete against each other, navigating the game's complex dynamics of deception, deduction, and persuasion. The framework introduces a dynamic turn-taking system based on bidding, mirroring real-world discussions where individuals strategically choose when to speak. We demonstrate the framework's utility through an arena-style tournament featuring Gemini and GPT models. Our results reveal distinct strengths and weaknesses in the models' strategic reasoning and communication. These findings highlight Werewolf Arena's potential as a challenging and scalable LLM benchmark.",
      "published": "2024-07-18T23:41:05Z"
    },
    "metadata": {
      "arxiv_id": "2407.13943",
      "title": "Werewolf Arena: A Case Study in LLM Evaluation via Social Deduction",
      "summary": "This paper introduces Werewolf Arena, a novel framework for evaluating large language models (LLMs) through the lens of the classic social deduction game, Werewolf. In Werewolf Arena, LLMs compete against each other, navigating the game's complex dynamics of deception, deduction, and persuasion. The framework introduces a dynamic turn-taking system based on bidding, mirroring real-world discussions where individuals strategically choose when to speak. We demonstrate the framework's utility through an arena-style tournament featuring Gemini and GPT models. Our results reveal distinct strengths and weaknesses in the models' strategic reasoning and communication. These findings highlight Werewolf Arena's potential as a challenging and scalable LLM benchmark.",
      "authors": [
        "Suma Bailis",
        "Jane Friedhoff",
        "Feiyang Chen"
      ],
      "published": "2024-07-18T23:41:05Z",
      "updated": "2024-07-18T23:41:05Z",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2407.13943v1",
      "landing_url": "https://arxiv.org/abs/2407.13943v1",
      "doi": "https://doi.org/10.48550/arXiv.2407.13943"
    },
    "queries": [
      {
        "anchor": "turn-taking",
        "search_term": "true full-duplex"
      },
      {
        "anchor": "turn-taking",
        "search_term": "pseudo full-duplex"
      },
      {
        "anchor": "turn-taking",
        "search_term": "engineered synchronization"
      },
      {
        "anchor": "turn-taking",
        "search_term": "learned synchronization"
      },
      {
        "anchor": "turn-taking",
        "search_term": "synchronous data"
      },
      {
        "anchor": "turn-taking",
        "search_term": "multichannel dialogue"
      },
      {
        "anchor": "turn-taking",
        "search_term": "behavioral arbitration"
      },
      {
        "anchor": "turn-taking",
        "search_term": "floor transfer offset"
      },
      {
        "anchor": "turn-taking",
        "search_term": "stop latency"
      },
      {
        "anchor": "turn-taking",
        "search_term": "interruption response delay"
      }
    ]
  },
  {
    "arxiv_id": "2407.15458",
    "anchor": "spoken language models",
    "search_term": "neural codec",
    "search_record": {
      "id": "http://arxiv.org/abs/2407.15458v4",
      "title": "EMO-Codec: An In-Depth Look at Emotion Preservation capacity of Legacy and Neural Codec Models With Subjective and Objective Evaluations",
      "summary": "The neural codec model reduces speech data transmission delay and serves as the foundational tokenizer for speech language models (speech LMs). Preserving emotional information in codecs is crucial for effective communication and context understanding. However, there is a lack of studies on emotion loss in existing codecs. This paper evaluates neural and legacy codecs using subjective and objective methods on emotion datasets like IEMOCAP. Our study identifies which codecs best preserve emotional information under various bitrate scenarios. We found that training codec models with both English and Chinese data had limited success in retaining emotional information in Chinese. Additionally, resynthesizing speech through these codecs degrades the performance of speech emotion recognition (SER), particularly for emotions like sadness, depression, fear, and disgust. Human listening tests confirmed these findings. This work guides future speech technology developments to ensure new codecs maintain the integrity of emotional information in speech.",
      "published": "2024-07-22T08:14:16Z"
    },
    "metadata": {
      "arxiv_id": "2407.15458",
      "title": "EMO-Codec: An In-Depth Look at Emotion Preservation capacity of Legacy and Neural Codec Models With Subjective and Objective Evaluations",
      "summary": "The neural codec model reduces speech data transmission delay and serves as the foundational tokenizer for speech language models (speech LMs). Preserving emotional information in codecs is crucial for effective communication and context understanding. However, there is a lack of studies on emotion loss in existing codecs. This paper evaluates neural and legacy codecs using subjective and objective methods on emotion datasets like IEMOCAP. Our study identifies which codecs best preserve emotional information under various bitrate scenarios. We found that training codec models with both English and Chinese data had limited success in retaining emotional information in Chinese. Additionally, resynthesizing speech through these codecs degrades the performance of speech emotion recognition (SER), particularly for emotions like sadness, depression, fear, and disgust. Human listening tests confirmed these findings. This work guides future speech technology developments to ensure new codecs maintain the integrity of emotional information in speech.",
      "authors": [
        "Wenze Ren",
        "Yi-Cheng Lin",
        "Huang-Cheng Chou",
        "Haibin Wu",
        "Yi-Chiao Wu",
        "Chi-Chun Lee",
        "Hung-yi Lee",
        "Yu Tsao"
      ],
      "published": "2024-07-22T08:14:16Z",
      "updated": "2024-07-30T12:37:35Z",
      "categories": [
        "eess.AS",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2407.15458v4",
      "landing_url": "https://arxiv.org/abs/2407.15458v4",
      "doi": "https://doi.org/10.48550/arXiv.2407.15458"
    },
    "queries": [
      {
        "anchor": "spoken language models",
        "search_term": "neural codec"
      },
      {
        "anchor": "synchronous dialogue",
        "search_term": "neural codec"
      },
      {
        "anchor": "turn-taking",
        "search_term": "neural codec"
      }
    ]
  },
  {
    "arxiv_id": "2407.15828",
    "anchor": "spoken language models",
    "search_term": "true full-duplex",
    "search_record": {
      "id": "http://arxiv.org/abs/2407.15828v1",
      "title": "J-CHAT: Japanese Large-scale Spoken Dialogue Corpus for Spoken Dialogue Language Modeling",
      "summary": "Spoken dialogue plays a crucial role in human-AI interactions, necessitating dialogue-oriented spoken language models (SLMs). To develop versatile SLMs, large-scale and diverse speech datasets are essential. Additionally, to ensure hiqh-quality speech generation, the data must be spontaneous like in-wild data and must be acoustically clean with noise removed. Despite the critical need, no open-source corpus meeting all these criteria has been available. This study addresses this gap by constructing and releasing a large-scale spoken dialogue corpus, named Japanese Corpus for Human-AI Talks (J-CHAT), which is publicly accessible. Furthermore, this paper presents a language-independent method for corpus construction and describes experiments on dialogue generation using SLMs trained on J-CHAT. Experimental results indicate that the collected data from multiple domains by our method improve the naturalness and meaningfulness of dialogue generation.",
      "published": "2024-07-22T17:46:50Z"
    },
    "metadata": {
      "arxiv_id": "2407.15828",
      "title": "J-CHAT: Japanese Large-scale Spoken Dialogue Corpus for Spoken Dialogue Language Modeling",
      "summary": "Spoken dialogue plays a crucial role in human-AI interactions, necessitating dialogue-oriented spoken language models (SLMs). To develop versatile SLMs, large-scale and diverse speech datasets are essential. Additionally, to ensure hiqh-quality speech generation, the data must be spontaneous like in-wild data and must be acoustically clean with noise removed. Despite the critical need, no open-source corpus meeting all these criteria has been available. This study addresses this gap by constructing and releasing a large-scale spoken dialogue corpus, named Japanese Corpus for Human-AI Talks (J-CHAT), which is publicly accessible. Furthermore, this paper presents a language-independent method for corpus construction and describes experiments on dialogue generation using SLMs trained on J-CHAT. Experimental results indicate that the collected data from multiple domains by our method improve the naturalness and meaningfulness of dialogue generation.",
      "authors": [
        "Wataru Nakata",
        "Kentaro Seki",
        "Hitomi Yanaka",
        "Yuki Saito",
        "Shinnosuke Takamichi",
        "Hiroshi Saruwatari"
      ],
      "published": "2024-07-22T17:46:50Z",
      "updated": "2024-07-22T17:46:50Z",
      "categories": [
        "cs.CL",
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2407.15828v1",
      "landing_url": "https://arxiv.org/abs/2407.15828v1",
      "doi": "https://doi.org/10.48550/arXiv.2407.15828"
    },
    "queries": [
      {
        "anchor": "spoken language models",
        "search_term": "true full-duplex"
      },
      {
        "anchor": "spoken language models",
        "search_term": "pseudo full-duplex"
      },
      {
        "anchor": "spoken language models",
        "search_term": "engineered synchronization"
      },
      {
        "anchor": "spoken language models",
        "search_term": "learned synchronization"
      },
      {
        "anchor": "spoken language models",
        "search_term": "modular architecture"
      },
      {
        "anchor": "spoken language models",
        "search_term": "streaming architecture"
      },
      {
        "anchor": "spoken language models",
        "search_term": "real-time interaction"
      },
      {
        "anchor": "spoken language models",
        "search_term": "synchronous data"
      },
      {
        "anchor": "spoken language models",
        "search_term": "data scarcity"
      },
      {
        "anchor": "spoken language models",
        "search_term": "multichannel dialogue"
      },
      {
        "anchor": "spoken language models",
        "search_term": "behavioral arbitration"
      },
      {
        "anchor": "spoken language models",
        "search_term": "semantic coherence"
      },
      {
        "anchor": "spoken language models",
        "search_term": "neural codec"
      },
      {
        "anchor": "spoken language models",
        "search_term": "discrete tokens"
      },
      {
        "anchor": "spoken language models",
        "search_term": "audio tokens"
      },
      {
        "anchor": "spoken language models",
        "search_term": "floor transfer offset"
      },
      {
        "anchor": "spoken language models",
        "search_term": "stop latency"
      },
      {
        "anchor": "spoken language models",
        "search_term": "interruption response delay"
      }
    ]
  },
  {
    "arxiv_id": "2407.16591",
    "anchor": "spoken language models",
    "search_term": "real-time interaction",
    "search_record": {
      "id": "http://arxiv.org/abs/2407.16591v1",
      "title": "Real-Time Interactions Between Human Controllers and Remote Devices in Metaverse",
      "summary": "Supporting real-time interactions between human controllers and remote devices remains a challenging goal in the Metaverse due to the stringent requirements on computing workload, communication throughput, and round-trip latency. In this paper, we establish a novel framework for real-time interactions through the virtual models in the Metaverse. Specifically, we jointly predict the motion of the human controller for 1) proactive rendering in the Metaverse and 2) generating control commands to the real-world remote device in advance. The virtual model is decoupled into two components for rendering and control, respectively. To dynamically adjust the prediction horizons for rendering and control, we develop a two-step human-in-the-loop continuous reinforcement learning approach and use an expert policy to improve the training efficiency. An experimental prototype is built to verify our algorithm with different communication latencies. Compared with the baseline policy without prediction, our proposed method can reduce 1) the Motion-To-Photon (MTP) latency between human motion and rendering feedback and 2) the root mean squared error (RMSE) between human motion and real-world remote devices significantly.",
      "published": "2024-07-23T15:48:36Z"
    },
    "metadata": {
      "arxiv_id": "2407.16591",
      "title": "Real-Time Interactions Between Human Controllers and Remote Devices in Metaverse",
      "summary": "Supporting real-time interactions between human controllers and remote devices remains a challenging goal in the Metaverse due to the stringent requirements on computing workload, communication throughput, and round-trip latency. In this paper, we establish a novel framework for real-time interactions through the virtual models in the Metaverse. Specifically, we jointly predict the motion of the human controller for 1) proactive rendering in the Metaverse and 2) generating control commands to the real-world remote device in advance. The virtual model is decoupled into two components for rendering and control, respectively. To dynamically adjust the prediction horizons for rendering and control, we develop a two-step human-in-the-loop continuous reinforcement learning approach and use an expert policy to improve the training efficiency. An experimental prototype is built to verify our algorithm with different communication latencies. Compared with the baseline policy without prediction, our proposed method can reduce 1) the Motion-To-Photon (MTP) latency between human motion and rendering feedback and 2) the root mean squared error (RMSE) between human motion and real-world remote devices significantly.",
      "authors": [
        "Kan Chen",
        "Zhen Meng",
        "Xiangmin Xu",
        "Changyang She",
        "Philip G. Zhao"
      ],
      "published": "2024-07-23T15:48:36Z",
      "updated": "2024-07-23T15:48:36Z",
      "categories": [
        "cs.RO",
        "eess.SY"
      ],
      "pdf_url": "https://arxiv.org/pdf/2407.16591v1",
      "landing_url": "https://arxiv.org/abs/2407.16591v1",
      "doi": "https://doi.org/10.48550/arXiv.2407.16591"
    },
    "queries": [
      {
        "anchor": "spoken language models",
        "search_term": "real-time interaction"
      },
      {
        "anchor": "synchronous dialogue",
        "search_term": "real-time interaction"
      },
      {
        "anchor": "turn-taking",
        "search_term": "real-time interaction"
      }
    ]
  },
  {
    "arxiv_id": "2407.16743",
    "anchor": "synchronous dialogue",
    "search_term": "modular architecture",
    "search_record": {
      "id": "http://arxiv.org/abs/2407.16743v1",
      "title": "A high-efficiency plug-and-play superconducting qubit network",
      "summary": "Modular architectures are a promising approach to scale quantum devices to the point of fault tolerance and utility. Modularity is particularly appealing for superconducting qubits, as monolithically manufactured devices are limited in both system size and quality. Constructing complex quantum systems as networks of interchangeable modules can overcome this challenge through `Lego-like' assembly, reconfiguration, and expansion, in a spirit similar to modern classical computers. First prototypical superconducting quantum device networks have been demonstrated. Interfaces that simultaneously permit interchangeability and high-fidelity operations remain a crucial challenge, however. Here, we demonstrate a high-efficiency interconnect based on a detachable cable between superconducting qubit devices. We overcome the inevitable loss in a detachable connection through a fast pump scheme, enabling inter-module SWAP efficiencies at the 99%-level in less than 100 ns. We use this scheme to generate high-fidelity entanglement and operate a distributed logical dual-rail qubit. At the observed ~1% error rate, operations through the interconnect are at the threshold for fault-tolerance. These results introduce a modular architecture for scaling quantum processors with reconfigurable and expandable networks.",
      "published": "2024-07-23T17:58:59Z"
    },
    "metadata": {
      "arxiv_id": "2407.16743",
      "title": "A high-efficiency plug-and-play superconducting qubit network",
      "summary": "Modular architectures are a promising approach to scale quantum devices to the point of fault tolerance and utility. Modularity is particularly appealing for superconducting qubits, as monolithically manufactured devices are limited in both system size and quality. Constructing complex quantum systems as networks of interchangeable modules can overcome this challenge through `Lego-like' assembly, reconfiguration, and expansion, in a spirit similar to modern classical computers. First prototypical superconducting quantum device networks have been demonstrated. Interfaces that simultaneously permit interchangeability and high-fidelity operations remain a crucial challenge, however. Here, we demonstrate a high-efficiency interconnect based on a detachable cable between superconducting qubit devices. We overcome the inevitable loss in a detachable connection through a fast pump scheme, enabling inter-module SWAP efficiencies at the 99%-level in less than 100 ns. We use this scheme to generate high-fidelity entanglement and operate a distributed logical dual-rail qubit. At the observed ~1% error rate, operations through the interconnect are at the threshold for fault-tolerance. These results introduce a modular architecture for scaling quantum processors with reconfigurable and expandable networks.",
      "authors": [
        "Michael Mollenhauer",
        "Abdullah Irfan",
        "Xi Cao",
        "Supriya Mandal",
        "Wolfgang Pfaff"
      ],
      "published": "2024-07-23T17:58:59Z",
      "updated": "2024-07-23T17:58:59Z",
      "categories": [
        "quant-ph"
      ],
      "pdf_url": "https://arxiv.org/pdf/2407.16743v1",
      "landing_url": "https://arxiv.org/abs/2407.16743v1",
      "doi": "https://doi.org/10.1038/s41928-025-01404-3"
    },
    "queries": [
      {
        "anchor": "synchronous dialogue",
        "search_term": "modular architecture"
      }
    ]
  },
  {
    "arxiv_id": "2407.18691",
    "anchor": "synchronous dialogue",
    "search_term": "temporal dynamics",
    "search_record": {
      "id": "http://arxiv.org/abs/2407.18691v2",
      "title": "Graph Neural Networks for Virtual Sensing in Complex Systems: Addressing Heterogeneous Temporal Dynamics",
      "summary": "Real-time condition monitoring is crucial for the reliable and efficient operation of complex systems. However, relying solely on physical sensors can be limited due to their cost, placement constraints, or inability to directly measure certain critical parameters. Virtual sensing addresses these limitations by leveraging readily available sensor data and system knowledge to estimate inaccessible parameters or infer system states. The increasing complexity of industrial systems necessitates deployments of sensors with diverse modalities to provide a comprehensive understanding of system states. These sensors capture data at varying frequencies to monitor both rapid and slowly varying system dynamics, as well as local and global state evolutions of the systems. This leads to heterogeneous temporal dynamics, which, particularly under varying operational end environmental conditions, pose a significant challenge for accurate virtual sensing. To address this, we propose a Heterogeneous Temporal Graph Neural Network (HTGNN) framework. HTGNN explicitly models signals from diverse sensors and integrates operating conditions into the model architecture. We evaluate HTGNN using two newly released datasets: a bearing dataset with diverse load conditions for bearing load prediction and a year-long simulated dataset for predicting bridge live loads. Our results demonstrate that HTGNN significantly outperforms established baseline methods in both tasks, particularly under highly varying operating conditions. These results highlight HTGNN's potential as a robust and accurate virtual sensing approach for complex systems, paving the way for improved monitoring, predictive maintenance, and enhanced system performance. Our code and data are available under https://github.com/EPFL-IMOS/htgnn.",
      "published": "2024-07-26T12:16:53Z"
    },
    "metadata": {
      "arxiv_id": "2407.18691",
      "title": "Graph Neural Networks for Virtual Sensing in Complex Systems: Addressing Heterogeneous Temporal Dynamics",
      "summary": "Real-time condition monitoring is crucial for the reliable and efficient operation of complex systems. However, relying solely on physical sensors can be limited due to their cost, placement constraints, or inability to directly measure certain critical parameters. Virtual sensing addresses these limitations by leveraging readily available sensor data and system knowledge to estimate inaccessible parameters or infer system states. The increasing complexity of industrial systems necessitates deployments of sensors with diverse modalities to provide a comprehensive understanding of system states. These sensors capture data at varying frequencies to monitor both rapid and slowly varying system dynamics, as well as local and global state evolutions of the systems. This leads to heterogeneous temporal dynamics, which, particularly under varying operational end environmental conditions, pose a significant challenge for accurate virtual sensing. To address this, we propose a Heterogeneous Temporal Graph Neural Network (HTGNN) framework. HTGNN explicitly models signals from diverse sensors and integrates operating conditions into the model architecture. We evaluate HTGNN using two newly released datasets: a bearing dataset with diverse load conditions for bearing load prediction and a year-long simulated dataset for predicting bridge live loads. Our results demonstrate that HTGNN significantly outperforms established baseline methods in both tasks, particularly under highly varying operating conditions. These results highlight HTGNN's potential as a robust and accurate virtual sensing approach for complex systems, paving the way for improved monitoring, predictive maintenance, and enhanced system performance. Our code and data are available under https://github.com/EPFL-IMOS/htgnn.",
      "authors": [
        "Mengjie Zhao",
        "Cees Taal",
        "Stephan Baggerohr",
        "Olga Fink"
      ],
      "published": "2024-07-26T12:16:53Z",
      "updated": "2025-03-06T15:47:01Z",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CE"
      ],
      "pdf_url": "https://arxiv.org/pdf/2407.18691v2",
      "landing_url": "https://arxiv.org/abs/2407.18691v2",
      "doi": "https://doi.org/10.48550/arXiv.2407.18691"
    },
    "queries": [
      {
        "anchor": "synchronous dialogue",
        "search_term": "temporal dynamics"
      }
    ]
  },
  {
    "arxiv_id": "2407.19449",
    "anchor": "spoken language models",
    "search_term": "streaming architecture",
    "search_record": {
      "id": "http://arxiv.org/abs/2407.19449v4",
      "title": "A High-Throughput FPGA Accelerator for Lightweight CNNs With Balanced Dataflow",
      "summary": "FPGA accelerators for lightweight neural convolutional networks (LWCNNs) have recently attracted significant attention. Most existing LWCNN accelerators focus on single-Computing-Engine (CE) architecture with local optimization. However, these designs typically suffer from high on-chip/off-chip memory overhead and low computational efficiency due to their layer-by-layer dataflow and unified resource mapping mechanisms. To tackle these issues, a novel multi-CE-based accelerator with balanced dataflow is proposed to efficiently accelerate LWCNN through memory-oriented and computing-oriented optimizations. Firstly, a streaming architecture with hybrid CEs is designed to minimize off-chip memory access while maintaining a low cost of on-chip buffer size. Secondly, a balanced dataflow strategy is introduced for streaming architectures to enhance computational efficiency by improving efficient resource mapping and mitigating data congestion. Furthermore, a resource-aware memory and parallelism allocation methodology is proposed, based on a performance model, to achieve better performance and scalability. The proposed accelerator is evaluated on Xilinx ZC706 platform using MobileNetV2 and ShuffleNetV2.Implementation results demonstrate that the proposed accelerator can save up to 68.3% of on-chip memory size with reduced off-chip memory access compared to the reference design. It achieves an impressive performance of up to 2092.4 FPS and a state-of-the-art MAC efficiency of up to 94.58%, while maintaining a high DSP utilization of 95%, thus significantly outperforming current LWCNN accelerators.",
      "published": "2024-07-28T09:53:03Z"
    },
    "metadata": {
      "arxiv_id": "2407.19449",
      "title": "A High-Throughput FPGA Accelerator for Lightweight CNNs With Balanced Dataflow",
      "summary": "FPGA accelerators for lightweight neural convolutional networks (LWCNNs) have recently attracted significant attention. Most existing LWCNN accelerators focus on single-Computing-Engine (CE) architecture with local optimization. However, these designs typically suffer from high on-chip/off-chip memory overhead and low computational efficiency due to their layer-by-layer dataflow and unified resource mapping mechanisms. To tackle these issues, a novel multi-CE-based accelerator with balanced dataflow is proposed to efficiently accelerate LWCNN through memory-oriented and computing-oriented optimizations. Firstly, a streaming architecture with hybrid CEs is designed to minimize off-chip memory access while maintaining a low cost of on-chip buffer size. Secondly, a balanced dataflow strategy is introduced for streaming architectures to enhance computational efficiency by improving efficient resource mapping and mitigating data congestion. Furthermore, a resource-aware memory and parallelism allocation methodology is proposed, based on a performance model, to achieve better performance and scalability. The proposed accelerator is evaluated on Xilinx ZC706 platform using MobileNetV2 and ShuffleNetV2.Implementation results demonstrate that the proposed accelerator can save up to 68.3% of on-chip memory size with reduced off-chip memory access compared to the reference design. It achieves an impressive performance of up to 2092.4 FPS and a state-of-the-art MAC efficiency of up to 94.58%, while maintaining a high DSP utilization of 95%, thus significantly outperforming current LWCNN accelerators.",
      "authors": [
        "Zhiyuan Zhao",
        "Yihao Chen",
        "Pengcheng Feng",
        "Jixing Li",
        "Gang Chen",
        "Rongxuan Shen",
        "Huaxiang Lu"
      ],
      "published": "2024-07-28T09:53:03Z",
      "updated": "2024-12-16T12:22:03Z",
      "categories": [
        "cs.AR"
      ],
      "pdf_url": "https://arxiv.org/pdf/2407.19449v4",
      "landing_url": "https://arxiv.org/abs/2407.19449v4",
      "doi": "https://doi.org/10.48550/arXiv.2407.19449"
    },
    "queries": [
      {
        "anchor": "spoken language models",
        "search_term": "streaming architecture"
      },
      {
        "anchor": "synchronous dialogue",
        "search_term": "streaming architecture"
      },
      {
        "anchor": "turn-taking",
        "search_term": "streaming architecture"
      }
    ]
  },
  {
    "arxiv_id": "2407.21512",
    "anchor": "synchronous dialogue",
    "search_term": "real-time interaction",
    "search_record": {
      "id": "http://arxiv.org/abs/2407.21512v1",
      "title": "Interpreting and learning voice commands with a Large Language Model for a robot system",
      "summary": "Robots are increasingly common in industry and daily life, such as in nursing homes where they can assist staff. A key challenge is developing intuitive interfaces for easy communication. The use of Large Language Models (LLMs) like GPT-4 has enhanced robot capabilities, allowing for real-time interaction and decision-making. This integration improves robots' adaptability and functionality. This project focuses on merging LLMs with databases to improve decision-making and enable knowledge acquisition for request interpretation problems.",
      "published": "2024-07-31T10:30:31Z"
    },
    "metadata": {
      "arxiv_id": "2407.21512",
      "title": "Interpreting and learning voice commands with a Large Language Model for a robot system",
      "summary": "Robots are increasingly common in industry and daily life, such as in nursing homes where they can assist staff. A key challenge is developing intuitive interfaces for easy communication. The use of Large Language Models (LLMs) like GPT-4 has enhanced robot capabilities, allowing for real-time interaction and decision-making. This integration improves robots' adaptability and functionality. This project focuses on merging LLMs with databases to improve decision-making and enable knowledge acquisition for request interpretation problems.",
      "authors": [
        "Stanislau Stankevich",
        "Wojciech Dudek"
      ],
      "published": "2024-07-31T10:30:31Z",
      "updated": "2024-07-31T10:30:31Z",
      "categories": [
        "cs.RO",
        "cs.CL",
        "cs.NE"
      ],
      "pdf_url": "https://arxiv.org/pdf/2407.21512v1",
      "landing_url": "https://arxiv.org/abs/2407.21512v1",
      "doi": "https://doi.org/10.48550/arXiv.2407.21512"
    },
    "queries": [
      {
        "anchor": "synchronous dialogue",
        "search_term": "real-time interaction"
      }
    ]
  },
  {
    "arxiv_id": "2408.01180",
    "anchor": "spoken language models",
    "search_term": "audio tokens",
    "search_record": {
      "id": "http://arxiv.org/abs/2408.01180v1",
      "title": "Nested Music Transformer: Sequentially Decoding Compound Tokens in Symbolic Music and Audio Generation",
      "summary": "Representing symbolic music with compound tokens, where each token consists of several different sub-tokens representing a distinct musical feature or attribute, offers the advantage of reducing sequence length. While previous research has validated the efficacy of compound tokens in music sequence modeling, predicting all sub-tokens simultaneously can lead to suboptimal results as it may not fully capture the interdependencies between them. We introduce the Nested Music Transformer (NMT), an architecture tailored for decoding compound tokens autoregressively, similar to processing flattened tokens, but with low memory usage. The NMT consists of two transformers: the main decoder that models a sequence of compound tokens and the sub-decoder for modeling sub-tokens of each compound token. The experiment results showed that applying the NMT to compound tokens can enhance the performance in terms of better perplexity in processing various symbolic music datasets and discrete audio tokens from the MAESTRO dataset.",
      "published": "2024-08-02T11:02:38Z"
    },
    "metadata": {
      "arxiv_id": "2408.01180",
      "title": "Nested Music Transformer: Sequentially Decoding Compound Tokens in Symbolic Music and Audio Generation",
      "summary": "Representing symbolic music with compound tokens, where each token consists of several different sub-tokens representing a distinct musical feature or attribute, offers the advantage of reducing sequence length. While previous research has validated the efficacy of compound tokens in music sequence modeling, predicting all sub-tokens simultaneously can lead to suboptimal results as it may not fully capture the interdependencies between them. We introduce the Nested Music Transformer (NMT), an architecture tailored for decoding compound tokens autoregressively, similar to processing flattened tokens, but with low memory usage. The NMT consists of two transformers: the main decoder that models a sequence of compound tokens and the sub-decoder for modeling sub-tokens of each compound token. The experiment results showed that applying the NMT to compound tokens can enhance the performance in terms of better perplexity in processing various symbolic music datasets and discrete audio tokens from the MAESTRO dataset.",
      "authors": [
        "Jiwoo Ryu",
        "Hao-Wen Dong",
        "Jongmin Jung",
        "Dasaem Jeong"
      ],
      "published": "2024-08-02T11:02:38Z",
      "updated": "2024-08-02T11:02:38Z",
      "categories": [
        "cs.SD",
        "cs.IR",
        "cs.LG",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2408.01180v1",
      "landing_url": "https://arxiv.org/abs/2408.01180v1",
      "doi": "https://doi.org/10.48550/arXiv.2408.01180"
    },
    "queries": [
      {
        "anchor": "spoken language models",
        "search_term": "audio tokens"
      },
      {
        "anchor": "synchronous dialogue",
        "search_term": "audio tokens"
      },
      {
        "anchor": "turn-taking",
        "search_term": "audio tokens"
      }
    ]
  },
  {
    "arxiv_id": "2408.01688",
    "anchor": "synchronous dialogue",
    "search_term": "streaming architecture",
    "search_record": {
      "id": "http://arxiv.org/abs/2408.01688v2",
      "title": "SiamMo: Siamese Motion-Centric 3D Object Tracking",
      "summary": "Current 3D single object tracking methods primarily rely on the Siamese matching-based paradigm, which struggles with textureless and incomplete LiDAR point clouds. Conversely, the motion-centric paradigm avoids appearance matching, thus overcoming these issues. However, its complex multi-stage pipeline and the limited temporal modeling capability of a single-stream architecture constrain its potential. In this paper, we introduce SiamMo, a novel and simple Siamese motion-centric tracking approach. Unlike the traditional single-stream architecture, we employ Siamese feature extraction for motion-centric tracking. This decouples feature extraction from temporal fusion, significantly enhancing tracking performance. Additionally, we design a Spatio-Temporal Feature Aggregation module to integrate Siamese features at multiple scales, capturing motion information effectively. We also introduce a Box-aware Feature Encoding module to encode object size priors into motion estimation. SiamMo is a purely motion-centric tracker that eliminates the need for additional processes like segmentation and box refinement. Without whistles and bells, SiamMo not only surpasses state-of-the-art methods across multiple benchmarks but also demonstrates exceptional robustness in challenging scenarios. SiamMo sets a new record on the KITTI tracking benchmark with 90.1\\% precision while maintaining a high inference speed of 108 FPS. The code will be released at https://github.com/HDU-VRLab/SiamMo.",
      "published": "2024-08-03T07:02:01Z"
    },
    "metadata": {
      "arxiv_id": "2408.01688",
      "title": "SiamMo: Siamese Motion-Centric 3D Object Tracking",
      "summary": "Current 3D single object tracking methods primarily rely on the Siamese matching-based paradigm, which struggles with textureless and incomplete LiDAR point clouds. Conversely, the motion-centric paradigm avoids appearance matching, thus overcoming these issues. However, its complex multi-stage pipeline and the limited temporal modeling capability of a single-stream architecture constrain its potential. In this paper, we introduce SiamMo, a novel and simple Siamese motion-centric tracking approach. Unlike the traditional single-stream architecture, we employ Siamese feature extraction for motion-centric tracking. This decouples feature extraction from temporal fusion, significantly enhancing tracking performance. Additionally, we design a Spatio-Temporal Feature Aggregation module to integrate Siamese features at multiple scales, capturing motion information effectively. We also introduce a Box-aware Feature Encoding module to encode object size priors into motion estimation. SiamMo is a purely motion-centric tracker that eliminates the need for additional processes like segmentation and box refinement. Without whistles and bells, SiamMo not only surpasses state-of-the-art methods across multiple benchmarks but also demonstrates exceptional robustness in challenging scenarios. SiamMo sets a new record on the KITTI tracking benchmark with 90.1\\% precision while maintaining a high inference speed of 108 FPS. The code will be released at https://github.com/HDU-VRLab/SiamMo.",
      "authors": [
        "Yuxiang Yang",
        "Yingqi Deng",
        "Jing Zhang",
        "Hongjie Gu",
        "Zhekang Dong"
      ],
      "published": "2024-08-03T07:02:01Z",
      "updated": "2024-09-09T05:24:27Z",
      "categories": [
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2408.01688v2",
      "landing_url": "https://arxiv.org/abs/2408.01688v2",
      "doi": "https://doi.org/10.48550/arXiv.2408.01688"
    },
    "queries": [
      {
        "anchor": "synchronous dialogue",
        "search_term": "streaming architecture"
      }
    ]
  },
  {
    "arxiv_id": "2408.02622",
    "anchor": "spoken language models",
    "search_term": "real-time interaction",
    "search_record": {
      "id": "http://arxiv.org/abs/2408.02622v1",
      "title": "Language Model Can Listen While Speaking",
      "summary": "Dialogue serves as the most natural manner of human-computer interaction (HCI). Recent advancements in speech language models (SLM) have significantly enhanced speech-based conversational AI. However, these models are limited to turn-based conversation, lacking the ability to interact with humans in real-time spoken scenarios, for example, being interrupted when the generated content is not satisfactory. To address these limitations, we explore full duplex modeling (FDM) in interactive speech language models (iSLM), focusing on enhancing real-time interaction and, more explicitly, exploring the quintessential ability of interruption. We introduce a novel model design, namely listening-while-speaking language model (LSLM), an end-to-end system equipped with both listening and speaking channels. Our LSLM employs a token-based decoder-only TTS for speech generation and a streaming self-supervised learning (SSL) encoder for real-time audio input. LSLM fuses both channels for autoregressive generation and detects turn-taking in real time. Three fusion strategies -- early fusion, middle fusion, and late fusion -- are explored, with middle fusion achieving an optimal balance between speech generation and real-time interaction. Two experimental settings, command-based FDM and voice-based FDM, demonstrate LSLM's robustness to noise and sensitivity to diverse instructions. Our results highlight LSLM's capability to achieve duplex communication with minimal impact on existing systems. This study aims to advance the development of interactive speech dialogue systems, enhancing their applicability in real-world contexts.",
      "published": "2024-08-05T16:47:22Z"
    },
    "metadata": {
      "arxiv_id": "2408.02622",
      "title": "Language Model Can Listen While Speaking",
      "summary": "Dialogue serves as the most natural manner of human-computer interaction (HCI). Recent advancements in speech language models (SLM) have significantly enhanced speech-based conversational AI. However, these models are limited to turn-based conversation, lacking the ability to interact with humans in real-time spoken scenarios, for example, being interrupted when the generated content is not satisfactory. To address these limitations, we explore full duplex modeling (FDM) in interactive speech language models (iSLM), focusing on enhancing real-time interaction and, more explicitly, exploring the quintessential ability of interruption. We introduce a novel model design, namely listening-while-speaking language model (LSLM), an end-to-end system equipped with both listening and speaking channels. Our LSLM employs a token-based decoder-only TTS for speech generation and a streaming self-supervised learning (SSL) encoder for real-time audio input. LSLM fuses both channels for autoregressive generation and detects turn-taking in real time. Three fusion strategies -- early fusion, middle fusion, and late fusion -- are explored, with middle fusion achieving an optimal balance between speech generation and real-time interaction. Two experimental settings, command-based FDM and voice-based FDM, demonstrate LSLM's robustness to noise and sensitivity to diverse instructions. Our results highlight LSLM's capability to achieve duplex communication with minimal impact on existing systems. This study aims to advance the development of interactive speech dialogue systems, enhancing their applicability in real-world contexts.",
      "authors": [
        "Ziyang Ma",
        "Yakun Song",
        "Chenpeng Du",
        "Jian Cong",
        "Zhuo Chen",
        "Yuping Wang",
        "Yuxuan Wang",
        "Xie Chen"
      ],
      "published": "2024-08-05T16:47:22Z",
      "updated": "2024-08-05T16:47:22Z",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.HC",
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2408.02622v1",
      "landing_url": "https://arxiv.org/abs/2408.02622v1",
      "doi": "https://doi.org/10.48550/arXiv.2408.02622"
    },
    "queries": [
      {
        "anchor": "spoken language models",
        "search_term": "real-time interaction"
      },
      {
        "anchor": "synchronous dialogue",
        "search_term": "real-time interaction"
      },
      {
        "anchor": "turn-taking",
        "search_term": "real-time interaction"
      }
    ]
  },
  {
    "arxiv_id": "2408.02837",
    "anchor": "spoken language models",
    "search_term": "modular architecture",
    "search_record": {
      "id": "http://arxiv.org/abs/2408.02837v2",
      "title": "Modular Architectures and Entanglement Schemes for Error-Corrected Distributed Quantum Computation",
      "summary": "Connecting multiple smaller qubit modules by generating high-fidelity entangled states is a promising path for scaling quantum computing hardware. The performance of such a modular quantum computer is highly dependent on the quality and rate of entanglement generation. However, the optimal architectures and entanglement generation schemes are not yet established. Focusing on modular quantum computers with solid-state quantum hardware, we investigate a distributed surface code's error-correcting threshold and logical failure rate. We consider both emission-based and scattering-based entanglement generation schemes for the measurement of non-local stabilizers. Through quantum optical modeling, we link the performance of the quantum error correction code to the parameters of the underlying physical hardware and identify the necessary parameter regime for fault-tolerant modular quantum computation. In addition, we compare modular architectures with one or two data qubits per module. We find that the performance of the code depends significantly on the choice of entanglement generation scheme, while the two modular architectures have similar error-correcting thresholds. For some schemes, thresholds nearing the thresholds of non-distributed implementations ($\\sim0.4 \\%$) appear feasible with future parameters.",
      "published": "2024-08-05T21:20:03Z"
    },
    "metadata": {
      "arxiv_id": "2408.02837",
      "title": "Modular Architectures and Entanglement Schemes for Error-Corrected Distributed Quantum Computation",
      "summary": "Connecting multiple smaller qubit modules by generating high-fidelity entangled states is a promising path for scaling quantum computing hardware. The performance of such a modular quantum computer is highly dependent on the quality and rate of entanglement generation. However, the optimal architectures and entanglement generation schemes are not yet established. Focusing on modular quantum computers with solid-state quantum hardware, we investigate a distributed surface code's error-correcting threshold and logical failure rate. We consider both emission-based and scattering-based entanglement generation schemes for the measurement of non-local stabilizers. Through quantum optical modeling, we link the performance of the quantum error correction code to the parameters of the underlying physical hardware and identify the necessary parameter regime for fault-tolerant modular quantum computation. In addition, we compare modular architectures with one or two data qubits per module. We find that the performance of the code depends significantly on the choice of entanglement generation scheme, while the two modular architectures have similar error-correcting thresholds. For some schemes, thresholds nearing the thresholds of non-distributed implementations ($\\sim0.4 \\%$) appear feasible with future parameters.",
      "authors": [
        "Siddhant Singh",
        "Fenglei Gu",
        "Sébastian de Bone",
        "Eduardo Villaseñor",
        "David Elkouss",
        "Johannes Borregaard"
      ],
      "published": "2024-08-05T21:20:03Z",
      "updated": "2025-12-22T10:18:26Z",
      "categories": [
        "quant-ph"
      ],
      "pdf_url": "https://arxiv.org/pdf/2408.02837v2",
      "landing_url": "https://arxiv.org/abs/2408.02837v2",
      "doi": "https://doi.org/10.1038/s41534-025-01146-2"
    },
    "queries": [
      {
        "anchor": "spoken language models",
        "search_term": "modular architecture"
      },
      {
        "anchor": "synchronous dialogue",
        "search_term": "modular architecture"
      },
      {
        "anchor": "turn-taking",
        "search_term": "modular architecture"
      }
    ]
  },
  {
    "arxiv_id": "2408.06608",
    "anchor": "spoken language models",
    "search_term": "streaming architecture",
    "search_record": {
      "id": "http://arxiv.org/abs/2408.06608v1",
      "title": "Potamoi: Accelerating Neural Rendering via a Unified Streaming Architecture",
      "summary": "Neural Radiance Field (NeRF) has emerged as a promising alternative for photorealistic rendering. Despite recent algorithmic advancements, achieving real-time performance on today's resource-constrained devices remains challenging. In this paper, we identify the primary bottlenecks in current NeRF algorithms and introduce a unified algorithm-architecture co-design, Potamoi, designed to accommodate various NeRF algorithms. Specifically, we introduce a runtime system featuring a plug-and-play algorithm, SpaRW, which significantly reduces the per-frame computational workload and alleviates compute inefficiencies. Furthermore, our unified streaming pipeline coupled with customized hardware support effectively tames both SRAM and DRAM inefficiencies by minimizing repetitive DRAM access and completely eliminating SRAM bank conflicts. When evaluated against a baseline utilizing a dedicated DNN accelerator, our framework demonstrates a speed-up and energy reduction of 53.1$\\times$ and 67.7$\\times$, respectively, all while maintaining high visual quality with less than a 1.0 dB reduction in peak signal-to-noise ratio.",
      "published": "2024-08-13T03:39:44Z"
    },
    "metadata": {
      "arxiv_id": "2408.06608",
      "title": "Potamoi: Accelerating Neural Rendering via a Unified Streaming Architecture",
      "summary": "Neural Radiance Field (NeRF) has emerged as a promising alternative for photorealistic rendering. Despite recent algorithmic advancements, achieving real-time performance on today's resource-constrained devices remains challenging. In this paper, we identify the primary bottlenecks in current NeRF algorithms and introduce a unified algorithm-architecture co-design, Potamoi, designed to accommodate various NeRF algorithms. Specifically, we introduce a runtime system featuring a plug-and-play algorithm, SpaRW, which significantly reduces the per-frame computational workload and alleviates compute inefficiencies. Furthermore, our unified streaming pipeline coupled with customized hardware support effectively tames both SRAM and DRAM inefficiencies by minimizing repetitive DRAM access and completely eliminating SRAM bank conflicts. When evaluated against a baseline utilizing a dedicated DNN accelerator, our framework demonstrates a speed-up and energy reduction of 53.1$\\times$ and 67.7$\\times$, respectively, all while maintaining high visual quality with less than a 1.0 dB reduction in peak signal-to-noise ratio.",
      "authors": [
        "Yu Feng",
        "Weikai Lin",
        "Zihan Liu",
        "Jingwen Leng",
        "Minyi Guo",
        "Han Zhao",
        "Xiaofeng Hou",
        "Jieru Zhao",
        "Yuhao Zhu"
      ],
      "published": "2024-08-13T03:39:44Z",
      "updated": "2024-08-13T03:39:44Z",
      "categories": [
        "cs.AR",
        "cs.GR"
      ],
      "pdf_url": "https://arxiv.org/pdf/2408.06608v1",
      "landing_url": "https://arxiv.org/abs/2408.06608v1",
      "doi": "https://doi.org/10.48550/arXiv.2408.06608"
    },
    "queries": [
      {
        "anchor": "spoken language models",
        "search_term": "streaming architecture"
      },
      {
        "anchor": "synchronous dialogue",
        "search_term": "streaming architecture"
      },
      {
        "anchor": "turn-taking",
        "search_term": "streaming architecture"
      }
    ]
  },
  {
    "arxiv_id": "2408.07388",
    "anchor": "spoken language models",
    "search_term": "low latency",
    "search_record": {
      "id": "http://arxiv.org/abs/2408.07388v1",
      "title": "DPSNN: Spiking Neural Network for Low-Latency Streaming Speech Enhancement",
      "summary": "Speech enhancement (SE) improves communication in noisy environments, affecting areas such as automatic speech recognition, hearing aids, and telecommunications. With these domains typically being power-constrained and event-based while requiring low latency, neuromorphic algorithms in the form of spiking neural networks (SNNs) have great potential. Yet, current effective SNN solutions require a contextual sampling window imposing substantial latency, typically around 32ms, too long for many applications. Inspired by Dual-Path Spiking Neural Networks (DPSNNs) in classical neural networks, we develop a two-phase time-domain streaming SNN framework -- the Dual-Path Spiking Neural Network (DPSNN). In the DPSNN, the first phase uses Spiking Convolutional Neural Networks (SCNNs) to capture global contextual information, while the second phase uses Spiking Recurrent Neural Networks (SRNNs) to focus on frequency-related features. In addition, the regularizer suppresses activation to further enhance energy efficiency of our DPSNNs. Evaluating on the VCTK and Intel DNS Datasets, we demonstrate that our approach achieves the very low latency (approximately 5ms) required for applications like hearing aids, while demonstrating excellent signal-to-noise ratio (SNR), perceptual quality, and energy efficiency.",
      "published": "2024-08-14T09:08:43Z"
    },
    "metadata": {
      "arxiv_id": "2408.07388",
      "title": "DPSNN: Spiking Neural Network for Low-Latency Streaming Speech Enhancement",
      "summary": "Speech enhancement (SE) improves communication in noisy environments, affecting areas such as automatic speech recognition, hearing aids, and telecommunications. With these domains typically being power-constrained and event-based while requiring low latency, neuromorphic algorithms in the form of spiking neural networks (SNNs) have great potential. Yet, current effective SNN solutions require a contextual sampling window imposing substantial latency, typically around 32ms, too long for many applications. Inspired by Dual-Path Spiking Neural Networks (DPSNNs) in classical neural networks, we develop a two-phase time-domain streaming SNN framework -- the Dual-Path Spiking Neural Network (DPSNN). In the DPSNN, the first phase uses Spiking Convolutional Neural Networks (SCNNs) to capture global contextual information, while the second phase uses Spiking Recurrent Neural Networks (SRNNs) to focus on frequency-related features. In addition, the regularizer suppresses activation to further enhance energy efficiency of our DPSNNs. Evaluating on the VCTK and Intel DNS Datasets, we demonstrate that our approach achieves the very low latency (approximately 5ms) required for applications like hearing aids, while demonstrating excellent signal-to-noise ratio (SNR), perceptual quality, and energy efficiency.",
      "authors": [
        "Tao Sun",
        "Sander Bohté"
      ],
      "published": "2024-08-14T09:08:43Z",
      "updated": "2024-08-14T09:08:43Z",
      "categories": [
        "cs.SD",
        "cs.LG",
        "cs.NE",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2408.07388v1",
      "landing_url": "https://arxiv.org/abs/2408.07388v1",
      "doi": "https://doi.org/10.48550/arXiv.2408.07388"
    },
    "queries": [
      {
        "anchor": "spoken language models",
        "search_term": "low latency"
      },
      {
        "anchor": "synchronous dialogue",
        "search_term": "low latency"
      },
      {
        "anchor": "turn-taking",
        "search_term": "low latency"
      }
    ]
  },
  {
    "arxiv_id": "2408.08058",
    "anchor": "spoken language models",
    "search_term": "data scarcity",
    "search_record": {
      "id": "http://arxiv.org/abs/2408.08058v1",
      "title": "Navigating Data Scarcity using Foundation Models: A Benchmark of Few-Shot and Zero-Shot Learning Approaches in Medical Imaging",
      "summary": "Data scarcity is a major limiting factor for applying modern machine learning techniques to clinical tasks. Although sufficient data exists for some well-studied medical tasks, there remains a long tail of clinically relevant tasks with poor data availability. Recently, numerous foundation models have demonstrated high suitability for few-shot learning (FSL) and zero-shot learning (ZSL), potentially making them more accessible to practitioners. However, it remains unclear which foundation model performs best on FSL medical image analysis tasks and what the optimal methods are for learning from limited data. We conducted a comprehensive benchmark study of ZSL and FSL using 16 pretrained foundation models on 19 diverse medical imaging datasets. Our results indicate that BiomedCLIP, a model pretrained exclusively on medical data, performs best on average for very small training set sizes, while very large CLIP models pretrained on LAION-2B perform best with slightly more training samples. However, simply fine-tuning a ResNet-18 pretrained on ImageNet performs similarly with more than five training examples per class. Our findings also highlight the need for further research on foundation models specifically tailored for medical applications and the collection of more datasets to train these models.",
      "published": "2024-08-15T09:55:51Z"
    },
    "metadata": {
      "arxiv_id": "2408.08058",
      "title": "Navigating Data Scarcity using Foundation Models: A Benchmark of Few-Shot and Zero-Shot Learning Approaches in Medical Imaging",
      "summary": "Data scarcity is a major limiting factor for applying modern machine learning techniques to clinical tasks. Although sufficient data exists for some well-studied medical tasks, there remains a long tail of clinically relevant tasks with poor data availability. Recently, numerous foundation models have demonstrated high suitability for few-shot learning (FSL) and zero-shot learning (ZSL), potentially making them more accessible to practitioners. However, it remains unclear which foundation model performs best on FSL medical image analysis tasks and what the optimal methods are for learning from limited data. We conducted a comprehensive benchmark study of ZSL and FSL using 16 pretrained foundation models on 19 diverse medical imaging datasets. Our results indicate that BiomedCLIP, a model pretrained exclusively on medical data, performs best on average for very small training set sizes, while very large CLIP models pretrained on LAION-2B perform best with slightly more training samples. However, simply fine-tuning a ResNet-18 pretrained on ImageNet performs similarly with more than five training examples per class. Our findings also highlight the need for further research on foundation models specifically tailored for medical applications and the collection of more datasets to train these models.",
      "authors": [
        "Stefano Woerner",
        "Christian F. Baumgartner"
      ],
      "published": "2024-08-15T09:55:51Z",
      "updated": "2024-08-15T09:55:51Z",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2408.08058v1",
      "landing_url": "https://arxiv.org/abs/2408.08058v1",
      "doi": "https://doi.org/10.48550/arXiv.2408.08058"
    },
    "queries": [
      {
        "anchor": "spoken language models",
        "search_term": "data scarcity"
      },
      {
        "anchor": "synchronous dialogue",
        "search_term": "data scarcity"
      },
      {
        "anchor": "turn-taking",
        "search_term": "data scarcity"
      }
    ]
  },
  {
    "arxiv_id": "2408.08751",
    "anchor": "synchronous dialogue",
    "search_term": "semantic coherence",
    "search_record": {
      "id": "http://arxiv.org/abs/2408.08751v1",
      "title": "Comparative Analysis of Generative Models: Enhancing Image Synthesis with VAEs, GANs, and Stable Diffusion",
      "summary": "This paper examines three major generative modelling frameworks: Variational Autoencoders (VAEs), Generative Adversarial Networks (GANs), and Stable Diffusion models. VAEs are effective at learning latent representations but frequently yield blurry results. GANs can generate realistic images but face issues such as mode collapse. Stable Diffusion models, while producing high-quality images with strong semantic coherence, are demanding in terms of computational resources. Additionally, the paper explores how incorporating Grounding DINO and Grounded SAM with Stable Diffusion improves image accuracy by utilising sophisticated segmentation and inpainting techniques. The analysis guides on selecting suitable models for various applications and highlights areas for further research.",
      "published": "2024-08-16T13:50:50Z"
    },
    "metadata": {
      "arxiv_id": "2408.08751",
      "title": "Comparative Analysis of Generative Models: Enhancing Image Synthesis with VAEs, GANs, and Stable Diffusion",
      "summary": "This paper examines three major generative modelling frameworks: Variational Autoencoders (VAEs), Generative Adversarial Networks (GANs), and Stable Diffusion models. VAEs are effective at learning latent representations but frequently yield blurry results. GANs can generate realistic images but face issues such as mode collapse. Stable Diffusion models, while producing high-quality images with strong semantic coherence, are demanding in terms of computational resources. Additionally, the paper explores how incorporating Grounding DINO and Grounded SAM with Stable Diffusion improves image accuracy by utilising sophisticated segmentation and inpainting techniques. The analysis guides on selecting suitable models for various applications and highlights areas for further research.",
      "authors": [
        "Sanchayan Vivekananthan"
      ],
      "published": "2024-08-16T13:50:50Z",
      "updated": "2024-08-16T13:50:50Z",
      "categories": [
        "cs.CV",
        "eess.IV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2408.08751v1",
      "landing_url": "https://arxiv.org/abs/2408.08751v1",
      "doi": "https://doi.org/10.48550/arXiv.2408.08751"
    },
    "queries": [
      {
        "anchor": "synchronous dialogue",
        "search_term": "semantic coherence"
      }
    ]
  },
  {
    "arxiv_id": "2408.09027",
    "anchor": "synchronous dialogue",
    "search_term": "audio tokens",
    "search_record": {
      "id": "http://arxiv.org/abs/2408.09027v2",
      "title": "Efficient Autoregressive Audio Modeling via Next-Scale Prediction",
      "summary": "Audio generation has achieved remarkable progress with the advance of sophisticated generative models, such as diffusion models (DMs) and autoregressive (AR) models. However, due to the naturally significant sequence length of audio, the efficiency of audio generation remains an essential issue to be addressed, especially for AR models that are incorporated in large language models (LLMs). In this paper, we analyze the token length of audio tokenization and propose a novel \\textbf{S}cale-level \\textbf{A}udio \\textbf{T}okenizer (SAT), with improved residual quantization. Based on SAT, a scale-level \\textbf{A}coustic \\textbf{A}uto\\textbf{R}egressive (AAR) modeling framework is further proposed, which shifts the next-token AR prediction to next-scale AR prediction, significantly reducing the training cost and inference time. To validate the effectiveness of the proposed approach, we comprehensively analyze design choices and demonstrate the proposed AAR framework achieves a remarkable \\textbf{35}$\\times$ faster inference speed and +\\textbf{1.33} Fréchet Audio Distance (FAD) against baselines on the AudioSet benchmark. Code: \\url{https://github.com/qiuk2/AAR}.",
      "published": "2024-08-16T21:48:53Z"
    },
    "metadata": {
      "arxiv_id": "2408.09027",
      "title": "Efficient Autoregressive Audio Modeling via Next-Scale Prediction",
      "summary": "Audio generation has achieved remarkable progress with the advance of sophisticated generative models, such as diffusion models (DMs) and autoregressive (AR) models. However, due to the naturally significant sequence length of audio, the efficiency of audio generation remains an essential issue to be addressed, especially for AR models that are incorporated in large language models (LLMs). In this paper, we analyze the token length of audio tokenization and propose a novel \\textbf{S}cale-level \\textbf{A}udio \\textbf{T}okenizer (SAT), with improved residual quantization. Based on SAT, a scale-level \\textbf{A}coustic \\textbf{A}uto\\textbf{R}egressive (AAR) modeling framework is further proposed, which shifts the next-token AR prediction to next-scale AR prediction, significantly reducing the training cost and inference time. To validate the effectiveness of the proposed approach, we comprehensively analyze design choices and demonstrate the proposed AAR framework achieves a remarkable \\textbf{35}$\\times$ faster inference speed and +\\textbf{1.33} Fréchet Audio Distance (FAD) against baselines on the AudioSet benchmark. Code: \\url{https://github.com/qiuk2/AAR}.",
      "authors": [
        "Kai Qiu",
        "Xiang Li",
        "Hao Chen",
        "Jie Sun",
        "Jinglu Wang",
        "Zhe Lin",
        "Marios Savvides",
        "Bhiksha Raj"
      ],
      "published": "2024-08-16T21:48:53Z",
      "updated": "2024-12-16T21:50:56Z",
      "categories": [
        "cs.SD",
        "cs.AI",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2408.09027v2",
      "landing_url": "https://arxiv.org/abs/2408.09027v2",
      "doi": "https://doi.org/10.48550/arXiv.2408.09027"
    },
    "queries": [
      {
        "anchor": "synchronous dialogue",
        "search_term": "audio tokens"
      }
    ]
  },
  {
    "arxiv_id": "2408.11558",
    "anchor": "spoken language models",
    "search_term": "semantic coherence",
    "search_record": {
      "id": "http://arxiv.org/abs/2408.11558v1",
      "title": "GSTran: Joint Geometric and Semantic Coherence for Point Cloud Segmentation",
      "summary": "Learning meaningful local and global information remains a challenge in point cloud segmentation tasks. When utilizing local information, prior studies indiscriminately aggregates neighbor information from different classes to update query points, potentially compromising the distinctive feature of query points. In parallel, inaccurate modeling of long-distance contextual dependencies when utilizing global information can also impact model performance. To address these issues, we propose GSTran, a novel transformer network tailored for the segmentation task. The proposed network mainly consists of two principal components: a local geometric transformer and a global semantic transformer. In the local geometric transformer module, we explicitly calculate the geometric disparity within the local region. This enables amplifying the affinity with geometrically similar neighbor points while suppressing the association with other neighbors. In the global semantic transformer module, we design a multi-head voting strategy. This strategy evaluates semantic similarity across the entire spatial range, facilitating the precise capture of contextual dependencies. Experiments on ShapeNetPart and S3DIS benchmarks demonstrate the effectiveness of the proposed method, showing its superiority over other algorithms. The code is available at https://github.com/LAB123-tech/GSTran.",
      "published": "2024-08-21T12:12:37Z"
    },
    "metadata": {
      "arxiv_id": "2408.11558",
      "title": "GSTran: Joint Geometric and Semantic Coherence for Point Cloud Segmentation",
      "summary": "Learning meaningful local and global information remains a challenge in point cloud segmentation tasks. When utilizing local information, prior studies indiscriminately aggregates neighbor information from different classes to update query points, potentially compromising the distinctive feature of query points. In parallel, inaccurate modeling of long-distance contextual dependencies when utilizing global information can also impact model performance. To address these issues, we propose GSTran, a novel transformer network tailored for the segmentation task. The proposed network mainly consists of two principal components: a local geometric transformer and a global semantic transformer. In the local geometric transformer module, we explicitly calculate the geometric disparity within the local region. This enables amplifying the affinity with geometrically similar neighbor points while suppressing the association with other neighbors. In the global semantic transformer module, we design a multi-head voting strategy. This strategy evaluates semantic similarity across the entire spatial range, facilitating the precise capture of contextual dependencies. Experiments on ShapeNetPart and S3DIS benchmarks demonstrate the effectiveness of the proposed method, showing its superiority over other algorithms. The code is available at https://github.com/LAB123-tech/GSTran.",
      "authors": [
        "Abiao Li",
        "Chenlei Lv",
        "Guofeng Mei",
        "Yifan Zuo",
        "Jian Zhang",
        "Yuming Fang"
      ],
      "published": "2024-08-21T12:12:37Z",
      "updated": "2024-08-21T12:12:37Z",
      "categories": [
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2408.11558v1",
      "landing_url": "https://arxiv.org/abs/2408.11558v1",
      "doi": "https://doi.org/10.48550/arXiv.2408.11558"
    },
    "queries": [
      {
        "anchor": "spoken language models",
        "search_term": "semantic coherence"
      },
      {
        "anchor": "synchronous dialogue",
        "search_term": "semantic coherence"
      },
      {
        "anchor": "turn-taking",
        "search_term": "semantic coherence"
      }
    ]
  },
  {
    "arxiv_id": "2408.11842",
    "anchor": "spoken language models",
    "search_term": "low latency",
    "search_record": {
      "id": "http://arxiv.org/abs/2408.11842v2",
      "title": "Non-Causal to Causal SSL-Supported Transfer Learning: Towards a High-Performance Low-Latency Speech Vocoder",
      "summary": "Recently, BigVGAN has emerged as high-performance speech vocoder. Its sequence-to-sequence-based synthesis, however, prohibits usage in low-latency conversational applications. Our work addresses this shortcoming in three steps. First, we introduce low latency into BigVGAN via implementing causal convolutions, yielding decreased performance. Second, to regain performance, we propose a teacher-student transfer learning scheme to distill the high-delay non-causal BigVGAN into our low-latency causal vocoder. Third, taking advantage of a self-supervised learning (SSL) model, in our case wav2vec 2.0, we align its encoder speech representations extracted from our low-latency causal vocoder to the ground truth ones. In speaker-independent settings, both proposed training schemes notably elevate the performance of our low-latency vocoder, closing up to the original high-delay BigVGAN. At only 21% higher complexity, our best small causal vocoder achieves 3.96 PESQ and 1.25 MCD, excelling even the original small non-causal BigVGAN (3.64 PESQ) by 0.32 PESQ and 0.1 MCD points, respectively.",
      "published": "2024-08-07T12:49:40Z"
    },
    "metadata": {
      "arxiv_id": "2408.11842",
      "title": "Non-Causal to Causal SSL-Supported Transfer Learning: Towards a High-Performance Low-Latency Speech Vocoder",
      "summary": "Recently, BigVGAN has emerged as high-performance speech vocoder. Its sequence-to-sequence-based synthesis, however, prohibits usage in low-latency conversational applications. Our work addresses this shortcoming in three steps. First, we introduce low latency into BigVGAN via implementing causal convolutions, yielding decreased performance. Second, to regain performance, we propose a teacher-student transfer learning scheme to distill the high-delay non-causal BigVGAN into our low-latency causal vocoder. Third, taking advantage of a self-supervised learning (SSL) model, in our case wav2vec 2.0, we align its encoder speech representations extracted from our low-latency causal vocoder to the ground truth ones. In speaker-independent settings, both proposed training schemes notably elevate the performance of our low-latency vocoder, closing up to the original high-delay BigVGAN. At only 21% higher complexity, our best small causal vocoder achieves 3.96 PESQ and 1.25 MCD, excelling even the original small non-causal BigVGAN (3.64 PESQ) by 0.32 PESQ and 0.1 MCD points, respectively.",
      "authors": [
        "Renzheng Shi",
        "Andreas Bär",
        "Marvin Sach",
        "Wouter Tirry",
        "Tim Fingscheidt"
      ],
      "published": "2024-08-07T12:49:40Z",
      "updated": "2024-08-26T12:01:07Z",
      "categories": [
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2408.11842v2",
      "landing_url": "https://arxiv.org/abs/2408.11842v2",
      "doi": "https://doi.org/10.48550/arXiv.2408.11842"
    },
    "queries": [
      {
        "anchor": "spoken language models",
        "search_term": "low latency"
      },
      {
        "anchor": "synchronous dialogue",
        "search_term": "low latency"
      }
    ]
  },
  {
    "arxiv_id": "2408.13152",
    "anchor": "synchronous dialogue",
    "search_term": "data scarcity",
    "search_record": {
      "id": "http://arxiv.org/abs/2408.13152v2",
      "title": "Long-term Pre-training for Temporal Action Detection with Transformers",
      "summary": "Temporal action detection (TAD) is challenging, yet fundamental for real-world video applications. Recently, DETR-based models for TAD have been prevailing thanks to their unique benefits. However, transformers demand a huge dataset, and unfortunately data scarcity in TAD causes a severe degeneration. In this paper, we identify two crucial problems from data scarcity: attention collapse and imbalanced performance. To this end, we propose a new pre-training strategy, Long-Term Pre-training (LTP), tailored for transformers. LTP has two main components: 1) class-wise synthesis, 2) long-term pretext tasks. Firstly, we synthesize long-form video features by merging video snippets of a target class and non-target classes. They are analogous to untrimmed data used in TAD, despite being created from trimmed data. In addition, we devise two types of long-term pretext tasks to learn long-term dependency. They impose long-term conditions such as finding second-to-fourth or short-duration actions. Our extensive experiments show state-of-the-art performances in DETR-based methods on ActivityNet-v1.3 and THUMOS14 by a large margin. Moreover, we demonstrate that LTP significantly relieves the data scarcity issues in TAD.",
      "published": "2024-08-23T15:20:53Z"
    },
    "metadata": {
      "arxiv_id": "2408.13152",
      "title": "Long-term Pre-training for Temporal Action Detection with Transformers",
      "summary": "Temporal action detection (TAD) is challenging, yet fundamental for real-world video applications. Recently, DETR-based models for TAD have been prevailing thanks to their unique benefits. However, transformers demand a huge dataset, and unfortunately data scarcity in TAD causes a severe degeneration. In this paper, we identify two crucial problems from data scarcity: attention collapse and imbalanced performance. To this end, we propose a new pre-training strategy, Long-Term Pre-training (LTP), tailored for transformers. LTP has two main components: 1) class-wise synthesis, 2) long-term pretext tasks. Firstly, we synthesize long-form video features by merging video snippets of a target class and non-target classes. They are analogous to untrimmed data used in TAD, despite being created from trimmed data. In addition, we devise two types of long-term pretext tasks to learn long-term dependency. They impose long-term conditions such as finding second-to-fourth or short-duration actions. Our extensive experiments show state-of-the-art performances in DETR-based methods on ActivityNet-v1.3 and THUMOS14 by a large margin. Moreover, we demonstrate that LTP significantly relieves the data scarcity issues in TAD.",
      "authors": [
        "Jihwan Kim",
        "Miso Lee",
        "Jae-Pil Heo"
      ],
      "published": "2024-08-23T15:20:53Z",
      "updated": "2024-09-09T16:16:19Z",
      "categories": [
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2408.13152v2",
      "landing_url": "https://arxiv.org/abs/2408.13152v2",
      "doi": "https://doi.org/10.48550/arXiv.2408.13152"
    },
    "queries": [
      {
        "anchor": "synchronous dialogue",
        "search_term": "data scarcity"
      }
    ]
  },
  {
    "arxiv_id": "2408.13522",
    "anchor": "spoken language models",
    "search_term": "streaming architecture",
    "search_record": {
      "id": "http://arxiv.org/abs/2408.13522v1",
      "title": "StreamAAD: Decoding Spatial Auditory Attention with a Streaming Architecture",
      "summary": "In this paper, we present our approach for the Track 1 of the Chinese Auditory Attention Decoding (Chinese AAD) Challenge at ISCSLP 2024. Most existing spatial auditory attention decoding (Sp-AAD) methods employ an isolated window architecture, focusing solely on global invariant features without considering relationships between different decision windows, which can lead to suboptimal performance. To address this issue, we propose a novel streaming decoding architecture, termed StreamAAD. In StreamAAD, decision windows are input to the network as a sequential stream and decoded in order, allowing for the modeling of inter-window relationships. Additionally, we employ a model ensemble strategy, achieving significant better performance than the baseline, ranking First in the challenge.",
      "published": "2024-08-24T08:54:26Z"
    },
    "metadata": {
      "arxiv_id": "2408.13522",
      "title": "StreamAAD: Decoding Spatial Auditory Attention with a Streaming Architecture",
      "summary": "In this paper, we present our approach for the Track 1 of the Chinese Auditory Attention Decoding (Chinese AAD) Challenge at ISCSLP 2024. Most existing spatial auditory attention decoding (Sp-AAD) methods employ an isolated window architecture, focusing solely on global invariant features without considering relationships between different decision windows, which can lead to suboptimal performance. To address this issue, we propose a novel streaming decoding architecture, termed StreamAAD. In StreamAAD, decision windows are input to the network as a sequential stream and decoded in order, allowing for the modeling of inter-window relationships. Additionally, we employ a model ensemble strategy, achieving significant better performance than the baseline, ranking First in the challenge.",
      "authors": [
        "Zelin Qiu",
        "Dingding Yao",
        "Junfeng Li"
      ],
      "published": "2024-08-24T08:54:26Z",
      "updated": "2024-08-24T08:54:26Z",
      "categories": [
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2408.13522v1",
      "landing_url": "https://arxiv.org/abs/2408.13522v1",
      "doi": "https://doi.org/10.48550/arXiv.2408.13522"
    },
    "queries": [
      {
        "anchor": "spoken language models",
        "search_term": "streaming architecture"
      },
      {
        "anchor": "synchronous dialogue",
        "search_term": "streaming architecture"
      },
      {
        "anchor": "turn-taking",
        "search_term": "streaming architecture"
      }
    ]
  },
  {
    "arxiv_id": "2408.15503",
    "anchor": "synchronous dialogue",
    "search_term": "synchronous data",
    "search_record": {
      "id": "http://arxiv.org/abs/2408.15503v5",
      "title": "RoboSense: Large-scale Dataset and Benchmark for Egocentric Robot Perception and Navigation in Crowded and Unstructured Environments",
      "summary": "Reliable embodied perception from an egocentric perspective is challenging yet essential for autonomous navigation technology of intelligent mobile agents. With the growing demand of social robotics, near-field scene understanding becomes an important research topic in the areas of egocentric perceptual tasks related to navigation in both crowded and unstructured environments. Due to the complexity of environmental conditions and difficulty of surrounding obstacles owing to truncation and occlusion, the perception capability under this circumstance is still inferior. To further enhance the intelligence of mobile robots, in this paper, we setup an egocentric multi-sensor data collection platform based on 3 main types of sensors (Camera, LiDAR and Fisheye), which supports flexible sensor configurations to enable dynamic sight of view from ego-perspective, capturing either near or farther areas. Meanwhile, a large-scale multimodal dataset is constructed, named RoboSense, to facilitate egocentric robot perception. Specifically, RoboSense contains more than 133K synchronized data with 1.4M 3D bounding box and IDs annotated in the full $360^{\\circ}$ view, forming 216K trajectories across 7.6K temporal sequences. It has $270\\times$ and $18\\times$ as many annotations of surrounding obstacles within near ranges as the previous datasets collected for autonomous driving scenarios such as KITTI and nuScenes. Moreover, we define a novel matching criterion for near-field 3D perception and prediction metrics. Based on RoboSense, we formulate 6 popular tasks to facilitate the future research development, where the detailed analysis as well as benchmarks are also provided accordingly. Data desensitization measures have been conducted for privacy protection.",
      "published": "2024-08-28T03:17:40Z"
    },
    "metadata": {
      "arxiv_id": "2408.15503",
      "title": "RoboSense: Large-scale Dataset and Benchmark for Egocentric Robot Perception and Navigation in Crowded and Unstructured Environments",
      "summary": "Reliable embodied perception from an egocentric perspective is challenging yet essential for autonomous navigation technology of intelligent mobile agents. With the growing demand of social robotics, near-field scene understanding becomes an important research topic in the areas of egocentric perceptual tasks related to navigation in both crowded and unstructured environments. Due to the complexity of environmental conditions and difficulty of surrounding obstacles owing to truncation and occlusion, the perception capability under this circumstance is still inferior. To further enhance the intelligence of mobile robots, in this paper, we setup an egocentric multi-sensor data collection platform based on 3 main types of sensors (Camera, LiDAR and Fisheye), which supports flexible sensor configurations to enable dynamic sight of view from ego-perspective, capturing either near or farther areas. Meanwhile, a large-scale multimodal dataset is constructed, named RoboSense, to facilitate egocentric robot perception. Specifically, RoboSense contains more than 133K synchronized data with 1.4M 3D bounding box and IDs annotated in the full $360^{\\circ}$ view, forming 216K trajectories across 7.6K temporal sequences. It has $270\\times$ and $18\\times$ as many annotations of surrounding obstacles within near ranges as the previous datasets collected for autonomous driving scenarios such as KITTI and nuScenes. Moreover, we define a novel matching criterion for near-field 3D perception and prediction metrics. Based on RoboSense, we formulate 6 popular tasks to facilitate the future research development, where the detailed analysis as well as benchmarks are also provided accordingly. Data desensitization measures have been conducted for privacy protection.",
      "authors": [
        "Haisheng Su",
        "Feixiang Song",
        "Cong Ma",
        "Wei Wu",
        "Junchi Yan"
      ],
      "published": "2024-08-28T03:17:40Z",
      "updated": "2025-03-05T05:14:34Z",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2408.15503v5",
      "landing_url": "https://arxiv.org/abs/2408.15503v5",
      "doi": "https://doi.org/10.48550/arXiv.2408.15503"
    },
    "queries": [
      {
        "anchor": "synchronous dialogue",
        "search_term": "synchronous data"
      }
    ]
  },
  {
    "arxiv_id": "2408.15660",
    "anchor": "spoken language models",
    "search_term": "semantic coherence",
    "search_record": {
      "id": "http://arxiv.org/abs/2408.15660v1",
      "title": "Merging and Splitting Diffusion Paths for Semantically Coherent Panoramas",
      "summary": "Diffusion models have become the State-of-the-Art for text-to-image generation, and increasing research effort has been dedicated to adapting the inference process of pretrained diffusion models to achieve zero-shot capabilities. An example is the generation of panorama images, which has been tackled in recent works by combining independent diffusion paths over overlapping latent features, which is referred to as joint diffusion, obtaining perceptually aligned panoramas. However, these methods often yield semantically incoherent outputs and trade-off diversity for uniformity. To overcome this limitation, we propose the Merge-Attend-Diffuse operator, which can be plugged into different types of pretrained diffusion models used in a joint diffusion setting to improve the perceptual and semantical coherence of the generated panorama images. Specifically, we merge the diffusion paths, reprogramming self- and cross-attention to operate on the aggregated latent space. Extensive quantitative and qualitative experimental analysis, together with a user study, demonstrate that our method maintains compatibility with the input prompt and visual quality of the generated images while increasing their semantic coherence. We release the code at https://github.com/aimagelab/MAD.",
      "published": "2024-08-28T09:22:32Z"
    },
    "metadata": {
      "arxiv_id": "2408.15660",
      "title": "Merging and Splitting Diffusion Paths for Semantically Coherent Panoramas",
      "summary": "Diffusion models have become the State-of-the-Art for text-to-image generation, and increasing research effort has been dedicated to adapting the inference process of pretrained diffusion models to achieve zero-shot capabilities. An example is the generation of panorama images, which has been tackled in recent works by combining independent diffusion paths over overlapping latent features, which is referred to as joint diffusion, obtaining perceptually aligned panoramas. However, these methods often yield semantically incoherent outputs and trade-off diversity for uniformity. To overcome this limitation, we propose the Merge-Attend-Diffuse operator, which can be plugged into different types of pretrained diffusion models used in a joint diffusion setting to improve the perceptual and semantical coherence of the generated panorama images. Specifically, we merge the diffusion paths, reprogramming self- and cross-attention to operate on the aggregated latent space. Extensive quantitative and qualitative experimental analysis, together with a user study, demonstrate that our method maintains compatibility with the input prompt and visual quality of the generated images while increasing their semantic coherence. We release the code at https://github.com/aimagelab/MAD.",
      "authors": [
        "Fabio Quattrini",
        "Vittorio Pippi",
        "Silvia Cascianelli",
        "Rita Cucchiara"
      ],
      "published": "2024-08-28T09:22:32Z",
      "updated": "2024-08-28T09:22:32Z",
      "categories": [
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2408.15660v1",
      "landing_url": "https://arxiv.org/abs/2408.15660v1",
      "doi": "https://doi.org/10.48550/arXiv.2408.15660"
    },
    "queries": [
      {
        "anchor": "spoken language models",
        "search_term": "semantic coherence"
      },
      {
        "anchor": "synchronous dialogue",
        "search_term": "semantic coherence"
      },
      {
        "anchor": "turn-taking",
        "search_term": "semantic coherence"
      }
    ]
  },
  {
    "arxiv_id": "2408.16373",
    "anchor": "synchronous dialogue",
    "search_term": "discrete tokens",
    "search_record": {
      "id": "http://arxiv.org/abs/2408.16373v1",
      "title": "Enabling Beam Search for Language Model-Based Text-to-Speech Synthesis",
      "summary": "Tokenising continuous speech into sequences of discrete tokens and modelling them with language models (LMs) has led to significant success in text-to-speech (TTS) synthesis. Although these models can generate speech with high quality and naturalness, their synthesised samples can still suffer from artefacts, mispronunciation, word repeating, etc. In this paper, we argue these undesirable properties could partly be caused by the randomness of sampling-based strategies during the autoregressive decoding of LMs. Therefore, we look at maximisation-based decoding approaches and propose Temporal Repetition Aware Diverse Beam Search (TRAD-BS) to find the most probable sequences of the generated speech tokens. Experiments with two state-of-the-art LM-based TTS models demonstrate that our proposed maximisation-based decoding strategy generates speech with fewer mispronunciations and improved speaker consistency.",
      "published": "2024-08-29T09:31:06Z"
    },
    "metadata": {
      "arxiv_id": "2408.16373",
      "title": "Enabling Beam Search for Language Model-Based Text-to-Speech Synthesis",
      "summary": "Tokenising continuous speech into sequences of discrete tokens and modelling them with language models (LMs) has led to significant success in text-to-speech (TTS) synthesis. Although these models can generate speech with high quality and naturalness, their synthesised samples can still suffer from artefacts, mispronunciation, word repeating, etc. In this paper, we argue these undesirable properties could partly be caused by the randomness of sampling-based strategies during the autoregressive decoding of LMs. Therefore, we look at maximisation-based decoding approaches and propose Temporal Repetition Aware Diverse Beam Search (TRAD-BS) to find the most probable sequences of the generated speech tokens. Experiments with two state-of-the-art LM-based TTS models demonstrate that our proposed maximisation-based decoding strategy generates speech with fewer mispronunciations and improved speaker consistency.",
      "authors": [
        "Zehai Tu",
        "Guangyan Zhang",
        "Yiting Lu",
        "Adaeze Adigwe",
        "Simon King",
        "Yiwen Guo"
      ],
      "published": "2024-08-29T09:31:06Z",
      "updated": "2024-08-29T09:31:06Z",
      "categories": [
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2408.16373v1",
      "landing_url": "https://arxiv.org/abs/2408.16373v1",
      "doi": "https://doi.org/10.48550/arXiv.2408.16373"
    },
    "queries": [
      {
        "anchor": "synchronous dialogue",
        "search_term": "discrete tokens"
      }
    ]
  },
  {
    "arxiv_id": "2408.16725",
    "anchor": "synchronous dialogue",
    "search_term": "real-time interaction",
    "search_record": {
      "id": "http://arxiv.org/abs/2408.16725v3",
      "title": "Mini-Omni: Language Models Can Hear, Talk While Thinking in Streaming",
      "summary": "Recent advances in language models have achieved significant progress. GPT-4o, as a new milestone, has enabled real-time conversations with humans, demonstrating near-human natural fluency. Such human-computer interaction necessitates models with the capability to perform reasoning directly with the audio modality and generate output in streaming. However, this remains beyond the reach of current academic models, as they typically depend on extra TTS systems for speech synthesis, resulting in undesirable latency. This paper introduces the Mini-Omni, an audio-based end-to-end conversational model, capable of real-time speech interaction. To achieve this capability, we propose a text-instructed speech generation method, along with batch-parallel strategies during inference to further boost the performance. Our method also helps to retain the original model's language capabilities with minimal degradation, enabling other works to establish real-time interaction capabilities. We call this training method \"Any Model Can Talk\". We also introduce the VoiceAssistant-400K dataset to fine-tune models optimized for speech output. To our best knowledge, Mini-Omni is the first fully end-to-end, open-source model for real-time speech interaction, offering valuable potential for future research.",
      "published": "2024-08-29T17:18:53Z"
    },
    "metadata": {
      "arxiv_id": "2408.16725",
      "title": "Mini-Omni: Language Models Can Hear, Talk While Thinking in Streaming",
      "summary": "Recent advances in language models have achieved significant progress. GPT-4o, as a new milestone, has enabled real-time conversations with humans, demonstrating near-human natural fluency. Such human-computer interaction necessitates models with the capability to perform reasoning directly with the audio modality and generate output in streaming. However, this remains beyond the reach of current academic models, as they typically depend on extra TTS systems for speech synthesis, resulting in undesirable latency. This paper introduces the Mini-Omni, an audio-based end-to-end conversational model, capable of real-time speech interaction. To achieve this capability, we propose a text-instructed speech generation method, along with batch-parallel strategies during inference to further boost the performance. Our method also helps to retain the original model's language capabilities with minimal degradation, enabling other works to establish real-time interaction capabilities. We call this training method \"Any Model Can Talk\". We also introduce the VoiceAssistant-400K dataset to fine-tune models optimized for speech output. To our best knowledge, Mini-Omni is the first fully end-to-end, open-source model for real-time speech interaction, offering valuable potential for future research.",
      "authors": [
        "Zhifei Xie",
        "Changqiao Wu"
      ],
      "published": "2024-08-29T17:18:53Z",
      "updated": "2024-11-05T02:24:18Z",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.HC",
        "cs.LG",
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2408.16725v3",
      "landing_url": "https://arxiv.org/abs/2408.16725v3",
      "doi": "https://doi.org/10.48550/arXiv.2408.16725"
    },
    "queries": [
      {
        "anchor": "synchronous dialogue",
        "search_term": "real-time interaction"
      }
    ]
  },
  {
    "arxiv_id": "2408.17175",
    "anchor": "synchronous dialogue",
    "search_term": "audio tokens",
    "search_record": {
      "id": "http://arxiv.org/abs/2408.17175v3",
      "title": "Codec Does Matter: Exploring the Semantic Shortcoming of Codec for Audio Language Model",
      "summary": "Recent advancements in audio generation have been significantly propelled by the capabilities of Large Language Models (LLMs). The existing research on audio LLM has primarily focused on enhancing the architecture and scale of audio language models, as well as leveraging larger datasets, and generally, acoustic codecs, such as EnCodec, are used for audio tokenization. However, these codecs were originally designed for audio compression, which may lead to suboptimal performance in the context of audio LLM. Our research aims to address the shortcomings of current audio LLM codecs, particularly their challenges in maintaining semantic integrity in generated audio. For instance, existing methods like VALL-E, which condition acoustic token generation on text transcriptions, often suffer from content inaccuracies and elevated word error rates (WER) due to semantic misinterpretations of acoustic tokens, resulting in word skipping and errors. To overcome these issues, we propose a straightforward yet effective approach called X-Codec. X-Codec incorporates semantic features from a pre-trained semantic encoder before the Residual Vector Quantization (RVQ) stage and introduces a semantic reconstruction loss after RVQ. By enhancing the semantic ability of the codec, X-Codec significantly reduces WER in speech synthesis tasks and extends these benefits to non-speech applications, including music and sound generation. Our experiments in text-to-speech, music continuation, and text-to-sound tasks demonstrate that integrating semantic information substantially improves the overall performance of language models in audio generation. Our code and demo are available (Demo: https://x-codec-audio.github.io Code: https://github.com/zhenye234/xcodec)",
      "published": "2024-08-30T10:24:07Z"
    },
    "metadata": {
      "arxiv_id": "2408.17175",
      "title": "Codec Does Matter: Exploring the Semantic Shortcoming of Codec for Audio Language Model",
      "summary": "Recent advancements in audio generation have been significantly propelled by the capabilities of Large Language Models (LLMs). The existing research on audio LLM has primarily focused on enhancing the architecture and scale of audio language models, as well as leveraging larger datasets, and generally, acoustic codecs, such as EnCodec, are used for audio tokenization. However, these codecs were originally designed for audio compression, which may lead to suboptimal performance in the context of audio LLM. Our research aims to address the shortcomings of current audio LLM codecs, particularly their challenges in maintaining semantic integrity in generated audio. For instance, existing methods like VALL-E, which condition acoustic token generation on text transcriptions, often suffer from content inaccuracies and elevated word error rates (WER) due to semantic misinterpretations of acoustic tokens, resulting in word skipping and errors. To overcome these issues, we propose a straightforward yet effective approach called X-Codec. X-Codec incorporates semantic features from a pre-trained semantic encoder before the Residual Vector Quantization (RVQ) stage and introduces a semantic reconstruction loss after RVQ. By enhancing the semantic ability of the codec, X-Codec significantly reduces WER in speech synthesis tasks and extends these benefits to non-speech applications, including music and sound generation. Our experiments in text-to-speech, music continuation, and text-to-sound tasks demonstrate that integrating semantic information substantially improves the overall performance of language models in audio generation. Our code and demo are available (Demo: https://x-codec-audio.github.io Code: https://github.com/zhenye234/xcodec)",
      "authors": [
        "Zhen Ye",
        "Peiwen Sun",
        "Jiahe Lei",
        "Hongzhan Lin",
        "Xu Tan",
        "Zheqi Dai",
        "Qiuqiang Kong",
        "Jianyi Chen",
        "Jiahao Pan",
        "Qifeng Liu",
        "Yike Guo",
        "Wei Xue"
      ],
      "published": "2024-08-30T10:24:07Z",
      "updated": "2024-11-27T11:47:45Z",
      "categories": [
        "eess.AS",
        "cs.AI",
        "cs.CL",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2408.17175v3",
      "landing_url": "https://arxiv.org/abs/2408.17175v3",
      "doi": "https://doi.org/10.48550/arXiv.2408.17175"
    },
    "queries": [
      {
        "anchor": "synchronous dialogue",
        "search_term": "audio tokens"
      }
    ]
  },
  {
    "arxiv_id": "2409.01263",
    "anchor": "full-duplex",
    "search_term": "half duplex",
    "search_record": {
      "id": "http://arxiv.org/abs/2409.01263v1",
      "title": "Coordinated Half-Duplex/Full-Duplex Cooperative Rate-Splitting Multiple Access in Multi-Cell Networks",
      "summary": "This paper explores downlink Cooperative Rate-Splitting Multiple Access (C-RSMA) in a multi-cell wireless network with the assistance of Joint-Transmission Coordinated Multipoint (JT-CoMP). In this network, each cell consists of a base station (BS) equipped with multiple antennas, one or more cell-center users (CCU), and multiple cell-edge users (CEU) located at the edge of the cells. Through JT-CoMP, all the BSs collaborate to simultaneously transmit the data to all the users including the CCUs and CEUs. To enhance the signal quality for the CEUs, CCUs relay the common stream to the CEUs by operating in either half-duplex (HD) or full-duplex (FD) decode-and-forward (DF) relaying mode. In this setup, we aim to jointly optimize the beamforming vectors at the BS, the allocation of common stream rates, the transmit power at relaying users, i.e., CCUs, and the time slot fraction, aiming to maximize the minimum achievable data rate. However, the formulated optimization problem is non-convex and is challenging to solve directly. To address this challenge, we employ change-of-variables, first-order Taylor approximations, and a low-complexity algorithm based on Successive Convex Approximation (SCA). We demonstrate through simulation results the efficacy of the proposed scheme, in terms of average achievable data rate, and we compare its performance to that of four baseline schemes, including HD/FD cooperative non-orthogonal multiple access (C-NOMA), NOMA, and RSMA without user cooperation. The results show that the proposed FD C-RSMA can achieve 25% over FD C-NOMA and the proposed HD C-RSMA can achieve 19% over HD C-NOMA respectively, when the BS transmit power is 20 dBm.",
      "published": "2024-09-02T13:55:45Z"
    },
    "metadata": {
      "arxiv_id": "2409.01263",
      "title": "Coordinated Half-Duplex/Full-Duplex Cooperative Rate-Splitting Multiple Access in Multi-Cell Networks",
      "summary": "This paper explores downlink Cooperative Rate-Splitting Multiple Access (C-RSMA) in a multi-cell wireless network with the assistance of Joint-Transmission Coordinated Multipoint (JT-CoMP). In this network, each cell consists of a base station (BS) equipped with multiple antennas, one or more cell-center users (CCU), and multiple cell-edge users (CEU) located at the edge of the cells. Through JT-CoMP, all the BSs collaborate to simultaneously transmit the data to all the users including the CCUs and CEUs. To enhance the signal quality for the CEUs, CCUs relay the common stream to the CEUs by operating in either half-duplex (HD) or full-duplex (FD) decode-and-forward (DF) relaying mode. In this setup, we aim to jointly optimize the beamforming vectors at the BS, the allocation of common stream rates, the transmit power at relaying users, i.e., CCUs, and the time slot fraction, aiming to maximize the minimum achievable data rate. However, the formulated optimization problem is non-convex and is challenging to solve directly. To address this challenge, we employ change-of-variables, first-order Taylor approximations, and a low-complexity algorithm based on Successive Convex Approximation (SCA). We demonstrate through simulation results the efficacy of the proposed scheme, in terms of average achievable data rate, and we compare its performance to that of four baseline schemes, including HD/FD cooperative non-orthogonal multiple access (C-NOMA), NOMA, and RSMA without user cooperation. The results show that the proposed FD C-RSMA can achieve 25% over FD C-NOMA and the proposed HD C-RSMA can achieve 19% over HD C-NOMA respectively, when the BS transmit power is 20 dBm.",
      "authors": [
        "Mohamed Elhattab",
        "Shreya Khisa",
        "Chadi Assi",
        "Ali Ghrayeb",
        "Marwa Qaraqe",
        "Georges Kaddoum"
      ],
      "published": "2024-09-02T13:55:45Z",
      "updated": "2024-09-02T13:55:45Z",
      "categories": [
        "eess.SP"
      ],
      "pdf_url": "https://arxiv.org/pdf/2409.01263v1",
      "landing_url": "https://arxiv.org/abs/2409.01263v1",
      "doi": "https://doi.org/10.48550/arXiv.2409.01263"
    },
    "queries": [
      {
        "anchor": "full-duplex",
        "search_term": "half duplex"
      },
      {
        "anchor": "synchronous dialogue",
        "search_term": "half duplex"
      }
    ]
  },
  {
    "arxiv_id": "2409.01995",
    "anchor": "spoken language models",
    "search_term": "discrete tokens",
    "search_record": {
      "id": "http://arxiv.org/abs/2409.01995v4",
      "title": "vec2wav 2.0: Advancing Voice Conversion via Discrete Token Vocoders",
      "summary": "We propose a new speech discrete token vocoder, vec2wav 2.0, which advances voice conversion (VC). We use discrete tokens from speech self-supervised models as the content features of source speech, and treat VC as a prompted vocoding task. To amend the loss of speaker timbre in the content tokens, vec2wav 2.0 utilizes the WavLM features to provide strong timbre-dependent information. A novel adaptive Snake activation function is proposed to better incorporate timbre into the waveform reconstruction process. In this way, vec2wav 2.0 learns to alter the speaker timbre appropriately given different reference prompts. Also, no supervised data is required for vec2wav 2.0 to be effectively trained. Experimental results demonstrate that vec2wav 2.0 outperforms all other baselines to a considerable margin in terms of audio quality and speaker similarity in any-to-any VC. Ablation studies verify the effects made by the proposed techniques. Moreover, vec2wav 2.0 achieves competitive cross-lingual VC even only trained on monolingual corpus. Thus, vec2wav 2.0 shows timbre can potentially be manipulated only by speech token vocoders, pushing the frontiers of VC and speech synthesis.",
      "published": "2024-09-03T15:41:07Z"
    },
    "metadata": {
      "arxiv_id": "2409.01995",
      "title": "vec2wav 2.0: Advancing Voice Conversion via Discrete Token Vocoders",
      "summary": "We propose a new speech discrete token vocoder, vec2wav 2.0, which advances voice conversion (VC). We use discrete tokens from speech self-supervised models as the content features of source speech, and treat VC as a prompted vocoding task. To amend the loss of speaker timbre in the content tokens, vec2wav 2.0 utilizes the WavLM features to provide strong timbre-dependent information. A novel adaptive Snake activation function is proposed to better incorporate timbre into the waveform reconstruction process. In this way, vec2wav 2.0 learns to alter the speaker timbre appropriately given different reference prompts. Also, no supervised data is required for vec2wav 2.0 to be effectively trained. Experimental results demonstrate that vec2wav 2.0 outperforms all other baselines to a considerable margin in terms of audio quality and speaker similarity in any-to-any VC. Ablation studies verify the effects made by the proposed techniques. Moreover, vec2wav 2.0 achieves competitive cross-lingual VC even only trained on monolingual corpus. Thus, vec2wav 2.0 shows timbre can potentially be manipulated only by speech token vocoders, pushing the frontiers of VC and speech synthesis.",
      "authors": [
        "Yiwei Guo",
        "Zhihan Li",
        "Junjie Li",
        "Chenpeng Du",
        "Hankun Wang",
        "Shuai Wang",
        "Xie Chen",
        "Kai Yu"
      ],
      "published": "2024-09-03T15:41:07Z",
      "updated": "2025-05-24T13:50:34Z",
      "categories": [
        "eess.AS",
        "cs.AI",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2409.01995v4",
      "landing_url": "https://arxiv.org/abs/2409.01995v4",
      "doi": "https://doi.org/10.48550/arXiv.2409.01995"
    },
    "queries": [
      {
        "anchor": "spoken language models",
        "search_term": "discrete tokens"
      },
      {
        "anchor": "synchronous dialogue",
        "search_term": "discrete tokens"
      },
      {
        "anchor": "turn-taking",
        "search_term": "discrete tokens"
      }
    ]
  },
  {
    "arxiv_id": "2409.02376",
    "anchor": "synchronous dialogue",
    "search_term": "real-time interaction",
    "search_record": {
      "id": "http://arxiv.org/abs/2409.02376v1",
      "title": "Coral Model Generation from Single Images for Virtual Reality Applications",
      "summary": "With the rapid development of VR technology, the demand for high-quality 3D models is increasing. Traditional methods struggle with efficiency and quality in large-scale customization. This paper introduces a deep-learning framework that generates high-precision 3D coral models from a single image. Using the Coral dataset, the framework extracts geometric and texture features, performs 3D reconstruction, and optimizes design and material blending. Advanced optimization and polygon count control ensure shape accuracy, detail retention, and flexible output for various complexities, catering to high-quality rendering and real-time interaction needs.The project incorporates Explainable AI (XAI) to transform AI-generated models into interactive \"artworks,\" best viewed in VR and XR. This enhances model interpretability and human-machine collaboration. Real-time feedback in VR interactions displays information like coral species and habitat, enriching user experience. The generated models surpass traditional methods in detail, visual quality, and efficiency. This research offers an intelligent approach to 3D content creation for VR, lowering production barriers, and promoting widespread VR applications. Additionally, integrating XAI provides new insights into AI-generated visual content and advances research in 3D vision interpretability.",
      "published": "2024-09-04T01:54:20Z"
    },
    "metadata": {
      "arxiv_id": "2409.02376",
      "title": "Coral Model Generation from Single Images for Virtual Reality Applications",
      "summary": "With the rapid development of VR technology, the demand for high-quality 3D models is increasing. Traditional methods struggle with efficiency and quality in large-scale customization. This paper introduces a deep-learning framework that generates high-precision 3D coral models from a single image. Using the Coral dataset, the framework extracts geometric and texture features, performs 3D reconstruction, and optimizes design and material blending. Advanced optimization and polygon count control ensure shape accuracy, detail retention, and flexible output for various complexities, catering to high-quality rendering and real-time interaction needs.The project incorporates Explainable AI (XAI) to transform AI-generated models into interactive \"artworks,\" best viewed in VR and XR. This enhances model interpretability and human-machine collaboration. Real-time feedback in VR interactions displays information like coral species and habitat, enriching user experience. The generated models surpass traditional methods in detail, visual quality, and efficiency. This research offers an intelligent approach to 3D content creation for VR, lowering production barriers, and promoting widespread VR applications. Additionally, integrating XAI provides new insights into AI-generated visual content and advances research in 3D vision interpretability.",
      "authors": [
        "Jie Fu",
        "Shun Fu",
        "Mick Grierson"
      ],
      "published": "2024-09-04T01:54:20Z",
      "updated": "2024-09-04T01:54:20Z",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.GR",
        "cs.HC",
        "cs.MM"
      ],
      "pdf_url": "https://arxiv.org/pdf/2409.02376v1",
      "landing_url": "https://arxiv.org/abs/2409.02376v1",
      "doi": "https://doi.org/10.48550/arXiv.2409.02376"
    },
    "queries": [
      {
        "anchor": "synchronous dialogue",
        "search_term": "real-time interaction"
      }
    ]
  },
  {
    "arxiv_id": "2409.03701",
    "anchor": "spoken language models",
    "search_term": "true full-duplex",
    "search_record": {
      "id": "http://arxiv.org/abs/2409.03701v2",
      "title": "LAST: Language Model Aware Speech Tokenization",
      "summary": "Speech tokenization serves as the foundation of speech language model (LM), enabling them to perform various tasks such as spoken language modeling, text-to-speech, speech-to-text, etc. Most speech tokenizers are trained independently of the LM training process, relying on separate acoustic models and quantization methods. Following such an approach may create a mismatch between the tokenization process and its usage afterward. In this study, we propose a novel approach to training a speech tokenizer by leveraging objectives from pre-trained textual LMs. We advocate for the integration of this objective into the process of learning discrete speech representations. Our aim is to transform features from a pre-trained speech model into a new feature space that enables better clustering for speech LMs. We empirically investigate the impact of various model design choices, including speech vocabulary size and text LM size. Our results demonstrate the proposed tokenization method outperforms the evaluated baselines considering both spoken language modeling and speech-to-text. More importantly, unlike prior work, the proposed method allows the utilization of a single pre-trained LM for processing both speech and text inputs, setting it apart from conventional tokenization approaches.",
      "published": "2024-09-05T16:57:39Z"
    },
    "metadata": {
      "arxiv_id": "2409.03701",
      "title": "LAST: Language Model Aware Speech Tokenization",
      "summary": "Speech tokenization serves as the foundation of speech language model (LM), enabling them to perform various tasks such as spoken language modeling, text-to-speech, speech-to-text, etc. Most speech tokenizers are trained independently of the LM training process, relying on separate acoustic models and quantization methods. Following such an approach may create a mismatch between the tokenization process and its usage afterward. In this study, we propose a novel approach to training a speech tokenizer by leveraging objectives from pre-trained textual LMs. We advocate for the integration of this objective into the process of learning discrete speech representations. Our aim is to transform features from a pre-trained speech model into a new feature space that enables better clustering for speech LMs. We empirically investigate the impact of various model design choices, including speech vocabulary size and text LM size. Our results demonstrate the proposed tokenization method outperforms the evaluated baselines considering both spoken language modeling and speech-to-text. More importantly, unlike prior work, the proposed method allows the utilization of a single pre-trained LM for processing both speech and text inputs, setting it apart from conventional tokenization approaches.",
      "authors": [
        "Arnon Turetzky",
        "Yossi Adi"
      ],
      "published": "2024-09-05T16:57:39Z",
      "updated": "2024-09-10T14:45:15Z",
      "categories": [
        "cs.CL",
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2409.03701v2",
      "landing_url": "https://arxiv.org/abs/2409.03701v2",
      "doi": "https://doi.org/10.48550/arXiv.2409.03701"
    },
    "queries": [
      {
        "anchor": "spoken language models",
        "search_term": "true full-duplex"
      },
      {
        "anchor": "spoken language models",
        "search_term": "pseudo full-duplex"
      },
      {
        "anchor": "spoken language models",
        "search_term": "engineered synchronization"
      },
      {
        "anchor": "spoken language models",
        "search_term": "learned synchronization"
      },
      {
        "anchor": "spoken language models",
        "search_term": "modular architecture"
      },
      {
        "anchor": "spoken language models",
        "search_term": "streaming architecture"
      },
      {
        "anchor": "spoken language models",
        "search_term": "real-time interaction"
      },
      {
        "anchor": "spoken language models",
        "search_term": "synchronous data"
      },
      {
        "anchor": "spoken language models",
        "search_term": "multichannel dialogue"
      },
      {
        "anchor": "spoken language models",
        "search_term": "behavioral arbitration"
      },
      {
        "anchor": "spoken language models",
        "search_term": "semantic coherence"
      },
      {
        "anchor": "spoken language models",
        "search_term": "neural codec"
      },
      {
        "anchor": "spoken language models",
        "search_term": "discrete tokens"
      },
      {
        "anchor": "spoken language models",
        "search_term": "audio tokens"
      },
      {
        "anchor": "spoken language models",
        "search_term": "floor transfer offset"
      },
      {
        "anchor": "spoken language models",
        "search_term": "stop latency"
      },
      {
        "anchor": "spoken language models",
        "search_term": "interruption response delay"
      }
    ]
  },
  {
    "arxiv_id": "2409.04016",
    "anchor": "synchronous dialogue",
    "search_term": "neural codec",
    "search_record": {
      "id": "http://arxiv.org/abs/2409.04016v1",
      "title": "Investigating Neural Audio Codecs for Speech Language Model-Based Speech Generation",
      "summary": "Neural audio codec tokens serve as the fundamental building blocks for speech language model (SLM)-based speech generation. However, there is no systematic understanding on how the codec system affects the speech generation performance of the SLM. In this work, we examine codec tokens within SLM framework for speech generation to provide insights for effective codec design. We retrain existing high-performing neural codec models on the same data set and loss functions to compare their performance in a uniform setting. We integrate codec tokens into two SLM systems: masked-based parallel speech generation system and an auto-regressive (AR) plus non-auto-regressive (NAR) model-based system. Our findings indicate that better speech reconstruction in codec systems does not guarantee improved speech generation in SLM. A high-quality codec decoder is crucial for natural speech production in SLM, while speech intelligibility depends more on quantization mechanism.",
      "published": "2024-09-06T04:06:50Z"
    },
    "metadata": {
      "arxiv_id": "2409.04016",
      "title": "Investigating Neural Audio Codecs for Speech Language Model-Based Speech Generation",
      "summary": "Neural audio codec tokens serve as the fundamental building blocks for speech language model (SLM)-based speech generation. However, there is no systematic understanding on how the codec system affects the speech generation performance of the SLM. In this work, we examine codec tokens within SLM framework for speech generation to provide insights for effective codec design. We retrain existing high-performing neural codec models on the same data set and loss functions to compare their performance in a uniform setting. We integrate codec tokens into two SLM systems: masked-based parallel speech generation system and an auto-regressive (AR) plus non-auto-regressive (NAR) model-based system. Our findings indicate that better speech reconstruction in codec systems does not guarantee improved speech generation in SLM. A high-quality codec decoder is crucial for natural speech production in SLM, while speech intelligibility depends more on quantization mechanism.",
      "authors": [
        "Jiaqi Li",
        "Dongmei Wang",
        "Xiaofei Wang",
        "Yao Qian",
        "Long Zhou",
        "Shujie Liu",
        "Midia Yousefi",
        "Canrun Li",
        "Chung-Hsien Tsai",
        "Zhen Xiao",
        "Yanqing Liu",
        "Junkun Chen",
        "Sheng Zhao",
        "Jinyu Li",
        "Zhizheng Wu",
        "Michael Zeng"
      ],
      "published": "2024-09-06T04:06:50Z",
      "updated": "2024-09-06T04:06:50Z",
      "categories": [
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2409.04016v1",
      "landing_url": "https://arxiv.org/abs/2409.04016v1",
      "doi": "https://doi.org/10.48550/arXiv.2409.04016"
    },
    "queries": [
      {
        "anchor": "synchronous dialogue",
        "search_term": "neural codec"
      }
    ]
  },
  {
    "arxiv_id": "2409.04173",
    "anchor": "synchronous dialogue",
    "search_term": "neural codec",
    "search_record": {
      "id": "http://arxiv.org/abs/2409.04173v2",
      "title": "NPU-NTU System for Voice Privacy 2024 Challenge",
      "summary": "Speaker anonymization is an effective privacy protection solution that conceals the speaker's identity while preserving the linguistic content and paralinguistic information of the original speech. To establish a fair benchmark and facilitate comparison of speaker anonymization systems, the VoicePrivacy Challenge (VPC) was held in 2020 and 2022, with a new edition planned for 2024. In this paper, we describe our proposed speaker anonymization system for VPC 2024. Our system employs a disentangled neural codec architecture and a serial disentanglement strategy to gradually disentangle the global speaker identity and time-variant linguistic content and paralinguistic information. We introduce multiple distillation methods to disentangle linguistic content, speaker identity, and emotion. These methods include semantic distillation, supervised speaker distillation, and frame-level emotion distillation. Based on these distillations, we anonymize the original speaker identity using a weighted sum of a set of candidate speaker identities and a randomly generated speaker identity. Our system achieves the best trade-off of privacy protection and emotion preservation in VPC 2024.",
      "published": "2024-09-06T10:32:42Z"
    },
    "metadata": {
      "arxiv_id": "2409.04173",
      "title": "NPU-NTU System for Voice Privacy 2024 Challenge",
      "summary": "Speaker anonymization is an effective privacy protection solution that conceals the speaker's identity while preserving the linguistic content and paralinguistic information of the original speech. To establish a fair benchmark and facilitate comparison of speaker anonymization systems, the VoicePrivacy Challenge (VPC) was held in 2020 and 2022, with a new edition planned for 2024. In this paper, we describe our proposed speaker anonymization system for VPC 2024. Our system employs a disentangled neural codec architecture and a serial disentanglement strategy to gradually disentangle the global speaker identity and time-variant linguistic content and paralinguistic information. We introduce multiple distillation methods to disentangle linguistic content, speaker identity, and emotion. These methods include semantic distillation, supervised speaker distillation, and frame-level emotion distillation. Based on these distillations, we anonymize the original speaker identity using a weighted sum of a set of candidate speaker identities and a randomly generated speaker identity. Our system achieves the best trade-off of privacy protection and emotion preservation in VPC 2024.",
      "authors": [
        "Jixun Yao",
        "Nikita Kuzmin",
        "Qing Wang",
        "Pengcheng Guo",
        "Ziqian Ning",
        "Dake Guo",
        "Kong Aik Lee",
        "Eng-Siong Chng",
        "Lei Xie"
      ],
      "published": "2024-09-06T10:32:42Z",
      "updated": "2025-02-04T08:43:31Z",
      "categories": [
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2409.04173v2",
      "landing_url": "https://arxiv.org/abs/2409.04173v2",
      "doi": "https://doi.org/10.48550/arXiv.2409.04173"
    },
    "queries": [
      {
        "anchor": "synchronous dialogue",
        "search_term": "neural codec"
      }
    ]
  },
  {
    "arxiv_id": "2409.05784",
    "anchor": "synchronous dialogue",
    "search_term": "discrete tokens",
    "search_record": {
      "id": "http://arxiv.org/abs/2409.05784v2",
      "title": "Vector Quantized Diffusion Model Based Speech Bandwidth Extension",
      "summary": "Recent advancements in neural audio codec (NAC) unlock new potential in audio signal processing. Studies have increasingly explored leveraging the latent features of NAC for various speech signal processing tasks. This paper introduces the first approach to speech bandwidth extension (BWE) that utilizes the discrete features obtained from NAC. By restoring high-frequency details within highly compressed discrete tokens, this approach enhances speech intelligibility and naturalness. Based on Vector Quantized Diffusion, the proposed framework combines the strengths of advanced NAC, diffusion models, and Mamba-2 to reconstruct high-frequency speech components. Extensive experiments demonstrate that this method exhibits superior performance across both log-spectral distance and ViSQOL, significantly improving speech quality.",
      "published": "2024-09-09T16:46:54Z"
    },
    "metadata": {
      "arxiv_id": "2409.05784",
      "title": "Vector Quantized Diffusion Model Based Speech Bandwidth Extension",
      "summary": "Recent advancements in neural audio codec (NAC) unlock new potential in audio signal processing. Studies have increasingly explored leveraging the latent features of NAC for various speech signal processing tasks. This paper introduces the first approach to speech bandwidth extension (BWE) that utilizes the discrete features obtained from NAC. By restoring high-frequency details within highly compressed discrete tokens, this approach enhances speech intelligibility and naturalness. Based on Vector Quantized Diffusion, the proposed framework combines the strengths of advanced NAC, diffusion models, and Mamba-2 to reconstruct high-frequency speech components. Extensive experiments demonstrate that this method exhibits superior performance across both log-spectral distance and ViSQOL, significantly improving speech quality.",
      "authors": [
        "Yuan Fang",
        "Jinglin Bai",
        "Jiajie Wang",
        "Xueliang Zhang"
      ],
      "published": "2024-09-09T16:46:54Z",
      "updated": "2024-09-14T14:25:52Z",
      "categories": [
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2409.05784v2",
      "landing_url": "https://arxiv.org/abs/2409.05784v2",
      "doi": "https://doi.org/10.48550/arXiv.2409.05784"
    },
    "queries": [
      {
        "anchor": "synchronous dialogue",
        "search_term": "discrete tokens"
      }
    ]
  },
  {
    "arxiv_id": "2409.07556",
    "anchor": "synchronous dialogue",
    "search_term": "neural codec",
    "search_record": {
      "id": "http://arxiv.org/abs/2409.07556v2",
      "title": "SSR-Speech: Towards Stable, Safe and Robust Zero-shot Text-based Speech Editing and Synthesis",
      "summary": "In this paper, we introduce SSR-Speech, a neural codec autoregressive model designed for stable, safe, and robust zero-shot textbased speech editing and text-to-speech synthesis. SSR-Speech is built on a Transformer decoder and incorporates classifier-free guidance to enhance the stability of the generation process. A watermark Encodec is proposed to embed frame-level watermarks into the edited regions of the speech so that which parts were edited can be detected. In addition, the waveform reconstruction leverages the original unedited speech segments, providing superior recovery compared to the Encodec model. Our approach achieves state-of-the-art performance in the RealEdit speech editing task and the LibriTTS text-to-speech task, surpassing previous methods. Furthermore, SSR-Speech excels in multi-span speech editing and also demonstrates remarkable robustness to background sounds. The source code and demos are released.",
      "published": "2024-09-11T18:24:07Z"
    },
    "metadata": {
      "arxiv_id": "2409.07556",
      "title": "SSR-Speech: Towards Stable, Safe and Robust Zero-shot Text-based Speech Editing and Synthesis",
      "summary": "In this paper, we introduce SSR-Speech, a neural codec autoregressive model designed for stable, safe, and robust zero-shot textbased speech editing and text-to-speech synthesis. SSR-Speech is built on a Transformer decoder and incorporates classifier-free guidance to enhance the stability of the generation process. A watermark Encodec is proposed to embed frame-level watermarks into the edited regions of the speech so that which parts were edited can be detected. In addition, the waveform reconstruction leverages the original unedited speech segments, providing superior recovery compared to the Encodec model. Our approach achieves state-of-the-art performance in the RealEdit speech editing task and the LibriTTS text-to-speech task, surpassing previous methods. Furthermore, SSR-Speech excels in multi-span speech editing and also demonstrates remarkable robustness to background sounds. The source code and demos are released.",
      "authors": [
        "Helin Wang",
        "Meng Yu",
        "Jiarui Hai",
        "Chen Chen",
        "Yuchen Hu",
        "Rilin Chen",
        "Najim Dehak",
        "Dong Yu"
      ],
      "published": "2024-09-11T18:24:07Z",
      "updated": "2025-01-02T03:07:29Z",
      "categories": [
        "eess.AS",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2409.07556v2",
      "landing_url": "https://arxiv.org/abs/2409.07556v2",
      "doi": "https://doi.org/10.48550/arXiv.2409.07556"
    },
    "queries": [
      {
        "anchor": "synchronous dialogue",
        "search_term": "neural codec"
      }
    ]
  },
  {
    "arxiv_id": "2409.07841",
    "anchor": "spoken language models",
    "search_term": "discrete tokens",
    "search_record": {
      "id": "http://arxiv.org/abs/2409.07841v3",
      "title": "TSELM: Target Speaker Extraction using Discrete Tokens and Language Models",
      "summary": "We propose TSELM, a novel target speaker extraction network that leverages discrete tokens and language models. TSELM utilizes multiple discretized layers from WavLM as input tokens and incorporates cross-attention mechanisms to integrate target speaker information. Language models are employed to capture the sequence dependencies, while a scalable HiFi-GAN is used to reconstruct the audio from the tokens. By applying a cross-entropy loss, TSELM models the probability distribution of output tokens, thus converting the complex regression problem of audio generation into a classification task. Experimental results show that TSELM achieves excellent results in speech quality and comparable results in speech intelligibility.",
      "published": "2024-09-12T08:41:07Z"
    },
    "metadata": {
      "arxiv_id": "2409.07841",
      "title": "TSELM: Target Speaker Extraction using Discrete Tokens and Language Models",
      "summary": "We propose TSELM, a novel target speaker extraction network that leverages discrete tokens and language models. TSELM utilizes multiple discretized layers from WavLM as input tokens and incorporates cross-attention mechanisms to integrate target speaker information. Language models are employed to capture the sequence dependencies, while a scalable HiFi-GAN is used to reconstruct the audio from the tokens. By applying a cross-entropy loss, TSELM models the probability distribution of output tokens, thus converting the complex regression problem of audio generation into a classification task. Experimental results show that TSELM achieves excellent results in speech quality and comparable results in speech intelligibility.",
      "authors": [
        "Beilong Tang",
        "Bang Zeng",
        "Ming Li"
      ],
      "published": "2024-09-12T08:41:07Z",
      "updated": "2024-09-17T01:41:32Z",
      "categories": [
        "cs.SD",
        "cs.LG",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2409.07841v3",
      "landing_url": "https://arxiv.org/abs/2409.07841v3",
      "doi": "https://doi.org/10.48550/arXiv.2409.07841"
    },
    "queries": [
      {
        "anchor": "spoken language models",
        "search_term": "discrete tokens"
      },
      {
        "anchor": "synchronous dialogue",
        "search_term": "discrete tokens"
      },
      {
        "anchor": "turn-taking",
        "search_term": "discrete tokens"
      }
    ]
  },
  {
    "arxiv_id": "2409.08797",
    "anchor": "synchronous dialogue",
    "search_term": "discrete tokens",
    "search_record": {
      "id": "http://arxiv.org/abs/2409.08797v2",
      "title": "Exploring SSL Discrete Speech Features for Zipformer-based Contextual ASR",
      "summary": "Self-supervised learning (SSL) based discrete speech representations are highly compact and domain adaptable. In this paper, SSL discrete speech features extracted from WavLM models are used as additional cross-utterance acoustic context features in Zipformer-Transducer ASR systems. The efficacy of replacing Fbank features with discrete token features for modelling either cross-utterance contexts (from preceding and future segments), or current utterance's internal contexts alone, or both at the same time, are demonstrated thoroughly on the Gigaspeech 1000-hr corpus. The best Zipformer-Transducer system using discrete tokens based cross-utterance context features outperforms the baseline using utterance internal context only with statistically significant word error rate (WER) reductions of 0.32% to 0.41% absolute (2.78% to 3.54% relative) on the dev and test data. The lowest published WER of 11.15% and 11.14% were obtained on the dev and test sets. Our work is open-source and publicly available at https://github.com/open-creator/icefall/tree/master/egs/gigaspeech/Context\\_ASR.",
      "published": "2024-09-13T13:01:09Z"
    },
    "metadata": {
      "arxiv_id": "2409.08797",
      "title": "Exploring SSL Discrete Speech Features for Zipformer-based Contextual ASR",
      "summary": "Self-supervised learning (SSL) based discrete speech representations are highly compact and domain adaptable. In this paper, SSL discrete speech features extracted from WavLM models are used as additional cross-utterance acoustic context features in Zipformer-Transducer ASR systems. The efficacy of replacing Fbank features with discrete token features for modelling either cross-utterance contexts (from preceding and future segments), or current utterance's internal contexts alone, or both at the same time, are demonstrated thoroughly on the Gigaspeech 1000-hr corpus. The best Zipformer-Transducer system using discrete tokens based cross-utterance context features outperforms the baseline using utterance internal context only with statistically significant word error rate (WER) reductions of 0.32% to 0.41% absolute (2.78% to 3.54% relative) on the dev and test data. The lowest published WER of 11.15% and 11.14% were obtained on the dev and test sets. Our work is open-source and publicly available at https://github.com/open-creator/icefall/tree/master/egs/gigaspeech/Context\\_ASR.",
      "authors": [
        "Mingyu Cui",
        "Yifan Yang",
        "Jiajun Deng",
        "Jiawen Kang",
        "Shujie Hu",
        "Tianzi Wang",
        "Zhaoqing Li",
        "Shiliang Zhang",
        "Xie Chen",
        "Xunying Liu"
      ],
      "published": "2024-09-13T13:01:09Z",
      "updated": "2025-06-10T03:12:23Z",
      "categories": [
        "cs.CL",
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2409.08797v2",
      "landing_url": "https://arxiv.org/abs/2409.08797v2",
      "doi": "https://doi.org/10.48550/arXiv.2409.08797"
    },
    "queries": [
      {
        "anchor": "synchronous dialogue",
        "search_term": "discrete tokens"
      }
    ]
  },
  {
    "arxiv_id": "2409.08805",
    "anchor": "full-duplex",
    "search_term": "discrete tokens",
    "search_record": {
      "id": "http://arxiv.org/abs/2409.08805v1",
      "title": "Exploring SSL Discrete Tokens for Multilingual ASR",
      "summary": "With the advancement of Self-supervised Learning (SSL) in speech-related tasks, there has been growing interest in utilizing discrete tokens generated by SSL for automatic speech recognition (ASR), as they offer faster processing techniques. However, previous studies primarily focused on multilingual ASR with Fbank features or English ASR with discrete tokens, leaving a gap in adapting discrete tokens for multilingual ASR scenarios. This study presents a comprehensive comparison of discrete tokens generated by various leading SSL models across multiple language domains. We aim to explore the performance and efficiency of speech discrete tokens across multiple language domains for both monolingual and multilingual ASR scenarios. Experimental results demonstrate that discrete tokens achieve comparable results against systems trained on Fbank features in ASR tasks across seven language domains with an average word error rate (WER) reduction of 0.31% and 1.76% absolute (2.80% and 15.70% relative) on dev and test sets respectively, with particularly WER reduction of 6.82% absolute (41.48% relative) on the Polish test set.",
      "published": "2024-09-13T13:13:39Z"
    },
    "metadata": {
      "arxiv_id": "2409.08805",
      "title": "Exploring SSL Discrete Tokens for Multilingual ASR",
      "summary": "With the advancement of Self-supervised Learning (SSL) in speech-related tasks, there has been growing interest in utilizing discrete tokens generated by SSL for automatic speech recognition (ASR), as they offer faster processing techniques. However, previous studies primarily focused on multilingual ASR with Fbank features or English ASR with discrete tokens, leaving a gap in adapting discrete tokens for multilingual ASR scenarios. This study presents a comprehensive comparison of discrete tokens generated by various leading SSL models across multiple language domains. We aim to explore the performance and efficiency of speech discrete tokens across multiple language domains for both monolingual and multilingual ASR scenarios. Experimental results demonstrate that discrete tokens achieve comparable results against systems trained on Fbank features in ASR tasks across seven language domains with an average word error rate (WER) reduction of 0.31% and 1.76% absolute (2.80% and 15.70% relative) on dev and test sets respectively, with particularly WER reduction of 6.82% absolute (41.48% relative) on the Polish test set.",
      "authors": [
        "Mingyu Cui",
        "Daxin Tan",
        "Yifan Yang",
        "Dingdong Wang",
        "Huimeng Wang",
        "Xiao Chen",
        "Xie Chen",
        "Xunying Liu"
      ],
      "published": "2024-09-13T13:13:39Z",
      "updated": "2024-09-13T13:13:39Z",
      "categories": [
        "cs.CL",
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2409.08805v1",
      "landing_url": "https://arxiv.org/abs/2409.08805v1",
      "doi": "https://doi.org/10.48550/arXiv.2409.08805"
    },
    "queries": [
      {
        "anchor": "full-duplex",
        "search_term": "discrete tokens"
      },
      {
        "anchor": "spoken language models",
        "search_term": "discrete tokens"
      },
      {
        "anchor": "synchronous dialogue",
        "search_term": "discrete tokens"
      },
      {
        "anchor": "turn-taking",
        "search_term": "discrete tokens"
      }
    ]
  },
  {
    "arxiv_id": "2409.09329",
    "anchor": "spoken language models",
    "search_term": "streaming architecture",
    "search_record": {
      "id": "http://arxiv.org/abs/2409.09329v1",
      "title": "Reputation-Driven Peer-to-Peer Live Streaming Architecture for Preventing Free-Riding",
      "summary": "We present a peer-to-peer (P2P) live-streaming architecture designed to address challenges such as free-riding, malicious peers, churn, and network instability through the integration of a reputation system. The proposed algorithm incentivizes active peer participation while discouraging opportunistic behaviors, with a reputation mechanism that rewards altruistic peers and penalizes free riders and malicious actors. To manage peer dynamics, the algorithm continuously updates the strategies and adjusts to changing neighbors. It also implements a request-to-join mechanism for flash crowd scenarios, allowing the source node to delegate requests to child nodes, forming an interconnected tree structure that efficiently handles high demand and maintains system stability. The decentralized reputation mechanism promotes long-term sustainability in the P2P live streaming system.",
      "published": "2024-09-14T06:16:15Z"
    },
    "metadata": {
      "arxiv_id": "2409.09329",
      "title": "Reputation-Driven Peer-to-Peer Live Streaming Architecture for Preventing Free-Riding",
      "summary": "We present a peer-to-peer (P2P) live-streaming architecture designed to address challenges such as free-riding, malicious peers, churn, and network instability through the integration of a reputation system. The proposed algorithm incentivizes active peer participation while discouraging opportunistic behaviors, with a reputation mechanism that rewards altruistic peers and penalizes free riders and malicious actors. To manage peer dynamics, the algorithm continuously updates the strategies and adjusts to changing neighbors. It also implements a request-to-join mechanism for flash crowd scenarios, allowing the source node to delegate requests to child nodes, forming an interconnected tree structure that efficiently handles high demand and maintains system stability. The decentralized reputation mechanism promotes long-term sustainability in the P2P live streaming system.",
      "authors": [
        "Rashmi Kushwaha",
        "Rahul Bhattacharyya",
        "Yatindra Nath Singh"
      ],
      "published": "2024-09-14T06:16:15Z",
      "updated": "2024-09-14T06:16:15Z",
      "categories": [
        "cs.NI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2409.09329v1",
      "landing_url": "https://arxiv.org/abs/2409.09329v1",
      "doi": "https://doi.org/10.48550/arXiv.2409.09329"
    },
    "queries": [
      {
        "anchor": "spoken language models",
        "search_term": "streaming architecture"
      },
      {
        "anchor": "synchronous dialogue",
        "search_term": "streaming architecture"
      },
      {
        "anchor": "turn-taking",
        "search_term": "streaming architecture"
      }
    ]
  },
  {
    "arxiv_id": "2409.09408",
    "anchor": "synchronous dialogue",
    "search_term": "data scarcity",
    "search_record": {
      "id": "http://arxiv.org/abs/2409.09408v3",
      "title": "Leveraging Self-Supervised Learning for Speaker Diarization",
      "summary": "End-to-end neural diarization has evolved considerably over the past few years, but data scarcity is still a major obstacle for further improvements. Self-supervised learning methods such as WavLM have shown promising performance on several downstream tasks, but their application on speaker diarization is somehow limited. In this work, we explore using WavLM to alleviate the problem of data scarcity for neural diarization training. We use the same pipeline as Pyannote and improve the local end-to-end neural diarization with WavLM and Conformer. Experiments on far-field AMI, AISHELL-4, and AliMeeting datasets show that our method substantially outperforms the Pyannote baseline and achieves new state-of-the-art results on AMI and AISHELL-4, respectively. In addition, by analyzing the system performance under different data quantity scenarios, we show that WavLM representations are much more robust against data scarcity than filterbank features, enabling less data hungry training strategies. Furthermore, we found that simulated data, usually used to train endto-end diarization models, does not help when using WavLM in our experiments. Additionally, we also evaluate our model on the recent CHiME8 NOTSOFAR-1 task where it achieves better performance than the Pyannote baseline. Our source code is publicly available at https://github.com/BUTSpeechFIT/DiariZen.",
      "published": "2024-09-14T10:49:06Z"
    },
    "metadata": {
      "arxiv_id": "2409.09408",
      "title": "Leveraging Self-Supervised Learning for Speaker Diarization",
      "summary": "End-to-end neural diarization has evolved considerably over the past few years, but data scarcity is still a major obstacle for further improvements. Self-supervised learning methods such as WavLM have shown promising performance on several downstream tasks, but their application on speaker diarization is somehow limited. In this work, we explore using WavLM to alleviate the problem of data scarcity for neural diarization training. We use the same pipeline as Pyannote and improve the local end-to-end neural diarization with WavLM and Conformer. Experiments on far-field AMI, AISHELL-4, and AliMeeting datasets show that our method substantially outperforms the Pyannote baseline and achieves new state-of-the-art results on AMI and AISHELL-4, respectively. In addition, by analyzing the system performance under different data quantity scenarios, we show that WavLM representations are much more robust against data scarcity than filterbank features, enabling less data hungry training strategies. Furthermore, we found that simulated data, usually used to train endto-end diarization models, does not help when using WavLM in our experiments. Additionally, we also evaluate our model on the recent CHiME8 NOTSOFAR-1 task where it achieves better performance than the Pyannote baseline. Our source code is publicly available at https://github.com/BUTSpeechFIT/DiariZen.",
      "authors": [
        "Jiangyu Han",
        "Federico Landini",
        "Johan Rohdin",
        "Anna Silnova",
        "Mireia Diez",
        "Lukas Burget"
      ],
      "published": "2024-09-14T10:49:06Z",
      "updated": "2024-10-21T07:46:22Z",
      "categories": [
        "eess.AS",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2409.09408v3",
      "landing_url": "https://arxiv.org/abs/2409.09408v3",
      "doi": "https://doi.org/10.48550/arXiv.2409.09408"
    },
    "queries": [
      {
        "anchor": "synchronous dialogue",
        "search_term": "data scarcity"
      }
    ]
  },
  {
    "arxiv_id": "2409.10058",
    "anchor": "synchronous dialogue",
    "search_term": "neural codec",
    "search_record": {
      "id": "http://arxiv.org/abs/2409.10058v1",
      "title": "StyleTTS-ZS: Efficient High-Quality Zero-Shot Text-to-Speech Synthesis with Distilled Time-Varying Style Diffusion",
      "summary": "The rapid development of large-scale text-to-speech (TTS) models has led to significant advancements in modeling diverse speaker prosody and voices. However, these models often face issues such as slow inference speeds, reliance on complex pre-trained neural codec representations, and difficulties in achieving naturalness and high similarity to reference speakers. To address these challenges, this work introduces StyleTTS-ZS, an efficient zero-shot TTS model that leverages distilled time-varying style diffusion to capture diverse speaker identities and prosodies. We propose a novel approach that represents human speech using input text and fixed-length time-varying discrete style codes to capture diverse prosodic variations, trained adversarially with multi-modal discriminators. A diffusion model is then built to sample this time-varying style code for efficient latent diffusion. Using classifier-free guidance, StyleTTS-ZS achieves high similarity to the reference speaker in the style diffusion process. Furthermore, to expedite sampling, the style diffusion model is distilled with perceptual loss using only 10k samples, maintaining speech quality and similarity while reducing inference speed by 90%. Our model surpasses previous state-of-the-art large-scale zero-shot TTS models in both naturalness and similarity, offering a 10-20 faster sampling speed, making it an attractive alternative for efficient large-scale zero-shot TTS systems. The audio demo, code and models are available at https://styletts-zs.github.io/.",
      "published": "2024-09-16T07:39:58Z"
    },
    "metadata": {
      "arxiv_id": "2409.10058",
      "title": "StyleTTS-ZS: Efficient High-Quality Zero-Shot Text-to-Speech Synthesis with Distilled Time-Varying Style Diffusion",
      "summary": "The rapid development of large-scale text-to-speech (TTS) models has led to significant advancements in modeling diverse speaker prosody and voices. However, these models often face issues such as slow inference speeds, reliance on complex pre-trained neural codec representations, and difficulties in achieving naturalness and high similarity to reference speakers. To address these challenges, this work introduces StyleTTS-ZS, an efficient zero-shot TTS model that leverages distilled time-varying style diffusion to capture diverse speaker identities and prosodies. We propose a novel approach that represents human speech using input text and fixed-length time-varying discrete style codes to capture diverse prosodic variations, trained adversarially with multi-modal discriminators. A diffusion model is then built to sample this time-varying style code for efficient latent diffusion. Using classifier-free guidance, StyleTTS-ZS achieves high similarity to the reference speaker in the style diffusion process. Furthermore, to expedite sampling, the style diffusion model is distilled with perceptual loss using only 10k samples, maintaining speech quality and similarity while reducing inference speed by 90%. Our model surpasses previous state-of-the-art large-scale zero-shot TTS models in both naturalness and similarity, offering a 10-20 faster sampling speed, making it an attractive alternative for efficient large-scale zero-shot TTS systems. The audio demo, code and models are available at https://styletts-zs.github.io/.",
      "authors": [
        "Yinghao Aaron Li",
        "Xilin Jiang",
        "Cong Han",
        "Nima Mesgarani"
      ],
      "published": "2024-09-16T07:39:58Z",
      "updated": "2024-09-16T07:39:58Z",
      "categories": [
        "eess.AS",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2409.10058v1",
      "landing_url": "https://arxiv.org/abs/2409.10058v1",
      "doi": "https://doi.org/10.48550/arXiv.2409.10058"
    },
    "queries": [
      {
        "anchor": "synchronous dialogue",
        "search_term": "neural codec"
      }
    ]
  },
  {
    "arxiv_id": "2409.10358",
    "anchor": "spoken language models",
    "search_term": "low latency",
    "search_record": {
      "id": "http://arxiv.org/abs/2409.10358v1",
      "title": "Ultra-Low Latency Speech Enhancement - A Comprehensive Study",
      "summary": "Speech enhancement models should meet very low latency requirements typically smaller than 5 ms for hearing assistive devices. While various low-latency techniques have been proposed, comparing these methods in a controlled setup using DNNs remains blank. Previous papers have variations in task, training data, scripts, and evaluation settings, which make fair comparison impossible. Moreover, all methods are tested on small, simulated datasets, making it difficult to fairly assess their performance in real-world conditions, which could impact the reliability of scientific findings. To address these issues, we comprehensively investigate various low-latency techniques using consistent training on large-scale data and evaluate with more relevant metrics on real-world data. Specifically, we explore the effectiveness of asymmetric windows, learnable windows, adaptive time domain filterbanks, and the future-frame prediction technique. Additionally, we examine whether increasing the model size can compensate for the reduced window size, as well as the novel Mamba architecture in low-latency environments.",
      "published": "2024-09-16T15:06:47Z"
    },
    "metadata": {
      "arxiv_id": "2409.10358",
      "title": "Ultra-Low Latency Speech Enhancement - A Comprehensive Study",
      "summary": "Speech enhancement models should meet very low latency requirements typically smaller than 5 ms for hearing assistive devices. While various low-latency techniques have been proposed, comparing these methods in a controlled setup using DNNs remains blank. Previous papers have variations in task, training data, scripts, and evaluation settings, which make fair comparison impossible. Moreover, all methods are tested on small, simulated datasets, making it difficult to fairly assess their performance in real-world conditions, which could impact the reliability of scientific findings. To address these issues, we comprehensively investigate various low-latency techniques using consistent training on large-scale data and evaluate with more relevant metrics on real-world data. Specifically, we explore the effectiveness of asymmetric windows, learnable windows, adaptive time domain filterbanks, and the future-frame prediction technique. Additionally, we examine whether increasing the model size can compensate for the reduced window size, as well as the novel Mamba architecture in low-latency environments.",
      "authors": [
        "Haibin Wu",
        "Sebastian Braun"
      ],
      "published": "2024-09-16T15:06:47Z",
      "updated": "2024-09-16T15:06:47Z",
      "categories": [
        "eess.AS",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2409.10358v1",
      "landing_url": "https://arxiv.org/abs/2409.10358v1",
      "doi": "https://doi.org/10.48550/arXiv.2409.10358"
    },
    "queries": [
      {
        "anchor": "spoken language models",
        "search_term": "low latency"
      },
      {
        "anchor": "synchronous dialogue",
        "search_term": "low latency"
      },
      {
        "anchor": "turn-taking",
        "search_term": "low latency"
      }
    ]
  },
  {
    "arxiv_id": "2409.11003",
    "anchor": "spoken language models",
    "search_term": "audio tokens",
    "search_record": {
      "id": "http://arxiv.org/abs/2409.11003v1",
      "title": "Single-stage TTS with Masked Audio Token Modeling and Semantic Knowledge Distillation",
      "summary": "Audio token modeling has become a powerful framework for speech synthesis, with two-stage approaches employing semantic tokens remaining prevalent. In this paper, we aim to simplify this process by introducing a semantic knowledge distillation method that enables high-quality speech generation in a single stage. Our proposed model improves speech quality, intelligibility, and speaker similarity compared to a single-stage baseline. Although two-stage systems still lead in intelligibility, our model significantly narrows the gap while delivering comparable speech quality. These findings showcase the potential of single-stage models to achieve efficient, high-quality TTS with a more compact and streamlined architecture.",
      "published": "2024-09-17T09:08:43Z"
    },
    "metadata": {
      "arxiv_id": "2409.11003",
      "title": "Single-stage TTS with Masked Audio Token Modeling and Semantic Knowledge Distillation",
      "summary": "Audio token modeling has become a powerful framework for speech synthesis, with two-stage approaches employing semantic tokens remaining prevalent. In this paper, we aim to simplify this process by introducing a semantic knowledge distillation method that enables high-quality speech generation in a single stage. Our proposed model improves speech quality, intelligibility, and speaker similarity compared to a single-stage baseline. Although two-stage systems still lead in intelligibility, our model significantly narrows the gap while delivering comparable speech quality. These findings showcase the potential of single-stage models to achieve efficient, high-quality TTS with a more compact and streamlined architecture.",
      "authors": [
        "Gerard I. Gállego",
        "Roy Fejgin",
        "Chunghsin Yeh",
        "Xiaoyu Liu",
        "Gautam Bhattacharya"
      ],
      "published": "2024-09-17T09:08:43Z",
      "updated": "2024-09-17T09:08:43Z",
      "categories": [
        "cs.SD",
        "cs.AI",
        "eess.AS",
        "eess.SP"
      ],
      "pdf_url": "https://arxiv.org/pdf/2409.11003v1",
      "landing_url": "https://arxiv.org/abs/2409.11003v1",
      "doi": "https://doi.org/10.48550/arXiv.2409.11003"
    },
    "queries": [
      {
        "anchor": "spoken language models",
        "search_term": "audio tokens"
      },
      {
        "anchor": "synchronous dialogue",
        "search_term": "audio tokens"
      },
      {
        "anchor": "turn-taking",
        "search_term": "audio tokens"
      }
    ]
  },
  {
    "arxiv_id": "2409.11228",
    "anchor": "synchronous dialogue",
    "search_term": "neural codec",
    "search_record": {
      "id": "http://arxiv.org/abs/2409.11228v2",
      "title": "Learning Source Disentanglement in Neural Audio Codec",
      "summary": "Neural audio codecs have significantly advanced audio compression by efficiently converting continuous audio signals into discrete tokens. These codecs preserve high-quality sound and enable sophisticated sound generation through generative models trained on these tokens. However, existing neural codec models are typically trained on large, undifferentiated audio datasets, neglecting the essential discrepancies between sound domains like speech, music, and environmental sound effects. This oversight complicates data modeling and poses additional challenges to the controllability of sound generation. To tackle these issues, we introduce the Source-Disentangled Neural Audio Codec (SD-Codec), a novel approach that combines audio coding and source separation. By jointly learning audio resynthesis and separation, SD-Codec explicitly assigns audio signals from different domains to distinct codebooks, sets of discrete representations. Experimental results indicate that SD-Codec not only maintains competitive resynthesis quality but also, supported by the separation results, demonstrates successful disentanglement of different sources in the latent space, thereby enhancing interpretability in audio codec and providing potential finer control over the audio generation process.",
      "published": "2024-09-17T14:21:02Z"
    },
    "metadata": {
      "arxiv_id": "2409.11228",
      "title": "Learning Source Disentanglement in Neural Audio Codec",
      "summary": "Neural audio codecs have significantly advanced audio compression by efficiently converting continuous audio signals into discrete tokens. These codecs preserve high-quality sound and enable sophisticated sound generation through generative models trained on these tokens. However, existing neural codec models are typically trained on large, undifferentiated audio datasets, neglecting the essential discrepancies between sound domains like speech, music, and environmental sound effects. This oversight complicates data modeling and poses additional challenges to the controllability of sound generation. To tackle these issues, we introduce the Source-Disentangled Neural Audio Codec (SD-Codec), a novel approach that combines audio coding and source separation. By jointly learning audio resynthesis and separation, SD-Codec explicitly assigns audio signals from different domains to distinct codebooks, sets of discrete representations. Experimental results indicate that SD-Codec not only maintains competitive resynthesis quality but also, supported by the separation results, demonstrates successful disentanglement of different sources in the latent space, thereby enhancing interpretability in audio codec and providing potential finer control over the audio generation process.",
      "authors": [
        "Xiaoyu Bie",
        "Xubo Liu",
        "Gaël Richard"
      ],
      "published": "2024-09-17T14:21:02Z",
      "updated": "2025-02-11T10:35:04Z",
      "categories": [
        "cs.SD",
        "cs.AI",
        "cs.LG",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2409.11228v2",
      "landing_url": "https://arxiv.org/abs/2409.11228v2",
      "doi": "https://doi.org/10.48550/arXiv.2409.11228"
    },
    "queries": [
      {
        "anchor": "synchronous dialogue",
        "search_term": "neural codec"
      }
    ]
  },
  {
    "arxiv_id": "2409.11630",
    "anchor": "full-duplex",
    "search_term": "neural codec",
    "search_record": {
      "id": "http://arxiv.org/abs/2409.11630v1",
      "title": "Speaking from Coarse to Fine: Improving Neural Codec Language Model via Multi-Scale Speech Coding and Generation",
      "summary": "The neural codec language model (CLM) has demonstrated remarkable performance in text-to-speech (TTS) synthesis. However, troubled by ``recency bias\", CLM lacks sufficient attention to coarse-grained information at a higher temporal scale, often producing unnatural or even unintelligible speech. This work proposes CoFi-Speech, a coarse-to-fine CLM-TTS approach, employing multi-scale speech coding and generation to address this issue. We train a multi-scale neural codec, CoFi-Codec, to encode speech into a multi-scale discrete representation, comprising multiple token sequences with different time resolutions. Then, we propose CoFi-LM that can generate this representation in two modes: the single-LM-based chain-of-scale generation and the multiple-LM-based stack-of-scale generation. In experiments, CoFi-Speech significantly outperforms single-scale baseline systems on naturalness and speaker similarity in zero-shot TTS. The analysis of multi-scale coding demonstrates the effectiveness of CoFi-Codec in learning multi-scale discrete speech representations while keeping high-quality speech reconstruction. The coarse-to-fine multi-scale generation, especially for the stack-of-scale approach, is also validated as a crucial approach in pursuing a high-quality neural codec language model for TTS.",
      "published": "2024-09-18T01:31:19Z"
    },
    "metadata": {
      "arxiv_id": "2409.11630",
      "title": "Speaking from Coarse to Fine: Improving Neural Codec Language Model via Multi-Scale Speech Coding and Generation",
      "summary": "The neural codec language model (CLM) has demonstrated remarkable performance in text-to-speech (TTS) synthesis. However, troubled by ``recency bias\", CLM lacks sufficient attention to coarse-grained information at a higher temporal scale, often producing unnatural or even unintelligible speech. This work proposes CoFi-Speech, a coarse-to-fine CLM-TTS approach, employing multi-scale speech coding and generation to address this issue. We train a multi-scale neural codec, CoFi-Codec, to encode speech into a multi-scale discrete representation, comprising multiple token sequences with different time resolutions. Then, we propose CoFi-LM that can generate this representation in two modes: the single-LM-based chain-of-scale generation and the multiple-LM-based stack-of-scale generation. In experiments, CoFi-Speech significantly outperforms single-scale baseline systems on naturalness and speaker similarity in zero-shot TTS. The analysis of multi-scale coding demonstrates the effectiveness of CoFi-Codec in learning multi-scale discrete speech representations while keeping high-quality speech reconstruction. The coarse-to-fine multi-scale generation, especially for the stack-of-scale approach, is also validated as a crucial approach in pursuing a high-quality neural codec language model for TTS.",
      "authors": [
        "Haohan Guo",
        "Fenglong Xie",
        "Dongchao Yang",
        "Xixin Wu",
        "Helen Meng"
      ],
      "published": "2024-09-18T01:31:19Z",
      "updated": "2024-09-18T01:31:19Z",
      "categories": [
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2409.11630v1",
      "landing_url": "https://arxiv.org/abs/2409.11630v1",
      "doi": "https://doi.org/10.48550/arXiv.2409.11630"
    },
    "queries": [
      {
        "anchor": "full-duplex",
        "search_term": "neural codec"
      },
      {
        "anchor": "spoken language models",
        "search_term": "neural codec"
      },
      {
        "anchor": "synchronous dialogue",
        "search_term": "neural codec"
      },
      {
        "anchor": "turn-taking",
        "search_term": "neural codec"
      }
    ]
  },
  {
    "arxiv_id": "2409.11727",
    "anchor": "synchronous dialogue",
    "search_term": "real-time interaction",
    "search_record": {
      "id": "http://arxiv.org/abs/2409.11727v1",
      "title": "Enabling Real-Time Conversations with Minimal Training Costs",
      "summary": "Large language models (LLMs) have demonstrated the ability to improve human efficiency through conversational interactions. Conventional LLM-powered dialogue systems, operating on a turn-based paradigm, preclude real-time interaction during response generation. To address this limitation, researchers have proposed duplex models. These models can dynamically adapt to user input, facilitating real-time interactive feedback. However, these methods typically require substantial computational resources to acquire the ability. To reduce overhead, this paper presents a new duplex decoding approach that enhances LLMs with duplex ability, requiring minimal additional training. Specifically, our method employs parallel decoding of queries and responses in conversations, effectively implementing a channel-division-multiplexing decoding strategy. Experimental results indicate that our proposed method significantly enhances the naturalness and human-likeness of user-AI interactions with minimal training costs.",
      "published": "2024-09-18T06:27:26Z"
    },
    "metadata": {
      "arxiv_id": "2409.11727",
      "title": "Enabling Real-Time Conversations with Minimal Training Costs",
      "summary": "Large language models (LLMs) have demonstrated the ability to improve human efficiency through conversational interactions. Conventional LLM-powered dialogue systems, operating on a turn-based paradigm, preclude real-time interaction during response generation. To address this limitation, researchers have proposed duplex models. These models can dynamically adapt to user input, facilitating real-time interactive feedback. However, these methods typically require substantial computational resources to acquire the ability. To reduce overhead, this paper presents a new duplex decoding approach that enhances LLMs with duplex ability, requiring minimal additional training. Specifically, our method employs parallel decoding of queries and responses in conversations, effectively implementing a channel-division-multiplexing decoding strategy. Experimental results indicate that our proposed method significantly enhances the naturalness and human-likeness of user-AI interactions with minimal training costs.",
      "authors": [
        "Wang Xu",
        "Shuo Wang",
        "Weilin Zhao",
        "Xu Han",
        "Yukun Yan",
        "Yudi Zhang",
        "Zhe Tao",
        "Zhiyuan Liu",
        "Wanxiang Che"
      ],
      "published": "2024-09-18T06:27:26Z",
      "updated": "2024-09-18T06:27:26Z",
      "categories": [
        "cs.CL"
      ],
      "pdf_url": "https://arxiv.org/pdf/2409.11727v1",
      "landing_url": "https://arxiv.org/abs/2409.11727v1",
      "doi": "https://doi.org/10.48550/arXiv.2409.11727"
    },
    "queries": [
      {
        "anchor": "synchronous dialogue",
        "search_term": "real-time interaction"
      }
    ]
  },
  {
    "arxiv_id": "2409.12117",
    "anchor": "synchronous dialogue",
    "search_term": "discrete tokens",
    "search_record": {
      "id": "http://arxiv.org/abs/2409.12117v1",
      "title": "Low Frame-rate Speech Codec: a Codec Designed for Fast High-quality Speech LLM Training and Inference",
      "summary": "Large language models (LLMs) have significantly advanced audio processing through audio codecs that convert audio into discrete tokens, enabling the application of language modeling techniques to audio data. However, audio codecs often operate at high frame rates, resulting in slow training and inference, especially for autoregressive models. To address this challenge, we present the Low Frame-rate Speech Codec (LFSC): a neural audio codec that leverages finite scalar quantization and adversarial training with large speech language models to achieve high-quality audio compression with a 1.89 kbps bitrate and 21.5 frames per second. We demonstrate that our novel codec can make the inference of LLM-based text-to-speech models around three times faster while improving intelligibility and producing quality comparable to previous models.",
      "published": "2024-09-18T16:39:10Z"
    },
    "metadata": {
      "arxiv_id": "2409.12117",
      "title": "Low Frame-rate Speech Codec: a Codec Designed for Fast High-quality Speech LLM Training and Inference",
      "summary": "Large language models (LLMs) have significantly advanced audio processing through audio codecs that convert audio into discrete tokens, enabling the application of language modeling techniques to audio data. However, audio codecs often operate at high frame rates, resulting in slow training and inference, especially for autoregressive models. To address this challenge, we present the Low Frame-rate Speech Codec (LFSC): a neural audio codec that leverages finite scalar quantization and adversarial training with large speech language models to achieve high-quality audio compression with a 1.89 kbps bitrate and 21.5 frames per second. We demonstrate that our novel codec can make the inference of LLM-based text-to-speech models around three times faster while improving intelligibility and producing quality comparable to previous models.",
      "authors": [
        "Edresson Casanova",
        "Ryan Langman",
        "Paarth Neekhara",
        "Shehzeen Hussain",
        "Jason Li",
        "Subhankar Ghosh",
        "Ante Jukić",
        "Sang-gil Lee"
      ],
      "published": "2024-09-18T16:39:10Z",
      "updated": "2024-09-18T16:39:10Z",
      "categories": [
        "eess.AS",
        "cs.CL",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2409.12117v1",
      "landing_url": "https://arxiv.org/abs/2409.12117v1",
      "doi": "https://doi.org/10.48550/arXiv.2409.12117"
    },
    "queries": [
      {
        "anchor": "synchronous dialogue",
        "search_term": "discrete tokens"
      }
    ]
  },
  {
    "arxiv_id": "2409.12139",
    "anchor": "synchronous dialogue",
    "search_term": "neural codec",
    "search_record": {
      "id": "http://arxiv.org/abs/2409.12139v3",
      "title": "Takin: A Cohort of Superior Quality Zero-shot Speech Generation Models",
      "summary": "With the advent of the big data and large language model era, zero-shot personalized rapid customization has emerged as a significant trend. In this report, we introduce Takin AudioLLM, a series of techniques and models, mainly including Takin TTS, Takin VC, and Takin Morphing, specifically designed for audiobook production. These models are capable of zero-shot speech production, generating high-quality speech that is nearly indistinguishable from real human speech and facilitating individuals to customize the speech content according to their own needs. Specifically, we first introduce Takin TTS, a neural codec language model that builds upon an enhanced neural speech codec and a multi-task training framework, capable of generating high-fidelity natural speech in a zero-shot way. For Takin VC, we advocate an effective content and timbre joint modeling approach to improve the speaker similarity, while advocating for a conditional flow matching based decoder to further enhance its naturalness and expressiveness. Last, we propose the Takin Morphing system with highly decoupled and advanced timbre and prosody modeling approaches, which enables individuals to customize speech production with their preferred timbre and prosody in a precise and controllable manner. Extensive experiments validate the effectiveness and robustness of our Takin AudioLLM series models. For detailed demos, please refer to https://everest-ai.github.io/takinaudiollm/.",
      "published": "2024-09-18T17:03:12Z"
    },
    "metadata": {
      "arxiv_id": "2409.12139",
      "title": "Takin: A Cohort of Superior Quality Zero-shot Speech Generation Models",
      "summary": "With the advent of the big data and large language model era, zero-shot personalized rapid customization has emerged as a significant trend. In this report, we introduce Takin AudioLLM, a series of techniques and models, mainly including Takin TTS, Takin VC, and Takin Morphing, specifically designed for audiobook production. These models are capable of zero-shot speech production, generating high-quality speech that is nearly indistinguishable from real human speech and facilitating individuals to customize the speech content according to their own needs. Specifically, we first introduce Takin TTS, a neural codec language model that builds upon an enhanced neural speech codec and a multi-task training framework, capable of generating high-fidelity natural speech in a zero-shot way. For Takin VC, we advocate an effective content and timbre joint modeling approach to improve the speaker similarity, while advocating for a conditional flow matching based decoder to further enhance its naturalness and expressiveness. Last, we propose the Takin Morphing system with highly decoupled and advanced timbre and prosody modeling approaches, which enables individuals to customize speech production with their preferred timbre and prosody in a precise and controllable manner. Extensive experiments validate the effectiveness and robustness of our Takin AudioLLM series models. For detailed demos, please refer to https://everest-ai.github.io/takinaudiollm/.",
      "authors": [
        "Sijing Chen",
        "Yuan Feng",
        "Laipeng He",
        "Tianwei He",
        "Wendi He",
        "Yanni Hu",
        "Bin Lin",
        "Yiting Lin",
        "Yu Pan",
        "Pengfei Tan",
        "Chengwei Tian",
        "Chen Wang",
        "Zhicheng Wang",
        "Ruoye Xie",
        "Jixun Yao",
        "Quanlei Yan",
        "Yuguang Yang",
        "Jianhao Ye",
        "Jingjing Yin",
        "Yanzhen Yu",
        "Huimin Zhang",
        "Xiang Zhang",
        "Guangcheng Zhao",
        "Hongbin Zhou",
        "Pengpeng Zou"
      ],
      "published": "2024-09-18T17:03:12Z",
      "updated": "2024-09-24T02:00:54Z",
      "categories": [
        "cs.SD",
        "cs.AI",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2409.12139v3",
      "landing_url": "https://arxiv.org/abs/2409.12139v3",
      "doi": "https://doi.org/10.48550/arXiv.2409.12139"
    },
    "queries": [
      {
        "anchor": "synchronous dialogue",
        "search_term": "neural codec"
      }
    ]
  },
  {
    "arxiv_id": "2409.12319",
    "anchor": "synchronous dialogue",
    "search_term": "audio tokens",
    "search_record": {
      "id": "http://arxiv.org/abs/2409.12319v2",
      "title": "Large Language Models are Strong Audio-Visual Speech Recognition Learners",
      "summary": "Multimodal large language models (MLLMs) have recently become a focal point of research due to their formidable multimodal understanding capabilities. For example, in the audio and speech domains, an LLM can be equipped with (automatic) speech recognition (ASR) abilities by just concatenating the audio tokens, computed with an audio encoder, and the text tokens to achieve state-of-the-art results. On the contrary, tasks like visual and audio-visual speech recognition (VSR/AVSR), which also exploit noise-invariant lip movement information, have received little or no attention. To bridge this gap, we propose Llama-AVSR, a new MLLM with strong audio-visual speech recognition capabilities. It leverages pre-trained audio and video encoders to produce modality-specific tokens which, together with the text tokens, are processed by a pre-trained LLM (e.g., Llama3.1-8B) to yield the resulting response in an auto-regressive fashion. Llama-AVSR requires a small number of trainable parameters as only modality-specific projectors and LoRA modules are trained whereas the multi-modal encoders and LLM are kept frozen. We evaluate our proposed approach on LRS3, the largest public AVSR benchmark, and we achieve new state-of-the-art results for the tasks of ASR and AVSR with a WER of 0.79% and 0.77%, respectively. To bolster our results, we investigate the key factors that underpin the effectiveness of Llama-AVSR: the choice of the pre-trained encoders and LLM, the efficient integration of LoRA modules, and the optimal performance-efficiency trade-off obtained via modality-aware compression rates.",
      "published": "2024-09-18T21:17:27Z"
    },
    "metadata": {
      "arxiv_id": "2409.12319",
      "title": "Large Language Models are Strong Audio-Visual Speech Recognition Learners",
      "summary": "Multimodal large language models (MLLMs) have recently become a focal point of research due to their formidable multimodal understanding capabilities. For example, in the audio and speech domains, an LLM can be equipped with (automatic) speech recognition (ASR) abilities by just concatenating the audio tokens, computed with an audio encoder, and the text tokens to achieve state-of-the-art results. On the contrary, tasks like visual and audio-visual speech recognition (VSR/AVSR), which also exploit noise-invariant lip movement information, have received little or no attention. To bridge this gap, we propose Llama-AVSR, a new MLLM with strong audio-visual speech recognition capabilities. It leverages pre-trained audio and video encoders to produce modality-specific tokens which, together with the text tokens, are processed by a pre-trained LLM (e.g., Llama3.1-8B) to yield the resulting response in an auto-regressive fashion. Llama-AVSR requires a small number of trainable parameters as only modality-specific projectors and LoRA modules are trained whereas the multi-modal encoders and LLM are kept frozen. We evaluate our proposed approach on LRS3, the largest public AVSR benchmark, and we achieve new state-of-the-art results for the tasks of ASR and AVSR with a WER of 0.79% and 0.77%, respectively. To bolster our results, we investigate the key factors that underpin the effectiveness of Llama-AVSR: the choice of the pre-trained encoders and LLM, the efficient integration of LoRA modules, and the optimal performance-efficiency trade-off obtained via modality-aware compression rates.",
      "authors": [
        "Umberto Cappellazzo",
        "Minsu Kim",
        "Honglie Chen",
        "Pingchuan Ma",
        "Stavros Petridis",
        "Daniele Falavigna",
        "Alessio Brutti",
        "Maja Pantic"
      ],
      "published": "2024-09-18T21:17:27Z",
      "updated": "2025-03-07T09:30:16Z",
      "categories": [
        "cs.CV",
        "cs.MM",
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2409.12319v2",
      "landing_url": "https://arxiv.org/abs/2409.12319v2",
      "doi": "https://doi.org/10.48550/arXiv.2409.12319"
    },
    "queries": [
      {
        "anchor": "synchronous dialogue",
        "search_term": "audio tokens"
      }
    ]
  },
  {
    "arxiv_id": "2409.14085",
    "anchor": "synchronous dialogue",
    "search_term": "neural codec",
    "search_record": {
      "id": "http://arxiv.org/abs/2409.14085v1",
      "title": "Codec-SUPERB @ SLT 2024: A lightweight benchmark for neural audio codec models",
      "summary": "Neural audio codec models are becoming increasingly important as they serve as tokenizers for audio, enabling efficient transmission or facilitating speech language modeling. The ideal neural audio codec should maintain content, paralinguistics, speaker characteristics, and audio information even at low bitrates. Recently, numerous advanced neural codec models have been proposed. However, codec models are often tested under varying experimental conditions. As a result, we introduce the Codec-SUPERB challenge at SLT 2024, designed to facilitate fair and lightweight comparisons among existing codec models and inspire advancements in the field. This challenge brings together representative speech applications and objective metrics, and carefully selects license-free datasets, sampling them into small sets to reduce evaluation computation costs. This paper presents the challenge's rules, datasets, five participant systems, results, and findings.",
      "published": "2024-09-21T09:39:36Z"
    },
    "metadata": {
      "arxiv_id": "2409.14085",
      "title": "Codec-SUPERB @ SLT 2024: A lightweight benchmark for neural audio codec models",
      "summary": "Neural audio codec models are becoming increasingly important as they serve as tokenizers for audio, enabling efficient transmission or facilitating speech language modeling. The ideal neural audio codec should maintain content, paralinguistics, speaker characteristics, and audio information even at low bitrates. Recently, numerous advanced neural codec models have been proposed. However, codec models are often tested under varying experimental conditions. As a result, we introduce the Codec-SUPERB challenge at SLT 2024, designed to facilitate fair and lightweight comparisons among existing codec models and inspire advancements in the field. This challenge brings together representative speech applications and objective metrics, and carefully selects license-free datasets, sampling them into small sets to reduce evaluation computation costs. This paper presents the challenge's rules, datasets, five participant systems, results, and findings.",
      "authors": [
        "Haibin Wu",
        "Xuanjun Chen",
        "Yi-Cheng Lin",
        "Kaiwei Chang",
        "Jiawei Du",
        "Ke-Han Lu",
        "Alexander H. Liu",
        "Ho-Lam Chung",
        "Yuan-Kuei Wu",
        "Dongchao Yang",
        "Songxiang Liu",
        "Yi-Chiao Wu",
        "Xu Tan",
        "James Glass",
        "Shinji Watanabe",
        "Hung-yi Lee"
      ],
      "published": "2024-09-21T09:39:36Z",
      "updated": "2024-09-21T09:39:36Z",
      "categories": [
        "eess.AS",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2409.14085v1",
      "landing_url": "https://arxiv.org/abs/2409.14085v1",
      "doi": "https://doi.org/10.48550/arXiv.2409.14085"
    },
    "queries": [
      {
        "anchor": "synchronous dialogue",
        "search_term": "neural codec"
      }
    ]
  },
  {
    "arxiv_id": "2409.14866",
    "anchor": "synchronous dialogue",
    "search_term": "semantic coherence",
    "search_record": {
      "id": "http://arxiv.org/abs/2409.14866v5",
      "title": "PAPILLON: Efficient and Stealthy Fuzz Testing-Powered Jailbreaks for LLMs",
      "summary": "Large Language Models (LLMs) have excelled in various tasks but are still vulnerable to jailbreaking attacks, where attackers create jailbreak prompts to mislead the model to produce harmful or offensive content. Current jailbreak methods either rely heavily on manually crafted templates, which pose challenges in scalability and adaptability, or struggle to generate semantically coherent prompts, making them easy to detect. Additionally, most existing approaches involve lengthy prompts, leading to higher query costs. In this paper, to remedy these challenges, we introduce a novel jailbreaking attack framework called PAPILLON, which is an automated, black-box jailbreaking attack framework that adapts the black-box fuzz testing approach with a series of customized designs. Instead of relying on manually crafted templates,PAPILLON starts with an empty seed pool, removing the need to search for any related jailbreaking templates. We also develop three novel question-dependent mutation strategies using an LLM helper to generate prompts that maintain semantic coherence while significantly reducing their length. Additionally, we implement a two-level judge module to accurately detect genuine successful jailbreaks. We evaluated PAPILLON on 7 representative LLMs and compared it with 5 state-of-the-art jailbreaking attack strategies. For proprietary LLM APIs, such as GPT-3.5 turbo, GPT-4, and Gemini-Pro, PAPILLONs achieves attack success rates of over 90%, 80%, and 74%, respectively, exceeding existing baselines by more than 60\\%. Additionally, PAPILLON can maintain high semantic coherence while significantly reducing the length of jailbreak prompts. When targeting GPT-4, PAPILLON can achieve over 78% attack success rate even with 100 tokens. Moreover, PAPILLON demonstrates transferability and is robust to state-of-the-art defenses. Code: https://github.com/aaFrostnova/Papillon",
      "published": "2024-09-23T10:03:09Z"
    },
    "metadata": {
      "arxiv_id": "2409.14866",
      "title": "PAPILLON: Efficient and Stealthy Fuzz Testing-Powered Jailbreaks for LLMs",
      "summary": "Large Language Models (LLMs) have excelled in various tasks but are still vulnerable to jailbreaking attacks, where attackers create jailbreak prompts to mislead the model to produce harmful or offensive content. Current jailbreak methods either rely heavily on manually crafted templates, which pose challenges in scalability and adaptability, or struggle to generate semantically coherent prompts, making them easy to detect. Additionally, most existing approaches involve lengthy prompts, leading to higher query costs. In this paper, to remedy these challenges, we introduce a novel jailbreaking attack framework called PAPILLON, which is an automated, black-box jailbreaking attack framework that adapts the black-box fuzz testing approach with a series of customized designs. Instead of relying on manually crafted templates,PAPILLON starts with an empty seed pool, removing the need to search for any related jailbreaking templates. We also develop three novel question-dependent mutation strategies using an LLM helper to generate prompts that maintain semantic coherence while significantly reducing their length. Additionally, we implement a two-level judge module to accurately detect genuine successful jailbreaks. We evaluated PAPILLON on 7 representative LLMs and compared it with 5 state-of-the-art jailbreaking attack strategies. For proprietary LLM APIs, such as GPT-3.5 turbo, GPT-4, and Gemini-Pro, PAPILLONs achieves attack success rates of over 90%, 80%, and 74%, respectively, exceeding existing baselines by more than 60\\%. Additionally, PAPILLON can maintain high semantic coherence while significantly reducing the length of jailbreak prompts. When targeting GPT-4, PAPILLON can achieve over 78% attack success rate even with 100 tokens. Moreover, PAPILLON demonstrates transferability and is robust to state-of-the-art defenses. Code: https://github.com/aaFrostnova/Papillon",
      "authors": [
        "Xueluan Gong",
        "Mingzhe Li",
        "Yilin Zhang",
        "Fengyuan Ran",
        "Chen Chen",
        "Yanjiao Chen",
        "Qian Wang",
        "Kwok-Yan Lam"
      ],
      "published": "2024-09-23T10:03:09Z",
      "updated": "2025-03-03T07:25:21Z",
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2409.14866v5",
      "landing_url": "https://arxiv.org/abs/2409.14866v5",
      "doi": "https://doi.org/10.48550/arXiv.2409.14866"
    },
    "queries": [
      {
        "anchor": "synchronous dialogue",
        "search_term": "semantic coherence"
      }
    ]
  },
  {
    "arxiv_id": "2409.15594",
    "anchor": "full-duplex",
    "search_term": "true full-duplex",
    "search_record": {
      "id": "http://arxiv.org/abs/2409.15594v1",
      "title": "Beyond Turn-Based Interfaces: Synchronous LLMs as Full-Duplex Dialogue Agents",
      "summary": "Despite broad interest in modeling spoken dialogue agents, most approaches are inherently \"half-duplex\" -- restricted to turn-based interaction with responses requiring explicit prompting by the user or implicit tracking of interruption or silence events. Human dialogue, by contrast, is \"full-duplex\" allowing for rich synchronicity in the form of quick and dynamic turn-taking, overlapping speech, and backchanneling. Technically, the challenge of achieving full-duplex dialogue with LLMs lies in modeling synchrony as pre-trained LLMs do not have a sense of \"time\". To bridge this gap, we propose Synchronous LLMs for full-duplex spoken dialogue modeling. We design a novel mechanism to integrate time information into Llama3-8b so that they run synchronously with the real-world clock. We also introduce a training recipe that uses 212k hours of synthetic spoken dialogue data generated from text dialogue data to create a model that generates meaningful and natural spoken dialogue, with just 2k hours of real-world spoken dialogue data. Synchronous LLMs outperform state-of-the-art in dialogue meaningfulness while maintaining naturalness. Finally, we demonstrate the model's ability to participate in full-duplex dialogue by simulating interaction between two agents trained on different datasets, while considering Internet-scale latencies of up to 240 ms. Webpage: https://syncllm.cs.washington.edu/.",
      "published": "2024-09-23T23:01:31Z"
    },
    "metadata": {
      "arxiv_id": "2409.15594",
      "title": "Beyond Turn-Based Interfaces: Synchronous LLMs as Full-Duplex Dialogue Agents",
      "summary": "Despite broad interest in modeling spoken dialogue agents, most approaches are inherently \"half-duplex\" -- restricted to turn-based interaction with responses requiring explicit prompting by the user or implicit tracking of interruption or silence events. Human dialogue, by contrast, is \"full-duplex\" allowing for rich synchronicity in the form of quick and dynamic turn-taking, overlapping speech, and backchanneling. Technically, the challenge of achieving full-duplex dialogue with LLMs lies in modeling synchrony as pre-trained LLMs do not have a sense of \"time\". To bridge this gap, we propose Synchronous LLMs for full-duplex spoken dialogue modeling. We design a novel mechanism to integrate time information into Llama3-8b so that they run synchronously with the real-world clock. We also introduce a training recipe that uses 212k hours of synthetic spoken dialogue data generated from text dialogue data to create a model that generates meaningful and natural spoken dialogue, with just 2k hours of real-world spoken dialogue data. Synchronous LLMs outperform state-of-the-art in dialogue meaningfulness while maintaining naturalness. Finally, we demonstrate the model's ability to participate in full-duplex dialogue by simulating interaction between two agents trained on different datasets, while considering Internet-scale latencies of up to 240 ms. Webpage: https://syncllm.cs.washington.edu/.",
      "authors": [
        "Bandhav Veluri",
        "Benjamin N Peloquin",
        "Bokai Yu",
        "Hongyu Gong",
        "Shyamnath Gollakota"
      ],
      "published": "2024-09-23T23:01:31Z",
      "updated": "2024-09-23T23:01:31Z",
      "categories": [
        "cs.CL",
        "cs.LG",
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2409.15594v1",
      "landing_url": "https://arxiv.org/abs/2409.15594v1",
      "doi": "https://doi.org/10.48550/arXiv.2409.15594"
    },
    "queries": [
      {
        "anchor": "full-duplex",
        "search_term": "true full-duplex"
      },
      {
        "anchor": "full-duplex",
        "search_term": "pseudo full-duplex"
      },
      {
        "anchor": "full-duplex",
        "search_term": "half duplex"
      },
      {
        "anchor": "full-duplex",
        "search_term": "engineered synchronization"
      },
      {
        "anchor": "full-duplex",
        "search_term": "learned synchronization"
      },
      {
        "anchor": "full-duplex",
        "search_term": "modular architecture"
      },
      {
        "anchor": "full-duplex",
        "search_term": "streaming architecture"
      },
      {
        "anchor": "full-duplex",
        "search_term": "real-time interaction"
      },
      {
        "anchor": "full-duplex",
        "search_term": "low latency"
      },
      {
        "anchor": "full-duplex",
        "search_term": "synchronous data"
      },
      {
        "anchor": "full-duplex",
        "search_term": "data scarcity"
      },
      {
        "anchor": "full-duplex",
        "search_term": "multichannel dialogue"
      },
      {
        "anchor": "full-duplex",
        "search_term": "temporal dynamics"
      },
      {
        "anchor": "full-duplex",
        "search_term": "behavioral arbitration"
      },
      {
        "anchor": "full-duplex",
        "search_term": "semantic coherence"
      },
      {
        "anchor": "full-duplex",
        "search_term": "neural codec"
      },
      {
        "anchor": "full-duplex",
        "search_term": "discrete tokens"
      },
      {
        "anchor": "full-duplex",
        "search_term": "audio tokens"
      },
      {
        "anchor": "full-duplex",
        "search_term": "floor transfer offset"
      },
      {
        "anchor": "full-duplex",
        "search_term": "stop latency"
      },
      {
        "anchor": "full-duplex",
        "search_term": "interruption response delay"
      }
    ]
  },
  {
    "arxiv_id": "2409.15897",
    "anchor": "spoken language models",
    "search_term": "neural codec",
    "search_record": {
      "id": "http://arxiv.org/abs/2409.15897v2",
      "title": "ESPnet-Codec: Comprehensive Training and Evaluation of Neural Codecs for Audio, Music, and Speech",
      "summary": "Neural codecs have become crucial to recent speech and audio generation research. In addition to signal compression capabilities, discrete codecs have also been found to enhance downstream training efficiency and compatibility with autoregressive language models. However, as extensive downstream applications are investigated, challenges have arisen in ensuring fair comparisons across diverse applications. To address these issues, we present a new open-source platform ESPnet-Codec, which is built on ESPnet and focuses on neural codec training and evaluation. ESPnet-Codec offers various recipes in audio, music, and speech for training and evaluation using several widely adopted codec models. Together with ESPnet-Codec, we present VERSA, a standalone evaluation toolkit, which provides a comprehensive evaluation of codec performance over 20 audio evaluation metrics. Notably, we demonstrate that ESPnet-Codec can be integrated into six ESPnet tasks, supporting diverse applications.",
      "published": "2024-09-24T09:16:11Z"
    },
    "metadata": {
      "arxiv_id": "2409.15897",
      "title": "ESPnet-Codec: Comprehensive Training and Evaluation of Neural Codecs for Audio, Music, and Speech",
      "summary": "Neural codecs have become crucial to recent speech and audio generation research. In addition to signal compression capabilities, discrete codecs have also been found to enhance downstream training efficiency and compatibility with autoregressive language models. However, as extensive downstream applications are investigated, challenges have arisen in ensuring fair comparisons across diverse applications. To address these issues, we present a new open-source platform ESPnet-Codec, which is built on ESPnet and focuses on neural codec training and evaluation. ESPnet-Codec offers various recipes in audio, music, and speech for training and evaluation using several widely adopted codec models. Together with ESPnet-Codec, we present VERSA, a standalone evaluation toolkit, which provides a comprehensive evaluation of codec performance over 20 audio evaluation metrics. Notably, we demonstrate that ESPnet-Codec can be integrated into six ESPnet tasks, supporting diverse applications.",
      "authors": [
        "Jiatong Shi",
        "Jinchuan Tian",
        "Yihan Wu",
        "Jee-weon Jung",
        "Jia Qi Yip",
        "Yoshiki Masuyama",
        "William Chen",
        "Yuning Wu",
        "Yuxun Tang",
        "Massa Baali",
        "Dareen Alharhi",
        "Dong Zhang",
        "Ruifan Deng",
        "Tejes Srivastava",
        "Haibin Wu",
        "Alexander H. Liu",
        "Bhiksha Raj",
        "Qin Jin",
        "Ruihua Song",
        "Shinji Watanabe"
      ],
      "published": "2024-09-24T09:16:11Z",
      "updated": "2025-02-24T18:34:41Z",
      "categories": [
        "eess.AS",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2409.15897v2",
      "landing_url": "https://arxiv.org/abs/2409.15897v2",
      "doi": "https://doi.org/10.48550/arXiv.2409.15897"
    },
    "queries": [
      {
        "anchor": "spoken language models",
        "search_term": "neural codec"
      },
      {
        "anchor": "synchronous dialogue",
        "search_term": "neural codec"
      },
      {
        "anchor": "turn-taking",
        "search_term": "neural codec"
      }
    ]
  },
  {
    "arxiv_id": "2409.15910",
    "anchor": "synchronous dialogue",
    "search_term": "real-time interaction",
    "search_record": {
      "id": "http://arxiv.org/abs/2409.15910v2",
      "title": "Enhancing IoT based Plant Health Monitoring through Advanced Human Plant Interaction using Large Language Models and Mobile Applications",
      "summary": "This paper presents the development of a novel plant communication application that allows plants to \"talk\" to humans using real-time sensor data and AI-powered language models. Utilizing soil sensors that track moisture, temperature, and nutrient levels, the system feeds this data into the Gemini API, where it is processed and transformed into natural language insights about the plant's health and \"mood.\" Developed using Flutter, Firebase, and ThingSpeak, the app offers a seamless user experience with real-time interaction capabilities. By fostering human-plant connectivity, this system enhances plant care practices, promotes sustainability, and introduces innovative applications for AI and IoT technologies in both personal and agricultural contexts. The paper explores the technical architecture, system integration, and broader implications of AI-driven plant communication.",
      "published": "2024-09-24T09:26:47Z"
    },
    "metadata": {
      "arxiv_id": "2409.15910",
      "title": "Enhancing IoT based Plant Health Monitoring through Advanced Human Plant Interaction using Large Language Models and Mobile Applications",
      "summary": "This paper presents the development of a novel plant communication application that allows plants to \"talk\" to humans using real-time sensor data and AI-powered language models. Utilizing soil sensors that track moisture, temperature, and nutrient levels, the system feeds this data into the Gemini API, where it is processed and transformed into natural language insights about the plant's health and \"mood.\" Developed using Flutter, Firebase, and ThingSpeak, the app offers a seamless user experience with real-time interaction capabilities. By fostering human-plant connectivity, this system enhances plant care practices, promotes sustainability, and introduces innovative applications for AI and IoT technologies in both personal and agricultural contexts. The paper explores the technical architecture, system integration, and broader implications of AI-driven plant communication.",
      "authors": [
        "Kriti Agarwal",
        "Samhruth Ananthanarayanan",
        "Srinitish Srinivasan",
        "Abirami S"
      ],
      "published": "2024-09-24T09:26:47Z",
      "updated": "2025-01-05T18:21:23Z",
      "categories": [
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2409.15910v2",
      "landing_url": "https://arxiv.org/abs/2409.15910v2",
      "doi": "https://doi.org/10.48550/arXiv.2409.15910"
    },
    "queries": [
      {
        "anchor": "synchronous dialogue",
        "search_term": "real-time interaction"
      }
    ]
  },
  {
    "arxiv_id": "2409.16830",
    "anchor": "synchronous dialogue",
    "search_term": "real-time interaction",
    "search_record": {
      "id": "http://arxiv.org/abs/2409.16830v1",
      "title": "OffRIPP: Offline RL-based Informative Path Planning",
      "summary": "Informative path planning (IPP) is a crucial task in robotics, where agents must design paths to gather valuable information about a target environment while adhering to resource constraints. Reinforcement learning (RL) has been shown to be effective for IPP, however, it requires environment interactions, which are risky and expensive in practice. To address this problem, we propose an offline RL-based IPP framework that optimizes information gain without requiring real-time interaction during training, offering safety and cost-efficiency by avoiding interaction, as well as superior performance and fast computation during execution -- key advantages of RL. Our framework leverages batch-constrained reinforcement learning to mitigate extrapolation errors, enabling the agent to learn from pre-collected datasets generated by arbitrary algorithms. We validate the framework through extensive simulations and real-world experiments. The numerical results show that our framework outperforms the baselines, demonstrating the effectiveness of the proposed approach.",
      "published": "2024-09-25T11:30:59Z"
    },
    "metadata": {
      "arxiv_id": "2409.16830",
      "title": "OffRIPP: Offline RL-based Informative Path Planning",
      "summary": "Informative path planning (IPP) is a crucial task in robotics, where agents must design paths to gather valuable information about a target environment while adhering to resource constraints. Reinforcement learning (RL) has been shown to be effective for IPP, however, it requires environment interactions, which are risky and expensive in practice. To address this problem, we propose an offline RL-based IPP framework that optimizes information gain without requiring real-time interaction during training, offering safety and cost-efficiency by avoiding interaction, as well as superior performance and fast computation during execution -- key advantages of RL. Our framework leverages batch-constrained reinforcement learning to mitigate extrapolation errors, enabling the agent to learn from pre-collected datasets generated by arbitrary algorithms. We validate the framework through extensive simulations and real-world experiments. The numerical results show that our framework outperforms the baselines, demonstrating the effectiveness of the proposed approach.",
      "authors": [
        "Srikar Babu Gadipudi",
        "Srujan Deolasee",
        "Siva Kailas",
        "Wenhao Luo",
        "Katia Sycara",
        "Woojun Kim"
      ],
      "published": "2024-09-25T11:30:59Z",
      "updated": "2024-09-25T11:30:59Z",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2409.16830v1",
      "landing_url": "https://arxiv.org/abs/2409.16830v1",
      "doi": "https://doi.org/10.48550/arXiv.2409.16830"
    },
    "queries": [
      {
        "anchor": "synchronous dialogue",
        "search_term": "real-time interaction"
      }
    ]
  },
  {
    "arxiv_id": "2409.19132",
    "anchor": "spoken language models",
    "search_term": "audio tokens",
    "search_record": {
      "id": "http://arxiv.org/abs/2409.19132v1",
      "title": "From Vision to Audio and Beyond: A Unified Model for Audio-Visual Representation and Generation",
      "summary": "Video encompasses both visual and auditory data, creating a perceptually rich experience where these two modalities complement each other. As such, videos are a valuable type of media for the investigation of the interplay between audio and visual elements. Previous studies of audio-visual modalities primarily focused on either audio-visual representation learning or generative modeling of a modality conditioned on the other, creating a disconnect between these two branches. A unified framework that learns representation and generates modalities has not been developed yet. In this work, we introduce a novel framework called Vision to Audio and Beyond (VAB) to bridge the gap between audio-visual representation learning and vision-to-audio generation. The key approach of VAB is that rather than working with raw video frames and audio data, VAB performs representation learning and generative modeling within latent spaces. In particular, VAB uses a pre-trained audio tokenizer and an image encoder to obtain audio tokens and visual features, respectively. It then performs the pre-training task of visual-conditioned masked audio token prediction. This training strategy enables the model to engage in contextual learning and simultaneous video-to-audio generation. After the pre-training phase, VAB employs the iterative-decoding approach to rapidly generate audio tokens conditioned on visual features. Since VAB is a unified model, its backbone can be fine-tuned for various audio-visual downstream tasks. Our experiments showcase the efficiency of VAB in producing high-quality audio from video, and its capability to acquire semantic audio-visual features, leading to competitive results in audio-visual retrieval and classification.",
      "published": "2024-09-27T20:26:34Z"
    },
    "metadata": {
      "arxiv_id": "2409.19132",
      "title": "From Vision to Audio and Beyond: A Unified Model for Audio-Visual Representation and Generation",
      "summary": "Video encompasses both visual and auditory data, creating a perceptually rich experience where these two modalities complement each other. As such, videos are a valuable type of media for the investigation of the interplay between audio and visual elements. Previous studies of audio-visual modalities primarily focused on either audio-visual representation learning or generative modeling of a modality conditioned on the other, creating a disconnect between these two branches. A unified framework that learns representation and generates modalities has not been developed yet. In this work, we introduce a novel framework called Vision to Audio and Beyond (VAB) to bridge the gap between audio-visual representation learning and vision-to-audio generation. The key approach of VAB is that rather than working with raw video frames and audio data, VAB performs representation learning and generative modeling within latent spaces. In particular, VAB uses a pre-trained audio tokenizer and an image encoder to obtain audio tokens and visual features, respectively. It then performs the pre-training task of visual-conditioned masked audio token prediction. This training strategy enables the model to engage in contextual learning and simultaneous video-to-audio generation. After the pre-training phase, VAB employs the iterative-decoding approach to rapidly generate audio tokens conditioned on visual features. Since VAB is a unified model, its backbone can be fine-tuned for various audio-visual downstream tasks. Our experiments showcase the efficiency of VAB in producing high-quality audio from video, and its capability to acquire semantic audio-visual features, leading to competitive results in audio-visual retrieval and classification.",
      "authors": [
        "Kun Su",
        "Xiulong Liu",
        "Eli Shlizerman"
      ],
      "published": "2024-09-27T20:26:34Z",
      "updated": "2024-09-27T20:26:34Z",
      "categories": [
        "cs.MM",
        "cs.CV",
        "cs.LG",
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2409.19132v1",
      "landing_url": "https://arxiv.org/abs/2409.19132v1",
      "doi": "https://doi.org/10.48550/arXiv.2409.19132"
    },
    "queries": [
      {
        "anchor": "spoken language models",
        "search_term": "audio tokens"
      },
      {
        "anchor": "synchronous dialogue",
        "search_term": "audio tokens"
      },
      {
        "anchor": "turn-taking",
        "search_term": "audio tokens"
      }
    ]
  },
  {
    "arxiv_id": "2409.19283",
    "anchor": "full-duplex",
    "search_term": "audio tokens",
    "search_record": {
      "id": "http://arxiv.org/abs/2409.19283v2",
      "title": "Analyzing and Mitigating Inconsistency in Discrete Audio Tokens for Neural Codec Language Models",
      "summary": "Building upon advancements in Large Language Models (LLMs), the field of audio processing has seen increased interest in training audio generation tasks with discrete audio token sequences. However, directly discretizing audio by neural audio codecs often results in sequences that fundamentally differ from text sequences. Unlike text, where text token sequences are deterministic, discrete audio tokens can exhibit significant variability based on contextual factors, while still producing perceptually identical audio segments. We refer to this phenomenon as \\textbf{Discrete Representation Inconsistency (DRI)}. This inconsistency can lead to a single audio segment being represented by multiple divergent sequences, which creates confusion in neural codec language models and results in omissions and repetitions during speech generation. In this paper, we quantitatively analyze the DRI phenomenon within popular audio tokenizers such as EnCodec. Our approach effectively mitigates the DRI phenomenon of the neural audio codec. Furthermore, extensive experiments on the neural codec language model over LibriTTS and large-scale MLS datases (44,000 hours) demonstrate the effectiveness and generality of our method. The demo of audio samples is available online~\\footnote{\\url{https://consistencyinneuralcodec.github.io}}.",
      "published": "2024-09-28T08:36:44Z"
    },
    "metadata": {
      "arxiv_id": "2409.19283",
      "title": "Analyzing and Mitigating Inconsistency in Discrete Audio Tokens for Neural Codec Language Models",
      "summary": "Building upon advancements in Large Language Models (LLMs), the field of audio processing has seen increased interest in training audio generation tasks with discrete audio token sequences. However, directly discretizing audio by neural audio codecs often results in sequences that fundamentally differ from text sequences. Unlike text, where text token sequences are deterministic, discrete audio tokens can exhibit significant variability based on contextual factors, while still producing perceptually identical audio segments. We refer to this phenomenon as \\textbf{Discrete Representation Inconsistency (DRI)}. This inconsistency can lead to a single audio segment being represented by multiple divergent sequences, which creates confusion in neural codec language models and results in omissions and repetitions during speech generation. In this paper, we quantitatively analyze the DRI phenomenon within popular audio tokenizers such as EnCodec. Our approach effectively mitigates the DRI phenomenon of the neural audio codec. Furthermore, extensive experiments on the neural codec language model over LibriTTS and large-scale MLS datases (44,000 hours) demonstrate the effectiveness and generality of our method. The demo of audio samples is available online~\\footnote{\\url{https://consistencyinneuralcodec.github.io}}.",
      "authors": [
        "Wenrui Liu",
        "Zhifang Guo",
        "Jin Xu",
        "Yuanjun Lv",
        "Yunfei Chu",
        "Zhou Zhao",
        "Junyang Lin"
      ],
      "published": "2024-09-28T08:36:44Z",
      "updated": "2024-10-04T22:34:38Z",
      "categories": [
        "eess.AS",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2409.19283v2",
      "landing_url": "https://arxiv.org/abs/2409.19283v2",
      "doi": "https://doi.org/10.48550/arXiv.2409.19283"
    },
    "queries": [
      {
        "anchor": "full-duplex",
        "search_term": "audio tokens"
      },
      {
        "anchor": "spoken language models",
        "search_term": "neural codec"
      },
      {
        "anchor": "spoken language models",
        "search_term": "audio tokens"
      },
      {
        "anchor": "synchronous dialogue",
        "search_term": "neural codec"
      },
      {
        "anchor": "synchronous dialogue",
        "search_term": "audio tokens"
      },
      {
        "anchor": "turn-taking",
        "search_term": "neural codec"
      },
      {
        "anchor": "turn-taking",
        "search_term": "audio tokens"
      }
    ]
  },
  {
    "arxiv_id": "2410.00025",
    "anchor": "spoken language models",
    "search_term": "true full-duplex",
    "search_record": {
      "id": "http://arxiv.org/abs/2410.00025v2",
      "title": "Improving Spoken Language Modeling with Phoneme Classification: A Simple Fine-tuning Approach",
      "summary": "Recent progress in Spoken Language Modeling has shown that learning language directly from speech is feasible. Generating speech through a pipeline that operates at the text level typically loses nuances, intonations, and non-verbal vocalizations. Modeling directly from speech opens up the path to more natural and expressive systems. On the other hand, speech-only systems require up to three orders of magnitude more data to catch up to their text-based counterparts in terms of their semantic abilities. We show that fine-tuning speech representation models on phoneme classification leads to more context-invariant representations, and language models trained on these units achieve comparable lexical comprehension to ones trained on hundred times more data.",
      "published": "2024-09-16T10:29:15Z"
    },
    "metadata": {
      "arxiv_id": "2410.00025",
      "title": "Improving Spoken Language Modeling with Phoneme Classification: A Simple Fine-tuning Approach",
      "summary": "Recent progress in Spoken Language Modeling has shown that learning language directly from speech is feasible. Generating speech through a pipeline that operates at the text level typically loses nuances, intonations, and non-verbal vocalizations. Modeling directly from speech opens up the path to more natural and expressive systems. On the other hand, speech-only systems require up to three orders of magnitude more data to catch up to their text-based counterparts in terms of their semantic abilities. We show that fine-tuning speech representation models on phoneme classification leads to more context-invariant representations, and language models trained on these units achieve comparable lexical comprehension to ones trained on hundred times more data.",
      "authors": [
        "Maxime Poli",
        "Emmanuel Chemla",
        "Emmanuel Dupoux"
      ],
      "published": "2024-09-16T10:29:15Z",
      "updated": "2024-10-30T17:46:22Z",
      "categories": [
        "cs.CL",
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2410.00025v2",
      "landing_url": "https://arxiv.org/abs/2410.00025v2",
      "doi": "https://doi.org/10.48550/arXiv.2410.00025"
    },
    "queries": [
      {
        "anchor": "spoken language models",
        "search_term": "true full-duplex"
      },
      {
        "anchor": "spoken language models",
        "search_term": "pseudo full-duplex"
      },
      {
        "anchor": "spoken language models",
        "search_term": "half duplex"
      },
      {
        "anchor": "spoken language models",
        "search_term": "engineered synchronization"
      },
      {
        "anchor": "spoken language models",
        "search_term": "learned synchronization"
      },
      {
        "anchor": "spoken language models",
        "search_term": "modular architecture"
      },
      {
        "anchor": "spoken language models",
        "search_term": "streaming architecture"
      },
      {
        "anchor": "spoken language models",
        "search_term": "real-time interaction"
      },
      {
        "anchor": "spoken language models",
        "search_term": "low latency"
      },
      {
        "anchor": "spoken language models",
        "search_term": "synchronous data"
      },
      {
        "anchor": "spoken language models",
        "search_term": "data scarcity"
      },
      {
        "anchor": "spoken language models",
        "search_term": "multichannel dialogue"
      },
      {
        "anchor": "spoken language models",
        "search_term": "temporal dynamics"
      },
      {
        "anchor": "spoken language models",
        "search_term": "behavioral arbitration"
      },
      {
        "anchor": "spoken language models",
        "search_term": "semantic coherence"
      },
      {
        "anchor": "spoken language models",
        "search_term": "neural codec"
      },
      {
        "anchor": "spoken language models",
        "search_term": "discrete tokens"
      },
      {
        "anchor": "spoken language models",
        "search_term": "audio tokens"
      },
      {
        "anchor": "spoken language models",
        "search_term": "floor transfer offset"
      },
      {
        "anchor": "spoken language models",
        "search_term": "stop latency"
      },
      {
        "anchor": "spoken language models",
        "search_term": "interruption response delay"
      }
    ]
  },
  {
    "arxiv_id": "2410.00037",
    "anchor": "spoken language models",
    "search_term": "audio tokens",
    "search_record": {
      "id": "http://arxiv.org/abs/2410.00037v2",
      "title": "Moshi: a speech-text foundation model for real-time dialogue",
      "summary": "We introduce Moshi, a speech-text foundation model and full-duplex spoken dialogue framework. Current systems for spoken dialogue rely on pipelines of independent components, namely voice activity detection, speech recognition, textual dialogue and text-to-speech. Such frameworks cannot emulate the experience of real conversations. First, their complexity induces a latency of several seconds between interactions. Second, text being the intermediate modality for dialogue, non-linguistic information that modifies meaning -- such as emotion or non-speech sounds -- is lost in the interaction. Finally, they rely on a segmentation into speaker turns, which does not take into account overlapping speech, interruptions and interjections. Moshi solves these independent issues altogether by casting spoken dialogue as speech-to-speech generation. Starting from a text language model backbone, Moshi generates speech as tokens from the residual quantizer of a neural audio codec, while modeling separately its own speech and that of the user into parallel streams. This allows for the removal of explicit speaker turns, and the modeling of arbitrary conversational dynamics. We moreover extend the hierarchical semantic-to-acoustic token generation of previous work to first predict time-aligned text tokens as a prefix to audio tokens. Not only this \"Inner Monologue\" method significantly improves the linguistic quality of generated speech, but we also illustrate how it can provide streaming speech recognition and text-to-speech. Our resulting model is the first real-time full-duplex spoken large language model, with a theoretical latency of 160ms, 200ms in practice, and is available at https://github.com/kyutai-labs/moshi.",
      "published": "2024-09-17T17:55:39Z"
    },
    "metadata": {
      "arxiv_id": "2410.00037",
      "title": "Moshi: a speech-text foundation model for real-time dialogue",
      "summary": "We introduce Moshi, a speech-text foundation model and full-duplex spoken dialogue framework. Current systems for spoken dialogue rely on pipelines of independent components, namely voice activity detection, speech recognition, textual dialogue and text-to-speech. Such frameworks cannot emulate the experience of real conversations. First, their complexity induces a latency of several seconds between interactions. Second, text being the intermediate modality for dialogue, non-linguistic information that modifies meaning -- such as emotion or non-speech sounds -- is lost in the interaction. Finally, they rely on a segmentation into speaker turns, which does not take into account overlapping speech, interruptions and interjections. Moshi solves these independent issues altogether by casting spoken dialogue as speech-to-speech generation. Starting from a text language model backbone, Moshi generates speech as tokens from the residual quantizer of a neural audio codec, while modeling separately its own speech and that of the user into parallel streams. This allows for the removal of explicit speaker turns, and the modeling of arbitrary conversational dynamics. We moreover extend the hierarchical semantic-to-acoustic token generation of previous work to first predict time-aligned text tokens as a prefix to audio tokens. Not only this \"Inner Monologue\" method significantly improves the linguistic quality of generated speech, but we also illustrate how it can provide streaming speech recognition and text-to-speech. Our resulting model is the first real-time full-duplex spoken large language model, with a theoretical latency of 160ms, 200ms in practice, and is available at https://github.com/kyutai-labs/moshi.",
      "authors": [
        "Alexandre Défossez",
        "Laurent Mazaré",
        "Manu Orsini",
        "Amélie Royer",
        "Patrick Pérez",
        "Hervé Jégou",
        "Edouard Grave",
        "Neil Zeghidour"
      ],
      "published": "2024-09-17T17:55:39Z",
      "updated": "2024-10-02T09:11:45Z",
      "categories": [
        "eess.AS",
        "cs.AI",
        "cs.CL",
        "cs.LG",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2410.00037v2",
      "landing_url": "https://arxiv.org/abs/2410.00037v2",
      "doi": "https://doi.org/10.48550/arXiv.2410.00037"
    },
    "queries": [
      {
        "anchor": "spoken language models",
        "search_term": "audio tokens"
      },
      {
        "anchor": "synchronous dialogue",
        "search_term": "audio tokens"
      }
    ]
  },
  {
    "arxiv_id": "2410.00502",
    "anchor": "spoken language models",
    "search_term": "semantic coherence",
    "search_record": {
      "id": "http://arxiv.org/abs/2410.00502v1",
      "title": "Multi-Target Cross-Lingual Summarization: a novel task and a language-neutral approach",
      "summary": "Cross-lingual summarization aims to bridge language barriers by summarizing documents in different languages. However, ensuring semantic coherence across languages is an overlooked challenge and can be critical in several contexts. To fill this gap, we introduce multi-target cross-lingual summarization as the task of summarizing a document into multiple target languages while ensuring that the produced summaries are semantically similar. We propose a principled re-ranking approach to this problem and a multi-criteria evaluation protocol to assess semantic coherence across target languages, marking a first step that will hopefully stimulate further research on this problem.",
      "published": "2024-10-01T08:33:57Z"
    },
    "metadata": {
      "arxiv_id": "2410.00502",
      "title": "Multi-Target Cross-Lingual Summarization: a novel task and a language-neutral approach",
      "summary": "Cross-lingual summarization aims to bridge language barriers by summarizing documents in different languages. However, ensuring semantic coherence across languages is an overlooked challenge and can be critical in several contexts. To fill this gap, we introduce multi-target cross-lingual summarization as the task of summarizing a document into multiple target languages while ensuring that the produced summaries are semantically similar. We propose a principled re-ranking approach to this problem and a multi-criteria evaluation protocol to assess semantic coherence across target languages, marking a first step that will hopefully stimulate further research on this problem.",
      "authors": [
        "Diogo Pernes",
        "Gonçalo M. Correia",
        "Afonso Mendes"
      ],
      "published": "2024-10-01T08:33:57Z",
      "updated": "2024-10-01T08:33:57Z",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2410.00502v1",
      "landing_url": "https://arxiv.org/abs/2410.00502v1",
      "doi": "https://doi.org/10.48550/arXiv.2410.00502"
    },
    "queries": [
      {
        "anchor": "spoken language models",
        "search_term": "semantic coherence"
      },
      {
        "anchor": "synchronous dialogue",
        "search_term": "semantic coherence"
      },
      {
        "anchor": "turn-taking",
        "search_term": "semantic coherence"
      }
    ]
  },
  {
    "arxiv_id": "2410.00822",
    "anchor": "synchronous dialogue",
    "search_term": "streaming architecture",
    "search_record": {
      "id": "http://arxiv.org/abs/2410.00822v2",
      "title": "VHASR: A Multimodal Speech Recognition System With Vision Hotwords",
      "summary": "The image-based multimodal automatic speech recognition (ASR) model enhances speech recognition performance by incorporating audio-related image. However, some works suggest that introducing image information to model does not help improving ASR performance. In this paper, we propose a novel approach effectively utilizing audio-related image information and set up VHASR, a multimodal speech recognition system that uses vision as hotwords to strengthen the model's speech recognition capability. Our system utilizes a dual-stream architecture, which firstly transcribes the text on the two streams separately, and then combines the outputs. We evaluate the proposed model on four datasets: Flickr8k, ADE20k, COCO, and OpenImages. The experimental results show that VHASR can effectively utilize key information in images to enhance the model's speech recognition ability. Its performance not only surpasses unimodal ASR, but also achieves SOTA among existing image-based multimodal ASR.",
      "published": "2024-10-01T16:06:02Z"
    },
    "metadata": {
      "arxiv_id": "2410.00822",
      "title": "VHASR: A Multimodal Speech Recognition System With Vision Hotwords",
      "summary": "The image-based multimodal automatic speech recognition (ASR) model enhances speech recognition performance by incorporating audio-related image. However, some works suggest that introducing image information to model does not help improving ASR performance. In this paper, we propose a novel approach effectively utilizing audio-related image information and set up VHASR, a multimodal speech recognition system that uses vision as hotwords to strengthen the model's speech recognition capability. Our system utilizes a dual-stream architecture, which firstly transcribes the text on the two streams separately, and then combines the outputs. We evaluate the proposed model on four datasets: Flickr8k, ADE20k, COCO, and OpenImages. The experimental results show that VHASR can effectively utilize key information in images to enhance the model's speech recognition ability. Its performance not only surpasses unimodal ASR, but also achieves SOTA among existing image-based multimodal ASR.",
      "authors": [
        "Jiliang Hu",
        "Zuchao Li",
        "Ping Wang",
        "Haojun Ai",
        "Lefei Zhang",
        "Hai Zhao"
      ],
      "published": "2024-10-01T16:06:02Z",
      "updated": "2024-10-04T18:30:06Z",
      "categories": [
        "cs.SD",
        "cs.CL",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2410.00822v2",
      "landing_url": "https://arxiv.org/abs/2410.00822v2",
      "doi": "https://doi.org/10.48550/arXiv.2410.00822"
    },
    "queries": [
      {
        "anchor": "synchronous dialogue",
        "search_term": "streaming architecture"
      }
    ]
  },
  {
    "arxiv_id": "2410.02225",
    "anchor": "synchronous dialogue",
    "search_term": "modular architecture",
    "search_record": {
      "id": "http://arxiv.org/abs/2410.02225v2",
      "title": "Open-source shape optimization for isogeometric shells using FEniCS and OpenMDAO",
      "summary": "We present an open-source Python framework for the shape optimization of complex shell structures using isogeometric analysis (IGA). IGA seamlessly integrates computer-aided design (CAD) and analysis models by employing non-uniform rational B-splines (NURBS) as basis functions, enabling the natural implementation of the Kirchhoff--Love shell model due to their higher order of continuity. We leverage the recently developed FEniCS-based analysis framework, PENGoLINS, for the direct structural analysis of shell structures consisting of a collection of NURBS patches through a penalty-based formulation. This contribution introduces the open-source implementation of gradient-based shape optimization for isogeometric Kirchhoff--Love shells with a modular architecture. Complex shell structures with non-matching intersections are handled using a free-form deformation (FFD) approach and a moving intersections formulation. The symbolic differentiation and code generation capabilities in FEniCS are utilized to compute the analytical derivatives. By integrating FEniCS with OpenMDAO, we build modular components that facilitate gradient-based shape optimization of shell structures. The modular architecture in this work supports future extensions and integration with other disciplines and solvers, making it highly customizable and suitable for a wide range of applications. We validate the design-analysis-optimization workflow through several benchmark problems and demonstrate its application to aircraft wing design optimization. The framework is implemented in a Python library named GOLDFISH (Gradient-based Optimization and Large-scale Design Framework for Isogeometric SHells) and the source code will be maintained at https://github.com/hanzhao2020/GOLDFISH.",
      "published": "2024-10-03T05:45:26Z"
    },
    "metadata": {
      "arxiv_id": "2410.02225",
      "title": "Open-source shape optimization for isogeometric shells using FEniCS and OpenMDAO",
      "summary": "We present an open-source Python framework for the shape optimization of complex shell structures using isogeometric analysis (IGA). IGA seamlessly integrates computer-aided design (CAD) and analysis models by employing non-uniform rational B-splines (NURBS) as basis functions, enabling the natural implementation of the Kirchhoff--Love shell model due to their higher order of continuity. We leverage the recently developed FEniCS-based analysis framework, PENGoLINS, for the direct structural analysis of shell structures consisting of a collection of NURBS patches through a penalty-based formulation. This contribution introduces the open-source implementation of gradient-based shape optimization for isogeometric Kirchhoff--Love shells with a modular architecture. Complex shell structures with non-matching intersections are handled using a free-form deformation (FFD) approach and a moving intersections formulation. The symbolic differentiation and code generation capabilities in FEniCS are utilized to compute the analytical derivatives. By integrating FEniCS with OpenMDAO, we build modular components that facilitate gradient-based shape optimization of shell structures. The modular architecture in this work supports future extensions and integration with other disciplines and solvers, making it highly customizable and suitable for a wide range of applications. We validate the design-analysis-optimization workflow through several benchmark problems and demonstrate its application to aircraft wing design optimization. The framework is implemented in a Python library named GOLDFISH (Gradient-based Optimization and Large-scale Design Framework for Isogeometric SHells) and the source code will be maintained at https://github.com/hanzhao2020/GOLDFISH.",
      "authors": [
        "Han Zhao",
        "John T. Hwang",
        "Jiun-Shyan Chen"
      ],
      "published": "2024-10-03T05:45:26Z",
      "updated": "2025-02-04T19:21:53Z",
      "categories": [
        "math.OC",
        "physics.comp-ph"
      ],
      "pdf_url": "https://arxiv.org/pdf/2410.02225v2",
      "landing_url": "https://arxiv.org/abs/2410.02225v2",
      "doi": "https://doi.org/10.48550/arXiv.2410.02225"
    },
    "queries": [
      {
        "anchor": "synchronous dialogue",
        "search_term": "modular architecture"
      }
    ]
  },
  {
    "arxiv_id": "2410.02651",
    "anchor": "spoken language models",
    "search_term": "modular architecture",
    "search_record": {
      "id": "http://arxiv.org/abs/2410.02651v2",
      "title": "CAX: Cellular Automata Accelerated in JAX",
      "summary": "Cellular automata have become a cornerstone for investigating emergence and self-organization across diverse scientific disciplines. However, the absence of a hardware-accelerated cellular automata library limits the exploration of new research directions, hinders collaboration, and impedes reproducibility. In this work, we introduce CAX (Cellular Automata Accelerated in JAX), a high-performance and flexible open-source library designed to accelerate cellular automata research. CAX delivers cutting-edge performance through hardware acceleration while maintaining flexibility through its modular architecture, intuitive API, and support for both discrete and continuous cellular automata in arbitrary dimensions. We demonstrate CAX's performance and flexibility through a wide range of benchmarks and applications. From classic models like elementary cellular automata and Conway's Game of Life to advanced applications such as growing neural cellular automata and self-classifying MNIST digits, CAX speeds up simulations up to 2,000 times faster. Furthermore, we demonstrate CAX's potential to accelerate research by presenting a collection of three novel cellular automata experiments, each implemented in just a few lines of code thanks to the library's modular architecture. Notably, we show that a simple one-dimensional cellular automaton can outperform GPT-4 on the 1D-ARC challenge.",
      "published": "2024-10-03T16:36:05Z"
    },
    "metadata": {
      "arxiv_id": "2410.02651",
      "title": "CAX: Cellular Automata Accelerated in JAX",
      "summary": "Cellular automata have become a cornerstone for investigating emergence and self-organization across diverse scientific disciplines. However, the absence of a hardware-accelerated cellular automata library limits the exploration of new research directions, hinders collaboration, and impedes reproducibility. In this work, we introduce CAX (Cellular Automata Accelerated in JAX), a high-performance and flexible open-source library designed to accelerate cellular automata research. CAX delivers cutting-edge performance through hardware acceleration while maintaining flexibility through its modular architecture, intuitive API, and support for both discrete and continuous cellular automata in arbitrary dimensions. We demonstrate CAX's performance and flexibility through a wide range of benchmarks and applications. From classic models like elementary cellular automata and Conway's Game of Life to advanced applications such as growing neural cellular automata and self-classifying MNIST digits, CAX speeds up simulations up to 2,000 times faster. Furthermore, we demonstrate CAX's potential to accelerate research by presenting a collection of three novel cellular automata experiments, each implemented in just a few lines of code thanks to the library's modular architecture. Notably, we show that a simple one-dimensional cellular automaton can outperform GPT-4 on the 1D-ARC challenge.",
      "authors": [
        "Maxence Faldor",
        "Antoine Cully"
      ],
      "published": "2024-10-03T16:36:05Z",
      "updated": "2025-03-11T11:34:10Z",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2410.02651v2",
      "landing_url": "https://arxiv.org/abs/2410.02651v2",
      "doi": "https://doi.org/10.48550/arXiv.2410.02651"
    },
    "queries": [
      {
        "anchor": "spoken language models",
        "search_term": "modular architecture"
      },
      {
        "anchor": "synchronous dialogue",
        "search_term": "modular architecture"
      }
    ]
  },
  {
    "arxiv_id": "2410.04029",
    "anchor": "spoken language models",
    "search_term": "true full-duplex",
    "search_record": {
      "id": "http://arxiv.org/abs/2410.04029v1",
      "title": "SyllableLM: Learning Coarse Semantic Units for Speech Language Models",
      "summary": "Language models require tokenized inputs. However, tokenization strategies for continuous data like audio and vision are often based on simple heuristics such as fixed sized convolutions or discrete clustering, which do not necessarily align with the semantic structure of the data. For speech in particular, the high resolution of waveforms (16,000 samples/second or more) presents a significant challenge as speech-based language models have had to use several times more tokens per word than text-based language models. In this work, we introduce a controllable self-supervised technique to merge speech representations into coarser syllable-like units while still preserving semantic information. We do this by 1) extracting noisy boundaries through analyzing correlations in pretrained encoder losses and 2) iteratively improving model representations with a novel distillation technique. Our method produces controllable-rate semantic units at as low as 5Hz and 60bps and achieves SotA in syllabic segmentation and clustering. Using these coarse tokens, we successfully train SyllableLM, a Speech Language Model (SpeechLM) that matches or outperforms current SotA SpeechLMs on a range of spoken language modeling tasks. SyllableLM also achieves significant improvements in efficiency with a 30x reduction in training compute and a 4x wall-clock inference speedup.",
      "published": "2024-10-05T04:29:55Z"
    },
    "metadata": {
      "arxiv_id": "2410.04029",
      "title": "SyllableLM: Learning Coarse Semantic Units for Speech Language Models",
      "summary": "Language models require tokenized inputs. However, tokenization strategies for continuous data like audio and vision are often based on simple heuristics such as fixed sized convolutions or discrete clustering, which do not necessarily align with the semantic structure of the data. For speech in particular, the high resolution of waveforms (16,000 samples/second or more) presents a significant challenge as speech-based language models have had to use several times more tokens per word than text-based language models. In this work, we introduce a controllable self-supervised technique to merge speech representations into coarser syllable-like units while still preserving semantic information. We do this by 1) extracting noisy boundaries through analyzing correlations in pretrained encoder losses and 2) iteratively improving model representations with a novel distillation technique. Our method produces controllable-rate semantic units at as low as 5Hz and 60bps and achieves SotA in syllabic segmentation and clustering. Using these coarse tokens, we successfully train SyllableLM, a Speech Language Model (SpeechLM) that matches or outperforms current SotA SpeechLMs on a range of spoken language modeling tasks. SyllableLM also achieves significant improvements in efficiency with a 30x reduction in training compute and a 4x wall-clock inference speedup.",
      "authors": [
        "Alan Baade",
        "Puyuan Peng",
        "David Harwath"
      ],
      "published": "2024-10-05T04:29:55Z",
      "updated": "2024-10-05T04:29:55Z",
      "categories": [
        "cs.CL",
        "cs.AI",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2410.04029v1",
      "landing_url": "https://arxiv.org/abs/2410.04029v1",
      "doi": "https://doi.org/10.48550/arXiv.2410.04029"
    },
    "queries": [
      {
        "anchor": "spoken language models",
        "search_term": "true full-duplex"
      },
      {
        "anchor": "spoken language models",
        "search_term": "pseudo full-duplex"
      },
      {
        "anchor": "spoken language models",
        "search_term": "engineered synchronization"
      },
      {
        "anchor": "spoken language models",
        "search_term": "learned synchronization"
      },
      {
        "anchor": "spoken language models",
        "search_term": "synchronous data"
      },
      {
        "anchor": "spoken language models",
        "search_term": "multichannel dialogue"
      },
      {
        "anchor": "spoken language models",
        "search_term": "behavioral arbitration"
      },
      {
        "anchor": "spoken language models",
        "search_term": "floor transfer offset"
      },
      {
        "anchor": "spoken language models",
        "search_term": "stop latency"
      },
      {
        "anchor": "spoken language models",
        "search_term": "interruption response delay"
      }
    ]
  },
  {
    "arxiv_id": "2410.04380",
    "anchor": "spoken language models",
    "search_term": "audio tokens",
    "search_record": {
      "id": "http://arxiv.org/abs/2410.04380v1",
      "title": "HALL-E: Hierarchical Neural Codec Language Model for Minute-Long Zero-Shot Text-to-Speech Synthesis",
      "summary": "Recently, Text-to-speech (TTS) models based on large language models (LLMs) that translate natural language text into sequences of discrete audio tokens have gained great research attention, with advances in neural audio codec (NAC) models using residual vector quantization (RVQ). However, long-form speech synthesis remains a significant challenge due to the high frame rate, which increases the length of audio tokens and makes it difficult for autoregressive language models to generate audio tokens for even a minute of speech. To address this challenge, this paper introduces two novel post-training approaches: 1) Multi-Resolution Requantization (MReQ) and 2) HALL-E. MReQ is a framework to reduce the frame rate of pre-trained NAC models. Specifically, it incorporates multi-resolution residual vector quantization (MRVQ) module that hierarchically reorganizes discrete audio tokens through teacher-student distillation. HALL-E is an LLM-based TTS model designed to predict hierarchical tokens of MReQ. Specifically, it incorporates the technique of using MRVQ sub-modules and continues training from a pre-trained LLM-based TTS model. Furthermore, to promote TTS research, we create MinutesSpeech, a new benchmark dataset consisting of 40k hours of filtered speech data for training and evaluating speech synthesis ranging from 3s up to 180s. In experiments, we demonstrated the effectiveness of our approaches by applying our post-training framework to VALL-E. We achieved the frame rate down to as low as 8 Hz, enabling the stable minitue-long speech synthesis in a single inference step. Audio samples, dataset, codes and pre-trained models are available at https://yutonishimura-v2.github.io/HALL-E_DEMO/.",
      "published": "2024-10-06T07:20:58Z"
    },
    "metadata": {
      "arxiv_id": "2410.04380",
      "title": "HALL-E: Hierarchical Neural Codec Language Model for Minute-Long Zero-Shot Text-to-Speech Synthesis",
      "summary": "Recently, Text-to-speech (TTS) models based on large language models (LLMs) that translate natural language text into sequences of discrete audio tokens have gained great research attention, with advances in neural audio codec (NAC) models using residual vector quantization (RVQ). However, long-form speech synthesis remains a significant challenge due to the high frame rate, which increases the length of audio tokens and makes it difficult for autoregressive language models to generate audio tokens for even a minute of speech. To address this challenge, this paper introduces two novel post-training approaches: 1) Multi-Resolution Requantization (MReQ) and 2) HALL-E. MReQ is a framework to reduce the frame rate of pre-trained NAC models. Specifically, it incorporates multi-resolution residual vector quantization (MRVQ) module that hierarchically reorganizes discrete audio tokens through teacher-student distillation. HALL-E is an LLM-based TTS model designed to predict hierarchical tokens of MReQ. Specifically, it incorporates the technique of using MRVQ sub-modules and continues training from a pre-trained LLM-based TTS model. Furthermore, to promote TTS research, we create MinutesSpeech, a new benchmark dataset consisting of 40k hours of filtered speech data for training and evaluating speech synthesis ranging from 3s up to 180s. In experiments, we demonstrated the effectiveness of our approaches by applying our post-training framework to VALL-E. We achieved the frame rate down to as low as 8 Hz, enabling the stable minitue-long speech synthesis in a single inference step. Audio samples, dataset, codes and pre-trained models are available at https://yutonishimura-v2.github.io/HALL-E_DEMO/.",
      "authors": [
        "Yuto Nishimura",
        "Takumi Hirose",
        "Masanari Ohi",
        "Hideki Nakayama",
        "Nakamasa Inoue"
      ],
      "published": "2024-10-06T07:20:58Z",
      "updated": "2024-10-06T07:20:58Z",
      "categories": [
        "eess.AS",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2410.04380v1",
      "landing_url": "https://arxiv.org/abs/2410.04380v1",
      "doi": "https://doi.org/10.48550/arXiv.2410.04380"
    },
    "queries": [
      {
        "anchor": "spoken language models",
        "search_term": "audio tokens"
      },
      {
        "anchor": "synchronous dialogue",
        "search_term": "neural codec"
      },
      {
        "anchor": "synchronous dialogue",
        "search_term": "audio tokens"
      },
      {
        "anchor": "turn-taking",
        "search_term": "audio tokens"
      }
    ]
  },
  {
    "arxiv_id": "2410.04534",
    "anchor": "synchronous dialogue",
    "search_term": "synchronous data",
    "search_record": {
      "id": "http://arxiv.org/abs/2410.04534v1",
      "title": "UniMuMo: Unified Text, Music and Motion Generation",
      "summary": "We introduce UniMuMo, a unified multimodal model capable of taking arbitrary text, music, and motion data as input conditions to generate outputs across all three modalities. To address the lack of time-synchronized data, we align unpaired music and motion data based on rhythmic patterns to leverage existing large-scale music-only and motion-only datasets. By converting music, motion, and text into token-based representation, our model bridges these modalities through a unified encoder-decoder transformer architecture. To support multiple generation tasks within a single framework, we introduce several architectural improvements. We propose encoding motion with a music codebook, mapping motion into the same feature space as music. We introduce a music-motion parallel generation scheme that unifies all music and motion generation tasks into a single transformer decoder architecture with a single training task of music-motion joint generation. Moreover, the model is designed by fine-tuning existing pre-trained single-modality models, significantly reducing computational demands. Extensive experiments demonstrate that UniMuMo achieves competitive results on all unidirectional generation benchmarks across music, motion, and text modalities. Quantitative results are available in the \\href{https://hanyangclarence.github.io/unimumo_demo/}{project page}.",
      "published": "2024-10-06T16:04:05Z"
    },
    "metadata": {
      "arxiv_id": "2410.04534",
      "title": "UniMuMo: Unified Text, Music and Motion Generation",
      "summary": "We introduce UniMuMo, a unified multimodal model capable of taking arbitrary text, music, and motion data as input conditions to generate outputs across all three modalities. To address the lack of time-synchronized data, we align unpaired music and motion data based on rhythmic patterns to leverage existing large-scale music-only and motion-only datasets. By converting music, motion, and text into token-based representation, our model bridges these modalities through a unified encoder-decoder transformer architecture. To support multiple generation tasks within a single framework, we introduce several architectural improvements. We propose encoding motion with a music codebook, mapping motion into the same feature space as music. We introduce a music-motion parallel generation scheme that unifies all music and motion generation tasks into a single transformer decoder architecture with a single training task of music-motion joint generation. Moreover, the model is designed by fine-tuning existing pre-trained single-modality models, significantly reducing computational demands. Extensive experiments demonstrate that UniMuMo achieves competitive results on all unidirectional generation benchmarks across music, motion, and text modalities. Quantitative results are available in the \\href{https://hanyangclarence.github.io/unimumo_demo/}{project page}.",
      "authors": [
        "Han Yang",
        "Kun Su",
        "Yutong Zhang",
        "Jiaben Chen",
        "Kaizhi Qian",
        "Gaowen Liu",
        "Chuang Gan"
      ],
      "published": "2024-10-06T16:04:05Z",
      "updated": "2024-10-06T16:04:05Z",
      "categories": [
        "cs.SD",
        "cs.CV",
        "cs.GR",
        "cs.LG",
        "cs.MM",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2410.04534v1",
      "landing_url": "https://arxiv.org/abs/2410.04534v1",
      "doi": "https://doi.org/10.48550/arXiv.2410.04534"
    },
    "queries": [
      {
        "anchor": "synchronous dialogue",
        "search_term": "synchronous data"
      }
    ]
  },
  {
    "arxiv_id": "2410.05985",
    "anchor": "spoken language models",
    "search_term": "synchronous data",
    "search_record": {
      "id": "http://arxiv.org/abs/2410.05985v3",
      "title": "Asynchronous Stochastic Gradient Descent with Decoupled Backpropagation and Layer-Wise Updates",
      "summary": "The increasing size of deep learning models has made distributed training across multiple devices essential. However, current methods such as distributed data-parallel training suffer from large communication and synchronization overheads when training across devices, leading to longer training times as a result of suboptimal hardware utilization. Asynchronous stochastic gradient descent (ASGD) methods can improve training speed, but are sensitive to delays due to both communication and differences throughput. Moreover, the backpropagation algorithm used within ASGD workers is bottlenecked by the interlocking between its forward and backward passes. Current methods also do not take advantage of the large differences in the computation required for the forward and backward passes. Therefore, we propose an extension to ASGD called Partial Decoupled ASGD (PD-ASGD) that addresses these issues. PD-ASGD uses separate threads for the forward and backward passes, decoupling the updates and allowing for a higher ratio of forward to backward threads than the usual 1:1 ratio, leading to higher throughput. PD-ASGD also performs layer-wise (partial) model updates concurrently across multiple threads. This reduces parameter staleness and consequently improves robustness to delays. Our approach yields close to state-of-the-art results while running up to $5.95\\times$ faster than synchronous data parallelism in the presence of delays, and up to $2.14\\times$ times faster than comparable ASGD algorithms by achieving higher model flops utilization. We mathematically describe the gradient bias introduced by our method, establish an upper bound, and prove convergence.",
      "published": "2024-10-08T12:32:36Z"
    },
    "metadata": {
      "arxiv_id": "2410.05985",
      "title": "Asynchronous Stochastic Gradient Descent with Decoupled Backpropagation and Layer-Wise Updates",
      "summary": "The increasing size of deep learning models has made distributed training across multiple devices essential. However, current methods such as distributed data-parallel training suffer from large communication and synchronization overheads when training across devices, leading to longer training times as a result of suboptimal hardware utilization. Asynchronous stochastic gradient descent (ASGD) methods can improve training speed, but are sensitive to delays due to both communication and differences throughput. Moreover, the backpropagation algorithm used within ASGD workers is bottlenecked by the interlocking between its forward and backward passes. Current methods also do not take advantage of the large differences in the computation required for the forward and backward passes. Therefore, we propose an extension to ASGD called Partial Decoupled ASGD (PD-ASGD) that addresses these issues. PD-ASGD uses separate threads for the forward and backward passes, decoupling the updates and allowing for a higher ratio of forward to backward threads than the usual 1:1 ratio, leading to higher throughput. PD-ASGD also performs layer-wise (partial) model updates concurrently across multiple threads. This reduces parameter staleness and consequently improves robustness to delays. Our approach yields close to state-of-the-art results while running up to $5.95\\times$ faster than synchronous data parallelism in the presence of delays, and up to $2.14\\times$ times faster than comparable ASGD algorithms by achieving higher model flops utilization. We mathematically describe the gradient bias introduced by our method, establish an upper bound, and prove convergence.",
      "authors": [
        "Cabrel Teguemne Fokam",
        "Khaleelulla Khan Nazeer",
        "Lukas König",
        "David Kappel",
        "Anand Subramoney"
      ],
      "published": "2024-10-08T12:32:36Z",
      "updated": "2025-02-07T13:33:12Z",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.NE"
      ],
      "pdf_url": "https://arxiv.org/pdf/2410.05985v3",
      "landing_url": "https://arxiv.org/abs/2410.05985v3",
      "doi": "https://doi.org/10.48550/arXiv.2410.05985"
    },
    "queries": [
      {
        "anchor": "spoken language models",
        "search_term": "synchronous data"
      },
      {
        "anchor": "synchronous dialogue",
        "search_term": "synchronous data"
      }
    ]
  },
  {
    "arxiv_id": "2410.05997",
    "anchor": "synchronous dialogue",
    "search_term": "audio tokens",
    "search_record": {
      "id": "http://arxiv.org/abs/2410.05997v1",
      "title": "An Eye for an Ear: Zero-shot Audio Description Leveraging an Image Captioner using Audiovisual Distribution Alignment",
      "summary": "Multimodal large language models have fueled progress in image captioning. These models, fine-tuned on vast image datasets, exhibit a deep understanding of semantic concepts. In this work, we show that this ability can be re-purposed for audio captioning, where the joint image-language decoder can be leveraged to describe auditory content associated with image sequences within videos featuring audiovisual content. This can be achieved via multimodal alignment. Yet, this multimodal alignment task is non-trivial due to the inherent disparity between audible and visible elements in real-world videos. Moreover, multimodal representation learning often relies on contrastive learning, facing the challenge of the so-called modality gap which hinders smooth integration between modalities. In this work, we introduce a novel methodology for bridging the audiovisual modality gap by matching the distributions of tokens produced by an audio backbone and those of an image captioner. Our approach aligns the audio token distribution with that of the image tokens, enabling the model to perform zero-shot audio captioning in an unsupervised fashion while keeping the initial image captioning component unaltered. This alignment allows for the use of either audio or audiovisual input by combining or substituting the image encoder with the aligned audio encoder. Our method achieves significantly improved performances in zero-shot audio captioning, compared to existing approaches.",
      "published": "2024-10-08T12:52:48Z"
    },
    "metadata": {
      "arxiv_id": "2410.05997",
      "title": "An Eye for an Ear: Zero-shot Audio Description Leveraging an Image Captioner using Audiovisual Distribution Alignment",
      "summary": "Multimodal large language models have fueled progress in image captioning. These models, fine-tuned on vast image datasets, exhibit a deep understanding of semantic concepts. In this work, we show that this ability can be re-purposed for audio captioning, where the joint image-language decoder can be leveraged to describe auditory content associated with image sequences within videos featuring audiovisual content. This can be achieved via multimodal alignment. Yet, this multimodal alignment task is non-trivial due to the inherent disparity between audible and visible elements in real-world videos. Moreover, multimodal representation learning often relies on contrastive learning, facing the challenge of the so-called modality gap which hinders smooth integration between modalities. In this work, we introduce a novel methodology for bridging the audiovisual modality gap by matching the distributions of tokens produced by an audio backbone and those of an image captioner. Our approach aligns the audio token distribution with that of the image tokens, enabling the model to perform zero-shot audio captioning in an unsupervised fashion while keeping the initial image captioning component unaltered. This alignment allows for the use of either audio or audiovisual input by combining or substituting the image encoder with the aligned audio encoder. Our method achieves significantly improved performances in zero-shot audio captioning, compared to existing approaches.",
      "authors": [
        "Hugo Malard",
        "Michel Olvera",
        "Stéphane Lathuiliere",
        "Slim Essid"
      ],
      "published": "2024-10-08T12:52:48Z",
      "updated": "2024-10-08T12:52:48Z",
      "categories": [
        "eess.AS",
        "cs.CV",
        "cs.LG",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2410.05997v1",
      "landing_url": "https://arxiv.org/abs/2410.05997v1",
      "doi": "https://doi.org/10.48550/arXiv.2410.05997"
    },
    "queries": [
      {
        "anchor": "synchronous dialogue",
        "search_term": "audio tokens"
      }
    ]
  },
  {
    "arxiv_id": "2410.06934",
    "anchor": "spoken language models",
    "search_term": "modular architecture",
    "search_record": {
      "id": "http://arxiv.org/abs/2410.06934v1",
      "title": "VEC-Sim: A Simulation Platform for Evaluating Service Caching and Computation Offloading Policies in Vehicular Edge Networks",
      "summary": "Computer simulation platforms offer an alternative solution by emulating complex systems in a controlled manner. However, existing Edge Computing (EC) simulators, as well as general-purpose vehicular network simulators, are not tailored for VEC and lack dedicated support for modeling the distinct access pattern, entity mobility trajectory and other unique characteristics of VEC networks. To fill this gap, this paper proposes VEC-Sim, a versatile simulation platform for in-depth evaluation and analysis of various service caching and computation offloading policies in VEC networks. VEC-Sim incorporates realistic mechanisms to replicate real-world access patterns, including service feature vector, vehicle mobility modeling, evolving service popularity, new service upload and user preference shifts, etc. Moreover, its modular architecture and extensive Application Programming Interfaces (APIs) allow seamless integration of customized scheduling policies and user-defined metrics. A comprehensive evaluation of VEC-Sim's capabilities is undertaken in comparison to real-world ground truths. Results prove it to be accurate in reproducing classical scheduling algorithms and extremely effective in conducting case studies.",
      "published": "2024-10-09T14:28:59Z"
    },
    "metadata": {
      "arxiv_id": "2410.06934",
      "title": "VEC-Sim: A Simulation Platform for Evaluating Service Caching and Computation Offloading Policies in Vehicular Edge Networks",
      "summary": "Computer simulation platforms offer an alternative solution by emulating complex systems in a controlled manner. However, existing Edge Computing (EC) simulators, as well as general-purpose vehicular network simulators, are not tailored for VEC and lack dedicated support for modeling the distinct access pattern, entity mobility trajectory and other unique characteristics of VEC networks. To fill this gap, this paper proposes VEC-Sim, a versatile simulation platform for in-depth evaluation and analysis of various service caching and computation offloading policies in VEC networks. VEC-Sim incorporates realistic mechanisms to replicate real-world access patterns, including service feature vector, vehicle mobility modeling, evolving service popularity, new service upload and user preference shifts, etc. Moreover, its modular architecture and extensive Application Programming Interfaces (APIs) allow seamless integration of customized scheduling policies and user-defined metrics. A comprehensive evaluation of VEC-Sim's capabilities is undertaken in comparison to real-world ground truths. Results prove it to be accurate in reproducing classical scheduling algorithms and extremely effective in conducting case studies.",
      "authors": [
        "Fan Wu",
        "Xiaolong Xu",
        "Muhammad Bilal",
        "Xiangwei Wang",
        "Hao Cheng",
        "Siyu Wu"
      ],
      "published": "2024-10-09T14:28:59Z",
      "updated": "2024-10-09T14:28:59Z",
      "categories": [
        "cs.NI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2410.06934v1",
      "landing_url": "https://arxiv.org/abs/2410.06934v1",
      "doi": "https://doi.org/10.1016/j.comnet.2024.110985"
    },
    "queries": [
      {
        "anchor": "spoken language models",
        "search_term": "modular architecture"
      },
      {
        "anchor": "synchronous dialogue",
        "search_term": "modular architecture"
      }
    ]
  },
  {
    "arxiv_id": "2410.07168",
    "anchor": "spoken language models",
    "search_term": "true full-duplex",
    "search_record": {
      "id": "http://arxiv.org/abs/2410.07168v2",
      "title": "Sylber: Syllabic Embedding Representation of Speech from Raw Audio",
      "summary": "Syllables are compositional units of spoken language that efficiently structure human speech perception and production. However, current neural speech representations lack such structure, resulting in dense token sequences that are costly to process. To bridge this gap, we propose a new model, Sylber, that produces speech representations with clean and robust syllabic structure. Specifically, we propose a self-supervised learning (SSL) framework that bootstraps syllabic embeddings by distilling from its own initial unsupervised syllabic segmentation. This results in a highly structured representation of speech features, offering three key benefits: 1) a fast, linear-time syllable segmentation algorithm, 2) efficient syllabic tokenization with an average of 4.27 tokens per second, and 3) novel phonological units suited for efficient spoken language modeling. Our proposed segmentation method is highly robust and generalizes to out-of-domain data and unseen languages without any tuning. By training token-to-speech generative models, fully intelligible speech can be reconstructed from Sylber tokens with a significantly lower bitrate than baseline SSL tokens. This suggests that our model effectively compresses speech into a compact sequence of tokens with minimal information loss. Lastly, we demonstrate that categorical perception-a linguistic phenomenon in speech perception-emerges naturally in Sylber, making the embedding space more categorical and sparse than previous speech features and thus supporting the high efficiency of our tokenization. Together, we present a novel SSL approach for representing speech as syllables, with significant potential for efficient speech tokenization and spoken language modeling.",
      "published": "2024-10-09T17:59:04Z"
    },
    "metadata": {
      "arxiv_id": "2410.07168",
      "title": "Sylber: Syllabic Embedding Representation of Speech from Raw Audio",
      "summary": "Syllables are compositional units of spoken language that efficiently structure human speech perception and production. However, current neural speech representations lack such structure, resulting in dense token sequences that are costly to process. To bridge this gap, we propose a new model, Sylber, that produces speech representations with clean and robust syllabic structure. Specifically, we propose a self-supervised learning (SSL) framework that bootstraps syllabic embeddings by distilling from its own initial unsupervised syllabic segmentation. This results in a highly structured representation of speech features, offering three key benefits: 1) a fast, linear-time syllable segmentation algorithm, 2) efficient syllabic tokenization with an average of 4.27 tokens per second, and 3) novel phonological units suited for efficient spoken language modeling. Our proposed segmentation method is highly robust and generalizes to out-of-domain data and unseen languages without any tuning. By training token-to-speech generative models, fully intelligible speech can be reconstructed from Sylber tokens with a significantly lower bitrate than baseline SSL tokens. This suggests that our model effectively compresses speech into a compact sequence of tokens with minimal information loss. Lastly, we demonstrate that categorical perception-a linguistic phenomenon in speech perception-emerges naturally in Sylber, making the embedding space more categorical and sparse than previous speech features and thus supporting the high efficiency of our tokenization. Together, we present a novel SSL approach for representing speech as syllables, with significant potential for efficient speech tokenization and spoken language modeling.",
      "authors": [
        "Cheol Jun Cho",
        "Nicholas Lee",
        "Akshat Gupta",
        "Dhruv Agarwal",
        "Ethan Chen",
        "Alan W Black",
        "Gopala K. Anumanchipalli"
      ],
      "published": "2024-10-09T17:59:04Z",
      "updated": "2025-03-02T09:16:05Z",
      "categories": [
        "cs.CL",
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2410.07168v2",
      "landing_url": "https://arxiv.org/abs/2410.07168v2",
      "doi": "https://doi.org/10.48550/arXiv.2410.07168"
    },
    "queries": [
      {
        "anchor": "spoken language models",
        "search_term": "true full-duplex"
      },
      {
        "anchor": "spoken language models",
        "search_term": "pseudo full-duplex"
      },
      {
        "anchor": "spoken language models",
        "search_term": "engineered synchronization"
      },
      {
        "anchor": "spoken language models",
        "search_term": "learned synchronization"
      },
      {
        "anchor": "spoken language models",
        "search_term": "modular architecture"
      },
      {
        "anchor": "spoken language models",
        "search_term": "streaming architecture"
      },
      {
        "anchor": "spoken language models",
        "search_term": "real-time interaction"
      },
      {
        "anchor": "spoken language models",
        "search_term": "synchronous data"
      },
      {
        "anchor": "spoken language models",
        "search_term": "multichannel dialogue"
      },
      {
        "anchor": "spoken language models",
        "search_term": "behavioral arbitration"
      },
      {
        "anchor": "spoken language models",
        "search_term": "semantic coherence"
      },
      {
        "anchor": "spoken language models",
        "search_term": "floor transfer offset"
      },
      {
        "anchor": "spoken language models",
        "search_term": "stop latency"
      },
      {
        "anchor": "spoken language models",
        "search_term": "interruption response delay"
      }
    ]
  },
  {
    "arxiv_id": "2410.08325",
    "anchor": "spoken language models",
    "search_term": "audio tokens",
    "search_record": {
      "id": "http://arxiv.org/abs/2410.08325v1",
      "title": "Low Bitrate High-Quality RVQGAN-based Discrete Speech Tokenizer",
      "summary": "Discrete Audio codecs (or audio tokenizers) have recently regained interest due to the ability of Large Language Models (LLMs) to learn their compressed acoustic representations. Various publicly available trainable discrete tokenizers recently demonstrated impressive results for audio tokenization, yet they mostly require high token rates to gain high-quality reconstruction. In this study, we fine-tuned an open-source general audio RVQGAN model using diverse open-source speech data, considering various recording conditions and quality levels. The resulting wideband (24kHz) speech-only model achieves speech reconstruction, which is nearly indistinguishable from PCM (pulse-code modulation) with a rate of 150-300 tokens per second (1500-3000 bps). The evaluation used comprehensive English speech data encompassing different recording conditions, including studio settings. Speech samples are made publicly available in http://ibm.biz/IS24SpeechRVQ . The model is officially released in https://huggingface.co/ibm/DAC.speech.v1.0",
      "published": "2024-10-10T19:29:05Z"
    },
    "metadata": {
      "arxiv_id": "2410.08325",
      "title": "Low Bitrate High-Quality RVQGAN-based Discrete Speech Tokenizer",
      "summary": "Discrete Audio codecs (or audio tokenizers) have recently regained interest due to the ability of Large Language Models (LLMs) to learn their compressed acoustic representations. Various publicly available trainable discrete tokenizers recently demonstrated impressive results for audio tokenization, yet they mostly require high token rates to gain high-quality reconstruction. In this study, we fine-tuned an open-source general audio RVQGAN model using diverse open-source speech data, considering various recording conditions and quality levels. The resulting wideband (24kHz) speech-only model achieves speech reconstruction, which is nearly indistinguishable from PCM (pulse-code modulation) with a rate of 150-300 tokens per second (1500-3000 bps). The evaluation used comprehensive English speech data encompassing different recording conditions, including studio settings. Speech samples are made publicly available in http://ibm.biz/IS24SpeechRVQ . The model is officially released in https://huggingface.co/ibm/DAC.speech.v1.0",
      "authors": [
        "Slava Shechtman",
        "Avihu Dekel"
      ],
      "published": "2024-10-10T19:29:05Z",
      "updated": "2024-10-10T19:29:05Z",
      "categories": [
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2410.08325v1",
      "landing_url": "https://arxiv.org/abs/2410.08325v1",
      "doi": "https://doi.org/10.21437/Interspeech.2024-2366"
    },
    "queries": [
      {
        "anchor": "spoken language models",
        "search_term": "audio tokens"
      },
      {
        "anchor": "synchronous dialogue",
        "search_term": "audio tokens"
      },
      {
        "anchor": "turn-taking",
        "search_term": "audio tokens"
      }
    ]
  },
  {
    "arxiv_id": "2410.09307",
    "anchor": "synchronous dialogue",
    "search_term": "modular architecture",
    "search_record": {
      "id": "http://arxiv.org/abs/2410.09307v1",
      "title": "Graph Neural Alchemist: An innovative fully modular architecture for time series-to-graph classification",
      "summary": "This paper introduces a novel Graph Neural Network (GNN) architecture for time series classification, based on visibility graph representations. Traditional time series classification methods often struggle with high computational complexity and inadequate capture of spatio-temporal dynamics. By representing time series as visibility graphs, it is possible to encode both spatial and temporal dependencies inherent to time series data, while being computationally efficient. Our architecture is fully modular, enabling flexible experimentation with different models and representations. We employ directed visibility graphs encoded with in-degree and PageRank features to improve the representation of time series, ensuring efficient computation while enhancing the model's ability to capture long-range dependencies in the data. We show the robustness and generalization capability of the proposed architecture across a diverse set of classification tasks and against a traditional model. Our work represents a significant advancement in the application of GNNs for time series analysis, offering a powerful and flexible framework for future research and practical implementations.",
      "published": "2024-10-12T00:03:40Z"
    },
    "metadata": {
      "arxiv_id": "2410.09307",
      "title": "Graph Neural Alchemist: An innovative fully modular architecture for time series-to-graph classification",
      "summary": "This paper introduces a novel Graph Neural Network (GNN) architecture for time series classification, based on visibility graph representations. Traditional time series classification methods often struggle with high computational complexity and inadequate capture of spatio-temporal dynamics. By representing time series as visibility graphs, it is possible to encode both spatial and temporal dependencies inherent to time series data, while being computationally efficient. Our architecture is fully modular, enabling flexible experimentation with different models and representations. We employ directed visibility graphs encoded with in-degree and PageRank features to improve the representation of time series, ensuring efficient computation while enhancing the model's ability to capture long-range dependencies in the data. We show the robustness and generalization capability of the proposed architecture across a diverse set of classification tasks and against a traditional model. Our work represents a significant advancement in the application of GNNs for time series analysis, offering a powerful and flexible framework for future research and practical implementations.",
      "authors": [
        "Paulo Coelho",
        "Raul Araju",
        "Luís Ramos",
        "Samir Saliba",
        "Renato Vimieiro"
      ],
      "published": "2024-10-12T00:03:40Z",
      "updated": "2024-10-12T00:03:40Z",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2410.09307v1",
      "landing_url": "https://arxiv.org/abs/2410.09307v1",
      "doi": "https://doi.org/10.48550/arXiv.2410.09307"
    },
    "queries": [
      {
        "anchor": "synchronous dialogue",
        "search_term": "modular architecture"
      }
    ]
  },
  {
    "arxiv_id": "2410.10812",
    "anchor": "synchronous dialogue",
    "search_term": "discrete tokens",
    "search_record": {
      "id": "http://arxiv.org/abs/2410.10812v1",
      "title": "HART: Efficient Visual Generation with Hybrid Autoregressive Transformer",
      "summary": "We introduce Hybrid Autoregressive Transformer (HART), an autoregressive (AR) visual generation model capable of directly generating 1024x1024 images, rivaling diffusion models in image generation quality. Existing AR models face limitations due to the poor image reconstruction quality of their discrete tokenizers and the prohibitive training costs associated with generating 1024px images. To address these challenges, we present the hybrid tokenizer, which decomposes the continuous latents from the autoencoder into two components: discrete tokens representing the big picture and continuous tokens representing the residual components that cannot be represented by the discrete tokens. The discrete component is modeled by a scalable-resolution discrete AR model, while the continuous component is learned with a lightweight residual diffusion module with only 37M parameters. Compared with the discrete-only VAR tokenizer, our hybrid approach improves reconstruction FID from 2.11 to 0.30 on MJHQ-30K, leading to a 31% generation FID improvement from 7.85 to 5.38. HART also outperforms state-of-the-art diffusion models in both FID and CLIP score, with 4.5-7.7x higher throughput and 6.9-13.4x lower MACs. Our code is open sourced at https://github.com/mit-han-lab/hart.",
      "published": "2024-10-14T17:59:42Z"
    },
    "metadata": {
      "arxiv_id": "2410.10812",
      "title": "HART: Efficient Visual Generation with Hybrid Autoregressive Transformer",
      "summary": "We introduce Hybrid Autoregressive Transformer (HART), an autoregressive (AR) visual generation model capable of directly generating 1024x1024 images, rivaling diffusion models in image generation quality. Existing AR models face limitations due to the poor image reconstruction quality of their discrete tokenizers and the prohibitive training costs associated with generating 1024px images. To address these challenges, we present the hybrid tokenizer, which decomposes the continuous latents from the autoencoder into two components: discrete tokens representing the big picture and continuous tokens representing the residual components that cannot be represented by the discrete tokens. The discrete component is modeled by a scalable-resolution discrete AR model, while the continuous component is learned with a lightweight residual diffusion module with only 37M parameters. Compared with the discrete-only VAR tokenizer, our hybrid approach improves reconstruction FID from 2.11 to 0.30 on MJHQ-30K, leading to a 31% generation FID improvement from 7.85 to 5.38. HART also outperforms state-of-the-art diffusion models in both FID and CLIP score, with 4.5-7.7x higher throughput and 6.9-13.4x lower MACs. Our code is open sourced at https://github.com/mit-han-lab/hart.",
      "authors": [
        "Haotian Tang",
        "Yecheng Wu",
        "Shang Yang",
        "Enze Xie",
        "Junsong Chen",
        "Junyu Chen",
        "Zhuoyang Zhang",
        "Han Cai",
        "Yao Lu",
        "Song Han"
      ],
      "published": "2024-10-14T17:59:42Z",
      "updated": "2024-10-14T17:59:42Z",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2410.10812v1",
      "landing_url": "https://arxiv.org/abs/2410.10812v1",
      "doi": "https://doi.org/10.48550/arXiv.2410.10812"
    },
    "queries": [
      {
        "anchor": "synchronous dialogue",
        "search_term": "discrete tokens"
      }
    ]
  },
  {
    "arxiv_id": "2410.11025",
    "anchor": "synchronous dialogue",
    "search_term": "neural codec",
    "search_record": {
      "id": "http://arxiv.org/abs/2410.11025v2",
      "title": "Code Drift: Towards Idempotent Neural Audio Codecs",
      "summary": "Neural codecs have demonstrated strong performance in high-fidelity compression of audio signals at low bitrates. The token-based representations produced by these codecs have proven particularly useful for generative modeling. While much research has focused on improvements in compression ratio and perceptual transparency, recent works have largely overlooked another desirable codec property -- idempotence, the stability of compressed outputs under multiple rounds of encoding. We find that state-of-the-art neural codecs exhibit varied degrees of idempotence, with some degrading audio outputs significantly after as few as three encodings. We investigate possible causes of low idempotence and devise a method for improving idempotence through fine-tuning a codec model. We then examine the effect of idempotence on a simple conditional generative modeling task, and find that increased idempotence can be achieved without negatively impacting downstream modeling performance -- potentially extending the usefulness of neural codecs for practical file compression and iterative generative modeling workflows.",
      "published": "2024-10-14T19:21:28Z"
    },
    "metadata": {
      "arxiv_id": "2410.11025",
      "title": "Code Drift: Towards Idempotent Neural Audio Codecs",
      "summary": "Neural codecs have demonstrated strong performance in high-fidelity compression of audio signals at low bitrates. The token-based representations produced by these codecs have proven particularly useful for generative modeling. While much research has focused on improvements in compression ratio and perceptual transparency, recent works have largely overlooked another desirable codec property -- idempotence, the stability of compressed outputs under multiple rounds of encoding. We find that state-of-the-art neural codecs exhibit varied degrees of idempotence, with some degrading audio outputs significantly after as few as three encodings. We investigate possible causes of low idempotence and devise a method for improving idempotence through fine-tuning a codec model. We then examine the effect of idempotence on a simple conditional generative modeling task, and find that increased idempotence can be achieved without negatively impacting downstream modeling performance -- potentially extending the usefulness of neural codecs for practical file compression and iterative generative modeling workflows.",
      "authors": [
        "Patrick O'Reilly",
        "Prem Seetharaman",
        "Jiaqi Su",
        "Zeyu Jin",
        "Bryan Pardo"
      ],
      "published": "2024-10-14T19:21:28Z",
      "updated": "2025-04-14T23:07:19Z",
      "categories": [
        "eess.AS",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2410.11025v2",
      "landing_url": "https://arxiv.org/abs/2410.11025v2",
      "doi": "https://doi.org/10.1109/ICASSP49660.2025.10890096"
    },
    "queries": [
      {
        "anchor": "synchronous dialogue",
        "search_term": "neural codec"
      }
    ]
  },
  {
    "arxiv_id": "2410.11339",
    "anchor": "spoken language models",
    "search_term": "synchronous data",
    "search_record": {
      "id": "http://arxiv.org/abs/2410.11339v2",
      "title": "EEG-based 90-Degree Turn Intention Detection for Brain-Computer Interface",
      "summary": "Electroencephalography (EEG)--based turn intention prediction for lower limb movement is important to build an efficient brain-computer interface (BCI) system. This study investigates the feasibility of intention detection of left-turn, right-turn, and straight walk by utilizing EEG signals obtained before the event occurrence. Synchronous data was collected using 31-channel EEG and IMU-based motion capture systems for nine healthy participants while performing left-turn, right-turn, and straight walk movements. EEG data was preprocessed with steps including Artifact Subspace Reconstruction (ASR), re-referencing, and Independent Component Analysis (ICA) to remove data noise. Feature extraction from the preprocessed EEG data involved computing various statistical measures (mean, median, standard deviation, skew, and kurtosis), and Hjorth parameters (activity, mobility, and complexity). Further, the feature selection was performed using the Random forest algorithm for the dimensionality reduction. The feature set obtained was utilized for 3-class classification using XG boost, gradient boosting, and support vector machine (SVM) with RBF kernel classifiers in a five-fold cross-validation scheme. Using the proposed intention detection methodology, the SVM classifier using an EEG window of 1.5 s and 0 s time-lag has the best decoding performance with mean accuracy, precision, and recall of 81.23%, 85.35%, and 83.92%, respectively, across the nine participants. The decoding analysis shows the feasibility of turn intention prediction for lower limb movement using the EEG signal before the event onset.",
      "published": "2024-10-15T07:12:06Z"
    },
    "metadata": {
      "arxiv_id": "2410.11339",
      "title": "EEG-based 90-Degree Turn Intention Detection for Brain-Computer Interface",
      "summary": "Electroencephalography (EEG)--based turn intention prediction for lower limb movement is important to build an efficient brain-computer interface (BCI) system. This study investigates the feasibility of intention detection of left-turn, right-turn, and straight walk by utilizing EEG signals obtained before the event occurrence. Synchronous data was collected using 31-channel EEG and IMU-based motion capture systems for nine healthy participants while performing left-turn, right-turn, and straight walk movements. EEG data was preprocessed with steps including Artifact Subspace Reconstruction (ASR), re-referencing, and Independent Component Analysis (ICA) to remove data noise. Feature extraction from the preprocessed EEG data involved computing various statistical measures (mean, median, standard deviation, skew, and kurtosis), and Hjorth parameters (activity, mobility, and complexity). Further, the feature selection was performed using the Random forest algorithm for the dimensionality reduction. The feature set obtained was utilized for 3-class classification using XG boost, gradient boosting, and support vector machine (SVM) with RBF kernel classifiers in a five-fold cross-validation scheme. Using the proposed intention detection methodology, the SVM classifier using an EEG window of 1.5 s and 0 s time-lag has the best decoding performance with mean accuracy, precision, and recall of 81.23%, 85.35%, and 83.92%, respectively, across the nine participants. The decoding analysis shows the feasibility of turn intention prediction for lower limb movement using the EEG signal before the event onset.",
      "authors": [
        "Pradyot Anand",
        "Anant Jain",
        "Suriya Prakash Muthukrishnan",
        "Shubhendu Bhasin",
        "Sitikantha Roy",
        "Lalan Kumar"
      ],
      "published": "2024-10-15T07:12:06Z",
      "updated": "2025-02-04T09:45:10Z",
      "categories": [
        "eess.SP"
      ],
      "pdf_url": "https://arxiv.org/pdf/2410.11339v2",
      "landing_url": "https://arxiv.org/abs/2410.11339v2",
      "doi": "https://doi.org/10.48550/arXiv.2410.11339"
    },
    "queries": [
      {
        "anchor": "spoken language models",
        "search_term": "synchronous data"
      },
      {
        "anchor": "synchronous dialogue",
        "search_term": "synchronous data"
      }
    ]
  },
  {
    "arxiv_id": "2410.11998",
    "anchor": "synchronous dialogue",
    "search_term": "synchronous data",
    "search_record": {
      "id": "http://arxiv.org/abs/2410.11998v1",
      "title": "From promise to practice: realizing high-performance decentralized training",
      "summary": "Decentralized training of deep neural networks has attracted significant attention for its theoretically superior scalability over synchronous data-parallel methods like All-Reduce. However, realizing this potential in multi-node training is challenging due to the complex design space that involves communication topologies, computation patterns, and optimization algorithms. This paper identifies three key factors that can lead to speedups over All-Reduce training and constructs a runtime model to determine when, how, and to what degree decentralization can yield shorter per-iteration runtimes. Furthermore, to support the decentralized training of transformer-based models, we study a decentralized Adam algorithm that allows for overlapping communications and computations, prove its convergence, and propose an accumulation technique to mitigate the high variance caused by small local batch sizes. We deploy the proposed approach in clusters with up to 64 GPUs and demonstrate its practicality and advantages in both runtime and generalization performance under a fixed iteration budget.",
      "published": "2024-10-15T19:04:56Z"
    },
    "metadata": {
      "arxiv_id": "2410.11998",
      "title": "From promise to practice: realizing high-performance decentralized training",
      "summary": "Decentralized training of deep neural networks has attracted significant attention for its theoretically superior scalability over synchronous data-parallel methods like All-Reduce. However, realizing this potential in multi-node training is challenging due to the complex design space that involves communication topologies, computation patterns, and optimization algorithms. This paper identifies three key factors that can lead to speedups over All-Reduce training and constructs a runtime model to determine when, how, and to what degree decentralization can yield shorter per-iteration runtimes. Furthermore, to support the decentralized training of transformer-based models, we study a decentralized Adam algorithm that allows for overlapping communications and computations, prove its convergence, and propose an accumulation technique to mitigate the high variance caused by small local batch sizes. We deploy the proposed approach in clusters with up to 64 GPUs and demonstrate its practicality and advantages in both runtime and generalization performance under a fixed iteration budget.",
      "authors": [
        "Zesen Wang",
        "Jiaojiao Zhang",
        "Xuyang Wu",
        "Mikael Johansson"
      ],
      "published": "2024-10-15T19:04:56Z",
      "updated": "2024-10-15T19:04:56Z",
      "categories": [
        "cs.DC",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2410.11998v1",
      "landing_url": "https://arxiv.org/abs/2410.11998v1",
      "doi": "https://doi.org/10.48550/arXiv.2410.11998"
    },
    "queries": [
      {
        "anchor": "synchronous dialogue",
        "search_term": "synchronous data"
      }
    ]
  },
  {
    "arxiv_id": "2410.13798",
    "anchor": "synchronous dialogue",
    "search_term": "discrete tokens",
    "search_record": {
      "id": "http://arxiv.org/abs/2410.13798v2",
      "title": "Learning Graph Quantized Tokenizers",
      "summary": "Transformers serve as the backbone architectures of Foundational Models, where domain-specific tokenizers allow them to adapt to various domains. Graph Transformers (GTs) have recently emerged as leading models in geometric deep learning, outperforming Graph Neural Networks (GNNs) in various graph learning tasks. However, the development of tokenizers for graphs has lagged behind other modalities. To address this, we introduce GQT (\\textbf{G}raph \\textbf{Q}uantized \\textbf{T}okenizer), which decouples tokenizer training from Transformer training by leveraging multi-task graph self-supervised learning, yielding robust and generalizable graph tokens. Furthermore, the GQT utilizes Residual Vector Quantization (RVQ) to learn hierarchical discrete tokens, resulting in significantly reduced memory requirements and improved generalization capabilities. By combining the GQT with token modulation, a Transformer encoder achieves state-of-the-art performance on 20 out of 22 benchmarks, including large-scale homophilic and heterophilic datasets.",
      "published": "2024-10-17T17:38:24Z"
    },
    "metadata": {
      "arxiv_id": "2410.13798",
      "title": "Learning Graph Quantized Tokenizers",
      "summary": "Transformers serve as the backbone architectures of Foundational Models, where domain-specific tokenizers allow them to adapt to various domains. Graph Transformers (GTs) have recently emerged as leading models in geometric deep learning, outperforming Graph Neural Networks (GNNs) in various graph learning tasks. However, the development of tokenizers for graphs has lagged behind other modalities. To address this, we introduce GQT (\\textbf{G}raph \\textbf{Q}uantized \\textbf{T}okenizer), which decouples tokenizer training from Transformer training by leveraging multi-task graph self-supervised learning, yielding robust and generalizable graph tokens. Furthermore, the GQT utilizes Residual Vector Quantization (RVQ) to learn hierarchical discrete tokens, resulting in significantly reduced memory requirements and improved generalization capabilities. By combining the GQT with token modulation, a Transformer encoder achieves state-of-the-art performance on 20 out of 22 benchmarks, including large-scale homophilic and heterophilic datasets.",
      "authors": [
        "Limei Wang",
        "Kaveh Hassani",
        "Si Zhang",
        "Dongqi Fu",
        "Baichuan Yuan",
        "Weilin Cong",
        "Zhigang Hua",
        "Hao Wu",
        "Ning Yao",
        "Bo Long"
      ],
      "published": "2024-10-17T17:38:24Z",
      "updated": "2025-04-02T03:04:44Z",
      "categories": [
        "cs.NE",
        "cs.AI",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2410.13798v2",
      "landing_url": "https://arxiv.org/abs/2410.13798v2",
      "doi": "https://doi.org/10.48550/arXiv.2410.13798"
    },
    "queries": [
      {
        "anchor": "synchronous dialogue",
        "search_term": "discrete tokens"
      }
    ]
  },
  {
    "arxiv_id": "2410.16044",
    "anchor": "turn-taking",
    "search_term": "true full-duplex",
    "search_record": {
      "id": "http://arxiv.org/abs/2410.16044v1",
      "title": "Large Language Models Know What To Say But Not When To Speak",
      "summary": "Turn-taking is a fundamental mechanism in human communication that ensures smooth and coherent verbal interactions. Recent advances in Large Language Models (LLMs) have motivated their use in improving the turn-taking capabilities of Spoken Dialogue Systems (SDS), such as their ability to respond at appropriate times. However, existing models often struggle to predict opportunities for speaking -- called Transition Relevance Places (TRPs) -- in natural, unscripted conversations, focusing only on turn-final TRPs and not within-turn TRPs. To address these limitations, we introduce a novel dataset of participant-labeled within-turn TRPs and use it to evaluate the performance of state-of-the-art LLMs in predicting opportunities for speaking. Our experiments reveal the current limitations of LLMs in modeling unscripted spoken interactions, highlighting areas for improvement and paving the way for more naturalistic dialogue systems.",
      "published": "2024-10-21T14:20:25Z"
    },
    "metadata": {
      "arxiv_id": "2410.16044",
      "title": "Large Language Models Know What To Say But Not When To Speak",
      "summary": "Turn-taking is a fundamental mechanism in human communication that ensures smooth and coherent verbal interactions. Recent advances in Large Language Models (LLMs) have motivated their use in improving the turn-taking capabilities of Spoken Dialogue Systems (SDS), such as their ability to respond at appropriate times. However, existing models often struggle to predict opportunities for speaking -- called Transition Relevance Places (TRPs) -- in natural, unscripted conversations, focusing only on turn-final TRPs and not within-turn TRPs. To address these limitations, we introduce a novel dataset of participant-labeled within-turn TRPs and use it to evaluate the performance of state-of-the-art LLMs in predicting opportunities for speaking. Our experiments reveal the current limitations of LLMs in modeling unscripted spoken interactions, highlighting areas for improvement and paving the way for more naturalistic dialogue systems.",
      "authors": [
        "Muhammad Umair",
        "Vasanth Sarathy",
        "JP de Ruiter"
      ],
      "published": "2024-10-21T14:20:25Z",
      "updated": "2024-10-21T14:20:25Z",
      "categories": [
        "cs.CL"
      ],
      "pdf_url": "https://arxiv.org/pdf/2410.16044v1",
      "landing_url": "https://arxiv.org/abs/2410.16044v1",
      "doi": "https://doi.org/10.48550/arXiv.2410.16044"
    },
    "queries": [
      {
        "anchor": "turn-taking",
        "search_term": "true full-duplex"
      },
      {
        "anchor": "turn-taking",
        "search_term": "pseudo full-duplex"
      },
      {
        "anchor": "turn-taking",
        "search_term": "engineered synchronization"
      },
      {
        "anchor": "turn-taking",
        "search_term": "learned synchronization"
      },
      {
        "anchor": "turn-taking",
        "search_term": "modular architecture"
      },
      {
        "anchor": "turn-taking",
        "search_term": "streaming architecture"
      },
      {
        "anchor": "turn-taking",
        "search_term": "real-time interaction"
      },
      {
        "anchor": "turn-taking",
        "search_term": "synchronous data"
      },
      {
        "anchor": "turn-taking",
        "search_term": "multichannel dialogue"
      },
      {
        "anchor": "turn-taking",
        "search_term": "behavioral arbitration"
      },
      {
        "anchor": "turn-taking",
        "search_term": "semantic coherence"
      },
      {
        "anchor": "turn-taking",
        "search_term": "floor transfer offset"
      },
      {
        "anchor": "turn-taking",
        "search_term": "stop latency"
      },
      {
        "anchor": "turn-taking",
        "search_term": "interruption response delay"
      }
    ]
  },
  {
    "arxiv_id": "2410.17913",
    "anchor": "synchronous dialogue",
    "search_term": "data scarcity",
    "search_record": {
      "id": "http://arxiv.org/abs/2410.17913v1",
      "title": "Deep learning for model correction of dynamical systems with data scarcity",
      "summary": "We present a deep learning framework for correcting existing dynamical system models utilizing only a scarce high-fidelity data set. In many practical situations, one has a low-fidelity model that can capture the dynamics reasonably well but lacks high resolution, due to the inherent limitation of the model and the complexity of the underlying physics. When high resolution data become available, it is natural to seek model correction to improve the resolution of the model predictions. We focus on the case when the amount of high-fidelity data is so small that most of the existing data driven modeling methods cannot be applied. In this paper, we address these challenges with a model-correction method which only requires a scarce high-fidelity data set. Our method first seeks a deep neural network (DNN) model to approximate the existing low-fidelity model. By using the scarce high-fidelity data, the method then corrects the DNN model via transfer learning (TL). After TL, an improved DNN model with high prediction accuracy to the underlying dynamics is obtained. One distinct feature of the propose method is that it does not assume a specific form of the model correction terms. Instead, it offers an inherent correction to the low-fidelity model via TL. A set of numerical examples are presented to demonstrate the effectiveness of the proposed method.",
      "published": "2024-10-23T14:33:11Z"
    },
    "metadata": {
      "arxiv_id": "2410.17913",
      "title": "Deep learning for model correction of dynamical systems with data scarcity",
      "summary": "We present a deep learning framework for correcting existing dynamical system models utilizing only a scarce high-fidelity data set. In many practical situations, one has a low-fidelity model that can capture the dynamics reasonably well but lacks high resolution, due to the inherent limitation of the model and the complexity of the underlying physics. When high resolution data become available, it is natural to seek model correction to improve the resolution of the model predictions. We focus on the case when the amount of high-fidelity data is so small that most of the existing data driven modeling methods cannot be applied. In this paper, we address these challenges with a model-correction method which only requires a scarce high-fidelity data set. Our method first seeks a deep neural network (DNN) model to approximate the existing low-fidelity model. By using the scarce high-fidelity data, the method then corrects the DNN model via transfer learning (TL). After TL, an improved DNN model with high prediction accuracy to the underlying dynamics is obtained. One distinct feature of the propose method is that it does not assume a specific form of the model correction terms. Instead, it offers an inherent correction to the low-fidelity model via TL. A set of numerical examples are presented to demonstrate the effectiveness of the proposed method.",
      "authors": [
        "Caroline Tatsuoka",
        "Dongbin Xiu"
      ],
      "published": "2024-10-23T14:33:11Z",
      "updated": "2024-10-23T14:33:11Z",
      "categories": [
        "cs.LG",
        "math.DS",
        "stat.ML"
      ],
      "pdf_url": "https://arxiv.org/pdf/2410.17913v1",
      "landing_url": "https://arxiv.org/abs/2410.17913v1",
      "doi": "https://doi.org/10.48550/arXiv.2410.17913"
    },
    "queries": [
      {
        "anchor": "synchronous dialogue",
        "search_term": "data scarcity"
      }
    ]
  },
  {
    "arxiv_id": "2410.19820",
    "anchor": "spoken language models",
    "search_term": "data scarcity",
    "search_record": {
      "id": "http://arxiv.org/abs/2410.19820v1",
      "title": "Advancing Histopathology with Deep Learning Under Data Scarcity: A Decade in Review",
      "summary": "Recent years witnessed remarkable progress in computational histopathology, largely fueled by deep learning. This brought the clinical adoption of deep learning-based tools within reach, promising significant benefits to healthcare, offering a valuable second opinion on diagnoses, streamlining complex tasks, and mitigating the risks of inconsistency and bias in clinical decisions. However, a well-known challenge is that deep learning models may contain up to billions of parameters; supervising their training effectively would require vast labeled datasets to achieve reliable generalization and noise resilience. In medical imaging, particularly histopathology, amassing such extensive labeled data collections places additional demands on clinicians and incurs higher costs, which hinders the art's progress. Addressing this challenge, researchers devised various strategies for leveraging deep learning with limited data and annotation availability. In this paper, we present a comprehensive review of deep learning applications in histopathology, with a focus on the challenges posed by data scarcity over the past decade. We systematically categorize and compare various approaches, evaluate their distinct contributions using benchmarking tables, and highlight their respective advantages and limitations. Additionally, we address gaps in existing reviews and identify underexplored research opportunities, underscoring the potential for future advancements in this field.",
      "published": "2024-10-18T07:29:48Z"
    },
    "metadata": {
      "arxiv_id": "2410.19820",
      "title": "Advancing Histopathology with Deep Learning Under Data Scarcity: A Decade in Review",
      "summary": "Recent years witnessed remarkable progress in computational histopathology, largely fueled by deep learning. This brought the clinical adoption of deep learning-based tools within reach, promising significant benefits to healthcare, offering a valuable second opinion on diagnoses, streamlining complex tasks, and mitigating the risks of inconsistency and bias in clinical decisions. However, a well-known challenge is that deep learning models may contain up to billions of parameters; supervising their training effectively would require vast labeled datasets to achieve reliable generalization and noise resilience. In medical imaging, particularly histopathology, amassing such extensive labeled data collections places additional demands on clinicians and incurs higher costs, which hinders the art's progress. Addressing this challenge, researchers devised various strategies for leveraging deep learning with limited data and annotation availability. In this paper, we present a comprehensive review of deep learning applications in histopathology, with a focus on the challenges posed by data scarcity over the past decade. We systematically categorize and compare various approaches, evaluate their distinct contributions using benchmarking tables, and highlight their respective advantages and limitations. Additionally, we address gaps in existing reviews and identify underexplored research opportunities, underscoring the potential for future advancements in this field.",
      "authors": [
        "Ahmad Obeid",
        "Said Boumaraf",
        "Anabia Sohail",
        "Taimur Hassan",
        "Sajid Javed",
        "Jorge Dias",
        "Mohammed Bennamoun",
        "Naoufel Werghi"
      ],
      "published": "2024-10-18T07:29:48Z",
      "updated": "2024-10-18T07:29:48Z",
      "categories": [
        "eess.IV",
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2410.19820v1",
      "landing_url": "https://arxiv.org/abs/2410.19820v1",
      "doi": "https://doi.org/10.48550/arXiv.2410.19820"
    },
    "queries": [
      {
        "anchor": "spoken language models",
        "search_term": "data scarcity"
      },
      {
        "anchor": "synchronous dialogue",
        "search_term": "data scarcity"
      },
      {
        "anchor": "turn-taking",
        "search_term": "data scarcity"
      }
    ]
  },
  {
    "arxiv_id": "2410.20256",
    "anchor": "synchronous dialogue",
    "search_term": "streaming architecture",
    "search_record": {
      "id": "http://arxiv.org/abs/2410.20256v1",
      "title": "That was not what I was aiming at! Differentiating human intent and outcome in a physically dynamic throwing task",
      "summary": "Recognising intent in collaborative human robot tasks can improve team performance and human perception of robots. Intent can differ from the observed outcome in the presence of mistakes which are likely in physically dynamic tasks. We created a dataset of 1227 throws of a ball at a target from 10 participants and observed that 47% of throws were mistakes with 16% completely missing the target. Our research leverages facial images capturing the person's reaction to the outcome of a throw to predict when the resulting throw is a mistake and then we determine the actual intent of the throw. The approach we propose for outcome prediction performs 38% better than the two-stream architecture used previously for this task on front-on videos. In addition, we propose a 1-D CNN model which is used in conjunction with priors learned from the frequency of mistakes to provide an end-to-end pipeline for outcome and intent recognition in this throwing task.",
      "published": "2024-10-26T19:24:46Z"
    },
    "metadata": {
      "arxiv_id": "2410.20256",
      "title": "That was not what I was aiming at! Differentiating human intent and outcome in a physically dynamic throwing task",
      "summary": "Recognising intent in collaborative human robot tasks can improve team performance and human perception of robots. Intent can differ from the observed outcome in the presence of mistakes which are likely in physically dynamic tasks. We created a dataset of 1227 throws of a ball at a target from 10 participants and observed that 47% of throws were mistakes with 16% completely missing the target. Our research leverages facial images capturing the person's reaction to the outcome of a throw to predict when the resulting throw is a mistake and then we determine the actual intent of the throw. The approach we propose for outcome prediction performs 38% better than the two-stream architecture used previously for this task on front-on videos. In addition, we propose a 1-D CNN model which is used in conjunction with priors learned from the frequency of mistakes to provide an end-to-end pipeline for outcome and intent recognition in this throwing task.",
      "authors": [
        "Vidullan Surendran",
        "Alan R. Wagner"
      ],
      "published": "2024-10-26T19:24:46Z",
      "updated": "2024-10-26T19:24:46Z",
      "categories": [
        "cs.RO",
        "cs.HC"
      ],
      "pdf_url": "https://arxiv.org/pdf/2410.20256v1",
      "landing_url": "https://arxiv.org/abs/2410.20256v1",
      "doi": "https://doi.org/10.1007/s10514-022-10074-5"
    },
    "queries": [
      {
        "anchor": "synchronous dialogue",
        "search_term": "streaming architecture"
      }
    ]
  },
  {
    "arxiv_id": "2410.21264",
    "anchor": "spoken language models",
    "search_term": "discrete tokens",
    "search_record": {
      "id": "http://arxiv.org/abs/2410.21264v2",
      "title": "LARP: Tokenizing Videos with a Learned Autoregressive Generative Prior",
      "summary": "We present LARP, a novel video tokenizer designed to overcome limitations in current video tokenization methods for autoregressive (AR) generative models. Unlike traditional patchwise tokenizers that directly encode local visual patches into discrete tokens, LARP introduces a holistic tokenization scheme that gathers information from the visual content using a set of learned holistic queries. This design allows LARP to capture more global and semantic representations, rather than being limited to local patch-level information. Furthermore, it offers flexibility by supporting an arbitrary number of discrete tokens, enabling adaptive and efficient tokenization based on the specific requirements of the task. To align the discrete token space with downstream AR generation tasks, LARP integrates a lightweight AR transformer as a training-time prior model that predicts the next token on its discrete latent space. By incorporating the prior model during training, LARP learns a latent space that is not only optimized for video reconstruction but is also structured in a way that is more conducive to autoregressive generation. Moreover, this process defines a sequential order for the discrete tokens, progressively pushing them toward an optimal configuration during training, ensuring smoother and more accurate AR generation at inference time. Comprehensive experiments demonstrate LARP's strong performance, achieving state-of-the-art FVD on the UCF101 class-conditional video generation benchmark. LARP enhances the compatibility of AR models with videos and opens up the potential to build unified high-fidelity multimodal large language models (MLLMs).",
      "published": "2024-10-28T17:57:07Z"
    },
    "metadata": {
      "arxiv_id": "2410.21264",
      "title": "LARP: Tokenizing Videos with a Learned Autoregressive Generative Prior",
      "summary": "We present LARP, a novel video tokenizer designed to overcome limitations in current video tokenization methods for autoregressive (AR) generative models. Unlike traditional patchwise tokenizers that directly encode local visual patches into discrete tokens, LARP introduces a holistic tokenization scheme that gathers information from the visual content using a set of learned holistic queries. This design allows LARP to capture more global and semantic representations, rather than being limited to local patch-level information. Furthermore, it offers flexibility by supporting an arbitrary number of discrete tokens, enabling adaptive and efficient tokenization based on the specific requirements of the task. To align the discrete token space with downstream AR generation tasks, LARP integrates a lightweight AR transformer as a training-time prior model that predicts the next token on its discrete latent space. By incorporating the prior model during training, LARP learns a latent space that is not only optimized for video reconstruction but is also structured in a way that is more conducive to autoregressive generation. Moreover, this process defines a sequential order for the discrete tokens, progressively pushing them toward an optimal configuration during training, ensuring smoother and more accurate AR generation at inference time. Comprehensive experiments demonstrate LARP's strong performance, achieving state-of-the-art FVD on the UCF101 class-conditional video generation benchmark. LARP enhances the compatibility of AR models with videos and opens up the potential to build unified high-fidelity multimodal large language models (MLLMs).",
      "authors": [
        "Hanyu Wang",
        "Saksham Suri",
        "Yixuan Ren",
        "Hao Chen",
        "Abhinav Shrivastava"
      ],
      "published": "2024-10-28T17:57:07Z",
      "updated": "2025-06-16T22:06:19Z",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2410.21264v2",
      "landing_url": "https://arxiv.org/abs/2410.21264v2",
      "doi": "https://doi.org/10.48550/arXiv.2410.21264"
    },
    "queries": [
      {
        "anchor": "spoken language models",
        "search_term": "discrete tokens"
      },
      {
        "anchor": "synchronous dialogue",
        "search_term": "discrete tokens"
      }
    ]
  },
  {
    "arxiv_id": "2410.21280",
    "anchor": "turn-taking",
    "search_term": "true full-duplex",
    "search_record": {
      "id": "http://arxiv.org/abs/2410.21280v1",
      "title": "TraderTalk: An LLM Behavioural ABM applied to Simulating Human Bilateral Trading Interactions",
      "summary": "We introduce a novel hybrid approach that augments Agent-Based Models (ABMs) with behaviors generated by Large Language Models (LLMs) to simulate human trading interactions. We call our model TraderTalk. Leveraging LLMs trained on extensive human-authored text, we capture detailed and nuanced representations of bilateral conversations in financial trading. Applying this Generative Agent-Based Model (GABM) to government bond markets, we replicate trading decisions between two stylised virtual humans. Our method addresses both structural challenges, such as coordinating turn-taking between realistic LLM-based agents, and design challenges, including the interpretation of LLM outputs by the agent model. By exploring prompt design opportunistically rather than systematically, we enhance the realism of agent interactions without exhaustive overfitting or model reliance. Our approach successfully replicates trade-to-order volume ratios observed in related asset markets, demonstrating the potential of LLM-augmented ABMs in financial simulations",
      "published": "2024-10-10T23:58:07Z"
    },
    "metadata": {
      "arxiv_id": "2410.21280",
      "title": "TraderTalk: An LLM Behavioural ABM applied to Simulating Human Bilateral Trading Interactions",
      "summary": "We introduce a novel hybrid approach that augments Agent-Based Models (ABMs) with behaviors generated by Large Language Models (LLMs) to simulate human trading interactions. We call our model TraderTalk. Leveraging LLMs trained on extensive human-authored text, we capture detailed and nuanced representations of bilateral conversations in financial trading. Applying this Generative Agent-Based Model (GABM) to government bond markets, we replicate trading decisions between two stylised virtual humans. Our method addresses both structural challenges, such as coordinating turn-taking between realistic LLM-based agents, and design challenges, including the interpretation of LLM outputs by the agent model. By exploring prompt design opportunistically rather than systematically, we enhance the realism of agent interactions without exhaustive overfitting or model reliance. Our approach successfully replicates trade-to-order volume ratios observed in related asset markets, demonstrating the potential of LLM-augmented ABMs in financial simulations",
      "authors": [
        "Alicia Vidler",
        "Toby Walsh"
      ],
      "published": "2024-10-10T23:58:07Z",
      "updated": "2024-10-10T23:58:07Z",
      "categories": [
        "q-fin.TR",
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2410.21280v1",
      "landing_url": "https://arxiv.org/abs/2410.21280v1",
      "doi": "https://doi.org/10.1109/ICA63002.2024.00042"
    },
    "queries": [
      {
        "anchor": "turn-taking",
        "search_term": "true full-duplex"
      },
      {
        "anchor": "turn-taking",
        "search_term": "pseudo full-duplex"
      },
      {
        "anchor": "turn-taking",
        "search_term": "engineered synchronization"
      },
      {
        "anchor": "turn-taking",
        "search_term": "synchronous data"
      },
      {
        "anchor": "turn-taking",
        "search_term": "multichannel dialogue"
      },
      {
        "anchor": "turn-taking",
        "search_term": "behavioral arbitration"
      },
      {
        "anchor": "turn-taking",
        "search_term": "floor transfer offset"
      },
      {
        "anchor": "turn-taking",
        "search_term": "stop latency"
      },
      {
        "anchor": "turn-taking",
        "search_term": "interruption response delay"
      }
    ]
  },
  {
    "arxiv_id": "2410.22448",
    "anchor": "spoken language models",
    "search_term": "neural codec",
    "search_record": {
      "id": "http://arxiv.org/abs/2410.22448v1",
      "title": "A Closer Look at Neural Codec Resynthesis: Bridging the Gap between Codec and Waveform Generation",
      "summary": "Neural Audio Codecs, initially designed as a compression technique, have gained more attention recently for speech generation. Codec models represent each audio frame as a sequence of tokens, i.e., discrete embeddings. The discrete and low-frequency nature of neural codecs introduced a new way to generate speech with token-based models. As these tokens encode information at various levels of granularity, from coarse to fine, most existing works focus on how to better generate the coarse tokens. In this paper, we focus on an equally important but often overlooked question: How can we better resynthesize the waveform from coarse tokens? We point out that both the choice of learning target and resynthesis approach have a dramatic impact on the generated audio quality. Specifically, we study two different strategies based on token prediction and regression, and introduce a new method based on Schrödinger Bridge. We examine how different design choices affect machine and human perception.",
      "published": "2024-10-29T18:29:39Z"
    },
    "metadata": {
      "arxiv_id": "2410.22448",
      "title": "A Closer Look at Neural Codec Resynthesis: Bridging the Gap between Codec and Waveform Generation",
      "summary": "Neural Audio Codecs, initially designed as a compression technique, have gained more attention recently for speech generation. Codec models represent each audio frame as a sequence of tokens, i.e., discrete embeddings. The discrete and low-frequency nature of neural codecs introduced a new way to generate speech with token-based models. As these tokens encode information at various levels of granularity, from coarse to fine, most existing works focus on how to better generate the coarse tokens. In this paper, we focus on an equally important but often overlooked question: How can we better resynthesize the waveform from coarse tokens? We point out that both the choice of learning target and resynthesis approach have a dramatic impact on the generated audio quality. Specifically, we study two different strategies based on token prediction and regression, and introduce a new method based on Schrödinger Bridge. We examine how different design choices affect machine and human perception.",
      "authors": [
        "Alexander H. Liu",
        "Qirui Wang",
        "Yuan Gong",
        "James Glass"
      ],
      "published": "2024-10-29T18:29:39Z",
      "updated": "2024-10-29T18:29:39Z",
      "categories": [
        "eess.AS",
        "cs.CL",
        "cs.LG",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2410.22448v1",
      "landing_url": "https://arxiv.org/abs/2410.22448v1",
      "doi": "https://doi.org/10.48550/arXiv.2410.22448"
    },
    "queries": [
      {
        "anchor": "spoken language models",
        "search_term": "neural codec"
      },
      {
        "anchor": "synchronous dialogue",
        "search_term": "neural codec"
      },
      {
        "anchor": "turn-taking",
        "search_term": "neural codec"
      }
    ]
  },
  {
    "arxiv_id": "2410.23320",
    "anchor": "synchronous dialogue",
    "search_term": "neural codec",
    "search_record": {
      "id": "http://arxiv.org/abs/2410.23320v2",
      "title": "Lina-Speech: Gated Linear Attention and Initial-State Tuning for Multi-Sample Prompting Text-To-Speech Synthesis",
      "summary": "Neural codec language models, built on transformer architecture, have revolutionized text-to-speech (TTS) synthesis, excelling in voice cloning by treating it as a prefix continuation task. However, their limited context length hinders their effectiveness to short speech samples. As a result, the voice cloning ability is restricted to a limited coverage and diversity of the speaker's prosody and style. Besides, adapting prosody, accent, or appropriate emotion from a short prefix remains a challenging task. Finally, the quadratic complexity of self-attention limits inference throughput. In this work, we introduce Lina-Speech, a TTS model with Gated Linear Attention (GLA) to replace standard self-attention as a principled backbone, improving inference throughput while matching state-of-the-art performance. Leveraging the stateful property of recurrent architecture, we introduce an Initial-State Tuning (IST) strategy that unlocks the possibility of multiple speech sample conditioning of arbitrary numbers and lengths and provides a comprehensive and efficient strategy for voice cloning and out-of-domain speaking style and emotion adaptation. We demonstrate the effectiveness of this approach for controlling fine-grained characteristics such as prosody and emotion. Code, checkpoints, and demo are freely available: https://github.com/theodorblackbird/lina-speech",
      "published": "2024-10-30T04:50:40Z"
    },
    "metadata": {
      "arxiv_id": "2410.23320",
      "title": "Lina-Speech: Gated Linear Attention and Initial-State Tuning for Multi-Sample Prompting Text-To-Speech Synthesis",
      "summary": "Neural codec language models, built on transformer architecture, have revolutionized text-to-speech (TTS) synthesis, excelling in voice cloning by treating it as a prefix continuation task. However, their limited context length hinders their effectiveness to short speech samples. As a result, the voice cloning ability is restricted to a limited coverage and diversity of the speaker's prosody and style. Besides, adapting prosody, accent, or appropriate emotion from a short prefix remains a challenging task. Finally, the quadratic complexity of self-attention limits inference throughput. In this work, we introduce Lina-Speech, a TTS model with Gated Linear Attention (GLA) to replace standard self-attention as a principled backbone, improving inference throughput while matching state-of-the-art performance. Leveraging the stateful property of recurrent architecture, we introduce an Initial-State Tuning (IST) strategy that unlocks the possibility of multiple speech sample conditioning of arbitrary numbers and lengths and provides a comprehensive and efficient strategy for voice cloning and out-of-domain speaking style and emotion adaptation. We demonstrate the effectiveness of this approach for controlling fine-grained characteristics such as prosody and emotion. Code, checkpoints, and demo are freely available: https://github.com/theodorblackbird/lina-speech",
      "authors": [
        "Théodor Lemerle",
        "Téo Guichoux",
        "Axel Roebel",
        "Nicolas Obin"
      ],
      "published": "2024-10-30T04:50:40Z",
      "updated": "2025-11-15T14:30:03Z",
      "categories": [
        "eess.AS",
        "cs.AI",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2410.23320v2",
      "landing_url": "https://arxiv.org/abs/2410.23320v2",
      "doi": "https://doi.org/10.48550/arXiv.2410.23320"
    },
    "queries": [
      {
        "anchor": "synchronous dialogue",
        "search_term": "neural codec"
      }
    ]
  },
  {
    "arxiv_id": "2410.23558",
    "anchor": "spoken language models",
    "search_term": "semantic coherence",
    "search_record": {
      "id": "http://arxiv.org/abs/2410.23558v3",
      "title": "Transferable & Stealthy Ensemble Attacks: A Black-Box Jailbreaking Framework for Large Language Models",
      "summary": "We present a novel black-box jailbreaking framework that integrates multiple LLM-as-Attacker strategies to deliver highly transferable and effective attacks. The framework is grounded in three key insights from prior jailbreaking research and practice: ensemble approaches outperform single methods in exposing aligned LLM vulnerabilities, malicious instructions vary in jailbreaking difficulty requiring tailored optimization, and disrupting semantic coherence of malicious prompts can manipulate their embeddings to boost success rates. Validated in the Competition for LLM and Agent Safety 2024, our solution achieved top rankings in the Jailbreaking Attack Track.",
      "published": "2024-10-31T01:55:33Z"
    },
    "metadata": {
      "arxiv_id": "2410.23558",
      "title": "Transferable & Stealthy Ensemble Attacks: A Black-Box Jailbreaking Framework for Large Language Models",
      "summary": "We present a novel black-box jailbreaking framework that integrates multiple LLM-as-Attacker strategies to deliver highly transferable and effective attacks. The framework is grounded in three key insights from prior jailbreaking research and practice: ensemble approaches outperform single methods in exposing aligned LLM vulnerabilities, malicious instructions vary in jailbreaking difficulty requiring tailored optimization, and disrupting semantic coherence of malicious prompts can manipulate their embeddings to boost success rates. Validated in the Competition for LLM and Agent Safety 2024, our solution achieved top rankings in the Jailbreaking Attack Track.",
      "authors": [
        "Yiqi Yang",
        "Hongye Fu"
      ],
      "published": "2024-10-31T01:55:33Z",
      "updated": "2025-11-06T07:56:24Z",
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2410.23558v3",
      "landing_url": "https://arxiv.org/abs/2410.23558v3",
      "doi": "https://doi.org/10.48550/arXiv.2410.23558"
    },
    "queries": [
      {
        "anchor": "spoken language models",
        "search_term": "semantic coherence"
      },
      {
        "anchor": "synchronous dialogue",
        "search_term": "semantic coherence"
      }
    ]
  },
  {
    "arxiv_id": "2410.23629",
    "anchor": "synchronous dialogue",
    "search_term": "synchronous data",
    "search_record": {
      "id": "http://arxiv.org/abs/2410.23629v2",
      "title": "Posture-Informed Muscular Force Learning for Robust Hand Pressure Estimation",
      "summary": "We present PiMForce, a novel framework that enhances hand pressure estimation by leveraging 3D hand posture information to augment forearm surface electromyography (sEMG) signals. Our approach utilizes detailed spatial information from 3D hand poses in conjunction with dynamic muscle activity from sEMG to enable accurate and robust whole-hand pressure measurements under diverse hand-object interactions. We also developed a multimodal data collection system that combines a pressure glove, an sEMG armband, and a markerless finger-tracking module. We created a comprehensive dataset from 21 participants, capturing synchronized data of hand posture, sEMG signals, and exerted hand pressure across various hand postures and hand-object interaction scenarios using our collection system. Our framework enables precise hand pressure estimation in complex and natural interaction scenarios. Our approach substantially mitigates the limitations of traditional sEMG-based or vision-based methods by integrating 3D hand posture information with sEMG signals. Video demos, data, and code are available online.",
      "published": "2024-10-31T04:42:43Z"
    },
    "metadata": {
      "arxiv_id": "2410.23629",
      "title": "Posture-Informed Muscular Force Learning for Robust Hand Pressure Estimation",
      "summary": "We present PiMForce, a novel framework that enhances hand pressure estimation by leveraging 3D hand posture information to augment forearm surface electromyography (sEMG) signals. Our approach utilizes detailed spatial information from 3D hand poses in conjunction with dynamic muscle activity from sEMG to enable accurate and robust whole-hand pressure measurements under diverse hand-object interactions. We also developed a multimodal data collection system that combines a pressure glove, an sEMG armband, and a markerless finger-tracking module. We created a comprehensive dataset from 21 participants, capturing synchronized data of hand posture, sEMG signals, and exerted hand pressure across various hand postures and hand-object interaction scenarios using our collection system. Our framework enables precise hand pressure estimation in complex and natural interaction scenarios. Our approach substantially mitigates the limitations of traditional sEMG-based or vision-based methods by integrating 3D hand posture information with sEMG signals. Video demos, data, and code are available online.",
      "authors": [
        "Kyungjin Seo",
        "Junghoon Seo",
        "Hanseok Jeong",
        "Sangpil Kim",
        "Sang Ho Yoon"
      ],
      "published": "2024-10-31T04:42:43Z",
      "updated": "2024-11-01T08:38:21Z",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.HC"
      ],
      "pdf_url": "https://arxiv.org/pdf/2410.23629v2",
      "landing_url": "https://arxiv.org/abs/2410.23629v2",
      "doi": "https://doi.org/10.48550/arXiv.2410.23629"
    },
    "queries": [
      {
        "anchor": "synchronous dialogue",
        "search_term": "synchronous data"
      }
    ]
  },
  {
    "arxiv_id": "2410.23684",
    "anchor": "synchronous dialogue",
    "search_term": "discrete tokens",
    "search_record": {
      "id": "http://arxiv.org/abs/2410.23684v2",
      "title": "Improbable Bigrams Expose Vulnerabilities of Incomplete Tokens in Byte-Level Tokenizers",
      "summary": "Tokenization is a crucial step that bridges human-readable text with model-readable discrete tokens. However, recent studies have revealed that tokenizers can be exploited to elicit unwanted model behaviors. In this work, we investigate incomplete tokens, i.e., undecodable tokens with stray bytes resulting from byte-level byte-pair encoding (BPE) tokenization. We hypothesize that such tokens are heavily reliant on their adjacent tokens and are fragile when paired with unfamiliar tokens. To demonstrate this vulnerability, we introduce improbable bigrams: out-of-distribution combinations of incomplete tokens designed to exploit their dependency. Our experiments show that improbable bigrams are significantly prone to hallucinatory behaviors. Surprisingly, the same phrases have drastically lower rates of hallucination (90% reduction in Llama3.1) when an alternative tokenization is used. We caution against the potential vulnerabilities introduced by byte-level BPE tokenizers, which may introduce blind spots to language models.",
      "published": "2024-10-31T07:19:44Z"
    },
    "metadata": {
      "arxiv_id": "2410.23684",
      "title": "Improbable Bigrams Expose Vulnerabilities of Incomplete Tokens in Byte-Level Tokenizers",
      "summary": "Tokenization is a crucial step that bridges human-readable text with model-readable discrete tokens. However, recent studies have revealed that tokenizers can be exploited to elicit unwanted model behaviors. In this work, we investigate incomplete tokens, i.e., undecodable tokens with stray bytes resulting from byte-level byte-pair encoding (BPE) tokenization. We hypothesize that such tokens are heavily reliant on their adjacent tokens and are fragile when paired with unfamiliar tokens. To demonstrate this vulnerability, we introduce improbable bigrams: out-of-distribution combinations of incomplete tokens designed to exploit their dependency. Our experiments show that improbable bigrams are significantly prone to hallucinatory behaviors. Surprisingly, the same phrases have drastically lower rates of hallucination (90% reduction in Llama3.1) when an alternative tokenization is used. We caution against the potential vulnerabilities introduced by byte-level BPE tokenizers, which may introduce blind spots to language models.",
      "authors": [
        "Eugene Jang",
        "Kimin Lee",
        "Jin-Woo Chung",
        "Keuntae Park",
        "Seungwon Shin"
      ],
      "published": "2024-10-31T07:19:44Z",
      "updated": "2025-10-10T15:55:08Z",
      "categories": [
        "cs.CL"
      ],
      "pdf_url": "https://arxiv.org/pdf/2410.23684v2",
      "landing_url": "https://arxiv.org/abs/2410.23684v2",
      "doi": "https://doi.org/10.48550/arXiv.2410.23684"
    },
    "queries": [
      {
        "anchor": "synchronous dialogue",
        "search_term": "discrete tokens"
      }
    ]
  },
  {
    "arxiv_id": "2410.23706",
    "anchor": "spoken language models",
    "search_term": "temporal dynamics",
    "search_record": {
      "id": "http://arxiv.org/abs/2410.23706v2",
      "title": "Asynchronous Jump Testing and Estimation in High Dimensions Under Complex Temporal Dynamics",
      "summary": "Most high dimensional changepoint detection methods assume the error process is stationary and changepoints occur synchronously across dimensions. The violation of these assumptions, which in applied settings is increasingly likely as the dimensionality of the time series being analyzed grows, can dramatically curtail the sensitivity or the accuracy of these methods. We propose AJDN (Asynchronous Jump Detection under Nonstationary noise). AJDN is a high dimensional multiscale jump detection method that tests and estimates jumps in an otherwise smoothly varying mean function for high dimensional time series with nonstationary noise where the jumps across dimensions may not occur at the same time. AJDN is correct in the sense that it detects the correct number of jumps with a prescribed probability asymptotically and its accuracy in estimating the locations of the jumps is asymptotically nearly optimal under the asynchronous jump assumption. Through a simulation study we demonstrate AJDN's robustness across a wide variety of stationary and nonstationary high dimensional time series, and we show its strong performance relative to some existing high dimensional changepoint detection methods. We apply AJDN to a seismic time series to demonstrate its ability to accurately detect jumps in real-world high dimensional time series with complex temporal dynamics.",
      "published": "2024-10-31T07:51:49Z"
    },
    "metadata": {
      "arxiv_id": "2410.23706",
      "title": "Asynchronous Jump Testing and Estimation in High Dimensions Under Complex Temporal Dynamics",
      "summary": "Most high dimensional changepoint detection methods assume the error process is stationary and changepoints occur synchronously across dimensions. The violation of these assumptions, which in applied settings is increasingly likely as the dimensionality of the time series being analyzed grows, can dramatically curtail the sensitivity or the accuracy of these methods. We propose AJDN (Asynchronous Jump Detection under Nonstationary noise). AJDN is a high dimensional multiscale jump detection method that tests and estimates jumps in an otherwise smoothly varying mean function for high dimensional time series with nonstationary noise where the jumps across dimensions may not occur at the same time. AJDN is correct in the sense that it detects the correct number of jumps with a prescribed probability asymptotically and its accuracy in estimating the locations of the jumps is asymptotically nearly optimal under the asynchronous jump assumption. Through a simulation study we demonstrate AJDN's robustness across a wide variety of stationary and nonstationary high dimensional time series, and we show its strong performance relative to some existing high dimensional changepoint detection methods. We apply AJDN to a seismic time series to demonstrate its ability to accurately detect jumps in real-world high dimensional time series with complex temporal dynamics.",
      "authors": [
        "Weichi Wu",
        "David Veitch",
        "Zhou Zhou"
      ],
      "published": "2024-10-31T07:51:49Z",
      "updated": "2024-11-23T04:40:55Z",
      "categories": [
        "stat.ME"
      ],
      "pdf_url": "https://arxiv.org/pdf/2410.23706v2",
      "landing_url": "https://arxiv.org/abs/2410.23706v2",
      "doi": "https://doi.org/10.48550/arXiv.2410.23706"
    },
    "queries": [
      {
        "anchor": "spoken language models",
        "search_term": "temporal dynamics"
      },
      {
        "anchor": "synchronous dialogue",
        "search_term": "temporal dynamics"
      }
    ]
  },
  {
    "arxiv_id": "2410.24177",
    "anchor": "spoken language models",
    "search_term": "true full-duplex",
    "search_record": {
      "id": "http://arxiv.org/abs/2410.24177v1",
      "title": "DC-Spin: A Speaker-invariant Speech Tokenizer for Spoken Language Models",
      "summary": "Spoken language models (SLMs) have gained increasing attention with advancements in text-based, decoder-only language models. SLMs process text and speech, enabling simultaneous speech understanding and generation. This paper presents Double-Codebook Speaker-invariant Clustering (DC-Spin), which aims to improve speech tokenization by bridging audio signals and SLM tokens. DC-Spin extracts speaker-invariant tokens rich in phonetic information and resilient to input variations, enhancing zero-shot SLM tasks and speech resynthesis. We propose a chunk-wise approach to enable streamable DC-Spin without retraining and degradation. Comparisons of tokenization methods (self-supervised and neural audio codecs), model scalability, and downstream task proxies show that tokens easily modeled by an n-gram LM or aligned with phonemes offer strong performance, providing insights for designing speech tokenizers for SLMs.",
      "published": "2024-10-31T17:43:13Z"
    },
    "metadata": {
      "arxiv_id": "2410.24177",
      "title": "DC-Spin: A Speaker-invariant Speech Tokenizer for Spoken Language Models",
      "summary": "Spoken language models (SLMs) have gained increasing attention with advancements in text-based, decoder-only language models. SLMs process text and speech, enabling simultaneous speech understanding and generation. This paper presents Double-Codebook Speaker-invariant Clustering (DC-Spin), which aims to improve speech tokenization by bridging audio signals and SLM tokens. DC-Spin extracts speaker-invariant tokens rich in phonetic information and resilient to input variations, enhancing zero-shot SLM tasks and speech resynthesis. We propose a chunk-wise approach to enable streamable DC-Spin without retraining and degradation. Comparisons of tokenization methods (self-supervised and neural audio codecs), model scalability, and downstream task proxies show that tokens easily modeled by an n-gram LM or aligned with phonemes offer strong performance, providing insights for designing speech tokenizers for SLMs.",
      "authors": [
        "Heng-Jui Chang",
        "Hongyu Gong",
        "Changhan Wang",
        "James Glass",
        "Yu-An Chung"
      ],
      "published": "2024-10-31T17:43:13Z",
      "updated": "2024-10-31T17:43:13Z",
      "categories": [
        "eess.AS",
        "cs.CL",
        "cs.LG",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2410.24177v1",
      "landing_url": "https://arxiv.org/abs/2410.24177v1",
      "doi": "https://doi.org/10.48550/arXiv.2410.24177"
    },
    "queries": [
      {
        "anchor": "spoken language models",
        "search_term": "true full-duplex"
      },
      {
        "anchor": "spoken language models",
        "search_term": "pseudo full-duplex"
      },
      {
        "anchor": "spoken language models",
        "search_term": "half duplex"
      },
      {
        "anchor": "spoken language models",
        "search_term": "engineered synchronization"
      },
      {
        "anchor": "spoken language models",
        "search_term": "learned synchronization"
      },
      {
        "anchor": "spoken language models",
        "search_term": "modular architecture"
      },
      {
        "anchor": "spoken language models",
        "search_term": "streaming architecture"
      },
      {
        "anchor": "spoken language models",
        "search_term": "real-time interaction"
      },
      {
        "anchor": "spoken language models",
        "search_term": "low latency"
      },
      {
        "anchor": "spoken language models",
        "search_term": "synchronous data"
      },
      {
        "anchor": "spoken language models",
        "search_term": "data scarcity"
      },
      {
        "anchor": "spoken language models",
        "search_term": "multichannel dialogue"
      },
      {
        "anchor": "spoken language models",
        "search_term": "temporal dynamics"
      },
      {
        "anchor": "spoken language models",
        "search_term": "behavioral arbitration"
      },
      {
        "anchor": "spoken language models",
        "search_term": "semantic coherence"
      },
      {
        "anchor": "spoken language models",
        "search_term": "neural codec"
      },
      {
        "anchor": "spoken language models",
        "search_term": "discrete tokens"
      },
      {
        "anchor": "spoken language models",
        "search_term": "audio tokens"
      },
      {
        "anchor": "spoken language models",
        "search_term": "floor transfer offset"
      },
      {
        "anchor": "spoken language models",
        "search_term": "stop latency"
      },
      {
        "anchor": "spoken language models",
        "search_term": "interruption response delay"
      }
    ]
  },
  {
    "arxiv_id": "2411.00726",
    "anchor": "synchronous dialogue",
    "search_term": "streaming architecture",
    "search_record": {
      "id": "http://arxiv.org/abs/2411.00726v1",
      "title": "Cross-Fundus Transformer for Multi-modal Diabetic Retinopathy Grading with Cataract",
      "summary": "Diabetic retinopathy (DR) is a leading cause of blindness worldwide and a common complication of diabetes. As two different imaging tools for DR grading, color fundus photography (CFP) and infrared fundus photography (IFP) are highly-correlated and complementary in clinical applications. To the best of our knowledge, this is the first study that explores a novel multi-modal deep learning framework to fuse the information from CFP and IFP towards more accurate DR grading. Specifically, we construct a dual-stream architecture Cross-Fundus Transformer (CFT) to fuse the ViT-based features of two fundus image modalities. In particular, a meticulously engineered Cross-Fundus Attention (CFA) module is introduced to capture the correspondence between CFP and IFP images. Moreover, we adopt both the single-modality and multi-modality supervisions to maximize the overall performance for DR grading. Extensive experiments on a clinical dataset consisting of 1,713 pairs of multi-modal fundus images demonstrate the superiority of our proposed method. Our code will be released for public access.",
      "published": "2024-11-01T16:38:49Z"
    },
    "metadata": {
      "arxiv_id": "2411.00726",
      "title": "Cross-Fundus Transformer for Multi-modal Diabetic Retinopathy Grading with Cataract",
      "summary": "Diabetic retinopathy (DR) is a leading cause of blindness worldwide and a common complication of diabetes. As two different imaging tools for DR grading, color fundus photography (CFP) and infrared fundus photography (IFP) are highly-correlated and complementary in clinical applications. To the best of our knowledge, this is the first study that explores a novel multi-modal deep learning framework to fuse the information from CFP and IFP towards more accurate DR grading. Specifically, we construct a dual-stream architecture Cross-Fundus Transformer (CFT) to fuse the ViT-based features of two fundus image modalities. In particular, a meticulously engineered Cross-Fundus Attention (CFA) module is introduced to capture the correspondence between CFP and IFP images. Moreover, we adopt both the single-modality and multi-modality supervisions to maximize the overall performance for DR grading. Extensive experiments on a clinical dataset consisting of 1,713 pairs of multi-modal fundus images demonstrate the superiority of our proposed method. Our code will be released for public access.",
      "authors": [
        "Fan Xiao",
        "Junlin Hou",
        "Ruiwei Zhao",
        "Rui Feng",
        "Haidong Zou",
        "Lina Lu",
        "Yi Xu",
        "Juzhao Zhang"
      ],
      "published": "2024-11-01T16:38:49Z",
      "updated": "2024-11-01T16:38:49Z",
      "categories": [
        "eess.IV",
        "cs.AI",
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2411.00726v1",
      "landing_url": "https://arxiv.org/abs/2411.00726v1",
      "doi": "https://doi.org/10.48550/arXiv.2411.00726"
    },
    "queries": [
      {
        "anchor": "synchronous dialogue",
        "search_term": "streaming architecture"
      }
    ]
  },
  {
    "arxiv_id": "2411.00813",
    "anchor": "synchronous dialogue",
    "search_term": "synchronous data",
    "search_record": {
      "id": "http://arxiv.org/abs/2411.00813v1",
      "title": "Personality Analysis from Online Short Video Platforms with Multi-domain Adaptation",
      "summary": "Personality analysis from online short videos has gained prominence due to its applications in personalized recommendation systems, sentiment analysis, and human-computer interaction. Traditional assessment methods, such as questionnaires based on the Big Five Personality Framework, are limited by self-report biases and are impractical for large-scale or real-time analysis. Leveraging the rich, multi-modal data present in short videos offers a promising alternative for more accurate personality inference. However, integrating these diverse and asynchronous modalities poses significant challenges, particularly in aligning time-varying data and ensuring models generalize well to new domains with limited labeled data. In this paper, we propose a novel multi-modal personality analysis framework that addresses these challenges by synchronizing and integrating features from multiple modalities and enhancing model generalization through domain adaptation. We introduce a timestamp-based modality alignment mechanism that synchronizes data based on spoken word timestamps, ensuring accurate correspondence across modalities and facilitating effective feature integration. To capture temporal dependencies and inter-modal interactions, we employ Bidirectional Long Short-Term Memory networks and self-attention mechanisms, allowing the model to focus on the most informative features for personality prediction. Furthermore, we develop a gradient-based domain adaptation method that transfers knowledge from multiple source domains to improve performance in target domains with scarce labeled data. Extensive experiments on real-world datasets demonstrate that our framework significantly outperforms existing methods in personality prediction tasks, highlighting its effectiveness in capturing complex behavioral cues and robustness in adapting to new domains.",
      "published": "2024-10-26T03:29:32Z"
    },
    "metadata": {
      "arxiv_id": "2411.00813",
      "title": "Personality Analysis from Online Short Video Platforms with Multi-domain Adaptation",
      "summary": "Personality analysis from online short videos has gained prominence due to its applications in personalized recommendation systems, sentiment analysis, and human-computer interaction. Traditional assessment methods, such as questionnaires based on the Big Five Personality Framework, are limited by self-report biases and are impractical for large-scale or real-time analysis. Leveraging the rich, multi-modal data present in short videos offers a promising alternative for more accurate personality inference. However, integrating these diverse and asynchronous modalities poses significant challenges, particularly in aligning time-varying data and ensuring models generalize well to new domains with limited labeled data. In this paper, we propose a novel multi-modal personality analysis framework that addresses these challenges by synchronizing and integrating features from multiple modalities and enhancing model generalization through domain adaptation. We introduce a timestamp-based modality alignment mechanism that synchronizes data based on spoken word timestamps, ensuring accurate correspondence across modalities and facilitating effective feature integration. To capture temporal dependencies and inter-modal interactions, we employ Bidirectional Long Short-Term Memory networks and self-attention mechanisms, allowing the model to focus on the most informative features for personality prediction. Furthermore, we develop a gradient-based domain adaptation method that transfers knowledge from multiple source domains to improve performance in target domains with scarce labeled data. Extensive experiments on real-world datasets demonstrate that our framework significantly outperforms existing methods in personality prediction tasks, highlighting its effectiveness in capturing complex behavioral cues and robustness in adapting to new domains.",
      "authors": [
        "Sixu An",
        "Xiangguo Sun",
        "Yicong Li",
        "Yu Yang",
        "Guandong Xu"
      ],
      "published": "2024-10-26T03:29:32Z",
      "updated": "2024-10-26T03:29:32Z",
      "categories": [
        "cs.MM",
        "cs.AI",
        "cs.CL",
        "cs.CV",
        "cs.CY",
        "cs.LG",
        "cs.SI",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2411.00813v1",
      "landing_url": "https://arxiv.org/abs/2411.00813v1",
      "doi": "https://doi.org/10.48550/arXiv.2411.00813"
    },
    "queries": [
      {
        "anchor": "synchronous dialogue",
        "search_term": "synchronous data"
      }
    ]
  },
  {
    "arxiv_id": "2411.01834",
    "anchor": "spoken language models",
    "search_term": "true full-duplex",
    "search_record": {
      "id": "http://arxiv.org/abs/2411.01834v2",
      "title": "Align-SLM: Textless Spoken Language Models with Reinforcement Learning from AI Feedback",
      "summary": "While textless Spoken Language Models (SLMs) have shown potential in end-to-end speech-to-speech modeling, they still lag behind text-based Large Language Models (LLMs) in terms of semantic coherence and relevance. This work introduces the Align-SLM framework, which leverages preference optimization inspired by Reinforcement Learning with AI Feedback (RLAIF) to enhance the semantic understanding of SLMs. Our approach generates multiple speech continuations from a given prompt and uses semantic metrics to create preference data for Direct Preference Optimization (DPO). We evaluate the framework using ZeroSpeech 2021 benchmarks for lexical and syntactic modeling, the spoken version of the StoryCloze dataset for semantic coherence, and other speech generation metrics, including the GPT4-o score and human evaluation. Experimental results show that our method achieves state-of-the-art performance for SLMs on most benchmarks, highlighting the importance of preference optimization to improve the semantics of SLMs.",
      "published": "2024-11-04T06:07:53Z"
    },
    "metadata": {
      "arxiv_id": "2411.01834",
      "title": "Align-SLM: Textless Spoken Language Models with Reinforcement Learning from AI Feedback",
      "summary": "While textless Spoken Language Models (SLMs) have shown potential in end-to-end speech-to-speech modeling, they still lag behind text-based Large Language Models (LLMs) in terms of semantic coherence and relevance. This work introduces the Align-SLM framework, which leverages preference optimization inspired by Reinforcement Learning with AI Feedback (RLAIF) to enhance the semantic understanding of SLMs. Our approach generates multiple speech continuations from a given prompt and uses semantic metrics to create preference data for Direct Preference Optimization (DPO). We evaluate the framework using ZeroSpeech 2021 benchmarks for lexical and syntactic modeling, the spoken version of the StoryCloze dataset for semantic coherence, and other speech generation metrics, including the GPT4-o score and human evaluation. Experimental results show that our method achieves state-of-the-art performance for SLMs on most benchmarks, highlighting the importance of preference optimization to improve the semantics of SLMs.",
      "authors": [
        "Guan-Ting Lin",
        "Prashanth Gurunath Shivakumar",
        "Aditya Gourav",
        "Yile Gu",
        "Ankur Gandhe",
        "Hung-yi Lee",
        "Ivan Bulyko"
      ],
      "published": "2024-11-04T06:07:53Z",
      "updated": "2025-05-27T16:17:52Z",
      "categories": [
        "cs.CL",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2411.01834v2",
      "landing_url": "https://arxiv.org/abs/2411.01834v2",
      "doi": "https://doi.org/10.48550/arXiv.2411.01834"
    },
    "queries": [
      {
        "anchor": "spoken language models",
        "search_term": "true full-duplex"
      },
      {
        "anchor": "spoken language models",
        "search_term": "pseudo full-duplex"
      },
      {
        "anchor": "spoken language models",
        "search_term": "half duplex"
      },
      {
        "anchor": "spoken language models",
        "search_term": "engineered synchronization"
      },
      {
        "anchor": "spoken language models",
        "search_term": "learned synchronization"
      },
      {
        "anchor": "spoken language models",
        "search_term": "modular architecture"
      },
      {
        "anchor": "spoken language models",
        "search_term": "streaming architecture"
      },
      {
        "anchor": "spoken language models",
        "search_term": "real-time interaction"
      },
      {
        "anchor": "spoken language models",
        "search_term": "low latency"
      },
      {
        "anchor": "spoken language models",
        "search_term": "synchronous data"
      },
      {
        "anchor": "spoken language models",
        "search_term": "data scarcity"
      },
      {
        "anchor": "spoken language models",
        "search_term": "multichannel dialogue"
      },
      {
        "anchor": "spoken language models",
        "search_term": "temporal dynamics"
      },
      {
        "anchor": "spoken language models",
        "search_term": "behavioral arbitration"
      },
      {
        "anchor": "spoken language models",
        "search_term": "semantic coherence"
      },
      {
        "anchor": "spoken language models",
        "search_term": "neural codec"
      },
      {
        "anchor": "spoken language models",
        "search_term": "discrete tokens"
      },
      {
        "anchor": "spoken language models",
        "search_term": "audio tokens"
      },
      {
        "anchor": "spoken language models",
        "search_term": "floor transfer offset"
      },
      {
        "anchor": "spoken language models",
        "search_term": "stop latency"
      },
      {
        "anchor": "spoken language models",
        "search_term": "interruption response delay"
      },
      {
        "anchor": "synchronous dialogue",
        "search_term": "semantic coherence"
      },
      {
        "anchor": "turn-taking",
        "search_term": "semantic coherence"
      }
    ]
  },
  {
    "arxiv_id": "2411.02456",
    "anchor": "spoken language models",
    "search_term": "data scarcity",
    "search_record": {
      "id": "http://arxiv.org/abs/2411.02456v1",
      "title": "A Study of Data Augmentation Techniques to Overcome Data Scarcity in Wound Classification using Deep Learning",
      "summary": "Chronic wounds are a significant burden on individuals and the healthcare system, affecting millions of people and incurring high costs. Wound classification using deep learning techniques is a promising approach for faster diagnosis and treatment initiation. However, lack of high quality data to train the ML models is a major challenge to realize the potential of ML in wound care. In fact, data limitations are the biggest challenge in studies using medical or forensic imaging today. We study data augmentation techniques that can be used to overcome the data scarcity limitations and unlock the potential of deep learning based solutions. In our study we explore a range of data augmentation techniques from geometric transformations of wound images to advanced GANs, to enrich and expand datasets. Using the Keras, Tensorflow, and Pandas libraries, we implemented the data augmentation techniques that can generate realistic wound images. We show that geometric data augmentation can improve classification performance, F1 scores, by up to 11% on top of state-of-the-art models, across several key classes of wounds. Our experiments with GAN based augmentation prove the viability of using DE-GANs to generate wound images with richer variations. Our study and results show that data augmentation is a valuable privacy-preserving tool with huge potential to overcome the data scarcity limitations and we believe it will be part of any real-world ML-based wound care system.",
      "published": "2024-11-04T00:24:50Z"
    },
    "metadata": {
      "arxiv_id": "2411.02456",
      "title": "A Study of Data Augmentation Techniques to Overcome Data Scarcity in Wound Classification using Deep Learning",
      "summary": "Chronic wounds are a significant burden on individuals and the healthcare system, affecting millions of people and incurring high costs. Wound classification using deep learning techniques is a promising approach for faster diagnosis and treatment initiation. However, lack of high quality data to train the ML models is a major challenge to realize the potential of ML in wound care. In fact, data limitations are the biggest challenge in studies using medical or forensic imaging today. We study data augmentation techniques that can be used to overcome the data scarcity limitations and unlock the potential of deep learning based solutions. In our study we explore a range of data augmentation techniques from geometric transformations of wound images to advanced GANs, to enrich and expand datasets. Using the Keras, Tensorflow, and Pandas libraries, we implemented the data augmentation techniques that can generate realistic wound images. We show that geometric data augmentation can improve classification performance, F1 scores, by up to 11% on top of state-of-the-art models, across several key classes of wounds. Our experiments with GAN based augmentation prove the viability of using DE-GANs to generate wound images with richer variations. Our study and results show that data augmentation is a valuable privacy-preserving tool with huge potential to overcome the data scarcity limitations and we believe it will be part of any real-world ML-based wound care system.",
      "authors": [
        "Harini Narayanan",
        "Sindhu Ghanta"
      ],
      "published": "2024-11-04T00:24:50Z",
      "updated": "2024-11-04T00:24:50Z",
      "categories": [
        "cs.CV",
        "eess.IV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2411.02456v1",
      "landing_url": "https://arxiv.org/abs/2411.02456v1",
      "doi": "https://doi.org/10.48550/arXiv.2411.02456"
    },
    "queries": [
      {
        "anchor": "spoken language models",
        "search_term": "data scarcity"
      },
      {
        "anchor": "synchronous dialogue",
        "search_term": "data scarcity"
      },
      {
        "anchor": "turn-taking",
        "search_term": "data scarcity"
      }
    ]
  },
  {
    "arxiv_id": "2411.03505",
    "anchor": "synchronous dialogue",
    "search_term": "data scarcity",
    "search_record": {
      "id": "http://arxiv.org/abs/2411.03505v1",
      "title": "SynthSet: Generative Diffusion Model for Semantic Segmentation in Precision Agriculture",
      "summary": "This paper introduces a methodology for generating synthetic annotated data to address data scarcity in semantic segmentation tasks within the precision agriculture domain. Utilizing Denoising Diffusion Probabilistic Models (DDPMs) and Generative Adversarial Networks (GANs), we propose a dual diffusion model architecture for synthesizing realistic annotated agricultural data, without any human intervention. We employ super-resolution to enhance the phenotypic characteristics of the synthesized images and their coherence with the corresponding generated masks. We showcase the utility of the proposed method for wheat head segmentation. The high quality of synthesized data underscores the effectiveness of the proposed methodology in generating image-mask pairs. Furthermore, models trained on our generated data exhibit promising performance when tested on an external, diverse dataset of real wheat fields. The results show the efficacy of the proposed methodology for addressing data scarcity for semantic segmentation tasks. Moreover, the proposed approach can be readily adapted for various segmentation tasks in precision agriculture and beyond.",
      "published": "2024-11-05T20:42:23Z"
    },
    "metadata": {
      "arxiv_id": "2411.03505",
      "title": "SynthSet: Generative Diffusion Model for Semantic Segmentation in Precision Agriculture",
      "summary": "This paper introduces a methodology for generating synthetic annotated data to address data scarcity in semantic segmentation tasks within the precision agriculture domain. Utilizing Denoising Diffusion Probabilistic Models (DDPMs) and Generative Adversarial Networks (GANs), we propose a dual diffusion model architecture for synthesizing realistic annotated agricultural data, without any human intervention. We employ super-resolution to enhance the phenotypic characteristics of the synthesized images and their coherence with the corresponding generated masks. We showcase the utility of the proposed method for wheat head segmentation. The high quality of synthesized data underscores the effectiveness of the proposed methodology in generating image-mask pairs. Furthermore, models trained on our generated data exhibit promising performance when tested on an external, diverse dataset of real wheat fields. The results show the efficacy of the proposed methodology for addressing data scarcity for semantic segmentation tasks. Moreover, the proposed approach can be readily adapted for various segmentation tasks in precision agriculture and beyond.",
      "authors": [
        "Andrew Heschl",
        "Mauricio Murillo",
        "Keyhan Najafian",
        "Farhad Maleki"
      ],
      "published": "2024-11-05T20:42:23Z",
      "updated": "2024-11-05T20:42:23Z",
      "categories": [
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2411.03505v1",
      "landing_url": "https://arxiv.org/abs/2411.03505v1",
      "doi": "https://doi.org/10.48550/arXiv.2411.03505"
    },
    "queries": [
      {
        "anchor": "synchronous dialogue",
        "search_term": "data scarcity"
      }
    ]
  },
  {
    "arxiv_id": "2411.03606",
    "anchor": "full-duplex",
    "search_term": "true full-duplex",
    "search_record": {
      "id": "http://arxiv.org/abs/2411.03606v2",
      "title": "Beam Tracking for Full-Duplex User Terminals in Low Earth Orbit Satellite Communication Systems",
      "summary": "This paper introduces a novel beam tracking scheme for full-duplex ground user terminals aiming to transmit uplink and receive downlink from two low Earth orbit (LEO) satellites at the same time and same frequency. Our proposed technique leverages observed phenomena from a recent measurement campaign to strategically select transmit and receive beams which couple low self-interference across the satellites' trajectories, thereby enabling in-band full-duplex operation. Our scheme takes a measurement-driven approach, meaning it does not rely on explicit knowledge of the self-interference channel and can inherently account for hardware impairments or other nonidealities. We show that our proposed scheme reliably selects beams which spatially cancel self-interference to below the noise floor, circumventing the need for digital/analog cancellation. Simulation results using satellite and orbital parameters published in 3GPP and FCC filings show that this substantial reduction in self-interference does not prohibitively compromise beamforming gain, allowing the user terminal to attain near-maximal SINRs, thus unlocking full-duplex operation.",
      "published": "2024-11-06T02:00:48Z"
    },
    "metadata": {
      "arxiv_id": "2411.03606",
      "title": "Beam Tracking for Full-Duplex User Terminals in Low Earth Orbit Satellite Communication Systems",
      "summary": "This paper introduces a novel beam tracking scheme for full-duplex ground user terminals aiming to transmit uplink and receive downlink from two low Earth orbit (LEO) satellites at the same time and same frequency. Our proposed technique leverages observed phenomena from a recent measurement campaign to strategically select transmit and receive beams which couple low self-interference across the satellites' trajectories, thereby enabling in-band full-duplex operation. Our scheme takes a measurement-driven approach, meaning it does not rely on explicit knowledge of the self-interference channel and can inherently account for hardware impairments or other nonidealities. We show that our proposed scheme reliably selects beams which spatially cancel self-interference to below the noise floor, circumventing the need for digital/analog cancellation. Simulation results using satellite and orbital parameters published in 3GPP and FCC filings show that this substantial reduction in self-interference does not prohibitively compromise beamforming gain, allowing the user terminal to attain near-maximal SINRs, thus unlocking full-duplex operation.",
      "authors": [
        "Chaeyeon Kim",
        "Joohyun Son",
        "Daesik Hong",
        "Ian P. Roberts"
      ],
      "published": "2024-11-06T02:00:48Z",
      "updated": "2024-11-13T23:39:07Z",
      "categories": [
        "eess.SP"
      ],
      "pdf_url": "https://arxiv.org/pdf/2411.03606v2",
      "landing_url": "https://arxiv.org/abs/2411.03606v2",
      "doi": "https://doi.org/10.48550/arXiv.2411.03606"
    },
    "queries": [
      {
        "anchor": "full-duplex",
        "search_term": "true full-duplex"
      },
      {
        "anchor": "full-duplex",
        "search_term": "pseudo full-duplex"
      },
      {
        "anchor": "full-duplex",
        "search_term": "engineered synchronization"
      },
      {
        "anchor": "full-duplex",
        "search_term": "learned synchronization"
      },
      {
        "anchor": "full-duplex",
        "search_term": "streaming architecture"
      },
      {
        "anchor": "full-duplex",
        "search_term": "real-time interaction"
      },
      {
        "anchor": "full-duplex",
        "search_term": "synchronous data"
      },
      {
        "anchor": "full-duplex",
        "search_term": "data scarcity"
      },
      {
        "anchor": "full-duplex",
        "search_term": "multichannel dialogue"
      },
      {
        "anchor": "full-duplex",
        "search_term": "behavioral arbitration"
      },
      {
        "anchor": "full-duplex",
        "search_term": "semantic coherence"
      },
      {
        "anchor": "full-duplex",
        "search_term": "neural codec"
      },
      {
        "anchor": "full-duplex",
        "search_term": "floor transfer offset"
      },
      {
        "anchor": "full-duplex",
        "search_term": "stop latency"
      },
      {
        "anchor": "full-duplex",
        "search_term": "interruption response delay"
      }
    ]
  },
  {
    "arxiv_id": "2411.03691",
    "anchor": "full-duplex",
    "search_term": "true full-duplex",
    "search_record": {
      "id": "http://arxiv.org/abs/2411.03691v1",
      "title": "Analog Beamforming Codebooks for Wideband Full-Duplex Millimeter-Wave Systems",
      "summary": "In full-duplex millimeter-wave (mmWave) systems, the effects of beam squint and the frequency-selectivity of self-interference exacerbate over wide bandwidths. This complicates the use of beamforming to cancel self-interference when communicating over bandwidths on the order of gigahertz. In this work, we present the first analog beamforming codebooks tailored to wideband full-duplex mmWave systems, designed to both combat beam squint and cancel frequency-selective self-interference. Our proposed design constructs such codebooks by minimizing self-interference across the entire band of interest while constraining the coverage provided by these codebooks across that same band. Simulation results using computational electromagnetics to model self-interference suggest that a full-duplex 60 GHz system with our design enjoys lower self-interference and delivers better coverage across bandwidths as wide as 6 GHz, when compared to similar codebook designs that ignore beam squint and/or frequency-selectivity. This allows our design to sustain higher SINRs and spectral efficiencies across wide bandwidths, unlocking the potentials of wideband full-duplex mmWave systems.",
      "published": "2024-11-06T06:19:07Z"
    },
    "metadata": {
      "arxiv_id": "2411.03691",
      "title": "Analog Beamforming Codebooks for Wideband Full-Duplex Millimeter-Wave Systems",
      "summary": "In full-duplex millimeter-wave (mmWave) systems, the effects of beam squint and the frequency-selectivity of self-interference exacerbate over wide bandwidths. This complicates the use of beamforming to cancel self-interference when communicating over bandwidths on the order of gigahertz. In this work, we present the first analog beamforming codebooks tailored to wideband full-duplex mmWave systems, designed to both combat beam squint and cancel frequency-selective self-interference. Our proposed design constructs such codebooks by minimizing self-interference across the entire band of interest while constraining the coverage provided by these codebooks across that same band. Simulation results using computational electromagnetics to model self-interference suggest that a full-duplex 60 GHz system with our design enjoys lower self-interference and delivers better coverage across bandwidths as wide as 6 GHz, when compared to similar codebook designs that ignore beam squint and/or frequency-selectivity. This allows our design to sustain higher SINRs and spectral efficiencies across wide bandwidths, unlocking the potentials of wideband full-duplex mmWave systems.",
      "authors": [
        "Sungho Cho",
        "Ian P. Roberts"
      ],
      "published": "2024-11-06T06:19:07Z",
      "updated": "2024-11-06T06:19:07Z",
      "categories": [
        "eess.SP"
      ],
      "pdf_url": "https://arxiv.org/pdf/2411.03691v1",
      "landing_url": "https://arxiv.org/abs/2411.03691v1",
      "doi": "https://doi.org/10.48550/arXiv.2411.03691"
    },
    "queries": [
      {
        "anchor": "full-duplex",
        "search_term": "true full-duplex"
      },
      {
        "anchor": "full-duplex",
        "search_term": "pseudo full-duplex"
      },
      {
        "anchor": "full-duplex",
        "search_term": "engineered synchronization"
      },
      {
        "anchor": "full-duplex",
        "search_term": "learned synchronization"
      },
      {
        "anchor": "full-duplex",
        "search_term": "modular architecture"
      },
      {
        "anchor": "full-duplex",
        "search_term": "streaming architecture"
      },
      {
        "anchor": "full-duplex",
        "search_term": "real-time interaction"
      },
      {
        "anchor": "full-duplex",
        "search_term": "low latency"
      },
      {
        "anchor": "full-duplex",
        "search_term": "synchronous data"
      },
      {
        "anchor": "full-duplex",
        "search_term": "data scarcity"
      },
      {
        "anchor": "full-duplex",
        "search_term": "multichannel dialogue"
      },
      {
        "anchor": "full-duplex",
        "search_term": "temporal dynamics"
      },
      {
        "anchor": "full-duplex",
        "search_term": "behavioral arbitration"
      },
      {
        "anchor": "full-duplex",
        "search_term": "semantic coherence"
      },
      {
        "anchor": "full-duplex",
        "search_term": "neural codec"
      },
      {
        "anchor": "full-duplex",
        "search_term": "discrete tokens"
      },
      {
        "anchor": "full-duplex",
        "search_term": "audio tokens"
      },
      {
        "anchor": "full-duplex",
        "search_term": "floor transfer offset"
      },
      {
        "anchor": "full-duplex",
        "search_term": "stop latency"
      },
      {
        "anchor": "full-duplex",
        "search_term": "interruption response delay"
      }
    ]
  },
  {
    "arxiv_id": "2411.04142",
    "anchor": "spoken language models",
    "search_term": "true full-duplex",
    "search_record": {
      "id": "http://arxiv.org/abs/2411.04142v1",
      "title": "Unified Pathological Speech Analysis with Prompt Tuning",
      "summary": "Pathological speech analysis has been of interest in the detection of certain diseases like depression and Alzheimer's disease and attracts much interest from researchers. However, previous pathological speech analysis models are commonly designed for a specific disease while overlooking the connection between diseases, which may constrain performance and lower training efficiency. Instead of fine-tuning deep models for different tasks, prompt tuning is a much more efficient training paradigm. We thus propose a unified pathological speech analysis system for as many as three diseases with the prompt tuning technique. This system uses prompt tuning to adjust only a small part of the parameters to detect different diseases from speeches of possible patients. Our system leverages a pre-trained spoken language model and demonstrates strong performance across multiple disorders while only fine-tuning a fraction of the parameters. This efficient training approach leads to faster convergence and improved F1 scores by allowing knowledge to be shared across tasks. Our experiments on Alzheimer's disease, Depression, and Parkinson's disease show competitive results, highlighting the effectiveness of our method in pathological speech analysis.",
      "published": "2024-11-05T06:47:26Z"
    },
    "metadata": {
      "arxiv_id": "2411.04142",
      "title": "Unified Pathological Speech Analysis with Prompt Tuning",
      "summary": "Pathological speech analysis has been of interest in the detection of certain diseases like depression and Alzheimer's disease and attracts much interest from researchers. However, previous pathological speech analysis models are commonly designed for a specific disease while overlooking the connection between diseases, which may constrain performance and lower training efficiency. Instead of fine-tuning deep models for different tasks, prompt tuning is a much more efficient training paradigm. We thus propose a unified pathological speech analysis system for as many as three diseases with the prompt tuning technique. This system uses prompt tuning to adjust only a small part of the parameters to detect different diseases from speeches of possible patients. Our system leverages a pre-trained spoken language model and demonstrates strong performance across multiple disorders while only fine-tuning a fraction of the parameters. This efficient training approach leads to faster convergence and improved F1 scores by allowing knowledge to be shared across tasks. Our experiments on Alzheimer's disease, Depression, and Parkinson's disease show competitive results, highlighting the effectiveness of our method in pathological speech analysis.",
      "authors": [
        "Fei Yang",
        "Xuenan Xu",
        "Mengyue Wu",
        "Kai Yu"
      ],
      "published": "2024-11-05T06:47:26Z",
      "updated": "2024-11-05T06:47:26Z",
      "categories": [
        "eess.AS",
        "cs.CL",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2411.04142v1",
      "landing_url": "https://arxiv.org/abs/2411.04142v1",
      "doi": "https://doi.org/10.48550/arXiv.2411.04142"
    },
    "queries": [
      {
        "anchor": "spoken language models",
        "search_term": "true full-duplex"
      },
      {
        "anchor": "spoken language models",
        "search_term": "pseudo full-duplex"
      },
      {
        "anchor": "spoken language models",
        "search_term": "engineered synchronization"
      },
      {
        "anchor": "spoken language models",
        "search_term": "learned synchronization"
      },
      {
        "anchor": "spoken language models",
        "search_term": "synchronous data"
      },
      {
        "anchor": "spoken language models",
        "search_term": "multichannel dialogue"
      },
      {
        "anchor": "spoken language models",
        "search_term": "behavioral arbitration"
      },
      {
        "anchor": "spoken language models",
        "search_term": "floor transfer offset"
      },
      {
        "anchor": "spoken language models",
        "search_term": "stop latency"
      },
      {
        "anchor": "spoken language models",
        "search_term": "interruption response delay"
      }
    ]
  },
  {
    "arxiv_id": "2411.05001",
    "anchor": "synchronous dialogue",
    "search_term": "discrete tokens",
    "search_record": {
      "id": "http://arxiv.org/abs/2411.05001v1",
      "title": "Analyzing The Language of Visual Tokens",
      "summary": "With the introduction of transformer-based models for vision and language tasks, such as LLaVA and Chameleon, there has been renewed interest in the discrete tokenized representation of images. These models often treat image patches as discrete tokens, analogous to words in natural language, learning joint alignments between visual and human languages. However, little is known about the statistical behavior of these visual languages - whether they follow similar frequency distributions, grammatical structures, or topologies as natural languages. In this paper, we take a natural-language-centric approach to analyzing discrete visual languages and uncover striking similarities and fundamental differences. We demonstrate that, although visual languages adhere to Zipfian distributions, higher token innovation drives greater entropy and lower compression, with tokens predominantly representing object parts, indicating intermediate granularity. We also show that visual languages lack cohesive grammatical structures, leading to higher perplexity and weaker hierarchical organization compared to natural languages. Finally, we demonstrate that, while vision models align more closely with natural languages than other models, this alignment remains significantly weaker than the cohesion found within natural languages. Through these experiments, we demonstrate how understanding the statistical properties of discrete visual languages can inform the design of more effective computer vision models.",
      "published": "2024-11-07T18:59:28Z"
    },
    "metadata": {
      "arxiv_id": "2411.05001",
      "title": "Analyzing The Language of Visual Tokens",
      "summary": "With the introduction of transformer-based models for vision and language tasks, such as LLaVA and Chameleon, there has been renewed interest in the discrete tokenized representation of images. These models often treat image patches as discrete tokens, analogous to words in natural language, learning joint alignments between visual and human languages. However, little is known about the statistical behavior of these visual languages - whether they follow similar frequency distributions, grammatical structures, or topologies as natural languages. In this paper, we take a natural-language-centric approach to analyzing discrete visual languages and uncover striking similarities and fundamental differences. We demonstrate that, although visual languages adhere to Zipfian distributions, higher token innovation drives greater entropy and lower compression, with tokens predominantly representing object parts, indicating intermediate granularity. We also show that visual languages lack cohesive grammatical structures, leading to higher perplexity and weaker hierarchical organization compared to natural languages. Finally, we demonstrate that, while vision models align more closely with natural languages than other models, this alignment remains significantly weaker than the cohesion found within natural languages. Through these experiments, we demonstrate how understanding the statistical properties of discrete visual languages can inform the design of more effective computer vision models.",
      "authors": [
        "David M. Chan",
        "Rodolfo Corona",
        "Joonyong Park",
        "Cheol Jun Cho",
        "Yutong Bai",
        "Trevor Darrell"
      ],
      "published": "2024-11-07T18:59:28Z",
      "updated": "2024-11-07T18:59:28Z",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2411.05001v1",
      "landing_url": "https://arxiv.org/abs/2411.05001v1",
      "doi": "https://doi.org/10.48550/arXiv.2411.05001"
    },
    "queries": [
      {
        "anchor": "synchronous dialogue",
        "search_term": "discrete tokens"
      }
    ]
  },
  {
    "arxiv_id": "2411.05361",
    "anchor": "spoken language models",
    "search_term": "true full-duplex",
    "search_record": {
      "id": "http://arxiv.org/abs/2411.05361v2",
      "title": "Dynamic-SUPERB Phase-2: A Collaboratively Expanding Benchmark for Measuring the Capabilities of Spoken Language Models with 180 Tasks",
      "summary": "Multimodal foundation models, such as Gemini and ChatGPT, have revolutionized human-machine interactions by seamlessly integrating various forms of data. Developing a universal spoken language model that comprehends a wide range of natural language instructions is critical for bridging communication gaps and facilitating more intuitive interactions. However, the absence of a comprehensive evaluation benchmark poses a significant challenge. We present Dynamic-SUPERB Phase-2, an open and evolving benchmark for the comprehensive evaluation of instruction-based universal speech models. Building upon the first generation, this second version incorporates 125 new tasks contributed collaboratively by the global research community, expanding the benchmark to a total of 180 tasks, making it the largest benchmark for speech and audio evaluation. While the first generation of Dynamic-SUPERB was limited to classification tasks, Dynamic-SUPERB Phase-2 broadens its evaluation capabilities by introducing a wide array of novel and diverse tasks, including regression and sequence generation, across speech, music, and environmental audio. Evaluation results show that no model performed well universally. SALMONN-13B excelled in English ASR and Qwen2-Audio-7B-Instruct showed high accuracy in emotion recognition, but current models still require further innovations to handle a broader range of tasks. We open-source all task data and the evaluation pipeline at https://github.com/dynamic-superb/dynamic-superb.",
      "published": "2024-11-08T06:33:22Z"
    },
    "metadata": {
      "arxiv_id": "2411.05361",
      "title": "Dynamic-SUPERB Phase-2: A Collaboratively Expanding Benchmark for Measuring the Capabilities of Spoken Language Models with 180 Tasks",
      "summary": "Multimodal foundation models, such as Gemini and ChatGPT, have revolutionized human-machine interactions by seamlessly integrating various forms of data. Developing a universal spoken language model that comprehends a wide range of natural language instructions is critical for bridging communication gaps and facilitating more intuitive interactions. However, the absence of a comprehensive evaluation benchmark poses a significant challenge. We present Dynamic-SUPERB Phase-2, an open and evolving benchmark for the comprehensive evaluation of instruction-based universal speech models. Building upon the first generation, this second version incorporates 125 new tasks contributed collaboratively by the global research community, expanding the benchmark to a total of 180 tasks, making it the largest benchmark for speech and audio evaluation. While the first generation of Dynamic-SUPERB was limited to classification tasks, Dynamic-SUPERB Phase-2 broadens its evaluation capabilities by introducing a wide array of novel and diverse tasks, including regression and sequence generation, across speech, music, and environmental audio. Evaluation results show that no model performed well universally. SALMONN-13B excelled in English ASR and Qwen2-Audio-7B-Instruct showed high accuracy in emotion recognition, but current models still require further innovations to handle a broader range of tasks. We open-source all task data and the evaluation pipeline at https://github.com/dynamic-superb/dynamic-superb.",
      "authors": [
        "Chien-yu Huang",
        "Wei-Chih Chen",
        "Shu-wen Yang",
        "Andy T. Liu",
        "Chen-An Li",
        "Yu-Xiang Lin",
        "Wei-Cheng Tseng",
        "Anuj Diwan",
        "Yi-Jen Shih",
        "Jiatong Shi",
        "William Chen",
        "Chih-Kai Yang",
        "Wenze Ren",
        "Xuanjun Chen",
        "Chi-Yuan Hsiao",
        "Puyuan Peng",
        "Shih-Heng Wang",
        "Chun-Yi Kuan",
        "Ke-Han Lu",
        "Kai-Wei Chang",
        "Fabian Ritter-Gutierrez",
        "Kuan-Po Huang",
        "Siddhant Arora",
        "You-Kuan Lin",
        "Ming To Chuang",
        "Eunjung Yeo",
        "Kalvin Chang",
        "Chung-Ming Chien",
        "Kwanghee Choi",
        "Jun-You Wang",
        "Cheng-Hsiu Hsieh",
        "Yi-Cheng Lin",
        "Chee-En Yu",
        "I-Hsiang Chiu",
        "Heitor R. Guimarães",
        "Jionghao Han",
        "Tzu-Quan Lin",
        "Tzu-Yuan Lin",
        "Homu Chang",
        "Ting-Wu Chang",
        "Chun Wei Chen",
        "Shou-Jen Chen",
        "Yu-Hua Chen",
        "Hsi-Chun Cheng",
        "Kunal Dhawan",
        "Jia-Lin Fang",
        "Shi-Xin Fang",
        "Kuan-Yu Fang Chiang",
        "Chi An Fu",
        "Hsien-Fu Hsiao",
        "Ching Yu Hsu",
        "Shao-Syuan Huang",
        "Lee Chen Wei",
        "Hsi-Che Lin",
        "Hsuan-Hao Lin",
        "Hsuan-Ting Lin",
        "Jian-Ren Lin",
        "Ting-Chun Liu",
        "Li-Chun Lu",
        "Tsung-Min Pai",
        "Ankita Pasad",
        "Shih-Yun Shan Kuan",
        "Suwon Shon",
        "Yuxun Tang",
        "Yun-Shao Tsai",
        "Jui-Chiang Wei",
        "Tzu-Chieh Wei",
        "Chengxi Wu",
        "Dien-Ruei Wu",
        "Chao-Han Huck Yang",
        "Chieh-Chi Yang",
        "Jia Qi Yip",
        "Shao-Xiang Yuan",
        "Vahid Noroozi",
        "Zhehuai Chen",
        "Haibin Wu",
        "Karen Livescu",
        "David Harwath",
        "Shinji Watanabe",
        "Hung-yi Lee"
      ],
      "published": "2024-11-08T06:33:22Z",
      "updated": "2025-06-09T16:36:12Z",
      "categories": [
        "cs.CL",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2411.05361v2",
      "landing_url": "https://arxiv.org/abs/2411.05361v2",
      "doi": "https://doi.org/10.48550/arXiv.2411.05361"
    },
    "queries": [
      {
        "anchor": "spoken language models",
        "search_term": "true full-duplex"
      },
      {
        "anchor": "spoken language models",
        "search_term": "pseudo full-duplex"
      },
      {
        "anchor": "spoken language models",
        "search_term": "half duplex"
      },
      {
        "anchor": "spoken language models",
        "search_term": "engineered synchronization"
      },
      {
        "anchor": "spoken language models",
        "search_term": "learned synchronization"
      },
      {
        "anchor": "spoken language models",
        "search_term": "modular architecture"
      },
      {
        "anchor": "spoken language models",
        "search_term": "streaming architecture"
      },
      {
        "anchor": "spoken language models",
        "search_term": "real-time interaction"
      },
      {
        "anchor": "spoken language models",
        "search_term": "low latency"
      },
      {
        "anchor": "spoken language models",
        "search_term": "synchronous data"
      },
      {
        "anchor": "spoken language models",
        "search_term": "data scarcity"
      },
      {
        "anchor": "spoken language models",
        "search_term": "multichannel dialogue"
      },
      {
        "anchor": "spoken language models",
        "search_term": "temporal dynamics"
      },
      {
        "anchor": "spoken language models",
        "search_term": "behavioral arbitration"
      },
      {
        "anchor": "spoken language models",
        "search_term": "semantic coherence"
      },
      {
        "anchor": "spoken language models",
        "search_term": "neural codec"
      },
      {
        "anchor": "spoken language models",
        "search_term": "discrete tokens"
      },
      {
        "anchor": "spoken language models",
        "search_term": "audio tokens"
      },
      {
        "anchor": "spoken language models",
        "search_term": "floor transfer offset"
      },
      {
        "anchor": "spoken language models",
        "search_term": "stop latency"
      },
      {
        "anchor": "spoken language models",
        "search_term": "interruption response delay"
      }
    ]
  },
  {
    "arxiv_id": "2411.05679",
    "anchor": "spoken language models",
    "search_term": "audio tokens",
    "search_record": {
      "id": "http://arxiv.org/abs/2411.05679v3",
      "title": "Tell What You Hear From What You See -- Video to Audio Generation Through Text",
      "summary": "The content of visual and audio scenes is multi-faceted such that a video can be paired with various audio and vice-versa. Thereby, in video-to-audio generation task, it is imperative to introduce steering approaches for controlling the generated audio. While Video-to-Audio generation is a well-established generative task, existing methods lack such controllability. In this work, we propose VATT, a multi-modal generative framework that takes a video and an optional text prompt as input, and generates audio and optional textual description of the audio. Such a framework has two advantages: i) Video-to-Audio generation process can be refined and controlled via text which complements the context of visual information, and ii) The model can suggest what audio to generate for the video by generating audio captions. VATT consists of two key modules: VATT Converter, a LLM that is fine-tuned for instructions and includes a projection layer that maps video features to the LLM vector space; and VATT Audio, a transformer that generates audio tokens from visual frames and from optional text prompt using iterative parallel decoding. The audio tokens are converted to a waveform by pretrained neural codec. Experiments show that when VATT is compared to existing video-to-audio generation methods in objective metrics, it achieves competitive performance when the audio caption is not provided. When the audio caption is provided as a prompt, VATT achieves even more refined performance (lowest KLD score of 1.41). Furthermore, subjective studies show that VATT Audio has been chosen as preferred generated audio than audio generated by existing methods. VATT enables controllable video-to-audio generation through text as well as suggesting text prompts for videos through audio captions, unlocking novel applications such as text-guided video-to-audio generation and video-to-audio captioning.",
      "published": "2024-11-08T16:29:07Z"
    },
    "metadata": {
      "arxiv_id": "2411.05679",
      "title": "Tell What You Hear From What You See -- Video to Audio Generation Through Text",
      "summary": "The content of visual and audio scenes is multi-faceted such that a video can be paired with various audio and vice-versa. Thereby, in video-to-audio generation task, it is imperative to introduce steering approaches for controlling the generated audio. While Video-to-Audio generation is a well-established generative task, existing methods lack such controllability. In this work, we propose VATT, a multi-modal generative framework that takes a video and an optional text prompt as input, and generates audio and optional textual description of the audio. Such a framework has two advantages: i) Video-to-Audio generation process can be refined and controlled via text which complements the context of visual information, and ii) The model can suggest what audio to generate for the video by generating audio captions. VATT consists of two key modules: VATT Converter, a LLM that is fine-tuned for instructions and includes a projection layer that maps video features to the LLM vector space; and VATT Audio, a transformer that generates audio tokens from visual frames and from optional text prompt using iterative parallel decoding. The audio tokens are converted to a waveform by pretrained neural codec. Experiments show that when VATT is compared to existing video-to-audio generation methods in objective metrics, it achieves competitive performance when the audio caption is not provided. When the audio caption is provided as a prompt, VATT achieves even more refined performance (lowest KLD score of 1.41). Furthermore, subjective studies show that VATT Audio has been chosen as preferred generated audio than audio generated by existing methods. VATT enables controllable video-to-audio generation through text as well as suggesting text prompts for videos through audio captions, unlocking novel applications such as text-guided video-to-audio generation and video-to-audio captioning.",
      "authors": [
        "Xiulong Liu",
        "Kun Su",
        "Eli Shlizerman"
      ],
      "published": "2024-11-08T16:29:07Z",
      "updated": "2025-04-04T21:50:29Z",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG",
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2411.05679v3",
      "landing_url": "https://arxiv.org/abs/2411.05679v3",
      "doi": "https://doi.org/10.48550/arXiv.2411.05679"
    },
    "queries": [
      {
        "anchor": "spoken language models",
        "search_term": "audio tokens"
      },
      {
        "anchor": "synchronous dialogue",
        "search_term": "neural codec"
      },
      {
        "anchor": "synchronous dialogue",
        "search_term": "audio tokens"
      },
      {
        "anchor": "turn-taking",
        "search_term": "audio tokens"
      }
    ]
  },
  {
    "arxiv_id": "2411.06484",
    "anchor": "synchronous dialogue",
    "search_term": "modular architecture",
    "search_record": {
      "id": "http://arxiv.org/abs/2411.06484v2",
      "title": "ajdmom: A Python Package for Deriving Moment Formulas of Affine Jump Diffusion Processes",
      "summary": "We introduce ajdmom, a Python package designed for automatically deriving moment formulae for the well-established affine jump diffusion processes with state-independent jump intensities. ajdmom can produce explicit closed-form expressions for conditional and unconditional moments of any order, significantly enhancing the usability of these models. Additionally, ajdmom can compute partial derivatives of these moments with respect to the model parameters, offering a valuable tool for sensitivity analysis. The package's modular architecture makes it easy for adaptation and extension by researchers. ajdmom is open-source and readily available for installation from GitHub or the Python package index (PyPI).",
      "published": "2024-11-10T14:55:10Z"
    },
    "metadata": {
      "arxiv_id": "2411.06484",
      "title": "ajdmom: A Python Package for Deriving Moment Formulas of Affine Jump Diffusion Processes",
      "summary": "We introduce ajdmom, a Python package designed for automatically deriving moment formulae for the well-established affine jump diffusion processes with state-independent jump intensities. ajdmom can produce explicit closed-form expressions for conditional and unconditional moments of any order, significantly enhancing the usability of these models. Additionally, ajdmom can compute partial derivatives of these moments with respect to the model parameters, offering a valuable tool for sensitivity analysis. The package's modular architecture makes it easy for adaptation and extension by researchers. ajdmom is open-source and readily available for installation from GitHub or the Python package index (PyPI).",
      "authors": [
        "Yan-Feng Wu",
        "Jian-Qiang Hu"
      ],
      "published": "2024-11-10T14:55:10Z",
      "updated": "2025-04-07T02:27:58Z",
      "categories": [
        "q-fin.MF"
      ],
      "pdf_url": "https://arxiv.org/pdf/2411.06484v2",
      "landing_url": "https://arxiv.org/abs/2411.06484v2",
      "doi": "https://doi.org/10.48550/arXiv.2411.06484"
    },
    "queries": [
      {
        "anchor": "synchronous dialogue",
        "search_term": "modular architecture"
      }
    ]
  },
  {
    "arxiv_id": "2411.07111",
    "anchor": "spoken language models",
    "search_term": "true full-duplex",
    "search_record": {
      "id": "http://arxiv.org/abs/2411.07111v2",
      "title": "Building a Taiwanese Mandarin Spoken Language Model: A First Attempt",
      "summary": "This technical report presents our initial attempt to build a spoken large language model (LLM) for Taiwanese Mandarin, specifically tailored to enable real-time, speech-to-speech interaction in multi-turn conversations. Our end-to-end model incorporates a decoder-only transformer architecture and aims to achieve seamless interaction while preserving the conversational flow, including full-duplex capabilities allowing simultaneous speaking and listening. The paper also details the training process, including data preparation with synthesized dialogues and adjustments for real-time interaction. We also developed a platform to evaluate conversational fluency and response coherence in multi-turn dialogues. We hope the release of the report can contribute to the future development of spoken LLMs in Taiwanese Mandarin.",
      "published": "2024-11-11T16:37:40Z"
    },
    "metadata": {
      "arxiv_id": "2411.07111",
      "title": "Building a Taiwanese Mandarin Spoken Language Model: A First Attempt",
      "summary": "This technical report presents our initial attempt to build a spoken large language model (LLM) for Taiwanese Mandarin, specifically tailored to enable real-time, speech-to-speech interaction in multi-turn conversations. Our end-to-end model incorporates a decoder-only transformer architecture and aims to achieve seamless interaction while preserving the conversational flow, including full-duplex capabilities allowing simultaneous speaking and listening. The paper also details the training process, including data preparation with synthesized dialogues and adjustments for real-time interaction. We also developed a platform to evaluate conversational fluency and response coherence in multi-turn dialogues. We hope the release of the report can contribute to the future development of spoken LLMs in Taiwanese Mandarin.",
      "authors": [
        "Chih-Kai Yang",
        "Yu-Kuan Fu",
        "Chen-An Li",
        "Yi-Cheng Lin",
        "Yu-Xiang Lin",
        "Wei-Chih Chen",
        "Ho Lam Chung",
        "Chun-Yi Kuan",
        "Wei-Ping Huang",
        "Ke-Han Lu",
        "Tzu-Quan Lin",
        "Hsiu-Hsuan Wang",
        "En-Pei Hu",
        "Chan-Jan Hsu",
        "Liang-Hsuan Tseng",
        "I-Hsiang Chiu",
        "Ulin Sanga",
        "Xuanjun Chen",
        "Po-chun Hsu",
        "Shu-wen Yang",
        "Hung-yi Lee"
      ],
      "published": "2024-11-11T16:37:40Z",
      "updated": "2024-12-27T07:29:19Z",
      "categories": [
        "cs.CL",
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2411.07111v2",
      "landing_url": "https://arxiv.org/abs/2411.07111v2",
      "doi": "https://doi.org/10.48550/arXiv.2411.07111"
    },
    "queries": [
      {
        "anchor": "spoken language models",
        "search_term": "true full-duplex"
      },
      {
        "anchor": "spoken language models",
        "search_term": "pseudo full-duplex"
      },
      {
        "anchor": "spoken language models",
        "search_term": "engineered synchronization"
      },
      {
        "anchor": "spoken language models",
        "search_term": "learned synchronization"
      },
      {
        "anchor": "spoken language models",
        "search_term": "modular architecture"
      },
      {
        "anchor": "spoken language models",
        "search_term": "streaming architecture"
      },
      {
        "anchor": "spoken language models",
        "search_term": "real-time interaction"
      },
      {
        "anchor": "spoken language models",
        "search_term": "synchronous data"
      },
      {
        "anchor": "spoken language models",
        "search_term": "data scarcity"
      },
      {
        "anchor": "spoken language models",
        "search_term": "multichannel dialogue"
      },
      {
        "anchor": "spoken language models",
        "search_term": "behavioral arbitration"
      },
      {
        "anchor": "spoken language models",
        "search_term": "semantic coherence"
      },
      {
        "anchor": "spoken language models",
        "search_term": "neural codec"
      },
      {
        "anchor": "spoken language models",
        "search_term": "discrete tokens"
      },
      {
        "anchor": "spoken language models",
        "search_term": "audio tokens"
      },
      {
        "anchor": "spoken language models",
        "search_term": "floor transfer offset"
      },
      {
        "anchor": "spoken language models",
        "search_term": "stop latency"
      },
      {
        "anchor": "spoken language models",
        "search_term": "interruption response delay"
      },
      {
        "anchor": "synchronous dialogue",
        "search_term": "real-time interaction"
      }
    ]
  },
  {
    "arxiv_id": "2411.07449",
    "anchor": "spoken language models",
    "search_term": "temporal dynamics",
    "search_record": {
      "id": "http://arxiv.org/abs/2411.07449v3",
      "title": "Tracing the Roots: Leveraging Temporal Dynamics in Diffusion Trajectories for Origin Attribution",
      "summary": "Diffusion models have transformed image synthesis through iterative denoising, by defining trajectories from noise to coherent data. While their capabilities are widely celebrated, a critical challenge remains unaddressed: ensuring responsible use by verifying whether an image originates from a model's training set, its novel generations or external sources. We introduce a framework that analyzes diffusion trajectories for this purpose. Specifically, we demonstrate that temporal dynamics across the entire trajectory allow for more robust classification and challenge the widely-adopted \"Goldilocks zone\" conjecture, which posits that membership inference is effective only within narrow denoising stages. More fundamentally, we expose critical flaws in current membership inference practices by showing that representative methods fail under distribution shifts or when model-generated data is present. For model attribution, we demonstrate a first white-box approach directly applicable to diffusion. Ultimately, we propose the unification of data provenance into a single, cohesive framework tailored to modern generative systems.",
      "published": "2024-11-12T00:20:11Z"
    },
    "metadata": {
      "arxiv_id": "2411.07449",
      "title": "Tracing the Roots: Leveraging Temporal Dynamics in Diffusion Trajectories for Origin Attribution",
      "summary": "Diffusion models have transformed image synthesis through iterative denoising, by defining trajectories from noise to coherent data. While their capabilities are widely celebrated, a critical challenge remains unaddressed: ensuring responsible use by verifying whether an image originates from a model's training set, its novel generations or external sources. We introduce a framework that analyzes diffusion trajectories for this purpose. Specifically, we demonstrate that temporal dynamics across the entire trajectory allow for more robust classification and challenge the widely-adopted \"Goldilocks zone\" conjecture, which posits that membership inference is effective only within narrow denoising stages. More fundamentally, we expose critical flaws in current membership inference practices by showing that representative methods fail under distribution shifts or when model-generated data is present. For model attribution, we demonstrate a first white-box approach directly applicable to diffusion. Ultimately, we propose the unification of data provenance into a single, cohesive framework tailored to modern generative systems.",
      "authors": [
        "Andreas Floros",
        "Seyed-Mohsen Moosavi-Dezfooli",
        "Pier Luigi Dragotti"
      ],
      "published": "2024-11-12T00:20:11Z",
      "updated": "2025-12-18T22:55:10Z",
      "categories": [
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2411.07449v3",
      "landing_url": "https://arxiv.org/abs/2411.07449v3",
      "doi": "https://doi.org/10.48550/arXiv.2411.07449"
    },
    "queries": [
      {
        "anchor": "spoken language models",
        "search_term": "temporal dynamics"
      },
      {
        "anchor": "synchronous dialogue",
        "search_term": "temporal dynamics"
      }
    ]
  },
  {
    "arxiv_id": "2411.08472",
    "anchor": "synchronous dialogue",
    "search_term": "streaming architecture",
    "search_record": {
      "id": "http://arxiv.org/abs/2411.08472v1",
      "title": "A survey on Graph Deep Representation Learning for Facial Expression Recognition",
      "summary": "This comprehensive review delves deeply into the various methodologies applied to facial expression recognition (FER) through the lens of graph representation learning (GRL). Initially, we introduce the task of FER and the concepts of graph representation and GRL. Afterward, we discuss some of the most prevalent and valuable databases for this task. We explore promising approaches for graph representation in FER, including graph diffusion, spatio-temporal graphs, and multi-stream architectures. Finally, we identify future research opportunities and provide concluding remarks.",
      "published": "2024-11-13T09:46:08Z"
    },
    "metadata": {
      "arxiv_id": "2411.08472",
      "title": "A survey on Graph Deep Representation Learning for Facial Expression Recognition",
      "summary": "This comprehensive review delves deeply into the various methodologies applied to facial expression recognition (FER) through the lens of graph representation learning (GRL). Initially, we introduce the task of FER and the concepts of graph representation and GRL. Afterward, we discuss some of the most prevalent and valuable databases for this task. We explore promising approaches for graph representation in FER, including graph diffusion, spatio-temporal graphs, and multi-stream architectures. Finally, we identify future research opportunities and provide concluding remarks.",
      "authors": [
        "Théo Gueuret",
        "Akrem Sellami",
        "Chaabane Djeraba"
      ],
      "published": "2024-11-13T09:46:08Z",
      "updated": "2024-11-13T09:46:08Z",
      "categories": [
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2411.08472v1",
      "landing_url": "https://arxiv.org/abs/2411.08472v1",
      "doi": "https://doi.org/10.48550/arXiv.2411.08472"
    },
    "queries": [
      {
        "anchor": "synchronous dialogue",
        "search_term": "streaming architecture"
      }
    ]
  },
  {
    "arxiv_id": "2411.08742",
    "anchor": "spoken language models",
    "search_term": "discrete tokens",
    "search_record": {
      "id": "http://arxiv.org/abs/2411.08742v1",
      "title": "A Comparative Study of Discrete Speech Tokens for Semantic-Related Tasks with Large Language Models",
      "summary": "With the rise of Speech Large Language Models (Speech LLMs), there has been growing interest in discrete speech tokens for their ability to integrate with text-based tokens seamlessly. Compared to most studies that focus on continuous speech features, although discrete-token based LLMs have shown promising results on certain tasks, the performance gap between these two paradigms is rarely explored. In this paper, we present a fair and thorough comparison between discrete and continuous features across a variety of semantic-related tasks using a light-weight LLM (Qwen1.5-0.5B). Our findings reveal that continuous features generally outperform discrete tokens, particularly in tasks requiring fine-grained semantic understanding. Moreover, this study goes beyond surface-level comparison by identifying key factors behind the under-performance of discrete tokens, such as limited token granularity and inefficient information retention. To enhance the performance of discrete tokens, we explore potential aspects based on our analysis. We hope our results can offer new insights into the opportunities for advancing discrete speech tokens in Speech LLMs.",
      "published": "2024-11-13T16:20:20Z"
    },
    "metadata": {
      "arxiv_id": "2411.08742",
      "title": "A Comparative Study of Discrete Speech Tokens for Semantic-Related Tasks with Large Language Models",
      "summary": "With the rise of Speech Large Language Models (Speech LLMs), there has been growing interest in discrete speech tokens for their ability to integrate with text-based tokens seamlessly. Compared to most studies that focus on continuous speech features, although discrete-token based LLMs have shown promising results on certain tasks, the performance gap between these two paradigms is rarely explored. In this paper, we present a fair and thorough comparison between discrete and continuous features across a variety of semantic-related tasks using a light-weight LLM (Qwen1.5-0.5B). Our findings reveal that continuous features generally outperform discrete tokens, particularly in tasks requiring fine-grained semantic understanding. Moreover, this study goes beyond surface-level comparison by identifying key factors behind the under-performance of discrete tokens, such as limited token granularity and inefficient information retention. To enhance the performance of discrete tokens, we explore potential aspects based on our analysis. We hope our results can offer new insights into the opportunities for advancing discrete speech tokens in Speech LLMs.",
      "authors": [
        "Dingdong Wang",
        "Mingyu Cui",
        "Dongchao Yang",
        "Xueyuan Chen",
        "Helen Meng"
      ],
      "published": "2024-11-13T16:20:20Z",
      "updated": "2024-11-13T16:20:20Z",
      "categories": [
        "cs.CL",
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2411.08742v1",
      "landing_url": "https://arxiv.org/abs/2411.08742v1",
      "doi": "https://doi.org/10.48550/arXiv.2411.08742"
    },
    "queries": [
      {
        "anchor": "spoken language models",
        "search_term": "discrete tokens"
      },
      {
        "anchor": "synchronous dialogue",
        "search_term": "discrete tokens"
      }
    ]
  },
  {
    "arxiv_id": "2411.10423",
    "anchor": "synchronous dialogue",
    "search_term": "audio tokens",
    "search_record": {
      "id": "http://arxiv.org/abs/2411.10423v1",
      "title": "Back to Supervision: Boosting Word Boundary Detection through Frame Classification",
      "summary": "Speech segmentation at both word and phoneme levels is crucial for various speech processing tasks. It significantly aids in extracting meaningful units from an utterance, thus enabling the generation of discrete elements. In this work we propose a model-agnostic framework to perform word boundary detection in a supervised manner also employing a labels augmentation technique and an output-frame selection strategy. We trained and tested on the Buckeye dataset and only tested on TIMIT one, using state-of-the-art encoder models, including pre-trained solutions (Wav2Vec 2.0 and HuBERT), as well as convolutional and convolutional recurrent networks. Our method, with the HuBERT encoder, surpasses the performance of other state-of-the-art architectures, whether trained in supervised or self-supervised settings on the same datasets. Specifically, we achieved F-values of 0.8427 on the Buckeye dataset and 0.7436 on the TIMIT dataset, along with R-values of 0.8489 and 0.7807, respectively. These results establish a new state-of-the-art for both datasets. Beyond the immediate task, our approach offers a robust and efficient preprocessing method for future research in audio tokenization.",
      "published": "2024-11-15T18:43:29Z"
    },
    "metadata": {
      "arxiv_id": "2411.10423",
      "title": "Back to Supervision: Boosting Word Boundary Detection through Frame Classification",
      "summary": "Speech segmentation at both word and phoneme levels is crucial for various speech processing tasks. It significantly aids in extracting meaningful units from an utterance, thus enabling the generation of discrete elements. In this work we propose a model-agnostic framework to perform word boundary detection in a supervised manner also employing a labels augmentation technique and an output-frame selection strategy. We trained and tested on the Buckeye dataset and only tested on TIMIT one, using state-of-the-art encoder models, including pre-trained solutions (Wav2Vec 2.0 and HuBERT), as well as convolutional and convolutional recurrent networks. Our method, with the HuBERT encoder, surpasses the performance of other state-of-the-art architectures, whether trained in supervised or self-supervised settings on the same datasets. Specifically, we achieved F-values of 0.8427 on the Buckeye dataset and 0.7436 on the TIMIT dataset, along with R-values of 0.8489 and 0.7807, respectively. These results establish a new state-of-the-art for both datasets. Beyond the immediate task, our approach offers a robust and efficient preprocessing method for future research in audio tokenization.",
      "authors": [
        "Simone Carnemolla",
        "Salvatore Calcagno",
        "Simone Palazzo",
        "Daniela Giordano"
      ],
      "published": "2024-11-15T18:43:29Z",
      "updated": "2024-11-15T18:43:29Z",
      "categories": [
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2411.10423v1",
      "landing_url": "https://arxiv.org/abs/2411.10423v1",
      "doi": "https://doi.org/10.48550/arXiv.2411.10423"
    },
    "queries": [
      {
        "anchor": "synchronous dialogue",
        "search_term": "audio tokens"
      }
    ]
  },
  {
    "arxiv_id": "2411.11123",
    "anchor": "synchronous dialogue",
    "search_term": "neural codec",
    "search_record": {
      "id": "http://arxiv.org/abs/2411.11123v3",
      "title": "Pitch-and-Spectrum-Aware Singing Quality Assessment with Bias Correction and Model Fusion",
      "summary": "We participated in track 2 of the VoiceMOS Challenge 2024, which aimed to predict the mean opinion score (MOS) of singing samples. Our submission secured the first place among all participating teams, excluding the official baseline. In this paper, we further improve our submission and propose a novel Pitch-and-Spectrum-aware Singing Quality Assessment (PS-SQA) method. The PS-SQA is designed based on the self-supervised-learning (SSL) MOS predictor, incorporating singing pitch and spectral information, which are extracted using pitch histogram and non-quantized neural codec, respectively. Additionally, the PS-SQA introduces a bias correction strategy to address prediction biases caused by low-resource training samples, and employs model fusion technology to further enhance prediction accuracy. Experimental results confirm that our proposed PS-SQA significantly outperforms all competing systems across all system-level metrics, confirming its strong sing quality assessment capabilities.",
      "published": "2024-11-17T16:53:39Z"
    },
    "metadata": {
      "arxiv_id": "2411.11123",
      "title": "Pitch-and-Spectrum-Aware Singing Quality Assessment with Bias Correction and Model Fusion",
      "summary": "We participated in track 2 of the VoiceMOS Challenge 2024, which aimed to predict the mean opinion score (MOS) of singing samples. Our submission secured the first place among all participating teams, excluding the official baseline. In this paper, we further improve our submission and propose a novel Pitch-and-Spectrum-aware Singing Quality Assessment (PS-SQA) method. The PS-SQA is designed based on the self-supervised-learning (SSL) MOS predictor, incorporating singing pitch and spectral information, which are extracted using pitch histogram and non-quantized neural codec, respectively. Additionally, the PS-SQA introduces a bias correction strategy to address prediction biases caused by low-resource training samples, and employs model fusion technology to further enhance prediction accuracy. Experimental results confirm that our proposed PS-SQA significantly outperforms all competing systems across all system-level metrics, confirming its strong sing quality assessment capabilities.",
      "authors": [
        "Yu-Fei Shi",
        "Yang Ai",
        "Ye-Xin Lu",
        "Hui-Peng Du",
        "Zhen-Hua Ling"
      ],
      "published": "2024-11-17T16:53:39Z",
      "updated": "2024-12-23T12:42:03Z",
      "categories": [
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2411.11123v3",
      "landing_url": "https://arxiv.org/abs/2411.11123v3",
      "doi": "https://doi.org/10.48550/arXiv.2411.11123"
    },
    "queries": [
      {
        "anchor": "synchronous dialogue",
        "search_term": "neural codec"
      }
    ]
  },
  {
    "arxiv_id": "2411.11545",
    "anchor": "spoken language models",
    "search_term": "modular architecture",
    "search_record": {
      "id": "http://arxiv.org/abs/2411.11545v1",
      "title": "An Efficient Multicast Addressing Encoding Scheme for Multi-Core Neuromorphic Processors",
      "summary": "Multi-core neuromorphic processors are becoming increasingly significant due to their energy-efficient local computing and scalable modular architecture, particularly for event-based processing applications. However, minimizing the cost of inter-core communication, which accounts for the majority of energy usage, remains a challenging issue. Beyond optimizing circuit design at lower abstraction levels, an efficient multicast addressing scheme is crucial. We propose a hierarchical bit string encoding scheme that largely expands the addressing capability of state-of-the-art symbol-based schemes for the same number of routing bits. When put at work with a real neuromorphic task, this hierarchical bit string encoding achieves a reduction in area cost by approximately 29% and decreases energy consumption by about 50%.",
      "published": "2024-11-18T13:04:38Z"
    },
    "metadata": {
      "arxiv_id": "2411.11545",
      "title": "An Efficient Multicast Addressing Encoding Scheme for Multi-Core Neuromorphic Processors",
      "summary": "Multi-core neuromorphic processors are becoming increasingly significant due to their energy-efficient local computing and scalable modular architecture, particularly for event-based processing applications. However, minimizing the cost of inter-core communication, which accounts for the majority of energy usage, remains a challenging issue. Beyond optimizing circuit design at lower abstraction levels, an efficient multicast addressing scheme is crucial. We propose a hierarchical bit string encoding scheme that largely expands the addressing capability of state-of-the-art symbol-based schemes for the same number of routing bits. When put at work with a real neuromorphic task, this hierarchical bit string encoding achieves a reduction in area cost by approximately 29% and decreases energy consumption by about 50%.",
      "authors": [
        "Zhe Su",
        "Aron Bencsik",
        "Giacomo Indiveri",
        "Davide Bertozzi"
      ],
      "published": "2024-11-18T13:04:38Z",
      "updated": "2024-11-18T13:04:38Z",
      "categories": [
        "cs.AR",
        "cs.NE"
      ],
      "pdf_url": "https://arxiv.org/pdf/2411.11545v1",
      "landing_url": "https://arxiv.org/abs/2411.11545v1",
      "doi": "https://doi.org/10.48550/arXiv.2411.11545"
    },
    "queries": [
      {
        "anchor": "spoken language models",
        "search_term": "modular architecture"
      },
      {
        "anchor": "synchronous dialogue",
        "search_term": "modular architecture"
      },
      {
        "anchor": "turn-taking",
        "search_term": "modular architecture"
      }
    ]
  },
  {
    "arxiv_id": "2411.12271",
    "anchor": "spoken language models",
    "search_term": "real-time interaction",
    "search_record": {
      "id": "http://arxiv.org/abs/2411.12271v1",
      "title": "SMT-Layout: A MaxSMT-based Approach Supporting Real-time Interaction of Real-world GUI Layout",
      "summary": "Leveraging the flexible expressive ability of (Max)SMT and the powerful solving ability of SMT solvers, we propose a novel layout model named SMT-Layout. SMT-Layout is the first constraint-based layout model that can support real-time interaction for real-world GUI layout adapting to various screen sizes with only one specification. Previous works neglect the hierarchy information among widgets and thus cannot exploit the reasoning ability of solvers. For the first time, we introduce Boolean variables to encode the hierarchy relationship, boosting the reasoning ability of SMT solvers. The workflow is divided into two stages. At the development end, two novel preprocessing methods are proposed to simplify constraints and extract useful information in advance, easing the solving burden. After deploying constraints to the terminal end, SMT solvers are applied to solve constraints incrementally. Besides mainstream SMT solvers, a local search solver is customized to this scenario. Experiments show that SMT-Layout can support millisecond-level interaction for real-world layouts, even on devices with low computing power and rigorous memory limitations.",
      "published": "2024-11-19T06:48:53Z"
    },
    "metadata": {
      "arxiv_id": "2411.12271",
      "title": "SMT-Layout: A MaxSMT-based Approach Supporting Real-time Interaction of Real-world GUI Layout",
      "summary": "Leveraging the flexible expressive ability of (Max)SMT and the powerful solving ability of SMT solvers, we propose a novel layout model named SMT-Layout. SMT-Layout is the first constraint-based layout model that can support real-time interaction for real-world GUI layout adapting to various screen sizes with only one specification. Previous works neglect the hierarchy information among widgets and thus cannot exploit the reasoning ability of solvers. For the first time, we introduce Boolean variables to encode the hierarchy relationship, boosting the reasoning ability of SMT solvers. The workflow is divided into two stages. At the development end, two novel preprocessing methods are proposed to simplify constraints and extract useful information in advance, easing the solving burden. After deploying constraints to the terminal end, SMT solvers are applied to solve constraints incrementally. Besides mainstream SMT solvers, a local search solver is customized to this scenario. Experiments show that SMT-Layout can support millisecond-level interaction for real-world layouts, even on devices with low computing power and rigorous memory limitations.",
      "authors": [
        "Bohan Li",
        "Dawei Li",
        "Ming Fu",
        "Shaowei Cai"
      ],
      "published": "2024-11-19T06:48:53Z",
      "updated": "2024-11-19T06:48:53Z",
      "categories": [
        "cs.LO",
        "cs.HC"
      ],
      "pdf_url": "https://arxiv.org/pdf/2411.12271v1",
      "landing_url": "https://arxiv.org/abs/2411.12271v1",
      "doi": "https://doi.org/10.48550/arXiv.2411.12271"
    },
    "queries": [
      {
        "anchor": "spoken language models",
        "search_term": "real-time interaction"
      },
      {
        "anchor": "synchronous dialogue",
        "search_term": "real-time interaction"
      },
      {
        "anchor": "turn-taking",
        "search_term": "real-time interaction"
      }
    ]
  },
  {
    "arxiv_id": "2411.12619",
    "anchor": "synchronous dialogue",
    "search_term": "real-time interaction",
    "search_record": {
      "id": "http://arxiv.org/abs/2411.12619v1",
      "title": "Leveraging Virtual Reality and AI Tutoring for Language Learning: A Case Study of a Virtual Campus Environment with OpenAI GPT Integration with Unity 3D",
      "summary": "This paper presents a new approach to multiple language learning, with Hindi the language to be learnt in our case, by using the integration of virtual reality environments and AI enabled tutoring systems using OpenAIs GPT api calls. We have developed a scenario which has a virtual campus environment using Unity which focuses on a detailed representation of our universitys buildings 11th floor, where most of the cultural and technological activities take place. Within this virtual environment that we have created, we have an AI tutor powered by OpenAI's GPT model which was called using an api which moves around with the user. This provided language learning support in Hindi, as GPT is able to take care of language translation. Our approach mainly involves utilising speech to text, text to text conversion and text to speech capabilities to facilitate real time interaction between users and the AI tutor in the presence of internet. This research demonstrates the use of combining VR technology with AI tutoring for immersive language learning experiences and provides interaction.",
      "published": "2024-11-19T16:26:19Z"
    },
    "metadata": {
      "arxiv_id": "2411.12619",
      "title": "Leveraging Virtual Reality and AI Tutoring for Language Learning: A Case Study of a Virtual Campus Environment with OpenAI GPT Integration with Unity 3D",
      "summary": "This paper presents a new approach to multiple language learning, with Hindi the language to be learnt in our case, by using the integration of virtual reality environments and AI enabled tutoring systems using OpenAIs GPT api calls. We have developed a scenario which has a virtual campus environment using Unity which focuses on a detailed representation of our universitys buildings 11th floor, where most of the cultural and technological activities take place. Within this virtual environment that we have created, we have an AI tutor powered by OpenAI's GPT model which was called using an api which moves around with the user. This provided language learning support in Hindi, as GPT is able to take care of language translation. Our approach mainly involves utilising speech to text, text to text conversion and text to speech capabilities to facilitate real time interaction between users and the AI tutor in the presence of internet. This research demonstrates the use of combining VR technology with AI tutoring for immersive language learning experiences and provides interaction.",
      "authors": [
        "Adithya TG",
        "Abhinavaram N",
        "Gowri Srinivasa"
      ],
      "published": "2024-11-19T16:26:19Z",
      "updated": "2024-11-19T16:26:19Z",
      "categories": [
        "cs.HC",
        "cs.CL"
      ],
      "pdf_url": "https://arxiv.org/pdf/2411.12619v1",
      "landing_url": "https://arxiv.org/abs/2411.12619v1",
      "doi": "https://doi.org/10.48550/arXiv.2411.12619"
    },
    "queries": [
      {
        "anchor": "synchronous dialogue",
        "search_term": "real-time interaction"
      }
    ]
  },
  {
    "arxiv_id": "2411.13183",
    "anchor": "spoken language models",
    "search_term": "real-time interaction",
    "search_record": {
      "id": "http://arxiv.org/abs/2411.13183v2",
      "title": "ClickTrack: Towards Real-time Interactive Single Object Tracking",
      "summary": "Single object tracking(SOT) relies on precise object bounding box initialization. In this paper, we reconsidered the deficiencies in the current approaches to initializing single object trackers and propose a new paradigm for single object tracking algorithms, ClickTrack, a new paradigm using clicking interaction for real-time scenarios. Moreover, click as an input type inherently lack hierarchical information. To address ambiguity in certain special scenarios, we designed the Guided Click Refiner(GCR), which accepts point and optional textual information as inputs, transforming the point into the bounding box expected by the operator. The bounding box will be used as input of single object trackers. Experiments on LaSOT and GOT-10k benchmarks show that tracker combined with GCR achieves stable performance in real-time interactive scenarios. Furthermore, we explored the integration of GCR into the Segment Anything model(SAM), significantly reducing ambiguity issues when SAM receives point inputs.",
      "published": "2024-11-20T10:30:33Z"
    },
    "metadata": {
      "arxiv_id": "2411.13183",
      "title": "ClickTrack: Towards Real-time Interactive Single Object Tracking",
      "summary": "Single object tracking(SOT) relies on precise object bounding box initialization. In this paper, we reconsidered the deficiencies in the current approaches to initializing single object trackers and propose a new paradigm for single object tracking algorithms, ClickTrack, a new paradigm using clicking interaction for real-time scenarios. Moreover, click as an input type inherently lack hierarchical information. To address ambiguity in certain special scenarios, we designed the Guided Click Refiner(GCR), which accepts point and optional textual information as inputs, transforming the point into the bounding box expected by the operator. The bounding box will be used as input of single object trackers. Experiments on LaSOT and GOT-10k benchmarks show that tracker combined with GCR achieves stable performance in real-time interactive scenarios. Furthermore, we explored the integration of GCR into the Segment Anything model(SAM), significantly reducing ambiguity issues when SAM receives point inputs.",
      "authors": [
        "Kuiran Wang",
        "Xuehui Yu",
        "Wenwen Yu",
        "Guorong Li",
        "Xiangyuan Lan",
        "Qixiang Ye",
        "Jianbin Jiao",
        "Zhenjun Han"
      ],
      "published": "2024-11-20T10:30:33Z",
      "updated": "2024-11-24T14:35:40Z",
      "categories": [
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2411.13183v2",
      "landing_url": "https://arxiv.org/abs/2411.13183v2",
      "doi": "https://doi.org/10.48550/arXiv.2411.13183"
    },
    "queries": [
      {
        "anchor": "spoken language models",
        "search_term": "real-time interaction"
      },
      {
        "anchor": "synchronous dialogue",
        "search_term": "real-time interaction"
      },
      {
        "anchor": "turn-taking",
        "search_term": "real-time interaction"
      }
    ]
  },
  {
    "arxiv_id": "2411.13917",
    "anchor": "spoken language models",
    "search_term": "temporal dynamics",
    "search_record": {
      "id": "http://arxiv.org/abs/2411.13917v1",
      "title": "SpikEmo: Enhancing Emotion Recognition With Spiking Temporal Dynamics in Conversations",
      "summary": "In affective computing, the task of Emotion Recognition in Conversations (ERC) has emerged as a focal area of research. The primary objective of this task is to predict emotional states within conversations by analyzing multimodal data including text, audio, and video. While existing studies have progressed in extracting and fusing representations from multimodal data, they often overlook the temporal dynamics in the data during conversations. To address this challenge, we have developed the SpikEmo framework, which is based on spiking neurons and employs a Semantic & Dynamic Two-stage Modeling approach to more precisely capture the complex temporal features of multimodal emotional data. Additionally, to tackle the class imbalance and emotional semantic similarity problems in the ERC tasks, we have devised an innovative combination of loss functions that significantly enhances the model's performance when dealing with ERC data characterized by long-tail distributions. Extensive experiments conducted on multiple ERC benchmark datasets demonstrate that SpikEmo significantly outperforms existing state-of-the-art methods in ERC tasks. Our code is available at https://github.com/Yu-xm/SpikEmo.git.",
      "published": "2024-11-21T08:07:26Z"
    },
    "metadata": {
      "arxiv_id": "2411.13917",
      "title": "SpikEmo: Enhancing Emotion Recognition With Spiking Temporal Dynamics in Conversations",
      "summary": "In affective computing, the task of Emotion Recognition in Conversations (ERC) has emerged as a focal area of research. The primary objective of this task is to predict emotional states within conversations by analyzing multimodal data including text, audio, and video. While existing studies have progressed in extracting and fusing representations from multimodal data, they often overlook the temporal dynamics in the data during conversations. To address this challenge, we have developed the SpikEmo framework, which is based on spiking neurons and employs a Semantic & Dynamic Two-stage Modeling approach to more precisely capture the complex temporal features of multimodal emotional data. Additionally, to tackle the class imbalance and emotional semantic similarity problems in the ERC tasks, we have devised an innovative combination of loss functions that significantly enhances the model's performance when dealing with ERC data characterized by long-tail distributions. Extensive experiments conducted on multiple ERC benchmark datasets demonstrate that SpikEmo significantly outperforms existing state-of-the-art methods in ERC tasks. Our code is available at https://github.com/Yu-xm/SpikEmo.git.",
      "authors": [
        "Xiaomin Yu",
        "Feiyang Wang",
        "Ziyue Qiao"
      ],
      "published": "2024-11-21T08:07:26Z",
      "updated": "2024-11-21T08:07:26Z",
      "categories": [
        "cs.MM"
      ],
      "pdf_url": "https://arxiv.org/pdf/2411.13917v1",
      "landing_url": "https://arxiv.org/abs/2411.13917v1",
      "doi": "https://doi.org/10.48550/arXiv.2411.13917"
    },
    "queries": [
      {
        "anchor": "spoken language models",
        "search_term": "temporal dynamics"
      },
      {
        "anchor": "synchronous dialogue",
        "search_term": "temporal dynamics"
      },
      {
        "anchor": "turn-taking",
        "search_term": "temporal dynamics"
      }
    ]
  },
  {
    "arxiv_id": "2411.15405",
    "anchor": "turn-taking",
    "search_term": "true full-duplex",
    "search_record": {
      "id": "http://arxiv.org/abs/2411.15405v1",
      "title": "ML-SPEAK: A Theory-Guided Machine Learning Method for Studying and Predicting Conversational Turn-taking Patterns",
      "summary": "Predicting team dynamics from personality traits remains a fundamental challenge for the psychological sciences and team-based organizations. Understanding how team composition generates team processes can significantly advance team-based research along with providing practical guidelines for team staffing and training. Although the Input-Process-Output (IPO) model has been useful for studying these connections, the complex nature of team member interactions demands a more dynamic approach. We develop a computational model of conversational turn-taking within self-organized teams that can provide insight into the relationships between team member personality traits and team communication dynamics. We focus on turn-taking patterns between team members, independent of content, which can significantly influence team emergent states and outcomes while being objectively measurable and quantifiable. As our model is trained on conversational data from teams of given trait compositions, it can learn the relationships between individual traits and speaking behaviors and predict group-wide patterns of communication based on team trait composition alone. We first evaluate the performance of our model using simulated data and then apply it to real-world data collected from self-organized student teams. In comparison to baselines, our model is more accurate at predicting speaking turn sequences and can reveal new relationships between team member traits and their communication patterns. Our approach offers a more data-driven and dynamic understanding of team processes. By bridging the gap between individual personality traits and team communication patterns, our model has the potential to inform theories of team processes and provide powerful insights into optimizing team staffing and training.",
      "published": "2024-11-23T01:27:01Z"
    },
    "metadata": {
      "arxiv_id": "2411.15405",
      "title": "ML-SPEAK: A Theory-Guided Machine Learning Method for Studying and Predicting Conversational Turn-taking Patterns",
      "summary": "Predicting team dynamics from personality traits remains a fundamental challenge for the psychological sciences and team-based organizations. Understanding how team composition generates team processes can significantly advance team-based research along with providing practical guidelines for team staffing and training. Although the Input-Process-Output (IPO) model has been useful for studying these connections, the complex nature of team member interactions demands a more dynamic approach. We develop a computational model of conversational turn-taking within self-organized teams that can provide insight into the relationships between team member personality traits and team communication dynamics. We focus on turn-taking patterns between team members, independent of content, which can significantly influence team emergent states and outcomes while being objectively measurable and quantifiable. As our model is trained on conversational data from teams of given trait compositions, it can learn the relationships between individual traits and speaking behaviors and predict group-wide patterns of communication based on team trait composition alone. We first evaluate the performance of our model using simulated data and then apply it to real-world data collected from self-organized student teams. In comparison to baselines, our model is more accurate at predicting speaking turn sequences and can reveal new relationships between team member traits and their communication patterns. Our approach offers a more data-driven and dynamic understanding of team processes. By bridging the gap between individual personality traits and team communication patterns, our model has the potential to inform theories of team processes and provide powerful insights into optimizing team staffing and training.",
      "authors": [
        "Lisa R. O'Bryan",
        "Madeline Navarro",
        "Juan Segundo Hevia",
        "Santiago Segarra"
      ],
      "published": "2024-11-23T01:27:01Z",
      "updated": "2024-11-23T01:27:01Z",
      "categories": [
        "cs.CL",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2411.15405v1",
      "landing_url": "https://arxiv.org/abs/2411.15405v1",
      "doi": "https://doi.org/10.48550/arXiv.2411.15405"
    },
    "queries": [
      {
        "anchor": "turn-taking",
        "search_term": "true full-duplex"
      },
      {
        "anchor": "turn-taking",
        "search_term": "pseudo full-duplex"
      },
      {
        "anchor": "turn-taking",
        "search_term": "half duplex"
      },
      {
        "anchor": "turn-taking",
        "search_term": "engineered synchronization"
      },
      {
        "anchor": "turn-taking",
        "search_term": "learned synchronization"
      },
      {
        "anchor": "turn-taking",
        "search_term": "modular architecture"
      },
      {
        "anchor": "turn-taking",
        "search_term": "streaming architecture"
      },
      {
        "anchor": "turn-taking",
        "search_term": "real-time interaction"
      },
      {
        "anchor": "turn-taking",
        "search_term": "low latency"
      },
      {
        "anchor": "turn-taking",
        "search_term": "synchronous data"
      },
      {
        "anchor": "turn-taking",
        "search_term": "data scarcity"
      },
      {
        "anchor": "turn-taking",
        "search_term": "multichannel dialogue"
      },
      {
        "anchor": "turn-taking",
        "search_term": "temporal dynamics"
      },
      {
        "anchor": "turn-taking",
        "search_term": "behavioral arbitration"
      },
      {
        "anchor": "turn-taking",
        "search_term": "semantic coherence"
      },
      {
        "anchor": "turn-taking",
        "search_term": "neural codec"
      },
      {
        "anchor": "turn-taking",
        "search_term": "discrete tokens"
      },
      {
        "anchor": "turn-taking",
        "search_term": "audio tokens"
      },
      {
        "anchor": "turn-taking",
        "search_term": "floor transfer offset"
      },
      {
        "anchor": "turn-taking",
        "search_term": "stop latency"
      },
      {
        "anchor": "turn-taking",
        "search_term": "interruption response delay"
      }
    ]
  },
  {
    "arxiv_id": "2411.18138",
    "anchor": "full-duplex",
    "search_term": "audio tokens",
    "search_record": {
      "id": "http://arxiv.org/abs/2411.18138v1",
      "title": "SALMONN-omni: A Codec-free LLM for Full-duplex Speech Understanding and Generation",
      "summary": "Full-duplex multimodal large language models (LLMs) provide a unified framework for addressing diverse speech understanding and generation tasks, enabling more natural and seamless human-machine conversations. Unlike traditional modularised conversational AI systems, which separate speech recognition, understanding, and text-to-speech generation into distinct components, multimodal LLMs operate as single end-to-end models. This streamlined design eliminates error propagation across components and fully leverages the rich non-verbal information embedded in input speech signals. We introduce SALMONN-omni, a codec-free, full-duplex speech understanding and generation model capable of simultaneously listening to its own generated speech and background sounds while speaking. To support this capability, we propose a novel duplex spoken dialogue framework incorporating a ``thinking'' mechanism that facilitates asynchronous text and speech generation relying on embeddings instead of codecs (quantized speech and audio tokens). Experimental results demonstrate SALMONN-omni's versatility across a broad range of streaming speech tasks, including speech recognition, speech enhancement, and spoken question answering. Additionally, SALMONN-omni excels at managing turn-taking, barge-in, and echo cancellation scenarios, establishing its potential as a robust prototype for full-duplex conversational AI systems. To the best of our knowledge, SALMONN-omni is the first codec-free model of its kind. A full technical report along with model checkpoints will be released soon.",
      "published": "2024-11-27T08:38:57Z"
    },
    "metadata": {
      "arxiv_id": "2411.18138",
      "title": "SALMONN-omni: A Codec-free LLM for Full-duplex Speech Understanding and Generation",
      "summary": "Full-duplex multimodal large language models (LLMs) provide a unified framework for addressing diverse speech understanding and generation tasks, enabling more natural and seamless human-machine conversations. Unlike traditional modularised conversational AI systems, which separate speech recognition, understanding, and text-to-speech generation into distinct components, multimodal LLMs operate as single end-to-end models. This streamlined design eliminates error propagation across components and fully leverages the rich non-verbal information embedded in input speech signals. We introduce SALMONN-omni, a codec-free, full-duplex speech understanding and generation model capable of simultaneously listening to its own generated speech and background sounds while speaking. To support this capability, we propose a novel duplex spoken dialogue framework incorporating a ``thinking'' mechanism that facilitates asynchronous text and speech generation relying on embeddings instead of codecs (quantized speech and audio tokens). Experimental results demonstrate SALMONN-omni's versatility across a broad range of streaming speech tasks, including speech recognition, speech enhancement, and spoken question answering. Additionally, SALMONN-omni excels at managing turn-taking, barge-in, and echo cancellation scenarios, establishing its potential as a robust prototype for full-duplex conversational AI systems. To the best of our knowledge, SALMONN-omni is the first codec-free model of its kind. A full technical report along with model checkpoints will be released soon.",
      "authors": [
        "Wenyi Yu",
        "Siyin Wang",
        "Xiaoyu Yang",
        "Xianzhao Chen",
        "Xiaohai Tian",
        "Jun Zhang",
        "Guangzhi Sun",
        "Lu Lu",
        "Yuxuan Wang",
        "Chao Zhang"
      ],
      "published": "2024-11-27T08:38:57Z",
      "updated": "2024-11-27T08:38:57Z",
      "categories": [
        "eess.AS",
        "cs.AI",
        "cs.CL",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2411.18138v1",
      "landing_url": "https://arxiv.org/abs/2411.18138v1",
      "doi": "https://doi.org/10.48550/arXiv.2411.18138"
    },
    "queries": [
      {
        "anchor": "full-duplex",
        "search_term": "audio tokens"
      },
      {
        "anchor": "synchronous dialogue",
        "search_term": "audio tokens"
      },
      {
        "anchor": "turn-taking",
        "search_term": "audio tokens"
      }
    ]
  },
  {
    "arxiv_id": "2411.18447",
    "anchor": "synchronous dialogue",
    "search_term": "discrete tokens",
    "search_record": {
      "id": "http://arxiv.org/abs/2411.18447v1",
      "title": "Continuous Autoregressive Models with Noise Augmentation Avoid Error Accumulation",
      "summary": "Autoregressive models are typically applied to sequences of discrete tokens, but recent research indicates that generating sequences of continuous embeddings in an autoregressive manner is also feasible. However, such Continuous Autoregressive Models (CAMs) can suffer from a decline in generation quality over extended sequences due to error accumulation during inference. We introduce a novel method to address this issue by injecting random noise into the input embeddings during training. This procedure makes the model robust against varying error levels at inference. We further reduce error accumulation through an inference procedure that introduces low-level noise. Experiments on musical audio generation show that CAM substantially outperforms existing autoregressive and non-autoregressive approaches while preserving audio quality over extended sequences. This work paves the way for generating continuous embeddings in a purely autoregressive setting, opening new possibilities for real-time and interactive generative applications.",
      "published": "2024-11-27T15:38:20Z"
    },
    "metadata": {
      "arxiv_id": "2411.18447",
      "title": "Continuous Autoregressive Models with Noise Augmentation Avoid Error Accumulation",
      "summary": "Autoregressive models are typically applied to sequences of discrete tokens, but recent research indicates that generating sequences of continuous embeddings in an autoregressive manner is also feasible. However, such Continuous Autoregressive Models (CAMs) can suffer from a decline in generation quality over extended sequences due to error accumulation during inference. We introduce a novel method to address this issue by injecting random noise into the input embeddings during training. This procedure makes the model robust against varying error levels at inference. We further reduce error accumulation through an inference procedure that introduces low-level noise. Experiments on musical audio generation show that CAM substantially outperforms existing autoregressive and non-autoregressive approaches while preserving audio quality over extended sequences. This work paves the way for generating continuous embeddings in a purely autoregressive setting, opening new possibilities for real-time and interactive generative applications.",
      "authors": [
        "Marco Pasini",
        "Javier Nistal",
        "Stefan Lattner",
        "George Fazekas"
      ],
      "published": "2024-11-27T15:38:20Z",
      "updated": "2024-11-27T15:38:20Z",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2411.18447v1",
      "landing_url": "https://arxiv.org/abs/2411.18447v1",
      "doi": "https://doi.org/10.48550/arXiv.2411.18447"
    },
    "queries": [
      {
        "anchor": "synchronous dialogue",
        "search_term": "discrete tokens"
      }
    ]
  },
  {
    "arxiv_id": "2411.19167",
    "anchor": "synchronous dialogue",
    "search_term": "synchronous data",
    "search_record": {
      "id": "http://arxiv.org/abs/2411.19167v2",
      "title": "HOT3D: Hand and Object Tracking in 3D from Egocentric Multi-View Videos",
      "summary": "We introduce HOT3D, a publicly available dataset for egocentric hand and object tracking in 3D. The dataset offers over 833 minutes (3.7M+ images) of recordings that feature 19 subjects interacting with 33 diverse rigid objects. In addition to simple pick-up, observe, and put-down actions, the subjects perform actions typical for a kitchen, office, and living room environment. The recordings include multiple synchronized data streams containing egocentric multi-view RGB/monochrome images, eye gaze signal, scene point clouds, and 3D poses of cameras, hands, and objects. The dataset is recorded with two headsets from Meta: Project Aria, which is a research prototype of AI glasses, and Quest 3, a virtual-reality headset that has shipped millions of units. Ground-truth poses were obtained by a motion-capture system using small optical markers attached to hands and objects. Hand annotations are provided in the UmeTrack and MANO formats, and objects are represented by 3D meshes with PBR materials obtained by an in-house scanner. In our experiments, we demonstrate the effectiveness of multi-view egocentric data for three popular tasks: 3D hand tracking, model-based 6DoF object pose estimation, and 3D lifting of unknown in-hand objects. The evaluated multi-view methods, whose benchmarking is uniquely enabled by HOT3D, significantly outperform their single-view counterparts.",
      "published": "2024-11-28T14:09:42Z"
    },
    "metadata": {
      "arxiv_id": "2411.19167",
      "title": "HOT3D: Hand and Object Tracking in 3D from Egocentric Multi-View Videos",
      "summary": "We introduce HOT3D, a publicly available dataset for egocentric hand and object tracking in 3D. The dataset offers over 833 minutes (3.7M+ images) of recordings that feature 19 subjects interacting with 33 diverse rigid objects. In addition to simple pick-up, observe, and put-down actions, the subjects perform actions typical for a kitchen, office, and living room environment. The recordings include multiple synchronized data streams containing egocentric multi-view RGB/monochrome images, eye gaze signal, scene point clouds, and 3D poses of cameras, hands, and objects. The dataset is recorded with two headsets from Meta: Project Aria, which is a research prototype of AI glasses, and Quest 3, a virtual-reality headset that has shipped millions of units. Ground-truth poses were obtained by a motion-capture system using small optical markers attached to hands and objects. Hand annotations are provided in the UmeTrack and MANO formats, and objects are represented by 3D meshes with PBR materials obtained by an in-house scanner. In our experiments, we demonstrate the effectiveness of multi-view egocentric data for three popular tasks: 3D hand tracking, model-based 6DoF object pose estimation, and 3D lifting of unknown in-hand objects. The evaluated multi-view methods, whose benchmarking is uniquely enabled by HOT3D, significantly outperform their single-view counterparts.",
      "authors": [
        "Prithviraj Banerjee",
        "Sindi Shkodrani",
        "Pierre Moulon",
        "Shreyas Hampali",
        "Shangchen Han",
        "Fan Zhang",
        "Linguang Zhang",
        "Jade Fountain",
        "Edward Miller",
        "Selen Basol",
        "Richard Newcombe",
        "Robert Wang",
        "Jakob Julian Engel",
        "Tomas Hodan"
      ],
      "published": "2024-11-28T14:09:42Z",
      "updated": "2025-04-30T13:32:06Z",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.RO"
      ],
      "pdf_url": "https://arxiv.org/pdf/2411.19167v2",
      "landing_url": "https://arxiv.org/abs/2411.19167v2",
      "doi": "https://doi.org/10.48550/arXiv.2411.19167"
    },
    "queries": [
      {
        "anchor": "synchronous dialogue",
        "search_term": "synchronous data"
      }
    ]
  },
  {
    "arxiv_id": "2411.19527",
    "anchor": "spoken language models",
    "search_term": "discrete tokens",
    "search_record": {
      "id": "http://arxiv.org/abs/2411.19527v4",
      "title": "DisCoRD: Discrete Tokens to Continuous Motion via Rectified Flow Decoding",
      "summary": "Human motion is inherently continuous and dynamic, posing significant challenges for generative models. While discrete generation methods are widely used, they suffer from limited expressiveness and frame-wise noise artifacts. In contrast, continuous approaches produce smoother, more natural motion but often struggle to adhere to conditioning signals due to high-dimensional complexity and limited training data. To resolve this 'discord' between discrete and continuous representations we introduce DisCoRD: Discrete Tokens to Continuous Motion via Rectified Flow Decoding, a novel method that leverages rectified flow to decode discrete motion tokens in the continuous, raw motion space. Our core idea is to frame token decoding as a conditional generation task, ensuring that DisCoRD captures fine-grained dynamics and achieves smoother, more natural motions. Compatible with any discrete-based framework, our method enhances naturalness without compromising faithfulness to the conditioning signals on diverse settings. Extensive evaluations demonstrate that DisCoRD achieves state-of-the-art performance, with FID of 0.032 on HumanML3D and 0.169 on KIT-ML. These results establish DisCoRD as a robust solution for bridging the divide between discrete efficiency and continuous realism. Project website: https://whwjdqls.github.io/discord-motion/",
      "published": "2024-11-29T07:54:56Z"
    },
    "metadata": {
      "arxiv_id": "2411.19527",
      "title": "DisCoRD: Discrete Tokens to Continuous Motion via Rectified Flow Decoding",
      "summary": "Human motion is inherently continuous and dynamic, posing significant challenges for generative models. While discrete generation methods are widely used, they suffer from limited expressiveness and frame-wise noise artifacts. In contrast, continuous approaches produce smoother, more natural motion but often struggle to adhere to conditioning signals due to high-dimensional complexity and limited training data. To resolve this 'discord' between discrete and continuous representations we introduce DisCoRD: Discrete Tokens to Continuous Motion via Rectified Flow Decoding, a novel method that leverages rectified flow to decode discrete motion tokens in the continuous, raw motion space. Our core idea is to frame token decoding as a conditional generation task, ensuring that DisCoRD captures fine-grained dynamics and achieves smoother, more natural motions. Compatible with any discrete-based framework, our method enhances naturalness without compromising faithfulness to the conditioning signals on diverse settings. Extensive evaluations demonstrate that DisCoRD achieves state-of-the-art performance, with FID of 0.032 on HumanML3D and 0.169 on KIT-ML. These results establish DisCoRD as a robust solution for bridging the divide between discrete efficiency and continuous realism. Project website: https://whwjdqls.github.io/discord-motion/",
      "authors": [
        "Jungbin Cho",
        "Junwan Kim",
        "Jisoo Kim",
        "Minseo Kim",
        "Mingu Kang",
        "Sungeun Hong",
        "Tae-Hyun Oh",
        "Youngjae Yu"
      ],
      "published": "2024-11-29T07:54:56Z",
      "updated": "2025-08-07T08:03:29Z",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2411.19527v4",
      "landing_url": "https://arxiv.org/abs/2411.19527v4",
      "doi": "https://doi.org/10.48550/arXiv.2411.19527"
    },
    "queries": [
      {
        "anchor": "spoken language models",
        "search_term": "discrete tokens"
      },
      {
        "anchor": "synchronous dialogue",
        "search_term": "discrete tokens"
      },
      {
        "anchor": "turn-taking",
        "search_term": "discrete tokens"
      }
    ]
  },
  {
    "arxiv_id": "2412.00887",
    "anchor": "synchronous dialogue",
    "search_term": "real-time interaction",
    "search_record": {
      "id": "http://arxiv.org/abs/2412.00887v1",
      "title": "Playable Game Generation",
      "summary": "In recent years, Artificial Intelligence Generated Content (AIGC) has advanced from text-to-image generation to text-to-video and multimodal video synthesis. However, generating playable games presents significant challenges due to the stringent requirements for real-time interaction, high visual quality, and accurate simulation of game mechanics. Existing approaches often fall short, either lacking real-time capabilities or failing to accurately simulate interactive mechanics. To tackle the playability issue, we propose a novel method called \\emph{PlayGen}, which encompasses game data generation, an autoregressive DiT-based diffusion model, and a comprehensive playability-based evaluation framework. Validated on well-known 2D and 3D games, PlayGen achieves real-time interaction, ensures sufficient visual quality, and provides accurate interactive mechanics simulation. Notably, these results are sustained even after over 1000 frames of gameplay on an NVIDIA RTX 2060 GPU. Our code is publicly available: https://github.com/GreatX3/Playable-Game-Generation. Our playable demo generated by AI is: http://124.156.151.207.",
      "published": "2024-12-01T16:53:02Z"
    },
    "metadata": {
      "arxiv_id": "2412.00887",
      "title": "Playable Game Generation",
      "summary": "In recent years, Artificial Intelligence Generated Content (AIGC) has advanced from text-to-image generation to text-to-video and multimodal video synthesis. However, generating playable games presents significant challenges due to the stringent requirements for real-time interaction, high visual quality, and accurate simulation of game mechanics. Existing approaches often fall short, either lacking real-time capabilities or failing to accurately simulate interactive mechanics. To tackle the playability issue, we propose a novel method called \\emph{PlayGen}, which encompasses game data generation, an autoregressive DiT-based diffusion model, and a comprehensive playability-based evaluation framework. Validated on well-known 2D and 3D games, PlayGen achieves real-time interaction, ensures sufficient visual quality, and provides accurate interactive mechanics simulation. Notably, these results are sustained even after over 1000 frames of gameplay on an NVIDIA RTX 2060 GPU. Our code is publicly available: https://github.com/GreatX3/Playable-Game-Generation. Our playable demo generated by AI is: http://124.156.151.207.",
      "authors": [
        "Mingyu Yang",
        "Junyou Li",
        "Zhongbin Fang",
        "Sheng Chen",
        "Yangbin Yu",
        "Qiang Fu",
        "Wei Yang",
        "Deheng Ye"
      ],
      "published": "2024-12-01T16:53:02Z",
      "updated": "2024-12-01T16:53:02Z",
      "categories": [
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2412.00887v1",
      "landing_url": "https://arxiv.org/abs/2412.00887v1",
      "doi": "https://doi.org/10.48550/arXiv.2412.00887"
    },
    "queries": [
      {
        "anchor": "synchronous dialogue",
        "search_term": "real-time interaction"
      }
    ]
  },
  {
    "arxiv_id": "2412.01078",
    "anchor": "synchronous dialogue",
    "search_term": "real-time interaction",
    "search_record": {
      "id": "http://arxiv.org/abs/2412.01078v2",
      "title": "Advancing Speech Language Models by Scaling Supervised Fine-Tuning with Over 60,000 Hours of Synthetic Speech Dialogue Data",
      "summary": "The GPT-4o represents a significant milestone in enabling real-time interaction with large language models (LLMs) through speech, its remarkable low latency and high fluency not only capture attention but also stimulate research interest in the field. This real-time speech interaction is particularly valuable in scenarios requiring rapid feedback and immediate responses, dramatically enhancing user experience. However, there is a notable lack of research focused on real-time large speech language models, particularly for Chinese. In this work, we present KE-Omni, a seamless large speech language model built upon Ke-SpeechChat, a large-scale high-quality synthetic speech interaction dataset consisting of 7 million Chinese and English conversations, featuring 42,002 speakers, and totaling over 60,000 hours, This contributes significantly to the advancement of research and development in this field. The demos can be accessed at \\url{https://huggingface.co/spaces/KE-Team/KE-Omni}.",
      "published": "2024-12-02T03:31:46Z"
    },
    "metadata": {
      "arxiv_id": "2412.01078",
      "title": "Advancing Speech Language Models by Scaling Supervised Fine-Tuning with Over 60,000 Hours of Synthetic Speech Dialogue Data",
      "summary": "The GPT-4o represents a significant milestone in enabling real-time interaction with large language models (LLMs) through speech, its remarkable low latency and high fluency not only capture attention but also stimulate research interest in the field. This real-time speech interaction is particularly valuable in scenarios requiring rapid feedback and immediate responses, dramatically enhancing user experience. However, there is a notable lack of research focused on real-time large speech language models, particularly for Chinese. In this work, we present KE-Omni, a seamless large speech language model built upon Ke-SpeechChat, a large-scale high-quality synthetic speech interaction dataset consisting of 7 million Chinese and English conversations, featuring 42,002 speakers, and totaling over 60,000 hours, This contributes significantly to the advancement of research and development in this field. The demos can be accessed at \\url{https://huggingface.co/spaces/KE-Team/KE-Omni}.",
      "authors": [
        "Shuaijiang Zhao",
        "Tingwei Guo",
        "Bajian Xiang",
        "Tongtang Wan",
        "Qiang Niu",
        "Wei Zou",
        "Xiangang Li"
      ],
      "published": "2024-12-02T03:31:46Z",
      "updated": "2024-12-03T02:59:43Z",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.HC"
      ],
      "pdf_url": "https://arxiv.org/pdf/2412.01078v2",
      "landing_url": "https://arxiv.org/abs/2412.01078v2",
      "doi": "https://doi.org/10.48550/arXiv.2412.01078"
    },
    "queries": [
      {
        "anchor": "synchronous dialogue",
        "search_term": "real-time interaction"
      }
    ]
  },
  {
    "arxiv_id": "2412.04462",
    "anchor": "synchronous dialogue",
    "search_term": "streaming architecture",
    "search_record": {
      "id": "http://arxiv.org/abs/2412.04462v1",
      "title": "4Real-Video: Learning Generalizable Photo-Realistic 4D Video Diffusion",
      "summary": "We propose 4Real-Video, a novel framework for generating 4D videos, organized as a grid of video frames with both time and viewpoint axes. In this grid, each row contains frames sharing the same timestep, while each column contains frames from the same viewpoint. We propose a novel two-stream architecture. One stream performs viewpoint updates on columns, and the other stream performs temporal updates on rows. After each diffusion transformer layer, a synchronization layer exchanges information between the two token streams. We propose two implementations of the synchronization layer, using either hard or soft synchronization. This feedforward architecture improves upon previous work in three ways: higher inference speed, enhanced visual quality (measured by FVD, CLIP, and VideoScore), and improved temporal and viewpoint consistency (measured by VideoScore and Dust3R-Confidence).",
      "published": "2024-12-05T18:59:41Z"
    },
    "metadata": {
      "arxiv_id": "2412.04462",
      "title": "4Real-Video: Learning Generalizable Photo-Realistic 4D Video Diffusion",
      "summary": "We propose 4Real-Video, a novel framework for generating 4D videos, organized as a grid of video frames with both time and viewpoint axes. In this grid, each row contains frames sharing the same timestep, while each column contains frames from the same viewpoint. We propose a novel two-stream architecture. One stream performs viewpoint updates on columns, and the other stream performs temporal updates on rows. After each diffusion transformer layer, a synchronization layer exchanges information between the two token streams. We propose two implementations of the synchronization layer, using either hard or soft synchronization. This feedforward architecture improves upon previous work in three ways: higher inference speed, enhanced visual quality (measured by FVD, CLIP, and VideoScore), and improved temporal and viewpoint consistency (measured by VideoScore and Dust3R-Confidence).",
      "authors": [
        "Chaoyang Wang",
        "Peiye Zhuang",
        "Tuan Duc Ngo",
        "Willi Menapace",
        "Aliaksandr Siarohin",
        "Michael Vasilkovsky",
        "Ivan Skorokhodov",
        "Sergey Tulyakov",
        "Peter Wonka",
        "Hsin-Ying Lee"
      ],
      "published": "2024-12-05T18:59:41Z",
      "updated": "2024-12-05T18:59:41Z",
      "categories": [
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2412.04462v1",
      "landing_url": "https://arxiv.org/abs/2412.04462v1",
      "doi": "https://doi.org/10.48550/arXiv.2412.04462"
    },
    "queries": [
      {
        "anchor": "synchronous dialogue",
        "search_term": "streaming architecture"
      }
    ]
  },
  {
    "arxiv_id": "2412.04937",
    "anchor": "turn-taking",
    "search_term": "true full-duplex",
    "search_record": {
      "id": "http://arxiv.org/abs/2412.04937v2",
      "title": "Who Speaks Next? Multi-party AI Discussion Leveraging the Systematics of Turn-taking in Murder Mystery Games",
      "summary": "Multi-agent systems utilizing large language models (LLMs) have shown great promise in achieving natural dialogue. However, smooth dialogue control and autonomous decision making among agents still remain challenges. In this study, we focus on conversational norms such as adjacency pairs and turn-taking found in conversation analysis and propose a new framework called \"Murder Mystery Agents\" that applies these norms to AI agents' dialogue control. As an evaluation target, we employed the \"Murder Mystery\" game, a reasoning-type table-top role-playing game that requires complex social reasoning and information manipulation. In this game, players need to unravel the truth of the case based on fragmentary information through cooperation and bargaining. The proposed framework integrates next speaker selection based on adjacency pairs and a self-selection mechanism that takes agents' internal states into account to achieve more natural and strategic dialogue. To verify the effectiveness of this new approach, we analyzed utterances that led to dialogue breakdowns and conducted automatic evaluation using LLMs, as well as human evaluation using evaluation criteria developed for the Murder Mystery game. Experimental results showed that the implementation of the next speaker selection mechanism significantly reduced dialogue breakdowns and improved the ability of agents to share information and perform logical reasoning. The results of this study demonstrate that the systematics of turn-taking in human conversation are also effective in controlling dialogue among AI agents, and provide design guidelines for more advanced multi-agent dialogue systems.",
      "published": "2024-12-06T10:45:54Z"
    },
    "metadata": {
      "arxiv_id": "2412.04937",
      "title": "Who Speaks Next? Multi-party AI Discussion Leveraging the Systematics of Turn-taking in Murder Mystery Games",
      "summary": "Multi-agent systems utilizing large language models (LLMs) have shown great promise in achieving natural dialogue. However, smooth dialogue control and autonomous decision making among agents still remain challenges. In this study, we focus on conversational norms such as adjacency pairs and turn-taking found in conversation analysis and propose a new framework called \"Murder Mystery Agents\" that applies these norms to AI agents' dialogue control. As an evaluation target, we employed the \"Murder Mystery\" game, a reasoning-type table-top role-playing game that requires complex social reasoning and information manipulation. In this game, players need to unravel the truth of the case based on fragmentary information through cooperation and bargaining. The proposed framework integrates next speaker selection based on adjacency pairs and a self-selection mechanism that takes agents' internal states into account to achieve more natural and strategic dialogue. To verify the effectiveness of this new approach, we analyzed utterances that led to dialogue breakdowns and conducted automatic evaluation using LLMs, as well as human evaluation using evaluation criteria developed for the Murder Mystery game. Experimental results showed that the implementation of the next speaker selection mechanism significantly reduced dialogue breakdowns and improved the ability of agents to share information and perform logical reasoning. The results of this study demonstrate that the systematics of turn-taking in human conversation are also effective in controlling dialogue among AI agents, and provide design guidelines for more advanced multi-agent dialogue systems.",
      "authors": [
        "Ryota Nonomura",
        "Hiroki Mori"
      ],
      "published": "2024-12-06T10:45:54Z",
      "updated": "2025-02-21T04:08:06Z",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2412.04937v2",
      "landing_url": "https://arxiv.org/abs/2412.04937v2",
      "doi": "https://doi.org/10.48550/arXiv.2412.04937"
    },
    "queries": [
      {
        "anchor": "turn-taking",
        "search_term": "true full-duplex"
      },
      {
        "anchor": "turn-taking",
        "search_term": "pseudo full-duplex"
      },
      {
        "anchor": "turn-taking",
        "search_term": "half duplex"
      },
      {
        "anchor": "turn-taking",
        "search_term": "engineered synchronization"
      },
      {
        "anchor": "turn-taking",
        "search_term": "learned synchronization"
      },
      {
        "anchor": "turn-taking",
        "search_term": "modular architecture"
      },
      {
        "anchor": "turn-taking",
        "search_term": "streaming architecture"
      },
      {
        "anchor": "turn-taking",
        "search_term": "real-time interaction"
      },
      {
        "anchor": "turn-taking",
        "search_term": "low latency"
      },
      {
        "anchor": "turn-taking",
        "search_term": "synchronous data"
      },
      {
        "anchor": "turn-taking",
        "search_term": "data scarcity"
      },
      {
        "anchor": "turn-taking",
        "search_term": "multichannel dialogue"
      },
      {
        "anchor": "turn-taking",
        "search_term": "temporal dynamics"
      },
      {
        "anchor": "turn-taking",
        "search_term": "behavioral arbitration"
      },
      {
        "anchor": "turn-taking",
        "search_term": "semantic coherence"
      },
      {
        "anchor": "turn-taking",
        "search_term": "neural codec"
      },
      {
        "anchor": "turn-taking",
        "search_term": "discrete tokens"
      },
      {
        "anchor": "turn-taking",
        "search_term": "audio tokens"
      },
      {
        "anchor": "turn-taking",
        "search_term": "floor transfer offset"
      },
      {
        "anchor": "turn-taking",
        "search_term": "stop latency"
      },
      {
        "anchor": "turn-taking",
        "search_term": "interruption response delay"
      }
    ]
  },
  {
    "arxiv_id": "2412.08237",
    "anchor": "synchronous dialogue",
    "search_term": "audio tokens",
    "search_record": {
      "id": "http://arxiv.org/abs/2412.08237v2",
      "title": "TouchTTS: An Embarrassingly Simple TTS Framework that Everyone Can Touch",
      "summary": "It is well known that LLM-based systems are data-hungry. Recent LLM-based TTS works typically employ complex data processing pipelines to obtain high-quality training data. These sophisticated pipelines require excellent models at each stage (e.g., speech denoising, speech enhancement, speaker diarization, and punctuation models), which themselves demand high-quality training data and are rarely open-sourced. Even with state-of-the-art models, issues persist, such as incomplete background noise removal and misalignment between punctuation and actual speech pauses. Moreover, the stringent filtering strategies often retain only 10-30\\% of the original data, significantly impeding data scaling efforts. In this work, we leverage a noise-robust audio tokenizer (S3Tokenizer) to design a simplified yet effective TTS data processing pipeline that maintains data quality while substantially reducing data acquisition costs, achieving a data retention rate of over 50\\%. Beyond data scaling challenges, LLM-based TTS systems also incur higher deployment costs compared to conventional approaches. Current systems typically use LLMs solely for text-to-token generation, while requiring separate models (e.g., flow matching models) for token-to-waveform generation, which cannot be directly executed by LLM inference engines, further complicating deployment. To address these challenges, we eliminate redundant modules in both LLM and flow components, replacing the flow model backbone with an LLM architecture. Building upon this simplified flow backbone, we propose a unified architecture for both streaming and non-streaming inference, significantly reducing deployment costs. Finally, we explore the feasibility of unifying TTS and ASR tasks using the same data for training, thanks to the simplified pipeline and the S3Tokenizer that reduces the quality requirements for TTS training data.",
      "published": "2024-12-11T09:38:50Z"
    },
    "metadata": {
      "arxiv_id": "2412.08237",
      "title": "TouchTTS: An Embarrassingly Simple TTS Framework that Everyone Can Touch",
      "summary": "It is well known that LLM-based systems are data-hungry. Recent LLM-based TTS works typically employ complex data processing pipelines to obtain high-quality training data. These sophisticated pipelines require excellent models at each stage (e.g., speech denoising, speech enhancement, speaker diarization, and punctuation models), which themselves demand high-quality training data and are rarely open-sourced. Even with state-of-the-art models, issues persist, such as incomplete background noise removal and misalignment between punctuation and actual speech pauses. Moreover, the stringent filtering strategies often retain only 10-30\\% of the original data, significantly impeding data scaling efforts. In this work, we leverage a noise-robust audio tokenizer (S3Tokenizer) to design a simplified yet effective TTS data processing pipeline that maintains data quality while substantially reducing data acquisition costs, achieving a data retention rate of over 50\\%. Beyond data scaling challenges, LLM-based TTS systems also incur higher deployment costs compared to conventional approaches. Current systems typically use LLMs solely for text-to-token generation, while requiring separate models (e.g., flow matching models) for token-to-waveform generation, which cannot be directly executed by LLM inference engines, further complicating deployment. To address these challenges, we eliminate redundant modules in both LLM and flow components, replacing the flow model backbone with an LLM architecture. Building upon this simplified flow backbone, we propose a unified architecture for both streaming and non-streaming inference, significantly reducing deployment costs. Finally, we explore the feasibility of unifying TTS and ASR tasks using the same data for training, thanks to the simplified pipeline and the S3Tokenizer that reduces the quality requirements for TTS training data.",
      "authors": [
        "Xingchen Song",
        "Mengtao Xing",
        "Changwei Ma",
        "Shengqiang Li",
        "Di Wu",
        "Binbin Zhang",
        "Fuping Pan",
        "Dinghao Zhou",
        "Yuekai Zhang",
        "Shun Lei",
        "Zhendong Peng",
        "Zhiyong Wu"
      ],
      "published": "2024-12-11T09:38:50Z",
      "updated": "2024-12-12T10:01:11Z",
      "categories": [
        "cs.SD",
        "cs.CL",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2412.08237v2",
      "landing_url": "https://arxiv.org/abs/2412.08237v2",
      "doi": "https://doi.org/10.48550/arXiv.2412.08237"
    },
    "queries": [
      {
        "anchor": "synchronous dialogue",
        "search_term": "audio tokens"
      }
    ]
  },
  {
    "arxiv_id": "2412.08955",
    "anchor": "synchronous dialogue",
    "search_term": "semantic coherence",
    "search_record": {
      "id": "http://arxiv.org/abs/2412.08955v1",
      "title": "Align, Generate, Learn: A Novel Closed-Loop Framework for Cross-Lingual In-Context Learning",
      "summary": "Cross-lingual in-context learning (XICL) has emerged as a transformative paradigm for leveraging large language models (LLMs) to tackle multilingual tasks, especially for low-resource languages. However, existing approaches often rely on external retrievers or task-specific fine-tuning, limiting their scalability and generalizability. In this paper, we propose a novel self-supervised framework that harnesses the generative capabilities of LLMs to internally select and utilize task-relevant examples. Our method introduces two key objectives: a retrieval-generation alignment loss to optimize the quality of selected examples and a semantic coherence loss to ensure cross-lingual consistency. Through extensive experiments on multilingual benchmarks, our approach achieves state-of-the-art performance, significantly outperforming existing baselines. Further analysis highlights its robustness across diverse language families and its ability to generalize to unseen tasks. Human evaluations confirm the superior fluency, relevance, and semantic correctness of outputs generated by our method. This work provides a scalable, effective, and generalizable solution for cross-lingual in-context learning.",
      "published": "2024-12-12T05:36:51Z"
    },
    "metadata": {
      "arxiv_id": "2412.08955",
      "title": "Align, Generate, Learn: A Novel Closed-Loop Framework for Cross-Lingual In-Context Learning",
      "summary": "Cross-lingual in-context learning (XICL) has emerged as a transformative paradigm for leveraging large language models (LLMs) to tackle multilingual tasks, especially for low-resource languages. However, existing approaches often rely on external retrievers or task-specific fine-tuning, limiting their scalability and generalizability. In this paper, we propose a novel self-supervised framework that harnesses the generative capabilities of LLMs to internally select and utilize task-relevant examples. Our method introduces two key objectives: a retrieval-generation alignment loss to optimize the quality of selected examples and a semantic coherence loss to ensure cross-lingual consistency. Through extensive experiments on multilingual benchmarks, our approach achieves state-of-the-art performance, significantly outperforming existing baselines. Further analysis highlights its robustness across diverse language families and its ability to generalize to unseen tasks. Human evaluations confirm the superior fluency, relevance, and semantic correctness of outputs generated by our method. This work provides a scalable, effective, and generalizable solution for cross-lingual in-context learning.",
      "authors": [
        "Mateo Alejandro Rojas",
        "Rafael Carranza"
      ],
      "published": "2024-12-12T05:36:51Z",
      "updated": "2024-12-12T05:36:51Z",
      "categories": [
        "cs.CL"
      ],
      "pdf_url": "https://arxiv.org/pdf/2412.08955v1",
      "landing_url": "https://arxiv.org/abs/2412.08955v1",
      "doi": "https://doi.org/10.48550/arXiv.2412.08955"
    },
    "queries": [
      {
        "anchor": "synchronous dialogue",
        "search_term": "semantic coherence"
      }
    ]
  },
  {
    "arxiv_id": "2412.09405",
    "anchor": "synchronous dialogue",
    "search_term": "neural codec",
    "search_record": {
      "id": "http://arxiv.org/abs/2412.09405v1",
      "title": "Learned Compression for Compressed Learning",
      "summary": "Modern sensors produce increasingly rich streams of high-resolution data. Due to resource constraints, machine learning systems discard the vast majority of this information via resolution reduction. Compressed-domain learning allows models to operate on compact latent representations, allowing higher effective resolution for the same budget. However, existing compression systems are not ideal for compressed learning. Linear transform coding and end-to-end learned compression systems reduce bitrate, but do not uniformly reduce dimensionality; thus, they do not meaningfully increase efficiency. Generative autoencoders reduce dimensionality, but their adversarial or perceptual objectives lead to significant information loss. To address these limitations, we introduce WaLLoC (Wavelet Learned Lossy Compression), a neural codec architecture that combines linear transform coding with nonlinear dimensionality-reducing autoencoders. WaLLoC sandwiches a shallow, asymmetric autoencoder and entropy bottleneck between an invertible wavelet packet transform. Across several key metrics, WaLLoC outperforms the autoencoders used in state-of-the-art latent diffusion models. WaLLoC does not require perceptual or adversarial losses to represent high-frequency detail, providing compatibility with modalities beyond RGB images and stereo audio. WaLLoC's encoder consists almost entirely of linear operations, making it exceptionally efficient and suitable for mobile computing, remote sensing, and learning directly from compressed data. We demonstrate WaLLoC's capability for compressed-domain learning across several tasks, including image classification, colorization, document understanding, and music source separation. Our code, experiments, and pre-trained audio and image codecs are available at https://ut-sysml.org/walloc",
      "published": "2024-12-12T16:09:57Z"
    },
    "metadata": {
      "arxiv_id": "2412.09405",
      "title": "Learned Compression for Compressed Learning",
      "summary": "Modern sensors produce increasingly rich streams of high-resolution data. Due to resource constraints, machine learning systems discard the vast majority of this information via resolution reduction. Compressed-domain learning allows models to operate on compact latent representations, allowing higher effective resolution for the same budget. However, existing compression systems are not ideal for compressed learning. Linear transform coding and end-to-end learned compression systems reduce bitrate, but do not uniformly reduce dimensionality; thus, they do not meaningfully increase efficiency. Generative autoencoders reduce dimensionality, but their adversarial or perceptual objectives lead to significant information loss. To address these limitations, we introduce WaLLoC (Wavelet Learned Lossy Compression), a neural codec architecture that combines linear transform coding with nonlinear dimensionality-reducing autoencoders. WaLLoC sandwiches a shallow, asymmetric autoencoder and entropy bottleneck between an invertible wavelet packet transform. Across several key metrics, WaLLoC outperforms the autoencoders used in state-of-the-art latent diffusion models. WaLLoC does not require perceptual or adversarial losses to represent high-frequency detail, providing compatibility with modalities beyond RGB images and stereo audio. WaLLoC's encoder consists almost entirely of linear operations, making it exceptionally efficient and suitable for mobile computing, remote sensing, and learning directly from compressed data. We demonstrate WaLLoC's capability for compressed-domain learning across several tasks, including image classification, colorization, document understanding, and music source separation. Our code, experiments, and pre-trained audio and image codecs are available at https://ut-sysml.org/walloc",
      "authors": [
        "Dan Jacobellis",
        "Neeraja J. Yadwadkar"
      ],
      "published": "2024-12-12T16:09:57Z",
      "updated": "2024-12-12T16:09:57Z",
      "categories": [
        "eess.IV",
        "cs.CV",
        "cs.LG",
        "eess.AS",
        "eess.SP"
      ],
      "pdf_url": "https://arxiv.org/pdf/2412.09405v1",
      "landing_url": "https://arxiv.org/abs/2412.09405v1",
      "doi": "https://doi.org/10.48550/arXiv.2412.09405"
    },
    "queries": [
      {
        "anchor": "synchronous dialogue",
        "search_term": "neural codec"
      }
    ]
  },
  {
    "arxiv_id": "2412.10095",
    "anchor": "synchronous dialogue",
    "search_term": "data scarcity",
    "search_record": {
      "id": "http://arxiv.org/abs/2412.10095v2",
      "title": "HiTZ at VarDial 2025 NorSID: Overcoming Data Scarcity with Language Transfer and Automatic Data Annotation",
      "summary": "In this paper we present our submission for the NorSID Shared Task as part of the 2025 VarDial Workshop (Scherrer et al., 2025), consisting of three tasks: Intent Detection, Slot Filling and Dialect Identification, evaluated using data in different dialects of the Norwegian language. For Intent Detection and Slot Filling, we have fine-tuned a multitask model in a cross-lingual setting, to leverage the xSID dataset available in 17 languages. In the case of Dialect Identification, our final submission consists of a model fine-tuned on the provided development set, which has obtained the highest scores within our experiments. Our final results on the test set show that our models do not drop in performance compared to the development set, likely due to the domain-specificity of the dataset and the similar distribution of both subsets. Finally, we also report an in-depth analysis of the provided datasets and their artifacts, as well as other sets of experiments that have been carried out but did not yield the best results. Additionally, we present an analysis on the reasons why some methods have been more successful than others; mainly the impact of the combination of languages and domain-specificity of the training data on the results.",
      "published": "2024-12-13T12:31:06Z"
    },
    "metadata": {
      "arxiv_id": "2412.10095",
      "title": "HiTZ at VarDial 2025 NorSID: Overcoming Data Scarcity with Language Transfer and Automatic Data Annotation",
      "summary": "In this paper we present our submission for the NorSID Shared Task as part of the 2025 VarDial Workshop (Scherrer et al., 2025), consisting of three tasks: Intent Detection, Slot Filling and Dialect Identification, evaluated using data in different dialects of the Norwegian language. For Intent Detection and Slot Filling, we have fine-tuned a multitask model in a cross-lingual setting, to leverage the xSID dataset available in 17 languages. In the case of Dialect Identification, our final submission consists of a model fine-tuned on the provided development set, which has obtained the highest scores within our experiments. Our final results on the test set show that our models do not drop in performance compared to the development set, likely due to the domain-specificity of the dataset and the similar distribution of both subsets. Finally, we also report an in-depth analysis of the provided datasets and their artifacts, as well as other sets of experiments that have been carried out but did not yield the best results. Additionally, we present an analysis on the reasons why some methods have been more successful than others; mainly the impact of the combination of languages and domain-specificity of the training data on the results.",
      "authors": [
        "Jaione Bengoetxea",
        "Mikel Zubillaga",
        "Ekhi Azurmendi",
        "Maite Heredia",
        "Julen Etxaniz",
        "Markel Ferro",
        "Jeremy Barnes"
      ],
      "published": "2024-12-13T12:31:06Z",
      "updated": "2025-01-09T09:09:32Z",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2412.10095v2",
      "landing_url": "https://arxiv.org/abs/2412.10095v2",
      "doi": "https://doi.org/10.48550/arXiv.2412.10095"
    },
    "queries": [
      {
        "anchor": "synchronous dialogue",
        "search_term": "data scarcity"
      }
    ]
  },
  {
    "arxiv_id": "2412.11369",
    "anchor": "spoken language models",
    "search_term": "temporal dynamics",
    "search_record": {
      "id": "http://arxiv.org/abs/2412.11369v2",
      "title": "PSGraph: Differentially Private Streaming Graph Synthesis by Considering Temporal Dynamics",
      "summary": "Streaming graphs are ubiquitous in daily life, such as evolving social networks and dynamic communication systems. Due to the sensitive information contained in the graph, directly sharing the streaming graphs poses significant privacy risks. Differential privacy, offering strict theoretical guarantees, has emerged as a standard approach for private graph data synthesis. However, existing methods predominantly focus on static graph publishing, neglecting the intrinsic relationship between adjacent graphs, thereby resulting in limited performance in streaming data publishing scenarios. To address this gap, we propose PSGraph, the first differentially private streaming graph synthesis framework that integrates temporal dynamics. PSGraph adaptively adjusts the privacy budget allocation mechanism by analyzing the variations in the current graph compared to the previous one for conserving the privacy budget. Moreover, PSGraph aggregates information across various timestamps and adopts crucial post-processing techniques to enhance the synthetic streaming graphs. We conduct extensive experiments on four real-world datasets under five commonly used metrics. The experimental results demonstrate the superiority of our proposed PSGraph.",
      "published": "2024-12-16T01:56:32Z"
    },
    "metadata": {
      "arxiv_id": "2412.11369",
      "title": "PSGraph: Differentially Private Streaming Graph Synthesis by Considering Temporal Dynamics",
      "summary": "Streaming graphs are ubiquitous in daily life, such as evolving social networks and dynamic communication systems. Due to the sensitive information contained in the graph, directly sharing the streaming graphs poses significant privacy risks. Differential privacy, offering strict theoretical guarantees, has emerged as a standard approach for private graph data synthesis. However, existing methods predominantly focus on static graph publishing, neglecting the intrinsic relationship between adjacent graphs, thereby resulting in limited performance in streaming data publishing scenarios. To address this gap, we propose PSGraph, the first differentially private streaming graph synthesis framework that integrates temporal dynamics. PSGraph adaptively adjusts the privacy budget allocation mechanism by analyzing the variations in the current graph compared to the previous one for conserving the privacy budget. Moreover, PSGraph aggregates information across various timestamps and adopts crucial post-processing techniques to enhance the synthetic streaming graphs. We conduct extensive experiments on four real-world datasets under five commonly used metrics. The experimental results demonstrate the superiority of our proposed PSGraph.",
      "authors": [
        "Quan Yuan",
        "Zhikun Zhang",
        "Linkang Du",
        "Min Chen",
        "Mingyang Sun",
        "Yunjun Gao",
        "Michael Backes",
        "Shibo He",
        "Jiming Chen"
      ],
      "published": "2024-12-16T01:56:32Z",
      "updated": "2025-05-30T14:28:15Z",
      "categories": [
        "cs.CR"
      ],
      "pdf_url": "https://arxiv.org/pdf/2412.11369v2",
      "landing_url": "https://arxiv.org/abs/2412.11369v2",
      "doi": "https://doi.org/10.48550/arXiv.2412.11369"
    },
    "queries": [
      {
        "anchor": "spoken language models",
        "search_term": "temporal dynamics"
      },
      {
        "anchor": "synchronous dialogue",
        "search_term": "temporal dynamics"
      },
      {
        "anchor": "turn-taking",
        "search_term": "temporal dynamics"
      }
    ]
  },
  {
    "arxiv_id": "2412.11449",
    "anchor": "spoken language models",
    "search_term": "audio tokens",
    "search_record": {
      "id": "http://arxiv.org/abs/2412.11449v1",
      "title": "Whisper-GPT: A Hybrid Representation Audio Large Language Model",
      "summary": "We propose WHISPER-GPT: A generative large language model (LLM) for speech and music that allows us to work with continuous audio representations and discrete tokens simultaneously as part of a single architecture. There has been a huge surge in generative audio, speech, and music models that utilize discrete audio tokens derived from neural compression algorithms, e.g. ENCODEC. However, one of the major drawbacks of this approach is handling the context length. It blows up for high-fidelity generative architecture if one has to account for all the audio contents at various frequencies for the next token prediction. By combining continuous audio representation like the spectrogram and discrete acoustic tokens, we retain the best of both worlds: Have all the information needed from the audio at a specific time instance in a single token, yet allow LLM to predict the future token to allow for sampling and other benefits discrete space provides. We show how our architecture improves the perplexity and negative log-likelihood scores for the next token prediction compared to a token-based LLM for speech and music.",
      "published": "2024-12-16T05:03:48Z"
    },
    "metadata": {
      "arxiv_id": "2412.11449",
      "title": "Whisper-GPT: A Hybrid Representation Audio Large Language Model",
      "summary": "We propose WHISPER-GPT: A generative large language model (LLM) for speech and music that allows us to work with continuous audio representations and discrete tokens simultaneously as part of a single architecture. There has been a huge surge in generative audio, speech, and music models that utilize discrete audio tokens derived from neural compression algorithms, e.g. ENCODEC. However, one of the major drawbacks of this approach is handling the context length. It blows up for high-fidelity generative architecture if one has to account for all the audio contents at various frequencies for the next token prediction. By combining continuous audio representation like the spectrogram and discrete acoustic tokens, we retain the best of both worlds: Have all the information needed from the audio at a specific time instance in a single token, yet allow LLM to predict the future token to allow for sampling and other benefits discrete space provides. We show how our architecture improves the perplexity and negative log-likelihood scores for the next token prediction compared to a token-based LLM for speech and music.",
      "authors": [
        "Prateek Verma"
      ],
      "published": "2024-12-16T05:03:48Z",
      "updated": "2024-12-16T05:03:48Z",
      "categories": [
        "cs.SD",
        "cs.AI",
        "cs.CL",
        "cs.LG",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2412.11449v1",
      "landing_url": "https://arxiv.org/abs/2412.11449v1",
      "doi": "https://doi.org/10.48550/arXiv.2412.11449"
    },
    "queries": [
      {
        "anchor": "spoken language models",
        "search_term": "audio tokens"
      },
      {
        "anchor": "synchronous dialogue",
        "search_term": "audio tokens"
      },
      {
        "anchor": "turn-taking",
        "search_term": "audio tokens"
      }
    ]
  },
  {
    "arxiv_id": "2412.12832",
    "anchor": "synchronous dialogue",
    "search_term": "semantic coherence",
    "search_record": {
      "id": "http://arxiv.org/abs/2412.12832v2",
      "title": "DSGram: Dynamic Weighting Sub-Metrics for Grammatical Error Correction in the Era of Large Language Models",
      "summary": "Evaluating the performance of Grammatical Error Correction (GEC) models has become increasingly challenging, as large language model (LLM)-based GEC systems often produce corrections that diverge from provided gold references. This discrepancy undermines the reliability of traditional reference-based evaluation metrics. In this study, we propose a novel evaluation framework for GEC models, DSGram, integrating Semantic Coherence, Edit Level, and Fluency, and utilizing a dynamic weighting mechanism. Our framework employs the Analytic Hierarchy Process (AHP) in conjunction with large language models to ascertain the relative importance of various evaluation criteria. Additionally, we develop a dataset incorporating human annotations and LLM-simulated sentences to validate our algorithms and fine-tune more cost-effective models. Experimental results indicate that our proposed approach enhances the effectiveness of GEC model evaluations.",
      "published": "2024-12-17T11:54:16Z"
    },
    "metadata": {
      "arxiv_id": "2412.12832",
      "title": "DSGram: Dynamic Weighting Sub-Metrics for Grammatical Error Correction in the Era of Large Language Models",
      "summary": "Evaluating the performance of Grammatical Error Correction (GEC) models has become increasingly challenging, as large language model (LLM)-based GEC systems often produce corrections that diverge from provided gold references. This discrepancy undermines the reliability of traditional reference-based evaluation metrics. In this study, we propose a novel evaluation framework for GEC models, DSGram, integrating Semantic Coherence, Edit Level, and Fluency, and utilizing a dynamic weighting mechanism. Our framework employs the Analytic Hierarchy Process (AHP) in conjunction with large language models to ascertain the relative importance of various evaluation criteria. Additionally, we develop a dataset incorporating human annotations and LLM-simulated sentences to validate our algorithms and fine-tune more cost-effective models. Experimental results indicate that our proposed approach enhances the effectiveness of GEC model evaluations.",
      "authors": [
        "Jinxiang Xie",
        "Yilin Li",
        "Xunjian Yin",
        "Xiaojun Wan"
      ],
      "published": "2024-12-17T11:54:16Z",
      "updated": "2025-06-22T05:51:54Z",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2412.12832v2",
      "landing_url": "https://arxiv.org/abs/2412.12832v2",
      "doi": "https://doi.org/10.1609/aaai.v39i24.34746"
    },
    "queries": [
      {
        "anchor": "synchronous dialogue",
        "search_term": "semantic coherence"
      }
    ]
  },
  {
    "arxiv_id": "2412.15113",
    "anchor": "spoken language models",
    "search_term": "streaming architecture",
    "search_record": {
      "id": "http://arxiv.org/abs/2412.15113v2",
      "title": "Associative memory inspires improvements for in-context learning using a novel attention residual stream architecture",
      "summary": "Large language models (LLMs) demonstrate an impressive ability to utilise information within the context of their input sequences to appropriately respond to data unseen by the LLM during its training procedure. This ability is known as in-context learning (ICL). Humans and non-human animals demonstrate similar abilities, however their neural architectures differ substantially from LLMs. Despite this, a critical component within LLMs, the attention mechanism, resembles modern associative memory models, widely used in and influenced by the computational neuroscience community to model biological memory systems. Using this connection, we introduce an associative memory model capable of performing ICL. We use this as inspiration for a novel residual stream architecture which allows information to directly flow between attention heads. We test this architecture during training within a two-layer Transformer and show its ICL abilities manifest more quickly than without this modification. We then apply our architecture in small language models with 8 million and 1 billion parameters, focusing on attention head values, with results also indicating improved performance at these larger and more naturalistic scales.",
      "published": "2024-12-19T17:55:42Z"
    },
    "metadata": {
      "arxiv_id": "2412.15113",
      "title": "Associative memory inspires improvements for in-context learning using a novel attention residual stream architecture",
      "summary": "Large language models (LLMs) demonstrate an impressive ability to utilise information within the context of their input sequences to appropriately respond to data unseen by the LLM during its training procedure. This ability is known as in-context learning (ICL). Humans and non-human animals demonstrate similar abilities, however their neural architectures differ substantially from LLMs. Despite this, a critical component within LLMs, the attention mechanism, resembles modern associative memory models, widely used in and influenced by the computational neuroscience community to model biological memory systems. Using this connection, we introduce an associative memory model capable of performing ICL. We use this as inspiration for a novel residual stream architecture which allows information to directly flow between attention heads. We test this architecture during training within a two-layer Transformer and show its ICL abilities manifest more quickly than without this modification. We then apply our architecture in small language models with 8 million and 1 billion parameters, focusing on attention head values, with results also indicating improved performance at these larger and more naturalistic scales.",
      "authors": [
        "Thomas F Burns",
        "Tomoki Fukai",
        "Christopher J Earls"
      ],
      "published": "2024-12-19T17:55:42Z",
      "updated": "2025-08-04T12:51:56Z",
      "categories": [
        "cs.NE",
        "cs.AI",
        "cs.CL"
      ],
      "pdf_url": "https://arxiv.org/pdf/2412.15113v2",
      "landing_url": "https://arxiv.org/abs/2412.15113v2",
      "doi": "https://doi.org/10.48550/arXiv.2412.15113"
    },
    "queries": [
      {
        "anchor": "spoken language models",
        "search_term": "streaming architecture"
      },
      {
        "anchor": "synchronous dialogue",
        "search_term": "streaming architecture"
      },
      {
        "anchor": "turn-taking",
        "search_term": "streaming architecture"
      }
    ]
  },
  {
    "arxiv_id": "2412.15649",
    "anchor": "spoken language models",
    "search_term": "audio tokens",
    "search_record": {
      "id": "http://arxiv.org/abs/2412.15649v1",
      "title": "SLAM-Omni: Timbre-Controllable Voice Interaction System with Single-Stage Training",
      "summary": "Recent advancements highlight the potential of end-to-end real-time spoken dialogue systems, showcasing their low latency and high quality. In this paper, we introduce SLAM-Omni, a timbre-controllable, end-to-end voice interaction system with single-stage training. SLAM-Omni achieves zero-shot timbre control by modeling spoken language with semantic tokens and decoupling speaker information to a vocoder. By predicting grouped speech semantic tokens at each step, our method significantly reduces the sequence length of audio tokens, accelerating both training and inference. Additionally, we propose historical text prompting to compress dialogue history, facilitating efficient multi-round interactions. Comprehensive evaluations reveal that SLAM-Omni outperforms prior models of similar scale, requiring only 15 hours of training on 4 GPUs with limited data. Notably, it is the first spoken dialogue system to achieve competitive performance with a single-stage training approach, eliminating the need for pre-training on TTS or ASR tasks. Further experiments validate its multilingual and multi-turn dialogue capabilities on larger datasets.",
      "published": "2024-12-20T08:05:55Z"
    },
    "metadata": {
      "arxiv_id": "2412.15649",
      "title": "SLAM-Omni: Timbre-Controllable Voice Interaction System with Single-Stage Training",
      "summary": "Recent advancements highlight the potential of end-to-end real-time spoken dialogue systems, showcasing their low latency and high quality. In this paper, we introduce SLAM-Omni, a timbre-controllable, end-to-end voice interaction system with single-stage training. SLAM-Omni achieves zero-shot timbre control by modeling spoken language with semantic tokens and decoupling speaker information to a vocoder. By predicting grouped speech semantic tokens at each step, our method significantly reduces the sequence length of audio tokens, accelerating both training and inference. Additionally, we propose historical text prompting to compress dialogue history, facilitating efficient multi-round interactions. Comprehensive evaluations reveal that SLAM-Omni outperforms prior models of similar scale, requiring only 15 hours of training on 4 GPUs with limited data. Notably, it is the first spoken dialogue system to achieve competitive performance with a single-stage training approach, eliminating the need for pre-training on TTS or ASR tasks. Further experiments validate its multilingual and multi-turn dialogue capabilities on larger datasets.",
      "authors": [
        "Wenxi Chen",
        "Ziyang Ma",
        "Ruiqi Yan",
        "Yuzhe Liang",
        "Xiquan Li",
        "Ruiyang Xu",
        "Zhikang Niu",
        "Yanqiao Zhu",
        "Yifan Yang",
        "Zhanxun Liu",
        "Kai Yu",
        "Yuxuan Hu",
        "Jinyu Li",
        "Yan Lu",
        "Shujie Liu",
        "Xie Chen"
      ],
      "published": "2024-12-20T08:05:55Z",
      "updated": "2024-12-20T08:05:55Z",
      "categories": [
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2412.15649v1",
      "landing_url": "https://arxiv.org/abs/2412.15649v1",
      "doi": "https://doi.org/10.48550/arXiv.2412.15649"
    },
    "queries": [
      {
        "anchor": "spoken language models",
        "search_term": "audio tokens"
      },
      {
        "anchor": "synchronous dialogue",
        "search_term": "audio tokens"
      },
      {
        "anchor": "turn-taking",
        "search_term": "audio tokens"
      }
    ]
  },
  {
    "arxiv_id": "2412.16934",
    "anchor": "turn-taking",
    "search_term": "true full-duplex",
    "search_record": {
      "id": "http://arxiv.org/abs/2412.16934v1",
      "title": "Efficiently Solving Turn-Taking Stochastic Games with Extensive-Form Correlation",
      "summary": "We study equilibrium computation with extensive-form correlation in two-player turn-taking stochastic games. Our main results are two-fold: (1) We give an algorithm for computing a Stackelberg extensive-form correlated equilibrium (SEFCE), which runs in time polynomial in the size of the game, as well as the number of bits required to encode each input number. (2) We give an efficient algorithm for approximately computing an optimal extensive-form correlated equilibrium (EFCE) up to machine precision, i.e., the algorithm achieves approximation error $\\varepsilon$ in time polynomial in the size of the game, as well as $\\log(1 / \\varepsilon)$.\n  Our algorithm for SEFCE is the first polynomial-time algorithm for equilibrium computation with commitment in such a general class of stochastic games. Existing algorithms for SEFCE typically make stronger assumptions such as no chance moves, and are designed for extensive-form games in the less succinct tree form. Our algorithm for approximately optimal EFCE is, to our knowledge, the first algorithm that achieves 3 desiderata simultaneously: approximate optimality, polylogarithmic dependency on the approximation error, and compatibility with stochastic games in the more succinct graph form. Existing algorithms achieve at most 2 of these desiderata, often also relying on additional technical assumptions.",
      "published": "2024-12-22T09:12:05Z"
    },
    "metadata": {
      "arxiv_id": "2412.16934",
      "title": "Efficiently Solving Turn-Taking Stochastic Games with Extensive-Form Correlation",
      "summary": "We study equilibrium computation with extensive-form correlation in two-player turn-taking stochastic games. Our main results are two-fold: (1) We give an algorithm for computing a Stackelberg extensive-form correlated equilibrium (SEFCE), which runs in time polynomial in the size of the game, as well as the number of bits required to encode each input number. (2) We give an efficient algorithm for approximately computing an optimal extensive-form correlated equilibrium (EFCE) up to machine precision, i.e., the algorithm achieves approximation error $\\varepsilon$ in time polynomial in the size of the game, as well as $\\log(1 / \\varepsilon)$.\n  Our algorithm for SEFCE is the first polynomial-time algorithm for equilibrium computation with commitment in such a general class of stochastic games. Existing algorithms for SEFCE typically make stronger assumptions such as no chance moves, and are designed for extensive-form games in the less succinct tree form. Our algorithm for approximately optimal EFCE is, to our knowledge, the first algorithm that achieves 3 desiderata simultaneously: approximate optimality, polylogarithmic dependency on the approximation error, and compatibility with stochastic games in the more succinct graph form. Existing algorithms achieve at most 2 of these desiderata, often also relying on additional technical assumptions.",
      "authors": [
        "Hanrui Zhang",
        "Yu Cheng",
        "Vincent Conitzer"
      ],
      "published": "2024-12-22T09:12:05Z",
      "updated": "2024-12-22T09:12:05Z",
      "categories": [
        "cs.GT",
        "cs.AI",
        "cs.DS",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2412.16934v1",
      "landing_url": "https://arxiv.org/abs/2412.16934v1",
      "doi": "https://doi.org/10.48550/arXiv.2412.16934"
    },
    "queries": [
      {
        "anchor": "turn-taking",
        "search_term": "true full-duplex"
      },
      {
        "anchor": "turn-taking",
        "search_term": "pseudo full-duplex"
      },
      {
        "anchor": "turn-taking",
        "search_term": "half duplex"
      },
      {
        "anchor": "turn-taking",
        "search_term": "engineered synchronization"
      },
      {
        "anchor": "turn-taking",
        "search_term": "learned synchronization"
      },
      {
        "anchor": "turn-taking",
        "search_term": "modular architecture"
      },
      {
        "anchor": "turn-taking",
        "search_term": "streaming architecture"
      },
      {
        "anchor": "turn-taking",
        "search_term": "real-time interaction"
      },
      {
        "anchor": "turn-taking",
        "search_term": "low latency"
      },
      {
        "anchor": "turn-taking",
        "search_term": "synchronous data"
      },
      {
        "anchor": "turn-taking",
        "search_term": "data scarcity"
      },
      {
        "anchor": "turn-taking",
        "search_term": "multichannel dialogue"
      },
      {
        "anchor": "turn-taking",
        "search_term": "temporal dynamics"
      },
      {
        "anchor": "turn-taking",
        "search_term": "behavioral arbitration"
      },
      {
        "anchor": "turn-taking",
        "search_term": "semantic coherence"
      },
      {
        "anchor": "turn-taking",
        "search_term": "neural codec"
      },
      {
        "anchor": "turn-taking",
        "search_term": "discrete tokens"
      },
      {
        "anchor": "turn-taking",
        "search_term": "audio tokens"
      },
      {
        "anchor": "turn-taking",
        "search_term": "floor transfer offset"
      },
      {
        "anchor": "turn-taking",
        "search_term": "stop latency"
      },
      {
        "anchor": "turn-taking",
        "search_term": "interruption response delay"
      }
    ]
  },
  {
    "arxiv_id": "2412.17048",
    "anchor": "spoken language models",
    "search_term": "semantic coherence",
    "search_record": {
      "id": "http://arxiv.org/abs/2412.17048v1",
      "title": "Why Do Speech Language Models Fail to Generate Semantically Coherent Outputs? A Modality Evolving Perspective",
      "summary": "Although text-based large language models exhibit human-level writing ability and remarkable intelligence, speech language models (SLMs) still struggle to generate semantically coherent outputs. There are several potential reasons for this performance degradation: (A) speech tokens mainly provide phonetic information rather than semantic information, (B) the length of speech sequences is much longer than that of text sequences, and (C) paralinguistic information, such as prosody, introduces additional complexity and variability. In this paper, we explore the influence of three key factors separately by transiting the modality from text to speech in an evolving manner. Our findings reveal that the impact of the three factors varies. Factor A has a relatively minor impact, factor B influences syntactical and semantic modeling more obviously, and factor C exerts the most significant impact, particularly in the basic lexical modeling. Based on these findings, we provide insights into the unique challenges of training SLMs and highlight pathways to develop more effective end-to-end SLMs.",
      "published": "2024-12-22T14:59:19Z"
    },
    "metadata": {
      "arxiv_id": "2412.17048",
      "title": "Why Do Speech Language Models Fail to Generate Semantically Coherent Outputs? A Modality Evolving Perspective",
      "summary": "Although text-based large language models exhibit human-level writing ability and remarkable intelligence, speech language models (SLMs) still struggle to generate semantically coherent outputs. There are several potential reasons for this performance degradation: (A) speech tokens mainly provide phonetic information rather than semantic information, (B) the length of speech sequences is much longer than that of text sequences, and (C) paralinguistic information, such as prosody, introduces additional complexity and variability. In this paper, we explore the influence of three key factors separately by transiting the modality from text to speech in an evolving manner. Our findings reveal that the impact of the three factors varies. Factor A has a relatively minor impact, factor B influences syntactical and semantic modeling more obviously, and factor C exerts the most significant impact, particularly in the basic lexical modeling. Based on these findings, we provide insights into the unique challenges of training SLMs and highlight pathways to develop more effective end-to-end SLMs.",
      "authors": [
        "Hankun Wang",
        "Haoran Wang",
        "Yiwei Guo",
        "Zhihan Li",
        "Chenpeng Du",
        "Xie Chen",
        "Kai Yu"
      ],
      "published": "2024-12-22T14:59:19Z",
      "updated": "2024-12-22T14:59:19Z",
      "categories": [
        "eess.AS",
        "cs.CL",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2412.17048v1",
      "landing_url": "https://arxiv.org/abs/2412.17048v1",
      "doi": "https://doi.org/10.48550/arXiv.2412.17048"
    },
    "queries": [
      {
        "anchor": "spoken language models",
        "search_term": "semantic coherence"
      },
      {
        "anchor": "synchronous dialogue",
        "search_term": "semantic coherence"
      },
      {
        "anchor": "turn-taking",
        "search_term": "semantic coherence"
      }
    ]
  },
  {
    "arxiv_id": "2412.17164",
    "anchor": "spoken language models",
    "search_term": "temporal dynamics",
    "search_record": {
      "id": "http://arxiv.org/abs/2412.17164v1",
      "title": "Analysis of Speech Temporal Dynamics in the Context of Speaker Verification and Voice Anonymization",
      "summary": "In this paper, we investigate the impact of speech temporal dynamics in application to automatic speaker verification and speaker voice anonymization tasks. We propose several metrics to perform automatic speaker verification based only on phoneme durations. Experimental results demonstrate that phoneme durations leak some speaker information and can reveal speaker identity from both original and anonymized speech. Thus, this work emphasizes the importance of taking into account the speaker's speech rate and, more importantly, the speaker's phonetic duration characteristics, as well as the need to modify them in order to develop anonymization systems with strong privacy protection capacity.",
      "published": "2024-12-22T21:18:08Z"
    },
    "metadata": {
      "arxiv_id": "2412.17164",
      "title": "Analysis of Speech Temporal Dynamics in the Context of Speaker Verification and Voice Anonymization",
      "summary": "In this paper, we investigate the impact of speech temporal dynamics in application to automatic speaker verification and speaker voice anonymization tasks. We propose several metrics to perform automatic speaker verification based only on phoneme durations. Experimental results demonstrate that phoneme durations leak some speaker information and can reveal speaker identity from both original and anonymized speech. Thus, this work emphasizes the importance of taking into account the speaker's speech rate and, more importantly, the speaker's phonetic duration characteristics, as well as the need to modify them in order to develop anonymization systems with strong privacy protection capacity.",
      "authors": [
        "Natalia Tomashenko",
        "Emmanuel Vincent",
        "Marc Tommasi"
      ],
      "published": "2024-12-22T21:18:08Z",
      "updated": "2024-12-22T21:18:08Z",
      "categories": [
        "eess.AS",
        "cs.CL",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2412.17164v1",
      "landing_url": "https://arxiv.org/abs/2412.17164v1",
      "doi": "https://doi.org/10.1109/ICASSP49660.2025.10887896"
    },
    "queries": [
      {
        "anchor": "spoken language models",
        "search_term": "temporal dynamics"
      },
      {
        "anchor": "synchronous dialogue",
        "search_term": "temporal dynamics"
      },
      {
        "anchor": "turn-taking",
        "search_term": "temporal dynamics"
      }
    ]
  },
  {
    "arxiv_id": "2412.17571",
    "anchor": "synchronous dialogue",
    "search_term": "temporal dynamics",
    "search_record": {
      "id": "http://arxiv.org/abs/2412.17571v1",
      "title": "HPCNeuroNet: A Neuromorphic Approach Merging SNN Temporal Dynamics with Transformer Attention for FPGA-based Particle Physics",
      "summary": "This paper presents the innovative HPCNeuroNet model, a pioneering fusion of Spiking Neural Networks (SNNs), Transformers, and high-performance computing tailored for particle physics, particularly in particle identification from detector responses. Our approach leverages SNNs' intrinsic temporal dynamics and Transformers' robust attention mechanisms to enhance performance when discerning intricate particle interactions. At the heart of HPCNeuroNet lies the integration of the sequential dynamism inherent in SNNs with the context-aware attention capabilities of Transformers, enabling the model to precisely decode and interpret complex detector data. HPCNeuroNet is realized through the HLS4ML framework and optimized for deployment in FPGA environments. The model accuracy and scalability are also enhanced by this architectural choice. Benchmarked against machine learning models, HPCNeuroNet showcases better performance metrics, underlining its transformative potential in high-energy physics. We demonstrate that the combination of SNNs, Transformers, and FPGA-based high-performance computing in particle physics signifies a significant step forward and provides a strong foundation for future research.",
      "published": "2024-12-23T13:44:29Z"
    },
    "metadata": {
      "arxiv_id": "2412.17571",
      "title": "HPCNeuroNet: A Neuromorphic Approach Merging SNN Temporal Dynamics with Transformer Attention for FPGA-based Particle Physics",
      "summary": "This paper presents the innovative HPCNeuroNet model, a pioneering fusion of Spiking Neural Networks (SNNs), Transformers, and high-performance computing tailored for particle physics, particularly in particle identification from detector responses. Our approach leverages SNNs' intrinsic temporal dynamics and Transformers' robust attention mechanisms to enhance performance when discerning intricate particle interactions. At the heart of HPCNeuroNet lies the integration of the sequential dynamism inherent in SNNs with the context-aware attention capabilities of Transformers, enabling the model to precisely decode and interpret complex detector data. HPCNeuroNet is realized through the HLS4ML framework and optimized for deployment in FPGA environments. The model accuracy and scalability are also enhanced by this architectural choice. Benchmarked against machine learning models, HPCNeuroNet showcases better performance metrics, underlining its transformative potential in high-energy physics. We demonstrate that the combination of SNNs, Transformers, and FPGA-based high-performance computing in particle physics signifies a significant step forward and provides a strong foundation for future research.",
      "authors": [
        "Murat Isik",
        "Hiruna Vishwamith",
        "Jonathan Naoukin",
        "I. Can Dikmen"
      ],
      "published": "2024-12-23T13:44:29Z",
      "updated": "2024-12-23T13:44:29Z",
      "categories": [
        "cs.LG",
        "cs.AR",
        "physics.comp-ph"
      ],
      "pdf_url": "https://arxiv.org/pdf/2412.17571v1",
      "landing_url": "https://arxiv.org/abs/2412.17571v1",
      "doi": "https://doi.org/10.48550/arXiv.2412.17571"
    },
    "queries": [
      {
        "anchor": "synchronous dialogue",
        "search_term": "temporal dynamics"
      }
    ]
  },
  {
    "arxiv_id": "2412.18061",
    "anchor": "turn-taking",
    "search_term": "true full-duplex",
    "search_record": {
      "id": "http://arxiv.org/abs/2412.18061v1",
      "title": "Lla-VAP: LSTM Ensemble of Llama and VAP for Turn-Taking Prediction",
      "summary": "Turn-taking prediction is the task of anticipating when the speaker in a conversation will yield their turn to another speaker to begin speaking. This project expands on existing strategies for turn-taking prediction by employing a multi-modal ensemble approach that integrates large language models (LLMs) and voice activity projection (VAP) models. By combining the linguistic capabilities of LLMs with the temporal precision of VAP models, we aim to improve the accuracy and efficiency of identifying TRPs in both scripted and unscripted conversational scenarios. Our methods are evaluated on the In-Conversation Corpus (ICC) and Coached Conversational Preference Elicitation (CCPE) datasets, highlighting the strengths and limitations of current models while proposing a potentially more robust framework for enhanced prediction.",
      "published": "2024-12-24T00:20:38Z"
    },
    "metadata": {
      "arxiv_id": "2412.18061",
      "title": "Lla-VAP: LSTM Ensemble of Llama and VAP for Turn-Taking Prediction",
      "summary": "Turn-taking prediction is the task of anticipating when the speaker in a conversation will yield their turn to another speaker to begin speaking. This project expands on existing strategies for turn-taking prediction by employing a multi-modal ensemble approach that integrates large language models (LLMs) and voice activity projection (VAP) models. By combining the linguistic capabilities of LLMs with the temporal precision of VAP models, we aim to improve the accuracy and efficiency of identifying TRPs in both scripted and unscripted conversational scenarios. Our methods are evaluated on the In-Conversation Corpus (ICC) and Coached Conversational Preference Elicitation (CCPE) datasets, highlighting the strengths and limitations of current models while proposing a potentially more robust framework for enhanced prediction.",
      "authors": [
        "Hyunbae Jeon",
        "Frederic Guintu",
        "Rayvant Sahni"
      ],
      "published": "2024-12-24T00:20:38Z",
      "updated": "2024-12-24T00:20:38Z",
      "categories": [
        "cs.SD",
        "cs.CL",
        "cs.HC",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2412.18061v1",
      "landing_url": "https://arxiv.org/abs/2412.18061v1",
      "doi": "https://doi.org/10.48550/arXiv.2412.18061"
    },
    "queries": [
      {
        "anchor": "turn-taking",
        "search_term": "true full-duplex"
      },
      {
        "anchor": "turn-taking",
        "search_term": "pseudo full-duplex"
      },
      {
        "anchor": "turn-taking",
        "search_term": "half duplex"
      },
      {
        "anchor": "turn-taking",
        "search_term": "engineered synchronization"
      },
      {
        "anchor": "turn-taking",
        "search_term": "learned synchronization"
      },
      {
        "anchor": "turn-taking",
        "search_term": "modular architecture"
      },
      {
        "anchor": "turn-taking",
        "search_term": "streaming architecture"
      },
      {
        "anchor": "turn-taking",
        "search_term": "real-time interaction"
      },
      {
        "anchor": "turn-taking",
        "search_term": "low latency"
      },
      {
        "anchor": "turn-taking",
        "search_term": "synchronous data"
      },
      {
        "anchor": "turn-taking",
        "search_term": "data scarcity"
      },
      {
        "anchor": "turn-taking",
        "search_term": "multichannel dialogue"
      },
      {
        "anchor": "turn-taking",
        "search_term": "temporal dynamics"
      },
      {
        "anchor": "turn-taking",
        "search_term": "behavioral arbitration"
      },
      {
        "anchor": "turn-taking",
        "search_term": "semantic coherence"
      },
      {
        "anchor": "turn-taking",
        "search_term": "neural codec"
      },
      {
        "anchor": "turn-taking",
        "search_term": "discrete tokens"
      },
      {
        "anchor": "turn-taking",
        "search_term": "audio tokens"
      },
      {
        "anchor": "turn-taking",
        "search_term": "floor transfer offset"
      },
      {
        "anchor": "turn-taking",
        "search_term": "stop latency"
      },
      {
        "anchor": "turn-taking",
        "search_term": "interruption response delay"
      }
    ]
  },
  {
    "arxiv_id": "2412.18390",
    "anchor": "synchronous dialogue",
    "search_term": "discrete tokens",
    "search_record": {
      "id": "http://arxiv.org/abs/2412.18390v2",
      "title": "RDPM: Solve Diffusion Probabilistic Models via Recurrent Token Prediction",
      "summary": "Diffusion Probabilistic Models (DPMs) have emerged as the de facto approach for high-fidelity image synthesis, operating diffusion processes on continuous VAE latent, which significantly differ from the text generation methods employed by Large Language Models (LLMs). In this paper, we introduce a novel generative framework, the Recurrent Diffusion Probabilistic Model (RDPM), which enhances the diffusion process through a recurrent token prediction mechanism, thereby pioneering the field of Discrete Diffusion. By progressively introducing Gaussian noise into the latent representations of images and encoding them into vector-quantized tokens in a recurrent manner, RDPM facilitates a unique diffusion process on discrete-value domains. This process iteratively predicts the token codes for subsequent timesteps, transforming the initial standard Gaussian noise into the source data distribution, aligning with GPT-style models in terms of the loss function. RDPM demonstrates superior performance while benefiting from the speed advantage of requiring only a few inference steps. This model not only leverages the diffusion process to ensure high-quality generation but also converts continuous signals into a series of high-fidelity discrete tokens, thereby maintaining a unified optimization strategy with other discrete tokens, such as text. We anticipate that this work will contribute to the development of a unified model for multimodal generation, specifically by integrating continuous signal domains such as images, videos, and audio with text. We will release the code and model weights to the open-source community.",
      "published": "2024-12-24T12:28:19Z"
    },
    "metadata": {
      "arxiv_id": "2412.18390",
      "title": "RDPM: Solve Diffusion Probabilistic Models via Recurrent Token Prediction",
      "summary": "Diffusion Probabilistic Models (DPMs) have emerged as the de facto approach for high-fidelity image synthesis, operating diffusion processes on continuous VAE latent, which significantly differ from the text generation methods employed by Large Language Models (LLMs). In this paper, we introduce a novel generative framework, the Recurrent Diffusion Probabilistic Model (RDPM), which enhances the diffusion process through a recurrent token prediction mechanism, thereby pioneering the field of Discrete Diffusion. By progressively introducing Gaussian noise into the latent representations of images and encoding them into vector-quantized tokens in a recurrent manner, RDPM facilitates a unique diffusion process on discrete-value domains. This process iteratively predicts the token codes for subsequent timesteps, transforming the initial standard Gaussian noise into the source data distribution, aligning with GPT-style models in terms of the loss function. RDPM demonstrates superior performance while benefiting from the speed advantage of requiring only a few inference steps. This model not only leverages the diffusion process to ensure high-quality generation but also converts continuous signals into a series of high-fidelity discrete tokens, thereby maintaining a unified optimization strategy with other discrete tokens, such as text. We anticipate that this work will contribute to the development of a unified model for multimodal generation, specifically by integrating continuous signal domains such as images, videos, and audio with text. We will release the code and model weights to the open-source community.",
      "authors": [
        "Xiaoping Wu",
        "Jie Hu",
        "Xiaoming Wei"
      ],
      "published": "2024-12-24T12:28:19Z",
      "updated": "2024-12-25T12:12:10Z",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG",
        "cs.MM"
      ],
      "pdf_url": "https://arxiv.org/pdf/2412.18390v2",
      "landing_url": "https://arxiv.org/abs/2412.18390v2",
      "doi": "https://doi.org/10.48550/arXiv.2412.18390"
    },
    "queries": [
      {
        "anchor": "synchronous dialogue",
        "search_term": "discrete tokens"
      }
    ]
  },
  {
    "arxiv_id": "2412.18603",
    "anchor": "spoken language models",
    "search_term": "true full-duplex",
    "search_record": {
      "id": "http://arxiv.org/abs/2412.18603v2",
      "title": "Long-Form Speech Generation with Spoken Language Models",
      "summary": "We consider the generative modeling of speech over multiple minutes, a requirement for long-form multimedia generation and audio-native voice assistants. However, textless spoken language models struggle to generate plausible speech past tens of seconds, due to high temporal resolution of speech tokens causing loss of coherence, architectural issues with long-sequence training or extrapolation, and memory costs at inference time. From these considerations we derive SpeechSSM, the first speech language model family to learn from and sample long-form spoken audio (e.g., 16 minutes of read or extemporaneous speech) in a single decoding session without text intermediates. SpeechSSMs leverage recent advances in linear-time sequence modeling to greatly surpass current Transformer spoken LMs in coherence and efficiency on multi-minute generations while still matching them at the utterance level. As we found current spoken language evaluations uninformative, especially in this new long-form setting, we also introduce: LibriSpeech-Long, a benchmark for long-form speech evaluation; new embedding-based and LLM-judged metrics; and quality measurements over length and time. Speech samples, the LibriSpeech-Long dataset, and any future code or model releases can be found at https://google.github.io/tacotron/publications/speechssm/.",
      "published": "2024-12-24T18:56:46Z"
    },
    "metadata": {
      "arxiv_id": "2412.18603",
      "title": "Long-Form Speech Generation with Spoken Language Models",
      "summary": "We consider the generative modeling of speech over multiple minutes, a requirement for long-form multimedia generation and audio-native voice assistants. However, textless spoken language models struggle to generate plausible speech past tens of seconds, due to high temporal resolution of speech tokens causing loss of coherence, architectural issues with long-sequence training or extrapolation, and memory costs at inference time. From these considerations we derive SpeechSSM, the first speech language model family to learn from and sample long-form spoken audio (e.g., 16 minutes of read or extemporaneous speech) in a single decoding session without text intermediates. SpeechSSMs leverage recent advances in linear-time sequence modeling to greatly surpass current Transformer spoken LMs in coherence and efficiency on multi-minute generations while still matching them at the utterance level. As we found current spoken language evaluations uninformative, especially in this new long-form setting, we also introduce: LibriSpeech-Long, a benchmark for long-form speech evaluation; new embedding-based and LLM-judged metrics; and quality measurements over length and time. Speech samples, the LibriSpeech-Long dataset, and any future code or model releases can be found at https://google.github.io/tacotron/publications/speechssm/.",
      "authors": [
        "Se Jin Park",
        "Julian Salazar",
        "Aren Jansen",
        "Keisuke Kinoshita",
        "Yong Man Ro",
        "RJ Skerry-Ryan"
      ],
      "published": "2024-12-24T18:56:46Z",
      "updated": "2025-07-10T17:52:43Z",
      "categories": [
        "cs.CL",
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2412.18603v2",
      "landing_url": "https://arxiv.org/abs/2412.18603v2",
      "doi": "https://doi.org/10.48550/arXiv.2412.18603"
    },
    "queries": [
      {
        "anchor": "spoken language models",
        "search_term": "true full-duplex"
      },
      {
        "anchor": "spoken language models",
        "search_term": "pseudo full-duplex"
      },
      {
        "anchor": "spoken language models",
        "search_term": "half duplex"
      },
      {
        "anchor": "spoken language models",
        "search_term": "engineered synchronization"
      },
      {
        "anchor": "spoken language models",
        "search_term": "learned synchronization"
      },
      {
        "anchor": "spoken language models",
        "search_term": "modular architecture"
      },
      {
        "anchor": "spoken language models",
        "search_term": "streaming architecture"
      },
      {
        "anchor": "spoken language models",
        "search_term": "real-time interaction"
      },
      {
        "anchor": "spoken language models",
        "search_term": "low latency"
      },
      {
        "anchor": "spoken language models",
        "search_term": "synchronous data"
      },
      {
        "anchor": "spoken language models",
        "search_term": "data scarcity"
      },
      {
        "anchor": "spoken language models",
        "search_term": "multichannel dialogue"
      },
      {
        "anchor": "spoken language models",
        "search_term": "temporal dynamics"
      },
      {
        "anchor": "spoken language models",
        "search_term": "behavioral arbitration"
      },
      {
        "anchor": "spoken language models",
        "search_term": "semantic coherence"
      },
      {
        "anchor": "spoken language models",
        "search_term": "neural codec"
      },
      {
        "anchor": "spoken language models",
        "search_term": "discrete tokens"
      },
      {
        "anchor": "spoken language models",
        "search_term": "audio tokens"
      },
      {
        "anchor": "spoken language models",
        "search_term": "floor transfer offset"
      },
      {
        "anchor": "spoken language models",
        "search_term": "stop latency"
      },
      {
        "anchor": "spoken language models",
        "search_term": "interruption response delay"
      }
    ]
  },
  {
    "arxiv_id": "2501.00039",
    "anchor": "spoken language models",
    "search_term": "audio tokens",
    "search_record": {
      "id": "http://arxiv.org/abs/2501.00039v1",
      "title": "Speech Recognition With LLMs Adapted to Disordered Speech Using Reinforcement Learning",
      "summary": "We introduce a large language model (LLM) capable of processing speech inputs and show that tuning it further with reinforcement learning on human preference (RLHF) enables it to adapt better to disordered speech than traditional fine-tuning. Our method replaces low-frequency text tokens in an LLM's vocabulary with audio tokens and enables the model to recognize speech by fine-tuning it on speech with transcripts. We then use RL with rewards based on syntactic and semantic accuracy measures generalizing the LLM further to recognize disordered speech. While the resulting LLM does not outperform existing systems for speech recognition, we find that tuning with reinforcement learning using custom rewards leads to substantially better performance than supervised fine-tuning of the language model, specifically when adapting to speech in a different setting. This presents a compelling alternative tuning strategy for speech recognition using large language models.",
      "published": "2024-12-25T00:16:22Z"
    },
    "metadata": {
      "arxiv_id": "2501.00039",
      "title": "Speech Recognition With LLMs Adapted to Disordered Speech Using Reinforcement Learning",
      "summary": "We introduce a large language model (LLM) capable of processing speech inputs and show that tuning it further with reinforcement learning on human preference (RLHF) enables it to adapt better to disordered speech than traditional fine-tuning. Our method replaces low-frequency text tokens in an LLM's vocabulary with audio tokens and enables the model to recognize speech by fine-tuning it on speech with transcripts. We then use RL with rewards based on syntactic and semantic accuracy measures generalizing the LLM further to recognize disordered speech. While the resulting LLM does not outperform existing systems for speech recognition, we find that tuning with reinforcement learning using custom rewards leads to substantially better performance than supervised fine-tuning of the language model, specifically when adapting to speech in a different setting. This presents a compelling alternative tuning strategy for speech recognition using large language models.",
      "authors": [
        "Chirag Nagpal",
        "Subhashini Venugopalan",
        "Jimmy Tobin",
        "Marilyn Ladewig",
        "Katherine Heller",
        "Katrin Tomanek"
      ],
      "published": "2024-12-25T00:16:22Z",
      "updated": "2024-12-25T00:16:22Z",
      "categories": [
        "eess.AS",
        "cs.CL",
        "cs.LG",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2501.00039v1",
      "landing_url": "https://arxiv.org/abs/2501.00039v1",
      "doi": "https://doi.org/10.48550/arXiv.2501.00039"
    },
    "queries": [
      {
        "anchor": "spoken language models",
        "search_term": "audio tokens"
      },
      {
        "anchor": "synchronous dialogue",
        "search_term": "audio tokens"
      },
      {
        "anchor": "turn-taking",
        "search_term": "audio tokens"
      }
    ]
  },
  {
    "arxiv_id": "2501.00063",
    "anchor": "spoken language models",
    "search_term": "data scarcity",
    "search_record": {
      "id": "http://arxiv.org/abs/2501.00063v1",
      "title": "\"Generative Models for Financial Time Series Data: Enhancing Signal-to-Noise Ratio and Addressing Data Scarcity in A-Share Market",
      "summary": "The financial industry is increasingly seeking robust methods to address the challenges posed by data scarcity and low signal-to-noise ratios, which limit the application of deep learning techniques in stock market analysis. This paper presents two innovative generative model-based approaches to synthesize stock data, specifically tailored for different scenarios within the A-share market in China. The first method, a sector-based synthesis approach, enhances the signal-to-noise ratio of stock data by classifying the characteristics of stocks from various sectors in China's A-share market. This method employs an Approximate Non-Local Total Variation algorithm to smooth the generated data, a bandpass filtering method based on Fourier Transform to eliminate noise, and Denoising Diffusion Implicit Models to accelerate sampling speed. The second method, a recursive stock data synthesis approach based on pattern recognition, is designed to synthesize data for stocks with short listing periods and limited comparable companies. It leverages pattern recognition techniques and Markov models to learn and generate variable-length stock sequences, while introducing a sub-time-level data augmentation method to alleviate data scarcity issues.We validate the effectiveness of these methods through extensive experiments on various datasets, including those from the main board, STAR Market, Growth Enterprise Market Board, Beijing Stock Exchange, NASDAQ, NYSE, and AMEX. The results demonstrate that our synthesized data not only improve the performance of predictive models but also enhance the signal-to-noise ratio of individual stock signals in price trading strategies. Furthermore, the introduction of sub-time-level data significantly improves the quality of synthesized data.",
      "published": "2024-12-29T09:35:23Z"
    },
    "metadata": {
      "arxiv_id": "2501.00063",
      "title": "\"Generative Models for Financial Time Series Data: Enhancing Signal-to-Noise Ratio and Addressing Data Scarcity in A-Share Market",
      "summary": "The financial industry is increasingly seeking robust methods to address the challenges posed by data scarcity and low signal-to-noise ratios, which limit the application of deep learning techniques in stock market analysis. This paper presents two innovative generative model-based approaches to synthesize stock data, specifically tailored for different scenarios within the A-share market in China. The first method, a sector-based synthesis approach, enhances the signal-to-noise ratio of stock data by classifying the characteristics of stocks from various sectors in China's A-share market. This method employs an Approximate Non-Local Total Variation algorithm to smooth the generated data, a bandpass filtering method based on Fourier Transform to eliminate noise, and Denoising Diffusion Implicit Models to accelerate sampling speed. The second method, a recursive stock data synthesis approach based on pattern recognition, is designed to synthesize data for stocks with short listing periods and limited comparable companies. It leverages pattern recognition techniques and Markov models to learn and generate variable-length stock sequences, while introducing a sub-time-level data augmentation method to alleviate data scarcity issues.We validate the effectiveness of these methods through extensive experiments on various datasets, including those from the main board, STAR Market, Growth Enterprise Market Board, Beijing Stock Exchange, NASDAQ, NYSE, and AMEX. The results demonstrate that our synthesized data not only improve the performance of predictive models but also enhance the signal-to-noise ratio of individual stock signals in price trading strategies. Furthermore, the introduction of sub-time-level data significantly improves the quality of synthesized data.",
      "authors": [
        "Guangming Che"
      ],
      "published": "2024-12-29T09:35:23Z",
      "updated": "2024-12-29T09:35:23Z",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2501.00063v1",
      "landing_url": "https://arxiv.org/abs/2501.00063v1",
      "doi": "https://doi.org/10.48550/arXiv.2501.00063"
    },
    "queries": [
      {
        "anchor": "spoken language models",
        "search_term": "data scarcity"
      },
      {
        "anchor": "synchronous dialogue",
        "search_term": "data scarcity"
      },
      {
        "anchor": "turn-taking",
        "search_term": "data scarcity"
      }
    ]
  },
  {
    "arxiv_id": "2501.00383",
    "anchor": "turn-taking",
    "search_term": "true full-duplex",
    "search_record": {
      "id": "http://arxiv.org/abs/2501.00383v2",
      "title": "Proactive Conversational Agents with Inner Thoughts",
      "summary": "One of the long-standing aspirations in conversational AI is to allow them to autonomously take initiatives in conversations, i.e., being proactive. This is especially challenging for multi-party conversations. Prior NLP research focused mainly on predicting the next speaker from contexts like preceding conversations. In this paper, we demonstrate the limitations of such methods and rethink what it means for AI to be proactive in multi-party, human-AI conversations. We propose that just like humans, rather than merely reacting to turn-taking cues, a proactive AI formulates its own inner thoughts during a conversation, and seeks the right moment to contribute. Through a formative study with 24 participants and inspiration from linguistics and cognitive psychology, we introduce the Inner Thoughts framework. Our framework equips AI with a continuous, covert train of thoughts in parallel to the overt communication process, which enables it to proactively engage by modeling its intrinsic motivation to express these thoughts. We instantiated this framework into two real-time systems: an AI playground web app and a chatbot. Through a technical evaluation and user studies with human participants, our framework significantly surpasses existing baselines on aspects like anthropomorphism, coherence, intelligence, and turn-taking appropriateness.",
      "published": "2024-12-31T10:41:56Z"
    },
    "metadata": {
      "arxiv_id": "2501.00383",
      "title": "Proactive Conversational Agents with Inner Thoughts",
      "summary": "One of the long-standing aspirations in conversational AI is to allow them to autonomously take initiatives in conversations, i.e., being proactive. This is especially challenging for multi-party conversations. Prior NLP research focused mainly on predicting the next speaker from contexts like preceding conversations. In this paper, we demonstrate the limitations of such methods and rethink what it means for AI to be proactive in multi-party, human-AI conversations. We propose that just like humans, rather than merely reacting to turn-taking cues, a proactive AI formulates its own inner thoughts during a conversation, and seeks the right moment to contribute. Through a formative study with 24 participants and inspiration from linguistics and cognitive psychology, we introduce the Inner Thoughts framework. Our framework equips AI with a continuous, covert train of thoughts in parallel to the overt communication process, which enables it to proactively engage by modeling its intrinsic motivation to express these thoughts. We instantiated this framework into two real-time systems: an AI playground web app and a chatbot. Through a technical evaluation and user studies with human participants, our framework significantly surpasses existing baselines on aspects like anthropomorphism, coherence, intelligence, and turn-taking appropriateness.",
      "authors": [
        "Xingyu Bruce Liu",
        "Shitao Fang",
        "Weiyan Shi",
        "Chien-Sheng Wu",
        "Takeo Igarashi",
        "Xiang Anthony Chen"
      ],
      "published": "2024-12-31T10:41:56Z",
      "updated": "2025-02-18T08:53:06Z",
      "categories": [
        "cs.HC",
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2501.00383v2",
      "landing_url": "https://arxiv.org/abs/2501.00383v2",
      "doi": "https://doi.org/10.48550/arXiv.2501.00383"
    },
    "queries": [
      {
        "anchor": "turn-taking",
        "search_term": "true full-duplex"
      },
      {
        "anchor": "turn-taking",
        "search_term": "pseudo full-duplex"
      },
      {
        "anchor": "turn-taking",
        "search_term": "engineered synchronization"
      },
      {
        "anchor": "turn-taking",
        "search_term": "learned synchronization"
      },
      {
        "anchor": "turn-taking",
        "search_term": "streaming architecture"
      },
      {
        "anchor": "turn-taking",
        "search_term": "real-time interaction"
      },
      {
        "anchor": "turn-taking",
        "search_term": "synchronous data"
      },
      {
        "anchor": "turn-taking",
        "search_term": "multichannel dialogue"
      },
      {
        "anchor": "turn-taking",
        "search_term": "behavioral arbitration"
      },
      {
        "anchor": "turn-taking",
        "search_term": "floor transfer offset"
      },
      {
        "anchor": "turn-taking",
        "search_term": "stop latency"
      },
      {
        "anchor": "turn-taking",
        "search_term": "interruption response delay"
      }
    ]
  },
  {
    "arxiv_id": "2501.00805",
    "anchor": "spoken language models",
    "search_term": "semantic coherence",
    "search_record": {
      "id": "http://arxiv.org/abs/2501.00805v1",
      "title": "SLIDE: Integrating Speech Language Model with LLM for Spontaneous Spoken Dialogue Generation",
      "summary": "Recently, ``textless\" speech language models (SLMs) based on speech units have made huge progress in generating naturalistic speech, including non-verbal vocalizations. However, the generated speech samples often lack semantic coherence. In this paper, we propose SLM and LLM Integration for spontaneous spoken Dialogue gEneration (SLIDE). Specifically, we first utilize an LLM to generate the textual content of spoken dialogue. Next, we convert the textual dialogues into phoneme sequences and use a two-tower transformer-based duration predictor to predict the duration of each phoneme. Finally, an SLM conditioned on the spoken phoneme sequences is used to vocalize the textual dialogue. Experimental results on the Fisher dataset demonstrate that our system can generate naturalistic spoken dialogue while maintaining high semantic coherence.",
      "published": "2025-01-01T11:11:07Z"
    },
    "metadata": {
      "arxiv_id": "2501.00805",
      "title": "SLIDE: Integrating Speech Language Model with LLM for Spontaneous Spoken Dialogue Generation",
      "summary": "Recently, ``textless\" speech language models (SLMs) based on speech units have made huge progress in generating naturalistic speech, including non-verbal vocalizations. However, the generated speech samples often lack semantic coherence. In this paper, we propose SLM and LLM Integration for spontaneous spoken Dialogue gEneration (SLIDE). Specifically, we first utilize an LLM to generate the textual content of spoken dialogue. Next, we convert the textual dialogues into phoneme sequences and use a two-tower transformer-based duration predictor to predict the duration of each phoneme. Finally, an SLM conditioned on the spoken phoneme sequences is used to vocalize the textual dialogue. Experimental results on the Fisher dataset demonstrate that our system can generate naturalistic spoken dialogue while maintaining high semantic coherence.",
      "authors": [
        "Haitian Lu",
        "Gaofeng Cheng",
        "Liuping Luo",
        "Leying Zhang",
        "Yanmin Qian",
        "Pengyuan Zhang"
      ],
      "published": "2025-01-01T11:11:07Z",
      "updated": "2025-01-01T11:11:07Z",
      "categories": [
        "eess.AS",
        "cs.CL",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2501.00805v1",
      "landing_url": "https://arxiv.org/abs/2501.00805v1",
      "doi": "https://doi.org/10.48550/arXiv.2501.00805"
    },
    "queries": [
      {
        "anchor": "spoken language models",
        "search_term": "semantic coherence"
      },
      {
        "anchor": "synchronous dialogue",
        "search_term": "semantic coherence"
      },
      {
        "anchor": "turn-taking",
        "search_term": "semantic coherence"
      }
    ]
  },
  {
    "arxiv_id": "2501.00823",
    "anchor": "spoken language models",
    "search_term": "modular architecture",
    "search_record": {
      "id": "http://arxiv.org/abs/2501.00823v2",
      "title": "Decoupling Knowledge and Reasoning in Transformers: A Modular Architecture with Generalized Cross-Attention",
      "summary": "Transformers have achieved remarkable success across diverse domains, but their monolithic architecture presents challenges in interpretability, adaptability, and scalability. This paper introduces a novel modular Transformer architecture that explicitly decouples knowledge and reasoning through a generalized cross-attention mechanism to a globally shared knowledge base with layer-specific transformations, specifically designed for effective knowledge retrieval. Critically, we provide a rigorous mathematical derivation demonstrating that the Feed-Forward Network (FFN) in a standard Transformer is a specialized case (a closure) of this generalized cross-attention, revealing its role in implicit knowledge retrieval and validating our design. This theoretical framework provides a new lens for understanding FFNs and lays the foundation for future research exploring enhanced interpretability, adaptability, and scalability, enabling richer interplay with external knowledge bases and other systems.",
      "published": "2025-01-01T12:55:57Z"
    },
    "metadata": {
      "arxiv_id": "2501.00823",
      "title": "Decoupling Knowledge and Reasoning in Transformers: A Modular Architecture with Generalized Cross-Attention",
      "summary": "Transformers have achieved remarkable success across diverse domains, but their monolithic architecture presents challenges in interpretability, adaptability, and scalability. This paper introduces a novel modular Transformer architecture that explicitly decouples knowledge and reasoning through a generalized cross-attention mechanism to a globally shared knowledge base with layer-specific transformations, specifically designed for effective knowledge retrieval. Critically, we provide a rigorous mathematical derivation demonstrating that the Feed-Forward Network (FFN) in a standard Transformer is a specialized case (a closure) of this generalized cross-attention, revealing its role in implicit knowledge retrieval and validating our design. This theoretical framework provides a new lens for understanding FFNs and lays the foundation for future research exploring enhanced interpretability, adaptability, and scalability, enabling richer interplay with external knowledge bases and other systems.",
      "authors": [
        "Zhenyu Guo",
        "Wenguang Chen"
      ],
      "published": "2025-01-01T12:55:57Z",
      "updated": "2025-01-06T14:26:41Z",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "pdf_url": "https://arxiv.org/pdf/2501.00823v2",
      "landing_url": "https://arxiv.org/abs/2501.00823v2",
      "doi": "https://doi.org/10.48550/arXiv.2501.00823"
    },
    "queries": [
      {
        "anchor": "spoken language models",
        "search_term": "modular architecture"
      },
      {
        "anchor": "synchronous dialogue",
        "search_term": "modular architecture"
      }
    ]
  },
  {
    "arxiv_id": "2501.01108",
    "anchor": "synchronous dialogue",
    "search_term": "neural codec",
    "search_record": {
      "id": "http://arxiv.org/abs/2501.01108v2",
      "title": "MuQ: Self-Supervised Music Representation Learning with Mel Residual Vector Quantization",
      "summary": "Recent years have witnessed the success of foundation models pre-trained with self-supervised learning (SSL) in various music informatics understanding tasks, including music tagging, instrument classification, key detection, and more. In this paper, we propose a self-supervised music representation learning model for music understanding. Distinguished from previous studies adopting random projection or existing neural codec, the proposed model, named MuQ, is trained to predict tokens generated by Mel Residual Vector Quantization (Mel-RVQ). Our Mel-RVQ utilizes residual linear projection structure for Mel spectrum quantization to enhance the stability and efficiency of target extraction and lead to better performance. Experiments in a large variety of downstream tasks demonstrate that MuQ outperforms previous self-supervised music representation models with only 0.9K hours of open-source pre-training data. Scaling up the data to over 160K hours and adopting iterative training consistently improve the model performance. To further validate the strength of our model, we present MuQ-MuLan, a joint music-text embedding model based on contrastive learning, which achieves state-of-the-art performance in the zero-shot music tagging task on the MagnaTagATune dataset. Code and checkpoints are open source in https://github.com/tencent-ailab/MuQ.",
      "published": "2025-01-02T07:08:29Z"
    },
    "metadata": {
      "arxiv_id": "2501.01108",
      "title": "MuQ: Self-Supervised Music Representation Learning with Mel Residual Vector Quantization",
      "summary": "Recent years have witnessed the success of foundation models pre-trained with self-supervised learning (SSL) in various music informatics understanding tasks, including music tagging, instrument classification, key detection, and more. In this paper, we propose a self-supervised music representation learning model for music understanding. Distinguished from previous studies adopting random projection or existing neural codec, the proposed model, named MuQ, is trained to predict tokens generated by Mel Residual Vector Quantization (Mel-RVQ). Our Mel-RVQ utilizes residual linear projection structure for Mel spectrum quantization to enhance the stability and efficiency of target extraction and lead to better performance. Experiments in a large variety of downstream tasks demonstrate that MuQ outperforms previous self-supervised music representation models with only 0.9K hours of open-source pre-training data. Scaling up the data to over 160K hours and adopting iterative training consistently improve the model performance. To further validate the strength of our model, we present MuQ-MuLan, a joint music-text embedding model based on contrastive learning, which achieves state-of-the-art performance in the zero-shot music tagging task on the MagnaTagATune dataset. Code and checkpoints are open source in https://github.com/tencent-ailab/MuQ.",
      "authors": [
        "Haina Zhu",
        "Yizhi Zhou",
        "Hangting Chen",
        "Jianwei Yu",
        "Ziyang Ma",
        "Rongzhi Gu",
        "Yi Luo",
        "Wei Tan",
        "Xie Chen"
      ],
      "published": "2025-01-02T07:08:29Z",
      "updated": "2025-01-03T08:35:34Z",
      "categories": [
        "cs.SD",
        "cs.AI",
        "cs.CL",
        "cs.LG",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2501.01108v2",
      "landing_url": "https://arxiv.org/abs/2501.01108v2",
      "doi": "https://doi.org/10.48550/arXiv.2501.01108"
    },
    "queries": [
      {
        "anchor": "synchronous dialogue",
        "search_term": "neural codec"
      }
    ]
  },
  {
    "arxiv_id": "2501.01231",
    "anchor": "spoken language models",
    "search_term": "neural codec",
    "search_record": {
      "id": "http://arxiv.org/abs/2501.01231v1",
      "title": "Exploiting Latent Properties to Optimize Neural Codecs",
      "summary": "End-to-end image and video codecs are becoming increasingly competitive, compared to traditional compression techniques that have been developed through decades of manual engineering efforts. These trainable codecs have many advantages over traditional techniques, such as their straightforward adaptation to perceptual distortion metrics and high performance in specific fields thanks to their learning ability. However, current state-of-the-art neural codecs do not fully exploit the benefits of vector quantization and the existence of the entropy gradient in decoding devices. In this paper, we propose to leverage these two properties (vector quantization and entropy gradient) to improve the performance of off-the-shelf codecs. Firstly, we demonstrate that using non-uniform scalar quantization cannot improve performance over uniform quantization. We thus suggest using predefined optimal uniform vector quantization to improve performance. Secondly, we show that the entropy gradient, available at the decoder, is correlated with the reconstruction error gradient, which is not available at the decoder. We therefore use the former as a proxy to enhance compression performance. Our experimental results show that these approaches save between 1 to 3% of the rate for the same quality across various pretrained methods. In addition, the entropy gradient based solution improves traditional codec performance significantly as well.",
      "published": "2025-01-02T12:45:31Z"
    },
    "metadata": {
      "arxiv_id": "2501.01231",
      "title": "Exploiting Latent Properties to Optimize Neural Codecs",
      "summary": "End-to-end image and video codecs are becoming increasingly competitive, compared to traditional compression techniques that have been developed through decades of manual engineering efforts. These trainable codecs have many advantages over traditional techniques, such as their straightforward adaptation to perceptual distortion metrics and high performance in specific fields thanks to their learning ability. However, current state-of-the-art neural codecs do not fully exploit the benefits of vector quantization and the existence of the entropy gradient in decoding devices. In this paper, we propose to leverage these two properties (vector quantization and entropy gradient) to improve the performance of off-the-shelf codecs. Firstly, we demonstrate that using non-uniform scalar quantization cannot improve performance over uniform quantization. We thus suggest using predefined optimal uniform vector quantization to improve performance. Secondly, we show that the entropy gradient, available at the decoder, is correlated with the reconstruction error gradient, which is not available at the decoder. We therefore use the former as a proxy to enhance compression performance. Our experimental results show that these approaches save between 1 to 3% of the rate for the same quality across various pretrained methods. In addition, the entropy gradient based solution improves traditional codec performance significantly as well.",
      "authors": [
        "Muhammet Balcilar",
        "Bharath Bhushan Damodaran",
        "Karam Naser",
        "Franck Galpin",
        "Pierre Hellier"
      ],
      "published": "2025-01-02T12:45:31Z",
      "updated": "2025-01-02T12:45:31Z",
      "categories": [
        "cs.CV",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2501.01231v1",
      "landing_url": "https://arxiv.org/abs/2501.01231v1",
      "doi": "https://doi.org/10.1109/TIP.2024.352281"
    },
    "queries": [
      {
        "anchor": "spoken language models",
        "search_term": "neural codec"
      },
      {
        "anchor": "synchronous dialogue",
        "search_term": "neural codec"
      },
      {
        "anchor": "turn-taking",
        "search_term": "neural codec"
      }
    ]
  },
  {
    "arxiv_id": "2501.03218",
    "anchor": "spoken language models",
    "search_term": "real-time interaction",
    "search_record": {
      "id": "http://arxiv.org/abs/2501.03218v1",
      "title": "Dispider: Enabling Video LLMs with Active Real-Time Interaction via Disentangled Perception, Decision, and Reaction",
      "summary": "Active Real-time interaction with video LLMs introduces a new paradigm for human-computer interaction, where the model not only understands user intent but also responds while continuously processing streaming video on the fly. Unlike offline video LLMs, which analyze the entire video before answering questions, active real-time interaction requires three capabilities: 1) Perception: real-time video monitoring and interaction capturing. 2) Decision: raising proactive interaction in proper situations, 3) Reaction: continuous interaction with users. However, inherent conflicts exist among the desired capabilities. The Decision and Reaction require a contrary Perception scale and grain, and the autoregressive decoding blocks the real-time Perception and Decision during the Reaction. To unify the conflicted capabilities within a harmonious system, we present Dispider, a system that disentangles Perception, Decision, and Reaction. Dispider features a lightweight proactive streaming video processing module that tracks the video stream and identifies optimal moments for interaction. Once the interaction is triggered, an asynchronous interaction module provides detailed responses, while the processing module continues to monitor the video in the meantime. Our disentangled and asynchronous design ensures timely, contextually accurate, and computationally efficient responses, making Dispider ideal for active real-time interaction for long-duration video streams. Experiments show that Dispider not only maintains strong performance in conventional video QA tasks, but also significantly surpasses previous online models in streaming scenario responses, thereby validating the effectiveness of our architecture. The code and model are released at \\url{https://github.com/Mark12Ding/Dispider}.",
      "published": "2025-01-06T18:55:10Z"
    },
    "metadata": {
      "arxiv_id": "2501.03218",
      "title": "Dispider: Enabling Video LLMs with Active Real-Time Interaction via Disentangled Perception, Decision, and Reaction",
      "summary": "Active Real-time interaction with video LLMs introduces a new paradigm for human-computer interaction, where the model not only understands user intent but also responds while continuously processing streaming video on the fly. Unlike offline video LLMs, which analyze the entire video before answering questions, active real-time interaction requires three capabilities: 1) Perception: real-time video monitoring and interaction capturing. 2) Decision: raising proactive interaction in proper situations, 3) Reaction: continuous interaction with users. However, inherent conflicts exist among the desired capabilities. The Decision and Reaction require a contrary Perception scale and grain, and the autoregressive decoding blocks the real-time Perception and Decision during the Reaction. To unify the conflicted capabilities within a harmonious system, we present Dispider, a system that disentangles Perception, Decision, and Reaction. Dispider features a lightweight proactive streaming video processing module that tracks the video stream and identifies optimal moments for interaction. Once the interaction is triggered, an asynchronous interaction module provides detailed responses, while the processing module continues to monitor the video in the meantime. Our disentangled and asynchronous design ensures timely, contextually accurate, and computationally efficient responses, making Dispider ideal for active real-time interaction for long-duration video streams. Experiments show that Dispider not only maintains strong performance in conventional video QA tasks, but also significantly surpasses previous online models in streaming scenario responses, thereby validating the effectiveness of our architecture. The code and model are released at \\url{https://github.com/Mark12Ding/Dispider}.",
      "authors": [
        "Rui Qian",
        "Shuangrui Ding",
        "Xiaoyi Dong",
        "Pan Zhang",
        "Yuhang Zang",
        "Yuhang Cao",
        "Dahua Lin",
        "Jiaqi Wang"
      ],
      "published": "2025-01-06T18:55:10Z",
      "updated": "2025-01-06T18:55:10Z",
      "categories": [
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2501.03218v1",
      "landing_url": "https://arxiv.org/abs/2501.03218v1",
      "doi": "https://doi.org/10.48550/arXiv.2501.03218"
    },
    "queries": [
      {
        "anchor": "spoken language models",
        "search_term": "real-time interaction"
      },
      {
        "anchor": "synchronous dialogue",
        "search_term": "real-time interaction"
      },
      {
        "anchor": "turn-taking",
        "search_term": "real-time interaction"
      }
    ]
  },
  {
    "arxiv_id": "2501.04379",
    "anchor": "spoken language models",
    "search_term": "discrete tokens",
    "search_record": {
      "id": "http://arxiv.org/abs/2501.04379v1",
      "title": "Phone-purity Guided Discrete Tokens for Dysarthric Speech Recognition",
      "summary": "Discrete tokens extracted provide efficient and domain adaptable speech features. Their application to disordered speech that exhibits articulation imprecision and large mismatch against normal voice remains unexplored. To improve their phonetic discrimination that is weakened during unsupervised K-means or vector quantization of continuous features, this paper proposes novel phone-purity guided (PPG) discrete tokens for dysarthric speech recognition. Phonetic label supervision is used to regularize maximum likelihood and reconstruction error costs used in standard K-means and VAE-VQ based discrete token extraction. Experiments conducted on the UASpeech corpus suggest that the proposed PPG discrete token features extracted from HuBERT consistently outperform hybrid TDNN and End-to-End (E2E) Conformer systems using non-PPG based K-means or VAE-VQ tokens across varying codebook sizes by statistically significant word error rate (WER) reductions up to 0.99\\% and 1.77\\% absolute (3.21\\% and 4.82\\% relative) respectively on the UASpeech test set of 16 dysarthric speakers. The lowest WER of 23.25\\% was obtained by combining systems using different token features. Consistent improvements on the phone purity metric were also achieved. T-SNE visualization further demonstrates sharper decision boundaries were produced between K-means/VAE-VQ clusters after introducing phone-purity guidance.",
      "published": "2025-01-08T09:45:14Z"
    },
    "metadata": {
      "arxiv_id": "2501.04379",
      "title": "Phone-purity Guided Discrete Tokens for Dysarthric Speech Recognition",
      "summary": "Discrete tokens extracted provide efficient and domain adaptable speech features. Their application to disordered speech that exhibits articulation imprecision and large mismatch against normal voice remains unexplored. To improve their phonetic discrimination that is weakened during unsupervised K-means or vector quantization of continuous features, this paper proposes novel phone-purity guided (PPG) discrete tokens for dysarthric speech recognition. Phonetic label supervision is used to regularize maximum likelihood and reconstruction error costs used in standard K-means and VAE-VQ based discrete token extraction. Experiments conducted on the UASpeech corpus suggest that the proposed PPG discrete token features extracted from HuBERT consistently outperform hybrid TDNN and End-to-End (E2E) Conformer systems using non-PPG based K-means or VAE-VQ tokens across varying codebook sizes by statistically significant word error rate (WER) reductions up to 0.99\\% and 1.77\\% absolute (3.21\\% and 4.82\\% relative) respectively on the UASpeech test set of 16 dysarthric speakers. The lowest WER of 23.25\\% was obtained by combining systems using different token features. Consistent improvements on the phone purity metric were also achieved. T-SNE visualization further demonstrates sharper decision boundaries were produced between K-means/VAE-VQ clusters after introducing phone-purity guidance.",
      "authors": [
        "Huimeng Wang",
        "Xurong Xie",
        "Mengzhe Geng",
        "Shujie Hu",
        "Haoning Xu",
        "Youjun Chen",
        "Zhaoqing Li",
        "Jiajun Deng",
        "Xunying Liu"
      ],
      "published": "2025-01-08T09:45:14Z",
      "updated": "2025-01-08T09:45:14Z",
      "categories": [
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2501.04379v1",
      "landing_url": "https://arxiv.org/abs/2501.04379v1",
      "doi": "https://doi.org/10.48550/arXiv.2501.04379"
    },
    "queries": [
      {
        "anchor": "spoken language models",
        "search_term": "discrete tokens"
      },
      {
        "anchor": "synchronous dialogue",
        "search_term": "discrete tokens"
      },
      {
        "anchor": "turn-taking",
        "search_term": "discrete tokens"
      }
    ]
  },
  {
    "arxiv_id": "2501.04877",
    "anchor": "turn-taking",
    "search_term": "true full-duplex",
    "search_record": {
      "id": "http://arxiv.org/abs/2501.04877v1",
      "title": "Real-Time Textless Dialogue Generation",
      "summary": "Recent advancements in large language models (LLMs) have led to significant progress in text-based dialogue systems. These systems can now generate high-quality responses that are accurate and coherent across a wide range of topics and tasks. However, spoken dialogue systems still lag behind in terms of naturalness. They tend to produce robotic interactions, with issues such as slow response times, overly generic or cautious replies, and a lack of natural rhythm and fluid turn-taking. This shortcoming is largely due to the over-reliance on the traditional cascaded design, which involve separate, sequential components, as well as the use of text as an intermediate representation. This paper propose a real-time, textless spoken dialogue generation model (RTTL-DG) that aims to overcome these challenges. Our system enables fluid turn-taking and generates responses with minimal delay by processing streaming spoken conversation directly. Additionally, our model incorporates backchannels, filters, laughter, and other paralinguistic signals, which are often absent in cascaded dialogue systems, to create more natural and human-like interactions. The implementations and generated samples are available in our repository: https://github.com/mailong25/rts2s-dg",
      "published": "2025-01-08T23:21:43Z"
    },
    "metadata": {
      "arxiv_id": "2501.04877",
      "title": "Real-Time Textless Dialogue Generation",
      "summary": "Recent advancements in large language models (LLMs) have led to significant progress in text-based dialogue systems. These systems can now generate high-quality responses that are accurate and coherent across a wide range of topics and tasks. However, spoken dialogue systems still lag behind in terms of naturalness. They tend to produce robotic interactions, with issues such as slow response times, overly generic or cautious replies, and a lack of natural rhythm and fluid turn-taking. This shortcoming is largely due to the over-reliance on the traditional cascaded design, which involve separate, sequential components, as well as the use of text as an intermediate representation. This paper propose a real-time, textless spoken dialogue generation model (RTTL-DG) that aims to overcome these challenges. Our system enables fluid turn-taking and generates responses with minimal delay by processing streaming spoken conversation directly. Additionally, our model incorporates backchannels, filters, laughter, and other paralinguistic signals, which are often absent in cascaded dialogue systems, to create more natural and human-like interactions. The implementations and generated samples are available in our repository: https://github.com/mailong25/rts2s-dg",
      "authors": [
        "Long Mai",
        "Julie Carson-Berndsen"
      ],
      "published": "2025-01-08T23:21:43Z",
      "updated": "2025-01-08T23:21:43Z",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2501.04877v1",
      "landing_url": "https://arxiv.org/abs/2501.04877v1",
      "doi": "https://doi.org/10.48550/arXiv.2501.04877"
    },
    "queries": [
      {
        "anchor": "turn-taking",
        "search_term": "true full-duplex"
      },
      {
        "anchor": "turn-taking",
        "search_term": "pseudo full-duplex"
      },
      {
        "anchor": "turn-taking",
        "search_term": "engineered synchronization"
      },
      {
        "anchor": "turn-taking",
        "search_term": "learned synchronization"
      },
      {
        "anchor": "turn-taking",
        "search_term": "modular architecture"
      },
      {
        "anchor": "turn-taking",
        "search_term": "streaming architecture"
      },
      {
        "anchor": "turn-taking",
        "search_term": "real-time interaction"
      },
      {
        "anchor": "turn-taking",
        "search_term": "synchronous data"
      },
      {
        "anchor": "turn-taking",
        "search_term": "multichannel dialogue"
      },
      {
        "anchor": "turn-taking",
        "search_term": "behavioral arbitration"
      },
      {
        "anchor": "turn-taking",
        "search_term": "semantic coherence"
      },
      {
        "anchor": "turn-taking",
        "search_term": "floor transfer offset"
      },
      {
        "anchor": "turn-taking",
        "search_term": "stop latency"
      },
      {
        "anchor": "turn-taking",
        "search_term": "interruption response delay"
      }
    ]
  },
  {
    "arxiv_id": "2501.04962",
    "anchor": "spoken language models",
    "search_term": "true full-duplex",
    "search_record": {
      "id": "http://arxiv.org/abs/2501.04962v4",
      "title": "VoxEval: Benchmarking the Knowledge Understanding Capabilities of End-to-End Spoken Language Models",
      "summary": "With the rising need for speech-based interaction models, end-to-end Spoken Language Models (SLMs) have emerged as a promising solution. While these models require comprehensive world knowledge for meaningful and reliable human interactions, existing question-answering (QA) benchmarks fall short in evaluating SLMs' knowledge understanding due to their inability to support end-to-end speech evaluation and account for varied input audio conditions. To address these limitations, we present VoxEval, a novel SpeechQA benchmark that assesses SLMs' knowledge understanding through pure speech interactions. Our benchmark 1) uniquely maintains speech format for both inputs and outputs, 2) evaluates model robustness across diverse input audio conditions, and 3) pioneers the assessment of complex tasks like mathematical reasoning in spoken format. Systematic evaluation demonstrates that VoxEval presents significant challenges to current SLMs, revealing their sensitivity to varying audio conditions and highlighting the need to enhance reasoning capabilities in future development. We hope this benchmark could guide the advancement of more sophisticated and reliable SLMs. VoxEval dataset is available at: https://github.com/dreamtheater123/VoxEval",
      "published": "2025-01-09T04:30:12Z"
    },
    "metadata": {
      "arxiv_id": "2501.04962",
      "title": "VoxEval: Benchmarking the Knowledge Understanding Capabilities of End-to-End Spoken Language Models",
      "summary": "With the rising need for speech-based interaction models, end-to-end Spoken Language Models (SLMs) have emerged as a promising solution. While these models require comprehensive world knowledge for meaningful and reliable human interactions, existing question-answering (QA) benchmarks fall short in evaluating SLMs' knowledge understanding due to their inability to support end-to-end speech evaluation and account for varied input audio conditions. To address these limitations, we present VoxEval, a novel SpeechQA benchmark that assesses SLMs' knowledge understanding through pure speech interactions. Our benchmark 1) uniquely maintains speech format for both inputs and outputs, 2) evaluates model robustness across diverse input audio conditions, and 3) pioneers the assessment of complex tasks like mathematical reasoning in spoken format. Systematic evaluation demonstrates that VoxEval presents significant challenges to current SLMs, revealing their sensitivity to varying audio conditions and highlighting the need to enhance reasoning capabilities in future development. We hope this benchmark could guide the advancement of more sophisticated and reliable SLMs. VoxEval dataset is available at: https://github.com/dreamtheater123/VoxEval",
      "authors": [
        "Wenqian Cui",
        "Xiaoqi Jiao",
        "Ziqiao Meng",
        "Irwin King"
      ],
      "published": "2025-01-09T04:30:12Z",
      "updated": "2025-05-27T16:14:30Z",
      "categories": [
        "cs.CL",
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2501.04962v4",
      "landing_url": "https://arxiv.org/abs/2501.04962v4",
      "doi": "https://doi.org/10.48550/arXiv.2501.04962"
    },
    "queries": [
      {
        "anchor": "spoken language models",
        "search_term": "true full-duplex"
      },
      {
        "anchor": "spoken language models",
        "search_term": "pseudo full-duplex"
      },
      {
        "anchor": "spoken language models",
        "search_term": "half duplex"
      },
      {
        "anchor": "spoken language models",
        "search_term": "engineered synchronization"
      },
      {
        "anchor": "spoken language models",
        "search_term": "learned synchronization"
      },
      {
        "anchor": "spoken language models",
        "search_term": "modular architecture"
      },
      {
        "anchor": "spoken language models",
        "search_term": "streaming architecture"
      },
      {
        "anchor": "spoken language models",
        "search_term": "real-time interaction"
      },
      {
        "anchor": "spoken language models",
        "search_term": "low latency"
      },
      {
        "anchor": "spoken language models",
        "search_term": "synchronous data"
      },
      {
        "anchor": "spoken language models",
        "search_term": "data scarcity"
      },
      {
        "anchor": "spoken language models",
        "search_term": "multichannel dialogue"
      },
      {
        "anchor": "spoken language models",
        "search_term": "temporal dynamics"
      },
      {
        "anchor": "spoken language models",
        "search_term": "behavioral arbitration"
      },
      {
        "anchor": "spoken language models",
        "search_term": "semantic coherence"
      },
      {
        "anchor": "spoken language models",
        "search_term": "neural codec"
      },
      {
        "anchor": "spoken language models",
        "search_term": "discrete tokens"
      },
      {
        "anchor": "spoken language models",
        "search_term": "audio tokens"
      },
      {
        "anchor": "spoken language models",
        "search_term": "floor transfer offset"
      },
      {
        "anchor": "spoken language models",
        "search_term": "stop latency"
      },
      {
        "anchor": "spoken language models",
        "search_term": "interruption response delay"
      }
    ]
  },
  {
    "arxiv_id": "2501.05345",
    "anchor": "spoken language models",
    "search_term": "synchronous data",
    "search_record": {
      "id": "http://arxiv.org/abs/2501.05345v1",
      "title": "Video-Conferencing Beyond Screen-Sharing and Thumbnail Webcam Videos: Gesture-Aware Augmented Reality Video for Data-Rich Remote Presentations",
      "summary": "Synchronous data-rich conversations are commonplace within enterprise organizations, taking place at varying degrees of formality between stakeholders at different levels of data literacy. In these conversations, representations of data are used to analyze past decisions, inform future course of action, as well as persuade customers, investors, and executives. However, it is difficult to conduct these conversations between remote stakeholders due to poor support for presenting data when video-conferencing, resulting in disappointing audience experiences. In this position statement, I reflect on our recent work incorporating multimodal interaction and augmented reality video, suggesting that video-conferencing does not need to be limited to screen-sharing and relegating a speaker's video to a separate thumbnail view. I also comment on future research directions and collaboration opportunities.",
      "published": "2025-01-09T16:21:12Z"
    },
    "metadata": {
      "arxiv_id": "2501.05345",
      "title": "Video-Conferencing Beyond Screen-Sharing and Thumbnail Webcam Videos: Gesture-Aware Augmented Reality Video for Data-Rich Remote Presentations",
      "summary": "Synchronous data-rich conversations are commonplace within enterprise organizations, taking place at varying degrees of formality between stakeholders at different levels of data literacy. In these conversations, representations of data are used to analyze past decisions, inform future course of action, as well as persuade customers, investors, and executives. However, it is difficult to conduct these conversations between remote stakeholders due to poor support for presenting data when video-conferencing, resulting in disappointing audience experiences. In this position statement, I reflect on our recent work incorporating multimodal interaction and augmented reality video, suggesting that video-conferencing does not need to be limited to screen-sharing and relegating a speaker's video to a separate thumbnail view. I also comment on future research directions and collaboration opportunities.",
      "authors": [
        "Matthew Brehmer"
      ],
      "published": "2025-01-09T16:21:12Z",
      "updated": "2025-01-09T16:21:12Z",
      "categories": [
        "cs.HC"
      ],
      "pdf_url": "https://arxiv.org/pdf/2501.05345v1",
      "landing_url": "https://arxiv.org/abs/2501.05345v1",
      "doi": "https://doi.org/10.48550/arXiv.2501.05345"
    },
    "queries": [
      {
        "anchor": "spoken language models",
        "search_term": "synchronous data"
      },
      {
        "anchor": "synchronous dialogue",
        "search_term": "synchronous data"
      }
    ]
  },
  {
    "arxiv_id": "2501.05787",
    "anchor": "spoken language models",
    "search_term": "true full-duplex",
    "search_record": {
      "id": "http://arxiv.org/abs/2501.05787v1",
      "title": "MARS6: A Small and Robust Hierarchical-Codec Text-to-Speech Model",
      "summary": "Codec-based text-to-speech (TTS) models have shown impressive quality with zero-shot voice cloning abilities. However, they often struggle with more expressive references or complex text inputs. We present MARS6, a robust encoder-decoder transformer for rapid, expressive TTS. MARS6 is built on recent improvements in spoken language modelling. Utilizing a hierarchical setup for its decoder, new speech tokens are processed at a rate of only 12 Hz, enabling efficient modelling of long-form text while retaining reconstruction quality. We combine several recent training and inference techniques to reduce repetitive generation and improve output stability and quality. This enables the 70M-parameter MARS6 to achieve similar performance to models many times larger. We show this in objective and subjective evaluations, comparing TTS output quality and reference speaker cloning ability. Project page: https://camb-ai.github.io/mars6-turbo/",
      "published": "2025-01-10T08:41:42Z"
    },
    "metadata": {
      "arxiv_id": "2501.05787",
      "title": "MARS6: A Small and Robust Hierarchical-Codec Text-to-Speech Model",
      "summary": "Codec-based text-to-speech (TTS) models have shown impressive quality with zero-shot voice cloning abilities. However, they often struggle with more expressive references or complex text inputs. We present MARS6, a robust encoder-decoder transformer for rapid, expressive TTS. MARS6 is built on recent improvements in spoken language modelling. Utilizing a hierarchical setup for its decoder, new speech tokens are processed at a rate of only 12 Hz, enabling efficient modelling of long-form text while retaining reconstruction quality. We combine several recent training and inference techniques to reduce repetitive generation and improve output stability and quality. This enables the 70M-parameter MARS6 to achieve similar performance to models many times larger. We show this in objective and subjective evaluations, comparing TTS output quality and reference speaker cloning ability. Project page: https://camb-ai.github.io/mars6-turbo/",
      "authors": [
        "Matthew Baas",
        "Pieter Scholtz",
        "Arnav Mehta",
        "Elliott Dyson",
        "Akshat Prakash",
        "Herman Kamper"
      ],
      "published": "2025-01-10T08:41:42Z",
      "updated": "2025-01-10T08:41:42Z",
      "categories": [
        "eess.AS",
        "cs.CL"
      ],
      "pdf_url": "https://arxiv.org/pdf/2501.05787v1",
      "landing_url": "https://arxiv.org/abs/2501.05787v1",
      "doi": "https://doi.org/10.48550/arXiv.2501.05787"
    },
    "queries": [
      {
        "anchor": "spoken language models",
        "search_term": "true full-duplex"
      },
      {
        "anchor": "spoken language models",
        "search_term": "pseudo full-duplex"
      },
      {
        "anchor": "spoken language models",
        "search_term": "engineered synchronization"
      },
      {
        "anchor": "spoken language models",
        "search_term": "learned synchronization"
      },
      {
        "anchor": "spoken language models",
        "search_term": "streaming architecture"
      },
      {
        "anchor": "spoken language models",
        "search_term": "synchronous data"
      },
      {
        "anchor": "spoken language models",
        "search_term": "multichannel dialogue"
      },
      {
        "anchor": "spoken language models",
        "search_term": "behavioral arbitration"
      },
      {
        "anchor": "spoken language models",
        "search_term": "floor transfer offset"
      },
      {
        "anchor": "spoken language models",
        "search_term": "stop latency"
      },
      {
        "anchor": "spoken language models",
        "search_term": "interruption response delay"
      }
    ]
  },
  {
    "arxiv_id": "2501.06514",
    "anchor": "full-duplex",
    "search_term": "neural codec",
    "search_record": {
      "id": "http://arxiv.org/abs/2501.06514v1",
      "title": "Neural Codec Source Tracing: Toward Comprehensive Attribution in Open-Set Condition",
      "summary": "Current research in audio deepfake detection is gradually transitioning from binary classification to multi-class tasks, referred as audio deepfake source tracing task. However, existing studies on source tracing consider only closed-set scenarios and have not considered the challenges posed by open-set conditions. In this paper, we define the Neural Codec Source Tracing (NCST) task, which is capable of performing open-set neural codec classification and interpretable ALM detection. Specifically, we constructed the ST-Codecfake dataset for the NCST task, which includes bilingual audio samples generated by 11 state-of-the-art neural codec methods and ALM-based out-ofdistribution (OOD) test samples. Furthermore, we establish a comprehensive source tracing benchmark to assess NCST models in open-set conditions. The experimental results reveal that although the NCST models perform well in in-distribution (ID) classification and OOD detection, they lack robustness in classifying unseen real audio. The ST-codecfake dataset and code are available.",
      "published": "2025-01-11T11:15:58Z"
    },
    "metadata": {
      "arxiv_id": "2501.06514",
      "title": "Neural Codec Source Tracing: Toward Comprehensive Attribution in Open-Set Condition",
      "summary": "Current research in audio deepfake detection is gradually transitioning from binary classification to multi-class tasks, referred as audio deepfake source tracing task. However, existing studies on source tracing consider only closed-set scenarios and have not considered the challenges posed by open-set conditions. In this paper, we define the Neural Codec Source Tracing (NCST) task, which is capable of performing open-set neural codec classification and interpretable ALM detection. Specifically, we constructed the ST-Codecfake dataset for the NCST task, which includes bilingual audio samples generated by 11 state-of-the-art neural codec methods and ALM-based out-ofdistribution (OOD) test samples. Furthermore, we establish a comprehensive source tracing benchmark to assess NCST models in open-set conditions. The experimental results reveal that although the NCST models perform well in in-distribution (ID) classification and OOD detection, they lack robustness in classifying unseen real audio. The ST-codecfake dataset and code are available.",
      "authors": [
        "Yuankun Xie",
        "Xiaopeng Wang",
        "Zhiyong Wang",
        "Ruibo Fu",
        "Zhengqi Wen",
        "Songjun Cao",
        "Long Ma",
        "Chenxing Li",
        "Haonnan Cheng",
        "Long Ye"
      ],
      "published": "2025-01-11T11:15:58Z",
      "updated": "2025-01-11T11:15:58Z",
      "categories": [
        "cs.SD",
        "cs.AI",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2501.06514v1",
      "landing_url": "https://arxiv.org/abs/2501.06514v1",
      "doi": "https://doi.org/10.48550/arXiv.2501.06514"
    },
    "queries": [
      {
        "anchor": "full-duplex",
        "search_term": "neural codec"
      },
      {
        "anchor": "spoken language models",
        "search_term": "neural codec"
      },
      {
        "anchor": "synchronous dialogue",
        "search_term": "neural codec"
      },
      {
        "anchor": "turn-taking",
        "search_term": "neural codec"
      }
    ]
  },
  {
    "arxiv_id": "2501.08102",
    "anchor": "spoken language models",
    "search_term": "semantic coherence",
    "search_record": {
      "id": "http://arxiv.org/abs/2501.08102v6",
      "title": "Consistency of Responses and Continuations Generated by Large Language Models on Social Media",
      "summary": "Large Language Models (LLMs) demonstrate remarkable capabilities in text generation, yet their emotional consistency and semantic coherence in social media contexts remain insufficiently understood. This study investigates how LLMs handle emotional content and maintain semantic relationships through continuation and response tasks using three open-source models: Gemma, Llama3 and Llama3.3 and one commercial Model:Claude. By analyzing climate change discussions from Twitter and Reddit, we examine emotional transitions, intensity patterns, and semantic consistency between human-authored and LLM-generated content. Our findings reveal that while both models maintain high semantic coherence, they exhibit distinct emotional patterns: these models show a strong tendency to moderate negative emotions. When the input text carries negative emotions such as anger, disgust, fear, or sadness, LLM tends to generate content with more neutral emotions, or even convert them into positive emotions such as joy or surprise. At the same time, we compared the LLM-generated content with human-authored content. The four models systematically generated responses with reduced emotional intensity and showed a preference for neutral rational emotions in the response task. In addition, these models all maintained a high semantic similarity with the original text, although their performance in the continuation task and the response task was different. These findings provide deep insights into the emotion and semantic processing capabilities of LLM, which are of great significance for its deployment in social media environments and human-computer interaction design.",
      "published": "2025-01-14T13:19:47Z"
    },
    "metadata": {
      "arxiv_id": "2501.08102",
      "title": "Consistency of Responses and Continuations Generated by Large Language Models on Social Media",
      "summary": "Large Language Models (LLMs) demonstrate remarkable capabilities in text generation, yet their emotional consistency and semantic coherence in social media contexts remain insufficiently understood. This study investigates how LLMs handle emotional content and maintain semantic relationships through continuation and response tasks using three open-source models: Gemma, Llama3 and Llama3.3 and one commercial Model:Claude. By analyzing climate change discussions from Twitter and Reddit, we examine emotional transitions, intensity patterns, and semantic consistency between human-authored and LLM-generated content. Our findings reveal that while both models maintain high semantic coherence, they exhibit distinct emotional patterns: these models show a strong tendency to moderate negative emotions. When the input text carries negative emotions such as anger, disgust, fear, or sadness, LLM tends to generate content with more neutral emotions, or even convert them into positive emotions such as joy or surprise. At the same time, we compared the LLM-generated content with human-authored content. The four models systematically generated responses with reduced emotional intensity and showed a preference for neutral rational emotions in the response task. In addition, these models all maintained a high semantic similarity with the original text, although their performance in the continuation task and the response task was different. These findings provide deep insights into the emotion and semantic processing capabilities of LLM, which are of great significance for its deployment in social media environments and human-computer interaction design.",
      "authors": [
        "Wentao Xu",
        "Wenlu Fan",
        "Yuqi Zhu",
        "Bin Wang"
      ],
      "published": "2025-01-14T13:19:47Z",
      "updated": "2025-11-04T02:32:20Z",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.HC"
      ],
      "pdf_url": "https://arxiv.org/pdf/2501.08102v6",
      "landing_url": "https://arxiv.org/abs/2501.08102v6",
      "doi": "https://doi.org/10.48550/arXiv.2501.08102"
    },
    "queries": [
      {
        "anchor": "spoken language models",
        "search_term": "semantic coherence"
      },
      {
        "anchor": "synchronous dialogue",
        "search_term": "semantic coherence"
      }
    ]
  },
  {
    "arxiv_id": "2501.08848",
    "anchor": "spoken language models",
    "search_term": "modular architecture",
    "search_record": {
      "id": "http://arxiv.org/abs/2501.08848v2",
      "title": "RouteNet-Gauss: Hardware-Enhanced Network Modeling with Machine Learning",
      "summary": "Network simulation is pivotal in network modeling, assisting with tasks ranging from capacity planning to performance estimation. Traditional approaches such as Discrete Event Simulation (DES) face limitations in terms of computational cost and accuracy. This paper introduces RouteNet-Gauss, a novel integration of a testbed network with a Machine Learning (ML) model to address these challenges. By using the testbed as a hardware accelerator, RouteNet-Gauss generates training datasets rapidly and simulates network scenarios with high fidelity to real-world conditions. Experimental results show that RouteNet-Gauss significantly reduces prediction errors by up to 95% and achieves a 488x speedup in inference time compared to state-of-the-art DES-based methods. RouteNet-Gauss's modular architecture is dynamically constructed based on the specific characteristics of the network scenario, such as topology and routing. This enables it to understand and generalize to different network configurations beyond those seen during training, including networks up to 10x larger. Additionally, it supports Temporal Aggregated Performance Estimation (TAPE), providing configurable temporal granularity and maintaining high accuracy in flow performance metrics. This approach shows promise in improving both simulation efficiency and accuracy, offering a valuable tool for network operators.",
      "published": "2025-01-15T15:00:11Z"
    },
    "metadata": {
      "arxiv_id": "2501.08848",
      "title": "RouteNet-Gauss: Hardware-Enhanced Network Modeling with Machine Learning",
      "summary": "Network simulation is pivotal in network modeling, assisting with tasks ranging from capacity planning to performance estimation. Traditional approaches such as Discrete Event Simulation (DES) face limitations in terms of computational cost and accuracy. This paper introduces RouteNet-Gauss, a novel integration of a testbed network with a Machine Learning (ML) model to address these challenges. By using the testbed as a hardware accelerator, RouteNet-Gauss generates training datasets rapidly and simulates network scenarios with high fidelity to real-world conditions. Experimental results show that RouteNet-Gauss significantly reduces prediction errors by up to 95% and achieves a 488x speedup in inference time compared to state-of-the-art DES-based methods. RouteNet-Gauss's modular architecture is dynamically constructed based on the specific characteristics of the network scenario, such as topology and routing. This enables it to understand and generalize to different network configurations beyond those seen during training, including networks up to 10x larger. Additionally, it supports Temporal Aggregated Performance Estimation (TAPE), providing configurable temporal granularity and maintaining high accuracy in flow performance metrics. This approach shows promise in improving both simulation efficiency and accuracy, offering a valuable tool for network operators.",
      "authors": [
        "Carlos Güemes-Palau",
        "Miquel Ferriol-Galmés",
        "Jordi Paillisse-Vilanova",
        "Albert López-Brescó",
        "Pere Barlet-Ros",
        "Albert Cabellos-Aparicio"
      ],
      "published": "2025-01-15T15:00:11Z",
      "updated": "2025-09-03T16:19:20Z",
      "categories": [
        "cs.NI",
        "cs.AI",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2501.08848v2",
      "landing_url": "https://arxiv.org/abs/2501.08848v2",
      "doi": "https://doi.org/10.48550/arXiv.2501.08848"
    },
    "queries": [
      {
        "anchor": "spoken language models",
        "search_term": "modular architecture"
      },
      {
        "anchor": "synchronous dialogue",
        "search_term": "modular architecture"
      }
    ]
  },
  {
    "arxiv_id": "2501.08946",
    "anchor": "turn-taking",
    "search_term": "true full-duplex",
    "search_record": {
      "id": "http://arxiv.org/abs/2501.08946v1",
      "title": "Applying General Turn-taking Models to Conversational Human-Robot Interaction",
      "summary": "Turn-taking is a fundamental aspect of conversation, but current Human-Robot Interaction (HRI) systems often rely on simplistic, silence-based models, leading to unnatural pauses and interruptions. This paper investigates, for the first time, the application of general turn-taking models, specifically TurnGPT and Voice Activity Projection (VAP), to improve conversational dynamics in HRI. These models are trained on human-human dialogue data using self-supervised learning objectives, without requiring domain-specific fine-tuning. We propose methods for using these models in tandem to predict when a robot should begin preparing responses, take turns, and handle potential interruptions. We evaluated the proposed system in a within-subject study against a traditional baseline system, using the Furhat robot with 39 adults in a conversational setting, in combination with a large language model for autonomous response generation. The results show that participants significantly prefer the proposed system, and it significantly reduces response delays and interruptions.",
      "published": "2025-01-15T16:49:22Z"
    },
    "metadata": {
      "arxiv_id": "2501.08946",
      "title": "Applying General Turn-taking Models to Conversational Human-Robot Interaction",
      "summary": "Turn-taking is a fundamental aspect of conversation, but current Human-Robot Interaction (HRI) systems often rely on simplistic, silence-based models, leading to unnatural pauses and interruptions. This paper investigates, for the first time, the application of general turn-taking models, specifically TurnGPT and Voice Activity Projection (VAP), to improve conversational dynamics in HRI. These models are trained on human-human dialogue data using self-supervised learning objectives, without requiring domain-specific fine-tuning. We propose methods for using these models in tandem to predict when a robot should begin preparing responses, take turns, and handle potential interruptions. We evaluated the proposed system in a within-subject study against a traditional baseline system, using the Furhat robot with 39 adults in a conversational setting, in combination with a large language model for autonomous response generation. The results show that participants significantly prefer the proposed system, and it significantly reduces response delays and interruptions.",
      "authors": [
        "Gabriel Skantze",
        "Bahar Irfan"
      ],
      "published": "2025-01-15T16:49:22Z",
      "updated": "2025-01-15T16:49:22Z",
      "categories": [
        "cs.CL",
        "cs.RO"
      ],
      "pdf_url": "https://arxiv.org/pdf/2501.08946v1",
      "landing_url": "https://arxiv.org/abs/2501.08946v1",
      "doi": "https://doi.org/10.48550/arXiv.2501.08946"
    },
    "queries": [
      {
        "anchor": "turn-taking",
        "search_term": "true full-duplex"
      },
      {
        "anchor": "turn-taking",
        "search_term": "pseudo full-duplex"
      },
      {
        "anchor": "turn-taking",
        "search_term": "half duplex"
      },
      {
        "anchor": "turn-taking",
        "search_term": "engineered synchronization"
      },
      {
        "anchor": "turn-taking",
        "search_term": "learned synchronization"
      },
      {
        "anchor": "turn-taking",
        "search_term": "modular architecture"
      },
      {
        "anchor": "turn-taking",
        "search_term": "streaming architecture"
      },
      {
        "anchor": "turn-taking",
        "search_term": "real-time interaction"
      },
      {
        "anchor": "turn-taking",
        "search_term": "low latency"
      },
      {
        "anchor": "turn-taking",
        "search_term": "synchronous data"
      },
      {
        "anchor": "turn-taking",
        "search_term": "data scarcity"
      },
      {
        "anchor": "turn-taking",
        "search_term": "multichannel dialogue"
      },
      {
        "anchor": "turn-taking",
        "search_term": "temporal dynamics"
      },
      {
        "anchor": "turn-taking",
        "search_term": "behavioral arbitration"
      },
      {
        "anchor": "turn-taking",
        "search_term": "semantic coherence"
      },
      {
        "anchor": "turn-taking",
        "search_term": "neural codec"
      },
      {
        "anchor": "turn-taking",
        "search_term": "discrete tokens"
      },
      {
        "anchor": "turn-taking",
        "search_term": "audio tokens"
      },
      {
        "anchor": "turn-taking",
        "search_term": "floor transfer offset"
      },
      {
        "anchor": "turn-taking",
        "search_term": "stop latency"
      },
      {
        "anchor": "turn-taking",
        "search_term": "interruption response delay"
      }
    ]
  },
  {
    "arxiv_id": "2501.08995",
    "anchor": "spoken language models",
    "search_term": "data scarcity",
    "search_record": {
      "id": "http://arxiv.org/abs/2501.08995v2",
      "title": "VECT-GAN: A variationally encoded generative model for overcoming data scarcity in pharmaceutical science",
      "summary": "Data scarcity in pharmaceutical research has led to reliance on labour-intensive trial-and-error approaches for development rather than data-driven methods. While Machine Learning offers a solution, existing datasets are often small and noisy, limiting their utility. To address this, we developed a Variationally Encoded Conditional Tabular Generative Adversarial Network (VECT-GAN), a novel generative model specifically designed for augmenting small, noisy datasets. We introduce a pipeline where data is augmented before regression model development and demonstrate that this consistently and significantly improves performance over other state-of-the-art tabular generative models. We apply this pipeline across six pharmaceutical datasets, and highlight its real-world applicability by developing novel polymers with medically desirable mucoadhesive properties, which we made and experimentally characterised. Additionally, we pre-train the model on the ChEMBL database of drug-like molecules, leveraging knowledge distillation to enhance its generalisability, making it readily available for use on pharmaceutical datasets containing small molecules, an extremely common pharmaceutical task. We demonstrate the power of synthetic data for regularising small tabular datasets, highlighting its potential to become standard practice in pharmaceutical model development, and make our method, including VECT-GAN pre-trained on ChEMBL available as a pip package.",
      "published": "2025-01-15T18:23:33Z"
    },
    "metadata": {
      "arxiv_id": "2501.08995",
      "title": "VECT-GAN: A variationally encoded generative model for overcoming data scarcity in pharmaceutical science",
      "summary": "Data scarcity in pharmaceutical research has led to reliance on labour-intensive trial-and-error approaches for development rather than data-driven methods. While Machine Learning offers a solution, existing datasets are often small and noisy, limiting their utility. To address this, we developed a Variationally Encoded Conditional Tabular Generative Adversarial Network (VECT-GAN), a novel generative model specifically designed for augmenting small, noisy datasets. We introduce a pipeline where data is augmented before regression model development and demonstrate that this consistently and significantly improves performance over other state-of-the-art tabular generative models. We apply this pipeline across six pharmaceutical datasets, and highlight its real-world applicability by developing novel polymers with medically desirable mucoadhesive properties, which we made and experimentally characterised. Additionally, we pre-train the model on the ChEMBL database of drug-like molecules, leveraging knowledge distillation to enhance its generalisability, making it readily available for use on pharmaceutical datasets containing small molecules, an extremely common pharmaceutical task. We demonstrate the power of synthetic data for regularising small tabular datasets, highlighting its potential to become standard practice in pharmaceutical model development, and make our method, including VECT-GAN pre-trained on ChEMBL available as a pip package.",
      "authors": [
        "Youssef Abdalla",
        "Marrisa Taub",
        "Eleanor Hilton",
        "Priya Akkaraju",
        "Alexander Milanovic",
        "Mine Orlu",
        "Abdul W. Basit",
        "Michael T Cook",
        "Tapabrata Chakraborti",
        "David Shorthouse"
      ],
      "published": "2025-01-15T18:23:33Z",
      "updated": "2025-01-17T08:58:48Z",
      "categories": [
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2501.08995v2",
      "landing_url": "https://arxiv.org/abs/2501.08995v2",
      "doi": "https://doi.org/10.48550/arXiv.2501.08995"
    },
    "queries": [
      {
        "anchor": "spoken language models",
        "search_term": "data scarcity"
      },
      {
        "anchor": "synchronous dialogue",
        "search_term": "data scarcity"
      },
      {
        "anchor": "turn-taking",
        "search_term": "data scarcity"
      }
    ]
  },
  {
    "arxiv_id": "2501.13479",
    "anchor": "synchronous dialogue",
    "search_term": "data scarcity",
    "search_record": {
      "id": "http://arxiv.org/abs/2501.13479v1",
      "title": "Adaptive Few-Shot Learning (AFSL): Tackling Data Scarcity with Stability, Robustness, and Versatility",
      "summary": "Few-shot learning (FSL) enables machine learning models to generalize effectively with minimal labeled data, making it crucial for data-scarce domains such as healthcare, robotics, and natural language processing. Despite its potential, FSL faces challenges including sensitivity to initialization, difficulty in adapting to diverse domains, and vulnerability to noisy datasets. To address these issues, this paper introduces Adaptive Few-Shot Learning (AFSL), a framework that integrates advancements in meta-learning, domain alignment, noise resilience, and multi-modal integration. AFSL consists of four key modules: a Dynamic Stability Module for performance consistency, a Contextual Domain Alignment Module for domain adaptation, a Noise-Adaptive Resilience Module for handling noisy data, and a Multi-Modal Fusion Module for integrating diverse modalities. This work also explores strategies such as task-aware data augmentation, semi-supervised learning, and explainable AI techniques to enhance the applicability and robustness of FSL. AFSL provides scalable, reliable, and impactful solutions for real-world, high-stakes domains.",
      "published": "2025-01-23T08:51:49Z"
    },
    "metadata": {
      "arxiv_id": "2501.13479",
      "title": "Adaptive Few-Shot Learning (AFSL): Tackling Data Scarcity with Stability, Robustness, and Versatility",
      "summary": "Few-shot learning (FSL) enables machine learning models to generalize effectively with minimal labeled data, making it crucial for data-scarce domains such as healthcare, robotics, and natural language processing. Despite its potential, FSL faces challenges including sensitivity to initialization, difficulty in adapting to diverse domains, and vulnerability to noisy datasets. To address these issues, this paper introduces Adaptive Few-Shot Learning (AFSL), a framework that integrates advancements in meta-learning, domain alignment, noise resilience, and multi-modal integration. AFSL consists of four key modules: a Dynamic Stability Module for performance consistency, a Contextual Domain Alignment Module for domain adaptation, a Noise-Adaptive Resilience Module for handling noisy data, and a Multi-Modal Fusion Module for integrating diverse modalities. This work also explores strategies such as task-aware data augmentation, semi-supervised learning, and explainable AI techniques to enhance the applicability and robustness of FSL. AFSL provides scalable, reliable, and impactful solutions for real-world, high-stakes domains.",
      "authors": [
        "Rishabh Agrawal"
      ],
      "published": "2025-01-23T08:51:49Z",
      "updated": "2025-01-23T08:51:49Z",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2501.13479v1",
      "landing_url": "https://arxiv.org/abs/2501.13479v1",
      "doi": "https://doi.org/10.48550/arXiv.2501.13479"
    },
    "queries": [
      {
        "anchor": "synchronous dialogue",
        "search_term": "data scarcity"
      }
    ]
  },
  {
    "arxiv_id": "2501.13630",
    "anchor": "spoken language models",
    "search_term": "real-time interaction",
    "search_record": {
      "id": "http://arxiv.org/abs/2501.13630v1",
      "title": "VARFVV: View-Adaptive Real-Time Interactive Free-View Video Streaming with Edge Computing",
      "summary": "Free-view video (FVV) allows users to explore immersive video content from multiple views. However, delivering FVV poses significant challenges due to the uncertainty in view switching, combined with the substantial bandwidth and computational resources required to transmit and decode multiple video streams, which may result in frequent playback interruptions. Existing approaches, either client-based or cloud-based, struggle to meet high Quality of Experience (QoE) requirements under limited bandwidth and computational resources. To address these issues, we propose VARFVV, a bandwidth- and computationally-efficient system that enables real-time interactive FVV streaming with high QoE and low switching delay. Specifically, VARFVV introduces a low-complexity FVV generation scheme that reassembles multiview video frames at the edge server based on user-selected view tracks, eliminating the need for transcoding and significantly reducing computational overhead. This design makes it well-suited for large-scale, mobile-based UHD FVV experiences. Furthermore, we present a popularity-adaptive bit allocation method, leveraging a graph neural network, that predicts view popularity and dynamically adjusts bit allocation to maximize QoE within bandwidth constraints. We also construct an FVV dataset comprising 330 videos from 10 scenes, including basketball, opera, etc. Extensive experiments show that VARFVV surpasses existing methods in video quality, switching latency, computational efficiency, and bandwidth usage, supporting over 500 users on a single edge server with a switching delay of 71.5ms. Our code and dataset are available at https://github.com/qianghu-huber/VARFVV.",
      "published": "2025-01-23T12:58:58Z"
    },
    "metadata": {
      "arxiv_id": "2501.13630",
      "title": "VARFVV: View-Adaptive Real-Time Interactive Free-View Video Streaming with Edge Computing",
      "summary": "Free-view video (FVV) allows users to explore immersive video content from multiple views. However, delivering FVV poses significant challenges due to the uncertainty in view switching, combined with the substantial bandwidth and computational resources required to transmit and decode multiple video streams, which may result in frequent playback interruptions. Existing approaches, either client-based or cloud-based, struggle to meet high Quality of Experience (QoE) requirements under limited bandwidth and computational resources. To address these issues, we propose VARFVV, a bandwidth- and computationally-efficient system that enables real-time interactive FVV streaming with high QoE and low switching delay. Specifically, VARFVV introduces a low-complexity FVV generation scheme that reassembles multiview video frames at the edge server based on user-selected view tracks, eliminating the need for transcoding and significantly reducing computational overhead. This design makes it well-suited for large-scale, mobile-based UHD FVV experiences. Furthermore, we present a popularity-adaptive bit allocation method, leveraging a graph neural network, that predicts view popularity and dynamically adjusts bit allocation to maximize QoE within bandwidth constraints. We also construct an FVV dataset comprising 330 videos from 10 scenes, including basketball, opera, etc. Extensive experiments show that VARFVV surpasses existing methods in video quality, switching latency, computational efficiency, and bandwidth usage, supporting over 500 users on a single edge server with a switching delay of 71.5ms. Our code and dataset are available at https://github.com/qianghu-huber/VARFVV.",
      "authors": [
        "Qiang Hu",
        "Qihan He",
        "Houqiang Zhong",
        "Guo Lu",
        "Xiaoyun Zhang",
        "Guangtao Zhai",
        "Yanfeng Wang"
      ],
      "published": "2025-01-23T12:58:58Z",
      "updated": "2025-01-23T12:58:58Z",
      "categories": [
        "cs.MM"
      ],
      "pdf_url": "https://arxiv.org/pdf/2501.13630v1",
      "landing_url": "https://arxiv.org/abs/2501.13630v1",
      "doi": "https://doi.org/10.48550/arXiv.2501.13630"
    },
    "queries": [
      {
        "anchor": "spoken language models",
        "search_term": "real-time interaction"
      },
      {
        "anchor": "synchronous dialogue",
        "search_term": "real-time interaction"
      },
      {
        "anchor": "turn-taking",
        "search_term": "real-time interaction"
      }
    ]
  },
  {
    "arxiv_id": "2501.15368",
    "anchor": "synchronous dialogue",
    "search_term": "audio tokens",
    "search_record": {
      "id": "http://arxiv.org/abs/2501.15368v1",
      "title": "Baichuan-Omni-1.5 Technical Report",
      "summary": "We introduce Baichuan-Omni-1.5, an omni-modal model that not only has omni-modal understanding capabilities but also provides end-to-end audio generation capabilities. To achieve fluent and high-quality interaction across modalities without compromising the capabilities of any modality, we prioritized optimizing three key aspects. First, we establish a comprehensive data cleaning and synthesis pipeline for multimodal data, obtaining about 500B high-quality data (text, audio, and vision). Second, an audio-tokenizer (Baichuan-Audio-Tokenizer) has been designed to capture both semantic and acoustic information from audio, enabling seamless integration and enhanced compatibility with MLLM. Lastly, we designed a multi-stage training strategy that progressively integrates multimodal alignment and multitask fine-tuning, ensuring effective synergy across all modalities. Baichuan-Omni-1.5 leads contemporary models (including GPT4o-mini and MiniCPM-o 2.6) in terms of comprehensive omni-modal capabilities. Notably, it achieves results comparable to leading models such as Qwen2-VL-72B across various multimodal medical benchmarks.",
      "published": "2025-01-26T02:19:03Z"
    },
    "metadata": {
      "arxiv_id": "2501.15368",
      "title": "Baichuan-Omni-1.5 Technical Report",
      "summary": "We introduce Baichuan-Omni-1.5, an omni-modal model that not only has omni-modal understanding capabilities but also provides end-to-end audio generation capabilities. To achieve fluent and high-quality interaction across modalities without compromising the capabilities of any modality, we prioritized optimizing three key aspects. First, we establish a comprehensive data cleaning and synthesis pipeline for multimodal data, obtaining about 500B high-quality data (text, audio, and vision). Second, an audio-tokenizer (Baichuan-Audio-Tokenizer) has been designed to capture both semantic and acoustic information from audio, enabling seamless integration and enhanced compatibility with MLLM. Lastly, we designed a multi-stage training strategy that progressively integrates multimodal alignment and multitask fine-tuning, ensuring effective synergy across all modalities. Baichuan-Omni-1.5 leads contemporary models (including GPT4o-mini and MiniCPM-o 2.6) in terms of comprehensive omni-modal capabilities. Notably, it achieves results comparable to leading models such as Qwen2-VL-72B across various multimodal medical benchmarks.",
      "authors": [
        "Yadong Li",
        "Jun Liu",
        "Tao Zhang",
        "Tao Zhang",
        "Song Chen",
        "Tianpeng Li",
        "Zehuan Li",
        "Lijun Liu",
        "Lingfeng Ming",
        "Guosheng Dong",
        "Da Pan",
        "Chong Li",
        "Yuanbo Fang",
        "Dongdong Kuang",
        "Mingrui Wang",
        "Chenglin Zhu",
        "Youwei Zhang",
        "Hongyu Guo",
        "Fengyu Zhang",
        "Yuran Wang",
        "Bowen Ding",
        "Wei Song",
        "Xu Li",
        "Yuqi Huo",
        "Zheng Liang",
        "Shusen Zhang",
        "Xin Wu",
        "Shuai Zhao",
        "Linchu Xiong",
        "Yozhen Wu",
        "Jiahui Ye",
        "Wenhao Lu",
        "Bowen Li",
        "Yan Zhang",
        "Yaqi Zhou",
        "Xin Chen",
        "Lei Su",
        "Hongda Zhang",
        "Fuzhong Chen",
        "Xuezhen Dong",
        "Na Nie",
        "Zhiying Wu",
        "Bin Xiao",
        "Ting Li",
        "Shunya Dang",
        "Ping Zhang",
        "Yijia Sun",
        "Jincheng Wu",
        "Jinjie Yang",
        "Xionghai Lin",
        "Zhi Ma",
        "Kegeng Wu",
        "Jia li",
        "Aiyuan Yang",
        "Hui Liu",
        "Jianqiang Zhang",
        "Xiaoxi Chen",
        "Guangwei Ai",
        "Wentao Zhang",
        "Yicong Chen",
        "Xiaoqin Huang",
        "Kun Li",
        "Wenjing Luo",
        "Yifei Duan",
        "Lingling Zhu",
        "Ran Xiao",
        "Zhe Su",
        "Jiani Pu",
        "Dian Wang",
        "Xu Jia",
        "Tianyu Zhang",
        "Mengyu Ai",
        "Mang Wang",
        "Yujing Qiao",
        "Lei Zhang",
        "Yanjun Shen",
        "Fan Yang",
        "Miao Zhen",
        "Yijie Zhou",
        "Mingyang Chen",
        "Fei Li",
        "Chenzheng Zhu",
        "Keer Lu",
        "Yaqi Zhao",
        "Hao Liang",
        "Youquan Li",
        "Yanzhao Qin",
        "Linzhuang Sun",
        "Jianhua Xu",
        "Haoze Sun",
        "Mingan Lin",
        "Zenan Zhou",
        "Weipeng Chen"
      ],
      "published": "2025-01-26T02:19:03Z",
      "updated": "2025-01-26T02:19:03Z",
      "categories": [
        "cs.CL",
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2501.15368v1",
      "landing_url": "https://arxiv.org/abs/2501.15368v1",
      "doi": "https://doi.org/10.48550/arXiv.2501.15368"
    },
    "queries": [
      {
        "anchor": "synchronous dialogue",
        "search_term": "audio tokens"
      }
    ]
  },
  {
    "arxiv_id": "2501.15438",
    "anchor": "synchronous dialogue",
    "search_term": "data scarcity",
    "search_record": {
      "id": "http://arxiv.org/abs/2501.15438v1",
      "title": "Cross-Modal Transfer from Memes to Videos: Addressing Data Scarcity in Hateful Video Detection",
      "summary": "Detecting hate speech in online content is essential to ensuring safer digital spaces. While significant progress has been made in text and meme modalities, video-based hate speech detection remains under-explored, hindered by a lack of annotated datasets and the high cost of video annotation. This gap is particularly problematic given the growing reliance on large models, which demand substantial amounts of training data. To address this challenge, we leverage meme datasets as both a substitution and an augmentation strategy for training hateful video detection models. Our approach introduces a human-assisted reannotation pipeline to align meme dataset labels with video datasets, ensuring consistency with minimal labeling effort. Using two state-of-the-art vision-language models, we demonstrate that meme data can substitute for video data in resource-scarce scenarios and augment video datasets to achieve further performance gains. Our results consistently outperform state-of-the-art benchmarks, showcasing the potential of cross-modal transfer learning for advancing hateful video detection. Dataset and code are available at https://github.com/Social-AI-Studio/CrossModalTransferLearning.",
      "published": "2025-01-26T07:50:14Z"
    },
    "metadata": {
      "arxiv_id": "2501.15438",
      "title": "Cross-Modal Transfer from Memes to Videos: Addressing Data Scarcity in Hateful Video Detection",
      "summary": "Detecting hate speech in online content is essential to ensuring safer digital spaces. While significant progress has been made in text and meme modalities, video-based hate speech detection remains under-explored, hindered by a lack of annotated datasets and the high cost of video annotation. This gap is particularly problematic given the growing reliance on large models, which demand substantial amounts of training data. To address this challenge, we leverage meme datasets as both a substitution and an augmentation strategy for training hateful video detection models. Our approach introduces a human-assisted reannotation pipeline to align meme dataset labels with video datasets, ensuring consistency with minimal labeling effort. Using two state-of-the-art vision-language models, we demonstrate that meme data can substitute for video data in resource-scarce scenarios and augment video datasets to achieve further performance gains. Our results consistently outperform state-of-the-art benchmarks, showcasing the potential of cross-modal transfer learning for advancing hateful video detection. Dataset and code are available at https://github.com/Social-AI-Studio/CrossModalTransferLearning.",
      "authors": [
        "Han Wang",
        "Rui Yang Tan",
        "Roy Ka-Wei Lee"
      ],
      "published": "2025-01-26T07:50:14Z",
      "updated": "2025-01-26T07:50:14Z",
      "categories": [
        "cs.CV",
        "cs.MM"
      ],
      "pdf_url": "https://arxiv.org/pdf/2501.15438v1",
      "landing_url": "https://arxiv.org/abs/2501.15438v1",
      "doi": "https://doi.org/10.1145/3696410.3714534"
    },
    "queries": [
      {
        "anchor": "synchronous dialogue",
        "search_term": "data scarcity"
      }
    ]
  },
  {
    "arxiv_id": "2501.17790",
    "anchor": "synchronous dialogue",
    "search_term": "neural codec",
    "search_record": {
      "id": "http://arxiv.org/abs/2501.17790v1",
      "title": "BreezyVoice: Adapting TTS for Taiwanese Mandarin with Enhanced Polyphone Disambiguation -- Challenges and Insights",
      "summary": "We present BreezyVoice, a Text-to-Speech (TTS) system specifically adapted for Taiwanese Mandarin, highlighting phonetic control abilities to address the unique challenges of polyphone disambiguation in the language. Building upon CosyVoice, we incorporate a $S^{3}$ tokenizer, a large language model (LLM), an optimal-transport conditional flow matching model (OT-CFM), and a grapheme to phoneme prediction model, to generate realistic speech that closely mimics human utterances. Our evaluation demonstrates BreezyVoice's superior performance in both general and code-switching contexts, highlighting its robustness and effectiveness in generating high-fidelity speech. Additionally, we address the challenges of generalizability in modeling long-tail speakers and polyphone disambiguation. Our approach significantly enhances performance and offers valuable insights into the workings of neural codec TTS systems.",
      "published": "2025-01-29T17:31:26Z"
    },
    "metadata": {
      "arxiv_id": "2501.17790",
      "title": "BreezyVoice: Adapting TTS for Taiwanese Mandarin with Enhanced Polyphone Disambiguation -- Challenges and Insights",
      "summary": "We present BreezyVoice, a Text-to-Speech (TTS) system specifically adapted for Taiwanese Mandarin, highlighting phonetic control abilities to address the unique challenges of polyphone disambiguation in the language. Building upon CosyVoice, we incorporate a $S^{3}$ tokenizer, a large language model (LLM), an optimal-transport conditional flow matching model (OT-CFM), and a grapheme to phoneme prediction model, to generate realistic speech that closely mimics human utterances. Our evaluation demonstrates BreezyVoice's superior performance in both general and code-switching contexts, highlighting its robustness and effectiveness in generating high-fidelity speech. Additionally, we address the challenges of generalizability in modeling long-tail speakers and polyphone disambiguation. Our approach significantly enhances performance and offers valuable insights into the workings of neural codec TTS systems.",
      "authors": [
        "Chan-Jan Hsu",
        "Yi-Cheng Lin",
        "Chia-Chun Lin",
        "Wei-Chih Chen",
        "Ho Lam Chung",
        "Chen-An Li",
        "Yi-Chang Chen",
        "Chien-Yu Yu",
        "Ming-Ji Lee",
        "Chien-Cheng Chen",
        "Ru-Heng Huang",
        "Hung-yi Lee",
        "Da-Shan Shiu"
      ],
      "published": "2025-01-29T17:31:26Z",
      "updated": "2025-01-29T17:31:26Z",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2501.17790v1",
      "landing_url": "https://arxiv.org/abs/2501.17790v1",
      "doi": "https://doi.org/10.48550/arXiv.2501.17790"
    },
    "queries": [
      {
        "anchor": "synchronous dialogue",
        "search_term": "neural codec"
      }
    ]
  },
  {
    "arxiv_id": "2501.18103",
    "anchor": "turn-taking",
    "search_term": "true full-duplex",
    "search_record": {
      "id": "http://arxiv.org/abs/2501.18103v1",
      "title": "Beyond Turn-taking: Introducing Text-based Overlap into Human-LLM Interactions",
      "summary": "Traditional text-based human-AI interactions often adhere to a strict turn-taking approach. In this research, we propose a novel approach that incorporates overlapping messages, mirroring natural human conversations. Through a formative study, we observed that even in text-based contexts, users instinctively engage in overlapping behaviors like \"A: Today I went to-\" \"B: yeah.\" To capitalize on these insights, we developed OverlapBot, a prototype chatbot where both AI and users can initiate overlapping. Our user study revealed that OverlapBot was perceived as more communicative and immersive than traditional turn-taking chatbot, fostering faster and more natural interactions. Our findings contribute to the understanding of design space for overlapping interactions. We also provide recommendations for implementing overlap-capable AI interactions to enhance the fluidity and engagement of text-based conversations.",
      "published": "2025-01-30T03:01:01Z"
    },
    "metadata": {
      "arxiv_id": "2501.18103",
      "title": "Beyond Turn-taking: Introducing Text-based Overlap into Human-LLM Interactions",
      "summary": "Traditional text-based human-AI interactions often adhere to a strict turn-taking approach. In this research, we propose a novel approach that incorporates overlapping messages, mirroring natural human conversations. Through a formative study, we observed that even in text-based contexts, users instinctively engage in overlapping behaviors like \"A: Today I went to-\" \"B: yeah.\" To capitalize on these insights, we developed OverlapBot, a prototype chatbot where both AI and users can initiate overlapping. Our user study revealed that OverlapBot was perceived as more communicative and immersive than traditional turn-taking chatbot, fostering faster and more natural interactions. Our findings contribute to the understanding of design space for overlapping interactions. We also provide recommendations for implementing overlap-capable AI interactions to enhance the fluidity and engagement of text-based conversations.",
      "authors": [
        "JiWoo Kim",
        "Minsuk Chang",
        "JinYeong Bak"
      ],
      "published": "2025-01-30T03:01:01Z",
      "updated": "2025-01-30T03:01:01Z",
      "categories": [
        "cs.HC",
        "cs.CL"
      ],
      "pdf_url": "https://arxiv.org/pdf/2501.18103v1",
      "landing_url": "https://arxiv.org/abs/2501.18103v1",
      "doi": "https://doi.org/10.48550/arXiv.2501.18103"
    },
    "queries": [
      {
        "anchor": "turn-taking",
        "search_term": "true full-duplex"
      },
      {
        "anchor": "turn-taking",
        "search_term": "pseudo full-duplex"
      },
      {
        "anchor": "turn-taking",
        "search_term": "half duplex"
      },
      {
        "anchor": "turn-taking",
        "search_term": "engineered synchronization"
      },
      {
        "anchor": "turn-taking",
        "search_term": "learned synchronization"
      },
      {
        "anchor": "turn-taking",
        "search_term": "modular architecture"
      },
      {
        "anchor": "turn-taking",
        "search_term": "streaming architecture"
      },
      {
        "anchor": "turn-taking",
        "search_term": "real-time interaction"
      },
      {
        "anchor": "turn-taking",
        "search_term": "low latency"
      },
      {
        "anchor": "turn-taking",
        "search_term": "synchronous data"
      },
      {
        "anchor": "turn-taking",
        "search_term": "data scarcity"
      },
      {
        "anchor": "turn-taking",
        "search_term": "multichannel dialogue"
      },
      {
        "anchor": "turn-taking",
        "search_term": "temporal dynamics"
      },
      {
        "anchor": "turn-taking",
        "search_term": "behavioral arbitration"
      },
      {
        "anchor": "turn-taking",
        "search_term": "semantic coherence"
      },
      {
        "anchor": "turn-taking",
        "search_term": "neural codec"
      },
      {
        "anchor": "turn-taking",
        "search_term": "discrete tokens"
      },
      {
        "anchor": "turn-taking",
        "search_term": "audio tokens"
      },
      {
        "anchor": "turn-taking",
        "search_term": "floor transfer offset"
      },
      {
        "anchor": "turn-taking",
        "search_term": "stop latency"
      },
      {
        "anchor": "turn-taking",
        "search_term": "interruption response delay"
      }
    ]
  },
  {
    "arxiv_id": "2501.18332",
    "anchor": "spoken language models",
    "search_term": "streaming architecture",
    "search_record": {
      "id": "http://arxiv.org/abs/2501.18332v1",
      "title": "Adaptive Video Streaming with AI-Based Optimization for Dynamic Network Conditions",
      "summary": "The increase in video streaming has presented a challenge of handling stream request effectively, especially over networks that are variable. This paper describes a new adaptive video streaming architecture capable of changing the video quality and buffer size depending on the data and latency of streamed video. For video streaming VLC media player was used where network performance data were obtained through Python scripts with very accurate data rate and latency measurement. The collected data is analyzed using Gemini AI, containing characteristics of the machine learning algorithm that recognizes the best resolution of videos and the buffer sizes. Through the features of real-time monitoring and artificial intelligence decision making, the proposed framework improves the user experience by reducing the occurrence of buffering events while at the same time increasing the video quality. Our findings therefore confirm that the proposed solution based on artificial intelligence increases video quality and flexibility. This study advances knowledge of adaptive streaming and offers an argument about how intelligent datadriven approaches and AI may be useful tools for enhancing the delivery of video in practical environments.",
      "published": "2025-01-30T13:20:23Z"
    },
    "metadata": {
      "arxiv_id": "2501.18332",
      "title": "Adaptive Video Streaming with AI-Based Optimization for Dynamic Network Conditions",
      "summary": "The increase in video streaming has presented a challenge of handling stream request effectively, especially over networks that are variable. This paper describes a new adaptive video streaming architecture capable of changing the video quality and buffer size depending on the data and latency of streamed video. For video streaming VLC media player was used where network performance data were obtained through Python scripts with very accurate data rate and latency measurement. The collected data is analyzed using Gemini AI, containing characteristics of the machine learning algorithm that recognizes the best resolution of videos and the buffer sizes. Through the features of real-time monitoring and artificial intelligence decision making, the proposed framework improves the user experience by reducing the occurrence of buffering events while at the same time increasing the video quality. Our findings therefore confirm that the proposed solution based on artificial intelligence increases video quality and flexibility. This study advances knowledge of adaptive streaming and offers an argument about how intelligent datadriven approaches and AI may be useful tools for enhancing the delivery of video in practical environments.",
      "authors": [
        "Mohammad Tarik",
        "Qutaiba Ibrahim"
      ],
      "published": "2025-01-30T13:20:23Z",
      "updated": "2025-01-30T13:20:23Z",
      "categories": [
        "cs.NI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2501.18332v1",
      "landing_url": "https://arxiv.org/abs/2501.18332v1",
      "doi": "https://doi.org/10.48550/arXiv.2501.18332"
    },
    "queries": [
      {
        "anchor": "spoken language models",
        "search_term": "streaming architecture"
      },
      {
        "anchor": "synchronous dialogue",
        "search_term": "streaming architecture"
      },
      {
        "anchor": "turn-taking",
        "search_term": "streaming architecture"
      }
    ]
  },
  {
    "arxiv_id": "2502.00033",
    "anchor": "spoken language models",
    "search_term": "real-time interaction",
    "search_record": {
      "id": "http://arxiv.org/abs/2502.00033v1",
      "title": "STRIELAD -- A Scalable Toolkit for Real-time Interactive Exploration of Large Atmospheric Datasets",
      "summary": "Technological advances in high performance computing and maturing physical models allow scientists to simulate weather and climate evolutions with an increasing accuracy. While this improved accuracy allows us to explore complex dynamical interactions within such physical systems, inconceivable a few years ago, it also results in grand challenges regarding the data visualization and analytics process. We present STRIELAD, a scalable weather analytics toolkit, which allows for interactive exploration and real-time visualization of such large scale datasets. It combines parallel and distributed feature extraction using high-performance computing resources with smart level-of-detail rendering methods to assure interactivity during the complete analysis process.",
      "published": "2025-01-24T09:31:22Z"
    },
    "metadata": {
      "arxiv_id": "2502.00033",
      "title": "STRIELAD -- A Scalable Toolkit for Real-time Interactive Exploration of Large Atmospheric Datasets",
      "summary": "Technological advances in high performance computing and maturing physical models allow scientists to simulate weather and climate evolutions with an increasing accuracy. While this improved accuracy allows us to explore complex dynamical interactions within such physical systems, inconceivable a few years ago, it also results in grand challenges regarding the data visualization and analytics process. We present STRIELAD, a scalable weather analytics toolkit, which allows for interactive exploration and real-time visualization of such large scale datasets. It combines parallel and distributed feature extraction using high-performance computing resources with smart level-of-detail rendering methods to assure interactivity during the complete analysis process.",
      "authors": [
        "Simon Schneegans",
        "Lori Neary",
        "Markus Flatken",
        "Andreas Gerndt"
      ],
      "published": "2025-01-24T09:31:22Z",
      "updated": "2025-01-24T09:31:22Z",
      "categories": [
        "cs.HC",
        "cs.DC",
        "cs.GR"
      ],
      "pdf_url": "https://arxiv.org/pdf/2502.00033v1",
      "landing_url": "https://arxiv.org/abs/2502.00033v1",
      "doi": "https://doi.org/10.48550/arXiv.2502.00033"
    },
    "queries": [
      {
        "anchor": "spoken language models",
        "search_term": "real-time interaction"
      },
      {
        "anchor": "synchronous dialogue",
        "search_term": "real-time interaction"
      },
      {
        "anchor": "turn-taking",
        "search_term": "real-time interaction"
      }
    ]
  },
  {
    "arxiv_id": "2502.00459",
    "anchor": "synchronous dialogue",
    "search_term": "audio tokens",
    "search_record": {
      "id": "http://arxiv.org/abs/2502.00459v2",
      "title": "AudioGenX: Explainability on Text-to-Audio Generative Models",
      "summary": "Text-to-audio generation models (TAG) have achieved significant advances in generating audio conditioned on text descriptions. However, a critical challenge lies in the lack of transparency regarding how each textual input impacts the generated audio. To address this issue, we introduce AudioGenX, an Explainable AI (XAI) method that provides explanations for text-to-audio generation models by highlighting the importance of input tokens. AudioGenX optimizes an Explainer by leveraging factual and counterfactual objective functions to provide faithful explanations at the audio token level. This method offers a detailed and comprehensive understanding of the relationship between text inputs and audio outputs, enhancing both the explainability and trustworthiness of TAG models. Extensive experiments demonstrate the effectiveness of AudioGenX in producing faithful explanations, benchmarked against existing methods using novel evaluation metrics specifically designed for audio generation tasks.",
      "published": "2025-02-01T15:37:42Z"
    },
    "metadata": {
      "arxiv_id": "2502.00459",
      "title": "AudioGenX: Explainability on Text-to-Audio Generative Models",
      "summary": "Text-to-audio generation models (TAG) have achieved significant advances in generating audio conditioned on text descriptions. However, a critical challenge lies in the lack of transparency regarding how each textual input impacts the generated audio. To address this issue, we introduce AudioGenX, an Explainable AI (XAI) method that provides explanations for text-to-audio generation models by highlighting the importance of input tokens. AudioGenX optimizes an Explainer by leveraging factual and counterfactual objective functions to provide faithful explanations at the audio token level. This method offers a detailed and comprehensive understanding of the relationship between text inputs and audio outputs, enhancing both the explainability and trustworthiness of TAG models. Extensive experiments demonstrate the effectiveness of AudioGenX in producing faithful explanations, benchmarked against existing methods using novel evaluation metrics specifically designed for audio generation tasks.",
      "authors": [
        "Hyunju Kang",
        "Geonhee Han",
        "Yoonjae Jeong",
        "Hogun Park"
      ],
      "published": "2025-02-01T15:37:42Z",
      "updated": "2025-02-04T04:00:01Z",
      "categories": [
        "cs.SD",
        "cs.AI",
        "cs.LG",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2502.00459v2",
      "landing_url": "https://arxiv.org/abs/2502.00459v2",
      "doi": "https://doi.org/10.1609/aaai.v39i17.33950"
    },
    "queries": [
      {
        "anchor": "synchronous dialogue",
        "search_term": "audio tokens"
      }
    ]
  },
  {
    "arxiv_id": "2502.02942",
    "anchor": "synchronous dialogue",
    "search_term": "neural codec",
    "search_record": {
      "id": "http://arxiv.org/abs/2502.02942v1",
      "title": "GenSE: Generative Speech Enhancement via Language Models using Hierarchical Modeling",
      "summary": "Semantic information refers to the meaning conveyed through words, phrases, and contextual relationships within a given linguistic structure. Humans can leverage semantic information, such as familiar linguistic patterns and contextual cues, to reconstruct incomplete or masked speech signals in noisy environments. However, existing speech enhancement (SE) approaches often overlook the rich semantic information embedded in speech, which is crucial for improving intelligibility, speaker consistency, and overall quality of enhanced speech signals. To enrich the SE model with semantic information, we employ language models as an efficient semantic learner and propose a comprehensive framework tailored for language model-based speech enhancement, called \\textit{GenSE}. Specifically, we approach SE as a conditional language modeling task rather than a continuous signal regression problem defined in existing works. This is achieved by tokenizing speech signals into semantic tokens using a pre-trained self-supervised model and into acoustic tokens using a custom-designed single-quantizer neural codec model. To improve the stability of language model predictions, we propose a hierarchical modeling method that decouples the generation of clean semantic tokens and clean acoustic tokens into two distinct stages. Moreover, we introduce a token chain prompting mechanism during the acoustic token generation stage to ensure timbre consistency throughout the speech enhancement process. Experimental results on benchmark datasets demonstrate that our proposed approach outperforms state-of-the-art SE systems in terms of speech quality and generalization capability.",
      "published": "2025-02-05T07:14:39Z"
    },
    "metadata": {
      "arxiv_id": "2502.02942",
      "title": "GenSE: Generative Speech Enhancement via Language Models using Hierarchical Modeling",
      "summary": "Semantic information refers to the meaning conveyed through words, phrases, and contextual relationships within a given linguistic structure. Humans can leverage semantic information, such as familiar linguistic patterns and contextual cues, to reconstruct incomplete or masked speech signals in noisy environments. However, existing speech enhancement (SE) approaches often overlook the rich semantic information embedded in speech, which is crucial for improving intelligibility, speaker consistency, and overall quality of enhanced speech signals. To enrich the SE model with semantic information, we employ language models as an efficient semantic learner and propose a comprehensive framework tailored for language model-based speech enhancement, called \\textit{GenSE}. Specifically, we approach SE as a conditional language modeling task rather than a continuous signal regression problem defined in existing works. This is achieved by tokenizing speech signals into semantic tokens using a pre-trained self-supervised model and into acoustic tokens using a custom-designed single-quantizer neural codec model. To improve the stability of language model predictions, we propose a hierarchical modeling method that decouples the generation of clean semantic tokens and clean acoustic tokens into two distinct stages. Moreover, we introduce a token chain prompting mechanism during the acoustic token generation stage to ensure timbre consistency throughout the speech enhancement process. Experimental results on benchmark datasets demonstrate that our proposed approach outperforms state-of-the-art SE systems in terms of speech quality and generalization capability.",
      "authors": [
        "Jixun Yao",
        "Hexin Liu",
        "Chen Chen",
        "Yuchen Hu",
        "EngSiong Chng",
        "Lei Xie"
      ],
      "published": "2025-02-05T07:14:39Z",
      "updated": "2025-02-05T07:14:39Z",
      "categories": [
        "eess.AS",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2502.02942v1",
      "landing_url": "https://arxiv.org/abs/2502.02942v1",
      "doi": "https://doi.org/10.48550/arXiv.2502.02942"
    },
    "queries": [
      {
        "anchor": "synchronous dialogue",
        "search_term": "neural codec"
      }
    ]
  },
  {
    "arxiv_id": "2502.03382",
    "anchor": "spoken language models",
    "search_term": "audio tokens",
    "search_record": {
      "id": "http://arxiv.org/abs/2502.03382v2",
      "title": "High-Fidelity Simultaneous Speech-To-Speech Translation",
      "summary": "We introduce Hibiki, a decoder-only model for simultaneous speech translation. Hibiki leverages a multistream language model to synchronously process source and target speech, and jointly produces text and audio tokens to perform speech-to-text and speech-to-speech translation. We furthermore address the fundamental challenge of simultaneous interpretation, which unlike its consecutive counterpart, where one waits for the end of the source utterance to start translating, adapts its flow to accumulate just enough context to produce a correct translation in real-time, chunk by chunk. To do so, we introduce a weakly-supervised method that leverages the perplexity of an off-the-shelf text translation system to identify optimal delays on a per-word basis and create aligned synthetic data. After supervised training, Hibiki performs adaptive, simultaneous speech translation with vanilla temperature sampling. On a French-English simultaneous speech translation task, Hibiki demonstrates state-of-the-art performance in translation quality, speaker fidelity and naturalness. Moreover, the simplicity of its inference process makes it compatible with batched translation and even real-time on-device deployment. We provide examples as well as models and inference code.",
      "published": "2025-02-05T17:18:55Z"
    },
    "metadata": {
      "arxiv_id": "2502.03382",
      "title": "High-Fidelity Simultaneous Speech-To-Speech Translation",
      "summary": "We introduce Hibiki, a decoder-only model for simultaneous speech translation. Hibiki leverages a multistream language model to synchronously process source and target speech, and jointly produces text and audio tokens to perform speech-to-text and speech-to-speech translation. We furthermore address the fundamental challenge of simultaneous interpretation, which unlike its consecutive counterpart, where one waits for the end of the source utterance to start translating, adapts its flow to accumulate just enough context to produce a correct translation in real-time, chunk by chunk. To do so, we introduce a weakly-supervised method that leverages the perplexity of an off-the-shelf text translation system to identify optimal delays on a per-word basis and create aligned synthetic data. After supervised training, Hibiki performs adaptive, simultaneous speech translation with vanilla temperature sampling. On a French-English simultaneous speech translation task, Hibiki demonstrates state-of-the-art performance in translation quality, speaker fidelity and naturalness. Moreover, the simplicity of its inference process makes it compatible with batched translation and even real-time on-device deployment. We provide examples as well as models and inference code.",
      "authors": [
        "Tom Labiausse",
        "Laurent Mazaré",
        "Edouard Grave",
        "Patrick Pérez",
        "Alexandre Défossez",
        "Neil Zeghidour"
      ],
      "published": "2025-02-05T17:18:55Z",
      "updated": "2025-02-26T09:31:58Z",
      "categories": [
        "cs.CL",
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2502.03382v2",
      "landing_url": "https://arxiv.org/abs/2502.03382v2",
      "doi": "https://doi.org/10.48550/arXiv.2502.03382"
    },
    "queries": [
      {
        "anchor": "spoken language models",
        "search_term": "audio tokens"
      },
      {
        "anchor": "synchronous dialogue",
        "search_term": "audio tokens"
      }
    ]
  },
  {
    "arxiv_id": "2502.04770",
    "anchor": "spoken language models",
    "search_term": "neural codec",
    "search_record": {
      "id": "http://arxiv.org/abs/2502.04770v1",
      "title": "Efficient Evaluation of Quantization-Effects in Neural Codecs",
      "summary": "Neural codecs, comprising an encoder, quantizer, and decoder, enable signal transmission at exceptionally low bitrates. Training these systems requires techniques like the straight-through estimator, soft-to-hard annealing, or statistical quantizer emulation to allow a non-zero gradient across the quantizer. Evaluating the effect of quantization in neural codecs, like the influence of gradient passing techniques on the whole system, is often costly and time-consuming due to training demands and the lack of affordable and reliable metrics. This paper proposes an efficient evaluation framework for neural codecs using simulated data with a defined number of bits and low-complexity neural encoders/decoders to emulate the non-linear behavior in larger networks. Our system is highly efficient in terms of training time and computational and hardware requirements, allowing us to uncover distinct behaviors in neural codecs. We propose a modification to stabilize training with the straight-through estimator based on our findings. We validate our findings against an internal neural audio codec and against the state-of-the-art descript-audio-codec.",
      "published": "2025-02-07T09:11:19Z"
    },
    "metadata": {
      "arxiv_id": "2502.04770",
      "title": "Efficient Evaluation of Quantization-Effects in Neural Codecs",
      "summary": "Neural codecs, comprising an encoder, quantizer, and decoder, enable signal transmission at exceptionally low bitrates. Training these systems requires techniques like the straight-through estimator, soft-to-hard annealing, or statistical quantizer emulation to allow a non-zero gradient across the quantizer. Evaluating the effect of quantization in neural codecs, like the influence of gradient passing techniques on the whole system, is often costly and time-consuming due to training demands and the lack of affordable and reliable metrics. This paper proposes an efficient evaluation framework for neural codecs using simulated data with a defined number of bits and low-complexity neural encoders/decoders to emulate the non-linear behavior in larger networks. Our system is highly efficient in terms of training time and computational and hardware requirements, allowing us to uncover distinct behaviors in neural codecs. We propose a modification to stabilize training with the straight-through estimator based on our findings. We validate our findings against an internal neural audio codec and against the state-of-the-art descript-audio-codec.",
      "authors": [
        "Wolfgang Mack",
        "Ahmed Mustafa",
        "Rafał Łaganowski",
        "Samer Hijazy"
      ],
      "published": "2025-02-07T09:11:19Z",
      "updated": "2025-02-07T09:11:19Z",
      "categories": [
        "eess.AS",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2502.04770v1",
      "landing_url": "https://arxiv.org/abs/2502.04770v1",
      "doi": "https://doi.org/10.48550/arXiv.2502.04770"
    },
    "queries": [
      {
        "anchor": "spoken language models",
        "search_term": "neural codec"
      },
      {
        "anchor": "synchronous dialogue",
        "search_term": "neural codec"
      },
      {
        "anchor": "turn-taking",
        "search_term": "neural codec"
      }
    ]
  },
  {
    "arxiv_id": "2502.05606",
    "anchor": "synchronous dialogue",
    "search_term": "semantic coherence",
    "search_record": {
      "id": "http://arxiv.org/abs/2502.05606v3",
      "title": "FreeBlend: Advancing Concept Blending with Staged Feedback-Driven Interpolation Diffusion",
      "summary": "Concept blending is a promising yet underexplored area in generative models. While recent approaches, such as embedding mixing and latent modification based on structural sketches, have been proposed, they often suffer from incompatible semantic information and discrepancies in shape and appearance. In this work, we introduce FreeBlend, an effective, training-free framework designed to address these challenges. To mitigate cross-modal loss and enhance feature detail, we leverage transferred image embeddings as conditional inputs. The framework employs a stepwise increasing interpolation strategy between latents, progressively adjusting the blending ratio to seamlessly integrate auxiliary features. Additionally, we introduce a feedback-driven mechanism that updates the auxiliary latents in reverse order, facilitating global blending and preventing rigid or unnatural outputs. Extensive experiments demonstrate that our method significantly improves both the semantic coherence and visual quality of blended images, yielding compelling and coherent results.",
      "published": "2025-02-08T15:25:03Z"
    },
    "metadata": {
      "arxiv_id": "2502.05606",
      "title": "FreeBlend: Advancing Concept Blending with Staged Feedback-Driven Interpolation Diffusion",
      "summary": "Concept blending is a promising yet underexplored area in generative models. While recent approaches, such as embedding mixing and latent modification based on structural sketches, have been proposed, they often suffer from incompatible semantic information and discrepancies in shape and appearance. In this work, we introduce FreeBlend, an effective, training-free framework designed to address these challenges. To mitigate cross-modal loss and enhance feature detail, we leverage transferred image embeddings as conditional inputs. The framework employs a stepwise increasing interpolation strategy between latents, progressively adjusting the blending ratio to seamlessly integrate auxiliary features. Additionally, we introduce a feedback-driven mechanism that updates the auxiliary latents in reverse order, facilitating global blending and preventing rigid or unnatural outputs. Extensive experiments demonstrate that our method significantly improves both the semantic coherence and visual quality of blended images, yielding compelling and coherent results.",
      "authors": [
        "Yufan Zhou",
        "Haoyu Shen",
        "Huan Wang"
      ],
      "published": "2025-02-08T15:25:03Z",
      "updated": "2025-11-09T11:33:47Z",
      "categories": [
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2502.05606v3",
      "landing_url": "https://arxiv.org/abs/2502.05606v3",
      "doi": "https://doi.org/10.48550/arXiv.2502.05606"
    },
    "queries": [
      {
        "anchor": "synchronous dialogue",
        "search_term": "semantic coherence"
      }
    ]
  },
  {
    "arxiv_id": "2502.08181",
    "anchor": "spoken language models",
    "search_term": "data scarcity",
    "search_record": {
      "id": "http://arxiv.org/abs/2502.08181v1",
      "title": "Latest Advancements Towards Catastrophic Forgetting under Data Scarcity: A Comprehensive Survey on Few-Shot Class Incremental Learning",
      "summary": "Data scarcity significantly complicates the continual learning problem, i.e., how a deep neural network learns in dynamic environments with very few samples. However, the latest progress of few-shot class incremental learning (FSCIL) methods and related studies show insightful knowledge on how to tackle the problem. This paper presents a comprehensive survey on FSCIL that highlights several important aspects i.e. comprehensive and formal objectives of FSCIL approaches, the importance of prototype rectifications, the new learning paradigms based on pre-trained model and language-guided mechanism, the deeper analysis of FSCIL performance metrics and evaluation, and the practical contexts of FSCIL in various areas. Our extensive discussion presents the open challenges, potential solutions, and future directions of FSCIL.",
      "published": "2025-02-12T07:39:44Z"
    },
    "metadata": {
      "arxiv_id": "2502.08181",
      "title": "Latest Advancements Towards Catastrophic Forgetting under Data Scarcity: A Comprehensive Survey on Few-Shot Class Incremental Learning",
      "summary": "Data scarcity significantly complicates the continual learning problem, i.e., how a deep neural network learns in dynamic environments with very few samples. However, the latest progress of few-shot class incremental learning (FSCIL) methods and related studies show insightful knowledge on how to tackle the problem. This paper presents a comprehensive survey on FSCIL that highlights several important aspects i.e. comprehensive and formal objectives of FSCIL approaches, the importance of prototype rectifications, the new learning paradigms based on pre-trained model and language-guided mechanism, the deeper analysis of FSCIL performance metrics and evaluation, and the practical contexts of FSCIL in various areas. Our extensive discussion presents the open challenges, potential solutions, and future directions of FSCIL.",
      "authors": [
        "M. Anwar Ma'sum",
        "Mahardhika Pratama",
        "Igor Skrjanc"
      ],
      "published": "2025-02-12T07:39:44Z",
      "updated": "2025-02-12T07:39:44Z",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2502.08181v1",
      "landing_url": "https://arxiv.org/abs/2502.08181v1",
      "doi": "https://doi.org/10.48550/arXiv.2502.08181"
    },
    "queries": [
      {
        "anchor": "spoken language models",
        "search_term": "data scarcity"
      },
      {
        "anchor": "synchronous dialogue",
        "search_term": "data scarcity"
      },
      {
        "anchor": "turn-taking",
        "search_term": "data scarcity"
      }
    ]
  },
  {
    "arxiv_id": "2502.08302",
    "anchor": "synchronous dialogue",
    "search_term": "discrete tokens",
    "search_record": {
      "id": "http://arxiv.org/abs/2502.08302v1",
      "title": "HDT: Hierarchical Discrete Transformer for Multivariate Time Series Forecasting",
      "summary": "Generative models have gained significant attention in multivariate time series forecasting (MTS), particularly due to their ability to generate high-fidelity samples. Forecasting the probability distribution of multivariate time series is a challenging yet practical task. Although some recent attempts have been made to handle this task, two major challenges persist: 1) some existing generative methods underperform in high-dimensional multivariate time series forecasting, which is hard to scale to higher dimensions; 2) the inherent high-dimensional multivariate attributes constrain the forecasting lengths of existing generative models. In this paper, we point out that discrete token representations can model high-dimensional MTS with faster inference time, and forecasting the target with long-term trends of itself can extend the forecasting length with high accuracy. Motivated by this, we propose a vector quantized framework called Hierarchical Discrete Transformer (HDT) that models time series into discrete token representations with l2 normalization enhanced vector quantized strategy, in which we transform the MTS forecasting into discrete tokens generation. To address the limitations of generative models in long-term forecasting, we propose a hierarchical discrete Transformer. This model captures the discrete long-term trend of the target at the low level and leverages this trend as a condition to generate the discrete representation of the target at the high level that introduces the features of the target itself to extend the forecasting length in high-dimensional MTS. Extensive experiments on five popular MTS datasets verify the effectiveness of our proposed method.",
      "published": "2025-02-12T11:03:51Z"
    },
    "metadata": {
      "arxiv_id": "2502.08302",
      "title": "HDT: Hierarchical Discrete Transformer for Multivariate Time Series Forecasting",
      "summary": "Generative models have gained significant attention in multivariate time series forecasting (MTS), particularly due to their ability to generate high-fidelity samples. Forecasting the probability distribution of multivariate time series is a challenging yet practical task. Although some recent attempts have been made to handle this task, two major challenges persist: 1) some existing generative methods underperform in high-dimensional multivariate time series forecasting, which is hard to scale to higher dimensions; 2) the inherent high-dimensional multivariate attributes constrain the forecasting lengths of existing generative models. In this paper, we point out that discrete token representations can model high-dimensional MTS with faster inference time, and forecasting the target with long-term trends of itself can extend the forecasting length with high accuracy. Motivated by this, we propose a vector quantized framework called Hierarchical Discrete Transformer (HDT) that models time series into discrete token representations with l2 normalization enhanced vector quantized strategy, in which we transform the MTS forecasting into discrete tokens generation. To address the limitations of generative models in long-term forecasting, we propose a hierarchical discrete Transformer. This model captures the discrete long-term trend of the target at the low level and leverages this trend as a condition to generate the discrete representation of the target at the high level that introduces the features of the target itself to extend the forecasting length in high-dimensional MTS. Extensive experiments on five popular MTS datasets verify the effectiveness of our proposed method.",
      "authors": [
        "Shibo Feng",
        "Peilin Zhao",
        "Liu Liu",
        "Pengcheng Wu",
        "Zhiqi Shen"
      ],
      "published": "2025-02-12T11:03:51Z",
      "updated": "2025-02-12T11:03:51Z",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2502.08302v1",
      "landing_url": "https://arxiv.org/abs/2502.08302v1",
      "doi": "https://doi.org/10.48550/arXiv.2502.08302"
    },
    "queries": [
      {
        "anchor": "synchronous dialogue",
        "search_term": "discrete tokens"
      }
    ]
  },
  {
    "arxiv_id": "2502.08939",
    "anchor": "synchronous dialogue",
    "search_term": "audio tokens",
    "search_record": {
      "id": "http://arxiv.org/abs/2502.08939v1",
      "title": "TokenSynth: A Token-based Neural Synthesizer for Instrument Cloning and Text-to-Instrument",
      "summary": "Recent advancements in neural audio codecs have enabled the use of tokenized audio representations in various audio generation tasks, such as text-to-speech, text-to-audio, and text-to-music generation. Leveraging this approach, we propose TokenSynth, a novel neural synthesizer that utilizes a decoder-only transformer to generate desired audio tokens from MIDI tokens and CLAP (Contrastive Language-Audio Pretraining) embedding, which has timbre-related information. Our model is capable of performing instrument cloning, text-to-instrument synthesis, and text-guided timbre manipulation without any fine-tuning. This flexibility enables diverse sound design and intuitive timbre control. We evaluated the quality of the synthesized audio, the timbral similarity between synthesized and target audio/text, and synthesis accuracy (i.e., how accurately it follows the input MIDI) using objective measures. TokenSynth demonstrates the potential of leveraging advanced neural audio codecs and transformers to create powerful and versatile neural synthesizers. The source code, model weights, and audio demos are available at: https://github.com/KyungsuKim42/tokensynth",
      "published": "2025-02-13T03:40:30Z"
    },
    "metadata": {
      "arxiv_id": "2502.08939",
      "title": "TokenSynth: A Token-based Neural Synthesizer for Instrument Cloning and Text-to-Instrument",
      "summary": "Recent advancements in neural audio codecs have enabled the use of tokenized audio representations in various audio generation tasks, such as text-to-speech, text-to-audio, and text-to-music generation. Leveraging this approach, we propose TokenSynth, a novel neural synthesizer that utilizes a decoder-only transformer to generate desired audio tokens from MIDI tokens and CLAP (Contrastive Language-Audio Pretraining) embedding, which has timbre-related information. Our model is capable of performing instrument cloning, text-to-instrument synthesis, and text-guided timbre manipulation without any fine-tuning. This flexibility enables diverse sound design and intuitive timbre control. We evaluated the quality of the synthesized audio, the timbral similarity between synthesized and target audio/text, and synthesis accuracy (i.e., how accurately it follows the input MIDI) using objective measures. TokenSynth demonstrates the potential of leveraging advanced neural audio codecs and transformers to create powerful and versatile neural synthesizers. The source code, model weights, and audio demos are available at: https://github.com/KyungsuKim42/tokensynth",
      "authors": [
        "Kyungsu Kim",
        "Junghyun Koo",
        "Sungho Lee",
        "Haesun Joung",
        "Kyogu Lee"
      ],
      "published": "2025-02-13T03:40:30Z",
      "updated": "2025-02-13T03:40:30Z",
      "categories": [
        "cs.SD",
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2502.08939v1",
      "landing_url": "https://arxiv.org/abs/2502.08939v1",
      "doi": "https://doi.org/10.48550/arXiv.2502.08939"
    },
    "queries": [
      {
        "anchor": "synchronous dialogue",
        "search_term": "audio tokens"
      }
    ]
  },
  {
    "arxiv_id": "2502.08996",
    "anchor": "spoken language models",
    "search_term": "half duplex",
    "search_record": {
      "id": "http://arxiv.org/abs/2502.08996v2",
      "title": "Masked Modulation: High-Throughput Half-Duplex ISAC Transmission Waveform Design",
      "summary": "Integrated sensing and communication (ISAC) enables numerous innovative wireless applications. Communication-centric design is a practical choice for the construction of the sixth generation (6G) ISAC networks. Continuous-wave-based ISAC systems, with orthogonal frequency-division multiplexing (OFDM) being a representative example, suffer from the self-interference (SI) problem, and hence are less suitable for long-range sensing. On the other hand, pulse-based half-duplex ISAC systems are free of SI, but are also less favourable for high-throughput communication scenarios.\n  In this treatise, we propose MASked Modulation (MASM), a half-duplex ISAC waveform design scheme, which minimises a range blindness metric, termed as \"mainlobe fluctuation\", given a duty cycle (proportional to communication throughput) constraint. In particular, MASM is capable of supporting high-throughput communication ($\\sim$50% duty cycle) under mild mainlobe fluctuation. Moreover, MASM can be flexibly adapted to frame-level waveform designs by operating on the slow-time scale. In terms of optimal transmit mask design, a set of masks is shown to be ideal in the sense of sidelobe level and mainlobe fluctuation intensity.",
      "published": "2025-02-13T06:20:39Z"
    },
    "metadata": {
      "arxiv_id": "2502.08996",
      "title": "Masked Modulation: High-Throughput Half-Duplex ISAC Transmission Waveform Design",
      "summary": "Integrated sensing and communication (ISAC) enables numerous innovative wireless applications. Communication-centric design is a practical choice for the construction of the sixth generation (6G) ISAC networks. Continuous-wave-based ISAC systems, with orthogonal frequency-division multiplexing (OFDM) being a representative example, suffer from the self-interference (SI) problem, and hence are less suitable for long-range sensing. On the other hand, pulse-based half-duplex ISAC systems are free of SI, but are also less favourable for high-throughput communication scenarios.\n  In this treatise, we propose MASked Modulation (MASM), a half-duplex ISAC waveform design scheme, which minimises a range blindness metric, termed as \"mainlobe fluctuation\", given a duty cycle (proportional to communication throughput) constraint. In particular, MASM is capable of supporting high-throughput communication ($\\sim$50% duty cycle) under mild mainlobe fluctuation. Moreover, MASM can be flexibly adapted to frame-level waveform designs by operating on the slow-time scale. In terms of optimal transmit mask design, a set of masks is shown to be ideal in the sense of sidelobe level and mainlobe fluctuation intensity.",
      "authors": [
        "Yifeng Xiong",
        "Junsheng Mu",
        "Shuangyang Li",
        "Marco Lops",
        "Jianhua Zhang"
      ],
      "published": "2025-02-13T06:20:39Z",
      "updated": "2025-05-25T09:27:30Z",
      "categories": [
        "cs.IT"
      ],
      "pdf_url": "https://arxiv.org/pdf/2502.08996v2",
      "landing_url": "https://arxiv.org/abs/2502.08996v2",
      "doi": "https://doi.org/10.48550/arXiv.2502.08996"
    },
    "queries": [
      {
        "anchor": "spoken language models",
        "search_term": "half duplex"
      },
      {
        "anchor": "synchronous dialogue",
        "search_term": "half duplex"
      },
      {
        "anchor": "turn-taking",
        "search_term": "half duplex"
      }
    ]
  },
  {
    "arxiv_id": "2502.09282",
    "anchor": "synchronous dialogue",
    "search_term": "streaming architecture",
    "search_record": {
      "id": "http://arxiv.org/abs/2502.09282v4",
      "title": "MsEdF: A Multi-stream Encoder-decoder Framework for Remote Sensing Image Captioning",
      "summary": "Remote sensing images contain complex spatial patterns and semantic structures, which makes the captioning model difficult to accurately describe. Encoder-decoder architectures have become the widely used approach for RSIC by translating visual content into descriptive text. However, many existing methods rely on a single-stream architecture, which weakens the model to accurately describe the image. Such single-stream architectures typically struggle to extract diverse spatial features or capture complex semantic relationships, limiting their effectiveness in scenes with high intraclass similarity or contextual ambiguity. In this work, we propose a novel Multi-stream Encoder-decoder Framework (MsEdF) which improves the performance of RSIC by optimizing both the spatial representation and language generation of encoder-decoder architecture. The encoder fuses information from two complementary image encoders, thereby promoting feature diversity through the integration of multiscale and structurally distinct cues. To improve the capture of context-aware descriptions, we refine the input sequence's semantic modeling on the decoder side using a stacked GRU architecture with an element-wise aggregation scheme. Experiments on three benchmark RSIC datasets show that MsEdF outperforms several baseline models.",
      "published": "2025-02-13T12:54:13Z"
    },
    "metadata": {
      "arxiv_id": "2502.09282",
      "title": "MsEdF: A Multi-stream Encoder-decoder Framework for Remote Sensing Image Captioning",
      "summary": "Remote sensing images contain complex spatial patterns and semantic structures, which makes the captioning model difficult to accurately describe. Encoder-decoder architectures have become the widely used approach for RSIC by translating visual content into descriptive text. However, many existing methods rely on a single-stream architecture, which weakens the model to accurately describe the image. Such single-stream architectures typically struggle to extract diverse spatial features or capture complex semantic relationships, limiting their effectiveness in scenes with high intraclass similarity or contextual ambiguity. In this work, we propose a novel Multi-stream Encoder-decoder Framework (MsEdF) which improves the performance of RSIC by optimizing both the spatial representation and language generation of encoder-decoder architecture. The encoder fuses information from two complementary image encoders, thereby promoting feature diversity through the integration of multiscale and structurally distinct cues. To improve the capture of context-aware descriptions, we refine the input sequence's semantic modeling on the decoder side using a stacked GRU architecture with an element-wise aggregation scheme. Experiments on three benchmark RSIC datasets show that MsEdF outperforms several baseline models.",
      "authors": [
        "Swadhin Das",
        "Raksha Sharma"
      ],
      "published": "2025-02-13T12:54:13Z",
      "updated": "2025-10-28T04:40:41Z",
      "categories": [
        "cs.CV",
        "cs.HC",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2502.09282v4",
      "landing_url": "https://arxiv.org/abs/2502.09282v4",
      "doi": "https://doi.org/10.48550/arXiv.2502.09282"
    },
    "queries": [
      {
        "anchor": "synchronous dialogue",
        "search_term": "streaming architecture"
      }
    ]
  },
  {
    "arxiv_id": "2502.09577",
    "anchor": "turn-taking",
    "search_term": "true full-duplex",
    "search_record": {
      "id": "http://arxiv.org/abs/2502.09577v2",
      "title": "Polymind: Parallel Visual Diagramming with Large Language Models to Support Prewriting Through Microtasks",
      "summary": "Prewriting is the process of generating and organising ideas before a first draft. It consists of a combination of informal, iterative, and semi-structured strategies such as visual diagramming, which poses a challenge for collaborating with large language models (LLMs) in a turn-taking conversational manner. We present Polymind, a visual diagramming tool that leverages multiple LLM-powered agents to support prewriting. The system features a parallel collaboration workflow in place of the turn-taking conversational interactions. It defines multiple ``microtasks'' to simulate group collaboration scenarios such as collaborative writing and group brainstorming. Instead of repetitively prompting a chatbot for various purposes, Polymind enables users to orchestrate multiple microtasks simultaneously. Users can configure and delegate customised microtasks, and manage their microtasks by specifying task requirements and toggling visibility and initiative. Our evaluation revealed that, compared to ChatGPT, users had more customizability over collaboration with Polymind, and were thus able to quickly expand personalised writing ideas during prewriting.",
      "published": "2025-02-13T18:34:52Z"
    },
    "metadata": {
      "arxiv_id": "2502.09577",
      "title": "Polymind: Parallel Visual Diagramming with Large Language Models to Support Prewriting Through Microtasks",
      "summary": "Prewriting is the process of generating and organising ideas before a first draft. It consists of a combination of informal, iterative, and semi-structured strategies such as visual diagramming, which poses a challenge for collaborating with large language models (LLMs) in a turn-taking conversational manner. We present Polymind, a visual diagramming tool that leverages multiple LLM-powered agents to support prewriting. The system features a parallel collaboration workflow in place of the turn-taking conversational interactions. It defines multiple ``microtasks'' to simulate group collaboration scenarios such as collaborative writing and group brainstorming. Instead of repetitively prompting a chatbot for various purposes, Polymind enables users to orchestrate multiple microtasks simultaneously. Users can configure and delegate customised microtasks, and manage their microtasks by specifying task requirements and toggling visibility and initiative. Our evaluation revealed that, compared to ChatGPT, users had more customizability over collaboration with Polymind, and were thus able to quickly expand personalised writing ideas during prewriting.",
      "authors": [
        "Qian Wan",
        "Jiannan Li",
        "Huanchen Wang",
        "Zhicong Lu"
      ],
      "published": "2025-02-13T18:34:52Z",
      "updated": "2025-08-12T12:49:59Z",
      "categories": [
        "cs.HC"
      ],
      "pdf_url": "https://arxiv.org/pdf/2502.09577v2",
      "landing_url": "https://arxiv.org/abs/2502.09577v2",
      "doi": "https://doi.org/10.48550/arXiv.2502.09577"
    },
    "queries": [
      {
        "anchor": "turn-taking",
        "search_term": "true full-duplex"
      },
      {
        "anchor": "turn-taking",
        "search_term": "pseudo full-duplex"
      },
      {
        "anchor": "turn-taking",
        "search_term": "engineered synchronization"
      },
      {
        "anchor": "turn-taking",
        "search_term": "learned synchronization"
      },
      {
        "anchor": "turn-taking",
        "search_term": "modular architecture"
      },
      {
        "anchor": "turn-taking",
        "search_term": "streaming architecture"
      },
      {
        "anchor": "turn-taking",
        "search_term": "real-time interaction"
      },
      {
        "anchor": "turn-taking",
        "search_term": "synchronous data"
      },
      {
        "anchor": "turn-taking",
        "search_term": "multichannel dialogue"
      },
      {
        "anchor": "turn-taking",
        "search_term": "behavioral arbitration"
      },
      {
        "anchor": "turn-taking",
        "search_term": "semantic coherence"
      },
      {
        "anchor": "turn-taking",
        "search_term": "floor transfer offset"
      },
      {
        "anchor": "turn-taking",
        "search_term": "stop latency"
      },
      {
        "anchor": "turn-taking",
        "search_term": "interruption response delay"
      }
    ]
  },
  {
    "arxiv_id": "2502.11882",
    "anchor": "synchronous dialogue",
    "search_term": "real-time interaction",
    "search_record": {
      "id": "http://arxiv.org/abs/2502.11882v5",
      "title": "Leveraging Dual Process Theory in Language Agent Framework for Real-time Simultaneous Human-AI Collaboration",
      "summary": "Agents built on large language models (LLMs) have excelled in turn-by-turn human-AI collaboration but struggle with simultaneous tasks requiring real-time interaction. Latency issues and the challenge of inferring variable human strategies hinder their ability to make autonomous decisions without explicit instructions. Through experiments with current independent System 1 and System 2 methods, we validate the necessity of using Dual Process Theory (DPT) in real-time tasks. We propose DPT-Agent, a novel language agent framework that integrates System 1 and System 2 for efficient real-time simultaneous human-AI collaboration. DPT-Agent's System 1 uses a Finite-state Machine (FSM) and code-as-policy for fast, intuitive, and controllable decision-making. DPT-Agent's System 2 integrates Theory of Mind (ToM) and asynchronous reflection to infer human intentions and perform reasoning-based autonomous decisions. We demonstrate the effectiveness of DPT-Agent through further experiments with rule-based agents and human collaborators, showing significant improvements over mainstream LLM-based frameworks. DPT-Agent can effectively help LLMs convert correct slow thinking and reasoning into executable actions, thereby improving performance. To the best of our knowledge, DPT-Agent is the first language agent framework that achieves successful real-time simultaneous human-AI collaboration autonomously. Code of DPT-Agent can be found in https://github.com/sjtu-marl/DPT-Agent.",
      "published": "2025-02-17T15:09:45Z"
    },
    "metadata": {
      "arxiv_id": "2502.11882",
      "title": "Leveraging Dual Process Theory in Language Agent Framework for Real-time Simultaneous Human-AI Collaboration",
      "summary": "Agents built on large language models (LLMs) have excelled in turn-by-turn human-AI collaboration but struggle with simultaneous tasks requiring real-time interaction. Latency issues and the challenge of inferring variable human strategies hinder their ability to make autonomous decisions without explicit instructions. Through experiments with current independent System 1 and System 2 methods, we validate the necessity of using Dual Process Theory (DPT) in real-time tasks. We propose DPT-Agent, a novel language agent framework that integrates System 1 and System 2 for efficient real-time simultaneous human-AI collaboration. DPT-Agent's System 1 uses a Finite-state Machine (FSM) and code-as-policy for fast, intuitive, and controllable decision-making. DPT-Agent's System 2 integrates Theory of Mind (ToM) and asynchronous reflection to infer human intentions and perform reasoning-based autonomous decisions. We demonstrate the effectiveness of DPT-Agent through further experiments with rule-based agents and human collaborators, showing significant improvements over mainstream LLM-based frameworks. DPT-Agent can effectively help LLMs convert correct slow thinking and reasoning into executable actions, thereby improving performance. To the best of our knowledge, DPT-Agent is the first language agent framework that achieves successful real-time simultaneous human-AI collaboration autonomously. Code of DPT-Agent can be found in https://github.com/sjtu-marl/DPT-Agent.",
      "authors": [
        "Shao Zhang",
        "Xihuai Wang",
        "Wenhao Zhang",
        "Chaoran Li",
        "Junru Song",
        "Tingyu Li",
        "Lin Qiu",
        "Xuezhi Cao",
        "Xunliang Cai",
        "Wen Yao",
        "Weinan Zhang",
        "Xinbing Wang",
        "Ying Wen"
      ],
      "published": "2025-02-17T15:09:45Z",
      "updated": "2025-05-28T12:14:14Z",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.HC",
        "cs.LG",
        "cs.MA"
      ],
      "pdf_url": "https://arxiv.org/pdf/2502.11882v5",
      "landing_url": "https://arxiv.org/abs/2502.11882v5",
      "doi": "https://doi.org/10.48550/arXiv.2502.11882"
    },
    "queries": [
      {
        "anchor": "synchronous dialogue",
        "search_term": "real-time interaction"
      }
    ]
  },
  {
    "arxiv_id": "2502.11901",
    "anchor": "spoken language models",
    "search_term": "data scarcity",
    "search_record": {
      "id": "http://arxiv.org/abs/2502.11901v2",
      "title": "Building A Proof-Oriented Programmer That Is 64% Better Than GPT-4o Under Data Scarcity",
      "summary": "Existing LMs struggle with proof-oriented programming due to data scarcity, which manifest in two key ways: (1) a lack of sufficient corpora for proof-oriented programming languages such as F*, and (2) the absence of large-scale, project-level proof-oriented implementations that can teach the model the intricate reasoning process when performing proof-oriented programming. We present the first on synthetic data augmentation for project level proof oriented programming for both generation and repair. Our method addresses data scarcity by synthesizing basic proof-oriented programming problems for proficiency in that language; incorporating diverse coding data for reasoning capability elicitation and creating new proofs and repair data within existing repositories. This approach enables language models to both synthesize and repair proofs for function- and repository-level code. We show that our fine-tuned 14B parameter model, PoPilot, can exceed the performance of the models that outperforms GPT-4o in project-level proof-oriented programming by 64% relative margin, and can improve GPT-4o's performance by 54% by repairing its outputs over GPT-4o's self-repair.",
      "published": "2025-02-17T15:24:11Z"
    },
    "metadata": {
      "arxiv_id": "2502.11901",
      "title": "Building A Proof-Oriented Programmer That Is 64% Better Than GPT-4o Under Data Scarcity",
      "summary": "Existing LMs struggle with proof-oriented programming due to data scarcity, which manifest in two key ways: (1) a lack of sufficient corpora for proof-oriented programming languages such as F*, and (2) the absence of large-scale, project-level proof-oriented implementations that can teach the model the intricate reasoning process when performing proof-oriented programming. We present the first on synthetic data augmentation for project level proof oriented programming for both generation and repair. Our method addresses data scarcity by synthesizing basic proof-oriented programming problems for proficiency in that language; incorporating diverse coding data for reasoning capability elicitation and creating new proofs and repair data within existing repositories. This approach enables language models to both synthesize and repair proofs for function- and repository-level code. We show that our fine-tuned 14B parameter model, PoPilot, can exceed the performance of the models that outperforms GPT-4o in project-level proof-oriented programming by 64% relative margin, and can improve GPT-4o's performance by 54% by repairing its outputs over GPT-4o's self-repair.",
      "authors": [
        "Dylan Zhang",
        "Justin Wang",
        "Tianran Sun"
      ],
      "published": "2025-02-17T15:24:11Z",
      "updated": "2025-04-13T23:38:44Z",
      "categories": [
        "cs.CL",
        "cs.PL",
        "cs.SE"
      ],
      "pdf_url": "https://arxiv.org/pdf/2502.11901v2",
      "landing_url": "https://arxiv.org/abs/2502.11901v2",
      "doi": "https://doi.org/10.48550/arXiv.2502.11901"
    },
    "queries": [
      {
        "anchor": "spoken language models",
        "search_term": "data scarcity"
      },
      {
        "anchor": "synchronous dialogue",
        "search_term": "data scarcity"
      },
      {
        "anchor": "turn-taking",
        "search_term": "data scarcity"
      }
    ]
  },
  {
    "arxiv_id": "2502.12369",
    "anchor": "spoken language models",
    "search_term": "synchronous data",
    "search_record": {
      "id": "http://arxiv.org/abs/2502.12369v2",
      "title": "An a posteriori data-driven method for phase-averaged optical measurements",
      "summary": "Phase-averaging is a fundamental approach for investigating periodic and non-stationary phenomena. In fluid dynamics, these can be generated by rotating blades such as propellers/turbines or by pulsed jets. Traditional phase-averaging approaches often rely on synchronized data acquisition systems, which might require high-speed cameras, light sources, and precise delay generators and encoders, making them expensive and sometimes unfeasible. This work proposes an a posteriori data-driven approach that reconstructs phase information from randomly acquired uncorrelated photographic frames (snapshots) using the ISOMAP algorithm. The technique enables accurate reordering of snapshots in the phase space and subsequent computation of the phase-averaged flow field without the need for synchronization. The framework was validated through numerical simulations and experimental fluid dynamics datasets from an optical setup featuring single- and multi-propeller configurations. The results demonstrate that the proposed method effectively captures the periodic flow characteristics while addressing the challenges related to synchronization and hardware limitations. Furthermore, the ability to apply this technique to archival datasets extends its applicability to a wide range of experimental fluid dynamics studies. This approach provides a scalable and cost-effective alternative to traditional methods for the analysis of periodic phenomena.",
      "published": "2025-02-17T23:19:45Z"
    },
    "metadata": {
      "arxiv_id": "2502.12369",
      "title": "An a posteriori data-driven method for phase-averaged optical measurements",
      "summary": "Phase-averaging is a fundamental approach for investigating periodic and non-stationary phenomena. In fluid dynamics, these can be generated by rotating blades such as propellers/turbines or by pulsed jets. Traditional phase-averaging approaches often rely on synchronized data acquisition systems, which might require high-speed cameras, light sources, and precise delay generators and encoders, making them expensive and sometimes unfeasible. This work proposes an a posteriori data-driven approach that reconstructs phase information from randomly acquired uncorrelated photographic frames (snapshots) using the ISOMAP algorithm. The technique enables accurate reordering of snapshots in the phase space and subsequent computation of the phase-averaged flow field without the need for synchronization. The framework was validated through numerical simulations and experimental fluid dynamics datasets from an optical setup featuring single- and multi-propeller configurations. The results demonstrate that the proposed method effectively captures the periodic flow characteristics while addressing the challenges related to synchronization and hardware limitations. Furthermore, the ability to apply this technique to archival datasets extends its applicability to a wide range of experimental fluid dynamics studies. This approach provides a scalable and cost-effective alternative to traditional methods for the analysis of periodic phenomena.",
      "authors": [
        "Enrico Amico",
        "Sara Montagner",
        "Jacopo Serpieri",
        "Gioacchino Cafiero"
      ],
      "published": "2025-02-17T23:19:45Z",
      "updated": "2025-02-19T06:07:01Z",
      "categories": [
        "physics.flu-dyn",
        "physics.data-an"
      ],
      "pdf_url": "https://arxiv.org/pdf/2502.12369v2",
      "landing_url": "https://arxiv.org/abs/2502.12369v2",
      "doi": "https://doi.org/10.48550/arXiv.2502.12369"
    },
    "queries": [
      {
        "anchor": "spoken language models",
        "search_term": "synchronous data"
      },
      {
        "anchor": "synchronous dialogue",
        "search_term": "synchronous data"
      }
    ]
  },
  {
    "arxiv_id": "2502.12448",
    "anchor": "spoken language models",
    "search_term": "discrete tokens",
    "search_record": {
      "id": "http://arxiv.org/abs/2502.12448v1",
      "title": "From Principles to Applications: A Comprehensive Survey of Discrete Tokenizers in Generation, Comprehension, Recommendation, and Information Retrieval",
      "summary": "Discrete tokenizers have emerged as indispensable components in modern machine learning systems, particularly within the context of autoregressive modeling and large language models (LLMs). These tokenizers serve as the critical interface that transforms raw, unstructured data from diverse modalities into discrete tokens, enabling LLMs to operate effectively across a wide range of tasks. Despite their central role in generation, comprehension, and recommendation systems, a comprehensive survey dedicated to discrete tokenizers remains conspicuously absent in the literature. This paper addresses this gap by providing a systematic review of the design principles, applications, and challenges of discrete tokenizers. We begin by dissecting the sub-modules of tokenizers and systematically demonstrate their internal mechanisms to provide a comprehensive understanding of their functionality and design. Building on this foundation, we synthesize state-of-the-art methods, categorizing them into multimodal generation and comprehension tasks, and semantic tokens for personalized recommendations. Furthermore, we critically analyze the limitations of existing tokenizers and outline promising directions for future research. By presenting a unified framework for understanding discrete tokenizers, this survey aims to guide researchers and practitioners in addressing open challenges and advancing the field, ultimately contributing to the development of more robust and versatile AI systems.",
      "published": "2025-02-18T02:29:51Z"
    },
    "metadata": {
      "arxiv_id": "2502.12448",
      "title": "From Principles to Applications: A Comprehensive Survey of Discrete Tokenizers in Generation, Comprehension, Recommendation, and Information Retrieval",
      "summary": "Discrete tokenizers have emerged as indispensable components in modern machine learning systems, particularly within the context of autoregressive modeling and large language models (LLMs). These tokenizers serve as the critical interface that transforms raw, unstructured data from diverse modalities into discrete tokens, enabling LLMs to operate effectively across a wide range of tasks. Despite their central role in generation, comprehension, and recommendation systems, a comprehensive survey dedicated to discrete tokenizers remains conspicuously absent in the literature. This paper addresses this gap by providing a systematic review of the design principles, applications, and challenges of discrete tokenizers. We begin by dissecting the sub-modules of tokenizers and systematically demonstrate their internal mechanisms to provide a comprehensive understanding of their functionality and design. Building on this foundation, we synthesize state-of-the-art methods, categorizing them into multimodal generation and comprehension tasks, and semantic tokens for personalized recommendations. Furthermore, we critically analyze the limitations of existing tokenizers and outline promising directions for future research. By presenting a unified framework for understanding discrete tokenizers, this survey aims to guide researchers and practitioners in addressing open challenges and advancing the field, ultimately contributing to the development of more robust and versatile AI systems.",
      "authors": [
        "Jian Jia",
        "Jingtong Gao",
        "Ben Xue",
        "Junhao Wang",
        "Qingpeng Cai",
        "Quan Chen",
        "Xiangyu Zhao",
        "Peng Jiang",
        "Kun Gai"
      ],
      "published": "2025-02-18T02:29:51Z",
      "updated": "2025-02-18T02:29:51Z",
      "categories": [
        "cs.IR"
      ],
      "pdf_url": "https://arxiv.org/pdf/2502.12448v1",
      "landing_url": "https://arxiv.org/abs/2502.12448v1",
      "doi": "https://doi.org/10.48550/arXiv.2502.12448"
    },
    "queries": [
      {
        "anchor": "spoken language models",
        "search_term": "discrete tokens"
      },
      {
        "anchor": "synchronous dialogue",
        "search_term": "discrete tokens"
      },
      {
        "anchor": "turn-taking",
        "search_term": "discrete tokens"
      }
    ]
  },
  {
    "arxiv_id": "2502.13472",
    "anchor": "full-duplex",
    "search_term": "true full-duplex",
    "search_record": {
      "id": "http://arxiv.org/abs/2502.13472v2",
      "title": "FlexDuo: A Pluggable System for Enabling Full-Duplex Capabilities in Speech Dialogue Systems",
      "summary": "Full-Duplex Speech Dialogue Systems (Full-Duplex SDS) have significantly enhanced the naturalness of human-machine interaction by enabling real-time bidirectional communication. However, existing approaches face challenges such as difficulties in independent module optimization and contextual noise interference due to highly coupled architectural designs and oversimplified binary state modeling. This paper proposes FlexDuo, a flexible full-duplex control module that decouples duplex control from spoken dialogue systems through a plug-and-play architectural design. Furthermore, inspired by human information-filtering mechanisms in conversations, we introduce an explicit Idle state. On one hand, the Idle state filters redundant noise and irrelevant audio to enhance dialogue quality. On the other hand, it establishes a semantic integrity-based buffering mechanism, reducing the risk of mutual interruptions while ensuring accurate response transitions. Experimental results on the Fisher corpus demonstrate that FlexDuo reduces the false interruption rate by 24.9% and improves response accuracy by 7.6% compared to integrated full-duplex dialogue system baselines. It also outperforms voice activity detection (VAD) controlled baseline systems in both Chinese and English dialogue quality. The proposed modular architecture and state-based dialogue model provide a novel technical pathway for building flexible and efficient duplex dialogue systems.",
      "published": "2025-02-19T06:51:34Z"
    },
    "metadata": {
      "arxiv_id": "2502.13472",
      "title": "FlexDuo: A Pluggable System for Enabling Full-Duplex Capabilities in Speech Dialogue Systems",
      "summary": "Full-Duplex Speech Dialogue Systems (Full-Duplex SDS) have significantly enhanced the naturalness of human-machine interaction by enabling real-time bidirectional communication. However, existing approaches face challenges such as difficulties in independent module optimization and contextual noise interference due to highly coupled architectural designs and oversimplified binary state modeling. This paper proposes FlexDuo, a flexible full-duplex control module that decouples duplex control from spoken dialogue systems through a plug-and-play architectural design. Furthermore, inspired by human information-filtering mechanisms in conversations, we introduce an explicit Idle state. On one hand, the Idle state filters redundant noise and irrelevant audio to enhance dialogue quality. On the other hand, it establishes a semantic integrity-based buffering mechanism, reducing the risk of mutual interruptions while ensuring accurate response transitions. Experimental results on the Fisher corpus demonstrate that FlexDuo reduces the false interruption rate by 24.9% and improves response accuracy by 7.6% compared to integrated full-duplex dialogue system baselines. It also outperforms voice activity detection (VAD) controlled baseline systems in both Chinese and English dialogue quality. The proposed modular architecture and state-based dialogue model provide a novel technical pathway for building flexible and efficient duplex dialogue systems.",
      "authors": [
        "Borui Liao",
        "Yulong Xu",
        "Jiao Ou",
        "Kaiyuan Yang",
        "Weihua Jian",
        "Pengfei Wan",
        "Di Zhang"
      ],
      "published": "2025-02-19T06:51:34Z",
      "updated": "2025-05-29T03:32:21Z",
      "categories": [
        "cs.CL",
        "cs.HC"
      ],
      "pdf_url": "https://arxiv.org/pdf/2502.13472v2",
      "landing_url": "https://arxiv.org/abs/2502.13472v2",
      "doi": "https://doi.org/10.48550/arXiv.2502.13472"
    },
    "queries": [
      {
        "anchor": "full-duplex",
        "search_term": "true full-duplex"
      },
      {
        "anchor": "full-duplex",
        "search_term": "pseudo full-duplex"
      },
      {
        "anchor": "full-duplex",
        "search_term": "engineered synchronization"
      },
      {
        "anchor": "full-duplex",
        "search_term": "learned synchronization"
      },
      {
        "anchor": "full-duplex",
        "search_term": "modular architecture"
      },
      {
        "anchor": "full-duplex",
        "search_term": "streaming architecture"
      },
      {
        "anchor": "full-duplex",
        "search_term": "real-time interaction"
      },
      {
        "anchor": "full-duplex",
        "search_term": "low latency"
      },
      {
        "anchor": "full-duplex",
        "search_term": "synchronous data"
      },
      {
        "anchor": "full-duplex",
        "search_term": "data scarcity"
      },
      {
        "anchor": "full-duplex",
        "search_term": "multichannel dialogue"
      },
      {
        "anchor": "full-duplex",
        "search_term": "temporal dynamics"
      },
      {
        "anchor": "full-duplex",
        "search_term": "behavioral arbitration"
      },
      {
        "anchor": "full-duplex",
        "search_term": "semantic coherence"
      },
      {
        "anchor": "full-duplex",
        "search_term": "neural codec"
      },
      {
        "anchor": "full-duplex",
        "search_term": "discrete tokens"
      },
      {
        "anchor": "full-duplex",
        "search_term": "audio tokens"
      },
      {
        "anchor": "full-duplex",
        "search_term": "floor transfer offset"
      },
      {
        "anchor": "full-duplex",
        "search_term": "stop latency"
      },
      {
        "anchor": "full-duplex",
        "search_term": "interruption response delay"
      }
    ]
  },
  {
    "arxiv_id": "2502.14145",
    "anchor": "full-duplex",
    "search_term": "true full-duplex",
    "search_record": {
      "id": "http://arxiv.org/abs/2502.14145v2",
      "title": "LLM-Enhanced Dialogue Management for Full-Duplex Spoken Dialogue Systems",
      "summary": "Achieving full-duplex communication in spoken dialogue systems (SDS) requires real-time coordination between listening, speaking, and thinking. This paper proposes a semantic voice activity detection (VAD) module as a dialogue manager (DM) to efficiently manage turn-taking in full-duplex SDS. Implemented as a lightweight (0.5B) LLM fine-tuned on full-duplex conversation data, the semantic VAD predicts four control tokens to regulate turn-switching and turn-keeping, distinguishing between intentional and unintentional barge-ins while detecting query completion for handling user pauses and hesitations. By processing input speech in short intervals, the semantic VAD enables real-time decision-making, while the core dialogue engine (CDE) is only activated for response generation, reducing computational overhead. This design allows independent DM optimization without retraining the CDE, balancing interaction accuracy and inference efficiency for scalable, next-generation full-duplex SDS.",
      "published": "2025-02-19T23:15:13Z"
    },
    "metadata": {
      "arxiv_id": "2502.14145",
      "title": "LLM-Enhanced Dialogue Management for Full-Duplex Spoken Dialogue Systems",
      "summary": "Achieving full-duplex communication in spoken dialogue systems (SDS) requires real-time coordination between listening, speaking, and thinking. This paper proposes a semantic voice activity detection (VAD) module as a dialogue manager (DM) to efficiently manage turn-taking in full-duplex SDS. Implemented as a lightweight (0.5B) LLM fine-tuned on full-duplex conversation data, the semantic VAD predicts four control tokens to regulate turn-switching and turn-keeping, distinguishing between intentional and unintentional barge-ins while detecting query completion for handling user pauses and hesitations. By processing input speech in short intervals, the semantic VAD enables real-time decision-making, while the core dialogue engine (CDE) is only activated for response generation, reducing computational overhead. This design allows independent DM optimization without retraining the CDE, balancing interaction accuracy and inference efficiency for scalable, next-generation full-duplex SDS.",
      "authors": [
        "Hao Zhang",
        "Weiwei Li",
        "Rilin Chen",
        "Vinay Kothapally",
        "Meng Yu",
        "Dong Yu"
      ],
      "published": "2025-02-19T23:15:13Z",
      "updated": "2025-02-24T19:08:11Z",
      "categories": [
        "cs.CL",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2502.14145v2",
      "landing_url": "https://arxiv.org/abs/2502.14145v2",
      "doi": "https://doi.org/10.48550/arXiv.2502.14145"
    },
    "queries": [
      {
        "anchor": "full-duplex",
        "search_term": "true full-duplex"
      },
      {
        "anchor": "full-duplex",
        "search_term": "pseudo full-duplex"
      },
      {
        "anchor": "full-duplex",
        "search_term": "engineered synchronization"
      },
      {
        "anchor": "full-duplex",
        "search_term": "learned synchronization"
      },
      {
        "anchor": "full-duplex",
        "search_term": "modular architecture"
      },
      {
        "anchor": "full-duplex",
        "search_term": "streaming architecture"
      },
      {
        "anchor": "full-duplex",
        "search_term": "real-time interaction"
      },
      {
        "anchor": "full-duplex",
        "search_term": "low latency"
      },
      {
        "anchor": "full-duplex",
        "search_term": "synchronous data"
      },
      {
        "anchor": "full-duplex",
        "search_term": "data scarcity"
      },
      {
        "anchor": "full-duplex",
        "search_term": "multichannel dialogue"
      },
      {
        "anchor": "full-duplex",
        "search_term": "temporal dynamics"
      },
      {
        "anchor": "full-duplex",
        "search_term": "behavioral arbitration"
      },
      {
        "anchor": "full-duplex",
        "search_term": "semantic coherence"
      },
      {
        "anchor": "full-duplex",
        "search_term": "neural codec"
      },
      {
        "anchor": "full-duplex",
        "search_term": "discrete tokens"
      },
      {
        "anchor": "full-duplex",
        "search_term": "audio tokens"
      },
      {
        "anchor": "full-duplex",
        "search_term": "floor transfer offset"
      },
      {
        "anchor": "full-duplex",
        "search_term": "stop latency"
      },
      {
        "anchor": "full-duplex",
        "search_term": "interruption response delay"
      },
      {
        "anchor": "turn-taking",
        "search_term": "true full-duplex"
      },
      {
        "anchor": "turn-taking",
        "search_term": "pseudo full-duplex"
      },
      {
        "anchor": "turn-taking",
        "search_term": "engineered synchronization"
      },
      {
        "anchor": "turn-taking",
        "search_term": "learned synchronization"
      },
      {
        "anchor": "turn-taking",
        "search_term": "synchronous data"
      },
      {
        "anchor": "turn-taking",
        "search_term": "multichannel dialogue"
      },
      {
        "anchor": "turn-taking",
        "search_term": "behavioral arbitration"
      },
      {
        "anchor": "turn-taking",
        "search_term": "floor transfer offset"
      },
      {
        "anchor": "turn-taking",
        "search_term": "stop latency"
      },
      {
        "anchor": "turn-taking",
        "search_term": "interruption response delay"
      }
    ]
  },
  {
    "arxiv_id": "2502.15466",
    "anchor": "spoken language models",
    "search_term": "data scarcity",
    "search_record": {
      "id": "http://arxiv.org/abs/2502.15466v1",
      "title": "Mitigating Data Scarcity in Time Series Analysis: A Foundation Model with Series-Symbol Data Generation",
      "summary": "Foundation models for time series analysis (TSA) have attracted significant attention. However, challenges such as data scarcity and data imbalance continue to hinder their development. To address this, we consider modeling complex systems through symbolic expressions that serve as semantic descriptors of time series. Building on this concept, we introduce a series-symbol (S2) dual-modulity data generation mechanism, enabling the unrestricted creation of high-quality time series data paired with corresponding symbolic representations. Leveraging the S2 dataset, we develop SymTime, a pre-trained foundation model for TSA. SymTime demonstrates competitive performance across five major TSA tasks when fine-tuned with downstream task, rivaling foundation models pre-trained on real-world datasets. This approach underscores the potential of dual-modality data generation and pretraining mechanisms in overcoming data scarcity and enhancing task performance.",
      "published": "2025-02-21T13:43:24Z"
    },
    "metadata": {
      "arxiv_id": "2502.15466",
      "title": "Mitigating Data Scarcity in Time Series Analysis: A Foundation Model with Series-Symbol Data Generation",
      "summary": "Foundation models for time series analysis (TSA) have attracted significant attention. However, challenges such as data scarcity and data imbalance continue to hinder their development. To address this, we consider modeling complex systems through symbolic expressions that serve as semantic descriptors of time series. Building on this concept, we introduce a series-symbol (S2) dual-modulity data generation mechanism, enabling the unrestricted creation of high-quality time series data paired with corresponding symbolic representations. Leveraging the S2 dataset, we develop SymTime, a pre-trained foundation model for TSA. SymTime demonstrates competitive performance across five major TSA tasks when fine-tuned with downstream task, rivaling foundation models pre-trained on real-world datasets. This approach underscores the potential of dual-modality data generation and pretraining mechanisms in overcoming data scarcity and enhancing task performance.",
      "authors": [
        "Wenxuan Wang",
        "Kai Wu",
        "Yujian Betterest Li",
        "Dan Wang",
        "Xiaoyu Zhang",
        "Jing Liu"
      ],
      "published": "2025-02-21T13:43:24Z",
      "updated": "2025-02-21T13:43:24Z",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2502.15466v1",
      "landing_url": "https://arxiv.org/abs/2502.15466v1",
      "doi": "https://doi.org/10.48550/arXiv.2502.15466"
    },
    "queries": [
      {
        "anchor": "spoken language models",
        "search_term": "data scarcity"
      },
      {
        "anchor": "synchronous dialogue",
        "search_term": "data scarcity"
      },
      {
        "anchor": "turn-taking",
        "search_term": "data scarcity"
      }
    ]
  },
  {
    "arxiv_id": "2502.16162",
    "anchor": "synchronous dialogue",
    "search_term": "data scarcity",
    "search_record": {
      "id": "http://arxiv.org/abs/2502.16162v1",
      "title": "Patch Stitching Data Augmentation for Cancer Classification in Pathology Images",
      "summary": "Computational pathology, integrating computational methods and digital imaging, has shown to be effective in advancing disease diagnosis and prognosis. In recent years, the development of machine learning and deep learning has greatly bolstered the power of computational pathology. However, there still remains the issue of data scarcity and data imbalance, which can have an adversarial effect on any computational method. In this paper, we introduce an efficient and effective data augmentation strategy to generate new pathology images from the existing pathology images and thus enrich datasets without additional data collection or annotation costs. To evaluate the proposed method, we employed two sets of colorectal cancer datasets and obtained improved classification results, suggesting that the proposed simple approach holds the potential for alleviating the data scarcity and imbalance in computational pathology.",
      "published": "2025-02-22T09:34:50Z"
    },
    "metadata": {
      "arxiv_id": "2502.16162",
      "title": "Patch Stitching Data Augmentation for Cancer Classification in Pathology Images",
      "summary": "Computational pathology, integrating computational methods and digital imaging, has shown to be effective in advancing disease diagnosis and prognosis. In recent years, the development of machine learning and deep learning has greatly bolstered the power of computational pathology. However, there still remains the issue of data scarcity and data imbalance, which can have an adversarial effect on any computational method. In this paper, we introduce an efficient and effective data augmentation strategy to generate new pathology images from the existing pathology images and thus enrich datasets without additional data collection or annotation costs. To evaluate the proposed method, we employed two sets of colorectal cancer datasets and obtained improved classification results, suggesting that the proposed simple approach holds the potential for alleviating the data scarcity and imbalance in computational pathology.",
      "authors": [
        "Jiamu Wang",
        "Chang-Su Kim",
        "Jin Tae Kwak"
      ],
      "published": "2025-02-22T09:34:50Z",
      "updated": "2025-02-22T09:34:50Z",
      "categories": [
        "eess.IV",
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2502.16162v1",
      "landing_url": "https://arxiv.org/abs/2502.16162v1",
      "doi": "https://doi.org/10.48550/arXiv.2502.16162"
    },
    "queries": [
      {
        "anchor": "synchronous dialogue",
        "search_term": "data scarcity"
      }
    ]
  },
  {
    "arxiv_id": "2502.16584",
    "anchor": "synchronous dialogue",
    "search_term": "audio tokens",
    "search_record": {
      "id": "http://arxiv.org/abs/2502.16584v1",
      "title": "Audio-FLAN: A Preliminary Release",
      "summary": "Recent advancements in audio tokenization have significantly enhanced the integration of audio capabilities into large language models (LLMs). However, audio understanding and generation are often treated as distinct tasks, hindering the development of truly unified audio-language models. While instruction tuning has demonstrated remarkable success in improving generalization and zero-shot learning across text and vision, its application to audio remains largely unexplored. A major obstacle is the lack of comprehensive datasets that unify audio understanding and generation. To address this, we introduce Audio-FLAN, a large-scale instruction-tuning dataset covering 80 diverse tasks across speech, music, and sound domains, with over 100 million instances. Audio-FLAN lays the foundation for unified audio-language models that can seamlessly handle both understanding (e.g., transcription, comprehension) and generation (e.g., speech, music, sound) tasks across a wide range of audio domains in a zero-shot manner. The Audio-FLAN dataset is available on HuggingFace and GitHub and will be continuously updated.",
      "published": "2025-02-23T14:24:15Z"
    },
    "metadata": {
      "arxiv_id": "2502.16584",
      "title": "Audio-FLAN: A Preliminary Release",
      "summary": "Recent advancements in audio tokenization have significantly enhanced the integration of audio capabilities into large language models (LLMs). However, audio understanding and generation are often treated as distinct tasks, hindering the development of truly unified audio-language models. While instruction tuning has demonstrated remarkable success in improving generalization and zero-shot learning across text and vision, its application to audio remains largely unexplored. A major obstacle is the lack of comprehensive datasets that unify audio understanding and generation. To address this, we introduce Audio-FLAN, a large-scale instruction-tuning dataset covering 80 diverse tasks across speech, music, and sound domains, with over 100 million instances. Audio-FLAN lays the foundation for unified audio-language models that can seamlessly handle both understanding (e.g., transcription, comprehension) and generation (e.g., speech, music, sound) tasks across a wide range of audio domains in a zero-shot manner. The Audio-FLAN dataset is available on HuggingFace and GitHub and will be continuously updated.",
      "authors": [
        "Liumeng Xue",
        "Ziya Zhou",
        "Jiahao Pan",
        "Zixuan Li",
        "Shuai Fan",
        "Yinghao Ma",
        "Sitong Cheng",
        "Dongchao Yang",
        "Haohan Guo",
        "Yujia Xiao",
        "Xinsheng Wang",
        "Zixuan Shen",
        "Chuanbo Zhu",
        "Xinshen Zhang",
        "Tianchi Liu",
        "Ruibin Yuan",
        "Zeyue Tian",
        "Haohe Liu",
        "Emmanouil Benetos",
        "Ge Zhang",
        "Yike Guo",
        "Wei Xue"
      ],
      "published": "2025-02-23T14:24:15Z",
      "updated": "2025-02-23T14:24:15Z",
      "categories": [
        "cs.SD",
        "cs.AI",
        "cs.CL",
        "cs.MM",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2502.16584v1",
      "landing_url": "https://arxiv.org/abs/2502.16584v1",
      "doi": "https://doi.org/10.48550/arXiv.2502.16584"
    },
    "queries": [
      {
        "anchor": "synchronous dialogue",
        "search_term": "audio tokens"
      }
    ]
  },
  {
    "arxiv_id": "2502.16897",
    "anchor": "spoken language models",
    "search_term": "neural codec",
    "search_record": {
      "id": "http://arxiv.org/abs/2502.16897v2",
      "title": "Balancing Speech Understanding and Generation Using Continual Pre-training for Codec-based Speech LLM",
      "summary": "Recent advances in speech language models (LLMs) have extended textual LLMs to the speech domain, but balancing speech understanding and generation remains challenging, especially with codec-based representations. We propose a continual pre-training (CPT) framework that adapts a textual LLM to handle codec-discretized speech, mitigating modality mismatch and preserving linguistic reasoning. Our unified model supports both understanding and generation, achieving strong results across ASR, TTS, S2T-Trans, and S2S-Trans. Notably, we present the first end-to-end, single-pass S2S-Trans system using only neural codec tokens, without intermediate transcriptions, translations, or semantic tokens. CPT proves essential for cross-modal alignment and task generalization, making it a powerful tool for building robust, unified speech LLMs.",
      "published": "2025-02-24T06:50:40Z"
    },
    "metadata": {
      "arxiv_id": "2502.16897",
      "title": "Balancing Speech Understanding and Generation Using Continual Pre-training for Codec-based Speech LLM",
      "summary": "Recent advances in speech language models (LLMs) have extended textual LLMs to the speech domain, but balancing speech understanding and generation remains challenging, especially with codec-based representations. We propose a continual pre-training (CPT) framework that adapts a textual LLM to handle codec-discretized speech, mitigating modality mismatch and preserving linguistic reasoning. Our unified model supports both understanding and generation, achieving strong results across ASR, TTS, S2T-Trans, and S2S-Trans. Notably, we present the first end-to-end, single-pass S2S-Trans system using only neural codec tokens, without intermediate transcriptions, translations, or semantic tokens. CPT proves essential for cross-modal alignment and task generalization, making it a powerful tool for building robust, unified speech LLMs.",
      "authors": [
        "Jiatong Shi",
        "Chunlei Zhang",
        "Jinchuan Tian",
        "Junrui Ni",
        "Hao Zhang",
        "Shinji Watanabe",
        "Dong Yu"
      ],
      "published": "2025-02-24T06:50:40Z",
      "updated": "2025-11-27T18:46:39Z",
      "categories": [
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2502.16897v2",
      "landing_url": "https://arxiv.org/abs/2502.16897v2",
      "doi": "https://doi.org/10.48550/arXiv.2502.16897"
    },
    "queries": [
      {
        "anchor": "spoken language models",
        "search_term": "neural codec"
      },
      {
        "anchor": "synchronous dialogue",
        "search_term": "neural codec"
      }
    ]
  },
  {
    "arxiv_id": "2502.17239",
    "anchor": "spoken language models",
    "search_term": "audio tokens",
    "search_record": {
      "id": "http://arxiv.org/abs/2502.17239v1",
      "title": "Baichuan-Audio: A Unified Framework for End-to-End Speech Interaction",
      "summary": "We introduce Baichuan-Audio, an end-to-end audio large language model that seamlessly integrates audio understanding and generation. It features a text-guided aligned speech generation mechanism, enabling real-time speech interaction with both comprehension and generation capabilities. Baichuan-Audio leverages a pre-trained ASR model, followed by multi-codebook discretization of speech at a frame rate of 12.5 Hz. This multi-codebook setup ensures that speech tokens retain both semantic and acoustic information. To further enhance modeling, an independent audio head is employed to process audio tokens, effectively capturing their unique characteristics. To mitigate the loss of intelligence during pre-training and preserve the original capabilities of the LLM, we propose a two-stage pre-training strategy that maintains language understanding while enhancing audio modeling. Following alignment, the model excels in real-time speech-based conversation and exhibits outstanding question-answering capabilities, demonstrating its versatility and efficiency. The proposed model demonstrates superior performance in real-time spoken dialogue and exhibits strong question-answering abilities. Our code, model and training data are available at https://github.com/baichuan-inc/Baichuan-Audio",
      "published": "2025-02-24T15:16:34Z"
    },
    "metadata": {
      "arxiv_id": "2502.17239",
      "title": "Baichuan-Audio: A Unified Framework for End-to-End Speech Interaction",
      "summary": "We introduce Baichuan-Audio, an end-to-end audio large language model that seamlessly integrates audio understanding and generation. It features a text-guided aligned speech generation mechanism, enabling real-time speech interaction with both comprehension and generation capabilities. Baichuan-Audio leverages a pre-trained ASR model, followed by multi-codebook discretization of speech at a frame rate of 12.5 Hz. This multi-codebook setup ensures that speech tokens retain both semantic and acoustic information. To further enhance modeling, an independent audio head is employed to process audio tokens, effectively capturing their unique characteristics. To mitigate the loss of intelligence during pre-training and preserve the original capabilities of the LLM, we propose a two-stage pre-training strategy that maintains language understanding while enhancing audio modeling. Following alignment, the model excels in real-time speech-based conversation and exhibits outstanding question-answering capabilities, demonstrating its versatility and efficiency. The proposed model demonstrates superior performance in real-time spoken dialogue and exhibits strong question-answering abilities. Our code, model and training data are available at https://github.com/baichuan-inc/Baichuan-Audio",
      "authors": [
        "Tianpeng Li",
        "Jun Liu",
        "Tao Zhang",
        "Yuanbo Fang",
        "Da Pan",
        "Mingrui Wang",
        "Zheng Liang",
        "Zehuan Li",
        "Mingan Lin",
        "Guosheng Dong",
        "Jianhua Xu",
        "Haoze Sun",
        "Zenan Zhou",
        "Weipeng Chen"
      ],
      "published": "2025-02-24T15:16:34Z",
      "updated": "2025-02-24T15:16:34Z",
      "categories": [
        "cs.CL",
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2502.17239v1",
      "landing_url": "https://arxiv.org/abs/2502.17239v1",
      "doi": "https://doi.org/10.48550/arXiv.2502.17239"
    },
    "queries": [
      {
        "anchor": "spoken language models",
        "search_term": "audio tokens"
      },
      {
        "anchor": "synchronous dialogue",
        "search_term": "audio tokens"
      }
    ]
  },
  {
    "arxiv_id": "2502.17255",
    "anchor": "synchronous dialogue",
    "search_term": "streaming architecture",
    "search_record": {
      "id": "http://arxiv.org/abs/2502.17255v1",
      "title": "MDN: Mamba-Driven Dualstream Network For Medical Hyperspectral Image Segmentation",
      "summary": "Medical Hyperspectral Imaging (MHSI) offers potential for computational pathology and precision medicine. However, existing CNN and Transformer struggle to balance segmentation accuracy and speed due to high spatial-spectral dimensionality. In this study, we leverage Mamba's global context modeling to propose a dual-stream architecture for joint spatial-spectral feature extraction. To address the limitation of Mamba's unidirectional aggregation, we introduce a recurrent spectral sequence representation to capture low-redundancy global spectral features. Experiments on a public Multi-Dimensional Choledoch dataset and a private Cervical Cancer dataset show that our method outperforms state-of-the-art approaches in segmentation accuracy while minimizing resource usage and achieving the fastest inference speed. Our code will be available at https://github.com/DeepMed-Lab-ECNU/MDN.",
      "published": "2025-02-24T15:35:37Z"
    },
    "metadata": {
      "arxiv_id": "2502.17255",
      "title": "MDN: Mamba-Driven Dualstream Network For Medical Hyperspectral Image Segmentation",
      "summary": "Medical Hyperspectral Imaging (MHSI) offers potential for computational pathology and precision medicine. However, existing CNN and Transformer struggle to balance segmentation accuracy and speed due to high spatial-spectral dimensionality. In this study, we leverage Mamba's global context modeling to propose a dual-stream architecture for joint spatial-spectral feature extraction. To address the limitation of Mamba's unidirectional aggregation, we introduce a recurrent spectral sequence representation to capture low-redundancy global spectral features. Experiments on a public Multi-Dimensional Choledoch dataset and a private Cervical Cancer dataset show that our method outperforms state-of-the-art approaches in segmentation accuracy while minimizing resource usage and achieving the fastest inference speed. Our code will be available at https://github.com/DeepMed-Lab-ECNU/MDN.",
      "authors": [
        "Shijie Lin",
        "Boxiang Yun",
        "Wei Shen",
        "Qingli Li",
        "Anqiang Yang",
        "Yan Wang"
      ],
      "published": "2025-02-24T15:35:37Z",
      "updated": "2025-02-24T15:35:37Z",
      "categories": [
        "eess.IV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2502.17255v1",
      "landing_url": "https://arxiv.org/abs/2502.17255v1",
      "doi": "https://doi.org/10.48550/arXiv.2502.17255"
    },
    "queries": [
      {
        "anchor": "synchronous dialogue",
        "search_term": "streaming architecture"
      }
    ]
  },
  {
    "arxiv_id": "2502.17581",
    "anchor": "synchronous dialogue",
    "search_term": "real-time interaction",
    "search_record": {
      "id": "http://arxiv.org/abs/2502.17581v1",
      "title": "Intention Recognition in Real-Time Interactive Navigation Maps",
      "summary": "In this demonstration, we develop IntentRec4Maps, a system to recognise users' intentions in interactive maps for real-world navigation. IntentRec4Maps uses the Google Maps Platform as the real-world interactive map, and a very effective approach for recognising users' intentions in real-time. We showcase the recognition process of IntentRec4Maps using two different Path-Planners and a Large Language Model (LLM).\n  GitHub: https://github.com/PeijieZ/IntentRec4Maps",
      "published": "2025-02-24T19:04:18Z"
    },
    "metadata": {
      "arxiv_id": "2502.17581",
      "title": "Intention Recognition in Real-Time Interactive Navigation Maps",
      "summary": "In this demonstration, we develop IntentRec4Maps, a system to recognise users' intentions in interactive maps for real-world navigation. IntentRec4Maps uses the Google Maps Platform as the real-world interactive map, and a very effective approach for recognising users' intentions in real-time. We showcase the recognition process of IntentRec4Maps using two different Path-Planners and a Large Language Model (LLM).\n  GitHub: https://github.com/PeijieZ/IntentRec4Maps",
      "authors": [
        "Peijie Zhao",
        "Zunayed Arefin",
        "Felipe Meneguzzi",
        "Ramon Fraga Pereira"
      ],
      "published": "2025-02-24T19:04:18Z",
      "updated": "2025-02-24T19:04:18Z",
      "categories": [
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2502.17581v1",
      "landing_url": "https://arxiv.org/abs/2502.17581v1",
      "doi": "https://doi.org/10.48550/arXiv.2502.17581"
    },
    "queries": [
      {
        "anchor": "synchronous dialogue",
        "search_term": "real-time interaction"
      }
    ]
  },
  {
    "arxiv_id": "2502.18671",
    "anchor": "synchronous dialogue",
    "search_term": "synchronous data",
    "search_record": {
      "id": "http://arxiv.org/abs/2502.18671v1",
      "title": "Wireless sensor networks data synchronization using node MCU memory for precision agriculture applications",
      "summary": "Wireless Sensor Networks have risen as a highly promising technology suitable for precision agriculture implementations, enabling efficient monitoring and control of agricultural processes. In precision agriculture, accurate and synchronized data collection is crucial for effective analysis and decision making. Using principles of information theory, we can define conditions and parameters that influence the efficient transmission and processing of information. Existing technologies have limitations in maintaining consistent time references, handling node failures, and unreliable communication links, leading to inaccurate data readings. Reliable data storage is demanding now-a-days for storing data on local monitoring station as well as in online live server. Sometime internet is not working properly due to congestion and there is frequent packet loss. Current solutions often synchronize records based on database timestamps, leading to record duplication and waste storage. Both databases synchronize each other after internet restoration. By providing synchronization among nodes and data, accuracy and storage will be saved in IoT based WSNs for precision agriculture applications. A prototype Node-MCU internal memory is used as a resource for achieving data synchronization. This proposed work generates record ID from Node MCU EEPROM which helps in records synchronization if there is any packet loss at the local server or at the online server to maintain synchronization accuracy despite unreliable communication links. Experiment shows that for a particular duration Node MCU generated 2364 packets and packet loss at local server was 08 and at online server was 174 packets. Results shows that after synchronization 99.87% packets were synchronized. Using previous technique of timestamp, the redundancy was 70% which reduced to 0% using our proposed technique.",
      "published": "2025-02-25T22:11:14Z"
    },
    "metadata": {
      "arxiv_id": "2502.18671",
      "title": "Wireless sensor networks data synchronization using node MCU memory for precision agriculture applications",
      "summary": "Wireless Sensor Networks have risen as a highly promising technology suitable for precision agriculture implementations, enabling efficient monitoring and control of agricultural processes. In precision agriculture, accurate and synchronized data collection is crucial for effective analysis and decision making. Using principles of information theory, we can define conditions and parameters that influence the efficient transmission and processing of information. Existing technologies have limitations in maintaining consistent time references, handling node failures, and unreliable communication links, leading to inaccurate data readings. Reliable data storage is demanding now-a-days for storing data on local monitoring station as well as in online live server. Sometime internet is not working properly due to congestion and there is frequent packet loss. Current solutions often synchronize records based on database timestamps, leading to record duplication and waste storage. Both databases synchronize each other after internet restoration. By providing synchronization among nodes and data, accuracy and storage will be saved in IoT based WSNs for precision agriculture applications. A prototype Node-MCU internal memory is used as a resource for achieving data synchronization. This proposed work generates record ID from Node MCU EEPROM which helps in records synchronization if there is any packet loss at the local server or at the online server to maintain synchronization accuracy despite unreliable communication links. Experiment shows that for a particular duration Node MCU generated 2364 packets and packet loss at local server was 08 and at online server was 174 packets. Results shows that after synchronization 99.87% packets were synchronized. Using previous technique of timestamp, the redundancy was 70% which reduced to 0% using our proposed technique.",
      "authors": [
        "Kashif Sattar",
        "Muhammad Arslan",
        "Saqib Majeed",
        "Salim Iqbal"
      ],
      "published": "2025-02-25T22:11:14Z",
      "updated": "2025-02-25T22:11:14Z",
      "categories": [
        "cs.NI",
        "cs.IT"
      ],
      "pdf_url": "https://arxiv.org/pdf/2502.18671v1",
      "landing_url": "https://arxiv.org/abs/2502.18671v1",
      "doi": "https://doi.org/10.48550/arXiv.2502.18671"
    },
    "queries": [
      {
        "anchor": "synchronous dialogue",
        "search_term": "synchronous data"
      }
    ]
  },
  {
    "arxiv_id": "2502.18925",
    "anchor": "spoken language models",
    "search_term": "data scarcity",
    "search_record": {
      "id": "http://arxiv.org/abs/2502.18925v1",
      "title": "BeamVQ: Beam Search with Vector Quantization to Mitigate Data Scarcity in Physical Spatiotemporal Forecasting",
      "summary": "In practice, physical spatiotemporal forecasting can suffer from data scarcity, because collecting large-scale data is non-trivial, especially for extreme events. Hence, we propose \\method{}, a novel probabilistic framework to realize iterative self-training with new self-ensemble strategies, achieving better physical consistency and generalization on extreme events. Following any base forecasting model, we can encode its deterministic outputs into a latent space and retrieve multiple codebook entries to generate probabilistic outputs. Then BeamVQ extends the beam search from discrete spaces to the continuous state spaces in this field. We can further employ domain-specific metrics (e.g., Critical Success Index for extreme events) to filter out the top-k candidates and develop the new self-ensemble strategy by combining the high-quality candidates. The self-ensemble can not only improve the inference quality and robustness but also iteratively augment the training datasets during continuous self-training. Consequently, BeamVQ realizes the exploration of rare but critical phenomena beyond the original dataset. Comprehensive experiments on different benchmarks and backbones show that BeamVQ consistently reduces forecasting MSE (up to 39%), enhancing extreme events detection and proving its effectiveness in handling data scarcity.",
      "published": "2025-02-26T08:27:25Z"
    },
    "metadata": {
      "arxiv_id": "2502.18925",
      "title": "BeamVQ: Beam Search with Vector Quantization to Mitigate Data Scarcity in Physical Spatiotemporal Forecasting",
      "summary": "In practice, physical spatiotemporal forecasting can suffer from data scarcity, because collecting large-scale data is non-trivial, especially for extreme events. Hence, we propose \\method{}, a novel probabilistic framework to realize iterative self-training with new self-ensemble strategies, achieving better physical consistency and generalization on extreme events. Following any base forecasting model, we can encode its deterministic outputs into a latent space and retrieve multiple codebook entries to generate probabilistic outputs. Then BeamVQ extends the beam search from discrete spaces to the continuous state spaces in this field. We can further employ domain-specific metrics (e.g., Critical Success Index for extreme events) to filter out the top-k candidates and develop the new self-ensemble strategy by combining the high-quality candidates. The self-ensemble can not only improve the inference quality and robustness but also iteratively augment the training datasets during continuous self-training. Consequently, BeamVQ realizes the exploration of rare but critical phenomena beyond the original dataset. Comprehensive experiments on different benchmarks and backbones show that BeamVQ consistently reduces forecasting MSE (up to 39%), enhancing extreme events detection and proving its effectiveness in handling data scarcity.",
      "authors": [
        "Weiyan Wang",
        "Xingjian Shi",
        "Ruiqi Shu",
        "Yuan Gao",
        "Rui Ray Chen",
        "Kun Wang",
        "Fan Xu",
        "Jinbao Xue",
        "Shuaipeng Li",
        "Yangyu Tao",
        "Di Wang",
        "Hao Wu",
        "Xiaomeng Huang"
      ],
      "published": "2025-02-26T08:27:25Z",
      "updated": "2025-02-26T08:27:25Z",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2502.18925v1",
      "landing_url": "https://arxiv.org/abs/2502.18925v1",
      "doi": "https://doi.org/10.48550/arXiv.2502.18925"
    },
    "queries": [
      {
        "anchor": "spoken language models",
        "search_term": "data scarcity"
      },
      {
        "anchor": "synchronous dialogue",
        "search_term": "data scarcity"
      },
      {
        "anchor": "turn-taking",
        "search_term": "data scarcity"
      }
    ]
  },
  {
    "arxiv_id": "2502.18954",
    "anchor": "synchronous dialogue",
    "search_term": "synchronous data",
    "search_record": {
      "id": "http://arxiv.org/abs/2502.18954v2",
      "title": "Bidirectionalization For The Common People",
      "summary": "This paper presents an innovative approach to applying bidirectional transformations (BX) in practice. To introduce BX to a wider audience of technologists, engineers, and researchers, we have chosen to use C# to develop Bifrons - a library of BX lenses that replaces domain-specific programming languages (DSL) in practical use. The proposed approach simplifies the implementation effort for two-way transformations by using simple symmetric lenses as the initial design pattern. It ensures correctness within reason by providing a simple lens-testing framework. We demonstrate the usability of BX lenses in a realistic scenario by using Bifrons to perform a case study experiment synchronizing data from two structurally and technologically heterogeneous databases.",
      "published": "2025-02-26T09:05:13Z"
    },
    "metadata": {
      "arxiv_id": "2502.18954",
      "title": "Bidirectionalization For The Common People",
      "summary": "This paper presents an innovative approach to applying bidirectional transformations (BX) in practice. To introduce BX to a wider audience of technologists, engineers, and researchers, we have chosen to use C# to develop Bifrons - a library of BX lenses that replaces domain-specific programming languages (DSL) in practical use. The proposed approach simplifies the implementation effort for two-way transformations by using simple symmetric lenses as the initial design pattern. It ensures correctness within reason by providing a simple lens-testing framework. We demonstrate the usability of BX lenses in a realistic scenario by using Bifrons to perform a case study experiment synchronizing data from two structurally and technologically heterogeneous databases.",
      "authors": [
        "Juraj Dončević",
        "Mario Brčić",
        "Danijel Mlinarić"
      ],
      "published": "2025-02-26T09:05:13Z",
      "updated": "2025-03-06T13:17:02Z",
      "categories": [
        "cs.SE"
      ],
      "pdf_url": "https://arxiv.org/pdf/2502.18954v2",
      "landing_url": "https://arxiv.org/abs/2502.18954v2",
      "doi": "https://doi.org/10.48550/arXiv.2502.18954"
    },
    "queries": [
      {
        "anchor": "synchronous dialogue",
        "search_term": "synchronous data"
      }
    ]
  },
  {
    "arxiv_id": "2502.19548",
    "anchor": "synchronous dialogue",
    "search_term": "audio tokens",
    "search_record": {
      "id": "http://arxiv.org/abs/2502.19548v2",
      "title": "When Large Language Models Meet Speech: A Survey on Integration Approaches",
      "summary": "Recent advancements in large language models (LLMs) have spurred interest in expanding their application beyond text-based tasks. A large number of studies have explored integrating other modalities with LLMs, notably speech modality, which is naturally related to text. This paper surveys the integration of speech with LLMs, categorizing the methodologies into three primary approaches: text-based, latent-representation-based, and audio-token-based integration. We also demonstrate how these methods are applied across various speech-related applications and highlight the challenges in this field to offer inspiration for",
      "published": "2025-02-26T20:40:49Z"
    },
    "metadata": {
      "arxiv_id": "2502.19548",
      "title": "When Large Language Models Meet Speech: A Survey on Integration Approaches",
      "summary": "Recent advancements in large language models (LLMs) have spurred interest in expanding their application beyond text-based tasks. A large number of studies have explored integrating other modalities with LLMs, notably speech modality, which is naturally related to text. This paper surveys the integration of speech with LLMs, categorizing the methodologies into three primary approaches: text-based, latent-representation-based, and audio-token-based integration. We also demonstrate how these methods are applied across various speech-related applications and highlight the challenges in this field to offer inspiration for",
      "authors": [
        "Zhengdong Yang",
        "Shuichiro Shimizu",
        "Yahan Yu",
        "Chenhui Chu"
      ],
      "published": "2025-02-26T20:40:49Z",
      "updated": "2025-09-09T09:27:16Z",
      "categories": [
        "cs.CL",
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2502.19548v2",
      "landing_url": "https://arxiv.org/abs/2502.19548v2",
      "doi": "https://doi.org/10.48550/arXiv.2502.19548"
    },
    "queries": [
      {
        "anchor": "synchronous dialogue",
        "search_term": "audio tokens"
      }
    ]
  },
  {
    "arxiv_id": "2502.19630",
    "anchor": "synchronous dialogue",
    "search_term": "synchronous data",
    "search_record": {
      "id": "http://arxiv.org/abs/2502.19630v1",
      "title": "Ev-3DOD: Pushing the Temporal Boundaries of 3D Object Detection with Event Cameras",
      "summary": "Detecting 3D objects in point clouds plays a crucial role in autonomous driving systems. Recently, advanced multi-modal methods incorporating camera information have achieved notable performance. For a safe and effective autonomous driving system, algorithms that excel not only in accuracy but also in speed and low latency are essential. However, existing algorithms fail to meet these requirements due to the latency and bandwidth limitations of fixed frame rate sensors, e.g., LiDAR and camera. To address this limitation, we introduce asynchronous event cameras into 3D object detection for the first time. We leverage their high temporal resolution and low bandwidth to enable high-speed 3D object detection. Our method enables detection even during inter-frame intervals when synchronized data is unavailable, by retrieving previous 3D information through the event camera. Furthermore, we introduce the first event-based 3D object detection dataset, DSEC-3DOD, which includes ground-truth 3D bounding boxes at 100 FPS, establishing the first benchmark for event-based 3D detectors. The code and dataset are available at https://github.com/mickeykang16/Ev3DOD.",
      "published": "2025-02-26T23:51:25Z"
    },
    "metadata": {
      "arxiv_id": "2502.19630",
      "title": "Ev-3DOD: Pushing the Temporal Boundaries of 3D Object Detection with Event Cameras",
      "summary": "Detecting 3D objects in point clouds plays a crucial role in autonomous driving systems. Recently, advanced multi-modal methods incorporating camera information have achieved notable performance. For a safe and effective autonomous driving system, algorithms that excel not only in accuracy but also in speed and low latency are essential. However, existing algorithms fail to meet these requirements due to the latency and bandwidth limitations of fixed frame rate sensors, e.g., LiDAR and camera. To address this limitation, we introduce asynchronous event cameras into 3D object detection for the first time. We leverage their high temporal resolution and low bandwidth to enable high-speed 3D object detection. Our method enables detection even during inter-frame intervals when synchronized data is unavailable, by retrieving previous 3D information through the event camera. Furthermore, we introduce the first event-based 3D object detection dataset, DSEC-3DOD, which includes ground-truth 3D bounding boxes at 100 FPS, establishing the first benchmark for event-based 3D detectors. The code and dataset are available at https://github.com/mickeykang16/Ev3DOD.",
      "authors": [
        "Hoonhee Cho",
        "Jae-young Kang",
        "Youngho Kim",
        "Kuk-Jin Yoon"
      ],
      "published": "2025-02-26T23:51:25Z",
      "updated": "2025-02-26T23:51:25Z",
      "categories": [
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2502.19630v1",
      "landing_url": "https://arxiv.org/abs/2502.19630v1",
      "doi": "https://doi.org/10.48550/arXiv.2502.19630"
    },
    "queries": [
      {
        "anchor": "synchronous dialogue",
        "search_term": "synchronous data"
      }
    ]
  },
  {
    "arxiv_id": "2502.20319",
    "anchor": "synchronous dialogue",
    "search_term": "data scarcity",
    "search_record": {
      "id": "http://arxiv.org/abs/2502.20319v1",
      "title": "Impilict Runge-Kutta based sparse identification of governing equations in biologically motivated systems",
      "summary": "Identifying governing equations in physical and biological systems from datasets remains a long-standing challenge across various scientific disciplines, providing mechanistic insights into complex system evolution. Common methods like sparse identification of nonlinear dynamics (SINDy) often rely on precise derivative estimations, making them vulnerable to data scarcity and noise. This study presents a novel data-driven framework by integrating high order implicit Runge-Kutta methods (IRKs) with the sparse identification, termed IRK-SINDy. The framework exhibits remarkable robustness to data scarcity and noise by leveraging the lower stepsize constraint of IRKs. Two methods for incorporating IRKs into sparse regression are introduced: one employs iterative schemes for numerically solving nonlinear algebraic system of equations, while the other utilizes deep neural networks to predict stage values of IRKs. The performance of IRK-SINDy is demonstrated through numerical experiments on benchmark problems with varied dynamical behaviors, including linear and nonlinear oscillators, the Lorenz system, and biologically relevant models like predator-prey dynamics, logistic growth, and the FitzHugh-Nagumo model. Results indicate that IRK-SINDy outperforms conventional SINDy and the RK4-SINDy framework, particularly under conditions of extreme data scarcity and noise, yielding interpretable and generalizable models.",
      "published": "2025-02-27T17:44:37Z"
    },
    "metadata": {
      "arxiv_id": "2502.20319",
      "title": "Impilict Runge-Kutta based sparse identification of governing equations in biologically motivated systems",
      "summary": "Identifying governing equations in physical and biological systems from datasets remains a long-standing challenge across various scientific disciplines, providing mechanistic insights into complex system evolution. Common methods like sparse identification of nonlinear dynamics (SINDy) often rely on precise derivative estimations, making them vulnerable to data scarcity and noise. This study presents a novel data-driven framework by integrating high order implicit Runge-Kutta methods (IRKs) with the sparse identification, termed IRK-SINDy. The framework exhibits remarkable robustness to data scarcity and noise by leveraging the lower stepsize constraint of IRKs. Two methods for incorporating IRKs into sparse regression are introduced: one employs iterative schemes for numerically solving nonlinear algebraic system of equations, while the other utilizes deep neural networks to predict stage values of IRKs. The performance of IRK-SINDy is demonstrated through numerical experiments on benchmark problems with varied dynamical behaviors, including linear and nonlinear oscillators, the Lorenz system, and biologically relevant models like predator-prey dynamics, logistic growth, and the FitzHugh-Nagumo model. Results indicate that IRK-SINDy outperforms conventional SINDy and the RK4-SINDy framework, particularly under conditions of extreme data scarcity and noise, yielding interpretable and generalizable models.",
      "authors": [
        "Mehrdad Anvari",
        "Hamidreza Marasi",
        "Hossein Kheiri"
      ],
      "published": "2025-02-27T17:44:37Z",
      "updated": "2025-02-27T17:44:37Z",
      "categories": [
        "math.DS",
        "cs.LG",
        "math.NA",
        "q-bio.QM"
      ],
      "pdf_url": "https://arxiv.org/pdf/2502.20319v1",
      "landing_url": "https://arxiv.org/abs/2502.20319v1",
      "doi": "https://doi.org/10.48550/arXiv.2502.20319"
    },
    "queries": [
      {
        "anchor": "synchronous dialogue",
        "search_term": "data scarcity"
      }
    ]
  },
  {
    "arxiv_id": "2503.00084",
    "anchor": "spoken language models",
    "search_term": "audio tokens",
    "search_record": {
      "id": "http://arxiv.org/abs/2503.00084v1",
      "title": "InspireMusic: Integrating Super Resolution and Large Language Model for High-Fidelity Long-Form Music Generation",
      "summary": "We introduce InspireMusic, a framework integrated super resolution and large language model for high-fidelity long-form music generation. A unified framework generates high-fidelity music, songs, and audio, which incorporates an autoregressive transformer with a super-resolution flow-matching model. This framework enables the controllable generation of high-fidelity long-form music at a higher sampling rate from both text and audio prompts. Our model differs from previous approaches, as we utilize an audio tokenizer with one codebook that contains richer semantic information, thereby reducing training costs and enhancing efficiency. This combination enables us to achieve high-quality audio generation with long-form coherence of up to $8$ minutes. Then, an autoregressive transformer model based on Qwen 2.5 predicts audio tokens. Next, we employ a super-resolution flow-matching model to generate high-sampling rate audio with fine-grained details learned from an acoustic codec model. Comprehensive experiments show that the InspireMusic-1.5B-Long model has a comparable performance to recent top-tier open-source systems, including MusicGen and Stable Audio 2.0, on subjective and objective evaluations. The code and pre-trained models are released at https://github.com/FunAudioLLM/InspireMusic.",
      "published": "2025-02-28T09:58:25Z"
    },
    "metadata": {
      "arxiv_id": "2503.00084",
      "title": "InspireMusic: Integrating Super Resolution and Large Language Model for High-Fidelity Long-Form Music Generation",
      "summary": "We introduce InspireMusic, a framework integrated super resolution and large language model for high-fidelity long-form music generation. A unified framework generates high-fidelity music, songs, and audio, which incorporates an autoregressive transformer with a super-resolution flow-matching model. This framework enables the controllable generation of high-fidelity long-form music at a higher sampling rate from both text and audio prompts. Our model differs from previous approaches, as we utilize an audio tokenizer with one codebook that contains richer semantic information, thereby reducing training costs and enhancing efficiency. This combination enables us to achieve high-quality audio generation with long-form coherence of up to $8$ minutes. Then, an autoregressive transformer model based on Qwen 2.5 predicts audio tokens. Next, we employ a super-resolution flow-matching model to generate high-sampling rate audio with fine-grained details learned from an acoustic codec model. Comprehensive experiments show that the InspireMusic-1.5B-Long model has a comparable performance to recent top-tier open-source systems, including MusicGen and Stable Audio 2.0, on subjective and objective evaluations. The code and pre-trained models are released at https://github.com/FunAudioLLM/InspireMusic.",
      "authors": [
        "Chong Zhang",
        "Yukun Ma",
        "Qian Chen",
        "Wen Wang",
        "Shengkui Zhao",
        "Zexu Pan",
        "Hao Wang",
        "Chongjia Ni",
        "Trung Hieu Nguyen",
        "Kun Zhou",
        "Yidi Jiang",
        "Chaohong Tan",
        "Zhifu Gao",
        "Zhihao Du",
        "Bin Ma"
      ],
      "published": "2025-02-28T09:58:25Z",
      "updated": "2025-02-28T09:58:25Z",
      "categories": [
        "cs.SD",
        "cs.AI",
        "cs.CL",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2503.00084v1",
      "landing_url": "https://arxiv.org/abs/2503.00084v1",
      "doi": "https://doi.org/10.48550/arXiv.2503.00084"
    },
    "queries": [
      {
        "anchor": "spoken language models",
        "search_term": "audio tokens"
      },
      {
        "anchor": "synchronous dialogue",
        "search_term": "audio tokens"
      },
      {
        "anchor": "turn-taking",
        "search_term": "audio tokens"
      }
    ]
  },
  {
    "arxiv_id": "2503.00951",
    "anchor": "spoken language models",
    "search_term": "temporal dynamics",
    "search_record": {
      "id": "http://arxiv.org/abs/2503.00951v1",
      "title": "Dynamical Diffusion: Learning Temporal Dynamics with Diffusion Models",
      "summary": "Diffusion models have emerged as powerful generative frameworks by progressively adding noise to data through a forward process and then reversing this process to generate realistic samples. While these models have achieved strong performance across various tasks and modalities, their application to temporal predictive learning remains underexplored. Existing approaches treat predictive learning as a conditional generation problem, but often fail to fully exploit the temporal dynamics inherent in the data, leading to challenges in generating temporally coherent sequences. To address this, we introduce Dynamical Diffusion (DyDiff), a theoretically sound framework that incorporates temporally aware forward and reverse processes. Dynamical Diffusion explicitly models temporal transitions at each diffusion step, establishing dependencies on preceding states to better capture temporal dynamics. Through the reparameterization trick, Dynamical Diffusion achieves efficient training and inference similar to any standard diffusion model. Extensive experiments across scientific spatiotemporal forecasting, video prediction, and time series forecasting demonstrate that Dynamical Diffusion consistently improves performance in temporal predictive tasks, filling a crucial gap in existing methodologies. Code is available at this repository: https://github.com/thuml/dynamical-diffusion.",
      "published": "2025-03-02T16:10:32Z"
    },
    "metadata": {
      "arxiv_id": "2503.00951",
      "title": "Dynamical Diffusion: Learning Temporal Dynamics with Diffusion Models",
      "summary": "Diffusion models have emerged as powerful generative frameworks by progressively adding noise to data through a forward process and then reversing this process to generate realistic samples. While these models have achieved strong performance across various tasks and modalities, their application to temporal predictive learning remains underexplored. Existing approaches treat predictive learning as a conditional generation problem, but often fail to fully exploit the temporal dynamics inherent in the data, leading to challenges in generating temporally coherent sequences. To address this, we introduce Dynamical Diffusion (DyDiff), a theoretically sound framework that incorporates temporally aware forward and reverse processes. Dynamical Diffusion explicitly models temporal transitions at each diffusion step, establishing dependencies on preceding states to better capture temporal dynamics. Through the reparameterization trick, Dynamical Diffusion achieves efficient training and inference similar to any standard diffusion model. Extensive experiments across scientific spatiotemporal forecasting, video prediction, and time series forecasting demonstrate that Dynamical Diffusion consistently improves performance in temporal predictive tasks, filling a crucial gap in existing methodologies. Code is available at this repository: https://github.com/thuml/dynamical-diffusion.",
      "authors": [
        "Xingzhuo Guo",
        "Yu Zhang",
        "Baixu Chen",
        "Haoran Xu",
        "Jianmin Wang",
        "Mingsheng Long"
      ],
      "published": "2025-03-02T16:10:32Z",
      "updated": "2025-03-02T16:10:32Z",
      "categories": [
        "cs.LG",
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2503.00951v1",
      "landing_url": "https://arxiv.org/abs/2503.00951v1",
      "doi": "https://doi.org/10.48550/arXiv.2503.00951"
    },
    "queries": [
      {
        "anchor": "spoken language models",
        "search_term": "temporal dynamics"
      },
      {
        "anchor": "synchronous dialogue",
        "search_term": "temporal dynamics"
      },
      {
        "anchor": "turn-taking",
        "search_term": "temporal dynamics"
      }
    ]
  },
  {
    "arxiv_id": "2503.01174",
    "anchor": "turn-taking",
    "search_term": "true full-duplex",
    "search_record": {
      "id": "http://arxiv.org/abs/2503.01174v1",
      "title": "Talking Turns: Benchmarking Audio Foundation Models on Turn-Taking Dynamics",
      "summary": "The recent wave of audio foundation models (FMs) could provide new capabilities for conversational modeling. However, there have been limited efforts to evaluate these audio FMs comprehensively on their ability to have natural and interactive conversations. To engage in meaningful conversation with the end user, we would want the FMs to additionally perform a fluent succession of turns without too much overlapping speech or long stretches of silence. Inspired by this, we ask whether the recently proposed audio FMs can understand, predict, and perform turn-taking events? To answer this, we propose a novel evaluation protocol that can assess spoken dialog system's turn-taking capabilities using a supervised model as a judge that has been trained to predict turn-taking events in human-human conversations. Using this protocol, we present the first comprehensive user study that evaluates existing spoken dialogue systems on their ability to perform turn-taking events and reveal many interesting insights, such as they sometimes do not understand when to speak up, can interrupt too aggressively and rarely backchannel. We further evaluate multiple open-source and proprietary audio FMs accessible through APIs on carefully curated test benchmarks from Switchboard to measure their ability to understand and predict turn-taking events and identify significant room for improvement. We will open source our evaluation platform to promote the development of advanced conversational AI systems.",
      "published": "2025-03-03T04:46:04Z"
    },
    "metadata": {
      "arxiv_id": "2503.01174",
      "title": "Talking Turns: Benchmarking Audio Foundation Models on Turn-Taking Dynamics",
      "summary": "The recent wave of audio foundation models (FMs) could provide new capabilities for conversational modeling. However, there have been limited efforts to evaluate these audio FMs comprehensively on their ability to have natural and interactive conversations. To engage in meaningful conversation with the end user, we would want the FMs to additionally perform a fluent succession of turns without too much overlapping speech or long stretches of silence. Inspired by this, we ask whether the recently proposed audio FMs can understand, predict, and perform turn-taking events? To answer this, we propose a novel evaluation protocol that can assess spoken dialog system's turn-taking capabilities using a supervised model as a judge that has been trained to predict turn-taking events in human-human conversations. Using this protocol, we present the first comprehensive user study that evaluates existing spoken dialogue systems on their ability to perform turn-taking events and reveal many interesting insights, such as they sometimes do not understand when to speak up, can interrupt too aggressively and rarely backchannel. We further evaluate multiple open-source and proprietary audio FMs accessible through APIs on carefully curated test benchmarks from Switchboard to measure their ability to understand and predict turn-taking events and identify significant room for improvement. We will open source our evaluation platform to promote the development of advanced conversational AI systems.",
      "authors": [
        "Siddhant Arora",
        "Zhiyun Lu",
        "Chung-Cheng Chiu",
        "Ruoming Pang",
        "Shinji Watanabe"
      ],
      "published": "2025-03-03T04:46:04Z",
      "updated": "2025-03-03T04:46:04Z",
      "categories": [
        "cs.CL",
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2503.01174v1",
      "landing_url": "https://arxiv.org/abs/2503.01174v1",
      "doi": "https://doi.org/10.48550/arXiv.2503.01174"
    },
    "queries": [
      {
        "anchor": "turn-taking",
        "search_term": "true full-duplex"
      },
      {
        "anchor": "turn-taking",
        "search_term": "pseudo full-duplex"
      },
      {
        "anchor": "turn-taking",
        "search_term": "half duplex"
      },
      {
        "anchor": "turn-taking",
        "search_term": "engineered synchronization"
      },
      {
        "anchor": "turn-taking",
        "search_term": "learned synchronization"
      },
      {
        "anchor": "turn-taking",
        "search_term": "modular architecture"
      },
      {
        "anchor": "turn-taking",
        "search_term": "streaming architecture"
      },
      {
        "anchor": "turn-taking",
        "search_term": "real-time interaction"
      },
      {
        "anchor": "turn-taking",
        "search_term": "low latency"
      },
      {
        "anchor": "turn-taking",
        "search_term": "synchronous data"
      },
      {
        "anchor": "turn-taking",
        "search_term": "data scarcity"
      },
      {
        "anchor": "turn-taking",
        "search_term": "multichannel dialogue"
      },
      {
        "anchor": "turn-taking",
        "search_term": "temporal dynamics"
      },
      {
        "anchor": "turn-taking",
        "search_term": "behavioral arbitration"
      },
      {
        "anchor": "turn-taking",
        "search_term": "semantic coherence"
      },
      {
        "anchor": "turn-taking",
        "search_term": "neural codec"
      },
      {
        "anchor": "turn-taking",
        "search_term": "discrete tokens"
      },
      {
        "anchor": "turn-taking",
        "search_term": "audio tokens"
      },
      {
        "anchor": "turn-taking",
        "search_term": "floor transfer offset"
      },
      {
        "anchor": "turn-taking",
        "search_term": "stop latency"
      },
      {
        "anchor": "turn-taking",
        "search_term": "interruption response delay"
      }
    ]
  },
  {
    "arxiv_id": "2503.01266",
    "anchor": "spoken language models",
    "search_term": "data scarcity",
    "search_record": {
      "id": "http://arxiv.org/abs/2503.01266v1",
      "title": "Voice Cloning for Dysarthric Speech Synthesis: Addressing Data Scarcity in Speech-Language Pathology",
      "summary": "This study explores voice cloning to generate synthetic speech replicating the unique patterns of individuals with dysarthria. Using the TORGO dataset, we address data scarcity and privacy challenges in speech-language pathology. Our contributions include demonstrating that voice cloning preserves dysarthric speech characteristics, analyzing differences between real and synthetic data, and discussing implications for diagnostics, rehabilitation, and communication. We cloned voices from dysarthric and control speakers using a commercial platform, ensuring gender-matched synthetic voices. A licensed speech-language pathologist (SLP) evaluated a subset for dysarthria, speaker gender, and synthetic indicators. The SLP correctly identified dysarthria in all cases and speaker gender in 95% but misclassified 30% of synthetic samples as real, indicating high realism. Our results suggest synthetic speech effectively captures disordered characteristics and that voice cloning has advanced to produce high-quality data resembling real speech, even to trained professionals. This has critical implications for healthcare, where synthetic data can mitigate data scarcity, protect privacy, and enhance AI-driven diagnostics. By enabling the creation of diverse, high-quality speech datasets, voice cloning can improve generalizable models, personalize therapy, and advance assistive technologies for dysarthria.\n  We publicly release our synthetic dataset to foster further research and collaboration, aiming to develop robust models that improve patient outcomes in speech-language pathology.",
      "published": "2025-03-03T07:44:49Z"
    },
    "metadata": {
      "arxiv_id": "2503.01266",
      "title": "Voice Cloning for Dysarthric Speech Synthesis: Addressing Data Scarcity in Speech-Language Pathology",
      "summary": "This study explores voice cloning to generate synthetic speech replicating the unique patterns of individuals with dysarthria. Using the TORGO dataset, we address data scarcity and privacy challenges in speech-language pathology. Our contributions include demonstrating that voice cloning preserves dysarthric speech characteristics, analyzing differences between real and synthetic data, and discussing implications for diagnostics, rehabilitation, and communication. We cloned voices from dysarthric and control speakers using a commercial platform, ensuring gender-matched synthetic voices. A licensed speech-language pathologist (SLP) evaluated a subset for dysarthria, speaker gender, and synthetic indicators. The SLP correctly identified dysarthria in all cases and speaker gender in 95% but misclassified 30% of synthetic samples as real, indicating high realism. Our results suggest synthetic speech effectively captures disordered characteristics and that voice cloning has advanced to produce high-quality data resembling real speech, even to trained professionals. This has critical implications for healthcare, where synthetic data can mitigate data scarcity, protect privacy, and enhance AI-driven diagnostics. By enabling the creation of diverse, high-quality speech datasets, voice cloning can improve generalizable models, personalize therapy, and advance assistive technologies for dysarthria.\n  We publicly release our synthetic dataset to foster further research and collaboration, aiming to develop robust models that improve patient outcomes in speech-language pathology.",
      "authors": [
        "Birger Moell",
        "Fredrik Sand Aronsson"
      ],
      "published": "2025-03-03T07:44:49Z",
      "updated": "2025-03-03T07:44:49Z",
      "categories": [
        "cs.SD",
        "cs.AI",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2503.01266v1",
      "landing_url": "https://arxiv.org/abs/2503.01266v1",
      "doi": "https://doi.org/10.48550/arXiv.2503.01266"
    },
    "queries": [
      {
        "anchor": "spoken language models",
        "search_term": "data scarcity"
      },
      {
        "anchor": "synchronous dialogue",
        "search_term": "data scarcity"
      },
      {
        "anchor": "turn-taking",
        "search_term": "data scarcity"
      }
    ]
  },
  {
    "arxiv_id": "2503.03100",
    "anchor": "spoken language models",
    "search_term": "synchronous data",
    "search_record": {
      "id": "http://arxiv.org/abs/2503.03100v1",
      "title": "Car-STAGE: Automated framework for large-scale high-dimensional simulated time-series data generation based on user-defined criteria",
      "summary": "Generating large-scale sensing datasets through photo-realistic simulation is an important aspect of many robotics applications such as autonomous driving. In this paper, we consider the problem of synchronous data collection from the open-source CARLA simulator using multiple sensors attached to vehicle based on user-defined criteria. We propose a novel, one-step framework that we refer to as Car-STAGE, based on CARLA simulator, to generate data using a graphical user interface (GUI) defining configuration parameters to data collection without any user intervention. This framework can utilize the user-defined configuration parameters such as choice of maps, number and configurations of sensors, environmental and lighting conditions etc. to run the simulation in the background, collecting high-dimensional sensor data from diverse sensors such as RGB Camera, LiDAR, Radar, Depth Camera, IMU Sensor, GNSS Sensor, Semantic Segmentation Camera, Instance Segmentation Camera, and Optical Flow Camera along with the ground-truths of the individual actors and storing the sensor data as well as ground-truth labels in a local or cloud-based database. The framework uses multiple threads where a main thread runs the server, a worker thread deals with queue and frame number and the rest of the threads processes the sensor data. The other way we derive speed up over the native implementation is by memory mapping the raw binary data into the disk and then converting the data into known formats at the end of data collection. We show that using these techniques, we gain a significant speed up over frames, under an increasing set of sensors and over the number of spawned objects.",
      "published": "2025-03-05T01:32:56Z"
    },
    "metadata": {
      "arxiv_id": "2503.03100",
      "title": "Car-STAGE: Automated framework for large-scale high-dimensional simulated time-series data generation based on user-defined criteria",
      "summary": "Generating large-scale sensing datasets through photo-realistic simulation is an important aspect of many robotics applications such as autonomous driving. In this paper, we consider the problem of synchronous data collection from the open-source CARLA simulator using multiple sensors attached to vehicle based on user-defined criteria. We propose a novel, one-step framework that we refer to as Car-STAGE, based on CARLA simulator, to generate data using a graphical user interface (GUI) defining configuration parameters to data collection without any user intervention. This framework can utilize the user-defined configuration parameters such as choice of maps, number and configurations of sensors, environmental and lighting conditions etc. to run the simulation in the background, collecting high-dimensional sensor data from diverse sensors such as RGB Camera, LiDAR, Radar, Depth Camera, IMU Sensor, GNSS Sensor, Semantic Segmentation Camera, Instance Segmentation Camera, and Optical Flow Camera along with the ground-truths of the individual actors and storing the sensor data as well as ground-truth labels in a local or cloud-based database. The framework uses multiple threads where a main thread runs the server, a worker thread deals with queue and frame number and the rest of the threads processes the sensor data. The other way we derive speed up over the native implementation is by memory mapping the raw binary data into the disk and then converting the data into known formats at the end of data collection. We show that using these techniques, we gain a significant speed up over frames, under an increasing set of sensors and over the number of spawned objects.",
      "authors": [
        "Asma A. Almutairi",
        "David J. LeBlanc",
        "Arpan Kusari"
      ],
      "published": "2025-03-05T01:32:56Z",
      "updated": "2025-03-05T01:32:56Z",
      "categories": [
        "cs.RO"
      ],
      "pdf_url": "https://arxiv.org/pdf/2503.03100v1",
      "landing_url": "https://arxiv.org/abs/2503.03100v1",
      "doi": "https://doi.org/10.48550/arXiv.2503.03100"
    },
    "queries": [
      {
        "anchor": "spoken language models",
        "search_term": "synchronous data"
      },
      {
        "anchor": "synchronous dialogue",
        "search_term": "synchronous data"
      }
    ]
  },
  {
    "arxiv_id": "2503.03304",
    "anchor": "spoken language models",
    "search_term": "neural codec",
    "search_record": {
      "id": "http://arxiv.org/abs/2503.03304v1",
      "title": "On the Relation Between Speech Quality and Quantized Latent Representations of Neural Codecs",
      "summary": "Neural audio signal codecs have attracted significant attention in recent years. In essence, the impressive low bitrate achieved by such encoders is enabled by learning an abstract representation that captures the properties of encoded signals, e.g., speech. In this work, we investigate the relation between the latent representation of the input signal learned by a neural codec and the quality of speech signals. To do so, we introduce Latent-representation-to-Quantization error Ratio (LQR) measures, which quantify the distance from the idealized neural codec's speech signal model for a given speech signal. We compare the proposed metrics to intrusive measures as well as data-driven supervised methods using two subjective speech quality datasets. This analysis shows that the proposed LQR correlates strongly (up to 0.9 Pearson's correlation) with the subjective quality of speech. Despite being a non-intrusive metric, this yields a competitive performance with, or even better than, other pre-trained and intrusive measures. These results show that LQR is a promising basis for more sophisticated speech quality measures.",
      "published": "2025-03-05T09:37:14Z"
    },
    "metadata": {
      "arxiv_id": "2503.03304",
      "title": "On the Relation Between Speech Quality and Quantized Latent Representations of Neural Codecs",
      "summary": "Neural audio signal codecs have attracted significant attention in recent years. In essence, the impressive low bitrate achieved by such encoders is enabled by learning an abstract representation that captures the properties of encoded signals, e.g., speech. In this work, we investigate the relation between the latent representation of the input signal learned by a neural codec and the quality of speech signals. To do so, we introduce Latent-representation-to-Quantization error Ratio (LQR) measures, which quantify the distance from the idealized neural codec's speech signal model for a given speech signal. We compare the proposed metrics to intrusive measures as well as data-driven supervised methods using two subjective speech quality datasets. This analysis shows that the proposed LQR correlates strongly (up to 0.9 Pearson's correlation) with the subjective quality of speech. Despite being a non-intrusive metric, this yields a competitive performance with, or even better than, other pre-trained and intrusive measures. These results show that LQR is a promising basis for more sophisticated speech quality measures.",
      "authors": [
        "Mhd Modar Halimeh",
        "Matteo Torcoli",
        "Philipp Grundhuber",
        "Emanuël A. P. Habets"
      ],
      "published": "2025-03-05T09:37:14Z",
      "updated": "2025-03-05T09:37:14Z",
      "categories": [
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2503.03304v1",
      "landing_url": "https://arxiv.org/abs/2503.03304v1",
      "doi": "https://doi.org/10.48550/arXiv.2503.03304"
    },
    "queries": [
      {
        "anchor": "spoken language models",
        "search_term": "neural codec"
      },
      {
        "anchor": "synchronous dialogue",
        "search_term": "neural codec"
      },
      {
        "anchor": "turn-taking",
        "search_term": "neural codec"
      }
    ]
  },
  {
    "arxiv_id": "2503.03474",
    "anchor": "spoken language models",
    "search_term": "true full-duplex",
    "search_record": {
      "id": "http://arxiv.org/abs/2503.03474v1",
      "title": "Enhancing Spoken Discourse Modeling in Language Models Using Gestural Cues",
      "summary": "Research in linguistics shows that non-verbal cues, such as gestures, play a crucial role in spoken discourse. For example, speakers perform hand gestures to indicate topic shifts, helping listeners identify transitions in discourse. In this work, we investigate whether the joint modeling of gestures using human motion sequences and language can improve spoken discourse modeling in language models. To integrate gestures into language models, we first encode 3D human motion sequences into discrete gesture tokens using a VQ-VAE. These gesture token embeddings are then aligned with text embeddings through feature alignment, mapping them into the text embedding space. To evaluate the gesture-aligned language model on spoken discourse, we construct text infilling tasks targeting three key discourse cues grounded in linguistic research: discourse connectives, stance markers, and quantifiers. Results show that incorporating gestures enhances marker prediction accuracy across the three tasks, highlighting the complementary information that gestures can offer in modeling spoken discourse. We view this work as an initial step toward leveraging non-verbal cues to advance spoken language modeling in language models.",
      "published": "2025-03-05T13:10:07Z"
    },
    "metadata": {
      "arxiv_id": "2503.03474",
      "title": "Enhancing Spoken Discourse Modeling in Language Models Using Gestural Cues",
      "summary": "Research in linguistics shows that non-verbal cues, such as gestures, play a crucial role in spoken discourse. For example, speakers perform hand gestures to indicate topic shifts, helping listeners identify transitions in discourse. In this work, we investigate whether the joint modeling of gestures using human motion sequences and language can improve spoken discourse modeling in language models. To integrate gestures into language models, we first encode 3D human motion sequences into discrete gesture tokens using a VQ-VAE. These gesture token embeddings are then aligned with text embeddings through feature alignment, mapping them into the text embedding space. To evaluate the gesture-aligned language model on spoken discourse, we construct text infilling tasks targeting three key discourse cues grounded in linguistic research: discourse connectives, stance markers, and quantifiers. Results show that incorporating gestures enhances marker prediction accuracy across the three tasks, highlighting the complementary information that gestures can offer in modeling spoken discourse. We view this work as an initial step toward leveraging non-verbal cues to advance spoken language modeling in language models.",
      "authors": [
        "Varsha Suresh",
        "M. Hamza Mughal",
        "Christian Theobalt",
        "Vera Demberg"
      ],
      "published": "2025-03-05T13:10:07Z",
      "updated": "2025-03-05T13:10:07Z",
      "categories": [
        "cs.CL"
      ],
      "pdf_url": "https://arxiv.org/pdf/2503.03474v1",
      "landing_url": "https://arxiv.org/abs/2503.03474v1",
      "doi": "https://doi.org/10.48550/arXiv.2503.03474"
    },
    "queries": [
      {
        "anchor": "spoken language models",
        "search_term": "true full-duplex"
      },
      {
        "anchor": "spoken language models",
        "search_term": "pseudo full-duplex"
      },
      {
        "anchor": "spoken language models",
        "search_term": "engineered synchronization"
      },
      {
        "anchor": "spoken language models",
        "search_term": "learned synchronization"
      },
      {
        "anchor": "spoken language models",
        "search_term": "synchronous data"
      },
      {
        "anchor": "spoken language models",
        "search_term": "multichannel dialogue"
      },
      {
        "anchor": "spoken language models",
        "search_term": "behavioral arbitration"
      },
      {
        "anchor": "spoken language models",
        "search_term": "floor transfer offset"
      },
      {
        "anchor": "spoken language models",
        "search_term": "stop latency"
      },
      {
        "anchor": "spoken language models",
        "search_term": "interruption response delay"
      }
    ]
  },
  {
    "arxiv_id": "2503.04721",
    "anchor": "full-duplex",
    "search_term": "true full-duplex",
    "search_record": {
      "id": "http://arxiv.org/abs/2503.04721v3",
      "title": "Full-Duplex-Bench: A Benchmark to Evaluate Full-duplex Spoken Dialogue Models on Turn-taking Capabilities",
      "summary": "Spoken dialogue modeling poses challenges beyond text-based language modeling, requiring real-time interaction, turn-taking, and backchanneling. While most Spoken Dialogue Models (SDMs) operate in half-duplex mode-processing one turn at a time - emerging full-duplex SDMs can listen and speak simultaneously, enabling more natural conversations. However, current evaluations remain limited, focusing mainly on turn-based metrics or coarse corpus-level analyses. To address this, we introduce Full-Duplex-Bench, a benchmark that systematically evaluates key interactive behaviors: pause handling, backchanneling, turn-taking, and interruption management. Our framework uses automatic metrics for consistent, reproducible assessment and provides a fair, fast evaluation setup. By releasing our benchmark and code, we aim to advance spoken dialogue modeling and foster the development of more natural and engaging SDMs.",
      "published": "2025-03-06T18:59:16Z"
    },
    "metadata": {
      "arxiv_id": "2503.04721",
      "title": "Full-Duplex-Bench: A Benchmark to Evaluate Full-duplex Spoken Dialogue Models on Turn-taking Capabilities",
      "summary": "Spoken dialogue modeling poses challenges beyond text-based language modeling, requiring real-time interaction, turn-taking, and backchanneling. While most Spoken Dialogue Models (SDMs) operate in half-duplex mode-processing one turn at a time - emerging full-duplex SDMs can listen and speak simultaneously, enabling more natural conversations. However, current evaluations remain limited, focusing mainly on turn-based metrics or coarse corpus-level analyses. To address this, we introduce Full-Duplex-Bench, a benchmark that systematically evaluates key interactive behaviors: pause handling, backchanneling, turn-taking, and interruption management. Our framework uses automatic metrics for consistent, reproducible assessment and provides a fair, fast evaluation setup. By releasing our benchmark and code, we aim to advance spoken dialogue modeling and foster the development of more natural and engaging SDMs.",
      "authors": [
        "Guan-Ting Lin",
        "Jiachen Lian",
        "Tingle Li",
        "Qirui Wang",
        "Gopala Anumanchipalli",
        "Alexander H. Liu",
        "Hung-yi Lee"
      ],
      "published": "2025-03-06T18:59:16Z",
      "updated": "2025-08-16T05:46:19Z",
      "categories": [
        "cs.CL",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2503.04721v3",
      "landing_url": "https://arxiv.org/abs/2503.04721v3",
      "doi": "https://doi.org/10.48550/arXiv.2503.04721"
    },
    "queries": [
      {
        "anchor": "full-duplex",
        "search_term": "true full-duplex"
      },
      {
        "anchor": "full-duplex",
        "search_term": "pseudo full-duplex"
      },
      {
        "anchor": "full-duplex",
        "search_term": "half duplex"
      },
      {
        "anchor": "full-duplex",
        "search_term": "engineered synchronization"
      },
      {
        "anchor": "full-duplex",
        "search_term": "learned synchronization"
      },
      {
        "anchor": "full-duplex",
        "search_term": "modular architecture"
      },
      {
        "anchor": "full-duplex",
        "search_term": "streaming architecture"
      },
      {
        "anchor": "full-duplex",
        "search_term": "real-time interaction"
      },
      {
        "anchor": "full-duplex",
        "search_term": "low latency"
      },
      {
        "anchor": "full-duplex",
        "search_term": "synchronous data"
      },
      {
        "anchor": "full-duplex",
        "search_term": "data scarcity"
      },
      {
        "anchor": "full-duplex",
        "search_term": "multichannel dialogue"
      },
      {
        "anchor": "full-duplex",
        "search_term": "temporal dynamics"
      },
      {
        "anchor": "full-duplex",
        "search_term": "behavioral arbitration"
      },
      {
        "anchor": "full-duplex",
        "search_term": "semantic coherence"
      },
      {
        "anchor": "full-duplex",
        "search_term": "neural codec"
      },
      {
        "anchor": "full-duplex",
        "search_term": "discrete tokens"
      },
      {
        "anchor": "full-duplex",
        "search_term": "audio tokens"
      },
      {
        "anchor": "full-duplex",
        "search_term": "floor transfer offset"
      },
      {
        "anchor": "full-duplex",
        "search_term": "stop latency"
      },
      {
        "anchor": "full-duplex",
        "search_term": "interruption response delay"
      },
      {
        "anchor": "synchronous dialogue",
        "search_term": "real-time interaction"
      },
      {
        "anchor": "turn-taking",
        "search_term": "true full-duplex"
      },
      {
        "anchor": "turn-taking",
        "search_term": "pseudo full-duplex"
      },
      {
        "anchor": "turn-taking",
        "search_term": "half duplex"
      },
      {
        "anchor": "turn-taking",
        "search_term": "engineered synchronization"
      },
      {
        "anchor": "turn-taking",
        "search_term": "learned synchronization"
      },
      {
        "anchor": "turn-taking",
        "search_term": "modular architecture"
      },
      {
        "anchor": "turn-taking",
        "search_term": "streaming architecture"
      },
      {
        "anchor": "turn-taking",
        "search_term": "real-time interaction"
      },
      {
        "anchor": "turn-taking",
        "search_term": "low latency"
      },
      {
        "anchor": "turn-taking",
        "search_term": "synchronous data"
      },
      {
        "anchor": "turn-taking",
        "search_term": "data scarcity"
      },
      {
        "anchor": "turn-taking",
        "search_term": "multichannel dialogue"
      },
      {
        "anchor": "turn-taking",
        "search_term": "temporal dynamics"
      },
      {
        "anchor": "turn-taking",
        "search_term": "behavioral arbitration"
      },
      {
        "anchor": "turn-taking",
        "search_term": "semantic coherence"
      },
      {
        "anchor": "turn-taking",
        "search_term": "neural codec"
      },
      {
        "anchor": "turn-taking",
        "search_term": "discrete tokens"
      },
      {
        "anchor": "turn-taking",
        "search_term": "audio tokens"
      },
      {
        "anchor": "turn-taking",
        "search_term": "floor transfer offset"
      },
      {
        "anchor": "turn-taking",
        "search_term": "stop latency"
      },
      {
        "anchor": "turn-taking",
        "search_term": "interruption response delay"
      }
    ]
  },
  {
    "arxiv_id": "2503.04786",
    "anchor": "synchronous dialogue",
    "search_term": "temporal dynamics",
    "search_record": {
      "id": "http://arxiv.org/abs/2503.04786v2",
      "title": "Analyzing the temporal dynamics of linguistic features contained in misinformation",
      "summary": "Consumption of misinformation can lead to negative consequences that impact the individual and society. To help mitigate the influence of misinformation on human beliefs, algorithmic labels providing context about content accuracy and source reliability have been developed. Since the linguistic features used by algorithms to estimate information accuracy can change across time, it is important to understand their temporal dynamics. As a result, this study uses natural language processing to analyze PolitiFact statements spanning between 2010 and 2024 to quantify how the sources and linguistic features of misinformation change between five-year time periods. The results show that statement sentiment has decreased significantly over time, reflecting a generally more negative tone in PolitiFact statements. Moreover, statements associated with misinformation realize significantly lower sentiment than accurate information. Additional analysis shows that recent time periods are dominated by sources from online social networks and other digital forums, such as blogs and viral images, that contain high levels of misinformation containing negative sentiment. In contrast, most statements during early time periods are attributed to individual sources (i.e., politicians) that are relatively balanced in accuracy ratings and contain statements with neutral or positive sentiment. Named-entity recognition was used to identify that presidential incumbents and candidates are relatively more prevalent in statements containing misinformation, while US states tend to be present in accurate information. Finally, entity labels associated with people and organizations are more common in misinformation, while accurate statements are more likely to contain numeric entity labels, such as percentages and dates.",
      "published": "2025-02-27T14:15:43Z"
    },
    "metadata": {
      "arxiv_id": "2503.04786",
      "title": "Analyzing the temporal dynamics of linguistic features contained in misinformation",
      "summary": "Consumption of misinformation can lead to negative consequences that impact the individual and society. To help mitigate the influence of misinformation on human beliefs, algorithmic labels providing context about content accuracy and source reliability have been developed. Since the linguistic features used by algorithms to estimate information accuracy can change across time, it is important to understand their temporal dynamics. As a result, this study uses natural language processing to analyze PolitiFact statements spanning between 2010 and 2024 to quantify how the sources and linguistic features of misinformation change between five-year time periods. The results show that statement sentiment has decreased significantly over time, reflecting a generally more negative tone in PolitiFact statements. Moreover, statements associated with misinformation realize significantly lower sentiment than accurate information. Additional analysis shows that recent time periods are dominated by sources from online social networks and other digital forums, such as blogs and viral images, that contain high levels of misinformation containing negative sentiment. In contrast, most statements during early time periods are attributed to individual sources (i.e., politicians) that are relatively balanced in accuracy ratings and contain statements with neutral or positive sentiment. Named-entity recognition was used to identify that presidential incumbents and candidates are relatively more prevalent in statements containing misinformation, while US states tend to be present in accurate information. Finally, entity labels associated with people and organizations are more common in misinformation, while accurate statements are more likely to contain numeric entity labels, such as percentages and dates.",
      "authors": [
        "Erik J Schlicht"
      ],
      "published": "2025-02-27T14:15:43Z",
      "updated": "2025-03-10T01:43:38Z",
      "categories": [
        "cs.CL",
        "cs.SI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2503.04786v2",
      "landing_url": "https://arxiv.org/abs/2503.04786v2",
      "doi": "https://doi.org/10.48550/arXiv.2503.04786"
    },
    "queries": [
      {
        "anchor": "synchronous dialogue",
        "search_term": "temporal dynamics"
      }
    ]
  },
  {
    "arxiv_id": "2503.06197",
    "anchor": "spoken language models",
    "search_term": "modular architecture",
    "search_record": {
      "id": "http://arxiv.org/abs/2503.06197v1",
      "title": "FALCON: A Framework for Fault Prediction in Open RAN Using Multi-Level Telemetry",
      "summary": "O-RAN has brought in deployment flexibility and intelligent RAN control for mobile operators through its disaggregated and modular architecture using open interfaces. However, this disaggregation introduces complexities in system integration and network management, as components are often sourced from different vendors. In addition, the operators who are relying on open source and virtualized components -- which are deployed on commodity hardware -- require additional resilient solutions as O-RAN deployments suffer from the risk of failures at multiple levels including infrastructure, platform, and RAN levels. To address these challenges, this paper proposes FALCON, a fault prediction framework for O-RAN, which leverages infrastructure-, platform-, and RAN-level telemetry to predict faults in virtualized O-RAN deployments. By aggregating and analyzing metrics from various components at different levels using AI/ML models, the FALCON framework enables proactive fault management, providing operators with actionable insights to implement timely preventive measures. The FALCON framework, using a Random Forest classifier, outperforms two other classifiers on the predicted telemetry, achieving an average accuracy and F1-score of more than 98%.",
      "published": "2025-03-08T12:40:55Z"
    },
    "metadata": {
      "arxiv_id": "2503.06197",
      "title": "FALCON: A Framework for Fault Prediction in Open RAN Using Multi-Level Telemetry",
      "summary": "O-RAN has brought in deployment flexibility and intelligent RAN control for mobile operators through its disaggregated and modular architecture using open interfaces. However, this disaggregation introduces complexities in system integration and network management, as components are often sourced from different vendors. In addition, the operators who are relying on open source and virtualized components -- which are deployed on commodity hardware -- require additional resilient solutions as O-RAN deployments suffer from the risk of failures at multiple levels including infrastructure, platform, and RAN levels. To address these challenges, this paper proposes FALCON, a fault prediction framework for O-RAN, which leverages infrastructure-, platform-, and RAN-level telemetry to predict faults in virtualized O-RAN deployments. By aggregating and analyzing metrics from various components at different levels using AI/ML models, the FALCON framework enables proactive fault management, providing operators with actionable insights to implement timely preventive measures. The FALCON framework, using a Random Forest classifier, outperforms two other classifiers on the predicted telemetry, achieving an average accuracy and F1-score of more than 98%.",
      "authors": [
        "Yaswanth Kumar LS",
        "Somya Jain",
        "Bheemarjuna Reddy Tamma",
        "Koteswararao Kondepu"
      ],
      "published": "2025-03-08T12:40:55Z",
      "updated": "2025-03-08T12:40:55Z",
      "categories": [
        "cs.NI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2503.06197v1",
      "landing_url": "https://arxiv.org/abs/2503.06197v1",
      "doi": "https://doi.org/10.48550/arXiv.2503.06197"
    },
    "queries": [
      {
        "anchor": "spoken language models",
        "search_term": "modular architecture"
      },
      {
        "anchor": "synchronous dialogue",
        "search_term": "modular architecture"
      },
      {
        "anchor": "turn-taking",
        "search_term": "modular architecture"
      }
    ]
  },
  {
    "arxiv_id": "2503.06241",
    "anchor": "turn-taking",
    "search_term": "true full-duplex",
    "search_record": {
      "id": "http://arxiv.org/abs/2503.06241v2",
      "title": "A Noise-Robust Turn-Taking System for Real-World Dialogue Robots: A Field Experiment",
      "summary": "Turn-taking is a crucial aspect of human-robot interaction, directly influencing conversational fluidity and user engagement. While previous research has explored turn-taking models in controlled environments, their robustness in real-world settings remains underexplored. In this study, we propose a noise-robust voice activity projection (VAP) model, based on a Transformer architecture, to enhance real-time turn-taking in dialogue robots. To evaluate the effectiveness of the proposed system, we conducted a field experiment in a shopping mall, comparing the VAP system with a conventional cloud-based speech recognition system. Our analysis covered both subjective user evaluations and objective behavioral analysis. The results showed that the proposed system significantly reduced response latency, leading to a more natural conversation where both the robot and users responded faster. The subjective evaluations suggested that faster responses contribute to a better interaction experience.",
      "published": "2025-03-08T14:53:20Z"
    },
    "metadata": {
      "arxiv_id": "2503.06241",
      "title": "A Noise-Robust Turn-Taking System for Real-World Dialogue Robots: A Field Experiment",
      "summary": "Turn-taking is a crucial aspect of human-robot interaction, directly influencing conversational fluidity and user engagement. While previous research has explored turn-taking models in controlled environments, their robustness in real-world settings remains underexplored. In this study, we propose a noise-robust voice activity projection (VAP) model, based on a Transformer architecture, to enhance real-time turn-taking in dialogue robots. To evaluate the effectiveness of the proposed system, we conducted a field experiment in a shopping mall, comparing the VAP system with a conventional cloud-based speech recognition system. Our analysis covered both subjective user evaluations and objective behavioral analysis. The results showed that the proposed system significantly reduced response latency, leading to a more natural conversation where both the robot and users responded faster. The subjective evaluations suggested that faster responses contribute to a better interaction experience.",
      "authors": [
        "Koji Inoue",
        "Yuki Okafuji",
        "Jun Baba",
        "Yoshiki Ohira",
        "Katsuya Hyodo",
        "Tatsuya Kawahara"
      ],
      "published": "2025-03-08T14:53:20Z",
      "updated": "2025-07-14T11:04:50Z",
      "categories": [
        "cs.RO",
        "cs.CL",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2503.06241v2",
      "landing_url": "https://arxiv.org/abs/2503.06241v2",
      "doi": "https://doi.org/10.48550/arXiv.2503.06241"
    },
    "queries": [
      {
        "anchor": "turn-taking",
        "search_term": "true full-duplex"
      },
      {
        "anchor": "turn-taking",
        "search_term": "pseudo full-duplex"
      },
      {
        "anchor": "turn-taking",
        "search_term": "half duplex"
      },
      {
        "anchor": "turn-taking",
        "search_term": "engineered synchronization"
      },
      {
        "anchor": "turn-taking",
        "search_term": "learned synchronization"
      },
      {
        "anchor": "turn-taking",
        "search_term": "modular architecture"
      },
      {
        "anchor": "turn-taking",
        "search_term": "streaming architecture"
      },
      {
        "anchor": "turn-taking",
        "search_term": "real-time interaction"
      },
      {
        "anchor": "turn-taking",
        "search_term": "low latency"
      },
      {
        "anchor": "turn-taking",
        "search_term": "synchronous data"
      },
      {
        "anchor": "turn-taking",
        "search_term": "data scarcity"
      },
      {
        "anchor": "turn-taking",
        "search_term": "multichannel dialogue"
      },
      {
        "anchor": "turn-taking",
        "search_term": "temporal dynamics"
      },
      {
        "anchor": "turn-taking",
        "search_term": "behavioral arbitration"
      },
      {
        "anchor": "turn-taking",
        "search_term": "semantic coherence"
      },
      {
        "anchor": "turn-taking",
        "search_term": "neural codec"
      },
      {
        "anchor": "turn-taking",
        "search_term": "discrete tokens"
      },
      {
        "anchor": "turn-taking",
        "search_term": "audio tokens"
      },
      {
        "anchor": "turn-taking",
        "search_term": "floor transfer offset"
      },
      {
        "anchor": "turn-taking",
        "search_term": "stop latency"
      },
      {
        "anchor": "turn-taking",
        "search_term": "interruption response delay"
      }
    ]
  },
  {
    "arxiv_id": "2503.09645",
    "anchor": "synchronous dialogue",
    "search_term": "discrete tokens",
    "search_record": {
      "id": "http://arxiv.org/abs/2503.09645v1",
      "title": "Global Position Aware Group Choreography using Large Language Model",
      "summary": "Dance serves as a profound and universal expression of human culture, conveying emotions and stories through movements synchronized with music. Although some current works have achieved satisfactory results in the task of single-person dance generation, the field of multi-person dance generation remains relatively novel. In this work, we present a group choreography framework that leverages recent advancements in Large Language Models (LLM) by modeling the group dance generation problem as a sequence-to-sequence translation task. Our framework consists of a tokenizer that transforms continuous features into discrete tokens, and an LLM that is fine-tuned to predict motion tokens given the audio tokens. We show that by proper tokenization of input modalities and careful design of the LLM training strategies, our framework can generate realistic and diverse group dances while maintaining strong music correlation and dancer-wise consistency. Extensive experiments and evaluations demonstrate that our framework achieves state-of-the-art performance.",
      "published": "2025-03-12T07:25:32Z"
    },
    "metadata": {
      "arxiv_id": "2503.09645",
      "title": "Global Position Aware Group Choreography using Large Language Model",
      "summary": "Dance serves as a profound and universal expression of human culture, conveying emotions and stories through movements synchronized with music. Although some current works have achieved satisfactory results in the task of single-person dance generation, the field of multi-person dance generation remains relatively novel. In this work, we present a group choreography framework that leverages recent advancements in Large Language Models (LLM) by modeling the group dance generation problem as a sequence-to-sequence translation task. Our framework consists of a tokenizer that transforms continuous features into discrete tokens, and an LLM that is fine-tuned to predict motion tokens given the audio tokens. We show that by proper tokenization of input modalities and careful design of the LLM training strategies, our framework can generate realistic and diverse group dances while maintaining strong music correlation and dancer-wise consistency. Extensive experiments and evaluations demonstrate that our framework achieves state-of-the-art performance.",
      "authors": [
        "Haozhou Pang",
        "Tianwei Ding",
        "Lanshan He",
        "Qi Gan"
      ],
      "published": "2025-03-12T07:25:32Z",
      "updated": "2025-03-12T07:25:32Z",
      "categories": [
        "cs.GR",
        "cs.CL",
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2503.09645v1",
      "landing_url": "https://arxiv.org/abs/2503.09645v1",
      "doi": "https://doi.org/10.48550/arXiv.2503.09645"
    },
    "queries": [
      {
        "anchor": "synchronous dialogue",
        "search_term": "discrete tokens"
      },
      {
        "anchor": "synchronous dialogue",
        "search_term": "audio tokens"
      }
    ]
  },
  {
    "arxiv_id": "2503.12102",
    "anchor": "synchronous dialogue",
    "search_term": "synchronous data",
    "search_record": {
      "id": "http://arxiv.org/abs/2503.12102v1",
      "title": "A Speech-to-Video Synthesis Approach Using Spatio-Temporal Diffusion for Vocal Tract MRI",
      "summary": "Understanding the relationship between vocal tract motion during speech and the resulting acoustic signal is crucial for aided clinical assessment and developing personalized treatment and rehabilitation strategies. Toward this goal, we introduce an audio-to-video generation framework for creating Real Time/cine-Magnetic Resonance Imaging (RT-/cine-MRI) visuals of the vocal tract from speech signals. Our framework first preprocesses RT-/cine-MRI sequences and speech samples to achieve temporal alignment, ensuring synchronization between visual and audio data. We then employ a modified stable diffusion model, integrating structural and temporal blocks, to effectively capture movement characteristics and temporal dynamics in the synchronized data. This process enables the generation of MRI sequences from new speech inputs, improving the conversion of audio into visual data. We evaluated our framework on healthy controls and tongue cancer patients by analyzing and comparing the vocal tract movements in synthesized videos. Our framework demonstrated adaptability to new speech inputs and effective generalization. In addition, positive human evaluations confirmed its effectiveness, with realistic and accurate visualizations, suggesting its potential for outpatient therapy and personalized simulation of vocal tract visualizations.",
      "published": "2025-03-15T12:12:50Z"
    },
    "metadata": {
      "arxiv_id": "2503.12102",
      "title": "A Speech-to-Video Synthesis Approach Using Spatio-Temporal Diffusion for Vocal Tract MRI",
      "summary": "Understanding the relationship between vocal tract motion during speech and the resulting acoustic signal is crucial for aided clinical assessment and developing personalized treatment and rehabilitation strategies. Toward this goal, we introduce an audio-to-video generation framework for creating Real Time/cine-Magnetic Resonance Imaging (RT-/cine-MRI) visuals of the vocal tract from speech signals. Our framework first preprocesses RT-/cine-MRI sequences and speech samples to achieve temporal alignment, ensuring synchronization between visual and audio data. We then employ a modified stable diffusion model, integrating structural and temporal blocks, to effectively capture movement characteristics and temporal dynamics in the synchronized data. This process enables the generation of MRI sequences from new speech inputs, improving the conversion of audio into visual data. We evaluated our framework on healthy controls and tongue cancer patients by analyzing and comparing the vocal tract movements in synthesized videos. Our framework demonstrated adaptability to new speech inputs and effective generalization. In addition, positive human evaluations confirmed its effectiveness, with realistic and accurate visualizations, suggesting its potential for outpatient therapy and personalized simulation of vocal tract visualizations.",
      "authors": [
        "Paula Andrea Pérez-Toro",
        "Tomás Arias-Vergara",
        "Fangxu Xing",
        "Xiaofeng Liu",
        "Maureen Stone",
        "Jiachen Zhuo",
        "Juan Rafael Orozco-Arroyave",
        "Elmar Nöth",
        "Jana Hutter",
        "Jerry L. Prince",
        "Andreas Maier",
        "Jonghye Woo"
      ],
      "published": "2025-03-15T12:12:50Z",
      "updated": "2025-03-15T12:12:50Z",
      "categories": [
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2503.12102v1",
      "landing_url": "https://arxiv.org/abs/2503.12102v1",
      "doi": "https://doi.org/10.48550/arXiv.2503.12102"
    },
    "queries": [
      {
        "anchor": "synchronous dialogue",
        "search_term": "synchronous data"
      }
    ]
  },
  {
    "arxiv_id": "2503.12115",
    "anchor": "spoken language models",
    "search_term": "neural codec",
    "search_record": {
      "id": "http://arxiv.org/abs/2503.12115v2",
      "title": "Universal Speech Token Learning via Low-Bitrate Neural Codec and Pretrained Representations",
      "summary": "Current large speech language models are mainly based on semantic tokens from discretization of self-supervised learned representations and acoustic tokens from a neural codec, following a semantic-modeling and acoustic-synthesis paradigm. However, semantic tokens discard paralinguistic attributes of speakers that is important for natural spoken communication, while prompt-based acoustic synthesis from semantic tokens has limits in recovering paralinguistic details and suffers from robustness issues, especially when there are domain gaps between the prompt and the target. This paper unifies two types of tokens and proposes the UniCodec, a universal speech token learning that encapsulates all semantics of speech, including linguistic and paralinguistic information, into a compact and semantically-disentangled unified token. Such a unified token can not only benefit speech language models in understanding with paralinguistic hints but also help speech generation with high-quality output. A low-bitrate neural codec is leveraged to learn such disentangled discrete representations at global and local scales, with knowledge distilled from self-supervised learned features. Extensive evaluations on multilingual datasets demonstrate its effectiveness in generating natural, expressive and long-term consistent output quality with paralinguistic attributes well preserved in several speech processing tasks.",
      "published": "2025-03-15T12:50:43Z"
    },
    "metadata": {
      "arxiv_id": "2503.12115",
      "title": "Universal Speech Token Learning via Low-Bitrate Neural Codec and Pretrained Representations",
      "summary": "Current large speech language models are mainly based on semantic tokens from discretization of self-supervised learned representations and acoustic tokens from a neural codec, following a semantic-modeling and acoustic-synthesis paradigm. However, semantic tokens discard paralinguistic attributes of speakers that is important for natural spoken communication, while prompt-based acoustic synthesis from semantic tokens has limits in recovering paralinguistic details and suffers from robustness issues, especially when there are domain gaps between the prompt and the target. This paper unifies two types of tokens and proposes the UniCodec, a universal speech token learning that encapsulates all semantics of speech, including linguistic and paralinguistic information, into a compact and semantically-disentangled unified token. Such a unified token can not only benefit speech language models in understanding with paralinguistic hints but also help speech generation with high-quality output. A low-bitrate neural codec is leveraged to learn such disentangled discrete representations at global and local scales, with knowledge distilled from self-supervised learned features. Extensive evaluations on multilingual datasets demonstrate its effectiveness in generating natural, expressive and long-term consistent output quality with paralinguistic attributes well preserved in several speech processing tasks.",
      "authors": [
        "Xue Jiang",
        "Xiulian Peng",
        "Yuan Zhang",
        "Yan Lu"
      ],
      "published": "2025-03-15T12:50:43Z",
      "updated": "2025-10-15T06:52:30Z",
      "categories": [
        "cs.SD",
        "cs.AI",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2503.12115v2",
      "landing_url": "https://arxiv.org/abs/2503.12115v2",
      "doi": "https://doi.org/10.1109/JSTSP.2024.3488557"
    },
    "queries": [
      {
        "anchor": "spoken language models",
        "search_term": "neural codec"
      },
      {
        "anchor": "synchronous dialogue",
        "search_term": "neural codec"
      },
      {
        "anchor": "turn-taking",
        "search_term": "neural codec"
      }
    ]
  },
  {
    "arxiv_id": "2503.12382",
    "anchor": "synchronous dialogue",
    "search_term": "neural codec",
    "search_record": {
      "id": "http://arxiv.org/abs/2503.12382v1",
      "title": "RENO: Real-Time Neural Compression for 3D LiDAR Point Clouds",
      "summary": "Despite the substantial advancements demonstrated by learning-based neural models in the LiDAR Point Cloud Compression (LPCC) task, realizing real-time compression - an indispensable criterion for numerous industrial applications - remains a formidable challenge. This paper proposes RENO, the first real-time neural codec for 3D LiDAR point clouds, achieving superior performance with a lightweight model. RENO skips the octree construction and directly builds upon the multiscale sparse tensor representation. Instead of the multi-stage inferring, RENO devises sparse occupancy codes, which exploit cross-scale correlation and derive voxels' occupancy in a one-shot manner, greatly saving processing time. Experimental results demonstrate that the proposed RENO achieves real-time coding speed, 10 fps at 14-bit depth on a desktop platform (e.g., one RTX 3090 GPU) for both encoding and decoding processes, while providing 12.25% and 48.34% bit-rate savings compared to G-PCCv23 and Draco, respectively, at a similar quality. RENO model size is merely 1MB, making it attractive for practical applications. The source code is available at https://github.com/NJUVISION/RENO.",
      "published": "2025-03-16T07:03:12Z"
    },
    "metadata": {
      "arxiv_id": "2503.12382",
      "title": "RENO: Real-Time Neural Compression for 3D LiDAR Point Clouds",
      "summary": "Despite the substantial advancements demonstrated by learning-based neural models in the LiDAR Point Cloud Compression (LPCC) task, realizing real-time compression - an indispensable criterion for numerous industrial applications - remains a formidable challenge. This paper proposes RENO, the first real-time neural codec for 3D LiDAR point clouds, achieving superior performance with a lightweight model. RENO skips the octree construction and directly builds upon the multiscale sparse tensor representation. Instead of the multi-stage inferring, RENO devises sparse occupancy codes, which exploit cross-scale correlation and derive voxels' occupancy in a one-shot manner, greatly saving processing time. Experimental results demonstrate that the proposed RENO achieves real-time coding speed, 10 fps at 14-bit depth on a desktop platform (e.g., one RTX 3090 GPU) for both encoding and decoding processes, while providing 12.25% and 48.34% bit-rate savings compared to G-PCCv23 and Draco, respectively, at a similar quality. RENO model size is merely 1MB, making it attractive for practical applications. The source code is available at https://github.com/NJUVISION/RENO.",
      "authors": [
        "Kang You",
        "Tong Chen",
        "Dandan Ding",
        "M. Salman Asif",
        "Zhan Ma"
      ],
      "published": "2025-03-16T07:03:12Z",
      "updated": "2025-03-16T07:03:12Z",
      "categories": [
        "cs.CV",
        "eess.IV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2503.12382v1",
      "landing_url": "https://arxiv.org/abs/2503.12382v1",
      "doi": "https://doi.org/10.48550/arXiv.2503.12382"
    },
    "queries": [
      {
        "anchor": "synchronous dialogue",
        "search_term": "neural codec"
      }
    ]
  },
  {
    "arxiv_id": "2503.14547",
    "anchor": "synchronous dialogue",
    "search_term": "synchronous data",
    "search_record": {
      "id": "http://arxiv.org/abs/2503.14547v1",
      "title": "Matching Skeleton-based Activity Representations with Heterogeneous Signals for HAR",
      "summary": "In human activity recognition (HAR), activity labels have typically been encoded in one-hot format, which has a recent shift towards using textual representations to provide contextual knowledge. Here, we argue that HAR should be anchored to physical motion data, as motion forms the basis of activity and applies effectively across sensing systems, whereas text is inherently limited. We propose SKELAR, a novel HAR framework that pretrains activity representations from skeleton data and matches them with heterogeneous HAR signals. Our method addresses two major challenges: (1) capturing core motion knowledge without context-specific details. We achieve this through a self-supervised coarse angle reconstruction task that recovers joint rotation angles, invariant to both users and deployments; (2) adapting the representations to downstream tasks with varying modalities and focuses. To address this, we introduce a self-attention matching module that dynamically prioritizes relevant body parts in a data-driven manner. Given the lack of corresponding labels in existing skeleton data, we establish MASD, a new HAR dataset with IMU, WiFi, and skeleton, collected from 20 subjects performing 27 activities. This is the first broadly applicable HAR dataset with time-synchronized data across three modalities. Experiments show that SKELAR achieves the state-of-the-art performance in both full-shot and few-shot settings. We also demonstrate that SKELAR can effectively leverage synthetic skeleton data to extend its use in scenarios without skeleton collections.",
      "published": "2025-03-17T18:43:06Z"
    },
    "metadata": {
      "arxiv_id": "2503.14547",
      "title": "Matching Skeleton-based Activity Representations with Heterogeneous Signals for HAR",
      "summary": "In human activity recognition (HAR), activity labels have typically been encoded in one-hot format, which has a recent shift towards using textual representations to provide contextual knowledge. Here, we argue that HAR should be anchored to physical motion data, as motion forms the basis of activity and applies effectively across sensing systems, whereas text is inherently limited. We propose SKELAR, a novel HAR framework that pretrains activity representations from skeleton data and matches them with heterogeneous HAR signals. Our method addresses two major challenges: (1) capturing core motion knowledge without context-specific details. We achieve this through a self-supervised coarse angle reconstruction task that recovers joint rotation angles, invariant to both users and deployments; (2) adapting the representations to downstream tasks with varying modalities and focuses. To address this, we introduce a self-attention matching module that dynamically prioritizes relevant body parts in a data-driven manner. Given the lack of corresponding labels in existing skeleton data, we establish MASD, a new HAR dataset with IMU, WiFi, and skeleton, collected from 20 subjects performing 27 activities. This is the first broadly applicable HAR dataset with time-synchronized data across three modalities. Experiments show that SKELAR achieves the state-of-the-art performance in both full-shot and few-shot settings. We also demonstrate that SKELAR can effectively leverage synthetic skeleton data to extend its use in scenarios without skeleton collections.",
      "authors": [
        "Shuheng Li",
        "Jiayun Zhang",
        "Xiaohan Fu",
        "Xiyuan Zhang",
        "Jingbo Shang",
        "Rajesh K. Gupta"
      ],
      "published": "2025-03-17T18:43:06Z",
      "updated": "2025-03-17T18:43:06Z",
      "categories": [
        "cs.CV",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2503.14547v1",
      "landing_url": "https://arxiv.org/abs/2503.14547v1",
      "doi": "https://doi.org/10.48550/arXiv.2503.14547"
    },
    "queries": [
      {
        "anchor": "synchronous dialogue",
        "search_term": "synchronous data"
      }
    ]
  },
  {
    "arxiv_id": "2503.14756",
    "anchor": "spoken language models",
    "search_term": "semantic coherence",
    "search_record": {
      "id": "http://arxiv.org/abs/2503.14756v2",
      "title": "SceneEval: Evaluating Semantic Coherence in Text-Conditioned 3D Indoor Scene Synthesis",
      "summary": "Despite recent advances in text-conditioned 3D indoor scene generation, there remain gaps in the evaluation of these methods. Existing metrics primarily assess the realism of generated scenes by comparing them to a set of ground-truth scenes, often overlooking alignment with the input text - a critical factor in determining how effectively a method meets user requirements. We present SceneEval, an evaluation framework designed to address this limitation. SceneEval includes metrics for both explicit user requirements, such as the presence of specific objects and their attributes described in the input text, and implicit expectations, like the absence of object collisions, providing a comprehensive assessment of scene quality. To facilitate evaluation, we introduce SceneEval-500, a dataset of scene descriptions with annotated ground-truth scene properties. We evaluate recent scene generation methods using SceneEval and demonstrate its ability to provide detailed assessments of the generated scenes, highlighting strengths and areas for improvement across multiple dimensions. Our results show that current methods struggle at generating scenes that meet user requirements, underscoring the need for further research in this direction.",
      "published": "2025-03-18T22:02:35Z"
    },
    "metadata": {
      "arxiv_id": "2503.14756",
      "title": "SceneEval: Evaluating Semantic Coherence in Text-Conditioned 3D Indoor Scene Synthesis",
      "summary": "Despite recent advances in text-conditioned 3D indoor scene generation, there remain gaps in the evaluation of these methods. Existing metrics primarily assess the realism of generated scenes by comparing them to a set of ground-truth scenes, often overlooking alignment with the input text - a critical factor in determining how effectively a method meets user requirements. We present SceneEval, an evaluation framework designed to address this limitation. SceneEval includes metrics for both explicit user requirements, such as the presence of specific objects and their attributes described in the input text, and implicit expectations, like the absence of object collisions, providing a comprehensive assessment of scene quality. To facilitate evaluation, we introduce SceneEval-500, a dataset of scene descriptions with annotated ground-truth scene properties. We evaluate recent scene generation methods using SceneEval and demonstrate its ability to provide detailed assessments of the generated scenes, highlighting strengths and areas for improvement across multiple dimensions. Our results show that current methods struggle at generating scenes that meet user requirements, underscoring the need for further research in this direction.",
      "authors": [
        "Hou In Ivan Tam",
        "Hou In Derek Pun",
        "Austin T. Wang",
        "Angel X. Chang",
        "Manolis Savva"
      ],
      "published": "2025-03-18T22:02:35Z",
      "updated": "2025-06-11T06:42:00Z",
      "categories": [
        "cs.GR",
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2503.14756v2",
      "landing_url": "https://arxiv.org/abs/2503.14756v2",
      "doi": "https://doi.org/10.48550/arXiv.2503.14756"
    },
    "queries": [
      {
        "anchor": "spoken language models",
        "search_term": "semantic coherence"
      },
      {
        "anchor": "synchronous dialogue",
        "search_term": "semantic coherence"
      }
    ]
  },
  {
    "arxiv_id": "2503.15036",
    "anchor": "spoken language models",
    "search_term": "semantic coherence",
    "search_record": {
      "id": "http://arxiv.org/abs/2503.15036v2",
      "title": "Multivariate Gaussian Topic Modelling: A novel approach to discover topics with greater semantic coherence",
      "summary": "An important aspect of text mining involves information retrieval in form of discovery of semantic themes (topics) from documents using topic modelling. While generative topic models like Latent Dirichlet Allocation (LDA) or Latent Semantic Analysis (LSA) elegantly model topics as probability distributions and are useful in identifying latent topics from large document corpora with minimal supervision, they suffer from difficulty in topic interpretability and reduced performance in shorter texts. Here we propose a novel Multivariate Gaussian Topic Model (MGTM). In this approach topics are presented as Multivariate Gaussian Distributions and documents as Gaussian Mixture Models. Applying EM algorithm on a document corpus, the various constituent Multivariate Gaussian distributions corresponding to the latent topics and their respective parameters are identified. Analysis of the parameters of each distribution helps identify the respective topic keywords, and from these key-words topic annotations are carried out. This approach is applied on 20 newsgroups dataset to demonstrate the interpretability benefits vis-`a-vis 4 other benchmark models. The effectiveness of this model in capturing the semantic theme of the topics with high interpretability is examined by calculating the topic coherence and comparing the coherence values with benchmark models. This model achieves a highest mean topic coherence (0.7) and median topic coherence (0.76) vis-`a-vis the benchmark models, demonstrating high effectiveness in identifying interpretable, semantically coherent topics.",
      "published": "2025-03-19T09:25:54Z"
    },
    "metadata": {
      "arxiv_id": "2503.15036",
      "title": "Multivariate Gaussian Topic Modelling: A novel approach to discover topics with greater semantic coherence",
      "summary": "An important aspect of text mining involves information retrieval in form of discovery of semantic themes (topics) from documents using topic modelling. While generative topic models like Latent Dirichlet Allocation (LDA) or Latent Semantic Analysis (LSA) elegantly model topics as probability distributions and are useful in identifying latent topics from large document corpora with minimal supervision, they suffer from difficulty in topic interpretability and reduced performance in shorter texts. Here we propose a novel Multivariate Gaussian Topic Model (MGTM). In this approach topics are presented as Multivariate Gaussian Distributions and documents as Gaussian Mixture Models. Applying EM algorithm on a document corpus, the various constituent Multivariate Gaussian distributions corresponding to the latent topics and their respective parameters are identified. Analysis of the parameters of each distribution helps identify the respective topic keywords, and from these key-words topic annotations are carried out. This approach is applied on 20 newsgroups dataset to demonstrate the interpretability benefits vis-`a-vis 4 other benchmark models. The effectiveness of this model in capturing the semantic theme of the topics with high interpretability is examined by calculating the topic coherence and comparing the coherence values with benchmark models. This model achieves a highest mean topic coherence (0.7) and median topic coherence (0.76) vis-`a-vis the benchmark models, demonstrating high effectiveness in identifying interpretable, semantically coherent topics.",
      "authors": [
        "Satyajeet Sahoo",
        "Jhareswar Maiti"
      ],
      "published": "2025-03-19T09:25:54Z",
      "updated": "2025-11-01T12:16:39Z",
      "categories": [
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2503.15036v2",
      "landing_url": "https://arxiv.org/abs/2503.15036v2",
      "doi": "https://doi.org/10.48550/arXiv.2503.15036"
    },
    "queries": [
      {
        "anchor": "spoken language models",
        "search_term": "semantic coherence"
      },
      {
        "anchor": "synchronous dialogue",
        "search_term": "semantic coherence"
      },
      {
        "anchor": "turn-taking",
        "search_term": "semantic coherence"
      }
    ]
  },
  {
    "arxiv_id": "2503.15796",
    "anchor": "spoken language models",
    "search_term": "data scarcity",
    "search_record": {
      "id": "http://arxiv.org/abs/2503.15796v1",
      "title": "Blend the Separated: Mixture of Synergistic Experts for Data-Scarcity Drug-Target Interaction Prediction",
      "summary": "Drug-target interaction prediction (DTI) is essential in various applications including drug discovery and clinical application. There are two perspectives of input data widely used in DTI prediction: Intrinsic data represents how drugs or targets are constructed, and extrinsic data represents how drugs or targets are related to other biological entities. However, any of the two perspectives of input data can be scarce for some drugs or targets, especially for those unpopular or newly discovered. Furthermore, ground-truth labels for specific interaction types can also be scarce. Therefore, we propose the first method to tackle DTI prediction under input data and/or label scarcity. To make our model functional when only one perspective of input data is available, we design two separate experts to process intrinsic and extrinsic data respectively and fuse them adaptively according to different samples. Furthermore, to make the two perspectives complement each other and remedy label scarcity, two experts synergize with each other in a mutually supervised way to exploit the enormous unlabeled data. Extensive experiments on 3 real-world datasets under different extents of input data scarcity and/or label scarcity demonstrate our model outperforms states of the art significantly and steadily, with a maximum improvement of 53.53%. We also test our model without any data scarcity and it still outperforms current methods.",
      "published": "2025-03-20T02:27:16Z"
    },
    "metadata": {
      "arxiv_id": "2503.15796",
      "title": "Blend the Separated: Mixture of Synergistic Experts for Data-Scarcity Drug-Target Interaction Prediction",
      "summary": "Drug-target interaction prediction (DTI) is essential in various applications including drug discovery and clinical application. There are two perspectives of input data widely used in DTI prediction: Intrinsic data represents how drugs or targets are constructed, and extrinsic data represents how drugs or targets are related to other biological entities. However, any of the two perspectives of input data can be scarce for some drugs or targets, especially for those unpopular or newly discovered. Furthermore, ground-truth labels for specific interaction types can also be scarce. Therefore, we propose the first method to tackle DTI prediction under input data and/or label scarcity. To make our model functional when only one perspective of input data is available, we design two separate experts to process intrinsic and extrinsic data respectively and fuse them adaptively according to different samples. Furthermore, to make the two perspectives complement each other and remedy label scarcity, two experts synergize with each other in a mutually supervised way to exploit the enormous unlabeled data. Extensive experiments on 3 real-world datasets under different extents of input data scarcity and/or label scarcity demonstrate our model outperforms states of the art significantly and steadily, with a maximum improvement of 53.53%. We also test our model without any data scarcity and it still outperforms current methods.",
      "authors": [
        "Xinlong Zhai",
        "Chunchen Wang",
        "Ruijia Wang",
        "Jiazheng Kang",
        "Shujie Li",
        "Boyu Chen",
        "Tengfei Ma",
        "Zikai Zhou",
        "Cheng Yang",
        "Chuan Shi"
      ],
      "published": "2025-03-20T02:27:16Z",
      "updated": "2025-03-20T02:27:16Z",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2503.15796v1",
      "landing_url": "https://arxiv.org/abs/2503.15796v1",
      "doi": "https://doi.org/10.48550/arXiv.2503.15796"
    },
    "queries": [
      {
        "anchor": "spoken language models",
        "search_term": "data scarcity"
      },
      {
        "anchor": "synchronous dialogue",
        "search_term": "data scarcity"
      },
      {
        "anchor": "turn-taking",
        "search_term": "data scarcity"
      }
    ]
  },
  {
    "arxiv_id": "2503.15973",
    "anchor": "spoken language models",
    "search_term": "temporal dynamics",
    "search_record": {
      "id": "http://arxiv.org/abs/2503.15973v2",
      "title": "STOP: Integrated Spatial-Temporal Dynamic Prompting for Video Understanding",
      "summary": "Pre-trained on tremendous image-text pairs, vision-language models like CLIP have demonstrated promising zero-shot generalization across numerous image-based tasks. However, extending these capabilities to video tasks remains challenging due to limited labeled video data and high training costs. Recent video prompting methods attempt to adapt CLIP for video tasks by introducing learnable prompts, but they typically rely on a single static prompt for all video sequences, overlooking the diverse temporal dynamics and spatial variations that exist across frames. This limitation significantly hinders the model's ability to capture essential temporal information for effective video understanding. To address this, we propose an integrated Spatial-TempOral dynamic Prompting (STOP) model which consists of two complementary modules, the intra-frame spatial prompting and inter-frame temporal prompting. Our intra-frame spatial prompts are designed to adaptively highlight discriminative regions within each frame by leveraging intra-frame attention and temporal variation, allowing the model to focus on areas with substantial temporal dynamics and capture fine-grained spatial details. Additionally, to highlight the varying importance of frames for video understanding, we further introduce inter-frame temporal prompts, dynamically inserting prompts between frames with high temporal variance as measured by frame similarity. This enables the model to prioritize key frames and enhances its capacity to understand temporal dependencies across sequences. Extensive experiments on various video benchmarks demonstrate that STOP consistently achieves superior performance against state-of-the-art methods. The code is available at https://github.com/zhoujiahuan1991/CVPR2025-STOP.",
      "published": "2025-03-20T09:16:20Z"
    },
    "metadata": {
      "arxiv_id": "2503.15973",
      "title": "STOP: Integrated Spatial-Temporal Dynamic Prompting for Video Understanding",
      "summary": "Pre-trained on tremendous image-text pairs, vision-language models like CLIP have demonstrated promising zero-shot generalization across numerous image-based tasks. However, extending these capabilities to video tasks remains challenging due to limited labeled video data and high training costs. Recent video prompting methods attempt to adapt CLIP for video tasks by introducing learnable prompts, but they typically rely on a single static prompt for all video sequences, overlooking the diverse temporal dynamics and spatial variations that exist across frames. This limitation significantly hinders the model's ability to capture essential temporal information for effective video understanding. To address this, we propose an integrated Spatial-TempOral dynamic Prompting (STOP) model which consists of two complementary modules, the intra-frame spatial prompting and inter-frame temporal prompting. Our intra-frame spatial prompts are designed to adaptively highlight discriminative regions within each frame by leveraging intra-frame attention and temporal variation, allowing the model to focus on areas with substantial temporal dynamics and capture fine-grained spatial details. Additionally, to highlight the varying importance of frames for video understanding, we further introduce inter-frame temporal prompts, dynamically inserting prompts between frames with high temporal variance as measured by frame similarity. This enables the model to prioritize key frames and enhances its capacity to understand temporal dependencies across sequences. Extensive experiments on various video benchmarks demonstrate that STOP consistently achieves superior performance against state-of-the-art methods. The code is available at https://github.com/zhoujiahuan1991/CVPR2025-STOP.",
      "authors": [
        "Zichen Liu",
        "Kunlun Xu",
        "Bing Su",
        "Xu Zou",
        "Yuxin Peng",
        "Jiahuan Zhou"
      ],
      "published": "2025-03-20T09:16:20Z",
      "updated": "2025-03-25T03:05:36Z",
      "categories": [
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2503.15973v2",
      "landing_url": "https://arxiv.org/abs/2503.15973v2",
      "doi": "https://doi.org/10.48550/arXiv.2503.15973"
    },
    "queries": [
      {
        "anchor": "spoken language models",
        "search_term": "temporal dynamics"
      },
      {
        "anchor": "synchronous dialogue",
        "search_term": "temporal dynamics"
      },
      {
        "anchor": "turn-taking",
        "search_term": "temporal dynamics"
      }
    ]
  },
  {
    "arxiv_id": "2503.16430",
    "anchor": "spoken language models",
    "search_term": "discrete tokens",
    "search_record": {
      "id": "http://arxiv.org/abs/2503.16430v3",
      "title": "Bridging Continuous and Discrete Tokens for Autoregressive Visual Generation",
      "summary": "Autoregressive visual generation models typically rely on tokenizers to compress images into tokens that can be predicted sequentially. A fundamental dilemma exists in token representation: discrete tokens enable straightforward modeling with standard cross-entropy loss, but suffer from information loss and tokenizer training instability; continuous tokens better preserve visual details, but require complex distribution modeling, complicating the generation pipeline. In this paper, we propose TokenBridge, which bridges this gap by maintaining the strong representation capacity of continuous tokens while preserving the modeling simplicity of discrete tokens. To achieve this, we decouple discretization from the tokenizer training process through post-training quantization that directly obtains discrete tokens from continuous representations. Specifically, we introduce a dimension-wise quantization strategy that independently discretizes each feature dimension, paired with a lightweight autoregressive prediction mechanism that efficiently model the resulting large token space. Extensive experiments show that our approach achieves reconstruction and generation quality on par with continuous methods while using standard categorical prediction. This work demonstrates that bridging discrete and continuous paradigms can effectively harness the strengths of both approaches, providing a promising direction for high-quality visual generation with simple autoregressive modeling. Project page: https://yuqingwang1029.github.io/TokenBridge.",
      "published": "2025-03-20T17:59:59Z"
    },
    "metadata": {
      "arxiv_id": "2503.16430",
      "title": "Bridging Continuous and Discrete Tokens for Autoregressive Visual Generation",
      "summary": "Autoregressive visual generation models typically rely on tokenizers to compress images into tokens that can be predicted sequentially. A fundamental dilemma exists in token representation: discrete tokens enable straightforward modeling with standard cross-entropy loss, but suffer from information loss and tokenizer training instability; continuous tokens better preserve visual details, but require complex distribution modeling, complicating the generation pipeline. In this paper, we propose TokenBridge, which bridges this gap by maintaining the strong representation capacity of continuous tokens while preserving the modeling simplicity of discrete tokens. To achieve this, we decouple discretization from the tokenizer training process through post-training quantization that directly obtains discrete tokens from continuous representations. Specifically, we introduce a dimension-wise quantization strategy that independently discretizes each feature dimension, paired with a lightweight autoregressive prediction mechanism that efficiently model the resulting large token space. Extensive experiments show that our approach achieves reconstruction and generation quality on par with continuous methods while using standard categorical prediction. This work demonstrates that bridging discrete and continuous paradigms can effectively harness the strengths of both approaches, providing a promising direction for high-quality visual generation with simple autoregressive modeling. Project page: https://yuqingwang1029.github.io/TokenBridge.",
      "authors": [
        "Yuqing Wang",
        "Zhijie Lin",
        "Yao Teng",
        "Yuanzhi Zhu",
        "Shuhuai Ren",
        "Jiashi Feng",
        "Xihui Liu"
      ],
      "published": "2025-03-20T17:59:59Z",
      "updated": "2025-08-29T02:25:47Z",
      "categories": [
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2503.16430v3",
      "landing_url": "https://arxiv.org/abs/2503.16430v3",
      "doi": "https://doi.org/10.48550/arXiv.2503.16430"
    },
    "queries": [
      {
        "anchor": "spoken language models",
        "search_term": "discrete tokens"
      },
      {
        "anchor": "synchronous dialogue",
        "search_term": "discrete tokens"
      },
      {
        "anchor": "turn-taking",
        "search_term": "discrete tokens"
      }
    ]
  },
  {
    "arxiv_id": "2503.16432",
    "anchor": "turn-taking",
    "search_term": "true full-duplex",
    "search_record": {
      "id": "http://arxiv.org/abs/2503.16432v1",
      "title": "Multimodal Transformer Models for Turn-taking Prediction: Effects on Conversational Dynamics of Human-Agent Interaction during Cooperative Gameplay",
      "summary": "This study investigates multimodal turn-taking prediction within human-agent interactions (HAI), particularly focusing on cooperative gaming environments. It comprises both model development and subsequent user study, aiming to refine our understanding and improve conversational dynamics in spoken dialogue systems (SDSs). For the modeling phase, we introduce a novel transformer-based deep learning (DL) model that simultaneously integrates multiple modalities - text, vision, audio, and contextual in-game data to predict turn-taking events in real-time. Our model employs a Crossmodal Transformer architecture to effectively fuse information from these diverse modalities, enabling more comprehensive turn-taking predictions. The model demonstrates superior performance compared to baseline models, achieving 87.3% accuracy and 83.0% macro F1 score. A human user study was then conducted to empirically evaluate the turn-taking DL model in an interactive scenario with a virtual avatar while playing the game \"Dont Starve Together\", comparing a control condition without turn-taking prediction (n=20) to an experimental condition with our model deployed (n=40). Both conditions included a mix of English and Korean speakers, since turn-taking cues are known to vary by culture. We then analyzed the interaction quality, examining aspects such as utterance counts, interruption frequency, and participant perceptions of the avatar. Results from the user study suggest that our multimodal turn-taking model not only enhances the fluidity and naturalness of human-agent conversations, but also maintains a balanced conversational dynamic without significantly altering dialogue frequency. The study provides in-depth insights into the influence of turn-taking abilities on user perceptions and interaction quality, underscoring the potential for more contextually adaptive and responsive conversational agents.",
      "published": "2025-02-05T23:00:49Z"
    },
    "metadata": {
      "arxiv_id": "2503.16432",
      "title": "Multimodal Transformer Models for Turn-taking Prediction: Effects on Conversational Dynamics of Human-Agent Interaction during Cooperative Gameplay",
      "summary": "This study investigates multimodal turn-taking prediction within human-agent interactions (HAI), particularly focusing on cooperative gaming environments. It comprises both model development and subsequent user study, aiming to refine our understanding and improve conversational dynamics in spoken dialogue systems (SDSs). For the modeling phase, we introduce a novel transformer-based deep learning (DL) model that simultaneously integrates multiple modalities - text, vision, audio, and contextual in-game data to predict turn-taking events in real-time. Our model employs a Crossmodal Transformer architecture to effectively fuse information from these diverse modalities, enabling more comprehensive turn-taking predictions. The model demonstrates superior performance compared to baseline models, achieving 87.3% accuracy and 83.0% macro F1 score. A human user study was then conducted to empirically evaluate the turn-taking DL model in an interactive scenario with a virtual avatar while playing the game \"Dont Starve Together\", comparing a control condition without turn-taking prediction (n=20) to an experimental condition with our model deployed (n=40). Both conditions included a mix of English and Korean speakers, since turn-taking cues are known to vary by culture. We then analyzed the interaction quality, examining aspects such as utterance counts, interruption frequency, and participant perceptions of the avatar. Results from the user study suggest that our multimodal turn-taking model not only enhances the fluidity and naturalness of human-agent conversations, but also maintains a balanced conversational dynamic without significantly altering dialogue frequency. The study provides in-depth insights into the influence of turn-taking abilities on user perceptions and interaction quality, underscoring the potential for more contextually adaptive and responsive conversational agents.",
      "authors": [
        "Young-Ho Bae",
        "Casey C. Bennett"
      ],
      "published": "2025-02-05T23:00:49Z",
      "updated": "2025-02-05T23:00:49Z",
      "categories": [
        "cs.HC",
        "cs.AI",
        "cs.CL"
      ],
      "pdf_url": "https://arxiv.org/pdf/2503.16432v1",
      "landing_url": "https://arxiv.org/abs/2503.16432v1",
      "doi": "https://doi.org/10.48550/arXiv.2503.16432"
    },
    "queries": [
      {
        "anchor": "turn-taking",
        "search_term": "true full-duplex"
      },
      {
        "anchor": "turn-taking",
        "search_term": "pseudo full-duplex"
      },
      {
        "anchor": "turn-taking",
        "search_term": "half duplex"
      },
      {
        "anchor": "turn-taking",
        "search_term": "engineered synchronization"
      },
      {
        "anchor": "turn-taking",
        "search_term": "learned synchronization"
      },
      {
        "anchor": "turn-taking",
        "search_term": "modular architecture"
      },
      {
        "anchor": "turn-taking",
        "search_term": "streaming architecture"
      },
      {
        "anchor": "turn-taking",
        "search_term": "real-time interaction"
      },
      {
        "anchor": "turn-taking",
        "search_term": "low latency"
      },
      {
        "anchor": "turn-taking",
        "search_term": "synchronous data"
      },
      {
        "anchor": "turn-taking",
        "search_term": "data scarcity"
      },
      {
        "anchor": "turn-taking",
        "search_term": "multichannel dialogue"
      },
      {
        "anchor": "turn-taking",
        "search_term": "temporal dynamics"
      },
      {
        "anchor": "turn-taking",
        "search_term": "behavioral arbitration"
      },
      {
        "anchor": "turn-taking",
        "search_term": "semantic coherence"
      },
      {
        "anchor": "turn-taking",
        "search_term": "neural codec"
      },
      {
        "anchor": "turn-taking",
        "search_term": "discrete tokens"
      },
      {
        "anchor": "turn-taking",
        "search_term": "audio tokens"
      },
      {
        "anchor": "turn-taking",
        "search_term": "floor transfer offset"
      },
      {
        "anchor": "turn-taking",
        "search_term": "stop latency"
      },
      {
        "anchor": "turn-taking",
        "search_term": "interruption response delay"
      }
    ]
  },
  {
    "arxiv_id": "2503.16875",
    "anchor": "synchronous dialogue",
    "search_term": "synchronous data",
    "search_record": {
      "id": "http://arxiv.org/abs/2503.16875v1",
      "title": "Federated Cross-Domain Click-Through Rate Prediction With Large Language Model Augmentation",
      "summary": "Accurately predicting click-through rates (CTR) under stringent privacy constraints poses profound challenges, particularly when user-item interactions are sparse and fragmented across domains. Conventional cross-domain CTR (CCTR) methods frequently assume homogeneous feature spaces and rely on centralized data sharing, neglecting complex inter-domain discrepancies and the subtle trade-offs imposed by privacy-preserving protocols. Here, we present Federated Cross-Domain CTR Prediction with Large Language Model Augmentation (FedCCTR-LM), a federated framework engineered to address these limitations by synchronizing data augmentation, representation disentanglement, and adaptive privacy protection. Our approach integrates three core innovations. First, the Privacy-Preserving Augmentation Network (PrivAugNet) employs large language models to enrich user and item representations and expand interaction sequences, mitigating data sparsity and feature incompleteness. Second, the Independent Domain-Specific Transformer with Contrastive Learning (IDST-CL) module disentangles domain-specific and shared user preferences, employing intra-domain representation alignment (IDRA) and crossdomain representation disentanglement (CDRD) to refine the learned embeddings and enhance knowledge transfer across domains. Finally, the Adaptive Local Differential Privacy (AdaLDP) mechanism dynamically calibrates noise injection to achieve an optimal balance between rigorous privacy guarantees and predictive accuracy. Empirical evaluations on four real-world datasets demonstrate that FedCCTR-LM substantially outperforms existing baselines, offering robust, privacy-preserving, and generalizable cross-domain CTR prediction in heterogeneous, federated environments.",
      "published": "2025-03-21T06:22:42Z"
    },
    "metadata": {
      "arxiv_id": "2503.16875",
      "title": "Federated Cross-Domain Click-Through Rate Prediction With Large Language Model Augmentation",
      "summary": "Accurately predicting click-through rates (CTR) under stringent privacy constraints poses profound challenges, particularly when user-item interactions are sparse and fragmented across domains. Conventional cross-domain CTR (CCTR) methods frequently assume homogeneous feature spaces and rely on centralized data sharing, neglecting complex inter-domain discrepancies and the subtle trade-offs imposed by privacy-preserving protocols. Here, we present Federated Cross-Domain CTR Prediction with Large Language Model Augmentation (FedCCTR-LM), a federated framework engineered to address these limitations by synchronizing data augmentation, representation disentanglement, and adaptive privacy protection. Our approach integrates three core innovations. First, the Privacy-Preserving Augmentation Network (PrivAugNet) employs large language models to enrich user and item representations and expand interaction sequences, mitigating data sparsity and feature incompleteness. Second, the Independent Domain-Specific Transformer with Contrastive Learning (IDST-CL) module disentangles domain-specific and shared user preferences, employing intra-domain representation alignment (IDRA) and crossdomain representation disentanglement (CDRD) to refine the learned embeddings and enhance knowledge transfer across domains. Finally, the Adaptive Local Differential Privacy (AdaLDP) mechanism dynamically calibrates noise injection to achieve an optimal balance between rigorous privacy guarantees and predictive accuracy. Empirical evaluations on four real-world datasets demonstrate that FedCCTR-LM substantially outperforms existing baselines, offering robust, privacy-preserving, and generalizable cross-domain CTR prediction in heterogeneous, federated environments.",
      "authors": [
        "Jiangcheng Qin",
        "Xueyuan Zhang",
        "Baisong Liu",
        "Jiangbo Qian",
        "Yangyang Wang"
      ],
      "published": "2025-03-21T06:22:42Z",
      "updated": "2025-03-21T06:22:42Z",
      "categories": [
        "cs.IR",
        "cs.CL",
        "cs.DC"
      ],
      "pdf_url": "https://arxiv.org/pdf/2503.16875v1",
      "landing_url": "https://arxiv.org/abs/2503.16875v1",
      "doi": "https://doi.org/10.48550/arXiv.2503.16875"
    },
    "queries": [
      {
        "anchor": "synchronous dialogue",
        "search_term": "synchronous data"
      }
    ]
  },
  {
    "arxiv_id": "2503.16980",
    "anchor": "spoken language models",
    "search_term": "discrete tokens",
    "search_record": {
      "id": "http://arxiv.org/abs/2503.16980v6",
      "title": "VQToken: Neural Discrete Token Representation Learning for Extreme Token Reduction in Video Large Language Models",
      "summary": "Token-based video representation has emerged as a promising approach for enabling large language models (LLMs) to interpret video content. However, existing token reduction techniques, such as pruning and merging, often disrupt essential positional embeddings and rely on continuous visual tokens sampled from nearby pixels with similar spatial-temporal locations. By removing only a small fraction of tokens, these methods still produce relatively lengthy continuous sequences, which falls short of the extreme compression required to balance computational efficiency and token count in video LLMs. In this paper, we introduce the novel task of Extreme Short Token Reduction, which aims to represent entire videos using a minimal set of discrete tokens. We propose VQToken, a neural discrete token representation framework that (i) applies adaptive vector quantization to continuous ViT embeddings to learn a compact codebook and (ii) preserves spatial-temporal positions via a token hash function by assigning each grid-level token to its nearest codebook entry. On the Extreme Short Token Reduction task, our VQToken compresses sequences to just 0.07 percent of their original length while incurring only a 0.66 percent drop in accuracy on the NextQA-MC benchmark. It also achieves comparable performance on ActNet-QA, Long Video Bench, and VideoMME. We further introduce the Token Information Density (TokDense) metric and formalize fixed-length and adaptive-length subtasks, achieving state-of-the-art results in both settings. Our approach dramatically lowers theoretical complexity, increases information density, drastically reduces token counts, and enables efficient video LLMs in resource-constrained environments.",
      "published": "2025-03-21T09:46:31Z"
    },
    "metadata": {
      "arxiv_id": "2503.16980",
      "title": "VQToken: Neural Discrete Token Representation Learning for Extreme Token Reduction in Video Large Language Models",
      "summary": "Token-based video representation has emerged as a promising approach for enabling large language models (LLMs) to interpret video content. However, existing token reduction techniques, such as pruning and merging, often disrupt essential positional embeddings and rely on continuous visual tokens sampled from nearby pixels with similar spatial-temporal locations. By removing only a small fraction of tokens, these methods still produce relatively lengthy continuous sequences, which falls short of the extreme compression required to balance computational efficiency and token count in video LLMs. In this paper, we introduce the novel task of Extreme Short Token Reduction, which aims to represent entire videos using a minimal set of discrete tokens. We propose VQToken, a neural discrete token representation framework that (i) applies adaptive vector quantization to continuous ViT embeddings to learn a compact codebook and (ii) preserves spatial-temporal positions via a token hash function by assigning each grid-level token to its nearest codebook entry. On the Extreme Short Token Reduction task, our VQToken compresses sequences to just 0.07 percent of their original length while incurring only a 0.66 percent drop in accuracy on the NextQA-MC benchmark. It also achieves comparable performance on ActNet-QA, Long Video Bench, and VideoMME. We further introduce the Token Information Density (TokDense) metric and formalize fixed-length and adaptive-length subtasks, achieving state-of-the-art results in both settings. Our approach dramatically lowers theoretical complexity, increases information density, drastically reduces token counts, and enables efficient video LLMs in resource-constrained environments.",
      "authors": [
        "Haichao Zhang",
        "Yun Fu"
      ],
      "published": "2025-03-21T09:46:31Z",
      "updated": "2025-09-29T01:09:31Z",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2503.16980v6",
      "landing_url": "https://arxiv.org/abs/2503.16980v6",
      "doi": "https://doi.org/10.48550/arXiv.2503.16980"
    },
    "queries": [
      {
        "anchor": "spoken language models",
        "search_term": "discrete tokens"
      },
      {
        "anchor": "synchronous dialogue",
        "search_term": "discrete tokens"
      },
      {
        "anchor": "turn-taking",
        "search_term": "discrete tokens"
      }
    ]
  },
  {
    "arxiv_id": "2503.17155",
    "anchor": "spoken language models",
    "search_term": "discrete tokens",
    "search_record": {
      "id": "http://arxiv.org/abs/2503.17155v1",
      "title": "D2C: Unlocking the Potential of Continuous Autoregressive Image Generation with Discrete Tokens",
      "summary": "In the domain of image generation, latent-based generative models occupy a dominant status; however, these models rely heavily on image tokenizer. To meet modeling requirements, autoregressive models possessing the characteristics of scalability and flexibility embrace a discrete-valued tokenizer, but face the challenge of poor image generation quality. In contrast, diffusion models take advantage of the continuous-valued tokenizer to achieve better generation quality but are subject to low efficiency and complexity. The existing hybrid models are mainly to compensate for information loss and simplify the diffusion learning process. The potential of merging discrete-valued and continuous-valued tokens in the field of image generation has not yet been explored. In this paper, we propose D2C, a novel two-stage method to enhance model generation capacity. In the first stage, the discrete-valued tokens representing coarse-grained image features are sampled by employing a small discrete-valued generator. Then in the second stage, the continuous-valued tokens representing fine-grained image features are learned conditioned on the discrete token sequence. In addition, we design two kinds of fusion modules for seamless interaction. On the ImageNet-256 benchmark, extensive experiment results validate that our model achieves superior performance compared with several continuous-valued and discrete-valued generative models on the class-conditional image generation tasks.",
      "published": "2025-03-21T13:58:49Z"
    },
    "metadata": {
      "arxiv_id": "2503.17155",
      "title": "D2C: Unlocking the Potential of Continuous Autoregressive Image Generation with Discrete Tokens",
      "summary": "In the domain of image generation, latent-based generative models occupy a dominant status; however, these models rely heavily on image tokenizer. To meet modeling requirements, autoregressive models possessing the characteristics of scalability and flexibility embrace a discrete-valued tokenizer, but face the challenge of poor image generation quality. In contrast, diffusion models take advantage of the continuous-valued tokenizer to achieve better generation quality but are subject to low efficiency and complexity. The existing hybrid models are mainly to compensate for information loss and simplify the diffusion learning process. The potential of merging discrete-valued and continuous-valued tokens in the field of image generation has not yet been explored. In this paper, we propose D2C, a novel two-stage method to enhance model generation capacity. In the first stage, the discrete-valued tokens representing coarse-grained image features are sampled by employing a small discrete-valued generator. Then in the second stage, the continuous-valued tokens representing fine-grained image features are learned conditioned on the discrete token sequence. In addition, we design two kinds of fusion modules for seamless interaction. On the ImageNet-256 benchmark, extensive experiment results validate that our model achieves superior performance compared with several continuous-valued and discrete-valued generative models on the class-conditional image generation tasks.",
      "authors": [
        "Panpan Wang",
        "Liqiang Niu",
        "Fandong Meng",
        "Jinan Xu",
        "Yufeng Chen",
        "Jie Zhou"
      ],
      "published": "2025-03-21T13:58:49Z",
      "updated": "2025-03-21T13:58:49Z",
      "categories": [
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2503.17155v1",
      "landing_url": "https://arxiv.org/abs/2503.17155v1",
      "doi": "https://doi.org/10.48550/arXiv.2503.17155"
    },
    "queries": [
      {
        "anchor": "spoken language models",
        "search_term": "discrete tokens"
      },
      {
        "anchor": "synchronous dialogue",
        "search_term": "discrete tokens"
      },
      {
        "anchor": "turn-taking",
        "search_term": "discrete tokens"
      }
    ]
  },
  {
    "arxiv_id": "2503.17760",
    "anchor": "spoken language models",
    "search_term": "discrete tokens",
    "search_record": {
      "id": "http://arxiv.org/abs/2503.17760v2",
      "title": "CODA: Repurposing Continuous VAEs for Discrete Tokenization",
      "summary": "Discrete visual tokenizers transform images into a sequence of tokens, enabling token-based visual generation akin to language models. However, this process is inherently challenging, as it requires both compressing visual signals into a compact representation and discretizing them into a fixed set of codes. Traditional discrete tokenizers typically learn the two tasks jointly, often leading to unstable training, low codebook utilization, and limited reconstruction quality. In this paper, we introduce \\textbf{CODA}(\\textbf{CO}ntinuous-to-\\textbf{D}iscrete \\textbf{A}daptation), a framework that decouples compression and discretization. Instead of training discrete tokenizers from scratch, CODA adapts off-the-shelf continuous VAEs -- already optimized for perceptual compression -- into discrete tokenizers via a carefully designed discretization process. By primarily focusing on discretization, CODA ensures stable and efficient training while retaining the strong visual fidelity of continuous VAEs. Empirically, with $\\mathbf{6 \\times}$ less training budget than standard VQGAN, our approach achieves a remarkable codebook utilization of 100% and notable reconstruction FID (rFID) of $\\mathbf{0.43}$ and $\\mathbf{1.34}$ for $8 \\times$ and $16 \\times$ compression on ImageNet 256$\\times$ 256 benchmark.",
      "published": "2025-03-22T12:59:00Z"
    },
    "metadata": {
      "arxiv_id": "2503.17760",
      "title": "CODA: Repurposing Continuous VAEs for Discrete Tokenization",
      "summary": "Discrete visual tokenizers transform images into a sequence of tokens, enabling token-based visual generation akin to language models. However, this process is inherently challenging, as it requires both compressing visual signals into a compact representation and discretizing them into a fixed set of codes. Traditional discrete tokenizers typically learn the two tasks jointly, often leading to unstable training, low codebook utilization, and limited reconstruction quality. In this paper, we introduce \\textbf{CODA}(\\textbf{CO}ntinuous-to-\\textbf{D}iscrete \\textbf{A}daptation), a framework that decouples compression and discretization. Instead of training discrete tokenizers from scratch, CODA adapts off-the-shelf continuous VAEs -- already optimized for perceptual compression -- into discrete tokenizers via a carefully designed discretization process. By primarily focusing on discretization, CODA ensures stable and efficient training while retaining the strong visual fidelity of continuous VAEs. Empirically, with $\\mathbf{6 \\times}$ less training budget than standard VQGAN, our approach achieves a remarkable codebook utilization of 100% and notable reconstruction FID (rFID) of $\\mathbf{0.43}$ and $\\mathbf{1.34}$ for $8 \\times$ and $16 \\times$ compression on ImageNet 256$\\times$ 256 benchmark.",
      "authors": [
        "Zeyu Liu",
        "Zanlin Ni",
        "Yeguo Hua",
        "Xin Deng",
        "Xiao Ma",
        "Cheng Zhong",
        "Gao Huang"
      ],
      "published": "2025-03-22T12:59:00Z",
      "updated": "2025-09-30T14:36:25Z",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2503.17760v2",
      "landing_url": "https://arxiv.org/abs/2503.17760v2",
      "doi": "https://doi.org/10.48550/arXiv.2503.17760"
    },
    "queries": [
      {
        "anchor": "spoken language models",
        "search_term": "discrete tokens"
      },
      {
        "anchor": "synchronous dialogue",
        "search_term": "discrete tokens"
      },
      {
        "anchor": "turn-taking",
        "search_term": "discrete tokens"
      }
    ]
  },
  {
    "arxiv_id": "2503.19034",
    "anchor": "synchronous dialogue",
    "search_term": "semantic coherence",
    "search_record": {
      "id": "http://arxiv.org/abs/2503.19034v1",
      "title": "Color Conditional Generation with Sliced Wasserstein Guidance",
      "summary": "We propose SW-Guidance, a training-free approach for image generation conditioned on the color distribution of a reference image. While it is possible to generate an image with fixed colors by first creating an image from a text prompt and then applying a color style transfer method, this approach often results in semantically meaningless colors in the generated image. Our method solves this problem by modifying the sampling process of a diffusion model to incorporate the differentiable Sliced 1-Wasserstein distance between the color distribution of the generated image and the reference palette. Our method outperforms state-of-the-art techniques for color-conditional generation in terms of color similarity to the reference, producing images that not only match the reference colors but also maintain semantic coherence with the original text prompt. Our source code is available at https://github.com/alobashev/sw-guidance/.",
      "published": "2025-03-24T18:06:03Z"
    },
    "metadata": {
      "arxiv_id": "2503.19034",
      "title": "Color Conditional Generation with Sliced Wasserstein Guidance",
      "summary": "We propose SW-Guidance, a training-free approach for image generation conditioned on the color distribution of a reference image. While it is possible to generate an image with fixed colors by first creating an image from a text prompt and then applying a color style transfer method, this approach often results in semantically meaningless colors in the generated image. Our method solves this problem by modifying the sampling process of a diffusion model to incorporate the differentiable Sliced 1-Wasserstein distance between the color distribution of the generated image and the reference palette. Our method outperforms state-of-the-art techniques for color-conditional generation in terms of color similarity to the reference, producing images that not only match the reference colors but also maintain semantic coherence with the original text prompt. Our source code is available at https://github.com/alobashev/sw-guidance/.",
      "authors": [
        "Alexander Lobashev",
        "Maria Larchenko",
        "Dmitry Guskov"
      ],
      "published": "2025-03-24T18:06:03Z",
      "updated": "2025-03-24T18:06:03Z",
      "categories": [
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2503.19034v1",
      "landing_url": "https://arxiv.org/abs/2503.19034v1",
      "doi": "https://doi.org/10.48550/arXiv.2503.19034"
    },
    "queries": [
      {
        "anchor": "synchronous dialogue",
        "search_term": "semantic coherence"
      }
    ]
  },
  {
    "arxiv_id": "2503.19611",
    "anchor": "synchronous dialogue",
    "search_term": "audio tokens",
    "search_record": {
      "id": "http://arxiv.org/abs/2503.19611v1",
      "title": "Analyzable Chain-of-Musical-Thought Prompting for High-Fidelity Music Generation",
      "summary": "Autoregressive (AR) models have demonstrated impressive capabilities in generating high-fidelity music. However, the conventional next-token prediction paradigm in AR models does not align with the human creative process in music composition, potentially compromising the musicality of generated samples. To overcome this limitation, we introduce MusiCoT, a novel chain-of-thought (CoT) prompting technique tailored for music generation. MusiCoT empowers the AR model to first outline an overall music structure before generating audio tokens, thereby enhancing the coherence and creativity of the resulting compositions. By leveraging the contrastive language-audio pretraining (CLAP) model, we establish a chain of \"musical thoughts\", making MusiCoT scalable and independent of human-labeled data, in contrast to conventional CoT methods. Moreover, MusiCoT allows for in-depth analysis of music structure, such as instrumental arrangements, and supports music referencing -- accepting variable-length audio inputs as optional style references. This innovative approach effectively addresses copying issues, positioning MusiCoT as a vital practical method for music prompting. Our experimental results indicate that MusiCoT consistently achieves superior performance across both objective and subjective metrics, producing music quality that rivals state-of-the-art generation models.\n  Our samples are available at https://MusiCoT.github.io/.",
      "published": "2025-03-25T12:51:21Z"
    },
    "metadata": {
      "arxiv_id": "2503.19611",
      "title": "Analyzable Chain-of-Musical-Thought Prompting for High-Fidelity Music Generation",
      "summary": "Autoregressive (AR) models have demonstrated impressive capabilities in generating high-fidelity music. However, the conventional next-token prediction paradigm in AR models does not align with the human creative process in music composition, potentially compromising the musicality of generated samples. To overcome this limitation, we introduce MusiCoT, a novel chain-of-thought (CoT) prompting technique tailored for music generation. MusiCoT empowers the AR model to first outline an overall music structure before generating audio tokens, thereby enhancing the coherence and creativity of the resulting compositions. By leveraging the contrastive language-audio pretraining (CLAP) model, we establish a chain of \"musical thoughts\", making MusiCoT scalable and independent of human-labeled data, in contrast to conventional CoT methods. Moreover, MusiCoT allows for in-depth analysis of music structure, such as instrumental arrangements, and supports music referencing -- accepting variable-length audio inputs as optional style references. This innovative approach effectively addresses copying issues, positioning MusiCoT as a vital practical method for music prompting. Our experimental results indicate that MusiCoT consistently achieves superior performance across both objective and subjective metrics, producing music quality that rivals state-of-the-art generation models.\n  Our samples are available at https://MusiCoT.github.io/.",
      "authors": [
        "Max W. Y. Lam",
        "Yijin Xing",
        "Weiya You",
        "Jingcheng Wu",
        "Zongyu Yin",
        "Fuqiang Jiang",
        "Hangyu Liu",
        "Feng Liu",
        "Xingda Li",
        "Wei-Tsung Lu",
        "Hanyu Chen",
        "Tong Feng",
        "Tianwei Zhao",
        "Chien-Hung Liu",
        "Xuchen Song",
        "Yang Li",
        "Yahui Zhou"
      ],
      "published": "2025-03-25T12:51:21Z",
      "updated": "2025-03-25T12:51:21Z",
      "categories": [
        "cs.SD",
        "cs.AI",
        "cs.MM",
        "eess.AS",
        "eess.SP"
      ],
      "pdf_url": "https://arxiv.org/pdf/2503.19611v1",
      "landing_url": "https://arxiv.org/abs/2503.19611v1",
      "doi": "https://doi.org/10.48550/arXiv.2503.19611"
    },
    "queries": [
      {
        "anchor": "synchronous dialogue",
        "search_term": "audio tokens"
      }
    ]
  },
  {
    "arxiv_id": "2503.20215",
    "anchor": "spoken language models",
    "search_term": "audio tokens",
    "search_record": {
      "id": "http://arxiv.org/abs/2503.20215v1",
      "title": "Qwen2.5-Omni Technical Report",
      "summary": "In this report, we present Qwen2.5-Omni, an end-to-end multimodal model designed to perceive diverse modalities, including text, images, audio, and video, while simultaneously generating text and natural speech responses in a streaming manner. To enable the streaming of multimodal information inputs, both audio and visual encoders utilize a block-wise processing approach. To synchronize the timestamps of video inputs with audio, we organize the audio and video sequentially in an interleaved manner and propose a novel position embedding approach, named TMRoPE(Time-aligned Multimodal RoPE). To concurrently generate text and speech while avoiding interference between the two modalities, we propose \\textbf{Thinker-Talker} architecture. In this framework, Thinker functions as a large language model tasked with text generation, while Talker is a dual-track autoregressive model that directly utilizes the hidden representations from the Thinker to produce audio tokens as output. Both the Thinker and Talker models are designed to be trained and inferred in an end-to-end manner. For decoding audio tokens in a streaming manner, we introduce a sliding-window DiT that restricts the receptive field, aiming to reduce the initial package delay. Qwen2.5-Omni is comparable with the similarly sized Qwen2.5-VL and outperforms Qwen2-Audio. Furthermore, Qwen2.5-Omni achieves state-of-the-art performance on multimodal benchmarks like Omni-Bench. Notably, Qwen2.5-Omni's performance in end-to-end speech instruction following is comparable to its capabilities with text inputs, as evidenced by benchmarks such as MMLU and GSM8K. As for speech generation, Qwen2.5-Omni's streaming Talker outperforms most existing streaming and non-streaming alternatives in robustness and naturalness.",
      "published": "2025-03-26T04:17:55Z"
    },
    "metadata": {
      "arxiv_id": "2503.20215",
      "title": "Qwen2.5-Omni Technical Report",
      "summary": "In this report, we present Qwen2.5-Omni, an end-to-end multimodal model designed to perceive diverse modalities, including text, images, audio, and video, while simultaneously generating text and natural speech responses in a streaming manner. To enable the streaming of multimodal information inputs, both audio and visual encoders utilize a block-wise processing approach. To synchronize the timestamps of video inputs with audio, we organize the audio and video sequentially in an interleaved manner and propose a novel position embedding approach, named TMRoPE(Time-aligned Multimodal RoPE). To concurrently generate text and speech while avoiding interference between the two modalities, we propose \\textbf{Thinker-Talker} architecture. In this framework, Thinker functions as a large language model tasked with text generation, while Talker is a dual-track autoregressive model that directly utilizes the hidden representations from the Thinker to produce audio tokens as output. Both the Thinker and Talker models are designed to be trained and inferred in an end-to-end manner. For decoding audio tokens in a streaming manner, we introduce a sliding-window DiT that restricts the receptive field, aiming to reduce the initial package delay. Qwen2.5-Omni is comparable with the similarly sized Qwen2.5-VL and outperforms Qwen2-Audio. Furthermore, Qwen2.5-Omni achieves state-of-the-art performance on multimodal benchmarks like Omni-Bench. Notably, Qwen2.5-Omni's performance in end-to-end speech instruction following is comparable to its capabilities with text inputs, as evidenced by benchmarks such as MMLU and GSM8K. As for speech generation, Qwen2.5-Omni's streaming Talker outperforms most existing streaming and non-streaming alternatives in robustness and naturalness.",
      "authors": [
        "Jin Xu",
        "Zhifang Guo",
        "Jinzheng He",
        "Hangrui Hu",
        "Ting He",
        "Shuai Bai",
        "Keqin Chen",
        "Jialin Wang",
        "Yang Fan",
        "Kai Dang",
        "Bin Zhang",
        "Xiong Wang",
        "Yunfei Chu",
        "Junyang Lin"
      ],
      "published": "2025-03-26T04:17:55Z",
      "updated": "2025-03-26T04:17:55Z",
      "categories": [
        "cs.CL",
        "cs.CV",
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2503.20215v1",
      "landing_url": "https://arxiv.org/abs/2503.20215v1",
      "doi": "https://doi.org/10.48550/arXiv.2503.20215"
    },
    "queries": [
      {
        "anchor": "spoken language models",
        "search_term": "audio tokens"
      },
      {
        "anchor": "synchronous dialogue",
        "search_term": "audio tokens"
      },
      {
        "anchor": "turn-taking",
        "search_term": "audio tokens"
      }
    ]
  },
  {
    "arxiv_id": "2503.20376",
    "anchor": "synchronous dialogue",
    "search_term": "semantic coherence",
    "search_record": {
      "id": "http://arxiv.org/abs/2503.20376v1",
      "title": "Dewey Long Context Embedding Model: A Technical Report",
      "summary": "This technical report presents the training methodology and evaluation results of the open-source dewey_en_beta embedding model. The increasing demand for retrieval-augmented generation (RAG) systems and the expanding context window capabilities of large language models (LLMs) have created critical challenges for conventional embedding models. Current approaches often struggle to maintain semantic coherence when processing documents exceeding typical sequence length limitations, significantly impacting retrieval performance in knowledge-intensive applications. This paper presents dewey_en_beta, a novel text embedding model that achieves excellent performance on MTEB (Eng, v2) and LongEmbed benchmark while supporting 128K token sequences. Our technical contribution centers on chunk alignment training, an innovative methodology that enables the simultaneous generation of localized chunk embeddings and global document-level representations through distillation. Information regarding the model release can be found at https://huggingface.co/infgrad/dewey_en_beta.",
      "published": "2025-03-26T09:55:00Z"
    },
    "metadata": {
      "arxiv_id": "2503.20376",
      "title": "Dewey Long Context Embedding Model: A Technical Report",
      "summary": "This technical report presents the training methodology and evaluation results of the open-source dewey_en_beta embedding model. The increasing demand for retrieval-augmented generation (RAG) systems and the expanding context window capabilities of large language models (LLMs) have created critical challenges for conventional embedding models. Current approaches often struggle to maintain semantic coherence when processing documents exceeding typical sequence length limitations, significantly impacting retrieval performance in knowledge-intensive applications. This paper presents dewey_en_beta, a novel text embedding model that achieves excellent performance on MTEB (Eng, v2) and LongEmbed benchmark while supporting 128K token sequences. Our technical contribution centers on chunk alignment training, an innovative methodology that enables the simultaneous generation of localized chunk embeddings and global document-level representations through distillation. Information regarding the model release can be found at https://huggingface.co/infgrad/dewey_en_beta.",
      "authors": [
        "Dun Zhang",
        "Panxiang Zou",
        "Yudong Zhou"
      ],
      "published": "2025-03-26T09:55:00Z",
      "updated": "2025-03-26T09:55:00Z",
      "categories": [
        "cs.IR"
      ],
      "pdf_url": "https://arxiv.org/pdf/2503.20376v1",
      "landing_url": "https://arxiv.org/abs/2503.20376v1",
      "doi": "https://doi.org/10.48550/arXiv.2503.20376"
    },
    "queries": [
      {
        "anchor": "synchronous dialogue",
        "search_term": "semantic coherence"
      }
    ]
  },
  {
    "arxiv_id": "2503.21123",
    "anchor": "synchronous dialogue",
    "search_term": "data scarcity",
    "search_record": {
      "id": "http://arxiv.org/abs/2503.21123v1",
      "title": "De Novo Functional Protein Sequence Generation: Overcoming Data Scarcity through Regeneration and Large Models",
      "summary": "Proteins are essential components of all living organisms and play a critical role in cellular survival. They have a broad range of applications, from clinical treatments to material engineering. This versatility has spurred the development of protein design, with amino acid sequence design being a crucial step in the process. Recent advancements in deep generative models have shown promise for protein sequence design. However, the scarcity of functional protein sequence data for certain types can hinder the training of these models, which often require large datasets. To address this challenge, we propose a hierarchical model named ProteinRG that can generate functional protein sequences using relatively small datasets. ProteinRG begins by generating a representation of a protein sequence, leveraging existing large protein sequence models, before producing a functional protein sequence. We have tested our model on various functional protein sequences and evaluated the results from three perspectives: multiple sequence alignment, t-SNE distribution analysis, and 3D structure prediction. The findings indicate that our generated protein sequences maintain both similarity to the original sequences and consistency with the desired functions. Moreover, our model demonstrates superior performance compared to other generative models for protein sequence generation.",
      "published": "2025-03-27T03:25:21Z"
    },
    "metadata": {
      "arxiv_id": "2503.21123",
      "title": "De Novo Functional Protein Sequence Generation: Overcoming Data Scarcity through Regeneration and Large Models",
      "summary": "Proteins are essential components of all living organisms and play a critical role in cellular survival. They have a broad range of applications, from clinical treatments to material engineering. This versatility has spurred the development of protein design, with amino acid sequence design being a crucial step in the process. Recent advancements in deep generative models have shown promise for protein sequence design. However, the scarcity of functional protein sequence data for certain types can hinder the training of these models, which often require large datasets. To address this challenge, we propose a hierarchical model named ProteinRG that can generate functional protein sequences using relatively small datasets. ProteinRG begins by generating a representation of a protein sequence, leveraging existing large protein sequence models, before producing a functional protein sequence. We have tested our model on various functional protein sequences and evaluated the results from three perspectives: multiple sequence alignment, t-SNE distribution analysis, and 3D structure prediction. The findings indicate that our generated protein sequences maintain both similarity to the original sequences and consistency with the desired functions. Moreover, our model demonstrates superior performance compared to other generative models for protein sequence generation.",
      "authors": [
        "Chenyu Ren",
        "Daihai He",
        "Jian Huang"
      ],
      "published": "2025-03-27T03:25:21Z",
      "updated": "2025-03-27T03:25:21Z",
      "categories": [
        "stat.AP"
      ],
      "pdf_url": "https://arxiv.org/pdf/2503.21123v1",
      "landing_url": "https://arxiv.org/abs/2503.21123v1",
      "doi": "https://doi.org/10.48550/arXiv.2503.21123"
    },
    "queries": [
      {
        "anchor": "synchronous dialogue",
        "search_term": "data scarcity"
      }
    ]
  },
  {
    "arxiv_id": "2503.22275",
    "anchor": "synchronous dialogue",
    "search_term": "discrete tokens",
    "search_record": {
      "id": "http://arxiv.org/abs/2503.22275v1",
      "title": "Make Some Noise: Towards LLM audio reasoning and generation using sound tokens",
      "summary": "Integrating audio comprehension and generation into large language models (LLMs) remains challenging due to the continuous nature of audio and the resulting high sampling rates. Here, we introduce a novel approach that combines Variational Quantization with Conditional Flow Matching to convert audio into ultra-low bitrate discrete tokens of 0.23kpbs, allowing for seamless integration with text tokens in LLMs. We fine-tuned a pretrained text-based LLM using Low-Rank Adaptation (LoRA) to assess its effectiveness in achieving true multimodal capabilities, i.e., audio comprehension and generation. Our tokenizer outperforms a traditional VQ-VAE across various datasets with diverse acoustic events. Despite the substantial loss of fine-grained details through audio tokenization, our multimodal LLM trained with discrete tokens achieves competitive results in audio comprehension with state-of-the-art methods, though audio generation is poor. Our results highlight the need for larger, more diverse datasets and improved evaluation metrics to advance multimodal LLM performance.",
      "published": "2025-03-28T09:43:47Z"
    },
    "metadata": {
      "arxiv_id": "2503.22275",
      "title": "Make Some Noise: Towards LLM audio reasoning and generation using sound tokens",
      "summary": "Integrating audio comprehension and generation into large language models (LLMs) remains challenging due to the continuous nature of audio and the resulting high sampling rates. Here, we introduce a novel approach that combines Variational Quantization with Conditional Flow Matching to convert audio into ultra-low bitrate discrete tokens of 0.23kpbs, allowing for seamless integration with text tokens in LLMs. We fine-tuned a pretrained text-based LLM using Low-Rank Adaptation (LoRA) to assess its effectiveness in achieving true multimodal capabilities, i.e., audio comprehension and generation. Our tokenizer outperforms a traditional VQ-VAE across various datasets with diverse acoustic events. Despite the substantial loss of fine-grained details through audio tokenization, our multimodal LLM trained with discrete tokens achieves competitive results in audio comprehension with state-of-the-art methods, though audio generation is poor. Our results highlight the need for larger, more diverse datasets and improved evaluation metrics to advance multimodal LLM performance.",
      "authors": [
        "Shivam Mehta",
        "Nebojsa Jojic",
        "Hannes Gamper"
      ],
      "published": "2025-03-28T09:43:47Z",
      "updated": "2025-03-28T09:43:47Z",
      "categories": [
        "eess.AS",
        "cs.AI",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2503.22275v1",
      "landing_url": "https://arxiv.org/abs/2503.22275v1",
      "doi": "https://doi.org/10.1109/ICASSP49660.2025.10888809"
    },
    "queries": [
      {
        "anchor": "synchronous dialogue",
        "search_term": "discrete tokens"
      },
      {
        "anchor": "synchronous dialogue",
        "search_term": "audio tokens"
      }
    ]
  },
  {
    "arxiv_id": "2503.23185",
    "anchor": "synchronous dialogue",
    "search_term": "real-time interaction",
    "search_record": {
      "id": "http://arxiv.org/abs/2503.23185v2",
      "title": "Real-time Video Prediction With Fast Video Interpolation Model and Prediction Training",
      "summary": "Transmission latency significantly affects users' quality of experience in real-time interaction and actuation. As latency is principally inevitable, video prediction can be utilized to mitigate the latency and ultimately enable zero-latency transmission. However, most of the existing video prediction methods are computationally expensive and impractical for real-time applications. In this work, we therefore propose real-time video prediction towards the zero-latency interaction over networks, called IFRVP (Intermediate Feature Refinement Video Prediction). Firstly, we propose three training methods for video prediction that extend frame interpolation models, where we utilize a simple convolution-only frame interpolation network based on IFRNet. Secondly, we introduce ELAN-based residual blocks into the prediction models to improve both inference speed and accuracy. Our evaluations show that our proposed models perform efficiently and achieve the best trade-off between prediction accuracy and computational speed among the existing video prediction methods. A demonstration movie is also provided at http://bit.ly/IFRVPDemo. The code will be released at https://github.com/FykAikawa/IFRVP.",
      "published": "2025-03-29T18:48:46Z"
    },
    "metadata": {
      "arxiv_id": "2503.23185",
      "title": "Real-time Video Prediction With Fast Video Interpolation Model and Prediction Training",
      "summary": "Transmission latency significantly affects users' quality of experience in real-time interaction and actuation. As latency is principally inevitable, video prediction can be utilized to mitigate the latency and ultimately enable zero-latency transmission. However, most of the existing video prediction methods are computationally expensive and impractical for real-time applications. In this work, we therefore propose real-time video prediction towards the zero-latency interaction over networks, called IFRVP (Intermediate Feature Refinement Video Prediction). Firstly, we propose three training methods for video prediction that extend frame interpolation models, where we utilize a simple convolution-only frame interpolation network based on IFRNet. Secondly, we introduce ELAN-based residual blocks into the prediction models to improve both inference speed and accuracy. Our evaluations show that our proposed models perform efficiently and achieve the best trade-off between prediction accuracy and computational speed among the existing video prediction methods. A demonstration movie is also provided at http://bit.ly/IFRVPDemo. The code will be released at https://github.com/FykAikawa/IFRVP.",
      "authors": [
        "Shota Hirose",
        "Kazuki Kotoyori",
        "Kasidis Arunruangsirilert",
        "Fangzheng Lin",
        "Heming Sun",
        "Jiro Katto"
      ],
      "published": "2025-03-29T18:48:46Z",
      "updated": "2025-04-04T14:29:20Z",
      "categories": [
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2503.23185v2",
      "landing_url": "https://arxiv.org/abs/2503.23185v2",
      "doi": "https://doi.org/10.48550/arXiv.2503.23185"
    },
    "queries": [
      {
        "anchor": "synchronous dialogue",
        "search_term": "real-time interaction"
      }
    ]
  },
  {
    "arxiv_id": "2503.23725",
    "anchor": "spoken language models",
    "search_term": "temporal dynamics",
    "search_record": {
      "id": "http://arxiv.org/abs/2503.23725v1",
      "title": "Exploring Temporal Dynamics in Event-based Eye Tracker",
      "summary": "Eye-tracking is a vital technology for human-computer interaction, especially in wearable devices such as AR, VR, and XR. The realization of high-speed and high-precision eye-tracking using frame-based image sensors is constrained by their limited temporal resolution, which impairs the accurate capture of rapid ocular dynamics, such as saccades and blinks. Event cameras, inspired by biological vision systems, are capable of perceiving eye movements with extremely low power consumption and ultra-high temporal resolution. This makes them a promising solution for achieving high-speed, high-precision tracking with rich temporal dynamics. In this paper, we propose TDTracker, an effective eye-tracking framework that captures rapid eye movements by thoroughly modeling temporal dynamics from both implicit and explicit perspectives. TDTracker utilizes 3D convolutional neural networks to capture implicit short-term temporal dynamics and employs a cascaded structure consisting of a Frequency-aware Module, GRU, and Mamba to extract explicit long-term temporal dynamics. Ultimately, a prediction heatmap is used for eye coordinate regression. Experimental results demonstrate that TDTracker achieves state-of-the-art (SOTA) performance on the synthetic SEET dataset and secured Third place in the CVPR event-based eye-tracking challenge 2025. Our code is available at https://github.com/rhwxmx/TDTracker.",
      "published": "2025-03-31T04:57:13Z"
    },
    "metadata": {
      "arxiv_id": "2503.23725",
      "title": "Exploring Temporal Dynamics in Event-based Eye Tracker",
      "summary": "Eye-tracking is a vital technology for human-computer interaction, especially in wearable devices such as AR, VR, and XR. The realization of high-speed and high-precision eye-tracking using frame-based image sensors is constrained by their limited temporal resolution, which impairs the accurate capture of rapid ocular dynamics, such as saccades and blinks. Event cameras, inspired by biological vision systems, are capable of perceiving eye movements with extremely low power consumption and ultra-high temporal resolution. This makes them a promising solution for achieving high-speed, high-precision tracking with rich temporal dynamics. In this paper, we propose TDTracker, an effective eye-tracking framework that captures rapid eye movements by thoroughly modeling temporal dynamics from both implicit and explicit perspectives. TDTracker utilizes 3D convolutional neural networks to capture implicit short-term temporal dynamics and employs a cascaded structure consisting of a Frequency-aware Module, GRU, and Mamba to extract explicit long-term temporal dynamics. Ultimately, a prediction heatmap is used for eye coordinate regression. Experimental results demonstrate that TDTracker achieves state-of-the-art (SOTA) performance on the synthetic SEET dataset and secured Third place in the CVPR event-based eye-tracking challenge 2025. Our code is available at https://github.com/rhwxmx/TDTracker.",
      "authors": [
        "Hongwei Ren",
        "Xiaopeng Lin",
        "Hongxiang Huang",
        "Yue Zhou",
        "Bojun Cheng"
      ],
      "published": "2025-03-31T04:57:13Z",
      "updated": "2025-03-31T04:57:13Z",
      "categories": [
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2503.23725v1",
      "landing_url": "https://arxiv.org/abs/2503.23725v1",
      "doi": "https://doi.org/10.48550/arXiv.2503.23725"
    },
    "queries": [
      {
        "anchor": "spoken language models",
        "search_term": "temporal dynamics"
      },
      {
        "anchor": "synchronous dialogue",
        "search_term": "temporal dynamics"
      },
      {
        "anchor": "turn-taking",
        "search_term": "temporal dynamics"
      }
    ]
  },
  {
    "arxiv_id": "2504.00711",
    "anchor": "spoken language models",
    "search_term": "semantic coherence",
    "search_record": {
      "id": "http://arxiv.org/abs/2504.00711v2",
      "title": "GraphMaster: Automated Graph Synthesis via LLM Agents in Data-Limited Environments",
      "summary": "The era of foundation models has revolutionized AI research, yet Graph Foundation Models (GFMs) remain constrained by the scarcity of large-scale graph corpora. Traditional graph data synthesis techniques primarily focus on simplistic structural operations, lacking the capacity to generate semantically rich nodes with meaningful textual attributes: a critical limitation for real-world applications. While large language models (LLMs) demonstrate exceptional text generation capabilities, their direct application to graph synthesis is impeded by context window limitations, hallucination phenomena, and structural consistency challenges. To address these issues, we introduce GraphMaster, the first multi-agent framework specifically designed for graph data synthesis in data-limited environments. GraphMaster orchestrates four specialized LLM agents (Manager, Perception, Enhancement, and Evaluation) that collaboratively optimize the synthesis process through iterative refinement, ensuring both semantic coherence and structural integrity. To rigorously evaluate our approach, we create new data-limited \"Sub\" variants of six standard graph benchmarks, specifically designed to test synthesis capabilities under realistic constraints. Additionally, we develop a novel interpretability assessment framework that combines human evaluation with a principled Grassmannian manifold-based analysis, providing both qualitative and quantitative measures of semantic coherence. Experimental results demonstrate that GraphMaster significantly outperforms traditional synthesis methods across multiple datasets, establishing a strong foundation for advancing GFMs in data-scarce environments.",
      "published": "2025-04-01T12:21:50Z"
    },
    "metadata": {
      "arxiv_id": "2504.00711",
      "title": "GraphMaster: Automated Graph Synthesis via LLM Agents in Data-Limited Environments",
      "summary": "The era of foundation models has revolutionized AI research, yet Graph Foundation Models (GFMs) remain constrained by the scarcity of large-scale graph corpora. Traditional graph data synthesis techniques primarily focus on simplistic structural operations, lacking the capacity to generate semantically rich nodes with meaningful textual attributes: a critical limitation for real-world applications. While large language models (LLMs) demonstrate exceptional text generation capabilities, their direct application to graph synthesis is impeded by context window limitations, hallucination phenomena, and structural consistency challenges. To address these issues, we introduce GraphMaster, the first multi-agent framework specifically designed for graph data synthesis in data-limited environments. GraphMaster orchestrates four specialized LLM agents (Manager, Perception, Enhancement, and Evaluation) that collaboratively optimize the synthesis process through iterative refinement, ensuring both semantic coherence and structural integrity. To rigorously evaluate our approach, we create new data-limited \"Sub\" variants of six standard graph benchmarks, specifically designed to test synthesis capabilities under realistic constraints. Additionally, we develop a novel interpretability assessment framework that combines human evaluation with a principled Grassmannian manifold-based analysis, providing both qualitative and quantitative measures of semantic coherence. Experimental results demonstrate that GraphMaster significantly outperforms traditional synthesis methods across multiple datasets, establishing a strong foundation for advancing GFMs in data-scarce environments.",
      "authors": [
        "Enjun Du",
        "Xunkai Li",
        "Tian Jin",
        "Zhihan Zhang",
        "Rong-Hua Li",
        "Guoren Wang"
      ],
      "published": "2025-04-01T12:21:50Z",
      "updated": "2025-05-05T13:57:12Z",
      "categories": [
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2504.00711v2",
      "landing_url": "https://arxiv.org/abs/2504.00711v2",
      "doi": "https://doi.org/10.48550/arXiv.2504.00711"
    },
    "queries": [
      {
        "anchor": "spoken language models",
        "search_term": "semantic coherence"
      },
      {
        "anchor": "synchronous dialogue",
        "search_term": "semantic coherence"
      }
    ]
  },
  {
    "arxiv_id": "2504.01523",
    "anchor": "spoken language models",
    "search_term": "data scarcity",
    "search_record": {
      "id": "http://arxiv.org/abs/2504.01523v1",
      "title": "Adapting Knowledge Prompt Tuning for Enhanced Automated Program Repair",
      "summary": "Automated Program Repair (APR) aims to enhance software reliability by automatically generating bug-fixing patches. Recent work has improved the state-of-the-art of APR by fine-tuning pre-trained large language models (LLMs), such as CodeT5, for APR. However, the effectiveness of fine-tuning becomes weakened in data scarcity scenarios, and data scarcity can be a common issue in practice, limiting fine-tuning performance. To alleviate this limitation, this paper adapts prompt tuning for enhanced APR and conducts a comprehensive study to evaluate its effectiveness in data scarcity scenarios, using three LLMs of different sizes and six diverse datasets across four programming languages. Prompt tuning rewrites the input to a model by adding extra prompt tokens and tunes both the model and the prompts on a small dataset. These tokens provide task-specific knowledge that can improve the model for APR, which is especially critical in data scarcity scenarios. Moreover, domain knowledge has proven crucial in many code intelligence tasks, but existing studies fail to leverage domain knowledge during the prompt tuning for APR. To close this gap, we introduce knowledge prompt tuning, an approach that adapts prompt tuning with six distinct types of code- or bug-related domain knowledge for APR. Our work, to the best of our knowledge, is the first to adapt and evaluate prompt tuning and the effectiveness of code- or bug-related domain knowledge for APR, particularly under data scarcity settings. Our evaluation results demonstrate that prompt tuning with knowledge generally outperforms fine-tuning under various experimental settings, achieving an average improvement of 87.33% over fine-tuning in data scarcity scenarios.",
      "published": "2025-04-02T09:10:02Z"
    },
    "metadata": {
      "arxiv_id": "2504.01523",
      "title": "Adapting Knowledge Prompt Tuning for Enhanced Automated Program Repair",
      "summary": "Automated Program Repair (APR) aims to enhance software reliability by automatically generating bug-fixing patches. Recent work has improved the state-of-the-art of APR by fine-tuning pre-trained large language models (LLMs), such as CodeT5, for APR. However, the effectiveness of fine-tuning becomes weakened in data scarcity scenarios, and data scarcity can be a common issue in practice, limiting fine-tuning performance. To alleviate this limitation, this paper adapts prompt tuning for enhanced APR and conducts a comprehensive study to evaluate its effectiveness in data scarcity scenarios, using three LLMs of different sizes and six diverse datasets across four programming languages. Prompt tuning rewrites the input to a model by adding extra prompt tokens and tunes both the model and the prompts on a small dataset. These tokens provide task-specific knowledge that can improve the model for APR, which is especially critical in data scarcity scenarios. Moreover, domain knowledge has proven crucial in many code intelligence tasks, but existing studies fail to leverage domain knowledge during the prompt tuning for APR. To close this gap, we introduce knowledge prompt tuning, an approach that adapts prompt tuning with six distinct types of code- or bug-related domain knowledge for APR. Our work, to the best of our knowledge, is the first to adapt and evaluate prompt tuning and the effectiveness of code- or bug-related domain knowledge for APR, particularly under data scarcity settings. Our evaluation results demonstrate that prompt tuning with knowledge generally outperforms fine-tuning under various experimental settings, achieving an average improvement of 87.33% over fine-tuning in data scarcity scenarios.",
      "authors": [
        "Xuemeng Cai",
        "Lingxiao Jiang"
      ],
      "published": "2025-04-02T09:10:02Z",
      "updated": "2025-04-02T09:10:02Z",
      "categories": [
        "cs.SE"
      ],
      "pdf_url": "https://arxiv.org/pdf/2504.01523v1",
      "landing_url": "https://arxiv.org/abs/2504.01523v1",
      "doi": "https://doi.org/10.48550/arXiv.2504.01523"
    },
    "queries": [
      {
        "anchor": "spoken language models",
        "search_term": "data scarcity"
      },
      {
        "anchor": "synchronous dialogue",
        "search_term": "data scarcity"
      }
    ]
  },
  {
    "arxiv_id": "2504.02386",
    "anchor": "spoken language models",
    "search_term": "neural codec",
    "search_record": {
      "id": "http://arxiv.org/abs/2504.02386v1",
      "title": "VoiceCraft-Dub: Automated Video Dubbing with Neural Codec Language Models",
      "summary": "We present VoiceCraft-Dub, a novel approach for automated video dubbing that synthesizes high-quality speech from text and facial cues. This task has broad applications in filmmaking, multimedia creation, and assisting voice-impaired individuals. Building on the success of Neural Codec Language Models (NCLMs) for speech synthesis, our method extends their capabilities by incorporating video features, ensuring that synthesized speech is time-synchronized and expressively aligned with facial movements while preserving natural prosody. To inject visual cues, we design adapters to align facial features with the NCLM token space and introduce audio-visual fusion layers to merge audio-visual information within the NCLM framework. Additionally, we curate CelebV-Dub, a new dataset of expressive, real-world videos specifically designed for automated video dubbing. Extensive experiments show that our model achieves high-quality, intelligible, and natural speech synthesis with accurate lip synchronization, outperforming existing methods in human perception and performing favorably in objective evaluations. We also adapt VoiceCraft-Dub for the video-to-speech task, demonstrating its versatility for various applications.",
      "published": "2025-04-03T08:24:47Z"
    },
    "metadata": {
      "arxiv_id": "2504.02386",
      "title": "VoiceCraft-Dub: Automated Video Dubbing with Neural Codec Language Models",
      "summary": "We present VoiceCraft-Dub, a novel approach for automated video dubbing that synthesizes high-quality speech from text and facial cues. This task has broad applications in filmmaking, multimedia creation, and assisting voice-impaired individuals. Building on the success of Neural Codec Language Models (NCLMs) for speech synthesis, our method extends their capabilities by incorporating video features, ensuring that synthesized speech is time-synchronized and expressively aligned with facial movements while preserving natural prosody. To inject visual cues, we design adapters to align facial features with the NCLM token space and introduce audio-visual fusion layers to merge audio-visual information within the NCLM framework. Additionally, we curate CelebV-Dub, a new dataset of expressive, real-world videos specifically designed for automated video dubbing. Extensive experiments show that our model achieves high-quality, intelligible, and natural speech synthesis with accurate lip synchronization, outperforming existing methods in human perception and performing favorably in objective evaluations. We also adapt VoiceCraft-Dub for the video-to-speech task, demonstrating its versatility for various applications.",
      "authors": [
        "Kim Sung-Bin",
        "Jeongsoo Choi",
        "Puyuan Peng",
        "Joon Son Chung",
        "Tae-Hyun Oh",
        "David Harwath"
      ],
      "published": "2025-04-03T08:24:47Z",
      "updated": "2025-04-03T08:24:47Z",
      "categories": [
        "cs.CV",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2504.02386v1",
      "landing_url": "https://arxiv.org/abs/2504.02386v1",
      "doi": "https://doi.org/10.48550/arXiv.2504.02386"
    },
    "queries": [
      {
        "anchor": "spoken language models",
        "search_term": "neural codec"
      },
      {
        "anchor": "synchronous dialogue",
        "search_term": "neural codec"
      },
      {
        "anchor": "turn-taking",
        "search_term": "neural codec"
      }
    ]
  },
  {
    "arxiv_id": "2504.03221",
    "anchor": "synchronous dialogue",
    "search_term": "temporal dynamics",
    "search_record": {
      "id": "http://arxiv.org/abs/2504.03221v1",
      "title": "Electromyography-Based Gesture Recognition: Hierarchical Feature Extraction for Enhanced Spatial-Temporal Dynamics",
      "summary": "Hand gesture recognition using multichannel surface electromyography (sEMG) is challenging due to unstable predictions and inefficient time-varying feature enhancement. To overcome the lack of signal based time-varying feature problems, we propose a lightweight squeeze-excitation deep learning-based multi stream spatial temporal dynamics time-varying feature extraction approach to build an effective sEMG-based hand gesture recognition system. Each branch of the proposed model was designed to extract hierarchical features, capturing both global and detailed spatial-temporal relationships to ensure feature effectiveness. The first branch, utilizing a Bidirectional-TCN (Bi-TCN), focuses on capturing long-term temporal dependencies by modelling past and future temporal contexts, providing a holistic view of gesture dynamics. The second branch, incorporating a 1D Convolutional layer, separable CNN, and Squeeze-and-Excitation (SE) block, efficiently extracts spatial-temporal features while emphasizing critical feature channels, enhancing feature relevance. The third branch, combining a Temporal Convolutional Network (TCN) and Bidirectional LSTM (BiLSTM), captures bidirectional temporal relationships and time-varying patterns. Outputs from all branches are fused using concatenation to capture subtle variations in the data and then refined with a channel attention module, selectively focusing on the most informative features while improving computational efficiency. The proposed model was tested on the Ninapro DB2, DB4, and DB5 datasets, achieving accuracy rates of 96.41%, 92.40%, and 93.34%, respectively. These results demonstrate the capability of the system to handle complex sEMG dynamics, offering advancements in prosthetic limb control and human-machine interface technologies with significant implications for assistive technologies.",
      "published": "2025-04-04T07:11:12Z"
    },
    "metadata": {
      "arxiv_id": "2504.03221",
      "title": "Electromyography-Based Gesture Recognition: Hierarchical Feature Extraction for Enhanced Spatial-Temporal Dynamics",
      "summary": "Hand gesture recognition using multichannel surface electromyography (sEMG) is challenging due to unstable predictions and inefficient time-varying feature enhancement. To overcome the lack of signal based time-varying feature problems, we propose a lightweight squeeze-excitation deep learning-based multi stream spatial temporal dynamics time-varying feature extraction approach to build an effective sEMG-based hand gesture recognition system. Each branch of the proposed model was designed to extract hierarchical features, capturing both global and detailed spatial-temporal relationships to ensure feature effectiveness. The first branch, utilizing a Bidirectional-TCN (Bi-TCN), focuses on capturing long-term temporal dependencies by modelling past and future temporal contexts, providing a holistic view of gesture dynamics. The second branch, incorporating a 1D Convolutional layer, separable CNN, and Squeeze-and-Excitation (SE) block, efficiently extracts spatial-temporal features while emphasizing critical feature channels, enhancing feature relevance. The third branch, combining a Temporal Convolutional Network (TCN) and Bidirectional LSTM (BiLSTM), captures bidirectional temporal relationships and time-varying patterns. Outputs from all branches are fused using concatenation to capture subtle variations in the data and then refined with a channel attention module, selectively focusing on the most informative features while improving computational efficiency. The proposed model was tested on the Ninapro DB2, DB4, and DB5 datasets, achieving accuracy rates of 96.41%, 92.40%, and 93.34%, respectively. These results demonstrate the capability of the system to handle complex sEMG dynamics, offering advancements in prosthetic limb control and human-machine interface technologies with significant implications for assistive technologies.",
      "authors": [
        "Jungpil Shin",
        "Abu Saleh Musa Miah",
        "Sota Konnai",
        "Shu Hoshitaka",
        "Pankoo Kim"
      ],
      "published": "2025-04-04T07:11:12Z",
      "updated": "2025-04-04T07:11:12Z",
      "categories": [
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2504.03221v1",
      "landing_url": "https://arxiv.org/abs/2504.03221v1",
      "doi": "https://doi.org/10.48550/arXiv.2504.03221"
    },
    "queries": [
      {
        "anchor": "synchronous dialogue",
        "search_term": "temporal dynamics"
      }
    ]
  },
  {
    "arxiv_id": "2504.03586",
    "anchor": "spoken language models",
    "search_term": "modular architecture",
    "search_record": {
      "id": "http://arxiv.org/abs/2504.03586v1",
      "title": "CAMINO: Cloud-native Autonomous Management and Intent-based Orchestrator",
      "summary": "This paper introduces CAMINO, a Cloud-native Autonomous Management and Intent-based Orchestrator designed to address the challenges of scalable, declarative, and cloud-native service management and orchestration. CAMINO leverages a modular architecture, the Configuration-as-Data (CaD) paradigm, and real-time resource monitoring to facilitate zero-touch provisioning across multi-edge infrastructure. By incorporating intent-driven orchestration and observability capabilities, CAMINO enables automated lifecycle management of network functions, ensuring optimized resource utilisation. The proposed solution abstracts complex configurations into high-level intents, offering a scalable approach to orchestrating services in distributed cloud-native infrastructures. This paper details CAMINO's system architecture, implementation, and key benefits, highlighting its effectiveness in cloud-native telecommunications environments.",
      "published": "2025-04-04T16:55:23Z"
    },
    "metadata": {
      "arxiv_id": "2504.03586",
      "title": "CAMINO: Cloud-native Autonomous Management and Intent-based Orchestrator",
      "summary": "This paper introduces CAMINO, a Cloud-native Autonomous Management and Intent-based Orchestrator designed to address the challenges of scalable, declarative, and cloud-native service management and orchestration. CAMINO leverages a modular architecture, the Configuration-as-Data (CaD) paradigm, and real-time resource monitoring to facilitate zero-touch provisioning across multi-edge infrastructure. By incorporating intent-driven orchestration and observability capabilities, CAMINO enables automated lifecycle management of network functions, ensuring optimized resource utilisation. The proposed solution abstracts complex configurations into high-level intents, offering a scalable approach to orchestrating services in distributed cloud-native infrastructures. This paper details CAMINO's system architecture, implementation, and key benefits, highlighting its effectiveness in cloud-native telecommunications environments.",
      "authors": [
        "Konstantinos Antonakoglou",
        "Ioannis Mavromatis",
        "Saptarshi Ghosh",
        "Mark Rouse",
        "Konstantinos Katsaros"
      ],
      "published": "2025-04-04T16:55:23Z",
      "updated": "2025-04-04T16:55:23Z",
      "categories": [
        "cs.NI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2504.03586v1",
      "landing_url": "https://arxiv.org/abs/2504.03586v1",
      "doi": "https://doi.org/10.48550/arXiv.2504.03586"
    },
    "queries": [
      {
        "anchor": "spoken language models",
        "search_term": "modular architecture"
      },
      {
        "anchor": "synchronous dialogue",
        "search_term": "modular architecture"
      },
      {
        "anchor": "turn-taking",
        "search_term": "modular architecture"
      }
    ]
  },
  {
    "arxiv_id": "2504.04710",
    "anchor": "spoken language models",
    "search_term": "synchronous data",
    "search_record": {
      "id": "http://arxiv.org/abs/2504.04710v1",
      "title": "TangibleNet: Synchronous Network Data Storytelling through Tangible Interactions in Augmented Reality",
      "summary": "Synchronous data-driven storytelling with network visualizations presents significant challenges due to the complexity of real-time manipulation of network components. While existing research addresses asynchronous scenarios, there is a lack of effective tools for live presentations. To address this gap, we developed TangibleNet, a projector-based AR prototype that allows presenters to interact with node-link diagrams using double-sided magnets during live presentations. The design process was informed by interviews with professionals experienced in synchronous data storytelling and workshops with 14 HCI/VIS researchers. Insights from the interviews helped identify key design considerations for integrating physical objects as interactive tools in presentation contexts. The workshops contributed to the development of a design space mapping user actions to interaction commands for node-link diagrams. Evaluation with 12 participants confirmed that TangibleNet supports intuitive interactions and enhances presenter autonomy, demonstrating its effectiveness for synchronous network-based data storytelling.",
      "published": "2025-04-07T03:46:46Z"
    },
    "metadata": {
      "arxiv_id": "2504.04710",
      "title": "TangibleNet: Synchronous Network Data Storytelling through Tangible Interactions in Augmented Reality",
      "summary": "Synchronous data-driven storytelling with network visualizations presents significant challenges due to the complexity of real-time manipulation of network components. While existing research addresses asynchronous scenarios, there is a lack of effective tools for live presentations. To address this gap, we developed TangibleNet, a projector-based AR prototype that allows presenters to interact with node-link diagrams using double-sided magnets during live presentations. The design process was informed by interviews with professionals experienced in synchronous data storytelling and workshops with 14 HCI/VIS researchers. Insights from the interviews helped identify key design considerations for integrating physical objects as interactive tools in presentation contexts. The workshops contributed to the development of a design space mapping user actions to interaction commands for node-link diagrams. Evaluation with 12 participants confirmed that TangibleNet supports intuitive interactions and enhances presenter autonomy, demonstrating its effectiveness for synchronous network-based data storytelling.",
      "authors": [
        "Kentaro Takahira",
        "Wong Kam-Kwai",
        "Leni Yang",
        "Xian Xu",
        "Takanori Fujiwara",
        "Huamin Qu"
      ],
      "published": "2025-04-07T03:46:46Z",
      "updated": "2025-04-07T03:46:46Z",
      "categories": [
        "cs.HC",
        "cs.GR"
      ],
      "pdf_url": "https://arxiv.org/pdf/2504.04710v1",
      "landing_url": "https://arxiv.org/abs/2504.04710v1",
      "doi": "https://doi.org/10.1145/3706598.3714265"
    },
    "queries": [
      {
        "anchor": "spoken language models",
        "search_term": "synchronous data"
      },
      {
        "anchor": "synchronous dialogue",
        "search_term": "synchronous data"
      },
      {
        "anchor": "turn-taking",
        "search_term": "synchronous data"
      }
    ]
  },
  {
    "arxiv_id": "2504.05217",
    "anchor": "synchronous dialogue",
    "search_term": "real-time interaction",
    "search_record": {
      "id": "http://arxiv.org/abs/2504.05217v1",
      "title": "LLM-Alignment Live-Streaming Recommendation",
      "summary": "In recent years, integrated short-video and live-streaming platforms have gained massive global adoption, offering dynamic content creation and consumption. Unlike pre-recorded short videos, live-streaming enables real-time interaction between authors and users, fostering deeper engagement. However, this dynamic nature introduces a critical challenge for recommendation systems (RecSys): the same live-streaming vastly different experiences depending on when a user watching. To optimize recommendations, a RecSys must accurately interpret the real-time semantics of live content and align them with user preferences.",
      "published": "2025-04-07T16:04:00Z"
    },
    "metadata": {
      "arxiv_id": "2504.05217",
      "title": "LLM-Alignment Live-Streaming Recommendation",
      "summary": "In recent years, integrated short-video and live-streaming platforms have gained massive global adoption, offering dynamic content creation and consumption. Unlike pre-recorded short videos, live-streaming enables real-time interaction between authors and users, fostering deeper engagement. However, this dynamic nature introduces a critical challenge for recommendation systems (RecSys): the same live-streaming vastly different experiences depending on when a user watching. To optimize recommendations, a RecSys must accurately interpret the real-time semantics of live content and align them with user preferences.",
      "authors": [
        "Yueyang Liu",
        "Jiangxia Cao",
        "Shen Wang",
        "Shuang Wen",
        "Xiang Chen",
        "Xiangyu Wu",
        "Shuang Yang",
        "Zhaojie Liu",
        "Kun Gai",
        "Guorui Zhou"
      ],
      "published": "2025-04-07T16:04:00Z",
      "updated": "2025-04-07T16:04:00Z",
      "categories": [
        "cs.IR"
      ],
      "pdf_url": "https://arxiv.org/pdf/2504.05217v1",
      "landing_url": "https://arxiv.org/abs/2504.05217v1",
      "doi": "https://doi.org/10.48550/arXiv.2504.05217"
    },
    "queries": [
      {
        "anchor": "synchronous dialogue",
        "search_term": "real-time interaction"
      }
    ]
  },
  {
    "arxiv_id": "2504.06189",
    "anchor": "synchronous dialogue",
    "search_term": "real-time interaction",
    "search_record": {
      "id": "http://arxiv.org/abs/2504.06189v1",
      "title": "Accessible and Pedagogically-Grounded Explainability for Human-Robot Interaction: A Framework Based on UDL and Symbolic Interfaces",
      "summary": "This paper presents a novel framework for accessible and pedagogically-grounded robot explainability, designed to support human-robot interaction (HRI) with users who have diverse cognitive, communicative, or learning needs. We combine principles from Universal Design for Learning (UDL) and Universal Design (UD) with symbolic communication strategies to facilitate the alignment of mental models between humans and robots. Our approach employs Asterics Grid and ARASAAC pictograms as a multimodal, interpretable front-end, integrated with a lightweight HTTP-to-ROS 2 bridge that enables real-time interaction and explanation triggering. We emphasize that explainability is not a one-way function but a bidirectional process, where human understanding and robot transparency must co-evolve. We further argue that in educational or assistive contexts, the role of a human mediator (e.g., a teacher) may be essential to support shared understanding. We validate our framework with examples of multimodal explanation boards and discuss how it can be extended to different scenarios in education, assistive robotics, and inclusive AI.",
      "published": "2025-04-08T16:33:52Z"
    },
    "metadata": {
      "arxiv_id": "2504.06189",
      "title": "Accessible and Pedagogically-Grounded Explainability for Human-Robot Interaction: A Framework Based on UDL and Symbolic Interfaces",
      "summary": "This paper presents a novel framework for accessible and pedagogically-grounded robot explainability, designed to support human-robot interaction (HRI) with users who have diverse cognitive, communicative, or learning needs. We combine principles from Universal Design for Learning (UDL) and Universal Design (UD) with symbolic communication strategies to facilitate the alignment of mental models between humans and robots. Our approach employs Asterics Grid and ARASAAC pictograms as a multimodal, interpretable front-end, integrated with a lightweight HTTP-to-ROS 2 bridge that enables real-time interaction and explanation triggering. We emphasize that explainability is not a one-way function but a bidirectional process, where human understanding and robot transparency must co-evolve. We further argue that in educational or assistive contexts, the role of a human mediator (e.g., a teacher) may be essential to support shared understanding. We validate our framework with examples of multimodal explanation boards and discuss how it can be extended to different scenarios in education, assistive robotics, and inclusive AI.",
      "authors": [
        "Francisco J. Rodríguez Lera",
        "Raquel Fernández Hernández",
        "Sonia Lopez González",
        "Miguel Angel González-Santamarta",
        "Francisco Jesús Rodríguez Sedano",
        "Camino Fernandez Llamas"
      ],
      "published": "2025-04-08T16:33:52Z",
      "updated": "2025-04-08T16:33:52Z",
      "categories": [
        "cs.RO",
        "cs.HC"
      ],
      "pdf_url": "https://arxiv.org/pdf/2504.06189v1",
      "landing_url": "https://arxiv.org/abs/2504.06189v1",
      "doi": "https://doi.org/10.1109/RO-MAN63969.2025.11217773"
    },
    "queries": [
      {
        "anchor": "synchronous dialogue",
        "search_term": "real-time interaction"
      }
    ]
  },
  {
    "arxiv_id": "2504.06959",
    "anchor": "spoken language models",
    "search_term": "temporal dynamics",
    "search_record": {
      "id": "http://arxiv.org/abs/2504.06959v1",
      "title": "Temporal dynamics of GHz acoustic waves in chipscale phononic integrated circuits",
      "summary": "Phononic integrated circuits, which manipulate GHz-frequency acoustic fields in μm-scale waveguides, provide new degrees of freedom for routing and manipulation of microwaves in deeply sub-wavelength geometries with associated implications for chipscale sensing and signal processing. The combination of low propagation loss, long interaction lengths and slow speed of sound put together with the large measurement bandwidths and high frequency resolution available from modern vector network analyzers (VNA) makes it feasible to visualize the temporal dynamics of propagating acoustic fields in these devices and see the device in action. Two representative examples we discuss here are pulse circulation and ringdown in an acoustic microring resonator, and the observation of (parasitic) multipath interference effects in waveguide resonator geometries. In the absence of fast 3D acoustic field imaging modalities, such time domain reflectometry based methods provide the best alternative for mapping interface reflection and loss, which becomes increasingly critical as these devices start to scale in complexity.",
      "published": "2025-04-09T15:09:58Z"
    },
    "metadata": {
      "arxiv_id": "2504.06959",
      "title": "Temporal dynamics of GHz acoustic waves in chipscale phononic integrated circuits",
      "summary": "Phononic integrated circuits, which manipulate GHz-frequency acoustic fields in μm-scale waveguides, provide new degrees of freedom for routing and manipulation of microwaves in deeply sub-wavelength geometries with associated implications for chipscale sensing and signal processing. The combination of low propagation loss, long interaction lengths and slow speed of sound put together with the large measurement bandwidths and high frequency resolution available from modern vector network analyzers (VNA) makes it feasible to visualize the temporal dynamics of propagating acoustic fields in these devices and see the device in action. Two representative examples we discuss here are pulse circulation and ringdown in an acoustic microring resonator, and the observation of (parasitic) multipath interference effects in waveguide resonator geometries. In the absence of fast 3D acoustic field imaging modalities, such time domain reflectometry based methods provide the best alternative for mapping interface reflection and loss, which becomes increasingly critical as these devices start to scale in complexity.",
      "authors": [
        "A. Fahad Malik",
        "Mahmut Bicer",
        "Krishna C. Balram"
      ],
      "published": "2025-04-09T15:09:58Z",
      "updated": "2025-04-09T15:09:58Z",
      "categories": [
        "physics.app-ph",
        "physics.optics"
      ],
      "pdf_url": "https://arxiv.org/pdf/2504.06959v1",
      "landing_url": "https://arxiv.org/abs/2504.06959v1",
      "doi": "https://doi.org/10.1109/JMEMS.2025.3593384"
    },
    "queries": [
      {
        "anchor": "spoken language models",
        "search_term": "temporal dynamics"
      },
      {
        "anchor": "synchronous dialogue",
        "search_term": "temporal dynamics"
      },
      {
        "anchor": "turn-taking",
        "search_term": "temporal dynamics"
      }
    ]
  },
  {
    "arxiv_id": "2504.07053",
    "anchor": "spoken language models",
    "search_term": "true full-duplex",
    "search_record": {
      "id": "http://arxiv.org/abs/2504.07053v2",
      "title": "TASTE: Text-Aligned Speech Tokenization and Embedding for Spoken Language Modeling",
      "summary": "Recent efforts target spoken language models (SLMs) that not only listen but also speak for more natural human-LLM interaction. Joint speech-text modeling is a promising direction to achieve this. However, the effectiveness of recent speech tokens for joint modeling remains underexplored. To address this, we introduce Text-Aligned Speech Tokenization and Embedding (TASTE), a method that directly addresses the modality gap by aligning speech token with the corresponding text transcription during the tokenization stage. We propose a method that can achieve this through a attention-based aggregation mechanism and with speech reconstruction as the training objective. We conduct extensive experiments and show that TASTE can preserve essential paralinguistic information while dramatically reducing the token sequence length. With TASTE, we perform straightforward joint spoken language modeling by using Low-Rank Adaptation on the pre-trained text LLM. Experimental results show that TASTE-based SLMs perform comparable to previous work on SALMON and StoryCloze; while significantly outperform other pre-trained SLMs on speech continuation across subjective and objective evaluations. To our knowledge, TASTE is the first end-to-end approach that utilizes a reconstruction objective to automatically learn a text-aligned speech tokenization and embedding suitable for spoken language modeling. Our demo, code, and model are available at https://mtkresearch.github.io/TASTE-SpokenLM.github.io.",
      "published": "2025-04-09T17:14:33Z"
    },
    "metadata": {
      "arxiv_id": "2504.07053",
      "title": "TASTE: Text-Aligned Speech Tokenization and Embedding for Spoken Language Modeling",
      "summary": "Recent efforts target spoken language models (SLMs) that not only listen but also speak for more natural human-LLM interaction. Joint speech-text modeling is a promising direction to achieve this. However, the effectiveness of recent speech tokens for joint modeling remains underexplored. To address this, we introduce Text-Aligned Speech Tokenization and Embedding (TASTE), a method that directly addresses the modality gap by aligning speech token with the corresponding text transcription during the tokenization stage. We propose a method that can achieve this through a attention-based aggregation mechanism and with speech reconstruction as the training objective. We conduct extensive experiments and show that TASTE can preserve essential paralinguistic information while dramatically reducing the token sequence length. With TASTE, we perform straightforward joint spoken language modeling by using Low-Rank Adaptation on the pre-trained text LLM. Experimental results show that TASTE-based SLMs perform comparable to previous work on SALMON and StoryCloze; while significantly outperform other pre-trained SLMs on speech continuation across subjective and objective evaluations. To our knowledge, TASTE is the first end-to-end approach that utilizes a reconstruction objective to automatically learn a text-aligned speech tokenization and embedding suitable for spoken language modeling. Our demo, code, and model are available at https://mtkresearch.github.io/TASTE-SpokenLM.github.io.",
      "authors": [
        "Liang-Hsuan Tseng",
        "Yi-Chang Chen",
        "Kuan-Yi Lee",
        "Da-Shan Shiu",
        "Hung-yi Lee"
      ],
      "published": "2025-04-09T17:14:33Z",
      "updated": "2025-05-22T14:49:03Z",
      "categories": [
        "cs.CL",
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2504.07053v2",
      "landing_url": "https://arxiv.org/abs/2504.07053v2",
      "doi": "https://doi.org/10.48550/arXiv.2504.07053"
    },
    "queries": [
      {
        "anchor": "spoken language models",
        "search_term": "true full-duplex"
      },
      {
        "anchor": "spoken language models",
        "search_term": "pseudo full-duplex"
      },
      {
        "anchor": "spoken language models",
        "search_term": "half duplex"
      },
      {
        "anchor": "spoken language models",
        "search_term": "engineered synchronization"
      },
      {
        "anchor": "spoken language models",
        "search_term": "learned synchronization"
      },
      {
        "anchor": "spoken language models",
        "search_term": "modular architecture"
      },
      {
        "anchor": "spoken language models",
        "search_term": "streaming architecture"
      },
      {
        "anchor": "spoken language models",
        "search_term": "real-time interaction"
      },
      {
        "anchor": "spoken language models",
        "search_term": "low latency"
      },
      {
        "anchor": "spoken language models",
        "search_term": "synchronous data"
      },
      {
        "anchor": "spoken language models",
        "search_term": "data scarcity"
      },
      {
        "anchor": "spoken language models",
        "search_term": "multichannel dialogue"
      },
      {
        "anchor": "spoken language models",
        "search_term": "temporal dynamics"
      },
      {
        "anchor": "spoken language models",
        "search_term": "behavioral arbitration"
      },
      {
        "anchor": "spoken language models",
        "search_term": "semantic coherence"
      },
      {
        "anchor": "spoken language models",
        "search_term": "neural codec"
      },
      {
        "anchor": "spoken language models",
        "search_term": "discrete tokens"
      },
      {
        "anchor": "spoken language models",
        "search_term": "audio tokens"
      },
      {
        "anchor": "spoken language models",
        "search_term": "floor transfer offset"
      },
      {
        "anchor": "spoken language models",
        "search_term": "stop latency"
      },
      {
        "anchor": "spoken language models",
        "search_term": "interruption response delay"
      }
    ]
  },
  {
    "arxiv_id": "2504.08528",
    "anchor": "spoken language models",
    "search_term": "true full-duplex",
    "search_record": {
      "id": "http://arxiv.org/abs/2504.08528v1",
      "title": "On The Landscape of Spoken Language Models: A Comprehensive Survey",
      "summary": "The field of spoken language processing is undergoing a shift from training custom-built, task-specific models toward using and optimizing spoken language models (SLMs) which act as universal speech processing systems. This trend is similar to the progression toward universal language models that has taken place in the field of (text) natural language processing. SLMs include both \"pure\" language models of speech -- models of the distribution of tokenized speech sequences -- and models that combine speech encoders with text language models, often including both spoken and written input or output. Work in this area is very diverse, with a range of terminology and evaluation settings. This paper aims to contribute an improved understanding of SLMs via a unifying literature survey of recent work in the context of the evolution of the field. Our survey categorizes the work in this area by model architecture, training, and evaluation choices, and describes some key challenges and directions for future work.",
      "published": "2025-04-11T13:40:53Z"
    },
    "metadata": {
      "arxiv_id": "2504.08528",
      "title": "On The Landscape of Spoken Language Models: A Comprehensive Survey",
      "summary": "The field of spoken language processing is undergoing a shift from training custom-built, task-specific models toward using and optimizing spoken language models (SLMs) which act as universal speech processing systems. This trend is similar to the progression toward universal language models that has taken place in the field of (text) natural language processing. SLMs include both \"pure\" language models of speech -- models of the distribution of tokenized speech sequences -- and models that combine speech encoders with text language models, often including both spoken and written input or output. Work in this area is very diverse, with a range of terminology and evaluation settings. This paper aims to contribute an improved understanding of SLMs via a unifying literature survey of recent work in the context of the evolution of the field. Our survey categorizes the work in this area by model architecture, training, and evaluation choices, and describes some key challenges and directions for future work.",
      "authors": [
        "Siddhant Arora",
        "Kai-Wei Chang",
        "Chung-Ming Chien",
        "Yifan Peng",
        "Haibin Wu",
        "Yossi Adi",
        "Emmanuel Dupoux",
        "Hung-Yi Lee",
        "Karen Livescu",
        "Shinji Watanabe"
      ],
      "published": "2025-04-11T13:40:53Z",
      "updated": "2025-04-11T13:40:53Z",
      "categories": [
        "cs.CL",
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2504.08528v1",
      "landing_url": "https://arxiv.org/abs/2504.08528v1",
      "doi": "https://doi.org/10.48550/arXiv.2504.08528"
    },
    "queries": [
      {
        "anchor": "spoken language models",
        "search_term": "true full-duplex"
      },
      {
        "anchor": "spoken language models",
        "search_term": "pseudo full-duplex"
      },
      {
        "anchor": "spoken language models",
        "search_term": "half duplex"
      },
      {
        "anchor": "spoken language models",
        "search_term": "engineered synchronization"
      },
      {
        "anchor": "spoken language models",
        "search_term": "learned synchronization"
      },
      {
        "anchor": "spoken language models",
        "search_term": "modular architecture"
      },
      {
        "anchor": "spoken language models",
        "search_term": "streaming architecture"
      },
      {
        "anchor": "spoken language models",
        "search_term": "real-time interaction"
      },
      {
        "anchor": "spoken language models",
        "search_term": "low latency"
      },
      {
        "anchor": "spoken language models",
        "search_term": "synchronous data"
      },
      {
        "anchor": "spoken language models",
        "search_term": "data scarcity"
      },
      {
        "anchor": "spoken language models",
        "search_term": "multichannel dialogue"
      },
      {
        "anchor": "spoken language models",
        "search_term": "temporal dynamics"
      },
      {
        "anchor": "spoken language models",
        "search_term": "behavioral arbitration"
      },
      {
        "anchor": "spoken language models",
        "search_term": "semantic coherence"
      },
      {
        "anchor": "spoken language models",
        "search_term": "neural codec"
      },
      {
        "anchor": "spoken language models",
        "search_term": "discrete tokens"
      },
      {
        "anchor": "spoken language models",
        "search_term": "audio tokens"
      },
      {
        "anchor": "spoken language models",
        "search_term": "floor transfer offset"
      },
      {
        "anchor": "spoken language models",
        "search_term": "stop latency"
      },
      {
        "anchor": "spoken language models",
        "search_term": "interruption response delay"
      }
    ]
  },
  {
    "arxiv_id": "2504.08772",
    "anchor": "synchronous dialogue",
    "search_term": "real-time interaction",
    "search_record": {
      "id": "http://arxiv.org/abs/2504.08772v1",
      "title": "Reward Generation via Large Vision-Language Model in Offline Reinforcement Learning",
      "summary": "In offline reinforcement learning (RL), learning from fixed datasets presents a promising solution for domains where real-time interaction with the environment is expensive or risky. However, designing dense reward signals for offline dataset requires significant human effort and domain expertise. Reinforcement learning with human feedback (RLHF) has emerged as an alternative, but it remains costly due to the human-in-the-loop process, prompting interest in automated reward generation models. To address this, we propose Reward Generation via Large Vision-Language Models (RG-VLM), which leverages the reasoning capabilities of LVLMs to generate rewards from offline data without human involvement. RG-VLM improves generalization in long-horizon tasks and can be seamlessly integrated with the sparse reward signals to enhance task performance, demonstrating its potential as an auxiliary reward signal.",
      "published": "2025-04-03T07:11:18Z"
    },
    "metadata": {
      "arxiv_id": "2504.08772",
      "title": "Reward Generation via Large Vision-Language Model in Offline Reinforcement Learning",
      "summary": "In offline reinforcement learning (RL), learning from fixed datasets presents a promising solution for domains where real-time interaction with the environment is expensive or risky. However, designing dense reward signals for offline dataset requires significant human effort and domain expertise. Reinforcement learning with human feedback (RLHF) has emerged as an alternative, but it remains costly due to the human-in-the-loop process, prompting interest in automated reward generation models. To address this, we propose Reward Generation via Large Vision-Language Models (RG-VLM), which leverages the reasoning capabilities of LVLMs to generate rewards from offline data without human involvement. RG-VLM improves generalization in long-horizon tasks and can be seamlessly integrated with the sparse reward signals to enhance task performance, demonstrating its potential as an auxiliary reward signal.",
      "authors": [
        "Younghwan Lee",
        "Tung M. Luu",
        "Donghoon Lee",
        "Chang D. Yoo"
      ],
      "published": "2025-04-03T07:11:18Z",
      "updated": "2025-04-03T07:11:18Z",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2504.08772v1",
      "landing_url": "https://arxiv.org/abs/2504.08772v1",
      "doi": "https://doi.org/10.48550/arXiv.2504.08772"
    },
    "queries": [
      {
        "anchor": "synchronous dialogue",
        "search_term": "real-time interaction"
      }
    ]
  },
  {
    "arxiv_id": "2504.12318",
    "anchor": "spoken language models",
    "search_term": "modular architecture",
    "search_record": {
      "id": "http://arxiv.org/abs/2504.12318v1",
      "title": "AUTONAV: A Toolfor Autonomous Navigation of Robots",
      "summary": "We present a tool AUTONAV that automates the mapping, localization, and path-planning tasks for autonomous navigation of robots. The modular architecture allows easy integration of various algorithms for these tasks for comparison. We present the generated maps and path-plans by AUTONAV in indoor simulation scenarios.",
      "published": "2025-04-10T16:37:30Z"
    },
    "metadata": {
      "arxiv_id": "2504.12318",
      "title": "AUTONAV: A Toolfor Autonomous Navigation of Robots",
      "summary": "We present a tool AUTONAV that automates the mapping, localization, and path-planning tasks for autonomous navigation of robots. The modular architecture allows easy integration of various algorithms for these tasks for comparison. We present the generated maps and path-plans by AUTONAV in indoor simulation scenarios.",
      "authors": [
        "Mir Md Sajid Sarwar",
        "Sudip Samanta",
        "Rajarshi Ray"
      ],
      "published": "2025-04-10T16:37:30Z",
      "updated": "2025-04-10T16:37:30Z",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2504.12318v1",
      "landing_url": "https://arxiv.org/abs/2504.12318v1",
      "doi": "https://doi.org/10.48550/arXiv.2504.12318"
    },
    "queries": [
      {
        "anchor": "spoken language models",
        "search_term": "modular architecture"
      },
      {
        "anchor": "synchronous dialogue",
        "search_term": "modular architecture"
      }
    ]
  }
]