[
  {
    "role": "system",
    "content": "你是系統性回顧的資料整理助理，需將研究助理的筆記轉為結構化 JSON。\n僅能輸出單一 JSON 物件，勿加入額外敘述或 Markdown。"
  },
  {
    "role": "user",
    "content": "以下內容結合了兩種來源：\n1) PDF Background (Survey Summaries)：模型閱讀本地 PDF 後的背景整理，僅供提供更準確的主題定義與條件靈感，來源欄請勿引用非 https 連結。\n2) Web Search Notes：OpenAI Web Search 所產出的即時筆記與來源。\n請輸出最終 JSON，並確保所有 source 欄位皆為 https URL。\n主題：On The Landscape of Spoken Language Models: A Comprehensive Survey；時間範圍：過去3年。\ninclusion_criteria.required 段落僅能包含主題定義逐字條款、recency 對齊條款，以及英文可評估性條款；其餘條件請歸入 any_of 群組。\n將以下筆記整合後再輸出：\n---\n### PDF Background (Survey Summaries)\n### PDF Topic Definition\n本篇綜述聚焦於「全雙工口語語言模型（Full-Duplex Spoken Language Models, FD-SLMs）」的發展現況與挑戰，強調從傳統輪流發言（turn-taking）的對話系統，進化到能夠實現同步聽說、重疊語音與即時打斷的人機互動。文章明確區分了「工程式同步」（模組化架構）與「學習式同步」（端到端架構）兩大設計路線，並提出一套涵蓋「時序動態」、「行為仲裁」、「語意連貫」及「聲學表現」的統一評估框架。核心概念包括認知並行（cognitive parallelism）、實時性（sub-200ms latency）、語音重疊管理、主動打斷與回饋（backchanneling）等，並針對現有主流FD-SLM進行比較分析，揭示資料稀缺、架構分歧及評估標準碎片化等關鍵挑戰。\n\n### PDF Key Trends\n- 近年趨勢聚焦於從「偽全雙工」轉向「真全雙工」語言模型，實現同步聽說與自然對話流暢性。\n- 研究資料來源逐漸從單聲道語音、會議語料庫，擴展到多聲道、同步標註的自然對話資料（如AMI、ICSI、Fisher等）。\n- 主流研究角度包含：架構創新（如雙塔Transformer、FSM控制器）、資料增強（合成TTS、多語言資源）、即時性優化（KV-cache、流式編碼）及行為評估標準化。\n- 評估面向由傳統的語音辨識/理解，擴展至即時互動、語音重疊處理、語意與行為一致性等多維度。\n- 開放原始碼與標準化基準（如FD-Bench、Full-Duplex-Bench）促進模型間的可比性與可重現性。\n\n### PDF Capability Highlights\n- 必須具備同步語音編碼與生成的並行運算能力。\n- 支援低於200毫秒的反應延遲以符合自然對話節奏。\n- 能夠辨識與處理語音重疊、打斷、回饋等複雜對話事件。\n- 需在多聲道或多模態輸入下維持語意與語音的連貫性。\n- 具備彈性架構以支援外部或內部控制機制（如FSM、控制token）。\n\n### PDF Inclusion Signals\n- 關鍵字：「Full-Duplex」、「同步對話」、「認知並行」、「流式語音處理」、「行為仲裁」。\n- 涉及「Engineered Synchronization」或「Learned Synchronization」架構討論（參見第3章）。\n- 針對「時序動態」、「語意連貫」、「聲學表現」等多維度評估（參見第4章、Fig. 3）。\n- 涉及公開多聲道同步語音資料集或合成數據生成（參見4.2節、Table 3）。\n- 關注於「人機自然互動」及「主動對話行為」的模型設計與評測。\n\n### PDF Exclusion Signals\n- 僅討論傳統輪流發言（half-duplex）或單向語音理解/合成的系統。\n- 只聚焦於ASR、TTS等單一語音技術，未涵蓋對話交互與同步行為。\n- 評估指標僅限於語音辨識率（WER）或音質分數，未涉及即時互動與行為仲裁。\n- 研究資料僅來自單聲道、無同步標註的語音語料。\n- 未討論架構層級的同步策略或認知並行設計。\n\n### PDF Notes\n- FROM TURN-TAKING TO SYNCHRONOUS DIALOGUE.pdf: 系統性梳理全雙工口語語言模型的演進、架構分類與評估框架，強調認知並行、時序動態與多維度行為評測，並列舉現有主流模型與資料集，對資料瓶頸與評估碎片化提出具體展望。\n### Web Search Notes\n以下內容依你的要求整理，作為**可直接用於系統性回顧（systematic review / survey）之文獻收錄與排除規則草案**。所有敘述均以可操作、可評估為原則，並以已發表且具明確年月日（YYYY-MM-DD）的 https 一手論文頁為依據。\n\n---\n\n### Topic Definition\n\n本主題聚焦於 **Spoken Language Models（SLMs，口語語言模型）** 的整體研究版圖。SLMs 指的是以「語音序列」本身作為核心建模對象的語言模型，其目標並非僅產生或辨識文字，而是直接在語音層級進行建模、理解、生成與推理。這類模型通常能以端到端（end-to-end）的方式，從原始聲學訊號出發，完成語音理解（SLU）、語音翻譯（ST）、語音對話、或語音生成等高階語言任務。\n\n近年研究趨勢顯示，SLMs 正由傳統任務導向、模組化的 ASR→NLP 管線，轉向「通用化」與「基礎模型（foundation model）」範式。這包含純語音語言模型（如離散化語音 token 的 LM），以及結合語音編碼器與大型語言模型（LLM）的混合式架構，並強調跨任務泛化能力、預訓練策略、以及統一的評估視角。([arxiv.org](https://arxiv.org/abs/2504.08528?utm_source=openai))\n\n---\n\n### Summary\n\n整體而言，Spoken Language Models 的研究正快速朝向通用化、端到端與跨模態融合發展。研究亮點包含以大規模自監督預訓練建立語音基礎模型、將語音直接納入語言模型推理流程，以及以統一 benchmark 系統性評估模型能力。這些趨勢使語音不再只是文字處理的前處理步驟，而成為語言建模的第一等公民。([arxiv.org](https://arxiv.org/abs/2504.08528?utm_source=openai))\n\n---\n\n### Summary Topics\n\nS1: Spoken Language Models 的定義、分類與歷史演進  \nS2: SLM 的模型架構（純語音 LM、語音–文字混合、Speech–LLM）  \nS3: 預訓練、微調與評估基準（foundation models、SUPERB 等）  \nS4: 應用面向與挑戰（SLU、ST、對話、泛化與評估困難）\n\n---\n\n### Inclusion Criteria (Required)\n\n1. **主題定義：**研究必須明確以「Spoken Language Models 或等價概念」為核心，將語音視為主要建模與推理對象，而非僅作為文字處理前的輸入訊號；研究需討論模型架構、訓練或評估面向。  \n   英文可評估性條件：論文需在標題、摘要或關鍵詞中出現 *spoken language model(s)*、*speech language model(s)*、或等價描述（如 language modeling of speech）。  \n   source: https://arxiv.org/abs/2504.08528  \n   topic ids: S1, S2 ([arxiv.org](https://arxiv.org/abs/2504.08528?utm_source=openai))\n\n2. 論文必須為可公開取得之英文學術研究（期刊、會議或 arXiv），並提供完整方法描述與實驗或系統性分析。  \n   source: https://arxiv.org/abs/2106.09009  \n   topic ids: S1 ([arxiv.org](https://arxiv.org/abs/2106.09009?utm_source=openai))\n\n3. 研究需明確說明模型評估方式（任務設定、benchmark 或分析框架），而非僅概念性討論。  \n   source: https://arxiv.org/abs/2404.09385  \n   topic ids: S3 ([arxiv.org](https://arxiv.org/abs/2404.09385?utm_source=openai))\n\n---\n\n### Inclusion Criteria (Any-of Groups)\n\n- Group A：模型架構與建模方式  \n  * Option: 探討「純語音語言模型」（如以離散語音 token 進行 LM 建模）。  \n    source: https://arxiv.org/abs/2504.08528  \n    topic ids: S2 ([arxiv.org](https://arxiv.org/abs/2504.08528?utm_source=openai))  \n  * Option: 探討「語音–文字或語音–LLM 混合式架構」，強調端到端語音理解或生成。  \n    source: https://arxiv.org/abs/2010.12283  \n    topic ids: S2 ([arxiv.org](https://arxiv.org/abs/2010.12283?utm_source=openai))\n\n- Group B：訓練與評估範式  \n  * Option: 使用大規模自監督或 foundation model 預訓練，並分析其跨任務泛化能力。  \n    source: https://arxiv.org/abs/2404.09385  \n    topic ids: S3 ([arxiv.org](https://arxiv.org/abs/2404.09385?utm_source=openai))\n\n---\n\n### Exclusion Criteria\n\n- 僅將語音作為輸入，核心研究仍為文字 NLP，且未對語音建模本身進行分析或創新（如單純 ASR→LLM 串接）。  \n  source: internal  \n  topic ids: S1\n\n- 僅針對單一下游任務（如特定指令辨識），未討論通用性、模型架構或語音語言建模觀點。  \n  source: https://arxiv.org/abs/2106.09009  \n  topic ids: S4 ([arxiv.org](https://arxiv.org/abs/2106.09009?utm_source=openai))\n\n- 非學術性來源（部落格、產品白皮書、未公開技術報告），或缺乏完整方法與評估描述者。  \n  source: internal  \n  topic ids: S1\n\n---\n\n### Sources\n\nhttps://arxiv.org/abs/2504.08528  \nhttps://arxiv.org/abs/2404.09385  \nhttps://arxiv.org/abs/2106.09009  \nhttps://arxiv.org/abs/2010.12283\n---"
  }
]